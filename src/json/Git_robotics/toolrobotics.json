{"interestingcomments": [{"Unnamed: 0": 1029, "autor": 9, "date": null, "content": "Deploying Deep Learning\nWelcome to our instructional guide for inference and realtime DNN vision library for NVIDIA Jetson Nano/TX1/TX2/Xavier NX/AGX Xavier.\nThis repo uses NVIDIA TensorRT for efficiently deploying neural networks onto the embedded Jetson platform, improving performance and power efficiency using graph optimizations, kernel fusion, and FP16/INT8 precision.\nVision primitives, such as imageNet for image recognition, detectNet for object detection, segNet for semantic segmentation, and poseNet for pose estimation inherit from the shared tensorNet object. Examples are provided for streaming from live camera feed and processing images. See the API Reference section for detailed reference documentation of the C++ and Python libraries.\nFollow the Hello AI World tutorial for running inference and transfer learning onboard your Jetson, including collecting your own datasets and training your own models. It covers image classification, object detection, semantic segmentation, pose estimation, and mono depth.\nTable of Contents\nHello AI World\nVideo Walkthroughs\nAPI Reference\nCode Examples\nPre-Trained Models\nSystem Requirements\nChange Log\n> JetPack 4.6 is now supported, along with updated containers.\n> Try the new Pose Estimation and Mono Depth tutorials!\n> See the Change Log for the latest updates and new features.\nHello AI World\nHello AI World can be run completely onboard your Jetson, including inferencing with TensorRT and transfer learning with PyTorch. The inference portion of Hello AI World - which includes coding your own image classification and object detection applications for Python or C++, and live camera demos - can be run on your Jetson in roughly two hours or less, while transfer learning is best left to leave running overnight.\nSystem Setup\nSetting up Jetson with JetPack\nRunning the Docker Container\nBuilding the Project from Source\nInference\nClassifying Images with ImageNet\nUsing the ImageNet Program on Jetson\nCoding Your Own Image Recognition Program (Python)\nCoding Your Own Image Recognition Program (C++)\nRunning the Live Camera Recognition Demo\nLocating Objects with DetectNet\nDetecting Objects from Images\nRunning the Live Camera Detection Demo\nCoding Your Own Object Detection Program\nSemantic Segmentation with SegNet\nSegmenting Images from the Command Line\nRunning the Live Camera Segmentation Demo\nPose Estimation with PoseNet\nMonocular Depth with DepthNet\nTraining\nTransfer Learning with PyTorch\nClassification/Recognition (ResNet-18)\nRe-training on the Cat/Dog Dataset\nRe-training on the PlantCLEF Dataset\nCollecting your own Classification Datasets\nObject Detection (SSD-Mobilenet)\nRe-training SSD-Mobilenet\nCollecting your own Detection Datasets\nAppendix\nCamera Streaming and Multimedia\nImage Manipulation with CUDA\nDeep Learning Nodes for ROS/ROS2\nVideo Walkthroughs\nBelow are screencasts of Hello AI World that were recorded for the Jetson AI Certification course:\nDescription Video\nHello AI World Setup\nDownload and run the Hello AI World container on Jetson Nano, test your camera feed, and see how to stream it over the network via RTP.\nImage Classification Inference\nCode your own Python program for image classification using Jetson Nano and deep learning, then experiment with realtime classification on a live camera stream.\nTraining Image Classification Models\nLearn how to train image classification models with PyTorch onboard Jetson Nano, and collect your own classification datasets to create custom models.\nObject Detection Inference\nCode your own Python program for object detection using Jetson Nano and deep learning, then experiment with realtime detection on a live camera stream.\nTraining Object Detection Models\nLearn how to train object detection models with PyTorch onboard Jetson Nano, and collect your own detection datasets to create custom models.\nSemantic Segmentation\nExperiment with fully-convolutional semantic segmentation networks on Jetson Nano, and run realtime segmentation on a live camera stream.\nAPI Reference\nBelow are links to reference documentation for the C++ and Python libraries from the repo:\njetson-inference\nC++ Python\nImage Recognition imageNet imageNet\nObject Detection detectNet detectNet\nSegmentation segNet segNet\nPose Estimation poseNet poseNet\nMonocular Depth depthNet depthNet\njetson-utils\nC++\nPython\nThese libraries are able to be used in external projects by linking to libjetson-inference and libjetson-utils.\nCode Examples\nIntroductory code walkthroughs of using the library are covered during these steps of the Hello AI World tutorial:\nCoding Your Own Image Recognition Program (Python)\nCoding Your Own Image Recognition Program (C++)\nAdditional C++ and Python samples for running the networks on static images and live camera streams can be found here:\nC++ Python\nImage Recognition imagenet.cpp imagenet.py\nObject Detection detectnet.cpp detectnet.py\nSegmentation segnet.cpp segnet.py\nPose Estimation posenet.cpp posenet.py\nMonocular Depth depthnet.cpp depthnet.py\nnote: for working with numpy arrays, see Converting to Numpy Arrays and Converting from Numpy Arrays\nThese examples will automatically be compiled while Building the Project from Source, and are able to run the pre-trained models listed below in addition to custom models provided by the user. Launch each example with --help for usage info.\nPre-Trained Models\nThe project comes with a number of pre-trained models that are available through the Model Downloader tool:\nImage Recognition\nNetwork CLI argument NetworkType enum\nAlexNet alexnet ALEXNET\nGoogleNet googlenet GOOGLENET\nGoogleNet-12 googlenet-12 GOOGLENET_12\nResNet-18 resnet-18 RESNET_18\nResNet-50 resnet-50 RESNET_50\nResNet-101 resnet-101 RESNET_101\nResNet-152 resnet-152 RESNET_152\nVGG-16 vgg-16 VGG-16\nVGG-19 vgg-19 VGG-19\nInception-v4 inception-v4 INCEPTION_V4\nObject Detection\nNetwork CLI argument NetworkType enum Object classes\nSSD-Mobilenet-v1 ssd-mobilenet-v1 SSD_MOBILENET_V1 91 (COCO classes)\nSSD-Mobilenet-v2 ssd-mobilenet-v2 SSD_MOBILENET_V2 91 (COCO classes)\nSSD-Inception-v2 ssd-inception-v2 SSD_INCEPTION_V2 91 (COCO classes)\nDetectNet-COCO-Dog coco-dog COCO_DOG dogs\nDetectNet-COCO-Bottle coco-bottle COCO_BOTTLE bottles\nDetectNet-COCO-Chair coco-chair COCO_CHAIR chairs\nDetectNet-COCO-Airplane coco-airplane COCO_AIRPLANE airplanes\nped-100 pednet PEDNET pedestrians\nmultiped-500 multiped PEDNET_MULTI pedestrians, luggage\nfacenet-120 facenet FACENET faces\nSemantic Segmentation\nDataset Resolution CLI Argument Accuracy Jetson Nano Jetson Xavier\nCityscapes 512x256 fcn-resnet18-cityscapes-512x256 83.3% 48 FPS 480 FPS\nCityscapes 1024x512 fcn-resnet18-cityscapes-1024x512 87.3% 12 FPS 175 FPS\nCityscapes 2048x1024 fcn-resnet18-cityscapes-2048x1024 89.6% 3 FPS 47 FPS\nDeepScene 576x320 fcn-resnet18-deepscene-576x320 96.4% 26 FPS 360 FPS\nDeepScene 864x480 fcn-resnet18-deepscene-864x480 96.9% 14 FPS 190 FPS\nMulti-Human 512x320 fcn-resnet18-mhp-512x320 86.5% 34 FPS 370 FPS\nMulti-Human 640x360 fcn-resnet18-mhp-512x320 87.1% 23 FPS 325 FPS\nPascal VOC 320x320 fcn-resnet18-voc-320x320 85.9% 45 FPS 508 FPS\nPascal VOC 512x320 fcn-resnet18-voc-512x320 88.5% 34 FPS 375 FPS\nSUN RGB-D 512x400 fcn-resnet18-sun-512x400 64.3% 28 FPS 340 FPS\nSUN RGB-D 640x512 fcn-resnet18-sun-640x512 65.1% 17 FPS 224 FPS\nIf the resolution is omitted from the CLI argument, the lowest resolution model is loaded\nAccuracy indicates the pixel classification accuracy across the model's validation dataset\nPerformance is measured for GPU FP16 mode with JetPack 4.2.1, nvpmodel 0 (MAX-N)\nLegacy Segmentation Models\nPose Estimation\nModel CLI argument NetworkType enum Keypoints\nPose-ResNet18-Body resnet18-body RESNET18_BODY 18\nPose-ResNet18-Hand resnet18-hand RESNET18_HAND 21\nPose-DenseNet121-Body densenet121-body DENSENET121_BODY 18\nRecommended System Requirements\nJetson Nano Developer Kit with JetPack 4.2 or newer (Ubuntu 18.04 aarch64).\nJetson Nano 2GB Developer Kit with JetPack 4.4.1 or newer (Ubuntu 18.04 aarch64).\nJetson Xavier NX Developer Kit with JetPack 4.4 or newer (Ubuntu 18.04 aarch64).\nJetson AGX Xavier Developer Kit with JetPack 4.0 or newer (Ubuntu 18.04 aarch64).\nJetson TX2 Developer Kit with JetPack 3.0 or newer (Ubuntu 16.04 aarch64).\nJetson TX1 Developer Kit with JetPack 2.3 or newer (Ubuntu 16.04 aarch64).\nThe Transfer Learning with PyTorch section of the tutorial speaks from the perspective of running PyTorch onboard Jetson for training DNNs, however the same PyTorch code can be used on a PC, server, or cloud instance with an NVIDIA discrete GPU for faster training.\nExtra Resources\nIn this area, links and resources for deep learning are listed:\nros_deep_learning - TensorRT inference ROS nodes\nNVIDIA AI IoT - NVIDIA Jetson GitHub repositories\nJetson eLinux Wiki - Jetson eLinux Wiki\nTwo Days to a Demo (DIGITS)\nnote: the DIGITS/Caffe tutorial from below is deprecated. It's recommended to follow the Transfer Learning with PyTorch tutorial from Hello AI World.\nExpand this section to see original DIGITS tutorial (deprecated)\nThe DIGITS tutorial includes training DNN's in the cloud or PC, and inference on the Jetson with TensorRT, and can take roughly two days or more depending on system setup, downloading the datasets, and the training speed of your GPU.\n\u00a9 2016-2019 NVIDIA | Table of Contents", "link": "https://github.com/dusty-nv/jetson-inference", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "deploying deep learning\nwelcome to our instructional guide for inference and realtime dnn vision library for nvidia jetson nano/tx1/tx2/xavier nx/agx xavier.\nthis repo uses nvidia tensorrt for efficiently deploying neural networks onto the embedded jetson platform, improving performance and power efficiency using graph optimizations, kernel fusion, and fp16/int8 precision.\nvision primitives, such as imagenet for image recognition, detectnet for object detection, segnet for semantic segmentation, and posenet for pose estimation inherit from the shared tensornet object. examples are provided for streaming from live camera feed and processing images. see the api reference section for detailed reference documentation of the c++ and python libraries.\nfollow the hello ai world tutorial for running inference and transfer learning onboard your jetson, including collecting your own datasets and training your own models. it covers image classification, object detection, semantic segmentation, pose estimation, and mono depth.\ntable of contents\nhello ai world\nvideo walkthroughs\napi reference\ncode examples\npre-trained models\nsystem requirements\nchange log\n> jetpack 4.6 is now supported, along with updated containers.\n> try the new pose estimation and mono depth tutorials!\n> see the change log for the latest updates and new features.\nhello ai world\nhello ai world can be run completely onboard your jetson, including inferencing with tensorrt and transfer learning with pytorch. the inference portion of hello ai world - which includes coding your own image classification and object detection applications for python or c++, and live camera demos - can be run on your jetson in roughly two hours or less, while transfer learning is best left to leave running overnight.\nsystem setup\nsetting up jetson with jetpack\nrunning the docker container\nbuilding the project from source\ninference\nclassifying images with imagenet\nusing the imagenet program on jetson\ncoding your own image recognition program (python)\ncoding your own image recognition program (c++)\nrunning the live camera recognition demo\nlocating objects with detectnet\ndetecting objects from images\nrunning the live camera detection demo\ncoding your own object detection program\nsemantic segmentation with segnet\nsegmenting images from the command line\nrunning the live camera segmentation demo\npose estimation with posenet\nmonocular depth with depthnet\ntraining\ntransfer learning with pytorch\nclassification/recognition (resnet-18)\nre-training on the cat/dog dataset\nre-training on the plantclef dataset\ncollecting your own classification datasets\nobject detection (ssd-mobilenet)\nre-training ssd-mobilenet\ncollecting your own detection datasets\nappendix\ncamera streaming and multimedia\nimage manipulation with cuda\ndeep learning nodes for ros/ros2\nvideo walkthroughs\nbelow are screencasts of hello ai world that were recorded for the jetson ai certification course:\ndescription video\nhello ai world setup\ndownload and run the hello ai world container on jetson nano, test your camera feed, and see how to stream it over the network via rtp.\nimage classification inference\ncode your own python program for image classification using jetson nano and deep learning, then experiment with realtime classification on a live camera stream.\ntraining image classification models\nlearn how to train image classification models with pytorch onboard jetson nano, and collect your own classification datasets to create custom models.\nobject detection inference\ncode your own python program for object detection using jetson nano and deep learning, then experiment with realtime detection on a live camera stream.\ntraining object detection models\nlearn how to train object detection models with pytorch onboard jetson nano, and collect your own detection datasets to create custom models.\nsemantic segmentation\nexperiment with fully-convolutional semantic segmentation networks on jetson nano, and run realtime segmentation on a live camera stream.\napi reference\nbelow are links to reference documentation for the c++ and python libraries from the repo:\njetson-inference\nc++ python\nimage recognition imagenet imagenet\nobject detection detectnet detectnet\nsegmentation segnet segnet\npose estimation posenet posenet\nmonocular depth depthnet depthnet\njetson-utils\nc++\npython\nthese libraries are able to be used in external projects by linking to libjetson-inference and libjetson-utils.\ncode examples\nintroductory code walkthroughs of using the library are covered during these steps of the hello ai world tutorial:\ncoding your own image recognition program (python)\ncoding your own image recognition program (c++)\nadditional c++ and python samples for running the networks on static images and live camera streams can be found here:\nc++ python\nimage recognition imagenet.cpp imagenet.py\nobject detection detectnet.cpp detectnet.py\nsegmentation segnet.cpp segnet.py\npose estimation posenet.cpp posenet.py\nmonocular depth depthnet.cpp depthnet.py\nnote: for working with numpy arrays, see converting to numpy arrays and converting from numpy arrays\nthese examples will automatically be compiled while building the project from source, and are able to run the pre-trained models listed below in addition to custom models provided by the user. launch each example with --help for usage info.\npre-trained models\nthe project comes with a number of pre-trained models that are available through the model downloader -----> tool !!! :\nimage recognition\nnetwork cli argument networktype enum\nalexnet alexnet alexnet\ngooglenet googlenet googlenet\ngooglenet-12 googlenet-12 googlenet_12\nresnet-18 resnet-18 resnet_18\nresnet-50 resnet-50 resnet_50\nresnet-101 resnet-101 resnet_101\nresnet-152 resnet-152 resnet_152\nvgg-16 vgg-16 vgg-16\nvgg-19 vgg-19 vgg-19\ninception-v4 inception-v4 inception_v4\nobject detection\nnetwork cli argument networktype enum object classes\nssd-mobilenet-v1 ssd-mobilenet-v1 ssd_mobilenet_v1 91 (coco classes)\nssd-mobilenet-v2 ssd-mobilenet-v2 ssd_mobilenet_v2 91 (coco classes)\nssd-inception-v2 ssd-inception-v2 ssd_inception_v2 91 (coco classes)\ndetectnet-coco-dog coco-dog coco_dog dogs\ndetectnet-coco-bottle coco-bottle coco_bottle bottles\ndetectnet-coco-chair coco-chair coco_chair chairs\ndetectnet-coco-airplane coco-airplane coco_airplane airplanes\nped-100 pednet pednet pedestrians\nmultiped-500 multiped pednet_multi pedestrians, luggage\nfacenet-120 facenet facenet faces\nsemantic segmentation\ndataset resolution cli argument accuracy jetson nano jetson xavier\ncityscapes 512x256 fcn-resnet18-cityscapes-512x256 83.3% 48 fps 480 fps\ncityscapes 1024x512 fcn-resnet18-cityscapes-1024x512 87.3% 12 fps 175 fps\ncityscapes 2048x1024 fcn-resnet18-cityscapes-2048x1024 89.6% 3 fps 47 fps\ndeepscene 576x320 fcn-resnet18-deepscene-576x320 96.4% 26 fps 360 fps\ndeepscene 864x480 fcn-resnet18-deepscene-864x480 96.9% 14 fps 190 fps\nmulti-human 512x320 fcn-resnet18-mhp-512x320 86.5% 34 fps 370 fps\nmulti-human 640x360 fcn-resnet18-mhp-512x320 87.1% 23 fps 325 fps\npascal voc 320x320 fcn-resnet18-voc-320x320 85.9% 45 fps 508 fps\npascal voc 512x320 fcn-resnet18-voc-512x320 88.5% 34 fps 375 fps\nsun rgb-d 512x400 fcn-resnet18-sun-512x400 64.3% 28 fps 340 fps\nsun rgb-d 640x512 fcn-resnet18-sun-640x512 65.1% 17 fps 224 fps\nif the resolution is omitted from the cli argument, the lowest resolution model is loaded\naccuracy indicates the pixel classification accuracy across the model's validation dataset\nperformance is measured for gpu fp16 mode with jetpack 4.2.1, nvpmodel 0 (max-n)\nlegacy segmentation models\npose estimation\nmodel cli argument networktype enum keypoints\npose-resnet18-body resnet18-body resnet18_body 18\npose-resnet18-hand resnet18-hand resnet18_hand 21\npose-densenet121-body densenet121-body densenet121_body 18\nrecommended system requirements\njetson nano developer kit with jetpack 4.2 or newer (ubuntu 18.04 aarch64).\njetson nano 2gb developer kit with jetpack 4.4.1 or newer (ubuntu 18.04 aarch64).\njetson xavier nx developer kit with jetpack 4.4 or newer (ubuntu 18.04 aarch64).\njetson agx xavier developer kit with jetpack 4.0 or newer (ubuntu 18.04 aarch64).\njetson tx2 developer kit with jetpack 3.0 or newer (ubuntu 16.04 aarch64).\njetson tx1 developer kit with jetpack 2.3 or newer (ubuntu 16.04 aarch64).\nthe transfer learning with pytorch section of the tutorial speaks from the perspective of running pytorch onboard jetson for training dnns, however the same pytorch code can be used on a pc, server, or cloud instance with an nvidia discrete gpu for faster training.\nextra resources\nin this area, links and resources for deep learning are listed:\nros_deep_learning - tensorrt inference ros nodes\nnvidia ai iot - nvidia jetson github repositories\njetson elinux wiki - jetson elinux wiki\ntwo days to a demo (digits)\nnote: the digits/caffe tutorial from below is deprecated. it's recommended to follow the transfer learning with pytorch tutorial from hello ai world.\nexpand this section to see original digits tutorial (deprecated)\nthe digits tutorial includes training dnn's in the cloud or pc, and inference on the jetson with tensorrt, and can take roughly two days or more depending on system setup, downloading the datasets, and the training speed of your gpu.\n\u00a9 2016-2019 nvidia | table of contents", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000009, "year": null}, {"Unnamed: 0": 1030, "autor": 10, "date": null, "content": "Awesome Self-Supervised Learning\nA curated list of awesome Self-Supervised Learning resources. Inspired by awesome-deep-vision, awesome-adversarial-machine-learning, awesome-deep-learning-papers, and awesome-architecture-search\nWhy Self-Supervised?\nSelf-Supervised Learning has become an exciting direction in AI community.\nJitendra Malik: \"Supervision is the opium of the AI researcher\"\nAlyosha Efros: \"The AI revolution will not be supervised\"\nYann LeCun: \"self-supervised learning is the cake, supervised learning is the icing on the cake, reinforcement learning is the cherry on the cake\"\nContributing\nPlease help contribute this list by contacting me or add pull request\nMarkdown format:\n- Paper Name.\n[[pdf]](link)\n[[code]](link)\n- Author 1, Author 2, and Author 3. *Conference Year*\nTable of Contents\nTheory\nComputer Vision (CV)\nSurvey\nImage Representation Learning\nVideo Representation Learning\nGeometry\nAudio\nOthers\nMachine Learning\nReinforcement Learning\nRecommendation Systems\nRobotics\nNatural Language Processing (NLP)\nAutomatic Speech Recognition (ASR)\nGraph\nTalks\nThesis\nBlog\nTheory\nA Theoretical Analysis of Contrastive Unsupervised Representation Learning. [pdf]\nSanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi. ICML 2019\nUnderstanding the Behaviour of Contrastive Loss. [pdf]\nFeng Wang and Huaping Liu. CVPR 2021\nPredicting What You Already Know Helps: Provable Self-Supervised Learning. [pdf]\nJason D. Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo.\nContrastive learning , multi-view redundancy , and linear models. [pdf]\nChristopher Tosh, Akshay Krishnamurthy, and Daniel Hsu.\nUnderstanding Self-supervised Learning with Dual Deep Networks. [pdf]\nYuandong Tian, Lantao Yu, Xinlei Chen, and Surya Ganguli.\nFor self-supervised learning, Rationality implies generalization, provably. [pdf]\nYamini Bansal, Gal Kaplun, and Boaz Barak.\nCan Pretext-Based Self-Supervised Learning Be Boosted by Downstream Data? A Theoretical Analysis. [pdf]\nJiaye Teng and Weiran Huang.\nComputer Vision\nSurvey\nContrastive Representation Learning: A Framework and Review [pdf]\nPhuc H. Le-Khac, Graham Healy, Alan F. Smeaton. IEEE Access 2020\nA Survey on Contrastive Self-supervised Learning [pdf]\nAshish Jaiswal, Ashwin R Babu, Mohammad Z Zadeh, Debapriya Banerjee, Fillia Makedon\nSelf-supervised Visual Feature Learning with Deep Neural Networks: A Survey. [pdf]\nLonglong Jing and Yingli Tian. T-PAMI 2020\nSelf-supervised Learning: Generative or Contrastive [pdf]\nXiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, Jie Tang.\nImage Representation Learning\nBenchmark code\nFAIR Self-Supervision Benchmark [pdf] [repo]: various benchmark (and legacy) tasks for evaluating quality of visual representations learned by various self-supervision approaches.\nHow Well Do Self-Supervised Models Transfer? [pdf] [repo]: A benchmark for evaluating self-supervision consisting of many-shot/few-shot recognition, object detection, surface normal estimation and semantic segmentation.\n2015\nUnsupervised Visual Representation Learning by Context Prediction. [pdf] [code]\nDoersch, Carl and Gupta, Abhinav and Efros, Alexei A. ICCV 2015\nUnsupervised Learning of Visual Representations using Videos. [pdf] [code]\nWang, Xiaolong and Gupta, Abhinav. ICCV 2015\nLearning to See by Moving. [pdf] [code]\nAgrawal, Pulkit and Carreira, Joao and Malik, Jitendra. ICCV 2015\nLearning image representations tied to ego-motion. [pdf] [code]\nJayaraman, Dinesh and Grauman, Kristen. ICCV 2015\n2016\nJoint Unsupervised Learning of Deep Representations and Image Clusters. [pdf] [code-torch] [code-caffe]\nJianwei Yang, Devi Parikh, Dhruv Batra. CVPR 2016\nUnsupervised Deep Embedding for Clustering Analysis. [pdf] [code]\nJunyuan Xie, Ross Girshick, and Ali Farhadi. ICML 2016\nSlow and steady feature analysis: higher order temporal coherence in video. [pdf]\nJayaraman, Dinesh and Grauman, Kristen. CVPR 2016\nContext Encoders: Feature Learning by Inpainting. [pdf] [code]\nPathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A. CVPR 2016\nColorful Image Colorization. [pdf] [code]\nZhang, Richard and Isola, Phillip and Efros, Alexei A. ECCV 2016\nUnsupervised Learning of Visual Representations by Solving Jigsaw Puzzles. [pdf] [code]\nNoroozi, Mehdi and Favaro, Paolo. ECCV 2016\nAmbient Sound Provides Supervision for Visual Learning. [pdf] [code]\nOwens, Andrew and Wu, Jiajun and McDermott, Josh and Freeman, William and Torralba, Antonio. ECCV 2016\nLearning Representations for Automatic Colorization. [pdf] [code]\nLarsson, Gustav and Maire, Michael and Shakhnarovich, Gregory. ECCV 2016\nUnsupervised Visual Representation Learning by Graph-based Consistent Constraints. [pdf] [code]\nLi, Dong and Hung, Wei-Chih and Huang, Jia-Bin and Wang, Shengjin and Ahuja, Narendra and Yang, Ming-Hsuan. ECCV 2016\n2017\nAdversarial Feature Learning. [pdf] [code]\nDonahue, Jeff and Krahenbuhl, Philipp and Darrell, Trevor. ICLR 2017\nSelf-supervised learning of visual features through embedding images into text topic spaces. [pdf] [code]\nL. Gomez* and Y. Patel* and M. Rusi\u00f1ol and D. Karatzas and C.V. Jawahar. CVPR 2017\nSplit-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction. [pdf] [code]\nZhang, Richard and Isola, Phillip and Efros, Alexei A. CVPR 2017\nLearning Features by Watching Objects Move. [pdf] [code]\nPathak, Deepak and Girshick, Ross and Dollar, Piotr and Darrell, Trevor and Hariharan, Bharath. CVPR 2017\nColorization as a Proxy Task for Visual Understanding. [pdf] [code]\nLarsson, Gustav and Maire, Michael and Shakhnarovich, Gregory. CVPR 2017\nDeepPermNet: Visual Permutation Learning. [pdf] [code]\nCruz, Rodrigo Santa and Fernando, Basura and Cherian, Anoop and Gould, Stephen. CVPR 2017\nUnsupervised Learning by Predicting Noise. [pdf] [code]\nBojanowski, Piotr and Joulin, Armand. ICML 2017\nMulti-task Self-Supervised Visual Learning. [pdf]\nDoersch, Carl and Zisserman, Andrew. ICCV 2017\nRepresentation Learning by Learning to Count. [pdf]\nNoroozi, Mehdi and Pirsiavash, Hamed and Favaro, Paolo. ICCV 2017\nTransitive Invariance for Self-supervised Visual Representation Learning. [pdf]\nWang, Xiaolong and He, Kaiming and Gupta, Abhinav. ICCV 2017\nLook, Listen and Learn. [pdf]\nRelja, Arandjelovic and Zisserman, Andrew. ICCV 2017\nUnsupervised Representation Learning by Sorting Sequences. [pdf] [code]\nHsin-Ying Lee, Jia-Bin Huang, Maneesh Kumar Singh, and Ming-Hsuan Yang. ICCV 2017\n2018\nUnsupervised Feature Learning via Non-parameteric Instance Discrimination [pdf] [code]\nZhirong Wu, Yuanjun Xiong and X Yu Stella and Dahua Lin. CVPR 2018\nLearning Image Representations by Completing Damaged Jigsaw Puzzles. [pdf] [code]\nKim, Dahun and Cho, Donghyeon and Yoo, Donggeun and Kweon, In So. WACV 2018\nUnsupervised Representation Learning by Predicting Image Rotations. [pdf] [code]\nSpyros Gidaris and Praveer Singh and Nikos Komodakis. ICLR 2018\nLearning Latent Representations in Neural Networks for Clustering through Pseudo Supervision and Graph-based Activity Regularization. [pdf] [code]\nOzsel Kilinc and Ismail Uysal. ICLR 2018\nImprovements to context based self-supervised learning. [pdf]\nTerrell Mundhenk and Daniel Ho and Barry Chen. CVPR 2018\nSelf-Supervised Feature Learning by Learning to Spot Artifacts. [pdf] [code]\nSimon Jenni and Universit\u00e4t Bern and Paolo Favaro. CVPR 2018\nBoosting Self-Supervised Learning via Knowledge Transfer. [pdf]\nMehdi Noroozi and Ananth Vinjimoor and Paolo Favaro and Hamed Pirsiavash. CVPR 2018\nCross-domain Self-supervised Multi-task Feature Learning Using Synthetic Imagery. [pdf] [code]\nZhongzheng Ren and Yong Jae Lee. CVPR 2018\nShapeCodes: Self-Supervised Feature Learning by Lifting Views to Viewgrids. [pdf]\nDinesh Jayaraman*, UC Berkeley; Ruohan Gao, University of Texas at Austin; Kristen Grauman. ECCV 2018\nDeep Clustering for Unsupervised Learning of Visual Features [pdf] [code]\nMathilde Caron, Piotr Bojanowski, Armand Joulin, Matthijs Douze. ECCV 2018\nCross Pixel Optical-Flow Similarity for Self-Supervised Learning. [pdf]\nAravindh Mahendran, James Thewlis, Andrea Vedaldi. ACCV 2018\n2019\nRepresentation Learning with Contrastive Predictive Coding. [pdf]\nAaron van den Oord, Yazhe Li, Oriol Vinyals.\nSelf-Supervised Learning via Conditional Motion Propagation. [pdf] [code]\nXiaohang Zhan, Xingang Pan, Ziwei Liu, Dahua Lin, and Chen Change Loy. CVPR 2019\nSelf-Supervised Representation Learning by Rotation Feature Decoupling. [pdf] [code]\nZeyu Feng; Chang Xu; Dacheng Tao. CVPR 2019\nRevisiting Self-Supervised Visual Representation Learning. [pdf] [code]\nAlexander Kolesnikov; Xiaohua Zhai; Lucas Beye. CVPR 2019\nSelf-Supervised GANs via Auxiliary Rotation Loss. [pdf] [code]\nTing Chen; Xiaohua Zhai; Marvin Ritter; Mario Lucic; Neil Houlsby. CVPR 2019\nAET vs. AED: Unsupervised Representation Learning by Auto-Encoding Transformations rather than Data. [pdf] [code]\nLiheng Zhang, Guo-Jun Qi, Liqiang Wang, Jiebo Luo. CVPR 2019\nUnsupervised Deep Learning by Neighbourhood Discovery. [pdf]. [code].\nJiabo Huang, Qi Dong, Shaogang Gong, Xiatian Zhu. ICML 2019\nContrastive Multiview Coding. [pdf] [code]\nYonglong Tian and Dilip Krishnan and Phillip Isola.\nLarge Scale Adversarial Representation Learning. [pdf]\nJeff Donahue, Karen Simonyan.\nLearning Representations by Maximizing Mutual Information Across Views. [pdf] [code]\nPhilip Bachman, R Devon Hjelm, William Buchwalter\nSelfie: Self-supervised Pretraining for Image Embedding. [pdf]\nTrieu H. Trinh, Minh-Thang Luong, Quoc V. Le\nData-Efficient Image Recognition with Contrastive Predictive Coding [pdf]\nOlivier J. He \u0301naff, Ali Razavi, Carl Doersch, S. M. Ali Eslami, Aaron van den Oord\nUsing Self-Supervised Learning Can Improve Model Robustness and Uncertainty [pdf] [code]\nDan Hendrycks, Mantas Mazeika, Saurav Kadavath, Dawn Song. NeurIPS 2019\nBoosting Few-Shot Visual Learning with Self-Supervision [pdf]\nPyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick P\u00e9rez, and Matthieu Cord. ICCV 2019\nSelf-Supervised Generalisation with Meta Auxiliary Learning [pdf] [code]\nShikun Liu, Andrew J. Davison, Edward Johns. NeurIPS 2019\nWasserstein Dependency Measure for Representation Learning [pdf] [code]\nSherjil Ozair, Corey Lynch, Yoshua Bengio, Aaron van den Oord, Sergey Levine, Pierre Sermanet. NeurIPS 2019\nScaling and Benchmarking Self-Supervised Visual Representation Learning [pdf] [code]\nPriya Goyal, Dhruv Mahajan, Abhinav Gupta, Ishan Misra. ICCV 2019\nUnsupervised Pre-Training of Image Features on Non-Curated Data [pdf] [code]\nMathilde Caron, Piotr Bojanowski, Julien Mairal, Armand Joulin. ICCV 2019 Oral\nS4L: Self-Supervised Semi-Supervised Learning [pdf] [code]\nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, Lucas Beyer. ICCV 2019\nSelf-supervised model adaptation for multimodal semantic segmentation. [pdf] [code]\nAbhinav Valada, Rohit Mohan, and Wolfram Burgard. IJCV 2019\n2020\nA critical analysis of self-supervision, or what we can learn from a single image [pdf] [code]\nYuki M. Asano, Christian Rupprecht, Andrea Vedaldi. ICLR 2020\nOn Mutual Information Maximization for Representation Learning [pdf] [code]\nMichael Tschannen, Josip Djolonga, Paul K. Rubenstein, Sylvain Gelly, Mario Lucic. ICLR 2020\nUnderstanding the Limitations of Variational Mutual Information Estimators [pdf] [code]\nJiaming Song, Stefano Ermon. ICLR 2020\nSelf-labelling via simultaneous clustering and representation learning [pdf] [blogpost] [code]\nYuki Markus Asano, Christian Rupprecht, Andrea Vedaldi. ICLR 2020 (Spotlight)\nSelf-supervised Label Augmentation via Input Transformations [pdf] [code]\nHankook Lee, Sung Ju Hwang, Jinwoo Shin. ICML 2020\nAutomatic Shortcut Removal for Self-Supervised Representation Learning [pdf]\nMatthias Minderer, Olivier Bachem, Neil Houlsby, Michael Tschannen\nA Simple Framework for Contrastive Learning of Visual Representations [pdf] [code]\nTing Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton. ICML 2020\nHow Useful is Self-Supervised Pretraining for Visual Tasks? [pdf] [code]\nAlejandro Newell, Jia Deng. CVPR 2020\nMomentum Contrast for Unsupervised Visual Representation Learning [pdf] [code]\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick. CVPR 2020\nClusterFit: Improving Generalization of Visual Representations [pdf]\nXueting Yan*, Ishan Misra*, Abhinav Gupta, Deepti Ghadiyaram**, Dhruv Mahajan**. CVPR 2020\nSelf-Supervised Learning of Pretext-Invariant Representations [pdf]\nIshan Misra, Laurens van der Maaten. CVPR 2020\nBootstrap Your Own Latent: A New Approach to Self-Supervised Learning [pdf] [unofficial-code]\nJean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, Michal Valko. NeurIPS 2020, Oral\nContrastive learning of global and local features for medical image segmentation with limited annotations [pdf] [code]\nKrishna Chaitanya, Ertunc Erdil, Neerav Karani, Ender Konukoglu. NeurIPS 2020, Oral\nUnsupervised Representation Learning by InvariancePropagation [pdf] [code]\nFeng Wang, Huaping Liu, Di Guo, Fuchun Sun. NeurIPS 2020, Spotlight\nBig Self-Supervised Models are Strong Semi-Supervised Learners [pdf] [code]\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey Hinton. NeurIPS 2020\nSelf-Supervised Prototypical Transfer Learning for Few-Shot Classification [pdf] [code]\nCarlos Medina, Arnout Devos, Matthias Grossglauser\nSCAN: Learning to Classify Images without Labels [pdf] [code]\nWouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, Luc Van Gool. ECCV 2020\nUnsupervised Learning of Visual Features by Contrasting Cluster Assignments [pdf] [code]\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin. NeurIPS 2020\nSelf-Supervised Relational Reasoning for Representation Learning [pdf] [code]\nMassimiliano Patacchiola, Amos Storkey. NeurIPS 2020, Spotlight\nExploring Simple Siamese Representation Learning [pdf] [unofficial-code]\nXinlei Chen, Kaiming He\nOnline Bag-of-Visual-Words Generation for Unsupervised Representation Learning [pdf] [code]\nSpyros Gidaris, Andrei Bursuc, Gilles Puy, Nikos Komodakis, Matthieu Cord, Patrick P\u00e9rez\nRethinking the Value of Labels for Improving Class-Imbalanced Learning [pdf] [code]\nYuzhe Yang, Zhi Xu. NeurIPS 2020\nDemystifying contrastive self-supervised learning: Invariances, augmentations and dataset biases [pdf]\nSenthil Purushwalkam, Abhinav Gupta. NeurIPS 2020\n2021\nBarlow twins: Self-supervised learning via redundancy reduction [pdf] [code]\nZbontar, J., Jing, L., Misra, I., LeCun, Y., & Deny, S.\nContrastive Semi-Supervised Learning for 2D Medical Image Segmentation [pdf]\nPrashant Pandey, Ajey Pai, Nisarg Bhatt, Prasenjit Das, Govind Makharia, Prathosh AP, Mausam. MICCAI 2021\nPropagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning [pdf] [code]\nZhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, and Han Hu. CVPR 2021\nHow Well Do Self-Supervised Models Transfer? [pdf] [code]\nLinus Ericsson, Henry Gouk, Timothy M. Hospedales. CVPR 2021\nVectorization and Rasterization: Self-Supervised Learning for Sketch and Handwriting.\n[code]\nAyan Kumar Bhunia, Pinaki nath Chowdhury, Yongxin Yang, Timothy Hospedales, Tao Xiang, Yi-Zhe Song. CVPR 2021\nSelfAugment: Automatic Augmentation Policies for Self-Supervised Learning [pdf] [code]\nColorado Reed, Sean Metzger, Aravind Srinivas, Trevor Darrell, Kurt Keutzer. CVPR 2021\nJigsaw Clustering for Unsupervised Visual Representation Learning [pdf] [code]\nPengguang Chen, Shu Liu, Jiaya Jia. CVPR 2021\nImproving Contrastive Learning by Visualizing Feature Transformation [pdf] [code]\nRui Zhu*, Bingchen Zhao*, Jingen Liu, Zhenglong Sun, Chang Wen Chen. ICCV 2021 Oral\nVideo Representation Learning\nUnsupervised Learning of Video Representations using LSTMs. [pdf] [code]\nSrivastava, Nitish and Mansimov, Elman and Salakhudinov, Ruslan. ICML 2015\nShuffle and Learn: Unsupervised Learning using Temporal Order Verification. [pdf] [code]\nIshan Misra, C. Lawrence Zitnick and Martial Hebert. ECCV 2016\nLSTM Self-Supervision for Detailed Behavior Analysis [pdf]\nBiagio Brattoli*, Uta B\u00fcchler*, Anna-Sophia Wahl, Martin E. Schwab, and Bj\u00f6rn Ommer. CVPR 2017\nSelf-Supervised Video Representation Learning With Odd-One-Out Networks. [pdf]\nBasura Fernando and Hakan Bilen and Efstratios Gavves and Stephen Gould. CVPR 2017\nUnsupervised Learning of Long-Term Motion Dynamics for Videos. [pdf]\nLuo, Zelun and Peng, Boya and Huang, De-An and Alahi, Alexandre and Fei-Fei, Li. CVPR 2017\nGeometry Guided Convolutional Neural Networks for Self-Supervised Video Representation Learning. [pdf]\nChuang Gan and Boqing Gong and Kun Liu and Hao Su and Leonidas J. Guibas. CVPR 2018\nImproving Spatiotemporal Self-Supervision by Deep Reinforcement Learning. [pdf]\nBiagio Brattoli*, Uta B\u00fcchler*, and Bj\u00f6rn Ommer. ECCV 2018\nSelf-supervised learning of a facial attribute embedding from video. [pdf]\nWiles, O., Koepke, A.S., Zisserman, A. BMVC 2018\nSelf-Supervised Video Representation Learning with Space-Time Cubic Puzzles. [pdf]\nKim, Dahun and Cho, Donghyeon and Yoo, Donggeun and Kweon, In So. AAAI 2019\nSelf-Supervised Spatio-Temporal Representation Learning for Videos by Predicting Motion and Appearance Statistics. [pdf]\nJiangliu Wang; Jianbo Jiao; Linchao Bao; Shengfeng He; Yunhui Liu; Wei Liu. CVPR 2019\nDynamoNet: Dynamic Action and Motion Network. [pdf]\nAli Diba; Vivek Sharma, Luc Van Gool, Rainer Stiefelhagen. ICCV 2019\nLearning Correspondence from the Cycle-consistency of Time. [pdf] [code]\nXiaolong Wang*, Allan Jabri* and Alexei A. Efros. CVPR 2019\nJoint-task Self-supervised Learning for Temporal Correspondence. [pdf] [code]\nXueting Li*, Sifei Liu*, Shalini De Mello, Xiaolong Wang, Jan Kautz, and Ming-Hsuan Yang. NIPS 2019\nSelf-Supervised Video Representation Learning Using Inter-Intra Contrstive Framework [pdf] [code]\nLi Tao, Xueting Wang*, Toshihiko Yamasaki. ACMMM 2020\nVideo Playback Rate Perception for Self-Supervised Spatio-Temporal Representation Learning [pdf] [Code]\nYuan Yao*, Chang Liu*, Dezhao Luo, Yu Zhou, Qixiang Ye. CVPR 2020\nSelf-Supervised Video Representation Learning by Pace Prediction [pdf] [code]\nJiangliu Wang, Jianbo Jiao, Yun-Hui Liu. ECCV 2020\nVideo Representation Learning by Recognizing Temporal Transformations [pdf] [code]\nSimon Jenni, Givi Meishvili, Paolo Favaro. ECCV 2020\nSelf-supervised Co-training for Video Representation Learning [pdf] [code]\nTengda Han, Weidi Xie, and Andrew Zisserman. NeurIPS 2020\nCycle-Contrast for Self-Supervised Video Representation Learning [pdf]\nQuan Kong, Wenpeng Wei, Ziwei Deng, Tomoaki Yoshinaga, and Tomokazu Murakami. NeurIPS 2020\nVideo Representation Learning with Visual Tempo Consistency [pdf] [code]\nCeyuan Yang, Yinghao Xu, Bo Dai, and Bolei Zhou\nSelf-supervised Video Representation Learning by Uncovering Spatio-temporal Statistics [pdf]\nJiangliu Wang, Jianbo Jiao, Linchao Bao, Shengfeng He, Wei Liu, and Yun-hui Liu\nSpatiotemporal Contrastive Video Representation Learning [pdf]\nRui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui\nSelf-Supervised Video Representation Using Pretext-Contrastive Learning [pdf]\nLi Tao, Xueting Wang, and Toshihiko Yamasaki\nUnsupervised Video Representation Learning by Bidirectional Feature Prediction [pdf]\nNadine Behrmann, Juergen Gall, and Mehdi Noroozi\nRSPNet: Relative Speed Perception for Unsupervised Video Representation Learning [pdf] [code]\nPeihao Chen, Deng Huang, Dongliang He, Xiang Long, Runhao Zeng, Shilei Wen, Mingkui Tan, and Chuang Gan. AAAI 2021\nHierarchically Decoupled Spatial-Temporal Contrast for Self-supervised Video Representation Learning [pdf]\nZehua Zhang and David Crandall\nCan Temporal Information Help with Contrastive Self-Supervised Learning? [pdf]\nYutong Bai, Haoqi Fan, Ishan Misra, Ganesh Venkatesh, Yongyi Lu, Yuyin Zhou, Qihang Yu, Vikas Chandra, and Alan Yuille\nEnhancing Unsupervised Video Representation Learning by Decoupling the Scene and the Motion [pdf] [code]\nJinpeng Wang, Yuting Gao, Ke Li, Jianguo Hu, Xinyang Jiang, Xiaowei Guo, Rongrong Ji, and Xing Sun. AAAI 2021\nSpace-Time Correspondence as a Contrastive Random Walk [pdf] [code] [project]\nAllan Jabri, Andrew Owens, Alexei A. Efros. NeurIPS 2020 Oral\nGeometry\nUnsupervised CNN for Single View Depth Estimation: Geometry to the Rescue. [pdf] [code]\nRavi Garg, Vijay Kumar BG, Gustavo Carneiro, Ian Reid. ECCV 2016\nSelf-supervised Learning of Motion Capture. [pdf] [code] [web]\nTung, Hsiao-Yu and Tung, Hsiao-Wei and Yumer, Ersin and Fragkiadaki, Katerina. NIPS 2017\nUnsupervised learning of object frames by dense equivariant image labelling. [pdf]\nJames Thewlis, Hakan Bilen, Andrea Vedaldi. NeurIPS 2017\nUnsupervised Learning of Depth and Ego-Motion from Video. [pdf] [code] [web]\nZhou, Tinghui and Brown, Matthew and Snavely, Noah and Lowe, David G. CVPR 2017\nActive Stereo Net: End-to-End Self-Supervised Learning for Active Stereo Systems. [project]\nYinda Zhang*, Sean Fanello, Sameh Khamis, Christoph Rhemann, Julien Valentin, Adarsh Kowdle, Vladimir Tankovich, Shahram Izadi, Thomas Funkhouser. ECCV 2018\nSelf-Supervised Relative Depth Learning for Urban Scene Understanding. [pdf] [project]\nHuaizu Jiang*, Erik Learned-Miller, Gustav Larsson, Michael Maire, Greg Shakhnarovich. ECCV 2018\nGeometry-Aware Learning of Maps for Camera Localization. [pdf] [code]\nSamarth Brahmbhatt, Jinwei Gu, Kihwan Kim, James Hays, and Jan Kautz. CVPR 2018\nSelf-supervised Learning of Geometrically Stable Features Through Probabilistic Introspection. [pdf] [web]\nDavid Novotny, Samuel Albanie, Diane Larlus, Andrea Vedaldi. CVPR 2018\nSelf-Supervised Learning of 3D Human Pose Using Multi-View Geometry. [pdf]\nMuhammed Kocabas; Salih Karagoz; Emre Akbas. CVPR 2019\nSelFlow: Self-Supervised Learning of Optical Flow. [pdf]\nJiangliu Wang; Jianbo Jiao; Linchao Bao; Shengfeng He; Yunhui Liu; Wei Liu. CVPR 2019\nUnsupervised Learning of Landmarks by Descriptor Vector Exchange. [pdf] [code] [web]\nJames Thewlis, Samuel Albanie, Hakan Bilen, Andrea Vedaldi. ICCV 2019\nAudio\nAudio-Visual Scene Analysis with Self-Supervised Multisensory Features. [pdf] [code]\nAndrew Owens, Alexei A. Efros. ECCV 2018\nObjects that Sound. [pdf]\nR. Arandjelovi\u0107, A. Zisserman. ECCV 2018\nLearning to Separate Object Sounds by Watching Unlabeled Video. [pdf] [project]\nRuohan Gao, Rogerio Feris, Kristen Grauman. ECCV 2018\nThe Sound of Pixels. [pdf] [project]\nZhao, Hang and Gan, Chuang and Rouditchenko, Andrew and Vondrick, Carl and McDermott, Josh and Torralba, Antonio. ECCV 2018\nLearnable PINs: Cross-Modal Embeddings for Person Identity. [pdf] [web]\nArsha Nagrani, Samuel Albanie, Andrew Zisserman. ECCV 2018\nCooperative Learning of Audio and Video Models from Self-Supervised Synchronization. [pdf]\nBruno Korbar,Dartmouth College, Du Tran, Lorenzo Torresani. NIPS 2018\nSelf-Supervised Generation of Spatial Audio for 360\u00b0 Video. [pdf]\nPedro Morgado, Nuno Nvasconcelos, Timothy Langlois, Oliver Wang. NIPS 2018\nTriCycle: Audio Representation Learning from Sensor Network Data Using Self-Supervision [pdf]\nMark Cartwright, Jason Cramer, Justin Salamon, Juan Pablo Bello. WASPAA 2019\nSelf-supervised audio-visual co-segmentation [pdf]\nAndrew Rouditchenko, Hang Zhao, Chuang Gan, Josh McDermott, and Antonio Torralba. ICASSP 2019\nDoes Visual Self-Supervision Improve Learning of Speech Representations? [pdf]\nAbhinav Shukla, Stavros Petridis, Maja Pantic\nThere is More than Meets the Eye: Self-Supervised Multi-Object Detection and Tracking with Sound by Distilling Multimodal Knowledge [pdf] [code]\nFrancisco Rivera Valverde, Juana Valeria Hurtado, and Abhinav Valada. CVPR 2021\nBYOL for Audio: Self-Supervised Learning for General-Purpose Audio Representation. [pdf] [code]\nDaisuke Niizumi; Daiki Takeuchi; Yasunori Ohishi IJCNN 2021\nOthers\nSelf-learning Scene-specific Pedestrian Detectors using a Progressive Latent Model. [pdf]\nQixiang Ye, Tianliang Zhang, Qiang Qiu, Baochang Zhang, Jie Chen, Guillermo Sapiro. CVPR 2017\nFree Supervision from Video Games. [pdf] [project+code]\nPhilipp Kr\u00e4henb\u00fchl. CVPR 2018\nFighting Fake News: Image Splice Detection via Learned Self-Consistency [pdf] [code]\nMinyoung Huh*, Andrew Liu*, Andrew Owens, Alexei A. Efros. ECCV 2018\nSelf-supervised Tracking by Colorization (Tracking Emerges by Colorizing Videos). [pdf]\nCarl Vondrick*, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama, Kevin Murphy. ECCV 2018\nHigh-Fidelity Image Generation With Fewer Labels. [pdf]\nMario Lucic*, Michael Tschannen*, Marvin Ritter*, Xiaohua Zhai, Olivier Bachem, Sylvain Gelly.\nSelf-supervised Fitting of Articulated Meshes to Point Clouds.\nChun-Liang Li, Tomas Simon, Jason Saragih, Barnab\u00e1s P\u00f3czos and Yaser Sheikh. CVPR 2019\nJust Go with the Flow: Self-Supervised Scene Flow Estimation [pdf] [code]\nHimangi Mittal, Brian Okorn, David Held. CVPR 2020\nSCOPS: Self-Supervised Co-Part Segmentation.\nWei-Chih Hung, Varun Jampani, Sifei Liu, Pavlo Molchanov, Ming-Hsuan Yang, and Jan Kautz. CVPR 2019\nSelf-Supervised Adaptation of High-Fidelity Face Models for Monocular Performance Tracking.\nJae Shin Yoon; Takaaki Shiratori; Shoou-I Yu; Hyun Soo Park. CVPR 2019\nMulti-Task Self-Supervised Object Detection via Recycling of Bounding Box Annotations. [pdf] [code]\nWonhee Lee; Joonil Na; Gunhee Kim. CVPR 2019\nSelf-Supervised Convolutional Subspace Clustering Network.\nJunjian Zhang; Chun-Guang Li; Chong You; Xianbiao Qi; Honggang Zhang; Jun Guo; Zhouchen Lin. CVPR 2019\nReinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation.\nXin Wang; Qiuyuan Huang; Asli Celikyilmaz; Jianfeng Gao; Dinghan Shen; Yuan-Fang Wang; William Yang Wang; Lei Zhang. CVPR 2019\nUnsupervised 3D Pose Estimation With Geometric Self-Supervision.\nChing-Hang Chen; Ambrish Tyagi; Amit Agrawal; Dylan Drover; Rohith MV; Stefan Stojanov; James M. Rehg. CVPR 2019\nLearning to Generate Grounded Image Captions without Localization Supervision. [pdf]\nChih-Yao Ma; Yannis Kalantidis; Ghassan AlRegib; Peter Vajda; Marcus Rohrbach; Zsolt Kira.\nVideoBERT: A Joint Model for Video and Language Representation Learning [pdf]\nChen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, Cordelia Schmid. ICCV 2019\nCountering Noisy Labels By Learning From Auxiliary Clean Labels [pdf]\nTsung Wei Tsai, Chongxuan Li, Jun Zhu\nMachine Learning\nSelf-taught Learning: Transfer Learning from Unlabeled Data. [pdf]\nRaina, Rajat and Battle, Alexis and Lee, Honglak and Packer, Benjamin and Ng, Andrew Y. ICML 2007\nRepresentation Learning: A Review and New Perspectives. [pdf]\nBengio, Yoshua and Courville, Aaron and Vincent, Pascal. TPAMI 2013.\nReinforcement Learning\nCuriosity-driven Exploration by Self-supervised Prediction. [pdf] [code]\nDeepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. ICML 2017\nLarge-Scale Study of Curiosity-Driven Learning. [pdf]\nYuri Burda*, Harri Edwards*, Deepak Pathak*, Amos Storkey, Trevor Darrell and Alexei A. Efros\nPlaying hard exploration games by watching YouTube. [pdf]\nYusuf Aytar, Tobias Pfaff, David Budden, Tom Le Paine, Ziyu Wang, Nando de Freitas. NIPS 2018\nUnsupervised State Representation Learning in Atari. [pdf] [code]\nAnkesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre C\u00f4t\u00e9, R Devon Hjelm. NeurIPS 2019\nRecommendation Systems\nSelf-supervised Learning for Deep Models in Recommendations. [pdf]\nTiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Aditya Menon, Lichan Hong, Ed H. Chi, Steve Tjoa, Jieqi (Jay)Kang, Evan Ettinger Preprint 2020\nRobotics\n2006\nImproving Robot Navigation Through Self-Supervised Online Learning [pdf]\nBoris Sofman, Ellie Lin, J. Andrew Bagnell, Nicolas Vandapel, and Anthony Stentz\nReverse Optical Flow for Self-Supervised Adaptive Autonomous Robot Navigation [pdf]\nA. Lookingbill, D. Lieb, J. Rogers and J. Curry\n2009\nLearning Long-Range Vision for Autonomous Off-Road Driving [pdf]\nRaia Hadsell, Pierre Sermanet, Jan Ben, Ayse Erkan, Marco Scoffier, Koray Kavukcuoglu, Urs Muller, Yann LeCun\n2012\nSelf-supervised terrain classification for planetary surface exploration rovers [pdf]\nChristopher A. Brooks, Karl Iagnemma\n2014\nTerrain Traversability Analysis Using Multi-Sensor Data Correlation by a Mobile Robot [pdf]\nMohammed Abdessamad Bekhti, Yuichi Kobayashi and Kazuki Matsumura\n2015\nOnline self-supervised learning for dynamic object segmentation [pdf]\nVitor Guizilini and Fabio Ramos, The International Journal of Robotics Research\nSelf-Supervised Online Learning of Basic Object Push Affordances [pdf]\nBarry Ridge, Ales Leonardis, Ales Ude, Miha Denisa, and Danijel Skocaj\nSelf-supervised learning of grasp dependent tool affordances on the iCub Humanoid robot [pdf]\nTanis Mar, Vadim Tikhanoff, Giorgio Metta, and Lorenzo Natale\n2016\nPersistent self-supervised learning principle: from stereo to monocular vision for obstacle avoidance [pdf]\nKevin van Hecke, Guido de Croon, Laurens van der Maaten, Daniel Hennes, and Dario Izzo\nThe Curious Robot: Learning Visual Representations via Physical Interactions. [pdf]\nLerrel Pinto and Dhiraj Gandhi and Yuanfeng Han and Yong-Lae Park and Abhinav Gupta. ECCV 2016\nLearning to Poke by Poking: Experiential Learning of Intuitive Physics. [pdf]\nAgrawal, Pulkit and Nair, Ashvin V and Abbeel, Pieter and Malik, Jitendra and Levine, Sergey. NIPS 2016\nSupersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours. [pdf]\nPinto, Lerrel and Gupta, Abhinav. ICRA 2016\n2017\nSupervision via Competition: Robot Adversaries for Learning Tasks. [pdf]\nPinto, Lerrel and Davidson, James and Gupta, Abhinav. ICRA 2017\nMulti-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge. [pdf] [Project]\nAndy Zeng, Kuan-Ting Yu, Shuran Song, Daniel Suo, Ed Walker Jr., Alberto Rodriguez, Jianxiong Xiao. ICRA 2017\nCombining Self-Supervised Learning and Imitation for Vision-Based Rope Manipulation. [pdf] [Project]\nAshvin Nair*, Dian Chen*, Pulkit Agrawal*, Phillip Isola, Pieter Abbeel, Jitendra Malik, Sergey Levine. ICRA 2017\nLearning to Fly by Crashing [pdf]\nDhiraj Gandhi, Lerrel Pinto, Abhinav Gupta IROS 2017\nSelf-supervised learning as an enabling technology for future space exploration robots: ISS experiments on monocular distance learning [pdf]\nK. van Hecke, G. C. de Croon, D. Hennes, T. P. Setterfield, A. Saenz- Otero, and D. Izzo\nUnsupervised Perceptual Rewards for Imitation Learning. [pdf] [project]\nSermanet, Pierre and Xu, Kelvin and Levine, Sergey. RSS 2017\nSelf-Supervised Visual Planning with Temporal Skip Connections. [pdf]\nFrederik Ebert, Chelsea Finn, Alex X. Lee, Sergey Levine. CoRL2017\n2018\nCASSL: Curriculum Accelerated Self-Supervised Learning. [pdf]\nAdithyavairavan Murali, Lerrel Pinto, Dhiraj Gandhi, Abhinav Gupta. ICRA 2018\nTime-Contrastive Networks: Self-Supervised Learning from Video. [pdf] [Project]\nPierre Sermanet and Corey Lynch and Yevgen Chebotar and Jasmine Hsu and Eric Jang and Stefan Schaal and Sergey Levine. ICRA 2018\nSelf-Supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation. [pdf]\nGregory Kahn, Adam Villaflor, Bosen Ding, Pieter Abbeel, Sergey Levine. ICRA 2018\nLearning Actionable Representations from Visual Observations. [pdf] [Project]\nDwibedi, Debidatta and Tompson, Jonathan and Lynch, Corey and Sermanet, Pierre. IROS 2018\nLearning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning. [pdf] [Project]\nAndy Zeng, Shuran Song, Stefan Welker, Johnny Lee, Alberto Rodriguez, Thomas Funkhouser. IROS 2018\nVisual Reinforcement Learning with Imagined Goals. [pdf] [Project]\nAshvin Nair*, Vitchyr Pong*, Murtaza Dalal, Shikhar Bahl, Steven Lin, Sergey Levine.NeurIPS 2018\nGrasp2Vec: Learning Object Representations from Self-Supervised Grasping. [pdf] [Project]\nEric Jang*, Coline Devin*, Vincent Vanhoucke, Sergey Levine. CoRL 2018\nRobustness via Retrying: Closed-Loop Robotic Manipulation with Self-Supervised Learning. [pdf] [Project]\nFrederik Ebert, Sudeep Dasari, Alex X. Lee, Sergey Levine, Chelsea Finn. CoRL 2018\n2019\nLearning Long-Range Perception Using Self-Supervision from Short-Range Sensors and Odometry. [pdf]\nMirko Nava, Jerome Guzzi, R. Omar Chavez-Garcia, Luca M. Gambardella, Alessandro Giusti. Robotics and Automation Letters\nLearning Latent Plans from Play. [pdf] [Project]\nCorey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, Pierre Sermanet\nSelf-Supervised Visual Terrain Classification from Unsupervised Acoustic Feature Learning. [pdf]\nJannik Zuern, Wolfram Burgard, Abhinav Valada\n2020\nAdversarial Skill Networks: Unsupervised Robot Skill Learning from Video. [pdf] [Project]\nOier Mees, Markus Merklinger, Gabriel Kalweit, Wolfram Burgard ICRA 2020\nNLP\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. [pdf] [link]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. NAACL 2019 Best Long Paper\nSelf-Supervised Dialogue Learning [pdf]\nJiawei Wu, Xin Wang, William Yang Wang. ACL 2019\nSelf-Supervised Learning for Contextualized Extractive Summarization [pdf]\nHong Wang, Xin Wang, Wenhan Xiong, Mo Yu, Xiaoxiao Guo, Shiyu Chang, William Yang Wang. ACL 2019\nA Mutual Information Maximization Perspective of Language Representation Learning [pdf]\nLingpeng Kong, Cyprien de Masson d'Autume, Lei Yu, Wang Ling, Zihang Dai, Dani Yogatama. ICLR 2020\nVL-BERT: Pre-training of Generic Visual-Linguistic Representations [pdf] [code]\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai. ICLR 2020\nA Simple and Effective Self-Supervised Contrastive Learning Framework for Aspect Detection [pdf] [code]\nTian Shi, Liuqing Li, Ping Wang, and Chandan K. Reddy. AAAI 2021\nSelf-Guided Contrastive Learning for BERT Sentence Representations [pdf] [code]\nTaeuk Kim, Kang Min Yoo, and Sang-goo Lee. ACL 2021\nASR\nwav2vec: Unsupervised Pre-Training for Speech Recognition [pdf] [code]\nSteffen Schneider, Alexei Baevski, Ronan Collobert, Michael Auli. INTERSPEECH 2019\nLearning Robust and Multilingual Speech Representations [pdf]\nKazuya Kawakami, Luyu Wang, Chris Dyer, Phil Blunsom, Aaron van den Oord. Findings of EMNLP 2020\nUnsupervised Pretraining Transfers Well Across Languages [pdf] [code]\nMorgane Riviere, Armand Joulin, Pierre-Emmanuel Mazare, Emmanuel Dupoux. ICASSP 2020\nvq-wav2vec: Self-Supervised Learning of Discrete Speech Representations [pdf]\nAlexei Baevski, Steffen Schneider, Michael Auli. ICLR 2020\nEffectiveness of Self-supervised Pre-training for Speech Recognition [pdf]\nAlexei Baevski, Michael Auli, Abdelrahman Mohamed. ICASSP 2020\nTowards Unsupervised Speech Recognition and Synthesis with Quantized Speech Representation Learning [pdf]\nAlexander H. Liu, Tao Tu, Hung-yi Lee, Lin-shan Lee. ICASSP 2020\nSelf-Training for End-to-End Speech Recognition [pdf]\nJacob Kahn, Ann Lee, Awni Hannun. ICASSP 2020\nGenerative Pre-Training for Speech with Autoregressive Predictive Coding [pdf] [code]\nYu-An Chung, James Glass. ICASSP 2020\nDisentangled Speech Embeddings using Cross-modal Self-supervision [pdf]\nArsha Nagrani, Joon Son Chung, Samuel Albanie, Andrew Zisserman. ICASSP 2020\nMulti-task Self-supervised Learning for Robust Speech Recognition [pdf]\nMirco Ravanelli, Jianyuan Zhong, Santiago Pascual, Pawel Swietojanski, Joao Monteiro, Jan Trmal, Yoshua Bengio. ICASSP 2020\nVisually Guided Self Supervised Learning of Speech Representations [pdf]\nAbhinav Shukla, Konstantinos Vougioukas, Pingchuan Ma, Stavros Petridis, Maja Pantic. ICASSP 2020\nMockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders [pdf] [code]\nAndy T. Liu, Shu-wen Yang, Po-Han Chi, Po-chun Hsu, Hung-yi Lee. ICASSP 2020\nVector-Quantized Autoregressive Predictive Coding [pdf] [code]\nYu-An Chung, Hao Tang, James Glass. Interspeech 2020\nwav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations [pdf] [code]\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, Michael Auli. NeurIPS 2020\nRobust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training [pdf] [code]\nWei-Ning Hsu, Anuroop Sriram, Alexei Baevski, Tatiana Likhomanenko, Qiantong Xu, Vineel Pratap, Jacob Kahn, Ann Lee, Ronan Collobert, Gabriel Synnaeve, Michael Auli\nHuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units [pdf] [code]\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed. ICASSP 2021\nUnsupervised Speech Recognition [pdf] [code]\nAlexei Baevski, Wei-Ning Hsu, Alexis Conneau, Michael Auli\nTERA: Self-Supervised Learning of Transformer Encoder Representation for Speech [pdf] [code]\nAndy T. Liu, Shang-Wen Li, Hung-yi Lee. IEEE/ACM TASLP 2021\nNon-Autoregressive Predictive Coding for Learning Speech Representations from Local Dependencies [pdf] [code]\nAlexander H. Liu, Yu-An Chung, James Glass. Interspeech 2021\nGraph\nDeep Graph Infomax [pdf] [code]\nPetar Veli\u010dkovi\u0107, William Fedus, William L. Hamilton, Pietro Li\u00f2, Yoshua Bengio, R Devon Hjelm. ICLR 2019\nWhen Does Self-Supervision Help Graph Convolutional Networks [pdf]\nYuning You, Tianlong Chen, Zhangyang Wang, Yang Shen. ICML 2020\nMulti-Stage Self-Supervised Learning for Graph Convolutional Networks on Graphs with Few Labels [pdf]\nKe Sun, Zhouchen Lin, Zhanxing Zhu. AAAI 2020\nGaining insight into SARS-CoV-2 infection and COVID-19 severity using self-supervised edge features and Graph Neural Networks [pdf]\nArijit Sehanobish, Neal G. Ravindra, David van Dijk. ICML 2020 Workshop\nDeep Graph Contrastive Representation Learning [pdf] [code]\nYanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, Liang Wang. ICML 2020 Workshop\nContrastive Multi-View Representation Learning on Graphs [pdf]\nKaveh Hassani, Amir Hosein Khasahmadi. ICML 2020\nGCC: Graph Contrastive Coding for Graph Neural Network Pre-Training [pdf]\nJiezhong Qiu, Qibin Chen, Yuxiao Dong. KDD 2020\nGPT-GNN: Generative Pre-Training of Graph Neural Networks [pdf] [code]\nZiniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, Yizhou Sun. KDD 2020\nSelf-supervised Learning on Graphs: Deep Insights and New Direction [pdf]\nWei Jin, Tyler Derr, Haochen Liu, Yiqi Wang, Suhang Wang, Zitao Liu, Jiliang Tang.\nSelf-Supervised Learning of Contextual Embeddings for Link Prediction in Heterogeneous Networks [pdf] [code]\nPing Wang, Khushbu Agarwal, Colby Ham, Sutanay Choudhury, and Chandan K. Reddy. WWW 2021\nSelf-Supervised Hyperboloid Representations from Logical Queries over Knowledge Graphs [pdf] [code]\nNurendra Choudhary, Nikhil Rao, Sumeet Katariya, Karthik Subbian, and Chandan K. Reddy. WWW 2021\nTalks\nThe power of Self-Learning Systems. Demis Hassabis (DeepMind). [link]\nSupersizing Self-Supervision: Learning Perception and Action without Human Supervision. Abhinav Gupta (CMU). [link]\nSelf-supervision, Meta-supervision, Curiosity: Making Computers Study Harder. Alyosha Efros (UCB) [link]\nUnsupervised Visual Learning Tutorial. CVPR 2018 [part 1] [part 2]\nSelf-Supervised Learning. Andrew Zisserman (Oxford & Deepmind). [pdf]\nGraph Embeddings, Content Understanding, & Self-Supervised Learning. Yann LeCun. (NYU & FAIR) [pdf] [video]\nSelf-supervised learning: could machines learn like humans? Yann LeCun @EPFL. [video]\nWeek 9 (b): CS294-158 Deep Unsupervised Learning(Spring 2019). Alyosha Efros @UC Berkeley. [video]\nThesis\nSupervision Beyond Manual Annotations for Learning Visual Representations. Carl Doersch. [pdf].\nImage Synthesis for Self-Supervised Visual Representation Learning. Richard Zhang. [pdf].\nVisual Learning beyond Direct Supervision. Tinghui Zhou. [pdf].\nVisual Learning with Minimal Human Supervision. Ishan Misra. [pdf].\nBlog\nSelf-Supervised Representation Learning. Lilian Weng. [link].\nSelf Supervised Representation Learning in NLP. Amit Chaudhary. [link].\nThe Illustrated [Self-Supervised Learning], [SimCLR], [PIRL], [Self-Labelling], [FixMatch], [DeepCluster]. Amit Chaudhary.\nContrastive Self-Supervised Learning. Ankesh Anand. [link].\nLicense\nTo the extent possible under law, Zhongzheng Ren has waived all copyright and related or neighboring rights to this work.", "link": "https://github.com/jason718/awesome-self-supervised-learning", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "awesome self-supervised learning\na curated list of awesome self-supervised learning resources. inspired by awesome-deep-vision, awesome-adversarial-machine-learning, awesome-deep-learning-papers, and awesome-architecture-search\nwhy self-supervised?\nself-supervised learning has become an exciting direction in ai community.\njitendra malik: \"supervision is the opium of the ai researcher\"\nalyosha efros: \"the ai revolution will not be supervised\"\nyann lecun: \"self-supervised learning is the cake, supervised learning is the icing on the cake, reinforcement learning is the cherry on the cake\"\ncontributing\nplease help contribute this list by contacting me or add pull request\nmarkdown format:\n- paper name.\n[[pdf]](link)\n[[code]](link)\n- author 1, author 2, and author 3. *conference year*\ntable of contents\ntheory\ncomputer vision (cv)\nsurvey\nimage representation learning\nvideo representation learning\ngeometry\naudio\nothers\nmachine learning\nreinforcement learning\nrecommendation systems\nrobotics\nnatural language processing (nlp)\nautomatic speech recognition (asr)\ngraph\ntalks\nthesis\nblog\ntheory\na theoretical analysis of contrastive unsupervised representation learning. [pdf]\nsanjeev arora, hrishikesh khandeparkar, mikhail khodak, orestis plevrakis, and nikunj saunshi. icml 2019\nunderstanding the behaviour of contrastive loss. [pdf]\nfeng wang and huaping liu. cvpr 2021\npredicting what you already know helps: provable self-supervised learning. [pdf]\njason d. lee, qi lei, nikunj saunshi, and jiacheng zhuo.\ncontrastive learning , multi-view redundancy , and linear models. [pdf]\nchristopher tosh, akshay krishnamurthy, and daniel hsu.\nunderstanding self-supervised learning with dual deep networks. [pdf]\nyuandong tian, lantao yu, xinlei chen, and surya ganguli.\nfor self-supervised learning, rationality implies generalization, provably. [pdf]\nyamini bansal, gal kaplun, and boaz barak.\ncan pretext-based self-supervised learning be boosted by downstream data? a theoretical analysis. [pdf]\njiaye teng and weiran huang.\ncomputer vision\nsurvey\ncontrastive representation learning: a framework and review [pdf]\nphuc h. le-khac, graham healy, alan f. smeaton. ieee access 2020\na survey on contrastive self-supervised learning [pdf]\nashish jaiswal, ashwin r babu, mohammad z zadeh, debapriya banerjee, fillia makedon\nself-supervised visual feature learning with deep neural networks: a survey. [pdf]\nlonglong jing and yingli tian. t-pami 2020\nself-supervised learning: generative or contrastive [pdf]\nxiao liu, fanjin zhang, zhenyu hou, li mian, zhaoyu wang, jing zhang, jie tang.\nimage representation learning\nbenchmark code\nfair self-supervision benchmark [pdf] [repo]: various benchmark (and legacy) tasks for evaluating quality of visual representations learned by various self-supervision approaches.\nhow well do self-supervised models transfer? [pdf] [repo]: a benchmark for evaluating self-supervision consisting of many-shot/few-shot recognition, object detection, surface normal estimation and semantic segmentation.\n2015\nunsupervised visual representation learning by context prediction. [pdf] [code]\ndoersch, carl and gupta, abhinav and efros, alexei a. iccv 2015\nunsupervised learning of visual representations using videos. [pdf] [code]\nwang, xiaolong and gupta, abhinav. iccv 2015\nlearning to see by moving. [pdf] [code]\nagrawal, pulkit and carreira, joao and malik, jitendra. iccv 2015\nlearning image representations tied to ego-motion. [pdf] [code]\njayaraman, dinesh and grauman, kristen. iccv 2015\n2016\njoint unsupervised learning of deep representations and image clusters. [pdf] [code-torch] [code-caffe]\njianwei yang, devi parikh, dhruv batra. cvpr 2016\nunsupervised deep embedding for clustering analysis. [pdf] [code]\njunyuan xie, ross girshick, and ali farhadi. icml 2016\nslow and steady feature analysis: higher order temporal coherence in video. [pdf]\njayaraman, dinesh and grauman, kristen. cvpr 2016\ncontext encoders: feature learning by inpainting. [pdf] [code]\npathak, deepak and krahenbuhl, philipp and donahue, jeff and darrell, trevor and efros, alexei a. cvpr 2016\ncolorful image colorization. [pdf] [code]\nzhang, richard and isola, phillip and efros, alexei a. eccv 2016\nunsupervised learning of visual representations by solving jigsaw puzzles. [pdf] [code]\nnoroozi, mehdi and favaro, paolo. eccv 2016\nambient sound provides supervision for visual learning. [pdf] [code]\nowens, andrew and wu, jiajun and mcdermott, josh and freeman, william and torralba, antonio. eccv 2016\nlearning representations for automatic colorization. [pdf] [code]\nlarsson, gustav and maire, michael and shakhnarovich, gregory. eccv 2016\nunsupervised visual representation learning by graph-based consistent constraints. [pdf] [code]\nli, dong and hung, wei-chih and huang, jia-bin and wang, shengjin and ahuja, narendra and yang, ming-hsuan. eccv 2016\n2017\nadversarial feature learning. [pdf] [code]\ndonahue, jeff and krahenbuhl, philipp and darrell, trevor. iclr 2017\nself-supervised learning of visual features through embedding images into text topic spaces. [pdf] [code]\nl. gomez* and y. patel* and m. rusi\u00f1ol and d. karatzas and c.v. jawahar. cvpr 2017\nsplit-brain autoencoders: unsupervised learning by cross-channel prediction. [pdf] [code]\nzhang, richard and isola, phillip and efros, alexei a. cvpr 2017\nlearning features by watching objects move. [pdf] [code]\npathak, deepak and girshick, ross and dollar, piotr and darrell, trevor and hariharan, bharath. cvpr 2017\ncolorization as a proxy task for visual understanding. [pdf] [code]\nlarsson, gustav and maire, michael and shakhnarovich, gregory. cvpr 2017\ndeeppermnet: visual permutation learning. [pdf] [code]\ncruz, rodrigo santa and fernando, basura and cherian, anoop and gould, stephen. cvpr 2017\nunsupervised learning by predicting noise. [pdf] [code]\nbojanowski, piotr and joulin, armand. icml 2017\nmulti-task self-supervised visual learning. [pdf]\ndoersch, carl and zisserman, andrew. iccv 2017\nrepresentation learning by learning to count. [pdf]\nnoroozi, mehdi and pirsiavash, hamed and favaro, paolo. iccv 2017\ntransitive invariance for self-supervised visual representation learning. [pdf]\nwang, xiaolong and he, kaiming and gupta, abhinav. iccv 2017\nlook, listen and learn. [pdf]\nrelja, arandjelovic and zisserman, andrew. iccv 2017\nunsupervised representation learning by sorting sequences. [pdf] [code]\nhsin-ying lee, jia-bin huang, maneesh kumar singh, and ming-hsuan yang. iccv 2017\n2018\nunsupervised feature learning via non-parameteric instance discrimination [pdf] [code]\nzhirong wu, yuanjun xiong and x yu stella and dahua lin. cvpr 2018\nlearning image representations by completing damaged jigsaw puzzles. [pdf] [code]\nkim, dahun and cho, donghyeon and yoo, donggeun and kweon, in so. wacv 2018\nunsupervised representation learning by predicting image rotations. [pdf] [code]\nspyros gidaris and praveer singh and nikos komodakis. iclr 2018\nlearning latent representations in neural networks for clustering through pseudo supervision and graph-based activity regularization. [pdf] [code]\nozsel kilinc and ismail uysal. iclr 2018\nimprovements to context based self-supervised learning. [pdf]\nterrell mundhenk and daniel ho and barry chen. cvpr 2018\nself-supervised feature learning by learning to spot artifacts. [pdf] [code]\nsimon jenni and universit\u00e4t bern and paolo favaro. cvpr 2018\nboosting self-supervised learning via knowledge transfer. [pdf]\nmehdi noroozi and ananth vinjimoor and paolo favaro and hamed pirsiavash. cvpr 2018\ncross-domain self-supervised multi-task feature learning using synthetic imagery. [pdf] [code]\nzhongzheng ren and yong jae lee. cvpr 2018\nshapecodes: self-supervised feature learning by lifting views to viewgrids. [pdf]\ndinesh jayaraman*, uc berkeley; ruohan gao, university of texas at austin; kristen grauman. eccv 2018\ndeep clustering for unsupervised learning of visual features [pdf] [code]\nmathilde caron, piotr bojanowski, armand joulin, matthijs douze. eccv 2018\ncross pixel optical-flow similarity for self-supervised learning. [pdf]\naravindh mahendran, james thewlis, andrea vedaldi. accv 2018\n2019\nrepresentation learning with contrastive predictive coding. [pdf]\naaron van den oord, yazhe li, oriol vinyals.\nself-supervised learning via conditional motion propagation. [pdf] [code]\nxiaohang zhan, xingang pan, ziwei liu, dahua lin, and chen change loy. cvpr 2019\nself-supervised representation learning by rotation feature decoupling. [pdf] [code]\nzeyu feng; chang xu; dacheng tao. cvpr 2019\nrevisiting self-supervised visual representation learning. [pdf] [code]\nalexander kolesnikov; xiaohua zhai; lucas beye. cvpr 2019\nself-supervised gans via auxiliary rotation loss. [pdf] [code]\nting chen; xiaohua zhai; marvin ritter; mario lucic; neil houlsby. cvpr 2019\naet vs. aed: unsupervised representation learning by auto-encoding transformations rather than data. [pdf] [code]\nliheng zhang, guo-jun qi, liqiang wang, jiebo luo. cvpr 2019\nunsupervised deep learning by neighbourhood discovery. [pdf]. [code].\njiabo huang, qi dong, shaogang gong, xiatian zhu. icml 2019\ncontrastive multiview coding. [pdf] [code]\nyonglong tian and dilip krishnan and phillip isola.\nlarge scale adversarial representation learning. [pdf]\njeff donahue, karen simonyan.\nlearning representations by maximizing mutual information across views. [pdf] [code]\nphilip bachman, r devon hjelm, william buchwalter\nselfie: self-supervised pretraining for image embedding. [pdf]\ntrieu h. trinh, minh-thang luong, quoc v. le\ndata-efficient image recognition with contrastive predictive coding [pdf]\nolivier j. he \u0301naff, ali razavi, carl doersch, s. m. ali eslami, aaron van den oord\nusing self-supervised learning can improve model robustness and uncertainty [pdf] [code]\ndan hendrycks, mantas mazeika, saurav kadavath, dawn song. neurips 2019\nboosting few-shot visual learning with self-supervision [pdf]\npyros gidaris, andrei bursuc, nikos komodakis, patrick p\u00e9rez, and matthieu cord. iccv 2019\nself-supervised generalisation with meta auxiliary learning [pdf] [code]\nshikun liu, andrew j. davison, edward johns. neurips 2019\nwasserstein dependency measure for representation learning [pdf] [code]\nsherjil ozair, corey lynch, yoshua bengio, aaron van den oord, sergey levine, pierre sermanet. neurips 2019\nscaling and benchmarking self-supervised visual representation learning [pdf] [code]\npriya goyal, dhruv mahajan, abhinav gupta, ishan misra. iccv 2019\nunsupervised pre-training of image features on non-curated data [pdf] [code]\nmathilde caron, piotr bojanowski, julien mairal, armand joulin. iccv 2019 oral\ns4l: self-supervised semi-supervised learning [pdf] [code]\nxiaohua zhai, avital oliver, alexander kolesnikov, lucas beyer. iccv 2019\nself-supervised model adaptation for multimodal semantic segmentation. [pdf] [code]\nabhinav valada, rohit mohan, and wolfram burgard. ijcv 2019\n2020\na critical analysis of self-supervision, or what we can learn from a single image [pdf] [code]\nyuki m. asano, christian rupprecht, andrea vedaldi. iclr 2020\non mutual information maximization for representation learning [pdf] [code]\nmichael tschannen, josip djolonga, paul k. rubenstein, sylvain gelly, mario lucic. iclr 2020\nunderstanding the limitations of variational mutual information estimators [pdf] [code]\njiaming song, stefano ermon. iclr 2020\nself-labelling via simultaneous clustering and representation learning [pdf] [blogpost] [code]\nyuki markus asano, christian rupprecht, andrea vedaldi. iclr 2020 (spotlight)\nself-supervised label augmentation via input transformations [pdf] [code]\nhankook lee, sung ju hwang, jinwoo shin. icml 2020\nautomatic shortcut removal for self-supervised representation learning [pdf]\nmatthias minderer, olivier bachem, neil houlsby, michael tschannen\na simple framework for contrastive learning of visual representations [pdf] [code]\nting chen, simon kornblith, mohammad norouzi, geoffrey hinton. icml 2020\nhow useful is self-supervised pretraining for visual tasks? [pdf] [code]\nalejandro newell, jia deng. cvpr 2020\nmomentum contrast for unsupervised visual representation learning [pdf] [code]\nkaiming he, haoqi fan, yuxin wu, saining xie, ross girshick. cvpr 2020\nclusterfit: improving generalization of visual representations [pdf]\nxueting yan*, ishan misra*, abhinav gupta, deepti ghadiyaram**, dhruv mahajan**. cvpr 2020\nself-supervised learning of pretext-invariant representations [pdf]\nishan misra, laurens van der maaten. cvpr 2020\nbootstrap your own latent: a new approach to self-supervised learning [pdf] [unofficial-code]\njean-bastien grill, florian strub, florent altch\u00e9, corentin tallec, pierre h. richemond, elena buchatskaya, carl doersch, bernardo avila pires, zhaohan daniel guo, mohammad gheshlaghi azar, bilal piot, koray kavukcuoglu, r\u00e9mi munos, michal valko. neurips 2020, oral\ncontrastive learning of global and local features for medical image segmentation with limited annotations [pdf] [code]\nkrishna chaitanya, ertunc erdil, neerav karani, ender konukoglu. neurips 2020, oral\nunsupervised representation learning by invariancepropagation [pdf] [code]\nfeng wang, huaping liu, di guo, fuchun sun. neurips 2020, spotlight\nbig self-supervised models are strong semi-supervised learners [pdf] [code]\nting chen, simon kornblith, kevin swersky, mohammad norouzi, geoffrey hinton. neurips 2020\nself-supervised prototypical transfer learning for few-shot classification [pdf] [code]\ncarlos medina, arnout devos, matthias grossglauser\nscan: learning to classify images without labels [pdf] [code]\nwouter van gansbeke, simon vandenhende, stamatios georgoulis, marc proesmans, luc van gool. eccv 2020\nunsupervised learning of visual features by contrasting cluster assignments [pdf] [code]\nmathilde caron, ishan misra, julien mairal, priya goyal, piotr bojanowski, armand joulin. neurips 2020\nself-supervised relational reasoning for representation learning [pdf] [code]\nmassimiliano patacchiola, amos storkey. neurips 2020, spotlight\nexploring simple siamese representation learning [pdf] [unofficial-code]\nxinlei chen, kaiming he\nonline bag-of-visual-words generation for unsupervised representation learning [pdf] [code]\nspyros gidaris, andrei bursuc, gilles puy, nikos komodakis, matthieu cord, patrick p\u00e9rez\nrethinking the value of labels for improving class-imbalanced learning [pdf] [code]\nyuzhe yang, zhi xu. neurips 2020\ndemystifying contrastive self-supervised learning: invariances, augmentations and dataset biases [pdf]\nsenthil purushwalkam, abhinav gupta. neurips 2020\n2021\nbarlow twins: self-supervised learning via redundancy reduction [pdf] [code]\nzbontar, j., jing, l., misra, i., lecun, y., & deny, s.\ncontrastive semi-supervised learning for 2d medical image segmentation [pdf]\nprashant pandey, ajey pai, nisarg bhatt, prasenjit das, govind makharia, prathosh ap, mausam. miccai 2021\npropagate yourself: exploring pixel-level consistency for unsupervised visual representation learning [pdf] [code]\nzhenda xie, yutong lin, zheng zhang, yue cao, stephen lin, and han hu. cvpr 2021\nhow well do self-supervised models transfer? [pdf] [code]\nlinus ericsson, henry gouk, timothy m. hospedales. cvpr 2021\nvectorization and rasterization: self-supervised learning for sketch and handwriting.\n[code]\nayan kumar bhunia, pinaki nath chowdhury, yongxin yang, timothy hospedales, tao xiang, yi-zhe song. cvpr 2021\nselfaugment: automatic augmentation policies for self-supervised learning [pdf] [code]\ncolorado reed, sean metzger, aravind srinivas, trevor darrell, kurt keutzer. cvpr 2021\njigsaw clustering for unsupervised visual representation learning [pdf] [code]\npengguang chen, shu liu, jiaya jia. cvpr 2021\nimproving contrastive learning by visualizing feature transformation [pdf] [code]\nrui zhu*, bingchen zhao*, jingen liu, zhenglong sun, chang wen chen. iccv 2021 oral\nvideo representation learning\nunsupervised learning of video representations using lstms. [pdf] [code]\nsrivastava, nitish and mansimov, elman and salakhudinov, ruslan. icml 2015\nshuffle and learn: unsupervised learning using temporal order verification. [pdf] [code]\nishan misra, c. lawrence zitnick and martial hebert. eccv 2016\nlstm self-supervision for detailed behavior analysis [pdf]\nbiagio brattoli*, uta b\u00fcchler*, anna-sophia wahl, martin e. schwab, and bj\u00f6rn ommer. cvpr 2017\nself-supervised video representation learning with odd-one-out networks. [pdf]\nbasura fernando and hakan bilen and efstratios gavves and stephen gould. cvpr 2017\nunsupervised learning of long-term motion dynamics for videos. [pdf]\nluo, zelun and peng, boya and huang, de-an and alahi, alexandre and fei-fei, li. cvpr 2017\ngeometry guided convolutional neural networks for self-supervised video representation learning. [pdf]\nchuang gan and boqing gong and kun liu and hao su and leonidas j. guibas. cvpr 2018\nimproving spatiotemporal self-supervision by deep reinforcement learning. [pdf]\nbiagio brattoli*, uta b\u00fcchler*, and bj\u00f6rn ommer. eccv 2018\nself-supervised learning of a facial attribute embedding from video. [pdf]\nwiles, o., koepke, a.s., zisserman, a. bmvc 2018\nself-supervised video representation learning with space-time cubic puzzles. [pdf]\nkim, dahun and cho, donghyeon and yoo, donggeun and kweon, in so. aaai 2019\nself-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics. [pdf]\njiangliu wang; jianbo jiao; linchao bao; shengfeng he; yunhui liu; wei liu. cvpr 2019\ndynamonet: dynamic action and motion network. [pdf]\nali diba; vivek sharma, luc van gool, rainer stiefelhagen. iccv 2019\nlearning correspondence from the cycle-consistency of time. [pdf] [code]\nxiaolong wang*, allan jabri* and alexei a. efros. cvpr 2019\njoint-task self-supervised learning for temporal correspondence. [pdf] [code]\nxueting li*, sifei liu*, shalini de mello, xiaolong wang, jan kautz, and ming-hsuan yang. nips 2019\nself-supervised video representation learning using inter-intra contrstive framework [pdf] [code]\nli tao, xueting wang*, toshihiko yamasaki. acmmm 2020\nvideo playback rate perception for self-supervised spatio-temporal representation learning [pdf] [code]\nyuan yao*, chang liu*, dezhao luo, yu zhou, qixiang ye. cvpr 2020\nself-supervised video representation learning by pace prediction [pdf] [code]\njiangliu wang, jianbo jiao, yun-hui liu. eccv 2020\nvideo representation learning by recognizing temporal transformations [pdf] [code]\nsimon jenni, givi meishvili, paolo favaro. eccv 2020\nself-supervised co-training for video representation learning [pdf] [code]\ntengda han, weidi xie, and andrew zisserman. neurips 2020\ncycle-contrast for self-supervised video representation learning [pdf]\nquan kong, wenpeng wei, ziwei deng, tomoaki yoshinaga, and tomokazu murakami. neurips 2020\nvideo representation learning with visual tempo consistency [pdf] [code]\nceyuan yang, yinghao xu, bo dai, and bolei zhou\nself-supervised video representation learning by uncovering spatio-temporal statistics [pdf]\njiangliu wang, jianbo jiao, linchao bao, shengfeng he, wei liu, and yun-hui liu\nspatiotemporal contrastive video representation learning [pdf]\nrui qian, tianjian meng, boqing gong, ming-hsuan yang, huisheng wang, serge belongie, and yin cui\nself-supervised video representation using pretext-contrastive learning [pdf]\nli tao, xueting wang, and toshihiko yamasaki\nunsupervised video representation learning by bidirectional feature prediction [pdf]\nnadine behrmann, juergen gall, and mehdi noroozi\nrspnet: relative speed perception for unsupervised video representation learning [pdf] [code]\npeihao chen, deng huang, dongliang he, xiang long, runhao zeng, shilei wen, mingkui tan, and chuang gan. aaai 2021\nhierarchically decoupled spatial-temporal contrast for self-supervised video representation learning [pdf]\nzehua zhang and david crandall\ncan temporal information help with contrastive self-supervised learning? [pdf]\nyutong bai, haoqi fan, ishan misra, ganesh venkatesh, yongyi lu, yuyin zhou, qihang yu, vikas chandra, and alan yuille\nenhancing unsupervised video representation learning by decoupling the scene and the motion [pdf] [code]\njinpeng wang, yuting gao, ke li, jianguo hu, xinyang jiang, xiaowei guo, rongrong ji, and xing sun. aaai 2021\nspace-time correspondence as a contrastive random walk [pdf] [code] [project]\nallan jabri, andrew owens, alexei a. efros. neurips 2020 oral\ngeometry\nunsupervised cnn for single view depth estimation: geometry to the rescue. [pdf] [code]\nravi garg, vijay kumar bg, gustavo carneiro, ian reid. eccv 2016\nself-supervised learning of motion capture. [pdf] [code] [web]\ntung, hsiao-yu and tung, hsiao-wei and yumer, ersin and fragkiadaki, katerina. nips 2017\nunsupervised learning of object frames by dense equivariant image labelling. [pdf]\njames thewlis, hakan bilen, andrea vedaldi. neurips 2017\nunsupervised learning of depth and ego-motion from video. [pdf] [code] [web]\nzhou, tinghui and brown, matthew and snavely, noah and lowe, david g. cvpr 2017\nactive stereo net: end-to-end self-supervised learning for active stereo systems. [project]\nyinda zhang*, sean fanello, sameh khamis, christoph rhemann, julien valentin, adarsh kowdle, vladimir tankovich, shahram izadi, thomas funkhouser. eccv 2018\nself-supervised relative depth learning for urban scene understanding. [pdf] [project]\nhuaizu jiang*, erik learned-miller, gustav larsson, michael maire, greg shakhnarovich. eccv 2018\ngeometry-aware learning of maps for camera localization. [pdf] [code]\nsamarth brahmbhatt, jinwei gu, kihwan kim, james hays, and jan kautz. cvpr 2018\nself-supervised learning of geometrically stable features through probabilistic introspection. [pdf] [web]\ndavid novotny, samuel albanie, diane larlus, andrea vedaldi. cvpr 2018\nself-supervised learning of 3d human pose using multi-view geometry. [pdf]\nmuhammed kocabas; salih karagoz; emre akbas. cvpr 2019\nselflow: self-supervised learning of optical flow. [pdf]\njiangliu wang; jianbo jiao; linchao bao; shengfeng he; yunhui liu; wei liu. cvpr 2019\nunsupervised learning of landmarks by descriptor vector exchange. [pdf] [code] [web]\njames thewlis, samuel albanie, hakan bilen, andrea vedaldi. iccv 2019\naudio\naudio-visual scene analysis with self-supervised multisensory features. [pdf] [code]\nandrew owens, alexei a. efros. eccv 2018\nobjects that sound. [pdf]\nr. arandjelovi\u0107, a. zisserman. eccv 2018\nlearning to separate object sounds by watching unlabeled video. [pdf] [project]\nruohan gao, rogerio feris, kristen grauman. eccv 2018\nthe sound of pixels. [pdf] [project]\nzhao, hang and gan, chuang and rouditchenko, andrew and vondrick, carl and mcdermott, josh and torralba, antonio. eccv 2018\nlearnable pins: cross-modal embeddings for person identity. [pdf] [web]\narsha nagrani, samuel albanie, andrew zisserman. eccv 2018\ncooperative learning of audio and video models from self-supervised synchronization. [pdf]\nbruno korbar,dartmouth college, du tran, lorenzo torresani. nips 2018\nself-supervised generation of spatial audio for 360\u00b0 video. [pdf]\npedro morgado, nuno nvasconcelos, timothy langlois, oliver wang. nips 2018\ntricycle: audio representation learning from sensor network data using self-supervision [pdf]\nmark cartwright, jason cramer, justin salamon, juan pablo bello. waspaa 2019\nself-supervised audio-visual co-segmentation [pdf]\nandrew rouditchenko, hang zhao, chuang gan, josh mcdermott, and antonio torralba. icassp 2019\ndoes visual self-supervision improve learning of speech representations? [pdf]\nabhinav shukla, stavros petridis, maja pantic\nthere is more than meets the eye: self-supervised multi-object detection and tracking with sound by distilling multimodal knowledge [pdf] [code]\nfrancisco rivera valverde, juana valeria hurtado, and abhinav valada. cvpr 2021\nbyol for audio: self-supervised learning for general-purpose audio representation. [pdf] [code]\ndaisuke niizumi; daiki takeuchi; yasunori ohishi ijcnn 2021\nothers\nself-learning scene-specific pedestrian detectors using a progressive latent model. [pdf]\nqixiang ye, tianliang zhang, qiang qiu, baochang zhang, jie chen, guillermo sapiro. cvpr 2017\nfree supervision from video games. [pdf] [project+code]\nphilipp kr\u00e4henb\u00fchl. cvpr 2018\nfighting fake news: image splice detection via learned self-consistency [pdf] [code]\nminyoung huh*, andrew liu*, andrew owens, alexei a. efros. eccv 2018\nself-supervised tracking by colorization (tracking emerges by colorizing videos). [pdf]\ncarl vondrick*, abhinav shrivastava, alireza fathi, sergio guadarrama, kevin murphy. eccv 2018\nhigh-fidelity image generation with fewer labels. [pdf]\nmario lucic*, michael tschannen*, marvin ritter*, xiaohua zhai, olivier bachem, sylvain gelly.\nself-supervised fitting of articulated meshes to point clouds.\nchun-liang li, tomas simon, jason saragih, barnab\u00e1s p\u00f3czos and yaser sheikh. cvpr 2019\njust go with the flow: self-supervised scene flow estimation [pdf] [code]\nhimangi mittal, brian okorn, david held. cvpr 2020\nscops: self-supervised co-part segmentation.\nwei-chih hung, varun jampani, sifei liu, pavlo molchanov, ming-hsuan yang, and jan kautz. cvpr 2019\nself-supervised adaptation of high-fidelity face models for monocular performance tracking.\njae shin yoon; takaaki shiratori; shoou-i yu; hyun soo park. cvpr 2019\nmulti-task self-supervised object detection via recycling of bounding box annotations. [pdf] [code]\nwonhee lee; joonil na; gunhee kim. cvpr 2019\nself-supervised convolutional subspace clustering network.\njunjian zhang; chun-guang li; chong you; xianbiao qi; honggang zhang; jun guo; zhouchen lin. cvpr 2019\nreinforced cross-modal matching and self-supervised imitation learning for vision-language navigation.\nxin wang; qiuyuan huang; asli celikyilmaz; jianfeng gao; dinghan shen; yuan-fang wang; william yang wang; lei zhang. cvpr 2019\nunsupervised 3d pose estimation with geometric self-supervision.\nching-hang chen; ambrish tyagi; amit agrawal; dylan drover; rohith mv; stefan stojanov; james m. rehg. cvpr 2019\nlearning to generate grounded image captions without localization supervision. [pdf]\nchih-yao ma; yannis kalantidis; ghassan alregib; peter vajda; marcus rohrbach; zsolt kira.\nvideobert: a joint model for video and language representation learning [pdf]\nchen sun, austin myers, carl vondrick, kevin murphy, cordelia schmid. iccv 2019\ncountering noisy labels by learning from auxiliary clean labels [pdf]\ntsung wei tsai, chongxuan li, jun zhu\nmachine learning\nself-taught learning: transfer learning from unlabeled data. [pdf]\nraina, rajat and battle, alexis and lee, honglak and packer, benjamin and ng, andrew y. icml 2007\nrepresentation learning: a review and new perspectives. [pdf]\nbengio, yoshua and courville, aaron and vincent, pascal. tpami 2013.\nreinforcement learning\ncuriosity-driven exploration by self-supervised prediction. [pdf] [code]\ndeepak pathak, pulkit agrawal, alexei a. efros, and trevor darrell. icml 2017\nlarge-scale study of curiosity-driven learning. [pdf]\nyuri burda*, harri edwards*, deepak pathak*, amos storkey, trevor darrell and alexei a. efros\nplaying hard exploration games by watching youtube. [pdf]\nyusuf aytar, tobias pfaff, david budden, tom le paine, ziyu wang, nando de freitas. nips 2018\nunsupervised state representation learning in atari. [pdf] [code]\nankesh anand, evan racah, sherjil ozair, yoshua bengio, marc-alexandre c\u00f4t\u00e9, r devon hjelm. neurips 2019\nrecommendation systems\nself-supervised learning for deep models in recommendations. [pdf]\ntiansheng yao, xinyang yi, derek zhiyuan cheng, felix yu, aditya menon, lichan hong, ed h. chi, steve tjoa, jieqi (jay)kang, evan ettinger preprint 2020\nrobotics\n2006\nimproving robot navigation through self-supervised online learning [pdf]\nboris sofman, ellie lin, j. andrew bagnell, nicolas vandapel, and anthony stentz\nreverse optical flow for self-supervised adaptive autonomous robot navigation [pdf]\na. lookingbill, d. lieb, j. rogers and j. curry\n2009\nlearning long-range vision for autonomous off-road driving [pdf]\nraia hadsell, pierre sermanet, jan ben, ayse erkan, marco scoffier, koray kavukcuoglu, urs muller, yann lecun\n2012\nself-supervised terrain classification for planetary surface exploration rovers [pdf]\nchristopher a. brooks, karl iagnemma\n2014\nterrain traversability analysis using multi-sensor data correlation by a mobile robot [pdf]\nmohammed abdessamad bekhti, yuichi kobayashi and kazuki matsumura\n2015\nonline self-supervised learning for dynamic object segmentation [pdf]\nvitor guizilini and fabio ramos, the international journal of robotics research\nself-supervised online learning of basic object push affordances [pdf]\nbarry ridge, ales leonardis, ales ude, miha denisa, and danijel skocaj\nself-supervised learning of grasp dependent -----> tool !!!  affordances on the icub humanoid robot [pdf]\ntanis mar, vadim tikhanoff, giorgio metta, and lorenzo natale\n2016\npersistent self-supervised learning principle: from stereo to monocular vision for obstacle avoidance [pdf]\nkevin van hecke, guido de croon, laurens van der maaten, daniel hennes, and dario izzo\nthe curious robot: learning visual representations via physical interactions. [pdf]\nlerrel pinto and dhiraj gandhi and yuanfeng han and yong-lae park and abhinav gupta. eccv 2016\nlearning to poke by poking: experiential learning of intuitive physics. [pdf]\nagrawal, pulkit and nair, ashvin v and abbeel, pieter and malik, jitendra and levine, sergey. nips 2016\nsupersizing self-supervision: learning to grasp from 50k tries and 700 robot hours. [pdf]\npinto, lerrel and gupta, abhinav. icra 2016\n2017\nsupervision via competition: robot adversaries for learning tasks. [pdf]\npinto, lerrel and davidson, james and gupta, abhinav. icra 2017\nmulti-view self-supervised deep learning for 6d pose estimation in the amazon picking challenge. [pdf] [project]\nandy zeng, kuan-ting yu, shuran song, daniel suo, ed walker jr., alberto rodriguez, jianxiong xiao. icra 2017\ncombining self-supervised learning and imitation for vision-based rope manipulation. [pdf] [project]\nashvin nair*, dian chen*, pulkit agrawal*, phillip isola, pieter abbeel, jitendra malik, sergey levine. icra 2017\nlearning to fly by crashing [pdf]\ndhiraj gandhi, lerrel pinto, abhinav gupta iros 2017\nself-supervised learning as an enabling technology for future space exploration robots: iss experiments on monocular distance learning [pdf]\nk. van hecke, g. c. de croon, d. hennes, t. p. setterfield, a. saenz- otero, and d. izzo\nunsupervised perceptual rewards for imitation learning. [pdf] [project]\nsermanet, pierre and xu, kelvin and levine, sergey. rss 2017\nself-supervised visual planning with temporal skip connections. [pdf]\nfrederik ebert, chelsea finn, alex x. lee, sergey levine. corl2017\n2018\ncassl: curriculum accelerated self-supervised learning. [pdf]\nadithyavairavan murali, lerrel pinto, dhiraj gandhi, abhinav gupta. icra 2018\ntime-contrastive networks: self-supervised learning from video. [pdf] [project]\npierre sermanet and corey lynch and yevgen chebotar and jasmine hsu and eric jang and stefan schaal and sergey levine. icra 2018\nself-supervised deep reinforcement learning with generalized computation graphs for robot navigation. [pdf]\ngregory kahn, adam villaflor, bosen ding, pieter abbeel, sergey levine. icra 2018\nlearning actionable representations from visual observations. [pdf] [project]\ndwibedi, debidatta and tompson, jonathan and lynch, corey and sermanet, pierre. iros 2018\nlearning synergies between pushing and grasping with self-supervised deep reinforcement learning. [pdf] [project]\nandy zeng, shuran song, stefan welker, johnny lee, alberto rodriguez, thomas funkhouser. iros 2018\nvisual reinforcement learning with imagined goals. [pdf] [project]\nashvin nair*, vitchyr pong*, murtaza dalal, shikhar bahl, steven lin, sergey levine.neurips 2018\ngrasp2vec: learning object representations from self-supervised grasping. [pdf] [project]\neric jang*, coline devin*, vincent vanhoucke, sergey levine. corl 2018\nrobustness via retrying: closed-loop robotic manipulation with self-supervised learning. [pdf] [project]\nfrederik ebert, sudeep dasari, alex x. lee, sergey levine, chelsea finn. corl 2018\n2019\nlearning long-range perception using self-supervision from short-range sensors and odometry. [pdf]\nmirko nava, jerome guzzi, r. omar chavez-garcia, luca m. gambardella, alessandro giusti. robotics and automation letters\nlearning latent plans from play. [pdf] [project]\ncorey lynch, mohi khansari, ted xiao, vikash kumar, jonathan tompson, sergey levine, pierre sermanet\nself-supervised visual terrain classification from unsupervised acoustic feature learning. [pdf]\njannik zuern, wolfram burgard, abhinav valada\n2020\nadversarial skill networks: unsupervised robot skill learning from video. [pdf] [project]\noier mees, markus merklinger, gabriel kalweit, wolfram burgard icra 2020\nnlp\nbert: pre-training of deep bidirectional transformers for language understanding. [pdf] [link]\njacob devlin, ming-wei chang, kenton lee, kristina toutanova. naacl 2019 best long paper\nself-supervised dialogue learning [pdf]\njiawei wu, xin wang, william yang wang. acl 2019\nself-supervised learning for contextualized extractive summarization [pdf]\nhong wang, xin wang, wenhan xiong, mo yu, xiaoxiao guo, shiyu chang, william yang wang. acl 2019\na mutual information maximization perspective of language representation learning [pdf]\nlingpeng kong, cyprien de masson d'autume, lei yu, wang ling, zihang dai, dani yogatama. iclr 2020\nvl-bert: pre-training of generic visual-linguistic representations [pdf] [code]\nweijie su, xizhou zhu, yue cao, bin li, lewei lu, furu wei, jifeng dai. iclr 2020\na simple and effective self-supervised contrastive learning framework for aspect detection [pdf] [code]\ntian shi, liuqing li, ping wang, and chandan k. reddy. aaai 2021\nself-guided contrastive learning for bert sentence representations [pdf] [code]\ntaeuk kim, kang min yoo, and sang-goo lee. acl 2021\nasr\nwav2vec: unsupervised pre-training for speech recognition [pdf] [code]\nsteffen schneider, alexei baevski, ronan collobert, michael auli. interspeech 2019\nlearning robust and multilingual speech representations [pdf]\nkazuya kawakami, luyu wang, chris dyer, phil blunsom, aaron van den oord. findings of emnlp 2020\nunsupervised pretraining transfers well across languages [pdf] [code]\nmorgane riviere, armand joulin, pierre-emmanuel mazare, emmanuel dupoux. icassp 2020\nvq-wav2vec: self-supervised learning of discrete speech representations [pdf]\nalexei baevski, steffen schneider, michael auli. iclr 2020\neffectiveness of self-supervised pre-training for speech recognition [pdf]\nalexei baevski, michael auli, abdelrahman mohamed. icassp 2020\ntowards unsupervised speech recognition and synthesis with quantized speech representation learning [pdf]\nalexander h. liu, tao tu, hung-yi lee, lin-shan lee. icassp 2020\nself-training for end-to-end speech recognition [pdf]\njacob kahn, ann lee, awni hannun. icassp 2020\ngenerative pre-training for speech with autoregressive predictive coding [pdf] [code]\nyu-an chung, james glass. icassp 2020\ndisentangled speech embeddings using cross-modal self-supervision [pdf]\narsha nagrani, joon son chung, samuel albanie, andrew zisserman. icassp 2020\nmulti-task self-supervised learning for robust speech recognition [pdf]\nmirco ravanelli, jianyuan zhong, santiago pascual, pawel swietojanski, joao monteiro, jan trmal, yoshua bengio. icassp 2020\nvisually guided self supervised learning of speech representations [pdf]\nabhinav shukla, konstantinos vougioukas, pingchuan ma, stavros petridis, maja pantic. icassp 2020\nmockingjay: unsupervised speech representation learning with deep bidirectional transformer encoders [pdf] [code]\nandy t. liu, shu-wen yang, po-han chi, po-chun hsu, hung-yi lee. icassp 2020\nvector-quantized autoregressive predictive coding [pdf] [code]\nyu-an chung, hao tang, james glass. interspeech 2020\nwav2vec 2.0: a framework for self-supervised learning of speech representations [pdf] [code]\nalexei baevski, yuhao zhou, abdelrahman mohamed, michael auli. neurips 2020\nrobust wav2vec 2.0: analyzing domain shift in self-supervised pre-training [pdf] [code]\nwei-ning hsu, anuroop sriram, alexei baevski, tatiana likhomanenko, qiantong xu, vineel pratap, jacob kahn, ann lee, ronan collobert, gabriel synnaeve, michael auli\nhubert: self-supervised speech representation learning by masked prediction of hidden units [pdf] [code]\nwei-ning hsu, benjamin bolte, yao-hung hubert tsai, kushal lakhotia, ruslan salakhutdinov, abdelrahman mohamed. icassp 2021\nunsupervised speech recognition [pdf] [code]\nalexei baevski, wei-ning hsu, alexis conneau, michael auli\ntera: self-supervised learning of transformer encoder representation for speech [pdf] [code]\nandy t. liu, shang-wen li, hung-yi lee. ieee/acm taslp 2021\nnon-autoregressive predictive coding for learning speech representations from local dependencies [pdf] [code]\nalexander h. liu, yu-an chung, james glass. interspeech 2021\ngraph\ndeep graph infomax [pdf] [code]\npetar veli\u010dkovi\u0107, william fedus, william l. hamilton, pietro li\u00f2, yoshua bengio, r devon hjelm. iclr 2019\nwhen does self-supervision help graph convolutional networks [pdf]\nyuning you, tianlong chen, zhangyang wang, yang shen. icml 2020\nmulti-stage self-supervised learning for graph convolutional networks on graphs with few labels [pdf]\nke sun, zhouchen lin, zhanxing zhu. aaai 2020\ngaining insight into sars-cov-2 infection and covid-19 severity using self-supervised edge features and graph neural networks [pdf]\narijit sehanobish, neal g. ravindra, david van dijk. icml 2020 workshop\ndeep graph contrastive representation learning [pdf] [code]\nyanqiao zhu, yichen xu, feng yu, qiang liu, shu wu, liang wang. icml 2020 workshop\ncontrastive multi-view representation learning on graphs [pdf]\nkaveh hassani, amir hosein khasahmadi. icml 2020\ngcc: graph contrastive coding for graph neural network pre-training [pdf]\njiezhong qiu, qibin chen, yuxiao dong. kdd 2020\ngpt-gnn: generative pre-training of graph neural networks [pdf] [code]\nziniu hu, yuxiao dong, kuansan wang, kai-wei chang, yizhou sun. kdd 2020\nself-supervised learning on graphs: deep insights and new direction [pdf]\nwei jin, tyler derr, haochen liu, yiqi wang, suhang wang, zitao liu, jiliang tang.\nself-supervised learning of contextual embeddings for link prediction in heterogeneous networks [pdf] [code]\nping wang, khushbu agarwal, colby ham, sutanay choudhury, and chandan k. reddy. www 2021\nself-supervised hyperboloid representations from logical queries over knowledge graphs [pdf] [code]\nnurendra choudhary, nikhil rao, sumeet katariya, karthik subbian, and chandan k. reddy. www 2021\ntalks\nthe power of self-learning systems. demis hassabis (deepmind). [link]\nsupersizing self-supervision: learning perception and action without human supervision. abhinav gupta (cmu). [link]\nself-supervision, meta-supervision, curiosity: making computers study harder. alyosha efros (ucb) [link]\nunsupervised visual learning tutorial. cvpr 2018 [part 1] [part 2]\nself-supervised learning. andrew zisserman (oxford & deepmind). [pdf]\ngraph embeddings, content understanding, & self-supervised learning. yann lecun. (nyu & fair) [pdf] [video]\nself-supervised learning: could machines learn like humans? yann lecun @epfl. [video]\nweek 9 (b): cs294-158 deep unsupervised learning(spring 2019). alyosha efros @uc berkeley. [video]\nthesis\nsupervision beyond manual annotations for learning visual representations. carl doersch. [pdf].\nimage synthesis for self-supervised visual representation learning. richard zhang. [pdf].\nvisual learning beyond direct supervision. tinghui zhou. [pdf].\nvisual learning with minimal human supervision. ishan misra. [pdf].\nblog\nself-supervised representation learning. lilian weng. [link].\nself supervised representation learning in nlp. amit chaudhary. [link].\nthe illustrated [self-supervised learning], [simclr], [pirl], [self-labelling], [fixmatch], [deepcluster]. amit chaudhary.\ncontrastive self-supervised learning. ankesh anand. [link].\nlicense\nto the extent possible under law, zhongzheng ren has waived all copyright and related or neighboring rights to this work.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000010, "year": null}, {"Unnamed: 0": 1036, "autor": 16, "date": null, "content": "Stable Baselines3\nStable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of Stable Baselines.\nYou can read a detailed presentation of Stable Baselines3 in the v1.0 blog post or our JMLR paper.\nThese algorithms will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details.\nNote: despite its simplicity of use, Stable Baselines3 (SB3) assumes you have some knowledge about Reinforcement Learning (RL). You should not utilize this library without some practice. To that extent, we provide good resources in the documentation to get started with RL.\nMain Features\nThe performance of each algorithm was tested (see Results section in their respective page), you can take a look at the issues #48 and #49 for more details.\nFeatures Stable-Baselines3\nState of the art RL methods \u2714\ufe0f\nDocumentation \u2714\ufe0f\nCustom environments \u2714\ufe0f\nCustom policies \u2714\ufe0f\nCommon interface \u2714\ufe0f\nDict observation space support \u2714\ufe0f\nIpython / Notebook friendly \u2714\ufe0f\nTensorboard support \u2714\ufe0f\nPEP8 code style \u2714\ufe0f\nCustom callback \u2714\ufe0f\nHigh code coverage \u2714\ufe0f\nType hints \u2714\ufe0f\nPlanned features\nPlease take a look at the Roadmap and Milestones.\nMigration guide: from Stable-Baselines (SB2) to Stable-Baselines3 (SB3)\nA migration guide from SB2 to SB3 can be found in the documentation.\nDocumentation\nDocumentation is available online: https://stable-baselines3.readthedocs.io/\nRL Baselines3 Zoo: A Training Framework for Stable Baselines3 Reinforcement Learning Agents\nRL Baselines3 Zoo is a training framework for Reinforcement Learning (RL).\nIt provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\nIn addition, it includes a collection of tuned hyperparameters for common environments and RL algorithms, and agents trained with those settings.\nGoals of this repository:\nProvide a simple interface to train and enjoy RL agents\nBenchmark the different Reinforcement Learning algorithms\nProvide tuned hyperparameters for each environment and RL algorithm\nHave fun with the trained agents!\nGithub repo: https://github.com/DLR-RM/rl-baselines3-zoo\nDocumentation: https://stable-baselines3.readthedocs.io/en/master/guide/rl_zoo.html\nSB3-Contrib: Experimental RL Features\nWe implement experimental features in a separate contrib repository: SB3-Contrib\nThis allows SB3 to maintain a stable and compact core, while still providing the latest features, like Truncated Quantile Critics (TQC), Quantile Regression DQN (QR-DQN) or PPO with invalid action masking (Maskable PPO).\nDocumentation is available online: https://sb3-contrib.readthedocs.io/\nInstallation\nNote: Stable-Baselines3 supports PyTorch >= 1.8.1.\nPrerequisites\nStable Baselines3 requires python 3.7+.\nWindows 10\nTo install stable-baselines on Windows, please look at the documentation.\nInstall using pip\nInstall the Stable Baselines3 package:\npip install stable-baselines3[extra]\nNote: Some shells such as Zsh require quotation marks around brackets, i.e. pip install 'stable-baselines3[extra]' (More Info).\nThis includes an optional dependencies like Tensorboard, OpenCV or atari-py to train on atari games. If you do not need those, you can use:\npip install stable-baselines3\nPlease read the documentation for more details and alternatives (from source, using docker).\nExample\nMost of the library tries to follow a sklearn-like syntax for the Reinforcement Learning algorithms.\nHere is a quick example of how to train and run PPO on a cartpole environment:\nimport gym\nfrom stable_baselines3 import PPO\nenv = gym.make(\"CartPole-v1\")\nmodel = PPO(\"MlpPolicy\", env, verbose=1)\nmodel.learn(total_timesteps=10000)\nobs = env.reset()\nfor i in range(1000):\naction, _states = model.predict(obs, deterministic=True)\nobs, reward, done, info = env.step(action)\nenv.render()\nif done:\nobs = env.reset()\nenv.close()\nOr just train a model with a one liner if the environment is registered in Gym and if the policy is registered:\nfrom stable_baselines3 import PPO\nmodel = PPO('MlpPolicy', 'CartPole-v1').learn(10000)\nPlease read the documentation for more examples.\nTry it online with Colab Notebooks !\nAll the following examples can be executed online using Google colab notebooks:\nFull Tutorial\nAll Notebooks\nGetting Started\nTraining, Saving, Loading\nMultiprocessing\nMonitor Training and Plotting\nAtari Games\nRL Baselines Zoo\nPyBullet\nImplemented Algorithms\nName Recurrent Box Discrete MultiDiscrete MultiBinary Multi Processing\nA2C \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nDDPG \u274c \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f\nDQN \u274c \u274c \u2714\ufe0f \u274c \u274c \u2714\ufe0f\nHER \u274c \u2714\ufe0f \u2714\ufe0f \u274c \u274c \u274c\nPPO \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nSAC \u274c \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f\nTD3 \u274c \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f\nQR-DQN1 \u274c \u274c \u2714\ufe0f \u274c \u274c \u2714\ufe0f\nTQC1 \u274c \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f\nMaskable PPO1 \u274c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\n1: Implemented in SB3 Contrib GitHub repository.\nActions gym.spaces:\nBox: A N-dimensional box that containes every point in the action space.\nDiscrete: A list of possible actions, where each timestep only one of the actions can be used.\nMultiDiscrete: A list of possible actions, where each timestep only one action of each discrete set can be used.\nMultiBinary: A list of possible actions, where each timestep any of the actions can be used in any combination.\nTesting the installation\nAll unit tests in stable baselines3 can be run using pytest runner:\npip install pytest pytest-cov\nmake pytest\nYou can also do a static type check using pytype:\npip install pytype\nmake type\nCodestyle check with flake8:\npip install flake8\nmake lint\nProjects Using Stable-Baselines3\nWe try to maintain a list of project using stable-baselines3 in the documentation, please tell us when if you want your project to appear on this page ;)\nCiting the Project\nTo cite this repository in publications:\n@article{stable-baselines3,\nauthor = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},\ntitle = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},\njournal = {Journal of Machine Learning Research},\nyear = {2021},\nvolume = {22},\nnumber = {268},\npages = {1-8},\nurl = {http://jmlr.org/papers/v22/20-1364.html}\n}\nMaintainers\nStable-Baselines3 is currently maintained by Ashley Hill (aka @hill-a), Antonin Raffin (aka @araffin), Maximilian Ernestus (aka @ernestum), Adam Gleave (@AdamGleave) and Anssi Kanervisto (@Miffyli).\nImportant Note: We do not do technical support, nor consulting and don't answer personal questions per email. Please post your question on the RL Discord, Reddit or Stack Overflow in that case.\nHow To Contribute\nTo any interested in making the baselines better, there is still some documentation that needs to be done. If you want to contribute, please read CONTRIBUTING.md guide first.\nAcknowledgments\nThe initial work to develop Stable Baselines3 was partially funded by the project Reduced Complexity Models from the Helmholtz-Gemeinschaft Deutscher Forschungszentren.\nThe original version, Stable Baselines, was created in the robotics lab U2IS (INRIA Flowers team) at ENSTA ParisTech.\nLogo credits: L.M. Tenkes", "link": "https://github.com/DLR-RM/stable-baselines3", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "stable baselines3\nstable baselines3 (sb3) is a set of reliable implementations of reinforcement learning algorithms in pytorch. it is the next major version of stable baselines.\nyou can read a detailed presentation of stable baselines3 in the v1.0 blog post or our jmlr paper.\nthese algorithms will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. we expect these tools will be used as a base around which new ideas can be added, and as a -----> tool !!!  for comparing a new approach against existing ones. we also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details.\nnote: despite its simplicity of use, stable baselines3 (sb3) assumes you have some knowledge about reinforcement learning (rl). you should not utilize this library without some practice. to that extent, we provide good resources in the documentation to get started with rl.\nmain features\nthe performance of each algorithm was tested (see results section in their respective page), you can take a look at the issues #48 and #49 for more details.\nfeatures stable-baselines3\nstate of the art rl methods \u2714\ufe0f\ndocumentation \u2714\ufe0f\ncustom environments \u2714\ufe0f\ncustom policies \u2714\ufe0f\ncommon interface \u2714\ufe0f\ndict observation space support \u2714\ufe0f\nipython / notebook friendly \u2714\ufe0f\ntensorboard support \u2714\ufe0f\npep8 code style \u2714\ufe0f\ncustom callback \u2714\ufe0f\nhigh code coverage \u2714\ufe0f\ntype hints \u2714\ufe0f\nplanned features\nplease take a look at the roadmap and milestones.\nmigration guide: from stable-baselines (sb2) to stable-baselines3 (sb3)\na migration guide from sb2 to sb3 can be found in the documentation.\ndocumentation\ndocumentation is available online: https://stable-baselines3.readthedocs.io/\nrl baselines3 zoo: a training framework for stable baselines3 reinforcement learning agents\nrl baselines3 zoo is a training framework for reinforcement learning (rl).\nit provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\nin addition, it includes a collection of tuned hyperparameters for common environments and rl algorithms, and agents trained with those settings.\ngoals of this repository:\nprovide a simple interface to train and enjoy rl agents\nbenchmark the different reinforcement learning algorithms\nprovide tuned hyperparameters for each environment and rl algorithm\nhave fun with the trained agents!\ngithub repo: https://github.com/dlr-rm/rl-baselines3-zoo\ndocumentation: https://stable-baselines3.readthedocs.io/en/master/guide/rl_zoo.html\nsb3-contrib: experimental rl features\nwe implement experimental features in a separate contrib repository: sb3-contrib\nthis allows sb3 to maintain a stable and compact core, while still providing the latest features, like truncated quantile critics (tqc), quantile regression dqn (qr-dqn) or ppo with invalid action masking (maskable ppo).\ndocumentation is available online: https://sb3-contrib.readthedocs.io/\ninstallation\nnote: stable-baselines3 supports pytorch >= 1.8.1.\nprerequisites\nstable baselines3 requires python 3.7+.\nwindows 10\nto install stable-baselines on windows, please look at the documentation.\ninstall using pip\ninstall the stable baselines3 package:\npip install stable-baselines3[extra]\nnote: some shells such as zsh require quotation marks around brackets, i.e. pip install 'stable-baselines3[extra]' (more info).\nthis includes an optional dependencies like tensorboard, opencv or atari-py to train on atari games. if you do not need those, you can use:\npip install stable-baselines3\nplease read the documentation for more details and alternatives (from source, using docker).\nexample\nmost of the library tries to follow a sklearn-like syntax for the reinforcement learning algorithms.\nhere is a quick example of how to train and run ppo on a cartpole environment:\nimport gym\nfrom stable_baselines3 import ppo\nenv = gym.make(\"cartpole-v1\")\nmodel = ppo(\"mlppolicy\", env, verbose=1)\nmodel.learn(total_timesteps=10000)\nobs = env.reset()\nfor i in range(1000):\naction, _states = model.predict(obs, deterministic=true)\nobs, reward, done, info = env.step(action)\nenv.render()\nif done:\nobs = env.reset()\nenv.close()\nor just train a model with a one liner if the environment is registered in gym and if the policy is registered:\nfrom stable_baselines3 import ppo\nmodel = ppo('mlppolicy', 'cartpole-v1').learn(10000)\nplease read the documentation for more examples.\ntry it online with colab notebooks !\nall the following examples can be executed online using google colab notebooks:\nfull tutorial\nall notebooks\ngetting started\ntraining, saving, loading\nmultiprocessing\nmonitor training and plotting\natari games\nrl baselines zoo\npybullet\nimplemented algorithms\nname recurrent box discrete multidiscrete multibinary multi processing\na2c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nddpg \u274c \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f\ndqn \u274c \u274c \u2714\ufe0f \u274c \u274c \u2714\ufe0f\nher \u274c \u2714\ufe0f \u2714\ufe0f \u274c \u274c \u274c\nppo \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nsac \u274c \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f\ntd3 \u274c \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f\nqr-dqn1 \u274c \u274c \u2714\ufe0f \u274c \u274c \u2714\ufe0f\ntqc1 \u274c \u2714\ufe0f \u274c \u274c \u274c \u2714\ufe0f\nmaskable ppo1 \u274c \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\n1: implemented in sb3 contrib github repository.\nactions gym.spaces:\nbox: a n-dimensional box that containes every point in the action space.\ndiscrete: a list of possible actions, where each timestep only one of the actions can be used.\nmultidiscrete: a list of possible actions, where each timestep only one action of each discrete set can be used.\nmultibinary: a list of possible actions, where each timestep any of the actions can be used in any combination.\ntesting the installation\nall unit tests in stable baselines3 can be run using pytest runner:\npip install pytest pytest-cov\nmake pytest\nyou can also do a static type check using pytype:\npip install pytype\nmake type\ncodestyle check with flake8:\npip install flake8\nmake lint\nprojects using stable-baselines3\nwe try to maintain a list of project using stable-baselines3 in the documentation, please tell us when if you want your project to appear on this page ;)\nciting the project\nto cite this repository in publications:\n@article{stable-baselines3,\nauthor = {antonin raffin and ashley hill and adam gleave and anssi kanervisto and maximilian ernestus and noah dormann},\ntitle = {stable-baselines3: reliable reinforcement learning implementations},\njournal = {journal of machine learning research},\nyear = {2021},\nvolume = {22},\nnumber = {268},\npages = {1-8},\nurl = {http://jmlr.org/papers/v22/20-1364.html}\n}\nmaintainers\nstable-baselines3 is currently maintained by ashley hill (aka @hill-a), antonin raffin (aka @araffin), maximilian ernestus (aka @ernestum), adam gleave (@adamgleave) and anssi kanervisto (@miffyli).\nimportant note: we do not do technical support, nor consulting and don't answer personal questions per email. please post your question on the rl discord, reddit or stack overflow in that case.\nhow to contribute\nto any interested in making the baselines better, there is still some documentation that needs to be done. if you want to contribute, please read contributing.md guide first.\nacknowledgments\nthe initial work to develop stable baselines3 was partially funded by the project reduced complexity models from the helmholtz-gemeinschaft deutscher forschungszentren.\nthe original version, stable baselines, was created in the robotics lab u2is (inria flowers team) at ensta paristech.\nlogo credits: l.m. tenkes", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000016, "year": null}, {"Unnamed: 0": 1039, "autor": 19, "date": null, "content": "OpenCog\nThis git repository contains the \"OpenCog Framework\", which has served as a (scientific, technical) laboratory for researching, exploring and learning how to integrate AI algorithms and systems into humanoid robotic systems. Most of the activity within this particular repo has focused on integrating natural language chat, common-sense reasoning, assorted learning algorithms, and motor control of humanoid robots.\nA stated goal of the OpenCog project is to develop artificial general intelligence (AGI) systems. This is all and well; however, what can be found here, in this particular repo, is very far from that. The code here really is ... a laboratory for integrating various types of AI systems. As such, it is a compilation of several decades of work by a large and varying collection of students, researchers, professors and software engineers. As a laboratory, it is filled with all sorts of devices in varying states of working order, from well-polished to mostly-broken.\nSee also:\nROCCA - Rational OpenCog Controlled Agent. This is a different assemblage of assorted OpenCog components, so that they operate within Minecraft, in the OpenAI Gym. The focus is on learning with the pattern miner, and reasoning with PLN.\nOverview\nMost of the basic components used in OpenCog are distributed across various git repos, (mostly) grouped under https://github.com/opencog\nThis git repository contains a crude natural language processing pipeline, several embodied chatbots, and some control/action-selection mechanisms. These include:\nGhost, a Chatscript-compatible chatbot with additional capabilities for accepting visual sensory input, and for controlling robot movements.\nOpenPsi, a model of psychological states. Its currently a mashup of two unrelated ideas: a generic rule-class action-selection and planning system, and a model of human psychological states. An open to-do item is to untangle these two.\nAn assortment of natural language processing subsystems, including:\nNatural language generation (for expressing thoughts as sentences).\nNatural language input (for reading and hearing).\nRelex2logic, converting natural language to logic expressions.\nAssorted chatbots, some of which are embodied.\nA Lojban tool.\nPrerequisites\nTo build and run the system here, the packages listed below are required. Users of Ubuntu may use the dependency installer from the /opencog/octool repository. Docker containers with OpenCog preconfigured can be found in the opencog/docker repo.\ncogutil\nCommon OpenCog C++ utilities. https://github.com/opencog/cogutil It uses exactly the same build procedure as this package. Be sure to sudo make install at the end.\natomspace\nOpenCog Atomspace, a sophisticated (hyper-)graph database. https://github.com/opencog/atomspace It uses exactly the same build procedure as this package. Be sure to sudo make install at the end.\ncogserver\nOpenCog CogServer Network Server. https://github.com/opencog/cogserver It uses exactly the same build procedure as this package. Be sure to sudo make install at the end.\nattention\nOpenCog Attention Allocation subsystem. https://github.com/opencog/attention It uses exactly the same build procedure as this package. Be sure to sudo make install at the end.\nURE\nOpenCog Unified Rule Engine. https://github.com/opencog/ure Required for PLN It uses exactly the same build procedure as this package. Be sure to sudo make install at the end.\npln\nOpenCog Probabilistic Logic Networks reasoning system. https://github.com/opencog/pln It uses exactly the same build procedure as this package. Be sure to sudo make install at the end.\nspacetime\nOpenCog Spacetime Server - locations of objects in space and time. https://github.com/opencog/spacetime It uses exactly the same build procedure as this package. Be sure to sudo make install at the end.\nros-behavior-scripting\nVisual and auditory senses, robot motor control. https://github.com/opencog/ros-behavior-scripting It uses exactly the same build procedure as this package. Be sure to sudo make install at the end.\nlg-atomese\nNatural Language Parser for English, Russian, other languages. Required for natural language generation, and the chatbot. https://github.com/opencog/lg-atomese It uses exactly the same build procedure as this package. Be sure to sudo make install at the end.\nBuilding OpenCog\nPerform the following steps at the shell prompt:\ncd to project root dir\nmkdir build\ncd build\ncmake ..\nmake\nLibraries will be built into subdirectories within build, mirroring the structure of the source directory root.\nUnit tests\nTo build and run the unit tests, from the ./build directory enter (after building opencog as above):\nmake test", "link": "https://github.com/opencog/opencog", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "opencog\nthis git repository contains the \"opencog framework\", which has served as a (scientific, technical) laboratory for researching, exploring and learning how to integrate ai algorithms and systems into humanoid robotic systems. most of the activity within this particular repo has focused on integrating natural language chat, common-sense reasoning, assorted learning algorithms, and motor control of humanoid robots.\na stated goal of the opencog project is to develop artificial general intelligence (agi) systems. this is all and well; however, what can be found here, in this particular repo, is very far from that. the code here really is ... a laboratory for integrating various types of ai systems. as such, it is a compilation of several decades of work by a large and varying collection of students, researchers, professors and software engineers. as a laboratory, it is filled with all sorts of devices in varying states of working order, from well-polished to mostly-broken.\nsee also:\nrocca - rational opencog controlled agent. this is a different assemblage of assorted opencog components, so that they operate within minecraft, in the openai gym. the focus is on learning with the pattern miner, and reasoning with pln.\noverview\nmost of the basic components used in opencog are distributed across various git repos, (mostly) grouped under https://github.com/opencog\nthis git repository contains a crude natural language processing pipeline, several embodied chatbots, and some control/action-selection mechanisms. these include:\nghost, a chatscript-compatible chatbot with additional capabilities for accepting visual sensory input, and for controlling robot movements.\nopenpsi, a model of psychological states. its currently a mashup of two unrelated ideas: a generic rule-class action-selection and planning system, and a model of human psychological states. an open to-do item is to untangle these two.\nan assortment of natural language processing subsystems, including:\nnatural language generation (for expressing thoughts as sentences).\nnatural language input (for reading and hearing).\nrelex2logic, converting natural language to logic expressions.\nassorted chatbots, some of which are embodied.\na lojban -----> tool !!! .\nprerequisites\nto build and run the system here, the packages listed below are required. users of ubuntu may use the dependency installer from the /opencog/octool repository. docker containers with opencog preconfigured can be found in the opencog/docker repo.\ncogutil\ncommon opencog c++ utilities. https://github.com/opencog/cogutil it uses exactly the same build procedure as this package. be sure to sudo make install at the end.\natomspace\nopencog atomspace, a sophisticated (hyper-)graph database. https://github.com/opencog/atomspace it uses exactly the same build procedure as this package. be sure to sudo make install at the end.\ncogserver\nopencog cogserver network server. https://github.com/opencog/cogserver it uses exactly the same build procedure as this package. be sure to sudo make install at the end.\nattention\nopencog attention allocation subsystem. https://github.com/opencog/attention it uses exactly the same build procedure as this package. be sure to sudo make install at the end.\nure\nopencog unified rule engine. https://github.com/opencog/ure required for pln it uses exactly the same build procedure as this package. be sure to sudo make install at the end.\npln\nopencog probabilistic logic networks reasoning system. https://github.com/opencog/pln it uses exactly the same build procedure as this package. be sure to sudo make install at the end.\nspacetime\nopencog spacetime server - locations of objects in space and time. https://github.com/opencog/spacetime it uses exactly the same build procedure as this package. be sure to sudo make install at the end.\nros-behavior-scripting\nvisual and auditory senses, robot motor control. https://github.com/opencog/ros-behavior-scripting it uses exactly the same build procedure as this package. be sure to sudo make install at the end.\nlg-atomese\nnatural language parser for english, russian, other languages. required for natural language generation, and the chatbot. https://github.com/opencog/lg-atomese it uses exactly the same build procedure as this package. be sure to sudo make install at the end.\nbuilding opencog\nperform the following steps at the shell prompt:\ncd to project root dir\nmkdir build\ncd build\ncmake ..\nmake\nlibraries will be built into subdirectories within build, mirroring the structure of the source directory root.\nunit tests\nto build and run the unit tests, from the ./build directory enter (after building opencog as above):\nmake test", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000019, "year": null}, {"Unnamed: 0": 1045, "autor": 25, "date": null, "content": "Awesome Robotic Tooling\nA curated list of tooling for professional robotic development in C++ and Python with a touch of ROS, autonomous driving and aerospace\nTo stop reinventing the wheel you need to know about the wheel. This list is an attempt to show the variety of open and free tools in software and hardware development, which are useful in professional robotic development.\nYour contribution is necessary to keep this list alive, increase the quality and to expand it. You can read more about it's origin and how you can participate in the contribution guide and related blog post. All new project entries will have a tweet from protontypes.\nContents\nCommunication and Coordination\nDocumentation and Presentation\nRequirements and Safety\nArchitecture and Design\nFrameworks and Stacks\nDevelopment Environment\nCode and Run\nTemplate\nBuild and Deploy\nUnit and Integration Test\nLint and Format\nDebugging and Tracing\nVersion Control\nSimulation\nElectronics and Mechanics\nSensor Processing\nCalibration and Transformation\nPerception Pipeline\nMachine Learning\nParallel Processing\nImage Processing\nRadar Processing\nLidar and Point Cloud Processing\nLocalization and State Estimation\nSimultaneous Localization and Mapping\nLidar\nVisual\nVector Map\nPrediction\nBehavior and Decision\nPlanning and Control\nUser Interaction\nGraphical User Interface\nAcoustic User Interface\nCommand Line Interface\nData Visualization and Mission Control\nAnnotation\nPoint Cloud\nRViz\nOperation System\nMonitoring\nDatabase and Record\nNetwork Distributed File System\nServer Infrastructure and High Performance Computing\nEmbedded Operation System\nReal-Time Kernel\nNetwork and Middleware\nEthernet and Wireless Networking\nController Area Network\nSensor and Acuator Interfaces\nSecurity\nDatasets\nCommunication and Coordination\nAgile Development - Manifesto for Agile Software Development.\nGitflow - Makes parallel development very easy, by isolating new development from finished work.\nDeepL - An online translator that outperforms Google, Microsoft and Facebook.\nTaiga - Agile Projectmanagment Tool.\nKanboard - Minimalistic Kanban Board.\nkanban - Free, open source, self-hosted, Kanban board for GitLab issues.\nGitlab - Simple Selfhosted Gitlab Server with Docker.\nGogs - Build a simple, stable and extensible self-hosted Git service that can be setup in the most painless way.\nWekan - Meteor based Kanban Board.\nJIRA API - Python Library for REST API of Jira.\nTaiga API - Python Library for REST API of Taiga.\nChronos-Timetracker - Desktop client for JIRA. Track time, upload worklogs without a hassle.\nGrge - Grge is a daemon and command line utility augmenting GitLab.\ngitlab-triage - Gitlab's issues and merge requests triage, automated.\nHelpy - A modern, open source helpdesk customer support application.\nONLYOFFICE - A free open source collaborative system developed to manage documents, projects, customer relationship and email correspondence, all in one place.\ndiscourse - A platform for community discussion. Free, open, simple.\nGerrit - A code review and project management tool for Git based projects.\njitsi-meet - Secure, Simple and Scalable Video Conferences that you use as a standalone app or embed in your web application.\nmattermost - An open source, private cloud, Slack-alternative.\nopenproject - The leading open source project management software.\nleantime - Leantime is a lean project management system for innovators.\ngitter - Gitter is a chat and networking platform that helps to manage, grow and connect communities through messaging, content and discovery.\nDocumentation and Presentation\nTypora - A Minimalist Markdown Editor.\nMarkor - A Simple Markdown Editor for your Android Device.\nPandoc - Universal markup converter.\nYaspeller - Command line tool for spell checking.\nReadtheDocs - Build your local ReadtheDocs Server.\nDoxygen - Doxygen is the de facto standard tool for generating documentation from annotated C++ sources.\nSphinx - A tool that makes it easy to create intelligent and beautiful documentation for Python projects.\nWord-to-Markdown - A ruby gem to liberate content from Microsoft Word document.\npaperless - Index and archive all of your scanned paper documents.\ncarbon - Share beautiful images of your source code.\nundraw - Free Professional business SVGs easy to customize.\nasciinema - Lets you easily record terminal sessions and replay them in a terminal as well as in a web browser.\ninkscape - Inkscape is a professional vector graphics editor for Linux, Windows and macOS.\nReveal-Hugo - A Hugo theme for Reveal.js that makes authoring and customization a breeze. With it, you can turn any properly-formatted Hugo content into a HTML presentation.\nHugo-Webslides - This is a Hugo template to create WebSlides presentation using markdown.\njupyter2slides - Cloud Native Presentation Slides with Jupyter Notebook + Reveal.js.\npatat - Terminal-based presentations using Pandoc.\ngithub-changelog-generator - Automatically generate change log from your tags, issues, labels and pull requests on GitHub.\nGitLab-Release-Note-Generator - A Gitlab release note generator that generates release note on latest tag.\nOCRmyPDF - Adds an OCR text layer to scanned PDF files, allowing them to be searched.\npapermill - A tool for parameterizing, executing, and analyzing Jupyter Notebooks.\ndocsy - An example documentation site using the Docsy Hugo theme.\nactions-hugo - Deploy website based on Hugo to GitHub Pages.\noverleaf - An open-source online real-time collaborative LaTeX editor.\nlandslide - Generate HTML5 slideshows from markdown, ReST, or textile.\nlibreoffice-impress-templates - Freely-licensed LibreOffice Impress templates.\nopensourcedesign - Community and Resources for Free Design and Logo Creation.\nolive - A free non-linear video editor aiming to provide a fully-featured alternative to high-end professional video editing software.\nbuku - Browser-independent bookmark manager.\nswiftlatex - A WYSIWYG Browser-based LaTeX Editor.\nReLaXed - Allows complex PDF layouts to be defined with CSS and JavaScript, while writing the content in a friendly, minimal syntax close to Markdown or LaTeX.\nfoam - Foam is a personal knowledge management and sharing system inspired by Roam Research, built on Visual Studio Code and GitHub.\nCodiMD - Open Source Online Real-time collaborate on team documentation in markdown.\njupyter-book - Build interactive, publication-quality documents from Jupyter Notebooks.\nInvoiceNet - Deep neural network to extract intelligent information from invoice documents.\ntesseract - Open Source OCR Engine.\nmkdocs - A fast, simple and downright gorgeous static site generator that's geared towards building project documentation.\nPlotNeuralNet - Latex code for drawing neural networks for reports and presentation.\nExcalidraw - Virtual whiteboard for sketching hand-drawn like diagrams.\nSVGrepo - Download free SVG Vectors for commercial use.\ngollum - A simple, Git-powered wiki with a sweet API and local frontend.\nGanttLab - The easy to use, fully functional Gantt chart for GitLab and GitHub.\nZotero - A free, easy-to-use tool to help you collect, organize, cite, and share your research sources.\nRequirements and Safety\nawesome-safety-critical - List of resources about programming practices for writing safety-critical software.\nopen-autonomous-safety - OAS is a fully open-source library of Voyage's safety processes and testing procedures, designed to supplement existing safety programs at self-driving car startups across the world.\nCarND-Functional-Safety-Project - Create functional safety documents in this Udacity project.\nAutomated Valet Parking Safety Documents - Created to support the safe testing of the Automated Valet Parking function using the StreetDrone test vehicle in a car park.\nsafe_numerics - Replacements to standard numeric types which throw exceptions on errors.\nAir Vehicle C++ development coding standards - Provide direction and guidance to C++ programmers that will enable them to employ good programming style and proven programming practices leading to safe, reliable, testable, and maintainable code.\nAUTOSAR Coding Standard - Guidelines for the use of the C++14 language in critical and safety-related system.\nThe W-Model and Lean Scaled Agility for Engineering - Ford applied an agile V-Model method from Vector that can be used in safety related project management.\ndoorstop - Requirements management using version control.\ncapella - Comprehensive, extensible and field-proven MBSE tool and method to successfully design systems architecture.\nrobmosys - RobMoSys envisions an integrated approach built on top of the current code-centric robotic platforms, by applying model-driven methods and tools.\nPapyrus for Robotics - A graphical editing tool for robotic applications that complies with the RobMoSys approach.\nfossology - A toolkit you can run license, copyright and export control scans from the command line.\nScenarioArchitect - The Scenario Architect is a basic python tool to generate, import and export short scene snapshots.\nArchitecture and Design\nGuidelines - How to architect ROS-based systems.\nyEd - A powerful desktop application that can be used to quickly and effectively generate high-quality diagrams.\nyed_py - Generates graphML that can be opened in yEd.\nPlantuml - Web application to generate UML diagrams on-the-fly in your live documentation.\nrqt_graph - Provides a GUI plugin for visualizing the ROS computation graph.\nrqt_launchtree - An RQT plugin for hierarchical launchfile configuration introspection.\ncpp-dependencies - Tool to check C++ #include dependencies (dependency graphs created in .dot format).\npydeps - Python Module Dependency graphs.\naztarna - A footprinting tool for robots.\ndraw.io - A free online diagram software for making flowcharts, process diagrams, org charts, UML, ER and network diagrams.\nvscode-drawio - This extension integrates Draw.io into VS Code.\nArchitecture_Decision_Record - A document that captures an important architectural decision made along with its context and consequences.\nFrameworks and Stacks\nROS - (Robot Operating System) provides libraries and tools to help software developers create robot applications.\nawesome-ros2 - A curated list of awesome Robot Operating System Version 2.0 (ROS 2) resources and libraries.\nAutoware.Auto - Autoware.Auto applies best-in-class software engineering for autonomous driving.\nAutoware.ai - Autoware.AI is the world's first \"All-in-One\" open-source software for autonomous driving technology.\nOpenPilot - Open Source Adaptive Cruise Control (ACC) and Lane Keeping Assist System (LKAS).\nApollo - High performance, flexible architecture which accelerates the development, testing, and deployment of Autonomous Vehicles.\nPythonRobotics - This is a Python code collection of robotics algorithms, especially for autonomous navigation.\nStanford Self Driving Car Code - Stanford Code From Cars That Entered DARPA Grand Challenges.\nastrobee - Astrobee is a free-flying robot designed to operate as a payload inside the International Space Station (ISS).\nCARMAPlatform - Enables cooperative automated driving plug-in.\nAutomotive Grade Linux - Automotive Grade Linux is a collaborative open source project that is bringing together automakers, suppliers and technology companies to accelerate the development and adoption of a fully open software stack for the connected car.\nPX4 - An open source flight control software for drones and other unmanned vehicles.\nKubOS - An open-source software stack for satellites.\nmod_vehicle_dynamics_control - TUM Roborace Team Software Stack - Path tracking control, velocity control, curvature control and state estimation.\nAslan - Open source self-driving software for low speed environments.\nopen-source-rover - A build-it-yourself, 6-wheel rover based on the rovers on Mars from JPL.\npybotics - An open-source and peer-reviewed Python toolbox for robot kinematics and calibration.\nmakani - Contains the working Makani flight simulator, controller (autopilot), visualizer, and command center flight monitoring tools.\nmir_robot - This is a community project to use the MiR Robots with ROS.\nCOMPAS - Robotic fabrication package for the COMPAS Framework.\nJdeRobot Academy - JdeRobot Academy is an open source collection of exercises to learn robotics in a practical way.\nclover - ROS-based framework and RPi image to control PX4-powered drones.\nArduPilot - Open source control software for autonomous vehicles - copters/planes/rovers/boats/submersibles.\nF Prime - A component-driven framework that enables rapid development and deployment of spaceflight and other embedded software applications.\nDevelopment Environment\nCode and Run\nVim-ros - Vim plugin for ROS development.\nVisual Studio Code - Code editor for edit-build-debug cycle.\natom - Hackable text editor for the 21st century.\nTeletype - Share your workspace with team members and collaborate on code in real time in Atom.\nSublime - A sophisticated text editor for code, markup and prose.\nade-cli - The ADE Development Environment (ADE) uses docker and Gitlab to manage environments of per project development tools and optional volume images.\nrecipe-wizard - A Dockerfile generator for running OpenGL (GLX) applications with nvidia-docker2, CUDA, ROS, and Gazebo on a remote headless server system.\nJupyter ROS - Jupyter widget helpers for ROS, the Robot Operating System.\nros_rqt_plugin - The ROS Qt Creator Plug-in for Python.\nxeus-cling - Jupyter kernel for the C++ programming language.\nROS IDEs - This page collects experience and advice on using integrated development environments (IDEs) with ROS.\nTabNine - The all-language autocompleter.\nkite - Use machine learning to give you useful code completions for Python.\njedi - Autocompletion and static analysis library for python.\nroslibpy - Python ROS Bridge library allows to use Python and IronPython to interact with ROS, the open-source robotic middleware.\npybind11 - Seamless operability between C++11 and Python.\nSourcetrail - Free and open-source cross-platform source explorer.\nrebound - Command-line tool that instantly fetches Stack Overflow results when an exception is thrown.\nmybinder - Open notebooks in an executable environment, making your code immediately reproducible by anyone, anywhere.\nROSOnWindows - An experimental release of ROS1 for Windows.\nlive-share - Real-time collaborative development from the comfort of your favorite tools.\ncocalc - Collaborative Calculation in the Cloud.\nEasyClangComplete - Robust C/C++ code completion for Sublime Text 3.\nvscode-ros - Visual Studio Code extension for Robot Operating System (ROS) development.\nawesome-hpp - A curated list of awesome header-only C++ libraries.\nGitpod - An open source developer platform that automates the provisioning of ready-to-code development environments.\nTemplate\nROS - Template for ROS node standardization in C++.\nLaunch - Templates on how to create launch files for larger projects.\nBash - A bash scripting template incorporating best practices & several useful functions.\nURDF - Examples on how to create Unified Robot Description Format (URDF) for different kinds of robots.\nPython - Style guide to be followed in writing Python code for ROS.\nDocker - The Dockerfile in the minimal-ade project shows a minimal example of how to create a custom base image.\nVS Code ROS2 Workspace Template - Template for using VSCode as an IDE for ROS2 development.\nBuild and Deploy\nqemu-user-static - Enable an execution of different multi-architecture containers by QEMU and binfmt_misc.\nCross compile ROS 2 on QNX - Introduces how to cross compile ROS 2 on QNX.\nbloom - A release automation tool which makes releasing catkin packages easier.\nsuperflore - An extended platform release manager for Robot Operating System.\ncatkin_tools - Command line tools for working with catkin.\nindustrial_ci - Easy continuous integration repository for ROS repositories.\nros_gitlab_ci - Contains helper scripts and instructions on how to use Continuous Integration (CI) for ROS projects hosted on a GitLab instance.\ngitlab-runner - Runs tests and sends the results to GitLab.\ncolcon-core - Command line tool to improve the workflow of building, testing and using multiple software packages.\ngitlab-release - Simple python3 script to upload files (from ci) to the current projects release (tag).\nclang - This is a compiler front-end for the C family of languages (C, C++, Objective-C, and Objective-C++) which is built as part of the LLVM compiler infrastructure project.\ncatkin_virtualenv - Bundle python requirements in a catkin package via virtualenv.\npyenv - Simple Python version management.\naptly - Debian repository management tool.\ncross_compile - Assets used for ROS2 cross-compilation.\ndocker_images - Official Docker images maintained by OSRF on ROS(2) and Gazebo.\nrobot_upstart - Presents a suite of scripts to assist with launching background ROS processes on Ubuntu Linux PCs.\nrobot_systemd - Units for managing startup and shutdown of roscore and roslaunch.\nryo-iso - A modern ISO builder that streamlines the process of deploying a complete robot operating system from a yaml config file.\nnetwork_autoconfig - Automatic configuration of ROS networking for most use cases without impacting usage that require manual configuration.\nrosbuild - The ROS build farm.\ncros - A single thread pure C implementation of the ROS framework.\nUnit and Integration Test\nsetup-ros - This action sets up a ROS and ROS 2 environment for use in GitHub actions.\nUnitTesting - This page lays out the rationale, best practices, and policies for writing and running unit tests and integration tests for ROS.\ngoogletest - Google's C++ test framework.\npytest - The pytest framework makes it easy to write small tests, yet scales to support complex functional testing.\ndoctest - The fastest feature-rich C++11/14/17/20 single-header testing framework for unit tests and TDD.\nosrf_testing_tools_cpp - Contains testing tools for C++, and is used in OSRF projects.\ncode_coverage - ROS package to run coverage testing.\naction-ros-ci - GitHub Action to build and test ROS 2 packages using colcon.\nLint and Format\naction-ros-lint - GitHub action to run linters on ROS 2 packages.\ncppcheck - Static analysis of C/C++ code.\nhadolint - Dockerfile linter, validate inline bash, written in Haskell.\nshellcheck - A static analysis tool for shell scripts.\ncatkin_lint - Checks package configurations for the catkin build system of ROS.\npylint - Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions.\nblack - The uncompromising Python code formatter.\npydocstyle - A static analysis tool for checking compliance with Python docstring conventions.\nharos - Static analysis of ROS application code.\npydantic - Data parsing and validation using Python type hints.\nDebugging and Tracing\nheaptrack - Traces all memory allocations and annotates these events with stack traces.\nros2_tracing - Tracing tools for ROS 2.\nLinuxperf - Various Linux performance material.\nlptrace - It lets you see in real-time what functions a Python program is running.\npyre-check - Performant type-checking for python.\nFlameGraph - Visualize profiled code.\ngpuvis - GPU Trace Visualizer.\nsanitizer - AddressSanitizer, ThreadSanitizer, MemorySanitizer.\ncppinsights - C++ Insights - See your source code with the eyes of a compiler.\ninspect - The inspect module provides functions for learning about live objects, including modules, classes, instances, functions, and methods.\nRoslaunch Nodes in Valgrind or GDB - When debugging roscpp nodes that you are launching with roslaunch, you may wish to launch the node in a debugging program like gdb or valgrind instead.\npyperformance - Python Performance Benchmark Suite.\nqira - QIRA is a competitor to strace and gdb.\ngdb-frontend - GDBFrontend is an easy, flexible and extensionable gui debugger.\nlttng - An open source software toolkit which you can use to simultaneously trace the Linux kernel, user applications, and user libraries.\nros2-performance - Allows to easily create arbitrary ROS2 systems and then measures their performance.\nbcc - Tools for BPF-based Linux IO analysis, networking, monitoring, and more.\ntracy - A real time, nanosecond resolution, remote telemetry frame profiler for games and other applications.\nbpftrace - High-level tracing language for Linux eBPF.\npudb - Full-screen console debugger for Python.\nbackward-cpp - A beautiful stack trace pretty printer for C++.\ngdb-dashboard - GDB dashboard is a standalone .gdbinit file written using the Python API that enables a modular interface showing relevant information about the program being debugged.\nhotspot - The Linux perf GUI for performance analysis.\nmemory_profiler - A python module for monitoring memory consumption of a process as well as line-by-line analysis of memory consumption for python programs.\nros1_fuzzer - This fuzzer aims to help developers and researchers to find bugs and vulnerabilities in ROS nodes by performing fuzz tests over topics that the target nodes process.\nvscode-debug-visualizer - An extension for VS Code that visualizes data during debugging.\naction-tmate - Debug your GitHub Actions via SSH by using tmate to get access to the runner system itself.\nlibstatistics_collector - ROS 2 library providing classes to collect measurements and calculate statistics across them.\nsystem_metrics_collector - Lightweight, real-time system metrics collector for ROS2 systems.\nVersion Control\ngit-fuzzy - A CLI interface to git that relies heavily on fzf.\nmeld - Meld is a visual diff and merge tool that helps you compare files, directories, and version controlled projects.\ntig - Text-mode interface for git.\ngitg - A graphical user interface for git.\ngit-cola - The highly caffeinated Git GUI.\npython-gitlab - A Python package providing access to the GitLab server API.\nbfg-repo-cleaner - Removes large or troublesome blobs like git-filter-branch does, but faster.\nnbdime - Tools for diffing and merging of Jupyter notebooks.\nsemantic-release - Fully automated version management and package publishing.\ngo-semrel-gitab - Automate version management for Gitlab.\nGit-repo - Git-Repo helps manage many Git repositories, does the uploads to revision control systems, and automates parts of the development workflow.\ndive - A tool for exploring each layer in a docker image.\ndvc - Management and versioning of datasets and machine learning models.\nlearnGitBranching - A git repository visualizer, sandbox, and a series of educational tutorials and challenges.\ngitfs - You can mount a remote repository's branch locally, and any subsequent changes made to the files will be automatically committed to the remote.\ngit-secret - Encrypts files with permitted users' public keys, allowing users you trust to access encrypted data using pgp and their secret keys.\ngit-sweep - A command-line tool that helps you clean up Git branches that have been merged into master.\nlazygit - A simple terminal UI for git commands, written in Go with the gocui library.\nglab - An open-source GitLab command line tool.\nSimulation\nAI2-THOR - Python framework with a Unity backend providing interaction, navigation, and manipulation support for household based robotic agents, consisting of 200+ of custom scenes, 1500+ custom annotated objects, and 200+ actions.\nDrake - Drake aims to simulate even very complex dynamics of robots.\nWebots - Webots is an open source robot simulator compatible (among others) with ROS and ROS2.\nlgsv - LG Electronics America R&D Center has developed an HDRP Unity-based multi-robot simulator for autonomous vehicle developers.\ncarla - Open-source simulator for autonomous driving research.\nawesome-CARLA - A curated list of awesome CARLA tutorials, blogs, and related projects.\nros-bridge - ROS bridge for CARLA Simulator.\nscenario_runner - Traffic scenario definition and execution engine.\ndeepdive - End-to-end simulation for self-driving cars.\nuuv_simulator - Gazebo/ROS packages for underwater robotics simulation.\nAirSim - Open source simulator for autonomous vehicles built on Unreal Engine.\nself-driving-car-sim - A self-driving car simulator built with Unity.\nROSIntegration - Unreal Engine Plugin to enable ROS Support.\ngym-gazebo - An OpenAI gym extension for using Gazebo known as gym-gazebo.\ngym-pybullet-drones - PyBullet-based Gym environments for single and multi-agent reinforcement learning of quadcopter control.\nsafe-control-gym - PyBullet-based CartPole and Quadrotor environments\u2014with CasADi symbolic dynamics and constraints\u2014for safe and robust learning-based control.\nhighway-env - A collection of environments for autonomous driving and tactical decision-making tasks.\nVREP Interface - ROS Bridge for the VREP simulator.\ncar_demo - This is a simulation of a Prius in gazebo 9 with sensor data being published using ROS kinetic.\nsumo - Eclipse SUMO is an open source, highly portable, microscopic and continuous road traffic simulation package designed to handle large road networks.\nopen-simulation-interface - A generic interface for the environmental perception of automated driving functions in virtual scenarios.\nESIM - An Open Event Camera Simulator.\nMenge - Crowd Simulation Framework.\npedsim_ros - Pedestrian simulator powered by the social force model for Gazebo.\nopencrg - Open file formats and open source tools for the detailed description, creation and evaluation of road surfaces.\nesmini - A basic OpenSCENARIO player.\nOpenSceneGraph - An open source high performance 3D graphics toolkit, used by application developers in fields such as visual simulation, games, virtual reality, scientific visualization and modelling.\nmorse - An academic robotic simulator, based on the Blender Game Engine and the Bullet Physics engine.\nROSIntegrationVision - Support for ROS-enabled RGBD data acquisition in Unreal Engine Projects.\nfetch_gazebo - Contains the Gazebo simulation for Fetch Robotics Fetch and Freight Research Edition Robots.\nrotors_simulator - Provides some multirotor models.\nflow - A computational framework for deep RL and control experiments for traffic microsimulation.\ngnss-ins-sim - GNSS + inertial navigation, sensor fusion simulator. Motion trajectory generator, sensor models, and navigation.\nIgnition Robotics - Test control strategies in safety, and take advantage of simulation in continuous integration tests.\nsimulation assets for the SubT - This collection contains simulation assets for the SubT Challenge Virtual Competition in Gazebo.\ngazebo_ros_motors - Contains currently two motor plugins for Gazebo, one with an ideal speed controller and one without a controller that models a DC motor.\nmap2gazebo - ROS package for creating Gazebo environments from 2D maps.\nsim_vehicle_dynamics - Vehicle Dynamics Simulation Software of TUM Roborace Team.\ngym-carla - An OpenAI gym wrapper for CARLA simulator.\nsimbody - High-performance C++ multibody dynamics/physics library for simulating articulated biomechanical and mechanical systems like vehicles, robots, and the human skeleton.\ngazebo_models - This repository holds the Gazebo model database.\npylot - Autonomous driving platform running on the CARLA simulator.\nflightmare - Flightmare is composed of two main components: a configurable rendering engine built on Unity and a flexible physics engine for dynamics simulation.\nchamp - ROS Packages for CHAMP Quadruped Controller.\nrex-gym - OpenAI Gym environments for an open-source quadruped robot (SpotMicro).\nTrick - Developed at the NASA Johnson Space Center, is a powerful simulation development framework that enables users to build applications for all phases of space vehicle development.\nusv_sim_lsa - Unmanned Surface Vehicle simulation on Gazebo with water current and winds.\n42 - Simulation for spacecraft attitude control system analysis and design.\nComplete_Street_Rule - A scenario oriented design tool intended to enable users to quickly create procedurally generated multimodal streets in ArcGIS CityEngine.\nAutoCore simulation - Provides test environment for Autoware and still during early development, contents below may changed during updates.\nfields-ignition - Generate random crop fields for Ignition Gazebo.\nUnity-Robotics-Hub - Central repository for tools, tutorials, resources, and documentation for robotic simulation in Unity.\nBlueSky - The goal of BlueSky is to provide everybody who wants to visualize, analyze or simulate air traffic with a tool to do so without any restrictions, licenses or limitations.\nCloe - Empowers developers of automated-driving software components by providing a unified interface to closed-loop simulation.\nDynamic_logistics_Warehouse - Gazebo simulation of dynamics environment in warehouses.\nOpenCDA - A generalized framework for prototyping full-stack cooperative driving automation applications under CARLA+SUMO.\nElectronics and Mechanics\nHRIM - An information model for robot hardware.\nURDF - Repository for Unified Robot Description Format (URDF) parsing code.\nphobos - An add-on for Blender allowing to create URDF, SDF and SMURF robot models in a WYSIWYG environment.\nurdf-viz - Visualize URDF/XACRO file, URDF Viewer works on Windows/macOS/Linux.\nsolidworks_urdf_exporter - SolidWorks to URDF Exporter.\nFreeCAD - Your own 3D parametric modeler.\nkicad - A Cross Platform and Open Source Electronics Design Automation Suite.\nPcbDraw - Convert your KiCAD board into a nice looking 2D drawing suitable for pinout diagrams.\nkicad-3rd-party-tools - Tools made by others to augment the KiCad PCB EDA suite.\nPandaPower - An easy to use open source tool for power system modeling, analysis and optimization with a high degree of automation.\nLibrePCB - A powerful, innovative and intuitive EDA tool for everyone.\nopenscad - A software for creating solid 3D CAD models.\nngspice - A open source spice simulator for electric and electronic circuits.\nGNSS-SDR - GNSS-SDR provides interfaces for a wide range of radio frequency front-ends and raw sample file formats, generates processing outputs in standard formats.\nriscv - The Free and Open RISC Instruction Set Architecture.\nurdfpy - A simple and easy-to-use library for loading, manipulating, saving, and visualizing URDF files.\nFMPy - Simulate Functional Mockup Units (FMUs) in Python.\nFMIKit-Simulink - Import and export Functional Mock-up Units with Simulink.\noemof-solph - A modular open source framework to model energy supply systems.\nNASA-3D-Resources - Here you'll find a growing collection of 3D models, textures, and images from inside NASA.\nSUAVE - An Aircraft Design Toolbox.\nopem - The Open-Source PEMFC Simulation Tool (OPEM) is a modeling tool for evaluating the performance of proton exchange membrane fuel cells.\npvlib-python - A community supported tool that provides a set of functions and classes for simulating the performance of photovoltaic energy systems.\nWireViz - A tool for easily documenting cables, wiring harnesses and connector pinouts.\nHorizon - EDA is an Electronic Design Automation package supporting an integrated end-to-end workflow for printed circuit board design including parts management and schematic entry.\ntigl - The TiGL Geometry Library can be used for the computation and processing of aircraft geometries stored inside CPACS files.\nfoxBMS - A free, open and flexible development environment to design battery management systems.\ncadCAD - A Python package that assists in the processes of designing, testing and validating complex systems through simulation, with support for Monte Carlo methods, A/B testing and parameter sweeping.\nOpenMDAO - An open-source framework for efficient multidisciplinary optimization.\nODrive - The aim is to make it possible to use inexpensive brushless motors in high performance robotics projects.\nOpenTirePython - An open-source mathematical tire modelling library.\nInkscape Ray Optics - An extension for Inkscape that makes it easier to draw optical diagrams.\nOpenAeroStruct - A lightweight tool that performs aerostructural optimization using OpenMDAO.\nSensor Processing\nCalibration and Transformation\ntf2 - Transform library, which lets the user keep track of multiple coordinate frames over time.\nTriP - A Inverse Kinematics library for serial robots, parallel robots and hybrids of both.\nlidar_align - A simple method for finding the extrinsic calibration between a 3D lidar and a 6-dof pose sensor.\nkalibr - The Kalibr visual-inertial calibration toolbox.\nCalibnet - Self-Supervised Extrinsic Calibration using 3D Spatial Transformer Networks.\nlidar_camera_calibration - ROS package to find a rigid-body transformation between a LiDAR and a camera.\nILCC - Reflectance Intensity Assisted Automatic and Accurate Extrinsic Calibration of 3D LiDAR.\neasy_handeye - Simple, straighforward ROS library for hand-eye calibration.\nimu_utils - A ROS package tool to analyze the IMU performance.\nkalibr_allan - IMU Allan standard deviation charts for use with Kalibr and inertial kalman filters.\npyquaternion - A full-featured Python module for representing and using quaternions.\nrobot_calibration - This package offers calibration of a number of parameters of a robot, such as: 3D Camera intrinsics, extrinsics Joint angle offsets and robot frame offsets.\nmulti_sensor_calibration - Contains a calibration tool to calibrate a sensor setup consisting of lidars, radars and cameras.\nLiDARTag - A Real-Time Fiducial Tag using Point Clouds Lidar Data.\nmulticam_calibration - Extrinsic and intrinsic calbration of cameras.\nikpy - An Inverse Kinematics library aiming performance and modularity.\nlivox_camera_lidar_calibration - Calibrate the extrinsic parameters between Livox LiDAR and camera.\nlidar_camera_calibration - Camera LiDAR Calibration using ROS, OpenCV, and PCL.\ne2calib - Contains code that implements video reconstruction from event data for calibration.\nPerception Pipeline\nSARosPerceptionKitti - ROS package for the Perception (Sensor Processing, Detection, Tracking and Evaluation) of the KITTI Vision Benchmark Suite.\nmultiple-object-tracking-lidar - C++ implementation to Detect, track and classify multiple objects using LIDAR scans or point cloud.\ncadrl_ros - ROS package for dynamic obstacle avoidance for ground robots trained with deep RL.\nAugmentedAutoencoder - RGB-based pipeline for object detection and 6D pose estimation.\njsk_recognition - A stack for the perception packages which are used in JSK lab.\nGibsonEnv - Gibson Environments: Real-World Perception for Embodied Agents.\nmorefusion - Multi-object Reasoning for 6D Pose Estimation from Volumetric Fusion.\nMachine Learning\nDLIB - A toolkit for making real world machine learning and data analysis applications in C++.\nfastai - The fastai library simplifies training fast and accurate neural nets using modern best practices.\ntpot - A Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.\ndeap - Distributed Evolutionary Algorithms in Python.\ngym - A toolkit for developing and comparing reinforcement learning algorithms.\ntensorflow_ros_cpp - A ROS package that allows to do Tensorflow inference in C++ without the need to compile TF yourself.\nTensorflow Federated - TensorFlow Federated (TFF) is an open-source framework for machine learning and other computations on decentralized data.\nfinn - Fast, Scalable Quantized Neural Network Inference on FPGAs.\nneuropod - Neuropod is a library that provides a uniform interface to run deep learning models from multiple frameworks in C++ and Python.\nleela-zero - This is a fairly faithful reimplementation of the system described in the Alpha Go Zero paper \"Mastering the Game of Go without Human Knowledge\".\nTrax - A library for deep learning that focuses on sequence models and reinforcement learning.\nmlflow - A platform to streamline machine learning development, including tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\nNetron - Visualizer for neural network, deep learning and machine learning models.\nMNN - A blazing fast, lightweight deep learning framework, battle-tested by business-critical use cases in Alibaba.\nTensorforce - An open-source deep reinforcement learning framework, with an emphasis on modularized flexible library design and straightforward usability for applications in research and practice.\nDopamine - A research framework for fast prototyping of reinforcement learning algorithms.\ncatalyst - Was developed with a focus on reproducibility, fast experimentation and code/ideas reusing.\nray - A fast and simple framework for building and running distributed applications.\ntf-agents - A reliable, scalable and easy to use TensorFlow library for Contextual Bandits and Reinforcement Learning.\nReAgent - An open source end-to-end platform for applied reinforcement learning (RL) developed and used at Facebook.\nAwesome-Mobile-Machine-Learning - A curated list of awesome mobile machine learning resources for iOS, Android, and edge devices.\ncnn-explainer - Learning Convolutional Neural Networks with Interactive Visualization.\nmodelzoo - A collection of machine-learned models for use in autonomous driving applications.\nnnstreamer-ros - A set of Gstreamer plugins and ROS examples that allow Gstreamer developers to adopt neural network models easily and efficiently and neural network developers to manage neural network pipelines and their filters easily and efficiently.\nParallel Processing\ndask - Parallel computing with task scheduling for Python.\ncupy - NumPy-like API accelerated with CUDA.\nThrust - A C++ parallel programming library which resembles the C++ Standard Library.\nArrayFire - A general purpose GPU library.\nOpenMP - An application programming interface that supports multi-platform shared memory multiprocessing programming in C, C++, and Fortran.\nVexCL - VexCL is a C++ vector expression template library for OpenCL/CUDA/OpenMP.\nPYNQ - An open-source project from Xilinx that makes it easy to design embedded systems with Zynq All Programmable Systems on Chips.\nnumba - NumPy aware dynamic Python compiler using LLVM.\nTensorRT - A C++ library for high performance inference on NVIDIA GPUs and deep learning accelerators.\nlibcudacxx - Provides a heterogeneous implementation of the C++ Standard Library that can be used in and between CPU and GPU code.\nImage Processing\nCV-pretrained-model - A collection of computer vision pre-trained models.\nimage_pipeline - Fills the gap between getting raw images from a camera driver and higher-level vision processing.\ngstreamer - A pipeline-based multimedia framework that links together a wide variety of media processing systems to complete complex workflows.\nros2_openvino_toolkit - Provides a ROS-adaptered runtime framework of neural network which quickly deploys applications and solutions for vision inference.\nvision_visp - Wraps the ViSP moving edge tracker provided by the ViSP visual servoing library into a ROS package.\napriltag_ros - A ROS wrapper of the AprilTag 3 visual fiducial detector.\ndeep_object_pose - Deep Object Pose Estimation.\nDetectAndTrack - Detect-and-Track: Efficient Pose.\nSfMLearner - An unsupervised learning framework for depth and ego-motion estimation.\nimgaug - Image augmentation for machine learning experiments.\nvision_opencv - Packages for interfacing ROS with OpenCV, a library of programming functions for real time computer vision.\ndarknet_ros - YOLO ROS: Real-Time Object Detection for ROS.\nros_ncnn - YOLACT / YOLO ( among other things ) on NCNN inference engine for ROS.\ntf-pose-estimation - Deep Pose Estimation implemented using Tensorflow with Custom Architectures for fast inference.\nfind-object - Simple Qt interface to try OpenCV implementations of SIFT, SURF, FAST, BRIEF and other feature detectors and descriptors.\nyolact - A simple, fully convolutional model for real-time instance segmentation.\nKimera-Semantics - Real-Time 3D Semantic Reconstruction from 2D data.\ndetectron2 - A next-generation research platform for object detection and segmentation.\nOpenVX - Enables performance and power-optimized computer vision processing, especially important in embedded and real-time use cases.\n3d-vehicle-tracking - Official implementation of Joint Monocular 3D Vehicle Detection and Tracking.\npysot - The goal of PySOT is to provide a high-quality, high-performance codebase for visual tracking research.\nsemantic_slam - Real time semantic slam in ROS with a hand held RGB-D camera.\nkitti_scan_unfolding - We propose KITTI scan unfolding in our paper Scan-based Semantic Segmentation of LiDAR Point Clouds: An Experimental Study.\npacknet-sfm - Official PyTorch implementation of self-supervised monocular depth estimation methods invented by the ML Team at Toyota Research Institute (TRI).\nAB3DMOT - This work proposes a simple yet accurate real-time baseline 3D multi-object tracking system.\nmonoloco - Official implementation of \"MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty Estimation\" in PyTorch.\nPoly-YOLO - Builds on the original ideas of YOLOv3 and removes two of its weaknesses: a large amount of rewritten labels and inefficient distribution of anchors.\nsatellite-image-deep-learning - Resources for deep learning with satellite & aerial imagery.\nrobosat - Semantic segmentation on aerial and satellite imagery.\nbig_transfer - Model for General Visual Representation Learning created by Google Research.\nLEDNet - A Lightweight Encoder-Decoder Network for Real-time Semantic Segmentation.\nTorchSeg - This project aims at providing a fast, modular reference implementation for semantic segmentation models using PyTorch.\nsimpledet - A Simple and Versatile Framework for Object Detection and Instance Recognition.\nmeshroom - Meshroom is a free, open-source 3D Reconstruction Software based on the AliceVision Photogrammetric Computer Vision framework.\nEasyOCR - Ready-to-use Optical character recognition (OCR) with 40+ languages supported including Chinese, Japanese, Korean and Thai.\npytracking - A general python framework for visual object tracking and video object segmentation, based on PyTorch.\nros_deep_learning - Deep learning inference nodes for ROS with support for NVIDIA Jetson TX1/TX2/Xavier and TensorRT.\nhyperpose - HyperPose: A Flexible Library for Real-time Human Pose Estimation.\nfawkes - Privacy preserving tool against facial recognition systems.\nanonymizer - An anonymizer to obfuscate faces and license plates.\nopendatacam - Only saves surveyed meta-data, in particular the path an object moved or number of counted objects at a certain point.\nCam2BEV - TensorFlow Implementation for Computing a Semantically Segmented Bird's Eye View (BEV) Image Given the Images of Multiple Vehicle-Mounted Cameras.\nflownet2-pytorch - Pytorch implementation of FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks.\nSimd - C++ image processing and machine learning library with using of SIMD: SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, AVX, AVX2, AVX-512, VMX(Altivec) and VSX(Power7), NEON for ARM.\nAliceVision - A Photogrammetric Computer Vision Framework which provides a 3D Reconstruction and Camera Tracking algorithms.\nsatpy - A python library for reading and manipulating meteorological remote sensing data and writing it to various image and data file formats.\neo-learn - A collection of open source Python packages that have been developed to seamlessly access and process spatio-temporal image sequences acquired by any satellite fleet in a timely and automatic manner.\nlibvips - A fast image processing library with low memory needs.\nRadar Processing\npyroSAR - Framework for large-scale SAR satellite data processing.\nCameraRadarFusionNet - TUM Roborace Team Software Stack - Path tracking control, velocity control, curvature control and state estimation.\nLidar and Point Cloud Processing\ncilantro - A lean C++ library for working with point cloud data.\nopen3d - Open3D: A Modern Library for 3D Data Processing.\nSqueezeSeg - Implementation of SqueezeSeg, convolutional neural networks for LiDAR point clout segmentation.\npoint_cloud_io - ROS nodes to read and write point clouds from and to files (e.g. ply, vtk).\npython-pcl - Python bindings to the pointcloud library.\nlibpointmatcher - An \"Iterative Closest Point\" library for 2-D/3-D mapping in Robotics.\ndepth_clustering - Fast and robust clustering of point clouds generated with a Velodyne sensor.\nlidar-bonnetal - Semantic and Instance Segmentation of LiDAR point clouds for autonomous driving.\nCSF - LiDAR point cloud ground filtering / segmentation (bare earth extraction) method based on cloth simulation.\nrobot_body_filter - A highly configurable LaserScan/PointCloud2 filter that allows to dynamically remove the 3D body of the robot from the measurements.\ngrid_map - Universal grid map library for mobile robotic mapping.\nelevation_mapping - Robot-centric elevation mapping for rough terrain navigation.\nrangenet_lib - Contains simple usage explanations of how the RangeNet++ inference works with the TensorRT and C++ interface.\npointcloud_to_laserscan - Converts a 3D Point Cloud into a 2D laser scan.\noctomap - An Efficient Probabilistic 3D Mapping Framework Based on Octrees.\npptk - Point Processing Toolkit from HEREMaps.\ngpu-voxels - GPU-Voxels is a CUDA based library which allows high resolution volumetric collision detection between animated 3D models and live pointclouds from 3D sensors of all kinds.\nspatio_temporal_voxel_layer - A new voxel layer leveraging modern 3D graphics tools to modernize navigation environmental representations.\nLAStools - Award-winning software for efficient LiDAR processing.\nPCDet - A general PyTorch-based codebase for 3D object detection from point cloud.\nPDAL - A C++ BSD library for translating and manipulating point cloud data.\nPotreeConverter - Builds a potree octree from las, laz, binary ply, xyz or ptx files.\nfast_gicp - A collection of GICP-based fast point cloud registration algorithms.\nndt_omp - Multi-threaded and SSE friendly NDT algorithm.\nlaser_line_extraction - A ROS packages that extracts line segments from LaserScan messages.\nGo-ICP - Implementation of the Go-ICP algorithm for globally optimal 3D pointset registration.\nPointCNN - A simple and general framework for feature learning from point clouds.\nsegmenters_lib - The LiDAR segmenters library, for segmentation-based detection.\nMotionNet - Joint Perception and Motion Prediction for Autonomous Driving Based on Bird's Eye View Maps.\nPolarSeg - An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation.\ntraversability_mapping - Takes in point cloud from a Velodyne VLP-16 Lidar and outputs a traversability map for autonomous navigation in real-time.\nlidar_super_resolution - Simulation-based Lidar Super-resolution for Ground Vehicles.\nCupoch - A library that implements rapid 3D data processing and robotics computation using CUDA.\nlinefit_ground_segmentation - Implementation of the ground segmentation algorithm.\nDraco - A library for compressing and decompressing 3D geometric meshes and point clouds.\nVotenet - Deep Hough Voting for 3D Object Detection in Point Clouds.\nlidar_undistortion - Provides lidar motion undistortion based on an external 6DoF pose estimation input.\nsuperpoint_graph - Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs.\nRandLA-Net - Efficient Semantic Segmentation of Large-Scale Point Clouds.\nDet3D - A first 3D Object Detection toolbox which provides off the box implementations of many 3D object detection algorithms such as PointPillars, SECOND, PIXOR.\nOverlapNet - A modified Siamese Network that predicts the overlap and relative yaw angle of a pair of range images generated by 3D LiDAR scans.\nmp2p_icp - A repertory of multi primitive-to-primitive (MP2P) ICP algorithms in C++.\nOpenPCDet - A Toolbox for LiDAR-based 3D Object Detection.\ntorch-points3d - Pytorch framework for doing deep learning on point clouds.\nPolyFit - Polygonal Surface Reconstruction from Point Clouds.\nmmdetection3d - Next-generation platform for general 3D object detection.\ngpd - Takes a point cloud as input and produces pose estimates of viable grasps as output.\nSalsaNext - Uncertainty-aware Semantic Segmentation of LiDAR Point Clouds for Autonomous Driving.\nSuper-Fast-Accurate-3D-Object-Detection - Super Fast and Accurate 3D Object Detection based on 3D LiDAR Point Clouds (The PyTorch implementation).\nkaolin - A PyTorch Library for Accelerating 3D Deep Learning Research.\nCamVox - A low-cost SLAM system based on camera and Livox lidar.\nSA-SSD - Structure Aware Single-stage 3D Object Detection from Point Cloud.\ncuda-pcl - Accelerating Lidar for Robotics with NVIDIA CUDA-based PCL.\nLocalization and State Estimation\nevo - Python package for the evaluation of odometry and SLAM.\nrobot_localization - A package of nonlinear state estimation nodes.\nfuse - General architecture for performing sensor fusion live on a robot.\nGeographicLib - A C++ library for geographic projections.\nntripbrowser - A Python API for browsing NTRIP (Networked Transport of RTCM via Internet Protocol).\nimu_tools - IMU-related filters and visualizers.\nRTKLIB - A version of RTKLIB optimized for single and dual frequency low cost GPS receivers, especially u-blox receivers.\ngLAB - Performs precise modeling of GNSS observables (pseudorange and carrier phase) at the centimetre level, allowing standalone GPS positioning, PPP, SBAS and DGNSS.\nai-imu-dr - Contains the code of our novel accurate method for dead reckoning of wheeled vehicles based only on an IMU.\nKalman-and-Bayesian-Filters-in-Python - Kalman Filter book using Jupyter Notebook.\nmcl_3dl - A ROS node to perform a probabilistic 3-D/6-DOF localization system for mobile robots with 3-D LIDAR(s).\nse2lam - On-SE(2) Localization and Mapping for Ground Vehicles by Fusing Odometry and Vision.\nmmWave-localization-learning - ML-based positioning method from mmWave transmissions - with high accuracy and energy efficiency.\ndynamic_robot_localization - A ROS package that offers 3 DoF and 6 DoF localization using PCL and allows dynamic map update using OctoMap.\neagleye - An open-source software for vehicle localization utilizing GNSS and IMU.\npython-sgp4 - Python version of the SGP4 satellite position library.\nPROJ - Cartographic Projections and Coordinate Transformations Library.\nrpg_trajectory_evaluation - Implements common used trajectory evaluation methods for visual(-inertial) odometry.\npymap3d - Pure-Python (Numpy optional) 3D coordinate conversions for geospace ecef enu eci.\nlibRSF - A robust sensor fusion library for online localization.\nSimultaneous Localization and Mapping\nLidar\nloam_velodyne - Laser Odometry and Mapping (Loam) is a realtime method for state estimation and mapping using a 3D lidar.\nlio-mapping - Implementation of Tightly Coupled 3D Lidar Inertial Odometry and Mapping (LIO-mapping).\nA-LOAM - Advanced implementation of LOAM.\nFast LOAM - Fast and Optimized Lidar Odometry And Mapping.\nLIO_SAM - Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping.\ncartographer_ros - Provides ROS integration for Cartographer.\nloam_livox - A robust LiDAR Odometry and Mapping (LOAM) package for Livox-LiDAR.\nStaticMapping - Use LiDAR to map the static world.\nsemantic_suma - Semantic Mapping using Surfel Mapping and Semantic Segmentation.\nslam_toolbox - Slam Toolbox for lifelong mapping and localization in potentially massive maps with ROS .\nmaplab - An open visual-inertial mapping framework.\nhdl_graph_slam - An open source ROS package for real-time 6DOF SLAM using a 3D LIDAR.\ninteractive_slam - In contrast to existing automatic SLAM packages, we with minimal human effort.\nLeGO-LOAM - Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain.\npyslam - Contains a monocular Visual Odometry (VO) pipeline in Python.\nKitware SLAM - LiDAR-only visual SLAM developped by Kitware, as well as ROS and ParaView wrappings for easier use.\nhorizon_highway_slam - A robust, low drift, and real time highway SLAM package suitable for Livox Horizon lidar.\nmola - A Modular System for Localization and Mapping.\nDH3D - Deep Hierarchical 3D Descriptors for Robust Large-Scale 6DOF Relocalization.\nLaMa - LaMa is a C++11 software library for robotic localization and mapping.\nScan Context - Global LiDAR descriptor for place recognition and long-term localization.\nM-LOAM - Robust Odometry and Mapping for Multi-LiDAR Systems with Online Extrinsic Calibration.\nVisual\norb_slam_2_ros - A ROS implementation of ORB_SLAM2.\norbslam-map-saving-extension - In this extensions the map of ORB-features be saved to the disk as a reference for future runs along the same track.\ndso - Direct Sparse Odometry.\nviso2 - A ROS wrapper for libviso2, a library for visual odometry.\nxivo - X Inertial-aided Visual Odometry.\nrovio - Robust Visual Inertial Odometry Framework.\nLSD-SLAM - Large-Scale Direct Monocular SLAM is a real-time monocular SLAM.\nCubeSLAM and ORB SLAM - Monocular 3D Object Detection and SLAM Package of CubeSLAM and ORB SLAM.\nVINS-Fusion - A Robust and Versatile Multi-Sensor Visual-Inertial State Estimator.\nopenvslam - OpenVSLAM: A Versatile Visual SLAM Framework.\nbasalt - Visual-Inertial Mapping with Non-Linear Factor Recovery.\nKimera - A C++ library for real-time metric-semantic simultaneous localization and mapping, which uses camera images and inertial data to build a semantically annotated 3D mesh of the environment.\ntagslam - A ROS-based package for Simultaneous Localization and Mapping using AprilTag fiducial markers.\nLARVIO - A lightweight, accurate and robust monocular visual inertial odometry based on Multi-State Constraint Kalman Filter.\nfiducials - Simultaneous localization and mapping using fiducial markers.\nopen_vins - An open source platform for visual-inertial navigation research.\nORB_SLAM3 - ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM.\nAtlas - End-to-End 3D Scene Reconstruction from Posed Images.\nvilib - This library focuses on the front-end of VIO pipelines with CUDA.\nhloc - A modular toolbox for state-of-the-art 6-DoF visual localization. It implements Hierarchical Localization, leveraging image retrieval and feature matching, and is fast, accurate, and scalable.\nESVO - A novel pipeline for real-time visual odometry using a stereo event-based camera.\ngradslam - An open source differentiable dense SLAM library for PyTorch.\nVector Map\nOpenDRIVE - An open file format for the logical description of road networks.\nMapsModelsImporter - A Blender add-on to import models from google maps.\nLanelet2 - Map handling framework for automated driving.\nbarefoot - Online and Offline map matching that can be used stand-alone and in the cloud.\niD - The easy-to-use OpenStreetMap editor in JavaScript.\nRapiD - An enhanced version of iD for mapping with AI created by Facebook.\nsegmap - A map representation based on 3D segments.\nMapbox - A JavaScript library for interactive, customizable vector maps on the web.\nosrm-backend - Open Source Routing Machine - C++ backend.\nassuremapingtools - Desktop based tool for viewing, editing and saving road network maps for autonomous vehicle platforms such as Autoware.\ngeopandas - A project to add support for geographic data to pandas objects.\nMapToolbox - Plugins to make Autoware vector maps in Unity.\nimagery-index - An index of aerial and satellite imagery useful for mapping.\nmapillary_tools - A library for processing and uploading images to Mapillary.\nmapnik - Combines pixel-perfect image output with lightning-fast cartographic algorithms, and exposes interfaces in C++, Python, and Node.\ngdal - GDAL is an open source X/MIT licensed translator library for raster and vector geospatial data formats.\ngrass - GRASS GIS - free and open source Geographic Information System (GIS).\n3d-tiles - Specification for streaming massive heterogeneous 3D geospatial datasets.\nosmnx - Python for street networks. Retrieve, model, analyze, and visualize street networks and other spatial data from OpenStreetMap.\nPrediction\nAwesome-Interaction-aware-Trajectory-Prediction - A selection of state-of-the-art research materials on trajectory prediction.\nsgan - Socially Acceptable Trajectories with Generative Adversarial Networks.\nBehavior and Decision\nGroot - Graphical Editor to create BehaviorTrees. Compliant with BehaviorTree.CPP.\nBehaviorTree.CPP - Behavior Trees Library in C++.\nRAFCON - Uses hierarchical state machines, featuring concurrent state execution, to represent robot programs.\nROSPlan - Generic framework for task planning in a ROS system.\nad-rss-lib - Library implementing the Responsibility Sensitive Safety model (RSS) for Autonomous Vehicles.\nFlexBE - Graphical editor for hierarchical state machines, based on ROS's smach.\nsts_bt_library - This library provides the functionality to set up your own behavior tree logic by using the defined tree structures like Fallback, Sequence or Parallel Nodes.\nSMACC - An Event-Driven, Asynchronous, Behavioral State Machine Library for real-time ROS (Robotic Operating System) applications written in C++ .\npy_trees_ros - Behaviours, trees and utilities that extend py_trees for use with ROS.\nPlanning and Control\npacmod - Designed to allow the user to control a vehicle with the PACMod drive-by-wire system.\nmpcc - Model Predictive Contouring Controller for Autonomous Racing.\nrrt - C++ RRT (Rapidly-exploring Random Tree) implementation.\nHypridAStarTrailer - A path planning algorithm based on Hybrid A* for trailer truck.\npath_planner - Hybrid A* Path Planner for the KTH Research Concept Vehicle.\nopen_street_map - ROS packages for working with Open Street Map geographic information.\nOpen Source Car Control - An assemblage of software and hardware designs that enable computer control of modern cars in order to facilitate the development of autonomous vehicle technology.\nfastrack - A ROS implementation of Fast and Safe Tracking (FaSTrack).\ncommonroad - Composable benchmarks for motion planning on roads.\ntraffic-editor - A graphical editor for robot traffic flows.\nsteering_functions - Contains a C++ library that implements steering functions for car-like robots with limited turning radius.\nmoveit - Easy-to-use robotics manipulation platform for developing applications, evaluating designs, and building integrated products.\nflexible-collision-library - A library for performing three types of proximity queries on a pair of geometric models composed of triangles.\naikido - Artificial Intelligence for Kinematics, Dynamics, and Optimization.\ncasADi - A symbolic framework for numeric optimization implementing automatic differentiation in forward and reverse modes on sparse matrix-valued computational graphs.\nACADO Toolkit - A software environment and algorithm collection for automatic control and dynamic optimization.\ncontrol-toolbox - An efficient C++ library for control, estimation, optimization and motion planning in robotics.\nCrowdNav - Crowd-aware Robot Navigation with Attention-based Deep Reinforcement Learning.\nompl - Consists of many state-of-the-art sampling-based motion planning algorithms.\nopenrave - Open Robotics Automation Virtual Environment: An environment for testing, developing, and deploying robotics motion planning algorithms.\nteb_local_planner - An optimal trajectory planner considering distinctive topologies for mobile robots based on Timed-Elastic-Bands.\npinocchio - A fast and flexible implementation of Rigid Body Dynamics algorithms and their analytical derivatives.\nrmf_core - The rmf_core packages provide the centralized functions of the Robotics Middleware Framework (RMF).\nOpEn - A solver for Fast & Accurate Embedded Optimization for next-generation Robotics and Autonomous Systems.\nautogenu-jupyter - This project provides the continuation/GMRES method (C/GMRES method) based solvers for nonlinear model predictive control (NMPC) and an automatic code generator for NMPC.\nglobal_racetrajectory_optimization - This repository contains multiple approaches for generating global racetrajectories.\ntoppra - A library for computing the time-optimal path parametrization for robots subject to kinematic and dynamic constraints.\ntinyspline - TinySpline is a small, yet powerful library for interpolating, transforming, and querying arbitrary NURBS, B-Splines, and B\u00e9zier curves.\ndual quaternions ros - ROS python package for dual quaternion SLERP.\nmb planner - Aerial vehicle planner for tight spaces. Used in DARPA SubT Challenge.\nilqr - Iterative Linear Quadratic Regulator with auto-differentiatiable dynamics models.\nEGO-Planner - A lightweight gradient-based local planner without ESDF construction, which significantly reduces computation time compared to some state-of-the-art methods.\npykep - A scientific library providing basic tools for research in interplanetary trajectory design.\nam_traj - Alternating Minimization Based Trajectory Generation for Quadrotor Aggressive Flight.\nGraphBasedLocalTrajectoryPlanner - Was used on a real race vehicle during the Roborace Season Alpha and achieved speeds above 200km/h.\nse2_navigation - Pure pursuit controller and Reeds-Shepp sampling based planner for navigation in SE(2) space.\nRuckig - Instantaneous Motion Generation. Real-time. Jerk-constrained. Time-optimal.\nUser Interaction\nGraphical User Interface\nimgui - Designed to enable fast iterations and to empower programmers to create content creation tools and visualization / debug tools.\nqtpy - Provides an uniform layer to support PyQt5, PySide2, PyQt4 and PySide with a single codebase.\nmir - Mir is set of libraries for building Wayland based shells.\nrqt - A Qt-based framework for GUI development for ROS. It consists of three parts/metapackages.\ncage - This is Cage, a Wayland kiosk. A kiosk runs a single, maximized application.\nchilipie - Easy-to-use Raspberry Pi image for booting directly into full-screen Chrome.\npencil - A tool for making diagrams and GUI prototyping that everyone can use.\ndynamic_reconfigure - The focus of dynamic_reconfigure is on providing a standard way to expose a subset of a node's parameters to external reconfiguration.\nddynamic_reconfigure - Allows modifying parameters of a ROS node using the dynamic_reconfigure framework without having to write cfg files.\nelements - A lightweight, fine-grained, resolution independent, modular GUI library.\nNanoGUI - A minimalistic cross-platform widget library for OpenGL 3.x or higher.\nAcoustic User Interface\npyo - A Python module written in C containing classes for a wide variety of audio signal processing types.\nrhasspy - Rhasspy (pronounced RAH-SPEE) is an offline, multilingual voice assistant toolkit inspired by Jasper that works well with Home Assistant, Hass.io, and Node-RED.\nmycroft-core - Mycroft is a hackable open source voice assistant.\nDDSP - A library of differentiable versions of common DSP functions (such as synthesizers, waveshapers, and filters).\nNoiseTorch - Creates a virtual microphone that suppresses noise, in any application.\nDeepSpeech - An open source Speech-To-Text engine, using a model trained by machine learning techniques based on Baidu's Deep Speech research paper.\nwaveglow - A Flow-based Generative Network for Speech Synthesis.\nTTS - A deep learning toolkit for Text-to-Speech, battle-tested in research and production.\nCommand Line Interface\nthe-art-of-command-line - Master the command line, in one page.\ndotfiles of cornerman - Powerful zsh and vim dotfiles.\ndotbot - A tool that bootstraps your dotfiles.\nprompt-hjem - A beautiful zsh prompt.\nag - A code-searching tool similar to ack, but faster.\nfzf - A command-line fuzzy finder.\npkgtop - Interactive package manager and resource monitor designed for the GNU/Linux.\nasciimatics - A cross platform package to do curses-like operations, plus higher level APIs and widgets to create text UIs and ASCII art animations.\ngocui - Minimalist Go package aimed at creating Console User Interfaces.\nTerminalImageViewer - Small C++ program to display images in a (modern) terminal using RGB ANSI codes and unicode block graphics characters.\nrosshow - Visualize ROS topics inside a terminal with Unicode/ASCII art.\npython-prompt-toolkit - Library for building powerful interactive command line applications in Python.\nguake - Drop-down terminal for GNOME.\nwemux - Multi-User Tmux Made Easy.\ntmuxp - A session manager built on libtmux.\nmapscii - World map renderer for your console.\nterminator - The goal of this project is to produce a useful tool for arranging terminals.\nbat - A cat(1) clone with wings.\nfx - Command-line tool and terminal JSON viewer.\ntmate - Instant terminal sharing.\nData Visualization and Mission Control\nxdot - Interactive viewer for graphs written in Graphviz's dot language.\nguacamole - Clientless remote desktop gateway. It supports standard protocols like VNC, RDP, and SSH.\nros3djs - 3D Visualization Library for use with the ROS JavaScript Libraries.\nwebviz - Web-based visualization libraries like rviz.\nplotly.py - An open-source, interactive graphing library for Python.\nPlotJuggler - The timeseries visualization tool that you deserve.\nbokeh - Interactive Data Visualization in the browser, from Python.\nvoila - From Jupyter notebooks to standalone web applications and dashboards.\nPangolin - Pangolin is a lightweight portable rapid development library for managing OpenGL display / interaction and abstracting video input.\nrqt_bag - Provides a GUI plugin for displaying and replaying ROS bag files.\nkepler.gl - Kepler.gl is a powerful open source geospatial analysis tool for large-scale data sets.\nqgis_ros - Access bagged and live topic data in a highly featured GIS environment.\nopenmct - A web based mission control framework.\nweb_video_server - HTTP Streaming of ROS Image Topics in Multiple Formats.\nRVizWeb - Provides a convenient way of building and launching a web application with features similar to RViz.\nmarvros - MAVLink to ROS gateway with proxy for Ground Control Station.\noctave - Provides a convenient command line interface for solving linear and nonlinear problems numerically, and for performing other numerical experiments using a language that is mostly compatible with Matlab.\nstreetscape.gl - Streetscape.gl is a toolkit for visualizing autonomous and robotics data in the XVIZ protocol.\nurdf-loaders - URDF Loaders for Unity and THREE.js with example ATHLETE URDF File.\nobs-studio - Free and open source software for live streaming and screen recording.\nK3D-tools - Jupyter notebook extension for 3D visualization.\nPyQtGraph - Fast data visualization and GUI tools for scientific / engineering applications.\nipygany - 3-D Scientific Visualization in the Jupyter Notebook.\nFoxglove Studio - Web and desktop app for robotics visualization and debugging; actively maintained fork of webviz.\nROS-Mobile - Visualization and controlling application for Android.\nAnnotation\nlabelbox - The fastest way to annotate data to build and ship artificial intelligence applications.\nPixelAnnotationTool - Annotate quickly images.\nLabelImg - A graphical image annotation tool and label object bounding boxes in images.\ncvat - Powerful and efficient Computer Vision Annotation Tool (CVAT).\npoint_labeler - Tool for labeling of a single point clouds or a stream of point clouds.\nlabel-studio - Label Studio is a multi-type data labeling and annotation tool with standardized output format.\nnapari - A fast, interactive, multi-dimensional image viewer for python.\nsemantic-segmentation-editor - A web based labeling tool for creating AI training data sets (2D and 3D).\n3d-bat - 3D Bounding Box Annotation Tool for Point cloud and Image Labeling.\nlabelme - Image Polygonal Annotation with Python (polygon, rectangle, circle, line, point and image-level flag annotation).\nuniversal-data-tool - Collaborate & label any type of data, images, text, or documents, in an easy web interface or desktop app.\nBMW-Labeltool-Lite - Provides you with a easy to use labeling tool for State-of-the-art Deep Learning training purposes.\nPoint Cloud\nCloudCompare - CloudCompare is a 3D point cloud (and triangular mesh) processing software.\nPotree - WebGL point cloud viewer for large datasets.\npoint_cloud_viewer - Makes viewing massive point clouds easy and convenient.\nLidarView - Performs real-time visualization and easy processing of live captured 3D LiDAR data from Lidar sensors.\nVeloView - Performs real-time visualization of live captured 3D LiDAR data from Velodyne's HDL sensors.\nentwine - A data organization library for massive point clouds, designed to conquer datasets of trillions of points as well as desktop-scale point clouds.\npolyscope - A C++ & Python viewer for 3D data like meshes and point clouds.\nPcx - Point cloud importer & renderer for Unity.\nImmersivePoints - A web-application for virtual reality devices to explore 3D data in the most natural way possible.\nRViz\nmapviz - Modular ROS visualization tool for 2D data.\nrviz_cinematographer - Easy to use tools to create and edit trajectories for the rviz camera.\nrviz_satellite - Display internet satellite imagery in RViz.\nrviz_visual_tools - C++ API wrapper for displaying shapes and meshes in Rviz.\nxpp - Visualization of motion-plans for legged robots.\nrviz stereo - 3D stereo rendering displays a different view to each eye so that the scene appears to have depth.\njsk_visualization - Jsk visualization ros packages for rviz and rqt.\nmoveit_visual_tools - Helper functions for displaying and debugging MoveIt! data in Rviz via published markers.\nOperation System\nMonitoring\nrosmon - ROS node launcher & monitoring daemon.\nmultimaster_fkie - GUI-based management environment that is very useful to manage ROS-launch configurations and control running nodes.\ncollectd - A small daemon which collects system information periodically and provides mechanisms to store and monitor the values in a variety of ways.\nlnav - An enhanced log file viewer that takes advantage of any semantic information that can be gleaned from the files being viewed, such as timestamps and log levels.\nhtop - An interactive text-mode process viewer for Unix systems. It aims to be a better 'top'.\natop - System and process monitor for Linux with logging and replay function.\npsutil - Cross-platform lib for process and system monitoring in Python.\ngputil - A Python module for getting the GPU status from NVIDA GPUs using nvidia-smi programmically in Python.\ngpustat - A simple command-line utility for querying and monitoring GPU status.\nnvtop - NVIDIA GPUs htop like monitoring tool.\nShellHub - ShellHub is a modern SSH server for remotely accessing linux devices via command line (using any SSH client) or web-based user interface, designed as an alternative to sshd. Think ShellHub as centralized SSH for the the edge and cloud computing.\nSshwifty - Sshwifty is a SSH and Telnet connector made for the Web.\nspdlog - Very fast, header-only/compiled, C++ logging library.\nctop - Top-like interface for container metrics.\nntop - Web-based Traffic and Security Network Traffic Monitoring.\njupyterlab-nvdashboard - A JupyterLab extension for displaying dashboards of GPU usage.\nDatabase and Record\nncdu - Ncdu is a disk usage analyzer with an ncurses interface.\nborg - Deduplicating archiver with compression and authenticated encryption.\nbag-database - A server that catalogs bag files and provides a web-based UI for accessing them.\nmarv-robotics - MARV Robotics is a powerful and extensible data management platform.\nkitti2bag - Convert KITTI dataset to ROS bag file the easy way.\npykitti - Python tools for working with KITTI data.\nrosbag_editor - Create a rosbag from a given one, using a simple GUI.\nnextcloud - Nextcloud is a suite of client-server software for creating and using file hosting services.\nros_type_introspection - Deserialize ROS messages that are unknown at compilation time.\nsyncthing - A continuous file synchronization program.\nrqt_bag_exporter - Qt GUI to export ROS bag topics to files (CSV and/or video).\nxviz - A protocol for real-time transfer and visualization of autonomy data.\nkitti_to_rosbag - A Dataset tools for working with the KITTI dataset raw data and converting it to a ROS bag. Also allows a library for direct access to poses, velodyne scans, and images.\nros_numpy - Tools for converting ROS messages to and from numpy arrays.\nkitti_ros - A ROS-based player to replay KiTTI dataset.\nDuckDB - An embeddable SQL OLAP Database Management System.\nNetwork Distributed File System\nsshfs - File system based on the SSH File Transfer Protocol.\nmoosefs - A scalable distributed storage system.\nceph - A distributed object, block, and file storage platform.\nnfs - A distributed file system protocol originally developed by Sun Microsystems.\nansible-role-nfs - Installs NFS utilities on RedHat/CentOS or Debian/Ubuntu.\nServer Infrastructure and High Performance Computing\nmass - Self-service, remote installation of Windows, CentOS, ESXi and Ubuntu on real servers turns your data centre into a bare metal cloud.\npolyaxon - A platform for reproducing and managing the whole life cycle of machine learning and deep learning applications.\nlocalstack - A fully functional local AWS cloud stack. Develop and test your cloud & Serverless apps offline.\nnvidia-docker - Build and run Docker containers leveraging NVIDIA GPUs.\nkubeflow - Machine Learning Toolkit for Kubernetes.\nlog-pilot - Collect logs for docker containers.\ntraefik - The Cloud Native Edge Router.\ngraylog2-server - Free and open source log management.\nansible - Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy.\npyinfra - It can be used for ad-hoc command execution, service deployment, configuration management and more.\ndocker-py - A Python library for the Docker Engine API.\nnoVNC - VNC client using HTML5.\nSlurm - Slurm: A Highly Scalable Workload Manager.\njupyterhub - Multi-user server for Jupyter notebooks.\nPortainer - Making Docker management easy.\nenroot - A simple, yet powerful tool to turn traditional container/OS images into unprivileged sandboxes.\ndocker-firefox - Run a Docker Container with Firefox and noVNC for remote access to headless servers.\nluigi - A Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in.\ntriton-inference-server - NVIDIA Triton Inference Server provides a cloud inferencing solution optimized for NVIDIA GPUs.\ncudf - Provides a pandas-like API that will be familiar to data engineers & data scientists, so they can use it to easily accelerate their workflows without going into the details of CUDA programming.\nEmbedded Operation System\nvxworks7-ros2-build - Build system to automate the build of VxWorks 7 and ROS2.\nYocto - Produce tools and processes that enable the creation of Linux distributions for embedded software that are independent of the underlying architecture of the embedded hardware.\nAutomotive Graded Linux - A collaborative open source project that is bringing together automakers, suppliers and technology companies to build a Linux-based, open software platform for automotive applications that can serve as the de facto industry standard.\nbitbake - A generic task execution engine that allows shell and Python tasks to be run efficiently and in parallel while working within complex inter-task dependency constraints.\nJailhouse - Jailhouse is a partitioning Hypervisor based on Linux.\nXen - An open-source (GPL) type-1 or baremetal hypervisor.\nQEMU - A generic and open source machine emulator and virtualizer.\nqemu-xilinx - A fork of Quick EMUlator (QEMU) with improved support and modelling for the Xilinx platforms.\nrosserial - A ROS client library for small, embedded devices, such as Arduino.\nmeta-ros - OpenEmbedded Layer for ROS Applications.\nmeta-balena - Run Docker containers on embedded devices.\nmicro-ros - The major changes compared to \"regular\" ROS 2 is that micro-ROS uses a Real-Time Operating System (RTOS) instead of Linux, and DDS for eXtremely Resource Constrained Environments.\nnvidia-container-runtime - NVIDIA Container Runtime is a GPU aware container runtime, compatible with the Open Containers Initiative (OCI) specification used by Docker, CRI-O, and other popular container technologie.\nfusesoc - Package manager and build abstraction tool for FPGA/ASIC development.\njetson_easy - Automatically script to setup and configure your NVIDIA Jetson.\ndocker-jetpack-sdk - Allows for usage of the NVIDIA JetPack SDK within a docker container for download, flashing, and install.\nPressed - Provides a way to set answers to questions asked during the installation process of debian, without having to manually enter the answers while the installation is running.\njetson_stats - A package to monitoring and control your NVIDIA Jetson (Xavier NX, Nano, AGX Xavier, TX1, TX2) Works with all NVIDIA Jetson ecosystem.\nros_jetson_stats - The ROS jetson-stats wrapper. The status of your NVIDIA jetson in diagnostic messages.\nOpenCR - Open-source Control Module for ROS.\nacrn-hypervisor - Defines a device hypervisor reference stack and an architecture for running multiple software subsystems, managed securely, on a consolidated system by means of a virtual machine manager.\njetson-containers - Machine Learning Containers for Jetson and JetPack 4.4.\nReal-Time Kernel\nELISA - Project is to make it easier for companies to build and certify Linux-based safety-critical applications \u2013 systems whose failure could result in loss of human life, significant property damage or environmental damage.\nPREEMPT_RT kernel patch - Aim of the PREEMPT_RT kernel patch is to minimize the amount of kernel code that is non-preemptible.\nNetwork and Middleware\nperformance_test - Tool to test the performance of pub/sub based communication frameworks.\nrealtime_support - Minimal real-time testing utility for measuring jitter and latency.\nros1_bridge - ROS 2 package that provides bidirectional communication between ROS 1 and ROS 2.\nFast-RTPS - A Protocol, which provides publisher-subscriber communications over unreliable transports such as UDP, as defined and maintained by the Object Management Group (OMG) consortium.\nprotobuf - Google's data interchange format.\nopensplice - Vortex OpenSplice Community Edition.\ncyclonedds - Eclipse Cyclone DDS is a very performant and robust open-source DDS implementation.\niceoryx - An IPC middleware for POSIX-based systems.\nrosbridge_suite - Provides a JSON interface to ROS, allowing any client to send JSON to publish or subscribe to ROS topics, call ROS services, and more.\nros2arduino - This library helps the Arduino board communicate with the ROS2 using XRCE-DDS.\neCAL - The enhanced communication abstraction layer (eCAL) is a middleware that enables scalable, high performance interprocess communication on a single computer node or between different nodes in a computer network.\nAUTOSAR-Adaptive - The implementation of AUTOSAR Adaptive Platform based on the R19-11.\nocpp - The Open Charge Point Protocol (OCPP) is a network protocol for communication between electric vehicle chargers and a central backoffice system.\nmicro-ROS for Arduino - A experimental micro-ROS library for baremetal projects based on Arduino IDE or Arduino CLI.\nmqtt_bridge - Provides a functionality to bridge between ROS and MQTT in bidirectional.\nEthernet and Wireless Networking\nSOES - SOES is an EtherCAT slave stack written in C.\nnetplan - Simply create a YAML description of the required network interfaces and what each should be configured to do.\nairalab - AIRA is reference Robonomics network client for ROS-enabled cyber-physical systems.\nrdbox - RDBOX is a IT infrastructure for ROS robots.\nros_ethercat - This is a reimplementation of the main loop of pr2_ethercat without dependencies on PR2 software.\nwavemon - An ncurses-based monitoring application for wireless network devices.\nwireless - Making info about wireless networks available to ROS.\nptpd - PTP daemon (PTPd) is an implementation the Precision Time Protocol (PTP) version 2 as defined by 'IEEE Std 1588-2008'. PTP provides precise time coordination of Ethernet LAN connected computers.\niperf - A TCP, UDP, and SCTP network bandwidth measurement tool.\ntcpreplay - Pcap editing and replay tools.\nnethogs - It groups bandwidth by process.\npyshark - Python wrapper for tshark, allowing python packet parsing using wireshark dissectors.\npingtop - Ping multiple servers and show results in a top-like terminal UI.\ntermshark - A terminal UI for tshark, inspired by Wireshark.\nudpreplay - Replay UDP packets from a pcap file.\nopenwifi - Linux mac80211 compatible full-stack IEEE802.11/Wi-Fi design based on Software Defined Radio.\nController Area Network\nawesome CAN - A curated list of awesome CAN bus tools, hardware and resources.\nAndrOBD - Android OBD diagnostics with any ELM327 adapter.\nddt4all - DDT4All is a tool to create your own ECU parameters screens and connect to a CAN network with a cheap ELM327 interface.\ncabana - CAN visualizer and DBC maker.\nopendbc - The project to democratize access to the decoder ring of your car.\nlibuavcan - An open lightweight protocol designed for reliable communication in aerospace and robotic applications over robust vehicular networks such as CAN bus.\npython-can - The can package provides controller area network support for Python developers.\nCANopenNode - The internationally standardized (EN 50325-4) (CiA301) CAN-based higher-layer protocol for embedded control system.\npython-udsoncan - Python implementation of UDS (ISO-14229) standard.\nuds-c - Unified Diagnostics Service (UDS) and OBD-II (On Board Diagnostics for Vehicles) C Library.\ncantools - CAN BUS tools in Python 3.\nCANdevStudio - CANdevStudio aims to be cost-effective replacement for CAN simulation software. It can work with variety of CAN hardware interfaces.\ncan-utils - Linux-CAN / SocketCAN user space applications.\nros_canopen - CANopen driver framework for ROS.\ndecanstructor - The definitive ROS CAN analysis tool.\nkvaser_interface - This package was developed as a standardized way to access Kvaser CAN devices from ROS.\ncanmatrix - Converting CAN Database Formats .arxml .dbc .dbf .kcd.\nautosar - A set of python modules for working with AUTOSAR XML files.\ncanopen - A Python implementation of the CANopen standard. The aim of the project is to support the most common parts of the CiA 301 standard in a Pythonic interface.\nSavvyCAN - A Qt5 based cross platform tool which can be used to load, save, and capture canbus frames.\nOpen-Vehicle-Monitoring-System-3 - The system provides live monitoring of vehicle metrics like state of charge, temperatures, tyre pressures and diagnostic fault conditions.\nSensor and Acuator Interfaces\nTesla-API - Provides functionality to monitor and control the Model S (and future Tesla vehicles) remotely.\nflirpy - A Python library to interact with FLIR thermal imaging cameras and images.\nnerian_stereo - ROS node for Nerian's SceneScan and SP1 stereo vision sensors.\npymmw - This is a toolbox composed of Python scripts to interact with TI's evaluation module (BoosterPack) for the IWR1443 mmWave sensing device.\nti_mmwave_rospkg - TI mmWave radar ROS driver (with sensor fusion and hybrid).\npacmod3 - This ROS node is designed to allow the user to control a vehicle with the PACMod drive-by-wire system, board revision 3.\nros2_intel_realsense - These are packages for using Intel RealSense cameras (D400 series) with ROS2.\nsick_scan - This stack provides a ROS2 driver for the SICK TiM series of laser scanners.\nouster_example - Sample code for connecting to and configuring the OS1, reading and visualizing data, and interfacing with ROS.\nros2_ouster_drivers - These are an implementation of ROS2 drivers for the Ouster OS-1 3D lidars.\nlivox_ros_driver - A new ROS package, specially used to connect LiDAR products produced by Livox.\nvelodyne - A collection of ROS packages supporting Velodyne high definition 3D LIDARs.\nublox - Provides support for u-blox GPS receivers.\ncrazyflie_ros - ROS Driver for Bitcraze Crazyflie.\npointgrey_camera_driver - ROS driver for Pt. Grey cameras, based on the official FlyCapture2 SDK.\nnovatel_gps_driver - ROS driver for NovAtel GPS / GNSS receivers.\npylon-ros-camera - The official pylon ROS driver for Basler GigE Vision and USB3 Vision cameras.\nethz_piksi_ros - Contains (python) ROS drivers, tools, launch files, and wikis about how to use Piksi Real Time Kinematic (RTK) GPS device in ROS.\nsick_safetyscanners - A ROS Driver which reads the raw data from the SICK Safety Scanners and publishes the data as a laser_scan msg.\nbosch_imu_driver - A driver for the sensor IMU Bosch BNO055. It was implemented only the UART communication interface (correct sensor mode should be selected).\noxford_gps_eth - Ethernet interface to OxTS GPS receivers using the NCOM packet structure.\nifm3d - Library and Utilities for working with ifm pmd-based 3D ToF Cameras.\ncepton_sdk_redist - Provides ROS support for Cepton LiDAR.\njetson_csi_cam - A ROS package making it simple to use CSI cameras on the Nvidia Jetson TK1, TX1, or TX2 with ROS.\nros_astra_camera - A ROS driver for Orbbec 3D cameras.\nspot_ros - ROS Driver for Spot.\nblickfeld-scanner-lib - Cross-platform library to communicate with LiDAR devices of the Blickfeld GmbH.\nTauLidarCamera - The host-side API for building applications with the Tau LiDAR Camera.\nSecurity\nowasp-threat-dragon-desktop - Threat Dragon is a free, open-source, cross-platform threat modeling application including system diagramming and a rule engine to auto-generate threats/mitigations.\nlaunch_ros_sandbox - Can define launch files running nodes in restrained environments, such as Docker containers or separate user accounts with limited privileges.\nwolfssl - A small, fast, portable implementation of TLS/SSL for embedded devices to the cloud.\nCANalyzat0r - Security analysis toolkit for proprietary car protocols.\nRSF - Robot Security Framework (RSF) is a standardized methodology to perform security assessments in robotics.\nHow-to-Secure-A-Linux-Server - An evolving how-to guide for securing a Linux server.\nlynis - Security auditing tool for Linux, macOS, and UNIX-based systems. Assists with compliance testing (HIPAA/ISO27001/PCI DSS) and system hardening.\nOpenVPN - An open source VPN daemon.\nopenfortivpn - A client for PPP+SSL VPN tunnel services and compatible with Fortinet VPNs.\nWireGuard - WireGuard is a novel VPN that runs inside the Linux Kernel and utilizes state-of-the-art cryptography.\nssh-auditor - Scans for weak ssh passwords on your network.\nvulscan - Advanced vulnerability scanning with Nmap NSE.\nnmap-vulners - NSE script based on Vulners.com API.\nbrutespray - Automatically attempts default creds on found services.\nfail2ban - Daemon to ban hosts that cause multiple authentication errors.\nDependencyCheck - A software composition analysis utility that detects publicly disclosed vulnerabilities in application dependencies.\nFirejail - A SUID sandbox program that reduces the risk of security breaches by restricting the running environment of untrusted applications using Linux namespaces, seccomp-bpf and Linux capabilities.\nRVD - Robot Vulnerability Database. Community-contributed archive of robot vulnerabilities and weaknesses.\nros2_dds_security - Adding security enhancements by defining a Service Plugin Interface (SPI) architecture, a set of builtin implementations of the SPIs, and the security model enforced by the SPIs.\nSecurity-Enhanced Linux - A Linux kernel security module that provides a mechanism for supporting access control security policies, including mandatory access controls (MAC).\nOpenTitan - Will make the silicon Root of Trust design and implementation more transparent, trustworthy, and secure for enterprises, platform providers, and chip manufacturers. OpenTitan is administered by lowRISC CIC as a collaborative project to produce high quality, open IP for instantiation as a full-featured product.\nbandit - A tool designed to find common security issues in Python code.\nhardening - A quick way to make a Ubuntu server a bit more secure.\nPassbolt - Passbolt is a free and open source password manager that allows team members to store and share credentials securely.\ngopass - A password manager for the command line written in Go.\npass - The standard unix password manager.\nVault - A tool for securely accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, certificates, and more.\nlegion - An open source, easy-to-use, super-extensible and semi-automated network penetration testing framework that aids in discovery, reconnaissance and exploitation of information systems.\nopenscap - The oscap program is a command line tool that allows users to load, scan, validate, edit, and export SCAP documents.\nDatasets\nPapers With Code - Thousands of machine learning datasets provided by Papers With Code.\nKITTI-360 - This large-scale dataset contains 320k images and 100k laser scans in a driving distance of 73.7km.\nwaymo_ros - This is a ROS package to connect Waymo open dataset to ROS.\nwaymo-open-dataset - The Waymo Open Dataset is comprised of high-resolution sensor data collected by Waymo self-driving cars in a wide variety of conditions.\nFord Autonomous Vehicle Dataset - Ford presents a challenging multi-agent seasonal dataset collected by a fleet of Ford autonomous vehicles at different days and times.\nawesome-robotics-datasets - A collection of useful datasets for robotics and computer vision.\nnuscenes-devkit - The devkit of the nuScenes dataset.\ndataset-api - This is a repo of toolkit for ApolloScape Dataset, CVPR 2019 Workshop on Autonomous Driving Challenge and ECCV 2018 challenge.\nutbm_robocar_dataset - EU Long-term Dataset with Multiple Sensors for Autonomous Driving.\nDBNet - A Large-Scale Dataset for Driving Behavior Learning.\nargoverse-api - Official GitHub repository for Argoverse dataset.\nDDAD - A new autonomous driving benchmark from TRI (Toyota Research Institute) for long range (up to 250m) and dense depth estimation in challenging and diverse urban conditions.\npandaset-devkit - Public large-scale dataset for autonomous driving provided by Hesai & Scale.\na2d2_to_ros - Utilities for converting A2D2 data sets to ROS bags.\nawesome-satellite-imagery-datasets - List of satellite image training datasets with annotations for computer vision and deep learning.\nsentinelsat - Search and download Copernicus Sentinel satellite images.\nadas-dataset-form - Thermal Dataset for Algorithm Training.\nh3d - The H3D is a large scale full-surround 3D multi-object detection and tracking dataset from Honda.\nMapillary Vistas Dataset - A diverse street-level imagery dataset with pixel\u2011accurate and instance\u2011specific human annotations for understanding street scenes around the world.\nTensorFlow Datasets - TensorFlow Datasets provides many public datasets as tf.data.Datasets.\nracetrack-database - Contains center lines (x- and y-coordinates), track widths and race lines for over 20 race tracks (mainly F1 and DTM) all over the world.\nBlenderProc - A procedural Blender pipeline for photorealistic training image generation.\nAtlatec Sample Map Data - 3D map for autonomous driving and simulation created from nothing but two cameras and GPS in downtown San Francisco.\nLyft Level 5 Dataset - Level 5 is developing a self-driving system for the Lyft network. We're collecting and processing data from our autonomous fleet and sharing it with you.\nholicity - A City-Scale Data Platform for Learning Holistic 3D Structures.\nUTD19 - Largest multi-city traffic dataset publically available.\nASTYX HIRES2019 DATASET - Automotive Radar Dataset for Deep Learning Based 3D Object Detection.\nObjectron - A collection of short, object-centric video clips, which are accompanied by AR session metadata that includes camera poses, sparse point-clouds and characterization of the planar surfaces in the surrounding environment.\nONCE dataset - A large-scale autonomous driving dataset with 2D&3D object annotations.\nFootnotes\nThanks to the team of xpp for creating this awesome GIF we use.", "link": "https://github.com/protontypes/awesome-robotic-tooling", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "awesome robotic tooling\na curated list of tooling for professional robotic development in c++ and python with a touch of ros, autonomous driving and aerospace\nto stop reinventing the wheel you need to know about the wheel. this list is an attempt to show the variety of open and free tools in software and hardware development, which are useful in professional robotic development.\nyour contribution is necessary to keep this list alive, increase the quality and to expand it. you can read more about it's origin and how you can participate in the contribution guide and related blog post. all new project entries will have a tweet from protontypes.\ncontents\ncommunication and coordination\ndocumentation and presentation\nrequirements and safety\narchitecture and design\nframeworks and stacks\ndevelopment environment\ncode and run\ntemplate\nbuild and deploy\nunit and integration test\nlint and format\ndebugging and tracing\nversion control\nsimulation\nelectronics and mechanics\nsensor processing\ncalibration and transformation\nperception pipeline\nmachine learning\nparallel processing\nimage processing\nradar processing\nlidar and point cloud processing\nlocalization and state estimation\nsimultaneous localization and mapping\nlidar\nvisual\nvector map\nprediction\nbehavior and decision\nplanning and control\nuser interaction\ngraphical user interface\nacoustic user interface\ncommand line interface\ndata visualization and mission control\nannotation\npoint cloud\nrviz\noperation system\nmonitoring\ndatabase and record\nnetwork distributed file system\nserver infrastructure and high performance computing\nembedded operation system\nreal-time kernel\nnetwork and middleware\nethernet and wireless networking\ncontroller area network\nsensor and acuator interfaces\nsecurity\ndatasets\ncommunication and coordination\nagile development - manifesto for agile software development.\ngitflow - makes parallel development very easy, by isolating new development from finished work.\ndeepl - an online translator that outperforms google, microsoft and facebook.\ntaiga - agile projectmanagment -----> tool !!! .\nkanboard - minimalistic kanban board.\nkanban - free, open source, self-hosted, kanban board for gitlab issues.\ngitlab - simple selfhosted gitlab server with docker.\ngogs - build a simple, stable and extensible self-hosted git service that can be setup in the most painless way.\nwekan - meteor based kanban board.\njira api - python library for rest api of jira.\ntaiga api - python library for rest api of taiga.\nchronos-timetracker - desktop client for jira. track time, upload worklogs without a hassle.\ngrge - grge is a daemon and command line utility augmenting gitlab.\ngitlab-triage - gitlab's issues and merge requests triage, automated.\nhelpy - a modern, open source helpdesk customer support application.\nonlyoffice - a free open source collaborative system developed to manage documents, projects, customer relationship and email correspondence, all in one place.\ndiscourse - a platform for community discussion. free, open, simple.\ngerrit - a code review and project management tool for git based projects.\njitsi-meet - secure, simple and scalable video conferences that you use as a standalone app or embed in your web application.\nmattermost - an open source, private cloud, slack-alternative.\nopenproject - the leading open source project management software.\nleantime - leantime is a lean project management system for innovators.\ngitter - gitter is a chat and networking platform that helps to manage, grow and connect communities through messaging, content and discovery.\ndocumentation and presentation\ntypora - a minimalist markdown editor.\nmarkor - a simple markdown editor for your android device.\npandoc - universal markup converter.\nyaspeller - command line tool for spell checking.\nreadthedocs - build your local readthedocs server.\ndoxygen - doxygen is the de facto standard tool for generating documentation from annotated c++ sources.\nsphinx - a tool that makes it easy to create intelligent and beautiful documentation for python projects.\nword-to-markdown - a ruby gem to liberate content from microsoft word document.\npaperless - index and archive all of your scanned paper documents.\ncarbon - share beautiful images of your source code.\nundraw - free professional business svgs easy to customize.\nasciinema - lets you easily record terminal sessions and replay them in a terminal as well as in a web browser.\ninkscape - inkscape is a professional vector graphics editor for linux, windows and macos.\nreveal-hugo - a hugo theme for reveal.js that makes authoring and customization a breeze. with it, you can turn any properly-formatted hugo content into a html presentation.\nhugo-webslides - this is a hugo template to create webslides presentation using markdown.\njupyter2slides - cloud native presentation slides with jupyter notebook + reveal.js.\npatat - terminal-based presentations using pandoc.\ngithub-changelog-generator - automatically generate change log from your tags, issues, labels and pull requests on github.\ngitlab-release-note-generator - a gitlab release note generator that generates release note on latest tag.\nocrmypdf - adds an ocr text layer to scanned pdf files, allowing them to be searched.\npapermill - a tool for parameterizing, executing, and analyzing jupyter notebooks.\ndocsy - an example documentation site using the docsy hugo theme.\nactions-hugo - deploy website based on hugo to github pages.\noverleaf - an open-source online real-time collaborative latex editor.\nlandslide - generate html5 slideshows from markdown, rest, or textile.\nlibreoffice-impress-templates - freely-licensed libreoffice impress templates.\nopensourcedesign - community and resources for free design and logo creation.\nolive - a free non-linear video editor aiming to provide a fully-featured alternative to high-end professional video editing software.\nbuku - browser-independent bookmark manager.\nswiftlatex - a wysiwyg browser-based latex editor.\nrelaxed - allows complex pdf layouts to be defined with css and javascript, while writing the content in a friendly, minimal syntax close to markdown or latex.\nfoam - foam is a personal knowledge management and sharing system inspired by roam research, built on visual studio code and github.\ncodimd - open source online real-time collaborate on team documentation in markdown.\njupyter-book - build interactive, publication-quality documents from jupyter notebooks.\ninvoicenet - deep neural network to extract intelligent information from invoice documents.\ntesseract - open source ocr engine.\nmkdocs - a fast, simple and downright gorgeous static site generator that's geared towards building project documentation.\nplotneuralnet - latex code for drawing neural networks for reports and presentation.\nexcalidraw - virtual whiteboard for sketching hand-drawn like diagrams.\nsvgrepo - download free svg vectors for commercial use.\ngollum - a simple, git-powered wiki with a sweet api and local frontend.\nganttlab - the easy to use, fully functional gantt chart for gitlab and github.\nzotero - a free, easy-to-use tool to help you collect, organize, cite, and share your research sources.\nrequirements and safety\nawesome-safety-critical - list of resources about programming practices for writing safety-critical software.\nopen-autonomous-safety - oas is a fully open-source library of voyage's safety processes and testing procedures, designed to supplement existing safety programs at self-driving car startups across the world.\ncarnd-functional-safety-project - create functional safety documents in this udacity project.\nautomated valet parking safety documents - created to support the safe testing of the automated valet parking function using the streetdrone test vehicle in a car park.\nsafe_numerics - replacements to standard numeric types which throw exceptions on errors.\nair vehicle c++ development coding standards - provide direction and guidance to c++ programmers that will enable them to employ good programming style and proven programming practices leading to safe, reliable, testable, and maintainable code.\nautosar coding standard - guidelines for the use of the c++14 language in critical and safety-related system.\nthe w-model and lean scaled agility for engineering - ford applied an agile v-model method from vector that can be used in safety related project management.\ndoorstop - requirements management using version control.\ncapella - comprehensive, extensible and field-proven mbse tool and method to successfully design systems architecture.\nrobmosys - robmosys envisions an integrated approach built on top of the current code-centric robotic platforms, by applying model-driven methods and tools.\npapyrus for robotics - a graphical editing tool for robotic applications that complies with the robmosys approach.\nfossology - a toolkit you can run license, copyright and export control scans from the command line.\nscenarioarchitect - the scenario architect is a basic python tool to generate, import and export short scene snapshots.\narchitecture and design\nguidelines - how to architect ros-based systems.\nyed - a powerful desktop application that can be used to quickly and effectively generate high-quality diagrams.\nyed_py - generates graphml that can be opened in yed.\nplantuml - web application to generate uml diagrams on-the-fly in your live documentation.\nrqt_graph - provides a gui plugin for visualizing the ros computation graph.\nrqt_launchtree - an rqt plugin for hierarchical launchfile configuration introspection.\ncpp-dependencies - tool to check c++ #include dependencies (dependency graphs created in .dot format).\npydeps - python module dependency graphs.\naztarna - a footprinting tool for robots.\ndraw.io - a free online diagram software for making flowcharts, process diagrams, org charts, uml, er and network diagrams.\nvscode-drawio - this extension integrates draw.io into vs code.\narchitecture_decision_record - a document that captures an important architectural decision made along with its context and consequences.\nframeworks and stacks\nros - (robot operating system) provides libraries and tools to help software developers create robot applications.\nawesome-ros2 - a curated list of awesome robot operating system version 2.0 (ros 2) resources and libraries.\nautoware.auto - autoware.auto applies best-in-class software engineering for autonomous driving.\nautoware.ai - autoware.ai is the world's first \"all-in-one\" open-source software for autonomous driving technology.\nopenpilot - open source adaptive cruise control (acc) and lane keeping assist system (lkas).\napollo - high performance, flexible architecture which accelerates the development, testing, and deployment of autonomous vehicles.\npythonrobotics - this is a python code collection of robotics algorithms, especially for autonomous navigation.\nstanford self driving car code - stanford code from cars that entered darpa grand challenges.\nastrobee - astrobee is a free-flying robot designed to operate as a payload inside the international space station (iss).\ncarmaplatform - enables cooperative automated driving plug-in.\nautomotive grade linux - automotive grade linux is a collaborative open source project that is bringing together automakers, suppliers and technology companies to accelerate the development and adoption of a fully open software stack for the connected car.\npx4 - an open source flight control software for drones and other unmanned vehicles.\nkubos - an open-source software stack for satellites.\nmod_vehicle_dynamics_control - tum roborace team software stack - path tracking control, velocity control, curvature control and state estimation.\naslan - open source self-driving software for low speed environments.\nopen-source-rover - a build-it-yourself, 6-wheel rover based on the rovers on mars from jpl.\npybotics - an open-source and peer-reviewed python toolbox for robot kinematics and calibration.\nmakani - contains the working makani flight simulator, controller (autopilot), visualizer, and command center flight monitoring tools.\nmir_robot - this is a community project to use the mir robots with ros.\ncompas - robotic fabrication package for the compas framework.\njderobot academy - jderobot academy is an open source collection of exercises to learn robotics in a practical way.\nclover - ros-based framework and rpi image to control px4-powered drones.\nardupilot - open source control software for autonomous vehicles - copters/planes/rovers/boats/submersibles.\nf prime - a component-driven framework that enables rapid development and deployment of spaceflight and other embedded software applications.\ndevelopment environment\ncode and run\nvim-ros - vim plugin for ros development.\nvisual studio code - code editor for edit-build-debug cycle.\natom - hackable text editor for the 21st century.\nteletype - share your workspace with team members and collaborate on code in real time in atom.\nsublime - a sophisticated text editor for code, markup and prose.\nade-cli - the ade development environment (ade) uses docker and gitlab to manage environments of per project development tools and optional volume images.\nrecipe-wizard - a dockerfile generator for running opengl (glx) applications with nvidia-docker2, cuda, ros, and gazebo on a remote headless server system.\njupyter ros - jupyter widget helpers for ros, the robot operating system.\nros_rqt_plugin - the ros qt creator plug-in for python.\nxeus-cling - jupyter kernel for the c++ programming language.\nros ides - this page collects experience and advice on using integrated development environments (ides) with ros.\ntabnine - the all-language autocompleter.\nkite - use machine learning to give you useful code completions for python.\njedi - autocompletion and static analysis library for python.\nroslibpy - python ros bridge library allows to use python and ironpython to interact with ros, the open-source robotic middleware.\npybind11 - seamless operability between c++11 and python.\nsourcetrail - free and open-source cross-platform source explorer.\nrebound - command-line tool that instantly fetches stack overflow results when an exception is thrown.\nmybinder - open notebooks in an executable environment, making your code immediately reproducible by anyone, anywhere.\nrosonwindows - an experimental release of ros1 for windows.\nlive-share - real-time collaborative development from the comfort of your favorite tools.\ncocalc - collaborative calculation in the cloud.\neasyclangcomplete - robust c/c++ code completion for sublime text 3.\nvscode-ros - visual studio code extension for robot operating system (ros) development.\nawesome-hpp - a curated list of awesome header-only c++ libraries.\ngitpod - an open source developer platform that automates the provisioning of ready-to-code development environments.\ntemplate\nros - template for ros node standardization in c++.\nlaunch - templates on how to create launch files for larger projects.\nbash - a bash scripting template incorporating best practices & several useful functions.\nurdf - examples on how to create unified robot description format (urdf) for different kinds of robots.\npython - style guide to be followed in writing python code for ros.\ndocker - the dockerfile in the minimal-ade project shows a minimal example of how to create a custom base image.\nvs code ros2 workspace template - template for using vscode as an ide for ros2 development.\nbuild and deploy\nqemu-user-static - enable an execution of different multi-architecture containers by qemu and binfmt_misc.\ncross compile ros 2 on qnx - introduces how to cross compile ros 2 on qnx.\nbloom - a release automation tool which makes releasing catkin packages easier.\nsuperflore - an extended platform release manager for robot operating system.\ncatkin_tools - command line tools for working with catkin.\nindustrial_ci - easy continuous integration repository for ros repositories.\nros_gitlab_ci - contains helper scripts and instructions on how to use continuous integration (ci) for ros projects hosted on a gitlab instance.\ngitlab-runner - runs tests and sends the results to gitlab.\ncolcon-core - command line tool to improve the workflow of building, testing and using multiple software packages.\ngitlab-release - simple python3 script to upload files (from ci) to the current projects release (tag).\nclang - this is a compiler front-end for the c family of languages (c, c++, objective-c, and objective-c++) which is built as part of the llvm compiler infrastructure project.\ncatkin_virtualenv - bundle python requirements in a catkin package via virtualenv.\npyenv - simple python version management.\naptly - debian repository management tool.\ncross_compile - assets used for ros2 cross-compilation.\ndocker_images - official docker images maintained by osrf on ros(2) and gazebo.\nrobot_upstart - presents a suite of scripts to assist with launching background ros processes on ubuntu linux pcs.\nrobot_systemd - units for managing startup and shutdown of roscore and roslaunch.\nryo-iso - a modern iso builder that streamlines the process of deploying a complete robot operating system from a yaml config file.\nnetwork_autoconfig - automatic configuration of ros networking for most use cases without impacting usage that require manual configuration.\nrosbuild - the ros build farm.\ncros - a single thread pure c implementation of the ros framework.\nunit and integration test\nsetup-ros - this action sets up a ros and ros 2 environment for use in github actions.\nunittesting - this page lays out the rationale, best practices, and policies for writing and running unit tests and integration tests for ros.\ngoogletest - google's c++ test framework.\npytest - the pytest framework makes it easy to write small tests, yet scales to support complex functional testing.\ndoctest - the fastest feature-rich c++11/14/17/20 single-header testing framework for unit tests and tdd.\nosrf_testing_tools_cpp - contains testing tools for c++, and is used in osrf projects.\ncode_coverage - ros package to run coverage testing.\naction-ros-ci - github action to build and test ros 2 packages using colcon.\nlint and format\naction-ros-lint - github action to run linters on ros 2 packages.\ncppcheck - static analysis of c/c++ code.\nhadolint - dockerfile linter, validate inline bash, written in haskell.\nshellcheck - a static analysis tool for shell scripts.\ncatkin_lint - checks package configurations for the catkin build system of ros.\npylint - pylint is a python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions.\nblack - the uncompromising python code formatter.\npydocstyle - a static analysis tool for checking compliance with python docstring conventions.\nharos - static analysis of ros application code.\npydantic - data parsing and validation using python type hints.\ndebugging and tracing\nheaptrack - traces all memory allocations and annotates these events with stack traces.\nros2_tracing - tracing tools for ros 2.\nlinuxperf - various linux performance material.\nlptrace - it lets you see in real-time what functions a python program is running.\npyre-check - performant type-checking for python.\nflamegraph - visualize profiled code.\ngpuvis - gpu trace visualizer.\nsanitizer - addresssanitizer, threadsanitizer, memorysanitizer.\ncppinsights - c++ insights - see your source code with the eyes of a compiler.\ninspect - the inspect module provides functions for learning about live objects, including modules, classes, instances, functions, and methods.\nroslaunch nodes in valgrind or gdb - when debugging roscpp nodes that you are launching with roslaunch, you may wish to launch the node in a debugging program like gdb or valgrind instead.\npyperformance - python performance benchmark suite.\nqira - qira is a competitor to strace and gdb.\ngdb-frontend - gdbfrontend is an easy, flexible and extensionable gui debugger.\nlttng - an open source software toolkit which you can use to simultaneously trace the linux kernel, user applications, and user libraries.\nros2-performance - allows to easily create arbitrary ros2 systems and then measures their performance.\nbcc - tools for bpf-based linux io analysis, networking, monitoring, and more.\ntracy - a real time, nanosecond resolution, remote telemetry frame profiler for games and other applications.\nbpftrace - high-level tracing language for linux ebpf.\npudb - full-screen console debugger for python.\nbackward-cpp - a beautiful stack trace pretty printer for c++.\ngdb-dashboard - gdb dashboard is a standalone .gdbinit file written using the python api that enables a modular interface showing relevant information about the program being debugged.\nhotspot - the linux perf gui for performance analysis.\nmemory_profiler - a python module for monitoring memory consumption of a process as well as line-by-line analysis of memory consumption for python programs.\nros1_fuzzer - this fuzzer aims to help developers and researchers to find bugs and vulnerabilities in ros nodes by performing fuzz tests over topics that the target nodes process.\nvscode-debug-visualizer - an extension for vs code that visualizes data during debugging.\naction-tmate - debug your github actions via ssh by using tmate to get access to the runner system itself.\nlibstatistics_collector - ros 2 library providing classes to collect measurements and calculate statistics across them.\nsystem_metrics_collector - lightweight, real-time system metrics collector for ros2 systems.\nversion control\ngit-fuzzy - a cli interface to git that relies heavily on fzf.\nmeld - meld is a visual diff and merge tool that helps you compare files, directories, and version controlled projects.\ntig - text-mode interface for git.\ngitg - a graphical user interface for git.\ngit-cola - the highly caffeinated git gui.\npython-gitlab - a python package providing access to the gitlab server api.\nbfg-repo-cleaner - removes large or troublesome blobs like git-filter-branch does, but faster.\nnbdime - tools for diffing and merging of jupyter notebooks.\nsemantic-release - fully automated version management and package publishing.\ngo-semrel-gitab - automate version management for gitlab.\ngit-repo - git-repo helps manage many git repositories, does the uploads to revision control systems, and automates parts of the development workflow.\ndive - a tool for exploring each layer in a docker image.\ndvc - management and versioning of datasets and machine learning models.\nlearngitbranching - a git repository visualizer, sandbox, and a series of educational tutorials and challenges.\ngitfs - you can mount a remote repository's branch locally, and any subsequent changes made to the files will be automatically committed to the remote.\ngit-secret - encrypts files with permitted users' public keys, allowing users you trust to access encrypted data using pgp and their secret keys.\ngit-sweep - a command-line tool that helps you clean up git branches that have been merged into master.\nlazygit - a simple terminal ui for git commands, written in go with the gocui library.\nglab - an open-source gitlab command line tool.\nsimulation\nai2-thor - python framework with a unity backend providing interaction, navigation, and manipulation support for household based robotic agents, consisting of 200+ of custom scenes, 1500+ custom annotated objects, and 200+ actions.\ndrake - drake aims to simulate even very complex dynamics of robots.\nwebots - webots is an open source robot simulator compatible (among others) with ros and ros2.\nlgsv - lg electronics america r&d center has developed an hdrp unity-based multi-robot simulator for autonomous vehicle developers.\ncarla - open-source simulator for autonomous driving research.\nawesome-carla - a curated list of awesome carla tutorials, blogs, and related projects.\nros-bridge - ros bridge for carla simulator.\nscenario_runner - traffic scenario definition and execution engine.\ndeepdive - end-to-end simulation for self-driving cars.\nuuv_simulator - gazebo/ros packages for underwater robotics simulation.\nairsim - open source simulator for autonomous vehicles built on unreal engine.\nself-driving-car-sim - a self-driving car simulator built with unity.\nrosintegration - unreal engine plugin to enable ros support.\ngym-gazebo - an openai gym extension for using gazebo known as gym-gazebo.\ngym-pybullet-drones - pybullet-based gym environments for single and multi-agent reinforcement learning of quadcopter control.\nsafe-control-gym - pybullet-based cartpole and quadrotor environments\u2014with casadi symbolic dynamics and constraints\u2014for safe and robust learning-based control.\nhighway-env - a collection of environments for autonomous driving and tactical decision-making tasks.\nvrep interface - ros bridge for the vrep simulator.\ncar_demo - this is a simulation of a prius in gazebo 9 with sensor data being published using ros kinetic.\nsumo - eclipse sumo is an open source, highly portable, microscopic and continuous road traffic simulation package designed to handle large road networks.\nopen-simulation-interface - a generic interface for the environmental perception of automated driving functions in virtual scenarios.\nesim - an open event camera simulator.\nmenge - crowd simulation framework.\npedsim_ros - pedestrian simulator powered by the social force model for gazebo.\nopencrg - open file formats and open source tools for the detailed description, creation and evaluation of road surfaces.\nesmini - a basic openscenario player.\nopenscenegraph - an open source high performance 3d graphics toolkit, used by application developers in fields such as visual simulation, games, virtual reality, scientific visualization and modelling.\nmorse - an academic robotic simulator, based on the blender game engine and the bullet physics engine.\nrosintegrationvision - support for ros-enabled rgbd data acquisition in unreal engine projects.\nfetch_gazebo - contains the gazebo simulation for fetch robotics fetch and freight research edition robots.\nrotors_simulator - provides some multirotor models.\nflow - a computational framework for deep rl and control experiments for traffic microsimulation.\ngnss-ins-sim - gnss + inertial navigation, sensor fusion simulator. motion trajectory generator, sensor models, and navigation.\nignition robotics - test control strategies in safety, and take advantage of simulation in continuous integration tests.\nsimulation assets for the subt - this collection contains simulation assets for the subt challenge virtual competition in gazebo.\ngazebo_ros_motors - contains currently two motor plugins for gazebo, one with an ideal speed controller and one without a controller that models a dc motor.\nmap2gazebo - ros package for creating gazebo environments from 2d maps.\nsim_vehicle_dynamics - vehicle dynamics simulation software of tum roborace team.\ngym-carla - an openai gym wrapper for carla simulator.\nsimbody - high-performance c++ multibody dynamics/physics library for simulating articulated biomechanical and mechanical systems like vehicles, robots, and the human skeleton.\ngazebo_models - this repository holds the gazebo model database.\npylot - autonomous driving platform running on the carla simulator.\nflightmare - flightmare is composed of two main components: a configurable rendering engine built on unity and a flexible physics engine for dynamics simulation.\nchamp - ros packages for champ quadruped controller.\nrex-gym - openai gym environments for an open-source quadruped robot (spotmicro).\ntrick - developed at the nasa johnson space center, is a powerful simulation development framework that enables users to build applications for all phases of space vehicle development.\nusv_sim_lsa - unmanned surface vehicle simulation on gazebo with water current and winds.\n42 - simulation for spacecraft attitude control system analysis and design.\ncomplete_street_rule - a scenario oriented design tool intended to enable users to quickly create procedurally generated multimodal streets in arcgis cityengine.\nautocore simulation - provides test environment for autoware and still during early development, contents below may changed during updates.\nfields-ignition - generate random crop fields for ignition gazebo.\nunity-robotics-hub - central repository for tools, tutorials, resources, and documentation for robotic simulation in unity.\nbluesky - the goal of bluesky is to provide everybody who wants to visualize, analyze or simulate air traffic with a tool to do so without any restrictions, licenses or limitations.\ncloe - empowers developers of automated-driving software components by providing a unified interface to closed-loop simulation.\ndynamic_logistics_warehouse - gazebo simulation of dynamics environment in warehouses.\nopencda - a generalized framework for prototyping full-stack cooperative driving automation applications under carla+sumo.\nelectronics and mechanics\nhrim - an information model for robot hardware.\nurdf - repository for unified robot description format (urdf) parsing code.\nphobos - an add-on for blender allowing to create urdf, sdf and smurf robot models in a wysiwyg environment.\nurdf-viz - visualize urdf/xacro file, urdf viewer works on windows/macos/linux.\nsolidworks_urdf_exporter - solidworks to urdf exporter.\nfreecad - your own 3d parametric modeler.\nkicad - a cross platform and open source electronics design automation suite.\npcbdraw - convert your kicad board into a nice looking 2d drawing suitable for pinout diagrams.\nkicad-3rd-party-tools - tools made by others to augment the kicad pcb eda suite.\npandapower - an easy to use open source tool for power system modeling, analysis and optimization with a high degree of automation.\nlibrepcb - a powerful, innovative and intuitive eda tool for everyone.\nopenscad - a software for creating solid 3d cad models.\nngspice - a open source spice simulator for electric and electronic circuits.\ngnss-sdr - gnss-sdr provides interfaces for a wide range of radio frequency front-ends and raw sample file formats, generates processing outputs in standard formats.\nriscv - the free and open risc instruction set architecture.\nurdfpy - a simple and easy-to-use library for loading, manipulating, saving, and visualizing urdf files.\nfmpy - simulate functional mockup units (fmus) in python.\nfmikit-simulink - import and export functional mock-up units with simulink.\noemof-solph - a modular open source framework to model energy supply systems.\nnasa-3d-resources - here you'll find a growing collection of 3d models, textures, and images from inside nasa.\nsuave - an aircraft design toolbox.\nopem - the open-source pemfc simulation tool (opem) is a modeling tool for evaluating the performance of proton exchange membrane fuel cells.\npvlib-python - a community supported tool that provides a set of functions and classes for simulating the performance of photovoltaic energy systems.\nwireviz - a tool for easily documenting cables, wiring harnesses and connector pinouts.\nhorizon - eda is an electronic design automation package supporting an integrated end-to-end workflow for printed circuit board design including parts management and schematic entry.\ntigl - the tigl geometry library can be used for the computation and processing of aircraft geometries stored inside cpacs files.\nfoxbms - a free, open and flexible development environment to design battery management systems.\ncadcad - a python package that assists in the processes of designing, testing and validating complex systems through simulation, with support for monte carlo methods, a/b testing and parameter sweeping.\nopenmdao - an open-source framework for efficient multidisciplinary optimization.\nodrive - the aim is to make it possible to use inexpensive brushless motors in high performance robotics projects.\nopentirepython - an open-source mathematical tire modelling library.\ninkscape ray optics - an extension for inkscape that makes it easier to draw optical diagrams.\nopenaerostruct - a lightweight tool that performs aerostructural optimization using openmdao.\nsensor processing\ncalibration and transformation\ntf2 - transform library, which lets the user keep track of multiple coordinate frames over time.\ntrip - a inverse kinematics library for serial robots, parallel robots and hybrids of both.\nlidar_align - a simple method for finding the extrinsic calibration between a 3d lidar and a 6-dof pose sensor.\nkalibr - the kalibr visual-inertial calibration toolbox.\ncalibnet - self-supervised extrinsic calibration using 3d spatial transformer networks.\nlidar_camera_calibration - ros package to find a rigid-body transformation between a lidar and a camera.\nilcc - reflectance intensity assisted automatic and accurate extrinsic calibration of 3d lidar.\neasy_handeye - simple, straighforward ros library for hand-eye calibration.\nimu_utils - a ros package tool to analyze the imu performance.\nkalibr_allan - imu allan standard deviation charts for use with kalibr and inertial kalman filters.\npyquaternion - a full-featured python module for representing and using quaternions.\nrobot_calibration - this package offers calibration of a number of parameters of a robot, such as: 3d camera intrinsics, extrinsics joint angle offsets and robot frame offsets.\nmulti_sensor_calibration - contains a calibration tool to calibrate a sensor setup consisting of lidars, radars and cameras.\nlidartag - a real-time fiducial tag using point clouds lidar data.\nmulticam_calibration - extrinsic and intrinsic calbration of cameras.\nikpy - an inverse kinematics library aiming performance and modularity.\nlivox_camera_lidar_calibration - calibrate the extrinsic parameters between livox lidar and camera.\nlidar_camera_calibration - camera lidar calibration using ros, opencv, and pcl.\ne2calib - contains code that implements video reconstruction from event data for calibration.\nperception pipeline\nsarosperceptionkitti - ros package for the perception (sensor processing, detection, tracking and evaluation) of the kitti vision benchmark suite.\nmultiple-object-tracking-lidar - c++ implementation to detect, track and classify multiple objects using lidar scans or point cloud.\ncadrl_ros - ros package for dynamic obstacle avoidance for ground robots trained with deep rl.\naugmentedautoencoder - rgb-based pipeline for object detection and 6d pose estimation.\njsk_recognition - a stack for the perception packages which are used in jsk lab.\ngibsonenv - gibson environments: real-world perception for embodied agents.\nmorefusion - multi-object reasoning for 6d pose estimation from volumetric fusion.\nmachine learning\ndlib - a toolkit for making real world machine learning and data analysis applications in c++.\nfastai - the fastai library simplifies training fast and accurate neural nets using modern best practices.\ntpot - a python automated machine learning tool that optimizes machine learning pipelines using genetic programming.\ndeap - distributed evolutionary algorithms in python.\ngym - a toolkit for developing and comparing reinforcement learning algorithms.\ntensorflow_ros_cpp - a ros package that allows to do tensorflow inference in c++ without the need to compile tf yourself.\ntensorflow federated - tensorflow federated (tff) is an open-source framework for machine learning and other computations on decentralized data.\nfinn - fast, scalable quantized neural network inference on fpgas.\nneuropod - neuropod is a library that provides a uniform interface to run deep learning models from multiple frameworks in c++ and python.\nleela-zero - this is a fairly faithful reimplementation of the system described in the alpha go zero paper \"mastering the game of go without human knowledge\".\ntrax - a library for deep learning that focuses on sequence models and reinforcement learning.\nmlflow - a platform to streamline machine learning development, including tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\nnetron - visualizer for neural network, deep learning and machine learning models.\nmnn - a blazing fast, lightweight deep learning framework, battle-tested by business-critical use cases in alibaba.\ntensorforce - an open-source deep reinforcement learning framework, with an emphasis on modularized flexible library design and straightforward usability for applications in research and practice.\ndopamine - a research framework for fast prototyping of reinforcement learning algorithms.\ncatalyst - was developed with a focus on reproducibility, fast experimentation and code/ideas reusing.\nray - a fast and simple framework for building and running distributed applications.\ntf-agents - a reliable, scalable and easy to use tensorflow library for contextual bandits and reinforcement learning.\nreagent - an open source end-to-end platform for applied reinforcement learning (rl) developed and used at facebook.\nawesome-mobile-machine-learning - a curated list of awesome mobile machine learning resources for ios, android, and edge devices.\ncnn-explainer - learning convolutional neural networks with interactive visualization.\nmodelzoo - a collection of machine-learned models for use in autonomous driving applications.\nnnstreamer-ros - a set of gstreamer plugins and ros examples that allow gstreamer developers to adopt neural network models easily and efficiently and neural network developers to manage neural network pipelines and their filters easily and efficiently.\nparallel processing\ndask - parallel computing with task scheduling for python.\ncupy - numpy-like api accelerated with cuda.\nthrust - a c++ parallel programming library which resembles the c++ standard library.\narrayfire - a general purpose gpu library.\nopenmp - an application programming interface that supports multi-platform shared memory multiprocessing programming in c, c++, and fortran.\nvexcl - vexcl is a c++ vector expression template library for opencl/cuda/openmp.\npynq - an open-source project from xilinx that makes it easy to design embedded systems with zynq all programmable systems on chips.\nnumba - numpy aware dynamic python compiler using llvm.\ntensorrt - a c++ library for high performance inference on nvidia gpus and deep learning accelerators.\nlibcudacxx - provides a heterogeneous implementation of the c++ standard library that can be used in and between cpu and gpu code.\nimage processing\ncv-pretrained-model - a collection of computer vision pre-trained models.\nimage_pipeline - fills the gap between getting raw images from a camera driver and higher-level vision processing.\ngstreamer - a pipeline-based multimedia framework that links together a wide variety of media processing systems to complete complex workflows.\nros2_openvino_toolkit - provides a ros-adaptered runtime framework of neural network which quickly deploys applications and solutions for vision inference.\nvision_visp - wraps the visp moving edge tracker provided by the visp visual servoing library into a ros package.\napriltag_ros - a ros wrapper of the apriltag 3 visual fiducial detector.\ndeep_object_pose - deep object pose estimation.\ndetectandtrack - detect-and-track: efficient pose.\nsfmlearner - an unsupervised learning framework for depth and ego-motion estimation.\nimgaug - image augmentation for machine learning experiments.\nvision_opencv - packages for interfacing ros with opencv, a library of programming functions for real time computer vision.\ndarknet_ros - yolo ros: real-time object detection for ros.\nros_ncnn - yolact / yolo ( among other things ) on ncnn inference engine for ros.\ntf-pose-estimation - deep pose estimation implemented using tensorflow with custom architectures for fast inference.\nfind-object - simple qt interface to try opencv implementations of sift, surf, fast, brief and other feature detectors and descriptors.\nyolact - a simple, fully convolutional model for real-time instance segmentation.\nkimera-semantics - real-time 3d semantic reconstruction from 2d data.\ndetectron2 - a next-generation research platform for object detection and segmentation.\nopenvx - enables performance and power-optimized computer vision processing, especially important in embedded and real-time use cases.\n3d-vehicle-tracking - official implementation of joint monocular 3d vehicle detection and tracking.\npysot - the goal of pysot is to provide a high-quality, high-performance codebase for visual tracking research.\nsemantic_slam - real time semantic slam in ros with a hand held rgb-d camera.\nkitti_scan_unfolding - we propose kitti scan unfolding in our paper scan-based semantic segmentation of lidar point clouds: an experimental study.\npacknet-sfm - official pytorch implementation of self-supervised monocular depth estimation methods invented by the ml team at toyota research institute (tri).\nab3dmot - this work proposes a simple yet accurate real-time baseline 3d multi-object tracking system.\nmonoloco - official implementation of \"monoloco: monocular 3d pedestrian localization and uncertainty estimation\" in pytorch.\npoly-yolo - builds on the original ideas of yolov3 and removes two of its weaknesses: a large amount of rewritten labels and inefficient distribution of anchors.\nsatellite-image-deep-learning - resources for deep learning with satellite & aerial imagery.\nrobosat - semantic segmentation on aerial and satellite imagery.\nbig_transfer - model for general visual representation learning created by google research.\nlednet - a lightweight encoder-decoder network for real-time semantic segmentation.\ntorchseg - this project aims at providing a fast, modular reference implementation for semantic segmentation models using pytorch.\nsimpledet - a simple and versatile framework for object detection and instance recognition.\nmeshroom - meshroom is a free, open-source 3d reconstruction software based on the alicevision photogrammetric computer vision framework.\neasyocr - ready-to-use optical character recognition (ocr) with 40+ languages supported including chinese, japanese, korean and thai.\npytracking - a general python framework for visual object tracking and video object segmentation, based on pytorch.\nros_deep_learning - deep learning inference nodes for ros with support for nvidia jetson tx1/tx2/xavier and tensorrt.\nhyperpose - hyperpose: a flexible library for real-time human pose estimation.\nfawkes - privacy preserving tool against facial recognition systems.\nanonymizer - an anonymizer to obfuscate faces and license plates.\nopendatacam - only saves surveyed meta-data, in particular the path an object moved or number of counted objects at a certain point.\ncam2bev - tensorflow implementation for computing a semantically segmented bird's eye view (bev) image given the images of multiple vehicle-mounted cameras.\nflownet2-pytorch - pytorch implementation of flownet 2.0: evolution of optical flow estimation with deep networks.\nsimd - c++ image processing and machine learning library with using of simd: sse, sse2, sse3, ssse3, sse4.1, sse4.2, avx, avx2, avx-512, vmx(altivec) and vsx(power7), neon for arm.\nalicevision - a photogrammetric computer vision framework which provides a 3d reconstruction and camera tracking algorithms.\nsatpy - a python library for reading and manipulating meteorological remote sensing data and writing it to various image and data file formats.\neo-learn - a collection of open source python packages that have been developed to seamlessly access and process spatio-temporal image sequences acquired by any satellite fleet in a timely and automatic manner.\nlibvips - a fast image processing library with low memory needs.\nradar processing\npyrosar - framework for large-scale sar satellite data processing.\ncameraradarfusionnet - tum roborace team software stack - path tracking control, velocity control, curvature control and state estimation.\nlidar and point cloud processing\ncilantro - a lean c++ library for working with point cloud data.\nopen3d - open3d: a modern library for 3d data processing.\nsqueezeseg - implementation of squeezeseg, convolutional neural networks for lidar point clout segmentation.\npoint_cloud_io - ros nodes to read and write point clouds from and to files (e.g. ply, vtk).\npython-pcl - python bindings to the pointcloud library.\nlibpointmatcher - an \"iterative closest point\" library for 2-d/3-d mapping in robotics.\ndepth_clustering - fast and robust clustering of point clouds generated with a velodyne sensor.\nlidar-bonnetal - semantic and instance segmentation of lidar point clouds for autonomous driving.\ncsf - lidar point cloud ground filtering / segmentation (bare earth extraction) method based on cloth simulation.\nrobot_body_filter - a highly configurable laserscan/pointcloud2 filter that allows to dynamically remove the 3d body of the robot from the measurements.\ngrid_map - universal grid map library for mobile robotic mapping.\nelevation_mapping - robot-centric elevation mapping for rough terrain navigation.\nrangenet_lib - contains simple usage explanations of how the rangenet++ inference works with the tensorrt and c++ interface.\npointcloud_to_laserscan - converts a 3d point cloud into a 2d laser scan.\noctomap - an efficient probabilistic 3d mapping framework based on octrees.\npptk - point processing toolkit from heremaps.\ngpu-voxels - gpu-voxels is a cuda based library which allows high resolution volumetric collision detection between animated 3d models and live pointclouds from 3d sensors of all kinds.\nspatio_temporal_voxel_layer - a new voxel layer leveraging modern 3d graphics tools to modernize navigation environmental representations.\nlastools - award-winning software for efficient lidar processing.\npcdet - a general pytorch-based codebase for 3d object detection from point cloud.\npdal - a c++ bsd library for translating and manipulating point cloud data.\npotreeconverter - builds a potree octree from las, laz, binary ply, xyz or ptx files.\nfast_gicp - a collection of gicp-based fast point cloud registration algorithms.\nndt_omp - multi-threaded and sse friendly ndt algorithm.\nlaser_line_extraction - a ros packages that extracts line segments from laserscan messages.\ngo-icp - implementation of the go-icp algorithm for globally optimal 3d pointset registration.\npointcnn - a simple and general framework for feature learning from point clouds.\nsegmenters_lib - the lidar segmenters library, for segmentation-based detection.\nmotionnet - joint perception and motion prediction for autonomous driving based on bird's eye view maps.\npolarseg - an improved grid representation for online lidar point clouds semantic segmentation.\ntraversability_mapping - takes in point cloud from a velodyne vlp-16 lidar and outputs a traversability map for autonomous navigation in real-time.\nlidar_super_resolution - simulation-based lidar super-resolution for ground vehicles.\ncupoch - a library that implements rapid 3d data processing and robotics computation using cuda.\nlinefit_ground_segmentation - implementation of the ground segmentation algorithm.\ndraco - a library for compressing and decompressing 3d geometric meshes and point clouds.\nvotenet - deep hough voting for 3d object detection in point clouds.\nlidar_undistortion - provides lidar motion undistortion based on an external 6dof pose estimation input.\nsuperpoint_graph - large-scale point cloud semantic segmentation with superpoint graphs.\nrandla-net - efficient semantic segmentation of large-scale point clouds.\ndet3d - a first 3d object detection toolbox which provides off the box implementations of many 3d object detection algorithms such as pointpillars, second, pixor.\noverlapnet - a modified siamese network that predicts the overlap and relative yaw angle of a pair of range images generated by 3d lidar scans.\nmp2p_icp - a repertory of multi primitive-to-primitive (mp2p) icp algorithms in c++.\nopenpcdet - a toolbox for lidar-based 3d object detection.\ntorch-points3d - pytorch framework for doing deep learning on point clouds.\npolyfit - polygonal surface reconstruction from point clouds.\nmmdetection3d - next-generation platform for general 3d object detection.\ngpd - takes a point cloud as input and produces pose estimates of viable grasps as output.\nsalsanext - uncertainty-aware semantic segmentation of lidar point clouds for autonomous driving.\nsuper-fast-accurate-3d-object-detection - super fast and accurate 3d object detection based on 3d lidar point clouds (the pytorch implementation).\nkaolin - a pytorch library for accelerating 3d deep learning research.\ncamvox - a low-cost slam system based on camera and livox lidar.\nsa-ssd - structure aware single-stage 3d object detection from point cloud.\ncuda-pcl - accelerating lidar for robotics with nvidia cuda-based pcl.\nlocalization and state estimation\nevo - python package for the evaluation of odometry and slam.\nrobot_localization - a package of nonlinear state estimation nodes.\nfuse - general architecture for performing sensor fusion live on a robot.\ngeographiclib - a c++ library for geographic projections.\nntripbrowser - a python api for browsing ntrip (networked transport of rtcm via internet protocol).\nimu_tools - imu-related filters and visualizers.\nrtklib - a version of rtklib optimized for single and dual frequency low cost gps receivers, especially u-blox receivers.\nglab - performs precise modeling of gnss observables (pseudorange and carrier phase) at the centimetre level, allowing standalone gps positioning, ppp, sbas and dgnss.\nai-imu-dr - contains the code of our novel accurate method for dead reckoning of wheeled vehicles based only on an imu.\nkalman-and-bayesian-filters-in-python - kalman filter book using jupyter notebook.\nmcl_3dl - a ros node to perform a probabilistic 3-d/6-dof localization system for mobile robots with 3-d lidar(s).\nse2lam - on-se(2) localization and mapping for ground vehicles by fusing odometry and vision.\nmmwave-localization-learning - ml-based positioning method from mmwave transmissions - with high accuracy and energy efficiency.\ndynamic_robot_localization - a ros package that offers 3 dof and 6 dof localization using pcl and allows dynamic map update using octomap.\neagleye - an open-source software for vehicle localization utilizing gnss and imu.\npython-sgp4 - python version of the sgp4 satellite position library.\nproj - cartographic projections and coordinate transformations library.\nrpg_trajectory_evaluation - implements common used trajectory evaluation methods for visual(-inertial) odometry.\npymap3d - pure-python (numpy optional) 3d coordinate conversions for geospace ecef enu eci.\nlibrsf - a robust sensor fusion library for online localization.\nsimultaneous localization and mapping\nlidar\nloam_velodyne - laser odometry and mapping (loam) is a realtime method for state estimation and mapping using a 3d lidar.\nlio-mapping - implementation of tightly coupled 3d lidar inertial odometry and mapping (lio-mapping).\na-loam - advanced implementation of loam.\nfast loam - fast and optimized lidar odometry and mapping.\nlio_sam - tightly-coupled lidar inertial odometry via smoothing and mapping.\ncartographer_ros - provides ros integration for cartographer.\nloam_livox - a robust lidar odometry and mapping (loam) package for livox-lidar.\nstaticmapping - use lidar to map the static world.\nsemantic_suma - semantic mapping using surfel mapping and semantic segmentation.\nslam_toolbox - slam toolbox for lifelong mapping and localization in potentially massive maps with ros .\nmaplab - an open visual-inertial mapping framework.\nhdl_graph_slam - an open source ros package for real-time 6dof slam using a 3d lidar.\ninteractive_slam - in contrast to existing automatic slam packages, we with minimal human effort.\nlego-loam - lightweight and ground-optimized lidar odometry and mapping on variable terrain.\npyslam - contains a monocular visual odometry (vo) pipeline in python.\nkitware slam - lidar-only visual slam developped by kitware, as well as ros and paraview wrappings for easier use.\nhorizon_highway_slam - a robust, low drift, and real time highway slam package suitable for livox horizon lidar.\nmola - a modular system for localization and mapping.\ndh3d - deep hierarchical 3d descriptors for robust large-scale 6dof relocalization.\nlama - lama is a c++11 software library for robotic localization and mapping.\nscan context - global lidar descriptor for place recognition and long-term localization.\nm-loam - robust odometry and mapping for multi-lidar systems with online extrinsic calibration.\nvisual\norb_slam_2_ros - a ros implementation of orb_slam2.\norbslam-map-saving-extension - in this extensions the map of orb-features be saved to the disk as a reference for future runs along the same track.\ndso - direct sparse odometry.\nviso2 - a ros wrapper for libviso2, a library for visual odometry.\nxivo - x inertial-aided visual odometry.\nrovio - robust visual inertial odometry framework.\nlsd-slam - large-scale direct monocular slam is a real-time monocular slam.\ncubeslam and orb slam - monocular 3d object detection and slam package of cubeslam and orb slam.\nvins-fusion - a robust and versatile multi-sensor visual-inertial state estimator.\nopenvslam - openvslam: a versatile visual slam framework.\nbasalt - visual-inertial mapping with non-linear factor recovery.\nkimera - a c++ library for real-time metric-semantic simultaneous localization and mapping, which uses camera images and inertial data to build a semantically annotated 3d mesh of the environment.\ntagslam - a ros-based package for simultaneous localization and mapping using apriltag fiducial markers.\nlarvio - a lightweight, accurate and robust monocular visual inertial odometry based on multi-state constraint kalman filter.\nfiducials - simultaneous localization and mapping using fiducial markers.\nopen_vins - an open source platform for visual-inertial navigation research.\norb_slam3 - orb-slam3: an accurate open-source library for visual, visual-inertial and multi-map slam.\natlas - end-to-end 3d scene reconstruction from posed images.\nvilib - this library focuses on the front-end of vio pipelines with cuda.\nhloc - a modular toolbox for state-of-the-art 6-dof visual localization. it implements hierarchical localization, leveraging image retrieval and feature matching, and is fast, accurate, and scalable.\nesvo - a novel pipeline for real-time visual odometry using a stereo event-based camera.\ngradslam - an open source differentiable dense slam library for pytorch.\nvector map\nopendrive - an open file format for the logical description of road networks.\nmapsmodelsimporter - a blender add-on to import models from google maps.\nlanelet2 - map handling framework for automated driving.\nbarefoot - online and offline map matching that can be used stand-alone and in the cloud.\nid - the easy-to-use openstreetmap editor in javascript.\nrapid - an enhanced version of id for mapping with ai created by facebook.\nsegmap - a map representation based on 3d segments.\nmapbox - a javascript library for interactive, customizable vector maps on the web.\nosrm-backend - open source routing machine - c++ backend.\nassuremapingtools - desktop based tool for viewing, editing and saving road network maps for autonomous vehicle platforms such as autoware.\ngeopandas - a project to add support for geographic data to pandas objects.\nmaptoolbox - plugins to make autoware vector maps in unity.\nimagery-index - an index of aerial and satellite imagery useful for mapping.\nmapillary_tools - a library for processing and uploading images to mapillary.\nmapnik - combines pixel-perfect image output with lightning-fast cartographic algorithms, and exposes interfaces in c++, python, and node.\ngdal - gdal is an open source x/mit licensed translator library for raster and vector geospatial data formats.\ngrass - grass gis - free and open source geographic information system (gis).\n3d-tiles - specification for streaming massive heterogeneous 3d geospatial datasets.\nosmnx - python for street networks. retrieve, model, analyze, and visualize street networks and other spatial data from openstreetmap.\nprediction\nawesome-interaction-aware-trajectory-prediction - a selection of state-of-the-art research materials on trajectory prediction.\nsgan - socially acceptable trajectories with generative adversarial networks.\nbehavior and decision\ngroot - graphical editor to create behaviortrees. compliant with behaviortree.cpp.\nbehaviortree.cpp - behavior trees library in c++.\nrafcon - uses hierarchical state machines, featuring concurrent state execution, to represent robot programs.\nrosplan - generic framework for task planning in a ros system.\nad-rss-lib - library implementing the responsibility sensitive safety model (rss) for autonomous vehicles.\nflexbe - graphical editor for hierarchical state machines, based on ros's smach.\nsts_bt_library - this library provides the functionality to set up your own behavior tree logic by using the defined tree structures like fallback, sequence or parallel nodes.\nsmacc - an event-driven, asynchronous, behavioral state machine library for real-time ros (robotic operating system) applications written in c++ .\npy_trees_ros - behaviours, trees and utilities that extend py_trees for use with ros.\nplanning and control\npacmod - designed to allow the user to control a vehicle with the pacmod drive-by-wire system.\nmpcc - model predictive contouring controller for autonomous racing.\nrrt - c++ rrt (rapidly-exploring random tree) implementation.\nhypridastartrailer - a path planning algorithm based on hybrid a* for trailer truck.\npath_planner - hybrid a* path planner for the kth research concept vehicle.\nopen_street_map - ros packages for working with open street map geographic information.\nopen source car control - an assemblage of software and hardware designs that enable computer control of modern cars in order to facilitate the development of autonomous vehicle technology.\nfastrack - a ros implementation of fast and safe tracking (fastrack).\ncommonroad - composable benchmarks for motion planning on roads.\ntraffic-editor - a graphical editor for robot traffic flows.\nsteering_functions - contains a c++ library that implements steering functions for car-like robots with limited turning radius.\nmoveit - easy-to-use robotics manipulation platform for developing applications, evaluating designs, and building integrated products.\nflexible-collision-library - a library for performing three types of proximity queries on a pair of geometric models composed of triangles.\naikido - artificial intelligence for kinematics, dynamics, and optimization.\ncasadi - a symbolic framework for numeric optimization implementing automatic differentiation in forward and reverse modes on sparse matrix-valued computational graphs.\nacado toolkit - a software environment and algorithm collection for automatic control and dynamic optimization.\ncontrol-toolbox - an efficient c++ library for control, estimation, optimization and motion planning in robotics.\ncrowdnav - crowd-aware robot navigation with attention-based deep reinforcement learning.\nompl - consists of many state-of-the-art sampling-based motion planning algorithms.\nopenrave - open robotics automation virtual environment: an environment for testing, developing, and deploying robotics motion planning algorithms.\nteb_local_planner - an optimal trajectory planner considering distinctive topologies for mobile robots based on timed-elastic-bands.\npinocchio - a fast and flexible implementation of rigid body dynamics algorithms and their analytical derivatives.\nrmf_core - the rmf_core packages provide the centralized functions of the robotics middleware framework (rmf).\nopen - a solver for fast & accurate embedded optimization for next-generation robotics and autonomous systems.\nautogenu-jupyter - this project provides the continuation/gmres method (c/gmres method) based solvers for nonlinear model predictive control (nmpc) and an automatic code generator for nmpc.\nglobal_racetrajectory_optimization - this repository contains multiple approaches for generating global racetrajectories.\ntoppra - a library for computing the time-optimal path parametrization for robots subject to kinematic and dynamic constraints.\ntinyspline - tinyspline is a small, yet powerful library for interpolating, transforming, and querying arbitrary nurbs, b-splines, and b\u00e9zier curves.\ndual quaternions ros - ros python package for dual quaternion slerp.\nmb planner - aerial vehicle planner for tight spaces. used in darpa subt challenge.\nilqr - iterative linear quadratic regulator with auto-differentiatiable dynamics models.\nego-planner - a lightweight gradient-based local planner without esdf construction, which significantly reduces computation time compared to some state-of-the-art methods.\npykep - a scientific library providing basic tools for research in interplanetary trajectory design.\nam_traj - alternating minimization based trajectory generation for quadrotor aggressive flight.\ngraphbasedlocaltrajectoryplanner - was used on a real race vehicle during the roborace season alpha and achieved speeds above 200km/h.\nse2_navigation - pure pursuit controller and reeds-shepp sampling based planner for navigation in se(2) space.\nruckig - instantaneous motion generation. real-time. jerk-constrained. time-optimal.\nuser interaction\ngraphical user interface\nimgui - designed to enable fast iterations and to empower programmers to create content creation tools and visualization / debug tools.\nqtpy - provides an uniform layer to support pyqt5, pyside2, pyqt4 and pyside with a single codebase.\nmir - mir is set of libraries for building wayland based shells.\nrqt - a qt-based framework for gui development for ros. it consists of three parts/metapackages.\ncage - this is cage, a wayland kiosk. a kiosk runs a single, maximized application.\nchilipie - easy-to-use raspberry pi image for booting directly into full-screen chrome.\npencil - a tool for making diagrams and gui prototyping that everyone can use.\ndynamic_reconfigure - the focus of dynamic_reconfigure is on providing a standard way to expose a subset of a node's parameters to external reconfiguration.\nddynamic_reconfigure - allows modifying parameters of a ros node using the dynamic_reconfigure framework without having to write cfg files.\nelements - a lightweight, fine-grained, resolution independent, modular gui library.\nnanogui - a minimalistic cross-platform widget library for opengl 3.x or higher.\nacoustic user interface\npyo - a python module written in c containing classes for a wide variety of audio signal processing types.\nrhasspy - rhasspy (pronounced rah-spee) is an offline, multilingual voice assistant toolkit inspired by jasper that works well with home assistant, hass.io, and node-red.\nmycroft-core - mycroft is a hackable open source voice assistant.\nddsp - a library of differentiable versions of common dsp functions (such as synthesizers, waveshapers, and filters).\nnoisetorch - creates a virtual microphone that suppresses noise, in any application.\ndeepspeech - an open source speech-to-text engine, using a model trained by machine learning techniques based on baidu's deep speech research paper.\nwaveglow - a flow-based generative network for speech synthesis.\ntts - a deep learning toolkit for text-to-speech, battle-tested in research and production.\ncommand line interface\nthe-art-of-command-line - master the command line, in one page.\ndotfiles of cornerman - powerful zsh and vim dotfiles.\ndotbot - a tool that bootstraps your dotfiles.\nprompt-hjem - a beautiful zsh prompt.\nag - a code-searching tool similar to ack, but faster.\nfzf - a command-line fuzzy finder.\npkgtop - interactive package manager and resource monitor designed for the gnu/linux.\nasciimatics - a cross platform package to do curses-like operations, plus higher level apis and widgets to create text uis and ascii art animations.\ngocui - minimalist go package aimed at creating console user interfaces.\nterminalimageviewer - small c++ program to display images in a (modern) terminal using rgb ansi codes and unicode block graphics characters.\nrosshow - visualize ros topics inside a terminal with unicode/ascii art.\npython-prompt-toolkit - library for building powerful interactive command line applications in python.\nguake - drop-down terminal for gnome.\nwemux - multi-user tmux made easy.\ntmuxp - a session manager built on libtmux.\nmapscii - world map renderer for your console.\nterminator - the goal of this project is to produce a useful tool for arranging terminals.\nbat - a cat(1) clone with wings.\nfx - command-line tool and terminal json viewer.\ntmate - instant terminal sharing.\ndata visualization and mission control\nxdot - interactive viewer for graphs written in graphviz's dot language.\nguacamole - clientless remote desktop gateway. it supports standard protocols like vnc, rdp, and ssh.\nros3djs - 3d visualization library for use with the ros javascript libraries.\nwebviz - web-based visualization libraries like rviz.\nplotly.py - an open-source, interactive graphing library for python.\nplotjuggler - the timeseries visualization tool that you deserve.\nbokeh - interactive data visualization in the browser, from python.\nvoila - from jupyter notebooks to standalone web applications and dashboards.\npangolin - pangolin is a lightweight portable rapid development library for managing opengl display / interaction and abstracting video input.\nrqt_bag - provides a gui plugin for displaying and replaying ros bag files.\nkepler.gl - kepler.gl is a powerful open source geospatial analysis tool for large-scale data sets.\nqgis_ros - access bagged and live topic data in a highly featured gis environment.\nopenmct - a web based mission control framework.\nweb_video_server - http streaming of ros image topics in multiple formats.\nrvizweb - provides a convenient way of building and launching a web application with features similar to rviz.\nmarvros - mavlink to ros gateway with proxy for ground control station.\noctave - provides a convenient command line interface for solving linear and nonlinear problems numerically, and for performing other numerical experiments using a language that is mostly compatible with matlab.\nstreetscape.gl - streetscape.gl is a toolkit for visualizing autonomous and robotics data in the xviz protocol.\nurdf-loaders - urdf loaders for unity and three.js with example athlete urdf file.\nobs-studio - free and open source software for live streaming and screen recording.\nk3d-tools - jupyter notebook extension for 3d visualization.\npyqtgraph - fast data visualization and gui tools for scientific / engineering applications.\nipygany - 3-d scientific visualization in the jupyter notebook.\nfoxglove studio - web and desktop app for robotics visualization and debugging; actively maintained fork of webviz.\nros-mobile - visualization and controlling application for android.\nannotation\nlabelbox - the fastest way to annotate data to build and ship artificial intelligence applications.\npixelannotationtool - annotate quickly images.\nlabelimg - a graphical image annotation tool and label object bounding boxes in images.\ncvat - powerful and efficient computer vision annotation tool (cvat).\npoint_labeler - tool for labeling of a single point clouds or a stream of point clouds.\nlabel-studio - label studio is a multi-type data labeling and annotation tool with standardized output format.\nnapari - a fast, interactive, multi-dimensional image viewer for python.\nsemantic-segmentation-editor - a web based labeling tool for creating ai training data sets (2d and 3d).\n3d-bat - 3d bounding box annotation tool for point cloud and image labeling.\nlabelme - image polygonal annotation with python (polygon, rectangle, circle, line, point and image-level flag annotation).\nuniversal-data-tool - collaborate & label any type of data, images, text, or documents, in an easy web interface or desktop app.\nbmw-labeltool-lite - provides you with a easy to use labeling tool for state-of-the-art deep learning training purposes.\npoint cloud\ncloudcompare - cloudcompare is a 3d point cloud (and triangular mesh) processing software.\npotree - webgl point cloud viewer for large datasets.\npoint_cloud_viewer - makes viewing massive point clouds easy and convenient.\nlidarview - performs real-time visualization and easy processing of live captured 3d lidar data from lidar sensors.\nveloview - performs real-time visualization of live captured 3d lidar data from velodyne's hdl sensors.\nentwine - a data organization library for massive point clouds, designed to conquer datasets of trillions of points as well as desktop-scale point clouds.\npolyscope - a c++ & python viewer for 3d data like meshes and point clouds.\npcx - point cloud importer & renderer for unity.\nimmersivepoints - a web-application for virtual reality devices to explore 3d data in the most natural way possible.\nrviz\nmapviz - modular ros visualization tool for 2d data.\nrviz_cinematographer - easy to use tools to create and edit trajectories for the rviz camera.\nrviz_satellite - display internet satellite imagery in rviz.\nrviz_visual_tools - c++ api wrapper for displaying shapes and meshes in rviz.\nxpp - visualization of motion-plans for legged robots.\nrviz stereo - 3d stereo rendering displays a different view to each eye so that the scene appears to have depth.\njsk_visualization - jsk visualization ros packages for rviz and rqt.\nmoveit_visual_tools - helper functions for displaying and debugging moveit! data in rviz via published markers.\noperation system\nmonitoring\nrosmon - ros node launcher & monitoring daemon.\nmultimaster_fkie - gui-based management environment that is very useful to manage ros-launch configurations and control running nodes.\ncollectd - a small daemon which collects system information periodically and provides mechanisms to store and monitor the values in a variety of ways.\nlnav - an enhanced log file viewer that takes advantage of any semantic information that can be gleaned from the files being viewed, such as timestamps and log levels.\nhtop - an interactive text-mode process viewer for unix systems. it aims to be a better 'top'.\natop - system and process monitor for linux with logging and replay function.\npsutil - cross-platform lib for process and system monitoring in python.\ngputil - a python module for getting the gpu status from nvida gpus using nvidia-smi programmically in python.\ngpustat - a simple command-line utility for querying and monitoring gpu status.\nnvtop - nvidia gpus htop like monitoring tool.\nshellhub - shellhub is a modern ssh server for remotely accessing linux devices via command line (using any ssh client) or web-based user interface, designed as an alternative to sshd. think shellhub as centralized ssh for the the edge and cloud computing.\nsshwifty - sshwifty is a ssh and telnet connector made for the web.\nspdlog - very fast, header-only/compiled, c++ logging library.\nctop - top-like interface for container metrics.\nntop - web-based traffic and security network traffic monitoring.\njupyterlab-nvdashboard - a jupyterlab extension for displaying dashboards of gpu usage.\ndatabase and record\nncdu - ncdu is a disk usage analyzer with an ncurses interface.\nborg - deduplicating archiver with compression and authenticated encryption.\nbag-database - a server that catalogs bag files and provides a web-based ui for accessing them.\nmarv-robotics - marv robotics is a powerful and extensible data management platform.\nkitti2bag - convert kitti dataset to ros bag file the easy way.\npykitti - python tools for working with kitti data.\nrosbag_editor - create a rosbag from a given one, using a simple gui.\nnextcloud - nextcloud is a suite of client-server software for creating and using file hosting services.\nros_type_introspection - deserialize ros messages that are unknown at compilation time.\nsyncthing - a continuous file synchronization program.\nrqt_bag_exporter - qt gui to export ros bag topics to files (csv and/or video).\nxviz - a protocol for real-time transfer and visualization of autonomy data.\nkitti_to_rosbag - a dataset tools for working with the kitti dataset raw data and converting it to a ros bag. also allows a library for direct access to poses, velodyne scans, and images.\nros_numpy - tools for converting ros messages to and from numpy arrays.\nkitti_ros - a ros-based player to replay kitti dataset.\nduckdb - an embeddable sql olap database management system.\nnetwork distributed file system\nsshfs - file system based on the ssh file transfer protocol.\nmoosefs - a scalable distributed storage system.\nceph - a distributed object, block, and file storage platform.\nnfs - a distributed file system protocol originally developed by sun microsystems.\nansible-role-nfs - installs nfs utilities on redhat/centos or debian/ubuntu.\nserver infrastructure and high performance computing\nmass - self-service, remote installation of windows, centos, esxi and ubuntu on real servers turns your data centre into a bare metal cloud.\npolyaxon - a platform for reproducing and managing the whole life cycle of machine learning and deep learning applications.\nlocalstack - a fully functional local aws cloud stack. develop and test your cloud & serverless apps offline.\nnvidia-docker - build and run docker containers leveraging nvidia gpus.\nkubeflow - machine learning toolkit for kubernetes.\nlog-pilot - collect logs for docker containers.\ntraefik - the cloud native edge router.\ngraylog2-server - free and open source log management.\nansible - ansible is a radically simple it automation platform that makes your applications and systems easier to deploy.\npyinfra - it can be used for ad-hoc command execution, service deployment, configuration management and more.\ndocker-py - a python library for the docker engine api.\nnovnc - vnc client using html5.\nslurm - slurm: a highly scalable workload manager.\njupyterhub - multi-user server for jupyter notebooks.\nportainer - making docker management easy.\nenroot - a simple, yet powerful tool to turn traditional container/os images into unprivileged sandboxes.\ndocker-firefox - run a docker container with firefox and novnc for remote access to headless servers.\nluigi - a python module that helps you build complex pipelines of batch jobs. it handles dependency resolution, workflow management, visualization etc. it also comes with hadoop support built in.\ntriton-inference-server - nvidia triton inference server provides a cloud inferencing solution optimized for nvidia gpus.\ncudf - provides a pandas-like api that will be familiar to data engineers & data scientists, so they can use it to easily accelerate their workflows without going into the details of cuda programming.\nembedded operation system\nvxworks7-ros2-build - build system to automate the build of vxworks 7 and ros2.\nyocto - produce tools and processes that enable the creation of linux distributions for embedded software that are independent of the underlying architecture of the embedded hardware.\nautomotive graded linux - a collaborative open source project that is bringing together automakers, suppliers and technology companies to build a linux-based, open software platform for automotive applications that can serve as the de facto industry standard.\nbitbake - a generic task execution engine that allows shell and python tasks to be run efficiently and in parallel while working within complex inter-task dependency constraints.\njailhouse - jailhouse is a partitioning hypervisor based on linux.\nxen - an open-source (gpl) type-1 or baremetal hypervisor.\nqemu - a generic and open source machine emulator and virtualizer.\nqemu-xilinx - a fork of quick emulator (qemu) with improved support and modelling for the xilinx platforms.\nrosserial - a ros client library for small, embedded devices, such as arduino.\nmeta-ros - openembedded layer for ros applications.\nmeta-balena - run docker containers on embedded devices.\nmicro-ros - the major changes compared to \"regular\" ros 2 is that micro-ros uses a real-time operating system (rtos) instead of linux, and dds for extremely resource constrained environments.\nnvidia-container-runtime - nvidia container runtime is a gpu aware container runtime, compatible with the open containers initiative (oci) specification used by docker, cri-o, and other popular container technologie.\nfusesoc - package manager and build abstraction tool for fpga/asic development.\njetson_easy - automatically script to setup and configure your nvidia jetson.\ndocker-jetpack-sdk - allows for usage of the nvidia jetpack sdk within a docker container for download, flashing, and install.\npressed - provides a way to set answers to questions asked during the installation process of debian, without having to manually enter the answers while the installation is running.\njetson_stats - a package to monitoring and control your nvidia jetson (xavier nx, nano, agx xavier, tx1, tx2) works with all nvidia jetson ecosystem.\nros_jetson_stats - the ros jetson-stats wrapper. the status of your nvidia jetson in diagnostic messages.\nopencr - open-source control module for ros.\nacrn-hypervisor - defines a device hypervisor reference stack and an architecture for running multiple software subsystems, managed securely, on a consolidated system by means of a virtual machine manager.\njetson-containers - machine learning containers for jetson and jetpack 4.4.\nreal-time kernel\nelisa - project is to make it easier for companies to build and certify linux-based safety-critical applications \u2013 systems whose failure could result in loss of human life, significant property damage or environmental damage.\npreempt_rt kernel patch - aim of the preempt_rt kernel patch is to minimize the amount of kernel code that is non-preemptible.\nnetwork and middleware\nperformance_test - tool to test the performance of pub/sub based communication frameworks.\nrealtime_support - minimal real-time testing utility for measuring jitter and latency.\nros1_bridge - ros 2 package that provides bidirectional communication between ros 1 and ros 2.\nfast-rtps - a protocol, which provides publisher-subscriber communications over unreliable transports such as udp, as defined and maintained by the object management group (omg) consortium.\nprotobuf - google's data interchange format.\nopensplice - vortex opensplice community edition.\ncyclonedds - eclipse cyclone dds is a very performant and robust open-source dds implementation.\niceoryx - an ipc middleware for posix-based systems.\nrosbridge_suite - provides a json interface to ros, allowing any client to send json to publish or subscribe to ros topics, call ros services, and more.\nros2arduino - this library helps the arduino board communicate with the ros2 using xrce-dds.\necal - the enhanced communication abstraction layer (ecal) is a middleware that enables scalable, high performance interprocess communication on a single computer node or between different nodes in a computer network.\nautosar-adaptive - the implementation of autosar adaptive platform based on the r19-11.\nocpp - the open charge point protocol (ocpp) is a network protocol for communication between electric vehicle chargers and a central backoffice system.\nmicro-ros for arduino - a experimental micro-ros library for baremetal projects based on arduino ide or arduino cli.\nmqtt_bridge - provides a functionality to bridge between ros and mqtt in bidirectional.\nethernet and wireless networking\nsoes - soes is an ethercat slave stack written in c.\nnetplan - simply create a yaml description of the required network interfaces and what each should be configured to do.\nairalab - aira is reference robonomics network client for ros-enabled cyber-physical systems.\nrdbox - rdbox is a it infrastructure for ros robots.\nros_ethercat - this is a reimplementation of the main loop of pr2_ethercat without dependencies on pr2 software.\nwavemon - an ncurses-based monitoring application for wireless network devices.\nwireless - making info about wireless networks available to ros.\nptpd - ptp daemon (ptpd) is an implementation the precision time protocol (ptp) version 2 as defined by 'ieee std 1588-2008'. ptp provides precise time coordination of ethernet lan connected computers.\niperf - a tcp, udp, and sctp network bandwidth measurement tool.\ntcpreplay - pcap editing and replay tools.\nnethogs - it groups bandwidth by process.\npyshark - python wrapper for tshark, allowing python packet parsing using wireshark dissectors.\npingtop - ping multiple servers and show results in a top-like terminal ui.\ntermshark - a terminal ui for tshark, inspired by wireshark.\nudpreplay - replay udp packets from a pcap file.\nopenwifi - linux mac80211 compatible full-stack ieee802.11/wi-fi design based on software defined radio.\ncontroller area network\nawesome can - a curated list of awesome can bus tools, hardware and resources.\nandrobd - android obd diagnostics with any elm327 adapter.\nddt4all - ddt4all is a tool to create your own ecu parameters screens and connect to a can network with a cheap elm327 interface.\ncabana - can visualizer and dbc maker.\nopendbc - the project to democratize access to the decoder ring of your car.\nlibuavcan - an open lightweight protocol designed for reliable communication in aerospace and robotic applications over robust vehicular networks such as can bus.\npython-can - the can package provides controller area network support for python developers.\ncanopennode - the internationally standardized (en 50325-4) (cia301) can-based higher-layer protocol for embedded control system.\npython-udsoncan - python implementation of uds (iso-14229) standard.\nuds-c - unified diagnostics service (uds) and obd-ii (on board diagnostics for vehicles) c library.\ncantools - can bus tools in python 3.\ncandevstudio - candevstudio aims to be cost-effective replacement for can simulation software. it can work with variety of can hardware interfaces.\ncan-utils - linux-can / socketcan user space applications.\nros_canopen - canopen driver framework for ros.\ndecanstructor - the definitive ros can analysis tool.\nkvaser_interface - this package was developed as a standardized way to access kvaser can devices from ros.\ncanmatrix - converting can database formats .arxml .dbc .dbf .kcd.\nautosar - a set of python modules for working with autosar xml files.\ncanopen - a python implementation of the canopen standard. the aim of the project is to support the most common parts of the cia 301 standard in a pythonic interface.\nsavvycan - a qt5 based cross platform tool which can be used to load, save, and capture canbus frames.\nopen-vehicle-monitoring-system-3 - the system provides live monitoring of vehicle metrics like state of charge, temperatures, tyre pressures and diagnostic fault conditions.\nsensor and acuator interfaces\ntesla-api - provides functionality to monitor and control the model s (and future tesla vehicles) remotely.\nflirpy - a python library to interact with flir thermal imaging cameras and images.\nnerian_stereo - ros node for nerian's scenescan and sp1 stereo vision sensors.\npymmw - this is a toolbox composed of python scripts to interact with ti's evaluation module (boosterpack) for the iwr1443 mmwave sensing device.\nti_mmwave_rospkg - ti mmwave radar ros driver (with sensor fusion and hybrid).\npacmod3 - this ros node is designed to allow the user to control a vehicle with the pacmod drive-by-wire system, board revision 3.\nros2_intel_realsense - these are packages for using intel realsense cameras (d400 series) with ros2.\nsick_scan - this stack provides a ros2 driver for the sick tim series of laser scanners.\nouster_example - sample code for connecting to and configuring the os1, reading and visualizing data, and interfacing with ros.\nros2_ouster_drivers - these are an implementation of ros2 drivers for the ouster os-1 3d lidars.\nlivox_ros_driver - a new ros package, specially used to connect lidar products produced by livox.\nvelodyne - a collection of ros packages supporting velodyne high definition 3d lidars.\nublox - provides support for u-blox gps receivers.\ncrazyflie_ros - ros driver for bitcraze crazyflie.\npointgrey_camera_driver - ros driver for pt. grey cameras, based on the official flycapture2 sdk.\nnovatel_gps_driver - ros driver for novatel gps / gnss receivers.\npylon-ros-camera - the official pylon ros driver for basler gige vision and usb3 vision cameras.\nethz_piksi_ros - contains (python) ros drivers, tools, launch files, and wikis about how to use piksi real time kinematic (rtk) gps device in ros.\nsick_safetyscanners - a ros driver which reads the raw data from the sick safety scanners and publishes the data as a laser_scan msg.\nbosch_imu_driver - a driver for the sensor imu bosch bno055. it was implemented only the uart communication interface (correct sensor mode should be selected).\noxford_gps_eth - ethernet interface to oxts gps receivers using the ncom packet structure.\nifm3d - library and utilities for working with ifm pmd-based 3d tof cameras.\ncepton_sdk_redist - provides ros support for cepton lidar.\njetson_csi_cam - a ros package making it simple to use csi cameras on the nvidia jetson tk1, tx1, or tx2 with ros.\nros_astra_camera - a ros driver for orbbec 3d cameras.\nspot_ros - ros driver for spot.\nblickfeld-scanner-lib - cross-platform library to communicate with lidar devices of the blickfeld gmbh.\ntaulidarcamera - the host-side api for building applications with the tau lidar camera.\nsecurity\nowasp-threat-dragon-desktop - threat dragon is a free, open-source, cross-platform threat modeling application including system diagramming and a rule engine to auto-generate threats/mitigations.\nlaunch_ros_sandbox - can define launch files running nodes in restrained environments, such as docker containers or separate user accounts with limited privileges.\nwolfssl - a small, fast, portable implementation of tls/ssl for embedded devices to the cloud.\ncanalyzat0r - security analysis toolkit for proprietary car protocols.\nrsf - robot security framework (rsf) is a standardized methodology to perform security assessments in robotics.\nhow-to-secure-a-linux-server - an evolving how-to guide for securing a linux server.\nlynis - security auditing tool for linux, macos, and unix-based systems. assists with compliance testing (hipaa/iso27001/pci dss) and system hardening.\nopenvpn - an open source vpn daemon.\nopenfortivpn - a client for ppp+ssl vpn tunnel services and compatible with fortinet vpns.\nwireguard - wireguard is a novel vpn that runs inside the linux kernel and utilizes state-of-the-art cryptography.\nssh-auditor - scans for weak ssh passwords on your network.\nvulscan - advanced vulnerability scanning with nmap nse.\nnmap-vulners - nse script based on vulners.com api.\nbrutespray - automatically attempts default creds on found services.\nfail2ban - daemon to ban hosts that cause multiple authentication errors.\ndependencycheck - a software composition analysis utility that detects publicly disclosed vulnerabilities in application dependencies.\nfirejail - a suid sandbox program that reduces the risk of security breaches by restricting the running environment of untrusted applications using linux namespaces, seccomp-bpf and linux capabilities.\nrvd - robot vulnerability database. community-contributed archive of robot vulnerabilities and weaknesses.\nros2_dds_security - adding security enhancements by defining a service plugin interface (spi) architecture, a set of builtin implementations of the spis, and the security model enforced by the spis.\nsecurity-enhanced linux - a linux kernel security module that provides a mechanism for supporting access control security policies, including mandatory access controls (mac).\nopentitan - will make the silicon root of trust design and implementation more transparent, trustworthy, and secure for enterprises, platform providers, and chip manufacturers. opentitan is administered by lowrisc cic as a collaborative project to produce high quality, open ip for instantiation as a full-featured product.\nbandit - a tool designed to find common security issues in python code.\nhardening - a quick way to make a ubuntu server a bit more secure.\npassbolt - passbolt is a free and open source password manager that allows team members to store and share credentials securely.\ngopass - a password manager for the command line written in go.\npass - the standard unix password manager.\nvault - a tool for securely accessing secrets. a secret is anything that you want to tightly control access to, such as api keys, passwords, certificates, and more.\nlegion - an open source, easy-to-use, super-extensible and semi-automated network penetration testing framework that aids in discovery, reconnaissance and exploitation of information systems.\nopenscap - the oscap program is a command line tool that allows users to load, scan, validate, edit, and export scap documents.\ndatasets\npapers with code - thousands of machine learning datasets provided by papers with code.\nkitti-360 - this large-scale dataset contains 320k images and 100k laser scans in a driving distance of 73.7km.\nwaymo_ros - this is a ros package to connect waymo open dataset to ros.\nwaymo-open-dataset - the waymo open dataset is comprised of high-resolution sensor data collected by waymo self-driving cars in a wide variety of conditions.\nford autonomous vehicle dataset - ford presents a challenging multi-agent seasonal dataset collected by a fleet of ford autonomous vehicles at different days and times.\nawesome-robotics-datasets - a collection of useful datasets for robotics and computer vision.\nnuscenes-devkit - the devkit of the nuscenes dataset.\ndataset-api - this is a repo of toolkit for apolloscape dataset, cvpr 2019 workshop on autonomous driving challenge and eccv 2018 challenge.\nutbm_robocar_dataset - eu long-term dataset with multiple sensors for autonomous driving.\ndbnet - a large-scale dataset for driving behavior learning.\nargoverse-api - official github repository for argoverse dataset.\nddad - a new autonomous driving benchmark from tri (toyota research institute) for long range (up to 250m) and dense depth estimation in challenging and diverse urban conditions.\npandaset-devkit - public large-scale dataset for autonomous driving provided by hesai & scale.\na2d2_to_ros - utilities for converting a2d2 data sets to ros bags.\nawesome-satellite-imagery-datasets - list of satellite image training datasets with annotations for computer vision and deep learning.\nsentinelsat - search and download copernicus sentinel satellite images.\nadas-dataset-form - thermal dataset for algorithm training.\nh3d - the h3d is a large scale full-surround 3d multi-object detection and tracking dataset from honda.\nmapillary vistas dataset - a diverse street-level imagery dataset with pixel\u2011accurate and instance\u2011specific human annotations for understanding street scenes around the world.\ntensorflow datasets - tensorflow datasets provides many public datasets as tf.data.datasets.\nracetrack-database - contains center lines (x- and y-coordinates), track widths and race lines for over 20 race tracks (mainly f1 and dtm) all over the world.\nblenderproc - a procedural blender pipeline for photorealistic training image generation.\natlatec sample map data - 3d map for autonomous driving and simulation created from nothing but two cameras and gps in downtown san francisco.\nlyft level 5 dataset - level 5 is developing a self-driving system for the lyft network. we're collecting and processing data from our autonomous fleet and sharing it with you.\nholicity - a city-scale data platform for learning holistic 3d structures.\nutd19 - largest multi-city traffic dataset publically available.\nastyx hires2019 dataset - automotive radar dataset for deep learning based 3d object detection.\nobjectron - a collection of short, object-centric video clips, which are accompanied by ar session metadata that includes camera poses, sparse point-clouds and characterization of the planar surfaces in the surrounding environment.\nonce dataset - a large-scale autonomous driving dataset with 2d&3d object annotations.\nfootnotes\nthanks to the team of xpp for creating this awesome gif we use.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000025, "year": null}, {"Unnamed: 0": 1070, "autor": 50, "date": null, "content": "eProsima Fast DDS\neprosima Fast DDS (formerly Fast RTPS) is a C++ implementation of the DDS (Data Distribution Service) standard of the OMG (Object Management Group). eProsima Fast DDS implements the RTPS (Real Time Publish Subscribe) protocol, which provides publisher-subscriber communications over unreliable transports such as UDP, as defined and maintained by the Object Management Group (OMG) consortium. RTPS is also the wire interoperability protocol defined for the Data Distribution Service (DDS) standard. eProsima Fast DDS expose an API to access directly the RTPS protocol, giving the user full access to the protocol internals.\nSome of the main features of this library are:\nConfigurable best-effort and reliable publish-subscribe communication policies for real-time applications.\nPlug and play connectivity so that any new applications are automatically discovered by any other members of the network.\nModularity and scalability to allow continuous growth with complex and simple devices in the network.\nConfigurable network behavior and interchangeable transport layer: Choose the best protocol and system input/output channel combination for each deployment.\nTwo API Layers: a high-level Publisher-Subscriber one focused on usability (DDS) and a lower-level Writer-Reader one that provides finer access to the inner workings of the RTPS protocol.\neProsima Fast DDS has been adopted by multiple organizations in many sectors including these important cases:\nRobotics: ROS (Robotic Operating System) as their default middleware for ROS2 until and including the latest long term release Foxy Fitzroy.\nEU R&D: FIWARE Incubated GE.\nThis project is part of FIWARE. For more information check the FIWARE Catalogue entry for Robotics.\nWant us to share your project with the community?\nWrite to evaluation.support@eprosima.com or mention @EProsima on Twitter. We are curious to get to know your use case!\nSupported platforms\nLinux\nLinux-aarch64\nWindows\nMac\nInstallation Guide\nYou can get either a binary distribution of eprosima Fast DDS or compile the library yourself from source.\nInstallation from binaries\nThe latest, up to date binary release of eprosima Fast DDS can be obtained from the company website.\nInstallation from Source\nDependencies\nAsio and TinyXML2 libraries\nOn Linux, you can install these libraries using the package manager of your Linux distribution. For example, on Ubuntu you can install them by using its package manager with the next command.\nsudo apt install libasio-dev libtinyxml2-dev\nOn Windows, you can install these libraries using Chocolatey. First, download the following chocolatey packages from this ROS2 Github repository.\nasio.1.12.1.nupkg\ntinyxml2.6.0.0.nupkg\nOnce these packages are downloaded, open an administrative shell and execute the following command:\nchoco install -y -s <PATH\\TO\\DOWNLOADS\\> asio tinyxml2\nPlease replace <PATH\\TO\\DOWNLOADS> with the folder you downloaded the packages to.\nColcon installation\ncolcon is a command line tool to build sets of software packages. This section explains to use it to compile easily Fast-RTPS and its dependencies. First install ROS2 development tools (colcon and vcstool):\npip install -U colcon-common-extensions vcstool\nDownload the repos file that will be used to download Fast RTPS and its dependencies:\n$ mkdir fastdds_ws\n$ cd fastdds_ws\n$ wget https://raw.githubusercontent.com/eProsima/Fast-DDS/master/fastrtps.repos\n$ mkdir src\n$ vcs import src < fastrtps.repos\nFinally, use colcon to compile all software:\n$ colcon build\nManual installation\nBefore compiling manually Fast DDS you need to clone the following dependencies and compile them using CMake.\nFast CDR\n$ git clone https://github.com/eProsima/Fast-CDR.git\n$ mkdir Fast-CDR/build && cd Fast-CDR/build\n$ cmake ..\n$ cmake --build . --target install\nFoonathan memory\n$ git clone https://github.com/eProsima/foonathan_memory_vendor.git\n$ cd foonathan_memory_vendor\n$ mkdir build && cd build\n$ cmake ..\n$ cmake --build . --target install\nOnce all dependencies are installed, you will be able to compile and install Fast DDS.\n$ git clone https://github.com/eProsima/Fast-DDS.git\n$ mkdir Fast-DDS/build && cd Fast-DDS/build\n$ cmake ..\n$ cmake --build . --target install\nDocumentation\nYou can access the documentation online, which is hosted on Read the Docs.\nStart Page\nInstallation manual\nUser manual\nFast DDS-Gen manual\nRelease notes\nQuality Declaration\neprosima Fast DDS claims to be in the Quality Level 1 category based on the guidelines provided by ROS 2. See the Quality Declaration for more details.\nQuick Demo\neProsima provides the eProsima Fast DDS Suite Docker image for those who want a quick demonstration of Fast-DDS running on an Ubuntu platform. It can be downloaded from eProsima's downloads page.\nThis Docker image was built for Ubuntu 20.04 (Focal Fossa).\nTo run this container you need Docker installed. From a terminal run the following command\n$ sudo apt-get install docker.io\nLoad the docker image:\n$ docker load -i ubuntu-fastdds-suite:<FastDDS-Version>.tar\n$ docker tag ubuntu-fastdds-suite:<FastDDS-Version> ubuntu-fastdds-suite:latest\nRun the eProsima Fast DDS Suite Docker container:\n$ xhost local:root\n$ docker run -it --privileged -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix \\\nubuntu-fastdds-suite:<FastDDS-Version>\nThis Docker Image comes bundled with the following:\nShapes Demo\neProsima Shapes Demo is an application in which Publishers and Subscribers are shapes of different colors and sizes moving on a board. Each shape refers to its own topic: Square, Triangle or Circle. A single instance of the eProsima Shapes Demo can publish on or subscribe to several topics at a time.\nYou can read more about this application on the Shapes Demo documentation page.\nTo run this application once inside the Docker container run:\n$ ShapesDemo\neProsima Shapes Demo usage information can be found on the Shapes Demo First Steps page.\nFast DDS Monitor\neProsima Fast DDS Monitor is a graphical desktop application aimed at monitoring DDS environments deployed using the eProsima Fast DDS library. Thus, the user can monitor in real time the status of publication/subscription communications between DDS entities. They can also choose from a wide variety of communication parameters to be measured (latency, throughput,packet loss, etc.), as well as record and compute in real time statistical measurements on these parameters (mean, variance, standard deviation, etc.).\nYou can read more about this application on the Fast DDS Monitor documentation page.\nTo run this application once inside the Docker container run:\n$ fastdds_monitor\neProsima Fast DDS Monitor usage information can be found on the Fast DDS Monitor User Manual.\nFast DDS libraries and Examples\nIncluded in this Docker container is a set of binary examples that showcase several functionalities of the Fast DDS libraries. These examples' path can be accessed from a terminal by typing\n$ goToExamples\nFrom this folder you can access all examples, both for DDS and RTPS. We detail the steps to launch two such examples below.\nTo launch the Hello World example (a minimal example that will perform a Publisher/Subscriber match and start sending samples) you could run:\n$ goToExamples\n$ cd HelloWorldExample/bin\n$ tmux new-session \"./HelloWorldExample publisher 0 1000\" \\; \\\nsplit-window \"./HelloWorldExample subscriber\" \\; \\\nselect-layout even-vertical\nThis example is not constrained to the current instance. It's possible to run several instances of this container to check the communication between them by running the following from each container.\n$ goToExamples\n$ cd HelloWorldExample/bin\n$ ./HelloWorldExample publisher\nor\n$ goToExamples\n$ cd HelloWorldExample/bin\n$ ./HelloWorldExample subscriber\nAnother example you could launch is the Benchmark example. This example creates either a Publisher or a Subscriber and on a successful match starts sending samples. After a few seconds the process that launched the Publisher will show a report with the number of samples transmitted.\nOn the subscriber side, run:\n$ goToExamples\n$ cd Benchmark/bin\n$ ./Benchmark subscriber udp\nOn the publisher side, run:\n$ goToExamples\n$ cd Benchmark/bin\n$ ./Benchmark publisher udp\nGetting Help\nIf you need support you can reach us by mail at support@eProsima.com or by phone at +34 91 804 34 48.\nSupported by ROSIN - ROS-Industrial Quality-Assured Robot Software Components. More information: rosin-project.eu\nThis project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement no. 732287.", "link": "https://github.com/eProsima/Fast-DDS", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "eprosima fast dds\neprosima fast dds (formerly fast rtps) is a c++ implementation of the dds (data distribution service) standard of the omg (object management group). eprosima fast dds implements the rtps (real time publish subscribe) protocol, which provides publisher-subscriber communications over unreliable transports such as udp, as defined and maintained by the object management group (omg) consortium. rtps is also the wire interoperability protocol defined for the data distribution service (dds) standard. eprosima fast dds expose an api to access directly the rtps protocol, giving the user full access to the protocol internals.\nsome of the main features of this library are:\nconfigurable best-effort and reliable publish-subscribe communication policies for real-time applications.\nplug and play connectivity so that any new applications are automatically discovered by any other members of the network.\nmodularity and scalability to allow continuous growth with complex and simple devices in the network.\nconfigurable network behavior and interchangeable transport layer: choose the best protocol and system input/output channel combination for each deployment.\ntwo api layers: a high-level publisher-subscriber one focused on usability (dds) and a lower-level writer-reader one that provides finer access to the inner workings of the rtps protocol.\neprosima fast dds has been adopted by multiple organizations in many sectors including these important cases:\nrobotics: ros (robotic operating system) as their default middleware for ros2 until and including the latest long term release foxy fitzroy.\neu r&d: fiware incubated ge.\nthis project is part of fiware. for more information check the fiware catalogue entry for robotics.\nwant us to share your project with the community?\nwrite to evaluation.support@eprosima.com or mention @eprosima on twitter. we are curious to get to know your use case!\nsupported platforms\nlinux\nlinux-aarch64\nwindows\nmac\ninstallation guide\nyou can get either a binary distribution of eprosima fast dds or compile the library yourself from source.\ninstallation from binaries\nthe latest, up to date binary release of eprosima fast dds can be obtained from the company website.\ninstallation from source\ndependencies\nasio and tinyxml2 libraries\non linux, you can install these libraries using the package manager of your linux distribution. for example, on ubuntu you can install them by using its package manager with the next command.\nsudo apt install libasio-dev libtinyxml2-dev\non windows, you can install these libraries using chocolatey. first, download the following chocolatey packages from this ros2 github repository.\nasio.1.12.1.nupkg\ntinyxml2.6.0.0.nupkg\nonce these packages are downloaded, open an administrative shell and execute the following command:\nchoco install -y -s <path\\to\\downloads\\> asio tinyxml2\nplease replace <path\\to\\downloads> with the folder you downloaded the packages to.\ncolcon installation\ncolcon is a command line -----> tool !!!  to build sets of software packages. this section explains to use it to compile easily fast-rtps and its dependencies. first install ros2 development tools (colcon and vcstool):\npip install -u colcon-common-extensions vcstool\ndownload the repos file that will be used to download fast rtps and its dependencies:\n$ mkdir fastdds_ws\n$ cd fastdds_ws\n$ wget https://raw.githubusercontent.com/eprosima/fast-dds/master/fastrtps.repos\n$ mkdir src\n$ vcs import src < fastrtps.repos\nfinally, use colcon to compile all software:\n$ colcon build\nmanual installation\nbefore compiling manually fast dds you need to clone the following dependencies and compile them using cmake.\nfast cdr\n$ git clone https://github.com/eprosima/fast-cdr.git\n$ mkdir fast-cdr/build && cd fast-cdr/build\n$ cmake ..\n$ cmake --build . --target install\nfoonathan memory\n$ git clone https://github.com/eprosima/foonathan_memory_vendor.git\n$ cd foonathan_memory_vendor\n$ mkdir build && cd build\n$ cmake ..\n$ cmake --build . --target install\nonce all dependencies are installed, you will be able to compile and install fast dds.\n$ git clone https://github.com/eprosima/fast-dds.git\n$ mkdir fast-dds/build && cd fast-dds/build\n$ cmake ..\n$ cmake --build . --target install\ndocumentation\nyou can access the documentation online, which is hosted on read the docs.\nstart page\ninstallation manual\nuser manual\nfast dds-gen manual\nrelease notes\nquality declaration\neprosima fast dds claims to be in the quality level 1 category based on the guidelines provided by ros 2. see the quality declaration for more details.\nquick demo\neprosima provides the eprosima fast dds suite docker image for those who want a quick demonstration of fast-dds running on an ubuntu platform. it can be downloaded from eprosima's downloads page.\nthis docker image was built for ubuntu 20.04 (focal fossa).\nto run this container you need docker installed. from a terminal run the following command\n$ sudo apt-get install docker.io\nload the docker image:\n$ docker load -i ubuntu-fastdds-suite:<fastdds-version>.tar\n$ docker tag ubuntu-fastdds-suite:<fastdds-version> ubuntu-fastdds-suite:latest\nrun the eprosima fast dds suite docker container:\n$ xhost local:root\n$ docker run -it --privileged -e display=$display -v /tmp/.x11-unix:/tmp/.x11-unix \\\nubuntu-fastdds-suite:<fastdds-version>\nthis docker image comes bundled with the following:\nshapes demo\neprosima shapes demo is an application in which publishers and subscribers are shapes of different colors and sizes moving on a board. each shape refers to its own topic: square, triangle or circle. a single instance of the eprosima shapes demo can publish on or subscribe to several topics at a time.\nyou can read more about this application on the shapes demo documentation page.\nto run this application once inside the docker container run:\n$ shapesdemo\neprosima shapes demo usage information can be found on the shapes demo first steps page.\nfast dds monitor\neprosima fast dds monitor is a graphical desktop application aimed at monitoring dds environments deployed using the eprosima fast dds library. thus, the user can monitor in real time the status of publication/subscription communications between dds entities. they can also choose from a wide variety of communication parameters to be measured (latency, throughput,packet loss, etc.), as well as record and compute in real time statistical measurements on these parameters (mean, variance, standard deviation, etc.).\nyou can read more about this application on the fast dds monitor documentation page.\nto run this application once inside the docker container run:\n$ fastdds_monitor\neprosima fast dds monitor usage information can be found on the fast dds monitor user manual.\nfast dds libraries and examples\nincluded in this docker container is a set of binary examples that showcase several functionalities of the fast dds libraries. these examples' path can be accessed from a terminal by typing\n$ gotoexamples\nfrom this folder you can access all examples, both for dds and rtps. we detail the steps to launch two such examples below.\nto launch the hello world example (a minimal example that will perform a publisher/subscriber match and start sending samples) you could run:\n$ gotoexamples\n$ cd helloworldexample/bin\n$ tmux new-session \"./helloworldexample publisher 0 1000\" \\; \\\nsplit-window \"./helloworldexample subscriber\" \\; \\\nselect-layout even-vertical\nthis example is not constrained to the current instance. it's possible to run several instances of this container to check the communication between them by running the following from each container.\n$ gotoexamples\n$ cd helloworldexample/bin\n$ ./helloworldexample publisher\nor\n$ gotoexamples\n$ cd helloworldexample/bin\n$ ./helloworldexample subscriber\nanother example you could launch is the benchmark example. this example creates either a publisher or a subscriber and on a successful match starts sending samples. after a few seconds the process that launched the publisher will show a report with the number of samples transmitted.\non the subscriber side, run:\n$ gotoexamples\n$ cd benchmark/bin\n$ ./benchmark subscriber udp\non the publisher side, run:\n$ gotoexamples\n$ cd benchmark/bin\n$ ./benchmark publisher udp\ngetting help\nif you need support you can reach us by mail at support@eprosima.com or by phone at +34 91 804 34 48.\nsupported by rosin - ros-industrial quality-assured robot software components. more information: rosin-project.eu\nthis project has received funding from the european union\u2019s horizon 2020 research and innovation programme under grant agreement no. 732287.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000050, "year": null}, {"Unnamed: 0": 1077, "autor": 57, "date": null, "content": "pykitti\nThis package provides a minimal set of tools for working with the KITTI dataset [1] in Python. So far only the raw datasets and odometry benchmark datasets are supported, but we're working on adding support for the others. We welcome contributions from the community.\nInstallation\nUsing pip\nYou can install pykitti via pip using\npip install pykitti\nFrom source\nTo install the package from source, simply clone or download the repository to your machine\ngit clone https://github.com/utiasSTARS/pykitti.git\nand run the provided setup tool\ncd pykitti\npython setup.py install\nAssumptions\nThis package assumes that you have also downloaded the calibration data associated with the sequences you want to work on (these are separate files from the sequences themselves), and that the directory structure is unchanged from the original structure laid out in the KITTI zip files.\nNotation\nHomogeneous coordinate transformations are provided as 4x4 numpy.array objects and are denoted as T_destinationFrame_originFrame.\nPinhole camera intrinsics for camera N are provided as 3x3 numpy.array objects and are denoted as K_camN. Stereo pair baselines are given in meters as b_gray for the monochrome stereo pair (cam0 and cam1), and b_rgb for the color stereo pair (cam2 and cam3).\nExample\nMore detailed examples can be found in the demos directory, but the general idea is to specify what dataset you want to load, then access the parts you need and do something with them.\nCamera and velodyne data are available via generators for easy sequential access (e.g., for visual odometry), and by indexed getter methods for random access (e.g., for deep learning). Images are loaded as PIL.Image objects using Pillow.\nimport pykitti\nbasedir = '/your/dataset/dir'\ndate = '2011_09_26'\ndrive = '0019'\n# The 'frames' argument is optional - default: None, which loads the whole dataset.\n# Calibration, timestamps, and IMU data are read automatically.\n# Camera and velodyne data are available via properties that create generators\n# when accessed, or through getter methods that provide random access.\ndata = pykitti.raw(basedir, date, drive, frames=range(0, 50, 5))\n# dataset.calib: Calibration data are accessible as a named tuple\n# dataset.timestamps: Timestamps are parsed into a list of datetime objects\n# dataset.oxts: List of OXTS packets and 6-dof poses as named tuples\n# dataset.camN: Returns a generator that loads individual images from camera N\n# dataset.get_camN(idx): Returns the image from camera N at idx\n# dataset.gray: Returns a generator that loads monochrome stereo pairs (cam0, cam1)\n# dataset.get_gray(idx): Returns the monochrome stereo pair at idx\n# dataset.rgb: Returns a generator that loads RGB stereo pairs (cam2, cam3)\n# dataset.get_rgb(idx): Returns the RGB stereo pair at idx\n# dataset.velo: Returns a generator that loads velodyne scans as [x,y,z,reflectance]\n# dataset.get_velo(idx): Returns the velodyne scan at idx\npoint_velo = np.array([0,0,0,1])\npoint_cam0 = data.calib.T_cam0_velo.dot(point_velo)\npoint_imu = np.array([0,0,0,1])\npoint_w = [o.T_w_imu.dot(point_imu) for o in data.oxts]\nfor cam0_image in data.cam0:\n# do something\npass\ncam2_image, cam3_image = data.get_rgb(3)\nOpenCV\nPIL Image data can be converted to an OpenCV-friendly format using numpy and cv2.cvtColor:\nimg_np = np.array(img)\nimg_cv2 = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\nNote: This package does not actually require that OpenCV be installed on your system, except to run demo_raw_cv2.py.\nReferences\n[1] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, \"Vision meets robotics: The KITTI dataset,\" Int. J. Robot. Research (IJRR), vol. 32, no. 11, pp. 1231\u20131237, Sep. 2013. http://www.cvlibs.net/datasets/kitti/ `", "link": "https://github.com/utiasSTARS/pykitti", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "pykitti\nthis package provides a minimal set of tools for working with the kitti dataset [1] in python. so far only the raw datasets and odometry benchmark datasets are supported, but we're working on adding support for the others. we welcome contributions from the community.\ninstallation\nusing pip\nyou can install pykitti via pip using\npip install pykitti\nfrom source\nto install the package from source, simply clone or download the repository to your machine\ngit clone https://github.com/utiasstars/pykitti.git\nand run the provided setup -----> tool !!! \ncd pykitti\npython setup.py install\nassumptions\nthis package assumes that you have also downloaded the calibration data associated with the sequences you want to work on (these are separate files from the sequences themselves), and that the directory structure is unchanged from the original structure laid out in the kitti zip files.\nnotation\nhomogeneous coordinate transformations are provided as 4x4 numpy.array objects and are denoted as t_destinationframe_originframe.\npinhole camera intrinsics for camera n are provided as 3x3 numpy.array objects and are denoted as k_camn. stereo pair baselines are given in meters as b_gray for the monochrome stereo pair (cam0 and cam1), and b_rgb for the color stereo pair (cam2 and cam3).\nexample\nmore detailed examples can be found in the demos directory, but the general idea is to specify what dataset you want to load, then access the parts you need and do something with them.\ncamera and velodyne data are available via generators for easy sequential access (e.g., for visual odometry), and by indexed getter methods for random access (e.g., for deep learning). images are loaded as pil.image objects using pillow.\nimport pykitti\nbasedir = '/your/dataset/dir'\ndate = '2011_09_26'\ndrive = '0019'\n# the 'frames' argument is optional - default: none, which loads the whole dataset.\n# calibration, timestamps, and imu data are read automatically.\n# camera and velodyne data are available via properties that create generators\n# when accessed, or through getter methods that provide random access.\ndata = pykitti.raw(basedir, date, drive, frames=range(0, 50, 5))\n# dataset.calib: calibration data are accessible as a named tuple\n# dataset.timestamps: timestamps are parsed into a list of datetime objects\n# dataset.oxts: list of oxts packets and 6-dof poses as named tuples\n# dataset.camn: returns a generator that loads individual images from camera n\n# dataset.get_camn(idx): returns the image from camera n at idx\n# dataset.gray: returns a generator that loads monochrome stereo pairs (cam0, cam1)\n# dataset.get_gray(idx): returns the monochrome stereo pair at idx\n# dataset.rgb: returns a generator that loads rgb stereo pairs (cam2, cam3)\n# dataset.get_rgb(idx): returns the rgb stereo pair at idx\n# dataset.velo: returns a generator that loads velodyne scans as [x,y,z,reflectance]\n# dataset.get_velo(idx): returns the velodyne scan at idx\npoint_velo = np.array([0,0,0,1])\npoint_cam0 = data.calib.t_cam0_velo.dot(point_velo)\npoint_imu = np.array([0,0,0,1])\npoint_w = [o.t_w_imu.dot(point_imu) for o in data.oxts]\nfor cam0_image in data.cam0:\n# do something\npass\ncam2_image, cam3_image = data.get_rgb(3)\nopencv\npil image data can be converted to an opencv-friendly format using numpy and cv2.cvtcolor:\nimg_np = np.array(img)\nimg_cv2 = cv2.cvtcolor(img_np, cv2.color_rgb2bgr)\nnote: this package does not actually require that opencv be installed on your system, except to run demo_raw_cv2.py.\nreferences\n[1] a. geiger, p. lenz, c. stiller, and r. urtasun, \"vision meets robotics: the kitti dataset,\" int. j. robot. research (ijrr), vol. 32, no. 11, pp. 1231\u20131237, sep. 2013. http://www.cvlibs.net/datasets/kitti/ `", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000057, "year": null}, {"Unnamed: 0": 1082, "autor": 62, "date": null, "content": "Control Toolbox\n- Important note (July 2021): this library is currently only scarcely maintained,\n- it may take a while until we respond to bugs or feature requests.\nThis is the Control Toolbox, an efficient C++ library for control, estimation, optimization and motion planning in robotics.\nLink to the wiki, quickstart!\nFind the detailed documentation here.\nOverview\nThis is the ADRL Control Toolbox ('CT'), an open-source C++ library for efficient modelling, control, estimation, trajectory optimization and model predictive control. The CT is applicable to a broad class of dynamic systems, but features additional modelling tools specially designed for robotics. This page outlines its general concept, its major building blocks and highlights selected application examples.\nThe library contains several tools to design and evaluate controllers, model dynamical systems and solve optimal control problems. The CT was designed with the following features in mind:\nSystems and dynamics:\nintuitive modelling of systems governed by ordinary differential or difference equations.\nTrajectory optimization, optimal control and (nonlinear) model predictive control:\nintuitive modelling of cost functions and constraints\ncommon interfaces for optimal control solvers and nonlinear model predictive control\ncurrently supported algorithms:\nClassical Single Shooting\niLQR / iLQG (iterative Linear Quadratic Optimal Control)\nMultiple-shooting iLQR\nGauss-Newton-Multiple-Shooting (GNMS)\nClassical Direct Multiple Shooting (DMS)\nstandardized interfaces for the solvers\nIPOPT (first and second order)\nSNOPT\nHPIPM\ncustom Riccati-solver\nPerformance:\nsolve large scale optimal control problems in MPC fashion.\nRobot Modelling, Rigid Body Kinematics and Dynamics:\nstraight-forward interface to the state-of the art rigid body dynamics modelling tool RobCoGen.\nimplementation of a basic nonlinear-programming inverse kinematics solver for fix-base robots.\nAutomatic Differentiation:\nfirst and second order automatic differentiation of arbitrary vector-valued functions including cost functions and constraints\nautomatic differentiation and code generation of rigid body dynamics\nderivative code generation for maximum efficiency\nRobot Application Examples\nThe Control Toolbox has been used for Hardware and Simulation control tasks on flying, walking and ground robots.\nSlightly more complex optimization examples, including gait optimization for a quadruped, are availabe in ct_ros.\nWhat is the CT?\nA common tasks for researchers and practitioners in both the control and the robotics communities is to model systems, implement equations of motion and design model-based controllers, estimators, planning algorithms, etc. Sooner or later, one is confronted with questions of efficient implementation, computing derivative information, formulating cost functions and constraints or running controllers in model-predictive control fashion.\nThe Control Toolbox is specifically designed for these tasks. It is written entirely in C++ and has a strong focus on highly efficient code that can be run online (in the loop) on robots or other actuated hardware. A major contribution of the CT is its implementations of optimal control algorithms, spanning a range from simple LQR reference implementations to constrained model predictive control. The CT supports Automatic Differentiation (Auto-Diff) and allows to generate derivative code for arbitrary scalar and vector-valued functions. We designed the toolbox with usability in mind, allowing users to apply advanced concepts such as nonlinear model predictive control (NMPC) or numerical optimal control easily and with minimal effort. While we provide an interface to a state-of-the art Auto-Diff compatible robot modelling software, all other modules are independent of the a particular modelling framework, allowing the code to be interfaced with existing C/C++ code or libraries.\nThe CT has been successfully used in a variety of different projects, including a large number of hardware experiments, demonstrations and academic publications. Example hardware applications are online trajectory optimization with collision avoidance \\cite giftthaler2017autodiff, trajectory optimization for Quadrupeds \\cite neunert:2017:ral and mobile manipulators \\cite giftthaler2017efficient as well as NMPC on ground robots \\cite neunert2017mpc and UAVs \\cite neunert16hexrotor. The project originated from research conducted at the Agile & Dexterous Robotics Lab at ETH Zurich, but is continuously extended to cover more fields of applications and algorithms.\nScope of the CT\nSoftware is one of the key building blocks for robotic systems and there is a great effort in creating software tools and libraries for robotics. However, when it comes to control and especially Numerical Optimal Control, there are not many open source tools available that are both easy to use for fast development as well as efficient enough for online usage. While there exist mature toolboxes for Numerical Optimal Control and Trajectory Optimization, they are highly specialized, standalone tools that due not provide sufficient flexibility for other applications. Here is where the CT steps in. The CT has been designed from ground up to provide the tools needed for fast development and evaluation of control methods while being optimized for efficiency allowing for online operation. While the emphasis lies on control, the tools provided can also be used for simulation, estimation or optimization applications.\nIn contrast to other robotic software, CT is not a rigid integrated application but can be seen quite literal as a toolbox: It offers a variety of tools which can be used and combined to solve a task at hand. While ease-of-use has been a major criteria during the design and application examples are provided, using CT still requires programming and control knowledge. However, it frees the users from implementing standard methods that require in-depth experience with linear algebra or numerical methods. Furthermore, by using common definitions and types, a seamless integration between different components such as systems, controllers or integrators is provided, enabling fast prototyping.\nDesign and Implementation\nThe main focus of CT is efficiency, which is why it is fully implemented in C++. Since CT is designed as a toolbox rather than an integrated application, we tried to provide maximum flexibility to the users. Therefore, it is not tied to a specific middleware such as ROS and dependencies are kept at a minimum. The two essential dependencies for CT are Eigen and Kindr (which is based on Eigen). This Eigen dependency is intentional since Eigen is a defacto standard for linear algebra in C++, as it provides highly efficient implementations of standard matrix operations as well as more advanced linear algebra methods. Kindr is a header only Kinematics library which builds on top of it and provides data types for different rotation representations such as Quaternions, Euler angles or rotation matrices.\nStructure and Modules of the CT\nThe Control Toolbox consists of three main modules. The Core (ct_core) module, the Optimal Control (ct_optcon) module and the Rigid Body Dynamics (ct_rbd) module. There is a clear hierarchy between the modules. That means, the modules depend on each other in this order, e.g. you can use the core module without the optcon or rbd module.\nThe Core (ct_core) module provides general type definitions and mathematical tools. For example, it contains most data type definitions, definitions for systems and controllers, as well as basic functionality such as numerical integrators for differential equations.\nThe Optimal Control (ct_optcon) module builds on top of the Core module and adds infrastructure for defining and solving Optimal Control Problems. It contains the functionality for defining cost functions, constraints, solver backends and a general MPC wrapper.\nThe Rigid Body Dynamics (ct_rbd) module provides tools for modelling Rigid Body Dynamics systems and interfaces with ct_core and ct_optcon data types.\nFor testing as well as examples, we also provide the models module (ct_models) which contains various robot models including a quadruped, a robotic arm, a normal quadrotor and a quadrotor with slung load.\nThe four different main modules are detailed in the following.\nct_core (Core)\nDefinitions of fundamental types for control and simulation, such as dynamic systems (ct::core::System), states (ct::core::StateVector), controls (ct::core::Controller), or trajectories (ct::core::DiscreteTrajectoryBase).\nNumeric integration (ct::core::Integrator) with various ODE solvers including fixed step (ct::core::IntegratorEuler, ct::core::IntegratorRK4) and variable step (ct::core::IntegratorRK5Variable, ct::core::ODE45) integrators, as well as symplectic (semi-implicit) integrators.\nNumerical approximation of Trajectory Sensitivities (ct::core::Sensitivity , e.g. by forward-integrating variational differential equations)\nCommon feedback controllers (e.g. ct::core::PIDController)\nDerivatives/Jacobians of general functions using Numerical Differentiation (ct::core::DerivativesNumDiff) or Automatic-Differentiation with code-generation (ct::core::DerivativesCppadCG) and just-in-time (JIT) compilation (ct::core::DerivativesCppadJIT)\nct_optcon (Optimal Control)\nDefinitions for Optimal Control Problems (ct::optcon::OptConProblem) and Optimal Control Solvers (ct::optcon::OptConSolver)\nCostFunction toolbox allowing to construct cost functions from file and providing first-order and second-order approximations, see ct::optcon::CostFunction.\nConstraint toolbox for formulating constraints of Optimal Control Problems, as well as automatically computing their Jacobians.\nreference C++ implementations of the Linear Quadratic Regulator, infinite-horizon LQR and time-varying LQR (ct::optcon::LQR, ct::optcon::TVLQR)\nRiccati-solver (ct::optcon::GNRiccatiSolver) for unconstrained linear-quadratic optimal control problems, interface to high-performance third-party Riccati-solvers for constrained linear-quadratic optimal control problems\niterative non-linear Optimal Control solvers, i.e. Gauss-Newton solvers such as iLQR (ct::optcon::iLQR) and Gauss-Newton Multiple Shooting(ct::optcon::GNMS), constrained direct multiple-shooting (ct::optcon::DmsSolver)\nNon-Linear Model Predictive Control (ct::optcon::MPC)\nDefinitions for Nonlinear Programming Problems (NLPs, ct::optcon::Nlp) and interfaces to third-party NLP Solvers (ct::optcon::SnoptSolver and ct::optcon::IpoptSolver)\nct_rbd (Rigid Body Dynamics)\nStandard models for Rigid Body Dynamics\nDefinitions for the state of a Rigid Body System expressed as general coordinates (ct::rbd::RBDState)\nRoutines for different flavors of Forward and Inverse Dynamics (ct::rbd::Dynamics)\nRigid body and end-effector kinematics (ct::rbd::Kinematics)\nOperational Space Controllers\nBasic soft auto-differentiable contact model for arbitrary frames (ct::rbd::EEContactModel)\nActuator dynamics (ct::rbd::ActuatorDynamics)\nBackend uses RobCoGen, a highly efficient Rigid Body Dynamics library\nct_models\nVarious standard models for testing and evaluation including UAVs (ct::models::Quadrotor), ground robots, legged robots (ct::models::HyQ), robot arms (ct::models::HyA), inverted pendulums etc.\nMeans of creating linear approximation of these models\nHow to get started\nTo get started with the control toolbox, please build the repository documentation with doxygen and follow the \"Getting Started\" tutorial.\nSupport\ncontact the devs: control-toolbox-dev@googlegroups.com\nAcknowledgements\nContributors\nMarkus Giftthaler, markusgft (at) gmail (dot) com\nMichael Neunert\nMarkus St\u00e4uble\nFarbod Farshidian\nDiego Pardo\nTimothy Sandy\nJan Carius\nRuben Grandia\nHamza Merzic\nFunding\nThis software has been developed at the Agile & Dexterous Robotics Lab at ETH Zurich, Switzerland between 2014 and 2018. During that time, development has been made possible through financial support from the Swiss National Science Foundation (SNF) through a SNF Professorship award to Jonas Buchli and the National Competence Centers in Research (NCCR) Robotics and Digital Fabrication.\nLicence Information\nThe Control Toolbox is released under the BSD-2 clause license. Please see LICENCE.txt and NOTICE.txt\nHow to cite the CT\n@article{adrlCT,\ntitle={The control toolbox \u2014 An open-source C++ library for robotics, optimal and model predictive control},\nauthor={Markus Giftthaler and Michael Neunert and Markus St{\\\"a}uble and Jonas Buchli},\njournal={2018 IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR)},\nyear={2018},\npages={123-129}\n}\nEarlier Versions\nEarlier versions up to v2.3 are hosted on bitbucket, they can be found at https://bitbucket.org/adrlab/ct/wiki/Home\nRelated Publications\nThis toolbox has been used in, or has helped to realize the following academic publications:\nMarkus Giftthaler, Michael Neunert, Markus St\u00e4uble and Jonas Buchli. \u201cThe Control Toolbox - An Open-Source C++ Library for Robotics, Optimal and Model Predictive Control\u201d. IEEE Simpar 2018 (Best Student Paper Award). arXiv preprint\nMarkus Giftthaler, Michael Neunert, Markus St\u00e4uble, Jonas Buchli and Moritz Diehl. \u201cA Family of iterative Gauss-Newton Shooting Methods for Nonlinear Optimal Control\u201d. IROS 2018. arXiv preprint\nJan Carius, Ren\u00e9 Ranftl, Vladlen Koltun and Marco Hutter. \"Trajectory Optimization with Implicit Hard Contacts.\" IEEE Robotics and Automation Letters 3, no. 4 (2018): 3316-3323.\nMichael Neunert, Markus St\u00e4uble, Markus Giftthaler, Dario Bellicoso, Jan Carius, Christian Gehring, Marco Hutter and Jonas Buchli. \u201cWhole Body Model Predictive Control Through Contacts For Quadrupeds\u201d. IEEE Robotics and Automation Letters, 2017. arXiv preprint\nMarkus Giftthaler and Jonas Buchli. \u201cA Projection Approach to Equality Constrained Iterative Linear Quadratic Optimal Control\u201d. 2017 IEEE-RAS International Conference on Humanoid Robots, November 15-17, Birmingham, UK. IEEE Xplore\nMarkus Giftthaler, Michael Neunert, Markus St\u00e4uble, Marco Frigerio, Claudio Semini and Jonas Buchli. \u201cAutomatic Differentiation of Rigid Body Dynamics for Optimal Control and Estimation\u201d, Advanced Robotics, SIMPAR special issue. November 2017. arXiv preprint\nMichael Neunert, Markus Giftthaler, Marco Frigerio, Claudio Semini and Jonas Buchli. \u201cFast Derivatives of Rigid Body Dynamics for Control, Optimization and Estimation\u201d, 2016 IEEE International Conference on Simulation, Modelling, and Programming for Autonomous Robots, San Francisco. (Best Paper Award). IEEE Xplore\nMichael Neunert, Farbod Farshidian, Alexander W. Winkler, Jonas Buchli \"Trajectory Optimization Through Contacts and Automatic Gait Discovery for Quadrupeds\", IEEE Robotics and Automation Letters, IEEE Xplore\nMichael Neunert, C\u00e9dric de Crousaz, Fadri Furrer, Mina Kamel, Farbod Farshidian, Roland Siegwart, Jonas Buchli. \"Fast nonlinear model predictive control for unified trajectory optimization and tracking\", 2016 IEEE International Conference on Robotics and Automation (ICRA), IEEE Xplore\nMarkus Giftthaler, Farbod Farshidian, Timothy Sandy, Lukas Stadelmann and Jonas Buchli. \u201cEfficient Kinematic Planning for Mobile Manipulators with Non-holonomic Constraints Using Optimal Control\u201d, IEEE International Conference on Robotics and Automation, 2017, Singapore. arXiv preprint\nMarkus Giftthaler, Timothy Sandy, Kathrin D\u00f6rfler, Ian Brooks, Mark Buckingham, Gonzalo Rey, Matthias Kohler, Fabio Gramazio and Jonas Buchli. \u201cMobile Robotic Fabrication at 1:1 scale: the In situ Fabricator\u201d. Construction Robotics, Springer Journal no. 41693 arXiv preprint\nTimothy Sandy, Markus Giftthaler, Kathrin D\u00f6rfler, Matthias Kohler and Jonas Buchli. \u201cAutonomous Repositioning and Localization of an In situ Fabricator\u201d, IEEE International Conference on Robotics and Automation 2016, Stockholm, Sweden. IEEE Xplore\nMichael Neunert, Farbod Farshidian, Jonas Buchli (2014). Adaptive Real-time Nonlinear Model Predictive Motion Control. In IROS 2014 Workshop on Machine Learning in Planning and Control of Robot Motion preprint", "link": "https://github.com/ethz-adrl/control-toolbox", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "control toolbox\n- important note (july 2021): this library is currently only scarcely maintained,\n- it may take a while until we respond to bugs or feature requests.\nthis is the control toolbox, an efficient c++ library for control, estimation, optimization and motion planning in robotics.\nlink to the wiki, quickstart!\nfind the detailed documentation here.\noverview\nthis is the adrl control toolbox ('ct'), an open-source c++ library for efficient modelling, control, estimation, trajectory optimization and model predictive control. the ct is applicable to a broad class of dynamic systems, but features additional modelling tools specially designed for robotics. this page outlines its general concept, its major building blocks and highlights selected application examples.\nthe library contains several tools to design and evaluate controllers, model dynamical systems and solve optimal control problems. the ct was designed with the following features in mind:\nsystems and dynamics:\nintuitive modelling of systems governed by ordinary differential or difference equations.\ntrajectory optimization, optimal control and (nonlinear) model predictive control:\nintuitive modelling of cost functions and constraints\ncommon interfaces for optimal control solvers and nonlinear model predictive control\ncurrently supported algorithms:\nclassical single shooting\nilqr / ilqg (iterative linear quadratic optimal control)\nmultiple-shooting ilqr\ngauss-newton-multiple-shooting (gnms)\nclassical direct multiple shooting (dms)\nstandardized interfaces for the solvers\nipopt (first and second order)\nsnopt\nhpipm\ncustom riccati-solver\nperformance:\nsolve large scale optimal control problems in mpc fashion.\nrobot modelling, rigid body kinematics and dynamics:\nstraight-forward interface to the state-of the art rigid body dynamics modelling -----> tool !!!  robcogen.\nimplementation of a basic nonlinear-programming inverse kinematics solver for fix-base robots.\nautomatic differentiation:\nfirst and second order automatic differentiation of arbitrary vector-valued functions including cost functions and constraints\nautomatic differentiation and code generation of rigid body dynamics\nderivative code generation for maximum efficiency\nrobot application examples\nthe control toolbox has been used for hardware and simulation control tasks on flying, walking and ground robots.\nslightly more complex optimization examples, including gait optimization for a quadruped, are availabe in ct_ros.\nwhat is the ct?\na common tasks for researchers and practitioners in both the control and the robotics communities is to model systems, implement equations of motion and design model-based controllers, estimators, planning algorithms, etc. sooner or later, one is confronted with questions of efficient implementation, computing derivative information, formulating cost functions and constraints or running controllers in model-predictive control fashion.\nthe control toolbox is specifically designed for these tasks. it is written entirely in c++ and has a strong focus on highly efficient code that can be run online (in the loop) on robots or other actuated hardware. a major contribution of the ct is its implementations of optimal control algorithms, spanning a range from simple lqr reference implementations to constrained model predictive control. the ct supports automatic differentiation (auto-diff) and allows to generate derivative code for arbitrary scalar and vector-valued functions. we designed the toolbox with usability in mind, allowing users to apply advanced concepts such as nonlinear model predictive control (nmpc) or numerical optimal control easily and with minimal effort. while we provide an interface to a state-of-the art auto-diff compatible robot modelling software, all other modules are independent of the a particular modelling framework, allowing the code to be interfaced with existing c/c++ code or libraries.\nthe ct has been successfully used in a variety of different projects, including a large number of hardware experiments, demonstrations and academic publications. example hardware applications are online trajectory optimization with collision avoidance \\cite giftthaler2017autodiff, trajectory optimization for quadrupeds \\cite neunert:2017:ral and mobile manipulators \\cite giftthaler2017efficient as well as nmpc on ground robots \\cite neunert2017mpc and uavs \\cite neunert16hexrotor. the project originated from research conducted at the agile & dexterous robotics lab at eth zurich, but is continuously extended to cover more fields of applications and algorithms.\nscope of the ct\nsoftware is one of the key building blocks for robotic systems and there is a great effort in creating software tools and libraries for robotics. however, when it comes to control and especially numerical optimal control, there are not many open source tools available that are both easy to use for fast development as well as efficient enough for online usage. while there exist mature toolboxes for numerical optimal control and trajectory optimization, they are highly specialized, standalone tools that due not provide sufficient flexibility for other applications. here is where the ct steps in. the ct has been designed from ground up to provide the tools needed for fast development and evaluation of control methods while being optimized for efficiency allowing for online operation. while the emphasis lies on control, the tools provided can also be used for simulation, estimation or optimization applications.\nin contrast to other robotic software, ct is not a rigid integrated application but can be seen quite literal as a toolbox: it offers a variety of tools which can be used and combined to solve a task at hand. while ease-of-use has been a major criteria during the design and application examples are provided, using ct still requires programming and control knowledge. however, it frees the users from implementing standard methods that require in-depth experience with linear algebra or numerical methods. furthermore, by using common definitions and types, a seamless integration between different components such as systems, controllers or integrators is provided, enabling fast prototyping.\ndesign and implementation\nthe main focus of ct is efficiency, which is why it is fully implemented in c++. since ct is designed as a toolbox rather than an integrated application, we tried to provide maximum flexibility to the users. therefore, it is not tied to a specific middleware such as ros and dependencies are kept at a minimum. the two essential dependencies for ct are eigen and kindr (which is based on eigen). this eigen dependency is intentional since eigen is a defacto standard for linear algebra in c++, as it provides highly efficient implementations of standard matrix operations as well as more advanced linear algebra methods. kindr is a header only kinematics library which builds on top of it and provides data types for different rotation representations such as quaternions, euler angles or rotation matrices.\nstructure and modules of the ct\nthe control toolbox consists of three main modules. the core (ct_core) module, the optimal control (ct_optcon) module and the rigid body dynamics (ct_rbd) module. there is a clear hierarchy between the modules. that means, the modules depend on each other in this order, e.g. you can use the core module without the optcon or rbd module.\nthe core (ct_core) module provides general type definitions and mathematical tools. for example, it contains most data type definitions, definitions for systems and controllers, as well as basic functionality such as numerical integrators for differential equations.\nthe optimal control (ct_optcon) module builds on top of the core module and adds infrastructure for defining and solving optimal control problems. it contains the functionality for defining cost functions, constraints, solver backends and a general mpc wrapper.\nthe rigid body dynamics (ct_rbd) module provides tools for modelling rigid body dynamics systems and interfaces with ct_core and ct_optcon data types.\nfor testing as well as examples, we also provide the models module (ct_models) which contains various robot models including a quadruped, a robotic arm, a normal quadrotor and a quadrotor with slung load.\nthe four different main modules are detailed in the following.\nct_core (core)\ndefinitions of fundamental types for control and simulation, such as dynamic systems (ct::core::system), states (ct::core::statevector), controls (ct::core::controller), or trajectories (ct::core::discretetrajectorybase).\nnumeric integration (ct::core::integrator) with various ode solvers including fixed step (ct::core::integratoreuler, ct::core::integratorrk4) and variable step (ct::core::integratorrk5variable, ct::core::ode45) integrators, as well as symplectic (semi-implicit) integrators.\nnumerical approximation of trajectory sensitivities (ct::core::sensitivity , e.g. by forward-integrating variational differential equations)\ncommon feedback controllers (e.g. ct::core::pidcontroller)\nderivatives/jacobians of general functions using numerical differentiation (ct::core::derivativesnumdiff) or automatic-differentiation with code-generation (ct::core::derivativescppadcg) and just-in-time (jit) compilation (ct::core::derivativescppadjit)\nct_optcon (optimal control)\ndefinitions for optimal control problems (ct::optcon::optconproblem) and optimal control solvers (ct::optcon::optconsolver)\ncostfunction toolbox allowing to construct cost functions from file and providing first-order and second-order approximations, see ct::optcon::costfunction.\nconstraint toolbox for formulating constraints of optimal control problems, as well as automatically computing their jacobians.\nreference c++ implementations of the linear quadratic regulator, infinite-horizon lqr and time-varying lqr (ct::optcon::lqr, ct::optcon::tvlqr)\nriccati-solver (ct::optcon::gnriccatisolver) for unconstrained linear-quadratic optimal control problems, interface to high-performance third-party riccati-solvers for constrained linear-quadratic optimal control problems\niterative non-linear optimal control solvers, i.e. gauss-newton solvers such as ilqr (ct::optcon::ilqr) and gauss-newton multiple shooting(ct::optcon::gnms), constrained direct multiple-shooting (ct::optcon::dmssolver)\nnon-linear model predictive control (ct::optcon::mpc)\ndefinitions for nonlinear programming problems (nlps, ct::optcon::nlp) and interfaces to third-party nlp solvers (ct::optcon::snoptsolver and ct::optcon::ipoptsolver)\nct_rbd (rigid body dynamics)\nstandard models for rigid body dynamics\ndefinitions for the state of a rigid body system expressed as general coordinates (ct::rbd::rbdstate)\nroutines for different flavors of forward and inverse dynamics (ct::rbd::dynamics)\nrigid body and end-effector kinematics (ct::rbd::kinematics)\noperational space controllers\nbasic soft auto-differentiable contact model for arbitrary frames (ct::rbd::eecontactmodel)\nactuator dynamics (ct::rbd::actuatordynamics)\nbackend uses robcogen, a highly efficient rigid body dynamics library\nct_models\nvarious standard models for testing and evaluation including uavs (ct::models::quadrotor), ground robots, legged robots (ct::models::hyq), robot arms (ct::models::hya), inverted pendulums etc.\nmeans of creating linear approximation of these models\nhow to get started\nto get started with the control toolbox, please build the repository documentation with doxygen and follow the \"getting started\" tutorial.\nsupport\ncontact the devs: control-toolbox-dev@googlegroups.com\nacknowledgements\ncontributors\nmarkus giftthaler, markusgft (at) gmail (dot) com\nmichael neunert\nmarkus st\u00e4uble\nfarbod farshidian\ndiego pardo\ntimothy sandy\njan carius\nruben grandia\nhamza merzic\nfunding\nthis software has been developed at the agile & dexterous robotics lab at eth zurich, switzerland between 2014 and 2018. during that time, development has been made possible through financial support from the swiss national science foundation (snf) through a snf professorship award to jonas buchli and the national competence centers in research (nccr) robotics and digital fabrication.\nlicence information\nthe control toolbox is released under the bsd-2 clause license. please see licence.txt and notice.txt\nhow to cite the ct\n@article{adrlct,\ntitle={the control toolbox \u2014 an open-source c++ library for robotics, optimal and model predictive control},\nauthor={markus giftthaler and michael neunert and markus st{\\\"a}uble and jonas buchli},\njournal={2018 ieee international conference on simulation, modeling, and programming for autonomous robots (simpar)},\nyear={2018},\npages={123-129}\n}\nearlier versions\nearlier versions up to v2.3 are hosted on bitbucket, they can be found at https://bitbucket.org/adrlab/ct/wiki/home\nrelated publications\nthis toolbox has been used in, or has helped to realize the following academic publications:\nmarkus giftthaler, michael neunert, markus st\u00e4uble and jonas buchli. \u201cthe control toolbox - an open-source c++ library for robotics, optimal and model predictive control\u201d. ieee simpar 2018 (best student paper award). arxiv preprint\nmarkus giftthaler, michael neunert, markus st\u00e4uble, jonas buchli and moritz diehl. \u201ca family of iterative gauss-newton shooting methods for nonlinear optimal control\u201d. iros 2018. arxiv preprint\njan carius, ren\u00e9 ranftl, vladlen koltun and marco hutter. \"trajectory optimization with implicit hard contacts.\" ieee robotics and automation letters 3, no. 4 (2018): 3316-3323.\nmichael neunert, markus st\u00e4uble, markus giftthaler, dario bellicoso, jan carius, christian gehring, marco hutter and jonas buchli. \u201cwhole body model predictive control through contacts for quadrupeds\u201d. ieee robotics and automation letters, 2017. arxiv preprint\nmarkus giftthaler and jonas buchli. \u201ca projection approach to equality constrained iterative linear quadratic optimal control\u201d. 2017 ieee-ras international conference on humanoid robots, november 15-17, birmingham, uk. ieee xplore\nmarkus giftthaler, michael neunert, markus st\u00e4uble, marco frigerio, claudio semini and jonas buchli. \u201cautomatic differentiation of rigid body dynamics for optimal control and estimation\u201d, advanced robotics, simpar special issue. november 2017. arxiv preprint\nmichael neunert, markus giftthaler, marco frigerio, claudio semini and jonas buchli. \u201cfast derivatives of rigid body dynamics for control, optimization and estimation\u201d, 2016 ieee international conference on simulation, modelling, and programming for autonomous robots, san francisco. (best paper award). ieee xplore\nmichael neunert, farbod farshidian, alexander w. winkler, jonas buchli \"trajectory optimization through contacts and automatic gait discovery for quadrupeds\", ieee robotics and automation letters, ieee xplore\nmichael neunert, c\u00e9dric de crousaz, fadri furrer, mina kamel, farbod farshidian, roland siegwart, jonas buchli. \"fast nonlinear model predictive control for unified trajectory optimization and tracking\", 2016 ieee international conference on robotics and automation (icra), ieee xplore\nmarkus giftthaler, farbod farshidian, timothy sandy, lukas stadelmann and jonas buchli. \u201cefficient kinematic planning for mobile manipulators with non-holonomic constraints using optimal control\u201d, ieee international conference on robotics and automation, 2017, singapore. arxiv preprint\nmarkus giftthaler, timothy sandy, kathrin d\u00f6rfler, ian brooks, mark buckingham, gonzalo rey, matthias kohler, fabio gramazio and jonas buchli. \u201cmobile robotic fabrication at 1:1 scale: the in situ fabricator\u201d. construction robotics, springer journal no. 41693 arxiv preprint\ntimothy sandy, markus giftthaler, kathrin d\u00f6rfler, matthias kohler and jonas buchli. \u201cautonomous repositioning and localization of an in situ fabricator\u201d, ieee international conference on robotics and automation 2016, stockholm, sweden. ieee xplore\nmichael neunert, farbod farshidian, jonas buchli (2014). adaptive real-time nonlinear model predictive motion control. in iros 2014 workshop on machine learning in planning and control of robot motion preprint", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000062, "year": null}, {"Unnamed: 0": 1100, "autor": 80, "date": null, "content": "Robotics Toolbox for Python\nA Python implementation of the Robotics Toolbox for MATLAB\u00ae\nGitHub repository\nDocumentation\nWiki (examples and details)\nSynopsis\nThis toolbox brings robotics-specific functionality to Python, and leverages Python's advantages of portability, ubiquity and support, and the capability of the open-source ecosystem for linear algebra (numpy, scipy), graphics (matplotlib, three.js, WebGL), interactive development (jupyter, jupyterlab, mybinder.org), and documentation (sphinx).\nThe Toolbox provides tools for representing the kinematics and dynamics of serial-link manipulators - you can easily create your own in Denavit-Hartenberg form, import a URDF file, or use over 30 supplied models for well-known contemporary robots from Franka-Emika, Kinova, Universal Robotics, Rethink as well as classical robots such as the Puma 560 and the Stanford arm.\nThe toolbox will also support mobile robots with functions for robot motion models (unicycle, bicycle), path planning algorithms (bug, distance transform, D*, PRM), kinodynamic planning (lattice, RRT), localization (EKF, particle filter), map building (EKF) and simultaneous localization and mapping (EKF).\nThe Toolbox provides:\ncode that is mature and provides a point of comparison for other implementations of the same algorithms;\nroutines which are generally written in a straightforward manner which allows for easy understanding, perhaps at the expense of computational efficiency;\nsource code which can be read for learning and teaching;\nbackward compatability with the Robotics Toolbox for MATLAB\nThe Toolbox leverages the Spatial Maths Toolbox for Python to provide support for data types such as SO(n) and SE(n) matrices, quaternions, twists and spatial vectors.\nCode Example\nWe will load a model of the Franka-Emika Panda robot defined classically using modified (Craig's convention) Denavit-Hartenberg notation\nimport roboticstoolbox as rtb\nrobot = rtb.models.DH.Panda()\nprint(robot)\nPanda (by Franka Emika): 7 axes (RRRRRRR), modified DH parameters\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 a\u2c7c\u208b\u2081 \u2503 \u237a\u2c7c\u208b\u2081 \u2503 \u03b8\u2c7c \u2503 d\u2c7c \u2503 q\u207b \u2503 q\u207a \u2503\n\u2523\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252b\n\u2503 0.0 \u2503 0.0\u00b0 \u2503 q1 \u2503 0.333 \u2503 -166.0\u00b0 \u2503 166.0\u00b0 \u2503\n\u2503 0.0 \u2503 -90.0\u00b0 \u2503 q2 \u2503 0.0 \u2503 -101.0\u00b0 \u2503 101.0\u00b0 \u2503\n\u2503 0.0 \u2503 90.0\u00b0 \u2503 q3 \u2503 0.316 \u2503 -166.0\u00b0 \u2503 166.0\u00b0 \u2503\n\u2503 0.0825 \u2503 90.0\u00b0 \u2503 q4 \u2503 0.0 \u2503 -176.0\u00b0 \u2503 -4.0\u00b0 \u2503\n\u2503-0.0825 \u2503 -90.0\u00b0 \u2503 q5 \u2503 0.384 \u2503 -166.0\u00b0 \u2503 166.0\u00b0 \u2503\n\u2503 0.0 \u2503 90.0\u00b0 \u2503 q6 \u2503 0.0 \u2503 -1.0\u00b0 \u2503 215.0\u00b0 \u2503\n\u2503 0.088 \u2503 90.0\u00b0 \u2503 q7 \u2503 0.107 \u2503 -166.0\u00b0 \u2503 166.0\u00b0 \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502tool \u2502 t = 0, 0, 0.1; rpy/xyz = -45\u00b0, 0\u00b0, 0\u00b0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502name \u2502 q0 \u2502 q1 \u2502 q2 \u2502 q3 \u2502 q4 \u2502 q5 \u2502 q6 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 qz \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502\n\u2502 qr \u2502 0\u00b0 \u2502 -17.2\u00b0 \u2502 0\u00b0 \u2502 -126\u00b0 \u2502 0\u00b0 \u2502 115\u00b0 \u2502 45\u00b0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nT = robot.fkine(robot.qz) # forward kinematics\nprint(T)\n0.707107 0.707107 0 0.088\n0.707107 -0.707107 0 0\n0 0 -1 0.823\n0 0 0 1\n(Python prompts are not shown to make it easy to copy+paste the code, console output is indented)\nWe can solve inverse kinematics very easily. We first choose an SE(3) pose defined in terms of position and orientation (end-effector z-axis down (A=-Z) and finger orientation parallel to y-axis (O=+Y)).\nfrom spatialmath import SE3\nT = SE3(0.7, 0.2, 0.1) * SE3.OA([0, 1, 0], [0, 0, -1])\nsol = robot.ikine_LM(T) # solve IK\nprint(sol)\nIKsolution(q=array([ 0.2134, 1.867, -0.2264, 0.4825, 0.2198, 1.396, -2.037]), success=True, reason=None, iterations=12, residual=1.4517646473808178e-11)\nq_pickup = sol.q\nprint(robot.fkine(q_pickup)) # FK shows that desired end-effector pose was achieved\nOut[35]:\n-1 9.43001e-14 2.43909e-12 0.7\n9.43759e-14 1 7.2574e-13 0.2\n-2.43913e-12 7.2575e-13 -1 0.1\n0 0 0 1\nNote that because this robot is redundant we don't have any control over the arm configuration apart from end-effector pose, ie. we can't control the elbow height.\nWe can animate a path from the upright qz configuration to this pickup configuration\nqt = rtb.jtraj(robot.qz, q_pickup, 50)\nrobot.plot(qt.q, movie='panda1.gif')\nwhich uses the default matplotlib backend. Grey arrows show the joint axes and the colored frame shows the end-effector pose.\nLet's now load a URDF model of the same robot. The kinematic representation is no longer based on Denavit-Hartenberg parameters, it is now a rigid-body tree.\nrobot = rtb.models.URDF.Panda() # load URDF version of the Panda\nprint(robot) # display the model\npanda (by Franka Emika): 7 axes (RRRRRRR), ETS model\n\u250c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502id \u2502 link \u2502 parent \u2502 joint \u2502 ETS \u2502\n\u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0 \u2502 panda_link0 \u2502 _O_ \u2502 \u2502 {panda_link0} = {_O_} \u2502\n\u2502 1 \u2502 panda_link1 \u2502 panda_link0 \u2502 panda_joint1 \u2502 {panda_link1} = {panda_link0} * tz(0.333) * Rz(q0) \u2502\n\u2502 2 \u2502 panda_link2 \u2502 panda_link1 \u2502 panda_joint2 \u2502 {panda_link2} = {panda_link1} * Rx(-90\u00b0) * Rz(q1) \u2502\n\u2502 3 \u2502 panda_link3 \u2502 panda_link2 \u2502 panda_joint3 \u2502 {panda_link3} = {panda_link2} * ty(-0.316) * Rx(90\u00b0) * Rz(q2) \u2502\n\u2502 4 \u2502 panda_link4 \u2502 panda_link3 \u2502 panda_joint4 \u2502 {panda_link4} = {panda_link3} * tx(0.0825) * Rx(90\u00b0) * Rz(q3) \u2502\n\u2502 5 \u2502 panda_link5 \u2502 panda_link4 \u2502 panda_joint5 \u2502 {panda_link5} = {panda_link4} * tx(-0.0825) * ty(0.384) * Rx(-90\u00b0) * Rz(q4) \u2502\n\u2502 6 \u2502 panda_link6 \u2502 panda_link5 \u2502 panda_joint6 \u2502 {panda_link6} = {panda_link5} * Rx(90\u00b0) * Rz(q5) \u2502\n\u2502 7 \u2502 panda_link7 \u2502 panda_link6 \u2502 panda_joint7 \u2502 {panda_link7} = {panda_link6} * tx(0.088) * Rx(90\u00b0) * Rz(q6) \u2502\n\u2502 8 \u2502 @panda_link8 \u2502 panda_link7 \u2502 panda_joint8 \u2502 {panda_link8} = {panda_link7} * tz(0.107) \u2502\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502name \u2502 q0 \u2502 q1 \u2502 q2 \u2502 q3 \u2502 q4 \u2502 q5 \u2502 q6 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 qz \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502\n\u2502 qr \u2502 0\u00b0 \u2502 -17.2\u00b0 \u2502 0\u00b0 \u2502 -126\u00b0 \u2502 0\u00b0 \u2502 115\u00b0 \u2502 45\u00b0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nThe symbol @ indicates the link as an end-effector, a leaf node in the rigid-body tree.\nWe can instantiate our robot inside a browser-based 3d-simulation environment.\nfrom roboticstoolbox.backends.Swift import Swift # instantiate 3D browser-based visualizer\nbackend = Swift()\nbackend.launch() # activate it\nbackend.add(robot) # add robot to the 3D scene\nfor qk in qt.q: # for each joint configuration on trajectory\nrobot.q = qk # update the robot state\nbackend.step() # update visualization\nGetting going\nInstalling\nYou will need Python >= 3.6\nUsing pip\nInstall a snapshot from PyPI\npip3 install roboticstoolbox-python\nAvailable options are:\nvpython install VPython backend\ncollision install collision checking with pybullet\nPut the options in a comma separated list like\npip3 install roboticstoolbox-python[optionlist]\nSwift, a web-based visualizer, is installed as part of Robotics Toolbox.\nFrom GitHub\nTo install the bleeding-edge version from GitHub\ngit clone https://github.com/petercorke/robotics-toolbox-python.git\ncd robotics-toolbox-python\npip3 install -e .\nRun some examples\nThe notebooks folder contains some tutorial Jupyter notebooks which you can browse on GitHub.\nOr you can run them, and experiment with them, at mybinder.org.\nToolbox Research Applications\nThe toolbox is incredibly useful for developing and prototyping algorithms for research, thanks to the exhaustive set of well documented and mature robotic functions exposed through clean and painless APIs. Additionally, the ease at which a user can visualize their algorithm supports a rapid prototyping paradigm.\nPublication List\nJ. Haviland, N. S\u00fcnderhauf and P. Corke, \"A Holistic Approach to Reactive Mobile Manipulation,\". In the video, the robot is controlled using the Robotics toolbox for Python and features a recording from the Swift Simulator.\n[Arxiv Paper] [Project Website] [Video] [Code Example]\nJ. Haviland and P. Corke, \"NEO: A Novel Expeditious Optimisation Algorithm for Reactive Motion Control of Manipulators,\" in IEEE Robotics and Automation Letters, doi: 10.1109/LRA.2021.3056060. In the video, the robot is controlled using the Robotics toolbox for Python and features a recording from the Swift Simulator.\n[Arxiv Paper] [IEEE Xplore] [Project Website] [Video] [Code Example]\nA Purely-Reactive Manipulability-Maximising Motion Controller, J. Haviland and P. Corke. In the video, the robot is controlled using the Robotics toolbox for Python.\n[Paper] [Project Website] [Video] [Code Example]\nCommon Issues\nSee the common issues with fixes here.", "link": "https://github.com/petercorke/robotics-toolbox-python", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "robotics toolbox for python\na python implementation of the robotics toolbox for matlab\u00ae\ngithub repository\ndocumentation\nwiki (examples and details)\nsynopsis\nthis toolbox brings robotics-specific functionality to python, and leverages python's advantages of portability, ubiquity and support, and the capability of the open-source ecosystem for linear algebra (numpy, scipy), graphics (matplotlib, three.js, webgl), interactive development (jupyter, jupyterlab, mybinder.org), and documentation (sphinx).\nthe toolbox provides tools for representing the kinematics and dynamics of serial-link manipulators - you can easily create your own in denavit-hartenberg form, import a urdf file, or use over 30 supplied models for well-known contemporary robots from franka-emika, kinova, universal robotics, rethink as well as classical robots such as the puma 560 and the stanford arm.\nthe toolbox will also support mobile robots with functions for robot motion models (unicycle, bicycle), path planning algorithms (bug, distance transform, d*, prm), kinodynamic planning (lattice, rrt), localization (ekf, particle filter), map building (ekf) and simultaneous localization and mapping (ekf).\nthe toolbox provides:\ncode that is mature and provides a point of comparison for other implementations of the same algorithms;\nroutines which are generally written in a straightforward manner which allows for easy understanding, perhaps at the expense of computational efficiency;\nsource code which can be read for learning and teaching;\nbackward compatability with the robotics toolbox for matlab\nthe toolbox leverages the spatial maths toolbox for python to provide support for data types such as so(n) and se(n) matrices, quaternions, twists and spatial vectors.\ncode example\nwe will load a model of the franka-emika panda robot defined classically using modified (craig's convention) denavit-hartenberg notation\nimport roboticstoolbox as rtb\nrobot = rtb.models.dh.panda()\nprint(robot)\npanda (by franka emika): 7 axes (rrrrrrr), modified dh parameters\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 a\u2c7c\u208b\u2081 \u2503 \u237a\u2c7c\u208b\u2081 \u2503 \u03b8\u2c7c \u2503 d\u2c7c \u2503 q\u207b \u2503 q\u207a \u2503\n\u2523\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u254b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252b\n\u2503 0.0 \u2503 0.0\u00b0 \u2503 q1 \u2503 0.333 \u2503 -166.0\u00b0 \u2503 166.0\u00b0 \u2503\n\u2503 0.0 \u2503 -90.0\u00b0 \u2503 q2 \u2503 0.0 \u2503 -101.0\u00b0 \u2503 101.0\u00b0 \u2503\n\u2503 0.0 \u2503 90.0\u00b0 \u2503 q3 \u2503 0.316 \u2503 -166.0\u00b0 \u2503 166.0\u00b0 \u2503\n\u2503 0.0825 \u2503 90.0\u00b0 \u2503 q4 \u2503 0.0 \u2503 -176.0\u00b0 \u2503 -4.0\u00b0 \u2503\n\u2503-0.0825 \u2503 -90.0\u00b0 \u2503 q5 \u2503 0.384 \u2503 -166.0\u00b0 \u2503 166.0\u00b0 \u2503\n\u2503 0.0 \u2503 90.0\u00b0 \u2503 q6 \u2503 0.0 \u2503 -1.0\u00b0 \u2503 215.0\u00b0 \u2503\n\u2503 0.088 \u2503 90.0\u00b0 \u2503 q7 \u2503 0.107 \u2503 -166.0\u00b0 \u2503 166.0\u00b0 \u2503\n\u2517\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253b\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u251b\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502-----> tool !!!  \u2502 t = 0, 0, 0.1; rpy/xyz = -45\u00b0, 0\u00b0, 0\u00b0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502name \u2502 q0 \u2502 q1 \u2502 q2 \u2502 q3 \u2502 q4 \u2502 q5 \u2502 q6 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 qz \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502\n\u2502 qr \u2502 0\u00b0 \u2502 -17.2\u00b0 \u2502 0\u00b0 \u2502 -126\u00b0 \u2502 0\u00b0 \u2502 115\u00b0 \u2502 45\u00b0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nt = robot.fkine(robot.qz) # forward kinematics\nprint(t)\n0.707107 0.707107 0 0.088\n0.707107 -0.707107 0 0\n0 0 -1 0.823\n0 0 0 1\n(python prompts are not shown to make it easy to copy+paste the code, console output is indented)\nwe can solve inverse kinematics very easily. we first choose an se(3) pose defined in terms of position and orientation (end-effector z-axis down (a=-z) and finger orientation parallel to y-axis (o=+y)).\nfrom spatialmath import se3\nt = se3(0.7, 0.2, 0.1) * se3.oa([0, 1, 0], [0, 0, -1])\nsol = robot.ikine_lm(t) # solve ik\nprint(sol)\niksolution(q=array([ 0.2134, 1.867, -0.2264, 0.4825, 0.2198, 1.396, -2.037]), success=true, reason=none, iterations=12, residual=1.4517646473808178e-11)\nq_pickup = sol.q\nprint(robot.fkine(q_pickup)) # fk shows that desired end-effector pose was achieved\nout[35]:\n-1 9.43001e-14 2.43909e-12 0.7\n9.43759e-14 1 7.2574e-13 0.2\n-2.43913e-12 7.2575e-13 -1 0.1\n0 0 0 1\nnote that because this robot is redundant we don't have any control over the arm configuration apart from end-effector pose, ie. we can't control the elbow height.\nwe can animate a path from the upright qz configuration to this pickup configuration\nqt = rtb.jtraj(robot.qz, q_pickup, 50)\nrobot.plot(qt.q, movie='panda1.gif')\nwhich uses the default matplotlib backend. grey arrows show the joint axes and the colored frame shows the end-effector pose.\nlet's now load a urdf model of the same robot. the kinematic representation is no longer based on denavit-hartenberg parameters, it is now a rigid-body tree.\nrobot = rtb.models.urdf.panda() # load urdf version of the panda\nprint(robot) # display the model\npanda (by franka emika): 7 axes (rrrrrrr), ets model\n\u250c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502id \u2502 link \u2502 parent \u2502 joint \u2502 ets \u2502\n\u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 0 \u2502 panda_link0 \u2502 _o_ \u2502 \u2502 {panda_link0} = {_o_} \u2502\n\u2502 1 \u2502 panda_link1 \u2502 panda_link0 \u2502 panda_joint1 \u2502 {panda_link1} = {panda_link0} * tz(0.333) * rz(q0) \u2502\n\u2502 2 \u2502 panda_link2 \u2502 panda_link1 \u2502 panda_joint2 \u2502 {panda_link2} = {panda_link1} * rx(-90\u00b0) * rz(q1) \u2502\n\u2502 3 \u2502 panda_link3 \u2502 panda_link2 \u2502 panda_joint3 \u2502 {panda_link3} = {panda_link2} * ty(-0.316) * rx(90\u00b0) * rz(q2) \u2502\n\u2502 4 \u2502 panda_link4 \u2502 panda_link3 \u2502 panda_joint4 \u2502 {panda_link4} = {panda_link3} * tx(0.0825) * rx(90\u00b0) * rz(q3) \u2502\n\u2502 5 \u2502 panda_link5 \u2502 panda_link4 \u2502 panda_joint5 \u2502 {panda_link5} = {panda_link4} * tx(-0.0825) * ty(0.384) * rx(-90\u00b0) * rz(q4) \u2502\n\u2502 6 \u2502 panda_link6 \u2502 panda_link5 \u2502 panda_joint6 \u2502 {panda_link6} = {panda_link5} * rx(90\u00b0) * rz(q5) \u2502\n\u2502 7 \u2502 panda_link7 \u2502 panda_link6 \u2502 panda_joint7 \u2502 {panda_link7} = {panda_link6} * tx(0.088) * rx(90\u00b0) * rz(q6) \u2502\n\u2502 8 \u2502 @panda_link8 \u2502 panda_link7 \u2502 panda_joint8 \u2502 {panda_link8} = {panda_link7} * tz(0.107) \u2502\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502name \u2502 q0 \u2502 q1 \u2502 q2 \u2502 q3 \u2502 q4 \u2502 q5 \u2502 q6 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 qz \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502 0\u00b0 \u2502\n\u2502 qr \u2502 0\u00b0 \u2502 -17.2\u00b0 \u2502 0\u00b0 \u2502 -126\u00b0 \u2502 0\u00b0 \u2502 115\u00b0 \u2502 45\u00b0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nthe symbol @ indicates the link as an end-effector, a leaf node in the rigid-body tree.\nwe can instantiate our robot inside a browser-based 3d-simulation environment.\nfrom roboticstoolbox.backends.swift import swift # instantiate 3d browser-based visualizer\nbackend = swift()\nbackend.launch() # activate it\nbackend.add(robot) # add robot to the 3d scene\nfor qk in qt.q: # for each joint configuration on trajectory\nrobot.q = qk # update the robot state\nbackend.step() # update visualization\ngetting going\ninstalling\nyou will need python >= 3.6\nusing pip\ninstall a snapshot from pypi\npip3 install roboticstoolbox-python\navailable options are:\nvpython install vpython backend\ncollision install collision checking with pybullet\nput the options in a comma separated list like\npip3 install roboticstoolbox-python[optionlist]\nswift, a web-based visualizer, is installed as part of robotics toolbox.\nfrom github\nto install the bleeding-edge version from github\ngit clone https://github.com/petercorke/robotics-toolbox-python.git\ncd robotics-toolbox-python\npip3 install -e .\nrun some examples\nthe notebooks folder contains some tutorial jupyter notebooks which you can browse on github.\nor you can run them, and experiment with them, at mybinder.org.\ntoolbox research applications\nthe toolbox is incredibly useful for developing and prototyping algorithms for research, thanks to the exhaustive set of well documented and mature robotic functions exposed through clean and painless apis. additionally, the ease at which a user can visualize their algorithm supports a rapid prototyping paradigm.\npublication list\nj. haviland, n. s\u00fcnderhauf and p. corke, \"a holistic approach to reactive mobile manipulation,\". in the video, the robot is controlled using the robotics toolbox for python and features a recording from the swift simulator.\n[arxiv paper] [project website] [video] [code example]\nj. haviland and p. corke, \"neo: a novel expeditious optimisation algorithm for reactive motion control of manipulators,\" in ieee robotics and automation letters, doi: 10.1109/lra.2021.3056060. in the video, the robot is controlled using the robotics toolbox for python and features a recording from the swift simulator.\n[arxiv paper] [ieee xplore] [project website] [video] [code example]\na purely-reactive manipulability-maximising motion controller, j. haviland and p. corke. in the video, the robot is controlled using the robotics toolbox for python.\n[paper] [project website] [video] [code example]\ncommon issues\nsee the common issues with fixes here.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000080, "year": null}, {"Unnamed: 0": 1103, "autor": 83, "date": null, "content": "robosuite\n[Homepage] [White Paper] [Documentations] [ARISE Initiative]\nLatest Updates\n[10/19/2021] v1.3: Ray tracing and physically based rendering tools \u2728 and access to additional vision modalities \ud83c\udfa5 [video spotlight] [release notes]\n[02/17/2021] v1.2: Added observable sensor models \ud83d\udc40 and dynamics randomization \ud83c\udfb2 [release notes]\n[12/17/2020] v1.1: Refactored infrastructure and standardized model classes for much easier environment prototyping \ud83d\udd27 [release notes]\nrobosuite is a simulation framework powered by the MuJoCo physics engine for robot learning. It also offers a suite of benchmark environments for reproducible research. The current release (v1.3) features rendering tools, ground-truth of vision modalities, and camera utilities. This project is part of the broader Advancing Robot Intelligence through Simulated Environments (ARISE) Initiative, with the aim of lowering the barriers of entry for cutting-edge research at the intersection of AI and Robotics.\nData-driven algorithms, such as reinforcement learning and imitation learning, provide a powerful and generic tool in robotics. These learning paradigms, fueled by new advances in deep learning, have achieved some exciting successes in a variety of robot control problems. However, the challenges of reproducibility and the limited accessibility of robot hardware (especially during a pandemic) have impaired research progress. The overarching goal of robosuite is to provide researchers with:\na standardized set of benchmarking tasks for rigorous evaluation and algorithm development;\na modular design that offers great flexibility to design new robot simulation environments;\na high-quality implementation of robot controllers and off-the-shelf learning algorithms to lower the barriers to entry.\nThis framework was originally developed since late 2017 by researchers in Stanford Vision and Learning Lab (SVL) as an internal tool for robot learning research. Now it is actively maintained and used for robotics research projects in SVL and the UT Robot Perception and Learning Lab (RPL). We welcome community contributions to this project. For details please check out our contributing guidelines.\nThis release of robosuite contains seven robot models, eight gripper models, six controller modes, and nine standardized tasks. It also offers a modular design of APIs for building new environments with procedural generation. We highlight these primary features below:\nstandardized tasks: a set of standardized manipulation tasks of large diversity and varying complexity and RL benchmarking results for reproducible research;\nprocedural generation: modular APIs for programmatically creating new environments and new tasks as combinations of robot models, arenas, and parameterized 3D objects;\nrobot controllers: a selection of controller types to command the robots, such as joint-space velocity control, inverse kinematics control, operational space control, and 3D motion devices for teleoperation;\nmulti-modal sensors: heterogeneous types of sensory signals, including low-level physical states, RGB cameras, depth maps, and proprioception;\nhuman demonstrations: utilities for collecting human demonstrations, replaying demonstration datasets, and leveraging demonstration data for learning. Check out our sister project robomimic;\nphotorealistic rendering: integration with advanced graphics tools that provide real-time photorealistic renderings of simulated scenes.\nCitations\nPlease cite robosuite if you use this framework in your publications:\n@inproceedings{robosuite2020,\ntitle={robosuite: A Modular Simulation Framework and Benchmark for Robot Learning},\nauthor={Yuke Zhu and Josiah Wong and Ajay Mandlekar and Roberto Mart\\'{i}n-Mart\\'{i}n},\nbooktitle={arXiv preprint arXiv:2009.12293},\nyear={2020}\n}", "link": "https://github.com/ARISE-Initiative/robosuite", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "robosuite\n[homepage] [white paper] [documentations] [arise initiative]\nlatest updates\n[10/19/2021] v1.3: ray tracing and physically based rendering tools \u2728 and access to additional vision modalities \ud83c\udfa5 [video spotlight] [release notes]\n[02/17/2021] v1.2: added observable sensor models \ud83d\udc40 and dynamics randomization \ud83c\udfb2 [release notes]\n[12/17/2020] v1.1: refactored infrastructure and standardized model classes for much easier environment prototyping \ud83d\udd27 [release notes]\nrobosuite is a simulation framework powered by the mujoco physics engine for robot learning. it also offers a suite of benchmark environments for reproducible research. the current release (v1.3) features rendering tools, ground-truth of vision modalities, and camera utilities. this project is part of the broader advancing robot intelligence through simulated environments (arise) initiative, with the aim of lowering the barriers of entry for cutting-edge research at the intersection of ai and robotics.\ndata-driven algorithms, such as reinforcement learning and imitation learning, provide a powerful and generic -----> tool !!!  in robotics. these learning paradigms, fueled by new advances in deep learning, have achieved some exciting successes in a variety of robot control problems. however, the challenges of reproducibility and the limited accessibility of robot hardware (especially during a pandemic) have impaired research progress. the overarching goal of robosuite is to provide researchers with:\na standardized set of benchmarking tasks for rigorous evaluation and algorithm development;\na modular design that offers great flexibility to design new robot simulation environments;\na high-quality implementation of robot controllers and off-the-shelf learning algorithms to lower the barriers to entry.\nthis framework was originally developed since late 2017 by researchers in stanford vision and learning lab (svl) as an internal tool for robot learning research. now it is actively maintained and used for robotics research projects in svl and the ut robot perception and learning lab (rpl). we welcome community contributions to this project. for details please check out our contributing guidelines.\nthis release of robosuite contains seven robot models, eight gripper models, six controller modes, and nine standardized tasks. it also offers a modular design of apis for building new environments with procedural generation. we highlight these primary features below:\nstandardized tasks: a set of standardized manipulation tasks of large diversity and varying complexity and rl benchmarking results for reproducible research;\nprocedural generation: modular apis for programmatically creating new environments and new tasks as combinations of robot models, arenas, and parameterized 3d objects;\nrobot controllers: a selection of controller types to command the robots, such as joint-space velocity control, inverse kinematics control, operational space control, and 3d motion devices for teleoperation;\nmulti-modal sensors: heterogeneous types of sensory signals, including low-level physical states, rgb cameras, depth maps, and proprioception;\nhuman demonstrations: utilities for collecting human demonstrations, replaying demonstration datasets, and leveraging demonstration data for learning. check out our sister project robomimic;\nphotorealistic rendering: integration with advanced graphics tools that provide real-time photorealistic renderings of simulated scenes.\ncitations\nplease cite robosuite if you use this framework in your publications:\n@inproceedings{robosuite2020,\ntitle={robosuite: a modular simulation framework and benchmark for robot learning},\nauthor={yuke zhu and josiah wong and ajay mandlekar and roberto mart\\'{i}n-mart\\'{i}n},\nbooktitle={arxiv preprint arxiv:2009.12293},\nyear={2020}\n}", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000083, "year": null}, {"Unnamed: 0": 1117, "autor": 97, "date": null, "content": "Foxglove Studio\nFoxglove Studio is an integrated visualization and diagnosis tool for robotics, available in your browser or for download as a desktop app on Linux, Windows, and macOS.\nTo learn more, visit the following resources:\nAbout\nDocumentation\nRelease notes\nROS Wiki page\nBlog\nYou can also join us on the following platforms to ask questions, share feedback, and stay up to date on what our team is working on:\nGitHub Discussions\nSlack\nNewsletter\nTwitter\nLinkedIn\nInstallation\nVisit foxglove.dev/download or GitHub Releases to download the latest version.\nContributing\nFoxglove Studio is primarily written in TypeScript \u2013 contributions are welcome!\nAll contributors will need to agree to our Contributor License Agreement\nSupported development environments: Linux, Windows, macOS\nDependencies:\nNode.js v14+\nYarn \u2013 npm install -g yarn\nGit LFS\nVisual Studio Code \u2013 Recommended\nGetting started:\nClone repo\nRun yarn install\nLaunch the development environment:\n# To launch the desktop app (run both scripts concurrently):\n$ yarn serve # start webpack\n$ yarn start # launch electron\n# To launch the browser app:\n$ yarn web:serve\n# To launch the browser app using the dev backend server:\n$ yarn web:serve:dev\n# To launch the browser app using a local instance of the backend server:\n$ yarn web:serve:local\n# To launch the storybook:\n$ yarn storybook\n# Advanced usage: running webpack and electron on different computers (or VMs) on the same network\n$ yarn serve --host 192.168.xxx.yyy # the address where electron can reach the webpack dev server\n$ yarn dlx electron@13.0.0-beta.13 .webpack # launch the version of electron for the current computer's platform\nA Dockerfile to self-host the browser app is also available.\nOther useful commands:\n$ yarn run # list available commands\n$ yarn lint # lint all files\n$ yarn test # run all tests\n$ yarn test:watch # run tests on changed files\nCredits\nFoxglove Studio originally began as a fork of Webviz, an open source project developed by Cruise. The codebase has since changed significantly, with a port to TypeScript, more panels, additional data sources, improved layout management, new team features, and an Extension API.", "link": "https://github.com/foxglove/studio", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "foxglove studio\nfoxglove studio is an integrated visualization and diagnosis -----> tool !!!  for robotics, available in your browser or for download as a desktop app on linux, windows, and macos.\nto learn more, visit the following resources:\nabout\ndocumentation\nrelease notes\nros wiki page\nblog\nyou can also join us on the following platforms to ask questions, share feedback, and stay up to date on what our team is working on:\ngithub discussions\nslack\nnewsletter\ntwitter\nlinkedin\ninstallation\nvisit foxglove.dev/download or github releases to download the latest version.\ncontributing\nfoxglove studio is primarily written in typescript \u2013 contributions are welcome!\nall contributors will need to agree to our contributor license agreement\nsupported development environments: linux, windows, macos\ndependencies:\nnode.js v14+\nyarn \u2013 npm install -g yarn\ngit lfs\nvisual studio code \u2013 recommended\ngetting started:\nclone repo\nrun yarn install\nlaunch the development environment:\n# to launch the desktop app (run both scripts concurrently):\n$ yarn serve # start webpack\n$ yarn start # launch electron\n# to launch the browser app:\n$ yarn web:serve\n# to launch the browser app using the dev backend server:\n$ yarn web:serve:dev\n# to launch the browser app using a local instance of the backend server:\n$ yarn web:serve:local\n# to launch the storybook:\n$ yarn storybook\n# advanced usage: running webpack and electron on different computers (or vms) on the same network\n$ yarn serve --host 192.168.xxx.yyy # the address where electron can reach the webpack dev server\n$ yarn dlx electron@13.0.0-beta.13 .webpack # launch the version of electron for the current computer's platform\na dockerfile to self-host the browser app is also available.\nother useful commands:\n$ yarn run # list available commands\n$ yarn lint # lint all files\n$ yarn test # run all tests\n$ yarn test:watch # run tests on changed files\ncredits\nfoxglove studio originally began as a fork of webviz, an open source project developed by cruise. the codebase has since changed significantly, with a port to typescript, more panels, additional data sources, improved layout management, new team features, and an extension api.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000097, "year": null}, {"Unnamed: 0": 1121, "autor": 101, "date": null, "content": "Robotics Library\nThe Robotics Library (RL) is a self-contained C++ library for rigid body kinematics and dynamics, motion planning, and control. It covers spatial vector algebra, multibody systems, hardware abstraction, path planning, collision detection, and visualization. It is being used in research projects and in education, available under a BSD license, and free for use in commercial applications. RL runs on many different systems, including Linux, macOS, and Windows. It uses CMake as a build system and can be compiled with Clang, GCC, and Visual Studio.\nGetting Started\nWe offer precompiled Ubuntu packages on Launchpad as well as Windows binaries on GitHub for the latest release version, while Homebrew can be used on macOS to build corresponding packages. Tutorials on our website provide further information on how to develop applications using RL.\nThese tutorials include instructions on how to\ninstall the latest release on Ubuntu, Windows, or macOS,\ncreate your first program using RL on Linux or Windows,\nhave a look at our short API overview and our documentation,\ncreate your robot model with a kinematics and geometry definition,\nplan a collision-free path in your path planning scenario,\nbuild RL from source on Ubuntu, Windows, or macOS.\nNext Steps\nRL includes a number of demo applications and a selection of kinematics, geometry, and path planning examples that demonstrate how to use it for more advanced applications. Due to their size, a larger set of examples can be found in a separate repository.\nAmong several others, these demo applications include\na tool for converting between rotation matrices, angle axis, quaternions, and Euler angles,\nthe visualization of collision detection queries that can highlight intersections, minimum distance, and penetration depth,\na kinematics simulator that uses a TCP port for joint position updates,\na dynamics simulator that listens for joint torque updates,\nthe calculation of a collision-free path using a Probabilistic Roadmap or a Rapidly-Exploring Random Tree,\nthe visualization of path planning queries based on scenario definitions from an XML file,\nrobot forward and inverse dynamics using the Recursive Newton-Euler and Articulated-Body Algorithm methods,\nthe calculation of dynamics properties such as mass matrix, centrifugal and Coriolis forces, or gravity compensation,\nthe calculation and sending of a trajectory to a robot controller based on a cubic or quintic polynomial.\nPublications\nFor more detailed information on the design of the Robotics Library, please have a look at our IROS paper. The reference is\nMarkus Rickert and Andre Gaschler. Robotics Library: An object-oriented approach to robot applications. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 733\u2013740, Vancouver, BC, Canada, September 2017.\n@InProceedings{Rickert2017a,\nauthor = {Markus Rickert and Andre Gaschler},\ntitle = {{R}obotics {L}ibrary: An Object-Oriented Approach to Robot Applications},\nbooktitle = {Proceedings of the {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems},\nyear = {2017},\npages = {733--740},\naddress = {Vancouver, BC, Canada},\nmonth = sep,\ndoi = {10.1109/IROS.2017.8202232},\n}\nLicense\nAll source code files of RL are licensed under the permissive BSD 2-clause license. For the licenses of third-party dependencies, please refer to the respective projects.", "link": "https://github.com/roboticslibrary/rl", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "robotics library\nthe robotics library (rl) is a self-contained c++ library for rigid body kinematics and dynamics, motion planning, and control. it covers spatial vector algebra, multibody systems, hardware abstraction, path planning, collision detection, and visualization. it is being used in research projects and in education, available under a bsd license, and free for use in commercial applications. rl runs on many different systems, including linux, macos, and windows. it uses cmake as a build system and can be compiled with clang, gcc, and visual studio.\ngetting started\nwe offer precompiled ubuntu packages on launchpad as well as windows binaries on github for the latest release version, while homebrew can be used on macos to build corresponding packages. tutorials on our website provide further information on how to develop applications using rl.\nthese tutorials include instructions on how to\ninstall the latest release on ubuntu, windows, or macos,\ncreate your first program using rl on linux or windows,\nhave a look at our short api overview and our documentation,\ncreate your robot model with a kinematics and geometry definition,\nplan a collision-free path in your path planning scenario,\nbuild rl from source on ubuntu, windows, or macos.\nnext steps\nrl includes a number of demo applications and a selection of kinematics, geometry, and path planning examples that demonstrate how to use it for more advanced applications. due to their size, a larger set of examples can be found in a separate repository.\namong several others, these demo applications include\na -----> tool !!!  for converting between rotation matrices, angle axis, quaternions, and euler angles,\nthe visualization of collision detection queries that can highlight intersections, minimum distance, and penetration depth,\na kinematics simulator that uses a tcp port for joint position updates,\na dynamics simulator that listens for joint torque updates,\nthe calculation of a collision-free path using a probabilistic roadmap or a rapidly-exploring random tree,\nthe visualization of path planning queries based on scenario definitions from an xml file,\nrobot forward and inverse dynamics using the recursive newton-euler and articulated-body algorithm methods,\nthe calculation of dynamics properties such as mass matrix, centrifugal and coriolis forces, or gravity compensation,\nthe calculation and sending of a trajectory to a robot controller based on a cubic or quintic polynomial.\npublications\nfor more detailed information on the design of the robotics library, please have a look at our iros paper. the reference is\nmarkus rickert and andre gaschler. robotics library: an object-oriented approach to robot applications. in proceedings of the ieee/rsj international conference on intelligent robots and systems (iros), pages 733\u2013740, vancouver, bc, canada, september 2017.\n@inproceedings{rickert2017a,\nauthor = {markus rickert and andre gaschler},\ntitle = {{r}obotics {l}ibrary: an object-oriented approach to robot applications},\nbooktitle = {proceedings of the {ieee}/{rsj} international conference on intelligent robots and systems},\nyear = {2017},\npages = {733--740},\naddress = {vancouver, bc, canada},\nmonth = sep,\ndoi = {10.1109/iros.2017.8202232},\n}\nlicense\nall source code files of rl are licensed under the permissive bsd 2-clause license. for the licenses of third-party dependencies, please refer to the respective projects.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000101, "year": null}, {"Unnamed: 0": 1143, "autor": 123, "date": null, "content": "ROS + CamOdoCal Hand Eye Calibration\nThis is a ROS node integrating the Hand Eye Calibration implemented in CamOdoCal. See this stack exchange question explaining how Hand Eye Calibration works.\nExample uses include determining exact transforms with both positions and orientations of a:\ncamera attached to the floor is relative to a robot arm\ncamera attached to a robot arm tip relative to a robot arm\nset of cameras attached to a moving vehicle (this is what camodocal itself implements)\ntwo robot arms bolted together\nkeynote presentation explaining many details about hand eye calibration for those that are interested. Practical code and instructions to calibrate your robot can be found below.\nFeeding data into CamOdoCal\nEach measurement taken at a different time, position, and orientation narrows down the possible transforms that can represent the unknown X\nRecord a list of many transforms A and B taken between different time steps, or relative to the first time step\nRotations are in AxisAngle = UnitAxis*Angle format, or [x_axis,y_axis,z_axis]*\ud835\udf03_angle\n||UnitAxis||=1\n|| AxisAngle || = \ud835\udf03_angle\nTranslations are in the normal [x,y,z] format\nPass both vectors into EstimateHandEyeScrew()\nReturns X in the form of a 4x4 transform estimate\nWhen using this with a robot arm, move it around to a variety of poses and orientations, make sure any data sources that lag behind settle down, then record each pair of poses between the robot base and the robot tip, and between the eye/camera base and the marker, fiducial, or AR tag it is viewing.\nThis will save out a yaml file with the results. Be sure to load the results into your system using the data formatted as a rotation matrix, dual quaternion, or quaternion + translation. Roll Pitch Yaw can degenerate and will often be inaccurate!\nInstallation\nLinux\nAll dependencies can be installed via scripts in the robotics_setup repository on Ubuntu 14.04 or Ubuntu 16.04.\nMacOS\nOn OS X you can use homebrew and the homebrew-robotics tap to install all dependencies.\nROS (both Linux + MacOS)\nOnce you've completed the Linux or MacOS steps, follow normal ros source package installation procedures with catkin build.\nDependencies\nIf installing manually, be sure to follow the instructions for each library as there are specific steps required depending on your OS.\nROS indigo or kinetic\nOpenCV 2 or 3 with (recommended) nonfree components\nNote handeye_calib_camodocal does not call any nonfree components, but some users have had difficulty configuring CMake to compile and install all the other dependencies without them.\nOpenCV3 puts their nonfree components in opencv-contrib.\nEigen3\nceres-solver\nglog\nIf you encounter an error about providing \"FindGlog.cmake\" in CMAKE_MODULE_PATH, try installing glog from source.\ngflags\nExamples\nThere are example pre-recorded transforms in the example folder, all config files are expected to be in handeye_calib_camodocal/launch folder by default, but if that doesn't work try checking the ~/.ros/ folder.\nlaunch/handeye_example.launch\nThis configures the files transforms are loaded from and saved to, as well as rostopics if reading live data.\nexample/TransformPairsOutput.yml\nThis contains the set of transforms you record with the launch script, which are input into the solver.\nexample/CalibratedTransform.yml\nThis transform is your final results found by the solver.\nTo verify that the software is working run:\nroslaunch handeye_calib_camodocal handeye_example.launch\nYou should see output like the following:\n# INFO: Before refinement: H_12 =\n-0.962926 -0.156063 0.22004 -0.00802514\n-0.176531 0.981315 -0.0765322 0.0242905\n-0.203985 -0.112539 -0.972484 0.0550756\n0 0 0 1\nCeres Solver Report: Iterations: 89, Initial cost: 1.367791e+01, Final cost: 6.005694e-04, Termination: CONVERGENCE\n# INFO: After refinement: H_12 =\n-0.980558 0.184959 0.0655414 0.00771561\n0.0495028 -0.0900424 0.994707 0.0836796\n0.189881 0.978613 0.0791359 -0.00867321\n0 0 0 1\nResult from /ee_link to /ar_marker_0:\n-0.980558 0.184959 0.0655414 0.00771561\n0.0495028 -0.0900424 0.994707 0.0836796\n0.189881 0.978613 0.0791359 -0.00867321\n0 0 0 1\nTranslation (x,y,z) : 0.00771561 0.0836796 -0.00867321\nRotation (w,x,y,z): -0.046193, 0.0871038, 0.672938, 0.733099\nResult from /ar_marker_0 to /ee_link:\n-0.980558 0.184959 0.0655414 0.00771561\n0.0495028 -0.0900424 0.994707 0.0836796\n0.189881 0.978613 0.0791359 -0.00867321\n0 0 0 1\nInverted translation (x,y,z) : 0.00507012 0.0145954 -0.083056\nInverted rotation (w,x,y,z): -0.046193, 0.0871038, 0.672938, 0.733099\n0.046193 0.0871038 0.672938 0.733099\nWriting calibration to \"/home/cpaxton/catkin_ws/src/handeye_calib_camodocal/launch/CalibratedTransform.yml\"...\n[handeye_calib_camodocal-1] process has finished cleanly\nlog file: /home/cpaxton/.ros/log/a829db0a-f96b-11e6-b1dd-fc4dd43dd90b/handeye_calib_camodocal-1*.log\nall processes on machine have died, roslaunch will exit\nshutting down processing monitor...\n... shutting down processing monitor complete\ndone\nThe full terminal session can be found at:\nexample/terminal_session.txt\nRecording your own Transforms\nTo record your own session, modify launch/handeye_file.launch to specify the ROS topics that will publish the poses between which you wish to calibrate, then run:\nroslaunch handeye_calib_camodocal handeye_file.launch\nIf you have difficulty we cover just about every problem we've seen below in the troubleshooting section. It can also help to see this stack exchange question explaining how Hand Eye Calibration works\nAfter you run, be sure to back up TransformPairsInput.yml and CalibratedTransform.yml so you don't lose all the transforms and positions you saved!\nHow do I get transforms between the camera and an object it sees?\nIf a camera is calibrated it is possible to estimate the transform from the camera to a printed pattern with known dimensions. I don\u2019t recommend using a checkerboard for hand eye calibration because the pattern is ambiguous. Use something like:\nartoolkit.org\nhttps://github.com/ros-perception/ar_track_alvar\nThey provide instructions on how to set up your camera and create patterns that can be used to generate transforms.\nTroubleshooting\nSaved Files Not Loading?\nIf you have trouble finding the saved files, the default working directory of ROS applications is in the ~/.ros/ folder, so try looking there. Be sure to also check your launch file which is typically launch/handeye_file.launch this determines if transforms will be loaded from a running robot or saved files, as well as where save files are placed.\nCollecting Enough Data\nWe recommend you collect at least ~36 accurate transforms for a good calibration. If it fails to converge (i.e. you don't get a good result out), then you probably have your transforms flipped the wrong way or there is too much noise in your data to find a sufficiently accurate calibration.\nEliminating Sensor Noise\nOne simple method to help deal with this problem is to create a new node that reads the data you want to read and save a rolling average of the pose. This helps to stabilize the results. There are better methods such as a kalman filter that could handle this even better. If you take a rolling average, make sure each time you take the data the robot has been set in a single position for the entire duration of the time for which the rolling average is being taken, because any error here will throw off the results.\nExamples of \"too much noise\" when taking data\nIf there is too much noise you will probably see the following error:\nnormalization could not be handled. Your rotations and translations are probably either not aligned or not passed in properly\nThat means there is probably too much variation in the data you are reading to get an accurate solution. For example, if you watch the pose of an AR tag and it wobbles a little or flips this will prevent an accurate solution from being found. One way to help this is to ensure the system is completely stationary and then interpolate (average) the poses across several frames, again ensuring the system is completely stationary before recording the frame and then finally moving to the next position and repeating the process.\nYour cameras must be calibrated\nCamera calibration is very important! If they aren't calibrated then the poses being fed into the algorithm will be inaccurate, won't correspond, and thus the algorithm won't be able to find even a decent approximate solution and will just exit, printing an error.\nYour robot and cameras must be rigidly fixed\nHand eye calibration solves for a rigid body transform, so if the whole system isn't rigidly fixed the transform you are solving for is constantly changing and thus impossible to find accurately. For example, if you have a camera and a fixed robot base, check that your robot is securely bolted to a surface. Tighten those bolts up! Also ensure the camera is securely and rigidly fixed in place in a similar fasion. Check for any wobbling and make sure to wait for everything to become still before taking your data points.\nSanity Check by Physically Measuring\nSlight distortion or variation in time stamp while the arm moves slightly as you hold it can still throw it off. One additional way to test that is to have the arm go to two distant positions, and the length of the change in checkerboard poses should be equal to the length of the change in end effector tip poses assuming you can keep the orientation constant.\nSanity Check via Simulation\nIf you\u2019re concerned it is a bug in the algorithm you can run it in simulation with v-rep or gazebo (os + v-rep python script is in the repo) to verify it works, since that will avoid all physical measurement problems. From there you could consider taking more real data and incorporating the real data to narrow down the source of the problem.\nSanity Check Transforms and when loading from files\nIf you're loading from a file you've modified by hand, check if your matrices are transposed, inverted, or in very unusual cases even just the 3x3 Rotation component of the 4x4 rotation matrix may be transposed.\nExample output\nHere is an example output of what you should expect when a run is executed successfully:\nWriting pairs to \"/home/cpaxton/catkin_ws/src/handeye_calib_camodocal/launch/TransformPairsInput.yml\"...\nq[ INFO] [1473813682.393291696]: Calculating Calibration...\n# INFO: Before refinement: H_12 =\n-0.00160534 0.99916 0.0409473 -0.00813108\n-0.00487176 -0.0409546 0.999149 0.10692\n0.999987 0.00140449 0.00493341 0.0155885\n0 0 0 1\nCeres Solver Report: Iterations: 99, Initial cost: 1.882582e-05, Final cost: 1.607494e-05, Termination: CONVERGENCE\n# INFO: After refinement: H_12 =\n-0.00282176 0.999009 0.0444162 -0.00746998\n0.0121142 -0.0443789 0.998941 0.101617\n0.999923 0.00335684 -0.011977 -0.000671928\n0 0 0 1\nResult:\n-0.00282176 0.999009 0.0444162 -0.00746998\n0.0121142 -0.0443789 0.998941 0.101617\n0.999923 0.00335684 -0.011977 -0.000671928\n0 0 0 1\nTranslation: -0.00746998 0.101617 -0.000671928\nRotation: -0.48498 0.513209 0.492549 0.513209\nNote that this run is not a perfect one with errors of 5 mm over a motion of 1 m.\nCost\nOne key piece of information is the output of the cost function, which is a metric representing an estimate of solution accuracy:\nInitial cost: 1.882582e-05, Final cost: 1.607494e-05\nWith a really good run where the calibration is dead on the final cost should be on the order of 1e-13 or 1e-14.\nResults\nNow lets take a look at the results:\nTranslation: -0.00746998 0.101617 -0.000671928\nRotation: -0.48498 0.513209 0.492549 0.513209\nThe translation is in xyz format, and the rotation is in quaternion format. It is important to note that this tool and camodocal use the eigen Quaternion format which orders the four values stored in a quaternion wxyz. ROS launch files, by comparison store the data in the order xyzw. That means when copying the results into ROS you must move the first entry of the rotation to the end.\nHere is an example of all 7 numbers from above correctly put into a ros launch file:\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"endpoint_to_marker\" args=\" -0.00746998 0.101617 -0.000671928 0.513209 0.492549 0.513209 -0.48498 $(arg ee_frame) /endpoint_marker 10\"/>\nQuestions? Here is what we need to know.\nIf you try running this and have a question please create a diagram of your use case so we can understand how you are setting up the equations, then create a github issue.\nSee this stack exchange question explaining of how Hand Eye Calibration works for an example of such a diagram.\nAuthors\nAndrew Hundt ATHundt@gmail.com Felix Jonathan fjonath1@jhu.edu\nAcknowledgements\nHand-Eye Calibration Using Dual Quaternions\n@article{daniilidis1999hand,\ntitle={Hand-eye calibration using dual quaternions},\nauthor={Daniilidis, Konstantinos},\njournal={The International Journal of Robotics Research},\nvolume={18},\nnumber={3},\npages={286--298},\nyear={1999},\npublisher={SAGE Publications}\n}\nCamOdoCal\nLionel Heng, Bo Li, and Marc Pollefeys,\nCamOdoCal: Automatic Intrinsic and Extrinsic Calibration of a Rig with Multiple Generic Cameras and Odometry,\nIn Proc. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2013.\nLionel Heng, Mathias B\u00fcrki, Gim Hee Lee, Paul Furgale, Roland Siegwart, and Marc Pollefeys,\nInfrastructure-Based Calibration of a Multi-Camera Rig,\nIn Proc. IEEE International Conference on Robotics and Automation (ICRA), 2014.\nLionel Heng, Paul Furgale, and Marc Pollefeys,\nLeveraging Image-based Localization for Infrastructure-based Calibration of a Multi-camera Rig,\nJournal of Field Robotics (JFR), 2015.\nReferences\nStrobl, K., & Hirzinger, G. (2006) . Optimal hand-eye calibration. In 2006 IEEE/RSJ international conference on intelligent robots and systems (pp. 4647\u20134653), October 2006.\nTechnical University of Munich (TUM) CAMP lab wiki\nK. Daniilidis, \u201cHand\u2013Eye Calibration Using Dual Quaternions,\u201d Int. Journal of Robs. Research, vol. 18, no. 3, pp. 286\u2013298, June 1999.\nE. Bayro\u2013Corrochano, K. Daniilidis, and G. Sommer, \u201cMotor\u2013Algebra for 3D Kinematics: The Case of Hand\u2013Eye Calibration,\u201d Journal for Mathem. Imaging and Vision, vol. 13, no. 2, pp. 79\u2013100, Oct. 2000.\nF. Dornaika and R. Horaud, \u201cSimultaneous Robot\u2013World and Hand\u2013 Eye Calibration,\u201d IEEE Trans. on Robs. and Aut., vol. 14, no. 4, pp. 617\u2013622, August 1998.\nNote: figures and text are from mixed sources including the presentation author, the various papers referenced, and the TUM wiki.", "link": "https://github.com/jhu-lcsr/handeye_calib_camodocal", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "ros + camodocal hand eye calibration\nthis is a ros node integrating the hand eye calibration implemented in camodocal. see this stack exchange question explaining how hand eye calibration works.\nexample uses include determining exact transforms with both positions and orientations of a:\ncamera attached to the floor is relative to a robot arm\ncamera attached to a robot arm tip relative to a robot arm\nset of cameras attached to a moving vehicle (this is what camodocal itself implements)\ntwo robot arms bolted together\nkeynote presentation explaining many details about hand eye calibration for those that are interested. practical code and instructions to calibrate your robot can be found below.\nfeeding data into camodocal\neach measurement taken at a different time, position, and orientation narrows down the possible transforms that can represent the unknown x\nrecord a list of many transforms a and b taken between different time steps, or relative to the first time step\nrotations are in axisangle = unitaxis*angle format, or [x_axis,y_axis,z_axis]*\ud835\udf03_angle\n||unitaxis||=1\n|| axisangle || = \ud835\udf03_angle\ntranslations are in the normal [x,y,z] format\npass both vectors into estimatehandeyescrew()\nreturns x in the form of a 4x4 transform estimate\nwhen using this with a robot arm, move it around to a variety of poses and orientations, make sure any data sources that lag behind settle down, then record each pair of poses between the robot base and the robot tip, and between the eye/camera base and the marker, fiducial, or ar tag it is viewing.\nthis will save out a yaml file with the results. be sure to load the results into your system using the data formatted as a rotation matrix, dual quaternion, or quaternion + translation. roll pitch yaw can degenerate and will often be inaccurate!\ninstallation\nlinux\nall dependencies can be installed via scripts in the robotics_setup repository on ubuntu 14.04 or ubuntu 16.04.\nmacos\non os x you can use homebrew and the homebrew-robotics tap to install all dependencies.\nros (both linux + macos)\nonce you've completed the linux or macos steps, follow normal ros source package installation procedures with catkin build.\ndependencies\nif installing manually, be sure to follow the instructions for each library as there are specific steps required depending on your os.\nros indigo or kinetic\nopencv 2 or 3 with (recommended) nonfree components\nnote handeye_calib_camodocal does not call any nonfree components, but some users have had difficulty configuring cmake to compile and install all the other dependencies without them.\nopencv3 puts their nonfree components in opencv-contrib.\neigen3\nceres-solver\nglog\nif you encounter an error about providing \"findglog.cmake\" in cmake_module_path, try installing glog from source.\ngflags\nexamples\nthere are example pre-recorded transforms in the example folder, all config files are expected to be in handeye_calib_camodocal/launch folder by default, but if that doesn't work try checking the ~/.ros/ folder.\nlaunch/handeye_example.launch\nthis configures the files transforms are loaded from and saved to, as well as rostopics if reading live data.\nexample/transformpairsoutput.yml\nthis contains the set of transforms you record with the launch script, which are input into the solver.\nexample/calibratedtransform.yml\nthis transform is your final results found by the solver.\nto verify that the software is working run:\nroslaunch handeye_calib_camodocal handeye_example.launch\nyou should see output like the following:\n# info: before refinement: h_12 =\n-0.962926 -0.156063 0.22004 -0.00802514\n-0.176531 0.981315 -0.0765322 0.0242905\n-0.203985 -0.112539 -0.972484 0.0550756\n0 0 0 1\nceres solver report: iterations: 89, initial cost: 1.367791e+01, final cost: 6.005694e-04, termination: convergence\n# info: after refinement: h_12 =\n-0.980558 0.184959 0.0655414 0.00771561\n0.0495028 -0.0900424 0.994707 0.0836796\n0.189881 0.978613 0.0791359 -0.00867321\n0 0 0 1\nresult from /ee_link to /ar_marker_0:\n-0.980558 0.184959 0.0655414 0.00771561\n0.0495028 -0.0900424 0.994707 0.0836796\n0.189881 0.978613 0.0791359 -0.00867321\n0 0 0 1\ntranslation (x,y,z) : 0.00771561 0.0836796 -0.00867321\nrotation (w,x,y,z): -0.046193, 0.0871038, 0.672938, 0.733099\nresult from /ar_marker_0 to /ee_link:\n-0.980558 0.184959 0.0655414 0.00771561\n0.0495028 -0.0900424 0.994707 0.0836796\n0.189881 0.978613 0.0791359 -0.00867321\n0 0 0 1\ninverted translation (x,y,z) : 0.00507012 0.0145954 -0.083056\ninverted rotation (w,x,y,z): -0.046193, 0.0871038, 0.672938, 0.733099\n0.046193 0.0871038 0.672938 0.733099\nwriting calibration to \"/home/cpaxton/catkin_ws/src/handeye_calib_camodocal/launch/calibratedtransform.yml\"...\n[handeye_calib_camodocal-1] process has finished cleanly\nlog file: /home/cpaxton/.ros/log/a829db0a-f96b-11e6-b1dd-fc4dd43dd90b/handeye_calib_camodocal-1*.log\nall processes on machine have died, roslaunch will exit\nshutting down processing monitor...\n... shutting down processing monitor complete\ndone\nthe full terminal session can be found at:\nexample/terminal_session.txt\nrecording your own transforms\nto record your own session, modify launch/handeye_file.launch to specify the ros topics that will publish the poses between which you wish to calibrate, then run:\nroslaunch handeye_calib_camodocal handeye_file.launch\nif you have difficulty we cover just about every problem we've seen below in the troubleshooting section. it can also help to see this stack exchange question explaining how hand eye calibration works\nafter you run, be sure to back up transformpairsinput.yml and calibratedtransform.yml so you don't lose all the transforms and positions you saved!\nhow do i get transforms between the camera and an object it sees?\nif a camera is calibrated it is possible to estimate the transform from the camera to a printed pattern with known dimensions. i don\u2019t recommend using a checkerboard for hand eye calibration because the pattern is ambiguous. use something like:\nartoolkit.org\nhttps://github.com/ros-perception/ar_track_alvar\nthey provide instructions on how to set up your camera and create patterns that can be used to generate transforms.\ntroubleshooting\nsaved files not loading?\nif you have trouble finding the saved files, the default working directory of ros applications is in the ~/.ros/ folder, so try looking there. be sure to also check your launch file which is typically launch/handeye_file.launch this determines if transforms will be loaded from a running robot or saved files, as well as where save files are placed.\ncollecting enough data\nwe recommend you collect at least ~36 accurate transforms for a good calibration. if it fails to converge (i.e. you don't get a good result out), then you probably have your transforms flipped the wrong way or there is too much noise in your data to find a sufficiently accurate calibration.\neliminating sensor noise\none simple method to help deal with this problem is to create a new node that reads the data you want to read and save a rolling average of the pose. this helps to stabilize the results. there are better methods such as a kalman filter that could handle this even better. if you take a rolling average, make sure each time you take the data the robot has been set in a single position for the entire duration of the time for which the rolling average is being taken, because any error here will throw off the results.\nexamples of \"too much noise\" when taking data\nif there is too much noise you will probably see the following error:\nnormalization could not be handled. your rotations and translations are probably either not aligned or not passed in properly\nthat means there is probably too much variation in the data you are reading to get an accurate solution. for example, if you watch the pose of an ar tag and it wobbles a little or flips this will prevent an accurate solution from being found. one way to help this is to ensure the system is completely stationary and then interpolate (average) the poses across several frames, again ensuring the system is completely stationary before recording the frame and then finally moving to the next position and repeating the process.\nyour cameras must be calibrated\ncamera calibration is very important! if they aren't calibrated then the poses being fed into the algorithm will be inaccurate, won't correspond, and thus the algorithm won't be able to find even a decent approximate solution and will just exit, printing an error.\nyour robot and cameras must be rigidly fixed\nhand eye calibration solves for a rigid body transform, so if the whole system isn't rigidly fixed the transform you are solving for is constantly changing and thus impossible to find accurately. for example, if you have a camera and a fixed robot base, check that your robot is securely bolted to a surface. tighten those bolts up! also ensure the camera is securely and rigidly fixed in place in a similar fasion. check for any wobbling and make sure to wait for everything to become still before taking your data points.\nsanity check by physically measuring\nslight distortion or variation in time stamp while the arm moves slightly as you hold it can still throw it off. one additional way to test that is to have the arm go to two distant positions, and the length of the change in checkerboard poses should be equal to the length of the change in end effector tip poses assuming you can keep the orientation constant.\nsanity check via simulation\nif you\u2019re concerned it is a bug in the algorithm you can run it in simulation with v-rep or gazebo (os + v-rep python script is in the repo) to verify it works, since that will avoid all physical measurement problems. from there you could consider taking more real data and incorporating the real data to narrow down the source of the problem.\nsanity check transforms and when loading from files\nif you're loading from a file you've modified by hand, check if your matrices are transposed, inverted, or in very unusual cases even just the 3x3 rotation component of the 4x4 rotation matrix may be transposed.\nexample output\nhere is an example output of what you should expect when a run is executed successfully:\nwriting pairs to \"/home/cpaxton/catkin_ws/src/handeye_calib_camodocal/launch/transformpairsinput.yml\"...\nq[ info] [1473813682.393291696]: calculating calibration...\n# info: before refinement: h_12 =\n-0.00160534 0.99916 0.0409473 -0.00813108\n-0.00487176 -0.0409546 0.999149 0.10692\n0.999987 0.00140449 0.00493341 0.0155885\n0 0 0 1\nceres solver report: iterations: 99, initial cost: 1.882582e-05, final cost: 1.607494e-05, termination: convergence\n# info: after refinement: h_12 =\n-0.00282176 0.999009 0.0444162 -0.00746998\n0.0121142 -0.0443789 0.998941 0.101617\n0.999923 0.00335684 -0.011977 -0.000671928\n0 0 0 1\nresult:\n-0.00282176 0.999009 0.0444162 -0.00746998\n0.0121142 -0.0443789 0.998941 0.101617\n0.999923 0.00335684 -0.011977 -0.000671928\n0 0 0 1\ntranslation: -0.00746998 0.101617 -0.000671928\nrotation: -0.48498 0.513209 0.492549 0.513209\nnote that this run is not a perfect one with errors of 5 mm over a motion of 1 m.\ncost\none key piece of information is the output of the cost function, which is a metric representing an estimate of solution accuracy:\ninitial cost: 1.882582e-05, final cost: 1.607494e-05\nwith a really good run where the calibration is dead on the final cost should be on the order of 1e-13 or 1e-14.\nresults\nnow lets take a look at the results:\ntranslation: -0.00746998 0.101617 -0.000671928\nrotation: -0.48498 0.513209 0.492549 0.513209\nthe translation is in xyz format, and the rotation is in quaternion format. it is important to note that this -----> tool !!!  and camodocal use the eigen quaternion format which orders the four values stored in a quaternion wxyz. ros launch files, by comparison store the data in the order xyzw. that means when copying the results into ros you must move the first entry of the rotation to the end.\nhere is an example of all 7 numbers from above correctly put into a ros launch file:\n<node pkg=\"tf\" type=\"static_transform_publisher\" name=\"endpoint_to_marker\" args=\" -0.00746998 0.101617 -0.000671928 0.513209 0.492549 0.513209 -0.48498 $(arg ee_frame) /endpoint_marker 10\"/>\nquestions? here is what we need to know.\nif you try running this and have a question please create a diagram of your use case so we can understand how you are setting up the equations, then create a github issue.\nsee this stack exchange question explaining of how hand eye calibration works for an example of such a diagram.\nauthors\nandrew hundt athundt@gmail.com felix jonathan fjonath1@jhu.edu\nacknowledgements\nhand-eye calibration using dual quaternions\n@article{daniilidis1999hand,\ntitle={hand-eye calibration using dual quaternions},\nauthor={daniilidis, konstantinos},\njournal={the international journal of robotics research},\nvolume={18},\nnumber={3},\npages={286--298},\nyear={1999},\npublisher={sage publications}\n}\ncamodocal\nlionel heng, bo li, and marc pollefeys,\ncamodocal: automatic intrinsic and extrinsic calibration of a rig with multiple generic cameras and odometry,\nin proc. ieee/rsj international conference on intelligent robots and systems (iros), 2013.\nlionel heng, mathias b\u00fcrki, gim hee lee, paul furgale, roland siegwart, and marc pollefeys,\ninfrastructure-based calibration of a multi-camera rig,\nin proc. ieee international conference on robotics and automation (icra), 2014.\nlionel heng, paul furgale, and marc pollefeys,\nleveraging image-based localization for infrastructure-based calibration of a multi-camera rig,\njournal of field robotics (jfr), 2015.\nreferences\nstrobl, k., & hirzinger, g. (2006) . optimal hand-eye calibration. in 2006 ieee/rsj international conference on intelligent robots and systems (pp. 4647\u20134653), october 2006.\ntechnical university of munich (tum) camp lab wiki\nk. daniilidis, \u201chand\u2013eye calibration using dual quaternions,\u201d int. journal of robs. research, vol. 18, no. 3, pp. 286\u2013298, june 1999.\ne. bayro\u2013corrochano, k. daniilidis, and g. sommer, \u201cmotor\u2013algebra for 3d kinematics: the case of hand\u2013eye calibration,\u201d journal for mathem. imaging and vision, vol. 13, no. 2, pp. 79\u2013100, oct. 2000.\nf. dornaika and r. horaud, \u201csimultaneous robot\u2013world and hand\u2013 eye calibration,\u201d ieee trans. on robs. and aut., vol. 14, no. 4, pp. 617\u2013622, august 1998.\nnote: figures and text are from mixed sources including the presentation author, the various papers referenced, and the tum wiki.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000123, "year": null}, {"Unnamed: 0": 1144, "autor": 124, "date": null, "content": "Universal_Robots_ROS_Driver\nUniversal Robots have become a dominant supplier of lightweight, robotic manipulators for industry, as well as for scientific research and education. The Robot Operating System (ROS) has developed from a community-centered movement to a mature framework and quasi standard, providing a rich set of powerful tools for robot engineers and researchers, working in many different domains.\nWith the release of UR\u2019s new e-Series, the demand for a ROS driver that supports the new manipulators and the newest ROS releases and paradigms like ROS-control has increased further. The goal of this driver is to provide a stable and sustainable interface between UR robots and ROS that strongly benefit all parties.\nIt is the core value of Universal Robots, to empower people to achieve any goal within automation. The success criteria of this driver release is to follow this vision, by providing the ROS community with an easy to use, stable and powerful driver, that empowers the community to reach their goals in research and automation without struggling with unimportant technical challenges, instability or lacking features.\nAcknowledgment\nThis driver is forked from the ur_modern_driver.\nDeveloped in collaboration between:\nand .\nSupported by ROSIN - ROS-Industrial Quality-Assured Robot Software Components.\nMore information: rosin-project.eu\nThis project has received funding from the European Union\u2019s Horizon 2020\nresearch and innovation programme under grant agreement no. 732287.\nHow to report an issue\nBefore creating an issue, please have a look at the Troubleshooting section of this document.\nTo create an issue on the Issue Board please use the default template.\nHow to get help\nIf you need help using this driver, please see the ROS-category in the UR+ Developer Forum.\nFeatures\nWorks for all CB3 (with software version >= 3.7) and e-Series (software >= 5.1) robots and uses the RTDE interface for communication, whenever possible.\nFactory calibration of the robot inside ROS to reach Cartesian targets precisely.\nRealtime-enabled communication structure to robustly cope with the 2ms cycle time of the e-Series. To use this, compile and run it on a kernel with the PREEMPT_RT patch enabled. (See the Real-time setup guide on how to achieve this)\nTransparent integration of the teach-pendant. Using the URCaps system, a program is running on the robot that handles control commands sent from ROS side. With this, the robot can be paused, stopped and resumed without restarting the ROS driver. This will in the future also enable the usage of ROS-components as part of a more complex UR-program on the teach pendant. ROS-control of the robot can be quit using a service call to continue program execution on the TP.\nUse the robot's speed-scaling. When speed scaling is active due to safety constraints or the speed slider is used, this gets correctly handled on the ROS side, as well slowing down trajectory execution accordingly.\nNote: Other ros-controllers based on a position interface can be used with this driver, but may behave wrong if the speed slider isn't set to 100% or if speed scaling slows down the robot. Also, the pausing function can only be used if the default scaled trajectory controller is used.\nROS-Service-based replacement of most every-day TP-interactions offer using UR robots without interacting with the teach pendant at all, if desired. The robot can be started, stopped and even recovery from safety events can be done using ROS service- and action calls. See the driver's dashboard services and the robot_state_helper node for details.\nUse on-the-robot interpolation for both Cartesian and joint-based trajectories. This is extremely helpful if your application can not meet the real-time requirements of the driver. Special types of passthrough controllers forward the trajectories directly to the robot, which then takes care of interpolation between the waypoints to achieve best performance.\nPlease see the external feature list for a listing of all features supported by this driver.\nContents\nThis repository contains the new ur_robot_driver and a couple of helper packages, such as:\ncontroller_stopper: A small external tool that stops and restarts ros-controllers based on the robot's state. This can be helpful when the robot is in a state where it won't accept commands sent from ROS.\nur_calibration: Package around extracting and converting a robot's factory calibration information to make it usable by the robot_description.\nur_robot_driver: The actual driver package.\nRequirements\nThis driver requires a system setup with ROS. It is recommended to use Ubuntu 18.04 with ROS melodic, however using Ubuntu 20.04 with ROS noetic should also work.\nTo make sure that robot control isn't affected by system latencies, it is highly recommended to use a real-time kernel with the system. See the real-time setup guide on information how to set this up.\nPreliminary UR16e support\nThis driver supports all UR variants including the UR16e. However, upstream support for the UR16e is not finished, yet. When using the UR16e there is currently no support for gazebo or MoveIt!.\nSee #97 for details on using the latest upstream develop branch of ros_industrial/universal_robot which includes gazebo support for the ur16e, but no working MoveIt! support at the time of writing.\nBuilding\nNote: The driver consists of a C++ library that abstracts the robot's interfaces and a ROS driver on top of that. As the library can be built without ROS support, it is not a catkin package and therefore requires a different treatment when being built inside the workspace. See The alternative build method below if you'd like to build the library from source.\nIf you don't want to build the library from source, it is available as a binary package through the ROS distribution of ROS melodic and noetic. It will be installed automatically if you follow the steps below. If you'd like to also build the library from source, please follow the steps explained in the next section.\n# source global ros\n$ source /opt/ros/<your_ros_version>/setup.bash\n# create a catkin workspace\n$ mkdir -p catkin_ws/src && cd catkin_ws\n# clone the driver\n$ git clone https://github.com/UniversalRobots/Universal_Robots_ROS_Driver.git src/Universal_Robots_ROS_Driver\n# clone fork of the description. This is currently necessary, until the changes are merged upstream.\n$ git clone -b calibration_devel https://github.com/fmauch/universal_robot.git src/fmauch_universal_robot\n# install dependencies\n$ sudo apt update -qq\n$ rosdep update\n$ rosdep install --from-paths src --ignore-src -y\n# build the workspace\n$ catkin_make\n# activate the workspace (ie: source it)\n$ source devel/setup.bash\nAlternative: All-source build\nIf you would like to also build the library from source, clone the library into your workspace, as well and build it using either catkin_make_isolated or catkin build.\n$ source /opt/ros/<your_ros_version>/setup.bash\n$ mkdir -p catkin_ws/src && cd catkin_ws\n$ git clone -b boost https://github.com/UniversalRobots/Universal_Robots_Client_Library.git src/Universal_Robots_Client_Library\n$ git clone https://github.com/UniversalRobots/Universal_Robots_ROS_Driver.git src/Universal_Robots_ROS_Driver\n$ git clone -b calibration_devel https://github.com/fmauch/universal_robot.git src/fmauch_universal_robot\n$ sudo apt update -qq\n$ rosdep update\n$ rosdep install --from-paths src --ignore-src -y\n$ catkin_make_isolated\n$ source devel_isolated/setup.bash\nSetting up a UR robot for ur_robot_driver\nPrepare the robot\nFor using the ur_robot_driver with a real robot you need to install the externalcontrol-1.0.4.urcap which can be found inside the resources folder of this driver.\nNote: For installing this URCap a minimal PolyScope version of 3.7 or 5.1 (in case of e-Series) is necessary.\nFor installing the necessary URCap and creating a program, please see the individual tutorials on how to setup a CB3 robot or how to setup an e-Series robot.\nTo setup the tool communication on an e-Series robot, please consider the tool communication setup guide.\nPrepare the ROS PC\nFor using the driver make sure it is installed (either by the debian package or built from source inside a catkin workspace).\nExtract calibration information\nEach UR robot is calibrated inside the factory giving exact forward and inverse kinematics. To also make use of this in ROS, you first have to extract the calibration information from the robot.\nThough this step is not necessary to control the robot using this driver, it is highly recommended to do so, as otherwise endeffector positions might be off in the magnitude of centimeters.\nFor this, there exists a helper script:\n$ roslaunch ur_calibration calibration_correction.launch \\\nrobot_ip:=<robot_ip> target_filename:=\"${HOME}/my_robot_calibration.yaml\"\nFor the parameter robot_ip insert the IP address on which the ROS pc can reach the robot. As target_filename provide an absolute path where the result will be saved to.\nWe recommend keeping calibrations for all robots in your organization in a common package. See the package's documentation for details.\nQuick start\nOnce the driver is built and the externalcontrol URCap is installed on the robot, you are good to go ahead starting the driver. (Note: We do recommend, though, to extract your robot's calibration first.)\nTo actually start the robot driver use one of the existing launch files\n$ roslaunch ur_robot_driver <robot_type>_bringup.launch robot_ip:=192.168.56.101\nwhere <robot_type> is one of ur3, ur5, ur10, ur3e, ur5e, ur10e, ur16e. Note that in this example we load the calibration parameters for the robot \"ur10_example\".\nIf you calibrated your robot before, pass that calibration to the launch file:\n$ roslaunch ur_robot_driver <robot_type>_bringup.launch robot_ip:=192.168.56.101 \\\nkinematics_config:=$(rospack find ur_calibration)/etc/ur10_example_calibration.yaml\nIf the parameters in that file don't match the ones reported from the robot, the driver will output an error during startup, but will remain usable.\nFor more information on the launch file's parameters see its own documentation.\nOnce the robot driver is started, load the previously generated program on the robot panel that will start the External Control program node and execute it. From that moment on the robot is fully functional. You can make use of the Pause function or even Stop (\u23f9\ufe0f) the program. Simply press the Play button (\u25b6\ufe0f) again and the ROS driver will reconnect.\nInside the ROS terminal running the driver you should see the output Robot ready to receive control commands.\nTo control the robot using ROS, use the action server on\n/scaled_pos_joint_traj_controller/follow_joint_trajectory\nUse this with any client interface such as MoveIt! or simply the rqt_joint_trajectory_controller gui:\nrosrun rqt_joint_trajectory_controller rqt_joint_trajectory_controller\nYou may need to install rqt_joint_trajectory_controller by running:\nsudo apt install ros-<ROS-DISTRO>-rqt-joint-trajectory-controller\nwhere ROS-DISTRO will be replaced with your version of ROS.\nFor a more elaborate tutorial on how to get started, please see the usage example.\nReplacing the robot description\nIn a real-world scenario you will want to replace the robot description with a description containing the whole scene where the robot is acting in. For this, all the bringup launchfiles offer the argument robot_description_file that should point to a launchfile loading the robot description.\nWhile the load_urXXX.launch files from the ur_description package contain a lot of arguments to change the robot model, this driver only forwards the kinematics_config parameter file. For further adaption please create your own load_urXXX.launch file that fits your application and pass this to the urXXX_bringup.launch files from this package.\nIf you prefer decoupling loading the robot description and starting the driver, you can start the ur_control.launch launchfile directly after the robot_description has been uploaded to the parameter server.\nTroubleshooting\nThis section will cover some previously raised issues.\nI started everything, but I cannot control the robot.\nThe External Control program node from the URCap is not running on the robot. Make sure to create a program containing this node on the robot and start it. Inside the ROS terminal you should see the output Robot ready to receive control commands.\nNote: When interacting with the teach pendant, or sending other primary programs to the robot, the program will be stopped. On the ROS terminal you will see an output Connection to robot dropped, waiting for new connection. In those cases, restart program execution (e.g. by pressing the play button on the TP, or calling rosservice call /ur_hardware_interface/dashboard/play as explained here and here).\nIn general, make sure you've completed the following tasks:\nInstall and setup the External Control URCap as explained above (also setup the IP address of the ROS pc inside the URCap's installation.)\nStart the driver handing the IP of the robot as explained in the quickstart\nLoad and start the previously generated program on the TP.\nWhen starting the program on the TP, I get an error \"The connection to the remote PC could not be established\"\nMake sure, the IP address setup is correct, as described in the setup guides (CB3 robots, e-Series robots)\nNote: This error can also show up, when the ROS driver is not running.\nWhen starting the program on the TP, I get a C207A0 error.\nNote: With the current driver version this issue can only happen when the fieldbus is enabled after the ROS driver has been started. Otherwise you will run into #204 when starting the driver with an enabled EtherNet/IP fieldbus.\nMost probably, the EtherNet/IP fieldbus is enabled in the robot's installation. If your setup includes an Ethernet/IP fieldbus (note: EtherNet/IP != ethernet), make sure that it is connected properly. In the Ethernet/IP fieldbus Installation screen (e-series: Installation > Fieldbus > EtherNet/IP, CB3: Installation > EtherNet/IP) you can select the action that is being executed upon a loss of EtherNet/IP Scanner connection. If you select \"None\", save installation and program, then no exception is raised when no connection to the fieldbus scanner can be established (note: This is only to get the External Control running. You probably want to make sure that a connection to the fieldbus scanner can indeed be made). If you don't use EtherNet/IP fieldbusses at all, you can disable it in the same installation screen.\nWhen starting the driver, it crashes with Variable 'speed_slider_mask' is currently controlled by another RTDE client\nProbably, you are running into #204. Currently, this driver cannot be used together with an enabled EtherNet/IP fieldbus. Disable EtherNet/IP to workaround this error. #204 contains a guide how to do this.\nI cannot get a realtime kernel running together with an NVIDIA graphics card\nThis is a known issue and unfortunately we don't have a solution for this. The Nvidia kernel module seems to not compile with every kernel. We recommend to use a multi-machine ROS setup in this situation where a realtime-system is running the robot driver and a separate machine is performing the computations requiring the graphics card.\nWhy can't the driver use the extracted calibration info on startup?\nThis is mainly because parameters are loaded onto the parameter server before any nodes are started.\nThe robot_description concept inside ROS is not designed to be changed while a system is running. Consumers of the urdf/robot_description (in terms of other ROS nodes) will not update the model they have been loading initially. While technically the robot_description parameter could be altered during runtime and any node that is started afterwards would see the updated model, this would lead to an inconsistent application state (as some nodes will use the old model, while others use the updated one). In other words: It's not the driver that needs/benefits from this calibrated urdf, it's the rest of the ROS application and that will only see it if the calibrated version is present on the parameter server before nodes are started.\nAdditionally: If the calibration stored on the ROS side doesn't match the one of the robot controller, there's a good chance there is a reason for this and it would be better to make updating it a conscious decision by a human (as the driver would not know when updating the model would be convenient or safe). Having to run the calibration extraction/transformation as a separate step makes this possible and doesn't hide this step from the end user.\nCan this driver be used inside a combined hardware interface?\nYes, this is possible. However, if used inside a combined HW interface we recommend to enable non-blocking read functinality.\nI sent raw script code to the robot but it is not executed\nOn the e-Series the robot has to be in remote control mode to accept script code from an external source. This has to be switched from the Teach-Pendant.\nUsing the dashboard doesn't work\nOn the e-Series the robot has to be in remote control mode to accept certain calls on the dashboard server. See Available dashboard commands for details.\nPassthrough controllers: The robot does not fully reach trajectory points even though I have specified the path tolerance to be 0\nIf you are using a control modes that forwards trajectories to the robot, currently the path tolerance is ignored. The corresponding interface on the robot and client-library level exists in the form of a \"blend radius\", but is not utilized by this ROS driver. For more information see this issue.\nCan I use the Cartesian controllers together with MoveIt!?\nNot directly, no. MoveIt! plans a Cartesian path and then creates a joint trajectory out of that for execution, as the common interface to robot drivers in ROS is the FollowJointTrajectory action.\nFor supporting Cartesian controllers inside MoveIt! changes would have to be made to MoveIt! itself.", "link": "https://github.com/UniversalRobots/Universal_Robots_ROS_Driver", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "universal_robots_ros_driver\nuniversal robots have become a dominant supplier of lightweight, robotic manipulators for industry, as well as for scientific research and education. the robot operating system (ros) has developed from a community-centered movement to a mature framework and quasi standard, providing a rich set of powerful tools for robot engineers and researchers, working in many different domains.\nwith the release of ur\u2019s new e-series, the demand for a ros driver that supports the new manipulators and the newest ros releases and paradigms like ros-control has increased further. the goal of this driver is to provide a stable and sustainable interface between ur robots and ros that strongly benefit all parties.\nit is the core value of universal robots, to empower people to achieve any goal within automation. the success criteria of this driver release is to follow this vision, by providing the ros community with an easy to use, stable and powerful driver, that empowers the community to reach their goals in research and automation without struggling with unimportant technical challenges, instability or lacking features.\nacknowledgment\nthis driver is forked from the ur_modern_driver.\ndeveloped in collaboration between:\nand .\nsupported by rosin - ros-industrial quality-assured robot software components.\nmore information: rosin-project.eu\nthis project has received funding from the european union\u2019s horizon 2020\nresearch and innovation programme under grant agreement no. 732287.\nhow to report an issue\nbefore creating an issue, please have a look at the troubleshooting section of this document.\nto create an issue on the issue board please use the default template.\nhow to get help\nif you need help using this driver, please see the ros-category in the ur+ developer forum.\nfeatures\nworks for all cb3 (with software version >= 3.7) and e-series (software >= 5.1) robots and uses the rtde interface for communication, whenever possible.\nfactory calibration of the robot inside ros to reach cartesian targets precisely.\nrealtime-enabled communication structure to robustly cope with the 2ms cycle time of the e-series. to use this, compile and run it on a kernel with the preempt_rt patch enabled. (see the real-time setup guide on how to achieve this)\ntransparent integration of the teach-pendant. using the urcaps system, a program is running on the robot that handles control commands sent from ros side. with this, the robot can be paused, stopped and resumed without restarting the ros driver. this will in the future also enable the usage of ros-components as part of a more complex ur-program on the teach pendant. ros-control of the robot can be quit using a service call to continue program execution on the tp.\nuse the robot's speed-scaling. when speed scaling is active due to safety constraints or the speed slider is used, this gets correctly handled on the ros side, as well slowing down trajectory execution accordingly.\nnote: other ros-controllers based on a position interface can be used with this driver, but may behave wrong if the speed slider isn't set to 100% or if speed scaling slows down the robot. also, the pausing function can only be used if the default scaled trajectory controller is used.\nros-service-based replacement of most every-day tp-interactions offer using ur robots without interacting with the teach pendant at all, if desired. the robot can be started, stopped and even recovery from safety events can be done using ros service- and action calls. see the driver's dashboard services and the robot_state_helper node for details.\nuse on-the-robot interpolation for both cartesian and joint-based trajectories. this is extremely helpful if your application can not meet the real-time requirements of the driver. special types of passthrough controllers forward the trajectories directly to the robot, which then takes care of interpolation between the waypoints to achieve best performance.\nplease see the external feature list for a listing of all features supported by this driver.\ncontents\nthis repository contains the new ur_robot_driver and a couple of helper packages, such as:\ncontroller_stopper: a small external -----> tool !!!  that stops and restarts ros-controllers based on the robot's state. this can be helpful when the robot is in a state where it won't accept commands sent from ros.\nur_calibration: package around extracting and converting a robot's factory calibration information to make it usable by the robot_description.\nur_robot_driver: the actual driver package.\nrequirements\nthis driver requires a system setup with ros. it is recommended to use ubuntu 18.04 with ros melodic, however using ubuntu 20.04 with ros noetic should also work.\nto make sure that robot control isn't affected by system latencies, it is highly recommended to use a real-time kernel with the system. see the real-time setup guide on information how to set this up.\npreliminary ur16e support\nthis driver supports all ur variants including the ur16e. however, upstream support for the ur16e is not finished, yet. when using the ur16e there is currently no support for gazebo or moveit!.\nsee #97 for details on using the latest upstream develop branch of ros_industrial/universal_robot which includes gazebo support for the ur16e, but no working moveit! support at the time of writing.\nbuilding\nnote: the driver consists of a c++ library that abstracts the robot's interfaces and a ros driver on top of that. as the library can be built without ros support, it is not a catkin package and therefore requires a different treatment when being built inside the workspace. see the alternative build method below if you'd like to build the library from source.\nif you don't want to build the library from source, it is available as a binary package through the ros distribution of ros melodic and noetic. it will be installed automatically if you follow the steps below. if you'd like to also build the library from source, please follow the steps explained in the next section.\n# source global ros\n$ source /opt/ros/<your_ros_version>/setup.bash\n# create a catkin workspace\n$ mkdir -p catkin_ws/src && cd catkin_ws\n# clone the driver\n$ git clone https://github.com/universalrobots/universal_robots_ros_driver.git src/universal_robots_ros_driver\n# clone fork of the description. this is currently necessary, until the changes are merged upstream.\n$ git clone -b calibration_devel https://github.com/fmauch/universal_robot.git src/fmauch_universal_robot\n# install dependencies\n$ sudo apt update -qq\n$ rosdep update\n$ rosdep install --from-paths src --ignore-src -y\n# build the workspace\n$ catkin_make\n# activate the workspace (ie: source it)\n$ source devel/setup.bash\nalternative: all-source build\nif you would like to also build the library from source, clone the library into your workspace, as well and build it using either catkin_make_isolated or catkin build.\n$ source /opt/ros/<your_ros_version>/setup.bash\n$ mkdir -p catkin_ws/src && cd catkin_ws\n$ git clone -b boost https://github.com/universalrobots/universal_robots_client_library.git src/universal_robots_client_library\n$ git clone https://github.com/universalrobots/universal_robots_ros_driver.git src/universal_robots_ros_driver\n$ git clone -b calibration_devel https://github.com/fmauch/universal_robot.git src/fmauch_universal_robot\n$ sudo apt update -qq\n$ rosdep update\n$ rosdep install --from-paths src --ignore-src -y\n$ catkin_make_isolated\n$ source devel_isolated/setup.bash\nsetting up a ur robot for ur_robot_driver\nprepare the robot\nfor using the ur_robot_driver with a real robot you need to install the externalcontrol-1.0.4.urcap which can be found inside the resources folder of this driver.\nnote: for installing this urcap a minimal polyscope version of 3.7 or 5.1 (in case of e-series) is necessary.\nfor installing the necessary urcap and creating a program, please see the individual tutorials on how to setup a cb3 robot or how to setup an e-series robot.\nto setup the tool communication on an e-series robot, please consider the tool communication setup guide.\nprepare the ros pc\nfor using the driver make sure it is installed (either by the debian package or built from source inside a catkin workspace).\nextract calibration information\neach ur robot is calibrated inside the factory giving exact forward and inverse kinematics. to also make use of this in ros, you first have to extract the calibration information from the robot.\nthough this step is not necessary to control the robot using this driver, it is highly recommended to do so, as otherwise endeffector positions might be off in the magnitude of centimeters.\nfor this, there exists a helper script:\n$ roslaunch ur_calibration calibration_correction.launch \\\nrobot_ip:=<robot_ip> target_filename:=\"${home}/my_robot_calibration.yaml\"\nfor the parameter robot_ip insert the ip address on which the ros pc can reach the robot. as target_filename provide an absolute path where the result will be saved to.\nwe recommend keeping calibrations for all robots in your organization in a common package. see the package's documentation for details.\nquick start\nonce the driver is built and the externalcontrol urcap is installed on the robot, you are good to go ahead starting the driver. (note: we do recommend, though, to extract your robot's calibration first.)\nto actually start the robot driver use one of the existing launch files\n$ roslaunch ur_robot_driver <robot_type>_bringup.launch robot_ip:=192.168.56.101\nwhere <robot_type> is one of ur3, ur5, ur10, ur3e, ur5e, ur10e, ur16e. note that in this example we load the calibration parameters for the robot \"ur10_example\".\nif you calibrated your robot before, pass that calibration to the launch file:\n$ roslaunch ur_robot_driver <robot_type>_bringup.launch robot_ip:=192.168.56.101 \\\nkinematics_config:=$(rospack find ur_calibration)/etc/ur10_example_calibration.yaml\nif the parameters in that file don't match the ones reported from the robot, the driver will output an error during startup, but will remain usable.\nfor more information on the launch file's parameters see its own documentation.\nonce the robot driver is started, load the previously generated program on the robot panel that will start the external control program node and execute it. from that moment on the robot is fully functional. you can make use of the pause function or even stop (\u23f9\ufe0f) the program. simply press the play button (\u25b6\ufe0f) again and the ros driver will reconnect.\ninside the ros terminal running the driver you should see the output robot ready to receive control commands.\nto control the robot using ros, use the action server on\n/scaled_pos_joint_traj_controller/follow_joint_trajectory\nuse this with any client interface such as moveit! or simply the rqt_joint_trajectory_controller gui:\nrosrun rqt_joint_trajectory_controller rqt_joint_trajectory_controller\nyou may need to install rqt_joint_trajectory_controller by running:\nsudo apt install ros-<ros-distro>-rqt-joint-trajectory-controller\nwhere ros-distro will be replaced with your version of ros.\nfor a more elaborate tutorial on how to get started, please see the usage example.\nreplacing the robot description\nin a real-world scenario you will want to replace the robot description with a description containing the whole scene where the robot is acting in. for this, all the bringup launchfiles offer the argument robot_description_file that should point to a launchfile loading the robot description.\nwhile the load_urxxx.launch files from the ur_description package contain a lot of arguments to change the robot model, this driver only forwards the kinematics_config parameter file. for further adaption please create your own load_urxxx.launch file that fits your application and pass this to the urxxx_bringup.launch files from this package.\nif you prefer decoupling loading the robot description and starting the driver, you can start the ur_control.launch launchfile directly after the robot_description has been uploaded to the parameter server.\ntroubleshooting\nthis section will cover some previously raised issues.\ni started everything, but i cannot control the robot.\nthe external control program node from the urcap is not running on the robot. make sure to create a program containing this node on the robot and start it. inside the ros terminal you should see the output robot ready to receive control commands.\nnote: when interacting with the teach pendant, or sending other primary programs to the robot, the program will be stopped. on the ros terminal you will see an output connection to robot dropped, waiting for new connection. in those cases, restart program execution (e.g. by pressing the play button on the tp, or calling rosservice call /ur_hardware_interface/dashboard/play as explained here and here).\nin general, make sure you've completed the following tasks:\ninstall and setup the external control urcap as explained above (also setup the ip address of the ros pc inside the urcap's installation.)\nstart the driver handing the ip of the robot as explained in the quickstart\nload and start the previously generated program on the tp.\nwhen starting the program on the tp, i get an error \"the connection to the remote pc could not be established\"\nmake sure, the ip address setup is correct, as described in the setup guides (cb3 robots, e-series robots)\nnote: this error can also show up, when the ros driver is not running.\nwhen starting the program on the tp, i get a c207a0 error.\nnote: with the current driver version this issue can only happen when the fieldbus is enabled after the ros driver has been started. otherwise you will run into #204 when starting the driver with an enabled ethernet/ip fieldbus.\nmost probably, the ethernet/ip fieldbus is enabled in the robot's installation. if your setup includes an ethernet/ip fieldbus (note: ethernet/ip != ethernet), make sure that it is connected properly. in the ethernet/ip fieldbus installation screen (e-series: installation > fieldbus > ethernet/ip, cb3: installation > ethernet/ip) you can select the action that is being executed upon a loss of ethernet/ip scanner connection. if you select \"none\", save installation and program, then no exception is raised when no connection to the fieldbus scanner can be established (note: this is only to get the external control running. you probably want to make sure that a connection to the fieldbus scanner can indeed be made). if you don't use ethernet/ip fieldbusses at all, you can disable it in the same installation screen.\nwhen starting the driver, it crashes with variable 'speed_slider_mask' is currently controlled by another rtde client\nprobably, you are running into #204. currently, this driver cannot be used together with an enabled ethernet/ip fieldbus. disable ethernet/ip to workaround this error. #204 contains a guide how to do this.\ni cannot get a realtime kernel running together with an nvidia graphics card\nthis is a known issue and unfortunately we don't have a solution for this. the nvidia kernel module seems to not compile with every kernel. we recommend to use a multi-machine ros setup in this situation where a realtime-system is running the robot driver and a separate machine is performing the computations requiring the graphics card.\nwhy can't the driver use the extracted calibration info on startup?\nthis is mainly because parameters are loaded onto the parameter server before any nodes are started.\nthe robot_description concept inside ros is not designed to be changed while a system is running. consumers of the urdf/robot_description (in terms of other ros nodes) will not update the model they have been loading initially. while technically the robot_description parameter could be altered during runtime and any node that is started afterwards would see the updated model, this would lead to an inconsistent application state (as some nodes will use the old model, while others use the updated one). in other words: it's not the driver that needs/benefits from this calibrated urdf, it's the rest of the ros application and that will only see it if the calibrated version is present on the parameter server before nodes are started.\nadditionally: if the calibration stored on the ros side doesn't match the one of the robot controller, there's a good chance there is a reason for this and it would be better to make updating it a conscious decision by a human (as the driver would not know when updating the model would be convenient or safe). having to run the calibration extraction/transformation as a separate step makes this possible and doesn't hide this step from the end user.\ncan this driver be used inside a combined hardware interface?\nyes, this is possible. however, if used inside a combined hw interface we recommend to enable non-blocking read functinality.\ni sent raw script code to the robot but it is not executed\non the e-series the robot has to be in remote control mode to accept script code from an external source. this has to be switched from the teach-pendant.\nusing the dashboard doesn't work\non the e-series the robot has to be in remote control mode to accept certain calls on the dashboard server. see available dashboard commands for details.\npassthrough controllers: the robot does not fully reach trajectory points even though i have specified the path tolerance to be 0\nif you are using a control modes that forwards trajectories to the robot, currently the path tolerance is ignored. the corresponding interface on the robot and client-library level exists in the form of a \"blend radius\", but is not utilized by this ros driver. for more information see this issue.\ncan i use the cartesian controllers together with moveit!?\nnot directly, no. moveit! plans a cartesian path and then creates a joint trajectory out of that for execution, as the common interface to robot drivers in ros is the followjointtrajectory action.\nfor supporting cartesian controllers inside moveit! changes would have to be made to moveit! itself.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000124, "year": null}, {"Unnamed: 0": 1150, "autor": 130, "date": null, "content": "Camera Laser Calibration Tool\n\u6807\u5b9a\u76f8\u5173\u7684\u7406\u8bba\u63a8\u5bfc\u4ee5\u53ca\u62d3\u5c55\u77e5\u8bc6\u8bf7\u9605\u8bfb\u77e5\u4e4e\u4e13\u680f\u6587\u7ae0\uff1ahttps://zhuanlan.zhihu.com/p/137501892\n1. \u4ecb\u7ecd\n\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e ROS \u7684\u5355\u7ebf\u6fc0\u5149\u548c\u76f8\u673a\u5916\u53c2\u6570\u81ea\u52a8\u6807\u5b9a\u4ee3\u7801\u3002\u6807\u5b9a\u539f\u7406\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u76f8\u673a\u901a\u8fc7\u4e8c\u7ef4\u7801\u4f30\u8ba1\u6807\u5b9a\u677f\u5e73\u9762\u5728\u76f8\u673a\u5750\u6807\u7cfb\u4e0b\u7684\u5e73\u9762\u65b9\u7a0b\uff0c\u7531\u4e8e\u6fc0\u5149\u70b9\u4e91\u843d\u5728\u5e73\u9762\u4e0a\uff0c\u5c06\u70b9\u4e91\u901a\u8fc7\u6fc0\u5149\u5750\u6807\u7cfb\u5230\u76f8\u673a\u5750\u6807\u7cfb\u7684\u5916\u53c2\u6570 $ T_{cl} $ \u8f6c\u6362\u5230\u76f8\u673a\u5750\u6807\u7cfb\uff0c\u6784\u5efa\u70b9\u5230\u5e73\u9762\u7684\u8ddd\u79bb\u4f5c\u4e3a\u8bef\u5dee\uff0c\u4f7f\u7528\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u8fdb\u884c\u6c42\u89e3\u3002\n2. \u7279\u5f81\n\u652f\u6301\u591a April tag \u683c\u5f0f\u3002 \u53ef\u4ee5\u9009\u62e9\u591a apriltag \u7684\u6807\u5b9a\u677f ( kalibr_tag.pdf ) \u6216\u8005\u81ea\u5df1\u6253\u5370\u4e00\u4e2a apriltag \u505a\u6807\u5b9a\u677f(apriltag.pdf)\u3002\n\u652f\u6301\u591a\u79cd\u76f8\u673a\u6a21\u578b\u3002radial-tangential (radtan) : (distortion_coeffs: [k1 k2 r1 r2]); equidistant (equi):distortion_coeffs: [k1 k2 k3 k4]). More info please visit kalibr website.\n\u6fc0\u5149\u7ebf\u81ea\u52a8\u68c0\u6d4b\uff0c\u65e0\u987b\u624b\u6807\u3002\u4f1a\u81ea\u52a8\u63d0\u53d6\u6fc0\u5149\u7ebf\u4e2d\u843d\u5728\u6807\u5b9a\u677f\u4e0a\u7684\u6fc0\u5149\u70b9\uff0c\u524d\u63d0\u662f\u6807\u5b9a\u677f\u5728\u6fc0\u5149\u6b63\u524d\u65b9 120 \u5ea6\u8303\u56f4\u5185\uff0c\u5e76\u4e14\u6fc0\u5149\u524d\u65b9 2m \u8303\u56f4\u5185\u53ea\u4f1a\u5b58\u5728\u4e00\u4e2a\u8fde\u7eed\u7684\u76f4\u7ebf\u7ebf\u6bb5\uff0c\u6240\u4ee5\u8bf7\u4f60\u5728\u7a7a\u65f7\u7684\u5730\u65b9\u91c7\u96c6\u6570\u636e\uff0c\u4e0d\u7136\u6fc0\u5149\u6570\u636e\u4f1a\u63d0\u53d6\u9519\u8bef\u3002\n\u5229\u7528\u6807\u5b9a\u677f\u7684\u8fb9\u754c\u7ea6\u675f\uff0c\u6807\u5b9a\u7ed3\u679c\u66f4\u52a0\u51c6\u786e\u3002\u8fd9\u4e2a\u662f\u9690\u85cf\u7684\u529f\u80fd\uff0c\u6682\u65f6\u672a\u5bf9\u5916\u5f00\u653e\u63a5\u53e3\uff0c\u9700\u8981\u4f60\u4fee\u6539\u4ee3\u7801\u3002\n3. \u7f16\u8bd1\u65b9\u6cd5\nmkdir LaserCameraCal_ws\nmkdir ../src\ncd src\ngit clone https://github.com/MegviiRobot/CamLaserCalibraTool\ncd ..\ncatkin_make -DCMAKE_BUILD_TYPE=Release\n4. \u8fd0\u884c\n4.1 \u4eff\u771f\u6570\u636e\n**\u5f3a\u70c8\u5efa\u8bae: **\u5148\u7528\u4eff\u771f\u6570\u636e\u8bd5\u8bd5\u8fd9\u4e2a\u6807\u5b9a\u7cfb\u7edf\uff0c\u7cfb\u7edf\u7684\u53ef\u89c2\u6027\u95ee\u9898\u5728\u4eff\u771f\u4ee3\u7801\u91cc\u90fd\u80fd\u627e\u5230\u9a8c\u8bc1\uff0c\u8fd9\u6837\u5c31\u80fd\u6307\u5bfc\u4f60\u5982\u4f55\u91c7\u96c6\u6570\u636e\u3002\ncd LaserCameraCal_ws\nsource devel/setup.bash\nrosrun lasercamcal_ros simulation_lasercamcal_node\n\u7279\u522b\u5730\uff0c\u8bf7\u4ed4\u7ec6\u9605\u8bfb main/calibr_simulation.cpp \uff0c\u4fee\u6539\u6570\u636e\u7684\u751f\u6210\u89c2\u5bdf\u7cfb\u7edf\u7684\u53ef\u89c2\u6027\u3002\n4.2 \u5b9e\u9645\u6570\u636e\uff0c\u8fd0\u884c\u524d\u7684\u51c6\u5907\n\u8bf7\u914d\u7f6e\u597d config/calibra_config.yaml \u6587\u4ef6\uff0c\u91cc\u9762\u6709\u76f8\u673a\u6a21\u578b\u53c2\u6570\uff0crosbag \u6570\u636e\u5305\u7684\u540d\u5b57\u548c\u4fdd\u5b58\u8def\u5f84\uff0c\u76f8\u673a\u6a21\u578b\u4ee5\u53ca\u6807\u5b9a\u677f\u7684\u5c3a\u5bf8\u548c\u7c7b\u578b\u7b49\u7b49\u3002 \u5177\u4f53\u8bf7\u53c2\u8003\u5bf9\u5e94\u7684 config.yaml\u3002\n\u91c7\u96c6\u6fc0\u5149\u6570\u636e\u5236\u4f5c rosbag\uff0c\u8bf7\u5c06\u6807\u5b9a\u677f\u653e\u4e8e\u6fc0\u5149\u548c\u76f8\u673a\u524d\u65b9 0.3m ~ 1.5m \u5de6\u53f3\uff0c\u5145\u5206\u8fd0\u52a8\u6807\u5b9a\u677f\uff08\u5404\u4e2a\u8f74\uff0c\u5404\u4e2a\u89d2\u5ea6\uff0c\u5404\u4e2a\u8ddd\u79bb\u90fd\u5145\u5206\u8fd0\u52a8\uff09\u3002\n4.3 \u5148\u8fd0\u884c kalibr \u68c0\u6d4b\u56fe\u50cf\u4e8c\u7ef4\u7801\n\u4f1a\u628a\u76f8\u673a\u548c\u6807\u5b9a\u677f\u4e4b\u95f4\u7684\u59ff\u6001\u4fdd\u5b58\u6210\u4e00\u4e2a txt\uff0c\u7528\u4e8e\u540e\u7eed\u6807\u5b9a\u3002\ncd LaserCameraCal_ws\nsource devel/setup.bash\nroslaunch lasercamcal_ros kalibra_apriltag.launch\n4.4 \u518d\u8fd0\u884c\u6fc0\u5149\u89c6\u89c9\u5916\u53c2\u6570\u6807\u5b9a\u4ee3\u7801\n\u4f1a\u81ea\u52a8\u68c0\u6d4b\u6fc0\u5149\u5728\u6807\u5b9a\u677f\u4e0a\u7684\u7ebf\u6bb5\uff0c\u5e76\u5b8c\u6210\u6807\u5b9a\u548c\u4fdd\u5b58\u7ed3\u679c\u3002\nroslaunch lasercamcal_ros calibra_offline.launch\n\u6fc0\u5149\u7ebf\u6761\u7684\u81ea\u52a8\u83b7\u53d6\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5176\u4e2d\u7ea2\u8272\u90e8\u5206\u4e3a\u6fc0\u5149\u68c0\u6d4b\u5230\u7684\u6807\u5b9a\u677f\u7ebf\u6bb5\uff0c\u8bf7\u6ce8\u610f\u4fdd\u6301\u6fc0\u5149\u524d\u65b9\u662f\u7a7a\u65f7\u533a\u57df\uff1a\n4.5 \u9a8c\u8bc1\u7ed3\u679c\nroslaunch lasercamcal_ros debug.launch\nrosbag play data.bag\n\u8fd0\u884c debug \u5de5\u5177\uff0c\u770b\u6fc0\u5149\u5149\u6761\u548c\u56fe\u50cf\u662f\u5426\u5bf9\u9f50\u7684\u6bd4\u8f83\u597d\u3002\n5. TODO\n\u652f\u6301\u4fee\u6539\u76f8\u673a\u91c7\u6837\u95f4\u9694\uff1f\uff1f\uff1f \u8fd8\u662f\u8bf4\u91c7\u7528\u81ea\u52a8\u5224\u65ad\u4fe1\u606f\u71b5\uff1f\n6. Authors\nYijia He, if you have any question, please contact heyijia_2013 at 163 dot com\nXiZhen Xiao\nXiao Liu, his homepage\n7. Ref:\n2004, Qilong Zhang, Extrinsic Calibration of a Camera and Laser Range Finder (improves camera calibration).", "link": "https://github.com/MegviiRobot/CamLaserCalibraTool", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "camera laser calibration -----> tool !!! \n\u6807\u5b9a\u76f8\u5173\u7684\u7406\u8bba\u63a8\u5bfc\u4ee5\u53ca\u62d3\u5c55\u77e5\u8bc6\u8bf7\u9605\u8bfb\u77e5\u4e4e\u4e13\u680f\u6587\u7ae0\uff1ahttps://zhuanlan.zhihu.com/p/137501892\n1. \u4ecb\u7ecd\n\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e ros \u7684\u5355\u7ebf\u6fc0\u5149\u548c\u76f8\u673a\u5916\u53c2\u6570\u81ea\u52a8\u6807\u5b9a\u4ee3\u7801\u3002\u6807\u5b9a\u539f\u7406\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u76f8\u673a\u901a\u8fc7\u4e8c\u7ef4\u7801\u4f30\u8ba1\u6807\u5b9a\u677f\u5e73\u9762\u5728\u76f8\u673a\u5750\u6807\u7cfb\u4e0b\u7684\u5e73\u9762\u65b9\u7a0b\uff0c\u7531\u4e8e\u6fc0\u5149\u70b9\u4e91\u843d\u5728\u5e73\u9762\u4e0a\uff0c\u5c06\u70b9\u4e91\u901a\u8fc7\u6fc0\u5149\u5750\u6807\u7cfb\u5230\u76f8\u673a\u5750\u6807\u7cfb\u7684\u5916\u53c2\u6570 $ t_{cl} $ \u8f6c\u6362\u5230\u76f8\u673a\u5750\u6807\u7cfb\uff0c\u6784\u5efa\u70b9\u5230\u5e73\u9762\u7684\u8ddd\u79bb\u4f5c\u4e3a\u8bef\u5dee\uff0c\u4f7f\u7528\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u8fdb\u884c\u6c42\u89e3\u3002\n2. \u7279\u5f81\n\u652f\u6301\u591a april tag \u683c\u5f0f\u3002 \u53ef\u4ee5\u9009\u62e9\u591a apriltag \u7684\u6807\u5b9a\u677f ( kalibr_tag.pdf ) \u6216\u8005\u81ea\u5df1\u6253\u5370\u4e00\u4e2a apriltag \u505a\u6807\u5b9a\u677f(apriltag.pdf)\u3002\n\u652f\u6301\u591a\u79cd\u76f8\u673a\u6a21\u578b\u3002radial-tangential (radtan) : (distortion_coeffs: [k1 k2 r1 r2]); equidistant (equi):distortion_coeffs: [k1 k2 k3 k4]). more info please visit kalibr website.\n\u6fc0\u5149\u7ebf\u81ea\u52a8\u68c0\u6d4b\uff0c\u65e0\u987b\u624b\u6807\u3002\u4f1a\u81ea\u52a8\u63d0\u53d6\u6fc0\u5149\u7ebf\u4e2d\u843d\u5728\u6807\u5b9a\u677f\u4e0a\u7684\u6fc0\u5149\u70b9\uff0c\u524d\u63d0\u662f\u6807\u5b9a\u677f\u5728\u6fc0\u5149\u6b63\u524d\u65b9 120 \u5ea6\u8303\u56f4\u5185\uff0c\u5e76\u4e14\u6fc0\u5149\u524d\u65b9 2m \u8303\u56f4\u5185\u53ea\u4f1a\u5b58\u5728\u4e00\u4e2a\u8fde\u7eed\u7684\u76f4\u7ebf\u7ebf\u6bb5\uff0c\u6240\u4ee5\u8bf7\u4f60\u5728\u7a7a\u65f7\u7684\u5730\u65b9\u91c7\u96c6\u6570\u636e\uff0c\u4e0d\u7136\u6fc0\u5149\u6570\u636e\u4f1a\u63d0\u53d6\u9519\u8bef\u3002\n\u5229\u7528\u6807\u5b9a\u677f\u7684\u8fb9\u754c\u7ea6\u675f\uff0c\u6807\u5b9a\u7ed3\u679c\u66f4\u52a0\u51c6\u786e\u3002\u8fd9\u4e2a\u662f\u9690\u85cf\u7684\u529f\u80fd\uff0c\u6682\u65f6\u672a\u5bf9\u5916\u5f00\u653e\u63a5\u53e3\uff0c\u9700\u8981\u4f60\u4fee\u6539\u4ee3\u7801\u3002\n3. \u7f16\u8bd1\u65b9\u6cd5\nmkdir lasercameracal_ws\nmkdir ../src\ncd src\ngit clone https://github.com/megviirobot/camlasercalibratool\ncd ..\ncatkin_make -dcmake_build_type=release\n4. \u8fd0\u884c\n4.1 \u4eff\u771f\u6570\u636e\n**\u5f3a\u70c8\u5efa\u8bae: **\u5148\u7528\u4eff\u771f\u6570\u636e\u8bd5\u8bd5\u8fd9\u4e2a\u6807\u5b9a\u7cfb\u7edf\uff0c\u7cfb\u7edf\u7684\u53ef\u89c2\u6027\u95ee\u9898\u5728\u4eff\u771f\u4ee3\u7801\u91cc\u90fd\u80fd\u627e\u5230\u9a8c\u8bc1\uff0c\u8fd9\u6837\u5c31\u80fd\u6307\u5bfc\u4f60\u5982\u4f55\u91c7\u96c6\u6570\u636e\u3002\ncd lasercameracal_ws\nsource devel/setup.bash\nrosrun lasercamcal_ros simulation_lasercamcal_node\n\u7279\u522b\u5730\uff0c\u8bf7\u4ed4\u7ec6\u9605\u8bfb main/calibr_simulation.cpp \uff0c\u4fee\u6539\u6570\u636e\u7684\u751f\u6210\u89c2\u5bdf\u7cfb\u7edf\u7684\u53ef\u89c2\u6027\u3002\n4.2 \u5b9e\u9645\u6570\u636e\uff0c\u8fd0\u884c\u524d\u7684\u51c6\u5907\n\u8bf7\u914d\u7f6e\u597d config/calibra_config.yaml \u6587\u4ef6\uff0c\u91cc\u9762\u6709\u76f8\u673a\u6a21\u578b\u53c2\u6570\uff0crosbag \u6570\u636e\u5305\u7684\u540d\u5b57\u548c\u4fdd\u5b58\u8def\u5f84\uff0c\u76f8\u673a\u6a21\u578b\u4ee5\u53ca\u6807\u5b9a\u677f\u7684\u5c3a\u5bf8\u548c\u7c7b\u578b\u7b49\u7b49\u3002 \u5177\u4f53\u8bf7\u53c2\u8003\u5bf9\u5e94\u7684 config.yaml\u3002\n\u91c7\u96c6\u6fc0\u5149\u6570\u636e\u5236\u4f5c rosbag\uff0c\u8bf7\u5c06\u6807\u5b9a\u677f\u653e\u4e8e\u6fc0\u5149\u548c\u76f8\u673a\u524d\u65b9 0.3m ~ 1.5m \u5de6\u53f3\uff0c\u5145\u5206\u8fd0\u52a8\u6807\u5b9a\u677f\uff08\u5404\u4e2a\u8f74\uff0c\u5404\u4e2a\u89d2\u5ea6\uff0c\u5404\u4e2a\u8ddd\u79bb\u90fd\u5145\u5206\u8fd0\u52a8\uff09\u3002\n4.3 \u5148\u8fd0\u884c kalibr \u68c0\u6d4b\u56fe\u50cf\u4e8c\u7ef4\u7801\n\u4f1a\u628a\u76f8\u673a\u548c\u6807\u5b9a\u677f\u4e4b\u95f4\u7684\u59ff\u6001\u4fdd\u5b58\u6210\u4e00\u4e2a txt\uff0c\u7528\u4e8e\u540e\u7eed\u6807\u5b9a\u3002\ncd lasercameracal_ws\nsource devel/setup.bash\nroslaunch lasercamcal_ros kalibra_apriltag.launch\n4.4 \u518d\u8fd0\u884c\u6fc0\u5149\u89c6\u89c9\u5916\u53c2\u6570\u6807\u5b9a\u4ee3\u7801\n\u4f1a\u81ea\u52a8\u68c0\u6d4b\u6fc0\u5149\u5728\u6807\u5b9a\u677f\u4e0a\u7684\u7ebf\u6bb5\uff0c\u5e76\u5b8c\u6210\u6807\u5b9a\u548c\u4fdd\u5b58\u7ed3\u679c\u3002\nroslaunch lasercamcal_ros calibra_offline.launch\n\u6fc0\u5149\u7ebf\u6761\u7684\u81ea\u52a8\u83b7\u53d6\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u5176\u4e2d\u7ea2\u8272\u90e8\u5206\u4e3a\u6fc0\u5149\u68c0\u6d4b\u5230\u7684\u6807\u5b9a\u677f\u7ebf\u6bb5\uff0c\u8bf7\u6ce8\u610f\u4fdd\u6301\u6fc0\u5149\u524d\u65b9\u662f\u7a7a\u65f7\u533a\u57df\uff1a\n4.5 \u9a8c\u8bc1\u7ed3\u679c\nroslaunch lasercamcal_ros debug.launch\nrosbag play data.bag\n\u8fd0\u884c debug \u5de5\u5177\uff0c\u770b\u6fc0\u5149\u5149\u6761\u548c\u56fe\u50cf\u662f\u5426\u5bf9\u9f50\u7684\u6bd4\u8f83\u597d\u3002\n5. todo\n\u652f\u6301\u4fee\u6539\u76f8\u673a\u91c7\u6837\u95f4\u9694\uff1f\uff1f\uff1f \u8fd8\u662f\u8bf4\u91c7\u7528\u81ea\u52a8\u5224\u65ad\u4fe1\u606f\u71b5\uff1f\n6. authors\nyijia he, if you have any question, please contact heyijia_2013 at 163 dot com\nxizhen xiao\nxiao liu, his homepage\n7. ref:\n2004, qilong zhang, extrinsic calibration of a camera and laser range finder (improves camera calibration).", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000130, "year": null}, {"Unnamed: 0": 1160, "autor": 140, "date": null, "content": "IROS2020-paper-list\nThe 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2020) has been held on Oct 25 \u2013 Nov 25, not been held in-person. With the continued resurgence of COVID-19 within the State of Nevada, in particular Las Vegas, along with the city\u2019s prohibiting large or even moderately-sized public gatherings, it makes it an impossibility to hold the in-person event as originally planned.\nIROS is one of the largest and most impacting robotics research conferences worldwide. It brings an international community of researchers, educators and practitioners to explore the frontier of science and technology in intelligent robots and systems, and discuss the latest advancements in this fast growing and exciting field.\nIROS 2020 is free with access to every Technical Talk, Plenary and Keynote, over 60 Workshops and Tutorials, the Competitions, and includes publishing of accepted papers in the IROS Proceedings and IEEE XPlore.\nThis list is edited by PaopaoRobot, \u6ce1\u6ce1\u673a\u5668\u4eba, the Chinese academic nonprofit organization. Recently we will classify these papers by topics. Welcome to follow our github and our WeChat Public Platform Account ( paopaorobot_slam ). Of course, you could contact with Yvon Shong.\ndownload link title\n0018 ARAS: Ambiguity-Aware Robust Active SLAM Based on Multi-Hypothesis State and Map Estimations\n0025 Dynamic Attention-Based Visual Odometry\n0031 Max Orientation Coverage: Efficient Path Planning to Avoid Collisions in the CNC Milling of 3D Objects\n0039 Learning an Uncertainty-Aware Object Detector for Autonomous Driving\n0046 Look and Listen: A Multi-Modality Late Fusion Approach to Scene Classification for Autonomous Machines\n0047 Variable Stiffness Control with Strict Frequency Domain Constraints for Physical Human-Robot Interaction\n0054 Proactive Estimation of Occlusions and Scene Coverage for Planning Next Best Views in an Unstructured Representation\n0055 Static Characteristics of Fire Hose Actuators and Design of a Compliant Pneumatic Rotary Drive for Robotics\n0060 Domain Transfer for Semantic Segmentation of LiDAR Data Using Deep Neural Networks\n0063 LIO-SAM: Tightly-Coupled Lidar Inertial Odometry Via Smoothing and Mapping\n0069 Roadmap Subsampling for Changing Environments\n0070 Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization\n0071 PresSense: Passive Respiration Sensing Via Ambient WiFi Signals in Noisy Environments\n0072 OrcVIO: Object Residual Constrained Visual-Inertial Odometry\n0073 BioARS: Designing Adaptive and Reconfigurable Bionic Assembly Robotic System with Inchworm Modules\n0074 Segmentation-Based 4D Registration of Plants Point Clouds for Phenotyping\n0078 Self-Supervised Attention Learning for Depth and Ego-Motion Estimation\n0079 SwarmLab: A MATLAB Drone Swarm Simulator\n0082 Non-Overlapping RGB-D Camera Network Calibration with Monocular Visual Odometry\n0083 Adaptive Reliable Shortest Path in Gaussian Process Regulated Environment\n0089 Assessment of Soil Strength Using a Robotically Deployed and Retrieved Penetrometer\n0092 SaD-SLAM: A Visual SLAM Based on Semantic and Depth Information\n0094 Zero-Tuning Grinding Process Methodology of Cyber-Physical Robot System\n0095 ARPDR: An Accurate and Robust Pedestrian Dead Reckoning System for Indoor Localization on Handheld Smartphones\n0096 KLIEP-Based Density Ratio Estimation for Semantically Consistent Synthetic to Real Images Adaptation in Urban Traffic Scenes\n0097 Graph-Based Hierarchical Knowledge Representation for Robot Task Transfer from Virtual to Physical World\n0099 An RLS-Based Instantaneous Velocity Estimator for Extended Radar Tracking\n0100 Informative Path Planning for Gas Distribution Mapping in Cluttered Environments\n0101 DSSF-Net: Dual-Task Segmentation and Self-Supervised Fitting Network for End-To-End Lane Mark Detection\n0102 Single-Shot Panoptic Segmentation\n0103 Leveraging Stereo-Camera Data for Real-Time Dynamic Obstacle Detection and Tracking\n0106 Combining Compliance Control, CAD Based Localization, and a Multi-Modal Gripper for Rapid and Robust Programming of Assembly Tasks\n0108 Model-Free, Vision-Based Object Identification and Contact Force Estimation with a Hyper-Adaptive Robotic Gripper\n0109 Human-Robot Interaction in a Shared Augmented Reality Workspace\n0112 One-Shot Informed Robotic Visual Search in the Wild\n0119 KOVIS: Keypoint-Based Visual Servoing with Zero-Shot Sim-To-Real Transfer for Robotics Manipulation\n0122 Learning Local Planners for Human-Aware Navigation in Indoor Environments\n0123 Confidence Guided Stereo 3D Object Detection with Split Depth Estimation\n0124 Learning Hybrid Object Kinematics for Efficient Hierarchical Planning under Uncertainty\n0128 Human-Aware Robot Navigation by Long-Term Movement Prediction\n0130 A Human-Robot Interface Based on Surface Electroencephalographic Sensors\n0132 Pit30M: A Benchmark for Global Localization in the Age of Self-Driving Cars\n0135 LiTAMIN: LiDAR Based Tracking and MappINg by Stabilized ICP for Geometry Approximation with Normal Distributions\n0141 Autonomous Obstacle Avoidance for UAV Based on Fusion of Radar and Monocular Camera\n0143 Anomaly Detection for Autonomous Guided Vehicles Using Bayesian Surprise\n0144 RegionNet: Region-Feature-Enhanced 3D Scene Understanding Network with Dual Spatial-Aware Discriminative Loss\n0146 Richer Aggregated Features for Optical Flow Estimation with Edge-Aware Refinement\n0152 Quantitative Operator Strategy Comparisons across Human Supervisory Control Scenarios\n0153 Multi-Task Control for a Quadruped Robot with Changeable Leg Configuration\n0155 Assisted Mobile Robot Teleoperation with Intent-Aligned Trajectories Via Biased Incremental Action Sampling\n0157 DeepLiDARFlow: A Deep Learning Architecture for Scene Flow Estimation Using Monocular Camera and Sparse LiDAR\n0160 Design of an Underactuated Peristaltic Robot on Soft Terrain\n0161 Accurate, Low-Latency Visual Perception for Autonomous Racing: Challenges, Mechanisms, and Practical Solutions\n0162 Catch the Ball: Accurate High-Speed Motions for Mobile Manipulators Via Inverse Dynamics Learning\n0164 NBVC: A Benchmark for Depth Estimation from Narrow-Baseline Video Clips\n0166 On-Plate Localization and Mapping for an Inspection Robot using Ultrasonic Guided Waves: A Proof of Concept\n0170 Semantic Graph Based Place Recognition for 3D Point Clouds\n0173 Barometer-based Tactile Skin for Anthropomorphic Robot Hand\n0176 CalibRCNN: Calibrating Camera and LiDAR by Recurrent Convolutional Neural Network and Geometric Constraints\n0179 Mechanical Design and Preliminary Performance Evaluation of a Passive Arm-Support Exoskeleton\n0184 Augmented Reality User Interfaces for Heterogeneous Multirobot Control\n0187 A Robotic Gripper Design and Integrated Solution towards Tunnel Boring Construction Equipment\n0191 Autonomous RGBD-Based Industrial Staircase Localization from Tracked Robots\n0192 Robust MUSIC-Based Sound Source Localization in Reverberant and Echoic Environments\n0193 Optimal-Power Configurations for Hover Solutions in Mono-Spinners\n0195 Hypothesis-Driven Skill Discovery for Hierarchical Deep Reinforcement Learning\n0196 A modified Hybrid Reciprocal Velocity Obstacles approach for multi-robot motion planning without communication\n0198 Rapidly Adaptable Legged Robots Via Evolutionary Meta-Learning\n0200 Robots Can Defuse High-Intensity Conflict Situations\n0201 Indoor Scene Recognition in 3D\n0202 SolarSLAM: Battery-Free Loop Closure for Indoor Localisation\n0204 Inner-Approximation of Manipulable and Reachable Regions Using Bilinear Matrix Inequalities\n0205 SCALE-Net: Scalable Vehicle Trajectory Prediction Network under Random Number of Interacting Vehicles Via Edge-Enhanced Graph Convolutional Neural Network\n0206 Exceeding the Maximum Speed Limit of the Joint Angle for the Redundant Tendon-Driven Structures of Musculoskeletal Humanoids\n0207 Tool Shape Optimization through Backpropagation of Neural Network\n0208 PlaNet of the Bayesians: Reconsidering and Improving Deep Planning Network by Incorporating Bayesian Inference\n0209 Tell Me What This Is: Few-Shot Incremental Object Learning by a Robot\n0213 A Horse Inspired Eight-wheel Unmanned Ground Vehicle with Four-swing Arms\n0219 Simultaneous Estimation of Vehicle Position and Data Delays Using Gaussian Process Based Moving Horizon Estimation\n0220 Globally Optimal Consensus Maximization for Robust Visual Inertial Localization in Point and Line Map\n0223 Learning Hierarchical Behavior and Motion Planning for Autonomous Driving\n0224 A Bayesian-Based Controller for Snake Robot Locomotion in Unstructured Environments\n0226 Personalized Online Learning with Pseudo-Ground Truth\n0228 Improving Autonomous Rover Guidance in Round-Trip Missions Using Dynamic Cost Map\n0229 An External Stabilization Unit for High-Precision Applications of Robot Manipulators\n0230 Design and Implementation of a Pipeline Inspection Robot with Camera Image Compensation\n0231 Magnetically Actuated Pick-And-Place Operations of Cellular Micro-Rings for High-Speed Assembly of Micro-Scale Biological Tube\n0233 Exploration of Unknown Environments with a Tethered Mobile Robot\n0234 Robot-To-Robot Relative Pose Estimation Based on Semidefinite Relaxation Optimization\n0235 Robust Pedestrian Tracking in Crowd Scenarios Using an Adaptive GMM-Based Framework\n0236 Variable In-Hand Manipulations for Tactile-Driven Robot Hand Via CNN-LSTM\n0238 SMA Actuated Low-Weight Bio-Inspired Claws for Grasping and Perching Using Flapping Wing Aerial Systems\n0240 Reconstruction of 3D Flight Trajectories from Ad-Hoc Camera Networks\n0241 3D Odor Source Localization Using a Micro Aerial Vehicle: System Design and Performance Evaluation\n0244 Set-Membership Extrinsic Calibration of a 3D LiDAR and a Camera\n0248 BARK: Open Behavior Benchmarking in Multi-Agent Environments\n0249 LiDAR Panoptic Segmentation for Autonomous Driving\n0250 Learning and Sequencing of Object-Centric Manipulation Skills for Industrial Tasks\n0257 Ground Texture Based Localization: Do We Need to Detect Keypoints?\n0261 Diabolo Orientation Stabilization by Learning Predictive Model for Unstable Unknown-Dynamics Juggling Manipulation\n0263 Sample-Efficient Learning for Industrial Assembly Using Qgraph-Bounded DDPG\n0264 ROSflight: A Lean Open-Source Research Autopilot\n0265 A Model-Based Approach to Acoustic Reflector Localization with a Robotic Platform\n0267 Explainable and Efficient Sequential Correlation Network for 3D Single Person Concurrent Activity Detection\n0268 Guaranteed Parameter Estimation of Hunt-Crossley Model with Chebyshev Polynomial Approximation for Teleoperation\n0272 Software Development Framework for Cooperating Robots with High-Level Mission Specification\n0277 Learning to Collide: An Adaptive Safety-Critical Scenarios Generating Method\n0278 No Map, No Problem: A Local Sensing Approach for Navigation in Human-Made Spaces Using Signs\n0281 A Learning-based Robotic Bin-picking with Flexibly Customizable Grasping Conditions\n0284 An Augmented Reality Human-Robot Physical Collaboration Interface Design for Shared, Large-Scale, Labour-Intensive Manufacturing Tasks\n0287 Motion Planning for Heterogeneous Unmanned Systems under Partial Observation from UAV\n0289 TTR-Based Reward for Reinforcement Learning with Implicit Model Priors\n0290 ETRI-Activity3D: A Large-Scale RGB-D Dataset for Robots to Recognize Daily Activities of the Elderly\n0292 A Flexible Dual-Core Optical Waveguide Sensor for Simultaneous and Continuous Measurement of Contact Force and Position\n0295 Multimodal Aggregation Approach for Memory Vision-Voice Indoor Navigation with Meta-Learning\n0297 DR^2Track: Towards Real-Time Visual Tracking for UAV Via Distractor Repressed Dynamic Regression\n0301 Robust Real-Time Monitoring of Human Task Advancement for Collaborative Robotics Applications\n0304 Virtual Reality for Robots\n0309 Design of a Linear Gravity Compensator for a Prismatic Joint\n0311 Applications of Stretch Reflex for the Upper Limb of Musculoskeletal Humanoids: Protective Behavior, Postural Stability, and Active Induction\n0313 Practical Verification of Neural Network Enabled State Estimation System for Robotics\n0314 A Control Scheme for Haptic Inspection and Partial Modification of Kinematic Behaviors\n0315 Identification of Dynamic Parameters for Rigid Robots Based on Polynomial Approximation\n0317 SpoxelNet: Spherical Voxel-Based Deep Place Recognition for 3D Point Clouds of Crowded Indoor Spaces\n0323 Multi-Instance Aware Localization for End-To-End Imitation Learning\n0326 Cooperative Simultaneous Tracking and Jamming for Disabling a Rogue Drone\n0329 Human-Robot Trust Assessment Using Motion Tracking & Galvanic Skin Response\n0330 Simultaneous Planning for Item Picking and Placing by Deep Reinforcement Learning\n0334 Grasping Detection Network with Uncertainty Estimation for Confidence-Driven Semi-Supervised Domain Adaptation\n0335 3D Gaze Estimation for Head-Mounted Devices based on Visual Saliency\n0336 Learning Hierarchical Acquisition Functions for Bayesian Optimization\n0338 Reinforcement Learning in Latent Action Sequence Space\n0339 Automatic Synthesis of Human Motion from Temporal Logic Specifications\n0342 UAV-AdNet: Unsupervised Anomaly Detection Using Deep Neural Networks for Aerial Surveillance\n0348 Unsupervised Domain Adaptation for Transferring Plant Classification Systems to New Field Environments, Crops, and Robots\n0355 On-Chip Integration of Ultra-Thin Glass Cantilever for Physical Property Measurement Activated by Femtosecond Laser Impulse\n0356 Learning the sense of touch in simulation: a sim-to-real strategy for vision-based tactile sensing\n0358 Abductive Recognition of Context-Dependent Utterances in Human-Robot Interaction\n0359 From Points to Planes - Adding Planar Constraints to Monocular SLAM Factor Graphs\n0363 Enhanced Tracking Wall: A Real-Time Computing Method for Needle Injection on Haptic Simulators\n0364 Occlusion-Robust MVO: Multimotion Estimation through Occlusion Via Motion Closure\n0366 Spectral-GANs for High-Resolution 3D Point-Cloud Generation\n0368 Vision-Based Proprioceptive Sensing: Tip Position Estimation for a Soft Inflatable Bellow Actuator\n0369 Model Predictive Position and Force Trajectory Tracking Control for Robot-Environment Interaction\n0373 CoBigICP: Robust and Precise Point Set Registration Using Correntropy Metrics and Bidirectional Correspondence\n0378 Walking Human Trajectory Models and Their Application to Humanoid Robot Locomotion\n0382 Plug-And-Play SLAM: A Unified SLAM Architecture for Modularity and Ease of Use\n0383 Acquiring Mechanical Knowledge from 3D Point Clouds\n0384 A Visuo-Haptic Guidance Interface for the Mobile Collaborative Robotic Assistant (MOCA)\n0386 Robot Navigation in Crowded Environments Using Deep Reinforcement Learning\n0389 Robust Task and Motion Planning for Long-Horizon Problems\n0390 Tightly-Coupled Fusion of Global Positional Measurements in Optimization-Based Visual-Inertial Odometry\n0393 LegoBot: Automated Planning for Coordinated Multi-Robot Assembly of LEGO Structures\n0398 Progressive Automation of Periodic Tasks on Planar Surfaces of Unknown Pose with Hybrid Force/position Control\n0400 Smart Speaker vs. Social Robot in a Case of Hotel Room\n0403 EU Long-Term Dataset with Multiple Sensors for Autonomous Driving\n0404 Speed and Memory Efficient Dense RGB-D SLAM in Dynamic Scenes\n0405 Hand-Object Contact Force Synthesis for Manipulating Objects by Exploiting Environment\n0407 Hierarchical Reinforcement Learning Method for Autonomous Vehicle Behavior Planning\n0408 Automatic Lane Change Maneuver in Dynamic Environment Using Model Predictive Control Method\n0414 LaNoising: A Data-Driven Approach for 903nm ToF LiDAR Performance Modeling under Fog\n0415 On Parameter Estimation of Flexible Space Manipulator Systems\n0416 The SPIR: An Autonomous Underwater Robot for Bridge Pile Cleaning and Condition Assessment\n0417 Rapid Autonomous Semantic Mapping\n0418 Learning Soft Robotic Assembly Strategies from Successful and Failed Demonstrations\n0419 Mapping Thigh Motion to Knee Motion: Implications for Motion Planning of Active Prosthetic Knees\n0421 Comparison between Stationary and Crawling Multi-Arm Robotics for In-Space Assembly\n0425 Toward Analytical Modeling and Evaluation of Curvature-Dependent Distributed Friction Force in Tendon-Driven Continuum Manipulators\n0428 End-to-end Contextual Perception and Prediction with Interaction Transformer\n0432 Operational Space Formulation and Inverse Kinematics for an Arm Exoskeleton with Scapula Rotation\n0433 A Hamilton-Jacobi Formulation for Optimal Coordination of Heterogeneous Multiple Vehicle Systems\n0436 Human Gait Phase Recognition Using a Hidden Markov Model Framework\n0443 DaVinciNet: Joint Prediction of Motion and Surgical State in Robot-Assisted Surgery\n0455 Deep Imitation Learning of Sequential Fabric Smoothing from an Algorithmic Supervisor\n0457 Visual-Inertial-Wheel Odometry with Online Calibration\n0458 Lifelong Update of Semantic Maps in Dynamic Environments\n0460 Ultra Low-Cost Printable Folding Robots\n0463 Deep Keypoint-Based Camera Pose Estimation with Geometric Constraints\n0464 Generating New Lower Abstract Task Operator Using Grid-TLI\n0471 SpCoMapGAN: Spatial Concept Formation-Based Semantic Mapping with Generative Adversarial Networks\n0474 Applying Surface Normal Information in Drivable Area and Road Anomaly Detection for Ground Mobile Robots\n0476 Few-Shot Relation Learning with Attention for EEG-Based Motor Imagery Classification\n0478 Robot Sound Interpretation: Combining Sight and Sound in Learning-Based Control\n0479 Development of a Passive Skid for Multicopter Landing on Rough Terrain\n0480 Touch the Wind: Simultaneous Airflow, Drag and Interaction Sensing on a Multirotor\n0486 Online Localization with Imprecise Floor Space Maps Using Stochastic Gradient Descent\n0487 Wiping 3D-Objects Using Deep Learning Model Based on Image/Force/Joint Information\n0492 Fast LTL-Based Flexible Planning for Dual-Arm Manipulation\n0493 Multi-Robot Coordination with Agent-Server Architecture for Autonomous Navigation in Partially Unknown Environments\n0496 Perception-Aware Path Planning for UAVs Using Semantic Segmentation\n0500 L2B: Learning to Balance the Safety-Efficiency Trade-Off in Interactive Crowd-Aware Robot Navigation\n0504 UnRectDepthNet: Self-Supervised Monocular Depth Estimation Using a Generic Framework for Handling Common Camera Distortion Models\n0505 PC-NBV: A Point Cloud Based Deep Network for Efficient Next Best View Planning\n0508 Spatio-Temporal Ultrasonic Dataset: Learning Driving from Spatial and Temporal Ultrasonic Cues\n0511 Invisible Marker: Automatic Annotation of Segmentation Masks for Object Manipulation\n0512 A Comprehensive Trajectory Planner for a Person-Following ATV\n0513 Configuration Space Decomposition for Learning-Based Collision Checking in High-DOF Robots\n0516 A Novel Portable Cell Sonoporation Device Based on Open-Source Acoustofluidics\n0519 A Minimalistic Hyper Flexible Manipulator: Modeling and Control\n0523 A Two-Stage Automatic Latching System for the USVs Charging in Disturbed Berth\n0526 The Application of Navigation Technology for the Medical Assistive Devices Based on Aruco Recognition Technology\n0530 Physics-Based Dexterous Manipulations with Estimated Hand Poses and Residual Reinforcement Learning\n0532 A Time Optimal Reactive Collision Avoidance Method for UAVs Based on a Modified Collision Cone Approach\n0538 Computationally Efficient Obstacle Avoidance Trajectory Planner for UAVs Based on Heuristic Angular Search Method\n0544 UST: Unifying Spatio-Temporal Context for Trajectory Prediction in Autonomous Driving\n0552 DMLO: Deep Matching LiDAR Odometry\n0557 Latent Space Roadmap for Visual Action Planning of Deformable and Rigid Object Manipulation\n0558 Learning Visuomotor Policies for Aerial Navigation Using Cross-Modal Representations\n0559 A Neural Primitive Model with Sensorimotor Coordination for Dynamic Quadruped Locomotion with Malfunction Compensation\n0560 DenseFusion: Large-Scale Online Dense Pointcloud and DSM Mapping for UAVs\n0562 A Learning-Driven Framework with Spatial Optimization for Surgical Suture Thread Reconstruction and Autonomous Grasping under Multiple Topologies and Environmental Noises\n0566 OceanVoy: A Hybrid Energy Planning System for Autonomous Sailboat\n0567 Pattern Analysis and Parameters Optimization of Dynamic Movement Primitives for Learning Unknown Trajectories\n0568 Towards Understanding and Inferring the Crowd: Guided Second Order Attention Networks and Re-Identification for Multi-Object Tracking\n0572 Distributed Near-Optimal Multi-Robots Coordination in Heterogeneous Task Allocation\n0573 Dynamic Assistance for Human Balancing with Inertia of a Wearable Robotic Appendage\n0575 A Real-Time Unscented Kalman Filter on Manifolds for Challenging AUV Navigation\n0576 Indirect Object-To-Robot Pose Estimation from an External Monocular RGB Camera\n0581 A Causal Approach to Tool Affordance Learning\n0583 SeqSphereVLAD: Sequence Matching Enhanced Orientation-Invariant Place Recognition\n0589 Active 6D Multi-Object Pose Estimation in Cluttered Scenarios with Deep Reinforcement Learning\n0592 Continuous Tension Validation for Cable-Driven Parallel Robots\n0593 Predicting the Human Behaviour in Human-Robot Co-Assemblies: An Approach Based on Suffix Trees\n0596 Latent Replay for Real-Time Continual Learning\n0600 The Application of a Flexible Leader-Follower Control Algorithm to Different Mobile Autonomous Robots\n0614 Examination of Screen-Indicated Methods of Gait Training System with Real-Time Audiovisual Feedback Function of Ground Reaction Force\n0619 GOSMatch: Graph-of-Semantics Matching for Detecting Loop Closures in 3D LiDAR Data\n0620 Design, Analysis and Preliminary Validation of a 3-DOF Rotational Inertia Generator\n0621 Experiments on Whole-Body Control of a Dual-Arm Mobile Robot with the Set-Based Task-Priority Inverse Kinematics Algorithm\n0624 UAV Coverage Path Planning under Varying Power Constraints Using Deep Reinforcement Learning\n0625 A Model for Optimising the Size of Climbing Robots for Navigating Truss Structures\n0626 Reinforcement Learning-Based Hierarchical Control for Path Following of a Salamander-Like Robot\n0629 An Obstacle-crossing Strategy Based on the Fast Self-reconfiguration for Modular Sphere Robots\n0631 Online Weight-Adaptive Nonlinear Model Predictive Control\n0634 FlowControl: Optical Flow Based Visual Servoing\n0635 Geometrical Interpretation and Detection of Multiple Task Conflicts using a Coordinate Invariant Index\n0638 Laser2Vec: Similarity-Based Retrieval for Robotic Perception Data\n0641 Self-Adapting Recurrent Models for Object Pushing from Learning in Simulation\n0642 A Passivity-Based Bilateral Teleoperation Architecture using Distributed Nonlinear Model Predictive Control\n0644 A Theory of Fermat Paths for 3D Imaging Sonar Reconstruction\n0646 Lane Marking Verification for High Definition Map Maintenance Using Crowdsourced Images\n0652 TASC: Teammate Algorithm for Shared Cooperation\n0655 A Frequency-Dependent Impedance Controller for an Active-Macro/passive-Mini Robotic System\n0656 Robust and Efficient Post-Processing for Video Object Detection\n0659 Topology-Aware Self-Organizing Maps for Robotic Information Gathering\n0660 Gaze by Semi-Virtual Robotic Heads: Effects of Eye and Head Motion\n0665 Using Diverse Neural Networks for Safer Human Pose Estimation: Towards Making Neural Networks Know When They Don\u00ef\u00bft Know\n0668 Online Gain Setting Method for Path Tracking Using CMA-ES: Application to Off-Road Mobile Robot Control\n0669 Task Planning with Belief Behavior Trees\n0672 Anytime Kinodynamic Motion Planning Using Region-Guided Search\n0674 Robust Gait Design Insights from Studying a Compass Gait Biped with Foot Slipping\n0675 Completeness Seeking Probabilistic Coverage Estimation Using Uncertain State Estimates\n0679 IMU-Based Parameter Identification and Position Estimation in Twisted String Actuators\n0681 Laminar Jamming Flexure Joints for the Development of Variable Stiffness Robot Grippers and Hands\n0682 EDAN - an EMG-Controlled Daily Assistant to Help People with Physical Disabilities\n0683 Residual Pose: A Decoupled Approach for Depth-Based 3D Human Pose Estimation\n0685 The VCU-RVI Benchmark: Evaluating Visual Inertial Odometry for Indoor Navigation Applications with an RGB-D Camera\n0686 Voxel-Based Representation Learning for Place Recognition Based on 3D Point Clouds\n0687 A POMDP Treatment of Vehicle-Pedestrian Interaction: Implicit Coordination Via Uncertainty-Aware Planning\n0689 Analysis and Transfer of Human Movement Manipulability in Industry-Like Activities\n0694 A Collision-Resilient Aerial Vehicle with Icosahedron Tensegrity Structure\n0695 Category-Level 3D Non-Rigid Registration from Single-View RGB Images\n0697 An Implementation of the Adaptive Neuro-Fuzzy Inference System (ANFIS) for Odor Source Localization\n0701 Representation and Experience-Based Learning of Explainable Models for Robot Action Execution\n0702 Provably Safe Trajectory Optimization in the Presence of Uncertain Convex Obstacles\n0704 The Marathon 2: A Navigation System\n0707 Learning State-Dependent Losses for Inverse Dynamics Learning\n0709 Learning Your Way without Map or Compass: Panoramic Target Driven Visual Navigation\n0710 In-Flight Range Optimization of Multicopters Using Multivariable Extremum Seeking with Adaptive Step Size\n0711 Inferring Spatial Uncertainty in Object Detection\n0714 Deep Depth Estimation from Visual-Inertial SLAM\n0715 LAVAPilot: Lightweight UAVTrajectory Planner with Situational Awarenessfor Embedded Autonomy to Track and Locate Radio-Tags\n0716 Differential Image Based Robot to MRI Scanner Registration with Active Fiducial Markers for an MRI-Guided Robotic Catheter System\n0717 Acoustic Collision Detection and Localization for Robot Manipulators\n0718 Regulation of 2D Arm Stability against Unstable, Damping-Defined Environments in Physical Human-Robot Interaction\n0719 Detecting Usable Planar Regions for Legged Robot Locomotion\n0720 Reinforced Grounded Action Transformation for Sim-To-Real Transfer\n0721 Monocular Visual Shape Tracking and Servoing for Isometrically Deforming Objects\n0722 Probabilistic Multi-Modal Trajectory Prediction with Lane Attention for Autonomous Vehicles\n0724 Improving Disturbance Rejection and Dynamics of Cable Driven Parallel Robots with On-Board Propellers\n0728 Pi-Map: A Decision-Based Sensor Fusion with Global Optimization for Indoor Mapping\n0731 Learning the Latent Space of Robot Dynamics for Cutting Interaction Inference\n0732 Nonlinear Balance Control of an Unmanned Bicycle: Design and Experiments\n0739 Heterogeneous Vehicle Routing and Teaming with Gaussian Distributed Energy Uncertainty\n0740 Decentralised Self-Organising Maps for Multi-Robot Information Gathering\n0745 On a Videoing Control System Based on Object Detection and Tracking\n0750 Learning-Based Optimization Algorithms Combining Force Control Strategies for Peg-In-Hole Assembly\n0751 Fingertip Non-Contact Optoacoustic Sensor for Near-Distance Ranging and Thickness Differentiation for Robotic Grasping\n0752 A Momentum-Based Foot Placement Strategy for Stable Postural Control of Robotic Spring-Mass Running with Point Feet\n0753 Distributed Model Predictive Control for UAVs Collaborative Payload Transport\n0754 Monocular Camera Localization in Prior LiDAR Maps with 2D-3D Line Correspondences\n0755 Cleaning Robot Operation Decision Based on Causal Reasoning and Attribute Learning\n0764 A Thermoplastic Elastomer Belt Based Robotic Gripper\n0767 Interacting Multiple Model Navigation System for Quadrotor Micro Aerial Vehicles Subject to Rotor Drag\n0772 Meta Learning with Differentiable Closed-Form Solver for Fast Video Object Segmentation\n0774 Active Alignment Control-Based LED Communication for Underwater Robots\n0775 Semantic Trajectory Planning for Long-Distant Unmanned Aerial Vehicle Navigation in Urban Environments\n0777 High-Speed Catching by Multi-Vision Robot Hand\n0778 Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning on Graphs\n0781 Introduction to 7-DoF CoSMo-Arm : High Torque Density Manipulator based on CoSMoA and E-CoSMo\n0782 Learning-Based Controller Optimization for Repetitive Robotic Tasks\n0784 DUI-VIO: Depth Uncertainty Incorporated Visual Inertial Odometrybased on an RGB-D Camera\n0786 Wind and the City: Utilizing UAV-Based In-Situ Measurements for Estimating Urban Wind Fields\n0789 A Bottom-Up Framework for Construction of Structured Semantic 3D Scene Graph\n0790 Bi-Modal Hemispherical Sensors for Dynamic Locomotion and Manipulation\n0791 Underwater Monocular Image Depth Estimation Using Single-Beam Echosounder\n0792 TP-TIO: A Robust Thermal-Inertial Odometry with Deep ThermalPoint\n0795 Microdrone-Equipped Mobile Crawler Robot System, DIR-3, for High-Step Climbing and High-Place Inspection\n0796 Risk Vector-based Near miss Obstacle Avoidance for Autonomous Surface Vehicles\n0797 Sampling-Based Search for a Semi-Cooperative Target\n0802 Endoscopic Navigation Based on Three-Dimensional Structure Registration\n0805 Robotic Understanding of Spatial Relationships Using Neural-Logic Learning\n0807 Behaviorally Diverse Traffic Simulation via Reinforcement Learning\n0808 Cascaded Non-Local Neural Network for Point Cloud Semantic Segmentation\n0809 Fusing Concurrent Orthogonal Wide-Aperture Sonar Images for Dense Underwater 3D Reconstruction\n0814 Path Planning for Nonholonomic Multiple Mobile Robot System with Applications to Robotic Autonomous Luggage Trolley Collection at Airports\n0823 Adaptive Gait Pattern Generation of a Powered Exoskeleton by Iterative Learning of Human Behavior\n0828 A Deep Learning Based End-To-End Locomotion Mode Detection Method for Lower Limb Wearable Robot Control\n0835 Leveraging Planar Regularities for Point Line Visual-Inertial Odometry\n0837 High-Speed Hitting Grasping with Magripper, a Highly Backdrivable Gripper Using Magnetic Gear and Plastic Deformation Control\n0840 A Novel and Controllable Cell-Robot in Real Vascular Network for Target Tumor Therapy\n0844 Faster Healthcare Time Series Classification for Boosting Mortality Early Warning System\n0845 DiPE: Deeper into Photometric Errors for Unsupervised Learning of Depth and Ego-Motion from Monocular Videos\n0849 Clothoid-Based Moving Formation Control Using Virtual Structures\n0851 Water Based Magnification of Capacitive Proximity Sensors: Water Containers As Passive Human Detectors\n0852 Adaptive Robot-Assisted Feeding: An Online Learning Framework for Acquiring Previously Unseen Food Items\n0855 Remove, Then Revert: Static Point Cloud Map Construction Using Multiresolution Range Images\n0857 FreeBOT: A Freeform Modular Self-Reconfigurable Robot with Arbitrary Connection Point - Design and Implementation\n0865 A Target Tracking and Positioning Framework for Video Satellites Based on SLAM\n0867 360\u00c2\u00b0 Depth Estimation from Multiple Fisheye Images with Origami Crown Representation of Icosahedron\n0869 Augmented Memory for Correlation Filters in Real-Time UAV Tracking\n0870 Batch Normalization Masked Sparse Autoencoder for Robotic Grasping Detection\n0874 ROS-Lite: ROS Framework for NoC-Based Embedded Many-Core Platform\n0876 Utilizing Sacrificial Molding for Embedding Motion Controlling Endostructures in Soft Pneumatic Actuators\n0879 Incorporating Spatial Constraints into a Bayesian Tracking Framework for Improved Localisation in Agricultural Environments\n0888 SSP: Single Shot Future Trajectory Prediction\n0891 Stir to Pour: Efficient Calibration of Liquid Properties for Pouring Actions\n0892 Drive-Train Design in JAXON3-P and Realization of Jump Motions: Impact Mitigation and Force Control Performance for Dynamic Motions\n0895 Autonomous Task Planning and Situation Awareness in Robotic Surgery\n0896 Subsurface Sampling Robot for Time-Limited Asteroid Exploration\n0897 A Variable Impedance Control Strategy for Object Manipulation Considering Non-Rigid Grasp\n0898 Toward Hierarchical Self-Supervised Monocular Absolute Depth Estimation for Autonomous Driving Applications\n0900 Fruit Quality Control by Surface Analysis Using a Bio-Inspired Soft Tactile Sensor\n0902 AVP-SLAM: Semantic Visual Mapping and Localization for Autonomous Vehicles in the Parking Lot\n0904 Robust Internal Model Control for Motor Systems Based on Sliding Mode Technique and Extended State Observer\n0905 Organizing the Internet of Robotic Things: The Effect of Organization Structure on Users' Evaluation and Compliance Toward IoRT Service Platform\n0907 Locomotion Performance of a Configurable Paddle-Wheel Robot Over Dry Sandy Terrain\n0908 Can I Lift It? Humanoid Robot Reasoning about the Feasibility of Lifting a Heavy Box with Unknown Physical Properties\n0909 Adaptability Preserving Domain Decomposition for Stabilizing Sim2Real Reinforcement Learning\n0912 Robot-Assisted Ultrasound-Guided Biopsy on MR-Detected Breast Lesions\n0913 Tactile Event Based Grasping Algorithm Using Memorized Triggers and Mechanoreceptive Sensors\n0914 An Augmented Reality Interaction Interface for Autonomous Drone\n0919 Learning an Overlap-Based Observation Model for 3D LiDAR Localization\n0931 Automatic Targetless Extrinsic Calibration of Multiple 3D LiDARs and Radars\n0934 HD Map Change Detection with Cross-Domain Deep Metric Learning\n0938 Learning Agile Locomotion Via Adversarial Training\n0939 Path Planning for Mobile Manipulators under Nonholonomic and Task Constraints\n0942 Long-Reach Compact Robotic Arm with LMPA Joints for Monitoring of Reactor Interior\n0943 The Newer College Dataset Handheld LiDAR, Inertial and Vision with Ground Truth\n0944 Automatic Failure Recovery and Re-Initialization for Online UAV Tracking with Joint Scale and Aspect Ratio Optimization\n0947 Distributed Reinforcement Learning of Targeted Grasping with Active Vision for Mobile Manipulators\n0948 Vision-Based Gesture Recognition in Human-Robot Teams Using Synthetic Data\n0954 Joints-Space Metrics for Automatic Robotic Surgical Gestures Classification\n0955 Probabilistic Qualitative Localization and Mapping\n0956 Soft Tissue Simulation Environment to Learn Manipulation Tasks in Autonomous Robotic Surgery\n0960 Adaptive Potential Scanning for a Tomographic Tactile Sensor with High Spatio-Temporal Resolution\n0962 Modelling Social Interaction between Humans and Service Robots in Large Public Spaces\n0963 A Novel Trajectory Optimization for Affine Systems: Beyond Convex-Concave Procedure\n0970 Control of Magnetically-Driven Screws in a Viscoelastic Medium\n0975 A Particle Filter Technique for Human Pose Estimation in Case of Occlusion Exploiting Holographic Human Model and Virtualized Environment\n0976 Dual-SLAM: A Framework for Robust Single Camera Navigation\n0979 Point Cloud Completion by Learning Shape Priors\n0981 Task Planning from Complex Natural Instructions by a Collocating Robot\n0986 AutoLay: Benchmarking amodal layout estimation for autonomous driving\n0987 Centroids Triplet Network and Temporally-Consistent Embeddings for In-Situ Object Recognition\n0988 Accurate Mapping and Planning for Autonomous Racing\n0989 Optimal Robot Motion Planning in Constrained Workspaces Using Reinforcement Learning\n0991 Next-Best-View Planning for Surface Reconstruction of Large-Scale 3D Environments with Multiple UAVs\n0994 Multi-UAV Coverage Path Planning for the Inspection of Large and Complex Structures\n0995 Coordinate-Free Isoline Tracking in Unknown 2-D Scalar Fields\n0998 Diagnose Like a Clinician: Third-Order Attention Guided Lesion Amplification Network for WCE Image Classification\n0999 Lightweight Multi-Robot Communication Protocols for Information Synchronization\n1000 Hybrid Aerial-Ground Locomotion with a Single Passive Wheel\n1002 Redundancy Resolution under Hard Joint Constraints: A Generalized Approach to Rank Updates\n1005 A Mobile Robot Hand-Arm Teleoperation System by Vision and IMU\n1006 Standard Deep Generative Models for Density Estimation in Configuration Spaces: A Study of Benefits, Limits and Challenges\n1007 Unilateral Constraints for Torque-Based Whole-Body Control\n1013 Distributed Motion Control for Multiple Connected Surface Vessels\n1014 Towards Gradient-Based Actuationof Magnetic Soft Robots Using a Six-Coil Electromagnetic System\n1015 MOZARD: Multi-Modal Localization for Autonomous Vehicles in Urban Outdoor Environments\n1017 A Soft Humanoid Hand with In-Finger Visual Perception\n1019 Affordance-Based Grasping and Manipulation in Real World Applications\n1020 Computing High-Quality Clutter Removal Solutions for Multiple Robots\n1022 To Ask or Not to Ask: A User Annoyance Aware Preference Elicitation Framework for Social Robots\n1023 Visual Task Progress Estimation with Appearance Invariant Embeddings for Robot Control and Planning\n1025 Formalization of Robot Skills with Descriptive and Operational Models\n1028 Video Depth Estimation by Fusing Flow-to-Depth Proposals\n1029 Towards Dynamic Transparency: Robust Interaction Force Tracking Using Multi-Sensory Control on an Arm Exoskeleton\n1031 Faster Than FAST: GPU-Accelerated Frontend for High-Speed VIO\n1037 Accurate and Robust Teach and Repeat Navigation by Visual Place Recognition: A CNN Approach\n1043 Aerial Transportation of Unknown Payloads: Adaptive Path Tracking for Quadrotors\n1044 Design and Modeling of a Parallel Shifted-Routing Cable-Driven Continuum Manipulator for Endometrial Regeneration Surgery\n1045 Learning Continuous Object Representations from Point Cloud Data\n1047 Deep Tactile Experience: Estimating Tactile Sensor Output from Depth Sensor Data\n1049 Online Velocity Constraint Adaptation for Safe and Efficient Human-Robot Workspace Sharing\n1050 Learning Domain Randomization Distributions for Training Robust Locomotion Policies\n1051 An Optimized Tilt Mechanism for a New Steady-Hand Eye Robot\n1052 Estimating Pedestrian Crossing States Based on Single 2D Body Pose\n1055 Hybrid Fluidic Actuation for a Foam-Based Soft Actuator\n1057 Unsupervised Depth and Confidence Prediction from Monocular Images Using Bayesian Inference\n1060 CLOCs: Camera-LiDAR Object Candidates Fusion for 3D Object Detection\n1062 Polygonal Perception for Mobile Robots\n1064 Relational Graph Learning for Crowd Navigation\n1065 Learning High-Level Policies for Model Predictive Control\n1066 Autonomous Navigation in Complex Environments with Deep Multimodal Fusion Network\n1071 Kinematic Multibody Model Generation of Deformable Linear Objects from Point Clouds\n1074 Online Dynamic Motion Planning and Control for Wheeled Biped Robots\n1076 Bayesian Fusion of Unlabeled Vision and RF Data for Aerial Tracking of Ground Targets\n1077 Anatomical Mesh-Based Virtual Fixtures for Surgical Robots\n1078 Smart-Inspect: Micro Scale Localization and Classification of Smartphone Glass Defects for Industrial Automation\n1079 Automated Design and Construction of a Single Incision Laparoscopic System Adapted to the Required Workspace\n1080 Dense Decentralized Multi-Robot SLAM Based on Locally Consistent TSDF Submaps\n1081 Multiple Trajectory Prediction with Deep Temporal and Spatial Convolutional Neural Networks\n1083 Multi-Sparse Gaussian Process: Learning Based Semi-Parametric Control\n1085 Domain Adaptation for Outdoor Robot Traversability Estimation from RGB Data with Safety-Preserving Loss\n1086 GndNet: Fast Ground Plane Estimation and Point Cloud Segmentation for Autonomous Vehicles\n1088 A Bio-Inspired Framework for Joint Angle Estimation from Non-Collocated Sensors in Tendon-Driven Systems\n1091 Real-Time Spatio-Temporal LiDAR Point Cloud Compression\n1094 Solving Large-Scale Stochastic Orienteering Problems with Aggregation\n1102 An Energy-Based Approach for the Integration of Collaborative Redundant Robots in Restricted Work Environments\n1103 Wireless Electronic Skin with Integrated Pressure and Optical Proximity Sensing\n1105 End-to-End 3D Point Cloud Learning for Registration Task Using Virtual Correspondences\n1108 Safe Planning for Self-Driving Via Adaptive Constrained ILQR\n1109 Experimental flights of adaptive patterns for cloud exploration with UAVs\n1114 Learning to Live Life on the Edge: Online Learning for Data-Efficient Tactile Contour Following\n1116 Supportive Actions for Manipulation in Human-Robot Coworker Teams\n1118 Model-Based Quality-Diversity Search for Efficient Robot Learning\n1119 Autonomous Planning for Multiple Aerial Cinematographers\n1120 Supervised Autoencoder Joint Learning on Heterogeneous Tactile Sensory Data: Improving Material Classification Performance\n1121 Magnetized Cell-Robot Propelled by Magnetic Field for Cancer Killing\n1122 Basic Implementation of FPGA-GPU Dual SoC Hybrid Architecture for Low-Latency Multi-DOF Robot Motion Control\n1123 Synchronization of Microphones Based on Rank Minimization of Warped Spectrum for Asynchronous Distributed Recording\n1126 Robotic Micromanipulation of Biological Cells with Friction Force-Based Rotation Control\n1130 Multi-Robot Joint Visual-Inertial Localization and 3-D Moving Object Tracking\n1132 Modeling a Social Placement Cost to Extend Navigation among Movable Obstacles (NAMO) Algorithms\n1134 An Earthworm-Like Soft Robot with Integration of Single Pneumatic Actuator and Cellular Structures for Peristaltic Motion\n1135 IDOL: A Framework for IMU-DVS Odometry Using Lines\n1136 This or That: The Effect of Robot's Deictic Expression on User's Perception\n1137 Modeling and Control of a Hybrid Wheeled Jumping Robot\n1138 An In-Pipe Manipulator for Contamination-Less Rehabilitation of Water Distribution Pipes\n1140 Transferability in an 8-DoF Parallel Robot with a Configurable Platform\n1141 Optimizing Dynamic Trajectories for Robustness to Disturbances Using Polytopic Projections\n1142 Inter-Robot Range Measurements in Pose Graph Optimization\n1145 Robots versus Speakers: What Type of Central Smart Home Interface Consumers Prefer?\n1146 Reinforcement Co-Learning of Deep and Spiking Neural Networks for Energy-Efficient Mapless Navigation with Neuromorphic Hardware\n1148 Scaling Laws for Parallel Motor-Gearbox Arrangements\n1149 Passivity Filter for Variable Impedance Control\n1150 Long-Run Multi-Robot Planning under Uncertain Action Durations for Persistent Tasks\n1151 Template-Based Optimal Robot Design with Application to Passive-Dynamic Underactuated Flapping\n1155 Interactive Movement Primitives: Planning to Push Occluding Pieces for Fruit Picking\n1157 Modeling Cable-Driven Joint Dynamics and Friction: A Bond-Graph Approach\n1158 Verification of System-Wide Safety Properties of ROS Applications\n1159 Real-Time Detection of Broccoli Crops in 3D Point Clouds for Autonomous Robotic Harvesting\n1165 Understanding Contexts Inside Robot and Human Manipulation Tasks through Vision-Language Model and Ontology System in Video Streams\n1171 Representing Spatial Object Relations As Parametric Polar Distribution for Scene Manipulation Based on Verbal Commands\n1172 Z-Net: An Anisotropic 3D DCNN for Medical CT Volume Segmentation\n1176 Analysis, Development and Evaluation of Electro-Hydrostatic Technology for Lower Limb Prostheses Applications\n1177 Proprioceptive Sensor Fusion for Quadruped Robot State Estimation\n1180 Model Identification of a Small Omnidirectional Aquatic Surface Vehicle: A Practical Implementation\n1183 Reducing the Teleoperator's Cognitive Burden for Complex Contact Tasks Using Affordance Primitives\n1184 Point Cloud Based Reinforcement Learning for Sim-To-Real and Partial Observability in Visual Navigation\n1189 A Novel Inverse Kinematics Method for Upper-Limb Exoskeleton under Joint Coordination Constraints\n1190 Closing the Loop: Real-Time Perception and Control for Robust Collision Avoidance with Occluded Obstacles\n1191 Brainless Running: A Quasi-Quadruped Robot with Decentralized Spinal Reflexes by Solely Mechanical Devices\n1193 Unsupervised Learning of Dense Optical Flow, Depth and Egomotion with Event-Based Sensors\n1195 Autonomous Robot Navigation Based on Multi-Camera Perception\n1196 Improving Motion Planning for Surgical Robot with Active Constraints\n1197 Graduated Assignment Graph Matching for Realtime Matching of Image Wireframes\n1201 Integrating Model Predictive Control and Dynamic Waypoints Generation for Motion Planning in Surgical Scenario\n1203 Hierarchical Optimization Control of Redundant Manipulator for Robot-Assisted Minimally Invasive Surgery\n1204 Variable Pitch System for the Underwater Explorer Robot UX-1\n1205 Real-Time Optimal Control of an Autonomous RC Car with Minimum-Time Maneuvers and a Novel Kineto-Dynamical Model\n1208 PillarFlowNet: A Real-time Deep Multitask Network for LiDAR-based 3D Object Detection and Scene Flow Estimation\n1209 Intelligent Exploration and Autonomous Navigation in Confined Spaces\n1211 Model Identification of a Soft Robotic Neck\n1216 MHYRO: Modular HYbrid RObot for Contact Inspection and Maintenance in Oil&gas Plants\n1218 Auditory Feedback Effectiveness for Enabling Safe Sclera Force in Robot-Assisted Vitreoretinal Surgery: A Multi-User Study\n1220 Towards Micro Robot Hydrobatics: Vision-based Guidance, Navigation, and Control for Agile Underwater Vehicles in Confined Environments\n1222 A Decentralized Framework for Simultaneous Calibration, Localization and Mapping with Multiple LiDARs\n1231 Real-Time Detection of Distracted Driving Using Dual Cameras\n1232 A Biomimetic Tactile Fingerprint Induces Incipient Slip\n1233 Model Quality Aware RANSAC: A Robust Camera Motion Estimator\n1235 Synchrono: A Scalable, Physics-Based Simulation Platform for Testing Groups of Autonomous Vehicles And/or Robots\n1236 Cross Scene Prediction via Modeling Dynamic Correlation using Latent Space Shared Auto-Encoders\n1237 Towards Vision-Based Impedance Control for the Contact Inspection of Unknown Generically-Shaped Surfaces with a Fully-Actuated UAV\n1238 Squash-Box Feasibility Driven Differential Dynamic Programming\n1239 SGM-MDE: Semi-Global Optimization for Classification-Based Monocular Depth Estimation\n1240 Safety Considerations in Deep Control Policies with Safety Barrier Certificates under Uncertainty\n1244 Asynchronous Event-Based Line Tracking for Time-To-Contact Maneuvers in UAS\n1245 RobotVQA ' a Scene-Graph and Deep-Learning-Based Visual Question Answering System for Robot Manipulation\n1247 A Model-Free Solution for Stable Balancing and Locomotion of Floating-Base Legged Systems\n1248 Label Efficient Visual Abstractions for Autonomous Driving\n1249 Control Interface for Hands-Free Navigation of Standing Mobility Vehicles Based on Upper-Body Natural Movements\n1250 Resonating Magnetic Manipulation for 3D Path-Following and Blood Clot Removal Using a Rotating Swimmer\n1252 Distilling Location Proposals of Unknown Objects through Gaze Information for Human-Robot Interaction\n1253 Visualization of Intended Assistance for Acceptance of Shared Control\n1254 Learning Object Manipulation with Dexterous Hand-Arm Systems from Human Demonstration\n1255 Learning Accurate and Human-Like Driving Using Semantic Maps and Attention\n1259 Designing Environments Conducive to Interpretable Robot Behavior\n1263 Efficiency and Equity are Both Essential: A Generalized Traffic Signal Controller with Deep Reinforcement Learning\n1265 Reactive Receding Horizon Planning and Control for Quadrotors with Limited On-Board Sensing\n1266 SelfieDroneStick: A Natural Interface for Quadcopter Photography\n1275 Localization and Force-Feedback with Soft Magnetic Stickers for Precise Robot Manipulation\n1276 Robust Monocular Edge Visual Odometry through Coarse-To-Fine Data Association\n1279 TartanAir: A Dataset to Push the Limits of Visual SLAM\n1281 An SEM-Based Nanomanipulation System for Multi-Physical Characterization of Single InGaN/GaN Nanowires\n1282 Line Walking and Balancing for Legged Robots with Point Feet\n1283 Robust Gait Synthesis Combining Constrained Optimization and Imitation Learning\n1284 Edge-Based Visual Odometry with Stereo Cameras Using Multiple Oriented Quadtrees\n1285 Monocular Localization in HD Maps by Combining Semantic Segmentation and Distance Transform\n1288 Optimal Design of a Novel Spherical Scissor Linkage Remote Center of Motion Mechanism for Medical Robotics\n1292 Adversarial Generation of Informative Trajectories for Dynamics System Identification\n1295 Robust and Efficient Object Change Detection by Combining Global Semantic Information and Local Geometric Verification\n1296 Bounded Sub-Optimal Multi-Robot Path Planning Using Satisfiability Modulo Theory (SMT) Approach\n1297 Haptic Knowledge Transfer between Heterogeneous Robots Using Kernel Manifold Alignment\n1298 Competitive Coverage: (Full) Information As a GameChanger\n1299 Expressing Diverse Human Driving Behavior with ProbabilisticRewards and Online Inference\n1300 Proximal Deterministic Policy Gradient\n1301 Generalizing Learned Manipulation Skills in Practice\n1302 GR-SLAM: Vision-Based Sensor Fusion SLAM for Ground Robots on Complex Terrain\n1304 Automatic Gait Pattern Selection for Legged Robots\n1307 Learning Vision-Based Physics Intuition Models for Non-Disruptive Object Extraction\n1313 Lane-Attention: Predicting Vehicles' Moving Trajectories by Learning Their Attention Over Lanes\n1315 Robotic Table Tennis with Model-Free Reinforcement Learning\n1317 Consistent Covariance Pre-Integration for Invariant Filters with Delayed Measurements\n1319 Model-Based Specification of Control Architectures for Compliant Interaction with the Environment\n1321 Perception-Aware Path Finding and Following of Snake Robot in Unknown Environment\n1323 3D Coating Self-Assembly for Modular Robotic Scaffolds\n1326 Understanding Dynamic Scenes Using Graph Convolution Networks\n1328 Monocular Depth Prediction through Continuous 3D Loss\n1335 Peg-In-Hole Using 3D Workpiece Reconstruction and CNN-Based Hole Detection\n1337 Crop Height and Plot Estimation for Phenotyping from Unmanned Aerial Vehicles Using 3D LiDAR\n1338 Non-linear control under state constraints with validated trajectories for a mobile robot towing a trailer\n1340 ImitationFlow: Learning Deep Stable Stochastic Dynamic Systems by Normalizing Flows\n1341 A Unique Identifier Assignment Method for Distributed Modular Robots\n1344 A Framework for Real-Time and Personalisable Human Ergonomics Monitoring\n1346 Bio-Inspired Inverted Landing Strategy in a Small Aerial Robot Using Policy Gradient\n1347 Walking on TacTip Toes: A Tactile Sensing Foot for Walking Robots\n1350 Event-Based PID Controller Fully Realized in Neuromorphic Hardware: A One DoF Study\n1351 Modality-Buffet for Real-Time Object Detection\n1352 Traffic Control Gesture Recognition for Autonomous Vehicles\n1353 Emergent Adaptive Gait Generation through Hebbian Sensor-Motor Maps by Morphological Probing\n1356 PnuGrip: An Active Two-Phase Gripper for Dexterous Manipulation\n1357 robo-gym ' An Open Source Toolkit for Distributed Deep Reinforcement Learning on Real and Simulated Robots\n1358 LIC-Fusion 2.0: LiDAR-Inertial-Camera Odometry with Sliding-Window Plane-Feature Tracking\n1359 Experimental Evaluation of 3D-LIDAR Camera Extrinsic Calibration\n1362 Autonomous model-based assessment of mechanical failures of reconfigurable modular robots with a Conjugate Gradient solver\n1363 Spiking Neurons Ensemble for Movement Generation in Dynamically Changing Environments\n1365 Terrain-Adaptive Planning and Control of Complex Motions for Walking Excavators\n1369 Gimme Signals: Discriminative Signal Encoding for Multimodal Activity Recognition\n1370 Goal-Driven Variable Admittance Control for Robot Manual Guidance\n1372 A Scalable Framework for Robust Vehicle State Estimation with a Fusion of a Low-Cost IMU, the GNSS, Radar, a Camera and Lidar\n1375 Joint Feature Selection and Time Optimal Path Parametrization for High Speed Vision-Aided Navigation\n1377 Multi-Task Deep Learning for Depth-Based Person Perception in Mobile Robotics\n1378 Online Explanation Generation for Planning Tasks in Human-Robot Teaming\n1381 CinemAirSim: A Camera-Realistic Robotics Simulator for Cinematographic Purposes\n1384 Real-World Human-Robot Collaborative Reinforcement Learning\n1386 Kinodynamic Motion Planning for Multi-Legged Robot Jumping Via Mixed-Integer Convex Program\n1387 Who Make Drivers Stop? Towards Driver-Centric Risk Assessment: Risk Object Identification via Causal Inference\n1388 Adaptive Partitioning for Coordinated Multi-Agent Perimeter Defense\n1390 Tracking Strategy Based on Magnetic Sensors for Microrobot Navigation in the Cochlea\n1391 Learning Object Attributes with Category-Free Grounded Language from Deep Featurization\n1392 First Steps: Latent-Space Control with Semantic Constraints for Quadruped Locomotion\n1394 Self-Reconfiguration Planning of Adaptive Modular Robots with Triangular Structure Based on Extended Binary Trees\n1395 Fast Model Predictive Image-Based Visual Servoing for Quadrotors\n1396 What the HoloLens Maps Is Your Workspace: Fast Mapping and Set-Up of Robot Cells Via Head Mounted Displays and Augmented Reality\n1399 6D Pose Estimation for Flexible Production with Small Lot Sizes Based on CAD Models Using Gaussian Process Implicit Surfaces\n1400 Design of a New Electroactive Polymer Based Continuum Actuator for Endoscopic Surgical Robots\n1402 Efficient Object Search through Probability-Based Viewpoint Selection\n1405 A Probabilistic Shared-Control Framework for Mobile Robots\n1409 Model Predictive Control for a Tendon-Driven Surgical Robot with Safety Constraints in Kinematics and Dynamics\n1411 Vision-based Belt Manipulation by Humanoid Robot\n1414 3D Localization of a Sound Source Using Mobile Microphone Arrays Referenced by SLAM\n1418 Exploit Semantic and Public Prior Information in MonoSLAM\n1422 Fast Online Adaptation in Robotics through Meta-Learning Embeddings of Simulated Priors\n1425 Computational Design of Balanced Open Link Planar Mechanisms with Counterweights from User Sketches\n1427 An Underactuated Gripper Using Origami-Folding Inspired Variable Stiffness Flexure Hinges\n1428 Frontier Detection and Reachability Analysis for Efficient 2D Graph-SLAM Based Active Exploration\n1429 Towards the Development of a Robotic Transcatheter Delivery System for Mitral Valve Implant\n1430 B-Spline Surfaces for Range-Based Environment Mapping\n1431 Localizing against Drawn Maps Via Spline-Based Registration\n1432 Majorization Minimization Methods for Distributed Pose Graph Optimization with Convergence Guarantees\n1433 Deep Adversarial Reinforcement Learning for Object Disentangling\n1434 Dense Incremental Metric-Semantic Mapping Via Sparse Gaussian Process Regression\n1435 Synthesis of Control Barrier Functions Using a Supervised Machine Learning Approach\n1436 JRMOT: A Real-Time 3D Multi-Object Tracker and a New Large-Scale Dataset\n1438 Slope Handling for Quadruped Robots Using Deep Reinforcement Learning and Toe Trajectory Planning\n1439 Realistic and Interactive Robot Gaze\n1440 Gaussian Process Gradient Maps for Loop-Closure Detection in Unstructured Planetary Environments\n1442 Efficient Multiresolution Scrolling Grid for Stereo Vision-based MAV Obstacle Avoidance\n1443 Action Sequence Predictions of Vehicles in Urban Environments Using Map and Social Context\n1446 Decentralized Nonlinear MPC for Robust Cooperative Manipulation by Heterogeneous Aerial-Ground Robots\n1447 Joint-Level Control of the DLR Lightweight Robot SARA\n1452 Improving Unimodal Object Recognition with Multimodal Contrastive Learning\n1453 Observer-Based Control of Inflatable Robot with Variable Stiffness\n1454 Maximizing BCI Human Feedback Using Active Learning\n1457 LiDAR Iris for Loop-Closure Detection\n1460 Mixed Reality As a Bidirectional Communication Interface for Human-Robot Interaction\n1461 Visual Monitoring and Servoing of a Cutting Blade During Telerobotic Satellite Servicing\n1462 Human-Drone Interaction for Aerially Manipulated Drilling Using Haptic Feedback\n1463 MAPPER: Multi-Agent Path Planning with Evolutionary Reinforcement Learning in Mixed Dynamic Environments\n1465 No-Regret Shannon Entropy Regularized Neural Contextual Bandit Online Learning for Robotic Grasping\n1467 Learning of Tool Force Adjustment Skills by a Life-Sized Humanoid Using Deep Reinforcement Learning and Active Teaching Request\n1469 Towards Deep Learning Assisted Autonomous UAVs for Manipulation Tasks in GPS-Denied Environments\n1471 BIT-VO: Visual Odometry at 300 FPS Using Binary Features from the Focal Plane\n1472 Using Machine Learning for Material Detection with Capacitive Proximity Sensors\n1477 Intent-Driven Strategic Tactical Planning for Autonomous SiteInspection Using Cooperative Drones\n1478 Physical Human-Robot Interaction with Real Active Surfaces Using Haptic Rendering on Point Clouds\n1480 End-to-End Autonomous Driving Perception with Sequential Latent Representation Learning\n1482 The Robot As Scientist: Using Mental Simulation to Test Causal Hypotheses Extracted from Human Activities in Virtual Reality\n1483 Design of a High-Level Teleoperation Interface Resilient to the Effects of Unreliable Robot Autonomy\n1488 TactileSGNet: A Spiking Graph Neural Network for Event-Based Tactile Object Recognition\n1490 A Miniaturised Neuromorphic Tactile Sensor Integrated with an Anthropomorphic Robot Hand\n1493 Towards Unsupervised Learning for Instrument Segmentation in Robotic Surgery with Cycle-Consistent Adversarial Networks\n1498 Energy-Efficient Motion Planning for Multi-Modal Hybrid Locomotion\n1499 CNN-Based Foothold Selection for Mechanically Adaptive Soft Foot\n1503 A Topological Approach to Path Planning for a Magnetic Millirobot\n1504 Autonomous Detection and Assessment with Moving Sensors\n1505 Compliance Control of Cable-Suspended Aerial Manipulator Using Hierarchical Control Framework\n1507 Noncontact Estimation of Stiffness Based on Optical Coherence Elastography under Acoustic Radiation Pressure\n1508 Factor Graph Based 3D Multi-Object Tracking in Point Clouds\n1512 Application of Interacting Models to Estimate the Gait Speed of an Exoskeleton User\n1513 An Approach to Reduce Communication for Multi-Agent Mapping Applications\n1514 CUHK-AHU Dataset: Promoting Practical Self-Driving Applications in the Complex Airport Logistics, Hill and Urban Environments\n1515 Friction Identification in a Pneumatic Gripper\n1519 LC-GAN: Image-To-Image Translation Based on Generative Adversarial Network for Endoscopic Images\n1520 Learning Human Navigation Behavior Using Measured Human Trajectories in Crowded Spaces\n1527 Scaling up Multiagent Reinforcement Learning for Robotic Systems: Learn an Adaptive Sparse Communication Graph\n1537 Efficient Exploration in Constrained Environments with Goal-Oriented Reference Path\n1538 Kalman Filter Based Range Estimation and Clock Synchronization for Ultra Wide Band Networks\n1540 Haptic Sequential Monte Carlo Localization for Quadrupedal Locomotion in Vision-Denied Scenarios\n1543 Vision Only 3-D Shape Estimation for Autonomous Driving\n1544 LLAMA: Design and Control of an Omnidirectional Human Mission Scale Quadrupedal Robot\n1545 Variational Filtering with Copula Models for SLAM\n1546 Dynamic Parameter Estimation Utilizing Optimized Trajectories\n1551 Core-Centered Actuation for Biped Locomotion of Humanoid Robots\n1553 Dynamic Stability Control of Inverted-Pendulum-Type Robotic Wheelchair for Going Up and Down Stairs\n1560 Depth Completion Via Inductive Fusion of Planar LIDAR and Monocular Camera\n1565 FlexiVision: Teleporting the Surgeon's Eyes Via Robotic Flexible Endoscope and Head-Mounted Display\n1566 Towards Robust Visual Tracking for Unmanned Aerial Vehicle with Tri-Attentional Correlation Filters\n1568 Infusing Reachability-Based Safety into Planning and Control for Multi-Agent Interactions\n1569 Optimization-Based Hierarchical Motion Planning for Autonomous Racing\n1572 DR-SPAAM: A Spatial-Attention and Auto-Regressive Model for Person Detection in 2D Range Data\n1573 Functionally Divided Manipulation Synergy for Controlling Multi-Fingered Hands\n1574 Solving Cosserat Rod Models Via Collocation and the Magnus Expansion\n1576 SwingBot: Learning Physical Features from In-Hand Tactile Exploration for Dynamic Swing-Up Manipulation\n1577 Crowdsourced 3D Mapping: A Combined Multi-View Geometry and Self-Supervised Learning Approach\n1579 Visuomotor Mechanical Search: Learning to Retrieve Target Objects in Clutter\n1580 GRIF Net: Gated Region of Interest Fusion Network for Robust 3D Object Detection from Radar Point Cloud and Monocular Image\n1582 Semi-Autonomous Control of Leader-Follower Excavator using Admittance Control for Synchronization and Autonomy with Bifurcation and Stagnation for Human Interface\n1586 Active Improvement of Control Policies with Bayesian Gaussian Mixture Model\n1590 PLRC*: A Piecewise Linear Regression Complex for Approximating Optimal Robot Motion\n1591 Disappearance of Chaotic Attractor of Passive Dynamic Walking by Stretch-Bending Deformation in Basin of Attraction\n1593 Enabling Robot to Assist Human in Collaborative Assembly Using Convolutional Neural Networks\n1595 Estimating Motion Codes from Demonstration Videos\n1598 Dynamic Object Tracking and Masking for Visual SLAM\n1599 Evaluating the Efficacy of Parallel Elastic Actuators on High-Speed, Variable Stiffness Running\n1600 Active Preference Learning Using Maximum Regret\n1602 Approximated Dynamic Trait Models for Heterogeneous Multi-Robot Teams\n1603 Relevant Region Exploration on General Cost-Maps for Sampling-Based Motion Planning\n1605 Self-Supervised Object Tracking with Cycle-Consistent Siamese Networks\n1608 Resilient Coverage: Exploring the Local-To-Global Trade-Off\n1611 X-Ray: Mechanical Search for an Occluded Object by Minimizing Support of Learned Occupancy Distributions\n1613 Secure Route Planning Using Dynamic Games with Stopping States\n1619 Ospheel: Design of an Omnidirectional Spherical-Sectioned Wheel\n1620 SQUIRL: Robust and Efficient Learning from Video Demonstration of Long-Horizon Robotic Manipulation Tasks\n1623 Depth Estimation from Monocular Images and Sparse Radar Data\n1629 Learning to Switch CNNs with Model Agnostic Meta Learning for Fine Precision Visual Servoing\n1630 Relative Pose Estimation and Planar Reconstruction Via Superpixel-Driven Multiple Homographies\n1631 Crossing the Gap: A Deep Dive into Zero-Shot Sim-to-Real Transfer for Dynamics\n1632 Rapid Bipedal Gait Optimization in CasADi\n1634 Novel Design of a Soft Pump Driven by Super-Coiled Polymer Artificial Muscles\n1635 Automated Folding of a Deformable Thin Object through Robot Manipulators\n1639 Robot Learning in Mixed Adversarial and Collaborative Settings\n1640 Jumping Motion Generation for Humanoid Robot Using Arm Swing Effectively and Changing in Foot Contact Status\n1641 Antipodal Robotic Grasping Using Generative Residual Convolutional Neural Network\n1645 Hindsight for Foresight: Unsupervised Structured Dynamics Models from Physical Interaction\n1655 Resultant Radius of Curvature of Stylet-And-Tube Steerable Needles Based on the Mechanical Properties of the Soft Tissue, and the Needle\n1657 Enhanced Transfer Learning for Autonomous Driving with Systematic Accident Simulation\n1658 Observer-Based Disturbance Control for Small-Scale Collaborative Robotics\n1659 Anticipating Tumor Metastasis by Circulating Tumor Cells Captured by Acoustic Microstreaming\n1660 An Untethered Brittle Star-Inspired Soft Robot for Closed-Loop Underwater Locomotion\n1662 What to Do When You Can't Do It All: Temporal Logic Planning with Soft Temporal Logic Constraints\n1663 Dynamics and Aerial Attitude Control for Rapid Emergency Deployment of the Agile Ground Robot AGRO\n1665 A Framework for Human-Robot Interaction User Studies\n1671 A Mixed-Integer Model Predictive Control Approach to Motion Cueing in Immersive Wheelchair Simulator\n1674 Self-Sensing Soft Tactile Actuator for Fingertip Interface\n1675 Trajectory Tracking of a One-Link Flexible Arm Via Iterative Learning Control\n1677 Contextual Policy Search for Micro-Data Robot Motion Learning through Covariate Gaussian Process Latent Variable Models\n1678 A Framework for Online Updates to Safe Sets for Uncertain Dynamics\n1680 Contact Force Estimation and Regulation of a Position-Controlled Floating Base System without Joint Torque Information\n1685 Going Cognitive: A Demonstration of the Utility of Task-General Cognitive Architectures for Adaptive Robotic Task Performance\n1686 A Fast and Robust Place Recognition Approach for Stereo Visual Odometry Using LiDAR Descriptors\n1689 Developing Thermal Endoscope for Endoscopic Photothermal Therapy for Peritoneal Dissemination\n1690 Collision-Free Distributed Multi-Target Tracking Using Teams of Mobile Robots with Localization Uncertainty\n1692 HouseExpo: A Large-Scale 2D Indoor Layout Dataset for Learning-Based Algorithms on Mobile Robots\n1693 ReachFlow: An Online Safety Assurance Framework for Waypoint-Following of Self-Driving Cars\n1694 An Untethered Soft Cellular Robot with Variable Volume, Friction, and Unit-To-Unit Cohesion\n1695 Multi-Robot Task Allocation with Time Window and Ordering Constraints\n1696 A Distributed Scalar Field Mapping Strategy for Mobile Robots\n1698 TORM: Fast and Accurate Trajectory Optimization of Redundant Manipulator Given an End-Effector Path\n1703 IAN: Multi-Behavior Navigation Planning for Robots in Real, Crowded Environments\n1706 Contact Localization Using Velocity Constraints\n1707 Tensor Action Spaces for Multi-Agent Robot Transfer Learning\n1709 Deep Prediction of Swept Volume Geometries: Robots and Resolutions\n1710 Collaborative Programming of Conditional Robot Tasks\n1711 A Whisker-Inspired Fin Sensor for Multi-Directional Airflow Sensing\n1716 Seed: A Segmentation-Based Egocentric 3D Point Cloud Descriptor for Loop Closure Detection\n1719 Multi-Fingered Active Grasp Learning\n1720 Extended Performance Guarantees for Receding Horizon Search with Terminal Cost\n1721 Robot Learning from Demonstration with Tactile Signals for Geometry-Dependent Tasks\n1722 F-Siamese Tracker: A Frustum-Based Double Siamese Network for 3D Single Object Tracking\n1723 QSRNet: Estimating Qualitative Spatial Representations from RGB-D Images\n1724 Interactive Planning and Supervised Execution for High-Risk, High-Latency Teleoperation\n1725 L1-Adaptive MPPI Architecture for Robust and Agile Control of Multirotors\n1726 Highly Underactuated Radial Gripper for Automated Planar Grasping and Part Fixturing\n1727 Localization Uncertainty-Driven Adaptive Framework for Controlling Ground Vehicle Robots\n1728 Sim-To-Real with Domain Randomization for Tumbling Robot Control\n1729 Robust, Perception Based Control with Quadrotors\n1730 Analysis of Contact Stability and Contact Safety of a Robotic Intravascular Cardiac Catheter under Blood Flow Disturbances\n1732 Real-Time Multi-SLAM System for Agent Localization and 3D Mapping in Dynamic Scenarios\n1733 Deep R-Learning for Continual Area Sweeping\n1734 Fast Uncertainty Estimation for Deep Learning Based Optical Flow\n1735 Global Localization Over 2D Floor Plans with Free-Space Density Based on Depth Information\n1736 A Multi-Contact Motion Planning and Control Strategy for Physical Interaction Tasks Using a Humanoid Robot\n1737 Soft-Bubble Grippers for Robust and Perceptive Manipulation\n1738 Planning for Robust Visibility-Based Pursuit-Evasion\n1741 Robot Calligraphy Using Pseudospectral Optimal Controlin Conjunction with a Novel Dynamic Brush Model\n1743 Path Planning under MIMO Network Constraints for Throughput Enhancement in Multi-Robot Data Aggregation Tasks\n1744 Self-Supervised Neural Audio-Visual Sound Source Localization Via Probabilistic Spatial Modeling\n1745 Silicone-Based Capacitive E-Skin for Exteroception and Proprioception\n1746 SCAN: System for Camera Autonomous Navigation in Robotic-Assisted Surgery\n1749 Flight Control of Sliding Arm Quadcopter with Dynamic Structural Parameters\n1750 Domain-Adversarial and -Conditional State Space Model for Imitation Learning\n1752 MSDPN: Monocular Depth Prediction with Partial Laser Observation Using Multi-Stage Neural Networks\n1753 Optimizing Coordinate Choice for Locomotion Systems with Toroidal Shape Spaces\n1754 DeepURL: Deep Pose Estimation Framework for Underwater Relative Localization\n1755 Expert-Emulating Excavation Trajectory Planning for Autonomous Robotic Industrial Excavator\n1758 Soft Microrobotic Transmissions Enable Rapid Ground-Based Locomotion\n1762 Algorithm for Multi-Robot Chance-Constrained Generalized Assignment Problem with Stochastic Resource Consumption\n1763 Game-Theoretic Planning for Risk-Aware Interactive Agents\n1765 Demonstration of a Novel Phase Lag Controlled Roll Rotation Mechanism Using a Two-DOF Soft Swimming Robot\n1769 Learning to Locomote with Artificial Neural-Network and CPG-Based Control in a Soft Snake Robot\n1771 HeatNet: Bridging the Day-Night Domain Gap in Semantic Segmentation with Thermal Images\n1772 Building Plannable Representations with Mixed Reality\n1773 Parameter Identification for an Uncooperative Captured Satellite with Spinning Reaction Wheels\n1774 Design of a Highly-Maneuverable Pneumatic Soft Actuator Driven by Intrinsic SMA Coils (PneuSMA Actuator)\n1779 Collision Reaction Through Internal Stress Loading in Cooperative Manipulation\n1780 Pedestrian Motion Tracking by Using Inertial Sensors on the Smartphone\n1781 Blind Bin Picking of Small Screws Through In-finger Manipulation With Compliant Robotic Fingers\n1782 Data-Driven Distributionally Robust Electric Vehicle Balancing for Mobility-On-Demand Systems under Demand and Supply Uncertainties\n1788 A Concept of a Miniaturized MR Clutch Utilizing MR Fluid in Squeeze Mode\n1790 Enabling Remote Whole-Body Control with 5G Edge Computing\n1793 3D Printed Bio-Inspired Hair Sensor for Directional Airflow Sensing\n1794 Learning to Use Adaptive Motion Primitives in Search-Based Motion Planning for Navigation\n1795 RadarSLAM: Radar Based Large-Scale SLAM in All Weathers\n1799 Learning Orientation Distributions for Object Pose Estimation\n1804 Performance Characterization of an Algorithm to Estimate the Search Skill of a Human or Robot Agent\n1807 Learning an Optimal Sampling Distribution for Efficient Motion Planning\n1810 Autonomous Navigation and Obstacle Avoidance of a Snake Robot with Combined Velocity-Heading Control\n1811 Quadrotor-Enabled Autonomous Parking Occupancy Detection\n1812 The Masked Mapper: Masked Metric Mapping\n1813 Augmenting Control Policies with Motion Planning for Robust and Safe Multi-Robot Navigation\n1814 Collaborative Semantic Perception and Relative Localization Based on Map Matching\n1816 Weakly-Supervised Learning for Multimodal Human Activity Recognition in Human-Robot Collaboration Scenarios\n1817 Sim-To-Real Transfer of Bolting Tasks with Tight Tolerance\n1818 LiDAR Guided Small Obstacle Segmentation\n1819 Towards Transparent Robotic Planningvia Contrastive Explanations\n1820 Estimation of Object Class and Orientation from Multiple Viewpoints and Relative Camera Orientation Constraints\n1821 Information Driven Self-Calibration for Lidar-Inertial Systems\n1822 Safe and Effective Picking Paths in Clutter given Discrete Distributions of Object Poses\n1823 Shape Reconstruction of CCD Camera-Based Soft Tactile Sensors\n1824 Allocating Limited Sensing Resources to Accurately Map Dynamic Environments\n1825 Robust Micro-Particle Manipulation in a Microfluidic Channel Network Using Gravity-Induced Pressure Actuators\n1827 The Multi-Material Actuator for Variable Stiffness (MAVS): Design, Modeling, and Characterization of a Soft Actuator for Lateral Ankle Support\n1828 Dec-PPCPP: A Decentralized Predator--Prey-Based Approach to Adaptive Coverage Path Planning Amid Moving Obstacles\n1830 Data-Driven Distributed State Estimation and Behavior Modeling in Sensor Networks\n1833 Collision Risk Assessment Via Awareness Estimation Toward Robotic Attendant\n1834 Leveraging Multiple Environments for Learning and Decision Making: A Dismantling Use Case\n1836 Robust Dynamic State Estimation for Lateral Control of an Industrial Tractor Towing Multiple Passive Trailers\n1837 Output Only Fault Detection and Mitigation of Networks of Autonomous Vehicles\n1838 Markov Decision Processes with Unknown State Feature Values for Safe Exploration using Gaussian Processes\n1839 Design and Control of SLIDER: An Ultra-Lightweight, Knee-Less, Low-Cost Bipedal Walking Robot\n1840 Learning Bayes Filter Models for Tactile Localization\n1841 Occlusion Handling for Industrial Robots\n1844 Online BayesSim for Combined Simulator Parameter Inference and Policy Improvement\n1846 Roboat II: A Novel Autonomous Surface Vessel for Urban Environments\n1847 Human Grasp Classification for Reactive Human-To-Robot Handovers\n1851 Persistent Connected Power Constrained Surveillance with Unmanned Aerial Vehicles\n1853 Designing A Dummy Skin by Evaluating Contacts between A Human Hand and A Robot End Tip\n1854 Making Robots Draw a Vivid Portrait in Two Minutes\n1856 Unified Calibration for Multi-Camera Mult-LiDAR Systems Using a Single Checkerboard\n1857 3D Multi-Object Tracking: A Baseline and New Evaluation Metrics\n1858 Learning Consistency Pursued Correlation Filters for Real-Time UAV Tracking\n1860 Multi-Robot Containment and Disablement\n1861 Snapbot V2: A Reconfigurable Legged Robot with a Camera for Self Configuration Recognition\n1862 Stable Crawling Policy for Wearable SuperLimbs Attached to a Human with Tuned Impedance\n1864 Planning on the fast lane: Learning to interact using attention mechanisms in path integral inverse reinforcement learning\n1867 Identification of a Human Hand Kinematics by Measuring and Merging of Nail-Based Finger Motions\n1868 Tidying Deep Saliency Prediction Architectures\n1872 Deep Reinforcement Learning for Industrial Insertion Tasks with Visual Inputs and Natural Rewards\n1873 SideGuide: A Large-Scale Sidewalk Dataset for Guiding Impaired People\n1874 Autonomous Multi-Robot Assembly of Solar Array Modules: Experimental Analysis and Insights\n1875 Learning Transition Models with Time-Delayed Causal Relations\n1876 Lyapunov-Based Approach to Reactive Step Generation for Push Recovery of Biped Robots Via Hybrid Tracking Control of DCM\n1877 Versatile 3D Multi-Sensor Fusion for Lightweight 2D Localization\n1878 Efficient Trajectory Library Filtering for Quadrotor Flight in Unknown Environments\n1882 Towards General Infeasibility Proofs in Motion Planning\n1884 PillarFlow: End-To-End Birds-Eye-View Flow Estimation for Autonomous Driving\n1888 Self-Supervised Simultaneous Alignment and Change Detection\n1889 Multimodal Sensor Fusion with Differentiable Filters\n1890 Multi-Object Rearrangement with Monte Carlo Tree Search: A Case Study on Planar Nonprehensile Sorting\n1891 A Robust Multi-Stereo Visual-Inertial Odometry Pipeline\n1893 Multimodal Material Classification for Robots Using Spectroscopy and High Resolution Texture Imaging\n1895 SplitFlyer: a Modular Quadcoptor that Disassembles into Two Flying Robots\n1897 Multi-Agent Safe Planning with Gaussian Processes\n1898 Exponentially Stabilizing and Time-Varying Virtual Constraint Controllers for Dynamic Quadrupedal Bounding\n1899 Vision Global Localization with Semantic Segmentation and Interest Feature Points\n1900 An Electrocommunication System Using FSK Modulation and Deep Learning Based Demodulation for Underwater Robots\n1901 Online System for Dynamic Multi-contact Motion with Impact Force Based on Contact Wrench Estimation and Current-Based Torque Control\n1902 Learning Constraint-Based Planning Models from Demonstrations\n1903 Autonomous Vehicle Benchmarking Using Unbiased Metrics\n1907 Ultrasound-Guided Robotic Navigation with Deep Reinforcement Learning\n1908 Gait Training Robot with Intermittent Force Application Based on Prediction of Minimum Toe Clearance\n1909 Design and Implementation of a Haptic Measurement Glove to Create Realistic Human-Telerobot Interactions\n1911 Risk-Averse MPC via Visual-Inertial Input and Recurrent Networks for Online Collision Avoidance\n1912 Data Driven Online Multi-Robot Formation Planning\n1913 Learning-Based Distributionally Robust Motion Control with Gaussian Processes\n1915 Multi-Label Long Short-Term Memory for Construction Vehicle Activity Recognition with Imbalanced Supervision\n1916 Deep Mixture Density Network for Probabilistic Object Detection\n1917 Human Preference-Based Learning for High-Dimensional Optimization of Exoskeleton Walking Gaits\n1919 Online Planning in Uncertain and Dynamic Environment in the Presence of Multiple Mobile Vesicles\n1920 Reliable Chattering-Free Simulation of Friction Torque in Joints Presenting High Stiction\n1921 Generating Minimum-Snap Quadrotor Trajectories Really Fast\n1922 Data-Driven Characterization of Human Interaction for Model-based Control of Powered Prostheses\n1925 Development of a Maneuverable Un-Tethered Multi-fin Soft Robot\n1928 Choosing Classification Thresholds for Mobile Robot Coverage\n1930 Accelerating Bi-Directional Sampling-Based Search for Motion Planning of Non-Holonomic Mobile Manipulators\n1933 Online Configuration Selection for Redundant Arrays of Inertial Sensors: Application to Robotic Systems Covered with a Multimodal Artificial Skin\n1934 Synchronous Minimum-Time Cooperative Manipulation Using Distributed Model Predictive Control\n1935 Robust Force Tracking Impedance Control of an Ultrasonic Motor-Actuated End-Effector in a Soft Environment\n1936 Transferring Experience from Simulation to the Real World for Precise Pick-And-Place Tasks in Highly Cluttered Scenes\n1937 Uncertainty-aware Self-supervised 3D Data Association\n1941 Parts-Based Articulated Object Localization in Clutter Using Belief Propagation\n1942 Low-Viewpoint Forest Depth Dataset for Sparse Rover Swarms\n1943 True\u00c3\ufffddapt: Learning Smooth Online Trajectory Adaptation with Bounded Jerk, Acceleration and Velocity in Joint Space\n1944 Simple Means Faster: Real-Time Human Motion Forecasting in Monocular First Person Videos on CPU\n1947 H-Infinity-Optimal Tracking Controller for Three-Wheeled Omnidirectional Mobile Robots with Uncertain Dynamics\n1950 Construction of Multiple Hepatic Lobule like 3D Vascular Networks by Manipulating Magnetic Tweezers toward Tissue Engineering\n1951 Generating Alerts to Assist with Task Assignments in Human-Supervised Multi-Robot Teams Operating in Challenging Environments\n1952 Stochastic Grounded Action Transformation for Robot Learning in Simulation\n1955 GP-Based Runtime Planning, Learning, and Recovery for Safe UAV Operations under Unforeseen Disturbances\n1956 Fast Global Motion Planning for Dynamic Legged Robots\n1960 Development of a Pneumatically Driven Growing Sling to Assist Patient Transfer\n1961 DIAT (Depth-Infrared Image Annotation Transfer) for Training a Depth-Based Pig-Pose Detector\n1962 Towards Autonomous Control of Magnetic Suture Needles\n1963 Interactive Tactile Perception for Classification of Novel Object Instances\n1964 DGAZE: Driver Gaze Mapping on Road\n1965 PERCH 2.0 : Fast and Accurate GPU-Based Perception Via Search for Object Pose Estimation\n1966 Experimental Verification of Vibratory Conveyor System Based on Frequency Entrainment of Limit Cycle Walker\n1968 Anticipatory Human-Robot Collaboration Via Multi-Objective Trajectory Optimization\n1969 Se(3)-TrackNet: Data-Driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains\n1971 A Study on the Elongation Behaviour of Synthetic Fibre Ropes under Cyclic Loading\n1972 Adaptive Kernel Inference for Dense and Sharp Occupancy Grids\n1976 Control Framework for a Hybrid-steel Bridge Inspection Robot\n1979 DXSLAM: A Robust and Efficient Visual SLAM System with Deep Features\n1981 Online Exploration of Tunnel Networks Leveraging Topological CNN-Based World Predictions\n1982 Learning Visual Policies for Building 3D Shape Categories\n1984 On Screw Linear Interpolation for Point-To-Point Path Planning\n1986 Diminished Reality for Close Quarters Robotic Telemanipulation\n1987 Encoding Formulas As Deep Networks: Reinforcement Learning for Zero-Shot Execution of LTL Formulas\n1988 UWB-Based System for UAV Localization in GNSS-Denied Environments: Characterization and Dataset\n1990 Feeling the True Force in Haptic Telepresence for Flying Robots\n1991 EAO-SLAM: Monocular Semi-Dense Object SLAM Based on Ensemble Data Association\n1992 A Probabilistic Model for Planar Sliding of Objects with Unknown Material Properties: Identification and Robust Planning\n1993 A Data-Driven Framework for Proactive Intention-Aware Motion Planning of a Robot in a Human Environment\n1994 Adaptive Nonlinear Control for Perching of a Bioinspired Ornithopter\n1997 Identification of Effective Motion Primitives for Ground Vehicles\n1998 Velocity Regulation of 3D Bipedal Walking Robots with Uncertain Dynamics through Adaptive Neural Network Controller\n1999 Safe Path Planning with Multi-Model Risk Level Sets\n2001 Nonlinear Model Predictive Control of Hopping Model Using Approximate Step-To-Step Models for Navigation on Complex Terrain\n2002 Multi-mode Trajectory Optimization for Impact-aware Manipulation\n2003 Tumbling and Hopping Locomotion Control for a Minor Body Exploration Robot\n2008 Predictive Runtime Monitoring of Vehicle Models Using Bayesian Estimation and Reachability Analysis\n2010 Learning to Take Good Pictures of People with a Robot Photographer\n2012 Robust Control Synthesis and Verification for Wire-Borne Underactuated Brachiating Robots Using Sum-of-Squares Optimization\n2013 Fully Convolutional Geometric Features for Category-Level Object Alignment\n2014 Sequential Motion Planning for Bipedal Somersault Via Flywheel SLIP and Momentum Transmission with Task Space Control\n2015 Learning User-Preferred Mappings for Intuitive Robot Control\n2022 Object-Aware Centroid Voting for Monocular 3D Object Detection\n2023 Detection-Aware Trajectory Generation for a Drone Cinematographer\n2024 Scalable Collaborative Manipulation with Distributed Trajectory Planning\n2025 D2VO: Monocular Deep Direct Visual Odometry\n2027 Task-Motion Planning for Safe and Efficient Urban Driving\n2028 PaintPath: Defining Path Directionality in Maps for AutonomousGround Vehicles\n2029 Motion Prediction in Visual Object Tracking\n2031 Inertia-Decoupled Equations for Hardware-In-The-Loop Simulation of an Orbital Robot with External Forces\n2034 Integrated Benchmarking and Design for Reproducible and Accessible Evaluation of Robotic Agents\n2036 Linear Distributed Clustering Algorithm for Modular Robots Based Programmable Matter\n2038 A Compliance Control Method Based on Viscoelastic Model for Position-Controlled Humanoid Robots\n2040 Optimization-Based Path Planning for Person Following Using Following Field\n2044 Design and Experiments with LoCO AUV: A Low Cost Open-Source Autonomous Underwater Vehicle\n2045 Robust Ego and Object 6-DoF Motion Estimation and Tracking\n2050 Risk-Constrained Motion Planning for Robot Locomotion: Formulation and Running Robot Demonstration\n2051 The Pluggable Distributed Resource Allocator (PDRA): A Middleware for Distributed Computing in Mobile Robotic Networks\n2057 Extrinsic and Temporal Calibration of Automotive Radar and 3D LiDAR\n2061 Learning Human-Aware Robot Navigation from Physical Interaction Via Inverse Reinforcement Learning\n2062 Learning Optimized Human Motion Via Phase Space Analysis\n2064 Gain Scheduled Controller Design for Balancing an Autonomous Bicycle\n2068 A Bayesian Approach for Gas Source Localization in Large Indoor Environments\n2076 Gripping a Kitchen Knife on the Cutting Board\n2078 Bayesian Particles on Cyclic Graphs\n2079 When We First Met: Visual-Inertial Source Localization for Co-Robot Rendezvous\n2080 Wet Adhesion of Micro-Patterned Interfaces for Stable Grasping of Deformable Objects\n2081 Exploration Strategy Based on Validity of Actions in Deep Reinforcement Learning\n2082 A Compact, Cable-Driven, Activatable Soft Wrist with Six Degrees of Freedom for Assembly Tasks\n2083 MPC-Graph: Feedback Motion Planning Using Sparse Sampling Based Neighborhood Graph\n2084 Maintaining Stable Grasps During Highly Dynamic Robot Trajectories\n2086 CMetric: A Driving Behavior Measure Using Centrality Functions\n2091 Collaborative Interaction Models for Optimized Human Robot Teamwork\n2094 Decentralized Safe Reactive Planning under TWTL Specifications\n2097 Pedestrian Intention Prediction for Autonomous Driving Using a Multiple Stakeholder Perspective Model\n2098 KR-Net: A Dependable Visual Kidnap Recovery Network for Indoor Spaces\n2103 MixGAIL: Autonomous Driving Using Demonstrations with Mixed Qualities\n2105 An Untethered 216-Mg Insect-Sized Jumping Robot with Wireless Power Transmission\n2106 Vacuum Driven Auxetic Switching Structure and Its Application on a Gripper and Quadruped\n2107 Geomorphological Analysis Using Unpiloted Aircraft Systems, Structure from Motion, and Deep Learning\n2110 The Omega Turn: A Biologically-Inspired Turning Strategy for Elongated Limbless Robots\n2111 Getting to Know One Another: Calibrating Intent, Capabilities, and Trust for Human-Robot Collaboration\n2112 Emergence of Swing-to-Stance Transition from Interlocking Mechanism in Horse Hindlimb\n2113 Risk-Aware Planning and Assignment for Ground Vehicles Using Uncertain Perception from Aerial Vehicles\n2114 Robots Made from Ice: An Analysis of Manufacturing Techniques\n2116 REFORM: Recognizing F-Formations for Social Robots\n2118 Balanced Depth Completion between Dense Depth Inference and Sparse Range Measurements Via KISS-GP\n2119 Design and Control of SQUEEZE: A Spring-Augmented QUadrotor for intEractions with the Environment to SqueeZE-And-Fly\n2121 Decentralized Control Schemes for Stable Quadrupedal Locomotion: A Decomposition Approach from Centralized Controllers\n2122 Real-Time Robot End-Effector Pose Estimation with Deep Network\n2124 Mobile Robot Localization under Non-Gaussian Noise usingCorrentropy Similarity Metric\n2134 Dynamic Object Tracking for Self-Driving Cars Using Monocular Camera and LIDAR\n2137 A Novel Endoscope Design Using Spiral Technique for Robotic-Assisted Endoscopy Insertion\n2138 Driving through Ghosts: Behavioral Cloning with False Positives\n2139 Ultra-Wideband Aided UAV Positioning Using Incremental Smoothing with Ranges and Multilateration\n2141 Autonomous Navigation Over Europa Analogue Terrain for an Actively Articulated Wheel-On-Limb Rover\n2144 Kinematic Optimization of an Underactuated Anthropomorphic Prosthetic Hand\n2147 Material Mapping in Unknown Environments Using Tapping Sound\n2149 Optimizing a Continuum Manipulator's Search Policy through Model-Free Reinforcement Learning\n2150 Active Perception for Outdoor Localisation with an Omnidirectional Camera\n2151 Biomimetic Control Scheme for Musculoskeletal Humanoids Based on Motor Directional Tuning in the Brain\n2155 GPU Parallelization of Policy Iteration RRT#\n2158 Spatio-Temporal Attention Model for Tactile Texture Recognition\n2162 Motion Planning for Collision-resilient Mobile Robots in Obstacle-cluttered Unknown Environments with Risk Reward Trade-offs\n2164 Learning Topological Motion Primitives for Knot Planning\n2166 Semantic Segmentation of Underwater Imagery: Dataset and Benchmark\n2168 Go-CHART: A Miniature Remotely Accessible Self-Driving Car Robot\n2170 Better Together: Online Probabilistic Clique Change Detection in 3D Landmark-Based Maps\n2172 A Multigait Stringy Robot with Bi-Stable Soft-Bodied Structures in Multiple Viscous Environments\n2174 A Game-Theoretic Strategy-Aware Interaction Algorithm with Validation on Real Traffic Data\n2175 Towards Cooperative Transport of a Suspended Payload Via Two Aerial Robots with Inertial Sensing\n2178 An Online Training Method for Augmenting MPC with Deep Reinforcement Learning\n2179 Affordance-Based Mobile Robot Navigation among Movable Obstacles\n2180 Robotic Untangling of Herbs and Salads with Parallel Grippers\n2185 Target Tracking Control of a Wheel-less Snake Robot Based on a Supervised Multi-layered SNN\n2186 Probabilistic Semantic Mapping for Urban Autonomous Driving Applications\n2188 Cloth Region Segmentation for Robust Grasp Selection\n2189 Uncertainty Aware Texture Classification and Mapping Using Soft Tactile Sensors\n2193 Design and Control of Roller Grasper V2 for In-Hand Manipulation\n2195 Navigation on the Line: Traversability Analysis and Path Planning for Extreme-Terrain Rappelling Rovers\n2197 Navigation-Assistant Path Planning within a MAV Team\n2199 Deep Learning-Based Autonomous Scanning Electron Microscope\n2200 Multi-Agent Path Planning under Observation Schedule Constraints\n2209 Deep Inverse Sensor Models as Priors for evidential Occupancy Mapping\n2210 Quadrupedal Robotic Walking on Sloped Terrains Via Exact Decomposition into Coupled Bipedal Robots\n2214 GelTip: A Finger-Shaped Optical Tactile Sensor for Robotic Manipulation\n2215 PufferBot: Actuated Expandable Structures for Aerial Robots\n2221 Skill-based Programming Framework for Composable Reactive Robot Behaviors\n2222 Multiplicative Controller Fusion: Leveraging Algorithmic Priors for Sample-Efficient Reinforcement Learning and Safe Sim-To-Real Transfer\n2223 Targetless Calibration of LiDAR-IMU System Based on Continuous-Time Batch Estimation\n2224 A Distributed Range-Only Collision Avoidance Approach for Low-Cost Large-Scale Multi-Robot Systems\n2225 Localization Safety Validation for Autonomous Robots\n2227 Design and Evaluation of a Perching Hexacopter Drone for Energy Harvesting from Power Lines\n2228 From Human to Robot Everyday Activity\n2232 Dynamic Legged Manipulation of a Ball through Multi-Contact Optimization\n2235 A Geometric Perspective on Visual Imitation Learning\n2236 Robust Autonomous Navigation of a Small-Scale Quadruped Robot in Real-World Environments\n2240 The Personalization of Stiffness for an Ankle-Foot Prosthesis Emulator Using Human-In-The-Loop Optimization\n2243 Development of Selective Driving Joint Forceps Using Shape Memory Polymer\n2246 ProxEmo: Gait-Based Emotion Learning andMulti-View Proxemic Fusion for Socially-Aware Robot Navigation\n2250 Adaptive Dynamic Window Approach for Local Navigation\n2253 A Tip Mount for Transporting Sensors and Tools Using Soft Growing Robots\n2255 With Whom to Communicate: Learning Efficient Communication for Multi-Robot Collision Avoidance\n2256 PBP-Net: Point Projection and Back-Projection Network for 3D Point Cloud Segmentation\n2262 The Importance of Prior Knowledge in Precise Multimodal Prediction\n2264 Learning Skills to Patch Plans Based on Inaccurate Models\n2265 Cooperative Control of Mobile Robots with Stackelberg Learning\n2268 Design and Experimentation of a Variable Stiffness Bistable Gripper\n2269 Expedited Multi-Target Search with Guaranteed Performance Via Multi-Fidelity Gaussian Processes\n2270 MVLidarNet: Real-Time Multi-Class Scene Understanding for Autonomous Driving Using Multiple Views\n2271 Towards RL-Based Hydraulic Excavator Automation\n2274 Development and Analysis of Digging and Soil Removing Mechanisms for Mole-Bot: Bio-Inspired Mole-Like Drilling Robot\n2275 Explore Bravely: Wheeled-Legged Robots Traversing in Unknown Rough Environment\n2276 BRM Localization: UAV Localization in GNSS-Denied Environments Based on Matching of Numerical Map and UAV Images\n2277 Minimally Disruptive Connectivity Enhancement for Resilient Multi-Robot Teams\n2278 Multi-Modal Pneumatic Actuator for Twisting, Extension, and Bending\n2280 Whole-Game Motion Capturing of Team Sports: System Architecture and Integrated Calibration\n2281 Simultaneous Position-Stiffness Control of Antagonistically Driven Twisted-Coiled Polymer Actuators Using Model Predictive Control\n2282 Data-Driven Models with Expert Influence: A Hybrid Approach to Spatiotemporal Process Estimation\n2285 Meta-Reinforcement Learning for Robotic Industrial Insertion Tasks\n2286 Stochastic Neural Control Using Raw Pointcloud Data and Building Information Models\n2287 TT-TSDF: Memory-Efficient TSDF with Low-Rank Tensor Train Decomposition\n2290 HAMLET: A Hierarchical Multimodal Attention-Based Human Activity Recognition Algorithm\n2291 Dielecrophoretic Introduction of the Membrane Proteins into the BLM Platforms for the Electrophygiological Analysis Systems\n2292 Knuckles That Buckle: Compliant Underactuated Limbs with Joint Hysteresis Enable Minimalist Terrestrial Robots\n2293 Path Negotiation for Self-Interested Multirobot Vehicles in Shared Space\n2295 Dynamically Constrained Motion Planning Networks for Non-Holonomic Robots\n2297 3DMotion-Net: Learning Continuous Flow Function for 3D MotionPrediction\n2298 Sparse Discrete Communication Learning for Multi-Agent Cooperation through Backpropagation\n2301 Simultaneous Trajectory Optimization and Force Control with Soft Contact Mechanics\n2304 GP-SLAM+: Real-Time 3D Lidar SLAM Based on Improved Regionalized Gaussian Process Map Reconstruction\n2307 Impedance Control of Humanoid Walking on Uneven Terrain with Centroidal Momentum Dynamics Using Quadratic Programming\n2309 CAZSL: Zero-Shot Regression for Pushing Models by Generalizing through Context\n2313 Objective Functions for Principal Contact Estimation from Motion Based on the Geometrical Singular Condition\n2314 Autonomous Spot: Long-Range Autonomous Exploration of Extreme Environments with Legged Locomotion\n2315 Fast Texture Classification Using Tactile Neural Coding and Spiking Neural Network\n2318 City-Scale Grid-Topological Hybrid Maps for Autonomous Mobile Robot Navigation in Urban Area\n2324 Inspection-On-The-Fly Using Hybrid Physical Interaction Control for Aerial Manipulators\n2325 Real-Time Constrained Nonlinear Model Predictive Control on SO(3) for Dynamic Legged Locomotion\n2333 Telemanipulation with Chopsticks: Analyzing Human Factors in User Demonstrations\n2334 Three-Dimensional Posture Optimization for Biped Robot Stepping Over Large Ditch Based on a Ducted-Fan Propulsion System\n2342 STORM: Screw Theory Toolbox for Robot Manipulator and Mechanisms\n2349 Development of Deployable Bending Wrist for Minimally Invasive Laparoscopic Endoscope\n2353 Predictive Control of Connected Mixed Traffic under Random Communication Constraints\n2357 Real-time Virtual Coach using LSTM for Assisting Physical Therapists with End-effector-based Robot-assisted Gait Training\n2358 Decentralized Deep Reinforcement Learning for a Distributed and Adaptive Locomotion Controller of a Hexapod Robot\n2359 Adaptive Informative Sampling with Environment Partitioning for Heterogeneous Multi-Robot Systems\n2362 Animated Cassie: A Dynamic Relatable Robotic Character\n2370 Steering Magnetic Robots in Two Axes with One Pair of Maxwell Coils\n2443 Estimation and Control of Motor Core Temperature with Online Learning of Thermal Model Parameters: Application to Musculoskeletal Humanoids\n2447 Design and Modelling of a Minimally Actuated Serial Robot\n2448 A Passivity-Shortage Based Control Framework for Teleoperation with Time-Varying Delays\n2450 Improved FBG-Based Shape Sensing Methods for Vascular Catheterization Treatment\n2455 Energy-Efficient Locomotion Generation and Theoretical Analysis of a Quasi-Passive Dynamic Walker\n2459 Online Visual Place Recognition Via Saliency Re-Identification\n2460 2D Laser SLAM with General Features Represented by Implicit Functions\n2465 Incorporating Object Intrinsic Features within Deep Grasp Affordance Prediction\n2467 Extremum Seeking Control for Stiffness Auto-Tuning of a Quasi-Passive Ankle Exoskeleton\n2473 50 Benchmarks for Anthropomorphic Hand Function-Based Dexterity Classification and Kinematics-Based Hand Design\n2474 Risk-Aware Motion Planning for a Limbed Robot with Stochastic Gripping Forces Using Nonlinear Programming\n2477 A Flexible Robotic Depalletizing System for Supermarket Logistics\n2480 Next-Best-Sense: a multi-criteria robotic exploration strategy for RFID tags discovery\n2481 3D-Aware Scene Change Captioning from Multiview Images\n2483 AMAE: Adaptive Motion-Agnostic Encoder for Event-Based Object Classification\n2485 Meta-Learning Deep Visual Words for Fast Video Object Segmentation\n2487 Reactive Semantic Planning in Unexplored Semantic Environments Using Deep Perceptual Feedback\n2490 Semantic Localization Considering Uncertainty of Object Recognition\n2491 Perspective-2-Ellipsoid: Bridging the Gap between Object Detections and 6-DoF Camera Pose\n2494 A Point Cloud Registration Pipeline Using Gaussian Process Regression for Bathymetric SLAM\n2495 Collaborative Mission Planning for Long-Term Operation Considering Energy Limitations\n2496 Fast Tennis Swing Motion by Ball Trajectory Prediction and Joint Trajectory Modification in Standalone Humanoid Robot Real-Time System\n2497 Feedback Whole-Body Control of Wheeled Inverted Pendulum Humanoids Using Operational Space\n2501 Intermittent Insertion Control Method with Fine Needle for Adapting Lung Deformation Due to Breathing Motion\n2504 Object Recognition, Dynamic Contact Simulation, Detection, and Control of the Flexible Musculoskeletal Hand Using a Recurrent Neural Network with Parametric Bias\n2505 A Reconfigurable Gripper for Robotic Autonomous Depalletizing in Supermarket Logistics\n2506 Snatcher: A Highly Mobile Chameleon-Inspired Shooting and Rapidly Retracting Manipulator\n2509 Hybrid Force-Moment Braking Pulse: A Haptic Illusion to Increase the Perceived Hardness of Virtual Surfaces\n2512 A Multi-Link In-Pipe Inspection Robot Composed of Active and Passive Compliant Joints\n2514 Learning Scheduling Policies for Multi-Robot Coordination with Graph Attention Networks\n2515 Probabilistic Crowd GAN: Multimodal Pedestrian Trajectory Prediction Using a Graph Vehicle-Pedestrian Attention Network\n2516 DiversityGAN: Diversity-Aware Vehicle Motion Prediction Via Latent Semantic Sampling\n2517 Experience-Based Prediction of Unknown Environments for Enhanced Belief Space Planning\n2520 Forecasting Trajectory and Behavior of Road-Agents Using Spectral Clustering in Graph-LSTMs\n2522 Efficient Sampling-Based Maximum Entropy Inverse Reinforcement Learning with Application to Autonomous Driving\n2525 Delta Descriptors: Change-Based Place Representation for Robust Visual Localization\n2526 The Invariant Rauch-Tung-Striebel Smoother\n2527 New Formulation of Mixed-Integer Conic Programming for Globally Optimal Grasp Planning\n2528 Probabilistic Approach to Physical Object Disentangling\n2529 Drift-Free and Self-Aligned IMU-Based Human Gait Tracking System with Augmented Precision and Robustness\n2531 Cross-View Semantic Segmentation for Sensing Surroundings\n2532 Deep Context Maps: Agent Trajectory Prediction using Location-specific Latent Maps\n2533 Distributed Consistent Multi-Robot Semantic Localization and Mapping\n2537 Segmenting the Future\n2541 Task Priority Matrix at the Acceleration Level: Collision Avoidance under Relaxed Constraints\n2542 An Algorithm to Design Redundant Manipulators of Optimally Fault-Tolerant Kinematic Structure\n2544 Energy Management through Footstep Selection for Bipedal Robots\n2545 Exploring the Role of Palm Concavity and Adaptability in Soft Synergistic Robotic Hands\n2546 Deep Learning Based Real-Time OCT Image Segmentation and Correction for Robotic Needle Insertion Systems\n2548 PrimA6D: Rotational Primitive Reconstruction for Enhanced and Robust 6D Pose Estimation\n2549 Estimating an Object's Inertial Parameters by Robotic Pushing: A Data-Driven Approach\n2550 A Variable-Structure Robot Hand That Uses the Environment to Achieve General Purpose Grasps\n2552 Development of a Spherical 2-DOF Wrist Employing Spatial Parallelogram Structure\n2553 Milliscale Features Increase Friction of Soft Skin in Lubricated Contact\n2554 Self-supervised Learning for Precise Pick-and-place without Object Model\n2555 The Mag-Gripper: A Soft-Rigid Gripper Augmented with an Electromagnet to Precisely Handle Clothes\n2556 HDR Reconstruction Based on the Polarization Camera\n2561 Seeing through the Occluders: Robust Monocular 6-DOF Object Pose Tracking Via Model-Guided Video Object Segmentation\n2562 Robust Robotic Pouring Using Audition and Haptics\n2564 Active Vertical Takeoff of an Aquatic UAV\n2565 Lightweight High Voltage Generator for Untethered Electroadhesive Perching of Micro Air Vehicles\n2567 Alternating Minimization Based Trajectory Generation for Quadrotor Aggressive Flight\n2570 Delicate Fabric Handling Using a Soft Robotic Gripper with Embedded Microneedles\n2571 SplitFusion: Simultaneous Tracking and Mapping for Non-Rigid Scenes\n2572 Object-Based Pose Graph for Dynamic Indoor Environments\n2574 Asynchronous Adaptive Sampling and Reduced-Order Modeling of Dynamic Processes by Robot Teams Via Intermittently Connected Networks\n2575 Multi-UAV Surveillance with Minimum Information Idleness and Latency Constraints\n2576 An Electrostatic/Gecko-Inspired Adhesives Soft Robotic Gripper\n2577 EGAD! an Evolved Grasping Analysis Dataset for Diversity and Reproducibility in Robotic Manipulation\n2578 Deep Gated Multi-Modal Learning: In-hand Object Pose Changes Estimation using Tactile and Image Data\n2580 In-flight Efficient Controller Auto-tuning using a Pair of UAVs\n2581 Development and Evaluation of a Linear Series Clutch Actuator for Vertical Joint Application with Static Balancing\n2582 Shift-Adaptive Estimation of Joint Angle Using Instrumented Brace With Two Stretch Sensors Based on Gaussian Mixture Models\n2585 Rectangular Pyramid Partitioning Using Integrated Depth Sensors (RAPPIDS): A Fast Planner for Multicopter Navigation\n2587 Learning Motion Parameterizations of Mobile Pick and Place Actions from Observing Humans in Virtual Environments\n2588 Contact-Implicit Trajectory Optimization using an Analytically Solvable Contact Model for Locomotion on Variable Ground\n2589 Minor Change, Major Gains: The Effect of Orientation Formulation on Solving Time for Multi-Body Trajectory Optimization\n2590 Vehicle-In-The-Loop Framework for Testing Long-Term Autonomy in a Heterogeneous Marine Robot Swarm\n2591 A Versatile Gripper for Cloth Manipulation\n2594 Task-driven Perception and Manipulation for Constrained Placement of Unknown Objects\n2597 Inertial Velocity Estimation for Indoor Navigation through Magnetic Gradient-Based EKF and LSTM Learning Model\n2598 Wireless Soft Actuator Based on Liquid-Gas Phase Transition Controlled by Millimeter-Wave Irradiation\n2599 C*: Cross-Modal Simultaneous Tracking and Rendering for 6-DoF Monocular Camera Localization Beyond Modalities\n2600 Accurate estimation of the position and shape of the rolling joint in hyper-redundant manipulators\n2601 Contact Point Estimation Along Air Tube Based on Acoustic Sensing of Pneumatic System Noise\n2602 Footstep Modification Including Step Time and Angular Momentum Under Disturbances on Sparse Footholds\n2606 Coupled Task-Space Admittance Controller Using Dual Quaternion Logarithmic Mapping\n2608 A Framework for Recognition and Prediction of Human Motions in Human-Robot Collaboration Using Probabilistic Motion Models\n2610 Swarm Relays: Distributed Self-Healing Ground-And-Air Connectivity Chains\n2611 Learning Depth with Very Sparse Supervision\n2614 Improving Multirotor Landing Performance on Inclined Surfaces Using Reverse Thrust\n2616 A Tendon-Driven Robot Gripper with Passively Switchable Underactuated Surface and Its Physics Simulation Based Parameter Optimization\n2618 Point Cloud Projective Analysis for Part-based Grasp Planning\n2621 The ARCHES Space-Analogue Demonstration Mission: Towards Heterogeneous Teams of Autonomous Robots for Collaborative Scientific Sampling in Planetary Exploration\n2624 Terrain-Aware Path Planning and Map Update for Mars Sample Return Mission\n2626 Modeling and Experimental Verification of a Cable-Constrained Synchronous Rotating Mechanism Considering Friction Effect\n2627 MILiMAC: Flexible Catheter with Miniaturized Electromagnets As a Small-Footprint System for Microrobotic Tasks\n2628 Design, Modeling, and Control of a Coaxially Aligned Steerable (COAST) Guidewire Robot\n2630 Subject-Independent sEMG Pattern Recognition by Using a Muscle Source Activation Model\n2631 Optimal Pose Estimation Method for a Multi-Segment, Programmable Bevel-Tip Steerable Needle\n2632 Payload Optimization of Surgical Instruments with Rolling Joint Mechanisms\n2634 A Supernumerary Robotic Leg Powered by Magnetorheological Actuators to Assist Human Locomotion\n2636 IMU-Based Locomotor Intention Prediction for Real-Time Use in Transfemoral Prostheses\n2639 Modeling, Calibration, and Evaluation of a Tendon-Actuated Planar Parallel Continuum Robot\n2641 Pauses Provide Effective Control for an Underactuated Oscillating Swimming Robot\n2642 Optimal Linearization via Quadratic Programming\n2644 Hybrid Systems Differential Dynamic Programming for Whole-Body Motion Planning of Legged Robots\n2647 Denoising IMU Gyroscopes with Deep Learning for Open-Loop Attitude Estimation\n2648 Towards Real-Time Non-Gaussian SLAM for Underdetermined Navigation\n2651 Comparing Visual Odometry Systems in Actively Deforming Simulated Colon Environments\n2652 Variational Inference with Parameter Learning Applied to Vehicle Trajectory Estimation\n2653 Towards in Situ Backlash Estimation of Continuum Robots Using an Endoscopic Camera\n2654 Evolved Neuromorphic Control for High Speed Divergence-Based Landings of MAVs\n2655 Energy-Based Cooperative Control for Landing Fixed-Wing UAVs on Mobile Platforms under Communication Delays\n2656 Staging Energy Sources to Extend Flight Time of a Multirotor UAV\n2657 Dense Isometric Non-Rigid Shape-From-Motion Based on Graph Optimization and Edge Selection\n2666 Multimodal Teleoperation of Heterogeneous Robots within a Construction Environment\n2669 A Manipulability Criterion for Magnetic Actuation of Miniature Swimmers with Flexible Flagellum\n2670 Towards the Long-Endurance Flight of an Insect-Inspired, Tailless, Two-Winged, Flapping-Wing Flying Robot\n2672 Design of the uMAZE Platform and Microrobots for Independent Control and Micromanipulation Tasks\n2673 GLAS: Global-To-Local Safe Autonomy Synthesis for Multi-Robot Motion Planning with End-To-End Learning\n2674 Multi-Robot Coordinated Planning in Confined Environments under Kinematic Constraints\n2675 Evaluations of response characteristics of on-chip gel actuators for various single cell manipulations\n2686 Perception-Aware Human-Assisted Navigation of Mobile Robots on Persistent Trajectories\n2687 Development of Smartphone-Based Human-Robot Interfaces for Individuals with Disabilities\n2688 Autonomous and Cooperative Design of the Monitor Positions for a Team of UAVs to Maximize the Quantity and Quality of Detected Objects\n2691 Discontinuous and Smooth Depth Completion with Binary Anisotropic Diffusion Tensor\n2692 Integrating Features Acceleration in Visual Predictive Control\n2693 Spatiotemporal Calibration of Camera and 3D Laser Scanner\n2696 Towards In-Flight Transfer of Payloads between Multirotors\n2699 Coverage Path Planning with Track Spacing Adaptation for Autonomous Underwater Vehicles\n2700 Force-Ultrasound Fusion: Bringing Spine Robotic-US to the Next 'Level'\n2702 Ultrasound-Guided Wireless Tubular Robotic Anchoring System\n2704 Optic Nerve Sheath Fenestration with a Multi-Arm Continuum Robot\n2707 Automatic Shape Control of Deformable Wires Based on Model-Free Visual Servoing\n2708 Image Transformation and CNNs: A Strategy for Encoding Human Locomotor Intent for Autonomous Wearable Robots\n2711 An Augmented Reality Spatial Referencing System for Mobile Robots\n2712 Proxy-Based Approach for Position Synchronization of Delayed Robot Coupling without Sacrificing Performance\n2713 A Multi-Channel Reinforcement Learning Framework for Robotic Mirror Therapy\n2714 A New Delayless Adaptive Oscillator for Gait Assistance\n2715 Adaptive Precision-Enhancing Hand Rendering for Wearable Fingertip Tracking Devices\n2717 The 6-DoF Implementation of the Energy-Reflection Based Time Domain Passivity Approach with Preservation of Physical Coupling Behavior\n2718 Development of Exo-Glove for Measuring 3-Axis Forces Acting on the Human Finger without Obstructing Natural Human-Object Interaction\n2720 Detection and Control of Air Liquid Interface With an Open-Channel Microfluidic Chip for Circulating Tumor Cells Isolation From Human Whole Blood\n2721 Probabilistic End-to-End Vehicle Navigation in Complex Dynamic Environments with Multimodal Sensor Fusion\n2722 MEDUSA: A Multi-Environment Dual-Robot for Underwater Sample Acquisition\n2728 Perpendicular Curve-Based Incomplete Orientation Mapping for Teleoperation with DOF Asymmetry\n2731 A Distributed Pipeline for Scalable, Deconflicted Formation Flying\n2732 RILaaS: Robot Inference and Learning As a Service\n2733 Pac-Man Is Overkill\n2734 Energy Autonomy for Resource-Constrained Multi Robot Missions\n2736 Stable Flight of a Flapping-Wing Micro Air Vehicle under Wind Disturbance\n2737 SMALLBug: A 30-mg Crawling Robot Driven by a High-Frequency Flexible SMA Microactuator\n2739 Knowledge Transfer between Different UAVs for Trajectory Tracking\n2740 A Morphing Cargo Drone for Safe Flight in Proximity of Humans\n2741 Edge Enhanced Implicit Orientation Learning with Geometric Prior for 6D Pose Estimation\n2743 Development of Hiryu-II: A Long-Reach Articulated Modular Manipulator Driven by Thrusters\n2746 Loop-Net: Joint Unsupervised Disparity and Optical Flow Estimation of Stereo Videos with Spatiotemporal Loop Consistency\n2748 Real-Time Fusion Network for RGB-D Semantic Segmentation Incorporating Unexpected Obstacle Detection for Road-Driving Images\n2749 Game Theoretic Formation Design for Probabilistic Barrier Coverage\n2750 Dual-Arm Control for Enhanced Magnetic Manipulation\n2751 Defensive Escort Teams for Navigation in Crowds Via Multi-Agent Deep Reinforcement Learning\n2753 A Novel Coding Architecture for LiDAR Point Cloud Sequence\n2754 Time-Relative RTK-GNSS: GNSS Loop Closure in Pose Graph Optimization\n2758 ROVINS: Robust Omnidirectional Visual Inertial Navigation System\n2759 Dynamic and Versatile Humanoid Walking Via Embedding 3D Actuated SLIP Model with Hybrid LIP Based Stepping\n2760 Non-Linear Trajectory Optimization for Large Step-Ups: Application to the Humanoid Robot Atlas\n2762 SilhoNet-Fisheye: Adaptation of a ROI-Based Object Pose Estimation Network to Monocular Fisheye Images\n2765 Boosting Deep Open World Recognition by Clustering\n2768 Unsupervised Domain Adaptation through Inter-Modal Rotation for RGB-D Object Recognition\n2769 ALPHRED: A Multi-Modal Operations Quadruped Robot for Package Delivery Applications\n2771 Lio - a Personal Robot Assistant for Human-Robot Interaction and Care Applications\n2772 GMMLoc: Structure Consistent Visual Localization with Gaussian Mixture Models\n2774 Building Energy-Cost Maps from Aerial Images and Ground Robot Measurements with Semi-Supervised Deep Learning\n2776 Automatic Control Synthesis for Swarm Robots from Formation and Location-Based High-Level Specifications\n2777 Graph Neural Networks for Decentralized Multi-Robot Path Planning\n2778 LineSpyX: A Power Line Inspection Robot Based on Digital Radiography\n2779 Improving Visual SLAM in Car-Navigated Urban Environments with Appearance Maps\n2780 A Multi-System Chaotic Path Planner for Fast and Unpredictable Online Coverage of Terrains\n2781 Collision Avoidance Based on Robust Lexicographic Task Assignment\n2782 3D Instance Embedding Learning with a Structure-Aware Loss Function for Point Cloud Segmentation\n2783 Improving Tracking through Human-Robot Sensory Augmentation\n2784 Generating Reactive Approach Motions towards Allowable Manifolds Using Generalized Trajectories from Demonstrations\n2786 \"Good Robot!\": Efficient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer\n2787 An Actor-Based Programming Framework for Swarm Robotic Systems\n2788 Multi-Sensor Next-Best-View Planning As Matroid-Constrained Submodular Maximization\n2790 IDDA: A Large-Scale Multi-Domain Dataset for Autonomous Driving\n2794 Perceptive Model Predictive Control for Continuous Mobile Manipulation\n2795 MLOD: Awareness of Extrinsic Perturbation in Multi-LiDAR 3D Object Detection for Autonomous Driving\n2796 3D-MiniNet: Learning a 2D Representation from Point Clouds for Fast and Efficient 3D LIDAR Semantic Segmentation\n2797 Augmented Reality and Robotic-Assistance for Percutaneous Nephrolithotomy\n2798 Learning Gait Models with Varying Walking Speeds\n2800 On the Use of (lockable) Parallel Elasticity in Active Prosthetic Ankles\n2801 Compliant Control and Compensation for a Compact Cable-Driven Robotic Manipulator\n2802 Machine Learning Model Comparisons of User Independent & Dependent Intent Recognition Systems for Powered Prostheses\n2804 Manipulation Planning Using Object-Centered Predicates and Hierarchical Decomposition of Contextual Actions\n2807 Grasping in the Wild: Learning 6DoF Closed-Loop Grasping from Low-Cost Demonstrations\n2808 TLIO: Tight Learned Inertial Odometry\n2813 Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance\n2814 Persistent Intelligence, Surveillance, and Reconnaissance Using Multiple Autonomous Vehicles with Asynchronous Route Updates\n2815 Socially and Contextually Aware Human Motion and Pose Forecasting\n2817 A Realistic Simulation Environment for MRI-Based Robust Control of Untethered Magnetic Robots with Intra-Operational Imaging\n2820 Minimum Time - Minimum Jerk Optimal Traffic Management for AGVs\n2821 Topology-Guided Roadmap Construction with Dynamic Region Sampling\n2822 Finite-Horizon LQR Control of Quadrotors on SE_2(3)\n2826 Augmenting Visual Place Recognition with Structural Cues\n2827 A Control Scheme for Smooth Transition in Physical Human-Robot-Environment between Two Modes: Augmentation and Autonomous\n2828 Human Navigation Using Phantom Tactile Sensation Based Vibrotactile Feedback\n2829 Applying Force Perturbations Using a Wearable Robotic Neck Brace\n2832 Open-Loop Orientation Control Using Dynamic Magnetic Fields\n2833 Fluid-Structure Interaction Hydrodynamics Analysis on a Deformed Bionic Flipper with Non-Uniformly Distributed Stiffness\n2834 Computational Structure Design of a Bio-inspired Armwing Mechanism\n2836 Fully Actuated Body-Mounted Robotic System for MRI-Guided Lower Back Pain Injections: Initial Phantom and Cadaver Studies\n2838 Supervised Semi-Autonomous Control for Surgical Robot Based on Bayesian Optimization\n2840 Improving Low-Level Control of the Exoskeleton Atalante in Single Support by Compensating Joint Flexibility\n2842 Multi-Robot Active Sensing and Environmental Model Learning with Distributed Gaussian Process\n2843 Mixed-Integer Linear Programming Models for Multi-Robot Non-Adversarial Search\n2845 Energetic Passivity Decoding of Human Hip Joint for Physical Human-Robot Interaction\n2846 sEMG-based Human-in-the-Loop Control of Elbow Assistive Robots for Physical Tasks and Muscle Strength Training\n2847 Pneumatic Duplex-Chambered Inchworm Mechanism for Narrow Pipes Driven by Only Two Air Supply Lines\n2848 Long-Term Localization with Time Series Map Prediction for Mobile Robots in Dynamic Environments\n2849 Inverted and Inclined Climbing Using Capillary Adhesion in a Quadrupedal Insect-Scale Robot\n2850 Perceptive Locomotion in Rough Terrain -- Online Foothold Optimization\n2852 CaseCrawler: A Lightweight and Low-Profile Crawling Phone Case Robot\n2853 Waste Not, Want Not: Lessons in Rapid Quadrupedal Gait Termination from Thousands of Suboptimal Solutions\n2854 Learning Force Control for Contact-rich Manipulation Tasks with Rigid Position-controlled Robots\n2856 Elastomeric Continuously Variable Transmission Combined with Twisted String Actuator\n2860 R-Track: Separable Modular Climbing Robot Design for Wall-To-Wall Transition\n2861 A Soft, Modular, and Bi-Stable Dome Actuator for Programmable Multi-Modal Locomotion\n2862 Robotic Deep Rolling with Iterative Learning Motion and Force Control\n2867 The ASTAR High Speed Amphibious Sprawl Tuned Robot: Design and Experiments\n2869 Enhancement of Force Exertion Capability of a Mobile Manipulator by Kinematic Reconfiguration\n2871 Polylidar - Polygons from Triangular Meshes\n2872 ACMarker: Acoustic Camera-Based Fiducial Marker System in Underwater Environment\n2873 Toward Enabling a Hundred Drones to Land in a Minute\n2874 TiltDrone: A Fully-Actuated Tilting Quadrotor Platform\n2875 Interact with Me: An Exploratory Study on Interaction Factors for Active Physical Human-Robot Interaction\n2877 Coordinated Appendages Accumulate More Energy to Self-Right on the Ground\n2878 Unmanned Aerial Sensor Placement for Cluttered Environments\n2880 Asynchronous and Parallel Distributed Pose Graph Optimization\n2881 Gaussian Process Online Learning with a Sparse Data Stream\n2882 Towards Better Surgical Instrument Segmentation in Endoscopic Vision: Multi-Angle Feature Aggregation and Contour Supervision\n2884 Development of Dementia Care Training System Based on Augmented Reality and Whole Body Wearable Tactile Sensor\n2886 Data-Driven Disturbance Observers for Estimating External Forces on Soft Robots\n2888 Self-Sensing and Feedback Control for a Twin Coil Spring-Based Flexible Ultrasonic Motor\n2891 Convergence Analysis of Hybrid Control Systems in the Form of Backward Chained Behavior Trees\n2892 FireAnt3D: A 3D Self-Climbing Robot towards Non-Latticed Robotic Self-Assembly\n2895 Actor-Critic Reinforcement Learning for Control with Stability Guarantee\n2897 Matching Color Aerial Images and Underwater Sonar Images using Deep Learning for Underwater Localization\n2898 Fail-Safe Flight of a Fully-Actuated Quadcopter in a Single Motor Failure\n2899 Improvement in Measurement Area of 3D LiDAR for a Mobile Robot Using a Mirror Mounted on a Manipulator\n2901 Dynamic Median Consensus for Marine Multi-Robot Systems Using Acoustic Communication\n2906 Robust Loop Closure Method for Multi-robot Map Fusion by Integration of Consistency and Data Similarity\n2907 ModMan: An Advanced Reconfigurable Manipulator System with Genderless Connector and Automatic Kinematic Modeling Algorithm\n2909 Kubits: Solid-State Self-Reconfiguration with Programmable Magnets\n2910 Development of a Steep Slope Mobile Robot with Propulsion Adhesion\n2911 Online Exploration and Coverage Planning in Unknown Obstacle-Cluttered Environments\n2912 Optimization-Based Investigation of Bioinspired Variable Gearing of the Distributed Actuation Mechanism to Maximize Velocity and Force\n2913 Robotic Episodic Cognitive Learning Inspired by Hippocampal Spatial Cells\n2914 A Bio-Inspired Quadruped Robot Exploiting Flexible Shoulder for Stable and Efficient Walking\n2916 Development of a Running Hexapod Robot with Differentiated Front and Hind Leg Morphology and Functionality\n2919 Prediction of Backhoe Loading Motion via the Beta-Process Hidden Markov Model\n2921 Robust RL-Based Map-Less Local Planning: Using 2D Point Clouds as Observations\n2924 An Ionic Polymer Metal Composite (IPMC)-Driven Linear Peristaltic Microfluidic Pump\n2925 Low-Cost Coil-Shaped Optical Fiber Displacement Sensor for a Twisted and Coiled Polymer Fiber Actuator Unit\n2926 A Model-Based Sensor Fusion Approach for Force and Shape Estimation in Soft Robotics\n2933 Design of Fully Soft Actuator with Double-Helix Tendon Routing Path for Twisting Motion\n2939 VeREFINE: Integrating Object Pose Verification with Physics-Guided Iterative Refinement\n2941 A Dexterous Soft Robotic Hand for Delicate In-Hand Manipulation\n2942 Knowledge-Based Grasp Planning Using Dynamic Self-Organizing Network\n2943 Motion Planning for Dual-Arm Manipulation of Elastic Rods\n2945 Environment-Aware Grasp Strategy Planning in Clutter for a Variable Stiffness Hand\n2948 Online Acquisition of Close-Range Proximity Sensor Models for Precise Object Grasping and Verification\n2950 Object-Agnostic Dexterous Manipulation of Partially Constrained Trajectories\n2951 Describing Physics for Physical Reasoning: Force-Based Sequential Manipulation Planning\n2952 Vision and Force Based Autonomous Robotic Coating with Rollers\n2953 Model-Based Coupling for Co-Simulation of Robotic Contact Tasks\n2954 ECG: Edge-Aware Point Cloud Completion with Graph Convolution\n2955 Don't Forget the Past: Recurrent Depth Estimation from Monocular Video\n2961 AirCapRL: Autonomous Aerial Human Motion Capture Using Deep Reinforcement Learning\n2962 Model-Based Reinforcement Learning for Time-Optimal Velocity Control\n2964 Deep Reinforcement Learning for Tactile Robotics: Learning to Type on a Braille Keyboard\n2965 Ultra-thin Joint Torque Sensor with Enhanced Sensitivity for Robotic Application\n2967 State-Continuity Approximation of Markov Decision Processes Via Finite Element Methods for Autonomous System Planning\n2970 BLT Gripper: An Adaptive Gripper with Active Transition Capability between Precise Pinch and Compliant Grasp\n2971 Hierarchical Tracking Control with Arbitrary Task Dimensions: Application to Trajectory Tracking on Submanifolds\n2972 RIDM: Reinforced Inverse Dynamics Modeling for Learning from a Single Observed Demonstration\n2975 Learning Variable Impedance Control for Contact Sensitive Tasks\n2976 Person-Directed Pointing Gestures and Inter-Personal Relationship: Expression of Politeness to Friendliness by Android Robots\n2978 A Feasibility Study of Culture-Aware Cloud Services for Conversational Robots\n2979 Deep Reinforcement Learning for Safe Local Planning of a Ground Vehicle in Unknown Rough Terrain\n2984 Cluster-Based Penalty Scaling for Robust Pose Graph Optimization\n2985 Improved Data Association Using Buffered Pose Adjustment for Map-Aided Localization\n2986 HDMI-Loc: Exploiting High Definition Map Image for Precise Localization via Bitwise Particle Filter\n2987 A Control Barrier Function Approach for Maximizing Performance While Fulfilling to ISO/TS 15066 Regulations\n2988 Teleoperation and Contact Detection of a Waterjet-Actuated Soft Continuum Manipulator for Low-Cost Gastroscopy\n2990 Visual Coverage Path Planning for Urban Environments\n2993 Risk-Sensitive Sequential Action Control with Multi-Modal Human Trajectory Forecasting for Safe Crowd-Robot Interaction\n2995 Nonlinear MPC for Collision Avoidance and Control of UAVs with Dynamic Obstacles\n2996 Non-Gaussian Chance-Constrained Trajectory Planning for Autonomous Vehicles in the Presence of Uncertain Agents\n2998 Trajectory Planning Over Regular General Surfaces with Application in Robot-Guided Deposition Printing\n2999 An Opportunistic Strategy for Motion Planning in the Presence of Soft Task Constraints\n3002 Inverse Kinematics of Redundant Manipulators with Dynamic Bounds on Joint Movements\n3003 Human Perception-Optimized Planning for Comfortable VR Based Telepresence\n3004 A Disturbance-Aware Trajectory Planning Scheme based on Model Predictive Control\n3006 Model-Adaptive High-Speed Collision Detection for Serial-Chain Robot Manipulators\n3007 DeepMNavigate: Deep Reinforced Multi-Robot Navigation Unifying Local & Global Collision Avoidance\n3009 Quaternion-Based Trajectory Optimization of Human Postures for Inducing Target Muscle Activation Patterns\n3010 Fast Sequence Rejection for Multi-Goal Planning with Dubins Vehicle\n3012 Combining Speed and Separation Monitoring with Power and Force Limiting for Safe Collaborative Robotics Applications\n3013 A Unified NMPC Scheme for MAVs Navigation with 3D Collision Avoidance under Position Uncertainty\n3015 Frozone: Freezing-Free, Pedestrian-Friendly Navigation in Human Crowds\n3016 Modular, Accessible, Sensorized Objects for Evaluating the Grasping and Manipulation Capabilities of Grippers and Hands\n3017 Design, Modelling, and Implementation of a 7-DOF Cable-Driven Haptic Device with a Configurable Cable Platform\n3019 Asymptotically-Optimal Topological Nearest-Neighbor Filtering\n3020 Online Replanning with Human-In-The-Loop for Non-Prehensile Manipulation in Clutter ' a Trajectory Optimization Based Approach\n3021 Neural Manipulation Planning on Constraint Manifolds\n3022 Piezoelectric Grippers for Mobile Micromanipulation\n3023 Stable In-Grasp Manipulation with a Low-Cost Robot Hand by Using 3-Axis Tactile Sensors with a CNN\n3026 Feedback Enhanced Motion Planning for Autonomous Vehicles\n3027 Magnetically Programmable Cuboids for 2D Locomotion and Collaborative Assembly\n3030 Six-Axis Force/Torque Fingertip Sensor for an Anthropomorphic Robot\n3032 Low Latency Trajectory Predictions for Interaction Aware Highway Driving\n3033 APPLD: Adaptive Planner Parameter Learning from Demonstration\n3034 Precision Assembly of Heavy Objects Suspended with Multiple Cables from a Crane\n3035 Explicit Domain Adaptation with Loosely Coupled Samples\n3037 Adaptive Aerial Grasping and Perching with Dual Elasticity Combined Suction Cup\n3038 Target Search on Road Networks with Range-Constrained UAVs and Ground-Based Mobile Recharging Vehicles\n3041 Simultaneously Learning Corrections and Error Models for Geometry-Based Visual Odometry Methods\n3042 Safe Optimal Control under Parametric Uncertainties\n3044 Vitruvio: An Open-Source Leg Design Optimization Toolbox for Walking Robots\n3046 Alleviating the Burden of Labeling: Sentence Generation by Attention Branch Encoder-Decoder Network\n3050 Robot Gaze Behaviors in Human-To-Robot Handovers\n3051 Self-Attention Based Visual-Tactile Fusion Learning for Predicting Grasp Outcomes\n3052 Socially Assistive Robots at Work: Making Break-Taking Interventions More Pleasant, Enjoyable, and Engaging\n3053 Learn by Observation: Imitation Learning for Drone Patrolling from Videos of a Human Navigator\n3056 Lidar Essential Beam Model for Accurate Width Estimation of Thin Poles\n3060 Invariant Transform Experience Replay: Data Augmentation for Deep Reinforcement Learning\n3061 Imitation Learning Based on Bilateral Control for Human'Robot Cooperation\n3063 Autonomous Tissue Retraction in Robotic Assisted Minimally Invasive Surgery - A Feasibilty Study\n3064 6-Axis Force/Torque Sensor with a Novel Autonomous Weight Compensating Capability for Robotic Applications\n3066 Multi-Fingered Grasp Planning Via Inference in Deep Neural Networks\n3067 Collision Avoidance in Human-Robot Interaction Using Kinect Vision System Combined with Robot's Model and Data\n3068 Miniaturized robotics: The smallest camera operator bot pays tribute to David Bowie.\n3070 Development of \u0394-Type Mobile Robot Driven by 3 Standing Wave Type Piezoelectric Ultrasonic Motors\n3072 Lyapunov-Stable Orientation Estimator for Humanoid Robots\n3073 UFOMap: An Efficient Probabilistic 3D Mapping Framework That Embraces the Unknown\n3074 Machine Learning for Active Gravity Compensation in Robotics: Application to Neurological Rehabilitation Systems\n3075 Learning robust manipulation tasks involving contact using trajectory parameterized probabilistic principal component analysis\n3076 Unsupervised Pedestrian Pose Prediction -- A Deep Predictive Coding Network Approach for Autonomous Vehicle Perception\n3077 Stable Autonomous Spiral Stair Climbing of Tracked Vehicles Using Wall Reaction Force\n3079 TSBP: Tangent Space Belief Propagation for Manifold Learning\n3080 Exploiting Visual-Outer Shape for Tactile-Inner Shape Estimation of Objects Covered with Soft Materials\n3081 Heteroscedastic Uncertainty for Robust Generative Latent Dynamics\n3082 q-VAE for Disentangled Representation Learning and Latent Dynamical Systems\n3083 Can a Robot's Touches Express the Feeling of Kawaii Toward an Object?\n3084 Optimisation of Body-ground Contact for Augmenting the Whole-Body Loco-manipulation of Quadruped Robots\n3085 Communication Maintenance of Robotic Parasitic Antenna Arrays\n3087 Bilateral humanoid teleoperation system using whole-body exoskeleton cockpit TABLIS\n3088 Adaptive-Gains Enforcing Constraints in Closed-Loop QP Control\n3089 Multi-Contact Locomotion Planning for Humanoid Robot Based on Sustainable Contact Graph with Local Contact Modification\n3091 Self-Assessment of Grasp Affordance Transfer\n3093 XBot Real-Time Software Framework for Robotics: From the Developer to the User Perspective\n3094 Natural Criteria for Comparison of Pedestrian Flow Forecasting Models\n3097 Lloyd-Based Approach for Robots Navigation in Human-Shared Environments\n3098 Flight Path Planning of Solar Powered UAV for Sustainable Communication Relay\n3099 RoVaLL: Design and Development of a Multi-Terrain Towed Robot with Variable Lug-Length Wheels\n3102 Reconfigurable Soft Flexure Hinges Via Pinched Tubes\n3104 Rolling Soft Membrane-Driven Tensegrity Robots\n3105 Exploiting the Morphology of a Shape Memory Spring as the Active Backbone of a Highly Dexterous Tendril Robot (ATBR)\n3106 Retraction Mechanism of Soft Torus Robot with a Hydrostatic Skeleton\n3107 Integrated Actuation and Self-Sensing for Twisted-And-Coiled Actuators with Applications to Innervated Soft Robots\n3109 Self-Propelled Colonoscopy Robot Using Flexible Paddles\n3111 Self-Healing Cell Tactile Sensor by Ultraflexible Printed Electrodes\n3113 Assured Runtime Monitoring and Planning: Towards Verification of Neural Networks for Safe Autonomous Operations\n3114 Electromagnetic Actuation of Microrobots in a Simulated Vascular Structure with a Position Estimator Based Motion Controller\n3115 Wide Area Exploration System Using Passive-Follower Robots Towed by Multiple Winches\n3117 End-To-End Velocity Estimation for Autonomous Racing\n3118 End-to-end Tactile Feedback Loop: From Soft Sensor Skin over Deep GRU-Autoencoders to Tactile Stimulation\n3119 Parallel Haptic Rendering for Orthopedic Surgery Simulators\n3120 Simultaneous 3D Forming and Patterning Method of Realizing Soft IPMC Robots\n3121 Visual SLAM with Drift-Free Rotation Estimation in Manhattan World\n3123 Structure-SLAM: Low-Drift Monocular SLAM in Indoor Environments\n3124 SoftHandler: An Integrated Soft Robotic System for the Handling of Heterogeneous Objects\n3125 A Grasping-Centered Analysis for Cloth Manipulation\n3126 Information Correlated Levy Walk Exploration and Distributed Mapping Using a Swarm of Robots\n3127 Virtual IR Sensing for Planetary Rovers: Improved Terrain Classification and Thermal Inertia Estimation\n3128 Definition and Application of Variable Resistance Coefficient for Wheeled Mobile Robots on Deformable Terrain\n3129 A Passive pHRI Controller for Assisting the User in Partially Known Tasks\n3130 On the False Positives and False Negatives of the Jacobian Matrix in Kinematically Redundant Parallel Mechanisms\n3131 Gaussians on Riemannian Manifolds: Applications for Robot Learning and Adaptive Control\n3132 Torque-Bounded Admittance Control Realized by a Set-Valued Algebraic Feedback\n3133 Decoding Motor Skills of AI and Human Policies: A Study on Humanoid and Human Balance Control\n3134 Distributed Control for Cooperative Manipulation With Event-Triggered Communication\n3135 Calculating the Support Function of Complex Continuous Surfaces with Applications to Minimum Distance Computation and Optimal Grasp Planning\n3136 A Routing Framework for Heterogeneous Multi-Robot Teams in Exploration Tasks\n3137 Continuously Variable Stiffness Mechanism Using Nonuniform Patterns on Coaxial Tubes for Continuum Microsurgical Robot\n3138 Achieving Versatile Energy Efficiency With the WANDERER Biped Robot\n3139 In Vitro Design Investigation of a Rotating Helical Magnetic Swimmer for Combined 3-D Navigation and Blood Clot Removal\n3151 Marker-Based Mapping and Localization for Autonomous Valet Parking\n3152 Parameter Optimization for Loop Closure Detection in Closed\u00c2 Environments\n3153 Radar-Camera Sensor Fusion for Joint Object Detection\u00c2 and Distance Estimation in Autonomous Vehicles\n3154 SalsaNext Fast Uncertainty-aware Semantic Segmentation\u00c2 of LiDAR Point Clouds for Autonomous Driving\n3155 SDVTracker Real-Time Multi-Sensor Association and Tracking for\u00c2 Self-Driving Vehicles\n3156 Situation Awareness at Autonomous Vehicle Handover - Preliminary\u00c2 Results of a Quantitative Analysis\n3157 Towards Context-Aware Navigation for\u00c2 Long-Term Autonomy in Agricultural Environments\n3158 Efficient Sampling in POMDPs with Lipschitz Bandits\u00c2 for Motion Planning in Continuous Spaces\n3159 Impact of Traffic Lights on Trajectory Forecasting of Human-driven\u00c2 Vehicles Near Signalized Intersections\n3161 Semantic Grid Map based LiDAR Localization in Highly Dynamic\u00c2 Urban Scenarios\n3162 Acquiring Mechanical Knowledge from 3D Point Clouds\n3163 Representation and Experience-Based Learning of Explainable Models for Robot Action Execution\n3164 Emergent Adaptive Gait Generation through Hebbian Sensor-Motor Maps by Morphological Probing\n3165 Mixed Reality As a Bidirectional Communication Interface for Human-Robot Interaction\n3166 The Robot As Scientist: Using Mental Simulation to Test Causal Hypotheses Extracted from Human Activities in Virtual Reality\n3167 Graph-Based Hierarchical Knowledge Representation for Robot Task Transfer from Virtual to Physical World\n3168 An Optimized Tilt Mechanism for a New Steady-Hand Eye Robot\n3169 Optimization-Based Hierarchical Motion Planning for Autonomous Racing\n3170 SwingBot: Learning Physical Features from In-Hand Tactile Exploration for Dynamic Swing-Up Manipulation\n3182 Learning Visuomotor Policies for Aerial Navigation Using Cross-Modal Representations\n3183 Learning Vision-Based Physics Intuition Models for Non-Disruptive Object Extraction\n3184 Computational Design of Balanced Open Link Planar Mechanisms with Counterweights from User Sketches\n3238 Relative Pose Estimation and Planar Reconstruction Via Superpixel-Driven Multiple Homographies\n3239 Real-Time Constrained Nonlinear Model Predictive Control on SO(3) for Dynamic Legged Locomotion\n3240 Combining Compliance Control, CAD Based Localization, and a Multi-Modal Gripper for Rapid and Robust Programming of Assembly Tasks\n3241 FreeBOT: A Freeform Modular Self-Reconfigurable Robot with Arbitrary Connection Point - Design and Implementation\n3242 Computational Design of Balanced Open Link Planar Mechanisms with Counterweights from User Sketches\n3243 A Tip Mount for Transporting Sensors and Tools Using Soft Growing Robots\n3244 Robot Calligraphy Using Pseudospectral Optimal Controlin Conjunction with a Novel Dynamic Brush Model\n3245 Diabolo Orientation Stabilization by Learning Predictive Model for Unstable Unknown-Dynamics Juggling Manipulation\n3246 Towards Micro Robot Hydrobatics: Vision-based Guidance, Navigation, and Control for Agile Underwater Vehicles in Confined Environments\n3247 Animated Cassie: A Dynamic Relatable Robotic Character\n3248 Safety Considerations in Deep Control Policies with Safety Barrier Certificates under Uncertainty\n3249 Navigation on the Line: Traversability Analysis and Path Planning for Extreme-Terrain Rappelling Rovers\n3250 Autonomous Spot: Long-Range Autonomous Exploration of Extreme Environments with Legged Locomotion\n3251 Stable Autonomous Spiral Stair Climbing of Tracked Vehicles Using Wall Reaction Force\n3252 Unsupervised Domain Adaptation for Transferring Plant Classification Systems to New Field Environments, Crops, and Robots\n3253 DIAT (Depth-Infrared Image Annotation Transfer) for Training a Depth-Based Pig-Pose Detector\n3254 Incorporating Spatial Constraints into a Bayesian Tracking Framework for Improved Localisation in Agricultural Environments\n3255 Fruit Quality Control by Surface Analysis Using a Bio-Inspired Soft Tactile Sensor\n3256 Pit30M: A Benchmark for Global Localization in the Age of Self-Driving Cars\n3257 OceanVoy: A Hybrid Energy Planning System for Autonomous Sailboat\n3258 MHYRO: Modular HYbrid RObot for Contact Inspection and Maintenance in Oil&gas Plants\n3259 LLAMA: Design and Control of an Omnidirectional Human Mission Scale Quadrupedal Robot\n3475 RGB-D sensing of challenging deformable objects\n3476 Building 3D Deformable Object Models in Partially Observable Robotic Environments\n3477 SOMA: A Data-Driven Representation Framework for Semantic Soft Object Manipulation\n3478 Task-oriented Contact Adjustment in Deformable Objects Manipulation with Non-fixed Contact\n3479 Adaptive Shape Servoing of Elastic Rods using Parameterized Regression Features and Auto-Tuning Motion Controls\n3480 Automatic Shape Control of Deformable Rods Based on Data-Driven Implicit Sensorimotor Models\n3481 Assembly Strategy for Deformable Ring-Shaped Objects\n3482 MGSD: Multi-Modal Gaussian Shape Descriptors for Correspondence Matching of Linear and Planar Deformable Objects\n3494 Dual-armed manipulation planning for tethered tools\n3495 Prediction of tactile perception from vision on deformable objects\n3496 Shape control of elastoplastic deformable linear objects through reinforcement learning\n3497 Interaction identification through tactile sensing during cloth manipulation using a 3-axis touch sensor\n3498 Toward a general framework for 3D deformable object grasping and manipulation\n3499 Experimental multi-camera setup for perception of dynamic objects\n3500 Real-time state estimation of deformable objects with dynamical simulation\n3601 Human-Robot Collaborative Carrying Using Visual and Force Sensing\n3622 Toward Detecting Anomalies in Activities for Daily Living with a Mobile Robot Using Plan Recognition\n3623 Human-Aware Robot Behavior in Healthcare Facilities\n3624 An Interactive Drink Serving Social Robot: Initial System Implementation\n3625 Towards Whole Arm Manipulation for Outpatient Care\n3626 Morphological Switching Robots to Support Independent Living for Older Adults\n3627 Towards Conversational Interfaces and Visual Memory Representation for Social Robots helping the Elderly\n3628 On New Research Guidelines for the Deployment of Socially Assistive Robots for Elder Care Amidst the COVID-19 Pandemic\n3629 Towards Physical Human-Robot Interaction using Force Support for Nursing Care Bed Activities\n3712 Service robot teaching assistant in school class-room\n3713 Infant abnormal behavior classification through weakly supervised learning\n3714 Amusing Androids: The Argument for Humour in Healthcare Robotics\n3715 Towards Explainable Diagnosis of Alzheimer's\n3716 VOTE400(Voide Of The Elderly 400 Hours): A Speech Dataset to Study Voice Interface for Elderly-Care\n3717 Toward a Reinforcement Learning Based Framework for Learning Cognitive Empathy in Human-Robot Interactions\n3718 Improve identity recognition with occlusion detection-based feature selection\n3719 ETRI Activity3D: A Large Scale RGB D Dataset for Robots to Recognize Daily Activities of the Elderly\n3720 Deep Emotion Change Detection for Human-robot Interaction\n3721 Efficiency Analysis of Multi-Head Attention Models for Social Dynamics Prediction\n3722 Leveraging Reinforcement Learning for Human Motor Skill Acquisition\n3723 Efficient Learning of Socially Aware Robot Approaching Behavior Toward Groups via Meta-Reinforcement Learning", "link": "https://github.com/PaoPaoRobot/IROS2020-paper-list", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "iros2020-paper-list\nthe 2020 ieee/rsj international conference on intelligent robots and systems (iros 2020) has been held on oct 25 \u2013 nov 25, not been held in-person. with the continued resurgence of covid-19 within the state of nevada, in particular las vegas, along with the city\u2019s prohibiting large or even moderately-sized public gatherings, it makes it an impossibility to hold the in-person event as originally planned.\niros is one of the largest and most impacting robotics research conferences worldwide. it brings an international community of researchers, educators and practitioners to explore the frontier of science and technology in intelligent robots and systems, and discuss the latest advancements in this fast growing and exciting field.\niros 2020 is free with access to every technical talk, plenary and keynote, over 60 workshops and tutorials, the competitions, and includes publishing of accepted papers in the iros proceedings and ieee xplore.\nthis list is edited by paopaorobot, \u6ce1\u6ce1\u673a\u5668\u4eba, the chinese academic nonprofit organization. recently we will classify these papers by topics. welcome to follow our github and our wechat public platform account ( paopaorobot_slam ). of course, you could contact with yvon shong.\ndownload link title\n0018 aras: ambiguity-aware robust active slam based on multi-hypothesis state and map estimations\n0025 dynamic attention-based visual odometry\n0031 max orientation coverage: efficient path planning to avoid collisions in the cnc milling of 3d objects\n0039 learning an uncertainty-aware object detector for autonomous driving\n0046 look and listen: a multi-modality late fusion approach to scene classification for autonomous machines\n0047 variable stiffness control with strict frequency domain constraints for physical human-robot interaction\n0054 proactive estimation of occlusions and scene coverage for planning next best views in an unstructured representation\n0055 static characteristics of fire hose actuators and design of a compliant pneumatic rotary drive for robotics\n0060 domain transfer for semantic segmentation of lidar data using deep neural networks\n0063 lio-sam: tightly-coupled lidar inertial odometry via smoothing and mapping\n0069 roadmap subsampling for changing environments\n0070 sim2real transfer for reinforcement learning without dynamics randomization\n0071 pressense: passive respiration sensing via ambient wifi signals in noisy environments\n0072 orcvio: object residual constrained visual-inertial odometry\n0073 bioars: designing adaptive and reconfigurable bionic assembly robotic system with inchworm modules\n0074 segmentation-based 4d registration of plants point clouds for phenotyping\n0078 self-supervised attention learning for depth and ego-motion estimation\n0079 swarmlab: a matlab drone swarm simulator\n0082 non-overlapping rgb-d camera network calibration with monocular visual odometry\n0083 adaptive reliable shortest path in gaussian process regulated environment\n0089 assessment of soil strength using a robotically deployed and retrieved penetrometer\n0092 sad-slam: a visual slam based on semantic and depth information\n0094 zero-tuning grinding process methodology of cyber-physical robot system\n0095 arpdr: an accurate and robust pedestrian dead reckoning system for indoor localization on handheld smartphones\n0096 kliep-based density ratio estimation for semantically consistent synthetic to real images adaptation in urban traffic scenes\n0097 graph-based hierarchical knowledge representation for robot task transfer from virtual to physical world\n0099 an rls-based instantaneous velocity estimator for extended radar tracking\n0100 informative path planning for gas distribution mapping in cluttered environments\n0101 dssf-net: dual-task segmentation and self-supervised fitting network for end-to-end lane mark detection\n0102 single-shot panoptic segmentation\n0103 leveraging stereo-camera data for real-time dynamic obstacle detection and tracking\n0106 combining compliance control, cad based localization, and a multi-modal gripper for rapid and robust programming of assembly tasks\n0108 model-free, vision-based object identification and contact force estimation with a hyper-adaptive robotic gripper\n0109 human-robot interaction in a shared augmented reality workspace\n0112 one-shot informed robotic visual search in the wild\n0119 kovis: keypoint-based visual servoing with zero-shot sim-to-real transfer for robotics manipulation\n0122 learning local planners for human-aware navigation in indoor environments\n0123 confidence guided stereo 3d object detection with split depth estimation\n0124 learning hybrid object kinematics for efficient hierarchical planning under uncertainty\n0128 human-aware robot navigation by long-term movement prediction\n0130 a human-robot interface based on surface electroencephalographic sensors\n0132 pit30m: a benchmark for global localization in the age of self-driving cars\n0135 litamin: lidar based tracking and mapping by stabilized icp for geometry approximation with normal distributions\n0141 autonomous obstacle avoidance for uav based on fusion of radar and monocular camera\n0143 anomaly detection for autonomous guided vehicles using bayesian surprise\n0144 regionnet: region-feature-enhanced 3d scene understanding network with dual spatial-aware discriminative loss\n0146 richer aggregated features for optical flow estimation with edge-aware refinement\n0152 quantitative operator strategy comparisons across human supervisory control scenarios\n0153 multi-task control for a quadruped robot with changeable leg configuration\n0155 assisted mobile robot teleoperation with intent-aligned trajectories via biased incremental action sampling\n0157 deeplidarflow: a deep learning architecture for scene flow estimation using monocular camera and sparse lidar\n0160 design of an underactuated peristaltic robot on soft terrain\n0161 accurate, low-latency visual perception for autonomous racing: challenges, mechanisms, and practical solutions\n0162 catch the ball: accurate high-speed motions for mobile manipulators via inverse dynamics learning\n0164 nbvc: a benchmark for depth estimation from narrow-baseline video clips\n0166 on-plate localization and mapping for an inspection robot using ultrasonic guided waves: a proof of concept\n0170 semantic graph based place recognition for 3d point clouds\n0173 barometer-based tactile skin for anthropomorphic robot hand\n0176 calibrcnn: calibrating camera and lidar by recurrent convolutional neural network and geometric constraints\n0179 mechanical design and preliminary performance evaluation of a passive arm-support exoskeleton\n0184 augmented reality user interfaces for heterogeneous multirobot control\n0187 a robotic gripper design and integrated solution towards tunnel boring construction equipment\n0191 autonomous rgbd-based industrial staircase localization from tracked robots\n0192 robust music-based sound source localization in reverberant and echoic environments\n0193 optimal-power configurations for hover solutions in mono-spinners\n0195 hypothesis-driven skill discovery for hierarchical deep reinforcement learning\n0196 a modified hybrid reciprocal velocity obstacles approach for multi-robot motion planning without communication\n0198 rapidly adaptable legged robots via evolutionary meta-learning\n0200 robots can defuse high-intensity conflict situations\n0201 indoor scene recognition in 3d\n0202 solarslam: battery-free loop closure for indoor localisation\n0204 inner-approximation of manipulable and reachable regions using bilinear matrix inequalities\n0205 scale-net: scalable vehicle trajectory prediction network under random number of interacting vehicles via edge-enhanced graph convolutional neural network\n0206 exceeding the maximum speed limit of the joint angle for the redundant tendon-driven structures of musculoskeletal humanoids\n0207 -----> tool !!!  shape optimization through backpropagation of neural network\n0208 planet of the bayesians: reconsidering and improving deep planning network by incorporating bayesian inference\n0209 tell me what this is: few-shot incremental object learning by a robot\n0213 a horse inspired eight-wheel unmanned ground vehicle with four-swing arms\n0219 simultaneous estimation of vehicle position and data delays using gaussian process based moving horizon estimation\n0220 globally optimal consensus maximization for robust visual inertial localization in point and line map\n0223 learning hierarchical behavior and motion planning for autonomous driving\n0224 a bayesian-based controller for snake robot locomotion in unstructured environments\n0226 personalized online learning with pseudo-ground truth\n0228 improving autonomous rover guidance in round-trip missions using dynamic cost map\n0229 an external stabilization unit for high-precision applications of robot manipulators\n0230 design and implementation of a pipeline inspection robot with camera image compensation\n0231 magnetically actuated pick-and-place operations of cellular micro-rings for high-speed assembly of micro-scale biological tube\n0233 exploration of unknown environments with a tethered mobile robot\n0234 robot-to-robot relative pose estimation based on semidefinite relaxation optimization\n0235 robust pedestrian tracking in crowd scenarios using an adaptive gmm-based framework\n0236 variable in-hand manipulations for tactile-driven robot hand via cnn-lstm\n0238 sma actuated low-weight bio-inspired claws for grasping and perching using flapping wing aerial systems\n0240 reconstruction of 3d flight trajectories from ad-hoc camera networks\n0241 3d odor source localization using a micro aerial vehicle: system design and performance evaluation\n0244 set-membership extrinsic calibration of a 3d lidar and a camera\n0248 bark: open behavior benchmarking in multi-agent environments\n0249 lidar panoptic segmentation for autonomous driving\n0250 learning and sequencing of object-centric manipulation skills for industrial tasks\n0257 ground texture based localization: do we need to detect keypoints?\n0261 diabolo orientation stabilization by learning predictive model for unstable unknown-dynamics juggling manipulation\n0263 sample-efficient learning for industrial assembly using qgraph-bounded ddpg\n0264 rosflight: a lean open-source research autopilot\n0265 a model-based approach to acoustic reflector localization with a robotic platform\n0267 explainable and efficient sequential correlation network for 3d single person concurrent activity detection\n0268 guaranteed parameter estimation of hunt-crossley model with chebyshev polynomial approximation for teleoperation\n0272 software development framework for cooperating robots with high-level mission specification\n0277 learning to collide: an adaptive safety-critical scenarios generating method\n0278 no map, no problem: a local sensing approach for navigation in human-made spaces using signs\n0281 a learning-based robotic bin-picking with flexibly customizable grasping conditions\n0284 an augmented reality human-robot physical collaboration interface design for shared, large-scale, labour-intensive manufacturing tasks\n0287 motion planning for heterogeneous unmanned systems under partial observation from uav\n0289 ttr-based reward for reinforcement learning with implicit model priors\n0290 etri-activity3d: a large-scale rgb-d dataset for robots to recognize daily activities of the elderly\n0292 a flexible dual-core optical waveguide sensor for simultaneous and continuous measurement of contact force and position\n0295 multimodal aggregation approach for memory vision-voice indoor navigation with meta-learning\n0297 dr^2track: towards real-time visual tracking for uav via distractor repressed dynamic regression\n0301 robust real-time monitoring of human task advancement for collaborative robotics applications\n0304 virtual reality for robots\n0309 design of a linear gravity compensator for a prismatic joint\n0311 applications of stretch reflex for the upper limb of musculoskeletal humanoids: protective behavior, postural stability, and active induction\n0313 practical verification of neural network enabled state estimation system for robotics\n0314 a control scheme for haptic inspection and partial modification of kinematic behaviors\n0315 identification of dynamic parameters for rigid robots based on polynomial approximation\n0317 spoxelnet: spherical voxel-based deep place recognition for 3d point clouds of crowded indoor spaces\n0323 multi-instance aware localization for end-to-end imitation learning\n0326 cooperative simultaneous tracking and jamming for disabling a rogue drone\n0329 human-robot trust assessment using motion tracking & galvanic skin response\n0330 simultaneous planning for item picking and placing by deep reinforcement learning\n0334 grasping detection network with uncertainty estimation for confidence-driven semi-supervised domain adaptation\n0335 3d gaze estimation for head-mounted devices based on visual saliency\n0336 learning hierarchical acquisition functions for bayesian optimization\n0338 reinforcement learning in latent action sequence space\n0339 automatic synthesis of human motion from temporal logic specifications\n0342 uav-adnet: unsupervised anomaly detection using deep neural networks for aerial surveillance\n0348 unsupervised domain adaptation for transferring plant classification systems to new field environments, crops, and robots\n0355 on-chip integration of ultra-thin glass cantilever for physical property measurement activated by femtosecond laser impulse\n0356 learning the sense of touch in simulation: a sim-to-real strategy for vision-based tactile sensing\n0358 abductive recognition of context-dependent utterances in human-robot interaction\n0359 from points to planes - adding planar constraints to monocular slam factor graphs\n0363 enhanced tracking wall: a real-time computing method for needle injection on haptic simulators\n0364 occlusion-robust mvo: multimotion estimation through occlusion via motion closure\n0366 spectral-gans for high-resolution 3d point-cloud generation\n0368 vision-based proprioceptive sensing: tip position estimation for a soft inflatable bellow actuator\n0369 model predictive position and force trajectory tracking control for robot-environment interaction\n0373 cobigicp: robust and precise point set registration using correntropy metrics and bidirectional correspondence\n0378 walking human trajectory models and their application to humanoid robot locomotion\n0382 plug-and-play slam: a unified slam architecture for modularity and ease of use\n0383 acquiring mechanical knowledge from 3d point clouds\n0384 a visuo-haptic guidance interface for the mobile collaborative robotic assistant (moca)\n0386 robot navigation in crowded environments using deep reinforcement learning\n0389 robust task and motion planning for long-horizon problems\n0390 tightly-coupled fusion of global positional measurements in optimization-based visual-inertial odometry\n0393 legobot: automated planning for coordinated multi-robot assembly of lego structures\n0398 progressive automation of periodic tasks on planar surfaces of unknown pose with hybrid force/position control\n0400 smart speaker vs. social robot in a case of hotel room\n0403 eu long-term dataset with multiple sensors for autonomous driving\n0404 speed and memory efficient dense rgb-d slam in dynamic scenes\n0405 hand-object contact force synthesis for manipulating objects by exploiting environment\n0407 hierarchical reinforcement learning method for autonomous vehicle behavior planning\n0408 automatic lane change maneuver in dynamic environment using model predictive control method\n0414 lanoising: a data-driven approach for 903nm tof lidar performance modeling under fog\n0415 on parameter estimation of flexible space manipulator systems\n0416 the spir: an autonomous underwater robot for bridge pile cleaning and condition assessment\n0417 rapid autonomous semantic mapping\n0418 learning soft robotic assembly strategies from successful and failed demonstrations\n0419 mapping thigh motion to knee motion: implications for motion planning of active prosthetic knees\n0421 comparison between stationary and crawling multi-arm robotics for in-space assembly\n0425 toward analytical modeling and evaluation of curvature-dependent distributed friction force in tendon-driven continuum manipulators\n0428 end-to-end contextual perception and prediction with interaction transformer\n0432 operational space formulation and inverse kinematics for an arm exoskeleton with scapula rotation\n0433 a hamilton-jacobi formulation for optimal coordination of heterogeneous multiple vehicle systems\n0436 human gait phase recognition using a hidden markov model framework\n0443 davincinet: joint prediction of motion and surgical state in robot-assisted surgery\n0455 deep imitation learning of sequential fabric smoothing from an algorithmic supervisor\n0457 visual-inertial-wheel odometry with online calibration\n0458 lifelong update of semantic maps in dynamic environments\n0460 ultra low-cost printable folding robots\n0463 deep keypoint-based camera pose estimation with geometric constraints\n0464 generating new lower abstract task operator using grid-tli\n0471 spcomapgan: spatial concept formation-based semantic mapping with generative adversarial networks\n0474 applying surface normal information in drivable area and road anomaly detection for ground mobile robots\n0476 few-shot relation learning with attention for eeg-based motor imagery classification\n0478 robot sound interpretation: combining sight and sound in learning-based control\n0479 development of a passive skid for multicopter landing on rough terrain\n0480 touch the wind: simultaneous airflow, drag and interaction sensing on a multirotor\n0486 online localization with imprecise floor space maps using stochastic gradient descent\n0487 wiping 3d-objects using deep learning model based on image/force/joint information\n0492 fast ltl-based flexible planning for dual-arm manipulation\n0493 multi-robot coordination with agent-server architecture for autonomous navigation in partially unknown environments\n0496 perception-aware path planning for uavs using semantic segmentation\n0500 l2b: learning to balance the safety-efficiency trade-off in interactive crowd-aware robot navigation\n0504 unrectdepthnet: self-supervised monocular depth estimation using a generic framework for handling common camera distortion models\n0505 pc-nbv: a point cloud based deep network for efficient next best view planning\n0508 spatio-temporal ultrasonic dataset: learning driving from spatial and temporal ultrasonic cues\n0511 invisible marker: automatic annotation of segmentation masks for object manipulation\n0512 a comprehensive trajectory planner for a person-following atv\n0513 configuration space decomposition for learning-based collision checking in high-dof robots\n0516 a novel portable cell sonoporation device based on open-source acoustofluidics\n0519 a minimalistic hyper flexible manipulator: modeling and control\n0523 a two-stage automatic latching system for the usvs charging in disturbed berth\n0526 the application of navigation technology for the medical assistive devices based on aruco recognition technology\n0530 physics-based dexterous manipulations with estimated hand poses and residual reinforcement learning\n0532 a time optimal reactive collision avoidance method for uavs based on a modified collision cone approach\n0538 computationally efficient obstacle avoidance trajectory planner for uavs based on heuristic angular search method\n0544 ust: unifying spatio-temporal context for trajectory prediction in autonomous driving\n0552 dmlo: deep matching lidar odometry\n0557 latent space roadmap for visual action planning of deformable and rigid object manipulation\n0558 learning visuomotor policies for aerial navigation using cross-modal representations\n0559 a neural primitive model with sensorimotor coordination for dynamic quadruped locomotion with malfunction compensation\n0560 densefusion: large-scale online dense pointcloud and dsm mapping for uavs\n0562 a learning-driven framework with spatial optimization for surgical suture thread reconstruction and autonomous grasping under multiple topologies and environmental noises\n0566 oceanvoy: a hybrid energy planning system for autonomous sailboat\n0567 pattern analysis and parameters optimization of dynamic movement primitives for learning unknown trajectories\n0568 towards understanding and inferring the crowd: guided second order attention networks and re-identification for multi-object tracking\n0572 distributed near-optimal multi-robots coordination in heterogeneous task allocation\n0573 dynamic assistance for human balancing with inertia of a wearable robotic appendage\n0575 a real-time unscented kalman filter on manifolds for challenging auv navigation\n0576 indirect object-to-robot pose estimation from an external monocular rgb camera\n0581 a causal approach to tool affordance learning\n0583 seqspherevlad: sequence matching enhanced orientation-invariant place recognition\n0589 active 6d multi-object pose estimation in cluttered scenarios with deep reinforcement learning\n0592 continuous tension validation for cable-driven parallel robots\n0593 predicting the human behaviour in human-robot co-assemblies: an approach based on suffix trees\n0596 latent replay for real-time continual learning\n0600 the application of a flexible leader-follower control algorithm to different mobile autonomous robots\n0614 examination of screen-indicated methods of gait training system with real-time audiovisual feedback function of ground reaction force\n0619 gosmatch: graph-of-semantics matching for detecting loop closures in 3d lidar data\n0620 design, analysis and preliminary validation of a 3-dof rotational inertia generator\n0621 experiments on whole-body control of a dual-arm mobile robot with the set-based task-priority inverse kinematics algorithm\n0624 uav coverage path planning under varying power constraints using deep reinforcement learning\n0625 a model for optimising the size of climbing robots for navigating truss structures\n0626 reinforcement learning-based hierarchical control for path following of a salamander-like robot\n0629 an obstacle-crossing strategy based on the fast self-reconfiguration for modular sphere robots\n0631 online weight-adaptive nonlinear model predictive control\n0634 flowcontrol: optical flow based visual servoing\n0635 geometrical interpretation and detection of multiple task conflicts using a coordinate invariant index\n0638 laser2vec: similarity-based retrieval for robotic perception data\n0641 self-adapting recurrent models for object pushing from learning in simulation\n0642 a passivity-based bilateral teleoperation architecture using distributed nonlinear model predictive control\n0644 a theory of fermat paths for 3d imaging sonar reconstruction\n0646 lane marking verification for high definition map maintenance using crowdsourced images\n0652 tasc: teammate algorithm for shared cooperation\n0655 a frequency-dependent impedance controller for an active-macro/passive-mini robotic system\n0656 robust and efficient post-processing for video object detection\n0659 topology-aware self-organizing maps for robotic information gathering\n0660 gaze by semi-virtual robotic heads: effects of eye and head motion\n0665 using diverse neural networks for safer human pose estimation: towards making neural networks know when they don\u00ef\u00bft know\n0668 online gain setting method for path tracking using cma-es: application to off-road mobile robot control\n0669 task planning with belief behavior trees\n0672 anytime kinodynamic motion planning using region-guided search\n0674 robust gait design insights from studying a compass gait biped with foot slipping\n0675 completeness seeking probabilistic coverage estimation using uncertain state estimates\n0679 imu-based parameter identification and position estimation in twisted string actuators\n0681 laminar jamming flexure joints for the development of variable stiffness robot grippers and hands\n0682 edan - an emg-controlled daily assistant to help people with physical disabilities\n0683 residual pose: a decoupled approach for depth-based 3d human pose estimation\n0685 the vcu-rvi benchmark: evaluating visual inertial odometry for indoor navigation applications with an rgb-d camera\n0686 voxel-based representation learning for place recognition based on 3d point clouds\n0687 a pomdp treatment of vehicle-pedestrian interaction: implicit coordination via uncertainty-aware planning\n0689 analysis and transfer of human movement manipulability in industry-like activities\n0694 a collision-resilient aerial vehicle with icosahedron tensegrity structure\n0695 category-level 3d non-rigid registration from single-view rgb images\n0697 an implementation of the adaptive neuro-fuzzy inference system (anfis) for odor source localization\n0701 representation and experience-based learning of explainable models for robot action execution\n0702 provably safe trajectory optimization in the presence of uncertain convex obstacles\n0704 the marathon 2: a navigation system\n0707 learning state-dependent losses for inverse dynamics learning\n0709 learning your way without map or compass: panoramic target driven visual navigation\n0710 in-flight range optimization of multicopters using multivariable extremum seeking with adaptive step size\n0711 inferring spatial uncertainty in object detection\n0714 deep depth estimation from visual-inertial slam\n0715 lavapilot: lightweight uavtrajectory planner with situational awarenessfor embedded autonomy to track and locate radio-tags\n0716 differential image based robot to mri scanner registration with active fiducial markers for an mri-guided robotic catheter system\n0717 acoustic collision detection and localization for robot manipulators\n0718 regulation of 2d arm stability against unstable, damping-defined environments in physical human-robot interaction\n0719 detecting usable planar regions for legged robot locomotion\n0720 reinforced grounded action transformation for sim-to-real transfer\n0721 monocular visual shape tracking and servoing for isometrically deforming objects\n0722 probabilistic multi-modal trajectory prediction with lane attention for autonomous vehicles\n0724 improving disturbance rejection and dynamics of cable driven parallel robots with on-board propellers\n0728 pi-map: a decision-based sensor fusion with global optimization for indoor mapping\n0731 learning the latent space of robot dynamics for cutting interaction inference\n0732 nonlinear balance control of an unmanned bicycle: design and experiments\n0739 heterogeneous vehicle routing and teaming with gaussian distributed energy uncertainty\n0740 decentralised self-organising maps for multi-robot information gathering\n0745 on a videoing control system based on object detection and tracking\n0750 learning-based optimization algorithms combining force control strategies for peg-in-hole assembly\n0751 fingertip non-contact optoacoustic sensor for near-distance ranging and thickness differentiation for robotic grasping\n0752 a momentum-based foot placement strategy for stable postural control of robotic spring-mass running with point feet\n0753 distributed model predictive control for uavs collaborative payload transport\n0754 monocular camera localization in prior lidar maps with 2d-3d line correspondences\n0755 cleaning robot operation decision based on causal reasoning and attribute learning\n0764 a thermoplastic elastomer belt based robotic gripper\n0767 interacting multiple model navigation system for quadrotor micro aerial vehicles subject to rotor drag\n0772 meta learning with differentiable closed-form solver for fast video object segmentation\n0774 active alignment control-based led communication for underwater robots\n0775 semantic trajectory planning for long-distant unmanned aerial vehicle navigation in urban environments\n0777 high-speed catching by multi-vision robot hand\n0778 autonomous exploration under uncertainty via deep reinforcement learning on graphs\n0781 introduction to 7-dof cosmo-arm : high torque density manipulator based on cosmoa and e-cosmo\n0782 learning-based controller optimization for repetitive robotic tasks\n0784 dui-vio: depth uncertainty incorporated visual inertial odometrybased on an rgb-d camera\n0786 wind and the city: utilizing uav-based in-situ measurements for estimating urban wind fields\n0789 a bottom-up framework for construction of structured semantic 3d scene graph\n0790 bi-modal hemispherical sensors for dynamic locomotion and manipulation\n0791 underwater monocular image depth estimation using single-beam echosounder\n0792 tp-tio: a robust thermal-inertial odometry with deep thermalpoint\n0795 microdrone-equipped mobile crawler robot system, dir-3, for high-step climbing and high-place inspection\n0796 risk vector-based near miss obstacle avoidance for autonomous surface vehicles\n0797 sampling-based search for a semi-cooperative target\n0802 endoscopic navigation based on three-dimensional structure registration\n0805 robotic understanding of spatial relationships using neural-logic learning\n0807 behaviorally diverse traffic simulation via reinforcement learning\n0808 cascaded non-local neural network for point cloud semantic segmentation\n0809 fusing concurrent orthogonal wide-aperture sonar images for dense underwater 3d reconstruction\n0814 path planning for nonholonomic multiple mobile robot system with applications to robotic autonomous luggage trolley collection at airports\n0823 adaptive gait pattern generation of a powered exoskeleton by iterative learning of human behavior\n0828 a deep learning based end-to-end locomotion mode detection method for lower limb wearable robot control\n0835 leveraging planar regularities for point line visual-inertial odometry\n0837 high-speed hitting grasping with magripper, a highly backdrivable gripper using magnetic gear and plastic deformation control\n0840 a novel and controllable cell-robot in real vascular network for target tumor therapy\n0844 faster healthcare time series classification for boosting mortality early warning system\n0845 dipe: deeper into photometric errors for unsupervised learning of depth and ego-motion from monocular videos\n0849 clothoid-based moving formation control using virtual structures\n0851 water based magnification of capacitive proximity sensors: water containers as passive human detectors\n0852 adaptive robot-assisted feeding: an online learning framework for acquiring previously unseen food items\n0855 remove, then revert: static point cloud map construction using multiresolution range images\n0857 freebot: a freeform modular self-reconfigurable robot with arbitrary connection point - design and implementation\n0865 a target tracking and positioning framework for video satellites based on slam\n0867 360\u00e2\u00b0 depth estimation from multiple fisheye images with origami crown representation of icosahedron\n0869 augmented memory for correlation filters in real-time uav tracking\n0870 batch normalization masked sparse autoencoder for robotic grasping detection\n0874 ros-lite: ros framework for noc-based embedded many-core platform\n0876 utilizing sacrificial molding for embedding motion controlling endostructures in soft pneumatic actuators\n0879 incorporating spatial constraints into a bayesian tracking framework for improved localisation in agricultural environments\n0888 ssp: single shot future trajectory prediction\n0891 stir to pour: efficient calibration of liquid properties for pouring actions\n0892 drive-train design in jaxon3-p and realization of jump motions: impact mitigation and force control performance for dynamic motions\n0895 autonomous task planning and situation awareness in robotic surgery\n0896 subsurface sampling robot for time-limited asteroid exploration\n0897 a variable impedance control strategy for object manipulation considering non-rigid grasp\n0898 toward hierarchical self-supervised monocular absolute depth estimation for autonomous driving applications\n0900 fruit quality control by surface analysis using a bio-inspired soft tactile sensor\n0902 avp-slam: semantic visual mapping and localization for autonomous vehicles in the parking lot\n0904 robust internal model control for motor systems based on sliding mode technique and extended state observer\n0905 organizing the internet of robotic things: the effect of organization structure on users' evaluation and compliance toward iort service platform\n0907 locomotion performance of a configurable paddle-wheel robot over dry sandy terrain\n0908 can i lift it? humanoid robot reasoning about the feasibility of lifting a heavy box with unknown physical properties\n0909 adaptability preserving domain decomposition for stabilizing sim2real reinforcement learning\n0912 robot-assisted ultrasound-guided biopsy on mr-detected breast lesions\n0913 tactile event based grasping algorithm using memorized triggers and mechanoreceptive sensors\n0914 an augmented reality interaction interface for autonomous drone\n0919 learning an overlap-based observation model for 3d lidar localization\n0931 automatic targetless extrinsic calibration of multiple 3d lidars and radars\n0934 hd map change detection with cross-domain deep metric learning\n0938 learning agile locomotion via adversarial training\n0939 path planning for mobile manipulators under nonholonomic and task constraints\n0942 long-reach compact robotic arm with lmpa joints for monitoring of reactor interior\n0943 the newer college dataset handheld lidar, inertial and vision with ground truth\n0944 automatic failure recovery and re-initialization for online uav tracking with joint scale and aspect ratio optimization\n0947 distributed reinforcement learning of targeted grasping with active vision for mobile manipulators\n0948 vision-based gesture recognition in human-robot teams using synthetic data\n0954 joints-space metrics for automatic robotic surgical gestures classification\n0955 probabilistic qualitative localization and mapping\n0956 soft tissue simulation environment to learn manipulation tasks in autonomous robotic surgery\n0960 adaptive potential scanning for a tomographic tactile sensor with high spatio-temporal resolution\n0962 modelling social interaction between humans and service robots in large public spaces\n0963 a novel trajectory optimization for affine systems: beyond convex-concave procedure\n0970 control of magnetically-driven screws in a viscoelastic medium\n0975 a particle filter technique for human pose estimation in case of occlusion exploiting holographic human model and virtualized environment\n0976 dual-slam: a framework for robust single camera navigation\n0979 point cloud completion by learning shape priors\n0981 task planning from complex natural instructions by a collocating robot\n0986 autolay: benchmarking amodal layout estimation for autonomous driving\n0987 centroids triplet network and temporally-consistent embeddings for in-situ object recognition\n0988 accurate mapping and planning for autonomous racing\n0989 optimal robot motion planning in constrained workspaces using reinforcement learning\n0991 next-best-view planning for surface reconstruction of large-scale 3d environments with multiple uavs\n0994 multi-uav coverage path planning for the inspection of large and complex structures\n0995 coordinate-free isoline tracking in unknown 2-d scalar fields\n0998 diagnose like a clinician: third-order attention guided lesion amplification network for wce image classification\n0999 lightweight multi-robot communication protocols for information synchronization\n1000 hybrid aerial-ground locomotion with a single passive wheel\n1002 redundancy resolution under hard joint constraints: a generalized approach to rank updates\n1005 a mobile robot hand-arm teleoperation system by vision and imu\n1006 standard deep generative models for density estimation in configuration spaces: a study of benefits, limits and challenges\n1007 unilateral constraints for torque-based whole-body control\n1013 distributed motion control for multiple connected surface vessels\n1014 towards gradient-based actuationof magnetic soft robots using a six-coil electromagnetic system\n1015 mozard: multi-modal localization for autonomous vehicles in urban outdoor environments\n1017 a soft humanoid hand with in-finger visual perception\n1019 affordance-based grasping and manipulation in real world applications\n1020 computing high-quality clutter removal solutions for multiple robots\n1022 to ask or not to ask: a user annoyance aware preference elicitation framework for social robots\n1023 visual task progress estimation with appearance invariant embeddings for robot control and planning\n1025 formalization of robot skills with descriptive and operational models\n1028 video depth estimation by fusing flow-to-depth proposals\n1029 towards dynamic transparency: robust interaction force tracking using multi-sensory control on an arm exoskeleton\n1031 faster than fast: gpu-accelerated frontend for high-speed vio\n1037 accurate and robust teach and repeat navigation by visual place recognition: a cnn approach\n1043 aerial transportation of unknown payloads: adaptive path tracking for quadrotors\n1044 design and modeling of a parallel shifted-routing cable-driven continuum manipulator for endometrial regeneration surgery\n1045 learning continuous object representations from point cloud data\n1047 deep tactile experience: estimating tactile sensor output from depth sensor data\n1049 online velocity constraint adaptation for safe and efficient human-robot workspace sharing\n1050 learning domain randomization distributions for training robust locomotion policies\n1051 an optimized tilt mechanism for a new steady-hand eye robot\n1052 estimating pedestrian crossing states based on single 2d body pose\n1055 hybrid fluidic actuation for a foam-based soft actuator\n1057 unsupervised depth and confidence prediction from monocular images using bayesian inference\n1060 clocs: camera-lidar object candidates fusion for 3d object detection\n1062 polygonal perception for mobile robots\n1064 relational graph learning for crowd navigation\n1065 learning high-level policies for model predictive control\n1066 autonomous navigation in complex environments with deep multimodal fusion network\n1071 kinematic multibody model generation of deformable linear objects from point clouds\n1074 online dynamic motion planning and control for wheeled biped robots\n1076 bayesian fusion of unlabeled vision and rf data for aerial tracking of ground targets\n1077 anatomical mesh-based virtual fixtures for surgical robots\n1078 smart-inspect: micro scale localization and classification of smartphone glass defects for industrial automation\n1079 automated design and construction of a single incision laparoscopic system adapted to the required workspace\n1080 dense decentralized multi-robot slam based on locally consistent tsdf submaps\n1081 multiple trajectory prediction with deep temporal and spatial convolutional neural networks\n1083 multi-sparse gaussian process: learning based semi-parametric control\n1085 domain adaptation for outdoor robot traversability estimation from rgb data with safety-preserving loss\n1086 gndnet: fast ground plane estimation and point cloud segmentation for autonomous vehicles\n1088 a bio-inspired framework for joint angle estimation from non-collocated sensors in tendon-driven systems\n1091 real-time spatio-temporal lidar point cloud compression\n1094 solving large-scale stochastic orienteering problems with aggregation\n1102 an energy-based approach for the integration of collaborative redundant robots in restricted work environments\n1103 wireless electronic skin with integrated pressure and optical proximity sensing\n1105 end-to-end 3d point cloud learning for registration task using virtual correspondences\n1108 safe planning for self-driving via adaptive constrained ilqr\n1109 experimental flights of adaptive patterns for cloud exploration with uavs\n1114 learning to live life on the edge: online learning for data-efficient tactile contour following\n1116 supportive actions for manipulation in human-robot coworker teams\n1118 model-based quality-diversity search for efficient robot learning\n1119 autonomous planning for multiple aerial cinematographers\n1120 supervised autoencoder joint learning on heterogeneous tactile sensory data: improving material classification performance\n1121 magnetized cell-robot propelled by magnetic field for cancer killing\n1122 basic implementation of fpga-gpu dual soc hybrid architecture for low-latency multi-dof robot motion control\n1123 synchronization of microphones based on rank minimization of warped spectrum for asynchronous distributed recording\n1126 robotic micromanipulation of biological cells with friction force-based rotation control\n1130 multi-robot joint visual-inertial localization and 3-d moving object tracking\n1132 modeling a social placement cost to extend navigation among movable obstacles (namo) algorithms\n1134 an earthworm-like soft robot with integration of single pneumatic actuator and cellular structures for peristaltic motion\n1135 idol: a framework for imu-dvs odometry using lines\n1136 this or that: the effect of robot's deictic expression on user's perception\n1137 modeling and control of a hybrid wheeled jumping robot\n1138 an in-pipe manipulator for contamination-less rehabilitation of water distribution pipes\n1140 transferability in an 8-dof parallel robot with a configurable platform\n1141 optimizing dynamic trajectories for robustness to disturbances using polytopic projections\n1142 inter-robot range measurements in pose graph optimization\n1145 robots versus speakers: what type of central smart home interface consumers prefer?\n1146 reinforcement co-learning of deep and spiking neural networks for energy-efficient mapless navigation with neuromorphic hardware\n1148 scaling laws for parallel motor-gearbox arrangements\n1149 passivity filter for variable impedance control\n1150 long-run multi-robot planning under uncertain action durations for persistent tasks\n1151 template-based optimal robot design with application to passive-dynamic underactuated flapping\n1155 interactive movement primitives: planning to push occluding pieces for fruit picking\n1157 modeling cable-driven joint dynamics and friction: a bond-graph approach\n1158 verification of system-wide safety properties of ros applications\n1159 real-time detection of broccoli crops in 3d point clouds for autonomous robotic harvesting\n1165 understanding contexts inside robot and human manipulation tasks through vision-language model and ontology system in video streams\n1171 representing spatial object relations as parametric polar distribution for scene manipulation based on verbal commands\n1172 z-net: an anisotropic 3d dcnn for medical ct volume segmentation\n1176 analysis, development and evaluation of electro-hydrostatic technology for lower limb prostheses applications\n1177 proprioceptive sensor fusion for quadruped robot state estimation\n1180 model identification of a small omnidirectional aquatic surface vehicle: a practical implementation\n1183 reducing the teleoperator's cognitive burden for complex contact tasks using affordance primitives\n1184 point cloud based reinforcement learning for sim-to-real and partial observability in visual navigation\n1189 a novel inverse kinematics method for upper-limb exoskeleton under joint coordination constraints\n1190 closing the loop: real-time perception and control for robust collision avoidance with occluded obstacles\n1191 brainless running: a quasi-quadruped robot with decentralized spinal reflexes by solely mechanical devices\n1193 unsupervised learning of dense optical flow, depth and egomotion with event-based sensors\n1195 autonomous robot navigation based on multi-camera perception\n1196 improving motion planning for surgical robot with active constraints\n1197 graduated assignment graph matching for realtime matching of image wireframes\n1201 integrating model predictive control and dynamic waypoints generation for motion planning in surgical scenario\n1203 hierarchical optimization control of redundant manipulator for robot-assisted minimally invasive surgery\n1204 variable pitch system for the underwater explorer robot ux-1\n1205 real-time optimal control of an autonomous rc car with minimum-time maneuvers and a novel kineto-dynamical model\n1208 pillarflownet: a real-time deep multitask network for lidar-based 3d object detection and scene flow estimation\n1209 intelligent exploration and autonomous navigation in confined spaces\n1211 model identification of a soft robotic neck\n1216 mhyro: modular hybrid robot for contact inspection and maintenance in oil&gas plants\n1218 auditory feedback effectiveness for enabling safe sclera force in robot-assisted vitreoretinal surgery: a multi-user study\n1220 towards micro robot hydrobatics: vision-based guidance, navigation, and control for agile underwater vehicles in confined environments\n1222 a decentralized framework for simultaneous calibration, localization and mapping with multiple lidars\n1231 real-time detection of distracted driving using dual cameras\n1232 a biomimetic tactile fingerprint induces incipient slip\n1233 model quality aware ransac: a robust camera motion estimator\n1235 synchrono: a scalable, physics-based simulation platform for testing groups of autonomous vehicles and/or robots\n1236 cross scene prediction via modeling dynamic correlation using latent space shared auto-encoders\n1237 towards vision-based impedance control for the contact inspection of unknown generically-shaped surfaces with a fully-actuated uav\n1238 squash-box feasibility driven differential dynamic programming\n1239 sgm-mde: semi-global optimization for classification-based monocular depth estimation\n1240 safety considerations in deep control policies with safety barrier certificates under uncertainty\n1244 asynchronous event-based line tracking for time-to-contact maneuvers in uas\n1245 robotvqa ' a scene-graph and deep-learning-based visual question answering system for robot manipulation\n1247 a model-free solution for stable balancing and locomotion of floating-base legged systems\n1248 label efficient visual abstractions for autonomous driving\n1249 control interface for hands-free navigation of standing mobility vehicles based on upper-body natural movements\n1250 resonating magnetic manipulation for 3d path-following and blood clot removal using a rotating swimmer\n1252 distilling location proposals of unknown objects through gaze information for human-robot interaction\n1253 visualization of intended assistance for acceptance of shared control\n1254 learning object manipulation with dexterous hand-arm systems from human demonstration\n1255 learning accurate and human-like driving using semantic maps and attention\n1259 designing environments conducive to interpretable robot behavior\n1263 efficiency and equity are both essential: a generalized traffic signal controller with deep reinforcement learning\n1265 reactive receding horizon planning and control for quadrotors with limited on-board sensing\n1266 selfiedronestick: a natural interface for quadcopter photography\n1275 localization and force-feedback with soft magnetic stickers for precise robot manipulation\n1276 robust monocular edge visual odometry through coarse-to-fine data association\n1279 tartanair: a dataset to push the limits of visual slam\n1281 an sem-based nanomanipulation system for multi-physical characterization of single ingan/gan nanowires\n1282 line walking and balancing for legged robots with point feet\n1283 robust gait synthesis combining constrained optimization and imitation learning\n1284 edge-based visual odometry with stereo cameras using multiple oriented quadtrees\n1285 monocular localization in hd maps by combining semantic segmentation and distance transform\n1288 optimal design of a novel spherical scissor linkage remote center of motion mechanism for medical robotics\n1292 adversarial generation of informative trajectories for dynamics system identification\n1295 robust and efficient object change detection by combining global semantic information and local geometric verification\n1296 bounded sub-optimal multi-robot path planning using satisfiability modulo theory (smt) approach\n1297 haptic knowledge transfer between heterogeneous robots using kernel manifold alignment\n1298 competitive coverage: (full) information as a gamechanger\n1299 expressing diverse human driving behavior with probabilisticrewards and online inference\n1300 proximal deterministic policy gradient\n1301 generalizing learned manipulation skills in practice\n1302 gr-slam: vision-based sensor fusion slam for ground robots on complex terrain\n1304 automatic gait pattern selection for legged robots\n1307 learning vision-based physics intuition models for non-disruptive object extraction\n1313 lane-attention: predicting vehicles' moving trajectories by learning their attention over lanes\n1315 robotic table tennis with model-free reinforcement learning\n1317 consistent covariance pre-integration for invariant filters with delayed measurements\n1319 model-based specification of control architectures for compliant interaction with the environment\n1321 perception-aware path finding and following of snake robot in unknown environment\n1323 3d coating self-assembly for modular robotic scaffolds\n1326 understanding dynamic scenes using graph convolution networks\n1328 monocular depth prediction through continuous 3d loss\n1335 peg-in-hole using 3d workpiece reconstruction and cnn-based hole detection\n1337 crop height and plot estimation for phenotyping from unmanned aerial vehicles using 3d lidar\n1338 non-linear control under state constraints with validated trajectories for a mobile robot towing a trailer\n1340 imitationflow: learning deep stable stochastic dynamic systems by normalizing flows\n1341 a unique identifier assignment method for distributed modular robots\n1344 a framework for real-time and personalisable human ergonomics monitoring\n1346 bio-inspired inverted landing strategy in a small aerial robot using policy gradient\n1347 walking on tactip toes: a tactile sensing foot for walking robots\n1350 event-based pid controller fully realized in neuromorphic hardware: a one dof study\n1351 modality-buffet for real-time object detection\n1352 traffic control gesture recognition for autonomous vehicles\n1353 emergent adaptive gait generation through hebbian sensor-motor maps by morphological probing\n1356 pnugrip: an active two-phase gripper for dexterous manipulation\n1357 robo-gym ' an open source toolkit for distributed deep reinforcement learning on real and simulated robots\n1358 lic-fusion 2.0: lidar-inertial-camera odometry with sliding-window plane-feature tracking\n1359 experimental evaluation of 3d-lidar camera extrinsic calibration\n1362 autonomous model-based assessment of mechanical failures of reconfigurable modular robots with a conjugate gradient solver\n1363 spiking neurons ensemble for movement generation in dynamically changing environments\n1365 terrain-adaptive planning and control of complex motions for walking excavators\n1369 gimme signals: discriminative signal encoding for multimodal activity recognition\n1370 goal-driven variable admittance control for robot manual guidance\n1372 a scalable framework for robust vehicle state estimation with a fusion of a low-cost imu, the gnss, radar, a camera and lidar\n1375 joint feature selection and time optimal path parametrization for high speed vision-aided navigation\n1377 multi-task deep learning for depth-based person perception in mobile robotics\n1378 online explanation generation for planning tasks in human-robot teaming\n1381 cinemairsim: a camera-realistic robotics simulator for cinematographic purposes\n1384 real-world human-robot collaborative reinforcement learning\n1386 kinodynamic motion planning for multi-legged robot jumping via mixed-integer convex program\n1387 who make drivers stop? towards driver-centric risk assessment: risk object identification via causal inference\n1388 adaptive partitioning for coordinated multi-agent perimeter defense\n1390 tracking strategy based on magnetic sensors for microrobot navigation in the cochlea\n1391 learning object attributes with category-free grounded language from deep featurization\n1392 first steps: latent-space control with semantic constraints for quadruped locomotion\n1394 self-reconfiguration planning of adaptive modular robots with triangular structure based on extended binary trees\n1395 fast model predictive image-based visual servoing for quadrotors\n1396 what the hololens maps is your workspace: fast mapping and set-up of robot cells via head mounted displays and augmented reality\n1399 6d pose estimation for flexible production with small lot sizes based on cad models using gaussian process implicit surfaces\n1400 design of a new electroactive polymer based continuum actuator for endoscopic surgical robots\n1402 efficient object search through probability-based viewpoint selection\n1405 a probabilistic shared-control framework for mobile robots\n1409 model predictive control for a tendon-driven surgical robot with safety constraints in kinematics and dynamics\n1411 vision-based belt manipulation by humanoid robot\n1414 3d localization of a sound source using mobile microphone arrays referenced by slam\n1418 exploit semantic and public prior information in monoslam\n1422 fast online adaptation in robotics through meta-learning embeddings of simulated priors\n1425 computational design of balanced open link planar mechanisms with counterweights from user sketches\n1427 an underactuated gripper using origami-folding inspired variable stiffness flexure hinges\n1428 frontier detection and reachability analysis for efficient 2d graph-slam based active exploration\n1429 towards the development of a robotic transcatheter delivery system for mitral valve implant\n1430 b-spline surfaces for range-based environment mapping\n1431 localizing against drawn maps via spline-based registration\n1432 majorization minimization methods for distributed pose graph optimization with convergence guarantees\n1433 deep adversarial reinforcement learning for object disentangling\n1434 dense incremental metric-semantic mapping via sparse gaussian process regression\n1435 synthesis of control barrier functions using a supervised machine learning approach\n1436 jrmot: a real-time 3d multi-object tracker and a new large-scale dataset\n1438 slope handling for quadruped robots using deep reinforcement learning and toe trajectory planning\n1439 realistic and interactive robot gaze\n1440 gaussian process gradient maps for loop-closure detection in unstructured planetary environments\n1442 efficient multiresolution scrolling grid for stereo vision-based mav obstacle avoidance\n1443 action sequence predictions of vehicles in urban environments using map and social context\n1446 decentralized nonlinear mpc for robust cooperative manipulation by heterogeneous aerial-ground robots\n1447 joint-level control of the dlr lightweight robot sara\n1452 improving unimodal object recognition with multimodal contrastive learning\n1453 observer-based control of inflatable robot with variable stiffness\n1454 maximizing bci human feedback using active learning\n1457 lidar iris for loop-closure detection\n1460 mixed reality as a bidirectional communication interface for human-robot interaction\n1461 visual monitoring and servoing of a cutting blade during telerobotic satellite servicing\n1462 human-drone interaction for aerially manipulated drilling using haptic feedback\n1463 mapper: multi-agent path planning with evolutionary reinforcement learning in mixed dynamic environments\n1465 no-regret shannon entropy regularized neural contextual bandit online learning for robotic grasping\n1467 learning of tool force adjustment skills by a life-sized humanoid using deep reinforcement learning and active teaching request\n1469 towards deep learning assisted autonomous uavs for manipulation tasks in gps-denied environments\n1471 bit-vo: visual odometry at 300 fps using binary features from the focal plane\n1472 using machine learning for material detection with capacitive proximity sensors\n1477 intent-driven strategic tactical planning for autonomous siteinspection using cooperative drones\n1478 physical human-robot interaction with real active surfaces using haptic rendering on point clouds\n1480 end-to-end autonomous driving perception with sequential latent representation learning\n1482 the robot as scientist: using mental simulation to test causal hypotheses extracted from human activities in virtual reality\n1483 design of a high-level teleoperation interface resilient to the effects of unreliable robot autonomy\n1488 tactilesgnet: a spiking graph neural network for event-based tactile object recognition\n1490 a miniaturised neuromorphic tactile sensor integrated with an anthropomorphic robot hand\n1493 towards unsupervised learning for instrument segmentation in robotic surgery with cycle-consistent adversarial networks\n1498 energy-efficient motion planning for multi-modal hybrid locomotion\n1499 cnn-based foothold selection for mechanically adaptive soft foot\n1503 a topological approach to path planning for a magnetic millirobot\n1504 autonomous detection and assessment with moving sensors\n1505 compliance control of cable-suspended aerial manipulator using hierarchical control framework\n1507 noncontact estimation of stiffness based on optical coherence elastography under acoustic radiation pressure\n1508 factor graph based 3d multi-object tracking in point clouds\n1512 application of interacting models to estimate the gait speed of an exoskeleton user\n1513 an approach to reduce communication for multi-agent mapping applications\n1514 cuhk-ahu dataset: promoting practical self-driving applications in the complex airport logistics, hill and urban environments\n1515 friction identification in a pneumatic gripper\n1519 lc-gan: image-to-image translation based on generative adversarial network for endoscopic images\n1520 learning human navigation behavior using measured human trajectories in crowded spaces\n1527 scaling up multiagent reinforcement learning for robotic systems: learn an adaptive sparse communication graph\n1537 efficient exploration in constrained environments with goal-oriented reference path\n1538 kalman filter based range estimation and clock synchronization for ultra wide band networks\n1540 haptic sequential monte carlo localization for quadrupedal locomotion in vision-denied scenarios\n1543 vision only 3-d shape estimation for autonomous driving\n1544 llama: design and control of an omnidirectional human mission scale quadrupedal robot\n1545 variational filtering with copula models for slam\n1546 dynamic parameter estimation utilizing optimized trajectories\n1551 core-centered actuation for biped locomotion of humanoid robots\n1553 dynamic stability control of inverted-pendulum-type robotic wheelchair for going up and down stairs\n1560 depth completion via inductive fusion of planar lidar and monocular camera\n1565 flexivision: teleporting the surgeon's eyes via robotic flexible endoscope and head-mounted display\n1566 towards robust visual tracking for unmanned aerial vehicle with tri-attentional correlation filters\n1568 infusing reachability-based safety into planning and control for multi-agent interactions\n1569 optimization-based hierarchical motion planning for autonomous racing\n1572 dr-spaam: a spatial-attention and auto-regressive model for person detection in 2d range data\n1573 functionally divided manipulation synergy for controlling multi-fingered hands\n1574 solving cosserat rod models via collocation and the magnus expansion\n1576 swingbot: learning physical features from in-hand tactile exploration for dynamic swing-up manipulation\n1577 crowdsourced 3d mapping: a combined multi-view geometry and self-supervised learning approach\n1579 visuomotor mechanical search: learning to retrieve target objects in clutter\n1580 grif net: gated region of interest fusion network for robust 3d object detection from radar point cloud and monocular image\n1582 semi-autonomous control of leader-follower excavator using admittance control for synchronization and autonomy with bifurcation and stagnation for human interface\n1586 active improvement of control policies with bayesian gaussian mixture model\n1590 plrc*: a piecewise linear regression complex for approximating optimal robot motion\n1591 disappearance of chaotic attractor of passive dynamic walking by stretch-bending deformation in basin of attraction\n1593 enabling robot to assist human in collaborative assembly using convolutional neural networks\n1595 estimating motion codes from demonstration videos\n1598 dynamic object tracking and masking for visual slam\n1599 evaluating the efficacy of parallel elastic actuators on high-speed, variable stiffness running\n1600 active preference learning using maximum regret\n1602 approximated dynamic trait models for heterogeneous multi-robot teams\n1603 relevant region exploration on general cost-maps for sampling-based motion planning\n1605 self-supervised object tracking with cycle-consistent siamese networks\n1608 resilient coverage: exploring the local-to-global trade-off\n1611 x-ray: mechanical search for an occluded object by minimizing support of learned occupancy distributions\n1613 secure route planning using dynamic games with stopping states\n1619 ospheel: design of an omnidirectional spherical-sectioned wheel\n1620 squirl: robust and efficient learning from video demonstration of long-horizon robotic manipulation tasks\n1623 depth estimation from monocular images and sparse radar data\n1629 learning to switch cnns with model agnostic meta learning for fine precision visual servoing\n1630 relative pose estimation and planar reconstruction via superpixel-driven multiple homographies\n1631 crossing the gap: a deep dive into zero-shot sim-to-real transfer for dynamics\n1632 rapid bipedal gait optimization in casadi\n1634 novel design of a soft pump driven by super-coiled polymer artificial muscles\n1635 automated folding of a deformable thin object through robot manipulators\n1639 robot learning in mixed adversarial and collaborative settings\n1640 jumping motion generation for humanoid robot using arm swing effectively and changing in foot contact status\n1641 antipodal robotic grasping using generative residual convolutional neural network\n1645 hindsight for foresight: unsupervised structured dynamics models from physical interaction\n1655 resultant radius of curvature of stylet-and-tube steerable needles based on the mechanical properties of the soft tissue, and the needle\n1657 enhanced transfer learning for autonomous driving with systematic accident simulation\n1658 observer-based disturbance control for small-scale collaborative robotics\n1659 anticipating tumor metastasis by circulating tumor cells captured by acoustic microstreaming\n1660 an untethered brittle star-inspired soft robot for closed-loop underwater locomotion\n1662 what to do when you can't do it all: temporal logic planning with soft temporal logic constraints\n1663 dynamics and aerial attitude control for rapid emergency deployment of the agile ground robot agro\n1665 a framework for human-robot interaction user studies\n1671 a mixed-integer model predictive control approach to motion cueing in immersive wheelchair simulator\n1674 self-sensing soft tactile actuator for fingertip interface\n1675 trajectory tracking of a one-link flexible arm via iterative learning control\n1677 contextual policy search for micro-data robot motion learning through covariate gaussian process latent variable models\n1678 a framework for online updates to safe sets for uncertain dynamics\n1680 contact force estimation and regulation of a position-controlled floating base system without joint torque information\n1685 going cognitive: a demonstration of the utility of task-general cognitive architectures for adaptive robotic task performance\n1686 a fast and robust place recognition approach for stereo visual odometry using lidar descriptors\n1689 developing thermal endoscope for endoscopic photothermal therapy for peritoneal dissemination\n1690 collision-free distributed multi-target tracking using teams of mobile robots with localization uncertainty\n1692 houseexpo: a large-scale 2d indoor layout dataset for learning-based algorithms on mobile robots\n1693 reachflow: an online safety assurance framework for waypoint-following of self-driving cars\n1694 an untethered soft cellular robot with variable volume, friction, and unit-to-unit cohesion\n1695 multi-robot task allocation with time window and ordering constraints\n1696 a distributed scalar field mapping strategy for mobile robots\n1698 torm: fast and accurate trajectory optimization of redundant manipulator given an end-effector path\n1703 ian: multi-behavior navigation planning for robots in real, crowded environments\n1706 contact localization using velocity constraints\n1707 tensor action spaces for multi-agent robot transfer learning\n1709 deep prediction of swept volume geometries: robots and resolutions\n1710 collaborative programming of conditional robot tasks\n1711 a whisker-inspired fin sensor for multi-directional airflow sensing\n1716 seed: a segmentation-based egocentric 3d point cloud descriptor for loop closure detection\n1719 multi-fingered active grasp learning\n1720 extended performance guarantees for receding horizon search with terminal cost\n1721 robot learning from demonstration with tactile signals for geometry-dependent tasks\n1722 f-siamese tracker: a frustum-based double siamese network for 3d single object tracking\n1723 qsrnet: estimating qualitative spatial representations from rgb-d images\n1724 interactive planning and supervised execution for high-risk, high-latency teleoperation\n1725 l1-adaptive mppi architecture for robust and agile control of multirotors\n1726 highly underactuated radial gripper for automated planar grasping and part fixturing\n1727 localization uncertainty-driven adaptive framework for controlling ground vehicle robots\n1728 sim-to-real with domain randomization for tumbling robot control\n1729 robust, perception based control with quadrotors\n1730 analysis of contact stability and contact safety of a robotic intravascular cardiac catheter under blood flow disturbances\n1732 real-time multi-slam system for agent localization and 3d mapping in dynamic scenarios\n1733 deep r-learning for continual area sweeping\n1734 fast uncertainty estimation for deep learning based optical flow\n1735 global localization over 2d floor plans with free-space density based on depth information\n1736 a multi-contact motion planning and control strategy for physical interaction tasks using a humanoid robot\n1737 soft-bubble grippers for robust and perceptive manipulation\n1738 planning for robust visibility-based pursuit-evasion\n1741 robot calligraphy using pseudospectral optimal controlin conjunction with a novel dynamic brush model\n1743 path planning under mimo network constraints for throughput enhancement in multi-robot data aggregation tasks\n1744 self-supervised neural audio-visual sound source localization via probabilistic spatial modeling\n1745 silicone-based capacitive e-skin for exteroception and proprioception\n1746 scan: system for camera autonomous navigation in robotic-assisted surgery\n1749 flight control of sliding arm quadcopter with dynamic structural parameters\n1750 domain-adversarial and -conditional state space model for imitation learning\n1752 msdpn: monocular depth prediction with partial laser observation using multi-stage neural networks\n1753 optimizing coordinate choice for locomotion systems with toroidal shape spaces\n1754 deepurl: deep pose estimation framework for underwater relative localization\n1755 expert-emulating excavation trajectory planning for autonomous robotic industrial excavator\n1758 soft microrobotic transmissions enable rapid ground-based locomotion\n1762 algorithm for multi-robot chance-constrained generalized assignment problem with stochastic resource consumption\n1763 game-theoretic planning for risk-aware interactive agents\n1765 demonstration of a novel phase lag controlled roll rotation mechanism using a two-dof soft swimming robot\n1769 learning to locomote with artificial neural-network and cpg-based control in a soft snake robot\n1771 heatnet: bridging the day-night domain gap in semantic segmentation with thermal images\n1772 building plannable representations with mixed reality\n1773 parameter identification for an uncooperative captured satellite with spinning reaction wheels\n1774 design of a highly-maneuverable pneumatic soft actuator driven by intrinsic sma coils (pneusma actuator)\n1779 collision reaction through internal stress loading in cooperative manipulation\n1780 pedestrian motion tracking by using inertial sensors on the smartphone\n1781 blind bin picking of small screws through in-finger manipulation with compliant robotic fingers\n1782 data-driven distributionally robust electric vehicle balancing for mobility-on-demand systems under demand and supply uncertainties\n1788 a concept of a miniaturized mr clutch utilizing mr fluid in squeeze mode\n1790 enabling remote whole-body control with 5g edge computing\n1793 3d printed bio-inspired hair sensor for directional airflow sensing\n1794 learning to use adaptive motion primitives in search-based motion planning for navigation\n1795 radarslam: radar based large-scale slam in all weathers\n1799 learning orientation distributions for object pose estimation\n1804 performance characterization of an algorithm to estimate the search skill of a human or robot agent\n1807 learning an optimal sampling distribution for efficient motion planning\n1810 autonomous navigation and obstacle avoidance of a snake robot with combined velocity-heading control\n1811 quadrotor-enabled autonomous parking occupancy detection\n1812 the masked mapper: masked metric mapping\n1813 augmenting control policies with motion planning for robust and safe multi-robot navigation\n1814 collaborative semantic perception and relative localization based on map matching\n1816 weakly-supervised learning for multimodal human activity recognition in human-robot collaboration scenarios\n1817 sim-to-real transfer of bolting tasks with tight tolerance\n1818 lidar guided small obstacle segmentation\n1819 towards transparent robotic planningvia contrastive explanations\n1820 estimation of object class and orientation from multiple viewpoints and relative camera orientation constraints\n1821 information driven self-calibration for lidar-inertial systems\n1822 safe and effective picking paths in clutter given discrete distributions of object poses\n1823 shape reconstruction of ccd camera-based soft tactile sensors\n1824 allocating limited sensing resources to accurately map dynamic environments\n1825 robust micro-particle manipulation in a microfluidic channel network using gravity-induced pressure actuators\n1827 the multi-material actuator for variable stiffness (mavs): design, modeling, and characterization of a soft actuator for lateral ankle support\n1828 dec-ppcpp: a decentralized predator--prey-based approach to adaptive coverage path planning amid moving obstacles\n1830 data-driven distributed state estimation and behavior modeling in sensor networks\n1833 collision risk assessment via awareness estimation toward robotic attendant\n1834 leveraging multiple environments for learning and decision making: a dismantling use case\n1836 robust dynamic state estimation for lateral control of an industrial tractor towing multiple passive trailers\n1837 output only fault detection and mitigation of networks of autonomous vehicles\n1838 markov decision processes with unknown state feature values for safe exploration using gaussian processes\n1839 design and control of slider: an ultra-lightweight, knee-less, low-cost bipedal walking robot\n1840 learning bayes filter models for tactile localization\n1841 occlusion handling for industrial robots\n1844 online bayessim for combined simulator parameter inference and policy improvement\n1846 roboat ii: a novel autonomous surface vessel for urban environments\n1847 human grasp classification for reactive human-to-robot handovers\n1851 persistent connected power constrained surveillance with unmanned aerial vehicles\n1853 designing a dummy skin by evaluating contacts between a human hand and a robot end tip\n1854 making robots draw a vivid portrait in two minutes\n1856 unified calibration for multi-camera mult-lidar systems using a single checkerboard\n1857 3d multi-object tracking: a baseline and new evaluation metrics\n1858 learning consistency pursued correlation filters for real-time uav tracking\n1860 multi-robot containment and disablement\n1861 snapbot v2: a reconfigurable legged robot with a camera for self configuration recognition\n1862 stable crawling policy for wearable superlimbs attached to a human with tuned impedance\n1864 planning on the fast lane: learning to interact using attention mechanisms in path integral inverse reinforcement learning\n1867 identification of a human hand kinematics by measuring and merging of nail-based finger motions\n1868 tidying deep saliency prediction architectures\n1872 deep reinforcement learning for industrial insertion tasks with visual inputs and natural rewards\n1873 sideguide: a large-scale sidewalk dataset for guiding impaired people\n1874 autonomous multi-robot assembly of solar array modules: experimental analysis and insights\n1875 learning transition models with time-delayed causal relations\n1876 lyapunov-based approach to reactive step generation for push recovery of biped robots via hybrid tracking control of dcm\n1877 versatile 3d multi-sensor fusion for lightweight 2d localization\n1878 efficient trajectory library filtering for quadrotor flight in unknown environments\n1882 towards general infeasibility proofs in motion planning\n1884 pillarflow: end-to-end birds-eye-view flow estimation for autonomous driving\n1888 self-supervised simultaneous alignment and change detection\n1889 multimodal sensor fusion with differentiable filters\n1890 multi-object rearrangement with monte carlo tree search: a case study on planar nonprehensile sorting\n1891 a robust multi-stereo visual-inertial odometry pipeline\n1893 multimodal material classification for robots using spectroscopy and high resolution texture imaging\n1895 splitflyer: a modular quadcoptor that disassembles into two flying robots\n1897 multi-agent safe planning with gaussian processes\n1898 exponentially stabilizing and time-varying virtual constraint controllers for dynamic quadrupedal bounding\n1899 vision global localization with semantic segmentation and interest feature points\n1900 an electrocommunication system using fsk modulation and deep learning based demodulation for underwater robots\n1901 online system for dynamic multi-contact motion with impact force based on contact wrench estimation and current-based torque control\n1902 learning constraint-based planning models from demonstrations\n1903 autonomous vehicle benchmarking using unbiased metrics\n1907 ultrasound-guided robotic navigation with deep reinforcement learning\n1908 gait training robot with intermittent force application based on prediction of minimum toe clearance\n1909 design and implementation of a haptic measurement glove to create realistic human-telerobot interactions\n1911 risk-averse mpc via visual-inertial input and recurrent networks for online collision avoidance\n1912 data driven online multi-robot formation planning\n1913 learning-based distributionally robust motion control with gaussian processes\n1915 multi-label long short-term memory for construction vehicle activity recognition with imbalanced supervision\n1916 deep mixture density network for probabilistic object detection\n1917 human preference-based learning for high-dimensional optimization of exoskeleton walking gaits\n1919 online planning in uncertain and dynamic environment in the presence of multiple mobile vesicles\n1920 reliable chattering-free simulation of friction torque in joints presenting high stiction\n1921 generating minimum-snap quadrotor trajectories really fast\n1922 data-driven characterization of human interaction for model-based control of powered prostheses\n1925 development of a maneuverable un-tethered multi-fin soft robot\n1928 choosing classification thresholds for mobile robot coverage\n1930 accelerating bi-directional sampling-based search for motion planning of non-holonomic mobile manipulators\n1933 online configuration selection for redundant arrays of inertial sensors: application to robotic systems covered with a multimodal artificial skin\n1934 synchronous minimum-time cooperative manipulation using distributed model predictive control\n1935 robust force tracking impedance control of an ultrasonic motor-actuated end-effector in a soft environment\n1936 transferring experience from simulation to the real world for precise pick-and-place tasks in highly cluttered scenes\n1937 uncertainty-aware self-supervised 3d data association\n1941 parts-based articulated object localization in clutter using belief propagation\n1942 low-viewpoint forest depth dataset for sparse rover swarms\n1943 true\u00e3\ufffddapt: learning smooth online trajectory adaptation with bounded jerk, acceleration and velocity in joint space\n1944 simple means faster: real-time human motion forecasting in monocular first person videos on cpu\n1947 h-infinity-optimal tracking controller for three-wheeled omnidirectional mobile robots with uncertain dynamics\n1950 construction of multiple hepatic lobule like 3d vascular networks by manipulating magnetic tweezers toward tissue engineering\n1951 generating alerts to assist with task assignments in human-supervised multi-robot teams operating in challenging environments\n1952 stochastic grounded action transformation for robot learning in simulation\n1955 gp-based runtime planning, learning, and recovery for safe uav operations under unforeseen disturbances\n1956 fast global motion planning for dynamic legged robots\n1960 development of a pneumatically driven growing sling to assist patient transfer\n1961 diat (depth-infrared image annotation transfer) for training a depth-based pig-pose detector\n1962 towards autonomous control of magnetic suture needles\n1963 interactive tactile perception for classification of novel object instances\n1964 dgaze: driver gaze mapping on road\n1965 perch 2.0 : fast and accurate gpu-based perception via search for object pose estimation\n1966 experimental verification of vibratory conveyor system based on frequency entrainment of limit cycle walker\n1968 anticipatory human-robot collaboration via multi-objective trajectory optimization\n1969 se(3)-tracknet: data-driven 6d pose tracking by calibrating image residuals in synthetic domains\n1971 a study on the elongation behaviour of synthetic fibre ropes under cyclic loading\n1972 adaptive kernel inference for dense and sharp occupancy grids\n1976 control framework for a hybrid-steel bridge inspection robot\n1979 dxslam: a robust and efficient visual slam system with deep features\n1981 online exploration of tunnel networks leveraging topological cnn-based world predictions\n1982 learning visual policies for building 3d shape categories\n1984 on screw linear interpolation for point-to-point path planning\n1986 diminished reality for close quarters robotic telemanipulation\n1987 encoding formulas as deep networks: reinforcement learning for zero-shot execution of ltl formulas\n1988 uwb-based system for uav localization in gnss-denied environments: characterization and dataset\n1990 feeling the true force in haptic telepresence for flying robots\n1991 eao-slam: monocular semi-dense object slam based on ensemble data association\n1992 a probabilistic model for planar sliding of objects with unknown material properties: identification and robust planning\n1993 a data-driven framework for proactive intention-aware motion planning of a robot in a human environment\n1994 adaptive nonlinear control for perching of a bioinspired ornithopter\n1997 identification of effective motion primitives for ground vehicles\n1998 velocity regulation of 3d bipedal walking robots with uncertain dynamics through adaptive neural network controller\n1999 safe path planning with multi-model risk level sets\n2001 nonlinear model predictive control of hopping model using approximate step-to-step models for navigation on complex terrain\n2002 multi-mode trajectory optimization for impact-aware manipulation\n2003 tumbling and hopping locomotion control for a minor body exploration robot\n2008 predictive runtime monitoring of vehicle models using bayesian estimation and reachability analysis\n2010 learning to take good pictures of people with a robot photographer\n2012 robust control synthesis and verification for wire-borne underactuated brachiating robots using sum-of-squares optimization\n2013 fully convolutional geometric features for category-level object alignment\n2014 sequential motion planning for bipedal somersault via flywheel slip and momentum transmission with task space control\n2015 learning user-preferred mappings for intuitive robot control\n2022 object-aware centroid voting for monocular 3d object detection\n2023 detection-aware trajectory generation for a drone cinematographer\n2024 scalable collaborative manipulation with distributed trajectory planning\n2025 d2vo: monocular deep direct visual odometry\n2027 task-motion planning for safe and efficient urban driving\n2028 paintpath: defining path directionality in maps for autonomousground vehicles\n2029 motion prediction in visual object tracking\n2031 inertia-decoupled equations for hardware-in-the-loop simulation of an orbital robot with external forces\n2034 integrated benchmarking and design for reproducible and accessible evaluation of robotic agents\n2036 linear distributed clustering algorithm for modular robots based programmable matter\n2038 a compliance control method based on viscoelastic model for position-controlled humanoid robots\n2040 optimization-based path planning for person following using following field\n2044 design and experiments with loco auv: a low cost open-source autonomous underwater vehicle\n2045 robust ego and object 6-dof motion estimation and tracking\n2050 risk-constrained motion planning for robot locomotion: formulation and running robot demonstration\n2051 the pluggable distributed resource allocator (pdra): a middleware for distributed computing in mobile robotic networks\n2057 extrinsic and temporal calibration of automotive radar and 3d lidar\n2061 learning human-aware robot navigation from physical interaction via inverse reinforcement learning\n2062 learning optimized human motion via phase space analysis\n2064 gain scheduled controller design for balancing an autonomous bicycle\n2068 a bayesian approach for gas source localization in large indoor environments\n2076 gripping a kitchen knife on the cutting board\n2078 bayesian particles on cyclic graphs\n2079 when we first met: visual-inertial source localization for co-robot rendezvous\n2080 wet adhesion of micro-patterned interfaces for stable grasping of deformable objects\n2081 exploration strategy based on validity of actions in deep reinforcement learning\n2082 a compact, cable-driven, activatable soft wrist with six degrees of freedom for assembly tasks\n2083 mpc-graph: feedback motion planning using sparse sampling based neighborhood graph\n2084 maintaining stable grasps during highly dynamic robot trajectories\n2086 cmetric: a driving behavior measure using centrality functions\n2091 collaborative interaction models for optimized human robot teamwork\n2094 decentralized safe reactive planning under twtl specifications\n2097 pedestrian intention prediction for autonomous driving using a multiple stakeholder perspective model\n2098 kr-net: a dependable visual kidnap recovery network for indoor spaces\n2103 mixgail: autonomous driving using demonstrations with mixed qualities\n2105 an untethered 216-mg insect-sized jumping robot with wireless power transmission\n2106 vacuum driven auxetic switching structure and its application on a gripper and quadruped\n2107 geomorphological analysis using unpiloted aircraft systems, structure from motion, and deep learning\n2110 the omega turn: a biologically-inspired turning strategy for elongated limbless robots\n2111 getting to know one another: calibrating intent, capabilities, and trust for human-robot collaboration\n2112 emergence of swing-to-stance transition from interlocking mechanism in horse hindlimb\n2113 risk-aware planning and assignment for ground vehicles using uncertain perception from aerial vehicles\n2114 robots made from ice: an analysis of manufacturing techniques\n2116 reform: recognizing f-formations for social robots\n2118 balanced depth completion between dense depth inference and sparse range measurements via kiss-gp\n2119 design and control of squeeze: a spring-augmented quadrotor for interactions with the environment to squeeze-and-fly\n2121 decentralized control schemes for stable quadrupedal locomotion: a decomposition approach from centralized controllers\n2122 real-time robot end-effector pose estimation with deep network\n2124 mobile robot localization under non-gaussian noise usingcorrentropy similarity metric\n2134 dynamic object tracking for self-driving cars using monocular camera and lidar\n2137 a novel endoscope design using spiral technique for robotic-assisted endoscopy insertion\n2138 driving through ghosts: behavioral cloning with false positives\n2139 ultra-wideband aided uav positioning using incremental smoothing with ranges and multilateration\n2141 autonomous navigation over europa analogue terrain for an actively articulated wheel-on-limb rover\n2144 kinematic optimization of an underactuated anthropomorphic prosthetic hand\n2147 material mapping in unknown environments using tapping sound\n2149 optimizing a continuum manipulator's search policy through model-free reinforcement learning\n2150 active perception for outdoor localisation with an omnidirectional camera\n2151 biomimetic control scheme for musculoskeletal humanoids based on motor directional tuning in the brain\n2155 gpu parallelization of policy iteration rrt#\n2158 spatio-temporal attention model for tactile texture recognition\n2162 motion planning for collision-resilient mobile robots in obstacle-cluttered unknown environments with risk reward trade-offs\n2164 learning topological motion primitives for knot planning\n2166 semantic segmentation of underwater imagery: dataset and benchmark\n2168 go-chart: a miniature remotely accessible self-driving car robot\n2170 better together: online probabilistic clique change detection in 3d landmark-based maps\n2172 a multigait stringy robot with bi-stable soft-bodied structures in multiple viscous environments\n2174 a game-theoretic strategy-aware interaction algorithm with validation on real traffic data\n2175 towards cooperative transport of a suspended payload via two aerial robots with inertial sensing\n2178 an online training method for augmenting mpc with deep reinforcement learning\n2179 affordance-based mobile robot navigation among movable obstacles\n2180 robotic untangling of herbs and salads with parallel grippers\n2185 target tracking control of a wheel-less snake robot based on a supervised multi-layered snn\n2186 probabilistic semantic mapping for urban autonomous driving applications\n2188 cloth region segmentation for robust grasp selection\n2189 uncertainty aware texture classification and mapping using soft tactile sensors\n2193 design and control of roller grasper v2 for in-hand manipulation\n2195 navigation on the line: traversability analysis and path planning for extreme-terrain rappelling rovers\n2197 navigation-assistant path planning within a mav team\n2199 deep learning-based autonomous scanning electron microscope\n2200 multi-agent path planning under observation schedule constraints\n2209 deep inverse sensor models as priors for evidential occupancy mapping\n2210 quadrupedal robotic walking on sloped terrains via exact decomposition into coupled bipedal robots\n2214 geltip: a finger-shaped optical tactile sensor for robotic manipulation\n2215 pufferbot: actuated expandable structures for aerial robots\n2221 skill-based programming framework for composable reactive robot behaviors\n2222 multiplicative controller fusion: leveraging algorithmic priors for sample-efficient reinforcement learning and safe sim-to-real transfer\n2223 targetless calibration of lidar-imu system based on continuous-time batch estimation\n2224 a distributed range-only collision avoidance approach for low-cost large-scale multi-robot systems\n2225 localization safety validation for autonomous robots\n2227 design and evaluation of a perching hexacopter drone for energy harvesting from power lines\n2228 from human to robot everyday activity\n2232 dynamic legged manipulation of a ball through multi-contact optimization\n2235 a geometric perspective on visual imitation learning\n2236 robust autonomous navigation of a small-scale quadruped robot in real-world environments\n2240 the personalization of stiffness for an ankle-foot prosthesis emulator using human-in-the-loop optimization\n2243 development of selective driving joint forceps using shape memory polymer\n2246 proxemo: gait-based emotion learning andmulti-view proxemic fusion for socially-aware robot navigation\n2250 adaptive dynamic window approach for local navigation\n2253 a tip mount for transporting sensors and tools using soft growing robots\n2255 with whom to communicate: learning efficient communication for multi-robot collision avoidance\n2256 pbp-net: point projection and back-projection network for 3d point cloud segmentation\n2262 the importance of prior knowledge in precise multimodal prediction\n2264 learning skills to patch plans based on inaccurate models\n2265 cooperative control of mobile robots with stackelberg learning\n2268 design and experimentation of a variable stiffness bistable gripper\n2269 expedited multi-target search with guaranteed performance via multi-fidelity gaussian processes\n2270 mvlidarnet: real-time multi-class scene understanding for autonomous driving using multiple views\n2271 towards rl-based hydraulic excavator automation\n2274 development and analysis of digging and soil removing mechanisms for mole-bot: bio-inspired mole-like drilling robot\n2275 explore bravely: wheeled-legged robots traversing in unknown rough environment\n2276 brm localization: uav localization in gnss-denied environments based on matching of numerical map and uav images\n2277 minimally disruptive connectivity enhancement for resilient multi-robot teams\n2278 multi-modal pneumatic actuator for twisting, extension, and bending\n2280 whole-game motion capturing of team sports: system architecture and integrated calibration\n2281 simultaneous position-stiffness control of antagonistically driven twisted-coiled polymer actuators using model predictive control\n2282 data-driven models with expert influence: a hybrid approach to spatiotemporal process estimation\n2285 meta-reinforcement learning for robotic industrial insertion tasks\n2286 stochastic neural control using raw pointcloud data and building information models\n2287 tt-tsdf: memory-efficient tsdf with low-rank tensor train decomposition\n2290 hamlet: a hierarchical multimodal attention-based human activity recognition algorithm\n2291 dielecrophoretic introduction of the membrane proteins into the blm platforms for the electrophygiological analysis systems\n2292 knuckles that buckle: compliant underactuated limbs with joint hysteresis enable minimalist terrestrial robots\n2293 path negotiation for self-interested multirobot vehicles in shared space\n2295 dynamically constrained motion planning networks for non-holonomic robots\n2297 3dmotion-net: learning continuous flow function for 3d motionprediction\n2298 sparse discrete communication learning for multi-agent cooperation through backpropagation\n2301 simultaneous trajectory optimization and force control with soft contact mechanics\n2304 gp-slam+: real-time 3d lidar slam based on improved regionalized gaussian process map reconstruction\n2307 impedance control of humanoid walking on uneven terrain with centroidal momentum dynamics using quadratic programming\n2309 cazsl: zero-shot regression for pushing models by generalizing through context\n2313 objective functions for principal contact estimation from motion based on the geometrical singular condition\n2314 autonomous spot: long-range autonomous exploration of extreme environments with legged locomotion\n2315 fast texture classification using tactile neural coding and spiking neural network\n2318 city-scale grid-topological hybrid maps for autonomous mobile robot navigation in urban area\n2324 inspection-on-the-fly using hybrid physical interaction control for aerial manipulators\n2325 real-time constrained nonlinear model predictive control on so(3) for dynamic legged locomotion\n2333 telemanipulation with chopsticks: analyzing human factors in user demonstrations\n2334 three-dimensional posture optimization for biped robot stepping over large ditch based on a ducted-fan propulsion system\n2342 storm: screw theory toolbox for robot manipulator and mechanisms\n2349 development of deployable bending wrist for minimally invasive laparoscopic endoscope\n2353 predictive control of connected mixed traffic under random communication constraints\n2357 real-time virtual coach using lstm for assisting physical therapists with end-effector-based robot-assisted gait training\n2358 decentralized deep reinforcement learning for a distributed and adaptive locomotion controller of a hexapod robot\n2359 adaptive informative sampling with environment partitioning for heterogeneous multi-robot systems\n2362 animated cassie: a dynamic relatable robotic character\n2370 steering magnetic robots in two axes with one pair of maxwell coils\n2443 estimation and control of motor core temperature with online learning of thermal model parameters: application to musculoskeletal humanoids\n2447 design and modelling of a minimally actuated serial robot\n2448 a passivity-shortage based control framework for teleoperation with time-varying delays\n2450 improved fbg-based shape sensing methods for vascular catheterization treatment\n2455 energy-efficient locomotion generation and theoretical analysis of a quasi-passive dynamic walker\n2459 online visual place recognition via saliency re-identification\n2460 2d laser slam with general features represented by implicit functions\n2465 incorporating object intrinsic features within deep grasp affordance prediction\n2467 extremum seeking control for stiffness auto-tuning of a quasi-passive ankle exoskeleton\n2473 50 benchmarks for anthropomorphic hand function-based dexterity classification and kinematics-based hand design\n2474 risk-aware motion planning for a limbed robot with stochastic gripping forces using nonlinear programming\n2477 a flexible robotic depalletizing system for supermarket logistics\n2480 next-best-sense: a multi-criteria robotic exploration strategy for rfid tags discovery\n2481 3d-aware scene change captioning from multiview images\n2483 amae: adaptive motion-agnostic encoder for event-based object classification\n2485 meta-learning deep visual words for fast video object segmentation\n2487 reactive semantic planning in unexplored semantic environments using deep perceptual feedback\n2490 semantic localization considering uncertainty of object recognition\n2491 perspective-2-ellipsoid: bridging the gap between object detections and 6-dof camera pose\n2494 a point cloud registration pipeline using gaussian process regression for bathymetric slam\n2495 collaborative mission planning for long-term operation considering energy limitations\n2496 fast tennis swing motion by ball trajectory prediction and joint trajectory modification in standalone humanoid robot real-time system\n2497 feedback whole-body control of wheeled inverted pendulum humanoids using operational space\n2501 intermittent insertion control method with fine needle for adapting lung deformation due to breathing motion\n2504 object recognition, dynamic contact simulation, detection, and control of the flexible musculoskeletal hand using a recurrent neural network with parametric bias\n2505 a reconfigurable gripper for robotic autonomous depalletizing in supermarket logistics\n2506 snatcher: a highly mobile chameleon-inspired shooting and rapidly retracting manipulator\n2509 hybrid force-moment braking pulse: a haptic illusion to increase the perceived hardness of virtual surfaces\n2512 a multi-link in-pipe inspection robot composed of active and passive compliant joints\n2514 learning scheduling policies for multi-robot coordination with graph attention networks\n2515 probabilistic crowd gan: multimodal pedestrian trajectory prediction using a graph vehicle-pedestrian attention network\n2516 diversitygan: diversity-aware vehicle motion prediction via latent semantic sampling\n2517 experience-based prediction of unknown environments for enhanced belief space planning\n2520 forecasting trajectory and behavior of road-agents using spectral clustering in graph-lstms\n2522 efficient sampling-based maximum entropy inverse reinforcement learning with application to autonomous driving\n2525 delta descriptors: change-based place representation for robust visual localization\n2526 the invariant rauch-tung-striebel smoother\n2527 new formulation of mixed-integer conic programming for globally optimal grasp planning\n2528 probabilistic approach to physical object disentangling\n2529 drift-free and self-aligned imu-based human gait tracking system with augmented precision and robustness\n2531 cross-view semantic segmentation for sensing surroundings\n2532 deep context maps: agent trajectory prediction using location-specific latent maps\n2533 distributed consistent multi-robot semantic localization and mapping\n2537 segmenting the future\n2541 task priority matrix at the acceleration level: collision avoidance under relaxed constraints\n2542 an algorithm to design redundant manipulators of optimally fault-tolerant kinematic structure\n2544 energy management through footstep selection for bipedal robots\n2545 exploring the role of palm concavity and adaptability in soft synergistic robotic hands\n2546 deep learning based real-time oct image segmentation and correction for robotic needle insertion systems\n2548 prima6d: rotational primitive reconstruction for enhanced and robust 6d pose estimation\n2549 estimating an object's inertial parameters by robotic pushing: a data-driven approach\n2550 a variable-structure robot hand that uses the environment to achieve general purpose grasps\n2552 development of a spherical 2-dof wrist employing spatial parallelogram structure\n2553 milliscale features increase friction of soft skin in lubricated contact\n2554 self-supervised learning for precise pick-and-place without object model\n2555 the mag-gripper: a soft-rigid gripper augmented with an electromagnet to precisely handle clothes\n2556 hdr reconstruction based on the polarization camera\n2561 seeing through the occluders: robust monocular 6-dof object pose tracking via model-guided video object segmentation\n2562 robust robotic pouring using audition and haptics\n2564 active vertical takeoff of an aquatic uav\n2565 lightweight high voltage generator for untethered electroadhesive perching of micro air vehicles\n2567 alternating minimization based trajectory generation for quadrotor aggressive flight\n2570 delicate fabric handling using a soft robotic gripper with embedded microneedles\n2571 splitfusion: simultaneous tracking and mapping for non-rigid scenes\n2572 object-based pose graph for dynamic indoor environments\n2574 asynchronous adaptive sampling and reduced-order modeling of dynamic processes by robot teams via intermittently connected networks\n2575 multi-uav surveillance with minimum information idleness and latency constraints\n2576 an electrostatic/gecko-inspired adhesives soft robotic gripper\n2577 egad! an evolved grasping analysis dataset for diversity and reproducibility in robotic manipulation\n2578 deep gated multi-modal learning: in-hand object pose changes estimation using tactile and image data\n2580 in-flight efficient controller auto-tuning using a pair of uavs\n2581 development and evaluation of a linear series clutch actuator for vertical joint application with static balancing\n2582 shift-adaptive estimation of joint angle using instrumented brace with two stretch sensors based on gaussian mixture models\n2585 rectangular pyramid partitioning using integrated depth sensors (rappids): a fast planner for multicopter navigation\n2587 learning motion parameterizations of mobile pick and place actions from observing humans in virtual environments\n2588 contact-implicit trajectory optimization using an analytically solvable contact model for locomotion on variable ground\n2589 minor change, major gains: the effect of orientation formulation on solving time for multi-body trajectory optimization\n2590 vehicle-in-the-loop framework for testing long-term autonomy in a heterogeneous marine robot swarm\n2591 a versatile gripper for cloth manipulation\n2594 task-driven perception and manipulation for constrained placement of unknown objects\n2597 inertial velocity estimation for indoor navigation through magnetic gradient-based ekf and lstm learning model\n2598 wireless soft actuator based on liquid-gas phase transition controlled by millimeter-wave irradiation\n2599 c*: cross-modal simultaneous tracking and rendering for 6-dof monocular camera localization beyond modalities\n2600 accurate estimation of the position and shape of the rolling joint in hyper-redundant manipulators\n2601 contact point estimation along air tube based on acoustic sensing of pneumatic system noise\n2602 footstep modification including step time and angular momentum under disturbances on sparse footholds\n2606 coupled task-space admittance controller using dual quaternion logarithmic mapping\n2608 a framework for recognition and prediction of human motions in human-robot collaboration using probabilistic motion models\n2610 swarm relays: distributed self-healing ground-and-air connectivity chains\n2611 learning depth with very sparse supervision\n2614 improving multirotor landing performance on inclined surfaces using reverse thrust\n2616 a tendon-driven robot gripper with passively switchable underactuated surface and its physics simulation based parameter optimization\n2618 point cloud projective analysis for part-based grasp planning\n2621 the arches space-analogue demonstration mission: towards heterogeneous teams of autonomous robots for collaborative scientific sampling in planetary exploration\n2624 terrain-aware path planning and map update for mars sample return mission\n2626 modeling and experimental verification of a cable-constrained synchronous rotating mechanism considering friction effect\n2627 milimac: flexible catheter with miniaturized electromagnets as a small-footprint system for microrobotic tasks\n2628 design, modeling, and control of a coaxially aligned steerable (coast) guidewire robot\n2630 subject-independent semg pattern recognition by using a muscle source activation model\n2631 optimal pose estimation method for a multi-segment, programmable bevel-tip steerable needle\n2632 payload optimization of surgical instruments with rolling joint mechanisms\n2634 a supernumerary robotic leg powered by magnetorheological actuators to assist human locomotion\n2636 imu-based locomotor intention prediction for real-time use in transfemoral prostheses\n2639 modeling, calibration, and evaluation of a tendon-actuated planar parallel continuum robot\n2641 pauses provide effective control for an underactuated oscillating swimming robot\n2642 optimal linearization via quadratic programming\n2644 hybrid systems differential dynamic programming for whole-body motion planning of legged robots\n2647 denoising imu gyroscopes with deep learning for open-loop attitude estimation\n2648 towards real-time non-gaussian slam for underdetermined navigation\n2651 comparing visual odometry systems in actively deforming simulated colon environments\n2652 variational inference with parameter learning applied to vehicle trajectory estimation\n2653 towards in situ backlash estimation of continuum robots using an endoscopic camera\n2654 evolved neuromorphic control for high speed divergence-based landings of mavs\n2655 energy-based cooperative control for landing fixed-wing uavs on mobile platforms under communication delays\n2656 staging energy sources to extend flight time of a multirotor uav\n2657 dense isometric non-rigid shape-from-motion based on graph optimization and edge selection\n2666 multimodal teleoperation of heterogeneous robots within a construction environment\n2669 a manipulability criterion for magnetic actuation of miniature swimmers with flexible flagellum\n2670 towards the long-endurance flight of an insect-inspired, tailless, two-winged, flapping-wing flying robot\n2672 design of the umaze platform and microrobots for independent control and micromanipulation tasks\n2673 glas: global-to-local safe autonomy synthesis for multi-robot motion planning with end-to-end learning\n2674 multi-robot coordinated planning in confined environments under kinematic constraints\n2675 evaluations of response characteristics of on-chip gel actuators for various single cell manipulations\n2686 perception-aware human-assisted navigation of mobile robots on persistent trajectories\n2687 development of smartphone-based human-robot interfaces for individuals with disabilities\n2688 autonomous and cooperative design of the monitor positions for a team of uavs to maximize the quantity and quality of detected objects\n2691 discontinuous and smooth depth completion with binary anisotropic diffusion tensor\n2692 integrating features acceleration in visual predictive control\n2693 spatiotemporal calibration of camera and 3d laser scanner\n2696 towards in-flight transfer of payloads between multirotors\n2699 coverage path planning with track spacing adaptation for autonomous underwater vehicles\n2700 force-ultrasound fusion: bringing spine robotic-us to the next 'level'\n2702 ultrasound-guided wireless tubular robotic anchoring system\n2704 optic nerve sheath fenestration with a multi-arm continuum robot\n2707 automatic shape control of deformable wires based on model-free visual servoing\n2708 image transformation and cnns: a strategy for encoding human locomotor intent for autonomous wearable robots\n2711 an augmented reality spatial referencing system for mobile robots\n2712 proxy-based approach for position synchronization of delayed robot coupling without sacrificing performance\n2713 a multi-channel reinforcement learning framework for robotic mirror therapy\n2714 a new delayless adaptive oscillator for gait assistance\n2715 adaptive precision-enhancing hand rendering for wearable fingertip tracking devices\n2717 the 6-dof implementation of the energy-reflection based time domain passivity approach with preservation of physical coupling behavior\n2718 development of exo-glove for measuring 3-axis forces acting on the human finger without obstructing natural human-object interaction\n2720 detection and control of air liquid interface with an open-channel microfluidic chip for circulating tumor cells isolation from human whole blood\n2721 probabilistic end-to-end vehicle navigation in complex dynamic environments with multimodal sensor fusion\n2722 medusa: a multi-environment dual-robot for underwater sample acquisition\n2728 perpendicular curve-based incomplete orientation mapping for teleoperation with dof asymmetry\n2731 a distributed pipeline for scalable, deconflicted formation flying\n2732 rilaas: robot inference and learning as a service\n2733 pac-man is overkill\n2734 energy autonomy for resource-constrained multi robot missions\n2736 stable flight of a flapping-wing micro air vehicle under wind disturbance\n2737 smallbug: a 30-mg crawling robot driven by a high-frequency flexible sma microactuator\n2739 knowledge transfer between different uavs for trajectory tracking\n2740 a morphing cargo drone for safe flight in proximity of humans\n2741 edge enhanced implicit orientation learning with geometric prior for 6d pose estimation\n2743 development of hiryu-ii: a long-reach articulated modular manipulator driven by thrusters\n2746 loop-net: joint unsupervised disparity and optical flow estimation of stereo videos with spatiotemporal loop consistency\n2748 real-time fusion network for rgb-d semantic segmentation incorporating unexpected obstacle detection for road-driving images\n2749 game theoretic formation design for probabilistic barrier coverage\n2750 dual-arm control for enhanced magnetic manipulation\n2751 defensive escort teams for navigation in crowds via multi-agent deep reinforcement learning\n2753 a novel coding architecture for lidar point cloud sequence\n2754 time-relative rtk-gnss: gnss loop closure in pose graph optimization\n2758 rovins: robust omnidirectional visual inertial navigation system\n2759 dynamic and versatile humanoid walking via embedding 3d actuated slip model with hybrid lip based stepping\n2760 non-linear trajectory optimization for large step-ups: application to the humanoid robot atlas\n2762 silhonet-fisheye: adaptation of a roi-based object pose estimation network to monocular fisheye images\n2765 boosting deep open world recognition by clustering\n2768 unsupervised domain adaptation through inter-modal rotation for rgb-d object recognition\n2769 alphred: a multi-modal operations quadruped robot for package delivery applications\n2771 lio - a personal robot assistant for human-robot interaction and care applications\n2772 gmmloc: structure consistent visual localization with gaussian mixture models\n2774 building energy-cost maps from aerial images and ground robot measurements with semi-supervised deep learning\n2776 automatic control synthesis for swarm robots from formation and location-based high-level specifications\n2777 graph neural networks for decentralized multi-robot path planning\n2778 linespyx: a power line inspection robot based on digital radiography\n2779 improving visual slam in car-navigated urban environments with appearance maps\n2780 a multi-system chaotic path planner for fast and unpredictable online coverage of terrains\n2781 collision avoidance based on robust lexicographic task assignment\n2782 3d instance embedding learning with a structure-aware loss function for point cloud segmentation\n2783 improving tracking through human-robot sensory augmentation\n2784 generating reactive approach motions towards allowable manifolds using generalized trajectories from demonstrations\n2786 \"good robot!\": efficient reinforcement learning for multi-step visual tasks with sim to real transfer\n2787 an actor-based programming framework for swarm robotic systems\n2788 multi-sensor next-best-view planning as matroid-constrained submodular maximization\n2790 idda: a large-scale multi-domain dataset for autonomous driving\n2794 perceptive model predictive control for continuous mobile manipulation\n2795 mlod: awareness of extrinsic perturbation in multi-lidar 3d object detection for autonomous driving\n2796 3d-mininet: learning a 2d representation from point clouds for fast and efficient 3d lidar semantic segmentation\n2797 augmented reality and robotic-assistance for percutaneous nephrolithotomy\n2798 learning gait models with varying walking speeds\n2800 on the use of (lockable) parallel elasticity in active prosthetic ankles\n2801 compliant control and compensation for a compact cable-driven robotic manipulator\n2802 machine learning model comparisons of user independent & dependent intent recognition systems for powered prostheses\n2804 manipulation planning using object-centered predicates and hierarchical decomposition of contextual actions\n2807 grasping in the wild: learning 6dof closed-loop grasping from low-cost demonstrations\n2808 tlio: tight learned inertial odometry\n2813 sim2real predictivity: does evaluation in simulation predict real-world performance\n2814 persistent intelligence, surveillance, and reconnaissance using multiple autonomous vehicles with asynchronous route updates\n2815 socially and contextually aware human motion and pose forecasting\n2817 a realistic simulation environment for mri-based robust control of untethered magnetic robots with intra-operational imaging\n2820 minimum time - minimum jerk optimal traffic management for agvs\n2821 topology-guided roadmap construction with dynamic region sampling\n2822 finite-horizon lqr control of quadrotors on se_2(3)\n2826 augmenting visual place recognition with structural cues\n2827 a control scheme for smooth transition in physical human-robot-environment between two modes: augmentation and autonomous\n2828 human navigation using phantom tactile sensation based vibrotactile feedback\n2829 applying force perturbations using a wearable robotic neck brace\n2832 open-loop orientation control using dynamic magnetic fields\n2833 fluid-structure interaction hydrodynamics analysis on a deformed bionic flipper with non-uniformly distributed stiffness\n2834 computational structure design of a bio-inspired armwing mechanism\n2836 fully actuated body-mounted robotic system for mri-guided lower back pain injections: initial phantom and cadaver studies\n2838 supervised semi-autonomous control for surgical robot based on bayesian optimization\n2840 improving low-level control of the exoskeleton atalante in single support by compensating joint flexibility\n2842 multi-robot active sensing and environmental model learning with distributed gaussian process\n2843 mixed-integer linear programming models for multi-robot non-adversarial search\n2845 energetic passivity decoding of human hip joint for physical human-robot interaction\n2846 semg-based human-in-the-loop control of elbow assistive robots for physical tasks and muscle strength training\n2847 pneumatic duplex-chambered inchworm mechanism for narrow pipes driven by only two air supply lines\n2848 long-term localization with time series map prediction for mobile robots in dynamic environments\n2849 inverted and inclined climbing using capillary adhesion in a quadrupedal insect-scale robot\n2850 perceptive locomotion in rough terrain -- online foothold optimization\n2852 casecrawler: a lightweight and low-profile crawling phone case robot\n2853 waste not, want not: lessons in rapid quadrupedal gait termination from thousands of suboptimal solutions\n2854 learning force control for contact-rich manipulation tasks with rigid position-controlled robots\n2856 elastomeric continuously variable transmission combined with twisted string actuator\n2860 r-track: separable modular climbing robot design for wall-to-wall transition\n2861 a soft, modular, and bi-stable dome actuator for programmable multi-modal locomotion\n2862 robotic deep rolling with iterative learning motion and force control\n2867 the astar high speed amphibious sprawl tuned robot: design and experiments\n2869 enhancement of force exertion capability of a mobile manipulator by kinematic reconfiguration\n2871 polylidar - polygons from triangular meshes\n2872 acmarker: acoustic camera-based fiducial marker system in underwater environment\n2873 toward enabling a hundred drones to land in a minute\n2874 tiltdrone: a fully-actuated tilting quadrotor platform\n2875 interact with me: an exploratory study on interaction factors for active physical human-robot interaction\n2877 coordinated appendages accumulate more energy to self-right on the ground\n2878 unmanned aerial sensor placement for cluttered environments\n2880 asynchronous and parallel distributed pose graph optimization\n2881 gaussian process online learning with a sparse data stream\n2882 towards better surgical instrument segmentation in endoscopic vision: multi-angle feature aggregation and contour supervision\n2884 development of dementia care training system based on augmented reality and whole body wearable tactile sensor\n2886 data-driven disturbance observers for estimating external forces on soft robots\n2888 self-sensing and feedback control for a twin coil spring-based flexible ultrasonic motor\n2891 convergence analysis of hybrid control systems in the form of backward chained behavior trees\n2892 fireant3d: a 3d self-climbing robot towards non-latticed robotic self-assembly\n2895 actor-critic reinforcement learning for control with stability guarantee\n2897 matching color aerial images and underwater sonar images using deep learning for underwater localization\n2898 fail-safe flight of a fully-actuated quadcopter in a single motor failure\n2899 improvement in measurement area of 3d lidar for a mobile robot using a mirror mounted on a manipulator\n2901 dynamic median consensus for marine multi-robot systems using acoustic communication\n2906 robust loop closure method for multi-robot map fusion by integration of consistency and data similarity\n2907 modman: an advanced reconfigurable manipulator system with genderless connector and automatic kinematic modeling algorithm\n2909 kubits: solid-state self-reconfiguration with programmable magnets\n2910 development of a steep slope mobile robot with propulsion adhesion\n2911 online exploration and coverage planning in unknown obstacle-cluttered environments\n2912 optimization-based investigation of bioinspired variable gearing of the distributed actuation mechanism to maximize velocity and force\n2913 robotic episodic cognitive learning inspired by hippocampal spatial cells\n2914 a bio-inspired quadruped robot exploiting flexible shoulder for stable and efficient walking\n2916 development of a running hexapod robot with differentiated front and hind leg morphology and functionality\n2919 prediction of backhoe loading motion via the beta-process hidden markov model\n2921 robust rl-based map-less local planning: using 2d point clouds as observations\n2924 an ionic polymer metal composite (ipmc)-driven linear peristaltic microfluidic pump\n2925 low-cost coil-shaped optical fiber displacement sensor for a twisted and coiled polymer fiber actuator unit\n2926 a model-based sensor fusion approach for force and shape estimation in soft robotics\n2933 design of fully soft actuator with double-helix tendon routing path for twisting motion\n2939 verefine: integrating object pose verification with physics-guided iterative refinement\n2941 a dexterous soft robotic hand for delicate in-hand manipulation\n2942 knowledge-based grasp planning using dynamic self-organizing network\n2943 motion planning for dual-arm manipulation of elastic rods\n2945 environment-aware grasp strategy planning in clutter for a variable stiffness hand\n2948 online acquisition of close-range proximity sensor models for precise object grasping and verification\n2950 object-agnostic dexterous manipulation of partially constrained trajectories\n2951 describing physics for physical reasoning: force-based sequential manipulation planning\n2952 vision and force based autonomous robotic coating with rollers\n2953 model-based coupling for co-simulation of robotic contact tasks\n2954 ecg: edge-aware point cloud completion with graph convolution\n2955 don't forget the past: recurrent depth estimation from monocular video\n2961 aircaprl: autonomous aerial human motion capture using deep reinforcement learning\n2962 model-based reinforcement learning for time-optimal velocity control\n2964 deep reinforcement learning for tactile robotics: learning to type on a braille keyboard\n2965 ultra-thin joint torque sensor with enhanced sensitivity for robotic application\n2967 state-continuity approximation of markov decision processes via finite element methods for autonomous system planning\n2970 blt gripper: an adaptive gripper with active transition capability between precise pinch and compliant grasp\n2971 hierarchical tracking control with arbitrary task dimensions: application to trajectory tracking on submanifolds\n2972 ridm: reinforced inverse dynamics modeling for learning from a single observed demonstration\n2975 learning variable impedance control for contact sensitive tasks\n2976 person-directed pointing gestures and inter-personal relationship: expression of politeness to friendliness by android robots\n2978 a feasibility study of culture-aware cloud services for conversational robots\n2979 deep reinforcement learning for safe local planning of a ground vehicle in unknown rough terrain\n2984 cluster-based penalty scaling for robust pose graph optimization\n2985 improved data association using buffered pose adjustment for map-aided localization\n2986 hdmi-loc: exploiting high definition map image for precise localization via bitwise particle filter\n2987 a control barrier function approach for maximizing performance while fulfilling to iso/ts 15066 regulations\n2988 teleoperation and contact detection of a waterjet-actuated soft continuum manipulator for low-cost gastroscopy\n2990 visual coverage path planning for urban environments\n2993 risk-sensitive sequential action control with multi-modal human trajectory forecasting for safe crowd-robot interaction\n2995 nonlinear mpc for collision avoidance and control of uavs with dynamic obstacles\n2996 non-gaussian chance-constrained trajectory planning for autonomous vehicles in the presence of uncertain agents\n2998 trajectory planning over regular general surfaces with application in robot-guided deposition printing\n2999 an opportunistic strategy for motion planning in the presence of soft task constraints\n3002 inverse kinematics of redundant manipulators with dynamic bounds on joint movements\n3003 human perception-optimized planning for comfortable vr based telepresence\n3004 a disturbance-aware trajectory planning scheme based on model predictive control\n3006 model-adaptive high-speed collision detection for serial-chain robot manipulators\n3007 deepmnavigate: deep reinforced multi-robot navigation unifying local & global collision avoidance\n3009 quaternion-based trajectory optimization of human postures for inducing target muscle activation patterns\n3010 fast sequence rejection for multi-goal planning with dubins vehicle\n3012 combining speed and separation monitoring with power and force limiting for safe collaborative robotics applications\n3013 a unified nmpc scheme for mavs navigation with 3d collision avoidance under position uncertainty\n3015 frozone: freezing-free, pedestrian-friendly navigation in human crowds\n3016 modular, accessible, sensorized objects for evaluating the grasping and manipulation capabilities of grippers and hands\n3017 design, modelling, and implementation of a 7-dof cable-driven haptic device with a configurable cable platform\n3019 asymptotically-optimal topological nearest-neighbor filtering\n3020 online replanning with human-in-the-loop for non-prehensile manipulation in clutter ' a trajectory optimization based approach\n3021 neural manipulation planning on constraint manifolds\n3022 piezoelectric grippers for mobile micromanipulation\n3023 stable in-grasp manipulation with a low-cost robot hand by using 3-axis tactile sensors with a cnn\n3026 feedback enhanced motion planning for autonomous vehicles\n3027 magnetically programmable cuboids for 2d locomotion and collaborative assembly\n3030 six-axis force/torque fingertip sensor for an anthropomorphic robot\n3032 low latency trajectory predictions for interaction aware highway driving\n3033 appld: adaptive planner parameter learning from demonstration\n3034 precision assembly of heavy objects suspended with multiple cables from a crane\n3035 explicit domain adaptation with loosely coupled samples\n3037 adaptive aerial grasping and perching with dual elasticity combined suction cup\n3038 target search on road networks with range-constrained uavs and ground-based mobile recharging vehicles\n3041 simultaneously learning corrections and error models for geometry-based visual odometry methods\n3042 safe optimal control under parametric uncertainties\n3044 vitruvio: an open-source leg design optimization toolbox for walking robots\n3046 alleviating the burden of labeling: sentence generation by attention branch encoder-decoder network\n3050 robot gaze behaviors in human-to-robot handovers\n3051 self-attention based visual-tactile fusion learning for predicting grasp outcomes\n3052 socially assistive robots at work: making break-taking interventions more pleasant, enjoyable, and engaging\n3053 learn by observation: imitation learning for drone patrolling from videos of a human navigator\n3056 lidar essential beam model for accurate width estimation of thin poles\n3060 invariant transform experience replay: data augmentation for deep reinforcement learning\n3061 imitation learning based on bilateral control for human'robot cooperation\n3063 autonomous tissue retraction in robotic assisted minimally invasive surgery - a feasibilty study\n3064 6-axis force/torque sensor with a novel autonomous weight compensating capability for robotic applications\n3066 multi-fingered grasp planning via inference in deep neural networks\n3067 collision avoidance in human-robot interaction using kinect vision system combined with robot's model and data\n3068 miniaturized robotics: the smallest camera operator bot pays tribute to david bowie.\n3070 development of \u03b4-type mobile robot driven by 3 standing wave type piezoelectric ultrasonic motors\n3072 lyapunov-stable orientation estimator for humanoid robots\n3073 ufomap: an efficient probabilistic 3d mapping framework that embraces the unknown\n3074 machine learning for active gravity compensation in robotics: application to neurological rehabilitation systems\n3075 learning robust manipulation tasks involving contact using trajectory parameterized probabilistic principal component analysis\n3076 unsupervised pedestrian pose prediction -- a deep predictive coding network approach for autonomous vehicle perception\n3077 stable autonomous spiral stair climbing of tracked vehicles using wall reaction force\n3079 tsbp: tangent space belief propagation for manifold learning\n3080 exploiting visual-outer shape for tactile-inner shape estimation of objects covered with soft materials\n3081 heteroscedastic uncertainty for robust generative latent dynamics\n3082 q-vae for disentangled representation learning and latent dynamical systems\n3083 can a robot's touches express the feeling of kawaii toward an object?\n3084 optimisation of body-ground contact for augmenting the whole-body loco-manipulation of quadruped robots\n3085 communication maintenance of robotic parasitic antenna arrays\n3087 bilateral humanoid teleoperation system using whole-body exoskeleton cockpit tablis\n3088 adaptive-gains enforcing constraints in closed-loop qp control\n3089 multi-contact locomotion planning for humanoid robot based on sustainable contact graph with local contact modification\n3091 self-assessment of grasp affordance transfer\n3093 xbot real-time software framework for robotics: from the developer to the user perspective\n3094 natural criteria for comparison of pedestrian flow forecasting models\n3097 lloyd-based approach for robots navigation in human-shared environments\n3098 flight path planning of solar powered uav for sustainable communication relay\n3099 rovall: design and development of a multi-terrain towed robot with variable lug-length wheels\n3102 reconfigurable soft flexure hinges via pinched tubes\n3104 rolling soft membrane-driven tensegrity robots\n3105 exploiting the morphology of a shape memory spring as the active backbone of a highly dexterous tendril robot (atbr)\n3106 retraction mechanism of soft torus robot with a hydrostatic skeleton\n3107 integrated actuation and self-sensing for twisted-and-coiled actuators with applications to innervated soft robots\n3109 self-propelled colonoscopy robot using flexible paddles\n3111 self-healing cell tactile sensor by ultraflexible printed electrodes\n3113 assured runtime monitoring and planning: towards verification of neural networks for safe autonomous operations\n3114 electromagnetic actuation of microrobots in a simulated vascular structure with a position estimator based motion controller\n3115 wide area exploration system using passive-follower robots towed by multiple winches\n3117 end-to-end velocity estimation for autonomous racing\n3118 end-to-end tactile feedback loop: from soft sensor skin over deep gru-autoencoders to tactile stimulation\n3119 parallel haptic rendering for orthopedic surgery simulators\n3120 simultaneous 3d forming and patterning method of realizing soft ipmc robots\n3121 visual slam with drift-free rotation estimation in manhattan world\n3123 structure-slam: low-drift monocular slam in indoor environments\n3124 softhandler: an integrated soft robotic system for the handling of heterogeneous objects\n3125 a grasping-centered analysis for cloth manipulation\n3126 information correlated levy walk exploration and distributed mapping using a swarm of robots\n3127 virtual ir sensing for planetary rovers: improved terrain classification and thermal inertia estimation\n3128 definition and application of variable resistance coefficient for wheeled mobile robots on deformable terrain\n3129 a passive phri controller for assisting the user in partially known tasks\n3130 on the false positives and false negatives of the jacobian matrix in kinematically redundant parallel mechanisms\n3131 gaussians on riemannian manifolds: applications for robot learning and adaptive control\n3132 torque-bounded admittance control realized by a set-valued algebraic feedback\n3133 decoding motor skills of ai and human policies: a study on humanoid and human balance control\n3134 distributed control for cooperative manipulation with event-triggered communication\n3135 calculating the support function of complex continuous surfaces with applications to minimum distance computation and optimal grasp planning\n3136 a routing framework for heterogeneous multi-robot teams in exploration tasks\n3137 continuously variable stiffness mechanism using nonuniform patterns on coaxial tubes for continuum microsurgical robot\n3138 achieving versatile energy efficiency with the wanderer biped robot\n3139 in vitro design investigation of a rotating helical magnetic swimmer for combined 3-d navigation and blood clot removal\n3151 marker-based mapping and localization for autonomous valet parking\n3152 parameter optimization for loop closure detection in closed\u00e2 environments\n3153 radar-camera sensor fusion for joint object detection\u00e2 and distance estimation in autonomous vehicles\n3154 salsanext fast uncertainty-aware semantic segmentation\u00e2 of lidar point clouds for autonomous driving\n3155 sdvtracker real-time multi-sensor association and tracking for\u00e2 self-driving vehicles\n3156 situation awareness at autonomous vehicle handover - preliminary\u00e2 results of a quantitative analysis\n3157 towards context-aware navigation for\u00e2 long-term autonomy in agricultural environments\n3158 efficient sampling in pomdps with lipschitz bandits\u00e2 for motion planning in continuous spaces\n3159 impact of traffic lights on trajectory forecasting of human-driven\u00e2 vehicles near signalized intersections\n3161 semantic grid map based lidar localization in highly dynamic\u00e2 urban scenarios\n3162 acquiring mechanical knowledge from 3d point clouds\n3163 representation and experience-based learning of explainable models for robot action execution\n3164 emergent adaptive gait generation through hebbian sensor-motor maps by morphological probing\n3165 mixed reality as a bidirectional communication interface for human-robot interaction\n3166 the robot as scientist: using mental simulation to test causal hypotheses extracted from human activities in virtual reality\n3167 graph-based hierarchical knowledge representation for robot task transfer from virtual to physical world\n3168 an optimized tilt mechanism for a new steady-hand eye robot\n3169 optimization-based hierarchical motion planning for autonomous racing\n3170 swingbot: learning physical features from in-hand tactile exploration for dynamic swing-up manipulation\n3182 learning visuomotor policies for aerial navigation using cross-modal representations\n3183 learning vision-based physics intuition models for non-disruptive object extraction\n3184 computational design of balanced open link planar mechanisms with counterweights from user sketches\n3238 relative pose estimation and planar reconstruction via superpixel-driven multiple homographies\n3239 real-time constrained nonlinear model predictive control on so(3) for dynamic legged locomotion\n3240 combining compliance control, cad based localization, and a multi-modal gripper for rapid and robust programming of assembly tasks\n3241 freebot: a freeform modular self-reconfigurable robot with arbitrary connection point - design and implementation\n3242 computational design of balanced open link planar mechanisms with counterweights from user sketches\n3243 a tip mount for transporting sensors and tools using soft growing robots\n3244 robot calligraphy using pseudospectral optimal controlin conjunction with a novel dynamic brush model\n3245 diabolo orientation stabilization by learning predictive model for unstable unknown-dynamics juggling manipulation\n3246 towards micro robot hydrobatics: vision-based guidance, navigation, and control for agile underwater vehicles in confined environments\n3247 animated cassie: a dynamic relatable robotic character\n3248 safety considerations in deep control policies with safety barrier certificates under uncertainty\n3249 navigation on the line: traversability analysis and path planning for extreme-terrain rappelling rovers\n3250 autonomous spot: long-range autonomous exploration of extreme environments with legged locomotion\n3251 stable autonomous spiral stair climbing of tracked vehicles using wall reaction force\n3252 unsupervised domain adaptation for transferring plant classification systems to new field environments, crops, and robots\n3253 diat (depth-infrared image annotation transfer) for training a depth-based pig-pose detector\n3254 incorporating spatial constraints into a bayesian tracking framework for improved localisation in agricultural environments\n3255 fruit quality control by surface analysis using a bio-inspired soft tactile sensor\n3256 pit30m: a benchmark for global localization in the age of self-driving cars\n3257 oceanvoy: a hybrid energy planning system for autonomous sailboat\n3258 mhyro: modular hybrid robot for contact inspection and maintenance in oil&gas plants\n3259 llama: design and control of an omnidirectional human mission scale quadrupedal robot\n3475 rgb-d sensing of challenging deformable objects\n3476 building 3d deformable object models in partially observable robotic environments\n3477 soma: a data-driven representation framework for semantic soft object manipulation\n3478 task-oriented contact adjustment in deformable objects manipulation with non-fixed contact\n3479 adaptive shape servoing of elastic rods using parameterized regression features and auto-tuning motion controls\n3480 automatic shape control of deformable rods based on data-driven implicit sensorimotor models\n3481 assembly strategy for deformable ring-shaped objects\n3482 mgsd: multi-modal gaussian shape descriptors for correspondence matching of linear and planar deformable objects\n3494 dual-armed manipulation planning for tethered tools\n3495 prediction of tactile perception from vision on deformable objects\n3496 shape control of elastoplastic deformable linear objects through reinforcement learning\n3497 interaction identification through tactile sensing during cloth manipulation using a 3-axis touch sensor\n3498 toward a general framework for 3d deformable object grasping and manipulation\n3499 experimental multi-camera setup for perception of dynamic objects\n3500 real-time state estimation of deformable objects with dynamical simulation\n3601 human-robot collaborative carrying using visual and force sensing\n3622 toward detecting anomalies in activities for daily living with a mobile robot using plan recognition\n3623 human-aware robot behavior in healthcare facilities\n3624 an interactive drink serving social robot: initial system implementation\n3625 towards whole arm manipulation for outpatient care\n3626 morphological switching robots to support independent living for older adults\n3627 towards conversational interfaces and visual memory representation for social robots helping the elderly\n3628 on new research guidelines for the deployment of socially assistive robots for elder care amidst the covid-19 pandemic\n3629 towards physical human-robot interaction using force support for nursing care bed activities\n3712 service robot teaching assistant in school class-room\n3713 infant abnormal behavior classification through weakly supervised learning\n3714 amusing androids: the argument for humour in healthcare robotics\n3715 towards explainable diagnosis of alzheimer's\n3716 vote400(voide of the elderly 400 hours): a speech dataset to study voice interface for elderly-care\n3717 toward a reinforcement learning based framework for learning cognitive empathy in human-robot interactions\n3718 improve identity recognition with occlusion detection-based feature selection\n3719 etri activity3d: a large scale rgb d dataset for robots to recognize daily activities of the elderly\n3720 deep emotion change detection for human-robot interaction\n3721 efficiency analysis of multi-head attention models for social dynamics prediction\n3722 leveraging reinforcement learning for human motor skill acquisition\n3723 efficient learning of socially aware robot approaching behavior toward groups via meta-reinforcement learning", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000140, "year": null}, {"Unnamed: 0": 1161, "autor": 141, "date": null, "content": "toppra: Time-Optimal Path Parameterization\nOverview\nSupports\nCiting toppra\nOverview\ntoppra is a library for computing the time-optimal path parametrization for robots subject to kinematic and dynamic constraints. In general, given the inputs:\na geometric path p(s), s in [0, s_end];\na list of constraints on joint velocity, joint accelerations, tool Cartesian velocity, et cetera.\ntoppra returns the time-optimal path parameterization: s_dot (s), from which the fastest trajectory q(t) that satisfies the given constraints can be found.\nDocumentation and tutorials are available here.\nYou can install the package with pip:\npip install toppra\nTo install from source for development:\npip install -r requirement3.txt\npip install -e .\nSupport\nBug tracking\nPlease report any issues, questions or feature request via Github issues tracker.\nHave a quick question? Try asking in our slack channel.\nContributions\nPull Requests are welcome! Create a Pull Request and we will review your proposal!\nCredits\ntoppra was originally developed by Hung Pham (Eureka Robotics, former CRI Group) and Ph\u1ea1m Quang C\u01b0ong (Eureka Robotics, CRI Group) with major contributions from talented contributors:\nJoseph Mirabel (C++ API)\nEdsterG (Python3 support).\nIf you have taken part in developing and supporting the library, feel free to add your name to the list.\nThe development is also generously supported by Eureka Robotics.\nCiting toppra\nIf you use this library for your research, we encourage you to\nreference the accompanying paper A new approach to Time-Optimal Path Parameterization based on Reachability Analysis, IEEE Transactions on Robotics, vol. 34(3), pp. 645-659, 2018.\nput a star on this repository.", "link": "https://github.com/hungpham2511/toppra", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "toppra: time-optimal path parameterization\noverview\nsupports\nciting toppra\noverview\ntoppra is a library for computing the time-optimal path parametrization for robots subject to kinematic and dynamic constraints. in general, given the inputs:\na geometric path p(s), s in [0, s_end];\na list of constraints on joint velocity, joint accelerations, -----> tool !!!  cartesian velocity, et cetera.\ntoppra returns the time-optimal path parameterization: s_dot (s), from which the fastest trajectory q(t) that satisfies the given constraints can be found.\ndocumentation and tutorials are available here.\nyou can install the package with pip:\npip install toppra\nto install from source for development:\npip install -r requirement3.txt\npip install -e .\nsupport\nbug tracking\nplease report any issues, questions or feature request via github issues tracker.\nhave a quick question? try asking in our slack channel.\ncontributions\npull requests are welcome! create a pull request and we will review your proposal!\ncredits\ntoppra was originally developed by hung pham (eureka robotics, former cri group) and ph\u1ea1m quang c\u01b0ong (eureka robotics, cri group) with major contributions from talented contributors:\njoseph mirabel (c++ api)\nedsterg (python3 support).\nif you have taken part in developing and supporting the library, feel free to add your name to the list.\nthe development is also generously supported by eureka robotics.\nciting toppra\nif you use this library for your research, we encourage you to\nreference the accompanying paper a new approach to time-optimal path parameterization based on reachability analysis, ieee transactions on robotics, vol. 34(3), pp. 645-659, 2018.\nput a star on this repository.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000141, "year": null}, {"Unnamed: 0": 1164, "autor": 144, "date": null, "content": "Bonnet: An Open-Source Training and Deployment Framework for Semantic Segmentation in Robotics.\nBy Andres Milioto @ University of Bonn.\n(for the new Pytorch version, go here)\nCityscapes Urban Scene understanding.\nStandalone Video Predictor - 512x256px\nStandalone Video Predictor - 768x384px (inception model)\nStandalone Video Predictor - 1024x512px\nPerson Segmentation\nUptown funk (inception model)\nCan't stop the feeling (inception model)\nCrop vs. Weed Semantic Segmentation.\nROS node prediction - Video\nDescription\nThis code provides a framework to easily add architectures and datasets, in order to train and deploy CNNs for a robot. It contains a full training pipeline in python using Tensorflow and OpenCV, and it also some C++ apps to deploy a frozen protobuf in ROS and standalone. The C++ library is made in a way which allows to add other backends (such as TensorRT and MvNCS), but only Tensorflow and TensorRT are implemented for now. For now, we will keep it this way because we are mostly interested in deployment for the Jetson and Drive platforms, but if you have a specific need, we accept pull requests!\nThe networks included is based of of many other architectures (see below), but not exactly a copy of any of them. As seen in the videos, they run very fast in both GPU and CPU, and they are designed with performance in mind, at the cost of a slight accuracy loss. Feel free to use it as a model to implement your own architecture.\nAll scripts have been tested on the following configurations:\nx86 Ubuntu 16.04 with an NVIDIA GeForce 940MX GPU (nvidia-384, CUDA9, CUDNN7, TF 1.7, TensorRT3)\nx86 Ubuntu 16.04 with an NVIDIA GTX1080Ti GPU (nvidia-375, CUDA9, CUDNN7, TF 1.7, TensorRT3)\nx86 Ubuntu 16.04 and 14.04 with no GPU (TF 1.7, running on CPU in NHWC mode, no TensorRT support)\nJetson TX2 (full Jetpack 3.2)\nWe also provide a Dockerfile to make it easy to run without worrying about the dependencies, which is based on the official nvidia/cuda image containing cuda9 and cudnn7. In order to build and run this image with support for X11 (to display the results), you can run this in the repo root directory (nvidia-docker should be used instead of vainilla docker):\n$ docker pull tano297/bonnet:cuda9-cudnn7-tf17-trt304\n$ nvidia-docker build -t bonnet .\n$ nvidia-docker run -ti --rm -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix -v $HOME/.Xauthority:/home/developer/.Xauthority -v /home/$USER/data:/shared --net=host --pid=host --ipc=host bonnet /bin/bash\n-v /home/$USER/data:/share can be replaced to point to wherever you store the data and trained models, in order to include the data inside the container for inference/training.\nDeployment\n/deploy_cpp contains C++ code for deployment on robot of the full pipeline, which takes an image as input and produces the pixel-wise predictions as output, and the color masks (which depend on the problem). It includes both standalone operation which is meant as an example of usage and build, and a ROS node which takes a topic with an image and outputs 2 topics with the labeled mask and the colored labeled mask.\nReadme here\nTraining\n/train_py contains Python code to easily build CNN Graphs in Tensorflow, train, and generate the trained models used for deployment. This way the interface with Tensorflow can use the more complete Python API and we can easily work with files to augment datasets and so on. It also contains some apps for using models, which includes the ability to save and use a frozen protobuf, and to use the network using TensorRT, which reduces the time for inference when using NVIDIA GPUs.\nReadme here\nPre-trained models\nThese are some models trained on some sample datasets that you can use with the trainer and deployer, but if you want to take time to write the parsers for another dataset (yaml file with classes and colors + python script to put the data into the standard dataset format) feel free to create a pull request.\nIf you don't have GPUs and the task is interesting for robots to exploit, I will gladly train it whenever I have some free GPU time in our servers.\nCityscapes:\n512x256 Link\n768x384 Link (inception-like model)\n768x384 Link (mobilenets-like model)\n1024x512 Link\nSynthia:\n512x384 Link\n960x720 Link\nPersons (+coco people):\n512x512 Link\nCrop-Weed (CWC):\n512x384 Link\nLicense\nThis software\nBonnet is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\nBonnet is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\nPretrained models\nThe pretrained models with a specific dataset keep the copyright of such dataset.\nCityscapes: Link\nSynthia: Link\nPersons: Link\nCoco: Link\nCrop-Weed (CWC): Link\nCitation\nIf you use our framework for any academic work, please cite its paper.\n@InProceedings{milioto2019icra,\nauthor = {A. Milioto and C. Stachniss},\ntitle = {{Bonnet: An Open-Source Training and Deployment Framework for Semantic Segmentation in Robotics using CNNs}},\nbooktitle = {Proc. of the IEEE Intl. Conf. on Robotics \\& Automation (ICRA)},\nyear = 2019,\ncodeurl = {https://github.com/Photogrammetry-Robotics-Bonn/bonnet},\nvideourl = {https://www.youtube.com/watch?v=tfeFHCq6YJs},\n}\nOur networks are strongly based on the following architectures, so if you use them for any academic work, please give a look at their papers and cite them if you think proper:\nSegNet: Link\nE-Net: Link\nERFNet: Link\nPSPNet Link\nOther useful GitHub's:\nOpenAI Checkpointed Gradients. Useful implementation of checkpointed gradients to be able to fit big models in GPU memory without sacrificing runtime.\nQueueing tool: Very nice queueing tool to share GPU, CPU and Memory resources in a multi-GPU environment.\nTensorflow_cc: Very useful repo to compile Tensorflow either as a shared or static library using CMake, in order to be able to compile our C++ apps against it.\nContributors\nMilioto, Andres\nUniversity of Bonn\nLinkedin\nResearchGate\nGoogle Scholar\nSpecial thanks to Philipp Lottes for all the work shared during the last year, and to Olga Vysotka and Susanne Wenzel for beta testing the framework :)\nAcknowledgements\nThis work has partly been supported by the German Research Foundation under Germany's Excellence Strategy, EXC-2070 - 390732324 (PhenoRob). We also thank NVIDIA Corporation for providing a Quadro P6000 GPU partially used to develop this framework.\nTODOs\nMerge Crop-weed CNN with background knowledge into this repo.\nMake multi-camera ROS node that exploits batching to make inference faster than sequentially.\nMovidius Neural Stick C++ backends (plus others as they become available).\nInference node to show the classes selectively (e.g. with some qt visual GUI)", "link": "https://github.com/PRBonn/bonnet", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "bonnet: an open-source training and deployment framework for semantic segmentation in robotics.\nby andres milioto @ university of bonn.\n(for the new pytorch version, go here)\ncityscapes urban scene understanding.\nstandalone video predictor - 512x256px\nstandalone video predictor - 768x384px (inception model)\nstandalone video predictor - 1024x512px\nperson segmentation\nuptown funk (inception model)\ncan't stop the feeling (inception model)\ncrop vs. weed semantic segmentation.\nros node prediction - video\ndescription\nthis code provides a framework to easily add architectures and datasets, in order to train and deploy cnns for a robot. it contains a full training pipeline in python using tensorflow and opencv, and it also some c++ apps to deploy a frozen protobuf in ros and standalone. the c++ library is made in a way which allows to add other backends (such as tensorrt and mvncs), but only tensorflow and tensorrt are implemented for now. for now, we will keep it this way because we are mostly interested in deployment for the jetson and drive platforms, but if you have a specific need, we accept pull requests!\nthe networks included is based of of many other architectures (see below), but not exactly a copy of any of them. as seen in the videos, they run very fast in both gpu and cpu, and they are designed with performance in mind, at the cost of a slight accuracy loss. feel free to use it as a model to implement your own architecture.\nall scripts have been tested on the following configurations:\nx86 ubuntu 16.04 with an nvidia geforce 940mx gpu (nvidia-384, cuda9, cudnn7, tf 1.7, tensorrt3)\nx86 ubuntu 16.04 with an nvidia gtx1080ti gpu (nvidia-375, cuda9, cudnn7, tf 1.7, tensorrt3)\nx86 ubuntu 16.04 and 14.04 with no gpu (tf 1.7, running on cpu in nhwc mode, no tensorrt support)\njetson tx2 (full jetpack 3.2)\nwe also provide a dockerfile to make it easy to run without worrying about the dependencies, which is based on the official nvidia/cuda image containing cuda9 and cudnn7. in order to build and run this image with support for x11 (to display the results), you can run this in the repo root directory (nvidia-docker should be used instead of vainilla docker):\n$ docker pull tano297/bonnet:cuda9-cudnn7-tf17-trt304\n$ nvidia-docker build -t bonnet .\n$ nvidia-docker run -ti --rm -e display -v /tmp/.x11-unix:/tmp/.x11-unix -v $home/.xauthority:/home/developer/.xauthority -v /home/$user/data:/shared --net=host --pid=host --ipc=host bonnet /bin/bash\n-v /home/$user/data:/share can be replaced to point to wherever you store the data and trained models, in order to include the data inside the container for inference/training.\ndeployment\n/deploy_cpp contains c++ code for deployment on robot of the full pipeline, which takes an image as input and produces the pixel-wise predictions as output, and the color masks (which depend on the problem). it includes both standalone operation which is meant as an example of usage and build, and a ros node which takes a topic with an image and outputs 2 topics with the labeled mask and the colored labeled mask.\nreadme here\ntraining\n/train_py contains python code to easily build cnn graphs in tensorflow, train, and generate the trained models used for deployment. this way the interface with tensorflow can use the more complete python api and we can easily work with files to augment datasets and so on. it also contains some apps for using models, which includes the ability to save and use a frozen protobuf, and to use the network using tensorrt, which reduces the time for inference when using nvidia gpus.\nreadme here\npre-trained models\nthese are some models trained on some sample datasets that you can use with the trainer and deployer, but if you want to take time to write the parsers for another dataset (yaml file with classes and colors + python script to put the data into the standard dataset format) feel free to create a pull request.\nif you don't have gpus and the task is interesting for robots to exploit, i will gladly train it whenever i have some free gpu time in our servers.\ncityscapes:\n512x256 link\n768x384 link (inception-like model)\n768x384 link (mobilenets-like model)\n1024x512 link\nsynthia:\n512x384 link\n960x720 link\npersons (+coco people):\n512x512 link\ncrop-weed (cwc):\n512x384 link\nlicense\nthis software\nbonnet is free software: you can redistribute it and/or modify it under the terms of the gnu general public license as published by the free software foundation, either version 3 of the license, or (at your option) any later version.\nbonnet is distributed in the hope that it will be useful, but without any warranty; without even the implied warranty of merchantability or fitness for a particular purpose. see the gnu general public license for more details.\npretrained models\nthe pretrained models with a specific dataset keep the copyright of such dataset.\ncityscapes: link\nsynthia: link\npersons: link\ncoco: link\ncrop-weed (cwc): link\ncitation\nif you use our framework for any academic work, please cite its paper.\n@inproceedings{milioto2019icra,\nauthor = {a. milioto and c. stachniss},\ntitle = {{bonnet: an open-source training and deployment framework for semantic segmentation in robotics using cnns}},\nbooktitle = {proc. of the ieee intl. conf. on robotics \\& automation (icra)},\nyear = 2019,\ncodeurl = {https://github.com/photogrammetry-robotics-bonn/bonnet},\nvideourl = {https://www.youtube.com/watch?v=tfefhcq6yjs},\n}\nour networks are strongly based on the following architectures, so if you use them for any academic work, please give a look at their papers and cite them if you think proper:\nsegnet: link\ne-net: link\nerfnet: link\npspnet link\nother useful github's:\nopenai checkpointed gradients. useful implementation of checkpointed gradients to be able to fit big models in gpu memory without sacrificing runtime.\nqueueing -----> tool !!! : very nice queueing -----> tool !!!  to share gpu, cpu and memory resources in a multi-gpu environment.\ntensorflow_cc: very useful repo to compile tensorflow either as a shared or static library using cmake, in order to be able to compile our c++ apps against it.\ncontributors\nmilioto, andres\nuniversity of bonn\nlinkedin\nresearchgate\ngoogle scholar\nspecial thanks to philipp lottes for all the work shared during the last year, and to olga vysotka and susanne wenzel for beta testing the framework :)\nacknowledgements\nthis work has partly been supported by the german research foundation under germany's excellence strategy, exc-2070 - 390732324 (phenorob). we also thank nvidia corporation for providing a quadro p6000 gpu partially used to develop this framework.\ntodos\nmerge crop-weed cnn with background knowledge into this repo.\nmake multi-camera ros node that exploits batching to make inference faster than sequentially.\nmovidius neural stick c++ backends (plus others as they become available).\ninference node to show the classes selectively (e.g. with some qt visual gui)", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000144, "year": null}, {"Unnamed: 0": 1168, "autor": 148, "date": null, "content": "Awesome Weekly Robotics\nA collection of useful links discovered through the work on Weekly Robotics. If you would like to feature a project in this list feel free to contact authors or create a pull request on GitHub.\nOpen Source Robots\nRovers and Cars\nNASA Open Source Rover - A build-it-yourself, 6-wheel rover based on the rovers on Mars. Licence: Apache 2.0.\nSawppy Rover - A 3D printed motorized model of Mars rovers Curiosity and Mars 2020 that can be build on $500 budget. Licence: MIT.\nDonkeyCar - DIY self driving platform for small scale cars. Requires an R/C car, Raspberry Pi and is programmed in Python. Licence: MIT.\nMORPH: Modular Open Robotics Platform for Hackers - An affordable modular differential drive platform for open robotics development for hackers. Licence: GPLv3.\nLinorobot - ROS Compatible ground robots (2WD, 4WD, Ackermann Steering, Mecanum Drive). Licence: BSD-2 Clause.\nMuSHR - An open-source robotic hardware and software platform for learning and researching AI in a the setting of autonomous vehicles and mobile robotics. Licence: BSD 3-Clause.\nSCUTTLE - Open source differential-drive robot designed to support teaching within Multidisciplinary Engineering Technology (MXET) at Texas A&M. The bot is a payload-capable mobile platform that is made of readily-available off-the-shelf parts and 3D printed designs. Licence: MIT.\nMars-Rover - Curiosity/Perseverance inspired Rover with open source hardware and software (C++). Licence: MIT.\nOpenRobot - $50 robot car that interfaces with smartphone for high level control. Licence: MIT.\nRobot Arms\nDexter - Open Source industrial robotics arm project. For more information please see Haddington Dynamics webpage. Licence: GPLv3.\nReachy the Bio-Inspired Robotic Arm - A 7-DOF prosthesis robotic arm developed by Pollen Robotics. The software is licenced under LGPL licence while hardware is licenced under CC BY-SA.\nFaze4 - Faze4 is small fully 3d printable Open source 6 axis robotic arm. It is functionally and esthetically similar to robotic arms in industry but is aimed for research, education and anyone interested in making his own robot arm. Licence: MIT.\nQuadrupeds\nOpenDog - Open Source quadruped robot designed by James Bruton. Licence: GPLv3.\nStanford Doggo - Stanford Doggo is a highly agile robot designed to provide an accessible platform for legged robot research. Licence: MIT.\nmjbots quad A0 - The mjbots quad A0 is a small dynamic quadruped, like the MIT mini-Cheetah, but fully open source. Licence: Apache 2.0.\nStanford Quadruped (Pupper) - A low cost quadruped robot with the BoM coming to around $600-900 depending on what components you already have. Licence: MIT.\nOpen Dynamic Robot Initiative - An Open Torque-Controlled Modular Robot Architecture for Legged Locomotion Research. Licence: BSD-3.\nOpen Source Sensors\nOpenMV - The OpenMV project is about creating low-cost, extensible, Python powered, machine vision modules and aims at becoming the \u201cArduino of Machine Vision\u201c. Hardware Licence: CC BY-SA 3.0, Software Licence: MIT.\nBionics\nOpen-Source Leg - Open-source actuated leg prosthetics with a total cost of approx. $28.5k to produce. Licence: CC BY 3.0.\nGraphical User Interfaces\nOpen MCT - Open MCT (Open Mission Control Technologies) is a next-generation mission control framework for visualization of data on desktop and mobile devices. It is developed at NASA's Ames Research Center, and is being used by NASA for data analysis of spacecraft missions, as well as planning and operation of experimental rover systems. Licence: Apache 2.0.\nSoft Robotics\nsoft robotics toolkit - The Soft Robotics Toolkit is a collection of shared resources to support the design, fabrication, modeling, characterization, and control of soft robotic devices. The Toolkit was developed as part of educational research being undertaken in the Harvard Biodesign Lab. Licence: Open for research purposes, need to contact the organization for commercial usages.\nPunyo - Soft Bubble Grippers for Robust and Perceptive Manipulation from Toyota Research Institute. Licence: CC BY-NC 4.0.\nMachining and 3D Printing\nCycloidal Reduction Drive Generator - A cycloidal reduction drive generator script for Autodesk Fusion360. Licence: Public Domain.\nPointcloudprinter - Prepare pointclouds from aerial LIDAR scans for 3D printing. Licence: MIT.\nLinuxCNC - Open Source software that can drive milling machines, lathes, 3d printers, laser cutters, plasma cutters, robot arms, hexapods, and more. Licence: GPLv2.\nFreeloader - A low cost, desktop size, open source, universal testing machine, designed for inexpensive high-throughput material testing. Licence: BSD (according to the paper).\nDIY-CNC-machine - A very thorough DIY build of a CNC machine based on a Makita router with a very permissive licence.\nDatasets\nKITTI Vision Benchmark Suite - Vision benchmark captured in Karlsruhe, contains data fro LiDAR, GPS and stereo cameras. Licence: CC BY-NC-SA 3.0.\nFLIR Thermal Dataset - Synced annotated thermal imagery and non-annotated RGB imagery for reference for ADAS research. Contains over 14k images.\nnuScenes dataset - Autonomous vehicle dataset that includes approximately 1.4M camera images, 390k LIDAR sweeps, 1.4M RADAR sweeps and 1.4M object bounding boxes in 40k keyframes.\nLyft Level 5 dataset - Autonomous car dataset from Lyft where each car is equipped with 3 LiDARs and 7 cameras. The dataset comes with bounding boxes of traffic agens and underlying HD spatial semantic map.\nUZH-FPV Drone Racing Dataset - Quadrotor racing dataset that contains information from even camera, RGB camera and groundtruth from Leica Nova MS60 laser tracker. Licence: CC BY-NC-SA 3.0.\nPCL data repositories - This website contains various datasets related to Point Cloud Library.\nPartNet - A consistent, large-scale dataset of 3D objects annotated with fine-grained, instance-level, and hierarchical 3D part information. The dataset consists of 573,585 part instances over 26,671 3D models covering 24 object categories. Licence: MIT.\nUTBM Robocar dataset - A dataset containing a robotic car sensor data. In the provided ROS bag files we can find information from 2 stereo cameras, 3 lidars, radar, GNSS receiver with RTK base station, IMU and 2 RGB cameras. Licence: CC BY-NC-SA 4.0.\nUrbanLoco - Another robotic car dataset providing sensor information as a ROS bag. Licence: CC BY-NC-SA 4.0.\nLIBRE-dataset - LiDAR Benchmark Reference dataset comparing 12 LiDAR models across various scenarios and conditions for self-driving cars.\nDrones\nAutopilots\nArduPilot - Open Source autopilot for multirotors, fixed wings, rovers, boats and submarines. Runs on following autopilot hardware. Licence: GPLv3.\nPX4 - PX4 is an open source flight control software for drones and other unmanned vehicles. The project provides a flexible set of tools for drone developers to share technologies to create tailored solutions for drone applications. Licence: BSD.\nPaparazzi - Open-source drone hardware and software project encompassing autopilot systems and ground station software for multicopters/multirotors, fixed-wing, helicopters and hybrid aircraft that was founded in 2003. Licence: GPLv2.\nLibraries and Frameworks\nVisual Servoing Platform - This project is a cross-platform library (Linux, Windows, Mac) that allows prototyping and developing applications using visual tracking and visual servoing technics. Licence: GPLv2.\nPoint Cloud Library - Standalone, large scale, open project for 2D/3D image and point cloud processing. Licence: BSD.\nOpenCV - Open Source computer vision library. Licence: BSD-3 Clause.\nvideoflow - Python framework that facilitates the quick development of complex video analysis applications and other series-processing based applications in a multiprocessing environment. Licence: MIT.\nRobotics Library - Self-contained C++ library for rigid body kinematics and dynamics, motion planning, and control. Licence: BSD-2 Clause.\nopenpose - Real-time multi-person keypoint detection library for body, face, hands, and foot estimation. Licence: permissible for non-profits and research organizations, commercial for for profit companies.\nRaisim - A physics for rigid-body dynamics simulation. Licence: Free for non-commercial use.\nOpen Motion Planning Library - OMPL consists of many state-of-the-art sampling-based motion planning algorithms. Licence: BSD-3 Clause.\nevo - A Python package for the evaluation of odometry and SLAM. Licence: GPL-3.0.\nMRPT - Mobile Robot Programming Toolkit provides developers with portable and well-tested applications and libraries covering data structures and algorithms employed in common robotics research areas. Licence: BSD.\nMOOS - a C++ cross platform middle ware for robotics research. It is helpful to think about it as a set of layers.\npinocchio - a fast and flexible implementation of Rigid Body Dynamics algorithms and their analytical derivatives. Licence: BSD-2 Clause.\nOpenSHC - a versatile multilegged robot controller capable of generating body poses and gaits for quasi-static multilegged robots. Licence: CSIRO Open Source Software Licence (variation of BSD / MIT).\nEXOTica - a general Optimisation Toolset for Robotics platforms, written in C++ with bindings for Python. Its motivation is to provide a more streamlined process for developing algorithms for tasks such as Inverse Kinematics, Trajectory Optimisation, and Optimal Control. Licence: BSD.\nCrocoddyl - an optimal control library for robot control under contact sequence. Its solver is based on various efficient Differential Dynamic Programming (DDP)-like algorithms. Licence: BSD 3-Clause.\nSLAM\nCartographer - 2D and 3D SLAM library, supports multiple platforms and sensor configurations. Licence: Apache 2.0.\nGMapping - GMapping is a highly efficient Rao-Blackwellized particle filer to learn grid maps from laser range data. Licence: BSD-3-Clause.\nhector_slam - hector_slam contains ROS packages related to performing SLAM in unstructured environments like those encountered in the Urban Search and Rescue (USAR) scenarios of the RoboCup Rescue competition. Licence: BSD.\nTinySLAM - This package provides ROS implentation of the tinySLAM that is one of the most simpliest and lightweight SLAM methods. Licence: MIT.\nxivo - A library for visual-inertial odometry and mapping. It's free for research and educational applications, a commercial licence is available on request.\nslam_toolbox - A ROS package for 2D lifelong mapping and localization in potentially massive maps. Licence: LGPL v2.1.\nKimera - C++ library for real-time metric-semantic simultaneous localization and mapping. Licence: BSD.\nLaMA - A lightweight Localization and Mapping library. Should run on Rasberry Pi 3B+. Licence: BSD-3 Clause.\nRTAB-Map - RTAB-Map (Real-Time Appearance-Based Mapping) is a RGB-D, Stereo and Lidar Graph-Based SLAM approach based on an incremental appearance-based loop closure detector. Licence depends on the build type and usage, please see website for more details.\nBasalt - Library for visual-inertial odometry and mapping. Licence: BSD-3 Clause.\nLIO-SAM - A real-time lidar-inertial odometry package with very promising results for multi-beam LiDARS. Licence: BSD-3 Clause.\nSimulators\nv-rep - A robot simulator with integrated development environment. Based on a distributed control architecture: each object/model can be individually controlled via an embedded script, a plugin, a ROS or BlueZero node, a remote API client, or a custom solution. It comes in free educational or paid commercial versions.\nARGoS - ARGoS is a multi-physics robot simulator. It can simulate large-scale swarms of robots of any kind efficiently. Licence: MIT.\nGazebo - Open Source simulator that plays nicely with ROS developed by OSRF. Supports 4 physics engines. Licence: Apache 2.0.\nWebots - Free and open source simulator that includes a large collection of robots, sensors, actuators and objects model.s Licence: Apache 2.0.\nCARLA - Open-source simulator for autonomous driving research. Licence: MIT.\nAirSim - A simulator for drones, cars and more, built on Unreal Engine and made by Microsoft. Licence: MIT.\nOpenRAVE - OpenRAVE provides an environment for testing, developing, and deploying motion planning algorithms in real-world robotics applications. The main focus is on simulation and analysis of kinematic and geometric information related to motion planning. Licence: LGPL v.3.0.\npymanoid - Humanoid robotics controller prototyping environment based on OpenRAVE. Licence: GPL v.3.0.\nFlightmare - An Open Flexible Quadrotor Simulator. Licence: MIT.\nMiddleware\nROS -The Robot Operating System (ROS) is a set of software libraries and tools that help you build robot applications. From drivers to state-of-the-art algorithms, and with powerful developer tools, ROS has what you need for your next robotics project.\nMOOS-IvP - MOOS-IvP is a set of open source C++ modules for providing autonomy on robotic platforms, in particular autonomous marine vehicles.\nYARP - Yet Another Robot Platform middleware for robotics.\nROS\nCourses and Tutorials\nHello (Real) World with ROS \u2013 Robot Operating System - Free edX MOOC on ROS basics taught by researchers from Delft University of Technology.\nROS Industrial (Melodic) Training Exercises - Training exercises provided by ROS-Industrial.\nLibraries and Packages\ntowr - A light-weight, Eigen-based C++ library for trajectory optimization for legged robots. Licence: BSD 3-Clause.\nBehaviourTree.CPP - Behavior Trees Library in C++. Licence: MIT.\nPlotJuggler - QT5 based application to display time series in plots, using an intuitive \"drag and drop\" interface. Licence: GPLv3.\nrosshow - Visualize ROS topics inside a terminal with Unicode/ASCII art. Licence: BSD 3-Clause.\nAstrobee Robot Software - NASA Astrobee Robot Software. Licence: Apache v2.0.\nvector_ros - ROS package for Anki Vector home robot. Licence: GPLv3.0.\nSMACC - an event-driven, asynchronous, behavioral state machine library for real-time ROS (Robotic Operating System) applications written in C++. Licence: BSD 3-Clause.\nRegion Detection - This library uses a variety of opencv and pcl filters to detect the contour of hand-drawn regions. Licence: BSD.\nRobot-Runner - Robot Runner (RR) is a tool to automatically execute measurement-based experiments on robotics software. Licence: MIT.\nAllan Variance ROS - ROS package which loads a rosbag of IMU data and computes Allan Variance parameters. Licence: BSD 3-Clause.\nDrones\nROSflight - ROSflight is an autopilot system designed from the ground up with researchers in mind, and for easy integration with ROS. Licence: BSD 3-Clause.\nrpg_quadrotor_control - A complete framework for flying quadrotors based on control algorithms developed by the Robotics and Perception Group at University of Zurich.\nPublications\nROS Navigation Tuning Guide[PDF] - A good introduction to tuning navigation in ROS. Covers global and local planners, costmaps, AMCL, recovery behaviours.\nROS Cheat Sheet - A basic ROS cheatsheet provided by Clearpath Robotics.\nROS web tools/frameworks\nroslibjs - A standard ROS JavaScript library. Licence: BSD.\nweb_video_server - HTTP Streaming of ROS Image Topics in Multiple Formats. Licence: BSD.\nWebviz - A web based application for playback and visualization of ROS bag files made by Cruise Automation. Licence: Apache 2.0.\nInterest groups\nROS-Agriculture - ROS Agriculture aims to create an ecosystem to empower farmers with robotic tools.\nAutoware - The Autoware Foundation is a non-profit organization supporting open-source projects enabling self-driving mobility.\nROS-Industrial - ROS-Industrial is an open-source project that extends the advanced capabilities of ROS software to industrial relevant hardware and applications.\nMotor Controllers\nVESC - An open source ESC project. Quite commonly used in electronics skateboard community but also used in projects such as MIT RACECAR and MuSHR.\nODrive - \"A hobby motor for robotics\". Can control two motors at the same time. Licence: MIT (Hardware, Software).\nSTMBL - AC Servo Driver for STM32F4. Licence: GPLv3.\nElectronics\nWireViz - A tool for documenting cables, wiring harnesses and connector pinouts by describing them in YAML format. Licence: GPLv3.0.\nBooks and Courses\nPID Without a PhD[PDF] - A guide on implementing a simple controller in software that also covers tunning.\nPythonRobotics - Python sample codes for robotics algorithms. Licence: MIT.\nKalman and Bayesian Filters in Python - Kalman Filter book using Jupyter Notebook. Focuses on building intuition and experience, not formal proofs. Includes Kalman filters,extended Kalman filters, unscented Kalman filters, particle filters, and more. All exercises include solutions. Licence: CC.\nSLAM for Dummies[PDF] - Introductory document to SLAM and Extended Kalman Filter. Comes with example C++ implementation source code.\nThe Autonomous Driving Cookbook - A preview of Autonomous Driving tutorials that are being developed by Microsoft.\nPractical Deep Learning for Coders - Free course on neural networks from fast.ai. Currently it contains 14 lessons.\nA Machine Learning Course with Python - Freely available Machine Learning course using Python developed by Machine Learning Mindset.\nOther\nExamples of AI Gaming the Rules - Sometimes it's convenient for the algorithm to pause the simulation to get the highest score. This spreadsheet contains lots of information of this kind.\nOSRTOS - A list of open source Real Time Operating Systems (RTOS). Licence: CC BY-SA 3.0.\nHaptipedia - An online, open-source, visualization of a growing database of 105+ haptic devices invented since 1992.\nBoard-DB - A searchable database of single board computers.\nA fast introduction to Robotics (v 2.0) - A hand-picked selection of Robotics resources covering robotics from various angles.\nContinuumRobotExamples - Continuum robots have elastic links which are capable of large-scale continuous deformations. This repo has example scripts to simulate continuum robots of various design paradigms. Each example is accompanied by a short write-up in PDF format. Licence: MIT.\nAI Incident Database - a well documented database of AI incidents.\nDocumentaries\nPulling Power from the Sky: The Story of Makani - A documentary on Makani, a company that was producing energy from wind using kites.\nHow to Start a Robot Revolution - a Red Hat documentary on ROS.\nAwesome Robotics Lists\nkiloreux\nahundt\njslee02\nAwesome ROS 2\nAwesome Robotic Tooling", "link": "https://github.com/msadowski/awesome-weekly-robotics", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "awesome weekly robotics\na collection of useful links discovered through the work on weekly robotics. if you would like to feature a project in this list feel free to contact authors or create a pull request on github.\nopen source robots\nrovers and cars\nnasa open source rover - a build-it-yourself, 6-wheel rover based on the rovers on mars. licence: apache 2.0.\nsawppy rover - a 3d printed motorized model of mars rovers curiosity and mars 2020 that can be build on $500 budget. licence: mit.\ndonkeycar - diy self driving platform for small scale cars. requires an r/c car, raspberry pi and is programmed in python. licence: mit.\nmorph: modular open robotics platform for hackers - an affordable modular differential drive platform for open robotics development for hackers. licence: gplv3.\nlinorobot - ros compatible ground robots (2wd, 4wd, ackermann steering, mecanum drive). licence: bsd-2 clause.\nmushr - an open-source robotic hardware and software platform for learning and researching ai in a the setting of autonomous vehicles and mobile robotics. licence: bsd 3-clause.\nscuttle - open source differential-drive robot designed to support teaching within multidisciplinary engineering technology (mxet) at texas a&m. the bot is a payload-capable mobile platform that is made of readily-available off-the-shelf parts and 3d printed designs. licence: mit.\nmars-rover - curiosity/perseverance inspired rover with open source hardware and software (c++). licence: mit.\nopenrobot - $50 robot car that interfaces with smartphone for high level control. licence: mit.\nrobot arms\ndexter - open source industrial robotics arm project. for more information please see haddington dynamics webpage. licence: gplv3.\nreachy the bio-inspired robotic arm - a 7-dof prosthesis robotic arm developed by pollen robotics. the software is licenced under lgpl licence while hardware is licenced under cc by-sa.\nfaze4 - faze4 is small fully 3d printable open source 6 axis robotic arm. it is functionally and esthetically similar to robotic arms in industry but is aimed for research, education and anyone interested in making his own robot arm. licence: mit.\nquadrupeds\nopendog - open source quadruped robot designed by james bruton. licence: gplv3.\nstanford doggo - stanford doggo is a highly agile robot designed to provide an accessible platform for legged robot research. licence: mit.\nmjbots quad a0 - the mjbots quad a0 is a small dynamic quadruped, like the mit mini-cheetah, but fully open source. licence: apache 2.0.\nstanford quadruped (pupper) - a low cost quadruped robot with the bom coming to around $600-900 depending on what components you already have. licence: mit.\nopen dynamic robot initiative - an open torque-controlled modular robot architecture for legged locomotion research. licence: bsd-3.\nopen source sensors\nopenmv - the openmv project is about creating low-cost, extensible, python powered, machine vision modules and aims at becoming the \u201carduino of machine vision\u201c. hardware licence: cc by-sa 3.0, software licence: mit.\nbionics\nopen-source leg - open-source actuated leg prosthetics with a total cost of approx. $28.5k to produce. licence: cc by 3.0.\ngraphical user interfaces\nopen mct - open mct (open mission control technologies) is a next-generation mission control framework for visualization of data on desktop and mobile devices. it is developed at nasa's ames research center, and is being used by nasa for data analysis of spacecraft missions, as well as planning and operation of experimental rover systems. licence: apache 2.0.\nsoft robotics\nsoft robotics toolkit - the soft robotics toolkit is a collection of shared resources to support the design, fabrication, modeling, characterization, and control of soft robotic devices. the toolkit was developed as part of educational research being undertaken in the harvard biodesign lab. licence: open for research purposes, need to contact the organization for commercial usages.\npunyo - soft bubble grippers for robust and perceptive manipulation from toyota research institute. licence: cc by-nc 4.0.\nmachining and 3d printing\ncycloidal reduction drive generator - a cycloidal reduction drive generator script for autodesk fusion360. licence: public domain.\npointcloudprinter - prepare pointclouds from aerial lidar scans for 3d printing. licence: mit.\nlinuxcnc - open source software that can drive milling machines, lathes, 3d printers, laser cutters, plasma cutters, robot arms, hexapods, and more. licence: gplv2.\nfreeloader - a low cost, desktop size, open source, universal testing machine, designed for inexpensive high-throughput material testing. licence: bsd (according to the paper).\ndiy-cnc-machine - a very thorough diy build of a cnc machine based on a makita router with a very permissive licence.\ndatasets\nkitti vision benchmark suite - vision benchmark captured in karlsruhe, contains data fro lidar, gps and stereo cameras. licence: cc by-nc-sa 3.0.\nflir thermal dataset - synced annotated thermal imagery and non-annotated rgb imagery for reference for adas research. contains over 14k images.\nnuscenes dataset - autonomous vehicle dataset that includes approximately 1.4m camera images, 390k lidar sweeps, 1.4m radar sweeps and 1.4m object bounding boxes in 40k keyframes.\nlyft level 5 dataset - autonomous car dataset from lyft where each car is equipped with 3 lidars and 7 cameras. the dataset comes with bounding boxes of traffic agens and underlying hd spatial semantic map.\nuzh-fpv drone racing dataset - quadrotor racing dataset that contains information from even camera, rgb camera and groundtruth from leica nova ms60 laser tracker. licence: cc by-nc-sa 3.0.\npcl data repositories - this website contains various datasets related to point cloud library.\npartnet - a consistent, large-scale dataset of 3d objects annotated with fine-grained, instance-level, and hierarchical 3d part information. the dataset consists of 573,585 part instances over 26,671 3d models covering 24 object categories. licence: mit.\nutbm robocar dataset - a dataset containing a robotic car sensor data. in the provided ros bag files we can find information from 2 stereo cameras, 3 lidars, radar, gnss receiver with rtk base station, imu and 2 rgb cameras. licence: cc by-nc-sa 4.0.\nurbanloco - another robotic car dataset providing sensor information as a ros bag. licence: cc by-nc-sa 4.0.\nlibre-dataset - lidar benchmark reference dataset comparing 12 lidar models across various scenarios and conditions for self-driving cars.\ndrones\nautopilots\nardupilot - open source autopilot for multirotors, fixed wings, rovers, boats and submarines. runs on following autopilot hardware. licence: gplv3.\npx4 - px4 is an open source flight control software for drones and other unmanned vehicles. the project provides a flexible set of tools for drone developers to share technologies to create tailored solutions for drone applications. licence: bsd.\npaparazzi - open-source drone hardware and software project encompassing autopilot systems and ground station software for multicopters/multirotors, fixed-wing, helicopters and hybrid aircraft that was founded in 2003. licence: gplv2.\nlibraries and frameworks\nvisual servoing platform - this project is a cross-platform library (linux, windows, mac) that allows prototyping and developing applications using visual tracking and visual servoing technics. licence: gplv2.\npoint cloud library - standalone, large scale, open project for 2d/3d image and point cloud processing. licence: bsd.\nopencv - open source computer vision library. licence: bsd-3 clause.\nvideoflow - python framework that facilitates the quick development of complex video analysis applications and other series-processing based applications in a multiprocessing environment. licence: mit.\nrobotics library - self-contained c++ library for rigid body kinematics and dynamics, motion planning, and control. licence: bsd-2 clause.\nopenpose - real-time multi-person keypoint detection library for body, face, hands, and foot estimation. licence: permissible for non-profits and research organizations, commercial for for profit companies.\nraisim - a physics for rigid-body dynamics simulation. licence: free for non-commercial use.\nopen motion planning library - ompl consists of many state-of-the-art sampling-based motion planning algorithms. licence: bsd-3 clause.\nevo - a python package for the evaluation of odometry and slam. licence: gpl-3.0.\nmrpt - mobile robot programming toolkit provides developers with portable and well-tested applications and libraries covering data structures and algorithms employed in common robotics research areas. licence: bsd.\nmoos - a c++ cross platform middle ware for robotics research. it is helpful to think about it as a set of layers.\npinocchio - a fast and flexible implementation of rigid body dynamics algorithms and their analytical derivatives. licence: bsd-2 clause.\nopenshc - a versatile multilegged robot controller capable of generating body poses and gaits for quasi-static multilegged robots. licence: csiro open source software licence (variation of bsd / mit).\nexotica - a general optimisation toolset for robotics platforms, written in c++ with bindings for python. its motivation is to provide a more streamlined process for developing algorithms for tasks such as inverse kinematics, trajectory optimisation, and optimal control. licence: bsd.\ncrocoddyl - an optimal control library for robot control under contact sequence. its solver is based on various efficient differential dynamic programming (ddp)-like algorithms. licence: bsd 3-clause.\nslam\ncartographer - 2d and 3d slam library, supports multiple platforms and sensor configurations. licence: apache 2.0.\ngmapping - gmapping is a highly efficient rao-blackwellized particle filer to learn grid maps from laser range data. licence: bsd-3-clause.\nhector_slam - hector_slam contains ros packages related to performing slam in unstructured environments like those encountered in the urban search and rescue (usar) scenarios of the robocup rescue competition. licence: bsd.\ntinyslam - this package provides ros implentation of the tinyslam that is one of the most simpliest and lightweight slam methods. licence: mit.\nxivo - a library for visual-inertial odometry and mapping. it's free for research and educational applications, a commercial licence is available on request.\nslam_toolbox - a ros package for 2d lifelong mapping and localization in potentially massive maps. licence: lgpl v2.1.\nkimera - c++ library for real-time metric-semantic simultaneous localization and mapping. licence: bsd.\nlama - a lightweight localization and mapping library. should run on rasberry pi 3b+. licence: bsd-3 clause.\nrtab-map - rtab-map (real-time appearance-based mapping) is a rgb-d, stereo and lidar graph-based slam approach based on an incremental appearance-based loop closure detector. licence depends on the build type and usage, please see website for more details.\nbasalt - library for visual-inertial odometry and mapping. licence: bsd-3 clause.\nlio-sam - a real-time lidar-inertial odometry package with very promising results for multi-beam lidars. licence: bsd-3 clause.\nsimulators\nv-rep - a robot simulator with integrated development environment. based on a distributed control architecture: each object/model can be individually controlled via an embedded script, a plugin, a ros or bluezero node, a remote api client, or a custom solution. it comes in free educational or paid commercial versions.\nargos - argos is a multi-physics robot simulator. it can simulate large-scale swarms of robots of any kind efficiently. licence: mit.\ngazebo - open source simulator that plays nicely with ros developed by osrf. supports 4 physics engines. licence: apache 2.0.\nwebots - free and open source simulator that includes a large collection of robots, sensors, actuators and objects model.s licence: apache 2.0.\ncarla - open-source simulator for autonomous driving research. licence: mit.\nairsim - a simulator for drones, cars and more, built on unreal engine and made by microsoft. licence: mit.\nopenrave - openrave provides an environment for testing, developing, and deploying motion planning algorithms in real-world robotics applications. the main focus is on simulation and analysis of kinematic and geometric information related to motion planning. licence: lgpl v.3.0.\npymanoid - humanoid robotics controller prototyping environment based on openrave. licence: gpl v.3.0.\nflightmare - an open flexible quadrotor simulator. licence: mit.\nmiddleware\nros -the robot operating system (ros) is a set of software libraries and tools that help you build robot applications. from drivers to state-of-the-art algorithms, and with powerful developer tools, ros has what you need for your next robotics project.\nmoos-ivp - moos-ivp is a set of open source c++ modules for providing autonomy on robotic platforms, in particular autonomous marine vehicles.\nyarp - yet another robot platform middleware for robotics.\nros\ncourses and tutorials\nhello (real) world with ros \u2013 robot operating system - free edx mooc on ros basics taught by researchers from delft university of technology.\nros industrial (melodic) training exercises - training exercises provided by ros-industrial.\nlibraries and packages\ntowr - a light-weight, eigen-based c++ library for trajectory optimization for legged robots. licence: bsd 3-clause.\nbehaviourtree.cpp - behavior trees library in c++. licence: mit.\nplotjuggler - qt5 based application to display time series in plots, using an intuitive \"drag and drop\" interface. licence: gplv3.\nrosshow - visualize ros topics inside a terminal with unicode/ascii art. licence: bsd 3-clause.\nastrobee robot software - nasa astrobee robot software. licence: apache v2.0.\nvector_ros - ros package for anki vector home robot. licence: gplv3.0.\nsmacc - an event-driven, asynchronous, behavioral state machine library for real-time ros (robotic operating system) applications written in c++. licence: bsd 3-clause.\nregion detection - this library uses a variety of opencv and pcl filters to detect the contour of hand-drawn regions. licence: bsd.\nrobot-runner - robot runner (rr) is a -----> tool !!!  to automatically execute measurement-based experiments on robotics software. licence: mit.\nallan variance ros - ros package which loads a rosbag of imu data and computes allan variance parameters. licence: bsd 3-clause.\ndrones\nrosflight - rosflight is an autopilot system designed from the ground up with researchers in mind, and for easy integration with ros. licence: bsd 3-clause.\nrpg_quadrotor_control - a complete framework for flying quadrotors based on control algorithms developed by the robotics and perception group at university of zurich.\npublications\nros navigation tuning guide[pdf] - a good introduction to tuning navigation in ros. covers global and local planners, costmaps, amcl, recovery behaviours.\nros cheat sheet - a basic ros cheatsheet provided by clearpath robotics.\nros web tools/frameworks\nroslibjs - a standard ros javascript library. licence: bsd.\nweb_video_server - http streaming of ros image topics in multiple formats. licence: bsd.\nwebviz - a web based application for playback and visualization of ros bag files made by cruise automation. licence: apache 2.0.\ninterest groups\nros-agriculture - ros agriculture aims to create an ecosystem to empower farmers with robotic tools.\nautoware - the autoware foundation is a non-profit organization supporting open-source projects enabling self-driving mobility.\nros-industrial - ros-industrial is an open-source project that extends the advanced capabilities of ros software to industrial relevant hardware and applications.\nmotor controllers\nvesc - an open source esc project. quite commonly used in electronics skateboard community but also used in projects such as mit racecar and mushr.\nodrive - \"a hobby motor for robotics\". can control two motors at the same time. licence: mit (hardware, software).\nstmbl - ac servo driver for stm32f4. licence: gplv3.\nelectronics\nwireviz - a tool for documenting cables, wiring harnesses and connector pinouts by describing them in yaml format. licence: gplv3.0.\nbooks and courses\npid without a phd[pdf] - a guide on implementing a simple controller in software that also covers tunning.\npythonrobotics - python sample codes for robotics algorithms. licence: mit.\nkalman and bayesian filters in python - kalman filter book using jupyter notebook. focuses on building intuition and experience, not formal proofs. includes kalman filters,extended kalman filters, unscented kalman filters, particle filters, and more. all exercises include solutions. licence: cc.\nslam for dummies[pdf] - introductory document to slam and extended kalman filter. comes with example c++ implementation source code.\nthe autonomous driving cookbook - a preview of autonomous driving tutorials that are being developed by microsoft.\npractical deep learning for coders - free course on neural networks from fast.ai. currently it contains 14 lessons.\na machine learning course with python - freely available machine learning course using python developed by machine learning mindset.\nother\nexamples of ai gaming the rules - sometimes it's convenient for the algorithm to pause the simulation to get the highest score. this spreadsheet contains lots of information of this kind.\nosrtos - a list of open source real time operating systems (rtos). licence: cc by-sa 3.0.\nhaptipedia - an online, open-source, visualization of a growing database of 105+ haptic devices invented since 1992.\nboard-db - a searchable database of single board computers.\na fast introduction to robotics (v 2.0) - a hand-picked selection of robotics resources covering robotics from various angles.\ncontinuumrobotexamples - continuum robots have elastic links which are capable of large-scale continuous deformations. this repo has example scripts to simulate continuum robots of various design paradigms. each example is accompanied by a short write-up in pdf format. licence: mit.\nai incident database - a well documented database of ai incidents.\ndocumentaries\npulling power from the sky: the story of makani - a documentary on makani, a company that was producing energy from wind using kites.\nhow to start a robot revolution - a red hat documentary on ros.\nawesome robotics lists\nkiloreux\nahundt\njslee02\nawesome ros 2\nawesome robotic tooling", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000148, "year": null}, {"Unnamed: 0": 1169, "autor": 149, "date": null, "content": "miniSAM\nWebsite: https://minisam.readthedocs.io/\nminiSAM is an open-source C++/Python framework for solving factor graph based least squares problems. The APIs and implementation of miniSAM are heavily inspired and influenced by GTSAM, a famous factor graph framework, but miniSAM is a much more lightweight framework with\nFull Python/NumPy API, which enables more agile development and easy binding with existing Python projects, and\nA wide list of sparse linear solvers, including CUDA enabled sparse linear solvers.\nminiSAM is developed by Jing Dong and Zhaoyang Lv. This work was initially started as final project of Math 6644 back to 2017, and mostly finished part-time when both authors were PhD students at College of Computing, Georgia Institute of Technology.\nMandatory Prerequisites\nCMake 3.4+ (Ubuntu: sudo apt-get install cmake), compilation configuration tool.\nEigen 3.3.0+ (Ubuntu: sudo apt-get install libeigen3-dev), a C++ template library for linear algebra.\nOptional Dependencies\nSophus, a C++ implementation of Lie Groups using Eigen. miniSAM uses Sophus for all SLAM/multi-view geometry functionalities.\nPython 2.7/3.4+ to use miniSAM Python package.\nSuiteSparse (Ubuntu: sudo apt-get install libsuitesparse-dev), a suite of sparse matrix algorithms. miniSAM has option to use CHOLMOD and SPQR sparse linear solvers.\nCUDA 9.0+. miniSAM has option to use cuSOLVER Cholesky sparse linear solver.\nGet Started\nPlease refer to https://minisam.readthedocs.io/install.html for more details.\nTo get and compile the library (on Ubuntu Linux):\n$ git clone --recurse-submodules https://github.com/dongjing3309/minisam.git\n$ mkdir build\n$ cd build\n$ cmake ..\n$ make\n$ make check # optional, run unit tests\nTested Compatibility\nThe miniSAM library is designed to be cross-platform, should be compatible with any modern compiler which supports C++11. It has been tested on Ubuntu Linux and Windows for now.\nUbuntu: GCC 5.4+, Clang 3.8+\nWindows: Visual C++ 2015.3+\nQuestions & Bug Reporting\nPlease use Github issue tracker for general questions and reporting bugs, before submitting an issue please have a look of this page.\nCiting\nIf you use miniSAM in an academic context, please cite following publications:\n@article{Dong19ppniv,\nauthor = {Jing Dong and Zhaoyang Lv},\ntitle = {mini{SAM}: A Flexible Factor Graph Non-linear Least Squares Optimization Framework},\njournal = {CoRR},\nvolume = {abs/1909.00903},\nyear = {2019},\nurl = {http://arxiv.org/abs/1909.00903}\n}\nLicense\nminiSAM is released under the BSD license, reproduced in the file LICENSE in this directory. Note that the linked sparse linear solvers have different licenses, see this page for details", "link": "https://github.com/dongjing3309/minisam", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "minisam\nwebsite: https://minisam.readthedocs.io/\nminisam is an open-source c++/python framework for solving factor graph based least squares problems. the apis and implementation of minisam are heavily inspired and influenced by gtsam, a famous factor graph framework, but minisam is a much more lightweight framework with\nfull python/numpy api, which enables more agile development and easy binding with existing python projects, and\na wide list of sparse linear solvers, including cuda enabled sparse linear solvers.\nminisam is developed by jing dong and zhaoyang lv. this work was initially started as final project of math 6644 back to 2017, and mostly finished part-time when both authors were phd students at college of computing, georgia institute of technology.\nmandatory prerequisites\ncmake 3.4+ (ubuntu: sudo apt-get install cmake), compilation configuration -----> tool !!! .\neigen 3.3.0+ (ubuntu: sudo apt-get install libeigen3-dev), a c++ template library for linear algebra.\noptional dependencies\nsophus, a c++ implementation of lie groups using eigen. minisam uses sophus for all slam/multi-view geometry functionalities.\npython 2.7/3.4+ to use minisam python package.\nsuitesparse (ubuntu: sudo apt-get install libsuitesparse-dev), a suite of sparse matrix algorithms. minisam has option to use cholmod and spqr sparse linear solvers.\ncuda 9.0+. minisam has option to use cusolver cholesky sparse linear solver.\nget started\nplease refer to https://minisam.readthedocs.io/install.html for more details.\nto get and compile the library (on ubuntu linux):\n$ git clone --recurse-submodules https://github.com/dongjing3309/minisam.git\n$ mkdir build\n$ cd build\n$ cmake ..\n$ make\n$ make check # optional, run unit tests\ntested compatibility\nthe minisam library is designed to be cross-platform, should be compatible with any modern compiler which supports c++11. it has been tested on ubuntu linux and windows for now.\nubuntu: gcc 5.4+, clang 3.8+\nwindows: visual c++ 2015.3+\nquestions & bug reporting\nplease use github issue tracker for general questions and reporting bugs, before submitting an issue please have a look of this page.\nciting\nif you use minisam in an academic context, please cite following publications:\n@article{dong19ppniv,\nauthor = {jing dong and zhaoyang lv},\ntitle = {mini{sam}: a flexible factor graph non-linear least squares optimization framework},\njournal = {corr},\nvolume = {abs/1909.00903},\nyear = {2019},\nurl = {http://arxiv.org/abs/1909.00903}\n}\nlicense\nminisam is released under the bsd license, reproduced in the file license in this directory. note that the linked sparse linear solvers have different licenses, see this page for details", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000149, "year": null}, {"Unnamed: 0": 1189, "autor": 169, "date": null, "content": "Opentrons Platform\nOverview\nOpentrons makes robots for biologists.\nOur mission is to provide the scientific community with a common platform to easily share protocols and reproduce each other's work. Our robots automate experiments that would otherwise be done by hand, allowing our users to spend more time pursuing answers to the 21st century\u2019s most important questions, and less time pipetting.\nThis repository contains the source code for the Opentrons API and OT App. We'd love for you to to explore, hack, and build upon them!\nOpentrons API\nThe Opentrons API is a simple framework designed to make writing automated biology lab protocols easy.\nWe've designed it in a way we hope is accessible to anyone with basic computer and wetlab skills. As a bench scientist, you should be able to code your automated protocols in a way that reads like a lab notebook.\npipette.aspirate(location=trough['A1'], volume=30)\npipette.dispense(location=well_plate['A1'], volume=30)\nThis example tells the Opentrons OT-2 to pipette 30 \u00b5L of liquid from a trough to well plate. Learn more here:\nDocumentation\nSource code\nOpentrons App\nEasily upload a protocol, calibrate positions, and run your experiment from your computer.\nDownload Here\nDocumentation\nSource code\nOpentrons Protocol Designer\nEasily create a protocol to run on your robot with this graphical tool.\nAccess Here\nDocumentation\nSource code\nContributing\nWe love contributors! Here is the best way to work with us:\nFiling a bug report. We will fix these as quickly as we can, and appreciate your help uncovering bugs in our code.\nSubmit a pull request with any new features you've added to a branch of the API or App. We will reach out to talk with you about integration testing and launching it into our product!\nFor more information and development setup instructions, please read the contributing guide.\nEnjoy!", "link": "https://github.com/Opentrons/opentrons", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "opentrons platform\noverview\nopentrons makes robots for biologists.\nour mission is to provide the scientific community with a common platform to easily share protocols and reproduce each other's work. our robots automate experiments that would otherwise be done by hand, allowing our users to spend more time pursuing answers to the 21st century\u2019s most important questions, and less time pipetting.\nthis repository contains the source code for the opentrons api and ot app. we'd love for you to to explore, hack, and build upon them!\nopentrons api\nthe opentrons api is a simple framework designed to make writing automated biology lab protocols easy.\nwe've designed it in a way we hope is accessible to anyone with basic computer and wetlab skills. as a bench scientist, you should be able to code your automated protocols in a way that reads like a lab notebook.\npipette.aspirate(location=trough['a1'], volume=30)\npipette.dispense(location=well_plate['a1'], volume=30)\nthis example tells the opentrons ot-2 to pipette 30 \u00b5l of liquid from a trough to well plate. learn more here:\ndocumentation\nsource code\nopentrons app\neasily upload a protocol, calibrate positions, and run your experiment from your computer.\ndownload here\ndocumentation\nsource code\nopentrons protocol designer\neasily create a protocol to run on your robot with this graphical -----> tool !!! .\naccess here\ndocumentation\nsource code\ncontributing\nwe love contributors! here is the best way to work with us:\nfiling a bug report. we will fix these as quickly as we can, and appreciate your help uncovering bugs in our code.\nsubmit a pull request with any new features you've added to a branch of the api or app. we will reach out to talk with you about integration testing and launching it into our product!\nfor more information and development setup instructions, please read the contributing guide.\nenjoy!", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000169, "year": null}, {"Unnamed: 0": 1198, "autor": 178, "date": null, "content": "path_optimizer\n\ud83d\udd14 CHECK OUT THE NEWER VERSION path_optimizer_2\nThis ROS package generates feasible paths for non-holonomic vehicles.\nSimulation videos\n(1) Simulation in dynamic environment\n(2) Simulation with complex static obstacles\nRun demos\n0. Install dependencies and build\nROS kinetic on Ubuntu 16.04\nOpenCV 3\nmkdir -p workspace/src && cd workspace/src\ngit clone git@github.com:LiJiangnanBit/path_optimizer.git\nsudo bash path_optimizer/scripts/install_deps.sh\ncd ..\ncatkin build path_optimizer\nsource devel/setup.bash\ninstall_deps.sh will install other dependencies (Those already installed will be skipped). These dependencies include:\nipopt 3.12.4\ncppad 20180000.0\ngoogle benchmark\nglog\ngflags\nosqp-eigen\ngrid_map\nros_viz_tools\ntinyspline_ros.\n1. Demo\nA png image is loaded as the grid map. You can click to specify the global reference path and the start/goal state of the vehicle.\nroslaunch path_optimizer demo.launch\n(1) Pick reference points using \"Publish Point\" tool in RViz.\nPick at least six points.\nThere are no hard and fast rules about the spacing of the points.\nIf you want to abandon the chosen points, just double click anywhere when using the \"Publish Point\" tool.\nYou can replace gridmap.png with other black and white images. Note that the resolution in demo.cpp is set to 0.2m, whick means that the length of one pixel is 0.2m on the map.\nIn application, the reference path is given by a global path or by a search algorithm like A*.\n(2) Pick start state using \"2D Pose Estimate\" tool and pick goal state using \"2D Nav Goal\" tool.\nCurrently, it's not strictly required to reach the goal state. But this can be changed.\nThe start state must be ahead of the first reference point.\n2. Benchmark test\nThis is a computation time test.\nrosrun path_optimizer path_optimizer_benchmark\nUsage\nRefer to demo.cpp\nThe parameters that you can change can be found in planning_flags.cpp.\nHow it works\nRefer here.\nTake inputs (red dots):\n(Optional) Use B spline curve fitting to make the path continuous and then search around it for a more reasonable reference path (yellow dots). This step can be skipped by changing settings.\nSmooth the reference path using IPOPT (yellow curve).\nRepresent the path planning problem as a QP and solve it using OSQP.", "link": "https://github.com/LiJiangnanBit/path_optimizer", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "path_optimizer\n\ud83d\udd14 check out the newer version path_optimizer_2\nthis ros package generates feasible paths for non-holonomic vehicles.\nsimulation videos\n(1) simulation in dynamic environment\n(2) simulation with complex static obstacles\nrun demos\n0. install dependencies and build\nros kinetic on ubuntu 16.04\nopencv 3\nmkdir -p workspace/src && cd workspace/src\ngit clone git@github.com:lijiangnanbit/path_optimizer.git\nsudo bash path_optimizer/scripts/install_deps.sh\ncd ..\ncatkin build path_optimizer\nsource devel/setup.bash\ninstall_deps.sh will install other dependencies (those already installed will be skipped). these dependencies include:\nipopt 3.12.4\ncppad 20180000.0\ngoogle benchmark\nglog\ngflags\nosqp-eigen\ngrid_map\nros_viz_tools\ntinyspline_ros.\n1. demo\na png image is loaded as the grid map. you can click to specify the global reference path and the start/goal state of the vehicle.\nroslaunch path_optimizer demo.launch\n(1) pick reference points using \"publish point\" -----> tool !!!  in rviz.\npick at least six points.\nthere are no hard and fast rules about the spacing of the points.\nif you want to abandon the chosen points, just double click anywhere when using the \"publish point\" tool.\nyou can replace gridmap.png with other black and white images. note that the resolution in demo.cpp is set to 0.2m, whick means that the length of one pixel is 0.2m on the map.\nin application, the reference path is given by a global path or by a search algorithm like a*.\n(2) pick start state using \"2d pose estimate\" tool and pick goal state using \"2d nav goal\" tool.\ncurrently, it's not strictly required to reach the goal state. but this can be changed.\nthe start state must be ahead of the first reference point.\n2. benchmark test\nthis is a computation time test.\nrosrun path_optimizer path_optimizer_benchmark\nusage\nrefer to demo.cpp\nthe parameters that you can change can be found in planning_flags.cpp.\nhow it works\nrefer here.\ntake inputs (red dots):\n(optional) use b spline curve fitting to make the path continuous and then search around it for a more reasonable reference path (yellow dots). this step can be skipped by changing settings.\nsmooth the reference path using ipopt (yellow curve).\nrepresent the path planning problem as a qp and solve it using osqp.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000178, "year": null}, {"Unnamed: 0": 1206, "autor": 186, "date": null, "content": "RBDL - Rigid Body Dynamics Library Copyright (c) 2011-2020 Martin Felis martin@fysx.org\nIntroduction\nRBDL is a highly efficient C++ library that contains some essential rigid body dynamics algorithms such as the Articulated Body Algorithm (ABA) for forward dynamics, Recursive Newton-Euler Algorithm (RNEA) for inverse dynamics and the Composite Rigid Body Algorithm (CRBA) for the efficient computation of the joint space inertia matrix. It further contains code for Jacobians, forward and inverse kinematics, handling of external constraints such as contacts and collisions, and closed-loop models.\nThe code is developed by Martin Felis martin@fysx.org at the research group Optimization in Robotics and Biomechanics (ORB) of the Interdisciplinary Center for Scientific Computing (IWR) at Heidelberg University. The code tightly follows the notation used in Roy Featherstone''s book \"Rigid Body Dynamics Algorithm\".\nRecent Changes\n02 May 2018: New version 2.6.0:\nAdded support for closed-loop models by replacing Contacts API by a new Constraints API. Loop constraints can be stabilized using Baumgarte stabilization. Special thanks to Davide Corradi for this contribution!\nNew constraint type CustomConstraint: a versatile interface to define more general types of constraints (e.g. time dependent), contributed by Matthew J. Millard.\nNew joint type JointTypeHelical that can be used for screwing motions (translations and simultaneous rotations), contributed by Stuart Anderson.\nAdded support to specify external forces on bodies on constrained forward dynamics and NonlinearEffects() (contributed by Matthew J. Millard)\nChanged Quaternion multiplication behaviour for a more standard convention: multiplying q1 (1,0,0,0) with q2 (0,1,0,0) results now in (0,0,1,0) instead of the previous (0,0,-1,0).\nRemoved Model::SetFloatingBaseBody(). Use JointTypeFloatingBase instead.\nLuaModel: extended specification to support ConstraintSets.\n28 April 2016: New version 2.5.0:\nAdded an experimental Cython based Python wrapper of RBDL. The API is very close to the C++ API. For a brief glimpse of the API see the file python/test_wrapper.py.\nMatthew Millard added CustomJoints which allow to create different joint types completely by user code. They are implemented as proxy joints for which their behaviour is specified using virtual functions.\nAdded CalcMInvTimesTau() that evaluates multiplication of the inverse of the joint space inertia matrix with a vector in O(n) time.\nAdded JointTypeFloatingBase which uses TX,TY,TZ and a spherical joint for the floating base joint.\nLoading of floating base URDF models must now be specified as a third parameter to URDFReadFromFile() and URDFReadFromString()\nAdded the URDF code from Bullet3 which gets used when ROS is not found. Otherwise use the URDF libraries found via Catkin.\nAdded CalcPointVelocity6D, CalcPointAcceleration6D, and CalcPointJacobian6D that compute both linear and angular quantities\nRemoved Model::SetFloatingBase (body). Use a 6-DoF joint or JointTypeFloatingBase instead.\nFixed building issues when building DLL with MSVC++.\n20 April 2016: New version 2.4.1:\nThis is a bugfix release that maintains binary compatibility and only fixes erroneous behaviour.\ncritical: fixed termination criterion for InverseKinematics. The termination criterion would be evaluated too early and thus report convergence too early. This was reported independently by Kevin Stein, Yun Fei, and Davide Corradi. Thanks for the reports!\ncritical: fixed CompositeRigidBodyAlgorithm when using spherical joints (thanks to S\u00e9bastien Barth\u00e9l\u00e9my for reporting!)\n23 February 2015: New version 2.4.0:\nAdded sparse range-space method ForwardDynamicsContactsRangeSpaceSparse() and ComputeContactImpulsesRangeSpaceSparse()\nAdded null-space method ForwardDynamicsContactsNullSpace() and ComputeContactImpulsesNullSpace()\nRenamed ForwardDynamicsContactsLagrangian() to ForwardDynamicsContactsDirect() and ComputeContactImpulsesLagrangian() to ComputeContactImpulsesDirect()\nRenamed ForwardDynamicsContacts() to ForwardDynamicsContactsKokkevis()\nRemoved/Fixed CalcAngularMomentum(). The function produced wrong values. The functionality has been integrated into CalcCenterOfMass().\nCalcPointJacobian() does not clear the argument of the result anymore. Caller has to ensure that the matrix was set to zero before using this function.\nAdded optional workspace parameters for ForwardDynamicsLagrangian() to optionally reduce memory allocations\nAdded JointTypeTranslationXYZ, JointTypeEulerXYZ, and JointTypeEulerYXZ which are equivalent to the emulated multidof joints but faster.\nAdded optional parameter to CalcCenterOfMass to compute angular momentum.\nAdded CalcBodySpatialJacobian()\nAdded CalcContactJacobian()\nAdded NonlinearEffects()\nAdded solving of linear systems using standard Householder QR\nLuaModel: Added LuaModelReadFromLuaState()\nURDFReader: Fixed various issues and using faster joints for floating base models\nVarious performance improvements\nFor a complete history see doc/api_changes.txt.\nDocumentation\nThe documentation is contained in the code and can be extracted with the tool doxygen.\nTo create the documentation simply run\ndoxygen Doxyfile\nwhich will generate the documentation in the subdirectory ./doc/html. The main page will then be located in ./doc/html/index.html.\nAn online version of the generated documentation can be found at https://rbdl.github.io.\nGetting RBDL\nThe latest stable code can be obtained from the master branch at\nhttps://github.com/rbdl/rbdl\nBuilding and Installation\nThe RBDL is built using CMake (http://www.cmake.org). To compile the library in a separate directory in Release mode use:\nmkdir build\ncd build/\ncmake -D CMAKE_BUILD_TYPE=Release ../\nmake\nFor optimal performance it is highly recommended to install the Eigen3 linear algebra library from http://eigen.tuxfamily.org. RBDL also comes with a simple, albeit much slower math library (SimpleMath) that can be used by enabling RBDL_USE_SIMPLE_MATH, i.e.:\ncmake -D RBDL_USE_SIMPLE_MATH=TRUE ../\nVCPKG package manager (for Windows, Linux and Mac)\nInstall vcpkg by making a local clone from its GitHub repo https://github.com/Microsoft/vcpkg. Then run the vcpkg-bootstrapper script to set it up. For detailed installation instructions, see Install vcpkg. To integrate vcpkg with your Visual Studio or Visual Studio Code development environment, see Integrate vcpkg. Then, to use vcpkg to install or update a library, see Manage libraries with vcpkg. For more information about vcpkg commands, see vcpkg command-line reference.\n\ud83d\udc40 RBDL is available in VCPKG since 2020-11 release\nPython Bindings\nRBDL can also build an experimental python wrapper that works with python 3 and python 2. To do this enable the the RBDL_BUILD_PYTHON_WRAPPER cmake options. This will build the wrapper for python 3, if you want to use python 2 instead you will also have to enable the RBDL_USE_PYTHON_2 cmake option. The result of this is an extra python directory in the build directory. From within which you can install it using setup.py. This is done automically when using make install\nCitation\nAn overview of the theoretical and implementation details has been published in [https://doi.org/10.1007/s10514-016-9574-0](Felis, M.L. Auton Robot (2017) 41: 495). To cite RBDL in your academic research you can use the following BibTeX entry:\n@Article{Felis2016,\nauthor=\"Felis, Martin L.\",\ntitle=\"RBDL: an efficient rigid-body dynamics library using recursive algorithms\",\njournal=\"Autonomous Robots\",\nyear=\"2016\",\npages=\"1--17\",\nissn=\"1573-7527\",\ndoi=\"10.1007/s10514-016-9574-0\",\nurl=\"http://dx.doi.org/10.1007/s10514-016-9574-0\"\n}\nLicensing\nThe library is published under the very permissive zlib free software license which should allow you to use the software wherever you need.\nThis is the full license text (zlib license):\nRBDL - Rigid Body Dynamics Library\nCopyright (c) 2011-2020 Martin Felis <martin@fysx.org>\nThis software is provided 'as-is', without any express or implied\nwarranty. In no event will the authors be held liable for any damages\narising from the use of this software.\nPermission is granted to anyone to use this software for any purpose,\nincluding commercial applications, and to alter it and redistribute it\nfreely, subject to the following restrictions:\n1. The origin of this software must not be misrepresented; you must not\nclaim that you wrote the original software. If you use this software\nin a product, an acknowledgment in the product documentation would be\nappreciated but is not required.\n2. Altered source versions must be plainly marked as such, and must not\nbe misrepresented as being the original software.\n3. This notice may not be removed or altered from any source\ndistribution.\nAcknowledgements\nWork on this library was originally funded by the Heidelberg Graduate School of Mathematical and Computational Methods for the Sciences (HGS), and the European FP7 projects ECHORD (grant number 231143) and Koroibot (grant number 611909).", "link": "https://github.com/rbdl/rbdl", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "rbdl - rigid body dynamics library copyright (c) 2011-2020 martin felis martin@fysx.org\nintroduction\nrbdl is a highly efficient c++ library that contains some essential rigid body dynamics algorithms such as the articulated body algorithm (aba) for forward dynamics, recursive newton-euler algorithm (rnea) for inverse dynamics and the composite rigid body algorithm (crba) for the efficient computation of the joint space inertia matrix. it further contains code for jacobians, forward and inverse kinematics, handling of external constraints such as contacts and collisions, and closed-loop models.\nthe code is developed by martin felis martin@fysx.org at the research group optimization in robotics and biomechanics (orb) of the interdisciplinary center for scientific computing (iwr) at heidelberg university. the code tightly follows the notation used in roy featherstone''s book \"rigid body dynamics algorithm\".\nrecent changes\n02 may 2018: new version 2.6.0:\nadded support for closed-loop models by replacing contacts api by a new constraints api. loop constraints can be stabilized using baumgarte stabilization. special thanks to davide corradi for this contribution!\nnew constraint type customconstraint: a versatile interface to define more general types of constraints (e.g. time dependent), contributed by matthew j. millard.\nnew joint type jointtypehelical that can be used for screwing motions (translations and simultaneous rotations), contributed by stuart anderson.\nadded support to specify external forces on bodies on constrained forward dynamics and nonlineareffects() (contributed by matthew j. millard)\nchanged quaternion multiplication behaviour for a more standard convention: multiplying q1 (1,0,0,0) with q2 (0,1,0,0) results now in (0,0,1,0) instead of the previous (0,0,-1,0).\nremoved model::setfloatingbasebody(). use jointtypefloatingbase instead.\nluamodel: extended specification to support constraintsets.\n28 april 2016: new version 2.5.0:\nadded an experimental cython based python wrapper of rbdl. the api is very close to the c++ api. for a brief glimpse of the api see the file python/test_wrapper.py.\nmatthew millard added customjoints which allow to create different joint types completely by user code. they are implemented as proxy joints for which their behaviour is specified using virtual functions.\nadded calcminvtimestau() that evaluates multiplication of the inverse of the joint space inertia matrix with a vector in o(n) time.\nadded jointtypefloatingbase which uses tx,ty,tz and a spherical joint for the floating base joint.\nloading of floating base urdf models must now be specified as a third parameter to urdfreadfromfile() and urdfreadfromstring()\nadded the urdf code from bullet3 which gets used when ros is not found. otherwise use the urdf libraries found via catkin.\nadded calcpointvelocity6d, calcpointacceleration6d, and calcpointjacobian6d that compute both linear and angular quantities\nremoved model::setfloatingbase (body). use a 6-dof joint or jointtypefloatingbase instead.\nfixed building issues when building dll with msvc++.\n20 april 2016: new version 2.4.1:\nthis is a bugfix release that maintains binary compatibility and only fixes erroneous behaviour.\ncritical: fixed termination criterion for inversekinematics. the termination criterion would be evaluated too early and thus report convergence too early. this was reported independently by kevin stein, yun fei, and davide corradi. thanks for the reports!\ncritical: fixed compositerigidbodyalgorithm when using spherical joints (thanks to s\u00e9bastien barth\u00e9l\u00e9my for reporting!)\n23 february 2015: new version 2.4.0:\nadded sparse range-space method forwarddynamicscontactsrangespacesparse() and computecontactimpulsesrangespacesparse()\nadded null-space method forwarddynamicscontactsnullspace() and computecontactimpulsesnullspace()\nrenamed forwarddynamicscontactslagrangian() to forwarddynamicscontactsdirect() and computecontactimpulseslagrangian() to computecontactimpulsesdirect()\nrenamed forwarddynamicscontacts() to forwarddynamicscontactskokkevis()\nremoved/fixed calcangularmomentum(). the function produced wrong values. the functionality has been integrated into calccenterofmass().\ncalcpointjacobian() does not clear the argument of the result anymore. caller has to ensure that the matrix was set to zero before using this function.\nadded optional workspace parameters for forwarddynamicslagrangian() to optionally reduce memory allocations\nadded jointtypetranslationxyz, jointtypeeulerxyz, and jointtypeeuleryxz which are equivalent to the emulated multidof joints but faster.\nadded optional parameter to calccenterofmass to compute angular momentum.\nadded calcbodyspatialjacobian()\nadded calccontactjacobian()\nadded nonlineareffects()\nadded solving of linear systems using standard householder qr\nluamodel: added luamodelreadfromluastate()\nurdfreader: fixed various issues and using faster joints for floating base models\nvarious performance improvements\nfor a complete history see doc/api_changes.txt.\ndocumentation\nthe documentation is contained in the code and can be extracted with the -----> tool !!!  doxygen.\nto create the documentation simply run\ndoxygen doxyfile\nwhich will generate the documentation in the subdirectory ./doc/html. the main page will then be located in ./doc/html/index.html.\nan online version of the generated documentation can be found at https://rbdl.github.io.\ngetting rbdl\nthe latest stable code can be obtained from the master branch at\nhttps://github.com/rbdl/rbdl\nbuilding and installation\nthe rbdl is built using cmake (http://www.cmake.org). to compile the library in a separate directory in release mode use:\nmkdir build\ncd build/\ncmake -d cmake_build_type=release ../\nmake\nfor optimal performance it is highly recommended to install the eigen3 linear algebra library from http://eigen.tuxfamily.org. rbdl also comes with a simple, albeit much slower math library (simplemath) that can be used by enabling rbdl_use_simple_math, i.e.:\ncmake -d rbdl_use_simple_math=true ../\nvcpkg package manager (for windows, linux and mac)\ninstall vcpkg by making a local clone from its github repo https://github.com/microsoft/vcpkg. then run the vcpkg-bootstrapper script to set it up. for detailed installation instructions, see install vcpkg. to integrate vcpkg with your visual studio or visual studio code development environment, see integrate vcpkg. then, to use vcpkg to install or update a library, see manage libraries with vcpkg. for more information about vcpkg commands, see vcpkg command-line reference.\n\ud83d\udc40 rbdl is available in vcpkg since 2020-11 release\npython bindings\nrbdl can also build an experimental python wrapper that works with python 3 and python 2. to do this enable the the rbdl_build_python_wrapper cmake options. this will build the wrapper for python 3, if you want to use python 2 instead you will also have to enable the rbdl_use_python_2 cmake option. the result of this is an extra python directory in the build directory. from within which you can install it using setup.py. this is done automically when using make install\ncitation\nan overview of the theoretical and implementation details has been published in [https://doi.org/10.1007/s10514-016-9574-0](felis, m.l. auton robot (2017) 41: 495). to cite rbdl in your academic research you can use the following bibtex entry:\n@article{felis2016,\nauthor=\"felis, martin l.\",\ntitle=\"rbdl: an efficient rigid-body dynamics library using recursive algorithms\",\njournal=\"autonomous robots\",\nyear=\"2016\",\npages=\"1--17\",\nissn=\"1573-7527\",\ndoi=\"10.1007/s10514-016-9574-0\",\nurl=\"http://dx.doi.org/10.1007/s10514-016-9574-0\"\n}\nlicensing\nthe library is published under the very permissive zlib free software license which should allow you to use the software wherever you need.\nthis is the full license text (zlib license):\nrbdl - rigid body dynamics library\ncopyright (c) 2011-2020 martin felis <martin@fysx.org>\nthis software is provided 'as-is', without any express or implied\nwarranty. in no event will the authors be held liable for any damages\narising from the use of this software.\npermission is granted to anyone to use this software for any purpose,\nincluding commercial applications, and to alter it and redistribute it\nfreely, subject to the following restrictions:\n1. the origin of this software must not be misrepresented; you must not\nclaim that you wrote the original software. if you use this software\nin a product, an acknowledgment in the product documentation would be\nappreciated but is not required.\n2. altered source versions must be plainly marked as such, and must not\nbe misrepresented as being the original software.\n3. this notice may not be removed or altered from any source\ndistribution.\nacknowledgements\nwork on this library was originally funded by the heidelberg graduate school of mathematical and computational methods for the sciences (hgs), and the european fp7 projects echord (grant number 231143) and koroibot (grant number 611909).", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000186, "year": null}, {"Unnamed: 0": 1208, "autor": 188, "date": null, "content": "awesome-ros-mobile-robot\nThis repository provides some useful resources and informations about autonomous mobile robots (AMR) research based on ROS. It would mainly focus on basic function of mobile robots(like odometry, SLAM, navigation and manipulation).\n(including both Chinese and English materials)\nhttp://wiki.ros.org/Distributions\nIndex:\n0.Robotics\n1.Robot-Operating-System(ROS)\n2.Robotic-Platform\n3.Robotic-Sensing\n4.Calibration\n5.Odometry\n6.SLAM\n7.Localization\n8.Mapping\n9.Navigation\n10.Manipulation\n11.Others (Non-tech)\n11-1. Famous robotics company\n11-2. Famous robotics conference&journal\n11-3. Famous robotics competition in Taiwan\n11-4. Famous ros organizations & activities\n0_Robotics\n\ud83d\udcda Books\n\"Introduction to Algorithms\", Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Clifford Stein\n\"Multiple View Geometry in Computer Vision\", Richard Hartley, Andrew Zisserman\n\"Probabilistic Robotics\", Sebastian Thrun\n\"Introduction to Linear Algebra\", Five Edition, Gilbert Strang\n\"Pattern Recognition and Machine Learning\", Christopher M. Bishop\n\"Introduction to autonomous mobile robots\" Siegwart, Roland, Illah Reza Nourbakhsh, and Davide Scaramuzza\n\"\u8996\u89ba SLAM \u5341\u56db\u8b1b\uff1a\u5f9e\u7406\u8ad6\u5230\u5be6\u8e10\", \u9ad8\u7fd4\n\ud83d\udcd6 Courses\n\"5 Minutes with Cyrill\" {Cyrill Stachniss} Cyrill Stachniss\nhttps://www.youtube.com/playlist?list=PLgnQpQtFTOGSO8HC48K9sPuNliY1qxzV9\n\"Matlab Lecture\" {Matlab}\nhttps://www.youtube.com/user/MATLAB/playlists\n\"Control System Lecture\" {Brian Douglas} Brian Douglas\nhttps://www.youtube.com/user/ControlLectures/playlists\n\"Robotics Sensing Related Lecture\" {Cyrill Stachniss} Cyrill Stachniss\nhttps://www.youtube.com/c/CyrillStachniss/playlists\n\"Robot Mapping\" {Universit\u00e4t of Freiburg} Cyrill Stachniss\nhttp://ais.informatik.uni-freiburg.de/teaching/ws13/mapping/\n\"Introduction to Mobile Robotics\" {Universit\u00e4t of Freiburg} Wolfram Burgard, et al.\nhttp://ais.informatik.uni-freiburg.de/teaching/ss13/robotics/\n\"Robotics (1)\" {NTU} Pei Chun Lin\nhttps://www.coursera.org/learn/robotics1, http://peichunlin.me.ntu.edu.tw/Homepage/Intro2Robotics.htm\n\"Control of Mobile Robots\" {Georgia Tech} Magnus Egerstedt\nhttps://www.coursera.org/learn/mobile-robot\"\n\"Modern Robotics: Mechanics, Planning, and Control\" {Northwestern University} Kevin Lynch\nhttps://www.coursera.org/specializations/modernrobotics\n\"Robotics\" {UPenn} Vijay Kumar, et al.\nhttps://zh-tw.coursera.org/specializations/robotics\n\"Linear algebra\" {NTU} Hung-yi Lee\nhttp://speech.ee.ntu.edu.tw/~tlkagk/courses_LA18.html\n\"Linear algebra\" {MIT} Gilbert Strang\nhttps://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/\n\"Machine Learning\" {NTU} Hung-yi Lee\nhttp://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.html\n\"Machine Learning\" {STANFORD} Andrew Ng\nhttps://www.coursera.org/learn/machine-learning\n\"Probabilistic Systems Analysis and Applied Probability\" {MIT} John Tsitsiklis\nhttps://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/\n\"Deep Reinforcement Learning\" {UCB} Sergey Levine\nhttp://rail.eecs.berkeley.edu/deeprlcourse/\n\"Vision Algorithms for Mobile Robotics\" {ETHZ} D. Scaramuzza\nhttp://rpg.ifi.uzh.ch/teaching.html\n\"Self-Driving Cars\" {TORONTO}\nhttps://www.coursera.org/specializations/self-driving-cars\n\ud83d\udcdc Paper Libraries\n\"IEEE Xplore Digital Library\": https://ieeexplore.ieee.org/Xplore/home.jsp\n\"arXiv.org e-Print archive\": https://arxiv.org/\n\"Google Scholar\": https://scholar.google.com/\n\"Sci-Hub\": https://sci-hub.tw/\n\"Airiti Library \": http://www.airitilibrary.com/home/index/\n\"National Digital Library of Theses and Dissertations in Taiwan\": https://ndltd.ncl.edu.tw\n1_Robot_Operating_System\n\ud83d\udc7e ROS Official Website\n\"The ROS Status\": https://status.ros.org/ (check if any server is down)\n\"The ROS Website\": https://www.ros.org (home)\n\"The ROS Wiki\": https://wiki.ros.org (pkg info)\n\"The ROS Documentation Site\": http://docs.ros.org (msg info)\n\"The ROS Answer\": https://answers.ros.org/questions/ (Q&A)\n\ud83d\udde3 ROS Related Blogs & Channels & Forums\n\"The construct\": https://www.youtube.com/channel/UCt6Lag-vv25fTX3e11mVY1Q\n\"JetsonHacks\": https://www.youtube.com/channel/UCQs0lwV6E4p7LQaGJ6fgy5Q\n\"\u9ce5\u54e5\u7684Linux\u79c1\u623f\u83dc\": http://linux.vbird.org/\n\"\u534a\u9592\u5c45\u58eb\"\uff1a https://www.cnblogs.com/gaoxiang12/\n\"\u6ce1\u6ce1\u6a5f\u5668\u4eba\u983b\u9053\": https://space.bilibili.com/38737757/\n\"\u6ce1\u6ce1\u6a5f\u5668\u4eba\u8ad6\u58c7\": http://paopaorobot.org/bbs/\n\ud83d\udcda Books\n\"C++ Primer\", Stanley B. Lippman, Jos\u00e9e Lajoie, Barbara E. Moo\n\"C++ Concurrency in Action, 2nd Edition\", Anthony Williams\n\"Design Patterns: Elements of Reusable Object-Oriented Software\", The \"Gang of Four\": Erich Gamma, Richard Helm, Ralph Johnson, John Vlissides\n\"Head First Design Patterns, 2nd Edition\", Eric Freeman, Elisabeth Robson\n\"Clean Code: A Handbook of Agile Software Craftsmanship\", Robert C. Martin\n\"ROS by Example\", python, Patrick Goebel\n\"Mastering ROS for Robotics Programming\", Lentin Joseph\n\"Learning ROS for Robotics Programming\", Enrique Fernandez et al.\n\"Programming Robots with ROS: A Practical Introduction to the Robot Operating System\", Morgan Quigley et al.\n\"\u6a5f\u5668\u4eba\u4f5c\u696d\u7cfb\u7d71ROS \u6dfa\u6790\", Jason M. O'Kane, \u8096\u8ecd\u6d69\u8b6f\n\"\u6a5f\u5668\u4eba\u4f5c\u696d\u7cfb\u7d71ROS \u53f2\u8a7136\u7bc7\", \u5f35\u65b0\u5b87, http://www.roseducation.org/docs/ROS_history.pdf\n\ud83e\udd1d System Integration Tool\nMultiple Machines with Multi-Master: http://wiki.ros.org/multimaster_fkie\nMultiple Machines with One Master: http://wiki.ros.org/ROS/NetworkSetup\nMultiple Tasks with Smach(state machine): http://wiki.ros.org/smach\nBridge for Non-ROS Programs(JSON API): https://wiki.ros.org/rosbridge_suite\nBridge communication between ROS 1 and ROS 2: https://github.com/ros2/ros1_bridge\n2_Robotic_Platform\n\ud83e\udd16 ROS Robot Overview\n\"Aerial, Ground, Marine, Manipulator, Component\": https://robots.ros.org/\n\ud83d\ude98 Wheel Robot Configurations\n(ref: Siegwart, Roland, Illah Reza Nourbakhsh, and Davide Scaramuzza. Introduction to autonomous mobile robots. MIT press, 2011, Table 2.1, p.34~36)\n\ud83d\ude97 Race Car Projects\n\"MIT\": https://mit-racecar.github.io\n\"Penn\": http://f1tenth.org/ [without slam, NAV]\n\"UCB\": http://www.barc-project.com/projects/ [without laser]\n\"Georgia Tech\": https://github.com/AutoRally [for outdoor]\n\"Taiwan Hypharos\": https://github.com/Hypha-ROS/hypharos_racecar\n\ud83e\udd16 ROS Mobile Robot Github\n\"turtlebot\": https://github.com/turtlebot\n\"turtlebot3\": https://github.com/ROBOTIS-GIT/turtlebot3\n\"clearpath husky\": https://github.com/husky\n\"clearpath jackel\": https://github.com/jackal\n\"Robotnik XL-GEN\": https://github.com/RobotnikAutomation/summit_xl_sim or summit_xl_common\n\"Robotnik RB-KAIROS\": https://github.com/RobotnikAutomation/rbkairos_sim or rbkairos_common\n\ud83e\udd16 ROS Mobile Manipulator Github\n\"Personal Robot 2 (PR2)\": https://github.com/PR2\n\"kuka youbot\": https://github.com/youbot\n\"fetch robotics\": https://github.com/fetchrobotics\n\"clearpath husky+UR5\": http://www.clearpathrobotics.com/assets/guides/husky/HuskyManip.html\n\"clearpath husky+dualUR5\": http://www.clearpathrobotics.com/assets/guides/husky/HuskyDualManip.html\n\"Robotnik RB-1\": https://github.com/RobotnikAutomation/rb1_sim or rb1_common\n\ud83e\udd16 ROS Manipulator Github\n\"Franka Emika panda\": https://github.com/frankaemika/franka_ros | https://github.com/ros-planning/panda_moveit_config\n\"Universal Robot 3/5/10/e\": https://github.com/ros-industrial/universal_robot\n\"Techman Robot\": https://github.com/kentsai0319/techman_robot\n\ud83d\udcbb Processing Unit (SBC/IPC)\nRaspberry Pi(RPI), BeagleBone Black(BBB), Odroid XU4, Odroid N2, Asus Tinker Board\nNVIDIA Jetson TX1, NVIDIA Jetson TX2, NVIDIA Jetson NANO, NVIDIA Jetson Xavier\nADLINK Neuron,\n\ud83d\udd79 Motor & Controller & Encoder\nElmo Motion Control Ltd\nRLS d.o.o. (Rotary and Linear Motion Sensors)\nDr. Fritz Faulhaber GmbH & Co. KG\nMaxon group motors & drivers\nDexmart motors & drivers (Trumman Technology Corp)\n3_Robotic_Sensing\n\ud83d\udcf7 RGB Camera\n\"usb camera\": http://wiki.ros.org/usb_cam\n\"gstream-based camera\": http://wiki.ros.org/gscam\n\"opencv camera\": http://wiki.ros.org/cv_camera\n\ud83d\udcf8 RGB-D Camera\n\"Microsoft kinectv1 with openni\": https://github.com/ros-drivers/openni_camera\n\"Microsoft kinectv1 with freenect\": https://github.com/ros-drivers/freenect_stack\n\"Microsoft kinect one/v2\": https://github.com/code-iai/iai_kinect2\n\"Asus xtion with openni2\": https://github.com/ros-drivers/openni2_camera\n\"Intel RealSense d455/d435/d435i/d415\": https://github.com/intel-ros/realsense\n\"Occipital Structure Sensor/Core\": https://structure.io/\n\ud83c\udfa5 Stereo Camera\n\"Stereolabs ZED\": http://wiki.ros.org/zed-ros-wrapper\n\"Carnegie Robotics MultiSense\u2122 S7\": http://docs.carnegierobotics.com/S7/\n\"e-Con Systems Tara Stereo Camera\": https://github.com/dilipkumar25/see3cam\n\"Nerian SP1\": http://wiki.ros.org/nerian_sp1\n\ud83d\udd26 Laser Rangefinder [laser scanners] [scanning rangefinder]\n\u2013 often represent 2D laser scanning\n\"hokuyo_urg\": http://wiki.ros.org/urg_node (old: http://wiki.ros.org/hokuyo_node\n\"hokuyo_utm\": http://wiki.ros.org/urg_node (old: http://wiki.ros.org/hokuyo_node\n\"ydlidar\": https://github.com/YDLIDAR/ydlidar_ros\n\"rplidar\": http://wiki.ros.org/rplidar\n\"sick\": http://wiki.ros.org/sick_scan\n\ud83d\udca1 LIDAR [light detection and ranging] [light imaging, detection, and ranging] [3D laser scanning ]\n\u2013 often represent 3D laser scanning\n\"Velodyne\": http://wiki.ros.org/velodyne\n\"Livox\": https://github.com/hku-mars/loam_livox\n\ud83c\udf4e IMU [inertial measurement unit]\n\"Xsense\": http://wiki.ros.org/xsens_driver\n\"MicroStrain 3DM-GX2\": http://wiki.ros.org/microstrain_3dmgx2_imu\n\"SparkFun 9DOF Razor IMUM0\": http://wiki.ros.org/razor_imu_9dof\n\ud83d\udea8 3D Scanning & Novel Sensing Device\n\"Kaarta\": https://www.kaarta.com/\n\"Matterport\": https://matterport.com/\n\"Microsoft azure-kinect-dk\": https://azure.microsoft.com/zh-tw/services/kinect-dk/\n\"Intel RealSense Tracking Camera T265\": https://www.intelrealsense.com/tracking-camera-t265/\n\"Intel RealSense LiDAR Camera L515\": https://www.intelrealsense.com/lidar-camera-l515/\n\ud83c\udf99 Microphone Array\n\"ReSpeaker Mic Array v2.0\": http://wiki.seeedstudio.com/ReSpeaker_Mic_Array_v2.0/\n\ud83d\udd0a Text To Speech (TTS)\n\"gTTS\": https://pypi.org/project/gTTS/\n\"sound_play\": http://wiki.ros.org/sound_play\n\ud83d\udde3 Speech Recognition / Speech To Text (STT)\n\"SpeechRecognition\": https://pypi.org/project/SpeechRecognition/\n\ud83d\ude80 Vocal Assistant\n\"Amazon Alexa\": https://www.amazon.com/Amazon-Echo-And-Alexa-Devices/b?ie=UTF8&node=9818047011\n\"Google Nest\": https://store.google.com/product/google_nest_mini\n\"Apple Homepod\": https://www.apple.com/tw/shop/buy-homepod/homepod/\n\"Mi AI Speaker\": https://www.mi.com/aispeaker\n\"ASUS Smart Speaker\": https://www.asus.com/tw/ASUS-Smart-Speaker/ASUS-Smart-Speaker-Xiao-Bu/\n\"PyAIML -- The Python AIML Interpreter\": https://github.com/cdwfs/pyaiml\n\ud83d\udc7e Matrix Barcode (Fiducial Marker Systems, or ARTag, or Auxiliary marker)\n\"ARTag\": http://wiki.ros.org/ar_track_alvar\n\"AprilTag\": http://wiki.ros.org/apriltag_ros\n\"CALTag\": http://www.cs.ubc.ca/labs/imager/tr/2010/Atcheson_VMV2010_CALTag/\n\"comparison\": Sagitov, Artur, et al. \"ARTag, AprilTag and CALTag Fiducial Marker Systems: Comparison in a Presence of Partial Marker Occlusion and Rotation.\" ICINCO (2). 2017.\n\ud83d\udd05 Learning-Based Feature Extractor\nAlexnet, VGG, ResNet, InceptionV3, DenseNet, GoogleNet, MobileNet, SqueezeNet, etc.\n\"Pytorch implementation\": https://pytorch.org/docs/stable/torchvision/models.html\n\ud83d\udd05 Learning-Based Object Detection\n\"Faster R-CNN\"\nRen, Shaoqing, et al. \"Faster r-cnn: Towards real-time object detection with region proposal networks.\" Advances in neural information processing systems. 2015.\n\"SSD\"\nLiu, Wei, et al. \"Ssd: Single shot multibox detector.\" European conference on computer vision. Springer, Cham, 2016.\n\"YOLOv3\": https://github.com/leggedrobotics/darknet_ros\n(v4) Bochkovskiy, Alexey, Chien-Yao Wang, and Hong-Yuan Mark Liao. \"YOLOv4: Optimal Speed and Accuracy of Object Detection.\" arXiv preprint arXiv:2004.10934 (2020).\n(v3) Redmon, Joseph, and Ali Farhadi. \"Yolov3: An incremental improvement.\" arXiv preprint arXiv:1804.02767 (2018).\n(v2) Redmon, Joseph, and Ali Farhadi. \"YOLO9000: better, faster, stronger.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n(v1) Redmon, Joseph, et al. \"You only look once: Unified, real-time object detection.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n\ud83d\udd05 Learning-Based Human Pose Estimation\n\"OpenPose\": https://github.com/CMU-Perceptual-Computing-Lab/openpose\n\"OpenPose-plugin\": https://github.com/ildoonet/tf-pose-estimation\n4_Calibration\n\ud83d\udcf7 Camera Calibration (Intrinsic and Extrinsic parameters)\n\"camera_calibration\": http://wiki.ros.org/camera_calibration\n\"format converter\": http://wiki.ros.org/camera_calibration_parsers\n\ud83d\udc41 Hand-Eye Calibration\n\"easy_handeye\": https://github.com/IFL-CAMP/easy_handeye\n\ud83c\udf4e IMU (Sparkfun Razer 9dof-razor-imu-m0) Calibration\n\"Github Wiki\": https://github.com/Razor-AHRS/razor-9dof-ahrs/wiki/Tutorial\n\"ROS Wiki\": http://wiki.ros.org/razor_imu_9dof\n\"Sparkfun Official Guide\": https://learn.sparkfun.com/tutorials/9dof-razor-imu-m0-hookup-guide/all\n\"My Calibration Guide\": https://github.com/shannon112/imu_calibration/blob/master/README.md\n5_Odometry\n\u2620\ufe0e Visual Based Ego-Motion Backbone\nComponents\nFeature Keypoint & Desciptor - SURF, SIFT, ORB\nFeature Matching - Brute-Force, FLANN\nhttps://docs.opencv.org/3.4/db/d27/tutorial_py_table_of_contents_feature2d.html\nOptical Flow - Lucas-Kanade (LK)\nMotion Estimation:\n2D-2D: Epipolar Geometry & Triangulation\n2D-3D: Perspective-n-Point (PnP) - P3P, DLT, EPnP, UPnP, BA\n3D-3D: Iterative Closest Point (ICP) - ICP(SVD), GICP, NDT, IPDA, Non-LinearOpt, point2point, point2plane\nDirect Method - Dense, Semi-Dense, Sparse\nSolutions\nExtract Feature Keypoint -> Desciptor -> Matching -> Motion Estimation\nExtract Feature Keypoint -> Optical Flow -> Motion Estimation\nExtract Feature Keypoint -> Sparse Direct Method\nSemi-Dense/Dense Direct Method\n\ud83d\udcda Odometry Survey Paper\nDelmerico, Jeffrey, and Davide Scaramuzza. \"A benchmark comparison of monocular visual-inertial odometry algorithms for flying robots.\" 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018.\nG. Huang, \"Visual-Inertial Navigation: A Concise Review,\" 2019 International Conference on Robotics and Automation (ICRA), Montreal, QC, Canada, 2019, pp. 9572-9582.\n\ud83c\udfc6 Odometry Algorithm Ranking\n\"KITTI\": http://www.cvlibs.net/datasets/kitti/eval_odometry.php\n\ud83d\ude96 Wheel Odometry\n\"ros_control\": http://wiki.ros.org/ros_control\nChitta, Sachin, et al. \"ros_control: A generic and simple control framework for ROS.\" (2017).\n\ud83d\udca1 2D Laser Based Odometry\n\"rf2o\": https://github.com/MAPIRlab/rf2o_laser_odometry\nM. Jaimez, J. Monroy, J. Gonzalez-Jimenez, Planar Odometry from a Radial Laser Scanner. A Range Flow-based Approach, IEEE International Conference on Robotics and Automation (ICRA), Stockholm, Sweden, pp. 4479-4485, 2016.\n\ud83d\udcf7 3D Visual Based Odometry (Mono)\n\"VINS-Mono\": https://github.com/HKUST-Aerial-Robotics/VINS-Mono\nQin, Tong, Peiliang Li, and Shaojie Shen. \"Vins-mono: A robust and versatile monocular visual-inertial state estimator.\" IEEE Transactions on Robotics 34.4 (2018): 1004-1020.\n\"SVO\": https://github.com/uzh-rpg/rpg_svo | http://rpg.ifi.uzh.ch/svo2.html Sparse Direct Method\nForster, Christian, Matia Pizzoli, and Davide Scaramuzza. \"SVO: Fast semi-direct monocular visual odometry.\" 2014 IEEE international conference on robotics and automation (ICRA). IEEE, 2014.\n\"DSO\": https://github.com/JakobEngel/dso Sparse Direct Method\nEngel, Jakob, Vladlen Koltun, and Daniel Cremers. \"Direct sparse odometry.\" IEEE transactions on pattern analysis and machine intelligence 40.3 (2017): 611-625.\n\"VISO2\": http://wiki.ros.org/viso2_ros | http://www.cvlibs.net/software/libviso/\nGeiger, Andreas, Julius Ziegler, and Christoph Stiller. \"Stereoscan: Dense 3d reconstruction in real-time.\" 2011 IEEE Intelligent Vehicles Symposium (IV). Ieee, 2011.\nKitt, Bernd, Andreas Geiger, and Henning Lategahn. \"Visual odometry based on stereo image sequences with ransac-based outlier rejection scheme.\" 2010 ieee intelligent vehicles symposium. IEEE, 2010.\n\"OKVIS\": https://github.com/ethz-asl/okvis | https://github.com/ethz-asl/okvis_ros\nLeutenegger, Stefan, et al. \"Keyframe-based visual\u2013inertial odometry using nonlinear optimization.\" The International Journal of Robotics Research 34.3 (2015): 314-334.\n\"ROVIO\": https://github.com/ethz-asl/rovio\nBloesch, Michael, et al. \"Robust visual inertial odometry using a direct EKF-based approach.\" 2015 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 2015.\nBloesch, Michael, et al. \"Iterated extended Kalman filter based visual-inertial odometry using direct photometric feedback.\" The International Journal of Robotics Research 36.10 (2017): 1053-1072.\n\"RotRocc+, RotRocc, ROCC, MonoROCC\"\nM. Buczko and V. Willert: Flow-Decoupled Normalized Reprojection Error for Visual Odometry. 19th IEEE Intelligent Transportation Systems Conference (ITSC) 2016.\nM. Buczko, V. Willert, J. Schwehr and J. Adamy: Self-Validation for Automotive Visual Odometry. IEEE Intelligent Vehicles Symposium (IV) 2018.\nM. Buczko and V. Willert: Monocular Outlier Detection for Visual Odometry. IEEE Intelligent Vehicles Symposium (IV) 2017.\nM. Buczko and V. Willert: How to Distinguish Inliers from Outliers in Visual Odometry for High-speed Automotive Applications. IEEE Intelligent Vehicles Symposium (IV) 2016.\n\ud83d\udcf8 3D RGB-D/Stereo Based Odometry\n\"VINS-Fusion\": https://github.com/HKUST-Aerial-Robotics/VINS-Fusion\nQin, Tong, and Shaojie Shen. \"Online temporal calibration for monocular visual-inertial systems.\" 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018.\n\"DVO\": https://github.com/tum-vision/dvo\nKerl, Christian, J\u00fcrgen Sturm, and Daniel Cremers. \"Robust odometry estimation for RGB-D cameras.\" 2013 IEEE International Conference on Robotics and Automation. IEEE, 2013. Steinbr\u00fccker, Frank, J\u00fcrgen Sturm, and Daniel Cremers. \"Real-time visual odometry from dense RGB-D images.\" 2011 IEEE international conference on computer vision workshops (ICCV Workshops). IEEE, 2011.\n\"SOFT\": https://github.com/Mayankm96/Stereo-Odometry-SOFT\nCvi\u0161ic, Igor, et al. \"Soft-slam: Computationally efficient stereo visual slam for autonomous uavs.\" Journal of field robotics (2017).\nCvi\u0161i\u0107, Igor, and Ivan Petrovi\u0107. \"Stereo odometry based on careful feature selection and tracking.\" 2015 European Conference on Mobile Robots (ECMR). IEEE, 2015.\n\"VISO2\": http://wiki.ros.org/viso2_ros | http://www.cvlibs.net/software/libviso/\nGeiger, Andreas, Julius Ziegler, and Christoph Stiller. \"Stereoscan: Dense 3d reconstruction in real-time.\" 2011 IEEE Intelligent Vehicles Symposium (IV). Ieee, 2011.\nKitt, Bernd, Andreas Geiger, and Henning Lategahn. \"Visual odometry based on stereo image sequences with ransac-based outlier rejection scheme.\" 2010 ieee intelligent vehicles symposium. IEEE, 2010.\n\ud83d\udd05 3D LiDAR Based Odometry\n\"LOAM & V-LOAM\": https://github.com/laboshinl/loam_velodyne\nJ Zhang, S Singh, \"LOAM: Lidar Odometry and Mapping in Real-time\", Robotics: Science and Systems Conference (RSS 2014)\nJ Zhang, S Singh, \"Visual-lidar Odometry and Mapping: Low-drift, Robust, and Fast\", IEEE International Conference on Robotics and Automation (ICRA)\nJ. Zhang, M. Kaess and S. Singh: Real-time Depth Enhanced Monocular Odometry. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2014.\n\"LIMO\": https://github.com/johannes-graeter/limo\nGraeter, Johannes, Alexander Wilczynski, and Martin Lauer. \"Limo: Lidar-monocular visual odometry.\" 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018.\n\ud83e\udd16 Learning Based Odometry\n\"DeepVO\": https://github.com/ChiWeiHsiao/DeepVO-pytorch | https://github.com/ildoonet/deepvo\nS. Wang, R. Clark, H. Wen and N. Trigoni, \"DeepVO: Towards end-to-end visual odometry with deep Recurrent Convolutional Neural Networks,\" 2017 IEEE International Conference on Robotics and Automation (ICRA), Singapore, 2017, pp. 2043-2050.\n\"VINET\": https://github.com/HTLife/VINet\nClark, Ronald, et al. \"VINet: Visual-Inertial Odometry as a Sequence-to-Sequence Learning Problem.\" AAAI. 2017.\n\ud83c\udf65 Odometry Fusion\nEKF | \"robot_pose_ekf\": http://wiki.ros.org/robot_pose_ekf\nEKF & UKF | \"robot_localization\": http://docs.ros.org/melodic/api/robot_localization/html/index.html\nMoore, Thomas, and Daniel Stouch. \"A generalized extended kalman filter implementation for the robot operating system.\" Intelligent autonomous systems 13. Springer, Cham, 2016. 335-348.\n6_SLAM\n\ud83c\udfdb SLAM Theorem & Tutorial\nT. Bailey and H. F. Durrant-Whyte, \u201cSimultaneous localisation and map- ping (SLAM): Part II\u201d, IEEE Robot. Auton. Syst., vol. 13, no. 3, pp. 108\u2013117, 2006.\nH. F. Durrant-Whyte and T. Bailey, \u201cSimultaneous localisation and map- ping (SLAM): Part I\u201d, IEEE Robot. Autom. Mag., vol. 13, no. 2, pp. 99\u2013110, Jun. 2006\nStrasdat, Hauke, Jos\u00e9 MM Montiel, and Andrew J. Davison. \"Visual SLAM: why filter?.\" Image and Vision Computing 30.2 (2012): 65-77. (comparison between filter and graph)\nGrisetti, Giorgio, et al. \"A tutorial on graph-based SLAM.\" IEEE Intelligent Transportation Systems Magazine 2.4 (2010): 31-43.\n\ud83d\udcda SLAM Survey Paper\nCesar Cadena ; Luca Carlone ; Henry Carrillo ; Yasir Latif ; Davide Scaramuzza ; Jos\u00e9 Neira ; Ian Reid ; John J. Leonard, \u201cPast, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age\u201d, IEEE Transactions on RoboticsYear: 2016, Volume: 32, Issue: 6Pages: 1309 - 1332\nJamiruddin, Redhwan, et al. \"Rgb-depth slam review.\" arXiv preprint arXiv:1805.07696 (2018).\nZollh\u00f6fer, Michael, et al. \"State of the Art on 3D Reconstruction with RGB\u2010D Cameras.\" Computer graphics forum. Vol. 37. No. 2. 2018.\n\u2620\ufe0e SLAM Backbone (Back-End)\nKalman Filter Based\nKalman Filter (KF), Extend Kalman Filte (EKF), Unscented Kalman Filte (UKF)\nExtended Information Filter (EIF), Sparse Extended Information Filter (SEIF)\nParticle Filter Based\nGmapping, FastSLAM, FastSLAM2.0\nGraph Optimization Based\nMethod: Bundle Adjustment(BA), Pose Graph, Factor Graph\nRegression Solution: Gaussian Netwon (GN), Leverberg Marquert(LM)\nEfficiently Solving: Cholesky Factorization, QR Decomposition, Conjugate Gradients\nCeres Solver Library: S. Agarwal and M. Keir. \"Ceres solver.\" [online]. Available: http://ceres-solver.org/\ng2o Library: K\u00fcmmerle, Rainer, et al. \"g 2 o: A general framework for graph optimization.\" 2011 IEEE International Conference on Robotics and Automation. IEEE, 2011.\nGTSAM: Dellaert, Frank. Factor graphs and GTSAM: A hands-on introduction. Georgia Institute of Technology, 2012.\niSAM: (1)Kaess, M., Ranganathan, A., and Dellaert, F. (2008). iSAM: Incremental smoothing and mapping.IEEE Trans. Robotics, 24(6):1365\u20131378. (2)Kaess, M., Johannsson, H., Roberts, R., Ila, V., Leonard, J., and Dellaert, F. (2012). iSAM2:Incremental smoothing and mapping using the Bayes tree.Intl. J. of Robotics Research, 31:217\u2013236. (iSAM2 is available as part of the GTSAM)\nSLAM++: Ila, Viorela, et al. \"SLAM++-A highly efficient and temporally scalable incremental SLAM framework.\" The International Journal of Robotics Research 36.2 (2017): 210-230.\nLearning Based\n\ud83d\udcd0 SLAM Benchmark (Dataset)\n\"The KITTI Vision Benchmark & Dataset\": http://www.cvlibs.net/datasets/kitti/\nGeiger, Andreas, Philip Lenz, and Raquel Urtasun. \"Are we ready for autonomous driving? the kitti vision benchmark suite.\" 2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2012.\n\"MIT Stata Center Dataset\": https://projects.csail.mit.edu/stata/#\nFallon, Maurice, et al. \"The mit stata center dataset.\" The International Journal of Robotics Research 32.14 (2013): 1695-1699.\n\"Radish Dataset\": http://ais.informatik.uni-freiburg.de/slamevaluation/datasets.php\nHoward and N. Roy, \u201cThe robotics data set repository (Radish),\u201d2003. [Online]. Available: http://radish.sourceforge.net/\n\"TUM RGB-D SLAM Benchmark & Dataset\": https://vision.in.tum.de/data/datasets/rgbd-dataset\nSturm, J\u00fcrgen, et al. \"A benchmark for the evaluation of RGB-D SLAM systems.\" 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2012.\n\"ICL-NUIM RGB-D Benchmark & Dataset\": https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html\nA. Handa, T. Whelan, J. McDonald, and A. J. Davison, \u201cA bench-mark for rgb-d visual odometry, 3d reconstruction and slam,\u201d inRobotics and automation (ICRA), 2014 IEEE international conferenceon. IEEE, 2014, pp. 1524\u20131531.\n\"EuRoC MAV Dataset\": https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets\nBurri, Michael, et al. \"The EuRoC micro aerial vehicle datasets.\" The International Journal of Robotics Research 35.10 (2016): 1157-1163.\n\"Benchmark\"\nR.K \u0308ummerle, B.Steder, C.Dornhege, M.Ruhnke, G.Grisetti, C.Stachniss, and A.Kleiner, \"On measuring the accuracy of SLAM algorithms,\" Autonomous Robots, vol. 27, no. 4, pp. 387\u2013407, 2009.\n\"Survey Paper\"\nCai, Ziyun, et al. \"RGB-D datasets using microsoft kinect or similar sensors: a survey.\" Multimedia Tools and Applications 76.3 (2017): 4313-4355.\n\ud83d\udca1 2D Laser Based SLAM\n\"Cartographer\": https://google-cartographer-ros.readthedocs.io/en/latest/\nWolfgang Hess ; Damon Kohler ; Holger Rapp ; Daniel Andor, \u201cReal-time loop closure in 2D LIDAR SLAM \u201d, 2016 IEEE International Conference on Robotics and Automation (ICRA), Stockholm, 2016, pp. 1271-1278.\n\"Gmapping\": http://wiki.ros.org/gmapping\nG. Grisetti, C. Stachniss and W. Burgard, \"Improved Techniques for Grid Mapping With Rao-Blackwellized Particle Filters,\" IEEE Transactions on Robotics, vol. 23, no. 1, pp. 34-46, Feb. 2007.\n\"Hector\": http://wiki.ros.org/hector_slam\nS. Kohlbrecher, O. von Stryk, J. Meyer and U. Klingauf, \"A flexible and scalable SLAM system with full 3D motion estimation,\" 2011 IEEE International Symposium on Safety, Security, and Rescue Robotics, Kyoto, 2011, pp. 155-160.\n\"Karto\": http://wiki.ros.org/slam_karto\nVincent, R., Limketkai, B., & Eriksen, M. (2010, April). Comparison of indoor robot localization techniques in the absence of GPS. In Detection and Sensing of Mines, Explosive Objects, and Obscured Targets XV (Vol. 7664, p. 76641Z). International Society for Optics and Photonics.\n\"FastSLAM\":\nMontemerlo, Michael, et al. \"FastSLAM: A factored solution to the simultaneous localization and mapping problem.\" Aaai/iaai 593598 (2002).\nMontemerlo, Michael, et al. \"FastSLAM 2.0: An improved particle filtering algorithm for simultaneous localization and mapping that provably converges.\" IJCAI. 2003.\n\ud83d\udcf7 3D Visual Based SLAM (Mono)\n\"MonoSLAM\": https://github.com/hanmekim/SceneLib2 Feature + EKF\nDavison, Andrew J., et al. \"MonoSLAM: Real-time single camera SLAM.\" IEEE transactions on pattern analysis and machine intelligence 29.6 (2007): 1052-1067.\n\"PTAM\": http://www.robots.ox.ac.uk/~gk/PTAM/ Feature + BA\nKlein, Georg, and David Murray. \"Parallel tracking and mapping for small AR workspaces.\" 2007 6th IEEE and ACM international symposium on mixed and augmented reality. IEEE, 2007.\n\"ORB-SLAM\": https://github.com/raulmur/ORB_SLAM2 Feature + (BA + Pose-Graph)\nRa\u00fal Mur-Artal, J. M. M. Montiel and Juan D. Tard\u00f3s. ORB-SLAM: A Versatile and Accurate Monocular SLAM System. IEEE Transactions on Robotics, vol. 31, no. 5, pp. 1147-1163, 2015.\nDorian G\u00e1lvez-L\u00f3pez and Juan D. Tard\u00f3s. Bags of Binary Words for Fast Place Recognition in Image Sequences. IEEE Transactions on Robotics, vol. 28, no. 5, pp. 1188-1197, 2012.\n\"LSD-SLAM\": Semi-dense Direct Method + Pose-Graph\nEngel, Jakob, Thomas Sch\u00f6ps, and Daniel Cremers. \"LSD-SLAM: Large-scale direct monocular SLAM.\" European conference on computer vision. Springer, Cham, 2014.\n\ud83d\udcf8 3D RGB-D/Stereo Based SLAM\n\"DTAM\": https://github.com/anuranbaka/OpenDTAM\nNewcombe, Richard A., Steven J. Lovegrove, and Andrew J. Davison. \"DTAM: Dense tracking and mapping in real-time.\" 2011 international conference on computer vision. IEEE, 2011.\n\"ORB-SLAM2\": https://github.com/raulmur/ORB_SLAM2\nRa\u00fal Mur-Artal and Juan D. Tard\u00f3s. ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras. IEEE Transactions on Robotics, vol. 33, no. 5, pp. 1255-1262, 2017.\n\"ORB-SLAM3\": https://github.com/UZ-SLAMLab/ORB_SLAM3\n[ORB-SLAM3] Carlos Campos, Richard Elvira, Juan J. G\u00f3mez Rodr\u00edguez, Jos\u00e9 M. M. Montiel and Juan D. Tard\u00f3s, ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM, Under review.\n[IMU-Initialization] Carlos Campos, J. M. M. Montiel and Juan D. Tard\u00f3s, Inertial-Only Optimization for Visual-Inertial Initialization, ICRA 2020.\n[ORBSLAM-Atlas] Richard Elvira, J. M. M. Montiel and Juan D. Tard\u00f3s, ORBSLAM-Atlas: a robust and accurate multi-map system, IROS 2019.\n[ORBSLAM-VI] Ra\u00fal Mur-Artal, and Juan D. Tard\u00f3s, Visual-inertial monocular SLAM with map reuse, IEEE Robotics and Automation Letters, vol. 2 no. 2, pp. 796-803, 2017.\n\"DVO-SLAM\": https://github.com/tum-vision/dvo_slam\nKerl, Christian, J\u00fcrgen Sturm, and Daniel Cremers. \"Dense visual SLAM for RGB-D cameras.\" 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2013.\n\"RGBDSLAMv2\": https://felixendres.github.io/rgbdslam_v2/\nEndres, Felix, et al. \"3-D mapping with an RGB-D camera.\" IEEE transactions on robotics 30.1 (2013): 177-187.\n\"RTAB-Map\": http://introlab.github.io/rtabmap/ | https://github.com/introlab/rtabmap_ros\nM. Labb\u00e9 and F. Michaud, \u201cRTAB-Map as an Open-Source Lidar and Visual SLAM Library for Large-Scale and Long-Term Online Operation,\u201d in Journal of Field Robotics, vol. 36, no. 2, pp. 416\u2013446, 2019. (Wiley) Universit \u0301e de Sherbrooke\nM. Labb\u00e9 and F. Michaud, \u201cLong-term online multi-session graph-based SPLAM with memory management,\u201d in Autonomous Robots, vol. 42, no. 6, pp. 1133-1150, 2018.\nM. Labb\u00e9 and F. Michaud, \u201cOnline Global Loop Closure Detection for Large-Scale Multi-Session Graph-Based SLAM,\u201d in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, 2014.\nM. Labb\u00e9 and F. Michaud, \u201cAppearance-Based Loop Closure Detection for Online Large-Scale and Long-Term Operation,\u201d in IEEE Transactions on Robotics, vol. 29, no. 3, pp. 734-745, 2013.\nM. Labb\u00e9 and F. Michaud, \u201cMemory management for real-time appearance-based loop closure detection,\u201d in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, 2011, pp. 1271\u20131276.\n\"KinectFusion\": https://www.microsoft.com/en-us/research/project/kinectfusion-project-page/\nIzadi, Shahram, et al. \"KinectFusion: real-time 3D reconstruction and interaction using a moving depth camera.\" Proceedings of the 24th annual ACM symposium on User interface software and technology. ACM, 2011. Newcombe, Richard A., et al. \"Kinectfusion: Real-time dense surface mapping and tracking.\" ISMAR. Vol. 11. No. 2011. 2011.\n\"ElasticFusion\": https://github.com/mp3guy/ElasticFusion\nWhelan, Thomas, et al. \"ElasticFusion: Dense SLAM without a pose graph.\" Robotics: Science and Systems, 2015.\n\"BundleFusion\": http://graphics.stanford.edu/projects/bundlefusion/\nDai, Angela, et al. \"Bundlefusion: Real-time globally consistent 3d reconstruction using on-the-fly surface reintegration.\" ACM Transactions on Graphics (ToG) 36.3 (2017): 24.\n\"KO-Fusion\": https://www.youtube.com/watch?v=yigoIYoY7Wg (mobile manipulator)\nHouseago, Charlie, Michael Bloesch, and Stefan Leutenegger. \"KO-Fusion: Dense Visual SLAM with Tightly-Coupled Kinematic and Odometric Tracking.\" 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019.\n\"arm-slam\": https://www.youtube.com/watch?v=QrFyaxFUs9w (manipulator)\nM. Klingensmith, S. S. Sirinivasa and M. Kaess, \"Articulated Robot Motion for Simultaneous Localization and Mapping (ARM-SLAM),\" in IEEE Robotics and Automation Letters, vol. 1, no. 2, pp. 1156-1163, July 2016.\n\ud83d\udd05 3D LiDAR Based SLAM\n\"Zebedee\": https://research.csiro.au/robotics/zebedee/ (handheld device)\nM. Bosse, R. Zlot and P. Flick, \"Zebedee: Design of a Spring-Mounted 3-D Range Sensor with Application to Mobile Mapping,\" in IEEE Transactions on Robotics, vol. 28, no. 5, pp. 1104-1119, Oct. 2012.\n\"Kaarta\": https://www.kaarta.com/ (handheld device)\nZhang, Ji, and Sanjiv Singh. \"Laser\u2013visual\u2013inertial odometry and mapping with high robustness and low drift.\" Journal of Field Robotics 35.8 (2018): 1242-1264.\n\"LIO-SAM\": https://github.com/TixiaoShan/LIO-SAM (handheld device)\nShan, Tixiao and Englot, Brendan and Meyers, Drew and Wang, Wei and Ratti, Carlo and Rus Daniela, \"LIO-SAM: Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping,\" 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Las Vegas, 2020\n\"hdl_graph_slam\": https://github.com/koide3/hdl_graph_slam\nKenji Koide, Jun Miura, and Emanuele Menegatti, A Portable 3D LIDAR-based System for Long-term and Wide-area People Behavior Measurement, Advanced Robotic Systems, 2019\n\"BLAM\": https://github.com/erik-nelson/blam\nE. Nelson, BLAM: berkeley localization and mapping, [online]. Available: https://github.com/erik-nelson/blam.\n\"Lego-LOAM\": https://github.com/RobustFieldAutonomyLab/LeGO-LOAM\nT. Shan and B. Englot, \"LeGO-LOAM: Lightweight and Ground- Optimized Lidar Odometry and Mapping on Variable Terrain,\" 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Madrid, 2018, pp. 4758- 4765.\n\"Cartographer\": https://google-cartographer-ros.readthedocs.io/en/latest/\nWolfgang Hess ; Damon Kohler ; Holger Rapp ; Daniel Andor, \u201cReal-time loop closure in 2D LIDAR SLAM \u201d, 2016 IEEE International Conference on Robotics and Automation (ICRA), Stockholm, 2016, pp. 1271-1278.\n\"IMLS-SLAM\"\nDeschaud, Jean-Emmanuel. \"IMLS-SLAM: scan-to-model matching based on 3D data.\" 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018.\n\ud83d\udc2d Cognitive Related SLAM\n\"ViTa-SLAM\": https://github.com/aalto-intelligent-robotics/ViTa-SLAM\nStruckmeier, Oliver, et al. \"ViTa-SLAM: A Bio-inspired Visuo-Tactile SLAM for Navigation while Interacting with Aliased Environments.\" 2019 IEEE International Conference on Cyborg and Bionic Systems (CBS). IEEE, 2019.\n\ud83c\udff7 Semantic Related SLAM\n\"Kimera\": https://github.com/MIT-SPARK/Kimera\nRosinol, Antoni, et al. \"Kimera: an open-source library for real-time metric-semantic localization and mapping.\" arXiv preprint arXiv:1910.02490 (2019).\n7_Localization\n\ud83d\udccc Localization on 2D Occupancy Grid Map\nAMCL: Adaptive (or KLD-sampling) Monte Carlo Localization: http://wiki.ros.org/amcl\nS. Thrun, W. Burgard, and D. Fox. Probabilistic Robotics. MIT Press, 2005.\nmrpt_localization: http://wiki.ros.org/mrpt_localization\nJ.L. Blanco, J. Gonzalez-Jimenez, J.A. Fernandez-Madrigal, \"Optimal Filtering for Non-Parametric Observation Models: Applications to Localization and SLAM\", The International Journal of Robotics Research (IJRR), vol. 29, no. 14, 2010.\nJ. Gonzalez-Jimenez, J.L. Blanco, C. Galindo, A. Ortiz-de-Galisteo, J.A. Fernandez-Madrigal, F.A. Moreno, J. Martinez, \"Mobile Robot Localization based on Ultra-Wide-Band Ranging: A Particle Filter Approach\", Robotics and Autonomous Systems, vol. 57, no. 5, pp. 496--507, 2009.\n\ud83c\udf32 SLAM Algorithms Support Pure Localization:\nCartographer, ORB_SLAM2, RTAB-Map\n8_Mapping\n\ud83d\udccd Basic Mapping Backbones\n\"2D Occupancy Grid Map\" (Binary or Probability)\n\"3D Occupancy Grid Map\" (Binary or Probability)\n\"Octomap\": https://octomap.github.io/ (for collision checking)\nAn Efficient Probabilistic 3D Mapping Framework Based on Octrees / 3D Probability Occupancy Grid Map\nHornung, Armin & Wurm, Kai & Bennewitz, Maren & Stachniss, Cyrill & Burgard, Wolfram, \"OctoMap: An efficient probabilistic 3D mapping framework based on octrees. Autonomous Robots.\", Autonomous Robots Journal (2013). 34. 10.1007/s10514-012-9321-0.\n\ud83d\uddfa Basic Mapping Methods\n\"map_server\": http://wiki.ros.org/map_server (loading, saving)\n\"octomap_server\": http://wiki.ros.org/octomap_server (loading, saving, mapping)\n\ud83d\udccd Advanced 3D Mapping Backbones\n\"Surfels\"\nPfister, Hanspeter, et al. \"Surfels: Surface elements as rendering primitives.\" Proceedings of the 27th annual conference on Computer graphics and interactive techniques. 2000.\n\"Truncated Signed Distance Function (SDF)\"\nCurless, Brian, and Marc Levoy. \"A volumetric method for building complex models from range images.\" Proceedings of the 23rd annual conference on Computer graphics and interactive techniques. 1996.\n\"Truncated Signed Distance Function (TSDF)\"\nR. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, A. J.Davison, P. Kohi, J. Shotton, S. Hodges, and A. Fitzgibbon, \u201cKinect-fusion: Real-time dense surface mapping and tracking,\u201d in Mixed and augmented reality (ISMAR), 2011 10th IEEE international symposiumon, pp. 127\u2013136, IEEE, 2011\n\"Euclidean Signed Distance Fields (ESDF)\" (for collision checking)\nRatliff, Nathan, et al. \"CHOMP: Gradient optimization techniques for efficient motion planning.\" 2009 IEEE International Conference on Robotics and Automation. IEEE, 2009.\n\ud83d\uddfa Advanced 3D Mapping Methods\n\"voxblox (ESDF&TSDF based)\": https://github.com/ethz-asl/voxblox\nHelen Oleynikova, et al. \u201cVoxblox: Incremental 3D Euclidean Signed Distance Fields for On-Board MAV Planning\u201d, in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017.\n\"OpenChisel (TSDF based)\": https://github.com/personalrobotics/OpenChisel\nKlingensmith, Matthew, et al. \"Chisel: Real Time Large Scale 3D Reconstruction Onboard a Mobile Device using Spatially Hashed Signed Distance Fields.\" Robotics: science and systems. Vol. 4. 2015.\n\"DenseSurfelMapping (Surfel based)\": https://github.com/HKUST-Aerial-Robotics/DenseSurfelMapping\nWang, Kaixuan, Fei Gao, and Shaojie Shen. \"Real-time scalable dense surfel mapping.\" 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019.\n9_Navigation\n\ud83d\ude97 ROS Navigation Stack (move_base architecture) https://github.com/ros-planning/navigation\n\"move_base\": http://wiki.ros.org/move_base\n\"move_base_flex\": http://wiki.ros.org/move_base_flex\n\ud83d\ude98 Global Planner\nglobal_planner, carrot_planner, navfn, sbpl_lattice_planner, srl_global_planner, voronoi_planner\n\"A* (A Star)\"\nHart, Peter E., Nils J. Nilsson, and Bertram Raphael. \"A formal basis for the heuristic determination of minimum cost paths.\" IEEE transactions on Systems Science and Cybernetics 4.2 (1968): 100-107.\n\"Dijkstra's\"\nDijkstra, Edsger W. \"A note on two problems in connexion with graphs.\" Numerische mathematik 1.1 (1959): 269-271.\n\ud83d\ude98 Local Planner\ndwa_local_planner, teb_local_planner, base_local_planner, eband_local_planner, robotino_local_planner, asr_ftc_local_planner, simple_local_planner\n\"Timed Elastic Band (TEB)\": http://wiki.ros.org/teb_local_planner\nC. R\u00f6smann, W. Feiten, T. W\u00f6sch, F. Hoffmann and T. Bertram: Efficient trajectory optimization using a sparse model. Proc. IEEE European Conference on Mobile Robots, Spain, Barcelona, Sept. 2013, pp. 138\u2013143.\nC. R\u00f6smann, F. Hoffmann and T. Bertram: Integrated online trajectory planning and optimization in distinctive topologies, Robotics and Autonomous Systems, Vol. 88, 2017, pp. 142\u2013153.\nC. R\u00f6smann, F. Hoffmann and T. Bertram: Kinodynamic Trajectory Optimization and Control for Car-Like Robots, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Vancouver, BC, Canada, Sept. 2017.\n\"Dynamic Window Approach (DWA)\": http://wiki.ros.org/dwa_local_planner\nD. Fox, W. Burgard and S. Thrun, \"The dynamic window approach to collision avoidance,\" in IEEE Robotics & Automation Magazine, vol. 4, no. 1, pp. 23-33, March 1997.\n\ud83d\ude98 Advanced Local Planner\nVelocity Obstacle (VO)\nFiorini, Paolo, and Zvi Shiller. \"Motion planning in dynamic environments using velocity obstacles.\" The International Journal of Robotics Research 17.7 (1998): 760-772.\nReciprocal Velocity Obstacle (RVO) http://gamma.cs.unc.edu/RVO2/ | https://github.com/daenny/collvoid\nVan den Berg, Jur, Ming Lin, and Dinesh Manocha. \"Reciprocal velocity obstacles for real-time multi-agent navigation.\" 2008 IEEE International Conference on Robotics and Automation. IEEE, 2008.\nOptimal Reciprocal Collision Avoidance (ORCA) http://gamma.cs.unc.edu/ORCA/\nVan Den Berg, Jur, et al. \"Reciprocal n-body collision avoidance.\" Robotics research. Springer, Berlin, Heidelberg, 2011. 3-19.\n\ud83d\ude98 Recovery Behavior\nrotate_recovery, move_slow_and_clear, stepback_and_steerturn_recovery\n\ud83c\udfce\ufe0f Novel Navigation Strategy\n\"MIT AerospaceControlsLab DRL navigation\": http://acl.mit.edu/projects/socially-acceptable-navigation\nChen, Y. F., Liu, S.-Y., Liu, M., Miller, J., and How, J. P., \u201cMotion Planning with Diffusion Maps,\u201d IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, Korea: 2016.\nChen, Yu Fan, et al. \"Decentralized non-communicating multiagent collision avoidance with deep reinforcement learning.\" 2017 IEEE international conference on robotics and automation (ICRA). IEEE, 2017.\nChen, Yu Fan, et al. \"Socially aware motion planning with deep reinforcement learning.\" 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2017.\nM. Everett, et al. \"Motion Planning Among Dynamic, Decision-Making Agents with Deep Reinforcement Learning,\" 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Madrid, Spain, 2018\n\"Google AI Research PRM-RL navigation\": https://ai.googleblog.com/2019/02/long-range-robotic-navigation-via.html\nA. Faust et al. \"PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-Based Planning,\" 2018 IEEE International Conference on Robotics and Automation (ICRA), Brisbane, QLD, 2018, pp. 5113-5120.\nH. L. Chiang, et al. \"Learning Navigation Behaviors End-to-End With AutoRL,\" in IEEE Robotics and Automation Letters, vol. 4, no. 2, pp. 2007-2014, April 2019.\nFrancis, Anthony, et al. \"Long-range indoor navigation with PRM-RL.\" IEEE Transactions on Robotics (2020).\n\"ETHz Autonomous System Lab navigation\": https://www.youtube.com/watch?v=GPp5mnybm8g | https://www.youtube.com/watch?v=h1rm0BW3eVE | https://www.youtube.com/watch?v=ZedKmXzwdgI\nPfeiffer, Mark, et al. \"Predicting actions to act predictably: Cooperative partial motion planning with maximum entropy models.\" 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2016.\nPfeiffer, Mark, et al. \"From perception to decision: A data-driven approach to end-to-end motion planning for autonomous ground robots.\" 2017 ieee international conference on robotics and automation (ICRA). IEEE, 2017.\n\ud83d\udec0 Coverage Navigation (cleaning or weeding robot)\n\"Survey Paper\":\nGalceran, Enric, and Marc Carreras. \"A survey on coverage path planning for robotics.\" Robotics and Autonomous systems 61.12 (2013): 1258-1276.\n10_Manipulation\n\ud83d\udd90\ufe0f ROS Moveit (move_group architecture) https://github.com/ros-planning/moveit.git\n\ud83d\udcda Planner Library\n\"Open Motion Planning Library (OMPL)\"\nWebsite: https://ompl.kavrakilab.org/\nIntro: https://moveit.ros.org/assets/pdfs/2013/icra2013tutorial/OMPLoverview-ICRA2013.pdf\nRoadmap Based Planner: PRM(Probabilistic roadmap), PRM*, Lazy-PRM, LazyPRM*\nTree Based Planner: RRTConnect (default), RRT(Rapidly-exploring random tree), RRT*, T-RRT, Bi-TRRT, LB-TRRT, SBL, STRIDE, KPIECE, B-KPIECE, LB-KPIECE, EST, Bi-EST, Proj-EST, PDST, SPARS, SPARS2\n\"Search Based Planning Library (SBPL)\"\nWebsite: http://www.sbpl.net/\nIntro: https://www.cs.cmu.edu/~maxim/files/tutorials/robschooltutorial_oct10.pdf\nSearch Based Planner: ARA*, Anytime D*, R*\n\"Covariant Hamiltonian Optimiza-tion for Motion Planning (CHOMP)\"\nIntro: https://www.ri.cmu.edu/pub_files/2009/5/icra09-chomp.pdf\nOrocos Kinematics and Dynamics Library (KDL) for FK/IK modeling\nhttps://www.orocos.org/kdl.html\n11_Others_Non_Tech_Part\n11-1_Famous Robotics Related Company\n\ud83c\udfec Robotic Companies\ncategories companies\nResearch center Toyota_Research_Institute(TRI), Microsoft_Research, Google_AI, DeepMind, Facebook_Artificial_Intelligence_Research(FAIR), Berkeley_Artificial_Intelligence_Research (BAIR), Nvidia_Research\nManipulator ABB, FANUC, KUKA, YASKAWA, Techman_Robot, HIWIN, Universal_Robots, Innfos\nMobile Robot(AGV, base only) Omron_Robotics, Clearpath_Robotics&OTTO_Motors, Amazon_Robotics(Kiva_System/Canvas_Tech), Yujin_Robotics, ROBOTIS, Fetch_Robotics, GreenTrans, KUKA, iRobot, Pal_Robotics, Robotnik\nService robot(with torso) Willow_Garage, Softbank_Robotics, Fetch_Robotics, Pal_Robotics, Innfos, Robotnik\nDual Arms ABB, Rethink_Robotics\nHumanoid Boston_Dynamics, Softbank_Robotics, Pal_Robotics, UBTECH_Robotics\nQuadruped Boston_Dynamics, Unitree_Robotics, MIT_Cheetah, ANYrobotics(ANYmal), Standford\uff3fDoggo, Innfos\nResearch Robot Willow_Garage(Pr2), Facebook(pyrobot), ROBOTIS(turtlebot3), Fetch_Robotics, Robotnik(RB-1)\nEducational Robot Kit Trossen_Robotics, Niryo, Oz_Robotics\nDrone Dji, Tello\nROS2.0 ADLINK(DDS), ROBOTIS(Turtlebot3)\nCleaningBot iRobot, Xiaomi\nGripper ROBOTIQ, TOYO\nSelf-Driving Cars Alphabet_Waymo, Uber_ATG, Apple_Project_Titan, Tesla, Toyota_Research_Institute(TRI), Baidu_Apollo, AutoX\nDelivery Robots Starship, Amazon_Robotics_Scout(Dispatch)\n11-2_Famous Robotics Publications\n\ud83d\udcdd Top conferences:\nIEEE International Conference on Robotics and Automation (ICRA)\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n\ud83c\udfe0 Related Societies:\nSociety Website Conferences / Transactions\nIEEE Robotics and Automation Society (RAS) https://www.ieee-ras.org/ https://ras.papercept.net/conferences/scripts/start.pl\nIEEE Industrial Electronics Society (IES) http://www.ieee-ies.org/ http://www.ieee-ies.org/conferences\nIEEE Control System Society (CSS) http://ieeecss.org/ http://ieeecss.org/conferences/general-information\nIEEE Systems, Man and Cybernetics (SMC) https://www.ieeesmc.org/ https://www.ieeesmc.org/conferences/calendar/\nAAAS Science Robotics https://robotics.sciencemag.org/ https://www.sciencemag.org/journals/robotics/call-for-papers\nConference on Robot Learning (CoRL) https://www.robot-learning.org https://www.robot-learning.org/program/paper-explorer\n\ud83d\udee0 Tools:\n\"Google Scholar Rank\": https://scholar.google.com/citations?view_op=top_venues&hl=en&vq=eng_robotics (h5-index, h5-median)\n\"Journal Citation Reports (JCR)\": https://jcr.clarivate.com/ (Impact Factor, Eigenfactor Score, Article Influence Score)\n\"Compress PDF Online\": https://www.pdf2go.com/compress-pdf\n11-3_Famous Robotics Competition\n\ud83c\udf0e Global:\n\"DARPA Robotics Challenge\": https://en.wikipedia.org/wiki/DARPA_Robotics_Challenge\n\"RoboCup\": https://en.wikipedia.org/wiki/RoboCup\n\"Amazon Robotics/Picking Challenge\": http://amazonpickingchallenge.org/\n\"ICRA Robot Competitions: including lots of competitions would be different every years\"\n\"IROS Robot Competitions: including lots of competitions would be different every years\"\n\ud83c\uddf9\ud83c\uddfc Taiwan:\n\"SKS \u65b0\u5149\u4fdd\u5168\u667a\u6167\u578b\u4fdd\u5168\u6a5f\u5668\u4eba\u7af6\u8cfd\": https://www.facebook.com/sksrobot/\n\"PMC \u5168\u570b\u667a\u6167\u6a5f\u5668\u4eba\u7af6\u8cfd Robot competition\": http://www.pmccontest.com/\n\"HIWIN \u4e0a\u9280\u667a\u6167\u6a5f\u68b0\u624b\u5be6\u4f5c\u7af6\u8cfd\": http://www.hiwin.org.tw/Awards/HIWIN_ROBOT/Original.aspx\n\"SiliconAwards \u65fa\u5b8f\u91d1\u77fd\u734e\": http://www.mxeduc.org.tw/SiliconAwards/\n11-4_Famous ROS Organizations & Activities\n\ud83d\ude80 ROS Related Work:\n\"ROS-industrial\": https://rosindustrial.org/\n\"ROS2.0\": https://design.ros2.org/\n\"ROS-H\": https://acutronicrobotics.com/technology/H-ROS/\"\n\ud83c\udfe2 Organizations/Communities:\n\"Open Source Robotics Foundation (OSRF)\": https://www.openrobotics.org/\n\"Open Source Robotics Corporation (OSRC)\": https://www.openrobotics.org/\n\"ROS.Taiwan\": https://www.facebook.com/groups/ros.taiwan/\n\"ROS.Taipei\": https://www.facebook.com/groups/ros.taipei/\n\ud83c\udfaa Activities:\n\"ROScon\": https://roscon.ros.org/\n\"ROSDevCon\": http://www.rosdevcon.com/\n\"ROS Summer School(CN)\": http://www.roseducation.org/\n\"ROS Summer School(TW)\": http://www.taoyuan-ros.com.tw/\nLicense", "link": "https://github.com/shannon112/awesome-ros-mobile-robot", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "awesome-ros-mobile-robot\nthis repository provides some useful resources and informations about autonomous mobile robots (amr) research based on ros. it would mainly focus on basic function of mobile robots(like odometry, slam, navigation and manipulation).\n(including both chinese and english materials)\nhttp://wiki.ros.org/distributions\nindex:\n0.robotics\n1.robot-operating-system(ros)\n2.robotic-platform\n3.robotic-sensing\n4.calibration\n5.odometry\n6.slam\n7.localization\n8.mapping\n9.navigation\n10.manipulation\n11.others (non-tech)\n11-1. famous robotics company\n11-2. famous robotics conference&journal\n11-3. famous robotics competition in taiwan\n11-4. famous ros organizations & activities\n0_robotics\n\ud83d\udcda books\n\"introduction to algorithms\", thomas h. cormen, charles e. leiserson, ronald l. rivest, clifford stein\n\"multiple view geometry in computer vision\", richard hartley, andrew zisserman\n\"probabilistic robotics\", sebastian thrun\n\"introduction to linear algebra\", five edition, gilbert strang\n\"pattern recognition and machine learning\", christopher m. bishop\n\"introduction to autonomous mobile robots\" siegwart, roland, illah reza nourbakhsh, and davide scaramuzza\n\"\u8996\u89ba slam \u5341\u56db\u8b1b\uff1a\u5f9e\u7406\u8ad6\u5230\u5be6\u8e10\", \u9ad8\u7fd4\n\ud83d\udcd6 courses\n\"5 minutes with cyrill\" {cyrill stachniss} cyrill stachniss\nhttps://www.youtube.com/playlist?list=plgnqpqtftogso8hc48k9spunliy1qxzv9\n\"matlab lecture\" {matlab}\nhttps://www.youtube.com/user/matlab/playlists\n\"control system lecture\" {brian douglas} brian douglas\nhttps://www.youtube.com/user/controllectures/playlists\n\"robotics sensing related lecture\" {cyrill stachniss} cyrill stachniss\nhttps://www.youtube.com/c/cyrillstachniss/playlists\n\"robot mapping\" {universit\u00e4t of freiburg} cyrill stachniss\nhttp://ais.informatik.uni-freiburg.de/teaching/ws13/mapping/\n\"introduction to mobile robotics\" {universit\u00e4t of freiburg} wolfram burgard, et al.\nhttp://ais.informatik.uni-freiburg.de/teaching/ss13/robotics/\n\"robotics (1)\" {ntu} pei chun lin\nhttps://www.coursera.org/learn/robotics1, http://peichunlin.me.ntu.edu.tw/homepage/intro2robotics.htm\n\"control of mobile robots\" {georgia tech} magnus egerstedt\nhttps://www.coursera.org/learn/mobile-robot\"\n\"modern robotics: mechanics, planning, and control\" {northwestern university} kevin lynch\nhttps://www.coursera.org/specializations/modernrobotics\n\"robotics\" {upenn} vijay kumar, et al.\nhttps://zh-tw.coursera.org/specializations/robotics\n\"linear algebra\" {ntu} hung-yi lee\nhttp://speech.ee.ntu.edu.tw/~tlkagk/courses_la18.html\n\"linear algebra\" {mit} gilbert strang\nhttps://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/\n\"machine learning\" {ntu} hung-yi lee\nhttp://speech.ee.ntu.edu.tw/~tlkagk/courses_ml19.html\n\"machine learning\" {stanford} andrew ng\nhttps://www.coursera.org/learn/machine-learning\n\"probabilistic systems analysis and applied probability\" {mit} john tsitsiklis\nhttps://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/\n\"deep reinforcement learning\" {ucb} sergey levine\nhttp://rail.eecs.berkeley.edu/deeprlcourse/\n\"vision algorithms for mobile robotics\" {ethz} d. scaramuzza\nhttp://rpg.ifi.uzh.ch/teaching.html\n\"self-driving cars\" {toronto}\nhttps://www.coursera.org/specializations/self-driving-cars\n\ud83d\udcdc paper libraries\n\"ieee xplore digital library\": https://ieeexplore.ieee.org/xplore/home.jsp\n\"arxiv.org e-print archive\": https://arxiv.org/\n\"google scholar\": https://scholar.google.com/\n\"sci-hub\": https://sci-hub.tw/\n\"airiti library \": http://www.airitilibrary.com/home/index/\n\"national digital library of theses and dissertations in taiwan\": https://ndltd.ncl.edu.tw\n1_robot_operating_system\n\ud83d\udc7e ros official website\n\"the ros status\": https://status.ros.org/ (check if any server is down)\n\"the ros website\": https://www.ros.org (home)\n\"the ros wiki\": https://wiki.ros.org (pkg info)\n\"the ros documentation site\": http://docs.ros.org (msg info)\n\"the ros answer\": https://answers.ros.org/questions/ (q&a)\n\ud83d\udde3 ros related blogs & channels & forums\n\"the construct\": https://www.youtube.com/channel/uct6lag-vv25ftx3e11mvy1q\n\"jetsonhacks\": https://www.youtube.com/channel/ucqs0lwv6e4p7lqagj6fgy5q\n\"\u9ce5\u54e5\u7684linux\u79c1\u623f\u83dc\": http://linux.vbird.org/\n\"\u534a\u9592\u5c45\u58eb\"\uff1a https://www.cnblogs.com/gaoxiang12/\n\"\u6ce1\u6ce1\u6a5f\u5668\u4eba\u983b\u9053\": https://space.bilibili.com/38737757/\n\"\u6ce1\u6ce1\u6a5f\u5668\u4eba\u8ad6\u58c7\": http://paopaorobot.org/bbs/\n\ud83d\udcda books\n\"c++ primer\", stanley b. lippman, jos\u00e9e lajoie, barbara e. moo\n\"c++ concurrency in action, 2nd edition\", anthony williams\n\"design patterns: elements of reusable object-oriented software\", the \"gang of four\": erich gamma, richard helm, ralph johnson, john vlissides\n\"head first design patterns, 2nd edition\", eric freeman, elisabeth robson\n\"clean code: a handbook of agile software craftsmanship\", robert c. martin\n\"ros by example\", python, patrick goebel\n\"mastering ros for robotics programming\", lentin joseph\n\"learning ros for robotics programming\", enrique fernandez et al.\n\"programming robots with ros: a practical introduction to the robot operating system\", morgan quigley et al.\n\"\u6a5f\u5668\u4eba\u4f5c\u696d\u7cfb\u7d71ros \u6dfa\u6790\", jason m. o'kane, \u8096\u8ecd\u6d69\u8b6f\n\"\u6a5f\u5668\u4eba\u4f5c\u696d\u7cfb\u7d71ros \u53f2\u8a7136\u7bc7\", \u5f35\u65b0\u5b87, http://www.roseducation.org/docs/ros_history.pdf\n\ud83e\udd1d system integration -----> tool !!! \nmultiple machines with multi-master: http://wiki.ros.org/multimaster_fkie\nmultiple machines with one master: http://wiki.ros.org/ros/networksetup\nmultiple tasks with smach(state machine): http://wiki.ros.org/smach\nbridge for non-ros programs(json api): https://wiki.ros.org/rosbridge_suite\nbridge communication between ros 1 and ros 2: https://github.com/ros2/ros1_bridge\n2_robotic_platform\n\ud83e\udd16 ros robot overview\n\"aerial, ground, marine, manipulator, component\": https://robots.ros.org/\n\ud83d\ude98 wheel robot configurations\n(ref: siegwart, roland, illah reza nourbakhsh, and davide scaramuzza. introduction to autonomous mobile robots. mit press, 2011, table 2.1, p.34~36)\n\ud83d\ude97 race car projects\n\"mit\": https://mit-racecar.github.io\n\"penn\": http://f1tenth.org/ [without slam, nav]\n\"ucb\": http://www.barc-project.com/projects/ [without laser]\n\"georgia tech\": https://github.com/autorally [for outdoor]\n\"taiwan hypharos\": https://github.com/hypha-ros/hypharos_racecar\n\ud83e\udd16 ros mobile robot github\n\"turtlebot\": https://github.com/turtlebot\n\"turtlebot3\": https://github.com/robotis-git/turtlebot3\n\"clearpath husky\": https://github.com/husky\n\"clearpath jackel\": https://github.com/jackal\n\"robotnik xl-gen\": https://github.com/robotnikautomation/summit_xl_sim or summit_xl_common\n\"robotnik rb-kairos\": https://github.com/robotnikautomation/rbkairos_sim or rbkairos_common\n\ud83e\udd16 ros mobile manipulator github\n\"personal robot 2 (pr2)\": https://github.com/pr2\n\"kuka youbot\": https://github.com/youbot\n\"fetch robotics\": https://github.com/fetchrobotics\n\"clearpath husky+ur5\": http://www.clearpathrobotics.com/assets/guides/husky/huskymanip.html\n\"clearpath husky+dualur5\": http://www.clearpathrobotics.com/assets/guides/husky/huskydualmanip.html\n\"robotnik rb-1\": https://github.com/robotnikautomation/rb1_sim or rb1_common\n\ud83e\udd16 ros manipulator github\n\"franka emika panda\": https://github.com/frankaemika/franka_ros | https://github.com/ros-planning/panda_moveit_config\n\"universal robot 3/5/10/e\": https://github.com/ros-industrial/universal_robot\n\"techman robot\": https://github.com/kentsai0319/techman_robot\n\ud83d\udcbb processing unit (sbc/ipc)\nraspberry pi(rpi), beaglebone black(bbb), odroid xu4, odroid n2, asus tinker board\nnvidia jetson tx1, nvidia jetson tx2, nvidia jetson nano, nvidia jetson xavier\nadlink neuron,\n\ud83d\udd79 motor & controller & encoder\nelmo motion control ltd\nrls d.o.o. (rotary and linear motion sensors)\ndr. fritz faulhaber gmbh & co. kg\nmaxon group motors & drivers\ndexmart motors & drivers (trumman technology corp)\n3_robotic_sensing\n\ud83d\udcf7 rgb camera\n\"usb camera\": http://wiki.ros.org/usb_cam\n\"gstream-based camera\": http://wiki.ros.org/gscam\n\"opencv camera\": http://wiki.ros.org/cv_camera\n\ud83d\udcf8 rgb-d camera\n\"microsoft kinectv1 with openni\": https://github.com/ros-drivers/openni_camera\n\"microsoft kinectv1 with freenect\": https://github.com/ros-drivers/freenect_stack\n\"microsoft kinect one/v2\": https://github.com/code-iai/iai_kinect2\n\"asus xtion with openni2\": https://github.com/ros-drivers/openni2_camera\n\"intel realsense d455/d435/d435i/d415\": https://github.com/intel-ros/realsense\n\"occipital structure sensor/core\": https://structure.io/\n\ud83c\udfa5 stereo camera\n\"stereolabs zed\": http://wiki.ros.org/zed-ros-wrapper\n\"carnegie robotics multisense\u2122 s7\": http://docs.carnegierobotics.com/s7/\n\"e-con systems tara stereo camera\": https://github.com/dilipkumar25/see3cam\n\"nerian sp1\": http://wiki.ros.org/nerian_sp1\n\ud83d\udd26 laser rangefinder [laser scanners] [scanning rangefinder]\n\u2013 often represent 2d laser scanning\n\"hokuyo_urg\": http://wiki.ros.org/urg_node (old: http://wiki.ros.org/hokuyo_node\n\"hokuyo_utm\": http://wiki.ros.org/urg_node (old: http://wiki.ros.org/hokuyo_node\n\"ydlidar\": https://github.com/ydlidar/ydlidar_ros\n\"rplidar\": http://wiki.ros.org/rplidar\n\"sick\": http://wiki.ros.org/sick_scan\n\ud83d\udca1 lidar [light detection and ranging] [light imaging, detection, and ranging] [3d laser scanning ]\n\u2013 often represent 3d laser scanning\n\"velodyne\": http://wiki.ros.org/velodyne\n\"livox\": https://github.com/hku-mars/loam_livox\n\ud83c\udf4e imu [inertial measurement unit]\n\"xsense\": http://wiki.ros.org/xsens_driver\n\"microstrain 3dm-gx2\": http://wiki.ros.org/microstrain_3dmgx2_imu\n\"sparkfun 9dof razor imum0\": http://wiki.ros.org/razor_imu_9dof\n\ud83d\udea8 3d scanning & novel sensing device\n\"kaarta\": https://www.kaarta.com/\n\"matterport\": https://matterport.com/\n\"microsoft azure-kinect-dk\": https://azure.microsoft.com/zh-tw/services/kinect-dk/\n\"intel realsense tracking camera t265\": https://www.intelrealsense.com/tracking-camera-t265/\n\"intel realsense lidar camera l515\": https://www.intelrealsense.com/lidar-camera-l515/\n\ud83c\udf99 microphone array\n\"respeaker mic array v2.0\": http://wiki.seeedstudio.com/respeaker_mic_array_v2.0/\n\ud83d\udd0a text to speech (tts)\n\"gtts\": https://pypi.org/project/gtts/\n\"sound_play\": http://wiki.ros.org/sound_play\n\ud83d\udde3 speech recognition / speech to text (stt)\n\"speechrecognition\": https://pypi.org/project/speechrecognition/\n\ud83d\ude80 vocal assistant\n\"amazon alexa\": https://www.amazon.com/amazon-echo-and-alexa-devices/b?ie=utf8&node=9818047011\n\"google nest\": https://store.google.com/product/google_nest_mini\n\"apple homepod\": https://www.apple.com/tw/shop/buy-homepod/homepod/\n\"mi ai speaker\": https://www.mi.com/aispeaker\n\"asus smart speaker\": https://www.asus.com/tw/asus-smart-speaker/asus-smart-speaker-xiao-bu/\n\"pyaiml -- the python aiml interpreter\": https://github.com/cdwfs/pyaiml\n\ud83d\udc7e matrix barcode (fiducial marker systems, or artag, or auxiliary marker)\n\"artag\": http://wiki.ros.org/ar_track_alvar\n\"apriltag\": http://wiki.ros.org/apriltag_ros\n\"caltag\": http://www.cs.ubc.ca/labs/imager/tr/2010/atcheson_vmv2010_caltag/\n\"comparison\": sagitov, artur, et al. \"artag, apriltag and caltag fiducial marker systems: comparison in a presence of partial marker occlusion and rotation.\" icinco (2). 2017.\n\ud83d\udd05 learning-based feature extractor\nalexnet, vgg, resnet, inceptionv3, densenet, googlenet, mobilenet, squeezenet, etc.\n\"pytorch implementation\": https://pytorch.org/docs/stable/torchvision/models.html\n\ud83d\udd05 learning-based object detection\n\"faster r-cnn\"\nren, shaoqing, et al. \"faster r-cnn: towards real-time object detection with region proposal networks.\" advances in neural information processing systems. 2015.\n\"ssd\"\nliu, wei, et al. \"ssd: single shot multibox detector.\" european conference on computer vision. springer, cham, 2016.\n\"yolov3\": https://github.com/leggedrobotics/darknet_ros\n(v4) bochkovskiy, alexey, chien-yao wang, and hong-yuan mark liao. \"yolov4: optimal speed and accuracy of object detection.\" arxiv preprint arxiv:2004.10934 (2020).\n(v3) redmon, joseph, and ali farhadi. \"yolov3: an incremental improvement.\" arxiv preprint arxiv:1804.02767 (2018).\n(v2) redmon, joseph, and ali farhadi. \"yolo9000: better, faster, stronger.\" proceedings of the ieee conference on computer vision and pattern recognition. 2017.\n(v1) redmon, joseph, et al. \"you only look once: unified, real-time object detection.\" proceedings of the ieee conference on computer vision and pattern recognition. 2016.\n\ud83d\udd05 learning-based human pose estimation\n\"openpose\": https://github.com/cmu-perceptual-computing-lab/openpose\n\"openpose-plugin\": https://github.com/ildoonet/tf-pose-estimation\n4_calibration\n\ud83d\udcf7 camera calibration (intrinsic and extrinsic parameters)\n\"camera_calibration\": http://wiki.ros.org/camera_calibration\n\"format converter\": http://wiki.ros.org/camera_calibration_parsers\n\ud83d\udc41 hand-eye calibration\n\"easy_handeye\": https://github.com/ifl-camp/easy_handeye\n\ud83c\udf4e imu (sparkfun razer 9dof-razor-imu-m0) calibration\n\"github wiki\": https://github.com/razor-ahrs/razor-9dof-ahrs/wiki/tutorial\n\"ros wiki\": http://wiki.ros.org/razor_imu_9dof\n\"sparkfun official guide\": https://learn.sparkfun.com/tutorials/9dof-razor-imu-m0-hookup-guide/all\n\"my calibration guide\": https://github.com/shannon112/imu_calibration/blob/master/readme.md\n5_odometry\n\u2620\ufe0e visual based ego-motion backbone\ncomponents\nfeature keypoint & desciptor - surf, sift, orb\nfeature matching - brute-force, flann\nhttps://docs.opencv.org/3.4/db/d27/tutorial_py_table_of_contents_feature2d.html\noptical flow - lucas-kanade (lk)\nmotion estimation:\n2d-2d: epipolar geometry & triangulation\n2d-3d: perspective-n-point (pnp) - p3p, dlt, epnp, upnp, ba\n3d-3d: iterative closest point (icp) - icp(svd), gicp, ndt, ipda, non-linearopt, point2point, point2plane\ndirect method - dense, semi-dense, sparse\nsolutions\nextract feature keypoint -> desciptor -> matching -> motion estimation\nextract feature keypoint -> optical flow -> motion estimation\nextract feature keypoint -> sparse direct method\nsemi-dense/dense direct method\n\ud83d\udcda odometry survey paper\ndelmerico, jeffrey, and davide scaramuzza. \"a benchmark comparison of monocular visual-inertial odometry algorithms for flying robots.\" 2018 ieee international conference on robotics and automation (icra). ieee, 2018.\ng. huang, \"visual-inertial navigation: a concise review,\" 2019 international conference on robotics and automation (icra), montreal, qc, canada, 2019, pp. 9572-9582.\n\ud83c\udfc6 odometry algorithm ranking\n\"kitti\": http://www.cvlibs.net/datasets/kitti/eval_odometry.php\n\ud83d\ude96 wheel odometry\n\"ros_control\": http://wiki.ros.org/ros_control\nchitta, sachin, et al. \"ros_control: a generic and simple control framework for ros.\" (2017).\n\ud83d\udca1 2d laser based odometry\n\"rf2o\": https://github.com/mapirlab/rf2o_laser_odometry\nm. jaimez, j. monroy, j. gonzalez-jimenez, planar odometry from a radial laser scanner. a range flow-based approach, ieee international conference on robotics and automation (icra), stockholm, sweden, pp. 4479-4485, 2016.\n\ud83d\udcf7 3d visual based odometry (mono)\n\"vins-mono\": https://github.com/hkust-aerial-robotics/vins-mono\nqin, tong, peiliang li, and shaojie shen. \"vins-mono: a robust and versatile monocular visual-inertial state estimator.\" ieee transactions on robotics 34.4 (2018): 1004-1020.\n\"svo\": https://github.com/uzh-rpg/rpg_svo | http://rpg.ifi.uzh.ch/svo2.html sparse direct method\nforster, christian, matia pizzoli, and davide scaramuzza. \"svo: fast semi-direct monocular visual odometry.\" 2014 ieee international conference on robotics and automation (icra). ieee, 2014.\n\"dso\": https://github.com/jakobengel/dso sparse direct method\nengel, jakob, vladlen koltun, and daniel cremers. \"direct sparse odometry.\" ieee transactions on pattern analysis and machine intelligence 40.3 (2017): 611-625.\n\"viso2\": http://wiki.ros.org/viso2_ros | http://www.cvlibs.net/software/libviso/\ngeiger, andreas, julius ziegler, and christoph stiller. \"stereoscan: dense 3d reconstruction in real-time.\" 2011 ieee intelligent vehicles symposium (iv). ieee, 2011.\nkitt, bernd, andreas geiger, and henning lategahn. \"visual odometry based on stereo image sequences with ransac-based outlier rejection scheme.\" 2010 ieee intelligent vehicles symposium. ieee, 2010.\n\"okvis\": https://github.com/ethz-asl/okvis | https://github.com/ethz-asl/okvis_ros\nleutenegger, stefan, et al. \"keyframe-based visual\u2013inertial odometry using nonlinear optimization.\" the international journal of robotics research 34.3 (2015): 314-334.\n\"rovio\": https://github.com/ethz-asl/rovio\nbloesch, michael, et al. \"robust visual inertial odometry using a direct ekf-based approach.\" 2015 ieee/rsj international conference on intelligent robots and systems (iros). ieee, 2015.\nbloesch, michael, et al. \"iterated extended kalman filter based visual-inertial odometry using direct photometric feedback.\" the international journal of robotics research 36.10 (2017): 1053-1072.\n\"rotrocc+, rotrocc, rocc, monorocc\"\nm. buczko and v. willert: flow-decoupled normalized reprojection error for visual odometry. 19th ieee intelligent transportation systems conference (itsc) 2016.\nm. buczko, v. willert, j. schwehr and j. adamy: self-validation for automotive visual odometry. ieee intelligent vehicles symposium (iv) 2018.\nm. buczko and v. willert: monocular outlier detection for visual odometry. ieee intelligent vehicles symposium (iv) 2017.\nm. buczko and v. willert: how to distinguish inliers from outliers in visual odometry for high-speed automotive applications. ieee intelligent vehicles symposium (iv) 2016.\n\ud83d\udcf8 3d rgb-d/stereo based odometry\n\"vins-fusion\": https://github.com/hkust-aerial-robotics/vins-fusion\nqin, tong, and shaojie shen. \"online temporal calibration for monocular visual-inertial systems.\" 2018 ieee/rsj international conference on intelligent robots and systems (iros). ieee, 2018.\n\"dvo\": https://github.com/tum-vision/dvo\nkerl, christian, j\u00fcrgen sturm, and daniel cremers. \"robust odometry estimation for rgb-d cameras.\" 2013 ieee international conference on robotics and automation. ieee, 2013. steinbr\u00fccker, frank, j\u00fcrgen sturm, and daniel cremers. \"real-time visual odometry from dense rgb-d images.\" 2011 ieee international conference on computer vision workshops (iccv workshops). ieee, 2011.\n\"soft\": https://github.com/mayankm96/stereo-odometry-soft\ncvi\u0161ic, igor, et al. \"soft-slam: computationally efficient stereo visual slam for autonomous uavs.\" journal of field robotics (2017).\ncvi\u0161i\u0107, igor, and ivan petrovi\u0107. \"stereo odometry based on careful feature selection and tracking.\" 2015 european conference on mobile robots (ecmr). ieee, 2015.\n\"viso2\": http://wiki.ros.org/viso2_ros | http://www.cvlibs.net/software/libviso/\ngeiger, andreas, julius ziegler, and christoph stiller. \"stereoscan: dense 3d reconstruction in real-time.\" 2011 ieee intelligent vehicles symposium (iv). ieee, 2011.\nkitt, bernd, andreas geiger, and henning lategahn. \"visual odometry based on stereo image sequences with ransac-based outlier rejection scheme.\" 2010 ieee intelligent vehicles symposium. ieee, 2010.\n\ud83d\udd05 3d lidar based odometry\n\"loam & v-loam\": https://github.com/laboshinl/loam_velodyne\nj zhang, s singh, \"loam: lidar odometry and mapping in real-time\", robotics: science and systems conference (rss 2014)\nj zhang, s singh, \"visual-lidar odometry and mapping: low-drift, robust, and fast\", ieee international conference on robotics and automation (icra)\nj. zhang, m. kaess and s. singh: real-time depth enhanced monocular odometry. ieee/rsj international conference on intelligent robots and systems (iros) 2014.\n\"limo\": https://github.com/johannes-graeter/limo\ngraeter, johannes, alexander wilczynski, and martin lauer. \"limo: lidar-monocular visual odometry.\" 2018 ieee/rsj international conference on intelligent robots and systems (iros). ieee, 2018.\n\ud83e\udd16 learning based odometry\n\"deepvo\": https://github.com/chiweihsiao/deepvo-pytorch | https://github.com/ildoonet/deepvo\ns. wang, r. clark, h. wen and n. trigoni, \"deepvo: towards end-to-end visual odometry with deep recurrent convolutional neural networks,\" 2017 ieee international conference on robotics and automation (icra), singapore, 2017, pp. 2043-2050.\n\"vinet\": https://github.com/htlife/vinet\nclark, ronald, et al. \"vinet: visual-inertial odometry as a sequence-to-sequence learning problem.\" aaai. 2017.\n\ud83c\udf65 odometry fusion\nekf | \"robot_pose_ekf\": http://wiki.ros.org/robot_pose_ekf\nekf & ukf | \"robot_localization\": http://docs.ros.org/melodic/api/robot_localization/html/index.html\nmoore, thomas, and daniel stouch. \"a generalized extended kalman filter implementation for the robot operating system.\" intelligent autonomous systems 13. springer, cham, 2016. 335-348.\n6_slam\n\ud83c\udfdb slam theorem & tutorial\nt. bailey and h. f. durrant-whyte, \u201csimultaneous localisation and map- ping (slam): part ii\u201d, ieee robot. auton. syst., vol. 13, no. 3, pp. 108\u2013117, 2006.\nh. f. durrant-whyte and t. bailey, \u201csimultaneous localisation and map- ping (slam): part i\u201d, ieee robot. autom. mag., vol. 13, no. 2, pp. 99\u2013110, jun. 2006\nstrasdat, hauke, jos\u00e9 mm montiel, and andrew j. davison. \"visual slam: why filter?.\" image and vision computing 30.2 (2012): 65-77. (comparison between filter and graph)\ngrisetti, giorgio, et al. \"a tutorial on graph-based slam.\" ieee intelligent transportation systems magazine 2.4 (2010): 31-43.\n\ud83d\udcda slam survey paper\ncesar cadena ; luca carlone ; henry carrillo ; yasir latif ; davide scaramuzza ; jos\u00e9 neira ; ian reid ; john j. leonard, \u201cpast, present, and future of simultaneous localization and mapping: towards the robust-perception age\u201d, ieee transactions on roboticsyear: 2016, volume: 32, issue: 6pages: 1309 - 1332\njamiruddin, redhwan, et al. \"rgb-depth slam review.\" arxiv preprint arxiv:1805.07696 (2018).\nzollh\u00f6fer, michael, et al. \"state of the art on 3d reconstruction with rgb\u2010d cameras.\" computer graphics forum. vol. 37. no. 2. 2018.\n\u2620\ufe0e slam backbone (back-end)\nkalman filter based\nkalman filter (kf), extend kalman filte (ekf), unscented kalman filte (ukf)\nextended information filter (eif), sparse extended information filter (seif)\nparticle filter based\ngmapping, fastslam, fastslam2.0\ngraph optimization based\nmethod: bundle adjustment(ba), pose graph, factor graph\nregression solution: gaussian netwon (gn), leverberg marquert(lm)\nefficiently solving: cholesky factorization, qr decomposition, conjugate gradients\nceres solver library: s. agarwal and m. keir. \"ceres solver.\" [online]. available: http://ceres-solver.org/\ng2o library: k\u00fcmmerle, rainer, et al. \"g 2 o: a general framework for graph optimization.\" 2011 ieee international conference on robotics and automation. ieee, 2011.\ngtsam: dellaert, frank. factor graphs and gtsam: a hands-on introduction. georgia institute of technology, 2012.\nisam: (1)kaess, m., ranganathan, a., and dellaert, f. (2008). isam: incremental smoothing and mapping.ieee trans. robotics, 24(6):1365\u20131378. (2)kaess, m., johannsson, h., roberts, r., ila, v., leonard, j., and dellaert, f. (2012). isam2:incremental smoothing and mapping using the bayes tree.intl. j. of robotics research, 31:217\u2013236. (isam2 is available as part of the gtsam)\nslam++: ila, viorela, et al. \"slam++-a highly efficient and temporally scalable incremental slam framework.\" the international journal of robotics research 36.2 (2017): 210-230.\nlearning based\n\ud83d\udcd0 slam benchmark (dataset)\n\"the kitti vision benchmark & dataset\": http://www.cvlibs.net/datasets/kitti/\ngeiger, andreas, philip lenz, and raquel urtasun. \"are we ready for autonomous driving? the kitti vision benchmark suite.\" 2012 ieee conference on computer vision and pattern recognition. ieee, 2012.\n\"mit stata center dataset\": https://projects.csail.mit.edu/stata/#\nfallon, maurice, et al. \"the mit stata center dataset.\" the international journal of robotics research 32.14 (2013): 1695-1699.\n\"radish dataset\": http://ais.informatik.uni-freiburg.de/slamevaluation/datasets.php\nhoward and n. roy, \u201cthe robotics data set repository (radish),\u201d2003. [online]. available: http://radish.sourceforge.net/\n\"tum rgb-d slam benchmark & dataset\": https://vision.in.tum.de/data/datasets/rgbd-dataset\nsturm, j\u00fcrgen, et al. \"a benchmark for the evaluation of rgb-d slam systems.\" 2012 ieee/rsj international conference on intelligent robots and systems. ieee, 2012.\n\"icl-nuim rgb-d benchmark & dataset\": https://www.doc.ic.ac.uk/~ahanda/vafric/iclnuim.html\na. handa, t. whelan, j. mcdonald, and a. j. davison, \u201ca bench-mark for rgb-d visual odometry, 3d reconstruction and slam,\u201d inrobotics and automation (icra), 2014 ieee international conferenceon. ieee, 2014, pp. 1524\u20131531.\n\"euroc mav dataset\": https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets\nburri, michael, et al. \"the euroc micro aerial vehicle datasets.\" the international journal of robotics research 35.10 (2016): 1157-1163.\n\"benchmark\"\nr.k \u0308ummerle, b.steder, c.dornhege, m.ruhnke, g.grisetti, c.stachniss, and a.kleiner, \"on measuring the accuracy of slam algorithms,\" autonomous robots, vol. 27, no. 4, pp. 387\u2013407, 2009.\n\"survey paper\"\ncai, ziyun, et al. \"rgb-d datasets using microsoft kinect or similar sensors: a survey.\" multimedia tools and applications 76.3 (2017): 4313-4355.\n\ud83d\udca1 2d laser based slam\n\"cartographer\": https://google-cartographer-ros.readthedocs.io/en/latest/\nwolfgang hess ; damon kohler ; holger rapp ; daniel andor, \u201creal-time loop closure in 2d lidar slam \u201d, 2016 ieee international conference on robotics and automation (icra), stockholm, 2016, pp. 1271-1278.\n\"gmapping\": http://wiki.ros.org/gmapping\ng. grisetti, c. stachniss and w. burgard, \"improved techniques for grid mapping with rao-blackwellized particle filters,\" ieee transactions on robotics, vol. 23, no. 1, pp. 34-46, feb. 2007.\n\"hector\": http://wiki.ros.org/hector_slam\ns. kohlbrecher, o. von stryk, j. meyer and u. klingauf, \"a flexible and scalable slam system with full 3d motion estimation,\" 2011 ieee international symposium on safety, security, and rescue robotics, kyoto, 2011, pp. 155-160.\n\"karto\": http://wiki.ros.org/slam_karto\nvincent, r., limketkai, b., & eriksen, m. (2010, april). comparison of indoor robot localization techniques in the absence of gps. in detection and sensing of mines, explosive objects, and obscured targets xv (vol. 7664, p. 76641z). international society for optics and photonics.\n\"fastslam\":\nmontemerlo, michael, et al. \"fastslam: a factored solution to the simultaneous localization and mapping problem.\" aaai/iaai 593598 (2002).\nmontemerlo, michael, et al. \"fastslam 2.0: an improved particle filtering algorithm for simultaneous localization and mapping that provably converges.\" ijcai. 2003.\n\ud83d\udcf7 3d visual based slam (mono)\n\"monoslam\": https://github.com/hanmekim/scenelib2 feature + ekf\ndavison, andrew j., et al. \"monoslam: real-time single camera slam.\" ieee transactions on pattern analysis and machine intelligence 29.6 (2007): 1052-1067.\n\"ptam\": http://www.robots.ox.ac.uk/~gk/ptam/ feature + ba\nklein, georg, and david murray. \"parallel tracking and mapping for small ar workspaces.\" 2007 6th ieee and acm international symposium on mixed and augmented reality. ieee, 2007.\n\"orb-slam\": https://github.com/raulmur/orb_slam2 feature + (ba + pose-graph)\nra\u00fal mur-artal, j. m. m. montiel and juan d. tard\u00f3s. orb-slam: a versatile and accurate monocular slam system. ieee transactions on robotics, vol. 31, no. 5, pp. 1147-1163, 2015.\ndorian g\u00e1lvez-l\u00f3pez and juan d. tard\u00f3s. bags of binary words for fast place recognition in image sequences. ieee transactions on robotics, vol. 28, no. 5, pp. 1188-1197, 2012.\n\"lsd-slam\": semi-dense direct method + pose-graph\nengel, jakob, thomas sch\u00f6ps, and daniel cremers. \"lsd-slam: large-scale direct monocular slam.\" european conference on computer vision. springer, cham, 2014.\n\ud83d\udcf8 3d rgb-d/stereo based slam\n\"dtam\": https://github.com/anuranbaka/opendtam\nnewcombe, richard a., steven j. lovegrove, and andrew j. davison. \"dtam: dense tracking and mapping in real-time.\" 2011 international conference on computer vision. ieee, 2011.\n\"orb-slam2\": https://github.com/raulmur/orb_slam2\nra\u00fal mur-artal and juan d. tard\u00f3s. orb-slam2: an open-source slam system for monocular, stereo and rgb-d cameras. ieee transactions on robotics, vol. 33, no. 5, pp. 1255-1262, 2017.\n\"orb-slam3\": https://github.com/uz-slamlab/orb_slam3\n[orb-slam3] carlos campos, richard elvira, juan j. g\u00f3mez rodr\u00edguez, jos\u00e9 m. m. montiel and juan d. tard\u00f3s, orb-slam3: an accurate open-source library for visual, visual-inertial and multi-map slam, under review.\n[imu-initialization] carlos campos, j. m. m. montiel and juan d. tard\u00f3s, inertial-only optimization for visual-inertial initialization, icra 2020.\n[orbslam-atlas] richard elvira, j. m. m. montiel and juan d. tard\u00f3s, orbslam-atlas: a robust and accurate multi-map system, iros 2019.\n[orbslam-vi] ra\u00fal mur-artal, and juan d. tard\u00f3s, visual-inertial monocular slam with map reuse, ieee robotics and automation letters, vol. 2 no. 2, pp. 796-803, 2017.\n\"dvo-slam\": https://github.com/tum-vision/dvo_slam\nkerl, christian, j\u00fcrgen sturm, and daniel cremers. \"dense visual slam for rgb-d cameras.\" 2013 ieee/rsj international conference on intelligent robots and systems. ieee, 2013.\n\"rgbdslamv2\": https://felixendres.github.io/rgbdslam_v2/\nendres, felix, et al. \"3-d mapping with an rgb-d camera.\" ieee transactions on robotics 30.1 (2013): 177-187.\n\"rtab-map\": http://introlab.github.io/rtabmap/ | https://github.com/introlab/rtabmap_ros\nm. labb\u00e9 and f. michaud, \u201crtab-map as an open-source lidar and visual slam library for large-scale and long-term online operation,\u201d in journal of field robotics, vol. 36, no. 2, pp. 416\u2013446, 2019. (wiley) universit \u0301e de sherbrooke\nm. labb\u00e9 and f. michaud, \u201clong-term online multi-session graph-based splam with memory management,\u201d in autonomous robots, vol. 42, no. 6, pp. 1133-1150, 2018.\nm. labb\u00e9 and f. michaud, \u201conline global loop closure detection for large-scale multi-session graph-based slam,\u201d in proceedings of the ieee/rsj international conference on intelligent robots and systems, 2014.\nm. labb\u00e9 and f. michaud, \u201cappearance-based loop closure detection for online large-scale and long-term operation,\u201d in ieee transactions on robotics, vol. 29, no. 3, pp. 734-745, 2013.\nm. labb\u00e9 and f. michaud, \u201cmemory management for real-time appearance-based loop closure detection,\u201d in proceedings of the ieee/rsj international conference on intelligent robots and systems, 2011, pp. 1271\u20131276.\n\"kinectfusion\": https://www.microsoft.com/en-us/research/project/kinectfusion-project-page/\nizadi, shahram, et al. \"kinectfusion: real-time 3d reconstruction and interaction using a moving depth camera.\" proceedings of the 24th annual acm symposium on user interface software and technology. acm, 2011. newcombe, richard a., et al. \"kinectfusion: real-time dense surface mapping and tracking.\" ismar. vol. 11. no. 2011. 2011.\n\"elasticfusion\": https://github.com/mp3guy/elasticfusion\nwhelan, thomas, et al. \"elasticfusion: dense slam without a pose graph.\" robotics: science and systems, 2015.\n\"bundlefusion\": http://graphics.stanford.edu/projects/bundlefusion/\ndai, angela, et al. \"bundlefusion: real-time globally consistent 3d reconstruction using on-the-fly surface reintegration.\" acm transactions on graphics (tog) 36.3 (2017): 24.\n\"ko-fusion\": https://www.youtube.com/watch?v=yigoiyoy7wg (mobile manipulator)\nhouseago, charlie, michael bloesch, and stefan leutenegger. \"ko-fusion: dense visual slam with tightly-coupled kinematic and odometric tracking.\" 2019 international conference on robotics and automation (icra). ieee, 2019.\n\"arm-slam\": https://www.youtube.com/watch?v=qrfyaxfus9w (manipulator)\nm. klingensmith, s. s. sirinivasa and m. kaess, \"articulated robot motion for simultaneous localization and mapping (arm-slam),\" in ieee robotics and automation letters, vol. 1, no. 2, pp. 1156-1163, july 2016.\n\ud83d\udd05 3d lidar based slam\n\"zebedee\": https://research.csiro.au/robotics/zebedee/ (handheld device)\nm. bosse, r. zlot and p. flick, \"zebedee: design of a spring-mounted 3-d range sensor with application to mobile mapping,\" in ieee transactions on robotics, vol. 28, no. 5, pp. 1104-1119, oct. 2012.\n\"kaarta\": https://www.kaarta.com/ (handheld device)\nzhang, ji, and sanjiv singh. \"laser\u2013visual\u2013inertial odometry and mapping with high robustness and low drift.\" journal of field robotics 35.8 (2018): 1242-1264.\n\"lio-sam\": https://github.com/tixiaoshan/lio-sam (handheld device)\nshan, tixiao and englot, brendan and meyers, drew and wang, wei and ratti, carlo and rus daniela, \"lio-sam: tightly-coupled lidar inertial odometry via smoothing and mapping,\" 2020 ieee/rsj international conference on intelligent robots and systems (iros), las vegas, 2020\n\"hdl_graph_slam\": https://github.com/koide3/hdl_graph_slam\nkenji koide, jun miura, and emanuele menegatti, a portable 3d lidar-based system for long-term and wide-area people behavior measurement, advanced robotic systems, 2019\n\"blam\": https://github.com/erik-nelson/blam\ne. nelson, blam: berkeley localization and mapping, [online]. available: https://github.com/erik-nelson/blam.\n\"lego-loam\": https://github.com/robustfieldautonomylab/lego-loam\nt. shan and b. englot, \"lego-loam: lightweight and ground- optimized lidar odometry and mapping on variable terrain,\" 2018 ieee/rsj international conference on intelligent robots and systems (iros), madrid, 2018, pp. 4758- 4765.\n\"cartographer\": https://google-cartographer-ros.readthedocs.io/en/latest/\nwolfgang hess ; damon kohler ; holger rapp ; daniel andor, \u201creal-time loop closure in 2d lidar slam \u201d, 2016 ieee international conference on robotics and automation (icra), stockholm, 2016, pp. 1271-1278.\n\"imls-slam\"\ndeschaud, jean-emmanuel. \"imls-slam: scan-to-model matching based on 3d data.\" 2018 ieee international conference on robotics and automation (icra). ieee, 2018.\n\ud83d\udc2d cognitive related slam\n\"vita-slam\": https://github.com/aalto-intelligent-robotics/vita-slam\nstruckmeier, oliver, et al. \"vita-slam: a bio-inspired visuo-tactile slam for navigation while interacting with aliased environments.\" 2019 ieee international conference on cyborg and bionic systems (cbs). ieee, 2019.\n\ud83c\udff7 semantic related slam\n\"kimera\": https://github.com/mit-spark/kimera\nrosinol, antoni, et al. \"kimera: an open-source library for real-time metric-semantic localization and mapping.\" arxiv preprint arxiv:1910.02490 (2019).\n7_localization\n\ud83d\udccc localization on 2d occupancy grid map\namcl: adaptive (or kld-sampling) monte carlo localization: http://wiki.ros.org/amcl\ns. thrun, w. burgard, and d. fox. probabilistic robotics. mit press, 2005.\nmrpt_localization: http://wiki.ros.org/mrpt_localization\nj.l. blanco, j. gonzalez-jimenez, j.a. fernandez-madrigal, \"optimal filtering for non-parametric observation models: applications to localization and slam\", the international journal of robotics research (ijrr), vol. 29, no. 14, 2010.\nj. gonzalez-jimenez, j.l. blanco, c. galindo, a. ortiz-de-galisteo, j.a. fernandez-madrigal, f.a. moreno, j. martinez, \"mobile robot localization based on ultra-wide-band ranging: a particle filter approach\", robotics and autonomous systems, vol. 57, no. 5, pp. 496--507, 2009.\n\ud83c\udf32 slam algorithms support pure localization:\ncartographer, orb_slam2, rtab-map\n8_mapping\n\ud83d\udccd basic mapping backbones\n\"2d occupancy grid map\" (binary or probability)\n\"3d occupancy grid map\" (binary or probability)\n\"octomap\": https://octomap.github.io/ (for collision checking)\nan efficient probabilistic 3d mapping framework based on octrees / 3d probability occupancy grid map\nhornung, armin & wurm, kai & bennewitz, maren & stachniss, cyrill & burgard, wolfram, \"octomap: an efficient probabilistic 3d mapping framework based on octrees. autonomous robots.\", autonomous robots journal (2013). 34. 10.1007/s10514-012-9321-0.\n\ud83d\uddfa basic mapping methods\n\"map_server\": http://wiki.ros.org/map_server (loading, saving)\n\"octomap_server\": http://wiki.ros.org/octomap_server (loading, saving, mapping)\n\ud83d\udccd advanced 3d mapping backbones\n\"surfels\"\npfister, hanspeter, et al. \"surfels: surface elements as rendering primitives.\" proceedings of the 27th annual conference on computer graphics and interactive techniques. 2000.\n\"truncated signed distance function (sdf)\"\ncurless, brian, and marc levoy. \"a volumetric method for building complex models from range images.\" proceedings of the 23rd annual conference on computer graphics and interactive techniques. 1996.\n\"truncated signed distance function (tsdf)\"\nr. a. newcombe, s. izadi, o. hilliges, d. molyneaux, d. kim, a. j.davison, p. kohi, j. shotton, s. hodges, and a. fitzgibbon, \u201ckinect-fusion: real-time dense surface mapping and tracking,\u201d in mixed and augmented reality (ismar), 2011 10th ieee international symposiumon, pp. 127\u2013136, ieee, 2011\n\"euclidean signed distance fields (esdf)\" (for collision checking)\nratliff, nathan, et al. \"chomp: gradient optimization techniques for efficient motion planning.\" 2009 ieee international conference on robotics and automation. ieee, 2009.\n\ud83d\uddfa advanced 3d mapping methods\n\"voxblox (esdf&tsdf based)\": https://github.com/ethz-asl/voxblox\nhelen oleynikova, et al. \u201cvoxblox: incremental 3d euclidean signed distance fields for on-board mav planning\u201d, in ieee/rsj international conference on intelligent robots and systems (iros), 2017.\n\"openchisel (tsdf based)\": https://github.com/personalrobotics/openchisel\nklingensmith, matthew, et al. \"chisel: real time large scale 3d reconstruction onboard a mobile device using spatially hashed signed distance fields.\" robotics: science and systems. vol. 4. 2015.\n\"densesurfelmapping (surfel based)\": https://github.com/hkust-aerial-robotics/densesurfelmapping\nwang, kaixuan, fei gao, and shaojie shen. \"real-time scalable dense surfel mapping.\" 2019 international conference on robotics and automation (icra). ieee, 2019.\n9_navigation\n\ud83d\ude97 ros navigation stack (move_base architecture) https://github.com/ros-planning/navigation\n\"move_base\": http://wiki.ros.org/move_base\n\"move_base_flex\": http://wiki.ros.org/move_base_flex\n\ud83d\ude98 global planner\nglobal_planner, carrot_planner, navfn, sbpl_lattice_planner, srl_global_planner, voronoi_planner\n\"a* (a star)\"\nhart, peter e., nils j. nilsson, and bertram raphael. \"a formal basis for the heuristic determination of minimum cost paths.\" ieee transactions on systems science and cybernetics 4.2 (1968): 100-107.\n\"dijkstra's\"\ndijkstra, edsger w. \"a note on two problems in connexion with graphs.\" numerische mathematik 1.1 (1959): 269-271.\n\ud83d\ude98 local planner\ndwa_local_planner, teb_local_planner, base_local_planner, eband_local_planner, robotino_local_planner, asr_ftc_local_planner, simple_local_planner\n\"timed elastic band (teb)\": http://wiki.ros.org/teb_local_planner\nc. r\u00f6smann, w. feiten, t. w\u00f6sch, f. hoffmann and t. bertram: efficient trajectory optimization using a sparse model. proc. ieee european conference on mobile robots, spain, barcelona, sept. 2013, pp. 138\u2013143.\nc. r\u00f6smann, f. hoffmann and t. bertram: integrated online trajectory planning and optimization in distinctive topologies, robotics and autonomous systems, vol. 88, 2017, pp. 142\u2013153.\nc. r\u00f6smann, f. hoffmann and t. bertram: kinodynamic trajectory optimization and control for car-like robots, ieee/rsj international conference on intelligent robots and systems (iros), vancouver, bc, canada, sept. 2017.\n\"dynamic window approach (dwa)\": http://wiki.ros.org/dwa_local_planner\nd. fox, w. burgard and s. thrun, \"the dynamic window approach to collision avoidance,\" in ieee robotics & automation magazine, vol. 4, no. 1, pp. 23-33, march 1997.\n\ud83d\ude98 advanced local planner\nvelocity obstacle (vo)\nfiorini, paolo, and zvi shiller. \"motion planning in dynamic environments using velocity obstacles.\" the international journal of robotics research 17.7 (1998): 760-772.\nreciprocal velocity obstacle (rvo) http://gamma.cs.unc.edu/rvo2/ | https://github.com/daenny/collvoid\nvan den berg, jur, ming lin, and dinesh manocha. \"reciprocal velocity obstacles for real-time multi-agent navigation.\" 2008 ieee international conference on robotics and automation. ieee, 2008.\noptimal reciprocal collision avoidance (orca) http://gamma.cs.unc.edu/orca/\nvan den berg, jur, et al. \"reciprocal n-body collision avoidance.\" robotics research. springer, berlin, heidelberg, 2011. 3-19.\n\ud83d\ude98 recovery behavior\nrotate_recovery, move_slow_and_clear, stepback_and_steerturn_recovery\n\ud83c\udfce\ufe0f novel navigation strategy\n\"mit aerospacecontrolslab drl navigation\": http://acl.mit.edu/projects/socially-acceptable-navigation\nchen, y. f., liu, s.-y., liu, m., miller, j., and how, j. p., \u201cmotion planning with diffusion maps,\u201d ieee/rsj international conference on intelligent robots and systems (iros), daejeon, korea: 2016.\nchen, yu fan, et al. \"decentralized non-communicating multiagent collision avoidance with deep reinforcement learning.\" 2017 ieee international conference on robotics and automation (icra). ieee, 2017.\nchen, yu fan, et al. \"socially aware motion planning with deep reinforcement learning.\" 2017 ieee/rsj international conference on intelligent robots and systems (iros). ieee, 2017.\nm. everett, et al. \"motion planning among dynamic, decision-making agents with deep reinforcement learning,\" 2018 ieee/rsj international conference on intelligent robots and systems (iros), madrid, spain, 2018\n\"google ai research prm-rl navigation\": https://ai.googleblog.com/2019/02/long-range-robotic-navigation-via.html\na. faust et al. \"prm-rl: long-range robotic navigation tasks by combining reinforcement learning and sampling-based planning,\" 2018 ieee international conference on robotics and automation (icra), brisbane, qld, 2018, pp. 5113-5120.\nh. l. chiang, et al. \"learning navigation behaviors end-to-end with autorl,\" in ieee robotics and automation letters, vol. 4, no. 2, pp. 2007-2014, april 2019.\nfrancis, anthony, et al. \"long-range indoor navigation with prm-rl.\" ieee transactions on robotics (2020).\n\"ethz autonomous system lab navigation\": https://www.youtube.com/watch?v=gpp5mnybm8g | https://www.youtube.com/watch?v=h1rm0bw3eve | https://www.youtube.com/watch?v=zedkmxzwdgi\npfeiffer, mark, et al. \"predicting actions to act predictably: cooperative partial motion planning with maximum entropy models.\" 2016 ieee/rsj international conference on intelligent robots and systems (iros). ieee, 2016.\npfeiffer, mark, et al. \"from perception to decision: a data-driven approach to end-to-end motion planning for autonomous ground robots.\" 2017 ieee international conference on robotics and automation (icra). ieee, 2017.\n\ud83d\udec0 coverage navigation (cleaning or weeding robot)\n\"survey paper\":\ngalceran, enric, and marc carreras. \"a survey on coverage path planning for robotics.\" robotics and autonomous systems 61.12 (2013): 1258-1276.\n10_manipulation\n\ud83d\udd90\ufe0f ros moveit (move_group architecture) https://github.com/ros-planning/moveit.git\n\ud83d\udcda planner library\n\"open motion planning library (ompl)\"\nwebsite: https://ompl.kavrakilab.org/\nintro: https://moveit.ros.org/assets/pdfs/2013/icra2013tutorial/omploverview-icra2013.pdf\nroadmap based planner: prm(probabilistic roadmap), prm*, lazy-prm, lazyprm*\ntree based planner: rrtconnect (default), rrt(rapidly-exploring random tree), rrt*, t-rrt, bi-trrt, lb-trrt, sbl, stride, kpiece, b-kpiece, lb-kpiece, est, bi-est, proj-est, pdst, spars, spars2\n\"search based planning library (sbpl)\"\nwebsite: http://www.sbpl.net/\nintro: https://www.cs.cmu.edu/~maxim/files/tutorials/robschooltutorial_oct10.pdf\nsearch based planner: ara*, anytime d*, r*\n\"covariant hamiltonian optimiza-tion for motion planning (chomp)\"\nintro: https://www.ri.cmu.edu/pub_files/2009/5/icra09-chomp.pdf\norocos kinematics and dynamics library (kdl) for fk/ik modeling\nhttps://www.orocos.org/kdl.html\n11_others_non_tech_part\n11-1_famous robotics related company\n\ud83c\udfec robotic companies\ncategories companies\nresearch center toyota_research_institute(tri), microsoft_research, google_ai, deepmind, facebook_artificial_intelligence_research(fair), berkeley_artificial_intelligence_research (bair), nvidia_research\nmanipulator abb, fanuc, kuka, yaskawa, techman_robot, hiwin, universal_robots, innfos\nmobile robot(agv, base only) omron_robotics, clearpath_robotics&otto_motors, amazon_robotics(kiva_system/canvas_tech), yujin_robotics, robotis, fetch_robotics, greentrans, kuka, irobot, pal_robotics, robotnik\nservice robot(with torso) willow_garage, softbank_robotics, fetch_robotics, pal_robotics, innfos, robotnik\ndual arms abb, rethink_robotics\nhumanoid boston_dynamics, softbank_robotics, pal_robotics, ubtech_robotics\nquadruped boston_dynamics, unitree_robotics, mit_cheetah, anyrobotics(anymal), standford\uff3fdoggo, innfos\nresearch robot willow_garage(pr2), facebook(pyrobot), robotis(turtlebot3), fetch_robotics, robotnik(rb-1)\neducational robot kit trossen_robotics, niryo, oz_robotics\ndrone dji, tello\nros2.0 adlink(dds), robotis(turtlebot3)\ncleaningbot irobot, xiaomi\ngripper robotiq, toyo\nself-driving cars alphabet_waymo, uber_atg, apple_project_titan, tesla, toyota_research_institute(tri), baidu_apollo, autox\ndelivery robots starship, amazon_robotics_scout(dispatch)\n11-2_famous robotics publications\n\ud83d\udcdd top conferences:\nieee international conference on robotics and automation (icra)\nieee/rsj international conference on intelligent robots and systems (iros)\n\ud83c\udfe0 related societies:\nsociety website conferences / transactions\nieee robotics and automation society (ras) https://www.ieee-ras.org/ https://ras.papercept.net/conferences/scripts/start.pl\nieee industrial electronics society (ies) http://www.ieee-ies.org/ http://www.ieee-ies.org/conferences\nieee control system society (css) http://ieeecss.org/ http://ieeecss.org/conferences/general-information\nieee systems, man and cybernetics (smc) https://www.ieeesmc.org/ https://www.ieeesmc.org/conferences/calendar/\naaas science robotics https://robotics.sciencemag.org/ https://www.sciencemag.org/journals/robotics/call-for-papers\nconference on robot learning (corl) https://www.robot-learning.org https://www.robot-learning.org/program/paper-explorer\n\ud83d\udee0 tools:\n\"google scholar rank\": https://scholar.google.com/citations?view_op=top_venues&hl=en&vq=eng_robotics (h5-index, h5-median)\n\"journal citation reports (jcr)\": https://jcr.clarivate.com/ (impact factor, eigenfactor score, article influence score)\n\"compress pdf online\": https://www.pdf2go.com/compress-pdf\n11-3_famous robotics competition\n\ud83c\udf0e global:\n\"darpa robotics challenge\": https://en.wikipedia.org/wiki/darpa_robotics_challenge\n\"robocup\": https://en.wikipedia.org/wiki/robocup\n\"amazon robotics/picking challenge\": http://amazonpickingchallenge.org/\n\"icra robot competitions: including lots of competitions would be different every years\"\n\"iros robot competitions: including lots of competitions would be different every years\"\n\ud83c\uddf9\ud83c\uddfc taiwan:\n\"sks \u65b0\u5149\u4fdd\u5168\u667a\u6167\u578b\u4fdd\u5168\u6a5f\u5668\u4eba\u7af6\u8cfd\": https://www.facebook.com/sksrobot/\n\"pmc \u5168\u570b\u667a\u6167\u6a5f\u5668\u4eba\u7af6\u8cfd robot competition\": http://www.pmccontest.com/\n\"hiwin \u4e0a\u9280\u667a\u6167\u6a5f\u68b0\u624b\u5be6\u4f5c\u7af6\u8cfd\": http://www.hiwin.org.tw/awards/hiwin_robot/original.aspx\n\"siliconawards \u65fa\u5b8f\u91d1\u77fd\u734e\": http://www.mxeduc.org.tw/siliconawards/\n11-4_famous ros organizations & activities\n\ud83d\ude80 ros related work:\n\"ros-industrial\": https://rosindustrial.org/\n\"ros2.0\": https://design.ros2.org/\n\"ros-h\": https://acutronicrobotics.com/technology/h-ros/\"\n\ud83c\udfe2 organizations/communities:\n\"open source robotics foundation (osrf)\": https://www.openrobotics.org/\n\"open source robotics corporation (osrc)\": https://www.openrobotics.org/\n\"ros.taiwan\": https://www.facebook.com/groups/ros.taiwan/\n\"ros.taipei\": https://www.facebook.com/groups/ros.taipei/\n\ud83c\udfaa activities:\n\"roscon\": https://roscon.ros.org/\n\"rosdevcon\": http://www.rosdevcon.com/\n\"ros summer school(cn)\": http://www.roseducation.org/\n\"ros summer school(tw)\": http://www.taoyuan-ros.com.tw/\nlicense", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000188, "year": null}, {"Unnamed: 0": 1220, "autor": 200, "date": null, "content": "ROS Control Center\nThis control center is a universal tool for controlling robots running ROS. It runs in the browser using a websocket connection and roslibjs from RobotWebTools.\nIn general, ROS Control Center offers an easy way to\nshow nodes, topics and service names.\nsubscribe and publish messages.\ncall services.\nshow and change parameters.\nFurther features like a console (rosout by default) output, a battery status view, a camera stream view or an advanced mode for better usability are implemented. Check it out!\nInstallation\nOn your ROS robot, the rosbridge_suite from Robot Web Tools needs to run. So to use the control center with your existing project, call roslaunch rosbridge_server rosbridge_websocket.launch to launch the websocket server. The control center runs on any computer (in particular without ROS...) in the same network. Open the ROS Control Center at https://pantor.github.io/ros-control-center. In the settings tab, you need to enter the IP address and port of your robot. Open the Control tab and reload.\nFeatures\nYou can have multiple saved settings for quick changes and several robots.\nROS Control Center supports images and camera streams via the web_video_server package. If camera and camera info messages are published according to the web_video_server standards, the stream is shown with the settings.\nFor your own custom message and service types, you can download this repository and start a server via http-server in the console. Then, navigate to index.html in a browser. You can add individual html templates into the app/topics/ or app/services/ folder. The path of your file must correspond to the ROS service or message type name. See the included ROS common messages and standard services as an examples. In your html file, you can write AngularJS code for additional customizing and formatting.\nThe right sidebar shows a logger output (rosout by default). On the left, group names are shown. ROS topics, services and parameters can be grouped together for a better overview. This works as follows:\nEvery topic, service or parameter name should correspond to /group-name/element-name like a URL. Global parameters should have no group name and are shown below the group list on the left side.\nEvery element name should start with a capital letter, as they are shown in the normal view mode. All elements (including the lowercase-names from system services, topics and parameters) can be shown by enabling the advanced view mode.\nIn the right bottom corner, a battery status bar can be shown; the battery topic can be adapted in the settings tab.\nContributing\nFor development, Node.js needs to be installed. Via npm install it will automatically download all development dependencies (from package.json). Type npm start in the terminal for a live development server. With vagrant ssh and roslaunch ros_test_project main.launch, you can start a test project in a virtual ubuntu machine.\nROS Control Center depends on:\nAngular as the general JavaScript and routing framework.\nBootstrap for design.\nroslib.js for ROS connection.\nContributions are always welcome!\nLicense\nROS Control Center is released with a BSD license. For full terms and conditions, see the LICENSE file.\nContributors\nSee here for a full list of contributors.", "link": "https://github.com/pantor/ros-control-center", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "ros control center\nthis control center is a universal -----> tool !!!  for controlling robots running ros. it runs in the browser using a websocket connection and roslibjs from robotwebtools.\nin general, ros control center offers an easy way to\nshow nodes, topics and service names.\nsubscribe and publish messages.\ncall services.\nshow and change parameters.\nfurther features like a console (rosout by default) output, a battery status view, a camera stream view or an advanced mode for better usability are implemented. check it out!\ninstallation\non your ros robot, the rosbridge_suite from robot web tools needs to run. so to use the control center with your existing project, call roslaunch rosbridge_server rosbridge_websocket.launch to launch the websocket server. the control center runs on any computer (in particular without ros...) in the same network. open the ros control center at https://pantor.github.io/ros-control-center. in the settings tab, you need to enter the ip address and port of your robot. open the control tab and reload.\nfeatures\nyou can have multiple saved settings for quick changes and several robots.\nros control center supports images and camera streams via the web_video_server package. if camera and camera info messages are published according to the web_video_server standards, the stream is shown with the settings.\nfor your own custom message and service types, you can download this repository and start a server via http-server in the console. then, navigate to index.html in a browser. you can add individual html templates into the app/topics/ or app/services/ folder. the path of your file must correspond to the ros service or message type name. see the included ros common messages and standard services as an examples. in your html file, you can write angularjs code for additional customizing and formatting.\nthe right sidebar shows a logger output (rosout by default). on the left, group names are shown. ros topics, services and parameters can be grouped together for a better overview. this works as follows:\nevery topic, service or parameter name should correspond to /group-name/element-name like a url. global parameters should have no group name and are shown below the group list on the left side.\nevery element name should start with a capital letter, as they are shown in the normal view mode. all elements (including the lowercase-names from system services, topics and parameters) can be shown by enabling the advanced view mode.\nin the right bottom corner, a battery status bar can be shown; the battery topic can be adapted in the settings tab.\ncontributing\nfor development, node.js needs to be installed. via npm install it will automatically download all development dependencies (from package.json). type npm start in the terminal for a live development server. with vagrant ssh and roslaunch ros_test_project main.launch, you can start a test project in a virtual ubuntu machine.\nros control center depends on:\nangular as the general javascript and routing framework.\nbootstrap for design.\nroslib.js for ros connection.\ncontributions are always welcome!\nlicense\nros control center is released with a bsd license. for full terms and conditions, see the license file.\ncontributors\nsee here for a full list of contributors.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000200, "year": null}, {"Unnamed: 0": 1221, "autor": 201, "date": null, "content": "open-quadruped\nAn open-source 3D-printed quadrupedal robot. Intuitive gait generation through 12-DOF Bezier Curves. Full 6-axis body pose manipulation. Custom 3DOF Leg Inverse Kinematics Model accounting for offsets.\nCurrent Status\nTo-Do List\nHardware // Redesign legs to move actuators closer to hip + reinforce all servo mounting.\nDesign // Research transition between gait & body pose.\nSoftware // Implementing ROS infrastructure for all control processes (including cpp on teensy).\nSoftware // Write & automate pytest unit tests.\nSoftware // Integrate unit tests with github actions.\nSimulation // Create URDF model. (or port collaborater's URDF: Maurice Rahme)\nSimulation // Gazebo\nSimulation // RViz Realtime Model\nDesign // Solve offset 3DOF IK model for right limbs OR validate that mirroring angles works for left/right legs\nSimulation // Implement WebGazebo clientside support for non-linux users\nSim-to-Real // Joint servo calibration\nSim-to-Real // Refining gait parameters\nSimulation // Reinforcement learning via ARS\nSoftware // Raibert Huerestic for converting desired velocities into gait parameters\nSoftware // ROS Nav Stack Layer\nPapers\nI've been formally documenting this project in the form of papers. You can find them here: adham-e.dev/papers\nDocumentation\nYou can find a summary of this project on my portfolio website's project page. I'm hosting most of the technical documentation for this project on this repo's wiki.\n3D Model\nIn the model folder, you can find all of the step & stl files that I used for my build of OpenQuadruped. There, you can also find a Bill of Materials, and some guides (in progress).\nHardware\nI made a custom pcb board to control the position and speed of 12 servos simultaneously, as well as interface with all of the sensors.\nYou can find the gerber files for the custom pcb in the hardware folder in this repository.\nYou can find the bill of materials for this project here.\nVisualization Usage\nNOTE: There are 3 different visualization tools: Gazebo Simulator (ROS + Physics), RViz (ROS), & my custom 3d-matplotlib visualization tool (body IK model + leg IK model). The latter tool is still under development, and I haven't ported it to use my new control library yet, so it is likely broken for the time being.\nTo try the visualization tool out, you'll need to run the animate.py python file with the proper libraries installed. (matplotlib 3.0.3 supported).\nYou can then use keyboard controls: use x, y, z, a, p, r to select (x axis, y axis, z axis, yaw, pitch, roll), and then the up and down buttons to increment the selected position. If you click '1' on your keyboard, it will reset the position.\nRight now, if you try to go to an impossible pose that would result in collisions, the body will do some weird things. If that happens, just click \"1\" on your keyboard to reset the position.\nNote: Pitch and Roll are currently not working. I am currently trying to fix that.\nMore Demos", "link": "https://github.com/adham-elarabawy/open-quadruped", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "open-quadruped\nan open-source 3d-printed quadrupedal robot. intuitive gait generation through 12-dof bezier curves. full 6-axis body pose manipulation. custom 3dof leg inverse kinematics model accounting for offsets.\ncurrent status\nto-do list\nhardware // redesign legs to move actuators closer to hip + reinforce all servo mounting.\ndesign // research transition between gait & body pose.\nsoftware // implementing ros infrastructure for all control processes (including cpp on teensy).\nsoftware // write & automate pytest unit tests.\nsoftware // integrate unit tests with github actions.\nsimulation // create urdf model. (or port collaborater's urdf: maurice rahme)\nsimulation // gazebo\nsimulation // rviz realtime model\ndesign // solve offset 3dof ik model for right limbs or validate that mirroring angles works for left/right legs\nsimulation // implement webgazebo clientside support for non-linux users\nsim-to-real // joint servo calibration\nsim-to-real // refining gait parameters\nsimulation // reinforcement learning via ars\nsoftware // raibert huerestic for converting desired velocities into gait parameters\nsoftware // ros nav stack layer\npapers\ni've been formally documenting this project in the form of papers. you can find them here: adham-e.dev/papers\ndocumentation\nyou can find a summary of this project on my portfolio website's project page. i'm hosting most of the technical documentation for this project on this repo's wiki.\n3d model\nin the model folder, you can find all of the step & stl files that i used for my build of openquadruped. there, you can also find a bill of materials, and some guides (in progress).\nhardware\ni made a custom pcb board to control the position and speed of 12 servos simultaneously, as well as interface with all of the sensors.\nyou can find the gerber files for the custom pcb in the hardware folder in this repository.\nyou can find the bill of materials for this project here.\nvisualization usage\nnote: there are 3 different visualization tools: gazebo simulator (ros + physics), rviz (ros), & my custom 3d-matplotlib visualization -----> tool !!!  (body ik model + leg ik model). the latter tool is still under development, and i haven't ported it to use my new control library yet, so it is likely broken for the time being.\nto try the visualization tool out, you'll need to run the animate.py python file with the proper libraries installed. (matplotlib 3.0.3 supported).\nyou can then use keyboard controls: use x, y, z, a, p, r to select (x axis, y axis, z axis, yaw, pitch, roll), and then the up and down buttons to increment the selected position. if you click '1' on your keyboard, it will reset the position.\nright now, if you try to go to an impossible pose that would result in collisions, the body will do some weird things. if that happens, just click \"1\" on your keyboard to reset the position.\nnote: pitch and roll are currently not working. i am currently trying to fix that.\nmore demos", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000201, "year": null}, {"Unnamed: 0": 1226, "autor": 206, "date": null, "content": "GPMP2\nThis library is an implementation of GPMP2 (Gaussian Process Motion Planner 2) algorithm described in Motion Planning as Probabilistic Inference using Gaussian Processes and Factor Graphs (RSS 2016). The core library is developed in C++ language with an optional Python 2.7 toolbox. GPMP2 was started at the Georgia Tech Robot Learning Lab, see THANKS for contributors.\nPrerequisites\nCMake >= 3.0 (Ubuntu: sudo apt-get install cmake), compilation configuration tool.\nBoost >= 1.50 (Ubuntu: sudo apt-get install libboost-all-dev), portable C++ source libraries.\nAnaconda2, virtual environment needed if installing python toolbox.\nGTSAM == wrap_export, a C++ library that implements smoothing and mapping (SAM) framework in robotics and vision. Here we use the factor graph implementations and inference/optimization tools provided by GTSAM.\nInstallation (C++ only)\nInstall GTSAM.\ngit clone https://github.com/borglab/gtsam.git\ncd gtsam\ngit checkout wrap-export\nmkdir build && cd build\ncmake ..\nmake check # optional, run unit tests\nsudo make install\nSetup paths.\necho 'export LD_LIBRARY_PATH=/usr/local/lib:${LD_LIBRARY_PATH}' >> ~/.bashrc\necho 'export LD_LIBRARY_PATH=/usr/local/share:${LD_LIBRARY_PATH}' >> ~/.bashrc\nsource ~/.bashrc\nInstall gpmp2.\ngit clone https://github.com/gtrll/gpmp2.git\ncd gpmp2 && mkdir build && cd build\ncmake ..\nmake check # optional, run unit tests\nsudo make install\nInstallation (C++ with Python toolbox)\nSetup virtual environment.\nconda create -n gpmp2 pip python=2.7\nconda activate gpmp2\npip install cython numpy scipy matplotlib\nconda deactivate\nInstall GTSAM.\nconda activate gpmp2\ngit clone https://github.com/borglab/gtsam.git\ncd gtsam\ngit checkout wrap-export\nmkdir build && cd build\ncmake -DGTSAM_INSTALL_CYTHON_TOOLBOX:=ON ..\nmake check # optional, run unit tests\nsudo make install\nconda deactivate\nSetup paths.\necho 'export LD_LIBRARY_PATH=/usr/local/lib:${LD_LIBRARY_PATH}' >> ~/.bashrc\necho 'export LD_LIBRARY_PATH=/usr/local/share:${LD_LIBRARY_PATH}' >> ~/.bashrc\necho 'export PYTHONPATH=/usr/local/cython:${PYTHONPATH}' >> ~/.bashrc\nsource ~/.bashrc\nInstall gpmp2.\nconda activate gpmp2\ngit clone https://github.com/gtrll/gpmp2.git\ncd gpmp2 && mkdir build && cd build\ncmake -DGPMP2_BUILD_PYTHON_TOOLBOX:=ON ..\nmake check # optional, run unit tests\nsudo make install\ncd ../gpmp2_python && pip install -e .\nconda deactivate\nCiting\nIf you use GPMP2 in an academic context, please cite following publications:\n@inproceedings{Mukadam-IJRR-18,\nAuthor = {Mustafa Mukadam and Jing Dong and Xinyan Yan and Frank Dellaert and Byron Boots},\nTitle = {Continuous-time {G}aussian Process Motion Planning via Probabilistic Inference},\njournal = {The International Journal of Robotics Research (IJRR)},\nvolume = {37},\nnumber = {11},\npages = {1319--1340},\nyear = {2018}\n}\n@inproceedings{Dong-RSS-16,\nAuthor = {Jing Dong and Mustafa Mukadam and Frank Dellaert and Byron Boots},\nTitle = {Motion Planning as Probabilistic Inference using {G}aussian Processes and Factor Graphs},\nbooktitle = {Proceedings of Robotics: Science and Systems (RSS)},\nyear = {2016}\n}\n@inproceedings{dong2018sparse,\ntitle={Sparse {G}aussian Processes on Matrix {L}ie Groups: A Unified Framework for Optimizing Continuous-Time Trajectories},\nauthor={Dong, Jing and Mukadam, Mustafa and Boots, Byron and Dellaert, Frank},\nbooktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},\npages={6497--6504},\nyear={2018},\norganization={IEEE}\n}\nLicense\nGPMP2 is released under the BSD license, reproduced in LICENSE.", "link": "https://github.com/gtrll/gpmp2", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "gpmp2\nthis library is an implementation of gpmp2 (gaussian process motion planner 2) algorithm described in motion planning as probabilistic inference using gaussian processes and factor graphs (rss 2016). the core library is developed in c++ language with an optional python 2.7 toolbox. gpmp2 was started at the georgia tech robot learning lab, see thanks for contributors.\nprerequisites\ncmake >= 3.0 (ubuntu: sudo apt-get install cmake), compilation configuration -----> tool !!! .\nboost >= 1.50 (ubuntu: sudo apt-get install libboost-all-dev), portable c++ source libraries.\nanaconda2, virtual environment needed if installing python toolbox.\ngtsam == wrap_export, a c++ library that implements smoothing and mapping (sam) framework in robotics and vision. here we use the factor graph implementations and inference/optimization tools provided by gtsam.\ninstallation (c++ only)\ninstall gtsam.\ngit clone https://github.com/borglab/gtsam.git\ncd gtsam\ngit checkout wrap-export\nmkdir build && cd build\ncmake ..\nmake check # optional, run unit tests\nsudo make install\nsetup paths.\necho 'export ld_library_path=/usr/local/lib:${ld_library_path}' >> ~/.bashrc\necho 'export ld_library_path=/usr/local/share:${ld_library_path}' >> ~/.bashrc\nsource ~/.bashrc\ninstall gpmp2.\ngit clone https://github.com/gtrll/gpmp2.git\ncd gpmp2 && mkdir build && cd build\ncmake ..\nmake check # optional, run unit tests\nsudo make install\ninstallation (c++ with python toolbox)\nsetup virtual environment.\nconda create -n gpmp2 pip python=2.7\nconda activate gpmp2\npip install cython numpy scipy matplotlib\nconda deactivate\ninstall gtsam.\nconda activate gpmp2\ngit clone https://github.com/borglab/gtsam.git\ncd gtsam\ngit checkout wrap-export\nmkdir build && cd build\ncmake -dgtsam_install_cython_toolbox:=on ..\nmake check # optional, run unit tests\nsudo make install\nconda deactivate\nsetup paths.\necho 'export ld_library_path=/usr/local/lib:${ld_library_path}' >> ~/.bashrc\necho 'export ld_library_path=/usr/local/share:${ld_library_path}' >> ~/.bashrc\necho 'export pythonpath=/usr/local/cython:${pythonpath}' >> ~/.bashrc\nsource ~/.bashrc\ninstall gpmp2.\nconda activate gpmp2\ngit clone https://github.com/gtrll/gpmp2.git\ncd gpmp2 && mkdir build && cd build\ncmake -dgpmp2_build_python_toolbox:=on ..\nmake check # optional, run unit tests\nsudo make install\ncd ../gpmp2_python && pip install -e .\nconda deactivate\nciting\nif you use gpmp2 in an academic context, please cite following publications:\n@inproceedings{mukadam-ijrr-18,\nauthor = {mustafa mukadam and jing dong and xinyan yan and frank dellaert and byron boots},\ntitle = {continuous-time {g}aussian process motion planning via probabilistic inference},\njournal = {the international journal of robotics research (ijrr)},\nvolume = {37},\nnumber = {11},\npages = {1319--1340},\nyear = {2018}\n}\n@inproceedings{dong-rss-16,\nauthor = {jing dong and mustafa mukadam and frank dellaert and byron boots},\ntitle = {motion planning as probabilistic inference using {g}aussian processes and factor graphs},\nbooktitle = {proceedings of robotics: science and systems (rss)},\nyear = {2016}\n}\n@inproceedings{dong2018sparse,\ntitle={sparse {g}aussian processes on matrix {l}ie groups: a unified framework for optimizing continuous-time trajectories},\nauthor={dong, jing and mukadam, mustafa and boots, byron and dellaert, frank},\nbooktitle={2018 ieee international conference on robotics and automation (icra)},\npages={6497--6504},\nyear={2018},\norganization={ieee}\n}\nlicense\ngpmp2 is released under the bsd license, reproduced in license.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000206, "year": null}, {"Unnamed: 0": 1248, "autor": 228, "date": null, "content": "rclnodejs - ROS2 Client Library for JavaScript\nBranch Linux Build macOS Build Windows Build\ndevelop\nmaster\nrclnodejs is a Node.js client library for the Robot Operating System (ROS 2). It provides a JavaScript API and tooling for ROS 2 programming. TypeScript declarations, i.e., (*.d.ts), are included to support use in TypeScript projects.\nHere's an example for how to create a ROS 2 node that publishes a string message in a few lines of JavaScript.\nconst rclnodejs = require('rclnodejs');\nrclnodejs.init().then(() => {\nconst node = new rclnodejs.Node('publisher_example_node');\nconst publisher = node.createPublisher('std_msgs/msg/String', 'topic');\npublisher.publish(`Hello ROS 2 from rclnodejs`);\nnode.spin();\n});\nDocumentation\nInstallation\nrclnodejs-cli\nAPI Documentation\nUsing TypeScript\nExamples\nEfficient Usage Tips\nFAQ and Known Issues\nBuilding from Scratch\nContributing\nInstallation\nPrerequisites\nBefore installing rclnodejs please ensure the following software is installed and configured on your system:\nNodejs version between 10.23.1 - 12.x.\nROS 2 SDK for details. DON'T FORGET TO SOURCE THE ROS 2 SETUP FILE\nInstalling rclnodejs\nInstall the rclnodejs version that is compatible with your version of ROS 2 (see table below).\nFor the most current version of rclnodejs run:\nnpm i rclnodejs\nTo install a specific version of rclnodejs use:\nnpm i rclnodejs@x.y.z\nRCLNODEJS Version Compatible ROS 2 Release\n0.20.0 (current) (API) Galactic Geochelone / Foxy Fitzroy / Eloquent Elusor\n0.10.3 Dashing Diademata - Patch 4\nNote: to install rclnodejs from GitHub: add \"rclnodejs\":\"RobotWebTools/rclnodejs#<branch>\" to your package.json depdendency section.\nrclnodejs-cli\nrclnodejs-cli is a companion project we recently launched to provide a commandline interface to a set of developer tools for working with this rclnodejs. You may find rclnodejs-cli particularly useful if you plan to create ROS 2 node(s) and launch files for working with multiple node orchestrations.\n_ _ _\n_ __ ___| |_ __ ___ __| | ___ (_)___\n| '__/ __| | '_ \\ / _ \\ / _` |/ _ \\| / __|\n| | | (__| | | | | (_) | (_| | __/| \\__ \\\n|_| \\___|_|_| |_|\\___/ \\__,_|\\___|/ |___/\n|__/\nUsage: rclnodejs [command] [options]\nOptions:\n-h, --help display help for command\nCommands:\ncreate-package [options] <package-name> Create a ROS2 package for Nodejs development.\ngenerate-ros-messages Generate JavaScript code from ROS2 IDL interfaces\nhelp [command] display help for command\nAPI Documentation\nAPI documentation is generated by jsdoc and can be viewed in the docs/ folder or on-line. To create a local copy of the documentation run npm run docs.\nUsing rclnodejs with TypeScript\nrclnodejs API can be used in TypeScript projects. You can find the TypeScript declaration files (*.d.ts) in the types/ folder.\nYour tsconfig.json file should include the following compiler options:\n{\n\"compilerOptions\": {\n\"module\": \"commonjs\",\n\"moduleResolution\": \"node\",\n\"target\": \"es6\"\n// your additional options here\n}\n}\nHere's a short rclnodejs TypeScript example:\nimport * as rclnodejs from 'rclnodejs';\nrclnodejs.init().then(() => {\nconst node = new rclnodejs.Node('publisher_example_node');\nconst publisher = node.createPublisher('std_msgs/msg/String', 'topic');\npublisher.publish(`Hello ROS 2 from rclnodejs`);\nnode.spin();\n});\nThe benefits of using TypeScript become evident when working with more complex use-cases. ROS messages are defined in the types/interfaces.d.ts module. This module is updated as part of the generate-ros-messages process described in the next section.\nROS2 Interface Message Generation (important)\nROS components communicate by sending and receiving messages described by the interface definition language (IDL). ROS client libraries such as rclnodejs are responsible for converting these IDL message descriptions into source code of their target language. For this, rclnodejs provides the npm binarygenerate-ros-messages script that reads the IDL message files of a ROS environment and generates corresponding JavaScript message interface files. Additionally, the tool generates the TypeScript interface.d.ts file containing declarations for each IDL message file.\nLearn more about ROS interfaces and IDL here.\nIn the following example rclnodejs loads a generated JavaScript message file corresponding to the ROS `std_msgs/msg/String' definition.\nimport * as rclnodejs from 'rclnodejs';\nlet stringMsgObject = rclnodejs.createMessageObject('std_msgs/msg/String');\nstringMsgObject.data = 'hello world';\nMaintaining Generated JavaScript Message Files\nMessage files are generated as a post-install step of the rclnodejs installation process. Thereafter, you will need to manually run the rclnodejs message generation script when new ROS message packages are installed for which your ROS2-nodejs project has a dependency.\nRunning generate-ros-messages Utility\nTo run the generate-ros-messages script from your Nodejs package, use the npx utility included in your Nodejs installation.\nnpx generate-ros-messages\nThe newly generated JavaScript files can be found at <yourproject>/node_modules/rclnodejs/generated/.\nContributing\nPlease read the Contributing Guide before making a pull request.\nThank you to all the people who already contributed to rclnodejs!\nLicense\nThis project abides by the Apache License 2.0.", "link": "https://github.com/RobotWebTools/rclnodejs", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "rclnodejs - ros2 client library for javascript\nbranch linux build macos build windows build\ndevelop\nmaster\nrclnodejs is a node.js client library for the robot operating system (ros 2). it provides a javascript api and tooling for ros 2 programming. typescript declarations, i.e., (*.d.ts), are included to support use in typescript projects.\nhere's an example for how to create a ros 2 node that publishes a string message in a few lines of javascript.\nconst rclnodejs = require('rclnodejs');\nrclnodejs.init().then(() => {\nconst node = new rclnodejs.node('publisher_example_node');\nconst publisher = node.createpublisher('std_msgs/msg/string', 'topic');\npublisher.publish(`hello ros 2 from rclnodejs`);\nnode.spin();\n});\ndocumentation\ninstallation\nrclnodejs-cli\napi documentation\nusing typescript\nexamples\nefficient usage tips\nfaq and known issues\nbuilding from scratch\ncontributing\ninstallation\nprerequisites\nbefore installing rclnodejs please ensure the following software is installed and configured on your system:\nnodejs version between 10.23.1 - 12.x.\nros 2 sdk for details. don't forget to source the ros 2 setup file\ninstalling rclnodejs\ninstall the rclnodejs version that is compatible with your version of ros 2 (see table below).\nfor the most current version of rclnodejs run:\nnpm i rclnodejs\nto install a specific version of rclnodejs use:\nnpm i rclnodejs@x.y.z\nrclnodejs version compatible ros 2 release\n0.20.0 (current) (api) galactic geochelone / foxy fitzroy / eloquent elusor\n0.10.3 dashing diademata - patch 4\nnote: to install rclnodejs from github: add \"rclnodejs\":\"robotwebtools/rclnodejs#<branch>\" to your package.json depdendency section.\nrclnodejs-cli\nrclnodejs-cli is a companion project we recently launched to provide a commandline interface to a set of developer tools for working with this rclnodejs. you may find rclnodejs-cli particularly useful if you plan to create ros 2 node(s) and launch files for working with multiple node orchestrations.\n_ _ _\n_ __ ___| |_ __ ___ __| | ___ (_)___\n| '__/ __| | '_ \\ / _ \\ / _` |/ _ \\| / __|\n| | | (__| | | | | (_) | (_| | __/| \\__ \\\n|_| \\___|_|_| |_|\\___/ \\__,_|\\___|/ |___/\n|__/\nusage: rclnodejs [command] [options]\noptions:\n-h, --help display help for command\ncommands:\ncreate-package [options] <package-name> create a ros2 package for nodejs development.\ngenerate-ros-messages generate javascript code from ros2 idl interfaces\nhelp [command] display help for command\napi documentation\napi documentation is generated by jsdoc and can be viewed in the docs/ folder or on-line. to create a local copy of the documentation run npm run docs.\nusing rclnodejs with typescript\nrclnodejs api can be used in typescript projects. you can find the typescript declaration files (*.d.ts) in the types/ folder.\nyour tsconfig.json file should include the following compiler options:\n{\n\"compileroptions\": {\n\"module\": \"commonjs\",\n\"moduleresolution\": \"node\",\n\"target\": \"es6\"\n// your additional options here\n}\n}\nhere's a short rclnodejs typescript example:\nimport * as rclnodejs from 'rclnodejs';\nrclnodejs.init().then(() => {\nconst node = new rclnodejs.node('publisher_example_node');\nconst publisher = node.createpublisher('std_msgs/msg/string', 'topic');\npublisher.publish(`hello ros 2 from rclnodejs`);\nnode.spin();\n});\nthe benefits of using typescript become evident when working with more complex use-cases. ros messages are defined in the types/interfaces.d.ts module. this module is updated as part of the generate-ros-messages process described in the next section.\nros2 interface message generation (important)\nros components communicate by sending and receiving messages described by the interface definition language (idl). ros client libraries such as rclnodejs are responsible for converting these idl message descriptions into source code of their target language. for this, rclnodejs provides the npm binarygenerate-ros-messages script that reads the idl message files of a ros environment and generates corresponding javascript message interface files. additionally, the -----> tool !!!  generates the typescript interface.d.ts file containing declarations for each idl message file.\nlearn more about ros interfaces and idl here.\nin the following example rclnodejs loads a generated javascript message file corresponding to the ros `std_msgs/msg/string' definition.\nimport * as rclnodejs from 'rclnodejs';\nlet stringmsgobject = rclnodejs.createmessageobject('std_msgs/msg/string');\nstringmsgobject.data = 'hello world';\nmaintaining generated javascript message files\nmessage files are generated as a post-install step of the rclnodejs installation process. thereafter, you will need to manually run the rclnodejs message generation script when new ros message packages are installed for which your ros2-nodejs project has a dependency.\nrunning generate-ros-messages utility\nto run the generate-ros-messages script from your nodejs package, use the npx utility included in your nodejs installation.\nnpx generate-ros-messages\nthe newly generated javascript files can be found at <yourproject>/node_modules/rclnodejs/generated/.\ncontributing\nplease read the contributing guide before making a pull request.\nthank you to all the people who already contributed to rclnodejs!\nlicense\nthis project abides by the apache license 2.0.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000228, "year": null}, {"Unnamed: 0": 1260, "autor": 240, "date": null, "content": "grasp_multiObject_multiGrasp\nThis is the implementation of our RA-L work 'Real-world Multi-object, Multi-grasp Detection'. The detector takes RGB-D image input and predicts multiple grasp candidates for a single object or multiple objects, in a single shot. The original arxiv paper can be found here. The final version will be updated after publication process.\nIf you find it helpful for your research, please consider citing:\n@inproceedings{chu2018deep,\ntitle = {Real-World Multiobject, Multigrasp Detection},\nauthor = {F. Chu and R. Xu and P. A. Vela},\njournal = {IEEE Robotics and Automation Letters},\nyear = {2018},\nvolume = {3},\nnumber = {4},\npages = {3355-3362},\nDOI = {10.1109/LRA.2018.2852777},\nISSN = {2377-3766},\nmonth = {Oct}\n}\nIf you encounter any questions, please contact me at fujenchu[at]gatech[dot]edu\nDemo\nClone this repository\ngit clone https://github.com/ivalab/grasp_multiObject_multiGrasp.git\ncd grasp_multiObject_multiGrasp\nBuild Cython modules\ncd lib\nmake clean\nmake\ncd ..\nInstall Python COCO API\ncd data\ngit clone https://github.com/pdollar/coco.git\ncd coco/PythonAPI\nmake\ncd ../../..\nDownload pretrained models\ntrained model for grasp on dropbox drive\nput under output/res50/train/default/\nRun demo\n./tools/demo_graspRGD.py --net res50 --dataset grasp\nyou can see images pop out.\nTrain\nGenerate data\n1-1. Download Cornell Dataset\n1-2. Run dataPreprocessingTest_fasterrcnn_split.m (please modify paths according to your structure)\n1-3. Follow 'Format Your Dataset' section here to check if your data follows VOC format\nTrain\n./experiments/scripts/train_faster_rcnn.sh 0 graspRGB res50\nROS version?\nYes! please find it HERE\nAcknowledgment\nThis repo borrows tons of code from\ntf-faster-rcnn by endernewton\nResources\nmulti-object grasp dataset\ngrasp annotation tool", "link": "https://github.com/ivalab/grasp_multiObject_multiGrasp", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "grasp_multiobject_multigrasp\nthis is the implementation of our ra-l work 'real-world multi-object, multi-grasp detection'. the detector takes rgb-d image input and predicts multiple grasp candidates for a single object or multiple objects, in a single shot. the original arxiv paper can be found here. the final version will be updated after publication process.\nif you find it helpful for your research, please consider citing:\n@inproceedings{chu2018deep,\ntitle = {real-world multiobject, multigrasp detection},\nauthor = {f. chu and r. xu and p. a. vela},\njournal = {ieee robotics and automation letters},\nyear = {2018},\nvolume = {3},\nnumber = {4},\npages = {3355-3362},\ndoi = {10.1109/lra.2018.2852777},\nissn = {2377-3766},\nmonth = {oct}\n}\nif you encounter any questions, please contact me at fujenchu[at]gatech[dot]edu\ndemo\nclone this repository\ngit clone https://github.com/ivalab/grasp_multiobject_multigrasp.git\ncd grasp_multiobject_multigrasp\nbuild cython modules\ncd lib\nmake clean\nmake\ncd ..\ninstall python coco api\ncd data\ngit clone https://github.com/pdollar/coco.git\ncd coco/pythonapi\nmake\ncd ../../..\ndownload pretrained models\ntrained model for grasp on dropbox drive\nput under output/res50/train/default/\nrun demo\n./tools/demo_grasprgd.py --net res50 --dataset grasp\nyou can see images pop out.\ntrain\ngenerate data\n1-1. download cornell dataset\n1-2. run datapreprocessingtest_fasterrcnn_split.m (please modify paths according to your structure)\n1-3. follow 'format your dataset' section here to check if your data follows voc format\ntrain\n./experiments/scripts/train_faster_rcnn.sh 0 grasprgb res50\nros version?\nyes! please find it here\nacknowledgment\nthis repo borrows tons of code from\ntf-faster-rcnn by endernewton\nresources\nmulti-object grasp dataset\ngrasp annotation -----> tool !!! ", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000240, "year": null}, {"Unnamed: 0": 1265, "autor": 245, "date": null, "content": "ROS2 Cookbook\nClient Libraries\nrclcpp API\nNodes and Components\nParameters\nPoint Clouds\nTime\nTF2\nWorkarounds\nrclpy API\nNodes\nParameters\nTime\nTF2\nros2launch\nNetworking (DDS & CycloneDDS)\nCommand Line Tools\nros2 run <pkg> <node>\nros2 node list\nros2 topic list\nros2 topic info <topic_name> --verbose gives details about QoS.\nros2 param list\ncolcon is the build tool.\nros2 doctor --report gives tons of information.\nCMake\nPackaging\nSetting bloom/git to always use ssh\nrosdep install --from-paths src --ignore-src --rosdistro=foxy -y\nPackage Documentation\nStatus Pages\nFoxy Debian Build Status\nRolling Debian Build Status\nCompare Foxy/Rolling\nLicense\nThe contents of this cookbook are placed in the Public Domain through the CC0 license. You can use code snippets in your code without attribution and without impact to your license choice. This cookbook is provided as-is and without any warranties of any kind. See the full text of the CC0 license.\nWhile not required by license, if you want to copy the entire cookbook somewhere, please give some attribution.", "link": "https://github.com/mikeferguson/ros2_cookbook", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "ros2 cookbook\nclient libraries\nrclcpp api\nnodes and components\nparameters\npoint clouds\ntime\ntf2\nworkarounds\nrclpy api\nnodes\nparameters\ntime\ntf2\nros2launch\nnetworking (dds & cyclonedds)\ncommand line tools\nros2 run <pkg> <node>\nros2 node list\nros2 topic list\nros2 topic info <topic_name> --verbose gives details about qos.\nros2 param list\ncolcon is the build -----> tool !!! .\nros2 doctor --report gives tons of information.\ncmake\npackaging\nsetting bloom/git to always use ssh\nrosdep install --from-paths src --ignore-src --rosdistro=foxy -y\npackage documentation\nstatus pages\nfoxy debian build status\nrolling debian build status\ncompare foxy/rolling\nlicense\nthe contents of this cookbook are placed in the public domain through the cc0 license. you can use code snippets in your code without attribution and without impact to your license choice. this cookbook is provided as-is and without any warranties of any kind. see the full text of the cc0 license.\nwhile not required by license, if you want to copy the entire cookbook somewhere, please give some attribution.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000245, "year": null}, {"Unnamed: 0": 1279, "autor": 259, "date": null, "content": "RoboticsAcademy: Learn Robotics, Artificial Intelligence and Computer Vision\nJdeRobot Academy is an open source collection of exercises to learn robotics in a practical way. Gazebo simulator is the main tool required, as ROS. Its latest documentation (including installation recipes, current available exercises and illustrative videos) is on its webpage.\nIf you are a contributor, please note that we use GitHub Pages and a Jekyll theme (MinimalMistakes) for Academy web page. Feel free to install Jekyll locally, so that, you can test your changes before submitting your pull-request.\nHow to contribute?\nTake a look at the contribute section to join this project.\nFor developers\nTo include a new exercise, add the folder with the exercise contents in exercises/static/exercises following the file name conventions. Then, create the entry in db.sqlite3. A simple way to do this is by using the Django admin page:\nRun python3.8 manage.py runserver.\nAccess http://127.0.0.1:8000/admin/ on a browser and log in with \"user\" and \"pass\".\nClick on \"add exercise\" and fill the fields: exercise id (folder name), name (name to display), state, language and description (description to display). Save and exit.\nCommit db.sqlite3 changes.\nHow to update static files version number\nFollow this steps when changing any js or css document:\n1\u00ba Make all the changes necesary to the required documents.\n2\u00ba When the changes are done and ready to commit, open settings.py (located on RoboticsAcademy/academy/settings.py).\n3\u00ba In setting.py, update VERSION with the current date (the format is DD/MM/YYYY so for example the date 17/06/2021 would look something like this VERSION = 17062021 ).\n4\u00ba Save and commit the changes.\nIf a new static file is created or you find a file that doesn't have (or updates) their version number, just add ?v={{SYS_VERSION}} to the end of the src.\nFor example: script src=\"{% static 'exercises/assets/js/utils.js would have his src update as follows: script src=\"{% static 'exercises/assets/js/utils.js?v={{SYS_VERSION}}' %}\"", "link": "https://github.com/JdeRobot/RoboticsAcademy", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "roboticsacademy: learn robotics, artificial intelligence and computer vision\njderobot academy is an open source collection of exercises to learn robotics in a practical way. gazebo simulator is the main -----> tool !!!  required, as ros. its latest documentation (including installation recipes, current available exercises and illustrative videos) is on its webpage.\nif you are a contributor, please note that we use github pages and a jekyll theme (minimalmistakes) for academy web page. feel free to install jekyll locally, so that, you can test your changes before submitting your pull-request.\nhow to contribute?\ntake a look at the contribute section to join this project.\nfor developers\nto include a new exercise, add the folder with the exercise contents in exercises/static/exercises following the file name conventions. then, create the entry in db.sqlite3. a simple way to do this is by using the django admin page:\nrun python3.8 manage.py runserver.\naccess http://127.0.0.1:8000/admin/ on a browser and log in with \"user\" and \"pass\".\nclick on \"add exercise\" and fill the fields: exercise id (folder name), name (name to display), state, language and description (description to display). save and exit.\ncommit db.sqlite3 changes.\nhow to update static files version number\nfollow this steps when changing any js or css document:\n1\u00ba make all the changes necesary to the required documents.\n2\u00ba when the changes are done and ready to commit, open settings.py (located on roboticsacademy/academy/settings.py).\n3\u00ba in setting.py, update version with the current date (the format is dd/mm/yyyy so for example the date 17/06/2021 would look something like this version = 17062021 ).\n4\u00ba save and commit the changes.\nif a new static file is created or you find a file that doesn't have (or updates) their version number, just add ?v={{sys_version}} to the end of the src.\nfor example: script src=\"{% static 'exercises/assets/js/utils.js would have his src update as follows: script src=\"{% static 'exercises/assets/js/utils.js?v={{sys_version}}' %}\"", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000259, "year": null}, {"Unnamed: 0": 1280, "autor": 260, "date": null, "content": "BARK - A Tool for Behavior benchmARKing\nBARK is a semantic simulation framework for autonomous driving. Its behavior model-centric design allows for the rapid development, training, and benchmarking of various decision-making algorithms. It is especially suited for computationally expensive tasks, such as reinforcement learning. A a good starting point, have a look at the content of our BARK-Tutorial on IROS 2020.\nUsage\n(A) Pip Package\nFor whom it is: Python evangelists implementing python behavior models or ML scientists using BARK-ML for learning behaviors.\nBark is available as PIP-Package for Ubuntu and MacOS for Python>=3.7. You can install the latest version with pip install bark-simulator. The Pip package supports full benchmarking functionality of existing behavior models and development of your models within python.\nAfter installing the package, you can have a look at the examples to check how to use BARK.\nHighway Example Merging Example Intersection Example\n(B) Build it from Source\nFor whom it is: C++ developers creating C++ behavior models, researchers performing benchmarks, or contributors to BARK.\nUse git clone https://github.com/bark-simulator/bark.git or download the repository from this page. Then follow the instructions at How to Install BARK.\nTo get step-by-step instructions on how to use BARK, you can run our IPython Notebook tutorials using bazel run //docs/tutorials:run. For a more detailed understanding of how BARK works, its concept and use cases have a look at our documentation.\nExample Benchmark is a running example of how to use BARK for benchmarking for scientific purposes.\nScientific Publications using BARK\nBARK: Open Behavior Benchmarking in Multi-Agent Environments (IROS 2020)\nGraph Neural Networks and Reinforcement Learning for Behavior Generation in Semantic Environments (IV 2020)\nCounterfactual Policy Evaluation for Decision-Making in Autonomous Driving (IROS 2020, PLC Workshop)\nModeling and Testing Multi-Agent Traffic Rules within Interactive Behavior Planning (IROS 2020, PLC Workshop)\nFormalizing Traffic Rules for Machine Interpretability (CAVS 2020)\nRobust Stochastic Bayesian Games for Behavior Space Coverage (RSS 2020, Workshop on Interaction and Decision-Making in Autonomous-Driving)\nRisk-Constrained Interactive Safety under Behavior Uncertainty for Autonomous Driving (IV 2021)\nRisk-Based Safety Envelopes for Autonomous Vehicles Under Perception Uncertainty (Arxiv)\nBARK Ecosystem\nThe BARK ecosystem is composed of multiple components that all share the common goal to develop and benchmark behavior models:\nBARK-ML: Machine learning library for decision-making in autonomous driving.\nBARK-MCTS: Integrates a template-based C++ Monte Carlo Tree Search Library into BARK to support development of both single- and multi-agent search methods.\nBARK-Rules-MCTS: Integrates traffic rules within Monte Carlo Tree Search with lexicographic ordering.\nBARK-MIQP: MINIVAN Planner based on MIQP for single- and multi-agent planning. Check out the build instructions.\nBARK-DB: Provides a framework to integrate multiple BARK scenario sets into a database. The database module supports binary serialization of randomly generated scenarios to ensure exact reproducibility of behavior benchmarks across systems.\nBARK-Rule-Monitoring: Provides runtime verification of Rules in Linear Temporal Logic (LTL) on simulated BARK traces.\nCARLA-Interface: A two-way interface between CARLA and BARK. BARK behavior models can control CARLA vehicles. CARLA controlled vehicles are mirrored to BARK.\nPaper\nIf you use BARK, please cite us using the following paper:\n@inproceedings{Bernhard2020,\ntitle = {BARK: Open Behavior Benchmarking in Multi-Agent Environments},\nauthor = {Bernhard, Julian and Esterle, Klemens and Hart, Patrick and Kessler, Tobias},\nbooktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\nurl = {https://arxiv.org/pdf/2003.02604.pdf},\nyear = {2020}\n}\nContributing\nPull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\nPlease make sure to update tests as appropriate.\nLicense\nBARK specific code is distributed under MIT License.", "link": "https://github.com/bark-simulator/bark", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "bark - a -----> tool !!!  for behavior benchmarking\nbark is a semantic simulation framework for autonomous driving. its behavior model-centric design allows for the rapid development, training, and benchmarking of various decision-making algorithms. it is especially suited for computationally expensive tasks, such as reinforcement learning. a a good starting point, have a look at the content of our bark-tutorial on iros 2020.\nusage\n(a) pip package\nfor whom it is: python evangelists implementing python behavior models or ml scientists using bark-ml for learning behaviors.\nbark is available as pip-package for ubuntu and macos for python>=3.7. you can install the latest version with pip install bark-simulator. the pip package supports full benchmarking functionality of existing behavior models and development of your models within python.\nafter installing the package, you can have a look at the examples to check how to use bark.\nhighway example merging example intersection example\n(b) build it from source\nfor whom it is: c++ developers creating c++ behavior models, researchers performing benchmarks, or contributors to bark.\nuse git clone https://github.com/bark-simulator/bark.git or download the repository from this page. then follow the instructions at how to install bark.\nto get step-by-step instructions on how to use bark, you can run our ipython notebook tutorials using bazel run //docs/tutorials:run. for a more detailed understanding of how bark works, its concept and use cases have a look at our documentation.\nexample benchmark is a running example of how to use bark for benchmarking for scientific purposes.\nscientific publications using bark\nbark: open behavior benchmarking in multi-agent environments (iros 2020)\ngraph neural networks and reinforcement learning for behavior generation in semantic environments (iv 2020)\ncounterfactual policy evaluation for decision-making in autonomous driving (iros 2020, plc workshop)\nmodeling and testing multi-agent traffic rules within interactive behavior planning (iros 2020, plc workshop)\nformalizing traffic rules for machine interpretability (cavs 2020)\nrobust stochastic bayesian games for behavior space coverage (rss 2020, workshop on interaction and decision-making in autonomous-driving)\nrisk-constrained interactive safety under behavior uncertainty for autonomous driving (iv 2021)\nrisk-based safety envelopes for autonomous vehicles under perception uncertainty (arxiv)\nbark ecosystem\nthe bark ecosystem is composed of multiple components that all share the common goal to develop and benchmark behavior models:\nbark-ml: machine learning library for decision-making in autonomous driving.\nbark-mcts: integrates a template-based c++ monte carlo tree search library into bark to support development of both single- and multi-agent search methods.\nbark-rules-mcts: integrates traffic rules within monte carlo tree search with lexicographic ordering.\nbark-miqp: minivan planner based on miqp for single- and multi-agent planning. check out the build instructions.\nbark-db: provides a framework to integrate multiple bark scenario sets into a database. the database module supports binary serialization of randomly generated scenarios to ensure exact reproducibility of behavior benchmarks across systems.\nbark-rule-monitoring: provides runtime verification of rules in linear temporal logic (ltl) on simulated bark traces.\ncarla-interface: a two-way interface between carla and bark. bark behavior models can control carla vehicles. carla controlled vehicles are mirrored to bark.\npaper\nif you use bark, please cite us using the following paper:\n@inproceedings{bernhard2020,\ntitle = {bark: open behavior benchmarking in multi-agent environments},\nauthor = {bernhard, julian and esterle, klemens and hart, patrick and kessler, tobias},\nbooktitle = {2020 ieee/rsj international conference on intelligent robots and systems (iros)},\nurl = {https://arxiv.org/pdf/2003.02604.pdf},\nyear = {2020}\n}\ncontributing\npull requests are welcome. for major changes, please open an issue first to discuss what you would like to change.\nplease make sure to update tests as appropriate.\nlicense\nbark specific code is distributed under mit license.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000260, "year": null}, {"Unnamed: 0": 1296, "autor": 276, "date": null, "content": "IKFastPy - UR5 IKFast Python Package\nThis is a lightweight Python wrapper over OpenRave's generated IKFast C++ executables for the UR5 robot arm (e-series XML files included). IKFast \"analytically solves robot inverse kinematics equations and generates optimized C++ files\" for fast runtime speeds (more about IKFast here). IKFast can be used in tandem with URScript speedj commands on UR robot arms for real-time motion planning, which was used to create the visual servoing demo shown on the right (part of an ongoing project on closed-loop grasping with deep learning). Why speedj? See this UR performance analysis report.\nNote: this package can be easily modified to support other robot arms.\nFiles\nur5.robot.xml - a custom OpenRave XML file describing the kinematics of the UR5 robot arm. Modify this if you change the arm or tool center point (TCP) position.\nikfast61.cpp - C++ code at the heart of IKFast, generated by OpenRave using ur5.robot.xml. No need to modify this.\nikfast.h - a C++ header file necessary for compiling ikfast61.cpp. No need to modify this.\nikfast_wrapper.cpp - a C++ wrapper around ikfast61.cpp. Includes forward kinematics in addition to the inverse kinematics provided by ikfast61.cpp. Modify this to change how FK and IK results are passed to your code.\nikfastpy.pyx, Kinematics.hpp, setup.py - Cython code to link C++ with Python.\ndemo.py - a demo in Python to test FK and IK calls to IKFast.\nInstallation\nThis implementation requires the following dependencies (tested on Ubuntu 16.04.4 LTS):\nNumPy, Cython. You can quickly install/update these dependencies by running the following:\npip install --user numpy Cython\nQuick Start\nCheckout this repository and compile the Cython wrapper:\ngit clone https://github.com/andyzeng/ikfastpy.git\ncd ikfastpy\npython setup.py build_ext --inplace\nRun the demo in Python to test FK and IK calls to IKFast:\npython demo.py\nImportant: ensure all rotation matrices are valid before feeding into IKFast, otherwise no IK solution will be detected. R is a rotation matrix if and only if R is orthogonal, i.e. RRT = RTR = I, and det(R) = 1.\nNote: IKFast does not return solutions for singularities. In most cases, an approximate IK solution can be found for singularities by slightly perturbing the target end effector pose before re-computing IK solutions.\nModifying Robot Kinematics with OpenRave\nDownload and install OpenRave. See these installation instructions for Ubuntu 16.04.\nModify the kinematics of the arm or TCP position (link6) by changing ur5.robot.xml respectively. You can find a description of the OpenRave XML file format here.\n(Optional) Debug the kinematics using OpenRave's viewer:\nopenrave ur5.robot.xml\n(Optional) Check the links in your file:\nopenrave-robot.py ur5.robot.xml --info links\nUse OpenRave to re-generate the IKFast C++ code ikfast61.cpp.\npython `openrave-config --python-dir`/openravepy/_openravepy_/ikfast.py --robot=ur5.robot.xml --iktype=transform6d --baselink=0 --eelink=6 --savefile=ikfast61.cpp --maxcasedepth 1\nCitation\nIf you find IKFast useful, please cite OpenRave:\n@phdthesis{diankov_thesis,\nauthor = \"Rosen Diankov\",\ntitle = \"Automated Construction of Robotic Manipulation Programs\",\nschool = \"Carnegie Mellon University, Robotics Institute\",\nmonth = \"August\",\nyear = \"2010\",\nnumber= \"CMU-RI-TR-10-29\",\nurl={http://www.programmingvision.com/rosen_diankov_thesis.pdf},\n}\nThis module was also a part of Visual Pushing and Grasping. If you find it useful in your work, please consider citing:\n@inproceedings{zeng2018learning,\ntitle={Learning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning},\nauthor={Zeng, Andy and Song, Shuran and Welker, Stefan and Lee, Johnny and Rodriguez, Alberto and Funkhouser, Thomas},\nbooktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\nyear={2018}\n}", "link": "https://github.com/andyzeng/ikfastpy", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "ikfastpy - ur5 ikfast python package\nthis is a lightweight python wrapper over openrave's generated ikfast c++ executables for the ur5 robot arm (e-series xml files included). ikfast \"analytically solves robot inverse kinematics equations and generates optimized c++ files\" for fast runtime speeds (more about ikfast here). ikfast can be used in tandem with urscript speedj commands on ur robot arms for real-time motion planning, which was used to create the visual servoing demo shown on the right (part of an ongoing project on closed-loop grasping with deep learning). why speedj? see this ur performance analysis report.\nnote: this package can be easily modified to support other robot arms.\nfiles\nur5.robot.xml - a custom openrave xml file describing the kinematics of the ur5 robot arm. modify this if you change the arm or -----> tool !!!  center point (tcp) position.\nikfast61.cpp - c++ code at the heart of ikfast, generated by openrave using ur5.robot.xml. no need to modify this.\nikfast.h - a c++ header file necessary for compiling ikfast61.cpp. no need to modify this.\nikfast_wrapper.cpp - a c++ wrapper around ikfast61.cpp. includes forward kinematics in addition to the inverse kinematics provided by ikfast61.cpp. modify this to change how fk and ik results are passed to your code.\nikfastpy.pyx, kinematics.hpp, setup.py - cython code to link c++ with python.\ndemo.py - a demo in python to test fk and ik calls to ikfast.\ninstallation\nthis implementation requires the following dependencies (tested on ubuntu 16.04.4 lts):\nnumpy, cython. you can quickly install/update these dependencies by running the following:\npip install --user numpy cython\nquick start\ncheckout this repository and compile the cython wrapper:\ngit clone https://github.com/andyzeng/ikfastpy.git\ncd ikfastpy\npython setup.py build_ext --inplace\nrun the demo in python to test fk and ik calls to ikfast:\npython demo.py\nimportant: ensure all rotation matrices are valid before feeding into ikfast, otherwise no ik solution will be detected. r is a rotation matrix if and only if r is orthogonal, i.e. rrt = rtr = i, and det(r) = 1.\nnote: ikfast does not return solutions for singularities. in most cases, an approximate ik solution can be found for singularities by slightly perturbing the target end effector pose before re-computing ik solutions.\nmodifying robot kinematics with openrave\ndownload and install openrave. see these installation instructions for ubuntu 16.04.\nmodify the kinematics of the arm or tcp position (link6) by changing ur5.robot.xml respectively. you can find a description of the openrave xml file format here.\n(optional) debug the kinematics using openrave's viewer:\nopenrave ur5.robot.xml\n(optional) check the links in your file:\nopenrave-robot.py ur5.robot.xml --info links\nuse openrave to re-generate the ikfast c++ code ikfast61.cpp.\npython `openrave-config --python-dir`/openravepy/_openravepy_/ikfast.py --robot=ur5.robot.xml --iktype=transform6d --baselink=0 --eelink=6 --savefile=ikfast61.cpp --maxcasedepth 1\ncitation\nif you find ikfast useful, please cite openrave:\n@phdthesis{diankov_thesis,\nauthor = \"rosen diankov\",\ntitle = \"automated construction of robotic manipulation programs\",\nschool = \"carnegie mellon university, robotics institute\",\nmonth = \"august\",\nyear = \"2010\",\nnumber= \"cmu-ri-tr-10-29\",\nurl={http://www.programmingvision.com/rosen_diankov_thesis.pdf},\n}\nthis module was also a part of visual pushing and grasping. if you find it useful in your work, please consider citing:\n@inproceedings{zeng2018learning,\ntitle={learning synergies between pushing and grasping with self-supervised deep reinforcement learning},\nauthor={zeng, andy and song, shuran and welker, stefan and lee, johnny and rodriguez, alberto and funkhouser, thomas},\nbooktitle={ieee/rsj international conference on intelligent robots and systems (iros)},\nyear={2018}\n}", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000276, "year": null}, {"Unnamed: 0": 1297, "autor": 277, "date": null, "content": "rosbag2video\nrosbag2video.py\nrosbag to video file conversion tool\nby Maximilian Laiacker 2020\npost@mlaiacker.de\nwith contributions from Abel Gabor 2019\nbaquatelle@gmail.com\nFor use with ROS2 bags, please refer to the foxy branch.\nFor use with ROS1 bags, please proceed with the instructions below.\nInstall:\nffmpeg is needed and can be installed on Ubuntu with:\nsudo apt install ffmpeg\nros and other stuff\nsudo apt install python3-roslib python3-sensor-msgs python3-opencv\nUsage:\nrosbag2video.py [--fps 25] [--rate 1] [-o outputfile] [-v] [-s] [-t topic] bagfile1 [bagfile2] ...\nConverts image sequence(s) in ros bag file(s) to video file(s) with fixed frame rate using ffmpeg\nffmpeg needs to be installed!\n--fps Sets FPS value that is passed to ffmpeg\nDefault is 25.\n-h Displays this help.\n--ofile (-o) sets output file name.\nIf no output file name (-o) is given the filename '<prefix><topic>.mp4' is used and default output codec is h264.\nMultiple image topics are supported only when -o option is _not_ used.\nffmpeg will guess the format according to given extension.\nCompressed and raw image messages are supported with mono8 and bgr8/rgb8/bggr8/rggb8 formats.\n--rate (-r) You may slow down or speed up the video.\nDefault is 1.0, that keeps the original speed.\n-s Shows each and every image extracted from the rosbag file (cv_bride is needed).\n--topic (-t) Only the images from topic \"topic\" are used for the video output.\n-v Verbose messages are displayed.\n--prefix (-p) set a output file name prefix othervise 'bagfile1' is used (if -o is not set).\n--start Optional start time in seconds.\n--end Optional end time in seconds.\nExample Output:\n./rosbag2video.py camera_and_state.bag\nrosbag2video, by Maximilian Laiacker 2020 and Abel Gabor 2019\n############# COMPRESSED IMAGE ######################\n/image_raw/compressed with datatype: sensor_msgs/CompressedImage\nframe= 77 fps= 13 q=28.0 size= 1280kB time=00:00:00.96 bitrate=10922.2kbits/s speed=0.156x", "link": "https://github.com/mlaiacker/rosbag2video", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "rosbag2video\nrosbag2video.py\nrosbag to video file conversion -----> tool !!! \nby maximilian laiacker 2020\npost@mlaiacker.de\nwith contributions from abel gabor 2019\nbaquatelle@gmail.com\nfor use with ros2 bags, please refer to the foxy branch.\nfor use with ros1 bags, please proceed with the instructions below.\ninstall:\nffmpeg is needed and can be installed on ubuntu with:\nsudo apt install ffmpeg\nros and other stuff\nsudo apt install python3-roslib python3-sensor-msgs python3-opencv\nusage:\nrosbag2video.py [--fps 25] [--rate 1] [-o outputfile] [-v] [-s] [-t topic] bagfile1 [bagfile2] ...\nconverts image sequence(s) in ros bag file(s) to video file(s) with fixed frame rate using ffmpeg\nffmpeg needs to be installed!\n--fps sets fps value that is passed to ffmpeg\ndefault is 25.\n-h displays this help.\n--ofile (-o) sets output file name.\nif no output file name (-o) is given the filename '<prefix><topic>.mp4' is used and default output codec is h264.\nmultiple image topics are supported only when -o option is _not_ used.\nffmpeg will guess the format according to given extension.\ncompressed and raw image messages are supported with mono8 and bgr8/rgb8/bggr8/rggb8 formats.\n--rate (-r) you may slow down or speed up the video.\ndefault is 1.0, that keeps the original speed.\n-s shows each and every image extracted from the rosbag file (cv_bride is needed).\n--topic (-t) only the images from topic \"topic\" are used for the video output.\n-v verbose messages are displayed.\n--prefix (-p) set a output file name prefix othervise 'bagfile1' is used (if -o is not set).\n--start optional start time in seconds.\n--end optional end time in seconds.\nexample output:\n./rosbag2video.py camera_and_state.bag\nrosbag2video, by maximilian laiacker 2020 and abel gabor 2019\n############# compressed image ######################\n/image_raw/compressed with datatype: sensor_msgs/compressedimage\nframe= 77 fps= 13 q=28.0 size= 1280kb time=00:00:00.96 bitrate=10922.2kbits/s speed=0.156x", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000277, "year": null}, {"Unnamed: 0": 1315, "autor": 295, "date": null, "content": "Dense 3D Reconstruction from Stereo\nThis is a ROS package for real-time 3D reconstruction from stereo images. Currently this version uses LIBELAS for generating dense disparity maps as a baseline. The method for generation of disparity maps can be changed based on user preferences.\nThis package serves as a visualization tool for dense disparity maps and point clouds. Additionally, a tool for transforming point clouds to a different reference frame is also included.\nUsually, the point clouds are formed in the reference frame of the left camera. For ground robots, often the point clouds need to be transformed to a different frame e.g., a reference frame with the origin at the centre of rotation of the robot projected on to the ground plane. These transformations are hard to calculate mathematically - this tool can be used to find the transformations visually.\nAuthor: Sourish Ghosh\nDependencies\nA C++ compiler (e.g., GCC)\nROS Indigo\ncmake\npopt\nBoost\nOpenCV\nStereo Calibration\nA calibrated pair of cameras is required for stereo rectification and calibration files should be stored in a .yml file. This repository contains all the tools and instructions to calibrate stereo cameras.\nThe rotation and translation matrices for the point cloud transformation should be named as XR and XT in the calibration file. XR should be a 3 x 3 matrix and XT should be a 3 x 1 matrix. Please see a sample calibration file in the calibration/ folder.\nCompiling\nClone the repository:\n$ git clone https://github.com/umass-amrl/stereo_dense_reconstruction\nFor compiling the ROS package, rosbuild is used. Add the path of the ROS package to ROS_PACKAGE_PATH and put the following line in your .bashrc file. Replace PATH by the actual path where you have cloned the repository:\n$ export ROS_PACKAGE_PATH=$ROS_PACKAGE_PATH:/PATH\nExecute the build.sh script:\n$ cd stereo_dense_reconstruction\n$ chmod +x build.sh\n$ ./build.sh\nRunning Dense 3D Reconstruction\n$ ./bin/dense_reconstruction [OPTION...]\nUsage: dense_reconstruction [OPTION...]\n-l, --left_topic=STR Left image topic name\n-r, --right_topic=STR Right image topic name\n-c, --calib_file=STR Stereo calibration file name\n-w, --calib_width=NUM Calibration image width\n-h, --calib_height=NUM Calibration image height\n-u, --out_width=NUM Rectified image width\n-v, --out_height=NUM Rectified image height\n-d, --debug=NUM Set d=1 for cam to robot frame calibration\nThis node outputs the dense disparity map as a grayscale image on the topic /camera/left/disparity_map and the corresponding point cloud on the topic /camera/left/point_cloud.\nA sample dataset can be found here.\nPoint Cloud Transformation\nThe point cloud can be viewed on rviz by running:\n$ rosrun rviz rviz\nTo transform the point cloud to a different reference frame, the XR and XT matrices (rotation and translation) in the calibration file need to be changed. This can be done real-time by the running:\n$ rosrun rqt_reconfigure rqt_reconfigure\nIf you change the Euler Angles in rqt_reconfigure you should be able to see the point cloud transform. Don't forget to set d=1 when running the dense_reconstruction node. This prints out the new transformation matrices as you transform the point cloud.\nLicense\nThis software is released under the GNU GPL v3 license.", "link": "https://github.com/ut-amrl/stereo_dense_reconstruction", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "dense 3d reconstruction from stereo\nthis is a ros package for real-time 3d reconstruction from stereo images. currently this version uses libelas for generating dense disparity maps as a baseline. the method for generation of disparity maps can be changed based on user preferences.\nthis package serves as a visualization -----> tool !!!  for dense disparity maps and point clouds. additionally, a tool for transforming point clouds to a different reference frame is also included.\nusually, the point clouds are formed in the reference frame of the left camera. for ground robots, often the point clouds need to be transformed to a different frame e.g., a reference frame with the origin at the centre of rotation of the robot projected on to the ground plane. these transformations are hard to calculate mathematically - this tool can be used to find the transformations visually.\nauthor: sourish ghosh\ndependencies\na c++ compiler (e.g., gcc)\nros indigo\ncmake\npopt\nboost\nopencv\nstereo calibration\na calibrated pair of cameras is required for stereo rectification and calibration files should be stored in a .yml file. this repository contains all the tools and instructions to calibrate stereo cameras.\nthe rotation and translation matrices for the point cloud transformation should be named as xr and xt in the calibration file. xr should be a 3 x 3 matrix and xt should be a 3 x 1 matrix. please see a sample calibration file in the calibration/ folder.\ncompiling\nclone the repository:\n$ git clone https://github.com/umass-amrl/stereo_dense_reconstruction\nfor compiling the ros package, rosbuild is used. add the path of the ros package to ros_package_path and put the following line in your .bashrc file. replace path by the actual path where you have cloned the repository:\n$ export ros_package_path=$ros_package_path:/path\nexecute the build.sh script:\n$ cd stereo_dense_reconstruction\n$ chmod +x build.sh\n$ ./build.sh\nrunning dense 3d reconstruction\n$ ./bin/dense_reconstruction [option...]\nusage: dense_reconstruction [option...]\n-l, --left_topic=str left image topic name\n-r, --right_topic=str right image topic name\n-c, --calib_file=str stereo calibration file name\n-w, --calib_width=num calibration image width\n-h, --calib_height=num calibration image height\n-u, --out_width=num rectified image width\n-v, --out_height=num rectified image height\n-d, --debug=num set d=1 for cam to robot frame calibration\nthis node outputs the dense disparity map as a grayscale image on the topic /camera/left/disparity_map and the corresponding point cloud on the topic /camera/left/point_cloud.\na sample dataset can be found here.\npoint cloud transformation\nthe point cloud can be viewed on rviz by running:\n$ rosrun rviz rviz\nto transform the point cloud to a different reference frame, the xr and xt matrices (rotation and translation) in the calibration file need to be changed. this can be done real-time by the running:\n$ rosrun rqt_reconfigure rqt_reconfigure\nif you change the euler angles in rqt_reconfigure you should be able to see the point cloud transform. don't forget to set d=1 when running the dense_reconstruction node. this prints out the new transformation matrices as you transform the point cloud.\nlicense\nthis software is released under the gnu gpl v3 license.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000295, "year": null}, {"Unnamed: 0": 1320, "autor": 300, "date": null, "content": "Air Learning\nAerial robotics is a cross-layer, interdisciplinary field. Air Learning is an effort to bridge seemingly disparate fields.\nDesigning an autonomous robot to perform a task involves interactions between various boundaries spanning from modeling the environment down to the choice of onboard computer platform available in the robot. Our goal through building Air Learning is to provide researchers with a cross-domain infrastructure that allows them to holistically study and evaluate reinforcement learning algorithms for autonomous aerial machines. Air Learning is built on top of several open-source tools such as Microsoft AirSim, OpenAI Gym, Stable-Baselines, and Epic games Unreal Engine.\nWe depict the main components of the Air Learning infrastructure below:\nKey features in this version of Air Learning includes:\nA photorealistic, configurable, and random environment generator based unreal game engine for facilitating domain randomization for the aerial robot.\nWe use Microsoft's AirSim plugin in our configurable environment generator for aerial robot model and physics. We augment the AirSim plugin by having an energy model for the aerial robot.\nOpenAI gym interface to environment generator to train a variety of reinforcement learning algorithms.\nA tight coupling between the parameters of environment generator and reinforcement learning to provide infrastructure to train aerial robot using curriculum learning.\nQuality of Flight metrics to evaluate the performance of reinforcement learning.\nBenchmarking the performance of reinforcement learning algorithm on different onboard computer platforms using hardware-in-the-loop methodology, thus allowing to study the problem holistically.\nSource seeking application\nAir Learning Environment Generator\nLearning algorithms are data hungry, and the availability of high-quality data is vital for the learning process. Also, an environment that is good to learn from should include different scenarios that are challenging for the robot. By adding these challenging situations, they learn to solve those challenges. For instance, for teaching a robot to navigate obstacles, the data set should have a wide variety of obstacles (materials, textures, speeds, etc.) during the training process.\nWe designed an environment generator specifically targeted for autonomous UAVs. Air Learning's environment generator creates high fidelity photo-realistic environments for the UAVs to fly in. The environment generator is built on top of UE4 and uses the AirSim UE4 plugin for the UAV model and flight physics. The environment generator with the AirSim plugin is exposed as OpenAI gym interface.\nThe environment generator has different configuration knobs for generating challenging environments. The configuration knobs available in the current version can be classified into two categories. The first category includes the parameters that can be controlled via a game configuration file. The second category consists of the parameters that can be controlled outside the game configuration file. The full list of settings/parameters that we can control are shown in the tabulated here:\nDemonstration of Air Learning Environment Generator\nHere is a simple demo to show how we can use Air Learning environment generator to convert a UE4 mesh into a dynamic obstacle.\nApplying different materials to the same obstacles:\nFor information on installing Air Learning environment generator, check the following repository or follow the air learning-ue4 sub-module in this repository.\nAir Learning RL\nDeep reinforcement learning is still a nascent field that is rapidly evolving. Hence, there is significant infrastructure overhead to integrate a simulator and evaluate new deep reinforcement learning algorithms for UAVs.\nSo, we expose our random environment generator and AirSim UE4 plugin as an OpenAI gym interface and integrate its popular reinforcement learning framework with stable baselines (which is based on OpenAI baselines) and Keras-RL. To expose our random environment generator into an OpenAI gym interface, we extend the work of AirGym to add support for environment randomization, a wide range of sensors (Depth image, Inertial Measurement Unit (IMU) data, RGB image, etc.) from AirSim and support exploring multimodal policies.\nCurrently, we support two reinforcement learning algorithms one for discrete actions control and one for continuous action control:\nDeep Q-Networks (DQN)\nProximal Policy Optimization (PPO)\nUsing Air Learning, we can train different reinforcement learning algorithms. Here is the video demonstration of using Air Learning environment generator used in RL training.\nHardware Evaluation\nOften aerial roboticists port the algorithm onto UAVs to validate the functionality of the algorithms. These UAVs can be custom built or commercially available off-the-shelf (COTS) UAVs but mostly have fixed hardware that can be used as onboard compute. A critical shortcoming of this approach is that the roboticist cannot experiment with hardware changes. More powerful hardware may (or may not) unlock additional capabilities during flight, but there is no way to know until the hardware is available on a real UAV so that the roboticist can physically experiment with the platform.\nReasons for wanting to do such exploration includes understanding the computational requirements of the system, quantifying the energy consumption implications as a result of interactions between the algorithm and the hardware, and so forth. Such evaluation is crucial to determine whether an algorithm is, in fact, feasible when ported to a real UAV with a specific hardware configuration and battery constraints.\nFor instance, a Parrot Bepop comes with a P7 dual-core CPU Cortex A9 and a Quad core GPU. It is not possible to fly the UAV assuming a different piece of hardware, such as the NVIDIA Xavier processor that is significantly more powerful; at the time of this writing, there is no COTS UAV that contains the Xavier platform. So, one would have to wait until a commercially viable platform is available. However, using Air Learning, one can experiment how the UAV would behave with a Xavier since the UAV is flying virtually.\nAir Learning uses Hardware-in-the-loop (HIL) methodology for system evaluation. A HIL simulation combines the benefits of the real design and the simulation by allowing them to interact with one another, as shown in the figure above. There are three core components in Air Learning HIL methodology:\nA high-end desktop that simulates a virtual environment flying the UAV.\nAn embedded system that runs the operating system, the deep reinforcement learning algorithms, policies, and associated software stack\nA flight controller that controls the flight of the UAV in the simulated environment.\nQuality of Flight Metrics\nThe success rate has been the sole criteria for evaluating reinforcement learning algorithms so far. It kind of makes sense for domains like computer games. However, for applying the success of these learning algorithms to a mobile robot (like UAVs), success rate alone seldom captures the efficacy of the algorithm. For instance, UAVs are heavily constrained by the compute capability as well as the amount of energy available onboard. To give a very high-level perspective, In self-driving cars, the compute prototypes alone consume about 2.5kW of power which translate to reduced range of the car. In contrast to the car, the situation is extremely dire for UAVs. For example:\nThe energy delta between a car and UAVs (High-end) is about 1000x. Hence an algorithm designed specifically for UAVs has to account for these constrains early on in the design phase. Hence when we developed Air Learning, we baked other metrics (beyond success rate) that quantifies the overall quality of flight. In this version of Air Learning, we have the following quality of flight metrics:\nSuccess rate\nEnergy per mission\nDistance Travelled\nFlight time\nHow to get it\nCurrently, Air Learning is tested on Windows 10. For Linux users, please stay tuned!\nThere are two parts to Air Learning.\nThe Air Learning environment generator (Installation instruction)\nAir Learning Reinforcement Learning Training (Installation instruction)\nParticipate\nPaper\nMore technical information and insights on using Air Learning can be found here:\n@article{krishnan2021air,\ntitle={Air Learning: a deep reinforcement learning gym for autonomous aerial robot visual navigation},\nauthor={Krishnan, Srivatsan and Boroujerdian, Behzad and Fu, William and Faust, Aleksandra and Reddi, Vijay Janapa},\njournal={Machine Learning},\npages={1--40},\nyear={2021},\npublisher={Springer}\n}\nPress Coverage\nTake a look at the press coverage and some of our future work in using this tool.\nContribute\nAir Learning aims in softening the boundaries between Robotics, Reinforcement learning, Controls, and System architecture by providing a cross-disciplinary infrastructure for researchers in this domain. If like us, you also believe in looking at the problem holistically, please reach out to us. Also, we welcome any contributions that make this tool better or easy to use.\nContributors\nSrivatsan Krishnan (Harvard University)\nBehzad Boroujerdian (The University of Texas at Austin)\nWilliam Fu (Harvard University)\nAleksandra Faust (Robotics at Google)\nVijay Janapa Reddi (Harvard University)\nBardienus Duisterhof (TU Delft/ Harvard University)\nContact (Maintainers)\nSrivatsan Krishnan (srivatsan@seas.harvard.edu)\nBardienus Duisterhof (b.p.duisterhof@gmail.com)", "link": "https://github.com/harvard-edge/AirLearning", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "air learning\naerial robotics is a cross-layer, interdisciplinary field. air learning is an effort to bridge seemingly disparate fields.\ndesigning an autonomous robot to perform a task involves interactions between various boundaries spanning from modeling the environment down to the choice of onboard computer platform available in the robot. our goal through building air learning is to provide researchers with a cross-domain infrastructure that allows them to holistically study and evaluate reinforcement learning algorithms for autonomous aerial machines. air learning is built on top of several open-source tools such as microsoft airsim, openai gym, stable-baselines, and epic games unreal engine.\nwe depict the main components of the air learning infrastructure below:\nkey features in this version of air learning includes:\na photorealistic, configurable, and random environment generator based unreal game engine for facilitating domain randomization for the aerial robot.\nwe use microsoft's airsim plugin in our configurable environment generator for aerial robot model and physics. we augment the airsim plugin by having an energy model for the aerial robot.\nopenai gym interface to environment generator to train a variety of reinforcement learning algorithms.\na tight coupling between the parameters of environment generator and reinforcement learning to provide infrastructure to train aerial robot using curriculum learning.\nquality of flight metrics to evaluate the performance of reinforcement learning.\nbenchmarking the performance of reinforcement learning algorithm on different onboard computer platforms using hardware-in-the-loop methodology, thus allowing to study the problem holistically.\nsource seeking application\nair learning environment generator\nlearning algorithms are data hungry, and the availability of high-quality data is vital for the learning process. also, an environment that is good to learn from should include different scenarios that are challenging for the robot. by adding these challenging situations, they learn to solve those challenges. for instance, for teaching a robot to navigate obstacles, the data set should have a wide variety of obstacles (materials, textures, speeds, etc.) during the training process.\nwe designed an environment generator specifically targeted for autonomous uavs. air learning's environment generator creates high fidelity photo-realistic environments for the uavs to fly in. the environment generator is built on top of ue4 and uses the airsim ue4 plugin for the uav model and flight physics. the environment generator with the airsim plugin is exposed as openai gym interface.\nthe environment generator has different configuration knobs for generating challenging environments. the configuration knobs available in the current version can be classified into two categories. the first category includes the parameters that can be controlled via a game configuration file. the second category consists of the parameters that can be controlled outside the game configuration file. the full list of settings/parameters that we can control are shown in the tabulated here:\ndemonstration of air learning environment generator\nhere is a simple demo to show how we can use air learning environment generator to convert a ue4 mesh into a dynamic obstacle.\napplying different materials to the same obstacles:\nfor information on installing air learning environment generator, check the following repository or follow the air learning-ue4 sub-module in this repository.\nair learning rl\ndeep reinforcement learning is still a nascent field that is rapidly evolving. hence, there is significant infrastructure overhead to integrate a simulator and evaluate new deep reinforcement learning algorithms for uavs.\nso, we expose our random environment generator and airsim ue4 plugin as an openai gym interface and integrate its popular reinforcement learning framework with stable baselines (which is based on openai baselines) and keras-rl. to expose our random environment generator into an openai gym interface, we extend the work of airgym to add support for environment randomization, a wide range of sensors (depth image, inertial measurement unit (imu) data, rgb image, etc.) from airsim and support exploring multimodal policies.\ncurrently, we support two reinforcement learning algorithms one for discrete actions control and one for continuous action control:\ndeep q-networks (dqn)\nproximal policy optimization (ppo)\nusing air learning, we can train different reinforcement learning algorithms. here is the video demonstration of using air learning environment generator used in rl training.\nhardware evaluation\noften aerial roboticists port the algorithm onto uavs to validate the functionality of the algorithms. these uavs can be custom built or commercially available off-the-shelf (cots) uavs but mostly have fixed hardware that can be used as onboard compute. a critical shortcoming of this approach is that the roboticist cannot experiment with hardware changes. more powerful hardware may (or may not) unlock additional capabilities during flight, but there is no way to know until the hardware is available on a real uav so that the roboticist can physically experiment with the platform.\nreasons for wanting to do such exploration includes understanding the computational requirements of the system, quantifying the energy consumption implications as a result of interactions between the algorithm and the hardware, and so forth. such evaluation is crucial to determine whether an algorithm is, in fact, feasible when ported to a real uav with a specific hardware configuration and battery constraints.\nfor instance, a parrot bepop comes with a p7 dual-core cpu cortex a9 and a quad core gpu. it is not possible to fly the uav assuming a different piece of hardware, such as the nvidia xavier processor that is significantly more powerful; at the time of this writing, there is no cots uav that contains the xavier platform. so, one would have to wait until a commercially viable platform is available. however, using air learning, one can experiment how the uav would behave with a xavier since the uav is flying virtually.\nair learning uses hardware-in-the-loop (hil) methodology for system evaluation. a hil simulation combines the benefits of the real design and the simulation by allowing them to interact with one another, as shown in the figure above. there are three core components in air learning hil methodology:\na high-end desktop that simulates a virtual environment flying the uav.\nan embedded system that runs the operating system, the deep reinforcement learning algorithms, policies, and associated software stack\na flight controller that controls the flight of the uav in the simulated environment.\nquality of flight metrics\nthe success rate has been the sole criteria for evaluating reinforcement learning algorithms so far. it kind of makes sense for domains like computer games. however, for applying the success of these learning algorithms to a mobile robot (like uavs), success rate alone seldom captures the efficacy of the algorithm. for instance, uavs are heavily constrained by the compute capability as well as the amount of energy available onboard. to give a very high-level perspective, in self-driving cars, the compute prototypes alone consume about 2.5kw of power which translate to reduced range of the car. in contrast to the car, the situation is extremely dire for uavs. for example:\nthe energy delta between a car and uavs (high-end) is about 1000x. hence an algorithm designed specifically for uavs has to account for these constrains early on in the design phase. hence when we developed air learning, we baked other metrics (beyond success rate) that quantifies the overall quality of flight. in this version of air learning, we have the following quality of flight metrics:\nsuccess rate\nenergy per mission\ndistance travelled\nflight time\nhow to get it\ncurrently, air learning is tested on windows 10. for linux users, please stay tuned!\nthere are two parts to air learning.\nthe air learning environment generator (installation instruction)\nair learning reinforcement learning training (installation instruction)\nparticipate\npaper\nmore technical information and insights on using air learning can be found here:\n@article{krishnan2021air,\ntitle={air learning: a deep reinforcement learning gym for autonomous aerial robot visual navigation},\nauthor={krishnan, srivatsan and boroujerdian, behzad and fu, william and faust, aleksandra and reddi, vijay janapa},\njournal={machine learning},\npages={1--40},\nyear={2021},\npublisher={springer}\n}\npress coverage\ntake a look at the press coverage and some of our future work in using this -----> tool !!! .\ncontribute\nair learning aims in softening the boundaries between robotics, reinforcement learning, controls, and system architecture by providing a cross-disciplinary infrastructure for researchers in this domain. if like us, you also believe in looking at the problem holistically, please reach out to us. also, we welcome any contributions that make this tool better or easy to use.\ncontributors\nsrivatsan krishnan (harvard university)\nbehzad boroujerdian (the university of texas at austin)\nwilliam fu (harvard university)\naleksandra faust (robotics at google)\nvijay janapa reddi (harvard university)\nbardienus duisterhof (tu delft/ harvard university)\ncontact (maintainers)\nsrivatsan krishnan (srivatsan@seas.harvard.edu)\nbardienus duisterhof (b.p.duisterhof@gmail.com)", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000300, "year": null}, {"Unnamed: 0": 1321, "autor": 301, "date": null, "content": "PyCozmo\nPyCozmo is a pure-Python communication library, alternative SDK, and application for the Cozmo robot . It allows controlling a Cozmo robot directly, without having to go through a mobile device, running the Cozmo app.\nThe library is loosely based on the Anki Cozmo Python SDK and the cozmoclad (\"C-Like Abstract Data\") library.\nThis project is a tool for exploring the hardware and software of the Digital Dream Labs (originally Anki) Cozmo robot. It is unstable and heavily under development.\nUsage\nBasic:\nimport time\nimport pycozmo\nwith pycozmo.connect() as cli:\ncli.set_head_angle(angle=0.6)\ntime.sleep(1)\nAdvanced:\nimport pycozmo\ncli = pycozmo.Client()\ncli.start()\ncli.connect()\ncli.wait_for_robot()\ncli.drive_wheels(lwheel_speed=50.0, rwheel_speed=50.0, duration=2.0)\ncli.disconnect()\ncli.stop()\nDocumentation\nhttps://pycozmo.readthedocs.io/\nRobot Support\nSensors:\nCamera\nCliff sensor\nAccelerometers\nGyro\nBattery voltage\nCube battery voltage\nCube accelerometers\nBackpack button (v1.5 hardware and newer)\nActuators:\nWheel motors\nHead motor\nLift motor\nOLED display\nSpeaker\nBackpack LEDs\nIR LED\nCube LEDs\nPlatform LEDs (when available)\nOn-board functions (see docs/functions.md for details:\nWi-Fi AP\nBluetooth LE\nLocalization\nPath tracking\nNV RAM storage\nOver-the-air (OTA) firmware updates\nOff-board functions (see docs/offboard_functions.md for details:\nProcedural face generation\nCozmo animations from FlatBuffers .bin files\nPersonality engine - work in progress\nCozmo behaviors - work in progress\nMotion detection\nObject (cube and platform) detection\nCube marker recognition\nFace detection\nFace recognition\nFacial expression estimation\nPet detection\nCamera calibration\nNavigation map building\nText-to-speech\nSongs\nExtra off-board functions:\nVector animations from FlatBuffers .bin files\nVector behaviors\nArUco marker recognition\nCozmo and Vector robot detection\nDrivable area estimation\nVoice commands\nIf you have ideas for other functionality share them via GitHub.\nTools\npycozmo_app.py - an alternative Cozmo application, implementing off-board functions (video).\npycozmo_dump.py - a command-line application that can read and annotate Cozmo communication from pcap files or capture it live using pypcap.\npycozmo_replay.py - a basic command-line application that can replay .pcap files back to Cozmo.\npycozmo_anim.py - a tool for examining and manipulating animation files.\npycozmo_update.py - a tool for over-the-air (OTA) updates of Cozmo's firmware.\npycozmo_protocol_generator.py - a tool for generating Cozmo protocol encoder code.\nNote: PyCozmo and pycozmo_protocol_generator.py in particular could be used as a base for creating a Cozmo protocol encoder code generator for languages other than Python (C/C++, Java, etc.).\nExamples\nBasic:\nminimal.py - minimal code to communicate with Cozmo, using PyCozmo\nextremes.py - demonstrates Cozmo lift and head control\nbackpack_lights.py - demonstrates Cozmo backpack LED control\ndisplay_image.py - demonstrates visualization of image files on Cozmo's display\nevents.py - demonstrates event handling\ncamera.py - demonstrates capturing a camera image\ngo_to_pose.py - demonstrates moving to a specific pose (position and orientation)\npath.py - demonstrates following a predefined path\nAdvanced:\ndisplay_lines.py - demonstrates 2D graphics, using PIL.ImageDraw on Cozmo's display (video)\nrc.py - turns Cozmo into an RC tank that can be driven with an XBox 360 Wireless controller or Logitech Gamepad F310\nvideo.py - demonstrates visualizing video captured from the camera back on display\ncube_lights.py - demonstrates cube connection and LED control\ncube_light_animation.py - demonstrates cube LED animation control\ncharger_lights.py - demonstrates Cozmo charging platform LED control\naudio.py - demonstrates 22 kHz, 16-bit, mono WAVE file playback through Cozmo's speaker\nnvram.py - demonstrates reading data from Cozmo's NVRAM (non-volatile memory)\nprocedural_face.py - demonstrates drawing a procedural face on Cozmo's display\nprocedural_face_show.py - demonstrates generating a procedural face\nprocedural_face_expressions.py - demonstrates displaying various facial expressions (video)\nanim.py - demonstrates animating Cozmo\nPyCozmo In The Wild\nExpressive Eyes - rendering various facial expressions using PyCozmo's procedural_face module\nan application that recognizes ArUco markers, using OpenCV\na ROS2 driver\nanother ROS2 driver\nConnecting to Cozmo over Wi-Fi\nA Wi-Fi connection needs to be established with Cozmo before using PyCozmo applications.\nWake up Cozmo by placing it on the charging platform\nMake Cozmo display it's Wi-Fi PSK by rising and lowering its lift\nScan for Cozmo's Wi-Fi SSID (depends on the OS)\nConnect using Cozmo's Wi-Fi PSK (depends on the OS)\nThis video summarizes the connection process.\nPyCozmo vs. the Cozmo SDK\nA Cozmo SDK application (aka \"game\") acts as a client to the Cozmo app (aka \"engine\") that runs on a mobile device. The low-level communication happens over USB and is handled by the cozmoclad library.\nIn contrast, an application using PyCozmo basically replaces the Cozmo app and acts as the \"engine\". PyCozmo handles the low-level UDP communication with Cozmo.\n+------------------+ +------------------+ +------------------+\n| SDK app | Cozmo SDK | Cozmo app | Cozmo | Cozmo |\n| \"game\" | cozmoclad | \"engine\" | protocol | \"robot\" |\n| | ----------------> | Wi-Fi client | ----------------> | Wi-Fi AP |\n| | USB | UDP client | UDP/Wi-Fi | UDP Server |\n+------------------+ +------------------+ +------------------+\nRequirements\nPython 3.6.0 or newer\nPillow 6.0.0 - Python image library\nFlatBuffers - serialization library\ndpkt - TCP/IP packet parsing library\nInstallation\nUsing pip:\npip install --user pycozmo\npycozmo_resources.py download\nFrom source:\ngit clone https://github.com/zayfod/pycozmo.git\ncd pycozmo\npython setup.py install --user\npycozmo_resources.py download\nFrom source, for development:\ngit clone git@github.com:zayfod/pycozmo.git\ncd pycozmo\npython setup.py develop --user\npip install --user -r requirements-dev.txt\npycozmo_resources.py download\nSupport\nBug reports and changes should be sent via GitHub:\nhttps://github.com/zayfod/pycozmo\nDDL Robot Discord server, channel #development-cozmo:\nhttps://discord.gg/ew92haS\nDisclaimer\nThis project is not affiliated with Digital Dream Labs or Anki.", "link": "https://github.com/zayfod/pycozmo", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "pycozmo\npycozmo is a pure-python communication library, alternative sdk, and application for the cozmo robot . it allows controlling a cozmo robot directly, without having to go through a mobile device, running the cozmo app.\nthe library is loosely based on the anki cozmo python sdk and the cozmoclad (\"c-like abstract data\") library.\nthis project is a -----> tool !!!  for exploring the hardware and software of the digital dream labs (originally anki) cozmo robot. it is unstable and heavily under development.\nusage\nbasic:\nimport time\nimport pycozmo\nwith pycozmo.connect() as cli:\ncli.set_head_angle(angle=0.6)\ntime.sleep(1)\nadvanced:\nimport pycozmo\ncli = pycozmo.client()\ncli.start()\ncli.connect()\ncli.wait_for_robot()\ncli.drive_wheels(lwheel_speed=50.0, rwheel_speed=50.0, duration=2.0)\ncli.disconnect()\ncli.stop()\ndocumentation\nhttps://pycozmo.readthedocs.io/\nrobot support\nsensors:\ncamera\ncliff sensor\naccelerometers\ngyro\nbattery voltage\ncube battery voltage\ncube accelerometers\nbackpack button (v1.5 hardware and newer)\nactuators:\nwheel motors\nhead motor\nlift motor\noled display\nspeaker\nbackpack leds\nir led\ncube leds\nplatform leds (when available)\non-board functions (see docs/functions.md for details:\nwi-fi ap\nbluetooth le\nlocalization\npath tracking\nnv ram storage\nover-the-air (ota) firmware updates\noff-board functions (see docs/offboard_functions.md for details:\nprocedural face generation\ncozmo animations from flatbuffers .bin files\npersonality engine - work in progress\ncozmo behaviors - work in progress\nmotion detection\nobject (cube and platform) detection\ncube marker recognition\nface detection\nface recognition\nfacial expression estimation\npet detection\ncamera calibration\nnavigation map building\ntext-to-speech\nsongs\nextra off-board functions:\nvector animations from flatbuffers .bin files\nvector behaviors\naruco marker recognition\ncozmo and vector robot detection\ndrivable area estimation\nvoice commands\nif you have ideas for other functionality share them via github.\ntools\npycozmo_app.py - an alternative cozmo application, implementing off-board functions (video).\npycozmo_dump.py - a command-line application that can read and annotate cozmo communication from pcap files or capture it live using pypcap.\npycozmo_replay.py - a basic command-line application that can replay .pcap files back to cozmo.\npycozmo_anim.py - a tool for examining and manipulating animation files.\npycozmo_update.py - a tool for over-the-air (ota) updates of cozmo's firmware.\npycozmo_protocol_generator.py - a tool for generating cozmo protocol encoder code.\nnote: pycozmo and pycozmo_protocol_generator.py in particular could be used as a base for creating a cozmo protocol encoder code generator for languages other than python (c/c++, java, etc.).\nexamples\nbasic:\nminimal.py - minimal code to communicate with cozmo, using pycozmo\nextremes.py - demonstrates cozmo lift and head control\nbackpack_lights.py - demonstrates cozmo backpack led control\ndisplay_image.py - demonstrates visualization of image files on cozmo's display\nevents.py - demonstrates event handling\ncamera.py - demonstrates capturing a camera image\ngo_to_pose.py - demonstrates moving to a specific pose (position and orientation)\npath.py - demonstrates following a predefined path\nadvanced:\ndisplay_lines.py - demonstrates 2d graphics, using pil.imagedraw on cozmo's display (video)\nrc.py - turns cozmo into an rc tank that can be driven with an xbox 360 wireless controller or logitech gamepad f310\nvideo.py - demonstrates visualizing video captured from the camera back on display\ncube_lights.py - demonstrates cube connection and led control\ncube_light_animation.py - demonstrates cube led animation control\ncharger_lights.py - demonstrates cozmo charging platform led control\naudio.py - demonstrates 22 khz, 16-bit, mono wave file playback through cozmo's speaker\nnvram.py - demonstrates reading data from cozmo's nvram (non-volatile memory)\nprocedural_face.py - demonstrates drawing a procedural face on cozmo's display\nprocedural_face_show.py - demonstrates generating a procedural face\nprocedural_face_expressions.py - demonstrates displaying various facial expressions (video)\nanim.py - demonstrates animating cozmo\npycozmo in the wild\nexpressive eyes - rendering various facial expressions using pycozmo's procedural_face module\nan application that recognizes aruco markers, using opencv\na ros2 driver\nanother ros2 driver\nconnecting to cozmo over wi-fi\na wi-fi connection needs to be established with cozmo before using pycozmo applications.\nwake up cozmo by placing it on the charging platform\nmake cozmo display it's wi-fi psk by rising and lowering its lift\nscan for cozmo's wi-fi ssid (depends on the os)\nconnect using cozmo's wi-fi psk (depends on the os)\nthis video summarizes the connection process.\npycozmo vs. the cozmo sdk\na cozmo sdk application (aka \"game\") acts as a client to the cozmo app (aka \"engine\") that runs on a mobile device. the low-level communication happens over usb and is handled by the cozmoclad library.\nin contrast, an application using pycozmo basically replaces the cozmo app and acts as the \"engine\". pycozmo handles the low-level udp communication with cozmo.\n+------------------+ +------------------+ +------------------+\n| sdk app | cozmo sdk | cozmo app | cozmo | cozmo |\n| \"game\" | cozmoclad | \"engine\" | protocol | \"robot\" |\n| | ----------------> | wi-fi client | ----------------> | wi-fi ap |\n| | usb | udp client | udp/wi-fi | udp server |\n+------------------+ +------------------+ +------------------+\nrequirements\npython 3.6.0 or newer\npillow 6.0.0 - python image library\nflatbuffers - serialization library\ndpkt - tcp/ip packet parsing library\ninstallation\nusing pip:\npip install --user pycozmo\npycozmo_resources.py download\nfrom source:\ngit clone https://github.com/zayfod/pycozmo.git\ncd pycozmo\npython setup.py install --user\npycozmo_resources.py download\nfrom source, for development:\ngit clone git@github.com:zayfod/pycozmo.git\ncd pycozmo\npython setup.py develop --user\npip install --user -r requirements-dev.txt\npycozmo_resources.py download\nsupport\nbug reports and changes should be sent via github:\nhttps://github.com/zayfod/pycozmo\nddl robot discord server, channel #development-cozmo:\nhttps://discord.gg/ew92has\ndisclaimer\nthis project is not affiliated with digital dream labs or anki.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000301, "year": null}, {"Unnamed: 0": 1328, "autor": 308, "date": null, "content": "ROS TCP Connector\nIntroduction\nThis repository contains two Unity packages: the ROS TCP Connector, for sending/receiving messages from ROS, and the Visualizations Package, for adding visualizations of incoming and outgoing messages in the Unity scene.\nInstallation\nUsing Unity 2020.2 or later, open the Package Manager from Window -> Package Manager.\nIn the Package Manager window, find and click the + button in the upper lefthand corner of the window. Select Add package from git URL....\nEnter the git URL for the desired package. Note: you can append a version tag to the end of the git url, like #v0.4.0 or #v0.5.0, to declare a specific package version, or exclude the tag to get the latest from the package's main branch.\nFor the ROS-TCP-Connector, enter https://github.com/Unity-Technologies/ROS-TCP-Connector.git?path=/com.unity.robotics.ros-tcp-connector.\nFor Visualizations, enter https://github.com/Unity-Technologies/ROS-TCP-Connector.git?path=/com.unity.robotics.visualizations.\nClick Add.\nTo install from a local clone of the repository, see installing a local package in the Unity manual.\nTutorials\nThis Unity package provides four main features:\nROSConnection: A component that sets up communication between ROS and Unity. See the Unity Robotics Hub for information and tutorials.\nMessage Generation: A tool to generate C# classes for ROS message types.\nVisualizations: A suite of default configurations and APIs to visualize incoming and outgoing information from ROS.\nSee the Nav2 SLAM Example for tutorials on using the Visualizations Package!\nYou can also view the package's Usage Information for more details on using the package in your own project.\nROSGeometry: A set of extensions that convert geometries between Unity and other coordinate frames.\nROS#\nSpecial thanks to the Siemens ROS# Project Team for driving the ROS-Unity Integration Open Source since 2017.\nCommunity and Feedback\nThe Unity Robotics projects are open-source and we encourage and welcome contributions. If you wish to contribute, be sure to review our contribution guidelines and code of conduct.\nSupport\nFor questions or discussions about Unity Robotics package installations or how to best set up and integrate your robotics projects, please create a new thread on the Unity Robotics forum and make sure to include as much detail as possible.\nFor feature requests, bugs, or other issues, please file a GitHub issue using the provided templates and the Robotics team will investigate as soon as possible.\nFor any other questions or feedback, connect directly with the Robotics team at unity-robotics@unity3d.com.\nLicense\nApache License 2.0", "link": "https://github.com/Unity-Technologies/ROS-TCP-Connector", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "ros tcp connector\nintroduction\nthis repository contains two unity packages: the ros tcp connector, for sending/receiving messages from ros, and the visualizations package, for adding visualizations of incoming and outgoing messages in the unity scene.\ninstallation\nusing unity 2020.2 or later, open the package manager from window -> package manager.\nin the package manager window, find and click the + button in the upper lefthand corner of the window. select add package from git url....\nenter the git url for the desired package. note: you can append a version tag to the end of the git url, like #v0.4.0 or #v0.5.0, to declare a specific package version, or exclude the tag to get the latest from the package's main branch.\nfor the ros-tcp-connector, enter https://github.com/unity-technologies/ros-tcp-connector.git?path=/com.unity.robotics.ros-tcp-connector.\nfor visualizations, enter https://github.com/unity-technologies/ros-tcp-connector.git?path=/com.unity.robotics.visualizations.\nclick add.\nto install from a local clone of the repository, see installing a local package in the unity manual.\ntutorials\nthis unity package provides four main features:\nrosconnection: a component that sets up communication between ros and unity. see the unity robotics hub for information and tutorials.\nmessage generation: a -----> tool !!!  to generate c# classes for ros message types.\nvisualizations: a suite of default configurations and apis to visualize incoming and outgoing information from ros.\nsee the nav2 slam example for tutorials on using the visualizations package!\nyou can also view the package's usage information for more details on using the package in your own project.\nrosgeometry: a set of extensions that convert geometries between unity and other coordinate frames.\nros#\nspecial thanks to the siemens ros# project team for driving the ros-unity integration open source since 2017.\ncommunity and feedback\nthe unity robotics projects are open-source and we encourage and welcome contributions. if you wish to contribute, be sure to review our contribution guidelines and code of conduct.\nsupport\nfor questions or discussions about unity robotics package installations or how to best set up and integrate your robotics projects, please create a new thread on the unity robotics forum and make sure to include as much detail as possible.\nfor feature requests, bugs, or other issues, please file a github issue using the provided templates and the robotics team will investigate as soon as possible.\nfor any other questions or feedback, connect directly with the robotics team at unity-robotics@unity3d.com.\nlicense\napache license 2.0", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000308, "year": null}, {"Unnamed: 0": 1333, "autor": 313, "date": null, "content": "Catalogue\nThis is an umbrella repository which briefly introduces each of the components (also referred as Generic Enablers) which are part of FIWARE and allow users and developers to easily navigate to the relevant source code repositories, documentation and Docker images.\nMore information about what FIWARE is and how to use it can be found within the FIWARE Tour Guide, the FIWARE Academy and the Step-by-Step Tutorials. Generic questions about FIWARE can be asked on Ask.FIWARE and developer-based questions can be submitted on Stack Overflow using the FIWARE tag.\ndeveloper.\u200bfiware.org \ud83d\udcda\nTour Guide \ud83c\udf93\nAcademy \ud83d\udd20\nTutorials \u2753\nAsk.\u200bFIWARE\nContents\nDetails\nBackground\nFIWARE brings a curated framework of open source software platform components which can be assembled together and with other third-party components to build platforms that support the development of Smart Solutions faster, easier and cheaper. The main and only mandatory component of any \u201cPowered by FIWARE\u201d platform or solution is a FIWARE Context Broker Generic Enabler, supplying a cornerstone function required in any smart solution: the need to manage context information, enabling to perform updates and bring access to context.\nFIWARE NGSI is the API exported by a FIWARE Context Broker, used for the integration of platform components within a \"Powered by FIWARE\" platform and by applications to update or consume context information. FIWARE NGSI API specifications have evolved over time, initially matching NGSI-v2 specifications, now aligning with the ETSI NGSI-LD standard. The FIWARE Community plays an active role in the evolution of ETSI NGSI-LD specifications which were based on NGSI-v2 and commits to deliver compatible open source implementations of the specs.\nBuilding around the FIWARE Context Broker, a rich suite of complementary open-source FIWARE Generic Enablers are available, dealing with the following:\nCore Context Management manipulates and stores context data so it can be used for further processesing\nInterfacing with the Internet of Things (IoT), Robots and third-party systems, for capturing updates on context information and translating required actuations.\nProcessing, analysis and visualization of context information, implementing the expected smart behaviour of applications and/or assisting end users in making smart decisions.\nContext Data/API management, publication and monetization, bringing support to usage control and the opportunity to publish and monetize part of managed context data.\nDeployment Tools such as Docker images and Helm-chart recipes are available for each Generic Enabler\nFIWARE is not about take it all or nothing. You are not forced to use these complementary FIWARE Generic Enablers but are free to use other third platform components as well to design the hybrid platform of your choice. As long as it uses the FIWARE Context Broker technology to manage context information, your platform can be labeled as \u201cPowered by FIWARE\u201d and solutions build on top as well. Listings of many FIWARE Ready devices and commercial \u201cPowered by FIWARE\u201d solutions can be found on the FIWARE Marketplace.\nTo be adopted as a Generic Enabler within FIWARE, each open source component must fulfil a number of contribution requirements. The technical direction of FIWARE is governed by the FIWARE Technical Steering Committee (TSC). Developers can also subscribe to the FIWARE TSC mailing list, join the FIWARE TSC regular confcalls as observer and follow the progress of the discussions through the FIWARE TSC meeting minutes.\n\ud83d\udccb\nContribution Requirements \ud83d\udc6a\nTechnical Steering Committee \ud83d\udce3\nMailing list \ud83d\udcc2\nMeeting minutes\nInstallation\nTo obtain the latest codebase of all FIWARE Generic Enablers:\ngit clone https://github.com/FIWARE/catalogue.git\ncd catalogue/\ngit submodule update --init --recursive\ngit submodule update --recursive --remote\nGeneric Enablers\nThe following sections describe the current list of Generic Enablers of the FIWARE platform structured into five architectural chapters.\nCore Context Management\nCore Context Broker components\nA Context Broker component is the core and mandatory component of any \u201cPowered by FIWARE\u201d platform or solution. It enables to manage context information in a highly decentralized and large-scale manner.\nThe Orion Context Broker Generic Enabler currently provides the FIWARE NGSI v2 API which is a simple yet powerful Restful API enabling to perform updates, queries or subscribe to changes on context information.\nThe following Context Broker implementations, supporting the ETSI NGSI-LD 1.3.1. API specification or higher are under incubation:\nThe Orion-LD Context Broker Generic Enabler is a NGSI-LD Broker, which supports both NGSI-LD and the NGSI-v2 APIs.\nThe Scorpio Broker Generic Enabler is an alternative NGSI-LD Broker which can also be used in federated environments\nThe Stellio Context Broker Generic Enabler is another alternative NGSI-LD Broker\nCore Data Connectors\nAccompanying a Context Broker component as part of Core Context Management, a number of Data Connectors are available:\nThe STH Comet Generic Enabler brings the means for storing a short-term history of context data (typically months) on MongoDB\nThe Cygnus Generic Enabler is enables a means of managing the history of context which is created as a stream of data and can be injected into multiple data sinks, including many popular databases such as PostgreSQL, MySQL, MongoDB or AWS DynamoDB or as Big Data platforms such as Hadoop, Storm or Spark. The component is based on Apache Flume\nThe Draco Generic Enabler is an alternative data persistence mechanism for managing the history of context. It is based on Apache NiFi and is a dataflow system based on the concepts of flow-based programming. It supports powerful and scalable directed graphs of data routing, transformation, and system mediation logic and also offers an intuitive graphical interface\nThe Cosmos Generic Enabler enables simpler Big Data analysis over context integrated with popular Big Data platforms (Spark and Flink).\nThe QuantumLeap Generic Enabler supports the storage of context data into a time series database (CrateDB and Timescale)\nThe following is a list of Generic Enablers under incubation within the area of Core Data Connectors systems:\nThe Canis Major Generic Enabler is a blockchain adaptor that supports various DLTs, the adaptor aims to submit the data to DLT using FIWARE Technologies\nThe NGSI.js library provides a series of JavaScript functions allowing developers to connect and push context data to any NGSI compliant context broker\nFurther information can be found on dedicated pages linked to Core Context Management\nInterface with IoT, Robots and Third-Party Systems\nA number of Generic Enablers are available making it easier to interface with the Internet of Things (IoT), Robots and Third-party systems for the purpose of gathering valuable context information or trigger actuations in response to context updates:\nThe IDAS Generic Enabler offers you a wide range of IoT Agents making it easier to interface with devices using the most widely used IoT protocols (LWM2M over CoaP, JSON or UltraLight over HTTP/MQTT, OPC-UA, Sigfox or LoRaWAN)\nIoT Agent for JSON - a bridge between HTTP/MQTT messaging (with a JSON payload) and NGSI\nIoT Agent for LWM2M - a bridge between the Lightweight M2M protocol and NGSI\nIoT Agent for Ultralight - a bridge between HTTP/MQTT messaging (with an UltraLight2.0 payload) and NGSI\nIoT Agent for LoRaWAN - a bridge between the LoRaWAN protocol and NGSI\nIoT Agent for OPC-UA - a bridge between the OPC Unified Architecture protocol and NGSI\nIoT Agent for Sigfox - a bridge between the Sigfox protocol and NGSI\nIoT Agent for ISOXML - a bridge between the ISOXML/ADAPT protocol for agricultural machinery and NGSI\nIoT Agent library - library for developing your own IoT Agent.\nThe Kurento Generic Enabler enables real-time processing of media streams supporting the transformation of video cameras into sensors as well as the incorporation of advanced application functions (integrated audiovisual communications, augmented reality, flexible media playing and recording, etc)\nThe following is a list of Generic Enablers under incubation within the area of IoT, Robotics and third-party systems:\nThe OpenVidu Generic Enabler is an abstraction layer for Kurento, making media processing easier to program.\nThe OpenMTC Incubated Generic Enabler brings an open source implementation of the OneM2M standard. A northbound interface with the Orion Context Broker is implemented as part of the product.\nThe Fast DDS Incubated Generic Enabler has been adopted as default middleware in ROS2, the widely known Robot Operating System, therefore it helps to interface with ROS2-based robotics systems.\nThe Micro XRCE-DDS Incubated Generic Enabler is a lite version of the DDS middleware, adapted to run in extremely constrained resource devices (e.g. micro-controllers).\nThe FIROS Incubated Generic Enabler works as a translator between the robotics domain and the cloud, transforming ROS messages into NGSI v2 and vice versa.\nThe Domibus Incubated Generic Enabler helps users to exchange electronic data and documents with one another in a reliable and trusted way.\nThe Oliot Incubated Generic Enabler is a mediation gateway which translates information from NGSI based platforms to EPCIS based IoT platforms.\nFurther information can be found on dedicated pages linked to IoT Agents and Robotics and Third-Party Systems\nContext Processing, Analysis and Visualization\nA number of Generic Enablers are available making it easier to process, analyze or visualize context information for the purpose of implementing the \u201csmart behaviour\u201d expected in any application:\nThe Wirecloud Generic Enabler brings a powerful web mashup platform making it easier to develop operational dashboards which are highly customizable by end users.\nThe FogFlow Generic Enabler is a distributed execution framework to support dynamic processing flows over cloud and edges.\nThe following is a list of Generic Enablers under incubation within the chapter:\nThe Perseo Generic Enabler introduces Complex Event Processing (CEP) defined using a rules-based system, enabling you to fire events which send HTTP requests, emails, tweets, SMS messages etc.\nFurther information can be found on dedicated pages linked to Context Processing, Analysis and Visualization\nContext Data/API Management, Publication and Monetization\nYou can implement secured access to components in the architecture of any \u201cPowered by FIWARE\u201d solution using Generic Enablers of the security chapter:\nThe Keyrock Identity Management Generic Enabler brings support to secure and private OAuth2-based authentication of users and devices, user profile management, privacy-preserving disposition of personal data, Single Sign-On (SSO) and Identity Federation across multiple administration domains.\nThe Wilma PEP Proxy Generic Enabler brings support of proxy functions within OAuth2-based authentication schemas. It also implements PEP functions within an XACML-based access control schema.\nThe AuthZForce PDP/PAP Generic Enabler brings support to PDP/PAP functions within an access control schema based on the XACML standard.\nThe incorporation of components and features to support Advanced Data Usage Control have been introduced as part of FIWARE Release 7.7. For a summary of vision and current status of work in this area, please visit the usage control pages found at: https://github.com/ging/fiware-usage-control\nThis area also contains a chapter for the publication and monetization of context data resources, available through the core Orion Context Broker component of your platform:\nThe CKAN extensions Generic Enabler brings a number of add-ons enabling to extend current capabilities of the world-leading CKAN Open Data publication platform to allow publication of datasets matching right-time context data, the assignment of access terms and policies to those datasets and the assignment of pricing and pay-per-use schemas to datasets.\nThe Biz Framework Generic Enabler brings backend support to Context API/Data monetization based on open TM Forum Business APIs.\nIdra is able to federate existing Open Data Management Systems based on heterogeneous technologies (e.g. CKAN, SOCRATA, DKAN etc.) providing a single API and a standard metadata format (DCAT-AP) to discover open datasets.\nThe following is a list of Generic Enablers under incubation within the area of API management, publication and monetization:\nAPInf API Management Framework is a tool for API owners to manage their APIs. It provides all the necessary features to run business with APIs and makes it easy for API consumers to find and start using the standard APIs.\nCoatRack is a third-party backend-to-backend communications framework facilitating API access, monitoring and monetization.\nFIWARE TRUE Connector enables trusted data exchange in order for context brokers to be able to become an active part of an International Data Spaces (IDS) Ecosystem\nSteelskin PEP is a PEP proxy meant to secure independent FIWARE components, by intercepting every request sent to the component, validating it against an Access Control component such as Keypass\nKeypass is multi-tenant XACML server with PAP and PDP capabilities.\nKeystone SCIM is an OpenStack Keystone extension that enables the management of User, Groups and Roles using the SCIM v1.1 standard\nKeystone SPASSWORD is an OpenStack Keystone extension that enables extra security checks over user passwords\nFurther information can be found on dedicated pages linked to Context Data/API Management, Publication and Monetization and Security\nDeployment Tools\nMost FIWARE components are available as Docker Images, direct links can be found within the descriptions of each generic enabler. A docker-compose.yml may also be available within the GitHub repository, but these should only be used in development environments. For fully scalable production-ready instances of FIWARE Generic Enablers please refer to the repository holding the FIWARE Helm Chart recipes or find FIWARE Helm Chart receipes directly on ArtifactoryHub\nData Spaces\nFIWARE brings the essential building blocks helping to create Data Spaces enabling access and share of data in an effective and trustworthy manner. The open source nature of FIWARE technologies foster creation of these data spaces as federated infrastructures where multiple providers can be involved and not just a few players. FIWARE is bringing today mature technologies, compatible with IDS and CEF Building Blocks, that may accelerate the delivery of data spaces to the market and the materialization of initiatives like GAIA-X in Europe.\nThe concrete selection of FIWARE components proposed as building blocks for the creation of Data Spaces can be found on dedicated pages linked to FIWARE for Data Spaces.\nManagement of FIWARE Releases\nThe individual components within the FIWARE catalogue are free to make semantic versioned releases at any time. Additionally, a labelling of the whole FIWARE catalogue takes place at regular intervals where all components are collected together and a FIWARE label added according to a set of well-defined rules. The FIWARE catalogue release schedule follows a strict timetable. Versions of components with the same FIWARE release label should be completely interoperable and work nicely with each other.\n\ud83c\udfaf Roadmap \ud83d\ude80 Release Notes\nAs well as source-code repositories and binaries, relevant components are available as Docker images and Helm-chart recipes for Kubernetes. Smart Data Models for NGSI-v2 and NGSI-LD help to define harmonised representation formats and semantics that will be used by applications both to consume and to publish data.\n\ud83c\udf10 Helm Charts Smart Data Models\nTesting\nFIWARE is ready to work in industrial environments. A continuous quality assurance process is running to warranty the level of reliability in FIWARE Generic Enablers. Following the energy labelling system applied by European Union for devices, each Generic Enabler is assigned with an overall label in the Catalogue (A+++, A++, A+, A, B, C) according to the obtained results in testing different aspects.\nA testing method has been defined to conduct documentation verification, functional tests (verification of Generic Enablers specifications and APIs testing) and non-functional tests such as performance, stability and scalability. The tests are executed for every new version of the Generic Enablers, providing valuable insights to the Generic Enabler owners for improving the next Generic Enabler release.\nAll the tests can be conducted by anyone since the files for defining and running the tests have been published in an open source repository.\nFull details of the test results can be found on the FIWARE site - a summary of the test results is presented for each enabler within the submodules below.\nMature Generic Enablers have been thoroughly tested and have already demonstrated useful role accepted within the FIWARE framework. Incubated Generic enablers (annotated with \ud83c\udf31) have also been tested, but are still in the process of maturing to full acceptance with the FIWARE system. Recent applicants to FIWARE (annotated with \ud83c\udd95) are new to FIWARE and will undergo testing in the forthcoming release.\nWilling to contribute your technology to FIWARE?\nThe application process for joining the FIWARE Initiative is described in the links below. To be adopted as a Generic Enabler within FIWARE, each open source component must fulfill a number of contribution requirements, follow common Open Source Best Practice and sign-up to the harmonized FIWARE Entity Contributor License Agreement. The technical direction of FIWARE is governed by the FIWARE Technical Steering Committee (TSC).\nYou can contribute to FIWARE technically by different means: contributing code for a given FIWARE component, helping to develop tutorials/training material which are made freely available to developers, supporting QA activities or supporting the operations of the FIWARE Lab.\n\ud83d\udce5\nApplication Form \ud83d\udccb\nContribution Requirements \u2795 \ud83d\udcbb\nEntity CLA \ud83d\udd27\nActive Contributors \u2795 \ud83d\udc64\nIndividual CLA\nLicense\nLicense: AGPL-3.0 \u00a9 2018-2021 FIWARE Foundation e.V.\nThe FIWARE Catalogue includes additional submodules which have been obtained under license - check the README of each submodule for more details. Some of these elements are available under a more permissive license than Affero General Public License.\nAdditional permission is given to reproduce the texts found within the root project under the Creative Commons CC-BY-4.0 License.\nSome FIWARE Components are distributed under GPL or AGPL open source licenses \u2026 Is it safe for me to use?\nAbsolutely. Issues with GPL (or AGPL) licenses are mostly related with the fact that different people assign different interpretations on the meaning of the term \u201cderivate work\u201d used in these licenses. Due to this, some people understand that there is a risk in just using software under GPL or AGPL licenses (even without modifying it).\nIn order to avoid any issue, FIWARE Generic Enabler owners who have decided to release their software using a GPL or AGPL license are required to make a public statement that states:\nPlease note that software derived as a result of modifying the source code of this software in order to fix a bug or incorporate enhancements is considered a derivative work of the product. Software that merely uses or aggregates (i.e. links to) an otherwise unmodified version of existing software is not considered a derivative work.\nThe public statement above is aimed at giving users confidence they can use FIWARE components even if they have been licensed under a copyleft license without this requiring that their applications have to be released as open source. The FIWARE Foundation has sought legal advice regarding this statement and has been told that incorporation of this paragraph is valid for this purpose as per the report produced by Across Legal/ID law partners (see summary report).\nThis means that there is absolute no risk that you are forced to release the software that you may have developed using FIWARE Generic Enablers under a GPL, AGPL or any other open source license.", "link": "https://github.com/FIWARE/catalogue", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "catalogue\nthis is an umbrella repository which briefly introduces each of the components (also referred as generic enablers) which are part of fiware and allow users and developers to easily navigate to the relevant source code repositories, documentation and docker images.\nmore information about what fiware is and how to use it can be found within the fiware tour guide, the fiware academy and the step-by-step tutorials. generic questions about fiware can be asked on ask.fiware and developer-based questions can be submitted on stack overflow using the fiware tag.\ndeveloper.\u200bfiware.org \ud83d\udcda\ntour guide \ud83c\udf93\nacademy \ud83d\udd20\ntutorials \u2753\nask.\u200bfiware\ncontents\ndetails\nbackground\nfiware brings a curated framework of open source software platform components which can be assembled together and with other third-party components to build platforms that support the development of smart solutions faster, easier and cheaper. the main and only mandatory component of any \u201cpowered by fiware\u201d platform or solution is a fiware context broker generic enabler, supplying a cornerstone function required in any smart solution: the need to manage context information, enabling to perform updates and bring access to context.\nfiware ngsi is the api exported by a fiware context broker, used for the integration of platform components within a \"powered by fiware\" platform and by applications to update or consume context information. fiware ngsi api specifications have evolved over time, initially matching ngsi-v2 specifications, now aligning with the etsi ngsi-ld standard. the fiware community plays an active role in the evolution of etsi ngsi-ld specifications which were based on ngsi-v2 and commits to deliver compatible open source implementations of the specs.\nbuilding around the fiware context broker, a rich suite of complementary open-source fiware generic enablers are available, dealing with the following:\ncore context management manipulates and stores context data so it can be used for further processesing\ninterfacing with the internet of things (iot), robots and third-party systems, for capturing updates on context information and translating required actuations.\nprocessing, analysis and visualization of context information, implementing the expected smart behaviour of applications and/or assisting end users in making smart decisions.\ncontext data/api management, publication and monetization, bringing support to usage control and the opportunity to publish and monetize part of managed context data.\ndeployment tools such as docker images and helm-chart recipes are available for each generic enabler\nfiware is not about take it all or nothing. you are not forced to use these complementary fiware generic enablers but are free to use other third platform components as well to design the hybrid platform of your choice. as long as it uses the fiware context broker technology to manage context information, your platform can be labeled as \u201cpowered by fiware\u201d and solutions build on top as well. listings of many fiware ready devices and commercial \u201cpowered by fiware\u201d solutions can be found on the fiware marketplace.\nto be adopted as a generic enabler within fiware, each open source component must fulfil a number of contribution requirements. the technical direction of fiware is governed by the fiware technical steering committee (tsc). developers can also subscribe to the fiware tsc mailing list, join the fiware tsc regular confcalls as observer and follow the progress of the discussions through the fiware tsc meeting minutes.\n\ud83d\udccb\ncontribution requirements \ud83d\udc6a\ntechnical steering committee \ud83d\udce3\nmailing list \ud83d\udcc2\nmeeting minutes\ninstallation\nto obtain the latest codebase of all fiware generic enablers:\ngit clone https://github.com/fiware/catalogue.git\ncd catalogue/\ngit submodule update --init --recursive\ngit submodule update --recursive --remote\ngeneric enablers\nthe following sections describe the current list of generic enablers of the fiware platform structured into five architectural chapters.\ncore context management\ncore context broker components\na context broker component is the core and mandatory component of any \u201cpowered by fiware\u201d platform or solution. it enables to manage context information in a highly decentralized and large-scale manner.\nthe orion context broker generic enabler currently provides the fiware ngsi v2 api which is a simple yet powerful restful api enabling to perform updates, queries or subscribe to changes on context information.\nthe following context broker implementations, supporting the etsi ngsi-ld 1.3.1. api specification or higher are under incubation:\nthe orion-ld context broker generic enabler is a ngsi-ld broker, which supports both ngsi-ld and the ngsi-v2 apis.\nthe scorpio broker generic enabler is an alternative ngsi-ld broker which can also be used in federated environments\nthe stellio context broker generic enabler is another alternative ngsi-ld broker\ncore data connectors\naccompanying a context broker component as part of core context management, a number of data connectors are available:\nthe sth comet generic enabler brings the means for storing a short-term history of context data (typically months) on mongodb\nthe cygnus generic enabler is enables a means of managing the history of context which is created as a stream of data and can be injected into multiple data sinks, including many popular databases such as postgresql, mysql, mongodb or aws dynamodb or as big data platforms such as hadoop, storm or spark. the component is based on apache flume\nthe draco generic enabler is an alternative data persistence mechanism for managing the history of context. it is based on apache nifi and is a dataflow system based on the concepts of flow-based programming. it supports powerful and scalable directed graphs of data routing, transformation, and system mediation logic and also offers an intuitive graphical interface\nthe cosmos generic enabler enables simpler big data analysis over context integrated with popular big data platforms (spark and flink).\nthe quantumleap generic enabler supports the storage of context data into a time series database (cratedb and timescale)\nthe following is a list of generic enablers under incubation within the area of core data connectors systems:\nthe canis major generic enabler is a blockchain adaptor that supports various dlts, the adaptor aims to submit the data to dlt using fiware technologies\nthe ngsi.js library provides a series of javascript functions allowing developers to connect and push context data to any ngsi compliant context broker\nfurther information can be found on dedicated pages linked to core context management\ninterface with iot, robots and third-party systems\na number of generic enablers are available making it easier to interface with the internet of things (iot), robots and third-party systems for the purpose of gathering valuable context information or trigger actuations in response to context updates:\nthe idas generic enabler offers you a wide range of iot agents making it easier to interface with devices using the most widely used iot protocols (lwm2m over coap, json or ultralight over http/mqtt, opc-ua, sigfox or lorawan)\niot agent for json - a bridge between http/mqtt messaging (with a json payload) and ngsi\niot agent for lwm2m - a bridge between the lightweight m2m protocol and ngsi\niot agent for ultralight - a bridge between http/mqtt messaging (with an ultralight2.0 payload) and ngsi\niot agent for lorawan - a bridge between the lorawan protocol and ngsi\niot agent for opc-ua - a bridge between the opc unified architecture protocol and ngsi\niot agent for sigfox - a bridge between the sigfox protocol and ngsi\niot agent for isoxml - a bridge between the isoxml/adapt protocol for agricultural machinery and ngsi\niot agent library - library for developing your own iot agent.\nthe kurento generic enabler enables real-time processing of media streams supporting the transformation of video cameras into sensors as well as the incorporation of advanced application functions (integrated audiovisual communications, augmented reality, flexible media playing and recording, etc)\nthe following is a list of generic enablers under incubation within the area of iot, robotics and third-party systems:\nthe openvidu generic enabler is an abstraction layer for kurento, making media processing easier to program.\nthe openmtc incubated generic enabler brings an open source implementation of the onem2m standard. a northbound interface with the orion context broker is implemented as part of the product.\nthe fast dds incubated generic enabler has been adopted as default middleware in ros2, the widely known robot operating system, therefore it helps to interface with ros2-based robotics systems.\nthe micro xrce-dds incubated generic enabler is a lite version of the dds middleware, adapted to run in extremely constrained resource devices (e.g. micro-controllers).\nthe firos incubated generic enabler works as a translator between the robotics domain and the cloud, transforming ros messages into ngsi v2 and vice versa.\nthe domibus incubated generic enabler helps users to exchange electronic data and documents with one another in a reliable and trusted way.\nthe oliot incubated generic enabler is a mediation gateway which translates information from ngsi based platforms to epcis based iot platforms.\nfurther information can be found on dedicated pages linked to iot agents and robotics and third-party systems\ncontext processing, analysis and visualization\na number of generic enablers are available making it easier to process, analyze or visualize context information for the purpose of implementing the \u201csmart behaviour\u201d expected in any application:\nthe wirecloud generic enabler brings a powerful web mashup platform making it easier to develop operational dashboards which are highly customizable by end users.\nthe fogflow generic enabler is a distributed execution framework to support dynamic processing flows over cloud and edges.\nthe following is a list of generic enablers under incubation within the chapter:\nthe perseo generic enabler introduces complex event processing (cep) defined using a rules-based system, enabling you to fire events which send http requests, emails, tweets, sms messages etc.\nfurther information can be found on dedicated pages linked to context processing, analysis and visualization\ncontext data/api management, publication and monetization\nyou can implement secured access to components in the architecture of any \u201cpowered by fiware\u201d solution using generic enablers of the security chapter:\nthe keyrock identity management generic enabler brings support to secure and private oauth2-based authentication of users and devices, user profile management, privacy-preserving disposition of personal data, single sign-on (sso) and identity federation across multiple administration domains.\nthe wilma pep proxy generic enabler brings support of proxy functions within oauth2-based authentication schemas. it also implements pep functions within an xacml-based access control schema.\nthe authzforce pdp/pap generic enabler brings support to pdp/pap functions within an access control schema based on the xacml standard.\nthe incorporation of components and features to support advanced data usage control have been introduced as part of fiware release 7.7. for a summary of vision and current status of work in this area, please visit the usage control pages found at: https://github.com/ging/fiware-usage-control\nthis area also contains a chapter for the publication and monetization of context data resources, available through the core orion context broker component of your platform:\nthe ckan extensions generic enabler brings a number of add-ons enabling to extend current capabilities of the world-leading ckan open data publication platform to allow publication of datasets matching right-time context data, the assignment of access terms and policies to those datasets and the assignment of pricing and pay-per-use schemas to datasets.\nthe biz framework generic enabler brings backend support to context api/data monetization based on open tm forum business apis.\nidra is able to federate existing open data management systems based on heterogeneous technologies (e.g. ckan, socrata, dkan etc.) providing a single api and a standard metadata format (dcat-ap) to discover open datasets.\nthe following is a list of generic enablers under incubation within the area of api management, publication and monetization:\napinf api management framework is a -----> tool !!!  for api owners to manage their apis. it provides all the necessary features to run business with apis and makes it easy for api consumers to find and start using the standard apis.\ncoatrack is a third-party backend-to-backend communications framework facilitating api access, monitoring and monetization.\nfiware true connector enables trusted data exchange in order for context brokers to be able to become an active part of an international data spaces (ids) ecosystem\nsteelskin pep is a pep proxy meant to secure independent fiware components, by intercepting every request sent to the component, validating it against an access control component such as keypass\nkeypass is multi-tenant xacml server with pap and pdp capabilities.\nkeystone scim is an openstack keystone extension that enables the management of user, groups and roles using the scim v1.1 standard\nkeystone spassword is an openstack keystone extension that enables extra security checks over user passwords\nfurther information can be found on dedicated pages linked to context data/api management, publication and monetization and security\ndeployment tools\nmost fiware components are available as docker images, direct links can be found within the descriptions of each generic enabler. a docker-compose.yml may also be available within the github repository, but these should only be used in development environments. for fully scalable production-ready instances of fiware generic enablers please refer to the repository holding the fiware helm chart recipes or find fiware helm chart receipes directly on artifactoryhub\ndata spaces\nfiware brings the essential building blocks helping to create data spaces enabling access and share of data in an effective and trustworthy manner. the open source nature of fiware technologies foster creation of these data spaces as federated infrastructures where multiple providers can be involved and not just a few players. fiware is bringing today mature technologies, compatible with ids and cef building blocks, that may accelerate the delivery of data spaces to the market and the materialization of initiatives like gaia-x in europe.\nthe concrete selection of fiware components proposed as building blocks for the creation of data spaces can be found on dedicated pages linked to fiware for data spaces.\nmanagement of fiware releases\nthe individual components within the fiware catalogue are free to make semantic versioned releases at any time. additionally, a labelling of the whole fiware catalogue takes place at regular intervals where all components are collected together and a fiware label added according to a set of well-defined rules. the fiware catalogue release schedule follows a strict timetable. versions of components with the same fiware release label should be completely interoperable and work nicely with each other.\n\ud83c\udfaf roadmap \ud83d\ude80 release notes\nas well as source-code repositories and binaries, relevant components are available as docker images and helm-chart recipes for kubernetes. smart data models for ngsi-v2 and ngsi-ld help to define harmonised representation formats and semantics that will be used by applications both to consume and to publish data.\n\ud83c\udf10 helm charts smart data models\ntesting\nfiware is ready to work in industrial environments. a continuous quality assurance process is running to warranty the level of reliability in fiware generic enablers. following the energy labelling system applied by european union for devices, each generic enabler is assigned with an overall label in the catalogue (a+++, a++, a+, a, b, c) according to the obtained results in testing different aspects.\na testing method has been defined to conduct documentation verification, functional tests (verification of generic enablers specifications and apis testing) and non-functional tests such as performance, stability and scalability. the tests are executed for every new version of the generic enablers, providing valuable insights to the generic enabler owners for improving the next generic enabler release.\nall the tests can be conducted by anyone since the files for defining and running the tests have been published in an open source repository.\nfull details of the test results can be found on the fiware site - a summary of the test results is presented for each enabler within the submodules below.\nmature generic enablers have been thoroughly tested and have already demonstrated useful role accepted within the fiware framework. incubated generic enablers (annotated with \ud83c\udf31) have also been tested, but are still in the process of maturing to full acceptance with the fiware system. recent applicants to fiware (annotated with \ud83c\udd95) are new to fiware and will undergo testing in the forthcoming release.\nwilling to contribute your technology to fiware?\nthe application process for joining the fiware initiative is described in the links below. to be adopted as a generic enabler within fiware, each open source component must fulfill a number of contribution requirements, follow common open source best practice and sign-up to the harmonized fiware entity contributor license agreement. the technical direction of fiware is governed by the fiware technical steering committee (tsc).\nyou can contribute to fiware technically by different means: contributing code for a given fiware component, helping to develop tutorials/training material which are made freely available to developers, supporting qa activities or supporting the operations of the fiware lab.\n\ud83d\udce5\napplication form \ud83d\udccb\ncontribution requirements \u2795 \ud83d\udcbb\nentity cla \ud83d\udd27\nactive contributors \u2795 \ud83d\udc64\nindividual cla\nlicense\nlicense: agpl-3.0 \u00a9 2018-2021 fiware foundation e.v.\nthe fiware catalogue includes additional submodules which have been obtained under license - check the readme of each submodule for more details. some of these elements are available under a more permissive license than affero general public license.\nadditional permission is given to reproduce the texts found within the root project under the creative commons cc-by-4.0 license.\nsome fiware components are distributed under gpl or agpl open source licenses \u2026 is it safe for me to use?\nabsolutely. issues with gpl (or agpl) licenses are mostly related with the fact that different people assign different interpretations on the meaning of the term \u201cderivate work\u201d used in these licenses. due to this, some people understand that there is a risk in just using software under gpl or agpl licenses (even without modifying it).\nin order to avoid any issue, fiware generic enabler owners who have decided to release their software using a gpl or agpl license are required to make a public statement that states:\nplease note that software derived as a result of modifying the source code of this software in order to fix a bug or incorporate enhancements is considered a derivative work of the product. software that merely uses or aggregates (i.e. links to) an otherwise unmodified version of existing software is not considered a derivative work.\nthe public statement above is aimed at giving users confidence they can use fiware components even if they have been licensed under a copyleft license without this requiring that their applications have to be released as open source. the fiware foundation has sought legal advice regarding this statement and has been told that incorporation of this paragraph is valid for this purpose as per the report produced by across legal/id law partners (see summary report).\nthis means that there is absolute no risk that you are forced to release the software that you may have developed using fiware generic enablers under a gpl, agpl or any other open source license.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000313, "year": null}, {"Unnamed: 0": 1339, "autor": 319, "date": null, "content": "Object Pose Estimation Demo\nThis tutorial will go through the steps necessary to perform pose estimation with a UR3 robotic arm in Unity. You\u2019ll gain experience integrating ROS with Unity, importing URDF models, collecting labeled training data, and training and deploying a deep learning model. By the end of this tutorial, you will be able to perform pick-and-place with a robot arm in Unity, using computer vision to perceive the object the robot picks up.\nWant to skip the tutorial and run the full demo? Check out our Quick Demo.\nWant to skip the tutorial and focus on collecting training data for the deep learning model? Check out our Quick Data-Collection Demo.\nNote: This project has been developed with Python 3 and ROS Noetic.\nTable of Contents\nPart 1: Create Unity scene with imported URDF\nPart 2: Setting up the scene for data collection\nPart 3: Data Collection and Model Training\nPart 4: Pick-and-Place\nPart 1: Create Unity Scene with Imported URDF\nThis part includes downloading and installing the Unity Editor, setting up a basic Unity scene, and importing a robot. We will import the UR3 robot arm using the URDF Importer package.\nPart 2: Setup the Scene for Data Collection\nThis part focuses on setting up the scene for data collection using the Unity Computer Vision Perception Package. You will learn how to use Perception Package Randomizers to randomize aspects of the scene in order to create variety in the training data.\nIf you would like to learn more about Randomizers, and apply domain randomization to this scene more thoroughly, check out our further exercises for the reader here.\nPart 3: Data Collection and Model Training\nThis part includes running data collection with the Perception Package, and using that data to train a deep learning model. The training step can take some time. If you'd like, you can skip that step by using our pre-trained model.\nTo measure the success of grasping in simulation using our pre-trained model for pose estimation, we did 100 trials and got the following results:\nSuccess Failures Percent Success\nWithout occlusion 82 5 94\nWith occlusion 7 6 54\nAll 89 11 89\nNote: Data for the above experiment was collected in Unity 2020.2.1f1.\nPart 4: Pick-and-Place\nThis part includes the preparation and setup necessary to run a pick-and-place task using MoveIt. Here, the cube pose is predicted by the trained deep learning model. Steps covered include:\nCreating and invoking a motion planning service in ROS\nSending captured RGB images from our scene to the ROS Pose Estimation node for inference\nUsing a Python script to run inference on our trained deep learning model\nMoving Unity Articulation Bodies based on a calculated trajectory\nControlling a gripping tool to successfully grasp and drop an object.\nSupport\nFor questions or discussions about Unity Robotics package installations or how to best set up and integrate your robotics projects, please create a new thread on the Unity Robotics forum and make sure to include as much detail as possible.\nFor feature requests, bugs, or other issues, please file a GitHub issue using the provided templates and the Robotics team will investigate as soon as possible.\nFor any other questions or feedback, connect directly with the Robotics team at unity-robotics@unity3d.com.\nMore from Unity Robotics\nVisit the Robotics Hub for more tutorials, tools, and information on robotics simulation in Unity!\nLicense\nApache License 2.0", "link": "https://github.com/Unity-Technologies/Robotics-Object-Pose-Estimation", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "object pose estimation demo\nthis tutorial will go through the steps necessary to perform pose estimation with a ur3 robotic arm in unity. you\u2019ll gain experience integrating ros with unity, importing urdf models, collecting labeled training data, and training and deploying a deep learning model. by the end of this tutorial, you will be able to perform pick-and-place with a robot arm in unity, using computer vision to perceive the object the robot picks up.\nwant to skip the tutorial and run the full demo? check out our quick demo.\nwant to skip the tutorial and focus on collecting training data for the deep learning model? check out our quick data-collection demo.\nnote: this project has been developed with python 3 and ros noetic.\ntable of contents\npart 1: create unity scene with imported urdf\npart 2: setting up the scene for data collection\npart 3: data collection and model training\npart 4: pick-and-place\npart 1: create unity scene with imported urdf\nthis part includes downloading and installing the unity editor, setting up a basic unity scene, and importing a robot. we will import the ur3 robot arm using the urdf importer package.\npart 2: setup the scene for data collection\nthis part focuses on setting up the scene for data collection using the unity computer vision perception package. you will learn how to use perception package randomizers to randomize aspects of the scene in order to create variety in the training data.\nif you would like to learn more about randomizers, and apply domain randomization to this scene more thoroughly, check out our further exercises for the reader here.\npart 3: data collection and model training\nthis part includes running data collection with the perception package, and using that data to train a deep learning model. the training step can take some time. if you'd like, you can skip that step by using our pre-trained model.\nto measure the success of grasping in simulation using our pre-trained model for pose estimation, we did 100 trials and got the following results:\nsuccess failures percent success\nwithout occlusion 82 5 94\nwith occlusion 7 6 54\nall 89 11 89\nnote: data for the above experiment was collected in unity 2020.2.1f1.\npart 4: pick-and-place\nthis part includes the preparation and setup necessary to run a pick-and-place task using moveit. here, the cube pose is predicted by the trained deep learning model. steps covered include:\ncreating and invoking a motion planning service in ros\nsending captured rgb images from our scene to the ros pose estimation node for inference\nusing a python script to run inference on our trained deep learning model\nmoving unity articulation bodies based on a calculated trajectory\ncontrolling a gripping -----> tool !!!  to successfully grasp and drop an object.\nsupport\nfor questions or discussions about unity robotics package installations or how to best set up and integrate your robotics projects, please create a new thread on the unity robotics forum and make sure to include as much detail as possible.\nfor feature requests, bugs, or other issues, please file a github issue using the provided templates and the robotics team will investigate as soon as possible.\nfor any other questions or feedback, connect directly with the robotics team at unity-robotics@unity3d.com.\nmore from unity robotics\nvisit the robotics hub for more tutorials, tools, and information on robotics simulation in unity!\nlicense\napache license 2.0", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000319, "year": null}, {"Unnamed: 0": 1352, "autor": 332, "date": null, "content": "OnShape to Robot (SDF/URDF)\nThis tool is based on the OnShape API to retrieve informations from an assembly and build an SDF or URDF model suitable for physics simulation.\nDocumentation\nExamples", "link": "https://github.com/Rhoban/onshape-to-robot", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "onshape to robot (sdf/urdf)\nthis -----> tool !!!  is based on the onshape api to retrieve informations from an assembly and build an sdf or urdf model suitable for physics simulation.\ndocumentation\nexamples", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000332, "year": null}, {"Unnamed: 0": 1368, "autor": 348, "date": null, "content": "iDynTree\niDynTree is a library of robots dynamics algorithms for control, estimation and simulation. It is specifically designed for free-floating robots, but it is possible to use it also with fixed-base robots.\nThe major characteristic features of iDynTree are:\nIt is written in C++, with Python and MATLAB bindings.\nIt uses an undirected graph data structure (iDynTree::Model) that is used to represent robots, to easily change the base link that you are using for your kinematics and dynamics computations without the need to reload your model or change your joint or link serializations. This is done as iDynTree was developed for floating-base robots such as humanoids, in which the most convenient choice of base link can change.\nIt contains support for reading and writing URDF files from a iDynTree::Model, making it useful to write tools that modify robot models and saves them back to file. This is done as iDynTree was meant to develop tools for identification of kinematics and dynamics parameters.\nIt defaults to use the mixed representation to represent link quantities (including the velocity and acceleration of the base link), but it can optionally use also body (left-trivialized) or inertial (right-trivialized) representation if requested. This is done because iDynTree was developed to satisfy the needs of research in synthesis of floating-base whole-body controllers. If you are not familiar with the different representation for 6D quantities, check Section 6 of \"Multibody dynamics notation (version 2)\".\nIt contains an implementation of the algorithm used in the iCub humanoid robot to estimate the joint torques without the need of collocated joint torque sensors, exploting the specific undirected graph data structure . This is done as this was one of the originally goal for the implementation of iDynTree. See the class iDynTree::ExtWrenchesAndJointTorquesEstimator and Chapter 6 of \"Modelling, Estimation and Identification of Humanoid Robots Dynamics\".\nTo avoid confusion, it is also useful to clarify what iDynTree is not:\nIt is not the fastest C++ library for kinematics and dynamics multibody computations for robotics. It is not slow, but if have an application in which you need the absolute fastest library, check out Pinocchio.\nIt is not a multibody simulator library. It provides the building blocks that you could use to build a multibody simulator, but it is not a multibody simulator per se. If you need a simulator library in C++, check out DART, Simbody, Drake, MuJoCo or the abstraction layer ignition-physics. If you need a simulator implemented in MATLAB/Simulink (built on iDynTree), check matlab-whole-body-simulator.\nContents\nInstallation\nLibrary Usage\nTools Usage\nReference Documentation\nAnnouncements\nDeveloper Documentation\nReference Paper\nAcknowledgments\nInstallation\nconda (recommended)\nYou can easily install the C++ and Python library with via conda-forge using the following command\nconda install -c conda-forge idyntree\nIf you need to install also the MATLAB bindings, you can install them with:\nconda install -c conda-forge -c robotology idyntree-matlab-bindings\nIf you are not familiar with conda or conda-forge, you can read an introduction document in conda-forge overview.\nrobotology-superbuild (advanced)\nIf you are installing iDynTree for use as part of iCub humanoid robot software installation, you may want to install iDynTree through the robotology-superbuild, an easy way to download, compile and install the robotology software on multiple operating systems, using the CMake build system and its extension YCM. To get iDynTree when using the robotology-superbuild, please enable the ROBOTOLOGY_ENABLE_DYNAMICS CMake option of the superbuild. If you want to install also iDynTree Python or MATLAB bindings, remember to enable the ROBOTOLOGY_USES_PYTHON or ROBOTOLOGY_USES_MATLAB options.\nBuild from source (advanced)\nIf you want to build iDynTree directly from source, you can check the documentation in doc/build-from-source.md.\nLibrary Usage\nUsage in C++\nOnce the library is installed, you can link it in C++ programs using CMake with as little effort as writing the following line of code in your project's CMakeLists.txt:\nfind_package(iDynTree REQUIRED)\ntarget_link_libraries(<target> PRIVATE iDynTree::idyntree-high-level iDynTree::idyntree-estimation)\nSee CMake's reference documentation if you need more info on the find_package or target_link_libraries CMake commands.\nUsage in MATLAB\nTo make sure that iDynTree is available in MATLAB, try to run some simple code that uses it:\np = iDynTree.Position()\nIf this is not working, make sure that you are launching matlab after having activated the conda environment (if you installed iDynTree via conda) or after having sourced por executed the correct setup script (if you installed iDynTree via the robotology-superbuild).\nTutorials\nThese tutorials describe how to use specific parts of iDynTree. Are you interested in a tutorial on a specific feature or algorithm that you can't find in this list? Just request it on an enhancement issue.\nTopic Location Language\nBasic usage of the KinDynComputations class together with the [Eigen C++ Matrix library to compute kinematics and dynamics quantities such as forward kinematics, inverse dynamics, mass matrix. examples/cxx/KinDynComputationsWithEigen/main.cpp C++\nHow to use the InverseKinematics class for the IK of an industrial fixed-base manipulator. examples/cxx/InverseKinematics/README.md C++\nUse of the ExtWrenchesAndJointTorquesEstimator class for computing offset for FT sensors examples/matlab/SixAxisFTOffsetEstimation/SixAxisFTOffsetEstimation.m MATLAB\nHow to get the axis of a revolute joint expressed in a arbitary frame using the KinDynComputations class examples/matlab/SensorsListParsing/SensorsListParsing.m MATLAB\nHow to read the Six Axis Force Torque sensors information contained in a URDF model. examples/matlab/GetJointAxesInWorldFrame.m MATLAB\nUsage of the MATLAB-native visualizer using the MATLAB high-level wrappers. examples/matlab/iDynTreeWrappers/visualizeRobot.m MATLAB\nBasic usage of the KinDynComputations class. examples/python/KinDynComputationsTutorial.py Python\nBasic usage of the MeshcatVisualizer class. examples/python/MeshcatVisualizerExample.ipynb Python\nTools Usage\niDynTree also includes some command line tools to use some of the functionality of the library without writing any line of code. The available command line tools are listed in the following, and each tool also includes an online help that is tipically available by passing the -h flag.\nidyntree-model-info\nTool that reads a model from a file, and print some useful information as specified via the command line.\nExample: Print the total mass of a given model\nidyntree-model-info -m <location-of-the-model> --total-mass\nidyntree-model-view\nTool that reads a model from a file and visualize it using the idyntree-visualizer library\nExample: Visualize a given model\nidyntree-model-view -m <location-of-the-model>\nidyntree-model-simplify-shapes\nTool that reads a model from a file, and returns in output the same model, but with all solid shapes of the model (both collision and visual) substituted with a primitive shape that approximates in some way the original solid shape. At the moment, the only conversion type provided is to approximate each solid shape of the model with its axis aligned bounding box.\nExample: Approximate a given model\nidyntree-model-simplify-shapes -m <location-of-the-input-model> -o <desired-location-of-the-output-model>\nReference Documentation\nThe documentation for the complete API of iDynTree is automatically extracted from the C++ code using Doxygen, and is available at the URL : https://robotology.github.io/idyntree.\nAnnouncements\nAnnouncements on new releases, API changes or other news are done on robotology/QA GitHub repository. You can watch that repository to get all the iDynTree-related announcements, that will always tagged with the announcement tag.\nDeveloper Documentation\nIf you want to contribute to iDynTree development, please check the Developer's FAQ.\nReference paper\nA paper describing some of the algorithms implemented in iDynTree and their use in a real world scenario can be downloaded here . If you're going to use this library for your work, please quote it within any resulting publication:\nF. Nori, S. Traversaro, J. Eljaik, F. Romano, A. Del Prete, D. Pucci \"iCub whole-body control through force regulation on rigid non-coplanar contacts\", Frontiers in Robotics and AI, 2015.\nThe bibtex code for including this citation is provided:\n@ARTICLE{10.3389/frobt.2015.00006,\nAUTHOR={Nori, Francesco and Traversaro, Silvio and Eljaik, Jorhabib and Romano, Francesco and Del Prete, Andrea and Pucci, Daniele},\nTITLE={iCub Whole-body Control through Force Regulation on Rigid Noncoplanar Contacts},\nJOURNAL={Frontiers in Robotics and AI},\nVOLUME={2},\nYEAR={2015},\nNUMBER={6},\nURL={http://www.frontiersin.org/humanoid_robotics/10.3389/frobt.2015.00006/abstract},\nDOI={10.3389/frobt.2015.00006},\nISSN={2296-9144}}\nAcknowledgments\nThe initial development of iDynTree was supported by the FP7 EU projects CoDyCo (No. 600716 ICT 2011.2.1 Cognitive Systems and Robotics) and Koroibot (No. 611909 ICT- 2013.2.1 Cognitive Systems and Robotics).\nThe development is now supported by the Artificial Mechanical Intelligence research line at the Italian Institute of Technology.\nLicense\niDynTree is licensed under either the GNU Lesser General Public License v3.0 :\nhttps://www.gnu.org/licenses/lgpl-3.0.html\nor the GNU Lesser General Public License v2.1 :\nhttps://www.gnu.org/licenses/old-licenses/lgpl-2.1.html\nat your option.", "link": "https://github.com/robotology/idyntree", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "idyntree\nidyntree is a library of robots dynamics algorithms for control, estimation and simulation. it is specifically designed for free-floating robots, but it is possible to use it also with fixed-base robots.\nthe major characteristic features of idyntree are:\nit is written in c++, with python and matlab bindings.\nit uses an undirected graph data structure (idyntree::model) that is used to represent robots, to easily change the base link that you are using for your kinematics and dynamics computations without the need to reload your model or change your joint or link serializations. this is done as idyntree was developed for floating-base robots such as humanoids, in which the most convenient choice of base link can change.\nit contains support for reading and writing urdf files from a idyntree::model, making it useful to write tools that modify robot models and saves them back to file. this is done as idyntree was meant to develop tools for identification of kinematics and dynamics parameters.\nit defaults to use the mixed representation to represent link quantities (including the velocity and acceleration of the base link), but it can optionally use also body (left-trivialized) or inertial (right-trivialized) representation if requested. this is done because idyntree was developed to satisfy the needs of research in synthesis of floating-base whole-body controllers. if you are not familiar with the different representation for 6d quantities, check section 6 of \"multibody dynamics notation (version 2)\".\nit contains an implementation of the algorithm used in the icub humanoid robot to estimate the joint torques without the need of collocated joint torque sensors, exploting the specific undirected graph data structure . this is done as this was one of the originally goal for the implementation of idyntree. see the class idyntree::extwrenchesandjointtorquesestimator and chapter 6 of \"modelling, estimation and identification of humanoid robots dynamics\".\nto avoid confusion, it is also useful to clarify what idyntree is not:\nit is not the fastest c++ library for kinematics and dynamics multibody computations for robotics. it is not slow, but if have an application in which you need the absolute fastest library, check out pinocchio.\nit is not a multibody simulator library. it provides the building blocks that you could use to build a multibody simulator, but it is not a multibody simulator per se. if you need a simulator library in c++, check out dart, simbody, drake, mujoco or the abstraction layer ignition-physics. if you need a simulator implemented in matlab/simulink (built on idyntree), check matlab-whole-body-simulator.\ncontents\ninstallation\nlibrary usage\ntools usage\nreference documentation\nannouncements\ndeveloper documentation\nreference paper\nacknowledgments\ninstallation\nconda (recommended)\nyou can easily install the c++ and python library with via conda-forge using the following command\nconda install -c conda-forge idyntree\nif you need to install also the matlab bindings, you can install them with:\nconda install -c conda-forge -c robotology idyntree-matlab-bindings\nif you are not familiar with conda or conda-forge, you can read an introduction document in conda-forge overview.\nrobotology-superbuild (advanced)\nif you are installing idyntree for use as part of icub humanoid robot software installation, you may want to install idyntree through the robotology-superbuild, an easy way to download, compile and install the robotology software on multiple operating systems, using the cmake build system and its extension ycm. to get idyntree when using the robotology-superbuild, please enable the robotology_enable_dynamics cmake option of the superbuild. if you want to install also idyntree python or matlab bindings, remember to enable the robotology_uses_python or robotology_uses_matlab options.\nbuild from source (advanced)\nif you want to build idyntree directly from source, you can check the documentation in doc/build-from-source.md.\nlibrary usage\nusage in c++\nonce the library is installed, you can link it in c++ programs using cmake with as little effort as writing the following line of code in your project's cmakelists.txt:\nfind_package(idyntree required)\ntarget_link_libraries(<target> private idyntree::idyntree-high-level idyntree::idyntree-estimation)\nsee cmake's reference documentation if you need more info on the find_package or target_link_libraries cmake commands.\nusage in matlab\nto make sure that idyntree is available in matlab, try to run some simple code that uses it:\np = idyntree.position()\nif this is not working, make sure that you are launching matlab after having activated the conda environment (if you installed idyntree via conda) or after having sourced por executed the correct setup script (if you installed idyntree via the robotology-superbuild).\ntutorials\nthese tutorials describe how to use specific parts of idyntree. are you interested in a tutorial on a specific feature or algorithm that you can't find in this list? just request it on an enhancement issue.\ntopic location language\nbasic usage of the kindyncomputations class together with the [eigen c++ matrix library to compute kinematics and dynamics quantities such as forward kinematics, inverse dynamics, mass matrix. examples/cxx/kindyncomputationswitheigen/main.cpp c++\nhow to use the inversekinematics class for the ik of an industrial fixed-base manipulator. examples/cxx/inversekinematics/readme.md c++\nuse of the extwrenchesandjointtorquesestimator class for computing offset for ft sensors examples/matlab/sixaxisftoffsetestimation/sixaxisftoffsetestimation.m matlab\nhow to get the axis of a revolute joint expressed in a arbitary frame using the kindyncomputations class examples/matlab/sensorslistparsing/sensorslistparsing.m matlab\nhow to read the six axis force torque sensors information contained in a urdf model. examples/matlab/getjointaxesinworldframe.m matlab\nusage of the matlab-native visualizer using the matlab high-level wrappers. examples/matlab/idyntreewrappers/visualizerobot.m matlab\nbasic usage of the kindyncomputations class. examples/python/kindyncomputationstutorial.py python\nbasic usage of the meshcatvisualizer class. examples/python/meshcatvisualizerexample.ipynb python\ntools usage\nidyntree also includes some command line tools to use some of the functionality of the library without writing any line of code. the available command line tools are listed in the following, and each -----> tool !!!  also includes an online help that is tipically available by passing the -h flag.\nidyntree-model-info\ntool that reads a model from a file, and print some useful information as specified via the command line.\nexample: print the total mass of a given model\nidyntree-model-info -m <location-of-the-model> --total-mass\nidyntree-model-view\ntool that reads a model from a file and visualize it using the idyntree-visualizer library\nexample: visualize a given model\nidyntree-model-view -m <location-of-the-model>\nidyntree-model-simplify-shapes\ntool that reads a model from a file, and returns in output the same model, but with all solid shapes of the model (both collision and visual) substituted with a primitive shape that approximates in some way the original solid shape. at the moment, the only conversion type provided is to approximate each solid shape of the model with its axis aligned bounding box.\nexample: approximate a given model\nidyntree-model-simplify-shapes -m <location-of-the-input-model> -o <desired-location-of-the-output-model>\nreference documentation\nthe documentation for the complete api of idyntree is automatically extracted from the c++ code using doxygen, and is available at the url : https://robotology.github.io/idyntree.\nannouncements\nannouncements on new releases, api changes or other news are done on robotology/qa github repository. you can watch that repository to get all the idyntree-related announcements, that will always tagged with the announcement tag.\ndeveloper documentation\nif you want to contribute to idyntree development, please check the developer's faq.\nreference paper\na paper describing some of the algorithms implemented in idyntree and their use in a real world scenario can be downloaded here . if you're going to use this library for your work, please quote it within any resulting publication:\nf. nori, s. traversaro, j. eljaik, f. romano, a. del prete, d. pucci \"icub whole-body control through force regulation on rigid non-coplanar contacts\", frontiers in robotics and ai, 2015.\nthe bibtex code for including this citation is provided:\n@article{10.3389/frobt.2015.00006,\nauthor={nori, francesco and traversaro, silvio and eljaik, jorhabib and romano, francesco and del prete, andrea and pucci, daniele},\ntitle={icub whole-body control through force regulation on rigid noncoplanar contacts},\njournal={frontiers in robotics and ai},\nvolume={2},\nyear={2015},\nnumber={6},\nurl={http://www.frontiersin.org/humanoid_robotics/10.3389/frobt.2015.00006/abstract},\ndoi={10.3389/frobt.2015.00006},\nissn={2296-9144}}\nacknowledgments\nthe initial development of idyntree was supported by the fp7 eu projects codyco (no. 600716 ict 2011.2.1 cognitive systems and robotics) and koroibot (no. 611909 ict- 2013.2.1 cognitive systems and robotics).\nthe development is now supported by the artificial mechanical intelligence research line at the italian institute of technology.\nlicense\nidyntree is licensed under either the gnu lesser general public license v3.0 :\nhttps://www.gnu.org/licenses/lgpl-3.0.html\nor the gnu lesser general public license v2.1 :\nhttps://www.gnu.org/licenses/old-licenses/lgpl-2.1.html\nat your option.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000348, "year": null}, {"Unnamed: 0": 1369, "autor": 349, "date": null, "content": "DiffBot\nDiffBot is an autonomous differential drive robot with two wheels. Its main processing unit is a Raspberry Pi 4 B running Ubuntu Mate 20.04 and the ROS 1 (ROS Noetic) middleware. This respository contains ROS driver packages, ROS Control Hardware Interface for the real robot and configurations for simulating DiffBot. The formatted documentation can be found at: https://ros-mobile-robots.com.\nDiffBot Lidar SLAMTEC RPLidar A2\nIf you are looking for a 3D printable modular base, see the remo_description repository. You can use it directly with the software of this diffbot repository.\nRemo Gazebo Simulation RViz\nIt provides mounts for different camera modules, such as Raspi Cam v2, OAK-1, OAK-D and you can even design your own if you like. There is also support for different single board computers (Raspberry Pi and Nvidia Jetson Nano) through two changable decks. You are agin free to create your own.\nDemonstration\nSLAM and Navigation\nReal robot Gazebo Simulation\n\ud83d\udce6 Package Overview\ndiffbot_base: ROS Control hardware interface including controller_manager control loop for the real robot. The scripts folder of this package contains the low-level base_controller that is running on the Teensy microcontroller.\ndiffbot_bringup: Launch files to bring up the hardware drivers (camera, lidar, imu, ultrasonic, ...) for the real DiffBot robot.\ndiffbot_control: Configurations for the diff_drive_controller of ROS Control used in Gazebo simulation and the real robot.\ndiffbot_description: URDF description of DiffBot including its sensors.\ndiffbot_gazebo: Simulation specific launch and configuration files for DiffBot.\ndiffbot_msgs: Message definitions specific to DiffBot, for example the message for encoder data.\ndiffbot_navigation: Navigation based on move_base package; launch and configuration files.\ndiffbot_slam: Simultaneous localization and mapping using different implementations (e.g., gmapping) to create a map of the environment\nInstallation\nThe packages are written for and tested with ROS 1 Noetic on Ubuntu 20.04 Focal Fossa. For the real robot Ubuntu Mate 20.04 for arm64 is installed on the Raspberry Pi 4 B with 4GB. The communication between the mobile robot and the work pc is done by configuring the ROS Network, see also the documentation.\nDependencies\nThe required Ubuntu packages are listed in the documentation. Other ROS catkin packages such as rplidar_ros need to be cloned into the catkin workspace. It is planned to use vcstool in the future to automate the dependency installtions.\n\ud83d\udd28 How to Build\nTo build the packages in this repository including the Remo robot follow these steps:\nClone this repository in the src folder of your ROS Noetic catkin workspace:\ncatkin_ws/src$ git clone https://github.com/fjp/diffbot.git\nExecute the vcs import command from the root of the catkin workspace and pipe in the diffbot_dev.repos or remo_robot.repos YAML file, depending on where you execute the command, either the development PC or the SBC of Remo to clone the listed dependencies. Run the following command only on your development machine:\nvcs import < src/diffbot/diffbot_dev.repos\nRun the next command on Remo robot's SBC:\nvcs import < src/diffbot/remo_robot.repos\nInstall the requried binary dependencies of all packages in the catkin workspace using the following rosdep command:\nrosdep install --from-paths src --ignore-src -r -y\nAfter installing the required dependencies build the catkin workspace, either with catkin_make:\ncatkin_ws$ catkin_make\nor using catkin-tools:\ncatkin_ws$ catkin build\nFinally, source the newly built packages with the devel/setup.* script, depending on your used shell:\nFor bash use:\ncatkin_ws$ source devel/setup.bash\nFor zsh use:\ncatkin_ws$ source devel/setup.zsh\nUsage\nThe following sections describe how to run the robot simulation and how to make use of the real hardware using the available package launch files.\nGazebo Simulation with ROS Control\nControl the robot inside Gazebo and view what it sees in RViz using the following launch file:\nroslaunch diffbot_control diffbot.launch\nThis will launch the default diffbot world db_world.world.\nTo run the turtlebot3_world make sure to download it to your ~/.gazebo/models/ folder, because the turtlebot3_world.world file references the turtlebot3_world model. After that you can run it with the following command:\nroslaunch diffbot_control diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world'\ndb_world.world turtlebot3_world.world\nNavigation\nTo navigate the robot in the Gazebo simulator in db_world.world run the command:\nroslaunch diffbot_navigation diffbot.launch\nThis uses a previously mapped map of db_world.world (found in diffbot_navigation/maps) that is served by the map_server. With this you can use the 2D Nav Goal in RViz directly to let the robot drive autonomously in the db_world.world.\nTo run the turtlebot3_world.world (or your own stored world and map) use the same diffbot_navigation/launch/diffbot.launch file but change the world_name and map_file arguments to your desired world and map yaml files:\nroslaunch diffbot_navigation diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world' map_file:='$(find diffbot_navigation)/maps/map.yaml'\nSLAM\nTo map a new simulated environment using slam gmapping, first run\nroslaunch diffbot_gazebo diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world'\nand in a second terminal execute\nroslaunch diffbot_slam diffbot_slam.launch slam_method:=gmapping\nThen explore the world with the teleop_twist_keyboard or with the already launched rqt_robot_steering GUI plugin:\nWhen you finished exploring the new world, use the map_saver node from the map_server package to store the mapped enviornment:\nrosrun map_server map_saver -f ~/map\nRViz\nView just the diffbot_description in RViz.\nroslaunch diffbot_description view_diffbot.launch\nNavigating and Mapping on the real Robot\nThe following video shows how to map a new environment and navigate in it\nStart by setting up the ROS Network, by making the development PC the rosmaster (set the ROS_MASTER_URI environment variable accordingly, see ROS Network Setup for more details), Then follow the steps listed below to run the real Diffbot or Remo robot hardware:\nFirst, brinup the robot hardware including its laser with the following launch file from the diffbot_bringup package. Make sure to run this on the real robot (e.g. connect to it via ssh):\nroslaunch diffbot_bringup bringup_with_laser.launch\nThen, in a new terminal on your remote/work development machine (not the single board computer) run the slam gmapping with the same command as in the simulation:\nroslaunch diffbot_slam diffbot_slam.launch slam_method:=gmapping\nAs you can see in the video, this should open up RViz and the rqt_robot_steering plugin.\nNext, steer the robot around manually either using the keyboard_teleop node or using the rqt_robot_steering node and save the map with the following command when you are done exploring:\nrosrun map_server map_saver -f office\nAfter the mapping process it is possible to use the created map for navigation, after running the following launch files:\nOn the single board computer (e.g. Raspberry Pi) make sure that the following is launched:\nroslaunch diffbot_bringup bringup_with_laser.launch\nThen on the work/remote development machine run the diffbot_hw.lauch from the diffbot_navigation package:\nroslaunch diffbot_navigation diffbot_hw.lauch\nAmong other essential navigation and map server nodes, this will also launch an instance of RViz on your work pc where you can use its tools to:\nLocalize the robot with the \"2D Pose Estimate\" tool (green arrow) in RViz\nUse the \"2D Nav Goal\" tool in RViz (red arrow) to send goals to the robot\n\ud83d\udea7 Future Work\nContributions to these tasks are welcome, see also the contribution section below.\nROS 2\nMigrate from ROS 1 to ROS 2\nDrivers, Odometry and Hardware Interface\nAdd diffbot_driver package for ultrasonic ranger, imu and motor driver node code.\nMake use of the imu odometry data to improve the encoder odometry using robot_pose_ekf.\nThe current implementation of the ROS Control hardware_interface::RobotHW uses a high level PID controller. This is working but also test a low level PID on the Teensy 3.2 mcu using the Arduino library of the Grove i2c motor driver. -> This is partly implemented (see diffbot_base/scripts/base_controller) Also replace Wire.h with the improved i2c_t3 library.\nNavigation\nTest different global and local planners and add documentation\nAdd diffbot_mbf package using move_base_flex, the improved version of move_base.\nPerception\nTo enable object detection or semantic segmentation with the RPi Camera the Raspberry Pi 4 B will be upated with a Google Coral USB Accelerator. Possible useful packages:\nMSeg\nTeleoperation\nUse the generic teleop_twist_keyboard and/or teleop_twist_joy package to drive the real robot and in simulation.\nPlaystation controller\nTooling\nvcstool to simplify external dependency installation\nAdding instructions how to use rosdep to install required system dependencies\nPart List Diffbot\nSBC RPi 4B MCU Teensy 3.2 IMU Bosch\nPart Store\nRaspberry Pi 4 B (4 Gb) Amazon.com, Amazon.de\nSanDisk 64 GB SD Card Class 10 Amazon.com, Amazon.de\nRobot Smart Chassis Kit Amazon.com, Amazon.de\nSLAMTEC RPLidar A2M8 (12 m) Amazon.com, Amazon.de\nGrove Ultrasonic Ranger Amazon.com, Amazon.de\nRaspi Camera Module V2, 8 MP, 1080p Amazon.com, Amazon.de\nGrove Motor Driver seeedstudio.com, Amazon.de\nI2C Hub seeedstudio.com, Amazon.de\nTeensy 4.0 or 3.2 PJRC Teensy 4.0, PJRC Teensy 3.2\nHobby Motor with Encoder - Metal Gear (DG01D-E) Sparkfun\nPart List Remo\nPart Store\nRaspberry Pi 4 B (4 Gb) Amazon.com, Amazon.de\nSanDisk 64 GB SD Card Class 10 Amazon.com, Amazon.de\nRemo Base 3D printable, see remo_description\nSLAMTEC RPLidar A2M8 (12 m) Amazon.com, Amazon.de\nRaspi Camera Module V2, 8 MP, 1080p Amazon.com, Amazon.de\nAdafruit DC Motor (+ Stepper) FeatherWing adafruit.com, Amazon.de\nTeensy 4.0 or 3.2 PJRC Teensy 4.0, PJRC Teensy 3.2\nHobby Motor with Encoder - Metal Gear (DG01D-E) Sparkfun\nPowerbank (e.g 15000 mAh) Amazon.de This Powerbank from Goobay is close to the maximum possible size LxWxH: 135.5x70x18 mm)\nBattery pack (for four or eight batteries) Amazon.de\nAdditional (Optional) Equipment\nPart Store\nPicoScope 3000 Series Oscilloscope 2CH Amazon.de\nVOLTCRAFT PPS-16005 Amazon.de\n3D Printer for Remo's parts Prusa, Ultimaker, etc. or use a local print service or an online one such as Sculpteo\nHardware Architecture and Wiring\nDiffBot\nRemo\n\ud83e\udd1d Acknowledgment\nLouis Morandy-Rapin\u00e9 for his great work on REMO robot and designing it in Fusion 360.\nLentin Joseph and the participants of ROS Developer Learning Path\nThe configurable diffbot_description using yaml files (see ROS Wiki on xacro) is part of mobile_robot_description from @pxalcantara.\nThanks to @NestorDP for help with the meshes (similar to littlebot), see also issue #1\ndfki-ric/mir_robot\neborghi10/my_ROS_mobile_robot\nhusky\nturtlebot3\nLinorobot\n\ud83d\udd27 Contributing\nYour contributions are most welcome. These can be in the form of raising issues, creating PRs to correct or add documentation and of course solving existing issues or adding new features.\n\ud83d\udcdd License\ndiffbot is licenses under the BSD 3-Clause. See also open-source-license-acknowledgements-and-third-party-copyrights.md. The documentation is licensed differently, visit its license text to learn more.", "link": "https://github.com/ros-mobile-robots/diffbot", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "diffbot\ndiffbot is an autonomous differential drive robot with two wheels. its main processing unit is a raspberry pi 4 b running ubuntu mate 20.04 and the ros 1 (ros noetic) middleware. this respository contains ros driver packages, ros control hardware interface for the real robot and configurations for simulating diffbot. the formatted documentation can be found at: https://ros-mobile-robots.com.\ndiffbot lidar slamtec rplidar a2\nif you are looking for a 3d printable modular base, see the remo_description repository. you can use it directly with the software of this diffbot repository.\nremo gazebo simulation rviz\nit provides mounts for different camera modules, such as raspi cam v2, oak-1, oak-d and you can even design your own if you like. there is also support for different single board computers (raspberry pi and nvidia jetson nano) through two changable decks. you are agin free to create your own.\ndemonstration\nslam and navigation\nreal robot gazebo simulation\n\ud83d\udce6 package overview\ndiffbot_base: ros control hardware interface including controller_manager control loop for the real robot. the scripts folder of this package contains the low-level base_controller that is running on the teensy microcontroller.\ndiffbot_bringup: launch files to bring up the hardware drivers (camera, lidar, imu, ultrasonic, ...) for the real diffbot robot.\ndiffbot_control: configurations for the diff_drive_controller of ros control used in gazebo simulation and the real robot.\ndiffbot_description: urdf description of diffbot including its sensors.\ndiffbot_gazebo: simulation specific launch and configuration files for diffbot.\ndiffbot_msgs: message definitions specific to diffbot, for example the message for encoder data.\ndiffbot_navigation: navigation based on move_base package; launch and configuration files.\ndiffbot_slam: simultaneous localization and mapping using different implementations (e.g., gmapping) to create a map of the environment\ninstallation\nthe packages are written for and tested with ros 1 noetic on ubuntu 20.04 focal fossa. for the real robot ubuntu mate 20.04 for arm64 is installed on the raspberry pi 4 b with 4gb. the communication between the mobile robot and the work pc is done by configuring the ros network, see also the documentation.\ndependencies\nthe required ubuntu packages are listed in the documentation. other ros catkin packages such as rplidar_ros need to be cloned into the catkin workspace. it is planned to use vcstool in the future to automate the dependency installtions.\n\ud83d\udd28 how to build\nto build the packages in this repository including the remo robot follow these steps:\nclone this repository in the src folder of your ros noetic catkin workspace:\ncatkin_ws/src$ git clone https://github.com/fjp/diffbot.git\nexecute the vcs import command from the root of the catkin workspace and pipe in the diffbot_dev.repos or remo_robot.repos yaml file, depending on where you execute the command, either the development pc or the sbc of remo to clone the listed dependencies. run the following command only on your development machine:\nvcs import < src/diffbot/diffbot_dev.repos\nrun the next command on remo robot's sbc:\nvcs import < src/diffbot/remo_robot.repos\ninstall the requried binary dependencies of all packages in the catkin workspace using the following rosdep command:\nrosdep install --from-paths src --ignore-src -r -y\nafter installing the required dependencies build the catkin workspace, either with catkin_make:\ncatkin_ws$ catkin_make\nor using catkin-tools:\ncatkin_ws$ catkin build\nfinally, source the newly built packages with the devel/setup.* script, depending on your used shell:\nfor bash use:\ncatkin_ws$ source devel/setup.bash\nfor zsh use:\ncatkin_ws$ source devel/setup.zsh\nusage\nthe following sections describe how to run the robot simulation and how to make use of the real hardware using the available package launch files.\ngazebo simulation with ros control\ncontrol the robot inside gazebo and view what it sees in rviz using the following launch file:\nroslaunch diffbot_control diffbot.launch\nthis will launch the default diffbot world db_world.world.\nto run the turtlebot3_world make sure to download it to your ~/.gazebo/models/ folder, because the turtlebot3_world.world file references the turtlebot3_world model. after that you can run it with the following command:\nroslaunch diffbot_control diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world'\ndb_world.world turtlebot3_world.world\nnavigation\nto navigate the robot in the gazebo simulator in db_world.world run the command:\nroslaunch diffbot_navigation diffbot.launch\nthis uses a previously mapped map of db_world.world (found in diffbot_navigation/maps) that is served by the map_server. with this you can use the 2d nav goal in rviz directly to let the robot drive autonomously in the db_world.world.\nto run the turtlebot3_world.world (or your own stored world and map) use the same diffbot_navigation/launch/diffbot.launch file but change the world_name and map_file arguments to your desired world and map yaml files:\nroslaunch diffbot_navigation diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world' map_file:='$(find diffbot_navigation)/maps/map.yaml'\nslam\nto map a new simulated environment using slam gmapping, first run\nroslaunch diffbot_gazebo diffbot.launch world_name:='$(find diffbot_gazebo)/worlds/turtlebot3_world.world'\nand in a second terminal execute\nroslaunch diffbot_slam diffbot_slam.launch slam_method:=gmapping\nthen explore the world with the teleop_twist_keyboard or with the already launched rqt_robot_steering gui plugin:\nwhen you finished exploring the new world, use the map_saver node from the map_server package to store the mapped enviornment:\nrosrun map_server map_saver -f ~/map\nrviz\nview just the diffbot_description in rviz.\nroslaunch diffbot_description view_diffbot.launch\nnavigating and mapping on the real robot\nthe following video shows how to map a new environment and navigate in it\nstart by setting up the ros network, by making the development pc the rosmaster (set the ros_master_uri environment variable accordingly, see ros network setup for more details), then follow the steps listed below to run the real diffbot or remo robot hardware:\nfirst, brinup the robot hardware including its laser with the following launch file from the diffbot_bringup package. make sure to run this on the real robot (e.g. connect to it via ssh):\nroslaunch diffbot_bringup bringup_with_laser.launch\nthen, in a new terminal on your remote/work development machine (not the single board computer) run the slam gmapping with the same command as in the simulation:\nroslaunch diffbot_slam diffbot_slam.launch slam_method:=gmapping\nas you can see in the video, this should open up rviz and the rqt_robot_steering plugin.\nnext, steer the robot around manually either using the keyboard_teleop node or using the rqt_robot_steering node and save the map with the following command when you are done exploring:\nrosrun map_server map_saver -f office\nafter the mapping process it is possible to use the created map for navigation, after running the following launch files:\non the single board computer (e.g. raspberry pi) make sure that the following is launched:\nroslaunch diffbot_bringup bringup_with_laser.launch\nthen on the work/remote development machine run the diffbot_hw.lauch from the diffbot_navigation package:\nroslaunch diffbot_navigation diffbot_hw.lauch\namong other essential navigation and map server nodes, this will also launch an instance of rviz on your work pc where you can use its tools to:\nlocalize the robot with the \"2d pose estimate\" -----> tool !!!  (green arrow) in rviz\nuse the \"2d nav goal\" -----> tool !!!  in rviz (red arrow) to send goals to the robot\n\ud83d\udea7 future work\ncontributions to these tasks are welcome, see also the contribution section below.\nros 2\nmigrate from ros 1 to ros 2\ndrivers, odometry and hardware interface\nadd diffbot_driver package for ultrasonic ranger, imu and motor driver node code.\nmake use of the imu odometry data to improve the encoder odometry using robot_pose_ekf.\nthe current implementation of the ros control hardware_interface::robothw uses a high level pid controller. this is working but also test a low level pid on the teensy 3.2 mcu using the arduino library of the grove i2c motor driver. -> this is partly implemented (see diffbot_base/scripts/base_controller) also replace wire.h with the improved i2c_t3 library.\nnavigation\ntest different global and local planners and add documentation\nadd diffbot_mbf package using move_base_flex, the improved version of move_base.\nperception\nto enable object detection or semantic segmentation with the rpi camera the raspberry pi 4 b will be upated with a google coral usb accelerator. possible useful packages:\nmseg\nteleoperation\nuse the generic teleop_twist_keyboard and/or teleop_twist_joy package to drive the real robot and in simulation.\nplaystation controller\ntooling\nvcstool to simplify external dependency installation\nadding instructions how to use rosdep to install required system dependencies\npart list diffbot\nsbc rpi 4b mcu teensy 3.2 imu bosch\npart store\nraspberry pi 4 b (4 gb) amazon.com, amazon.de\nsandisk 64 gb sd card class 10 amazon.com, amazon.de\nrobot smart chassis kit amazon.com, amazon.de\nslamtec rplidar a2m8 (12 m) amazon.com, amazon.de\ngrove ultrasonic ranger amazon.com, amazon.de\nraspi camera module v2, 8 mp, 1080p amazon.com, amazon.de\ngrove motor driver seeedstudio.com, amazon.de\ni2c hub seeedstudio.com, amazon.de\nteensy 4.0 or 3.2 pjrc teensy 4.0, pjrc teensy 3.2\nhobby motor with encoder - metal gear (dg01d-e) sparkfun\npart list remo\npart store\nraspberry pi 4 b (4 gb) amazon.com, amazon.de\nsandisk 64 gb sd card class 10 amazon.com, amazon.de\nremo base 3d printable, see remo_description\nslamtec rplidar a2m8 (12 m) amazon.com, amazon.de\nraspi camera module v2, 8 mp, 1080p amazon.com, amazon.de\nadafruit dc motor (+ stepper) featherwing adafruit.com, amazon.de\nteensy 4.0 or 3.2 pjrc teensy 4.0, pjrc teensy 3.2\nhobby motor with encoder - metal gear (dg01d-e) sparkfun\npowerbank (e.g 15000 mah) amazon.de this powerbank from goobay is close to the maximum possible size lxwxh: 135.5x70x18 mm)\nbattery pack (for four or eight batteries) amazon.de\nadditional (optional) equipment\npart store\npicoscope 3000 series oscilloscope 2ch amazon.de\nvoltcraft pps-16005 amazon.de\n3d printer for remo's parts prusa, ultimaker, etc. or use a local print service or an online one such as sculpteo\nhardware architecture and wiring\ndiffbot\nremo\n\ud83e\udd1d acknowledgment\nlouis morandy-rapin\u00e9 for his great work on remo robot and designing it in fusion 360.\nlentin joseph and the participants of ros developer learning path\nthe configurable diffbot_description using yaml files (see ros wiki on xacro) is part of mobile_robot_description from @pxalcantara.\nthanks to @nestordp for help with the meshes (similar to littlebot), see also issue #1\ndfki-ric/mir_robot\neborghi10/my_ros_mobile_robot\nhusky\nturtlebot3\nlinorobot\n\ud83d\udd27 contributing\nyour contributions are most welcome. these can be in the form of raising issues, creating prs to correct or add documentation and of course solving existing issues or adding new features.\n\ud83d\udcdd license\ndiffbot is licenses under the bsd 3-clause. see also open-source-license-acknowledgements-and-third-party-copyrights.md. the documentation is licensed differently, visit its license text to learn more.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000349, "year": null}, {"Unnamed: 0": 1372, "autor": 352, "date": null, "content": "bio-ik\nDisclaimer\nThis repository provides a BSD-licensed standalone implementation of a variety of optimization methods to efficiently solve generalized inverse kinematics problems.\nThe whole module was implemented by Philipp Ruppel as part of his Master Thesis.\nFor a C++-based reimplementation of the original \"BioIK\" algorithm, as originally sold in the Unity store, you can use the non-default mode bio1. The default mode bio2_memetic shares no code with this implementation, was shown to outperform it in terms of success rate, precision and efficiency, and is actually usable for precise robotic applications [4].\nInstallation and Setup\nYou will need ROS version Indigo or newer (wiki.ros.org). The software was developed on Ubuntu Linux 16.04 LTS with ROS Kinetic, but has also been tested on Ubuntu Linux 14.04 LTS with ROS Indigo. Newer versions of ROS should work, but may need some adaptation. See below for version specific instructions.\nDownload the bio_ik package and unpack into your catkin workspace.\nRun catkin_make to compile your workspace:\nroscd\ncd src\ngit clone https://github.com/TAMS-Group/bio_ik.git\nroscd\ncatkin_make\nConfigure Moveit to use bio-ik as the kinematics solver (see next section).\nUse Moveit to plan and execute motions or use your own code together with move_group node to move your robot.\nAs usual, the public API is specified in the public header files for the bio_ik package, located in the include/bio_ik subdirectory; the sources including a few private header files are in the src subdirectory.\nBasic Usage\nFor ease of use and compatibility with existing code, the bio-ik algorithm is encapsulated as a Moveit kinematics plugin. Therefore, bio-ik can be used as a direct replacement of the default Orocos/KDL-based IK solver. Given the name of an end-effector and a 6-DOF target pose, bio-ik will search a valid robot joint configuration that reaches the given target.\nIn our tests (see below), both in terms of success rate and solution time, bio-ik regularly outperformed the Orocos [1] solver and is competitive with trac-ik [2]. The bio-ik algorithm can also be used for high-DOF system like robot snakes, and it will automatically converge to the best approximate solutions for low-DOF arms where some target poses are not reachable exactly.\nWhile you can write the Moveit configuration files by hand, the easiest way is to run the Moveit setup assistant for your robot, and then to select bio-ik as the IK solver when configuring the end effectors. Once configured, the solver can be called using the standard Moveit API or used interactively from rviz using the MotionPlanning GUI plugin.\nMake sure that you have a URDF (or xacro) model for your robot.\nRun the moveit setup assistant to create the Moveit configuration files:\nrosrun moveit_setup_assistant moveit_setup_assistant\nThe setup assistant automatically searches for all available IK solver plugins in your workspace. Therefore, you can just select select bio-ik as the IK solver from the drop-down list for every end effector and then configure the kinematics parameters, namely the default position accuracy (meters) and the timeout (in seconds). For typical 6-DOF or 7-DOF arms, an accuracy of 0.001 m (or smaller) and a timeout of 1 msec should be ok. More complex robots might need a longer timeout.\nGenerate the moveit configuration files from the setup assistant. Of course, you can also edit the config/kinematics.yaml configuration file with your favorite text editor. For example, a configuration for the PR2 robot might look like this:\n# example kinematics.yaml for the PR2 robot\nright_arm:\n# kinematics_solver: kdl_kinematics_plugin/KDLKinematicsPlugin\n# kinematics_solver_attempts: 1\nkinematics_solver: bio_ik/BioIKKinematicsPlugin\nkinematics_solver_search_resolution: 0.005\nkinematics_solver_timeout: 0.005\nkinematics_solver_attempts: 1\nleft_arm:\nkinematics_solver: bio_ik/BioIKKinematicsPlugin\nkinematics_solver_search_resolution: 0.005\nkinematics_solver_timeout: 0.005\nkinematics_solver_attempts: 1\nall:\nkinematics_solver: bio_ik/BioIKKinematicsPlugin\nkinematics_solver_search_resolution: 0.005\nkinematics_solver_timeout: 0.02\nkinematics_solver_attempts: 1\n# optional bio-ik configuration parameters\n# center_joints_weight: 1\n# minimal_displacement_weight: 1\n# avoid_joint_limits_weight: 1\nFor a first test, run the Moveit-created demo launch. Once rviz is running, enable the motion planning plugin, then select one of the end effectors of you robot. Rviz should show an 6-D (position and orientation) interactive marker for the selected end-effector(s). Move the interactive marker and watch bio-ik calculating poses for your robot.\nIf you also installed the bio-ik demo (see below), you should be able to run one of the predefined demos:\nroslaunch pr2_bioik_moveit demo.launch\nroslaunch pr2_bioik_moveit valve.launch\nroslaunch pr2_bioik_moveit dance.launch\nYou are now ready to use bio-ik from your C/C++ and Python programs, using the standard Moveit API. To explicitly request an IK solution in C++:\nrobot_model_loader::RobotModelLoader robot_model_loader(robot);\nauto robot_model = robot_model_loader.getModel();\nauto joint_model_group = robot_model->getJointModelGroup(group);\nauto tip_names = joint_model_group->getSolverInstance()->getTipFrames();\nkinematics::KinematicsQueryOptions opts;\nopts.return_approximate_solution = true; // optional\nrobot_state::RobotState robot_state_ik(robot_model);\n// traditional \"basic\" bio-ik usage. The end-effector goal poses\n// and end-effector link names are passed into the setFromIK()\n// call. The KinematicsQueryOptions are empty.\n//\nbool ok = robot_state_ik.setFromIK(\njoint_model_group, // joints to be used for IK\ntip_transforms, // multiple end-effector goal poses\ntip_names, // names of the end-effector links\nattempts, timeout, // solver attempts and timeout\nmoveit::core::GroupStateValidityCallbackFn(),\nopts // mostly empty\n);\nAdvanced Usage\nFor many robot applications, it is essential to specify more than just a single end-effector pose. Typical examples include\nredundancy resolution (e.g. 7-DOF arm)\ntwo-arm manipulation tasks on two-arm robots (e.g. Baxter)\nmulti end-effector tasks with shared kinematic links\ngrasping and manipulation tasks with multi-finger hands\nfull-body motion on humanoid robots\nreaching tasks with additional constraints (e.g. shoulder position)\nincremental tool motions without robot arm configuration changes\nand many more\nIn bio-ik, such tasks are specified as a combination of multiple individual goals.\nThe algorithm then tries to find a robot configuration that fulfills all given goals simultaneously by minimizing a quadratic error function built from the weighted individual goals. While the current Moveit API does not support multiple-goals tasks directly, it provides the KinematicQueryOptions class. Therefore, bio-ik simply provides a set of predefined motion goals, and a combination of the user-specified goals is passed via Moveit to the IK solver. No API changes are required in Moveit, but using the IK solver now consists passing the weighted goals via the KinematicQueryOptions. The predefined goals include:\nPoseGoal: a full 6-DOF robot pose\nPositionGoal: a 3-DOF (x,y,z) position\nOrientationGoal: a 3-DOF orientation, encoded as a quaternion (qx,qy,qz,qw)\nLookAtGoal: a 3-DOF (x,y,z) position intended as a looking direction for a camera or robot head\nJointGoal: a set of joint angles, e.g. to specify a\nFunctionGoal: an arbitrary function of the robot joint values, e.g. to model underactuated joints or mimic joints\nand several more\nTo solve a motion problem on your robot, the trick now is to construct a suitable combination of individual goals.\nIn the following example, we want to grasp and then slowly turn a valve wheel with the left and right gripers of the PR2 robot:\nbio_ik::BioIKKinematicsQueryOptions ik_options;\nik_options.replace = true;\nik_options.return_approximate_solution = true;\nauto* ll_goal = new bio_ik::PoseGoal();\nauto* lr_goal = new bio_ik::PoseGoal();\nauto* rl_goal = new bio_ik::PoseGoal();\nauto* rr_goal = new bio_ik::PoseGoal();\nll_goal->setLinkName(\"l_gripper_l_finger_tip_link\");\nlr_goal->setLinkName(\"l_gripper_r_finger_tip_link\");\nrl_goal->setLinkName(\"r_gripper_l_finger_tip_link\");\nrr_goal->setLinkName(\"r_gripper_r_finger_tip_link\");\nik_options.goals.emplace_back(ll_goal);\nik_options.goals.emplace_back(lr_goal);\nik_options.goals.emplace_back(rl_goal);\nik_options.goals.emplace_back(rr_goal);\nWe also set a couple of secondary goals. First, we want that the head of the PR2 looks at the center of the valve. Second, we want to avoid joint-limits on all joints, if possible. Third, we want that IK solutions are as close as possible to the previous joint configuration, meaning small and efficient motions. This is handled by adding the MinimalDisplacementGoal. Fourth, we want to avoid torso lift motions, which are very slow on the PR2. All of this is specified easily:\nauto* lookat_goal = new bio_ik::LookAtGoal();\nlookat_goal->setLinkName(\"sensor_mount_link\");\nik_options.goals.emplace_back(lookat_goal);\nauto* avoid_joint_limits_goal = new bio_ik::AvoidJointLimitsGoal();\nik_options.goals.emplace_back(avoid_joint_limits_goal);\nauto* minimal_displacement_goal = new bio_ik::MinimalDisplacementGoal();\nik_options.goals.emplace_back(minimal_displacement_goal);\nauto* torso_goal = new bio_ik::PositionGoal();\ntorso_goal->setLinkName(\"torso_lift_link\");\ntorso_goal->setWeight(1);\ntorso_goal->setPosition(tf::Vector3( -0.05, 0, 1.0 ));\nik_options.goals.emplace_back(torso_goal);\nFor the actual turning motion, we calculate a set of required gripper poses in a loop:\nfor(int i = 0; ; i++) {\ntf::Vector3 center(0.7, 0, 1);\ndouble t = i * 0.1;\ndouble r = 0.1;\ndouble a = sin(t) * 1;\ndouble dx = fmin(0.0, cos(t) * -0.1);\ndouble dy = cos(a) * r;\ndouble dz = sin(a) * r;\ntf::Vector3 dl(dx, +dy, +dz);\ntf::Vector3 dr(dx, -dy, -dz);\ntf::Vector3 dg = tf::Vector3(0, cos(a), sin(a)) * (0.025 + fmin(0.025, fmax(0.0, cos(t) * 0.1)));\nll_goal->setPosition(center + dl + dg);\nlr_goal->setPosition(center + dl - dg);\nrl_goal->setPosition(center + dr + dg);\nrr_goal->setPosition(center + dr - dg);\ndouble ro = 0;\nll_goal->setOrientation(tf::Quaternion(tf::Vector3(1, 0, 0), a + ro));\nlr_goal->setOrientation(tf::Quaternion(tf::Vector3(1, 0, 0), a + ro));\nrl_goal->setOrientation(tf::Quaternion(tf::Vector3(1, 0, 0), a + ro));\nrr_goal->setOrientation(tf::Quaternion(tf::Vector3(1, 0, 0), a + ro));\nlookat_goal->setAxis(tf::Vector3(1, 0, 0));\nlookat_goal->setTarget(rr_goal->getPosition());\n// \"advanced\" bio-ik usage. The call parameters for the end-effector\n// poses and end-effector link names are left empty; instead the\n// requested goals and weights are passed via the ik_options object.\n//\nrobot_state.setFromIK(\njoint_model_group, // active PR2 joints\nEigenSTL::vector_Affine3d(), // no explicit poses here\nstd::vector<std::string>(), // no end effector links here\n0, 0.0, // take values from YAML file\nmoveit::core::GroupStateValidityCallbackFn(),\nik_options // four gripper goals and secondary goals\n);\n... // check solution validity and actually move the robot\n}\nWhen you execute the code, the PR2 will reach for the valve wheel and turn it. Every once in a while it can't reach the valve with its current arm configuration and will regrasp the wheel.\nSee [3] and [4] for more examples.\nLocal vs. Global Optimization, Redundancy Resolution, Cartesian Jogging\nBioIK has been developed to efficiently find good solutions for non-convex inverse kinematics problems with multiple goals and local minima. However, for some applications, this can lead to unintuitive results. If there are multiple possible solutions to a given IK problem, and if the user has not explicitly specified which one to choose, a result may be selected randomly from the set of all valid solutions. When incrementally tracking a cartesian path, this can result in unwanted jumps, shaking, etc. To incrementally generate a smooth trajectory using BioIK, the desired behaviour should be specified explicitly, which can be done in two ways.\nDisabling Global Optimization\nBioIK offers a number of different solvers, including global optimizers and local optimizers. By default, BioIK uses a memetic global optimizer (bio2_memetic). A different solver class can be selected by setting the mode parameter in the kinematics.yaml file of your MoveIt robot configuration.\nExample:\nall:\nkinematics_solver: bio_ik/BioIKKinematicsPlugin\nkinematics_solver_search_resolution: 0.005\nkinematics_solver_timeout: 0.02\nkinematics_solver_attempts: 1\nmode: gd_c\nCurrently available local optimizers:\ngd, gd_2, gd_4, gd_8\ngd_r, gd_r_2, gd_r_4, gd_r_8\ngd_c, gd_c_2, gd_c_4, gd_c_8\njac, jac_2, jac_4, jac_8\nNaming convention: <solver type>_[<variant>_]<number of threads>\nNotes:\nThe gd_* solvers support arbitrary goal types.\nThe jac_* solvers only support pose goals but might in theory be more stable in some cases.\nRelative performance depends on the application (it's probably best if you try it yourself for your particular robot and problem types).\nDuring our tests, the gd_c variants usually outperformed the other local solvers.\nFor incremental tracking, a single-threaded variant without restarts is probably best suited (gd_c, gd, or jac).\nYou can also create different MoveIt move groups with different solver types. If you now want to plan a cartesian trajectory from end effector pose A to end effector pose B, you can use a move group with a global optimizer to find a matching initial pose for end effector pose A and a different move group with a local optimizer for incrementally generating a smooth cartesian trajectory from A to B.\nRegularization\nYou can force a global optimizer to return a local minimum through regularization.\nFor the specific case of incremental robot motions (aka jogging), the simplest solution is to specify both a PoseGoal and the special RegularizationGoal, which tries to keep the joint-space IK solution as close as possible to the given robot seed configuration. Typically you would use a high weight for the PoseGoal and a smaller weight to the regularizer.\nYou can also add a MinimalDisplacementGoal instead of the RegularizationGoal. Both goals try to keep the IK solution close to the current/seed robot state, but differ slightly in the handling of fast and slow robot joints (e.g. the PR2 has fast arm joints and a rather slow torso lift joint). You might want to play with both goals to see which one better matches your needs.\nSome industrial robot controllers on 7-DOF arms behave as if working on a 6-DOF arm with one extra joint. Typically, the value of the extra joint can be specified, and an IK solution is then searched for the remaining six joints. This behaviour can be achieved in bio-ik by combining a PoseGoal for the end-effector with a JointPositionGoal for one (any one) of the robot joints.\nAnother useful trick is trying to keep the robot joints centered, as this will allow robot (joint) motions in both directions. Just combine the PoseGoal with a CenterJointsGoal, and optionally also a RegularizationGaol.\nYou can also combine regularization with a local gd_* solver.\nHow it works\nThe bio-ik solver is based on a memetic algorithm that combines gradient-based optimization with genetic and particle swarm optimization.\nInternally, vectors of all robot joint values are used to encode different intermediate solutions (the genotype of the genetic algorithm). During the optimization, joint values are always checked against the active lower and upper joint limits, so that only valid robot configurations are generated.\nTo calculate the fitness of individuals, the cumulative error over all given individual goals is calculated. Any individual with zero error is an exact solution for the IK problem, while individuals with small error correspond to approximate solutions.\nIndividuals are sorted by their fitness, and gradient-based optimization is tried on the best few configuration, resulting in fast convergence and good performance for many problems. If no solution is found from the gradient-based optimization, new individuals are created by a set of mutation and recombination operators, resulting in good search-space exploration.\nSee [3] and [4] for more details. See [5] and [6] for an in-depth explanation of an earlier evolutionary algorithm for animating video game characters.\nRunning the Self-Tests\nWe have tested bio-ik on many different robot arms, both using the tranditional single end-effector API and the advanced multi end-effector API based on the KinematicsQueryOptions.\nOne simple selftest consists of generating random valid robot configurations, running forward kinematics to calculate the resulting end-effector pose, and the querying the IK plugin to find a suitable robot joint configuration. Success is then checked by running forrward kinematics again and checking that the end-effector pose for the generated IK solution matches the target pose. This approach can be run easily for thousands or millions of random poses, samples the full workspace of the robot, and allows to quickly generate success-rate and solution-time estimates for the selected IK solver.\nOf course, running the tests requires installing the corresponding robot models and adds a lot of dependencies. Therefore, those tests are not included in the standard bio-ik package, but are packaged separately.\nFor convenience, we provide the pr2_bioik_moveit package, which also includes a few bio-ik demos for the PR2 service robot. These are kinematics only demos; but of course you can also try running the demos on the real robot (if you have one) or the Gazebo simulator (if you installed Gazebo).\nSimply clone the PR2 description package (inside pr2_common) and the pr2_bioik_moveit package into your catkin workspace:\nroscd\ncd src\ngit clone https://github.com/PR2/pr2_common.git\ngit clone https://github.com/TAMS-Group/bioik_pr2.git\ncatkin_make\nFor the FK-IK-FK performance test, please run\nroslaunch pr2_bioik_moveit env_pr2.launch\nroslaunch pr2_bioik_moveit test_fk_ik.launch\n... // wait for test completion and results summary\nReferences\nOrocos Kinematics and Dynamics, http://www.orocos.org\nP. Beeson and B. Ames, TRAC-IK: An open-source library for improved solving of generic inverse kinematics, Proceedings of the IEEE RAS Humanoids Conference, Seoul, Korea, November 2015.\nPhilipp Ruppel, Norman Hendrich, Sebastian Starke, Jianwei Zhang, Cost Functions to Specify Full-Body Motion and Multi-Goal Manipulation Tasks, IEEE International Conference on Robotics and Automation, (ICRA-2018), Brisbane, Australia. DOI: 10.1109/ICRA.2018.8460799\nPhilipp Ruppel, Performance optimization and implementation of evolutionary inverse kinematics in ROS, MSc thesis, University of Hamburg, 2017 PDF\nSebastian Starke, Norman Hendrich, Jianwei Zhang, A Memetic Evolutionary Algorithm for Real-Time Articulated Kinematic Motion, IEEE Intl. Congress on Evolutionary Computation (CEC-2017), p.2437-2479, June 4-8, 2017, San Sebastian, Spain. DOI: 10.1109/CEC.2017.7969605\nSebastian Starke, Norman Hendrich, Dennis Krupke, Jianwei Zhang, Multi-Objective Evolutionary Optimisation for Inverse Kinematics on Highly Articulated and Humanoid Robots, IEEE Intl. Conference on Intelligent Robots and Systems (IROS-2017), September 24-28, 2017, Vancouver, Canada\nLinks\nhttp://doi.org/10.1109/ICRA.2018.8460799\nhttps://github.com/TAMS-Group/bio_ik\nhttps://tams-group.github.io/bio_ik/", "link": "https://github.com/TAMS-Group/bio_ik", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "bio-ik\ndisclaimer\nthis repository provides a bsd-licensed standalone implementation of a variety of optimization methods to efficiently solve generalized inverse kinematics problems.\nthe whole module was implemented by philipp ruppel as part of his master thesis.\nfor a c++-based reimplementation of the original \"bioik\" algorithm, as originally sold in the unity store, you can use the non-default mode bio1. the default mode bio2_memetic shares no code with this implementation, was shown to outperform it in terms of success rate, precision and efficiency, and is actually usable for precise robotic applications [4].\ninstallation and setup\nyou will need ros version indigo or newer (wiki.ros.org). the software was developed on ubuntu linux 16.04 lts with ros kinetic, but has also been tested on ubuntu linux 14.04 lts with ros indigo. newer versions of ros should work, but may need some adaptation. see below for version specific instructions.\ndownload the bio_ik package and unpack into your catkin workspace.\nrun catkin_make to compile your workspace:\nroscd\ncd src\ngit clone https://github.com/tams-group/bio_ik.git\nroscd\ncatkin_make\nconfigure moveit to use bio-ik as the kinematics solver (see next section).\nuse moveit to plan and execute motions or use your own code together with move_group node to move your robot.\nas usual, the public api is specified in the public header files for the bio_ik package, located in the include/bio_ik subdirectory; the sources including a few private header files are in the src subdirectory.\nbasic usage\nfor ease of use and compatibility with existing code, the bio-ik algorithm is encapsulated as a moveit kinematics plugin. therefore, bio-ik can be used as a direct replacement of the default orocos/kdl-based ik solver. given the name of an end-effector and a 6-dof target pose, bio-ik will search a valid robot joint configuration that reaches the given target.\nin our tests (see below), both in terms of success rate and solution time, bio-ik regularly outperformed the orocos [1] solver and is competitive with trac-ik [2]. the bio-ik algorithm can also be used for high-dof system like robot snakes, and it will automatically converge to the best approximate solutions for low-dof arms where some target poses are not reachable exactly.\nwhile you can write the moveit configuration files by hand, the easiest way is to run the moveit setup assistant for your robot, and then to select bio-ik as the ik solver when configuring the end effectors. once configured, the solver can be called using the standard moveit api or used interactively from rviz using the motionplanning gui plugin.\nmake sure that you have a urdf (or xacro) model for your robot.\nrun the moveit setup assistant to create the moveit configuration files:\nrosrun moveit_setup_assistant moveit_setup_assistant\nthe setup assistant automatically searches for all available ik solver plugins in your workspace. therefore, you can just select select bio-ik as the ik solver from the drop-down list for every end effector and then configure the kinematics parameters, namely the default position accuracy (meters) and the timeout (in seconds). for typical 6-dof or 7-dof arms, an accuracy of 0.001 m (or smaller) and a timeout of 1 msec should be ok. more complex robots might need a longer timeout.\ngenerate the moveit configuration files from the setup assistant. of course, you can also edit the config/kinematics.yaml configuration file with your favorite text editor. for example, a configuration for the pr2 robot might look like this:\n# example kinematics.yaml for the pr2 robot\nright_arm:\n# kinematics_solver: kdl_kinematics_plugin/kdlkinematicsplugin\n# kinematics_solver_attempts: 1\nkinematics_solver: bio_ik/bioikkinematicsplugin\nkinematics_solver_search_resolution: 0.005\nkinematics_solver_timeout: 0.005\nkinematics_solver_attempts: 1\nleft_arm:\nkinematics_solver: bio_ik/bioikkinematicsplugin\nkinematics_solver_search_resolution: 0.005\nkinematics_solver_timeout: 0.005\nkinematics_solver_attempts: 1\nall:\nkinematics_solver: bio_ik/bioikkinematicsplugin\nkinematics_solver_search_resolution: 0.005\nkinematics_solver_timeout: 0.02\nkinematics_solver_attempts: 1\n# optional bio-ik configuration parameters\n# center_joints_weight: 1\n# minimal_displacement_weight: 1\n# avoid_joint_limits_weight: 1\nfor a first test, run the moveit-created demo launch. once rviz is running, enable the motion planning plugin, then select one of the end effectors of you robot. rviz should show an 6-d (position and orientation) interactive marker for the selected end-effector(s). move the interactive marker and watch bio-ik calculating poses for your robot.\nif you also installed the bio-ik demo (see below), you should be able to run one of the predefined demos:\nroslaunch pr2_bioik_moveit demo.launch\nroslaunch pr2_bioik_moveit valve.launch\nroslaunch pr2_bioik_moveit dance.launch\nyou are now ready to use bio-ik from your c/c++ and python programs, using the standard moveit api. to explicitly request an ik solution in c++:\nrobot_model_loader::robotmodelloader robot_model_loader(robot);\nauto robot_model = robot_model_loader.getmodel();\nauto joint_model_group = robot_model->getjointmodelgroup(group);\nauto tip_names = joint_model_group->getsolverinstance()->gettipframes();\nkinematics::kinematicsqueryoptions opts;\nopts.return_approximate_solution = true; // optional\nrobot_state::robotstate robot_state_ik(robot_model);\n// traditional \"basic\" bio-ik usage. the end-effector goal poses\n// and end-effector link names are passed into the setfromik()\n// call. the kinematicsqueryoptions are empty.\n//\nbool ok = robot_state_ik.setfromik(\njoint_model_group, // joints to be used for ik\ntip_transforms, // multiple end-effector goal poses\ntip_names, // names of the end-effector links\nattempts, timeout, // solver attempts and timeout\nmoveit::core::groupstatevaliditycallbackfn(),\nopts // mostly empty\n);\nadvanced usage\nfor many robot applications, it is essential to specify more than just a single end-effector pose. typical examples include\nredundancy resolution (e.g. 7-dof arm)\ntwo-arm manipulation tasks on two-arm robots (e.g. baxter)\nmulti end-effector tasks with shared kinematic links\ngrasping and manipulation tasks with multi-finger hands\nfull-body motion on humanoid robots\nreaching tasks with additional constraints (e.g. shoulder position)\nincremental -----> tool !!!  motions without robot arm configuration changes\nand many more\nin bio-ik, such tasks are specified as a combination of multiple individual goals.\nthe algorithm then tries to find a robot configuration that fulfills all given goals simultaneously by minimizing a quadratic error function built from the weighted individual goals. while the current moveit api does not support multiple-goals tasks directly, it provides the kinematicqueryoptions class. therefore, bio-ik simply provides a set of predefined motion goals, and a combination of the user-specified goals is passed via moveit to the ik solver. no api changes are required in moveit, but using the ik solver now consists passing the weighted goals via the kinematicqueryoptions. the predefined goals include:\nposegoal: a full 6-dof robot pose\npositiongoal: a 3-dof (x,y,z) position\norientationgoal: a 3-dof orientation, encoded as a quaternion (qx,qy,qz,qw)\nlookatgoal: a 3-dof (x,y,z) position intended as a looking direction for a camera or robot head\njointgoal: a set of joint angles, e.g. to specify a\nfunctiongoal: an arbitrary function of the robot joint values, e.g. to model underactuated joints or mimic joints\nand several more\nto solve a motion problem on your robot, the trick now is to construct a suitable combination of individual goals.\nin the following example, we want to grasp and then slowly turn a valve wheel with the left and right gripers of the pr2 robot:\nbio_ik::bioikkinematicsqueryoptions ik_options;\nik_options.replace = true;\nik_options.return_approximate_solution = true;\nauto* ll_goal = new bio_ik::posegoal();\nauto* lr_goal = new bio_ik::posegoal();\nauto* rl_goal = new bio_ik::posegoal();\nauto* rr_goal = new bio_ik::posegoal();\nll_goal->setlinkname(\"l_gripper_l_finger_tip_link\");\nlr_goal->setlinkname(\"l_gripper_r_finger_tip_link\");\nrl_goal->setlinkname(\"r_gripper_l_finger_tip_link\");\nrr_goal->setlinkname(\"r_gripper_r_finger_tip_link\");\nik_options.goals.emplace_back(ll_goal);\nik_options.goals.emplace_back(lr_goal);\nik_options.goals.emplace_back(rl_goal);\nik_options.goals.emplace_back(rr_goal);\nwe also set a couple of secondary goals. first, we want that the head of the pr2 looks at the center of the valve. second, we want to avoid joint-limits on all joints, if possible. third, we want that ik solutions are as close as possible to the previous joint configuration, meaning small and efficient motions. this is handled by adding the minimaldisplacementgoal. fourth, we want to avoid torso lift motions, which are very slow on the pr2. all of this is specified easily:\nauto* lookat_goal = new bio_ik::lookatgoal();\nlookat_goal->setlinkname(\"sensor_mount_link\");\nik_options.goals.emplace_back(lookat_goal);\nauto* avoid_joint_limits_goal = new bio_ik::avoidjointlimitsgoal();\nik_options.goals.emplace_back(avoid_joint_limits_goal);\nauto* minimal_displacement_goal = new bio_ik::minimaldisplacementgoal();\nik_options.goals.emplace_back(minimal_displacement_goal);\nauto* torso_goal = new bio_ik::positiongoal();\ntorso_goal->setlinkname(\"torso_lift_link\");\ntorso_goal->setweight(1);\ntorso_goal->setposition(tf::vector3( -0.05, 0, 1.0 ));\nik_options.goals.emplace_back(torso_goal);\nfor the actual turning motion, we calculate a set of required gripper poses in a loop:\nfor(int i = 0; ; i++) {\ntf::vector3 center(0.7, 0, 1);\ndouble t = i * 0.1;\ndouble r = 0.1;\ndouble a = sin(t) * 1;\ndouble dx = fmin(0.0, cos(t) * -0.1);\ndouble dy = cos(a) * r;\ndouble dz = sin(a) * r;\ntf::vector3 dl(dx, +dy, +dz);\ntf::vector3 dr(dx, -dy, -dz);\ntf::vector3 dg = tf::vector3(0, cos(a), sin(a)) * (0.025 + fmin(0.025, fmax(0.0, cos(t) * 0.1)));\nll_goal->setposition(center + dl + dg);\nlr_goal->setposition(center + dl - dg);\nrl_goal->setposition(center + dr + dg);\nrr_goal->setposition(center + dr - dg);\ndouble ro = 0;\nll_goal->setorientation(tf::quaternion(tf::vector3(1, 0, 0), a + ro));\nlr_goal->setorientation(tf::quaternion(tf::vector3(1, 0, 0), a + ro));\nrl_goal->setorientation(tf::quaternion(tf::vector3(1, 0, 0), a + ro));\nrr_goal->setorientation(tf::quaternion(tf::vector3(1, 0, 0), a + ro));\nlookat_goal->setaxis(tf::vector3(1, 0, 0));\nlookat_goal->settarget(rr_goal->getposition());\n// \"advanced\" bio-ik usage. the call parameters for the end-effector\n// poses and end-effector link names are left empty; instead the\n// requested goals and weights are passed via the ik_options object.\n//\nrobot_state.setfromik(\njoint_model_group, // active pr2 joints\neigenstl::vector_affine3d(), // no explicit poses here\nstd::vector<std::string>(), // no end effector links here\n0, 0.0, // take values from yaml file\nmoveit::core::groupstatevaliditycallbackfn(),\nik_options // four gripper goals and secondary goals\n);\n... // check solution validity and actually move the robot\n}\nwhen you execute the code, the pr2 will reach for the valve wheel and turn it. every once in a while it can't reach the valve with its current arm configuration and will regrasp the wheel.\nsee [3] and [4] for more examples.\nlocal vs. global optimization, redundancy resolution, cartesian jogging\nbioik has been developed to efficiently find good solutions for non-convex inverse kinematics problems with multiple goals and local minima. however, for some applications, this can lead to unintuitive results. if there are multiple possible solutions to a given ik problem, and if the user has not explicitly specified which one to choose, a result may be selected randomly from the set of all valid solutions. when incrementally tracking a cartesian path, this can result in unwanted jumps, shaking, etc. to incrementally generate a smooth trajectory using bioik, the desired behaviour should be specified explicitly, which can be done in two ways.\ndisabling global optimization\nbioik offers a number of different solvers, including global optimizers and local optimizers. by default, bioik uses a memetic global optimizer (bio2_memetic). a different solver class can be selected by setting the mode parameter in the kinematics.yaml file of your moveit robot configuration.\nexample:\nall:\nkinematics_solver: bio_ik/bioikkinematicsplugin\nkinematics_solver_search_resolution: 0.005\nkinematics_solver_timeout: 0.02\nkinematics_solver_attempts: 1\nmode: gd_c\ncurrently available local optimizers:\ngd, gd_2, gd_4, gd_8\ngd_r, gd_r_2, gd_r_4, gd_r_8\ngd_c, gd_c_2, gd_c_4, gd_c_8\njac, jac_2, jac_4, jac_8\nnaming convention: <solver type>_[<variant>_]<number of threads>\nnotes:\nthe gd_* solvers support arbitrary goal types.\nthe jac_* solvers only support pose goals but might in theory be more stable in some cases.\nrelative performance depends on the application (it's probably best if you try it yourself for your particular robot and problem types).\nduring our tests, the gd_c variants usually outperformed the other local solvers.\nfor incremental tracking, a single-threaded variant without restarts is probably best suited (gd_c, gd, or jac).\nyou can also create different moveit move groups with different solver types. if you now want to plan a cartesian trajectory from end effector pose a to end effector pose b, you can use a move group with a global optimizer to find a matching initial pose for end effector pose a and a different move group with a local optimizer for incrementally generating a smooth cartesian trajectory from a to b.\nregularization\nyou can force a global optimizer to return a local minimum through regularization.\nfor the specific case of incremental robot motions (aka jogging), the simplest solution is to specify both a posegoal and the special regularizationgoal, which tries to keep the joint-space ik solution as close as possible to the given robot seed configuration. typically you would use a high weight for the posegoal and a smaller weight to the regularizer.\nyou can also add a minimaldisplacementgoal instead of the regularizationgoal. both goals try to keep the ik solution close to the current/seed robot state, but differ slightly in the handling of fast and slow robot joints (e.g. the pr2 has fast arm joints and a rather slow torso lift joint). you might want to play with both goals to see which one better matches your needs.\nsome industrial robot controllers on 7-dof arms behave as if working on a 6-dof arm with one extra joint. typically, the value of the extra joint can be specified, and an ik solution is then searched for the remaining six joints. this behaviour can be achieved in bio-ik by combining a posegoal for the end-effector with a jointpositiongoal for one (any one) of the robot joints.\nanother useful trick is trying to keep the robot joints centered, as this will allow robot (joint) motions in both directions. just combine the posegoal with a centerjointsgoal, and optionally also a regularizationgaol.\nyou can also combine regularization with a local gd_* solver.\nhow it works\nthe bio-ik solver is based on a memetic algorithm that combines gradient-based optimization with genetic and particle swarm optimization.\ninternally, vectors of all robot joint values are used to encode different intermediate solutions (the genotype of the genetic algorithm). during the optimization, joint values are always checked against the active lower and upper joint limits, so that only valid robot configurations are generated.\nto calculate the fitness of individuals, the cumulative error over all given individual goals is calculated. any individual with zero error is an exact solution for the ik problem, while individuals with small error correspond to approximate solutions.\nindividuals are sorted by their fitness, and gradient-based optimization is tried on the best few configuration, resulting in fast convergence and good performance for many problems. if no solution is found from the gradient-based optimization, new individuals are created by a set of mutation and recombination operators, resulting in good search-space exploration.\nsee [3] and [4] for more details. see [5] and [6] for an in-depth explanation of an earlier evolutionary algorithm for animating video game characters.\nrunning the self-tests\nwe have tested bio-ik on many different robot arms, both using the tranditional single end-effector api and the advanced multi end-effector api based on the kinematicsqueryoptions.\none simple selftest consists of generating random valid robot configurations, running forward kinematics to calculate the resulting end-effector pose, and the querying the ik plugin to find a suitable robot joint configuration. success is then checked by running forrward kinematics again and checking that the end-effector pose for the generated ik solution matches the target pose. this approach can be run easily for thousands or millions of random poses, samples the full workspace of the robot, and allows to quickly generate success-rate and solution-time estimates for the selected ik solver.\nof course, running the tests requires installing the corresponding robot models and adds a lot of dependencies. therefore, those tests are not included in the standard bio-ik package, but are packaged separately.\nfor convenience, we provide the pr2_bioik_moveit package, which also includes a few bio-ik demos for the pr2 service robot. these are kinematics only demos; but of course you can also try running the demos on the real robot (if you have one) or the gazebo simulator (if you installed gazebo).\nsimply clone the pr2 description package (inside pr2_common) and the pr2_bioik_moveit package into your catkin workspace:\nroscd\ncd src\ngit clone https://github.com/pr2/pr2_common.git\ngit clone https://github.com/tams-group/bioik_pr2.git\ncatkin_make\nfor the fk-ik-fk performance test, please run\nroslaunch pr2_bioik_moveit env_pr2.launch\nroslaunch pr2_bioik_moveit test_fk_ik.launch\n... // wait for test completion and results summary\nreferences\norocos kinematics and dynamics, http://www.orocos.org\np. beeson and b. ames, trac-ik: an open-source library for improved solving of generic inverse kinematics, proceedings of the ieee ras humanoids conference, seoul, korea, november 2015.\nphilipp ruppel, norman hendrich, sebastian starke, jianwei zhang, cost functions to specify full-body motion and multi-goal manipulation tasks, ieee international conference on robotics and automation, (icra-2018), brisbane, australia. doi: 10.1109/icra.2018.8460799\nphilipp ruppel, performance optimization and implementation of evolutionary inverse kinematics in ros, msc thesis, university of hamburg, 2017 pdf\nsebastian starke, norman hendrich, jianwei zhang, a memetic evolutionary algorithm for real-time articulated kinematic motion, ieee intl. congress on evolutionary computation (cec-2017), p.2437-2479, june 4-8, 2017, san sebastian, spain. doi: 10.1109/cec.2017.7969605\nsebastian starke, norman hendrich, dennis krupke, jianwei zhang, multi-objective evolutionary optimisation for inverse kinematics on highly articulated and humanoid robots, ieee intl. conference on intelligent robots and systems (iros-2017), september 24-28, 2017, vancouver, canada\nlinks\nhttp://doi.org/10.1109/icra.2018.8460799\nhttps://github.com/tams-group/bio_ik\nhttps://tams-group.github.io/bio_ik/", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000352, "year": null}, {"Unnamed: 0": 1377, "autor": 357, "date": null, "content": "Mech is a language for developing data-driven, reactive systems like animations, games, and robots. It makes composing, transforming, and distributing data easy, allowing you to focus on the essential complexity of your project.\nYou can try Mech online at try.mech-lang.org.\nUsage and installation instructions can be found in the documentation or the main Mech repository.\nRead about progress on our blog, follow us on Twitter @MechLang, get live help on our Gitter channel, or join the mailing list.\nWelcome\nThis repository serves as a table of contents for the constellation of tools and utilities that comprise the Mech programming language:\nCore - The language runtime. It's a small dataflow engine that accepts transactions of changes, and applies them to a compute network.\nSyntax - A compiler for a textual Mech syntax.\nProgram - Coordinates distributed Mech cores as a coherent program.\nDocumentation\nRight now, most Mech features and syntax are undocumented. You can find some minimal documentation here, and also the beginning of a tutorial here.\nInstallation\nFrom Binary\nYou can download the the latest release for your platform here. Or, if you have Rust's Cargo tool installed you can use the following command:\n> cargo install mech\nFrom Source\nYou will need to install Rust (Mech only works on the \"Nightly\" release channel) before building Mech. When those are installed, follow these instructions:\n> git clone https://gitlab.com/mech-lang/mech\n> cd mech\n> cargo build --bin mech --release\nProject Status\nMech is currently in the v0.0.5 alpha stage of development. This means that while some features work and are tested, programs are still likely to crash and produce incorrect results. We've implemented some language features, but many are not yet implemented.\nFeel free to use the language for your own satisfaction, but please don't use it for anything important.\nLicense\nApache 2.0", "link": "https://github.com/mech-lang/mech", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "mech is a language for developing data-driven, reactive systems like animations, games, and robots. it makes composing, transforming, and distributing data easy, allowing you to focus on the essential complexity of your project.\nyou can try mech online at try.mech-lang.org.\nusage and installation instructions can be found in the documentation or the main mech repository.\nread about progress on our blog, follow us on twitter @mechlang, get live help on our gitter channel, or join the mailing list.\nwelcome\nthis repository serves as a table of contents for the constellation of tools and utilities that comprise the mech programming language:\ncore - the language runtime. it's a small dataflow engine that accepts transactions of changes, and applies them to a compute network.\nsyntax - a compiler for a textual mech syntax.\nprogram - coordinates distributed mech cores as a coherent program.\ndocumentation\nright now, most mech features and syntax are undocumented. you can find some minimal documentation here, and also the beginning of a tutorial here.\ninstallation\nfrom binary\nyou can download the the latest release for your platform here. or, if you have rust's cargo -----> tool !!!  installed you can use the following command:\n> cargo install mech\nfrom source\nyou will need to install rust (mech only works on the \"nightly\" release channel) before building mech. when those are installed, follow these instructions:\n> git clone https://gitlab.com/mech-lang/mech\n> cd mech\n> cargo build --bin mech --release\nproject status\nmech is currently in the v0.0.5 alpha stage of development. this means that while some features work and are tested, programs are still likely to crash and produce incorrect results. we've implemented some language features, but many are not yet implemented.\nfeel free to use the language for your own satisfaction, but please don't use it for anything important.\nlicense\napache 2.0", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000357, "year": null}, {"Unnamed: 0": 1378, "autor": 358, "date": null, "content": "For simplified Chinese version: \u7b80\u4f53\u4e2d\u6587\u7248\nImportant Notice:\nAfter using xArm C++ SDK as sub-module, the use of /xarm/set_tool_modbus service has been modified, compared with old version, the redundant '0x09' byte in response data has been removed\uff01\nDue to robot communication data format change, early users (xArm shipped before June 2019) are encouraged to upgrade their controller firmware immediately to drive the robot normally in future updates as well as to use newly developed functions. Please contact our staff to get instructions of the upgrade process. The old version robot driver can still be available in 'legacy' branch, however, it will not be updated any more.\nYou MUST follow chapter 3 to install additional packages needed before any usage of xarm_ros packages. Otherwise, unexpected errors may occur.\nIf developing with Moveit, it is highly recommended to use DIRECT network cable connection between controller box and your PC, and no intermediate switches or routers, or the communication latency may have a bad impact on trajectory execution.\nWhen updating this package, please remember to check the submodule update as well!\nContents:\n1. Introduction\n2. Update History\n3. Preparations (MUST DO)\n4. Get Started\n5. Package Description & Usage Guidance\n5.1 xarm_description\n5.2 xarm_gazebo\n5.3 xarm_controller\n5.4 xarm_bringup\n5.5 xarm7_moveit_config\n5.5.1 Add Custom Tool Model For Moveit\n5.6 xarm_planner\n5.7 xarm_api/xarm_msgs\n5.7.1 Starting xArm by ROS service (priority for the following operations)\n5.7.2 Joint space or Cartesian space command example(Velocity Control Added)\n5.7.3 Tool/Controller I/O Operations\n5.7.4 Getting status feedback\n5.7.5 Setting Tool Center Point Offset\n5.7.6 Clearing Errors\n5.7.7 Gripper Control\n5.7.8 Vacuum Gripper Control\n5.7.9 Tool Modbus communication\n6. Mode Change\n6.1 Mode Explanation\n6.2 Proper way to change modes\n7. xArm Vision (NEW)\n7.1 Installation of dependent packages\n7.2 Hand-eye Calibration Demo\n7.3 Vision Guided Grasping Demo\n7.4 Adding RealSense D435i model to simulated xArm\n8. Other Examples\n8.1 Multi-xArm5 (separate control)\n8.2 Servo_Cartesian\n8.3 Servo_Joint\n8.4 Dual xArm6 controlled with one moveGroup node\n8.5 An example of demonstrating redundancy resolution using MoveIt\n1. Introduction\nThis repository contains the 3D models of xArm series and demo packages for ROS development and simulations.Developing and testing environment: Ubuntu 16.04 + ROS Kinetic/Melodic.\nInstructions below is based on xArm7, other model user can replace 'xarm7' with 'xarm6' or 'xarm5' where applicable.\n2. Update Summary\nThis package is still under development and improvement, tests, bug fixes and new functions are to be updated regularly in the future.\nAdd xArm 7 description files, meshes and sample controller demos for ROS simulation and visualization.\nAdd Moveit! planner support to control Gazebo virtual model and real xArm, but the two can not launch together.\nAdd Direct control of real xArm through Moveit GUI, please use it with special care.\nAdd xArm hardware interface to use ROS position_controllers/JointTrajectoryController on real robot.\nAdd xArm 6 and xArm 5 simulation/real robot control support.\nAdd simulation model of xArm Gripper.\nAdd demo to control dual xArm6 through Moveit.\nAdd xArm Gripper action control.\nAdd xArm-with-gripper Moveit development packages.\nAdd vacuum gripper model and xArm-with-vacuum-gripper Moveit development packages (under /examples dir).\nThanks to Microsoft IoT, xarm_ros can now be compiled and run on Windows platform.\nAdd velocity control mode for joint and Cartesian space. (xArm controller firmware version >= 1.6.8 required)\nAdd support for custom tool model for Moveit\nAdd timed-out version of velocity control mode, for better safety consideration. (xArm controller firmware version >= 1.8.0 required)\nAdd xArm Vision and RealSense D435i related demo. Migrate previous \"xarm_device\" into xarm_vision/camera_demo.\n3. Preparations before using this package\n3.1 Install dependent package module\ngazebo_ros_pkgs: http://gazebosim.org/tutorials?tut=ros_installing (if use Gazebo)\nros_control: http://wiki.ros.org/ros_control (remember to select your correct ROS distribution)\nmoveit_core: https://moveit.ros.org/install/\n3.2 Go through the official tutorial documents\nROS Wiki: http://wiki.ros.org/\nGazebo Tutorial: http://gazebosim.org/tutorials\nGazebo ROS Control: http://gazebosim.org/tutorials/?tut=ros_control\nMoveit tutorial: http://docs.ros.org/kinetic/api/moveit_tutorials/html/\n3.3 Download the 'table' 3D model\nIn Gazebo simulator, navigate through the model database for 'table' item, drag and place the 3D model inside the virtual environment. It will then be downloaded locally, as 'table' is needed for running the demo.\n3.4 Install \"mimic_joint_plugin\" for xArm Gripper simulation in Gazebo\nIf simulating xArm Gripper in Gazebo is needed, mimic_joint_plugin by courtesy of Konstantinos Chatzilygeroudis (@costashatz) needs to be installed in order to make the mimic joints behave normally in Gazebo. Usage of this plugin is inspired by this tutorial from @mintar.\n12/22/2020: Refer to issue #53, Please Note this plugin has recently been deprecated, if you plan to use new version, please change \"libroboticsgroup_gazebo_mimic_joint_plugin.so\" to \"libroboticsgroup_upatras_gazebo_mimic_joint_plugin.so\" in file: xarm_ros/xarm_gripper/urdf/xarm_gripper.gazebo.xacro\n4. Getting started with 'xarm_ros'\n4.1 Create a catkin workspace.\nIf you already have a workspace, skip and move on to next part. Follow the instructions in this page. Please note that this readme instruction assumes the user continues to use '~/catkin_ws' as directory of the workspace.\n4.2 Obtain the package\n$ cd ~/catkin_ws/src\n$ git clone https://github.com/xArm-Developer/xarm_ros.git --recursive\n4.2.1 update the package\n$ cd ~/catkin_ws/src/xarm_ros\n$ git pull\n$ git submodule sync\n$ git submodule update --init --remote\n4.3 Install other dependent packages:\n$ rosdep update\n$ rosdep check --from-paths . --ignore-src --rosdistro kinetic\nPlease change 'kinetic' to the ROS distribution you use. If there are any missing dependencies listed. Run the following command to install:\n$ rosdep install --from-paths . --ignore-src --rosdistro kinetic -y\nAnd chane 'kinetic' to the ROS distribution you use.\n4.4 Build the code\n$ cd ~/catkin_ws\n$ catkin_make\n4.5 Source the setup script\n$ echo \"source ~/catkin_ws/devel/setup.bash\" >> ~/.bashrc\nSkip above operation if you already have that inside your ~/.bashrc. Then do:\n$ source ~/.bashrc\n4.6 First try out in RViz:\n$ roslaunch xarm_description xarm7_rviz_display.launch\n4.7 Run the demo in Gazebo simulator\n$ roslaunch xarm_gazebo xarm7_beside_table.launch [run_demo:=true] [add_gripper:=true] [add_vacuum_gripper:=true]\nAdd the \"run_demo\" option if you wish to see a pre-programed loop motion in action. The command trajectory is written in xarm_controller\\src\\sample_motion.cpp. And the trajectory in this demo is controlled by pure position interface.\nAdd the \"add_gripper\" option if you want to see the xArm Gripper attached at the tool end.\nAdd the \"add_vacuum_gripper\" option if you want to see the xArm Vacuum Gripper attached at the tool end. Please note ONLY ONE end effector can be attached.\n5. Package description & Usage Guidance\n5.1 xarm_description\nxArm description files, mesh files and gazebo plugin configurations, etc. It's not recommended to change the xarm description file since other packages depend on it.\n5.2 xarm_gazebo\nGazebo world description files and simulation launch files. User can add or build their own models in the simulation world file.\n5.3 xarm_controller\nController configurations, hardware_interface, robot command executable source, scripts and launch files. User can deploy their program inside this package or create their own. Note that effort controllers defined in xarm_controller/config are just examples for simulation purpose, when controlling the real arm, only 'position_controllers/JointTrajectoryController' interface is provided. User can add their self-defined controllers as well, refer to: http://wiki.ros.org/ros_control (controllers)\n5.4 xarm_bringup\nlaunch files to load xarm driver to enable direct control of real xArm hardware.\n5.5 xarm7_moveit_config\nPlease note: xarm_moveit_config related packages will limit all joints within [-pi, pi], it seems that moveit tend to generate plans involving greater joint motions if not limited within this range. This limit can be canceled by setting \"limited:=false\" in ...moveit_config/launch/planning_context.launch.\nThis package is partially generated by moveit_setup_assistant, could use with Moveit Planner and Rviz visualization. If you have Moveit! installed, you can try the demo.\n$ roslaunch xarm7_moveit_config demo.launch\nTo run Moveit! motion planner along with Gazebo simulator:\nIf no xArm gripper needed, first run:\n$ roslaunch xarm_gazebo xarm7_beside_table.launch\nThen in another terminal:\n$ roslaunch xarm7_moveit_config xarm7_moveit_gazebo.launch\nIf xArm gripper needs to be attached, first run:\n$ roslaunch xarm_gazebo xarm7_beside_table.launch add_gripper:=true\nThen in another terminal:\n$ roslaunch xarm7_gripper_moveit_config xarm7_gripper_moveit_gazebo.launch\nIf you have a satisfied motion planned in Moveit!, hit the \"Execute\" button and the virtual arm in Gazebo will execute the trajectory.\nIf xArm vacuum gripper needs to be attached, just replace \"gripper\" with \"vacuum_gripper\" in above gripper example.\nTo run Moveit! motion planner to control the real xArm:\nFirst make sure the xArm and the controller box are powered on, then execute:\n$ roslaunch xarm7_moveit_config realMove_exec.launch robot_ip:=<your controller box LAN IP address>\nExamine the terminal output and see if any error occured during the launch. If not, just play with the robot in Rviz and you can execute the sucessfully planned trajectory on real arm. But be sure it will not hit any surroundings before execution!\nTo run Moveit! motion planner to control the real xArm with xArm Gripper attached:\nFirst make sure the xArm and the controller box are powered on, then execute:\n$ roslaunch xarm7_gripper_moveit_config realMove_exec.launch robot_ip:=<your controller box LAN IP address>\nIt is better to use this package with real xArm gripper, since Moveit planner will take the gripper into account for collision detection.\nTo run Moveit! motion planner to control the real xArm with xArm Vacuum Gripper attached:\nFirst make sure the xArm and the controller box are powered on, then execute:\n$ roslaunch xarm7_vacuum_gripper_moveit_config realMove_exec.launch robot_ip:=<your controller box LAN IP address>\nIt is better to use this package with real xArm vacuum gripper, since Moveit planner will take the vacuum gripper into account for collision detection.\n5.5.1 Add custom tool model for Moveit\nThis part may require ROS Melodic or later versions to function well\nFor xarm5_moveit_config/xarm6_moveit_config/xarm7_moveit_config, customized tool models maybe added to the tool flange through quick-configuration parameters listed below\uff0cthus to enable Tool offset and 3D collision checking during Moveit motion planning. (Notice\uff1aconfiguration through '/xarm/set_tcp_offset' service will not be effective in Moveit planning!)\nExamples:\n# attaching box model:\n$ roslaunch xarm7_moveit_config demo.launch add_other_geometry:=true geometry_type:=box\n# attaching cylinder model:\n$ roslaunch xarm7_moveit_config demo.launch add_other_geometry:=true geometry_type:=cylinder\n# attaching sphere model:\n$ roslaunch xarm7_moveit_config demo.launch add_other_geometry:=true geometry_type:=sphere\n# attaching customized mesh model:\uff08Here take xarm vacuum_gripper as an example\uff0cif the mesh model could be placed in: 'xarm_description/meshes/other'directory\uff0c'geometry_mesh_filename' argument can be simplified to be just the filename\uff09\n$ roslaunch xarm7_moveit_config demo.launch add_other_geometry:=true geometry_type:=mesh geometry_mesh_filename:=package://xarm_description/meshes/vacuum_gripper/visual/vacuum_gripper.STL geometry_mesh_tcp_xyz:='\"0 0 0.126\"'\nArgument explanations:\nadd_other_geometry: default to be false\uff0cindicating whether to add other geometry model to the tool.\ngeometry_type: geometry shapes to be added\uff0cas one of 'box/cylinder/sphere/mesh', there are different parameters required for different types.\ngeometry_height: height of geometry shape\uff0cunit: meter\uff0cdefault value: 0.1\uff0ceffective for geometry_type: box/cylinder/sphere.\ngeometry_radius: radius of geometry shape\uff0cunit: meter\uff0cdefault value: 0.1\uff0ceffective for geometry_type: cylinder/sphere.\ngeometry_length: length of geometry shape\uff0cunit: meter\uff0cdefault value: 0.1\uff0ceffective for geometry_type: box.\ngeometry_width: width of geometry shape\uff0cunit: meter\uff0cdefault value: 0.1\uff0ceffective for geometry_type: box.\ngeometry_mesh_filename: geometry shape\uff0ceffective for geometry_type: mesh.\ngeometry_mesh_origin_xyz: position offset from mesh base coordinate to xarm tool-flange coordinate, default: \"0 0 0\"\uff0ceffective for geometry_type: mesh.\ngeometry_mesh_origin_rpy: orientation offset from mesh base coordinate to xarm tool-flange coordinate, default: \"0 0 0\"\uff0ceffective for geometry_type: mesh.\ngeometry_mesh_tcp_xyz: the positional TCP offset with respect to xarm tool-flange coordinate, default: \"0 0 0\"\uff0ceffective for geometry_type: mesh.\ngeometry_mesh_tcp_rpy: the orientational TCP offset with respect to xarm tool-flange coordinate, default: \"0 0 0\"\uff0ceffective for geometry_type: mesh.\n5.6 xarm_planner:\nThis implemented simple planner interface is based on move_group from Moveit! and provide ros service for users to do planning & execution based on the requested target, user can find detailed instructions on how to use it inside xarm_planner package.\nTo launch the xarm simple motion planner together with the real xArm:\n$ roslaunch xarm_planner xarm_planner_realHW.launch robot_ip:=<your controller box LAN IP address> robot_dof:=<7|6|5> add_(vacuum_)gripper:=<true|false>\nArgument 'robot_dof' specifies the number of joints of your xArm (default is 7). Now xarm_planner supports model with gripper or vacuum_gripper attached. Please specify \"add_gripper\" or \"add_vacuum_gripper\" argument if needed.\n5.7 xarm_api/xarm_msgs:\nThese two packages provide user with the ros service wrapper of the functions in xArm SDK. There are 12 types of motion command (service names) supported\uff0cplease set correct robot mode first, refer to mode change section:\nRobot Mode 0:\nmove_joint: joint space point to point command, given target joint angles, max joint velocity and acceleration. Corresponding function in SDK is \"set_servo_angle()\".\nmove_line: straight-line motion to the specified Cartesian Tool Centre Point(TCP) target. Corresponding function in SDK is \"set_position()\"[blending radius not specified].\nmove_lineb: straight-line motion, and blending continuously with next motion. Normally works in the form of a list of known via points followed by target Cartesian point. Each motion segment is straight-line with Arc blending at the via points, to make velocity continuous. Corresponding function in SDK is \"set_position()\"[wait=false and blending radius specified]. Please refer to move_test.cpp and blended_motion_test.py for example code, /xarm/wait_for_finish parameter has to be false for successful blending calculation.\nmove_jointb: joint space point to point motion, and blending continuously with next motion. It can be used together with \"move_lineb\" for joint-linear blending motions, as long as the via points are known, and blending radius is properly specified, velocity will be continuous during the execution. Corresponding function in SDK is \"set_servo_angle()\"[wait=false and blending radius specified]. Please refer to blended_motion_test.py for example code. /xarm/wait_for_finish parameter has to be false for successful blending calculation.\nmove_line_tool: straight-line motion based on the Tool coordinate system rather than the base system. Corresponding function in SDK is \"set_tool_position()\".\nPlease keep in mind that before calling the 4 motion services above, first set robot mode to be 0, then set robot state to be 0, by calling relavent services. Meaning of the commands are consistent with the descriptions in product user manual, other xarm API supported functions are also available as service call. Refer to xarm_msgs package for more details and usage guidance.\nmove_line_aa: straight-line motion, with orientation expressed in Axis-angle rather than roll-pitch-yaw angles. Please refer to xArm user manual for detailed explanation of axis-angle before using this command.\nRobot Mode 1:\nmove_servo_cart/move_servoj: streamed high-frequency trajectory command execution in Cartesian space or joint space. Corresponding functions in SDK are set_servo_cartesian() and set_servo_angle_j(). An alternative way to implement velocity control. Special RISK ASSESMENT is required before using them. Please read the guidance carefully at chapter 7.2-7.3.\nRobot Mode 4:\nvelo_move_joint/velo_move_joint_timed: Joint motion with specified velocity for each joint (unit: rad/s), with maximum joint acceleration configurable by set_max_acc_joint service.\nRobot Mode 5:\nvelo_move_line/velo_move_line_timed: Linear motion of TCP with specified velocity in mm/s (position) and rad/s (orientation in axis-angular_velocity), with maximum linear acceleration configurable by set_max_acc_line service.\nStarting xArm by ROS service:\nFirst startup the service server for xarm7, ip address is just an example:\n$ roslaunch xarm_bringup xarm7_server.launch robot_ip:=192.168.1.128\nThen make sure all the servo motors are enabled, refer to SetAxis.srv:\n$ rosservice call /xarm/motion_ctrl 8 1\nBefore any motion commands, set proper robot mode(0: POSE) and state(0: READY) in order, refer to SetInt16.srv:\n$ rosservice call /xarm/set_mode 0\n$ rosservice call /xarm/set_state 0\nJoint space or Cartesian space command example:\nPlease note that all the angles must use the unit of radian. All motion commands use the same type of srv request: Move.srv.\n1. Joint space motion:\nTo call joint space motion with max speed 0.35 rad/s and acceleration 7 rad/s^2:\n$ rosservice call /xarm/move_joint [0,0,0,0,0,0,0] 0.35 7 0 0\nTo go back to home (all joints at 0 rad) position with max speed 0.35 rad/s and acceleration 7 rad/s^2:\n$ rosservice call /xarm/go_home [] 0.35 7 0 0\n2. Cartesian space motion in Base coordinate:\nTo call Cartesian motion to the target expressed in robot BASE Coordinate, with max speed 200 mm/s and acceleration 2000 mm/s^2:\n$ rosservice call /xarm/move_line [250,100,300,3.14,0,0] 200 2000 0 0\n3. Cartesian space motion in Tool coordinate:\nTo call Cartesian motion expressed in robot TOOL Coordinate, with max speed 200 mm/s and acceleration 2000 mm/s^2, the following will move a relative motion (delta_x=50mm, delta_y=100mm, delta_z=100mm) along the current Tool coordinate, no orientation change:\n$ rosservice call /xarm/move_line_tool [50,100,100,0,0,0] 200 2000 0 0\n4. Cartesian space motion in Axis-angle orientation:\nCorresponding service for Axis-angle motion is MoveAxisAngle.srv. Please pay attention to the last two arguments: \"coord\" is 0 for motion with respect to (w.r.t.) Arm base coordinate system, and 1 for motion w.r.t. Tool coordinate system. \"relative\" is 0 for absolute target position w.r.t. specified coordinate system, and 1 for relative target position.\nFor example: to move 1.0 radian relatively around tool-frame Z-axis:\n$ rosservice call /xarm/move_line_aa \"pose: [0, 0, 0, 0, 0, 1.0]\nmvvelo: 30.0\nmvacc: 100.0\nmvtime: 0.0\ncoord: 1\nrelative: 1\"\nret: 0\nmessage: \"move_line_aa, ret = 0\"\nOr\n$ rosservice call /xarm/move_line_aa [0,0,0,0,0,1.0] 30.0 100.0 0.0 1 1\n\"mvtime\" is not meaningful in this command, just set it to 0. Another example: in base-frame, to move 122mm relatively along Y-axis, and rotate around X-axis for -0.5 radians:\n$ rosservice call /xarm/move_line_aa [0,122,0,-0.5,0,0] 30.0 100.0 0.0 0 1\n5. Joint velocity control:\n(xArm controller firmware version >= 1.6.8 required) If controlling joint velocity is desired, first switch to Mode 4 as descriped in mode change section. Please check the MoveVelo.srv first to understand the meanings of parameters reqired. If more than one joint are to move, set jnt_sync to 1 for synchronized acceleration/deceleration for all joints in motion, and if jnt_sync is 0, each joint will reach to its target velocity as fast as possible. coord parameter is not used here, just set it to 0. For example:\n# NO Timed-out version (will not stop until all-zero velocity command received!):\n$ rosservice call /xarm/velo_move_joint [0.1,-0.1,0,0,0,-0.3] 1 0\n# With Timed-out version(controller firmware version >= 1.8.0): (if next velocity command not received within 0.2 seconds, xArm will stop)\n$ rosservice call /xarm/velo_move_joint_timed [0.1,-0.1,0,0,0,-0.3] 1 0 0.2\nwill command the joints (for xArm6) to move in specified angular velocities (in rad/s) and they will reach to target velocities synchronously. The maximum joint acceleration can also be configured by (unit: rad/s^2):\n$ rosservice call /xarm/set_max_acc_joint 10.0 (maximum: 20.0 rad/s^2)\n6. Cartesian velocity control:\n(xArm controller firmware version >= 1.6.8 required) If controlling linar velocity of TCP towards certain direction is desired, first switch to Mode 5 as descriped in mode change section. Please check the MoveVelo.srv first to understand the meanings of parameters reqired. Set coord to 0 for motion in world/base coordinate system and 1 for tool coordinate system. jnt_sync parameter is not used here, just set it to 0. For example:\n# NO Timed-out version (will not stop until all-zero velocity command received!):\n$ rosservice call /xarm/velo_move_line [30,0,0,0,0,0] 0 1\n# With Timed-out version(controller firmware version >= 1.8.0): (if next velocity command not received within 0.2 seconds, xArm will stop)\n$ rosservice call /xarm/velo_move_line_timed [30,0,0,0,0,0] 0 1 0.2\nwill command xArm TCP move along X-axis of TOOL coordinate system with speed of 30 mm/s. The maximum linear acceleration can also be configured by (unit: mm/s^2):\n$ rosservice call /xarm/set_max_acc_line 5000.0 (maximum: 50000 mm/s^2)\nFor angular motion in orientation, please note the velocity is specified as axis-angular_velocity elements. That is, [the unit rotation axis vector] multiplied by [rotation velocity value(scalar)]. For example,\n# NO Timed-out version (will not stop until all-zero velocity command received!):\n$ rosservice call /xarm/velo_move_line [0,0,0,0.707,0,0] 0 0\n# With Timed-out version(controller firmware version >= 1.8.0): (if next velocity command not received within 0.2 seconds, xArm will stop)\n$ rosservice call /xarm/velo_move_line_timed [0,0,0,0.707,0,0] 0 0 0.2\nThis will command TCP to rotate along X-axis in BASE coordinates at about 45 degrees/sec. The maximum acceleration for orientation change is fixed.\nPlease Note: For no Timed-out version services: velocity motion can be stopped by either giving all 0 velocity command, or setting state to 4(STOP) and 0(READY) later for next motion. However, timed-out versions are more recommended for use, since it can be safe if network comminication or user program fails, controller firmware needs to be updated to v1.8.0 or later.\nMotion service Return:\nPlease Note the above motion services will return immediately by default. If you wish to return until actual motion is finished, set the ros parameter \"/xarm/wait_for_finish\" to be true in advance. That is:\n$ rosparam set /xarm/wait_for_finish true\nUpon success, 0 will be returned. If any error occurs, 1 will be returned.\nTool I/O Operations:\nWe provide 2 digital, 2 analog input port and 2 digital output signals at the end I/O connector.\n1. To get current 2 DIGITAL input states:\n$ rosservice call /xarm/get_digital_in\n2. To get one of the ANALOG input value:\n$ rosservice call /xarm/get_analog_in 1 (last argument: port number, can only be 1 or 2)\n3. To set one of the Digital output:\n$ rosservice call /xarm/set_digital_out 2 1 (Setting output 2 to be 1)\nYou have to make sure the operation is successful by checking responding \"ret\" to be 0.\nController I/O Operations:\nWe provide 8/16 digital input and 8/16 digital output ports at controller box for general usage.\n1. To get one of the controller DIGITAL input state:\n$ rosservice call /xarm/get_controller_din io_num (Notice: from 1 to 8, for CI0~CI7; from 9 to 16, for DI0~DI7[if any])\n2. To set one of the controller DIGITAL output:\n$ rosservice call /xarm/set_controller_dout io_num (Notice: from 1 to 8, for CO0~CO7; from 9 to 16, for DI0~DI7[if any]) logic (0 or 1)\nFor example:\n$ rosservice call /xarm/set_controller_dout 5 1 (Setting output 5 [lable C04] to be 1)\n3. To get one of the controller ANALOG input:\n$ rosservice call /xarm/get_controller_ain port_num (Notice: from 1 to 2, for AI0~AI1)\n4. To set one of the controller ANALOG output:\n$ rosservice call /xarm/set_controller_aout port_num (Notice: from 1 to 2, for AO0~AO1) analog_value\nFor example:\n$ rosservice call /xarm/set_controller_aout 2 3.3 (Setting port AO1 to be 3.3)\nYou have to make sure the operation is successful by checking responding \"ret\" to be 0.\nGetting status feedback:\nHaving connected with a real xArm robot by running 'xarm7_server.launch', user can subscribe to the topic \"xarm/xarm_states\" for feedback information about current robot states, including joint angles, TCP position, error/warning code, etc. Refer to RobotMsg.msg for content details.\nAnother option is subscribing to \"/joint_states\" topic, which is reporting in JointState.msg, however, currently only \"position\" field is valid; \"velocity\" is non-filtered numerical differentiation based on 2 adjacent position data, and \"effort\" feedback are current-based estimated values, not from direct torque sensor, so they are just for reference. In consideration of performance, current update rate of above two topics are set at 5Hz.\nSetting Tool Center Point Offset(only effective for xarm_api ROS service control):\nThe tool tip point offset values can be set by calling service \"/xarm/set_tcp_offset\". Refer to the figure below, please note this offset coordinate is expressed with respect to default tool frame (Frame B), which is located at flange center, with roll, pitch, yaw rotations of (PI, 0, 0) from base frame (Frame A).\nFor example:\n$ rosservice call /xarm/set_tcp_offset 0 0 20 0 0 0\nThis is to set tool frame position offset (x = 0 mm, y = 0 mm, z = 20 mm), and orientation (RPY) offset of ( 0, 0, 0 ) radians with respect to initial tool frame (Frame B in picture). Note this offset might be overwritten by xArm Stdudio if it is not consistent with the default value set in studio! It is recommended to do the same TCP default offset configuration in xArm studio if you want to use it alongside with ros service control.\nClearing Errors:\nSometimes controller may report error or warnings that would affect execution of further commands. The reasons may be power loss, position/speed limit violation, planning errors, etc. It needs additional intervention to clear. User can check error code in the message of topic \"xarm/xarm_states\" .\n$ rostopic echo /xarm/xarm_states\nIf it is non-zero, the corresponding reason can be found out in the user manual. After solving the problem, this error satus can be removed by calling service \"/xarm/clear_err\" with empty argument.\n$ rosservice call /xarm/clear_err\nIf using Moveit!, call \"/xarm/moveit_clear_err\" instead to avoid the need of setting mode 1 again manually.\n$ rosservice call /xarm/moveit_clear_err\nAfter calling this service, please check the err status again in 'xarm/xarm_states', if it becomes 0, the clearing is successful. Otherwise, it means the error/exception is not properly solved. If clearing error is successful, remember to set robot state to 0 to make it ready to move again!\nGripper Control:\nIf xArm Gripper (from UFACTORY) is attached to the tool end, the following services/actions can be called to operate or check the gripper.\n1. Gripper services:\n(1) First enable the griper and configure the grasp speed:\n$ rosservice call /xarm/gripper_config 1500\nProper range of the speed is from 1 to 5000. 1500 is used as an example. 'ret' value is 0 for success.\n(2) Give position command (open distance) to xArm gripper:\n$ rosservice call /xarm/gripper_move 500\nProper range of the open distance is from 0 to 850. 0 is closed, 850 is fully open. 500 is used as an example. 'ret' value is 0 for success.\n(3) To get the current status (position and error_code) of xArm gripper:\n$ rosservice call /xarm/gripper_state\nIf error code is non-zero, please refer to user manual for the cause of error, the \"/xarm/clear_err\" service can still be used to clear the error code of xArm Gripper.\n2. Gripper action:\nThe xArm gripper move action is defined in Move.action. The goal consists of target pulse position and the pulse speed. By setting \"true\" of \"use_gripper_action\" argument in xarm_bringup/launch/xarm7_server.launch, the action server will be started. Gripper action can be called by:\n$ rostopic pub -1 /xarm/gripper_move/goal xarm_gripper/MoveActionGoal \"header:\nseq: 0\nstamp:\nsecs: 0\nnsecs: 0\nframe_id: ''\ngoal_id:\nstamp:\nsecs: 0\nnsecs: 0\nid: ''\ngoal:\ntarget_pulse: 500.0\npulse_speed: 1500.0\"\nAlternatively:\n$ rosrun xarm_gripper gripper_client 500 1500\nVacuum Gripper Control:\nIf Vacuum Gripper (from UFACTORY) is attached to the tool end, the following service can be called to operate the vacuum gripper.\nTo turn on:\n$ rosservice call /xarm/vacuum_gripper_set 1\nTo turn off:\n$ rosservice call /xarm/vacuum_gripper_set 0\n0 will be returned upon successful execution.\nTool Modbus communication:\nIf modbus communication with the tool device is needed, please first set the proper baud rate and timeout parameters through the \"xarm/config_tool_modbus\" service (refer to ConfigToolModbus.srv). For example:\n$ rosservice call /xarm/config_tool_modbus 115200 20\nThe above command will configure the tool modbus baudrate to be 115200 bps and timeout threshold to be 20 ms. It is not necessary to configure again if these properties are not changed afterwards. Please note the first time to change the baud rate may return 1 (with error code 28), in fact it will succeed if the device is properly connected and there is no other exsisting controller errors. You can clear the error and call it once more to check if 0 is returned. Currently, only the following baud rates (bps) are supported: [4800, 9600, 19200, 38400, 57600, 115200, 230400, 460800, 921600, 1000000, 1500000, 2000000, 2500000].\nThen the communication can be conducted like (refer to SetToolModbus.srv):\n$ rosservice call /xarm/set_tool_modbus [0x01,0x06,0x00,0x0A,0x00,0x03] 6\nFirst argument would be the uint8(unsigned char) data array to be sent to the modbus tool device, and second is the number of characters to be received as a response from the device. This number should be the expected data byte length (without CRC bytes). For example, with some testing device the above instruction would reply:\nret: 0\nrespond_data: [1, 6, 0, 10, 0, 3]\nand actual feedback data frame is: [0x01, 0x06, 0x00, 0x0A, 0x00, 0x03], with the length of 6 bytes.\n6. Mode Change\nxArm may operate under different modes depending on different controling methods. Current mode can be checked in the message of topic \"xarm/xarm_states\". And there are circumstances that demand user to switch between operation modes.\n6.1 Mode Explanation\nMode 0 : xArm controller (Position) mode.\nMode 1 : External trajectory planner (position) mode.\nMode 2 : Free-Drive (zero gravity) mode.\nMode 3 : Reserved.\nMode 4 : Joint velocity control mode.\nMode 5 : Cartesian velocity control mode.\nMode 0 is the default when system initiates, and when error occurs(collision, overload, overspeed, etc), system will automatically switch to Mode 0. Also, all the motion plan services in xarm_api package or the SDK motion functions demand xArm to operate in Mode 0. Mode 1 is for external trajectory planner like Moveit! to bypass the integrated xArm planner to control the robot. Mode 2 is to enable free-drive operation, robot will enter Gravity compensated mode, however, be sure the mounting direction and payload are properly configured before setting to mode 2. Mode 4 is to control arm velocity in joint space. Mode 5 is to control arm (linear) velocity in Cartesian space.\n6.2 Proper way to change modes:\nIf collision or other error happens during the execution of a Moveit! planned trajectory, Mode will automatically switch from 1 to default mode 0 for safety purpose, and robot state will change to 4 (error state). The robot will not be able to execute any Moveit command again unless the mode is set back to 1. The following are the steps to switch back and enable Moveit control again:\n(1) Make sure the objects causing the collision are removed.\n(2) clear the error:\n$ rosservice call /xarm/clear_err\n(3) switch to the desired mode (Mode 2 for example), and set state to 0 for ready:\n$ rosservice call /xarm/set_mode 2\n$ rosservice call /xarm/set_state 0\n7. xArm Vision\nFor simple demonstrations of vision application development with xArm, including hand-eye calibration and object detection and grasping. Examples are based on Intel RealSense D435i depth camera.\n7.1 Installation of dependent packages:\nFirst enter the workspace source directory:\n$ cd ~/catkin_ws/src/\n7.1.1 Install RealSense developer library and ROS package\uff1a\nPlease refer to the installation steps at official webpage.\n7.1.2 Install 'aruco_ros', for hand-eye calibration\uff1a\nRefer to official Github:\n$ git clone -b kinetic-devel https://github.com/pal-robotics/aruco_ros.git\n7.1.3 Install 'easy_handeye', for hand-eye calibration\uff1a\nRefer to official Github:\n$ git clone https://github.com/IFL-CAMP/easy_handeye\n7.1.4 Install 'vision_visp' supporting package\uff1a\nRefer to official Github:\n$ git clone -b kinetic-devel https://github.com/lagadic/vision_visp.git\n7.1.5 Install 'find_object_2d', for object detection\uff1a\nRefer to official Github:\n$ sudo apt-get install ros-kinetic-find-object-2d\n7.1.6 Install other dependencies\uff1a\n$ cd ~/catkin_ws\nThen follow chapter 4.3.\n7.1.7 Build the whole workspace\uff1a\n$ catkin_make\n7.2 Hand-eye Calibration Demo\uff1a\nIf attaching RealSense D435i camera at tool end of xArm, with mechanical adapters, making a \"eye-on-hand\"(or eye-in-hand) configuration\uff0cthe following launch file can be used and modified for hand-eye calibration: (make sure the camera communication is functional and robot is properly switched on)\n$ roslaunch d435i_xarm_setup d435i_xarm_auto_calib.launch robot_dof:=your_xArm_DOF robot_ip:=your_xArm_IP\nThe aruco Marker used inside can be downloaded here, please remember the marker ID and marker size and modify them in the launch file accordingly. Refer to officialor other usage instructions online and finish the calibration with the GUI.\nIf calculation result is confirmed and saved\uff0cit will appear by default under ~/.ros/easy_handeye directory and can be used for transferring object coordinates to base frame. If the camera_stand provided by UFACTORY is used for fixing camera, a sample calibration result is stored at xarm_vision/d435i_xarm_setup/config/xarm_realsense_handeyecalibration_eye_on_hand_sample_result.yaml for this case.\n7.3 Vision Guided Grasping Demo:\nfind_object_2d is used for this demo for simple object detection and grasping. Hardware used in this part: RealSense D435i depth camera, UFACTORY camera stand and the xArm Gripper.\n1.Use moveit to drive xArm's motion\uff0crecommended for singularity and collision free execution, but will require a reliable network connection.\n$ roslaunch d435i_xarm_setup d435i_findobj2d_xarm_moveit_planner.launch robot_dof:=your_xArm_DOF robot_ip:=your_xArm_IP\nIf target object can be properly detected, to run the Grasping node:\n$ rosrun d435i_xarm_setup findobj2d_grasp_moveit\nPlease note it will use previously mentioned sample handeye calibration result, you can change it at publish_handeye_tf.launch. For node program source code, refer to: d435i_xarm_setup/src/findobj_grasp_moveit_planner.cpp.\n2.Alternatively, to drive xArm motion with ros service provided by 'xarm_api', in this way, real-time performance of network will not be required so strict as moveit way, but execution may fail in the middle if singularity or self-collision is about to occur.\n$ roslaunch d435i_xarm_setup d435i_findobj2d_xarm_api.launch robot_dof:=your_xArm_DOF robot_ip:=your_xArm_IP\nIf target object can be properly detected, to run the Grasping node:\n$ roslaunch d435i_xarm_setup grasp_node_xarm_api.launch\nPlease note it will use previously mentioned sample handeye calibration result, you can change it at publish_handeye_tf.launch. For node program source code, refer to: d435i_xarm_setup/src/findobj_grasp_xarm_api.cpp.\nPlease read and comprehend the source code and make necessary modifications before real application test, necessary modifications include preparation pose, grasping orientation, grasping depth, motion speed and so on. The identification target name in the code is \"object_1\", which corresponds to 1.png in /objects directory, users can add their own target in \"find_object_2d\" GUI, then modify the source_frame inside the code, for costomized application.\nTips: make sure the background is clean and the color is distinguished from the object, detection success rate can be higher if the target object has rich texture (features).\n7.4 Adding RealSense D435i model to simulated xArm\uff1a\nFor installation with camera stand provided by UFACTORY, the cam model can be attached by following modifications (use xarm7 as example):\n1.Together with xArm Gripper model: Set add_realsense_d435i default value to be true in xarm7_with_gripper.xacro.\n2.Together with xArm Vacuum Gripper model: Set add_realsense_d435i default value to be true in xarm7_with_vacuum_gripper.xacro.\n3.Purely the d435i: Set add_realsense_d435i default value to be true in xarm7_robot.urdf.xacro.\n8. Other Examples\nThere are some other application demo examples in the example package, which will be updated in the future, feel free to explore it.", "link": "https://github.com/xArm-Developer/xarm_ros", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "for simplified chinese version: \u7b80\u4f53\u4e2d\u6587\u7248\nimportant notice:\nafter using xarm c++ sdk as sub-module, the use of /xarm/set_tool_modbus service has been modified, compared with old version, the redundant '0x09' byte in response data has been removed\uff01\ndue to robot communication data format change, early users (xarm shipped before june 2019) are encouraged to upgrade their controller firmware immediately to drive the robot normally in future updates as well as to use newly developed functions. please contact our staff to get instructions of the upgrade process. the old version robot driver can still be available in 'legacy' branch, however, it will not be updated any more.\nyou must follow chapter 3 to install additional packages needed before any usage of xarm_ros packages. otherwise, unexpected errors may occur.\nif developing with moveit, it is highly recommended to use direct network cable connection between controller box and your pc, and no intermediate switches or routers, or the communication latency may have a bad impact on trajectory execution.\nwhen updating this package, please remember to check the submodule update as well!\ncontents:\n1. introduction\n2. update history\n3. preparations (must do)\n4. get started\n5. package description & usage guidance\n5.1 xarm_description\n5.2 xarm_gazebo\n5.3 xarm_controller\n5.4 xarm_bringup\n5.5 xarm7_moveit_config\n5.5.1 add custom -----> tool !!!  model for moveit\n5.6 xarm_planner\n5.7 xarm_api/xarm_msgs\n5.7.1 starting xarm by ros service (priority for the following operations)\n5.7.2 joint space or cartesian space command example(velocity control added)\n5.7.3 -----> tool !!! /controller i/o operations\n5.7.4 getting status feedback\n5.7.5 setting -----> tool !!!  center point offset\n5.7.6 clearing errors\n5.7.7 gripper control\n5.7.8 vacuum gripper control\n5.7.9 -----> tool !!!  modbus communication\n6. mode change\n6.1 mode explanation\n6.2 proper way to change modes\n7. xarm vision (new)\n7.1 installation of dependent packages\n7.2 hand-eye calibration demo\n7.3 vision guided grasping demo\n7.4 adding realsense d435i model to simulated xarm\n8. other examples\n8.1 multi-xarm5 (separate control)\n8.2 servo_cartesian\n8.3 servo_joint\n8.4 dual xarm6 controlled with one movegroup node\n8.5 an example of demonstrating redundancy resolution using moveit\n1. introduction\nthis repository contains the 3d models of xarm series and demo packages for ros development and simulations.developing and testing environment: ubuntu 16.04 + ros kinetic/melodic.\ninstructions below is based on xarm7, other model user can replace 'xarm7' with 'xarm6' or 'xarm5' where applicable.\n2. update summary\nthis package is still under development and improvement, tests, bug fixes and new functions are to be updated regularly in the future.\nadd xarm 7 description files, meshes and sample controller demos for ros simulation and visualization.\nadd moveit! planner support to control gazebo virtual model and real xarm, but the two can not launch together.\nadd direct control of real xarm through moveit gui, please use it with special care.\nadd xarm hardware interface to use ros position_controllers/jointtrajectorycontroller on real robot.\nadd xarm 6 and xarm 5 simulation/real robot control support.\nadd simulation model of xarm gripper.\nadd demo to control dual xarm6 through moveit.\nadd xarm gripper action control.\nadd xarm-with-gripper moveit development packages.\nadd vacuum gripper model and xarm-with-vacuum-gripper moveit development packages (under /examples dir).\nthanks to microsoft iot, xarm_ros can now be compiled and run on windows platform.\nadd velocity control mode for joint and cartesian space. (xarm controller firmware version >= 1.6.8 required)\nadd support for custom tool model for moveit\nadd timed-out version of velocity control mode, for better safety consideration. (xarm controller firmware version >= 1.8.0 required)\nadd xarm vision and realsense d435i related demo. migrate previous \"xarm_device\" into xarm_vision/camera_demo.\n3. preparations before using this package\n3.1 install dependent package module\ngazebo_ros_pkgs: http://gazebosim.org/tutorials?tut=ros_installing (if use gazebo)\nros_control: http://wiki.ros.org/ros_control (remember to select your correct ros distribution)\nmoveit_core: https://moveit.ros.org/install/\n3.2 go through the official tutorial documents\nros wiki: http://wiki.ros.org/\ngazebo tutorial: http://gazebosim.org/tutorials\ngazebo ros control: http://gazebosim.org/tutorials/?tut=ros_control\nmoveit tutorial: http://docs.ros.org/kinetic/api/moveit_tutorials/html/\n3.3 download the 'table' 3d model\nin gazebo simulator, navigate through the model database for 'table' item, drag and place the 3d model inside the virtual environment. it will then be downloaded locally, as 'table' is needed for running the demo.\n3.4 install \"mimic_joint_plugin\" for xarm gripper simulation in gazebo\nif simulating xarm gripper in gazebo is needed, mimic_joint_plugin by courtesy of konstantinos chatzilygeroudis (@costashatz) needs to be installed in order to make the mimic joints behave normally in gazebo. usage of this plugin is inspired by this tutorial from @mintar.\n12/22/2020: refer to issue #53, please note this plugin has recently been deprecated, if you plan to use new version, please change \"libroboticsgroup_gazebo_mimic_joint_plugin.so\" to \"libroboticsgroup_upatras_gazebo_mimic_joint_plugin.so\" in file: xarm_ros/xarm_gripper/urdf/xarm_gripper.gazebo.xacro\n4. getting started with 'xarm_ros'\n4.1 create a catkin workspace.\nif you already have a workspace, skip and move on to next part. follow the instructions in this page. please note that this readme instruction assumes the user continues to use '~/catkin_ws' as directory of the workspace.\n4.2 obtain the package\n$ cd ~/catkin_ws/src\n$ git clone https://github.com/xarm-developer/xarm_ros.git --recursive\n4.2.1 update the package\n$ cd ~/catkin_ws/src/xarm_ros\n$ git pull\n$ git submodule sync\n$ git submodule update --init --remote\n4.3 install other dependent packages:\n$ rosdep update\n$ rosdep check --from-paths . --ignore-src --rosdistro kinetic\nplease change 'kinetic' to the ros distribution you use. if there are any missing dependencies listed. run the following command to install:\n$ rosdep install --from-paths . --ignore-src --rosdistro kinetic -y\nand chane 'kinetic' to the ros distribution you use.\n4.4 build the code\n$ cd ~/catkin_ws\n$ catkin_make\n4.5 source the setup script\n$ echo \"source ~/catkin_ws/devel/setup.bash\" >> ~/.bashrc\nskip above operation if you already have that inside your ~/.bashrc. then do:\n$ source ~/.bashrc\n4.6 first try out in rviz:\n$ roslaunch xarm_description xarm7_rviz_display.launch\n4.7 run the demo in gazebo simulator\n$ roslaunch xarm_gazebo xarm7_beside_table.launch [run_demo:=true] [add_gripper:=true] [add_vacuum_gripper:=true]\nadd the \"run_demo\" option if you wish to see a pre-programed loop motion in action. the command trajectory is written in xarm_controller\\src\\sample_motion.cpp. and the trajectory in this demo is controlled by pure position interface.\nadd the \"add_gripper\" option if you want to see the xarm gripper attached at the tool end.\nadd the \"add_vacuum_gripper\" option if you want to see the xarm vacuum gripper attached at the tool end. please note only one end effector can be attached.\n5. package description & usage guidance\n5.1 xarm_description\nxarm description files, mesh files and gazebo plugin configurations, etc. it's not recommended to change the xarm description file since other packages depend on it.\n5.2 xarm_gazebo\ngazebo world description files and simulation launch files. user can add or build their own models in the simulation world file.\n5.3 xarm_controller\ncontroller configurations, hardware_interface, robot command executable source, scripts and launch files. user can deploy their program inside this package or create their own. note that effort controllers defined in xarm_controller/config are just examples for simulation purpose, when controlling the real arm, only 'position_controllers/jointtrajectorycontroller' interface is provided. user can add their self-defined controllers as well, refer to: http://wiki.ros.org/ros_control (controllers)\n5.4 xarm_bringup\nlaunch files to load xarm driver to enable direct control of real xarm hardware.\n5.5 xarm7_moveit_config\nplease note: xarm_moveit_config related packages will limit all joints within [-pi, pi], it seems that moveit tend to generate plans involving greater joint motions if not limited within this range. this limit can be canceled by setting \"limited:=false\" in ...moveit_config/launch/planning_context.launch.\nthis package is partially generated by moveit_setup_assistant, could use with moveit planner and rviz visualization. if you have moveit! installed, you can try the demo.\n$ roslaunch xarm7_moveit_config demo.launch\nto run moveit! motion planner along with gazebo simulator:\nif no xarm gripper needed, first run:\n$ roslaunch xarm_gazebo xarm7_beside_table.launch\nthen in another terminal:\n$ roslaunch xarm7_moveit_config xarm7_moveit_gazebo.launch\nif xarm gripper needs to be attached, first run:\n$ roslaunch xarm_gazebo xarm7_beside_table.launch add_gripper:=true\nthen in another terminal:\n$ roslaunch xarm7_gripper_moveit_config xarm7_gripper_moveit_gazebo.launch\nif you have a satisfied motion planned in moveit!, hit the \"execute\" button and the virtual arm in gazebo will execute the trajectory.\nif xarm vacuum gripper needs to be attached, just replace \"gripper\" with \"vacuum_gripper\" in above gripper example.\nto run moveit! motion planner to control the real xarm:\nfirst make sure the xarm and the controller box are powered on, then execute:\n$ roslaunch xarm7_moveit_config realmove_exec.launch robot_ip:=<your controller box lan ip address>\nexamine the terminal output and see if any error occured during the launch. if not, just play with the robot in rviz and you can execute the sucessfully planned trajectory on real arm. but be sure it will not hit any surroundings before execution!\nto run moveit! motion planner to control the real xarm with xarm gripper attached:\nfirst make sure the xarm and the controller box are powered on, then execute:\n$ roslaunch xarm7_gripper_moveit_config realmove_exec.launch robot_ip:=<your controller box lan ip address>\nit is better to use this package with real xarm gripper, since moveit planner will take the gripper into account for collision detection.\nto run moveit! motion planner to control the real xarm with xarm vacuum gripper attached:\nfirst make sure the xarm and the controller box are powered on, then execute:\n$ roslaunch xarm7_vacuum_gripper_moveit_config realmove_exec.launch robot_ip:=<your controller box lan ip address>\nit is better to use this package with real xarm vacuum gripper, since moveit planner will take the vacuum gripper into account for collision detection.\n5.5.1 add custom tool model for moveit\nthis part may require ros melodic or later versions to function well\nfor xarm5_moveit_config/xarm6_moveit_config/xarm7_moveit_config, customized tool models maybe added to the tool flange through quick-configuration parameters listed below\uff0cthus to enable tool offset and 3d collision checking during moveit motion planning. (notice\uff1aconfiguration through '/xarm/set_tcp_offset' service will not be effective in moveit planning!)\nexamples:\n# attaching box model:\n$ roslaunch xarm7_moveit_config demo.launch add_other_geometry:=true geometry_type:=box\n# attaching cylinder model:\n$ roslaunch xarm7_moveit_config demo.launch add_other_geometry:=true geometry_type:=cylinder\n# attaching sphere model:\n$ roslaunch xarm7_moveit_config demo.launch add_other_geometry:=true geometry_type:=sphere\n# attaching customized mesh model:\uff08here take xarm vacuum_gripper as an example\uff0cif the mesh model could be placed in: 'xarm_description/meshes/other'directory\uff0c'geometry_mesh_filename' argument can be simplified to be just the filename\uff09\n$ roslaunch xarm7_moveit_config demo.launch add_other_geometry:=true geometry_type:=mesh geometry_mesh_filename:=package://xarm_description/meshes/vacuum_gripper/visual/vacuum_gripper.stl geometry_mesh_tcp_xyz:='\"0 0 0.126\"'\nargument explanations:\nadd_other_geometry: default to be false\uff0cindicating whether to add other geometry model to the tool.\ngeometry_type: geometry shapes to be added\uff0cas one of 'box/cylinder/sphere/mesh', there are different parameters required for different types.\ngeometry_height: height of geometry shape\uff0cunit: meter\uff0cdefault value: 0.1\uff0ceffective for geometry_type: box/cylinder/sphere.\ngeometry_radius: radius of geometry shape\uff0cunit: meter\uff0cdefault value: 0.1\uff0ceffective for geometry_type: cylinder/sphere.\ngeometry_length: length of geometry shape\uff0cunit: meter\uff0cdefault value: 0.1\uff0ceffective for geometry_type: box.\ngeometry_width: width of geometry shape\uff0cunit: meter\uff0cdefault value: 0.1\uff0ceffective for geometry_type: box.\ngeometry_mesh_filename: geometry shape\uff0ceffective for geometry_type: mesh.\ngeometry_mesh_origin_xyz: position offset from mesh base coordinate to xarm tool-flange coordinate, default: \"0 0 0\"\uff0ceffective for geometry_type: mesh.\ngeometry_mesh_origin_rpy: orientation offset from mesh base coordinate to xarm tool-flange coordinate, default: \"0 0 0\"\uff0ceffective for geometry_type: mesh.\ngeometry_mesh_tcp_xyz: the positional tcp offset with respect to xarm tool-flange coordinate, default: \"0 0 0\"\uff0ceffective for geometry_type: mesh.\ngeometry_mesh_tcp_rpy: the orientational tcp offset with respect to xarm tool-flange coordinate, default: \"0 0 0\"\uff0ceffective for geometry_type: mesh.\n5.6 xarm_planner:\nthis implemented simple planner interface is based on move_group from moveit! and provide ros service for users to do planning & execution based on the requested target, user can find detailed instructions on how to use it inside xarm_planner package.\nto launch the xarm simple motion planner together with the real xarm:\n$ roslaunch xarm_planner xarm_planner_realhw.launch robot_ip:=<your controller box lan ip address> robot_dof:=<7|6|5> add_(vacuum_)gripper:=<true|false>\nargument 'robot_dof' specifies the number of joints of your xarm (default is 7). now xarm_planner supports model with gripper or vacuum_gripper attached. please specify \"add_gripper\" or \"add_vacuum_gripper\" argument if needed.\n5.7 xarm_api/xarm_msgs:\nthese two packages provide user with the ros service wrapper of the functions in xarm sdk. there are 12 types of motion command (service names) supported\uff0cplease set correct robot mode first, refer to mode change section:\nrobot mode 0:\nmove_joint: joint space point to point command, given target joint angles, max joint velocity and acceleration. corresponding function in sdk is \"set_servo_angle()\".\nmove_line: straight-line motion to the specified cartesian tool centre point(tcp) target. corresponding function in sdk is \"set_position()\"[blending radius not specified].\nmove_lineb: straight-line motion, and blending continuously with next motion. normally works in the form of a list of known via points followed by target cartesian point. each motion segment is straight-line with arc blending at the via points, to make velocity continuous. corresponding function in sdk is \"set_position()\"[wait=false and blending radius specified]. please refer to move_test.cpp and blended_motion_test.py for example code, /xarm/wait_for_finish parameter has to be false for successful blending calculation.\nmove_jointb: joint space point to point motion, and blending continuously with next motion. it can be used together with \"move_lineb\" for joint-linear blending motions, as long as the via points are known, and blending radius is properly specified, velocity will be continuous during the execution. corresponding function in sdk is \"set_servo_angle()\"[wait=false and blending radius specified]. please refer to blended_motion_test.py for example code. /xarm/wait_for_finish parameter has to be false for successful blending calculation.\nmove_line_tool: straight-line motion based on the tool coordinate system rather than the base system. corresponding function in sdk is \"set_tool_position()\".\nplease keep in mind that before calling the 4 motion services above, first set robot mode to be 0, then set robot state to be 0, by calling relavent services. meaning of the commands are consistent with the descriptions in product user manual, other xarm api supported functions are also available as service call. refer to xarm_msgs package for more details and usage guidance.\nmove_line_aa: straight-line motion, with orientation expressed in axis-angle rather than roll-pitch-yaw angles. please refer to xarm user manual for detailed explanation of axis-angle before using this command.\nrobot mode 1:\nmove_servo_cart/move_servoj: streamed high-frequency trajectory command execution in cartesian space or joint space. corresponding functions in sdk are set_servo_cartesian() and set_servo_angle_j(). an alternative way to implement velocity control. special risk assesment is required before using them. please read the guidance carefully at chapter 7.2-7.3.\nrobot mode 4:\nvelo_move_joint/velo_move_joint_timed: joint motion with specified velocity for each joint (unit: rad/s), with maximum joint acceleration configurable by set_max_acc_joint service.\nrobot mode 5:\nvelo_move_line/velo_move_line_timed: linear motion of tcp with specified velocity in mm/s (position) and rad/s (orientation in axis-angular_velocity), with maximum linear acceleration configurable by set_max_acc_line service.\nstarting xarm by ros service:\nfirst startup the service server for xarm7, ip address is just an example:\n$ roslaunch xarm_bringup xarm7_server.launch robot_ip:=192.168.1.128\nthen make sure all the servo motors are enabled, refer to setaxis.srv:\n$ rosservice call /xarm/motion_ctrl 8 1\nbefore any motion commands, set proper robot mode(0: pose) and state(0: ready) in order, refer to setint16.srv:\n$ rosservice call /xarm/set_mode 0\n$ rosservice call /xarm/set_state 0\njoint space or cartesian space command example:\nplease note that all the angles must use the unit of radian. all motion commands use the same type of srv request: move.srv.\n1. joint space motion:\nto call joint space motion with max speed 0.35 rad/s and acceleration 7 rad/s^2:\n$ rosservice call /xarm/move_joint [0,0,0,0,0,0,0] 0.35 7 0 0\nto go back to home (all joints at 0 rad) position with max speed 0.35 rad/s and acceleration 7 rad/s^2:\n$ rosservice call /xarm/go_home [] 0.35 7 0 0\n2. cartesian space motion in base coordinate:\nto call cartesian motion to the target expressed in robot base coordinate, with max speed 200 mm/s and acceleration 2000 mm/s^2:\n$ rosservice call /xarm/move_line [250,100,300,3.14,0,0] 200 2000 0 0\n3. cartesian space motion in tool coordinate:\nto call cartesian motion expressed in robot tool coordinate, with max speed 200 mm/s and acceleration 2000 mm/s^2, the following will move a relative motion (delta_x=50mm, delta_y=100mm, delta_z=100mm) along the current tool coordinate, no orientation change:\n$ rosservice call /xarm/move_line_tool [50,100,100,0,0,0] 200 2000 0 0\n4. cartesian space motion in axis-angle orientation:\ncorresponding service for axis-angle motion is moveaxisangle.srv. please pay attention to the last two arguments: \"coord\" is 0 for motion with respect to (w.r.t.) arm base coordinate system, and 1 for motion w.r.t. tool coordinate system. \"relative\" is 0 for absolute target position w.r.t. specified coordinate system, and 1 for relative target position.\nfor example: to move 1.0 radian relatively around tool-frame z-axis:\n$ rosservice call /xarm/move_line_aa \"pose: [0, 0, 0, 0, 0, 1.0]\nmvvelo: 30.0\nmvacc: 100.0\nmvtime: 0.0\ncoord: 1\nrelative: 1\"\nret: 0\nmessage: \"move_line_aa, ret = 0\"\nor\n$ rosservice call /xarm/move_line_aa [0,0,0,0,0,1.0] 30.0 100.0 0.0 1 1\n\"mvtime\" is not meaningful in this command, just set it to 0. another example: in base-frame, to move 122mm relatively along y-axis, and rotate around x-axis for -0.5 radians:\n$ rosservice call /xarm/move_line_aa [0,122,0,-0.5,0,0] 30.0 100.0 0.0 0 1\n5. joint velocity control:\n(xarm controller firmware version >= 1.6.8 required) if controlling joint velocity is desired, first switch to mode 4 as descriped in mode change section. please check the movevelo.srv first to understand the meanings of parameters reqired. if more than one joint are to move, set jnt_sync to 1 for synchronized acceleration/deceleration for all joints in motion, and if jnt_sync is 0, each joint will reach to its target velocity as fast as possible. coord parameter is not used here, just set it to 0. for example:\n# no timed-out version (will not stop until all-zero velocity command received!):\n$ rosservice call /xarm/velo_move_joint [0.1,-0.1,0,0,0,-0.3] 1 0\n# with timed-out version(controller firmware version >= 1.8.0): (if next velocity command not received within 0.2 seconds, xarm will stop)\n$ rosservice call /xarm/velo_move_joint_timed [0.1,-0.1,0,0,0,-0.3] 1 0 0.2\nwill command the joints (for xarm6) to move in specified angular velocities (in rad/s) and they will reach to target velocities synchronously. the maximum joint acceleration can also be configured by (unit: rad/s^2):\n$ rosservice call /xarm/set_max_acc_joint 10.0 (maximum: 20.0 rad/s^2)\n6. cartesian velocity control:\n(xarm controller firmware version >= 1.6.8 required) if controlling linar velocity of tcp towards certain direction is desired, first switch to mode 5 as descriped in mode change section. please check the movevelo.srv first to understand the meanings of parameters reqired. set coord to 0 for motion in world/base coordinate system and 1 for tool coordinate system. jnt_sync parameter is not used here, just set it to 0. for example:\n# no timed-out version (will not stop until all-zero velocity command received!):\n$ rosservice call /xarm/velo_move_line [30,0,0,0,0,0] 0 1\n# with timed-out version(controller firmware version >= 1.8.0): (if next velocity command not received within 0.2 seconds, xarm will stop)\n$ rosservice call /xarm/velo_move_line_timed [30,0,0,0,0,0] 0 1 0.2\nwill command xarm tcp move along x-axis of tool coordinate system with speed of 30 mm/s. the maximum linear acceleration can also be configured by (unit: mm/s^2):\n$ rosservice call /xarm/set_max_acc_line 5000.0 (maximum: 50000 mm/s^2)\nfor angular motion in orientation, please note the velocity is specified as axis-angular_velocity elements. that is, [the unit rotation axis vector] multiplied by [rotation velocity value(scalar)]. for example,\n# no timed-out version (will not stop until all-zero velocity command received!):\n$ rosservice call /xarm/velo_move_line [0,0,0,0.707,0,0] 0 0\n# with timed-out version(controller firmware version >= 1.8.0): (if next velocity command not received within 0.2 seconds, xarm will stop)\n$ rosservice call /xarm/velo_move_line_timed [0,0,0,0.707,0,0] 0 0 0.2\nthis will command tcp to rotate along x-axis in base coordinates at about 45 degrees/sec. the maximum acceleration for orientation change is fixed.\nplease note: for no timed-out version services: velocity motion can be stopped by either giving all 0 velocity command, or setting state to 4(stop) and 0(ready) later for next motion. however, timed-out versions are more recommended for use, since it can be safe if network comminication or user program fails, controller firmware needs to be updated to v1.8.0 or later.\nmotion service return:\nplease note the above motion services will return immediately by default. if you wish to return until actual motion is finished, set the ros parameter \"/xarm/wait_for_finish\" to be true in advance. that is:\n$ rosparam set /xarm/wait_for_finish true\nupon success, 0 will be returned. if any error occurs, 1 will be returned.\ntool i/o operations:\nwe provide 2 digital, 2 analog input port and 2 digital output signals at the end i/o connector.\n1. to get current 2 digital input states:\n$ rosservice call /xarm/get_digital_in\n2. to get one of the analog input value:\n$ rosservice call /xarm/get_analog_in 1 (last argument: port number, can only be 1 or 2)\n3. to set one of the digital output:\n$ rosservice call /xarm/set_digital_out 2 1 (setting output 2 to be 1)\nyou have to make sure the operation is successful by checking responding \"ret\" to be 0.\ncontroller i/o operations:\nwe provide 8/16 digital input and 8/16 digital output ports at controller box for general usage.\n1. to get one of the controller digital input state:\n$ rosservice call /xarm/get_controller_din io_num (notice: from 1 to 8, for ci0~ci7; from 9 to 16, for di0~di7[if any])\n2. to set one of the controller digital output:\n$ rosservice call /xarm/set_controller_dout io_num (notice: from 1 to 8, for co0~co7; from 9 to 16, for di0~di7[if any]) logic (0 or 1)\nfor example:\n$ rosservice call /xarm/set_controller_dout 5 1 (setting output 5 [lable c04] to be 1)\n3. to get one of the controller analog input:\n$ rosservice call /xarm/get_controller_ain port_num (notice: from 1 to 2, for ai0~ai1)\n4. to set one of the controller analog output:\n$ rosservice call /xarm/set_controller_aout port_num (notice: from 1 to 2, for ao0~ao1) analog_value\nfor example:\n$ rosservice call /xarm/set_controller_aout 2 3.3 (setting port ao1 to be 3.3)\nyou have to make sure the operation is successful by checking responding \"ret\" to be 0.\ngetting status feedback:\nhaving connected with a real xarm robot by running 'xarm7_server.launch', user can subscribe to the topic \"xarm/xarm_states\" for feedback information about current robot states, including joint angles, tcp position, error/warning code, etc. refer to robotmsg.msg for content details.\nanother option is subscribing to \"/joint_states\" topic, which is reporting in jointstate.msg, however, currently only \"position\" field is valid; \"velocity\" is non-filtered numerical differentiation based on 2 adjacent position data, and \"effort\" feedback are current-based estimated values, not from direct torque sensor, so they are just for reference. in consideration of performance, current update rate of above two topics are set at 5hz.\nsetting tool center point offset(only effective for xarm_api ros service control):\nthe tool tip point offset values can be set by calling service \"/xarm/set_tcp_offset\". refer to the figure below, please note this offset coordinate is expressed with respect to default tool frame (frame b), which is located at flange center, with roll, pitch, yaw rotations of (pi, 0, 0) from base frame (frame a).\nfor example:\n$ rosservice call /xarm/set_tcp_offset 0 0 20 0 0 0\nthis is to set tool frame position offset (x = 0 mm, y = 0 mm, z = 20 mm), and orientation (rpy) offset of ( 0, 0, 0 ) radians with respect to initial tool frame (frame b in picture). note this offset might be overwritten by xarm stdudio if it is not consistent with the default value set in studio! it is recommended to do the same tcp default offset configuration in xarm studio if you want to use it alongside with ros service control.\nclearing errors:\nsometimes controller may report error or warnings that would affect execution of further commands. the reasons may be power loss, position/speed limit violation, planning errors, etc. it needs additional intervention to clear. user can check error code in the message of topic \"xarm/xarm_states\" .\n$ rostopic echo /xarm/xarm_states\nif it is non-zero, the corresponding reason can be found out in the user manual. after solving the problem, this error satus can be removed by calling service \"/xarm/clear_err\" with empty argument.\n$ rosservice call /xarm/clear_err\nif using moveit!, call \"/xarm/moveit_clear_err\" instead to avoid the need of setting mode 1 again manually.\n$ rosservice call /xarm/moveit_clear_err\nafter calling this service, please check the err status again in 'xarm/xarm_states', if it becomes 0, the clearing is successful. otherwise, it means the error/exception is not properly solved. if clearing error is successful, remember to set robot state to 0 to make it ready to move again!\ngripper control:\nif xarm gripper (from ufactory) is attached to the tool end, the following services/actions can be called to operate or check the gripper.\n1. gripper services:\n(1) first enable the griper and configure the grasp speed:\n$ rosservice call /xarm/gripper_config 1500\nproper range of the speed is from 1 to 5000. 1500 is used as an example. 'ret' value is 0 for success.\n(2) give position command (open distance) to xarm gripper:\n$ rosservice call /xarm/gripper_move 500\nproper range of the open distance is from 0 to 850. 0 is closed, 850 is fully open. 500 is used as an example. 'ret' value is 0 for success.\n(3) to get the current status (position and error_code) of xarm gripper:\n$ rosservice call /xarm/gripper_state\nif error code is non-zero, please refer to user manual for the cause of error, the \"/xarm/clear_err\" service can still be used to clear the error code of xarm gripper.\n2. gripper action:\nthe xarm gripper move action is defined in move.action. the goal consists of target pulse position and the pulse speed. by setting \"true\" of \"use_gripper_action\" argument in xarm_bringup/launch/xarm7_server.launch, the action server will be started. gripper action can be called by:\n$ rostopic pub -1 /xarm/gripper_move/goal xarm_gripper/moveactiongoal \"header:\nseq: 0\nstamp:\nsecs: 0\nnsecs: 0\nframe_id: ''\ngoal_id:\nstamp:\nsecs: 0\nnsecs: 0\nid: ''\ngoal:\ntarget_pulse: 500.0\npulse_speed: 1500.0\"\nalternatively:\n$ rosrun xarm_gripper gripper_client 500 1500\nvacuum gripper control:\nif vacuum gripper (from ufactory) is attached to the tool end, the following service can be called to operate the vacuum gripper.\nto turn on:\n$ rosservice call /xarm/vacuum_gripper_set 1\nto turn off:\n$ rosservice call /xarm/vacuum_gripper_set 0\n0 will be returned upon successful execution.\ntool modbus communication:\nif modbus communication with the tool device is needed, please first set the proper baud rate and timeout parameters through the \"xarm/config_tool_modbus\" service (refer to configtoolmodbus.srv). for example:\n$ rosservice call /xarm/config_tool_modbus 115200 20\nthe above command will configure the tool modbus baudrate to be 115200 bps and timeout threshold to be 20 ms. it is not necessary to configure again if these properties are not changed afterwards. please note the first time to change the baud rate may return 1 (with error code 28), in fact it will succeed if the device is properly connected and there is no other exsisting controller errors. you can clear the error and call it once more to check if 0 is returned. currently, only the following baud rates (bps) are supported: [4800, 9600, 19200, 38400, 57600, 115200, 230400, 460800, 921600, 1000000, 1500000, 2000000, 2500000].\nthen the communication can be conducted like (refer to settoolmodbus.srv):\n$ rosservice call /xarm/set_tool_modbus [0x01,0x06,0x00,0x0a,0x00,0x03] 6\nfirst argument would be the uint8(unsigned char) data array to be sent to the modbus tool device, and second is the number of characters to be received as a response from the device. this number should be the expected data byte length (without crc bytes). for example, with some testing device the above instruction would reply:\nret: 0\nrespond_data: [1, 6, 0, 10, 0, 3]\nand actual feedback data frame is: [0x01, 0x06, 0x00, 0x0a, 0x00, 0x03], with the length of 6 bytes.\n6. mode change\nxarm may operate under different modes depending on different controling methods. current mode can be checked in the message of topic \"xarm/xarm_states\". and there are circumstances that demand user to switch between operation modes.\n6.1 mode explanation\nmode 0 : xarm controller (position) mode.\nmode 1 : external trajectory planner (position) mode.\nmode 2 : free-drive (zero gravity) mode.\nmode 3 : reserved.\nmode 4 : joint velocity control mode.\nmode 5 : cartesian velocity control mode.\nmode 0 is the default when system initiates, and when error occurs(collision, overload, overspeed, etc), system will automatically switch to mode 0. also, all the motion plan services in xarm_api package or the sdk motion functions demand xarm to operate in mode 0. mode 1 is for external trajectory planner like moveit! to bypass the integrated xarm planner to control the robot. mode 2 is to enable free-drive operation, robot will enter gravity compensated mode, however, be sure the mounting direction and payload are properly configured before setting to mode 2. mode 4 is to control arm velocity in joint space. mode 5 is to control arm (linear) velocity in cartesian space.\n6.2 proper way to change modes:\nif collision or other error happens during the execution of a moveit! planned trajectory, mode will automatically switch from 1 to default mode 0 for safety purpose, and robot state will change to 4 (error state). the robot will not be able to execute any moveit command again unless the mode is set back to 1. the following are the steps to switch back and enable moveit control again:\n(1) make sure the objects causing the collision are removed.\n(2) clear the error:\n$ rosservice call /xarm/clear_err\n(3) switch to the desired mode (mode 2 for example), and set state to 0 for ready:\n$ rosservice call /xarm/set_mode 2\n$ rosservice call /xarm/set_state 0\n7. xarm vision\nfor simple demonstrations of vision application development with xarm, including hand-eye calibration and object detection and grasping. examples are based on intel realsense d435i depth camera.\n7.1 installation of dependent packages:\nfirst enter the workspace source directory:\n$ cd ~/catkin_ws/src/\n7.1.1 install realsense developer library and ros package\uff1a\nplease refer to the installation steps at official webpage.\n7.1.2 install 'aruco_ros', for hand-eye calibration\uff1a\nrefer to official github:\n$ git clone -b kinetic-devel https://github.com/pal-robotics/aruco_ros.git\n7.1.3 install 'easy_handeye', for hand-eye calibration\uff1a\nrefer to official github:\n$ git clone https://github.com/ifl-camp/easy_handeye\n7.1.4 install 'vision_visp' supporting package\uff1a\nrefer to official github:\n$ git clone -b kinetic-devel https://github.com/lagadic/vision_visp.git\n7.1.5 install 'find_object_2d', for object detection\uff1a\nrefer to official github:\n$ sudo apt-get install ros-kinetic-find-object-2d\n7.1.6 install other dependencies\uff1a\n$ cd ~/catkin_ws\nthen follow chapter 4.3.\n7.1.7 build the whole workspace\uff1a\n$ catkin_make\n7.2 hand-eye calibration demo\uff1a\nif attaching realsense d435i camera at tool end of xarm, with mechanical adapters, making a \"eye-on-hand\"(or eye-in-hand) configuration\uff0cthe following launch file can be used and modified for hand-eye calibration: (make sure the camera communication is functional and robot is properly switched on)\n$ roslaunch d435i_xarm_setup d435i_xarm_auto_calib.launch robot_dof:=your_xarm_dof robot_ip:=your_xarm_ip\nthe aruco marker used inside can be downloaded here, please remember the marker id and marker size and modify them in the launch file accordingly. refer to officialor other usage instructions online and finish the calibration with the gui.\nif calculation result is confirmed and saved\uff0cit will appear by default under ~/.ros/easy_handeye directory and can be used for transferring object coordinates to base frame. if the camera_stand provided by ufactory is used for fixing camera, a sample calibration result is stored at xarm_vision/d435i_xarm_setup/config/xarm_realsense_handeyecalibration_eye_on_hand_sample_result.yaml for this case.\n7.3 vision guided grasping demo:\nfind_object_2d is used for this demo for simple object detection and grasping. hardware used in this part: realsense d435i depth camera, ufactory camera stand and the xarm gripper.\n1.use moveit to drive xarm's motion\uff0crecommended for singularity and collision free execution, but will require a reliable network connection.\n$ roslaunch d435i_xarm_setup d435i_findobj2d_xarm_moveit_planner.launch robot_dof:=your_xarm_dof robot_ip:=your_xarm_ip\nif target object can be properly detected, to run the grasping node:\n$ rosrun d435i_xarm_setup findobj2d_grasp_moveit\nplease note it will use previously mentioned sample handeye calibration result, you can change it at publish_handeye_tf.launch. for node program source code, refer to: d435i_xarm_setup/src/findobj_grasp_moveit_planner.cpp.\n2.alternatively, to drive xarm motion with ros service provided by 'xarm_api', in this way, real-time performance of network will not be required so strict as moveit way, but execution may fail in the middle if singularity or self-collision is about to occur.\n$ roslaunch d435i_xarm_setup d435i_findobj2d_xarm_api.launch robot_dof:=your_xarm_dof robot_ip:=your_xarm_ip\nif target object can be properly detected, to run the grasping node:\n$ roslaunch d435i_xarm_setup grasp_node_xarm_api.launch\nplease note it will use previously mentioned sample handeye calibration result, you can change it at publish_handeye_tf.launch. for node program source code, refer to: d435i_xarm_setup/src/findobj_grasp_xarm_api.cpp.\nplease read and comprehend the source code and make necessary modifications before real application test, necessary modifications include preparation pose, grasping orientation, grasping depth, motion speed and so on. the identification target name in the code is \"object_1\", which corresponds to 1.png in /objects directory, users can add their own target in \"find_object_2d\" gui, then modify the source_frame inside the code, for costomized application.\ntips: make sure the background is clean and the color is distinguished from the object, detection success rate can be higher if the target object has rich texture (features).\n7.4 adding realsense d435i model to simulated xarm\uff1a\nfor installation with camera stand provided by ufactory, the cam model can be attached by following modifications (use xarm7 as example):\n1.together with xarm gripper model: set add_realsense_d435i default value to be true in xarm7_with_gripper.xacro.\n2.together with xarm vacuum gripper model: set add_realsense_d435i default value to be true in xarm7_with_vacuum_gripper.xacro.\n3.purely the d435i: set add_realsense_d435i default value to be true in xarm7_robot.urdf.xacro.\n8. other examples\nthere are some other application demo examples in the example package, which will be updated in the future, feel free to explore it.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000358, "year": null}, {"Unnamed: 0": 1379, "autor": 359, "date": null, "content": "OCS2 Toolbox\nSummary\nOCS2 is a C++ toolbox tailored for Optimal Control for Switched Systems (OCS2). The toolbox provides an efficient implementation of the following algorith\nSLQ: Continuous-time domin DDP\niLQR: Discrete-time domain DDP\nSQP: Multiple-shooting algorithm based on HPIPM\nPISOC: Path integral stochatic optimal control\nOCS2 handles general path constraints through Augmented Lagrangian or relaxed barrier methods. To facilitate the application of OCS2 in robotic tasks, it provides the user with additional tools to set up the system dynamics (such as kinematic or dynamic models) and cost/constraints (such as self-collision avoidance and end-effector tracking) from a URDF model. The library also provides an automatic differentiation tool to calculate derivatives of the system dynamics, constraints, and cost. To facilitate its deployment on robotic platforms, the OCS2 provides tools for ROS interfaces. The toolbox\u2019s efficient and numerically stable implementations in conjunction with its user-friendly interface have paved the way for employing it on numerous robotic applications with limited onboard computation power.\nFor more information refer to the project's Documentation Page", "link": "https://github.com/leggedrobotics/ocs2", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "ocs2 toolbox\nsummary\nocs2 is a c++ toolbox tailored for optimal control for switched systems (ocs2). the toolbox provides an efficient implementation of the following algorith\nslq: continuous-time domin ddp\nilqr: discrete-time domain ddp\nsqp: multiple-shooting algorithm based on hpipm\npisoc: path integral stochatic optimal control\nocs2 handles general path constraints through augmented lagrangian or relaxed barrier methods. to facilitate the application of ocs2 in robotic tasks, it provides the user with additional tools to set up the system dynamics (such as kinematic or dynamic models) and cost/constraints (such as self-collision avoidance and end-effector tracking) from a urdf model. the library also provides an automatic differentiation -----> tool !!!  to calculate derivatives of the system dynamics, constraints, and cost. to facilitate its deployment on robotic platforms, the ocs2 provides tools for ros interfaces. the toolbox\u2019s efficient and numerically stable implementations in conjunction with its user-friendly interface have paved the way for employing it on numerous robotic applications with limited onboard computation power.\nfor more information refer to the project's documentation page", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000359, "year": null}, {"Unnamed: 0": 1385, "autor": 365, "date": null, "content": "This repository has been archived and is not maintained any further. Refer to alurity for future progress on robot footprinting and fingerprinting.\naztarna\nThis repository contains Alias Robotics' aztarna, a footprinting tool for robots.\nAlias Robotics supports original robot manufacturers assessing their security and improving their quality of software. By no means we encourage or promote the unauthorized tampering with running robotic systems. This can cause serious human harm and material damages.\nFor ROS\nA list of the ROS nodes present in the system (Publishers and Subscribers)\nFor each node, the published and subscribed topis including the topic type\nFor each node, the ROS services each of the nodes offer\nA list of all ROS parameters present in the Parameter Server\nA list of the active communications running in the system. A single communication includes the involved publiser/subscriber nodes and the topics\nFor SROS\nDetermining if the system is a SROS master.\nDetecting if demo configuration is in use.\nA list of the nodes found in the system. (Extended mode)\nA list of allow/deny policies for each node.\nPublishable topics.\nSubscriptable topics.\nExecutable services.\nReadable parameters.\nFor ROS2 (Funded under the ROSIN project)\nA list of ROS2 nodes present in each communication domains.\nA list of discovered topics on each communication domain.\nA list of discovered services on each communication domain.\nFor each node, the relationship of published and subscribed topics.\nFor each node, the services provided by that node.\nFor Industrial routers\nDetecting eWON, Moxa, Sierra Wireless and Westermo industrial routers.\nDefault credential checking for found routers.\nFor ROS Industrial packages (Funded under the ROSIN project)\nDetection of ROS Industrial Hosts.\nManufacturers:\nABB\nFanuc\nKuka\nInstalling\nFor production\nDirecly from PyPi\npip3 install aztarna\nor from the repository:\npip3 install .\nFor development\npip3 install -e .\nor\npython3 setup.py develop\nPython 3.6 and the setuptools package is required for installation. Python 3.7 is recommended.\nROS2 Module\nFor usage of the ROS2 footprinting module a ROS2 installation is required. Source the setup.bash script prior to launch.\nInstall with docker\ndocker build -t aztarna_docker .\nCode usage:\nusage: aztarna [-h] -t TYPE [-a ADDRESS] [-p PORTS] [-i INPUT_FILE]\n[-o OUT_FILE] [-e] [-r RATE] [-d DOMAIN] [--daemon] [--hidden]\n[--shodan] [--api-key API_KEY] [--passive PASSIVE]\nAztarna\noptional arguments:\n-h, --help show this help message and exit\n-t TYPE, --type TYPE <ROS/ros/SROS/sros/ROS2/ros2/IROUTERS/irouters> Scan\nROS, SROS, ROS2 hosts or Industrial routers\n-a ADDRESS, --address ADDRESS\nSingle address or network range to scan.\n-p PORTS, --ports PORTS\nPorts to scan (format: 13311 or 11111-11155 or\n1,2,3,4)\n-i INPUT_FILE, --input_file INPUT_FILE\nInput file of addresses to use for scanning\n-o OUT_FILE, --out_file OUT_FILE\nOutput file for the results\n-e, --extended Extended scan of the hosts\n-r RATE, --rate RATE Maximum simultaneous network connections\n-d DOMAIN, --domain DOMAIN\nROS 2 DOMAIN ID (ROS_DOMAIN_ID environmental\nvariable). Only applies to ROS 2.\n--daemon Use rclpy daemon (coming from ros2cli).\n--hidden Show hidden ROS 2 nodes. By default filtering\n_ros2cli*\n--shodan Use shodan for the scan types that support it.\n--api-key API_KEY Shodan API Key\n--passive PASSIVE Passive search for ROS2\nRun the code (example input file):\naztarna -t ROS -p 11311 -i ros_scan_s20.csv\nRun the code with Docker (example input file):\ndocker run -v <host_path>:/root -it aztarna_docker -t ROS -p 11311 -i <input_file>\nRun the code (example single ip address):\naztarna -t ROS -p 11311 -a 115.129.241.241\nRun the code (example subnet):\naztarna -t ROS -p 11311 -a 115.129.241.0/24\nRun the code (example single ip address, port range):\naztarna -t ROS -p 11311-11500 -a 115.129.241.241\nRun the code (example single ip address, port list):\naztarna -t ROS -p 11311,11312,11313 -a 115.129.241.241\nRun the code with ROS 2 (example exploring all ranges, 0-231)\naztarna -t ROS2\nRun the code with ROS 2 with ROS_DOMAIN_ID=15\naztarna -t ROS2 -d 15\nRun the code with ROS 2 using rclpy ros2cli daemon and with ROS_DOMAIN_ID=0 while showing hidden nodes\naztarna -t ros2 -d 0 --daemon --hidden\nRun de code with ROS 2 using passive mode to search the hosts. if you set 'any' as argument, is going to search on all interfaces in your system:\naztarna -t ros2 --passive any\nRun the code (example piping directly from zmap):\nzmap -p 11311 0.0.0.0/0 -q | aztarna -t SROS -p 11311\nRun the code (example search for industrial routers in shodan)\naztarna -t IROUTERS --shodan --api-key <yourshodanapikey>\nRun the code (example search for industrial routers in shodan, piping to file)\naztarna -t IROUTERS --shodan --api-key <yourshodanapikey> -o routers.csv\nCite our work\nIf you're using our work for your research, please cite us as:\n@ARTICLE{2018arXiv181209490V,\nauthor = {{Vilches}, V{\\'\\i}ctor Mayoral and {Mendia}, Gorka Olalde and\n{Baskaran}, Xabier Perez and {Cordero}, Alejandro Hern{\\'a}ndez\nand {Juan}, Lander Usategui San and {Gil-Uriarte}, Endika and\n{de Urabain}, Odei Olalde Saez and {Kirschgens}, Laura Alzola},\ntitle = \"{Aztarna, a footprinting tool for robots}\",\njournal = {arXiv e-prints},\nkeywords = {Computer Science - Cryptography and Security, Computer Science - Robotics},\nyear = 2018,\nmonth = Dec,\neid = {arXiv:1812.09490},\npages = {arXiv:1812.09490},\narchivePrefix = {arXiv},\neprint = {1812.09490},\nprimaryClass = {cs.CR},\nadsurl = {https://ui.adsabs.harvard.edu/\\#abs/2018arXiv181209490V},\nadsnote = {Provided by the SAO/NASA Astrophysics Data System}\n}\nSupported by ROSIN - ROS-Industrial Quality-Assured Robot Software Components. More information: rosin-project.eu\nThis repository was partly funded by ROSIN RedROS2-I FTP which received funding from the European Union\u2019s Horizon 2020 research and innovation programme under the project ROSIN with the grant agreement No 732287.", "link": "https://github.com/aliasrobotics/aztarna", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "this repository has been archived and is not maintained any further. refer to alurity for future progress on robot footprinting and fingerprinting.\naztarna\nthis repository contains alias robotics' aztarna, a footprinting -----> tool !!!  for robots.\nalias robotics supports original robot manufacturers assessing their security and improving their quality of software. by no means we encourage or promote the unauthorized tampering with running robotic systems. this can cause serious human harm and material damages.\nfor ros\na list of the ros nodes present in the system (publishers and subscribers)\nfor each node, the published and subscribed topis including the topic type\nfor each node, the ros services each of the nodes offer\na list of all ros parameters present in the parameter server\na list of the active communications running in the system. a single communication includes the involved publiser/subscriber nodes and the topics\nfor sros\ndetermining if the system is a sros master.\ndetecting if demo configuration is in use.\na list of the nodes found in the system. (extended mode)\na list of allow/deny policies for each node.\npublishable topics.\nsubscriptable topics.\nexecutable services.\nreadable parameters.\nfor ros2 (funded under the rosin project)\na list of ros2 nodes present in each communication domains.\na list of discovered topics on each communication domain.\na list of discovered services on each communication domain.\nfor each node, the relationship of published and subscribed topics.\nfor each node, the services provided by that node.\nfor industrial routers\ndetecting ewon, moxa, sierra wireless and westermo industrial routers.\ndefault credential checking for found routers.\nfor ros industrial packages (funded under the rosin project)\ndetection of ros industrial hosts.\nmanufacturers:\nabb\nfanuc\nkuka\ninstalling\nfor production\ndirecly from pypi\npip3 install aztarna\nor from the repository:\npip3 install .\nfor development\npip3 install -e .\nor\npython3 setup.py develop\npython 3.6 and the setuptools package is required for installation. python 3.7 is recommended.\nros2 module\nfor usage of the ros2 footprinting module a ros2 installation is required. source the setup.bash script prior to launch.\ninstall with docker\ndocker build -t aztarna_docker .\ncode usage:\nusage: aztarna [-h] -t type [-a address] [-p ports] [-i input_file]\n[-o out_file] [-e] [-r rate] [-d domain] [--daemon] [--hidden]\n[--shodan] [--api-key api_key] [--passive passive]\naztarna\noptional arguments:\n-h, --help show this help message and exit\n-t type, --type type <ros/ros/sros/sros/ros2/ros2/irouters/irouters> scan\nros, sros, ros2 hosts or industrial routers\n-a address, --address address\nsingle address or network range to scan.\n-p ports, --ports ports\nports to scan (format: 13311 or 11111-11155 or\n1,2,3,4)\n-i input_file, --input_file input_file\ninput file of addresses to use for scanning\n-o out_file, --out_file out_file\noutput file for the results\n-e, --extended extended scan of the hosts\n-r rate, --rate rate maximum simultaneous network connections\n-d domain, --domain domain\nros 2 domain id (ros_domain_id environmental\nvariable). only applies to ros 2.\n--daemon use rclpy daemon (coming from ros2cli).\n--hidden show hidden ros 2 nodes. by default filtering\n_ros2cli*\n--shodan use shodan for the scan types that support it.\n--api-key api_key shodan api key\n--passive passive passive search for ros2\nrun the code (example input file):\naztarna -t ros -p 11311 -i ros_scan_s20.csv\nrun the code with docker (example input file):\ndocker run -v <host_path>:/root -it aztarna_docker -t ros -p 11311 -i <input_file>\nrun the code (example single ip address):\naztarna -t ros -p 11311 -a 115.129.241.241\nrun the code (example subnet):\naztarna -t ros -p 11311 -a 115.129.241.0/24\nrun the code (example single ip address, port range):\naztarna -t ros -p 11311-11500 -a 115.129.241.241\nrun the code (example single ip address, port list):\naztarna -t ros -p 11311,11312,11313 -a 115.129.241.241\nrun the code with ros 2 (example exploring all ranges, 0-231)\naztarna -t ros2\nrun the code with ros 2 with ros_domain_id=15\naztarna -t ros2 -d 15\nrun the code with ros 2 using rclpy ros2cli daemon and with ros_domain_id=0 while showing hidden nodes\naztarna -t ros2 -d 0 --daemon --hidden\nrun de code with ros 2 using passive mode to search the hosts. if you set 'any' as argument, is going to search on all interfaces in your system:\naztarna -t ros2 --passive any\nrun the code (example piping directly from zmap):\nzmap -p 11311 0.0.0.0/0 -q | aztarna -t sros -p 11311\nrun the code (example search for industrial routers in shodan)\naztarna -t irouters --shodan --api-key <yourshodanapikey>\nrun the code (example search for industrial routers in shodan, piping to file)\naztarna -t irouters --shodan --api-key <yourshodanapikey> -o routers.csv\ncite our work\nif you're using our work for your research, please cite us as:\n@article{2018arxiv181209490v,\nauthor = {{vilches}, v{\\'\\i}ctor mayoral and {mendia}, gorka olalde and\n{baskaran}, xabier perez and {cordero}, alejandro hern{\\'a}ndez\nand {juan}, lander usategui san and {gil-uriarte}, endika and\n{de urabain}, odei olalde saez and {kirschgens}, laura alzola},\ntitle = \"{aztarna, a footprinting tool for robots}\",\njournal = {arxiv e-prints},\nkeywords = {computer science - cryptography and security, computer science - robotics},\nyear = 2018,\nmonth = dec,\neid = {arxiv:1812.09490},\npages = {arxiv:1812.09490},\narchiveprefix = {arxiv},\neprint = {1812.09490},\nprimaryclass = {cs.cr},\nadsurl = {https://ui.adsabs.harvard.edu/\\#abs/2018arxiv181209490v},\nadsnote = {provided by the sao/nasa astrophysics data system}\n}\nsupported by rosin - ros-industrial quality-assured robot software components. more information: rosin-project.eu\nthis repository was partly funded by rosin redros2-i ftp which received funding from the european union\u2019s horizon 2020 research and innovation programme under the project rosin with the grant agreement no 732287.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000365, "year": null}, {"Unnamed: 0": 1403, "autor": 383, "date": null, "content": "RAWSim-O\nRAWSim-O is a discrete event-based simulation for Robotic Mobile Fulfillment Systems. The intention of the simulation framework is to provide a tool for researching effects of multiple decision problems that occur when running such a system. For this, the framework enables easy extensibility for implementing new decision methods for the different decision problems. Copyright (C) 2017 Marius Merschformann\nFirst impressions\n2D view in action:\n3D view in action:\nQuick start\nOpen RAWSimO.sln with Visual Studio and select RAWSimO.Visualization as the project to execute. Under the \"Instances\" tab press the checkmark button to generate a default instance. Then go to the \"Base Controls\" tab and press the play button. Depending on the instance size and simulation settings instance generation and simulation initialization may take longer. You should see a first simulation running now.\nIn the following video you can find a short tutorial of RAWSim-O's core functionality for a quick start:\nController implementation quick start\nIn the following video you can find a very short tutorial for implementing your own controller logic:\nScreenshots\nImage showing a larger instance being simulated:\nImage showing a multi-level instance being simulated:\nImage showing more detailed information for path planning engines:\nImage showing a heatmap rendered using RAWSim-O that shows the locations robots were at over time:\nDemonstrator video\nA short video of the demonstrator application done with RAWSim-O and vacuum cleaning robots can be found here:\nGnuPlot\nSome of the functionality of RAWSim-O requires an installation of GnuPlot (see http://www.gnuplot.info/). GnuPlot's bin path must be added to the environment path variable.\nLicense\nThis program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\nYou should have received a copy of the GNU General Public License along with this program. If not, see http://www.gnu.org/licenses/.\nPublications\nRAWSim-O has been published here:\nMarius Merschformann, Lin Xie, Hanyi Li: RAWSim-O: A Simulation Framework for Robotic Mobile Fulfillment Systems, Logistics Research (2018), Volume 11, Issue 1, doi:10.23773/2018_8, link\nFurther reading on some projects using RAWSim-O can be found here (let me know, if yours is missing):\nMarius Merschformann, Lin Xie, Daniel Erdmann: Multi-Agent Path Finding with Kinematic Constraints for Robotic Mobile Fulfillment Systems, working paper preprint available at link\nMarius Merschformann, Tim Lamballais, Ren\u00e9 de Koster, Leena Suhl: Decision Rules for Robotic Mobile Fulfillment Systems, Operations Research Perspectives, Volume 6 (2019), doi:10.1016/j.orp.2019.100128, available at link\nMarius Merschformann: Active repositioning of storage units in Robotic Mobile Fulfillment Systems, Selected Papers of the Annual International Conference of the German Operations Research Society (GOR), Freie Universit\u00e4t Berlin, Germany, September 6-8, 2017, doi:10.1007/978-3-319-89920-6_51, link\nCredits\nThis work was created as a part of the RMFS project initiated by Prof. Lin Xie. More information about the project can be found here https://www.researchgate.net/project/automated-robotic-mobile-fulfillment-systems-RMFS and here https://www.leuphana.de/institute/iis/personen/lin-xie.html\nThe work was supported by the Ecopti GmbH (Beijing Hanning ZN Tech) and a scholarship of the International Graduate School - Dynamic Intelligent Systems of the University of Paderborn (https://pace.uni-paderborn.de/pace-phd-programs/igs/)\nContributers of the project\nMarius Merschformann, Lin Xie, Hanyi Li, Tim Lamballais Tessensohn, Daniel Erdmann, Lena Knickmeier, Jonas K\u00f6nig, Maik Herbort, Marcel Grawe\nAlphabet Soup\nSpecial thanks go to the authors of the Alphabet Soup framework that was used as a basis for implementation of RAWSim-O:\nHazard, Christopher J., Peter R. Wurman, and Raffaello D\u2019Andrea. \"Alphabet soup: A testbed for studying resource allocation in multi-vehicle systems.\" Proceedings of the 2006 AAAI Workshop on Auction Mechanisms for Robot Coordination. 2006.\nUsed software:\nThanks go out to the developers of the following software for enabling a more easy implementation of RAWSim-O.\nHelix Toolkit:\nhttps://github.com/helix-toolkit/helix-toolkit\nWriteableBitmapEx\nhttps://writeablebitmapex.codeplex.com/\nEmgu CV\nhttp://www.emgu.com/\nOpen CV\nhttp://opencv.org/\nHidLibrary\nhttps://github.com/mikeobrien/HidLibrary\ndirectshow.net library\nhttp://directshownet.sourceforge.net/\nBlink(1) library\nhttps://github.com/todbot/blink1\nZXing.Net\nhttps://zxingnet.codeplex.com/", "link": "https://github.com/merschformann/RAWSim-O", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "rawsim-o\nrawsim-o is a discrete event-based simulation for robotic mobile fulfillment systems. the intention of the simulation framework is to provide a -----> tool !!!  for researching effects of multiple decision problems that occur when running such a system. for this, the framework enables easy extensibility for implementing new decision methods for the different decision problems. copyright (c) 2017 marius merschformann\nfirst impressions\n2d view in action:\n3d view in action:\nquick start\nopen rawsimo.sln with visual studio and select rawsimo.visualization as the project to execute. under the \"instances\" tab press the checkmark button to generate a default instance. then go to the \"base controls\" tab and press the play button. depending on the instance size and simulation settings instance generation and simulation initialization may take longer. you should see a first simulation running now.\nin the following video you can find a short tutorial of rawsim-o's core functionality for a quick start:\ncontroller implementation quick start\nin the following video you can find a very short tutorial for implementing your own controller logic:\nscreenshots\nimage showing a larger instance being simulated:\nimage showing a multi-level instance being simulated:\nimage showing more detailed information for path planning engines:\nimage showing a heatmap rendered using rawsim-o that shows the locations robots were at over time:\ndemonstrator video\na short video of the demonstrator application done with rawsim-o and vacuum cleaning robots can be found here:\ngnuplot\nsome of the functionality of rawsim-o requires an installation of gnuplot (see http://www.gnuplot.info/). gnuplot's bin path must be added to the environment path variable.\nlicense\nthis program is free software: you can redistribute it and/or modify it under the terms of the gnu general public license as published by the free software foundation, either version 3 of the license, or (at your option) any later version.\nthis program is distributed in the hope that it will be useful, but without any warranty; without even the implied warranty of merchantability or fitness for a particular purpose. see the gnu general public license for more details.\nyou should have received a copy of the gnu general public license along with this program. if not, see http://www.gnu.org/licenses/.\npublications\nrawsim-o has been published here:\nmarius merschformann, lin xie, hanyi li: rawsim-o: a simulation framework for robotic mobile fulfillment systems, logistics research (2018), volume 11, issue 1, doi:10.23773/2018_8, link\nfurther reading on some projects using rawsim-o can be found here (let me know, if yours is missing):\nmarius merschformann, lin xie, daniel erdmann: multi-agent path finding with kinematic constraints for robotic mobile fulfillment systems, working paper preprint available at link\nmarius merschformann, tim lamballais, ren\u00e9 de koster, leena suhl: decision rules for robotic mobile fulfillment systems, operations research perspectives, volume 6 (2019), doi:10.1016/j.orp.2019.100128, available at link\nmarius merschformann: active repositioning of storage units in robotic mobile fulfillment systems, selected papers of the annual international conference of the german operations research society (gor), freie universit\u00e4t berlin, germany, september 6-8, 2017, doi:10.1007/978-3-319-89920-6_51, link\ncredits\nthis work was created as a part of the rmfs project initiated by prof. lin xie. more information about the project can be found here https://www.researchgate.net/project/automated-robotic-mobile-fulfillment-systems-rmfs and here https://www.leuphana.de/institute/iis/personen/lin-xie.html\nthe work was supported by the ecopti gmbh (beijing hanning zn tech) and a scholarship of the international graduate school - dynamic intelligent systems of the university of paderborn (https://pace.uni-paderborn.de/pace-phd-programs/igs/)\ncontributers of the project\nmarius merschformann, lin xie, hanyi li, tim lamballais tessensohn, daniel erdmann, lena knickmeier, jonas k\u00f6nig, maik herbort, marcel grawe\nalphabet soup\nspecial thanks go to the authors of the alphabet soup framework that was used as a basis for implementation of rawsim-o:\nhazard, christopher j., peter r. wurman, and raffaello d\u2019andrea. \"alphabet soup: a testbed for studying resource allocation in multi-vehicle systems.\" proceedings of the 2006 aaai workshop on auction mechanisms for robot coordination. 2006.\nused software:\nthanks go out to the developers of the following software for enabling a more easy implementation of rawsim-o.\nhelix toolkit:\nhttps://github.com/helix-toolkit/helix-toolkit\nwriteablebitmapex\nhttps://writeablebitmapex.codeplex.com/\nemgu cv\nhttp://www.emgu.com/\nopen cv\nhttp://opencv.org/\nhidlibrary\nhttps://github.com/mikeobrien/hidlibrary\ndirectshow.net library\nhttp://directshownet.sourceforge.net/\nblink(1) library\nhttps://github.com/todbot/blink1\nzxing.net\nhttps://zxingnet.codeplex.com/", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000383, "year": null}, {"Unnamed: 0": 1414, "autor": 394, "date": null, "content": "BowlerStudio\nDownload Latest\nUbuntu 18.04\nsudo add-apt-repository ppa:mad-hephaestus/commonwealthrobotics -y\nsudo apt-get update\nsudo apt-get install bowlerstudio\nbowlerstudio\nDebian\nsudo apt-get install software-properties-common python-software-properties\nsudo add-apt-repository \"deb http://ppa.launchpad.net/mad-hephaestus/commonwealthrobotics/ubuntu xenial main\" -y\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 71EA898B\nsudo add-apt-repository \"deb http://ppa.launchpad.net/mad-hephaestus/commonwealthrobotics/ubuntu bionic main\" -y\nsudo apt-get update\nsudo apt-get install bowlerstudio\nWhat is BowlerStudio?\nBowlerStudio assists you in every step of a robotics project from concept to completion. Tools enable users to:\nInterface with motors, sensors, and other electronics hardware.\nCreate 3d models for fabrication, and for simulating the motions of your project.\nGive your robot sight with image processing on camera feeds and Kinect data.\nOperate 3d printers and other CNC machines.\nCreate custom graphical user interfaces to control yours robots.\nCreate and control animations.\n==========\nThe Nitty-Gritty Version\nBowlerStudio Robotics development IDE is based on\nJCSG\nJava-Bowler\nJinput\nmotej\nUsb4Java\nNrJavaSerial\nBlueCove\nJavaFX 8 3d engine.\nJBullet physics engine ported from the popular Bullet C++ framework.\nJetty Web Framework\nBowlerStudio is a device manager, scripting engine, CAD package, and simulation tool all in one application. A user can develop the kinematic of an robot arm using the D-H parameters-based automatic kinematics engine. With this kinematics model, the user can then generate the CAD for new unique parts to match the kinematic model. The user can then export the model to an STL, and connect a Bowler 3d printer to BowlerStudio. The printer can print out the part (using the newly generated STL) while the user connects a DyIO and begins testing the servos with the kinematics model. When the print is done, the user can assemble the arm with the tested servos and run the model again to control the arm with Cartesian instructions. Once this is complete, the user can then attach a wiimote to train the robot arm through a set of tasks, recording them with the animation framework built into BowlerStudio. To be sure the arm is moving to the right place, the user can attach a webcam to the end and use OpenCV to verify the arm's position, or use the arm (in conjunction with the webcam with OpenCV enabled) to track and grab objects (IE \"eye-in-hand\" tracking).\nEvery step of this task can be performed from within BowlerStudio!\nLet's go through the main features:\nScripting With Gist\nAbout scripts and Gist\nScripts are bits of code that BowlerStudio can load and run. BowlerStudio allows you to open a local file and run it, but BowlerStudio is most powerful when the code lives on Github Gist (a code snippet hosting service from Github). Simply give BowlerStudio the URL for a Gist you want to load and execute. Gists can be selected and edited using the built in browser, or inline in another script using the Gist ID.\nJava and Groovy\nBowlerStudio can load and run scripts written in Java, Groovy, and Python. Which parser is used is determined by the file extension. Files that end in .java or .groovy will be run through the Groovy compiler. These Groovy scripts are compiled fully and run directly in the JVM. This means they will execute at full speed, just like a regular application.\nPython\nPython, on the other hand, by virtue of its structure, will generally execute much slower then Java. With the reduction in speed you get lots of flexibility and a clean and easy to understand syntax. The python code can also create and return objects to BowlerStudio (such as CAD CSG objects, or UI Tabs).\nReturn Objects\nA script can return a few object types that will be handled by BowlerStudio: Objects of type \"CSG\" and \"MeshView\" will be added to the 3d display. If a transform is added to either of these and updated by a script the user can move an object in the 3d view. Objects of type \"Tab\" will be added to the Tabmanager and displayed in BowlerStudio. This is an easy way to make control panels or state displays and monitors. Objects of type \"BowlerAbstractDevice\" (or any subclass) will be added to the connections manager and made available to all other scripts. These can be external devices or virtual communication bus devices. A bowler Server/Client pair is the preferred mechanism for communication between scripts.\nDevice Access\nAll scripts are passed all connected devices by name when the script is run. The name associated with the device in the connections tab is the name to use in the script to access that device. A script can also create and return a device (EG to connect to a specific device in order to give that device a specific name). The device returned will be added to the list of available devices and be available to other scripts. A user can define their own devices to facilitate communication between scripts.\nBowler Devices\nBowlerDevices (such as the Neuron Robotics DyIO) are devices that implement the Bowler Communication System. BowlerDevices are servers of features to applications. The DyIO, for example, is a server of microcontroller features. These devices implement a micro domain-specific language as a protocol. This language synchronizes the device with the application by building the communication system at runtime using a namespace/RPC system. As such, the device is treated as a collection of namespaces. Each namespace has a set of RPCs for communication, some synchronous (IE they are application initiated) and some asynchronous (IE device initiated). In addition, each RPC has full method introspection. This means that all parameters and datatypes (including how to pack and interpret all packets) are able to be queried over the communication system. A Library need only implement the core packet parser and every device will assemble its own communication layer live.\nCameras\nCameras can be connected to Bowler Studio using one of 3 supported drivers: OpenCV's native Java bindings are provided by installing OpenCV using your OS specific installer (unfortunately not available for Mac at this time). JavaCV is a meta-library that adds support for a wide range of camera device integrations and image processing options. This is a big project and integrated now with a full scripting system. CHDK-PTP-Java is a Java library that adds support for SLR Cannon cameras. CHDK is a camera OS that makes the cameras features available over USB, and the Java library makes those images and controls available to Java and or scripting engine.\nImage processing\nImage processing is provided be a variety of libraries included in this application. OpenCV and ARToolkit are some of the most widely used image recognition libraries available, and now you can use them directly from our scripting environment!\nKinematics Engine\nThe Bowler Kinematics engine is based on D-H parameters- the standard mathematical definition of kinematics chains. This standard simplifies the calculation process and allows us to run forward kinematics equations for arbitrary defined chains in real time. For inverse kinematics, a collection of kinematic engines are available for optimizing for speed and accuracy.\nReal-Time validated\nFor applications where real time is required, there is no need to leave the Bowler OS. The Bowler Java stack has been validated as real-time capable when run on JamaicaVM (the real-time Java implementation). The Bowler Kinematics engine is run in a real-time loop for neurosurgery applications (Bowler and Java are fast and reliable enough for brain surgery!)\n3D CAD\nUsers can write scripts using Java, Groovy or Python to generate CSG style CAD. This programmatic CAD engine JCSG was inspired by OpenSCAD, but implemented in pure Java with JavaFX visualizations. JCSG implements all basic shape generation and manipulation, using Java's library packaging and distribution for libraries of parts. Gist hosting of parts can also simplify sharing and loading of dependent libraries.\nVirtualization\nVirtual links\nVirtual PID devices allow users to make applications that can be simulated with virtual links before ever connecting a real device. Users can also use the PIDLab to learn about designing and implementing PID controllers with a built in motor physics simulation.\nVirtual Camera\nComing soon! Soon users will be able to interact with the 3D environment camera from their code just like a real camera. Users will be able to manipulate it just like it was another 3d object.\nVirtual Sensors\nComing soon! Soon we will be able to provide virtual sensor devices, simulating real world sensors within the 3d environment.\nMake A Contribution\nBowlerStudio is an open source project and is always looking for help with both the application code and the tutorials content.\nJava Contributions\nIf you are a Java developer, skip ahead to The Build Instructions. The application is a light plugin framework for UI, 3D and Device interaction. You can look at this repository for issues.\nAdding Tutorials\nAll of the content for BowlerStudio Tutorials is housed on our Neuronrobotics.github.io web page. Fork that repository and make contributions based on the README.md file in the root of the repository. To merge the changes into the main website, send a pull request with your changes to official repository.\nExamples of tutorials that need to be added are A simple Java Programming Introduction. This tutorial set would go through the basic syntax of java and what all of the symbols mean and how to use them.\nAnother example of a tutorial that could be added is one for JavaCad Cheatsheet where you would add a 'cheat sheet' of commands to use in the JavaCad system.\nIf a tutorial is missing and not described as needed by an issue, feel free to add additional issues.\nHow to Build BowlerStudio\nRequirements\nJava >= 1.8\nInternet connection (dependencies are downloaded automatically)\nIDE: Gradle Plugin (not necessary for command line usage)\nOpenCV 2.4.9 installed and configured in the library paths (OS installers preferred)\nThe Binary installer for BowlerStudio must be installed before develpment.\nIDE\nOpen the BowlerStudio Gradle project in your favorite IDE (tested with NetBeans 7.4) and build by calling the assemble task.\nCommand Line\nNavigate to the Gradle project (e.g., path/to/BowlerStudio) and enter the following command:\nBash (Linux/OS X/Cygwin/other Unix-like shell)\nUbuntu 18.04 Dependencies\nsudo apt install curl\nUbuntu 16.04 Dependencies\nsudo apt-get install git gradle\nUbuntu 14.04, install extra dependencies\nsudo add-apt-repository \"deb http://ppa.launchpad.net/mad-hephaestus/commonwealthrobotics/ubuntu xenial main\" -y\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 71EA898B\nsudo add-apt-repository \"deb http://us.archive.ubuntu.com/ubuntu/ trusty universe multiverse\"\nsudo apt-get update -qq\nsudo apt-get install -y --force-yes gradle\nAll Unix\nbowlerstudio # run this to get the JVM downloaded into $HOME/bin/java8/jre\ngit clone https://github.com/CommonWealthRobotics/BowlerStudio.git\nJAVA_HOME=$HOME/bin/java8/jre\nexport JAVA_HOME\ncd BowlerStudio\ngit checkout development\ngit submodule update --init --recursive\ngit submodule update --recursive\n./gradlew shadowJar\n$JAVA_HOME/bin/java -jar build/libs/BowlerStudio*.jar\nNow you can use the Eclipse Marketplace to install the Gradle Plugin\nWindows (CMD)\ngradlew jar\nHistory\nBowler Studio began Feb 11, 2015 with the goal of making a robotics IDE.", "link": "https://github.com/CommonWealthRobotics/BowlerStudio", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "bowlerstudio\ndownload latest\nubuntu 18.04\nsudo add-apt-repository ppa:mad-hephaestus/commonwealthrobotics -y\nsudo apt-get update\nsudo apt-get install bowlerstudio\nbowlerstudio\ndebian\nsudo apt-get install software-properties-common python-software-properties\nsudo add-apt-repository \"deb http://ppa.launchpad.net/mad-hephaestus/commonwealthrobotics/ubuntu xenial main\" -y\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 71ea898b\nsudo add-apt-repository \"deb http://ppa.launchpad.net/mad-hephaestus/commonwealthrobotics/ubuntu bionic main\" -y\nsudo apt-get update\nsudo apt-get install bowlerstudio\nwhat is bowlerstudio?\nbowlerstudio assists you in every step of a robotics project from concept to completion. tools enable users to:\ninterface with motors, sensors, and other electronics hardware.\ncreate 3d models for fabrication, and for simulating the motions of your project.\ngive your robot sight with image processing on camera feeds and kinect data.\noperate 3d printers and other cnc machines.\ncreate custom graphical user interfaces to control yours robots.\ncreate and control animations.\n==========\nthe nitty-gritty version\nbowlerstudio robotics development ide is based on\njcsg\njava-bowler\njinput\nmotej\nusb4java\nnrjavaserial\nbluecove\njavafx 8 3d engine.\njbullet physics engine ported from the popular bullet c++ framework.\njetty web framework\nbowlerstudio is a device manager, scripting engine, cad package, and simulation -----> tool !!!  all in one application. a user can develop the kinematic of an robot arm using the d-h parameters-based automatic kinematics engine. with this kinematics model, the user can then generate the cad for new unique parts to match the kinematic model. the user can then export the model to an stl, and connect a bowler 3d printer to bowlerstudio. the printer can print out the part (using the newly generated stl) while the user connects a dyio and begins testing the servos with the kinematics model. when the print is done, the user can assemble the arm with the tested servos and run the model again to control the arm with cartesian instructions. once this is complete, the user can then attach a wiimote to train the robot arm through a set of tasks, recording them with the animation framework built into bowlerstudio. to be sure the arm is moving to the right place, the user can attach a webcam to the end and use opencv to verify the arm's position, or use the arm (in conjunction with the webcam with opencv enabled) to track and grab objects (ie \"eye-in-hand\" tracking).\nevery step of this task can be performed from within bowlerstudio!\nlet's go through the main features:\nscripting with gist\nabout scripts and gist\nscripts are bits of code that bowlerstudio can load and run. bowlerstudio allows you to open a local file and run it, but bowlerstudio is most powerful when the code lives on github gist (a code snippet hosting service from github). simply give bowlerstudio the url for a gist you want to load and execute. gists can be selected and edited using the built in browser, or inline in another script using the gist id.\njava and groovy\nbowlerstudio can load and run scripts written in java, groovy, and python. which parser is used is determined by the file extension. files that end in .java or .groovy will be run through the groovy compiler. these groovy scripts are compiled fully and run directly in the jvm. this means they will execute at full speed, just like a regular application.\npython\npython, on the other hand, by virtue of its structure, will generally execute much slower then java. with the reduction in speed you get lots of flexibility and a clean and easy to understand syntax. the python code can also create and return objects to bowlerstudio (such as cad csg objects, or ui tabs).\nreturn objects\na script can return a few object types that will be handled by bowlerstudio: objects of type \"csg\" and \"meshview\" will be added to the 3d display. if a transform is added to either of these and updated by a script the user can move an object in the 3d view. objects of type \"tab\" will be added to the tabmanager and displayed in bowlerstudio. this is an easy way to make control panels or state displays and monitors. objects of type \"bowlerabstractdevice\" (or any subclass) will be added to the connections manager and made available to all other scripts. these can be external devices or virtual communication bus devices. a bowler server/client pair is the preferred mechanism for communication between scripts.\ndevice access\nall scripts are passed all connected devices by name when the script is run. the name associated with the device in the connections tab is the name to use in the script to access that device. a script can also create and return a device (eg to connect to a specific device in order to give that device a specific name). the device returned will be added to the list of available devices and be available to other scripts. a user can define their own devices to facilitate communication between scripts.\nbowler devices\nbowlerdevices (such as the neuron robotics dyio) are devices that implement the bowler communication system. bowlerdevices are servers of features to applications. the dyio, for example, is a server of microcontroller features. these devices implement a micro domain-specific language as a protocol. this language synchronizes the device with the application by building the communication system at runtime using a namespace/rpc system. as such, the device is treated as a collection of namespaces. each namespace has a set of rpcs for communication, some synchronous (ie they are application initiated) and some asynchronous (ie device initiated). in addition, each rpc has full method introspection. this means that all parameters and datatypes (including how to pack and interpret all packets) are able to be queried over the communication system. a library need only implement the core packet parser and every device will assemble its own communication layer live.\ncameras\ncameras can be connected to bowler studio using one of 3 supported drivers: opencv's native java bindings are provided by installing opencv using your os specific installer (unfortunately not available for mac at this time). javacv is a meta-library that adds support for a wide range of camera device integrations and image processing options. this is a big project and integrated now with a full scripting system. chdk-ptp-java is a java library that adds support for slr cannon cameras. chdk is a camera os that makes the cameras features available over usb, and the java library makes those images and controls available to java and or scripting engine.\nimage processing\nimage processing is provided be a variety of libraries included in this application. opencv and artoolkit are some of the most widely used image recognition libraries available, and now you can use them directly from our scripting environment!\nkinematics engine\nthe bowler kinematics engine is based on d-h parameters- the standard mathematical definition of kinematics chains. this standard simplifies the calculation process and allows us to run forward kinematics equations for arbitrary defined chains in real time. for inverse kinematics, a collection of kinematic engines are available for optimizing for speed and accuracy.\nreal-time validated\nfor applications where real time is required, there is no need to leave the bowler os. the bowler java stack has been validated as real-time capable when run on jamaicavm (the real-time java implementation). the bowler kinematics engine is run in a real-time loop for neurosurgery applications (bowler and java are fast and reliable enough for brain surgery!)\n3d cad\nusers can write scripts using java, groovy or python to generate csg style cad. this programmatic cad engine jcsg was inspired by openscad, but implemented in pure java with javafx visualizations. jcsg implements all basic shape generation and manipulation, using java's library packaging and distribution for libraries of parts. gist hosting of parts can also simplify sharing and loading of dependent libraries.\nvirtualization\nvirtual links\nvirtual pid devices allow users to make applications that can be simulated with virtual links before ever connecting a real device. users can also use the pidlab to learn about designing and implementing pid controllers with a built in motor physics simulation.\nvirtual camera\ncoming soon! soon users will be able to interact with the 3d environment camera from their code just like a real camera. users will be able to manipulate it just like it was another 3d object.\nvirtual sensors\ncoming soon! soon we will be able to provide virtual sensor devices, simulating real world sensors within the 3d environment.\nmake a contribution\nbowlerstudio is an open source project and is always looking for help with both the application code and the tutorials content.\njava contributions\nif you are a java developer, skip ahead to the build instructions. the application is a light plugin framework for ui, 3d and device interaction. you can look at this repository for issues.\nadding tutorials\nall of the content for bowlerstudio tutorials is housed on our neuronrobotics.github.io web page. fork that repository and make contributions based on the readme.md file in the root of the repository. to merge the changes into the main website, send a pull request with your changes to official repository.\nexamples of tutorials that need to be added are a simple java programming introduction. this tutorial set would go through the basic syntax of java and what all of the symbols mean and how to use them.\nanother example of a tutorial that could be added is one for javacad cheatsheet where you would add a 'cheat sheet' of commands to use in the javacad system.\nif a tutorial is missing and not described as needed by an issue, feel free to add additional issues.\nhow to build bowlerstudio\nrequirements\njava >= 1.8\ninternet connection (dependencies are downloaded automatically)\nide: gradle plugin (not necessary for command line usage)\nopencv 2.4.9 installed and configured in the library paths (os installers preferred)\nthe binary installer for bowlerstudio must be installed before develpment.\nide\nopen the bowlerstudio gradle project in your favorite ide (tested with netbeans 7.4) and build by calling the assemble task.\ncommand line\nnavigate to the gradle project (e.g., path/to/bowlerstudio) and enter the following command:\nbash (linux/os x/cygwin/other unix-like shell)\nubuntu 18.04 dependencies\nsudo apt install curl\nubuntu 16.04 dependencies\nsudo apt-get install git gradle\nubuntu 14.04, install extra dependencies\nsudo add-apt-repository \"deb http://ppa.launchpad.net/mad-hephaestus/commonwealthrobotics/ubuntu xenial main\" -y\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 71ea898b\nsudo add-apt-repository \"deb http://us.archive.ubuntu.com/ubuntu/ trusty universe multiverse\"\nsudo apt-get update -qq\nsudo apt-get install -y --force-yes gradle\nall unix\nbowlerstudio # run this to get the jvm downloaded into $home/bin/java8/jre\ngit clone https://github.com/commonwealthrobotics/bowlerstudio.git\njava_home=$home/bin/java8/jre\nexport java_home\ncd bowlerstudio\ngit checkout development\ngit submodule update --init --recursive\ngit submodule update --recursive\n./gradlew shadowjar\n$java_home/bin/java -jar build/libs/bowlerstudio*.jar\nnow you can use the eclipse marketplace to install the gradle plugin\nwindows (cmd)\ngradlew jar\nhistory\nbowler studio began feb 11, 2015 with the goal of making a robotics ide.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000394, "year": null}, {"Unnamed: 0": 1416, "autor": 396, "date": null, "content": "UWSim-NET: UWSim with Network Simulator\nThe new version of UWSim intregates a Network Simulator to be used along with the dccomms API. This simulator uses the NS3 libraries and the AquaSimNG as a NS3 module. The documentation related to simulate communications is a work in progress and will be included in the Wiki as soon as we can (please, be patience).\nWiki\nhttp://www.irs.uji.es/uwsim/wiki/index.php?title=Main_Page\nCitations\nPrats, M.; Perez, J.; Fernandez, J.J.; Sanz, P.J., \"An open source tool for simulation and supervision of underwater intervention missions\", 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 2577-2582, 7-12 Oct. 2012\nCentelles, D.; Soriano-Asensi, A.; Mart\u00ed, J.V.; Mar\u00edn, R.; Sanz, P.J. Underwater Wireless Communications for Cooperative Robotics with UWSim-NET. Appl. Sci. 2019, 9, 3526.\nIssues\nException GeographicLib::GeographicErr when starting uwsim\nIf you are getting the following error output when executing rosrun uwsim uwsim:\n...\ndata/objects/gun.osg\nStarting UWSim...\nLoading SimulatedDevices plugin: 'AcousticCommsDevice_Factory'\nLoading SimulatedDevices plugin: 'CustomCommsDevice_Factory'\nLoading SimulatedDevices plugin: 'DredgeTool_Factory'\nLoading SimulatedDevices plugin: 'ForceSensor_Factory'\nLoading SimulatedDevices plugin: 'SimDev_Echo_Factory'\n. Setting localized world: 7.1e-05s\nterminate called after throwing an instance of 'GeographicLib::GeographicErr'\nwhat(): File not readable /usr/share/GeographicLib/geoids/egm96-5.pgm\n/opt/ros/melodic/lib/uwsim/uwsim: line 23: 1452 Aborted (core dumped) rosrun uwsim uwsim_binary --dataPath ~/.uwsim/data $@\nRun the following commands to fix it:\nwget https://raw.githubusercontent.com/mavlink/mavros/master/mavros/scripts/install_geographiclib_datasets.sh\nchmod u+x install_geographiclib_datasets.sh\nsudo ./install_geographiclib_datasets.sh", "link": "https://github.com/uji-ros-pkg/underwater_simulation", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "uwsim-net: uwsim with network simulator\nthe new version of uwsim intregates a network simulator to be used along with the dccomms api. this simulator uses the ns3 libraries and the aquasimng as a ns3 module. the documentation related to simulate communications is a work in progress and will be included in the wiki as soon as we can (please, be patience).\nwiki\nhttp://www.irs.uji.es/uwsim/wiki/index.php?title=main_page\ncitations\nprats, m.; perez, j.; fernandez, j.j.; sanz, p.j., \"an open source -----> tool !!!  for simulation and supervision of underwater intervention missions\", 2012 ieee/rsj international conference on intelligent robots and systems (iros), pp. 2577-2582, 7-12 oct. 2012\ncentelles, d.; soriano-asensi, a.; mart\u00ed, j.v.; mar\u00edn, r.; sanz, p.j. underwater wireless communications for cooperative robotics with uwsim-net. appl. sci. 2019, 9, 3526.\nissues\nexception geographiclib::geographicerr when starting uwsim\nif you are getting the following error output when executing rosrun uwsim uwsim:\n...\ndata/objects/gun.osg\nstarting uwsim...\nloading simulateddevices plugin: 'acousticcommsdevice_factory'\nloading simulateddevices plugin: 'customcommsdevice_factory'\nloading simulateddevices plugin: 'dredgetool_factory'\nloading simulateddevices plugin: 'forcesensor_factory'\nloading simulateddevices plugin: 'simdev_echo_factory'\n. setting localized world: 7.1e-05s\nterminate called after throwing an instance of 'geographiclib::geographicerr'\nwhat(): file not readable /usr/share/geographiclib/geoids/egm96-5.pgm\n/opt/ros/melodic/lib/uwsim/uwsim: line 23: 1452 aborted (core dumped) rosrun uwsim uwsim_binary --datapath ~/.uwsim/data $@\nrun the following commands to fix it:\nwget https://raw.githubusercontent.com/mavlink/mavros/master/mavros/scripts/install_geographiclib_datasets.sh\nchmod u+x install_geographiclib_datasets.sh\nsudo ./install_geographiclib_datasets.sh", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000396, "year": null}, {"Unnamed: 0": 1419, "autor": 399, "date": null, "content": "Full-featured UAVCAN stack in Python\nPyUAVCAN is a full-featured implementation of the UAVCAN protocol stack intended for non-embedded, user-facing applications such as GUI software, diagnostic tools, automation scripts, prototypes, and various R&D cases.\nPyUAVCAN aims to support all features and transport layers of UAVCAN, be portable across all major platforms supporting Python, and be extensible to permit low-effort experimentation and testing of new protocol capabilities.\nIt is designed to support GNU/Linux, MS Windows, and macOS as first-class target platforms. However, the library does not rely on any platform-specific capabilities, so it should be usable with other systems as well.\nUAVCAN is an open technology for real-time intravehicular distributed computing and communication based on modern networking standards (Ethernet, CAN FD, etc.). The acronym UAVCAN stands for Uncomplicated Application-level Vehicular Computing And Networking.\nREAD THE DOCS: pyuavcan.readthedocs.io\nAsk questions: forum.uavcan.org\nSee also: Yakut -- a CLI tool for diagnostics and management of UAVCAN networks built on top of PyUAVCAN.", "link": "https://github.com/UAVCAN/pyuavcan", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "full-featured uavcan stack in python\npyuavcan is a full-featured implementation of the uavcan protocol stack intended for non-embedded, user-facing applications such as gui software, diagnostic tools, automation scripts, prototypes, and various r&d cases.\npyuavcan aims to support all features and transport layers of uavcan, be portable across all major platforms supporting python, and be extensible to permit low-effort experimentation and testing of new protocol capabilities.\nit is designed to support gnu/linux, ms windows, and macos as first-class target platforms. however, the library does not rely on any platform-specific capabilities, so it should be usable with other systems as well.\nuavcan is an open technology for real-time intravehicular distributed computing and communication based on modern networking standards (ethernet, can fd, etc.). the acronym uavcan stands for uncomplicated application-level vehicular computing and networking.\nread the docs: pyuavcan.readthedocs.io\nask questions: forum.uavcan.org\nsee also: yakut -- a cli -----> tool !!!  for diagnostics and management of uavcan networks built on top of pyuavcan.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000399, "year": null}, {"Unnamed: 0": 1436, "autor": 416, "date": null, "content": "Plankton\nOpen source simulator for maritime robotics researchers\nPlankton is an initiative aiming at simplifying robotics research in the maritime domain. It is a maritime environment simulator with an open source core, and a special focus on ease of use, stability, speed and sim to real. Its middleware of choice is ROS 2.\nWe intend to build a sustainable and open source core simulator for maritime robotics research. This core will support flexible specification of sensor suites and environmental conditions.\nThis project benefits from great open source advances in the simulation domain, mainly ROS, Gazebo and its plugin UUV Simulator. It is also built on data characterizing the needs of robotics researchers in terms of simulation. We gathered these data in our wiki, including the results of our own survey on simulation needs.\nRoadmap\nThe first iteration of the projet is built from UUV Simulator and gazebo 9. We made UUV Simulator compatible with ROS2. In the following months, we intend to improve the performance (speed) of the simulator, and to test different alternatives to gazebo 9 or 11. We will choose the best simulation framework according to our users present and future needs.\nWe released a beta version of UUV Simulator for ROS 2 in September. The last release in December 2020 made our simulator Plankton compatible with ROS 2 Foxy Fitzroy.\nContributing\nYou can contribute by reporting bugs, proposing new features, improving documentation, contributing to code.\nReporting bugs\nUse our issue section on GitHub. Please check before that the issue is not already reported.\nProposing new features\nPlease check first the list of feature requests. If it is not there and you think is a feature that might be interesting for users, please submit your request as a new issue.\nImproving documentation\nIf you feel something is missing in the documentation, please don't hesitate to open an issue to let us know. Even better, if you think you can improve it yourself, it would be a great contribution to the community!\nContributing to code\nSo you are considering making a code contribution, great! We love to have contributions from the community. Before starting hands-on on coding, please check out our issue board to see if we are already working on that, it would be a pity putting an effort into something just to discover that someone else was already working on that. In case of doubt or to discuss how to proceed, please contact one of us (or send an email to loic.mougeolle@naval-group.com).\nInstallation\nPlankton currently supports:\nROS 2 Foxy with Gazebo 9 or Gazebo 11 and Ubuntu 20.04\nROS 2 Eloquent, Ubuntu 18.04 and Gazebo 9\n1. Install ROS 2 Foxy\nIf you don\u2019t have ROS 2 Foxy installed, follow the instructions below and prefer to install the ros-foxy-desktop package: https://index.ros.org/doc/ros2/Installation/Foxy/Linux-Install-Debians/\n2. Install Gazebo 11\nIf you don't have Gazebo 11 installed, open a terminal and follow the steps below:\nsudo sh -c 'echo \"deb http://packages.osrfoundation.org/gazebo/ubuntu-stable `lsb_release -cs` main\" > /etc/apt/sources.list.d/gazebo-stable.list'\nwget https://packages.osrfoundation.org/gazebo.key -O - | sudo apt-key add -\nsudo apt-get update\nsudo apt-get install gazebo11\nsudo apt-get install libgazebo11-dev\nYou can write gazebo in a terminal to make sure that gazebo runs correctly. Write gazebo --version to ensure that the version number is 11.X.\nFor more information about the installation of Gazebo, refer to the official installation documentation (beware that the default installation command will not install the version 11): http://gazebosim.org/tutorials?tut=install_ubuntu&cat=install\n3. Install the ros packages for gazebo\nWrite in a terminal:\nsudo apt install ros-foxy-gazebo-ros-pkgs\nSee http://gazebosim.org/tutorials?tut=ros2_installing&cat=connect_ros for more detailed information about gazebo and ROS 2 connection.\n4. Build the Plankton plugin\nIf you don\u2019t have a ROS 2 workspace yet, create a workspace that will contain your projects. In this installation guide, we choose an original name for this directory, ros2_ws:\nmkdir -p ~/ros2_ws/src\nMove to the src directory:\ncd ~/ros2_ws/src\nMake sure git is installed:\nsudo apt install git\nNow, clone the Plankton repository:\ngit clone https://www.github.com/Liquid-ai/Plankton.git\nAt this point, you need to source 2 different files described below to configure ROS 2 and Gazebo environment variables:\nFor ROS 2 variables\nsource /opt/ros/foxy/setup.bash\nFor Gazebo\nsource /usr/share/gazebo/setup.sh\nIn order to download missing packages, install ROS dependency manager. Write the commands below in a terminal:\nsudo apt install python3-rosdep\nsudo rosdep init\nrosdep update\nBrowse to the root of your workspace and check for missing dependencies:\ncd ~/ros2_ws/\nrosdep install -i --from-path src --rosdistro foxy -y\nInstall Colcon, the build tool system:\nsudo apt install python3-colcon-common-extensions\nBuild the Plankton repository:\ncolcon build --packages-up-to plankton\nSource the file for your installation workspace (change the path accordingly)\nsource $HOME/ros2_ws/install/setup.bash\nNext step is\u2026 No next step, you are already done! If everything went well, you should be able to run example cases.\n5. Test the package\nNote: Every time you open a new terminal, you need to source 3 different files described below to configure ROS 2 and Gazebo environment variables. Write the following each time you start a new terminal to deal with ROS 2 / Gazebo stuff, or prefer to add them at the end of your .bashrc file with gedit ~/.bashrc. For the latter, don\u2019t forget to source your .bashrc to enforce the update after saving these changes, or open a fresh terminal.\nFor ROS 2 variables\nsource /opt/ros/foxy/setup.bash\nFor your installation workspace (change the path accordingly)\nsource $HOME/ros2_ws/install/setup.bash\nFor Gazebo\nsource /usr/share/gazebo/setup.sh\nLet's start testing Plankton. Open a new terminal (don\u2019t forget to source ROS 2 / Gazebo files if necessary) and write as below to open a gazebo world:\nros2 launch uuv_gazebo_worlds ocean_waves.launch\nOpen again a new terminal (don't forget to source ROS 2 / Gazebo files if necessary), and spawn the rexrov robot:\nros2 launch uuv_descriptions upload_rexrov.launch mode:=default x:=0 y:=0 z:=-20 namespace:=rexrov\nAdd a joystick node to control it remotely (new terminal needed) :\nros2 launch uuv_control_cascaded_pid joy_velocity.launch uuv_name:=rexrov model_name:=rexrov joy_id:=0\nHere, joy_id represents your joystick id, defined in your OS. You can determine what is your id by installing sudo apt-get install jstest-gtk and running it.\nA more complete documentation of the UUV Simulator features is available here:\nhttps://uuvsimulator.github.io/\nBeware that the UUV Simulator documentation is written for ROS 1, so you will have to make adjustments on given commands to make it work in Plankton.\nFAQ\nLogging and print information are not displayed in the terminal\nIt is a problem affecting eloquent and launch files. Prefix your ros2 launch command with stdbuf -o L. If you are using python launch file, you can launch your node with the emulate_tty argument:\nNode(\npackage='package_name',\nnode_executable='package_exec',\noutput='screen',\nemulate_tty=True,\n)\nThe Gazebo worlds show a black screen when using virtual machine\nTry to add the following line to your ~/.bashrc file\nexport LIBGL_ALWAYS_SOFTWARE=1\nExternal contributions\nPort of the ECA A9 AUV, initially developed for UUV Simulator, to Plankton: https://github.com/GSO-soslab/eca_a9_plankton\nLicense\nPlankton is distributed under Apache License version 2.0", "link": "https://github.com/Liquid-ai/Plankton", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "plankton\nopen source simulator for maritime robotics researchers\nplankton is an initiative aiming at simplifying robotics research in the maritime domain. it is a maritime environment simulator with an open source core, and a special focus on ease of use, stability, speed and sim to real. its middleware of choice is ros 2.\nwe intend to build a sustainable and open source core simulator for maritime robotics research. this core will support flexible specification of sensor suites and environmental conditions.\nthis project benefits from great open source advances in the simulation domain, mainly ros, gazebo and its plugin uuv simulator. it is also built on data characterizing the needs of robotics researchers in terms of simulation. we gathered these data in our wiki, including the results of our own survey on simulation needs.\nroadmap\nthe first iteration of the projet is built from uuv simulator and gazebo 9. we made uuv simulator compatible with ros2. in the following months, we intend to improve the performance (speed) of the simulator, and to test different alternatives to gazebo 9 or 11. we will choose the best simulation framework according to our users present and future needs.\nwe released a beta version of uuv simulator for ros 2 in september. the last release in december 2020 made our simulator plankton compatible with ros 2 foxy fitzroy.\ncontributing\nyou can contribute by reporting bugs, proposing new features, improving documentation, contributing to code.\nreporting bugs\nuse our issue section on github. please check before that the issue is not already reported.\nproposing new features\nplease check first the list of feature requests. if it is not there and you think is a feature that might be interesting for users, please submit your request as a new issue.\nimproving documentation\nif you feel something is missing in the documentation, please don't hesitate to open an issue to let us know. even better, if you think you can improve it yourself, it would be a great contribution to the community!\ncontributing to code\nso you are considering making a code contribution, great! we love to have contributions from the community. before starting hands-on on coding, please check out our issue board to see if we are already working on that, it would be a pity putting an effort into something just to discover that someone else was already working on that. in case of doubt or to discuss how to proceed, please contact one of us (or send an email to loic.mougeolle@naval-group.com).\ninstallation\nplankton currently supports:\nros 2 foxy with gazebo 9 or gazebo 11 and ubuntu 20.04\nros 2 eloquent, ubuntu 18.04 and gazebo 9\n1. install ros 2 foxy\nif you don\u2019t have ros 2 foxy installed, follow the instructions below and prefer to install the ros-foxy-desktop package: https://index.ros.org/doc/ros2/installation/foxy/linux-install-debians/\n2. install gazebo 11\nif you don't have gazebo 11 installed, open a terminal and follow the steps below:\nsudo sh -c 'echo \"deb http://packages.osrfoundation.org/gazebo/ubuntu-stable `lsb_release -cs` main\" > /etc/apt/sources.list.d/gazebo-stable.list'\nwget https://packages.osrfoundation.org/gazebo.key -o - | sudo apt-key add -\nsudo apt-get update\nsudo apt-get install gazebo11\nsudo apt-get install libgazebo11-dev\nyou can write gazebo in a terminal to make sure that gazebo runs correctly. write gazebo --version to ensure that the version number is 11.x.\nfor more information about the installation of gazebo, refer to the official installation documentation (beware that the default installation command will not install the version 11): http://gazebosim.org/tutorials?tut=install_ubuntu&cat=install\n3. install the ros packages for gazebo\nwrite in a terminal:\nsudo apt install ros-foxy-gazebo-ros-pkgs\nsee http://gazebosim.org/tutorials?tut=ros2_installing&cat=connect_ros for more detailed information about gazebo and ros 2 connection.\n4. build the plankton plugin\nif you don\u2019t have a ros 2 workspace yet, create a workspace that will contain your projects. in this installation guide, we choose an original name for this directory, ros2_ws:\nmkdir -p ~/ros2_ws/src\nmove to the src directory:\ncd ~/ros2_ws/src\nmake sure git is installed:\nsudo apt install git\nnow, clone the plankton repository:\ngit clone https://www.github.com/liquid-ai/plankton.git\nat this point, you need to source 2 different files described below to configure ros 2 and gazebo environment variables:\nfor ros 2 variables\nsource /opt/ros/foxy/setup.bash\nfor gazebo\nsource /usr/share/gazebo/setup.sh\nin order to download missing packages, install ros dependency manager. write the commands below in a terminal:\nsudo apt install python3-rosdep\nsudo rosdep init\nrosdep update\nbrowse to the root of your workspace and check for missing dependencies:\ncd ~/ros2_ws/\nrosdep install -i --from-path src --rosdistro foxy -y\ninstall colcon, the build -----> tool !!!  system:\nsudo apt install python3-colcon-common-extensions\nbuild the plankton repository:\ncolcon build --packages-up-to plankton\nsource the file for your installation workspace (change the path accordingly)\nsource $home/ros2_ws/install/setup.bash\nnext step is\u2026 no next step, you are already done! if everything went well, you should be able to run example cases.\n5. test the package\nnote: every time you open a new terminal, you need to source 3 different files described below to configure ros 2 and gazebo environment variables. write the following each time you start a new terminal to deal with ros 2 / gazebo stuff, or prefer to add them at the end of your .bashrc file with gedit ~/.bashrc. for the latter, don\u2019t forget to source your .bashrc to enforce the update after saving these changes, or open a fresh terminal.\nfor ros 2 variables\nsource /opt/ros/foxy/setup.bash\nfor your installation workspace (change the path accordingly)\nsource $home/ros2_ws/install/setup.bash\nfor gazebo\nsource /usr/share/gazebo/setup.sh\nlet's start testing plankton. open a new terminal (don\u2019t forget to source ros 2 / gazebo files if necessary) and write as below to open a gazebo world:\nros2 launch uuv_gazebo_worlds ocean_waves.launch\nopen again a new terminal (don't forget to source ros 2 / gazebo files if necessary), and spawn the rexrov robot:\nros2 launch uuv_descriptions upload_rexrov.launch mode:=default x:=0 y:=0 z:=-20 namespace:=rexrov\nadd a joystick node to control it remotely (new terminal needed) :\nros2 launch uuv_control_cascaded_pid joy_velocity.launch uuv_name:=rexrov model_name:=rexrov joy_id:=0\nhere, joy_id represents your joystick id, defined in your os. you can determine what is your id by installing sudo apt-get install jstest-gtk and running it.\na more complete documentation of the uuv simulator features is available here:\nhttps://uuvsimulator.github.io/\nbeware that the uuv simulator documentation is written for ros 1, so you will have to make adjustments on given commands to make it work in plankton.\nfaq\nlogging and print information are not displayed in the terminal\nit is a problem affecting eloquent and launch files. prefix your ros2 launch command with stdbuf -o l. if you are using python launch file, you can launch your node with the emulate_tty argument:\nnode(\npackage='package_name',\nnode_executable='package_exec',\noutput='screen',\nemulate_tty=true,\n)\nthe gazebo worlds show a black screen when using virtual machine\ntry to add the following line to your ~/.bashrc file\nexport libgl_always_software=1\nexternal contributions\nport of the eca a9 auv, initially developed for uuv simulator, to plankton: https://github.com/gso-soslab/eca_a9_plankton\nlicense\nplankton is distributed under apache license version 2.0", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000416, "year": null}, {"Unnamed: 0": 1437, "autor": 417, "date": null, "content": "UAVCAN GUI Tool (legacy)\nUAVCAN GUI Tool is a cross-platform (Windows/Linux/OSX) application for UAVCAN/CAN v0 management and diagnostics.\nThis is a legacy application; it is not compatible with the stable UAVCAN v1. There is work underway on the new GUI solution called Yukon that will support UAVCAN v1. While Yukon is unavailable, users of UAVCAN v1 are recommended to use the Yakut command line tool.\nRead the docs at legacy.uavcan.org/GUI_Tool.\nAsk questions at forum.uavcan.org.\nRead installation instructions:\nGNU/LINUX\nWINDOWS\nMACOS\nInstalling on GNU/Linux\nThe general approach is simple:\nInstall PyQt5 for Python 3 using your OS' package manager (e.g. APT).\nInstall the application itself from Git via PIP: pip3 install git+https://github.com/UAVCAN/gui_tool@master (it is not necessary to clone this repository manually). Alternatively, if you're a developer and you want to install your local copy, use pip3 install ..\nIt also may be necessary to install additional dependencies, depending on your distribution (see details below).\nOnce the application is installed, you should see new desktop entries available in your desktop menu; also a new executable uavcan_gui_tool will be available in your PATH. If your desktop environment doesn't update the menu automatically, you may want to do it manually, e.g. by invoking sudo update-desktop-database (command depends on the distribution).\nIt is also recommended to install Matplotlib - it is not used by the application itself, but it may come in handy when using the embedded IPython console.\nDebian-based distributions\nsudo apt-get install -y python3-pip python3-setuptools python3-wheel\nsudo apt-get install -y python3-numpy python3-pyqt5 python3-pyqt5.qtsvg git-core\nsudo pip3 install git+https://github.com/UAVCAN/gui_tool@master\nTroubleshooting\nIf installation fails with an error like below, try to install IPython directly with sudo pip3 install ipython:\nerror: Setup script exited with error in ipython setup command: Invalid environment marker: sys_platform == \"darwin\" and platform_python_implementation == \"CPython\"\nIf you're still unable to install the package, please open a ticket.\nRPM-based distributions\nMaintainers wanted\nFedora 29+\nsudo dnf install python3-PyQt5\nsudo pip3 install git+https://github.com/UAVCAN/gui_tool@master\nInstalling on Windows\nIn order to install this application, download and install the latest .msi package from here: https://files.zubax.com/products/org.uavcan.gui_tool/.\nBuilding the MSI package\nThese instructions are for developers only. End users should use pre-built MSI packages (see the link above).\nFirst, install dependencies:\nWinPython 3.4 or newer, pre-packaged with PyQt5. Make sure that python can be invoked from the terminal; if it can't, check your PATH.\nWindows 10 SDK. Free edition of Visual Studio is packaged with Windows SDK.\nHaving done that, execute the following (the script will prompt you for password to read the certificate file):\npython -m pip uninstall -y uavcan\npython -m pip uninstall -y uavcan_gui_tool\npython setup.py install\npython setup.py bdist_msi\nCollect the resulting signed MSI from dist/.\nInstalling on macOS\nOSX support is a bit lacking in the way that installation doesn't create an entry in the applications menu, but this issue should be fixed someday in the future. Other than that, everything appears to function more or less correctly. If you have a choice, it is recommended to use Linux or Windows instead, as these ports are supported much better at the moment.\nHomebrew option\nInstall the Homebrew package manager for OSX.\nRun the following commands:\nbrew install python3\nbrew postinstall python3\npip3 install PyQt5\npip3 install git+https://github.com/UAVCAN/gui_tool@master\nuavcan_gui_tool\nMacPorts option\nInstall XCode from App Store, install MacPorts from https://www.macports.org/install.php, then run the commands below. If you're prompted to install Command Line Developer Tools, agree.\nsudo port selfupdate\nsudo port install curl-ca-bundle py35-pip py35-pyqt5 py35-numpy\nsudo python3.5 -m pip install git+https://github.com/UAVCAN/gui_tool@master\nWe would like to provide prebuilt application packages instead of the mess above. Contributions adding this capability would be welcome.\nDevelopment\nReleasing new version\nFirst, deploy the new version to PyPI. In order to do that, perform the following steps:\nUpdate the version tuple in version.py, e.g. 1, 0, and commit this change.\nCreate a new tag with the same version number as in the version file, e.g. git tag -a 1.0 -m v1.0.\nPush to master: git push && git push --tags\nThen, build a Windows MSI package using the instructions above, and upload the resulting MSI to the distribution server.\nCode style\nPlease follow the Zubax Python Coding Conventions.", "link": "https://github.com/UAVCAN/gui_tool", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "uavcan gui -----> tool !!!  (legacy)\nuavcan gui -----> tool !!!  is a cross-platform (windows/linux/osx) application for uavcan/can v0 management and diagnostics.\nthis is a legacy application; it is not compatible with the stable uavcan v1. there is work underway on the new gui solution called yukon that will support uavcan v1. while yukon is unavailable, users of uavcan v1 are recommended to use the yakut command line tool.\nread the docs at legacy.uavcan.org/gui_tool.\nask questions at forum.uavcan.org.\nread installation instructions:\ngnu/linux\nwindows\nmacos\ninstalling on gnu/linux\nthe general approach is simple:\ninstall pyqt5 for python 3 using your os' package manager (e.g. apt).\ninstall the application itself from git via pip: pip3 install git+https://github.com/uavcan/gui_tool@master (it is not necessary to clone this repository manually). alternatively, if you're a developer and you want to install your local copy, use pip3 install ..\nit also may be necessary to install additional dependencies, depending on your distribution (see details below).\nonce the application is installed, you should see new desktop entries available in your desktop menu; also a new executable uavcan_gui_tool will be available in your path. if your desktop environment doesn't update the menu automatically, you may want to do it manually, e.g. by invoking sudo update-desktop-database (command depends on the distribution).\nit is also recommended to install matplotlib - it is not used by the application itself, but it may come in handy when using the embedded ipython console.\ndebian-based distributions\nsudo apt-get install -y python3-pip python3-setuptools python3-wheel\nsudo apt-get install -y python3-numpy python3-pyqt5 python3-pyqt5.qtsvg git-core\nsudo pip3 install git+https://github.com/uavcan/gui_tool@master\ntroubleshooting\nif installation fails with an error like below, try to install ipython directly with sudo pip3 install ipython:\nerror: setup script exited with error in ipython setup command: invalid environment marker: sys_platform == \"darwin\" and platform_python_implementation == \"cpython\"\nif you're still unable to install the package, please open a ticket.\nrpm-based distributions\nmaintainers wanted\nfedora 29+\nsudo dnf install python3-pyqt5\nsudo pip3 install git+https://github.com/uavcan/gui_tool@master\ninstalling on windows\nin order to install this application, download and install the latest .msi package from here: https://files.zubax.com/products/org.uavcan.gui_tool/.\nbuilding the msi package\nthese instructions are for developers only. end users should use pre-built msi packages (see the link above).\nfirst, install dependencies:\nwinpython 3.4 or newer, pre-packaged with pyqt5. make sure that python can be invoked from the terminal; if it can't, check your path.\nwindows 10 sdk. free edition of visual studio is packaged with windows sdk.\nhaving done that, execute the following (the script will prompt you for password to read the certificate file):\npython -m pip uninstall -y uavcan\npython -m pip uninstall -y uavcan_gui_tool\npython setup.py install\npython setup.py bdist_msi\ncollect the resulting signed msi from dist/.\ninstalling on macos\nosx support is a bit lacking in the way that installation doesn't create an entry in the applications menu, but this issue should be fixed someday in the future. other than that, everything appears to function more or less correctly. if you have a choice, it is recommended to use linux or windows instead, as these ports are supported much better at the moment.\nhomebrew option\ninstall the homebrew package manager for osx.\nrun the following commands:\nbrew install python3\nbrew postinstall python3\npip3 install pyqt5\npip3 install git+https://github.com/uavcan/gui_tool@master\nuavcan_gui_tool\nmacports option\ninstall xcode from app store, install macports from https://www.macports.org/install.php, then run the commands below. if you're prompted to install command line developer tools, agree.\nsudo port selfupdate\nsudo port install curl-ca-bundle py35-pip py35-pyqt5 py35-numpy\nsudo python3.5 -m pip install git+https://github.com/uavcan/gui_tool@master\nwe would like to provide prebuilt application packages instead of the mess above. contributions adding this capability would be welcome.\ndevelopment\nreleasing new version\nfirst, deploy the new version to pypi. in order to do that, perform the following steps:\nupdate the version tuple in version.py, e.g. 1, 0, and commit this change.\ncreate a new tag with the same version number as in the version file, e.g. git tag -a 1.0 -m v1.0.\npush to master: git push && git push --tags\nthen, build a windows msi package using the instructions above, and upload the resulting msi to the distribution server.\ncode style\nplease follow the zubax python coding conventions.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000417, "year": null}, {"Unnamed: 0": 1450, "autor": 430, "date": null, "content": "Multicopter Design\nAuthors: Tao Du, Adriana Schulz, Bo Zhu\nAffiliation: MIT CSAIL\nThis project develops a design, simulation and optimization tool for casual users to build their own multicopters. It was a research project led by the Computational Fabrication Group at MIT CSAIL in 2015.\nGallary\nPlease refer to docs/user_guide.pdf for more information.\nSupported platforms\nWindows: tested on Windows 10 64bit and Microsoft Visual Studio 2017.\nLinux: tested on Ubuntu 16.04 LTS and gcc 4.9.0.\nmacOS: tested on Sierra 10.12.3 and AppleClang 8.1.0.\nDependencies\nAll of them are by default not included in the distribution. You need to use git submodule and the provided script to download and configure them.\ngraphics_codebase: my own graphics viewer (git submodule + script).\nnlopt: an open-source C++ optimization library (git submodule).\ntinyxml2: an XML parser (git submodule).\nHow to build\nUse git to clone the project from github. Note that --recursive is necessary or you will have to pull the submodule manually:\ngit clone --recursive https://www.github.com/mit-gfx/multicopter_design.git\nRun the script corresponding to your system to download dependencies. For Windows:\n./windows_setup.ps1\nFor linux:\n./linux_setup.sh\nFor macOS:\n./macos_setup.sh\nBelow we will use MULTICOPTER_DESIGN_PROJECT to refer to the location of this project. The structure of the folder after you clone or download the project should look like:\nMULTICOPTER_DESIGN_PROJECT\ndocs/: a user guide.\nexternals/: external libraries (see above).\nprojects/: main source code.\ncopter_simulation/: the core library.\ncopter_viewer/: a viewer depending on copter_simulation.\nresources/:\ncopter/: XML files that define a copter. You are free to add your own XML files here.\nmesh/: 3D meshes we use to assemble our real copter. Files like connector.obj are 3D printable. In generally you should not touch any file here.\nmeasurement/: our propeller/motor/battery measurement data for your reference if you plan to do your own measurement.\n.gitignore\n.gitmodules\nCMakeLists.txt\nlinux_setup.sh\nmacos_setup.sh\nreadme.txt: this file.\nwindows_setup.ps1\nDepending on our operation system, follow the corresponding instructions:\nWindows\nOpen cmake-gui. Put your own MULTICOPTER_DESIGN_PROJECT in \"Where is the source code\", and put your desired build folder in \"Where to build the binaries\". The build folder can be any folder you like, but we recommend using an empty folder outside MULTICOPTER_DESIGN_PROJECT.\nClick \"Configure\", then selected \"Visual Studio 15 2017 Win64\" and click \"Finish\". Once it is done, click \"Generate\". Now navigate to your build folder, open the solution \"multicopter_design.sln\", and press F7 to build the solution.\nSet copter_viewer as your startup project, then press `F5' to run. You should be able to see an viewer showing a pentacopter by default.\nLinux\nMake sure you have OpenGL and dependency libraries installed:\nsudo apt-get install libgl1-mesa-dev mesa-common-dev xorg-dev\nsudo apt-get install libglu1-mesa libglu1-mesa-dev\nGo to the parent folder of your MULTICOPTER_DESIGN_PROJECT, and create an empty build folder:\ncd MULTICOPTER_DESIGN_PROJECT\ncd ../\nmkdir multicopter_design_build_gcc\ncd multicopter_design_build_gcc\nUse cmake to do an out-of-source build, and run make to compile:\ncmake ../multicopter_design\nmake\nOnce it is done you can try to run the default example:\ncd projects/copter_viewer\n./copter_viewer\nYou will see a pentacopter example by default.\nmacOS\nPretty much the same except that you do not need to install OpenGL libraries. Go to the parent folder of your MULTICOPTER_DESIGN_PROJECT, and create an empty build folder:\ncd MULTICOPTER_DESIGN_PROJECT\ncd ../\nmkdir multicopter_design_build_macos\ncd multicopter_design_build_macos\nUse cmake to do an out-of-source build, and run make to compile:\ncmake ../multicopter_design\nmake\nOnce it is done you can try to run the default example:\ncd projects/copter_viewer\n./copter_viewer\nYou will see a pentacopter example by default.\nUser interaction\nUse your mouse to change the view: scroll to scale, press the wheel to rotate and press the wheel + shift to translate.\nDrag the slide bar on the right to change the design. Once you are happy, scroll down and click the \"simulate\" button.\nUse your keyboard to control the motion of the copter (left/right/up/down arrows and a, d, w, s). Press p to pause/resume.\nAttribution\n@article{du2016computational,\ntitle={Computational multicopter design},\nauthor={Du, Tao and Schulz, Adriana and Zhu, Bo and Bickel, Bernd and Matusik, Wojciech},\njournal={ACM Transactions on Graphics (TOG)},\nvolume={35},\nnumber={6},\npages={227},\nyear={2016},\npublisher={ACM}\n}\nContact\nPlease email taodu@csail.mit.edu if you have general questions or comments. For troubleshooting please post an issue on github.", "link": "https://github.com/mit-gfx/multicopter_design", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "multicopter design\nauthors: tao du, adriana schulz, bo zhu\naffiliation: mit csail\nthis project develops a design, simulation and optimization -----> tool !!!  for casual users to build their own multicopters. it was a research project led by the computational fabrication group at mit csail in 2015.\ngallary\nplease refer to docs/user_guide.pdf for more information.\nsupported platforms\nwindows: tested on windows 10 64bit and microsoft visual studio 2017.\nlinux: tested on ubuntu 16.04 lts and gcc 4.9.0.\nmacos: tested on sierra 10.12.3 and appleclang 8.1.0.\ndependencies\nall of them are by default not included in the distribution. you need to use git submodule and the provided script to download and configure them.\ngraphics_codebase: my own graphics viewer (git submodule + script).\nnlopt: an open-source c++ optimization library (git submodule).\ntinyxml2: an xml parser (git submodule).\nhow to build\nuse git to clone the project from github. note that --recursive is necessary or you will have to pull the submodule manually:\ngit clone --recursive https://www.github.com/mit-gfx/multicopter_design.git\nrun the script corresponding to your system to download dependencies. for windows:\n./windows_setup.ps1\nfor linux:\n./linux_setup.sh\nfor macos:\n./macos_setup.sh\nbelow we will use multicopter_design_project to refer to the location of this project. the structure of the folder after you clone or download the project should look like:\nmulticopter_design_project\ndocs/: a user guide.\nexternals/: external libraries (see above).\nprojects/: main source code.\ncopter_simulation/: the core library.\ncopter_viewer/: a viewer depending on copter_simulation.\nresources/:\ncopter/: xml files that define a copter. you are free to add your own xml files here.\nmesh/: 3d meshes we use to assemble our real copter. files like connector.obj are 3d printable. in generally you should not touch any file here.\nmeasurement/: our propeller/motor/battery measurement data for your reference if you plan to do your own measurement.\n.gitignore\n.gitmodules\ncmakelists.txt\nlinux_setup.sh\nmacos_setup.sh\nreadme.txt: this file.\nwindows_setup.ps1\ndepending on our operation system, follow the corresponding instructions:\nwindows\nopen cmake-gui. put your own multicopter_design_project in \"where is the source code\", and put your desired build folder in \"where to build the binaries\". the build folder can be any folder you like, but we recommend using an empty folder outside multicopter_design_project.\nclick \"configure\", then selected \"visual studio 15 2017 win64\" and click \"finish\". once it is done, click \"generate\". now navigate to your build folder, open the solution \"multicopter_design.sln\", and press f7 to build the solution.\nset copter_viewer as your startup project, then press `f5' to run. you should be able to see an viewer showing a pentacopter by default.\nlinux\nmake sure you have opengl and dependency libraries installed:\nsudo apt-get install libgl1-mesa-dev mesa-common-dev xorg-dev\nsudo apt-get install libglu1-mesa libglu1-mesa-dev\ngo to the parent folder of your multicopter_design_project, and create an empty build folder:\ncd multicopter_design_project\ncd ../\nmkdir multicopter_design_build_gcc\ncd multicopter_design_build_gcc\nuse cmake to do an out-of-source build, and run make to compile:\ncmake ../multicopter_design\nmake\nonce it is done you can try to run the default example:\ncd projects/copter_viewer\n./copter_viewer\nyou will see a pentacopter example by default.\nmacos\npretty much the same except that you do not need to install opengl libraries. go to the parent folder of your multicopter_design_project, and create an empty build folder:\ncd multicopter_design_project\ncd ../\nmkdir multicopter_design_build_macos\ncd multicopter_design_build_macos\nuse cmake to do an out-of-source build, and run make to compile:\ncmake ../multicopter_design\nmake\nonce it is done you can try to run the default example:\ncd projects/copter_viewer\n./copter_viewer\nyou will see a pentacopter example by default.\nuser interaction\nuse your mouse to change the view: scroll to scale, press the wheel to rotate and press the wheel + shift to translate.\ndrag the slide bar on the right to change the design. once you are happy, scroll down and click the \"simulate\" button.\nuse your keyboard to control the motion of the copter (left/right/up/down arrows and a, d, w, s). press p to pause/resume.\nattribution\n@article{du2016computational,\ntitle={computational multicopter design},\nauthor={du, tao and schulz, adriana and zhu, bo and bickel, bernd and matusik, wojciech},\njournal={acm transactions on graphics (tog)},\nvolume={35},\nnumber={6},\npages={227},\nyear={2016},\npublisher={acm}\n}\ncontact\nplease email taodu@csail.mit.edu if you have general questions or comments. for troubleshooting please post an issue on github.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000430, "year": null}, {"Unnamed: 0": 1457, "autor": 437, "date": null, "content": "Reachy the Bio-Inspired Robotic Arm\nBeautiful, Easy-to-use, Versatile, Open Source Human-size robotic arm for your robotic applications!\nAbout\nReachy is a unique robot arm with a high level of mobility (7-Dofs). Different tool can be added on the arm's tip such as hook, gripper and 5 fingers hand. You can learn more about this project on its webpage and on youtube:\nReachy is designed and produced by Pollen Robotics, we offer product design and custom developpement, you can learn more on our website.\nOpen source\nReachy hardware and software are both open source and can be freely modified/custumized by the end user to fit its needs.\nThe software is built on top of the Pypot library and is distributed under LGPL Licence.\nThe hardware is made using OnShape under Creative Commons BY-SA licence licence and is accessible here\nDocumentation\nIn this repository, there are multiple examples explaining how to use reachy.\nA Quickstart on how to plug and start reachy can be found here.\nYou can learn more on the software architecture here.\nSeveral examples are available in the folder applications.\nThose notebooks can be viewed directly on GitHub or can also be run locally to directly experiment with your robot. See the jupyter documention for mor information on how to do that.\nInstallation\nReachy's control library is written in Python (>=2.7 or >= 3.4) and works on Win/Mac/Linux and Raspberry Pi. It allows for fast and simple control of your robotic arm. And also provides you example with more advanced features such as inverse kinematics, simulation, tracking, etc. It relies on the pypot library for the dynamixel motor communication.\nOn Raspberry-Pi, you can burn a ready to use image file (installation for both Python2 and Python3): here. You will need a 16Go SD card. You can plug a screen and keyboard directly on the Raspberry Pi or you can access it via ssh (hostname reachy, user pi and password reachy).\nIt can be installed from the source via pip.\ngit clone https://github.com/pollen-robotics/reachy.git\npip install -e ./reachy/software", "link": "https://github.com/pollen-robotics/reachy-legacy", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "reachy the bio-inspired robotic arm\nbeautiful, easy-to-use, versatile, open source human-size robotic arm for your robotic applications!\nabout\nreachy is a unique robot arm with a high level of mobility (7-dofs). different -----> tool !!!  can be added on the arm's tip such as hook, gripper and 5 fingers hand. you can learn more about this project on its webpage and on youtube:\nreachy is designed and produced by pollen robotics, we offer product design and custom developpement, you can learn more on our website.\nopen source\nreachy hardware and software are both open source and can be freely modified/custumized by the end user to fit its needs.\nthe software is built on top of the pypot library and is distributed under lgpl licence.\nthe hardware is made using onshape under creative commons by-sa licence licence and is accessible here\ndocumentation\nin this repository, there are multiple examples explaining how to use reachy.\na quickstart on how to plug and start reachy can be found here.\nyou can learn more on the software architecture here.\nseveral examples are available in the folder applications.\nthose notebooks can be viewed directly on github or can also be run locally to directly experiment with your robot. see the jupyter documention for mor information on how to do that.\ninstallation\nreachy's control library is written in python (>=2.7 or >= 3.4) and works on win/mac/linux and raspberry pi. it allows for fast and simple control of your robotic arm. and also provides you example with more advanced features such as inverse kinematics, simulation, tracking, etc. it relies on the pypot library for the dynamixel motor communication.\non raspberry-pi, you can burn a ready to use image file (installation for both python2 and python3): here. you will need a 16go sd card. you can plug a screen and keyboard directly on the raspberry pi or you can access it via ssh (hostname reachy, user pi and password reachy).\nit can be installed from the source via pip.\ngit clone https://github.com/pollen-robotics/reachy.git\npip install -e ./reachy/software", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000437, "year": null}, {"Unnamed: 0": 1474, "autor": 454, "date": null, "content": "Shuffleboard\nStructure\nShuffleboard is organized into three base projects: api, app, and plugins. plugins has additional subprojects that the main app depends on to provide data types, widgets, and data sources for basic FRC use.\nRunning\nShuffleboard is installed by the FRC vscode extension: Installation Instructions. It can be launched from the WPILib menu in Visual Studio Code (start tool). It can also be run manually by running the shuffleboard.vbs in c:\\Users\\public\\wpilib\\<year>\\tools (Windows) or shuffleboard.py in ~/wpilib/<year>/tools (Linux or Mac).\nRequirements\nJRE 11. Java 11 is required. No other version of Java is supported. Java 11 is installed by the WPILib installer.\nBuilding\nTo run shuffleboard use the command ./gradlew :app:run.\nTo build the APIs and utility classes used in plugin creation, use the command ./gradlew :api:shadowJar\nTo build the Shuffleboard application, use the command ./gradlew :app:shadowJar. By default, this will create an executable JAR for your operating system. To build for another OS, use one of the platform-specific builds:\nOS Command\nWindows 64-bit ./gradlew :app:shadowJar-win64\nWindows 32-bit ./gradlew :app:shadowJar-win32\nMac ./gradlew :app:shadowJar-mac64\nLinux 64-bit ./gradlew :app:shadowJar-linux64\nOnly the listed platforms are supported\nTo build all platform-specific JARs at once, use the command ./gradlew :app:shadowJarAllPlatforms\nRequirements\nJDK 11. JDK 11 is required. No other version of Java is supported.", "link": "https://github.com/wpilibsuite/shuffleboard", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "shuffleboard\nstructure\nshuffleboard is organized into three base projects: api, app, and plugins. plugins has additional subprojects that the main app depends on to provide data types, widgets, and data sources for basic frc use.\nrunning\nshuffleboard is installed by the frc vscode extension: installation instructions. it can be launched from the wpilib menu in visual studio code (start -----> tool !!! ). it can also be run manually by running the shuffleboard.vbs in c:\\users\\public\\wpilib\\<year>\\tools (windows) or shuffleboard.py in ~/wpilib/<year>/tools (linux or mac).\nrequirements\njre 11. java 11 is required. no other version of java is supported. java 11 is installed by the wpilib installer.\nbuilding\nto run shuffleboard use the command ./gradlew :app:run.\nto build the apis and utility classes used in plugin creation, use the command ./gradlew :api:shadowjar\nto build the shuffleboard application, use the command ./gradlew :app:shadowjar. by default, this will create an executable jar for your operating system. to build for another os, use one of the platform-specific builds:\nos command\nwindows 64-bit ./gradlew :app:shadowjar-win64\nwindows 32-bit ./gradlew :app:shadowjar-win32\nmac ./gradlew :app:shadowjar-mac64\nlinux 64-bit ./gradlew :app:shadowjar-linux64\nonly the listed platforms are supported\nto build all platform-specific jars at once, use the command ./gradlew :app:shadowjarallplatforms\nrequirements\njdk 11. jdk 11 is required. no other version of java is supported.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000454, "year": null}, {"Unnamed: 0": 1477, "autor": 457, "date": null, "content": "Godel\nApplication for demonstrating surface blending with ROS.\nGodel: Austrian logician and mathematician http://en.wikipedia.org/wiki/Kurt_G%C3%B6del\nInstallation\nInstall wstool in order manage the repos inside the workspace\nsudo apt install python-wstool\nCd into the 'src' directory of your catkin workspace and run the following:\nwstool init .\nwstool merge https://github.com/ros-industrial-consortium/godel/raw/kinetic-devel/godel.rosinstall\nwstool update\nrosdep install --from-paths . --ignore-src\nFinally, to build:\ncatkin build\nApplications\nRun blending demo in full simulation mode (simulated robot and sensor)\nroslaunch godel_irb2400_support irb2400_blending.launch\nRun the simulation with real point cloud data:\nroslaunch godel_irb2400_support irb2400_blending.launch real_pcd:=true pcd_location:=/path/to/file.pcd\nDownload pcd files and unzip in your HOME directory: https://s3-us-west-2.amazonaws.com/godelscanfiles/godel_point_cloud_data.zip\nRun blending demo in robot simulation mode (simulated robot and real sensor data)\nroslaunch godel_irb2400_support irb2400_blending.launch sim_sensor:=false\nRun blending demo in sensor simulation mode (real robot and simulated sensor)\nroslaunch godel_irb2400_support irb2400_blending.launch sim_robot:=false robot_ip:=[robot ip]\nRun blending demo in full real mode\nroslaunch godel_irb2400_support irb2400_blending.launch sim_sensor:=false\nsim_robot:=false robot_ip:=[robot ip]\nQt Glyph Loading Segfault (Kinetic)\nRviz on Kinetic is prone to a segmentation fault caused by internal functions in the Qt library. Our current work-around is to set the following environment variable:\nexport QT_NO_FT_CACHE=1\nPlanning and Meshing Plugins\nThe meshing_plugins_base and path_planning_plugins_base packages define the interfaces that Godel uses to both perform surface reconstruction and tool path generation based on those reconstructions.\nThe current default mesher works well for point clouds regions that can be accurately modeled by a convex polygon. The default blending tool planner only works well for planar meshes.\nWe are working on an experimental library for more arbitrary surface reconstruction and tool path planning in the noether package.\nTo use, first install the dependencies of noether, which include VTK 7.1 and PCL 1.8.\nClone the noether package into your workspace.\nClone the godel_noether package in your workspace.\nIn your godel robot support package, e.g. godel_irb2400_support, modify the config/plugins.yaml file to read:\nmeshing_plugin_name: \"godel_noether::NoetherMesher\"\nblend_tool_planning_plugin_name: \"godel_noether::NoetherPathPlanner\"\nKeyence Laser Scanner\nTo run the keyence laser scanner driver (replace KEYENCE_CONTROLLER_IP with the ip-address of your sensor):\nrosrun keyence_experimental keyence_driver_node _controller_ip:=KEYENCE_CONTROLLER_IP _frame_id:=keyence_sensor_optical_frame\nIf you have issues connecting, ensure that the IP address matches that of the controller and ensure that your computer is on the same subnet.\nTo acquire laser scans and score them, run the following (replace VOXEL_SIZE_IN_METERS with your desired voxel size):\nroslaunch godel_scan_analysis scan_analysis.launch world_frame:=world_frame scan_frame:=keyence_sensor_optical_frame voxel_leaf_size:=VOXEL_SIZE_IN_METERS", "link": "https://github.com/ros-industrial-consortium/godel", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "godel\napplication for demonstrating surface blending with ros.\ngodel: austrian logician and mathematician http://en.wikipedia.org/wiki/kurt_g%c3%b6del\ninstallation\ninstall wstool in order manage the repos inside the workspace\nsudo apt install python-wstool\ncd into the 'src' directory of your catkin workspace and run the following:\nwstool init .\nwstool merge https://github.com/ros-industrial-consortium/godel/raw/kinetic-devel/godel.rosinstall\nwstool update\nrosdep install --from-paths . --ignore-src\nfinally, to build:\ncatkin build\napplications\nrun blending demo in full simulation mode (simulated robot and sensor)\nroslaunch godel_irb2400_support irb2400_blending.launch\nrun the simulation with real point cloud data:\nroslaunch godel_irb2400_support irb2400_blending.launch real_pcd:=true pcd_location:=/path/to/file.pcd\ndownload pcd files and unzip in your home directory: https://s3-us-west-2.amazonaws.com/godelscanfiles/godel_point_cloud_data.zip\nrun blending demo in robot simulation mode (simulated robot and real sensor data)\nroslaunch godel_irb2400_support irb2400_blending.launch sim_sensor:=false\nrun blending demo in sensor simulation mode (real robot and simulated sensor)\nroslaunch godel_irb2400_support irb2400_blending.launch sim_robot:=false robot_ip:=[robot ip]\nrun blending demo in full real mode\nroslaunch godel_irb2400_support irb2400_blending.launch sim_sensor:=false\nsim_robot:=false robot_ip:=[robot ip]\nqt glyph loading segfault (kinetic)\nrviz on kinetic is prone to a segmentation fault caused by internal functions in the qt library. our current work-around is to set the following environment variable:\nexport qt_no_ft_cache=1\nplanning and meshing plugins\nthe meshing_plugins_base and path_planning_plugins_base packages define the interfaces that godel uses to both perform surface reconstruction and -----> tool !!!  path generation based on those reconstructions.\nthe current default mesher works well for point clouds regions that can be accurately modeled by a convex polygon. the default blending tool planner only works well for planar meshes.\nwe are working on an experimental library for more arbitrary surface reconstruction and tool path planning in the noether package.\nto use, first install the dependencies of noether, which include vtk 7.1 and pcl 1.8.\nclone the noether package into your workspace.\nclone the godel_noether package in your workspace.\nin your godel robot support package, e.g. godel_irb2400_support, modify the config/plugins.yaml file to read:\nmeshing_plugin_name: \"godel_noether::noethermesher\"\nblend_tool_planning_plugin_name: \"godel_noether::noetherpathplanner\"\nkeyence laser scanner\nto run the keyence laser scanner driver (replace keyence_controller_ip with the ip-address of your sensor):\nrosrun keyence_experimental keyence_driver_node _controller_ip:=keyence_controller_ip _frame_id:=keyence_sensor_optical_frame\nif you have issues connecting, ensure that the ip address matches that of the controller and ensure that your computer is on the same subnet.\nto acquire laser scans and score them, run the following (replace voxel_size_in_meters with your desired voxel size):\nroslaunch godel_scan_analysis scan_analysis.launch world_frame:=world_frame scan_frame:=keyence_sensor_optical_frame voxel_leaf_size:=voxel_size_in_meters", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000457, "year": null}, {"Unnamed: 0": 1488, "autor": 468, "date": null, "content": "CoSTAR Plan\nCoSTAR Plan is for deep learning with robots divided into two main parts: The CoSTAR Task Planner (CTP) library and CoSTAR Hyper.\nCoSTAR Task Planner (CTP)\nCode for the paper Visual Robot Task Planning.\nCoSTAR Hyper\nCode for the paper The CoSTAR Block Stacking Dataset: Learning with Workspace Constraints.\n@article{hundt2019costar,\ntitle={The CoSTAR Block Stacking Dataset: Learning with Workspace Constraints},\nauthor={Andrew Hundt and Varun Jain and Chia-Hung Lin and Chris Paxton and Gregory D. Hager},\njournal = {Intelligent Robots and Systems (IROS), 2019 IEEE International Conference on},\nyear = 2019,\nurl = {https://arxiv.org/abs/1810.11714}\n}\nCode instructions are in the CoSTAR Hyper README.md.\nSupported Datasets\nCoSTAR Block Stacking Dataset\nCornell Grasping Dataset\nGoogle Brain Grasping Dataset\nCoSTAR Task Planner (CTP)\nThe CoSTAR Planner is part of the larger CoSTAR project. It integrates some learning from demonstration and task planning capabilities into the larger CoSTAR framework in different ways.\nSpecifically it is a project for creating task and motion planning algorithms that use machine learning to solve challenging problems in a variety of domains. This code provides a testbed for complex task and motion planning search algorithms.\nThe goal is to describe example problems where the actor must move around in the world and plan complex interactions with other actors or the environment that correspond to high-level symbolic states. Among these is our Visual Task Planning project, in which robots learn representations of their world and use these to imagine possible futures, then use these for planning.\nTo run deep learning examples, you will need TensorFlow and Keras, plus a number of Python packages. To run robot experiments, you'll need a simulator (Gazebo or PyBullet), and ROS Indigo or Kinetic. Other versions of ROS may work but have not been tested. If you want to stick to the toy examples, you do not need to use this as a ROS package.\nAbout this repository: CTP is a single-repository project. As such, all the custom code you need should be in one place: here. There are exceptions, such as the CoSTAR Stack for real robot execution, but these are generally not necessary. The minimal installation of CTP is just to install the costar_models package as a normal python package ignoring everything else.\nCTP Datasets\nPyBullet Block Stacking download tar.gz\nSample Husky Data download tar.gz\nClassic CoSTAR Real Robot Data download tar.gz\nEarly version, deprecated in lieu of the full CoSTAR Block Stacking Dataset.\nContents\n0. Introduction\n1. Installation Guide\n1.1 Docker Instructions\n1.2 Application domains\n2. Approach: about CTP\n2.1 Software Design: high-level notes\n3. Machine Learning Models: using the command line tool\n3.1 Data collection: data collection with a real or simulated robot\n3.2 MARCC instructions: learning models using JHU's MARCC cluster\n3.3 Generative Adversarial Models\n3.4 SLURM Utilities: tools for using slurm on MARCC\n4. Creating and training a custom task: overview of task representations\n4.1 Generating a task from data: generate task from demonstrations\n4.2 Task Learning: specific details\n5. CoSTAR Simulation: simulation intro\n5.1 Simulation Experiments: information on experiment setup\n5.2 PyBullet Sim: an alternative to Gazebo that may be preferrable in some situations\n5.3 costar_bullet quick start: How to run tasks, generate datasets, train models, and extend costar_bullet with your own components.\n5.4 Adding a robot to the ROS code: NOT using Bullet sim\n6. Husky robot: Start the APL Husky sim\n7. TOM robot: introducing the TOM robot from TUM\n7.1 TOM Data: data necessary for TOM\n7.2 The Real TOM: details about parts of the system for running on the real TOM\n8. CoSTAR Robot: execution with a standard UR5\nPackage/folder layout\nCoSTAR Simulation: Gazebo simulation and ROS execution\nCoSTAR Task Plan: the high-level python planning library\nCoSTAR Gazebo Plugins: assorted plugins for integration\nCoSTAR Models: tools for learning deep neural networks\nCTP Tom: specific bringup and scenarios for the TOM robot from TU Munich\nCTP Visual: visual robot task planner\nsetup: contains setup scripts\nslurm: contains SLURM scripts for running on MARCC\ncommand: contains scripts with example CTP command-line calls\ndocs: markdown files for information that is not specific to a particular ROS package but to all of CTP\nphotos: example images\nlearning_planning_msgs: ROS messages for data collection when doing learning from demonstration in ROS\nOthers are temporary packages for various projects\nMany of these sections are a work in progress; if you have any questions shoot me an email (cpaxton@jhu.edu).\nContact\nThis code is maintained by:\nChris Paxton (cpaxton@jhu.edu).\nAndrew Hundt (ATHundt@gmail.com)\nCite\nVisual Robot Task Planning\n@article{paxton2018visual,\nauthor = {Chris Paxton and\nYotam Barnoy and\nKapil D. Katyal and\nRaman Arora and\nGregory D. Hager},\ntitle = {Visual Robot Task Planning},\njournal = {ArXiv},\nyear = {2018},\nurl = {http://arxiv.org/abs/1804.00062},\narchivePrefix = {arXiv},\neprint = {1804.00062},\nbiburl = {https://dblp.org/rec/bib/journals/corr/abs-1804-00062},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}\nTraining Frankenstein's Creature To Stack: HyperTree Architecture Search\n@article{hundt2018hypertree,\nauthor = {Andrew Hundt and Varun Jain and Chris Paxton and Gregory D. Hager},\ntitle = \"{Training Frankenstein's Creature to Stack: HyperTree Architecture Search}\",\njournal = {ArXiv},\narchivePrefix = {arXiv},\neprint = {1810.11714},\nyear = 2018,\nmonth = Oct,\nurl = {https://arxiv.org/abs/1810.11714}\n}", "link": "https://github.com/jhu-lcsr/costar_plan", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "costar plan\ncostar plan is for deep learning with robots divided into two main parts: the costar task planner (ctp) library and costar hyper.\ncostar task planner (ctp)\ncode for the paper visual robot task planning.\ncostar hyper\ncode for the paper the costar block stacking dataset: learning with workspace constraints.\n@article{hundt2019costar,\ntitle={the costar block stacking dataset: learning with workspace constraints},\nauthor={andrew hundt and varun jain and chia-hung lin and chris paxton and gregory d. hager},\njournal = {intelligent robots and systems (iros), 2019 ieee international conference on},\nyear = 2019,\nurl = {https://arxiv.org/abs/1810.11714}\n}\ncode instructions are in the costar hyper readme.md.\nsupported datasets\ncostar block stacking dataset\ncornell grasping dataset\ngoogle brain grasping dataset\ncostar task planner (ctp)\nthe costar planner is part of the larger costar project. it integrates some learning from demonstration and task planning capabilities into the larger costar framework in different ways.\nspecifically it is a project for creating task and motion planning algorithms that use machine learning to solve challenging problems in a variety of domains. this code provides a testbed for complex task and motion planning search algorithms.\nthe goal is to describe example problems where the actor must move around in the world and plan complex interactions with other actors or the environment that correspond to high-level symbolic states. among these is our visual task planning project, in which robots learn representations of their world and use these to imagine possible futures, then use these for planning.\nto run deep learning examples, you will need tensorflow and keras, plus a number of python packages. to run robot experiments, you'll need a simulator (gazebo or pybullet), and ros indigo or kinetic. other versions of ros may work but have not been tested. if you want to stick to the toy examples, you do not need to use this as a ros package.\nabout this repository: ctp is a single-repository project. as such, all the custom code you need should be in one place: here. there are exceptions, such as the costar stack for real robot execution, but these are generally not necessary. the minimal installation of ctp is just to install the costar_models package as a normal python package ignoring everything else.\nctp datasets\npybullet block stacking download tar.gz\nsample husky data download tar.gz\nclassic costar real robot data download tar.gz\nearly version, deprecated in lieu of the full costar block stacking dataset.\ncontents\n0. introduction\n1. installation guide\n1.1 docker instructions\n1.2 application domains\n2. approach: about ctp\n2.1 software design: high-level notes\n3. machine learning models: using the command line -----> tool !!! \n3.1 data collection: data collection with a real or simulated robot\n3.2 marcc instructions: learning models using jhu's marcc cluster\n3.3 generative adversarial models\n3.4 slurm utilities: tools for using slurm on marcc\n4. creating and training a custom task: overview of task representations\n4.1 generating a task from data: generate task from demonstrations\n4.2 task learning: specific details\n5. costar simulation: simulation intro\n5.1 simulation experiments: information on experiment setup\n5.2 pybullet sim: an alternative to gazebo that may be preferrable in some situations\n5.3 costar_bullet quick start: how to run tasks, generate datasets, train models, and extend costar_bullet with your own components.\n5.4 adding a robot to the ros code: not using bullet sim\n6. husky robot: start the apl husky sim\n7. tom robot: introducing the tom robot from tum\n7.1 tom data: data necessary for tom\n7.2 the real tom: details about parts of the system for running on the real tom\n8. costar robot: execution with a standard ur5\npackage/folder layout\ncostar simulation: gazebo simulation and ros execution\ncostar task plan: the high-level python planning library\ncostar gazebo plugins: assorted plugins for integration\ncostar models: tools for learning deep neural networks\nctp tom: specific bringup and scenarios for the tom robot from tu munich\nctp visual: visual robot task planner\nsetup: contains setup scripts\nslurm: contains slurm scripts for running on marcc\ncommand: contains scripts with example ctp command-line calls\ndocs: markdown files for information that is not specific to a particular ros package but to all of ctp\nphotos: example images\nlearning_planning_msgs: ros messages for data collection when doing learning from demonstration in ros\nothers are temporary packages for various projects\nmany of these sections are a work in progress; if you have any questions shoot me an email (cpaxton@jhu.edu).\ncontact\nthis code is maintained by:\nchris paxton (cpaxton@jhu.edu).\nandrew hundt (athundt@gmail.com)\ncite\nvisual robot task planning\n@article{paxton2018visual,\nauthor = {chris paxton and\nyotam barnoy and\nkapil d. katyal and\nraman arora and\ngregory d. hager},\ntitle = {visual robot task planning},\njournal = {arxiv},\nyear = {2018},\nurl = {http://arxiv.org/abs/1804.00062},\narchiveprefix = {arxiv},\neprint = {1804.00062},\nbiburl = {https://dblp.org/rec/bib/journals/corr/abs-1804-00062},\nbibsource = {dblp computer science bibliography, https://dblp.org}\n}\ntraining frankenstein's creature to stack: hypertree architecture search\n@article{hundt2018hypertree,\nauthor = {andrew hundt and varun jain and chris paxton and gregory d. hager},\ntitle = \"{training frankenstein's creature to stack: hypertree architecture search}\",\njournal = {arxiv},\narchiveprefix = {arxiv},\neprint = {1810.11714},\nyear = 2018,\nmonth = oct,\nurl = {https://arxiv.org/abs/1810.11714}\n}", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000468, "year": null}, {"Unnamed: 0": 1495, "autor": 475, "date": null, "content": "Certifiable Outlier-Robust Geometric Perception\nAbout\nThis repository holds the implementation for certifiably solving outlier-robust geometric perception problems to global optimality. The certifiable outlier-robust geometric perception framework contains two main modules:\nA sparse semidefinite programming relaxation (SSR) scheme that relaxes nonconvex outlier-robust perception problems into convex semidefinite programs (SDPs); and\nA novel SDP solver, called STRIDE, that solves the generated SDPs at an unprecedented scale and accuracy.\nWe proposed a preliminary version of the SSR scheme in our NeurIPS 2020 paper, and released a certifier (that certifies if a given estimate is optimal) based on Douglas-Rachford Splitting (DRS). Please switch to the NeurIPS2020 branch of this repo to checkout the NeurIPS2020 implementation.\nIf you find this library helpful or use it in your projects, please cite:\n@article{Yang21arXiv-certifiableperception,\ntitle={Certifiable Outlier-Robust Geometric Perception: Exact Semidefinite Relaxations and Scalable Global Optimization},\nauthor={Yang, Heng and Carlone, Luca},\njournal={arXiv preprint arXiv:2109.03349},\nyear={2021}\n}\nTutorial: Semidefinite Relaxation for Polynomial Optimization\nA general polynomial optimization problem (POP) is an optimization problem of the following standard form\nminimize_{x \u2208 R^d} f(x)\nsubject to h_i(x) == 0, i=1,...,l_h\ng_j(x) >= 0, j=1,...,l_g\nwhere x \u2208 R^d is a d-dimensional decision variable, f(x) is a scalar polynomial objective function, h_i(x),i=1,...,l_h are scalar polynomial functions that define equality constraints, and g_j(x),j=1,...,l_g are polynomial functions that define inequality constraints. POPs are in general nonconvex and NP-hard problems. For example, one can easily see that binary quadratic programming is an instance of POP, because binary constraints x_i \u2208 {+1, -1}, i=1,...,d can be easily written as polynomial equalities h_i(x) = x_i^2 - 1 = 0,i=1,...,d.\nSemidefinite relaxations are a powerful tool for solving nonconvex POPs to global optimality. In this repo, we provide tools that can help the user exploit the power of semidefinite relaxations with minimum efforts.\nDense Relaxation (Lasserre's Hierarchy)\nLasserre's hierarchy of moment relaxations is a standard technique for relaxing POPs into semidefinite programs (SDPs). The basic idea is as follows. Let kappa be a positive integer (called the relaxation order) such that 2*kappa is no smaller than the maximum degree of the defining polynomials f,h_i,g_j, and let v = [x]_kappa be the set of standard monomials in x with degree up to kappa. For example, suppose x = [x_1; x_2], then [x]_kappa with kappa = 2 leads to v = [1; x_1; x_2; x_1*x_2; x_1^2; x_2^2]. The essential idea of Lasserre's hierarchy is to express the original POP problem in the matrix variable X = v*v' (called the moment matrix, by construction X is positive semidefinite) and relax the POP into a convex SDP. In the seminal paper of Lasserre, it was proved that if kappa is chosen large enough, then the convex SDP can solve the original nonconvex POP to global optimality.\nAlthough the underlying mechanics of Lasserre's hierarchy can be complicated (the interested reader can refer to Section 2 of our paper for technical details), in this repo we provide a simple function that implements Lasserre's hierarchy:\n[SDP,info] = dense_sdp_relax(problem,kappa)\nwhere the inputs are:\nproblem: a Matlab structure that contains the following fields:\nvars: a vector of symbolic decision variables (i.e., x in the POP);\nobjective: a multivariate polynomial that specifies the objective function of the POP (i.e., f(x) in the POP);\nequality (optional): a vector of multivariate polynomials with dimension l_h that specifies the equality constraints of the POP (i.e., h_i(x),i=1,...,l_h in the POP);\ninequality (optional): a vector of multivariate polynomials with dimension l_g that specifies the inequality constraints of the POP (i.e., g_j(x),j=1,...,l_g in the POP).\nkappa (optional): a positive integer that specifies the relaxation order. If kappa is not provided, or provided such that 2*kappa is smaller than the maximum degree of the defining polynomials, then our implementation uses the minimum relaxation order kappa such that 2*kappa is no smaller than the maximum degree.\nand the outputs are:\nSDP: a Matlab structure that contains the following fields:\nblk,At,b,C: standard SDP problem data in SDPT3 format. The interested reader should check out SDPT3 user manual for details. Section 2.1 of our paper also gives a quick introduction.\nsedumi: a Matlab structure that provides the standard SDP problem data in sedumi format. The interested reader should checkout sedumi user manual for details. It is easy to convert sedumi format to MOSEK format, as we will show in one of the examples later.\ninfo: a Matlab structure that contains additional information about the relaxation.\nExample: Binary Quadratic Programming\nWe now use a simple example on binary quadratic programming (BQP) to illustrate how to supply the POP problem description to dense_sdp_relax. The sample code is given below.\n%% Generate random binary quadratic program\nd = 10; % BQP with d variables\nx = msspoly('x',d); % symbolic decision variables using SPOTLESS\nQ = randn(d,d); Q = Q + Q'; % a random symmetric matrix\nc = randn(d,1);\nf = x'*Q*x + c'*x; % objective function of the BQP\nh = x.^2 - 1; % equality constraints of the BQP (binary variables)\ng = [x(1)]; % ask the first variable to be positive\n%% Relax BQP into an SDP\nproblem.vars = x;\nproblem.objective = f;\nproblem.equality = h;\nproblem.inequality = g;\nkappa = 2; % relaxation order\n[SDP,info] = dense_sdp_relax(problem,kappa);\nIn this demo code, we first generate a random binary quadratic programming problem using the package SPOTLESS (which is a submodule of this repo), and then pass the problem structure with fields vars, objective, equality, and inequality to the function dense_sdp_relax to generate SDP relaxations. We recommend the user to run the script example_bqp.m to check how to solve the SDP data using MOSEK, and to see that SDP relaxations can actually solve BQP problems to global optimality.\nSparse Relaxation (Basis Reduction)\nComing soon.", "link": "https://github.com/MIT-SPARK/CertifiablyRobustPerception", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "certifiable outlier-robust geometric perception\nabout\nthis repository holds the implementation for certifiably solving outlier-robust geometric perception problems to global optimality. the certifiable outlier-robust geometric perception framework contains two main modules:\na sparse semidefinite programming relaxation (ssr) scheme that relaxes nonconvex outlier-robust perception problems into convex semidefinite programs (sdps); and\na novel sdp solver, called stride, that solves the generated sdps at an unprecedented scale and accuracy.\nwe proposed a preliminary version of the ssr scheme in our neurips 2020 paper, and released a certifier (that certifies if a given estimate is optimal) based on douglas-rachford splitting (drs). please switch to the neurips2020 branch of this repo to checkout the neurips2020 implementation.\nif you find this library helpful or use it in your projects, please cite:\n@article{yang21arxiv-certifiableperception,\ntitle={certifiable outlier-robust geometric perception: exact semidefinite relaxations and scalable global optimization},\nauthor={yang, heng and carlone, luca},\njournal={arxiv preprint arxiv:2109.03349},\nyear={2021}\n}\ntutorial: semidefinite relaxation for polynomial optimization\na general polynomial optimization problem (pop) is an optimization problem of the following standard form\nminimize_{x \u2208 r^d} f(x)\nsubject to h_i(x) == 0, i=1,...,l_h\ng_j(x) >= 0, j=1,...,l_g\nwhere x \u2208 r^d is a d-dimensional decision variable, f(x) is a scalar polynomial objective function, h_i(x),i=1,...,l_h are scalar polynomial functions that define equality constraints, and g_j(x),j=1,...,l_g are polynomial functions that define inequality constraints. pops are in general nonconvex and np-hard problems. for example, one can easily see that binary quadratic programming is an instance of pop, because binary constraints x_i \u2208 {+1, -1}, i=1,...,d can be easily written as polynomial equalities h_i(x) = x_i^2 - 1 = 0,i=1,...,d.\nsemidefinite relaxations are a powerful -----> tool !!!  for solving nonconvex pops to global optimality. in this repo, we provide tools that can help the user exploit the power of semidefinite relaxations with minimum efforts.\ndense relaxation (lasserre's hierarchy)\nlasserre's hierarchy of moment relaxations is a standard technique for relaxing pops into semidefinite programs (sdps). the basic idea is as follows. let kappa be a positive integer (called the relaxation order) such that 2*kappa is no smaller than the maximum degree of the defining polynomials f,h_i,g_j, and let v = [x]_kappa be the set of standard monomials in x with degree up to kappa. for example, suppose x = [x_1; x_2], then [x]_kappa with kappa = 2 leads to v = [1; x_1; x_2; x_1*x_2; x_1^2; x_2^2]. the essential idea of lasserre's hierarchy is to express the original pop problem in the matrix variable x = v*v' (called the moment matrix, by construction x is positive semidefinite) and relax the pop into a convex sdp. in the seminal paper of lasserre, it was proved that if kappa is chosen large enough, then the convex sdp can solve the original nonconvex pop to global optimality.\nalthough the underlying mechanics of lasserre's hierarchy can be complicated (the interested reader can refer to section 2 of our paper for technical details), in this repo we provide a simple function that implements lasserre's hierarchy:\n[sdp,info] = dense_sdp_relax(problem,kappa)\nwhere the inputs are:\nproblem: a matlab structure that contains the following fields:\nvars: a vector of symbolic decision variables (i.e., x in the pop);\nobjective: a multivariate polynomial that specifies the objective function of the pop (i.e., f(x) in the pop);\nequality (optional): a vector of multivariate polynomials with dimension l_h that specifies the equality constraints of the pop (i.e., h_i(x),i=1,...,l_h in the pop);\ninequality (optional): a vector of multivariate polynomials with dimension l_g that specifies the inequality constraints of the pop (i.e., g_j(x),j=1,...,l_g in the pop).\nkappa (optional): a positive integer that specifies the relaxation order. if kappa is not provided, or provided such that 2*kappa is smaller than the maximum degree of the defining polynomials, then our implementation uses the minimum relaxation order kappa such that 2*kappa is no smaller than the maximum degree.\nand the outputs are:\nsdp: a matlab structure that contains the following fields:\nblk,at,b,c: standard sdp problem data in sdpt3 format. the interested reader should check out sdpt3 user manual for details. section 2.1 of our paper also gives a quick introduction.\nsedumi: a matlab structure that provides the standard sdp problem data in sedumi format. the interested reader should checkout sedumi user manual for details. it is easy to convert sedumi format to mosek format, as we will show in one of the examples later.\ninfo: a matlab structure that contains additional information about the relaxation.\nexample: binary quadratic programming\nwe now use a simple example on binary quadratic programming (bqp) to illustrate how to supply the pop problem description to dense_sdp_relax. the sample code is given below.\n%% generate random binary quadratic program\nd = 10; % bqp with d variables\nx = msspoly('x',d); % symbolic decision variables using spotless\nq = randn(d,d); q = q + q'; % a random symmetric matrix\nc = randn(d,1);\nf = x'*q*x + c'*x; % objective function of the bqp\nh = x.^2 - 1; % equality constraints of the bqp (binary variables)\ng = [x(1)]; % ask the first variable to be positive\n%% relax bqp into an sdp\nproblem.vars = x;\nproblem.objective = f;\nproblem.equality = h;\nproblem.inequality = g;\nkappa = 2; % relaxation order\n[sdp,info] = dense_sdp_relax(problem,kappa);\nin this demo code, we first generate a random binary quadratic programming problem using the package spotless (which is a submodule of this repo), and then pass the problem structure with fields vars, objective, equality, and inequality to the function dense_sdp_relax to generate sdp relaxations. we recommend the user to run the script example_bqp.m to check how to solve the sdp data using mosek, and to see that sdp relaxations can actually solve bqp problems to global optimality.\nsparse relaxation (basis reduction)\ncoming soon.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000475, "year": null}, {"Unnamed: 0": 1501, "autor": 481, "date": null, "content": "urdf2webots\nThis tool converts URDF files into Webots PROTO files.\nInstall\nFrom pip\npip install urdf2webots\nOn macOS, export the pip binary path to the PATH: export PATH=\"/Users/$USER/Library/Python/3.7/bin:$PATH\"\nFrom Sources\ngit clone --recurse-submodules https://github.com/cyberbotics/urdf2webots.git\npip install --upgrade --editable urdf2webots\nUsage\nFrom pip\npython -m urdf2webots.importer --input=someRobot.urdf [--output=outputFile] [--box-collision] [--normal] [--static-base] [--tool-slot=linkName] [--name-to-def] [--help]\nArguments\nThe script accepts the following arguments:\n-h, --help: Show the help message and exit.\n--input=INFILE: Specifies the urdf file to convert.\n--output=OUTFILE: If set, specifies the path and, if ending in \".proto\", name of the resulting PROTO file. The filename minus the .proto extension will be the robot name.\n--normal: If set, the normals are exported if present in the URDF definition.\n--box-collision: If set, the bounding objects are approximated using boxes.\n--static-base: If set, the base link will have the option to be static (disable physics)\n--tool-slot=LinkName: Specify the link that you want to add a tool slot to (exact link name from urdf).\n--rotation=\"0 1 0 0\": Set the rotation field of your PROTO file. If your URDF file uses the z-axis as 'up', use --rotation=\"1 0 0 -1.5708\".\n--init-pos=JointPositions: Set the initial positions of your robot joints. Example: --init-pos=\"[1.2, 0.5, -1.5]\" would set the first 3 joints of your robot to the specified values, and leave the rest with their default value.\n--link-to-def: Creates a DEF with the link name for each solid to be able to access it using getFromProtoDef(defName)\n--joint-to-def: Creates a DEF with the joint name for each joint to be able to access it using getFromProtoDef(defName)\nIn your Python Code\nfrom urdf2webots.importer import convert2urdf\nconvert2urdf('MY_PATH/MY_URDF.urdf')\nIn-Depth Tutorial\nCheck out this tutorial for a more in-depth, step by step instruction, on how to:\nGenerate a URDF file from a ROS repository.\nConvert your URDF file to a Webots PROTO file.\nLoad your converted model into Webots and make final adjustments.\nNotes\nThis tool have been tested using Webots R2020b on Ubuntu16.04 and Windows.\nYou can find the sources of these URDF files here:\nuniversal robot: https://github.com/ros-industrial/universal_robot/tree/kinetic-devel/ur_description\npr2 robot: https://github.com/PR2/pr2_common/tree/kinetic-devel/pr2_description\nmotoman robot: https://github.com/ros-industrial/motoman/tree/kinetic-devel/motoman_sia20d_support\nkinova robot: https://github.com/Kinovarobotics/kinova-ros/tree/kinetic/kinova_description\ngait2392 human skeleton: https://github.com/cyberbotics/urdf2webots/tree/master/tests/sources/gait2392_simbody\nAcknowledgements\nSupported by ROSIN - ROS-Industrial Quality-Assured Robot Software Components.\nMore information: rosin-project.eu\nThis project has received funding from the European Union\u2019s Horizon 2020\nresearch and innovation programme under grant agreement no. 732287.\nSupported by OpenDR - Open Deep Learning Toolkit for Robotics.\nMore information: opendr.eu\nThis project has received funding from the European Union\u2019s Horizon 2020\nresearch and innovation programme under grant agreement no. 871449.", "link": "https://github.com/cyberbotics/urdf2webots", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "urdf2webots\nthis -----> tool !!!  converts urdf files into webots proto files.\ninstall\nfrom pip\npip install urdf2webots\non macos, export the pip binary path to the path: export path=\"/users/$user/library/python/3.7/bin:$path\"\nfrom sources\ngit clone --recurse-submodules https://github.com/cyberbotics/urdf2webots.git\npip install --upgrade --editable urdf2webots\nusage\nfrom pip\npython -m urdf2webots.importer --input=somerobot.urdf [--output=outputfile] [--box-collision] [--normal] [--static-base] [--tool-slot=linkname] [--name-to-def] [--help]\narguments\nthe script accepts the following arguments:\n-h, --help: show the help message and exit.\n--input=infile: specifies the urdf file to convert.\n--output=outfile: if set, specifies the path and, if ending in \".proto\", name of the resulting proto file. the filename minus the .proto extension will be the robot name.\n--normal: if set, the normals are exported if present in the urdf definition.\n--box-collision: if set, the bounding objects are approximated using boxes.\n--static-base: if set, the base link will have the option to be static (disable physics)\n--tool-slot=linkname: specify the link that you want to add a tool slot to (exact link name from urdf).\n--rotation=\"0 1 0 0\": set the rotation field of your proto file. if your urdf file uses the z-axis as 'up', use --rotation=\"1 0 0 -1.5708\".\n--init-pos=jointpositions: set the initial positions of your robot joints. example: --init-pos=\"[1.2, 0.5, -1.5]\" would set the first 3 joints of your robot to the specified values, and leave the rest with their default value.\n--link-to-def: creates a def with the link name for each solid to be able to access it using getfromprotodef(defname)\n--joint-to-def: creates a def with the joint name for each joint to be able to access it using getfromprotodef(defname)\nin your python code\nfrom urdf2webots.importer import convert2urdf\nconvert2urdf('my_path/my_urdf.urdf')\nin-depth tutorial\ncheck out this tutorial for a more in-depth, step by step instruction, on how to:\ngenerate a urdf file from a ros repository.\nconvert your urdf file to a webots proto file.\nload your converted model into webots and make final adjustments.\nnotes\nthis tool have been tested using webots r2020b on ubuntu16.04 and windows.\nyou can find the sources of these urdf files here:\nuniversal robot: https://github.com/ros-industrial/universal_robot/tree/kinetic-devel/ur_description\npr2 robot: https://github.com/pr2/pr2_common/tree/kinetic-devel/pr2_description\nmotoman robot: https://github.com/ros-industrial/motoman/tree/kinetic-devel/motoman_sia20d_support\nkinova robot: https://github.com/kinovarobotics/kinova-ros/tree/kinetic/kinova_description\ngait2392 human skeleton: https://github.com/cyberbotics/urdf2webots/tree/master/tests/sources/gait2392_simbody\nacknowledgements\nsupported by rosin - ros-industrial quality-assured robot software components.\nmore information: rosin-project.eu\nthis project has received funding from the european union\u2019s horizon 2020\nresearch and innovation programme under grant agreement no. 732287.\nsupported by opendr - open deep learning toolkit for robotics.\nmore information: opendr.eu\nthis project has received funding from the european union\u2019s horizon 2020\nresearch and innovation programme under grant agreement no. 871449.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000481, "year": null}, {"Unnamed: 0": 1510, "autor": 490, "date": null, "content": "grasp_multiObject\nRobotic grasp dataset for multi-object multi-grasp evaluation with RGB-D data.\nThis dataset is annotated using the same protocal as Cornell Dataset, and can be used as multi-object extension of Cornell Dataset.\nUsage\nDownload RGB-D data and put into grasp_multiObject/rgbd/\nEach testing data has one RGB image (rgb_xxxx) and one depth image (depth_xxxx).\nThe corresponding grasp annotation (rgb_xxxx_annotations) can be found in grasp_multiObject/annotation/\nGenerate RG-D data?\nmkdir rgd\nrun rgbd2rgd\nyou will have RG-D data in grasp_multiObject/rgd/\nCrop images?\nmkdir rgd_cropped320\nmkdir rgb_cropped320\nrun image2txt\nyou will have cropped RGB and RGD images in grasp_multiObject/rgd_cropped320/ and grasp_multiObject/rgb_cropped320/, respectively.\nalso, you will have corresponding annotation files, as well as a full list of image path.\nVisualize grasp?\nrun visualizationGripper\nthis file shows a simple example to visualize ground truth grasps\nAnnotate your own data?\ngit clone https://github.com/ivalab/grasp_annotation_tool\nyou can annotate grasps on your own data with this simple tool!\nBoth dataset and annotation tool can also be found here\nCitation\nIf you find it helpful for your research, please consider citing:\n@inproceedings{chu2018deep,\ntitle = {Real-World Multiobject, Multigrasp Detection},\nauthor = {F. Chu and R. Xu and P. A. Vela},\njournal = {IEEE Robotics and Automation Letters},\nyear = {2018},\nvolume = {3},\nnumber = {4},\npages = {3355-3362},\nDOI = {10.1109/LRA.2018.2852777},\nISSN = {2377-3766},\nmonth = {Oct}\n}\nIf you encounter any questions, please contact me at fujenchu[at]gatech[dot]edu", "link": "https://github.com/ivalab/grasp_multiObject", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "grasp_multiobject\nrobotic grasp dataset for multi-object multi-grasp evaluation with rgb-d data.\nthis dataset is annotated using the same protocal as cornell dataset, and can be used as multi-object extension of cornell dataset.\nusage\ndownload rgb-d data and put into grasp_multiobject/rgbd/\neach testing data has one rgb image (rgb_xxxx) and one depth image (depth_xxxx).\nthe corresponding grasp annotation (rgb_xxxx_annotations) can be found in grasp_multiobject/annotation/\ngenerate rg-d data?\nmkdir rgd\nrun rgbd2rgd\nyou will have rg-d data in grasp_multiobject/rgd/\ncrop images?\nmkdir rgd_cropped320\nmkdir rgb_cropped320\nrun image2txt\nyou will have cropped rgb and rgd images in grasp_multiobject/rgd_cropped320/ and grasp_multiobject/rgb_cropped320/, respectively.\nalso, you will have corresponding annotation files, as well as a full list of image path.\nvisualize grasp?\nrun visualizationgripper\nthis file shows a simple example to visualize ground truth grasps\nannotate your own data?\ngit clone https://github.com/ivalab/grasp_annotation_tool\nyou can annotate grasps on your own data with this simple -----> tool !!! !\nboth dataset and annotation tool can also be found here\ncitation\nif you find it helpful for your research, please consider citing:\n@inproceedings{chu2018deep,\ntitle = {real-world multiobject, multigrasp detection},\nauthor = {f. chu and r. xu and p. a. vela},\njournal = {ieee robotics and automation letters},\nyear = {2018},\nvolume = {3},\nnumber = {4},\npages = {3355-3362},\ndoi = {10.1109/lra.2018.2852777},\nissn = {2377-3766},\nmonth = {oct}\n}\nif you encounter any questions, please contact me at fujenchu[at]gatech[dot]edu", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000490, "year": null}, {"Unnamed: 0": 1514, "autor": 494, "date": null, "content": "RBDL ORB - Rigid Body Dynamics Library (ORB Version) Copyright (c) 2018-2021 Felix Richter felix.richter@informatik.uni-heidelberg.de\nIntroduction\nRBDL is a highly efficient C++ library that contains some essential rigid body dynamics algorithms such as the Articulated Body Algorithm (ABA) for forward dynamics, Recursive Newton-Euler Algorithm (RNEA) for inverse dynamics and the Composite Rigid Body Algorithm (CRBA) for the efficient computation of the joint space inertia matrix. It further contains code for Jacobians, forward and inverse kinematics, handling of external constraints such as contacts and collisions, and closed-loop models.\nThe code was originally developed by Martin Felis martin@fysx.org at the research group Optimization in Robotics and Biomechanics (ORB) of the Interdisciplinary Center for Scientific Computing (IWR) and Institute of Computer Engineering at Heidelberg University. The code closely follows the notation used in Roy Featherstone's book \"Rigid Body Dynamics Algorithm\".\nThis repository contains the version of RBDL that is maintained by the members of the ORB research group.\nDocumentation\nThe documentation is contained in the code and can be extracted with the tool doxygen.\nTo create the documentation simply run\ndoxygen Doxyfile\nwhich will generate the documentation in the subdirectory ./doc/html. The main page will then be located in ./doc/html/index.html.\nVCPKG package manager (for Windows, Linux and Mac)\nAs of 08-2021 rbdl-orb is part of microsofts vcpkg, a tool to manage c++ dependencies on all major operating systems. The luamodel and urdfmodel addon are installed via vcpkg as well, other addons may be added in the future as well.\nInstall vcpkg by making a local clone from its GitHub repo https://github.com/Microsoft/vcpkg. Then run the vcpkg-bootstrapper script to set it up. For detailed installation instructions, see Install vcpkg. To integrate vcpkg with your Visual Studio or Visual Studio Code development environment, see Integrate vcpkg. Then, to use vcpkg to install or update a library, see Manage libraries with vcpkg. For more information about vcpkg commands, see vcpkg command-line reference.\nBuilding RBDL from Source\nThe official rbdl-orb git repository can be cloned from\nhttps://github.com/ORB-HD/rbdl-orb\n(See https://git-scm.com/downloads/guis/ for git clients.)\nTo make sure all submodules are correctly downloaded, clone the repository recursively!\ngit clone --recurive https://github.com/ORB-HD/rbdl-orb\nUpgrading from an older version of RBDL\nFor convenience there is a script to upgrade to the newest RBDL repository version.\n./upgrade.sh\nIt pulls the latest commits from master and also checks out the correct version of all sub repositories. Manual upgrading requires doing the following:\ngit pull origin master\ngit submodule update --init\nBuilding and Installation\nLinux: RBDL\nPrior to installation update the apt system. Open a terminal and type\nsudo apt update\nsudo apt upgrade\nInstall git\nsudo apt install git-core\nInstall cmake\nsudo apt install cmake\nInstall Eigen3 RBDL uses Eigen3 for efficient computations (http://eigen.tuxfamily.org).\nsudo apt install libeigen3-dev\nInstall a c++ compiler The choice of compiler can have a large effect on performance. Consider evaluating a few different compilers, such as Clang, for the best performance.\nsudo apt-get install build-essential\nInstall cmake-curses (optional) If you are planning on taking advantage of the many addons and other build options we recommend that you use cmake-curses as it makes the build configuration process faster and less prone to error.\nsudo apt install cmake-curses-gui\nInstall Catch2 (optional) Install Catch2 if you want to run RBDL's test code.\nAt the moment most linux distributions do not have catch2 in their repositories yet. So the recommended install approach is to build it from source.\n$ git clone --branch v2.x https://github.com/catchorg/Catch2.git\n$ cd Catch2\n$ cmake -Bbuild -H. -DBUILD_TESTING=OFF\n$ sudo cmake --build build/ --target install\nBuild RBDL using CMake (http://www.cmake.org). To compile the library in a separate directory in Release mode use:\nmkdir /rbdl-build\ncd rbdl-build/\ncmake -D CMAKE_BUILD_TYPE=Release ../rbdl\nmake\nIf you have installed cmake-curses-gui you can see all of the available build options by running cmake-curses\nmkdir /rbdl-build\ncd rbdl-build/\nccmake ../rbdl\nat which point you will see full list of build options for RBDL. We recommend that you build and run RBDL's test code at least once by building RBDL with\nRBDL_BUILD_TESTS ON\nRUN_AUTOMATIC_TESTS ON\nLinux: RBDL's documentation\nInstall doxygen\nsudo apt install doxygen\nBuild the doxygen:\nOpen a terminal in the RBDL source directory and type\ndoxygen Doxyfile\nOpen the file doc/html/index.html in a web-browser.\nLinux: RBDL's examples\nInstall Boost (optional) Boost is needed to run many of the example simulations that come with RBDL.\nsudo apt install libboost-all-dev\nLinux: RBDL's addon dependencies\nluamodel addon:\nIf you'd like to load model files written in Lua to RBDL. Without this addon you will need to build models programmatically, or read them in using the URDF addon. To do so:\nInstall Lua51\nsudo apt install lua5.1\nsudo apt install liblua5.1-0-dev\nBuild RBDL with\nRBDL_BUILD_ADDON_LUAMODEL ON\nurdf addon\nIf you'd like to load model files written in URDF to RBDL. This addon uses the URDF_Parser library which is included as a submodule. You will need to have cloned the repository recursively! If you missed doing that you can intialize the submodules (from a terminal within the source directory) after the fact with:\ngit submodule init\ngit submodule update\nBuild RBDL with\nRBDL_BUILD_ADDON_URDFREADER ON\nmuscle addon\nIf you'd like to include muscles in your RBDL muscles, such as those in Millard et al., then build RBDL with\nRBDL_BUILD_ADDON_GEOMETRY ON\nRBDL_BUILD_ADDON_MUSCLE ON\nThe geometry addon is a dependency which cmake will automatically include\nMillard M, Emonds AL, Harant M, Mombaur K. A reduced muscle model and planar musculoskeletal model fit for the simulation of whole-body movements. Journal of biomechanics. 2019 Apr 10.\nmuscle addon: muscle fitting option\nIf you'd like to make use of the muscle fitting algorithms detailed in Millard et al.\nInstall Ipopt. One of the easier ways to do this is to follow these instructions from Ipopt's online documentation which guides you through the process. Instructions to build the code appear in the README located in the Ipopt folder\nConfigure RBDL's cmake file with these flags set to 'On'\nRBDL_BUILD_ADDON_GEOMETRY ON\nRBDL_BUILD_ADDON_LUAMODEL ON\nRBDL_BUILD_ADDON_MUSCLE ON\nRBDL_BUILD_ADDON_MUSCLE_FITTING ON\nSet the CUSTOM_IPOPT_PATH to the main Ipopt directory.\nBuild RBDL\nUpdate your .bashrc file so that Ipopt's lib folder is in LD_LIBRARY_PATH\nexport IPOPT_HOME=/home/mjhmilla/dev/Ipopt-3.12.8\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$IPOPT_HOME/lib\nAs of March 2019 all of the muscle fitting code has been tested with Ipopt-3.12.8.\nMillard M, Emonds AL, Harant M, Mombaur K. A reduced muscle model and planar musculoskeletal model fit for the simulation of whole-body movements. Journal of biomechanics. 2019 Apr 10.\nWindows\nAlthough RBDL can be installed on Windows, none of the ORB members currently uses Windows and so we are unable to provide detailed instructions.\nPython Bindings\nRBDL can also build an experimental python wrapper that works with python 3 and python 2. To do this enable the the RBDL_BUILD_PYTHON_WRAPPER cmake options. This will build the wrapper for python 3, if you want to use python 2 instead you will also have to enable the RBDL_USE_PYTHON_2 cmake option. The result of this is an extra python directory in the build directory. From within which you can install it using setup.py. This is done automatically when using make install\nLinux: Python wrapper dependencies\nInstall Python3, NumPy, SciPy, & Matplotlib (optional) Most of RBDL is accessible through Python. If you are interested in using the RBDL through Python these instructions:\nIf you are using Ubuntu 18.04 or later python3 comes pre-installed.\nTo check if you have python3, in a command shell type\npython3 -V\nIf you already have python3 installed system-wide then you can get the remaining libraries with\nsudo apt install cython3 python3-numpy python3-scipy python3-matplotlib\nIf you are not using Ubuntu 18.04, and do not currently have python3, please look for instructions online to install these libraries on your system.\nBuild and install RBDL with the\nRBDL_BUILD_PYTHON_WRAPPER : ON\n(Note: you may need sudo privileges to install the rbdl.egg_info file to usr/local/lib/python directory.) 3. Add RBDL to Python's path Update your .bashrc file so that python can find the python version of rbdl. To do this you need to add the path to 'rbdl-build/python' to the PYTHONPATH which can be done by adding the following line to your .bashrc file.\nexport PYTHONPATH=$PYTHONPATH:<path-to-the-RBDL-build-directory>/python\nResources to learn more\nThere are four main ways to learn about anything that appears in RBDL:\nThe examples folder\nThere are a set of deep-dive examples which are accompanied by detailed documentation: if you are new to RBDL start here first.\nThere are also a set of minimalistic examples\nThe examples cover the basics reasonably well, but currently many advanced items (quaternion joints, custom-joints, custom-constraints, muscle-fitting) do not have examples.\nThe Doxygen documentation\nThe Doxygen for methods and components that were developed recently are covered in great detail (e.g. the Millard2016TorqueMuscle class in the muscle addon).\nDoxygen for more well established methods are more sparsely documented.\nThe test code;\nA minimalistic example of every command and modeling component can be found in the test code (e.g. in rbdl/tests, addons/geometry/tests, addons/muscle/tests, etc).\nA specific command can be easily found by using a text editor that can search an entire directory (e.g. sublime text) of text files for a keyword.\nThe literature.\nIn addition to Featherstone's text and Felis's papers there are a number of exciting methods and modeling tools which are included in RBDL.\nThe appropriate literature references are mentioned in the doxygen for the method in question.\nCitation\nAn overview of the theoretical and implementation details has been published in [https://doi.org/10.1007/s10514-016-9574-0](Felis, M.L. Auton Robot (2017) 41: 495). To cite RBDL in your academic research you can use the following BibTeX entry:\n@Article{Felis2016,\nauthor=\"Felis, Martin L.\",\ntitle=\"RBDL: an efficient rigid-body dynamics library using recursive algorithms\",\njournal=\"Autonomous Robots\",\nyear=\"2016\",\npages=\"1--17\",\nissn=\"1573-7527\",\ndoi=\"10.1007/s10514-016-9574-0\",\nurl=\"http://dx.doi.org/10.1007/s10514-016-9574-0\"\n}\nLicensing\nThe library is published under the very permissive zlib free software license which should allow you to use the software wherever you need.\nThis is the full license text (zlib license):\nRBDL - Rigid Body Dynamics Library\nCopyright (c) 2011-2020 Martin Felis <martin@fysx.org>\nThis software is provided 'as-is', without any express or implied\nwarranty. In no event will the authors be held liable for any damages\narising from the use of this software.\nPermission is granted to anyone to use this software for any purpose,\nincluding commercial applications, and to alter it and redistribute it\nfreely, subject to the following restrictions:\n1. The origin of this software must not be misrepresented; you must not\nclaim that you wrote the original software. If you use this software\nin a product, an acknowledgment in the product documentation would be\nappreciated but is not required.\n2. Altered source versions must be plainly marked as such, and must not\nbe misrepresented as being the original software.\n3. This notice may not be removed or altered from any source\ndistribution.\nAcknowledgements\nWork on this library was originally funded by the Heidelberg Graduate School of Mathematical and Computational Methods for the Sciences (HGS), and the European FP7 projects ECHORD (grant number 231143) and Koroibot (grant number 611909).\nWork on the geometry and muscle addons was completed by Matthew Millard. Financial support from Deutsche Forschungs Gemeinschaft grant no. MI 2109/1-1 and from the European Commission within the H2020 project Spexor (GA 687662) is gratefully acknowledged.", "link": "https://github.com/ORB-HD/rbdl-orb", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "rbdl orb - rigid body dynamics library (orb version) copyright (c) 2018-2021 felix richter felix.richter@informatik.uni-heidelberg.de\nintroduction\nrbdl is a highly efficient c++ library that contains some essential rigid body dynamics algorithms such as the articulated body algorithm (aba) for forward dynamics, recursive newton-euler algorithm (rnea) for inverse dynamics and the composite rigid body algorithm (crba) for the efficient computation of the joint space inertia matrix. it further contains code for jacobians, forward and inverse kinematics, handling of external constraints such as contacts and collisions, and closed-loop models.\nthe code was originally developed by martin felis martin@fysx.org at the research group optimization in robotics and biomechanics (orb) of the interdisciplinary center for scientific computing (iwr) and institute of computer engineering at heidelberg university. the code closely follows the notation used in roy featherstone's book \"rigid body dynamics algorithm\".\nthis repository contains the version of rbdl that is maintained by the members of the orb research group.\ndocumentation\nthe documentation is contained in the code and can be extracted with the -----> tool !!!  doxygen.\nto create the documentation simply run\ndoxygen doxyfile\nwhich will generate the documentation in the subdirectory ./doc/html. the main page will then be located in ./doc/html/index.html.\nvcpkg package manager (for windows, linux and mac)\nas of 08-2021 rbdl-orb is part of microsofts vcpkg, a tool to manage c++ dependencies on all major operating systems. the luamodel and urdfmodel addon are installed via vcpkg as well, other addons may be added in the future as well.\ninstall vcpkg by making a local clone from its github repo https://github.com/microsoft/vcpkg. then run the vcpkg-bootstrapper script to set it up. for detailed installation instructions, see install vcpkg. to integrate vcpkg with your visual studio or visual studio code development environment, see integrate vcpkg. then, to use vcpkg to install or update a library, see manage libraries with vcpkg. for more information about vcpkg commands, see vcpkg command-line reference.\nbuilding rbdl from source\nthe official rbdl-orb git repository can be cloned from\nhttps://github.com/orb-hd/rbdl-orb\n(see https://git-scm.com/downloads/guis/ for git clients.)\nto make sure all submodules are correctly downloaded, clone the repository recursively!\ngit clone --recurive https://github.com/orb-hd/rbdl-orb\nupgrading from an older version of rbdl\nfor convenience there is a script to upgrade to the newest rbdl repository version.\n./upgrade.sh\nit pulls the latest commits from master and also checks out the correct version of all sub repositories. manual upgrading requires doing the following:\ngit pull origin master\ngit submodule update --init\nbuilding and installation\nlinux: rbdl\nprior to installation update the apt system. open a terminal and type\nsudo apt update\nsudo apt upgrade\ninstall git\nsudo apt install git-core\ninstall cmake\nsudo apt install cmake\ninstall eigen3 rbdl uses eigen3 for efficient computations (http://eigen.tuxfamily.org).\nsudo apt install libeigen3-dev\ninstall a c++ compiler the choice of compiler can have a large effect on performance. consider evaluating a few different compilers, such as clang, for the best performance.\nsudo apt-get install build-essential\ninstall cmake-curses (optional) if you are planning on taking advantage of the many addons and other build options we recommend that you use cmake-curses as it makes the build configuration process faster and less prone to error.\nsudo apt install cmake-curses-gui\ninstall catch2 (optional) install catch2 if you want to run rbdl's test code.\nat the moment most linux distributions do not have catch2 in their repositories yet. so the recommended install approach is to build it from source.\n$ git clone --branch v2.x https://github.com/catchorg/catch2.git\n$ cd catch2\n$ cmake -bbuild -h. -dbuild_testing=off\n$ sudo cmake --build build/ --target install\nbuild rbdl using cmake (http://www.cmake.org). to compile the library in a separate directory in release mode use:\nmkdir /rbdl-build\ncd rbdl-build/\ncmake -d cmake_build_type=release ../rbdl\nmake\nif you have installed cmake-curses-gui you can see all of the available build options by running cmake-curses\nmkdir /rbdl-build\ncd rbdl-build/\nccmake ../rbdl\nat which point you will see full list of build options for rbdl. we recommend that you build and run rbdl's test code at least once by building rbdl with\nrbdl_build_tests on\nrun_automatic_tests on\nlinux: rbdl's documentation\ninstall doxygen\nsudo apt install doxygen\nbuild the doxygen:\nopen a terminal in the rbdl source directory and type\ndoxygen doxyfile\nopen the file doc/html/index.html in a web-browser.\nlinux: rbdl's examples\ninstall boost (optional) boost is needed to run many of the example simulations that come with rbdl.\nsudo apt install libboost-all-dev\nlinux: rbdl's addon dependencies\nluamodel addon:\nif you'd like to load model files written in lua to rbdl. without this addon you will need to build models programmatically, or read them in using the urdf addon. to do so:\ninstall lua51\nsudo apt install lua5.1\nsudo apt install liblua5.1-0-dev\nbuild rbdl with\nrbdl_build_addon_luamodel on\nurdf addon\nif you'd like to load model files written in urdf to rbdl. this addon uses the urdf_parser library which is included as a submodule. you will need to have cloned the repository recursively! if you missed doing that you can intialize the submodules (from a terminal within the source directory) after the fact with:\ngit submodule init\ngit submodule update\nbuild rbdl with\nrbdl_build_addon_urdfreader on\nmuscle addon\nif you'd like to include muscles in your rbdl muscles, such as those in millard et al., then build rbdl with\nrbdl_build_addon_geometry on\nrbdl_build_addon_muscle on\nthe geometry addon is a dependency which cmake will automatically include\nmillard m, emonds al, harant m, mombaur k. a reduced muscle model and planar musculoskeletal model fit for the simulation of whole-body movements. journal of biomechanics. 2019 apr 10.\nmuscle addon: muscle fitting option\nif you'd like to make use of the muscle fitting algorithms detailed in millard et al.\ninstall ipopt. one of the easier ways to do this is to follow these instructions from ipopt's online documentation which guides you through the process. instructions to build the code appear in the readme located in the ipopt folder\nconfigure rbdl's cmake file with these flags set to 'on'\nrbdl_build_addon_geometry on\nrbdl_build_addon_luamodel on\nrbdl_build_addon_muscle on\nrbdl_build_addon_muscle_fitting on\nset the custom_ipopt_path to the main ipopt directory.\nbuild rbdl\nupdate your .bashrc file so that ipopt's lib folder is in ld_library_path\nexport ipopt_home=/home/mjhmilla/dev/ipopt-3.12.8\nexport ld_library_path=$ld_library_path:$ipopt_home/lib\nas of march 2019 all of the muscle fitting code has been tested with ipopt-3.12.8.\nmillard m, emonds al, harant m, mombaur k. a reduced muscle model and planar musculoskeletal model fit for the simulation of whole-body movements. journal of biomechanics. 2019 apr 10.\nwindows\nalthough rbdl can be installed on windows, none of the orb members currently uses windows and so we are unable to provide detailed instructions.\npython bindings\nrbdl can also build an experimental python wrapper that works with python 3 and python 2. to do this enable the the rbdl_build_python_wrapper cmake options. this will build the wrapper for python 3, if you want to use python 2 instead you will also have to enable the rbdl_use_python_2 cmake option. the result of this is an extra python directory in the build directory. from within which you can install it using setup.py. this is done automatically when using make install\nlinux: python wrapper dependencies\ninstall python3, numpy, scipy, & matplotlib (optional) most of rbdl is accessible through python. if you are interested in using the rbdl through python these instructions:\nif you are using ubuntu 18.04 or later python3 comes pre-installed.\nto check if you have python3, in a command shell type\npython3 -v\nif you already have python3 installed system-wide then you can get the remaining libraries with\nsudo apt install cython3 python3-numpy python3-scipy python3-matplotlib\nif you are not using ubuntu 18.04, and do not currently have python3, please look for instructions online to install these libraries on your system.\nbuild and install rbdl with the\nrbdl_build_python_wrapper : on\n(note: you may need sudo privileges to install the rbdl.egg_info file to usr/local/lib/python directory.) 3. add rbdl to python's path update your .bashrc file so that python can find the python version of rbdl. to do this you need to add the path to 'rbdl-build/python' to the pythonpath which can be done by adding the following line to your .bashrc file.\nexport pythonpath=$pythonpath:<path-to-the-rbdl-build-directory>/python\nresources to learn more\nthere are four main ways to learn about anything that appears in rbdl:\nthe examples folder\nthere are a set of deep-dive examples which are accompanied by detailed documentation: if you are new to rbdl start here first.\nthere are also a set of minimalistic examples\nthe examples cover the basics reasonably well, but currently many advanced items (quaternion joints, custom-joints, custom-constraints, muscle-fitting) do not have examples.\nthe doxygen documentation\nthe doxygen for methods and components that were developed recently are covered in great detail (e.g. the millard2016torquemuscle class in the muscle addon).\ndoxygen for more well established methods are more sparsely documented.\nthe test code;\na minimalistic example of every command and modeling component can be found in the test code (e.g. in rbdl/tests, addons/geometry/tests, addons/muscle/tests, etc).\na specific command can be easily found by using a text editor that can search an entire directory (e.g. sublime text) of text files for a keyword.\nthe literature.\nin addition to featherstone's text and felis's papers there are a number of exciting methods and modeling tools which are included in rbdl.\nthe appropriate literature references are mentioned in the doxygen for the method in question.\ncitation\nan overview of the theoretical and implementation details has been published in [https://doi.org/10.1007/s10514-016-9574-0](felis, m.l. auton robot (2017) 41: 495). to cite rbdl in your academic research you can use the following bibtex entry:\n@article{felis2016,\nauthor=\"felis, martin l.\",\ntitle=\"rbdl: an efficient rigid-body dynamics library using recursive algorithms\",\njournal=\"autonomous robots\",\nyear=\"2016\",\npages=\"1--17\",\nissn=\"1573-7527\",\ndoi=\"10.1007/s10514-016-9574-0\",\nurl=\"http://dx.doi.org/10.1007/s10514-016-9574-0\"\n}\nlicensing\nthe library is published under the very permissive zlib free software license which should allow you to use the software wherever you need.\nthis is the full license text (zlib license):\nrbdl - rigid body dynamics library\ncopyright (c) 2011-2020 martin felis <martin@fysx.org>\nthis software is provided 'as-is', without any express or implied\nwarranty. in no event will the authors be held liable for any damages\narising from the use of this software.\npermission is granted to anyone to use this software for any purpose,\nincluding commercial applications, and to alter it and redistribute it\nfreely, subject to the following restrictions:\n1. the origin of this software must not be misrepresented; you must not\nclaim that you wrote the original software. if you use this software\nin a product, an acknowledgment in the product documentation would be\nappreciated but is not required.\n2. altered source versions must be plainly marked as such, and must not\nbe misrepresented as being the original software.\n3. this notice may not be removed or altered from any source\ndistribution.\nacknowledgements\nwork on this library was originally funded by the heidelberg graduate school of mathematical and computational methods for the sciences (hgs), and the european fp7 projects echord (grant number 231143) and koroibot (grant number 611909).\nwork on the geometry and muscle addons was completed by matthew millard. financial support from deutsche forschungs gemeinschaft grant no. mi 2109/1-1 and from the european commission within the h2020 project spexor (ga 687662) is gratefully acknowledged.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000494, "year": null}, {"Unnamed: 0": 1520, "autor": 500, "date": null, "content": "OpenRooms Dataset Release\nZhengqin Li, Ting-Wei Yu, Shen Sang, Sarah Wang, Meng Song, Yuhan Liu, Yu-Ying Yeh, Rui Zhu, Nitesh Gundavarapu, Jia Shi, Sai Bi, Zexiang Xu, Hong-Xing Yu, Kalyan Sunkavalli, Milo\u0161 Ha\u0161an, Ravi Ramamoorthi, Manmohan Chandraker\nNews\n[09/10/21]. All the rendered ground-truths are available for downloading.\n[08/07/21]. Please send an email to OpenRoomsDataset@gmail.com if you hope to receive the newest update.\n[06/29/21]. We released two demo scenes with all ground-truths (Demo and Demo.zip). Please use the two scenes to determine usage for your applications.\n[05/19/21]. We released all rendered images (Images and Images.zip).\nAbout\nThis is the release webpage for the OpenRooms dataset. We first introduce the rendered images and various ground-truths. Next, we introduce how to render your own images based on the OpenRooms dataset creation framework. For each type of data, we offer two kinds of formats, zip files and individual folders, so users may choose whether to download the whole dataset more efficiently or download individual folders for different scenes. We recommend using Rclone to avoid slow or unstable downloads.\nOpenRooms is a collaboration between researchers from UCSD and Adobe. We acknowledge generous support from NSF, ONR, Adobe and Google. For any questions, please email: openroomsdataset@gmail.com.\nDataset Overview\nWe render six versions of images for all the scenes. Those rendered results are saved in 6 folders: main_xml, main_xml1, mainDiffMat_xml, mainDiffMat_xml1, mainDiffLight_xml and mainDiffLight_xml1. All 6 versions are built with the same CAD models. main_xml, mainDiffMat_xml, mainDiffLight_xml share one set of camera views while main_xml1, mainDiffMat_xml1 and mainDiffLight_xml1 share the other set of camera views. main_xml(1) and mainDiffMat_xml(1) have the same lighting but different materials while main_xml(1) and mainDiffLight_xml(1) have the same materials but different lighting. Both the lighting and material configuration of main_xml and main_xml1 are different. We believe this configuration can potentially help us develope novel applications for image editing. Two example scenes from main_xml, mainDiffMat_xml and mainDiffLight_xml are shown in the below.\nNews: We currently only release the rendered images of the dataset. All ground-truths will be released in a few days. The dataset creation pipeline will also be released soon.\nRendered Images and Ground-truths\nAll rendered images and the corresponding ground-truths are saved in folder data/rendering/data/. In the following, we will detail each type of rendered data and how to read and interpret them. Two example scenes with images and all ground-truths are included in Demo and Demo.zip. The training/testing split of the scenes can be found in train.txt and test.txt.\nImage and Image.zip: The 480 \u00d7 640 HDR images im_*.hdr, which can be read with the python command.\nim = cv2.imread('im_1.hdr', -1)[:, :, ::-1]\nWe render images for main_xml(1), mainDiffMat_xml(1) and mainDiffLight_xml(1).\nMaterial and Material.zip: The 480 \u00d7 640 diffuse albedo maps imbaseColor_*.png and roughness map imroughness_*.png. Note that the diffuse albedo map is saved in sRGB space. To load it into linear RGB space, we can use the following python commands. The roughness map is saved in linear space and can be read directly.\nim = cv2.imread('imbaseColor_1.hdr')[:, :, ::-1]\nim = (im.astype(np.float32 ) / 255.0) ** (2.2)\nWe only render the diffuse albedo maps and roughness maps for main_xml(1) and mainDiffMat_xml(1) because mainDiffLight_xml(1) share the same material maps with the main_xml(1).\nGeometry and Geometry.zip: The 480 \u00d7 640 normal maps imnomral_*.png and depth maps imdepth_*.dat. The R, G, B channel of the normal map corresponds to right, up, backward direction of the image plane. To load the depth map, we can use the following python commands.\nwith open('imdepth_1.dat', 'rb') as fIn:\n# Read the height and width of depth\nhBuffer = fIn.read(4)\nheight = struct.unpack('i', hBuffer)[0]\nwBuffer = fIn.read(4)\nwidth = struct.unpack('i', wBuffer)[0]\n# Read depth\ndBuffer = fIn.read(4 * width * height )\ndepth = np.array(\nstruct.unpack('f' * height * width, dBuffer ),\ndtype=np.float32 )\ndepth = depth.reshape(height, width)\nWe render normal maps for main_xml(1) and mainDiffMat_xml(1), and depth maps for main_xml(1).\nMask and Mask.zip: The 480 \u00d7 460 grey scale mask immask_*.png for light sources. The pixel value 0 represents the region of environment maps. The pixel value 0.5 represents the region of lamps. Otherwise, the pixel value will be 1. We render the ground-truth masks for main_xml(1) and mainDiffLight_xml(1).\nSVLighting and SVLighting.zip: The (120 \u00d7 16) \u00d7 (160 \u00d7 32) per-pixel environment maps imenv_*.hdr. The spatial resolution is 120 x 160 while the environment map resolution is 16 x 32. To read the per-pixel environment maps, we can use the following python commands.\n# Read the envmap of resolution 1920 x 5120 x 3 in RGB format\nenv = cv2.imread('imenv_1', -1)[:, :, ::-1]\n# Reshape and permute the per-pixel environment maps\nenv = env.reshape(120, 16, 160, 32, 3)\nenv = env.transpose(0, 2, 1, 3, 4)\nWe render per-pixel environment maps for main_xml(1), mainDiffMat_xml(1) and mainDiffLight_xml(1). Since the total size of per-pixel environment maps is 4.0 TB, we do not provide an extra .zip format for downloading. Please consider using the tool Rclone if you hope to download all the per-pixel environment maps.\nSVSG and SVSG.zip: The ground-truth spatially-varying spherical Gaussian (SG) parameters imsgEnv_*.h5, computed from this optimization code. We generate the ground-truth SG parameters for main_xml(1), mainDiffMat_xml(1) and mainDiffLight_xml(1). For the detailed format, please refer to the optimization code.\nShading and Shading.zip: The 120 \u00d7 160 diffuse shading imshading_*.hdr computed by intergrating the per-pixel environment maps. We render shading for main_xml(1), mainDiffMat_xml(1) and mainDiffLight_xml(1).\nSVLightingDirect and SVLightingDirect.zip: The (30 \u00d7 16) \u00d7 (40 \u00d7 32) per-pixel environment maps with direct illumination imenvDirect_*.hdr only. The spatial resolution is 30 \u00d7 40 while the environment maps resolution is 16 \u00d7 32. The direct per-pixel environment maps can be load the same way as the per-pixel environment maps. We only render direct per-pixel environment maps for main_xml(1) and mainDiffLight_xml(1) because the direct illumination of mainDiffMat_xml(1) is the same as main_xml(1).\nShadingDirect and ShadingDirect.zip: The 120 \u00d7 160 direct shading imshadingDirect_*.rgbe. To load the direct shading, we can use the following python command.\nim = cv2.imread('imshadingDirect_1.rgbe', -1)[:, :, ::-1]\nAgain, we only render direct shading for main_xml(1) and mainDiffLight_xml(1)\nSemanticLabel and SemanticLabel.zip: The 480 \u00d7 640 semantic segmentation label imsemLabel_*.npy. We provide semantic labels for 45 classes of commonly seen objects and layout for indoor scenes. The 45 classes can be found in semanticLabels.txt. We only render the semantic labels for main_xml(1).\nLightSource and LightSource.zip: The light source information, including geometry, shadow and direct shading of each light source. In each scene directory, light_x directory corresponds to im_x.hdr, where x = 0, 1, 2, 3 ... In each light_x directory, you will see files with numbers in their names. The numbers correspond to the light source ID, i.e. if the IDs are from 0 to 4, then there are 5 light sources in this scene.\nGeometry: We provide geometry annotation for windows and lamps box_*.dat for main_xml(1) only. To read the annotation, we can use the following python commmands.\nwith open('box_0.dat', 'rb') as fIn:\ninfo = pickle.load(fIn )\nThere are 3 items saved in the dictionary, which we list blow.\nisWindow: True if the light source is a window, false if the light source is a lamp.\nbox3D: The 3D bounding box of the light source, including center center, orientation xAxis, yAxis, zAxis and size xLen, yLen, zLen.\nbox2D: The 2D bounding box of the light source on the image plane x1, y1, x2, y2.\nMask: The 120 \u00d7 160 2D binary masks for light sources mask*.png. We only provide the masks for main_xml(1).\nDirect shading: The 120 \u00d7 160 direct shading for each light source imDS*.rgbe. We provide the direction shading for main_xml(1) and mainDiffLight_xml(1).\nDirect shading without occlusion: The 120 \u00d7 160 direct shading without occlusion for each light source imNoOcclu*.rgbe. We provide the direction shading for main_xml(1) and mainDiffLight_xml(1).\nShadow: The 120 \u00d7 160 shadow maps for each light source imShadow*.png. We render the shadow map for main_xml(1) only.\nFriction and Friction.zip: The friction coefficients computed from our SVBRDF following the method proposed by Zhang et al. We compute the friction coefficients for main_xml(1) and mainDiffLight_xml(1)\nDataset Creation\nGPU renderer: The Optix-based GPU path tracer for rendering. Please refer to the github repository for detailed instructions.\nTileable texture synthesis: The tielable texture synthesis code to make sure that the SVBRDF maps are tileable. Please refer to the github repository for more details.\nSpherical gaussian optimization: The code to fit per-pixel environment map with spherical Gaussian lobes, using LBFGS optimization. Please refer to the github repository for detailed instructions.\nThe CAD models, environment maps, materials and code required to recreate the dataset will be released soon.\nApplications\nInverse Rendering: Trained on our dataset, we achieved state-of-the-arts on some inverse rendering metrics, especially the lighting estimation. Please refer to our github repository for the training and testing code.\nRobotics: Our robotics applications will come soon.\nRelated Datasets\nThe OpenRooms dataset is built on several prior works, as noted below. Please refer to them for the respective license details and terms of use when creating your own dataset using OpenRooms.\nScanNet dataset: The real 3D scans of indoor scenes.\nScan2cad dataset: The alignment of CAD models to the scanned point clouds.\nLaval outdoor lighting dataset: HDR outdoor environment maps\nHDRI Haven lighting dataset: HDR outdoor environment maps\nPartNet dataset: CAD models\nAdobe Stock: High-quality microfacet SVBRDF texture maps. Please license the materials from Adobe Stock.", "link": "https://github.com/ViLab-UCSD/OpenRooms", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "openrooms dataset release\nzhengqin li, ting-wei yu, shen sang, sarah wang, meng song, yuhan liu, yu-ying yeh, rui zhu, nitesh gundavarapu, jia shi, sai bi, zexiang xu, hong-xing yu, kalyan sunkavalli, milo\u0161 ha\u0161an, ravi ramamoorthi, manmohan chandraker\nnews\n[09/10/21]. all the rendered ground-truths are available for downloading.\n[08/07/21]. please send an email to openroomsdataset@gmail.com if you hope to receive the newest update.\n[06/29/21]. we released two demo scenes with all ground-truths (demo and demo.zip). please use the two scenes to determine usage for your applications.\n[05/19/21]. we released all rendered images (images and images.zip).\nabout\nthis is the release webpage for the openrooms dataset. we first introduce the rendered images and various ground-truths. next, we introduce how to render your own images based on the openrooms dataset creation framework. for each type of data, we offer two kinds of formats, zip files and individual folders, so users may choose whether to download the whole dataset more efficiently or download individual folders for different scenes. we recommend using rclone to avoid slow or unstable downloads.\nopenrooms is a collaboration between researchers from ucsd and adobe. we acknowledge generous support from nsf, onr, adobe and google. for any questions, please email: openroomsdataset@gmail.com.\ndataset overview\nwe render six versions of images for all the scenes. those rendered results are saved in 6 folders: main_xml, main_xml1, maindiffmat_xml, maindiffmat_xml1, maindifflight_xml and maindifflight_xml1. all 6 versions are built with the same cad models. main_xml, maindiffmat_xml, maindifflight_xml share one set of camera views while main_xml1, maindiffmat_xml1 and maindifflight_xml1 share the other set of camera views. main_xml(1) and maindiffmat_xml(1) have the same lighting but different materials while main_xml(1) and maindifflight_xml(1) have the same materials but different lighting. both the lighting and material configuration of main_xml and main_xml1 are different. we believe this configuration can potentially help us develope novel applications for image editing. two example scenes from main_xml, maindiffmat_xml and maindifflight_xml are shown in the below.\nnews: we currently only release the rendered images of the dataset. all ground-truths will be released in a few days. the dataset creation pipeline will also be released soon.\nrendered images and ground-truths\nall rendered images and the corresponding ground-truths are saved in folder data/rendering/data/. in the following, we will detail each type of rendered data and how to read and interpret them. two example scenes with images and all ground-truths are included in demo and demo.zip. the training/testing split of the scenes can be found in train.txt and test.txt.\nimage and image.zip: the 480 \u00d7 640 hdr images im_*.hdr, which can be read with the python command.\nim = cv2.imread('im_1.hdr', -1)[:, :, ::-1]\nwe render images for main_xml(1), maindiffmat_xml(1) and maindifflight_xml(1).\nmaterial and material.zip: the 480 \u00d7 640 diffuse albedo maps imbasecolor_*.png and roughness map imroughness_*.png. note that the diffuse albedo map is saved in srgb space. to load it into linear rgb space, we can use the following python commands. the roughness map is saved in linear space and can be read directly.\nim = cv2.imread('imbasecolor_1.hdr')[:, :, ::-1]\nim = (im.astype(np.float32 ) / 255.0) ** (2.2)\nwe only render the diffuse albedo maps and roughness maps for main_xml(1) and maindiffmat_xml(1) because maindifflight_xml(1) share the same material maps with the main_xml(1).\ngeometry and geometry.zip: the 480 \u00d7 640 normal maps imnomral_*.png and depth maps imdepth_*.dat. the r, g, b channel of the normal map corresponds to right, up, backward direction of the image plane. to load the depth map, we can use the following python commands.\nwith open('imdepth_1.dat', 'rb') as fin:\n# read the height and width of depth\nhbuffer = fin.read(4)\nheight = struct.unpack('i', hbuffer)[0]\nwbuffer = fin.read(4)\nwidth = struct.unpack('i', wbuffer)[0]\n# read depth\ndbuffer = fin.read(4 * width * height )\ndepth = np.array(\nstruct.unpack('f' * height * width, dbuffer ),\ndtype=np.float32 )\ndepth = depth.reshape(height, width)\nwe render normal maps for main_xml(1) and maindiffmat_xml(1), and depth maps for main_xml(1).\nmask and mask.zip: the 480 \u00d7 460 grey scale mask immask_*.png for light sources. the pixel value 0 represents the region of environment maps. the pixel value 0.5 represents the region of lamps. otherwise, the pixel value will be 1. we render the ground-truth masks for main_xml(1) and maindifflight_xml(1).\nsvlighting and svlighting.zip: the (120 \u00d7 16) \u00d7 (160 \u00d7 32) per-pixel environment maps imenv_*.hdr. the spatial resolution is 120 x 160 while the environment map resolution is 16 x 32. to read the per-pixel environment maps, we can use the following python commands.\n# read the envmap of resolution 1920 x 5120 x 3 in rgb format\nenv = cv2.imread('imenv_1', -1)[:, :, ::-1]\n# reshape and permute the per-pixel environment maps\nenv = env.reshape(120, 16, 160, 32, 3)\nenv = env.transpose(0, 2, 1, 3, 4)\nwe render per-pixel environment maps for main_xml(1), maindiffmat_xml(1) and maindifflight_xml(1). since the total size of per-pixel environment maps is 4.0 tb, we do not provide an extra .zip format for downloading. please consider using the -----> tool !!!  rclone if you hope to download all the per-pixel environment maps.\nsvsg and svsg.zip: the ground-truth spatially-varying spherical gaussian (sg) parameters imsgenv_*.h5, computed from this optimization code. we generate the ground-truth sg parameters for main_xml(1), maindiffmat_xml(1) and maindifflight_xml(1). for the detailed format, please refer to the optimization code.\nshading and shading.zip: the 120 \u00d7 160 diffuse shading imshading_*.hdr computed by intergrating the per-pixel environment maps. we render shading for main_xml(1), maindiffmat_xml(1) and maindifflight_xml(1).\nsvlightingdirect and svlightingdirect.zip: the (30 \u00d7 16) \u00d7 (40 \u00d7 32) per-pixel environment maps with direct illumination imenvdirect_*.hdr only. the spatial resolution is 30 \u00d7 40 while the environment maps resolution is 16 \u00d7 32. the direct per-pixel environment maps can be load the same way as the per-pixel environment maps. we only render direct per-pixel environment maps for main_xml(1) and maindifflight_xml(1) because the direct illumination of maindiffmat_xml(1) is the same as main_xml(1).\nshadingdirect and shadingdirect.zip: the 120 \u00d7 160 direct shading imshadingdirect_*.rgbe. to load the direct shading, we can use the following python command.\nim = cv2.imread('imshadingdirect_1.rgbe', -1)[:, :, ::-1]\nagain, we only render direct shading for main_xml(1) and maindifflight_xml(1)\nsemanticlabel and semanticlabel.zip: the 480 \u00d7 640 semantic segmentation label imsemlabel_*.npy. we provide semantic labels for 45 classes of commonly seen objects and layout for indoor scenes. the 45 classes can be found in semanticlabels.txt. we only render the semantic labels for main_xml(1).\nlightsource and lightsource.zip: the light source information, including geometry, shadow and direct shading of each light source. in each scene directory, light_x directory corresponds to im_x.hdr, where x = 0, 1, 2, 3 ... in each light_x directory, you will see files with numbers in their names. the numbers correspond to the light source id, i.e. if the ids are from 0 to 4, then there are 5 light sources in this scene.\ngeometry: we provide geometry annotation for windows and lamps box_*.dat for main_xml(1) only. to read the annotation, we can use the following python commmands.\nwith open('box_0.dat', 'rb') as fin:\ninfo = pickle.load(fin )\nthere are 3 items saved in the dictionary, which we list blow.\niswindow: true if the light source is a window, false if the light source is a lamp.\nbox3d: the 3d bounding box of the light source, including center center, orientation xaxis, yaxis, zaxis and size xlen, ylen, zlen.\nbox2d: the 2d bounding box of the light source on the image plane x1, y1, x2, y2.\nmask: the 120 \u00d7 160 2d binary masks for light sources mask*.png. we only provide the masks for main_xml(1).\ndirect shading: the 120 \u00d7 160 direct shading for each light source imds*.rgbe. we provide the direction shading for main_xml(1) and maindifflight_xml(1).\ndirect shading without occlusion: the 120 \u00d7 160 direct shading without occlusion for each light source imnoocclu*.rgbe. we provide the direction shading for main_xml(1) and maindifflight_xml(1).\nshadow: the 120 \u00d7 160 shadow maps for each light source imshadow*.png. we render the shadow map for main_xml(1) only.\nfriction and friction.zip: the friction coefficients computed from our svbrdf following the method proposed by zhang et al. we compute the friction coefficients for main_xml(1) and maindifflight_xml(1)\ndataset creation\ngpu renderer: the optix-based gpu path tracer for rendering. please refer to the github repository for detailed instructions.\ntileable texture synthesis: the tielable texture synthesis code to make sure that the svbrdf maps are tileable. please refer to the github repository for more details.\nspherical gaussian optimization: the code to fit per-pixel environment map with spherical gaussian lobes, using lbfgs optimization. please refer to the github repository for detailed instructions.\nthe cad models, environment maps, materials and code required to recreate the dataset will be released soon.\napplications\ninverse rendering: trained on our dataset, we achieved state-of-the-arts on some inverse rendering metrics, especially the lighting estimation. please refer to our github repository for the training and testing code.\nrobotics: our robotics applications will come soon.\nrelated datasets\nthe openrooms dataset is built on several prior works, as noted below. please refer to them for the respective license details and terms of use when creating your own dataset using openrooms.\nscannet dataset: the real 3d scans of indoor scenes.\nscan2cad dataset: the alignment of cad models to the scanned point clouds.\nlaval outdoor lighting dataset: hdr outdoor environment maps\nhdri haven lighting dataset: hdr outdoor environment maps\npartnet dataset: cad models\nadobe stock: high-quality microfacet svbrdf texture maps. please license the materials from adobe stock.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000500, "year": null}, {"Unnamed: 0": 1534, "autor": 514, "date": null, "content": "DDS plugin for Eclipse zenoh\nBackground\nThe Data Distribution Service (DDS) is a standard for data-centric publish subscribe. Whilst DDS has been around for quite some time and has a long history of deployments in various industries, it has recently gained quite a bit of attentions thanks to its adoption by the Robotic Operating System (ROS2) -- where it is used for communication between ROS2 nodes.\nRobot Swarms and Edge Robotics\nAs mentioned above, ROS2 has adopted DDS as the mechanism to exchange data between nodes within and potentially across a robot. That said, due to some of the very core assumptions at the foundations of the DDS wire-protocol, beside the fact that it leverages UDP/IP multicast for communication, it is not so straightforward to scale DDS communication over a WAN or across multiple LANs. Zenoh, on the other hand was designed since its inception to operate at Internet Scale.\nThus, the main motivations to have a zenoh bridge for DDS are:\nFacilitate the interconnection of robot swarms.\nSupport use cases of edge robotics.\nGive the possibility to use zenoh's geo-distributed storage and query system to better manage robot's data.\nHow to build it\nIn order to build the zenoh bridge for DDS you need first to install the following dependencies:\nRust\nOn Linux, make sure the llvm and clang development packages are installed:\non Debians do: sudo apt install llvm-dev libclang-dev\non CentOS or RHEL do: sudo yum install llvm-devel clang-devel\non Alpine do: apk install llvm11-dev clang-dev\nCMake (to build CycloneDDS which is a native dependency)\nOnce these dependencies are in place, you may clone the repository on your machine:\n$ git clone https://github.com/eclipse-zenoh/zenoh-plugin-dds.git\n$ cd zenoh-plugin-dds\nYou can then choose between building the zenoh bridge for DDS:\nas a dynamically loaded plugin:\n$ cargo build --release -p zplugin-dds\nThe plugin shared library (*.so on Linux, *.dylib on Mac OS, *.dll on Windows) will be generated in the target/release subdirectory.\nor as a standalone executable binary:\n$ cargo build --release -p zenoh-bridge-dds\nThe zenoh-bridge-dds binary will be generated in the target/release sub-directory.\nROS2 package\nIf you're a ROS2 user, you can also build zenoh-bridge-dds as a ROS package running:\nrosdep install --from-paths . --ignore-src -r -y\ncolcon build --packages-select zenoh-bridge-dds\nThe rosdep command will automatically install Rust and clang as build dependencies.\nDocker image\nThe zenoh-bridge-dds standalone executable is also available as a Docker images for both amd64 and arm64. To get it, do:\ndocker pull eclipse/zenoh-bridge-dds:master for the master branch version\n\u26a0\ufe0f However, notice that it's usage is limited to Docker on Linux and using the --net host option.\nThe cause being that DDS uses UDP multicast and Docker doesn't support UDP multicast between a container and its host (see cases moby/moby#23659, moby/libnetwork#2397 or moby/libnetwork#552). The only known way to make it work is to use the --net host option that is only supported on Linux hosts.\nUsage: docker run --init --net host eclipse/zenoh-bridge-dds\nIt supports the same command line arguments than the zenoh-bridge-dds (see below or check with -h argument).\nFor a quick test with ROS2 turtlesim\nPrerequisites:\nA ROS2 environment (no matter the DDS implementation as soon as it implements the standard DDSI protocol - the default Eclipse CycloneDDS being just fine)\nThe turtlesim package\n1 host, 2 ROS domains\nFor a quick test on a single host, you can run the turtlesim_node and the turtle_teleop_key on distinct ROS domains. As soon as you run 2 zenoh-bridge-dds (1 per domain) the turtle_teleop_key can drive the turtlesim_node.\nHere are the commands to run:\nROS_DOMAIN_ID=1 ros2 run turtlesim turtlesim_node\nROS_DOMAIN_ID=2 ros2 run turtlesim turtle_teleop_key\n./target/release/zenoh-bridge-dds -d 1\n./target/release/zenoh-bridge-dds -d 2\nNotice that by default the 2 bridges will discover each other using UDP multicast.\n2 hosts, avoiding UDP multicast communication\nBy default DDS (and thus ROS2) uses UDP multicast for discovery and publications. But on some networks, UDP multicast is not or badly supported.\nIn such cases, deploying the zenoh-bridge-dds on both hosts will make it to:\nlimit the DDS discovery traffic, as detailled in this blog\nroute all the DDS publications made on UDP multicast by each node through the zenoh protocol that by default uses TCP.\nHere are the commands to test this configuration with turtlesim:\non host 1:\nROS_DOMAIN_ID=1 ros2 run turtlesim turtlesim_node\n./target/release/zenoh-bridge-dds -d 1\non host 2:\nROS_DOMAIN_ID=2 ros2 run turtlesim turtle_teleop_key\n./target/release/zenoh-bridge-dds -d 2 -e tcp/<host-1-ip>:7447 - where <host-1-ip> is the IP of host 1\nNotice that to avoid unwanted direct DDS communication, 2 disctinct ROS domains are still used.\n2 hosts, with an intermediate zenoh router in the cloud\nIn case your 2 hosts can't have a point-to-point communication, you could leverage a zenoh router deployed in a cloud instance (any Linux VM will do the job). You just need to configure your cloud instanse with a public IP and authorize the TCP port 7447.\n\u26a0\ufe0f the zenoh protocol is still under development leading to possible incompatibilities between the bridge and the router if their zenoh version differ. Please make sure you use a zenoh router built from a recent commit id from its master branch.\nHere are the commands to test this configuration with turtlesim:\non cloud VM:\nzenohd\non host 1:\nros2 run turtlesim turtlesim_node\n./target/release/zenoh-bridge-dds -e tcp/<cloud-ip>:7447\nwhere <cloud-ip> is the IP of your cloud instance\non host 2:\nros2 run turtlesim turtle_teleop_key\n./target/release/zenoh-bridge-dds -e tcp/<cloud-ip>:7447\nwhere <cloud-ip> is the IP of your cloud instance\nNotice that there is no need to use distinct ROS domain here, since the 2 hosts are not supposed to directly communicate with each other.\nMore advanced usage for ROS2\nFull support of ROS graph and topic lists via the forward discovery mode\nBy default the bridge doesn't route throught zenoh the DDS discovery traffic to the remote bridges.\nMeaning that, in case you use 2 zenoh-bridge-dds to interconnect 2 DDS domains, the DDS entities discovered in one domain won't be advertised in the other domain. Thus, the DDS data will be routed between the 2 domains only if matching readers and writers are declared in the 2 domains independently.\nThis default behaviour has an impact on ROS2 behaviour: on one side of the bridge the ROS graph might not reflect all the nodes from the other side of the bridge. The ros2 topic list command might not list all the topics declared on the other side. And the ROS graph is limited to the nodes in each domain.\nBut using the --fwd-discovery (or -f) option for all bridges make them behave differently:\neach bridge will forward via zenoh the local DDS discovery data to the remote bridges (in a more compact way than the original DDS discovery traffic)\neach bridge receiving DDS discovery data via zenoh will create a replica of the DDS reader or writer, with similar QoS. Those replicas will serve the route to/from zenoh, and will be discovered by the ROS2 nodes.\neach bridge will forward the ros_discovery_info data (in a less intensive way than the original publications) to the remote bridges. On reception, the remote bridges will convert the original entities' GIDs into the GIDs of the corresponding replicas, and re-publish on DDS the ros_discovery_info. The full ROS graph can then be discovered by the ROS2 nodes on each host.\nLimiting the ROS2 topics, services, parameters or actions to be routed\nBy default 2 zenoh bridges will route all ROS2 topics and services for which they detect a Writer on one side and a Reader on the other side. But you might want to avoid some topics and services to be routed by the bridge.\nStarting zenoh-bridge-dds you can use the --allow argument to specify the subset of topics and services that will be routed by the bridge. This argument accepts a string wich is a regular expression that must match a substring of an allowed zenoh resource (see details of mapping of ROS2 names to zenoh resources).\nHere are some examples of usage:\n--allow value allowed ROS2 communication\n/rosout /rosout\n/rosout|/turtle1/cmd_vel|/turtle1/rotate_absolute /rosout\n/turtle1/cmd_vel\n/turtle1/rotate_absolute\n/rosout|/turtle1/ /rosout and all /turtle1 topics, services, parameters and actions\n/turtle1/.* all topics and services with name containing /turtle1/\n/turtle1/ same: all topics, services, parameters and actions with name containing /turtle1/\n/rt/turtle1 all topics with name containing /turtle1 (no services, parameters or actions)\n/rq/turtle1|/rr/turtle1 all services and parameters with name containing /turtle1 (no topics or actions)\n/rq/turtlesim/.*parameter|/rr/turtlesim/.*parameter all parameters with name containing /turtlesim (no topics, services or actions)\n/rq/turtle1/.*/_action|/rr/turtle1/.*/_action all actions with name containing /turtle1 (no topics, services or parameters)\nRunning several robots without changing the ROS2 configuration\nIf you run similar robots in the same network, they will by default all us the same DDS topics, leading to interferences in their operations.\nA simple way to address this issue using the zenoh bridge is to:\ndeploy 1 zenoh bridge per robot\nhave each bridge started with the --scope \"/<id>\" argument, each robot having its own id.\nmake sure each robot cannot directly communicate via DDS with another robot by setting a distinct domain per robot, or configuring its network interface to not route UDP multicast outside the host.\nUsing the --scope option, a prefix is added to each zenoh resource published/subscribed by the bridge (more details in mapping of ROS2 names to zenoh resources). To interact with a robot, a remote ROS2 application must use a zenoh bridge configured with the same scope than the robot.\nCloser integration of ROS2 with zenoh\nAs you understood, using the zenoh bridge, each ROS2 publications and subscriptions are mapped to a zenoh resource. Therefore, its relatively easy to develop an application using one of the zenoh APIs to interact with one or more robot at the same time.\nSee in details how to achieve that in this blog.\nAll zenoh-bridge-dds command line arguments\nzenoh-bridge-dds accepts the following arguments:\nzenoh-related arguments:\n-m, --mode <MODE> : The zenoh session mode. Default: peer Possible values: peer or client.\nSee zenoh documentation for more details.\n-l, --listener <LOCATOR> : The locators the bridge will listen on for zenoh protocol. Can be specified multiple times. Example of locator: tcp/localhost:7447.\n-e, --peer <LOCATOR> : zenoh peers locators the bridge will try to connect to (typically another bridge or a zenoh router). Example of locator: tcp/<ip-address>:7447.\n--no-multicast-scouting : disable the zenoh scouting protocol that allows automatic discovery of zenoh peers and routers.\n-i, --id <hex_string> : The identifier (as an hexadecimal string - e.g.: 0A0B23...) that the zenoh bridge must use. WARNING: this identifier must be unique in the system! If not set, a random UUIDv4 will be used.\n--group-member-id <ID> : The bridges are supervising each other via a group membership algorithm implemented over zenoh. This option allows to set a custom identifier for the bridge, that will be used in group membership algorithm (if not specified, the zenoh UUID is used).\n--group-lease <Duration> : The lease duration (in seconds) used in group membership algorithm (default: 3 seconds)\n--rest-plugin : activate the zenoh REST API, available by default on port 8000.\n--rest-http-port <rest-http-port> : set the REST API http port (default: 8000)\nDDS-related arguments:\n-d, --domain <ID> : The DDS Domain ID (if using with ROS this should be the same as ROS_DOMAIN_ID)\n-f, --fwd-discovery : When set, rather than creating a local route when discovering a local DDS entity, this discovery info is forwarded to the remote plugins/bridges. Those will create the routes, including a replica of the discovered entity. More details here\n-s, --scope <String> : A string used as prefix to scope DDS traffic when mapped to zenoh resources.\n-a, --allow <String> : A regular expression matching the set of 'partition/topic-name' that must be routed. By default, all partitions and topic are allowed.\nExamples of expressions:\n.*/TopicA will allow only the TopicA to be routed, whatever the partition.\nPartitionX/.* will allow all the topics to be routed, but only on PartitionX.\ncmd_vel|rosout will allow only the topics containing cmd_vel or rosout in their name or partition name to be routed.\n--dds-max-frequency <String>... : specifies a maximum frequency of data routing over zenoh per-topic. The string must have the format \"regex=float\" where:\n\"regex\" is a regular expression matching the set of 'partition/topic-name' for which the data (per DDS instance) must be routedat no higher rate than associated max frequency (same syntax than --allow option).\n\"float\" is the maximum frequency in Hertz; if publication rate is higher, downsampling will occur when routing.\n(usable multiple times)\n-w, --generalise-pub <String> : A list of key expressions to use for generalising the declaration of the zenoh publications, and thus minimizing the discovery traffic (usable multiple times). See this blog for more details.\n-r, --generalise-sub <String> : A list of key expressions to use for generalising the declaration of the zenoh subscriptions, and thus minimizing the discovery traffic (usable multiple times). See this blog for more details.\nAdmin space\nThe zenoh bridge for DDS exposes and administration space allowing to browse the DDS entities that have been discovered (with their QoS), and the routes that have been established between DDS and zenoh. This administration space is accessible via any zenoh API, including the REST API that you can activate at zenoh-bridge-dds startup using the --rest-plugin argument.\nThe zenoh-bridge-dds exposes this administration space with paths prefixed by /@/service/<uuid>/dds (where <uuid> is the unique identifier of the bridge instance). The informations are then organized with such paths:\n/@/service/<uuid>/dds/version : the bridge version\n/@/service/<uuid>/dds/config : the bridge configuration\n/@/service/<uuid>/dds/participant/<gid>/reader/<gid>/<topic> : a discovered DDS reader on <topic>\n/@/service/<uuid>/dds/participant/<gid>/writer/<gid>/<topic> : a discovered DDS reader on <topic>\n/@/service/<uuid>/dds/route/from_dds/<zenoh-resource> : a route established from a DDS writer to a zenoh resource named <zenoh-resource> (see mapping rules).\n/@/service/<uuid>/dds/route/to_dds/<zenoh-resource> : a route established from a zenoh resource named <zenoh-resource> (see mapping rules)..\nExample of queries on administration space using the REST API with the curl command line tool (don't forget to activate the REST API with --rest-plugin argument):\nList all the DDS entities that have been discovered:\ncurl http://localhost:8000:/@/service/**/participant/**\nList all established routes:\ncurl http://localhost:8000:/@/service/**/route/**\nList all discovered DDS entities and established route for topic cmd_vel:\ncurl http://localhost:8000:/@/service/**/cmd_vel\nPro tip: pipe the result into jq command for JSON pretty print or transformation.\nArchitecture details\nWhether it's built as a library or as a standalone executable, the zenoh bridge for DDS do the same things:\nin default mode:\nit discovers the DDS readers and writers declared by any DDS application, via the standard DDS discovery protocol (that uses UDP multicast)\nit creates a mirror DDS writer or reader for each discovered reader or writer (using the same QoS)\nif maps the discovered DDS topics and partitions to zenoh resources (see mapping details below)\nit forwards user's data from a DDS topic to the corresponding zenoh resource, and vice versa\nit does not forward to the remote bridge any DDS discovery information\nin \"forward discovery\" mode\nit behaves as described here\nMapping of DDS topics to zenoh resources\nThe mapping between DDS and zenoh is rather straightforward. Given a DDS Reader/Writer for topic A in a given partition P, then the equivalent zenoh resource will be named as /P/A. If no partition is defined, the equivalent zenoh resource will be named as /A.\nOptionally, the bridge can be configured with a scope that will be used as a prefix to each zenoh resource. That is, for scope /S the equivalent zenoh resource will be /S/P/A for a topic A and a partition P, and /S/A for a topic without partition.\nMapping ROS2 names to zenoh resources\nThe mapping from ROS2 topics and services name to DDS topics is specified here. Notice that ROS2 does not use the DDS partitions.\nAs a consequence of this mapping and of the DDS to zenoh mapping specified above, here are some examples of mapping from ROS2 names to zenoh resources:\nROS2 names DDS Topics names zenoh resources names (no scope) zenohs resources names (if scope=\"/scope\")\ntopic: /rosout rt/rosout /rt/rosout /scope/rt/rosout\ntopic: /turtle1/cmd_vel rt/turtle1/cmd_vel /rt/turtle1/cmd_vel /scope/rt/turtle1/cmd_vel\nservice: /turtle1/set_pen rq/turtle1/set_penRequest\nrr/turtle1/set_penReply /rq/turtle1/set_penRequest\n/rr/turtle1/set_penReply /scope/rq/turtle1/set_penRequest\n/scope/rr/turtle1/set_penReply\naction: /turtle1/rotate_absolute rq/turtle1/rotate_absolute/_action/send_goalRequest\nrr/turtle1/rotate_absolute/_action/send_goalReply\nrq/turtle1/rotate_absolute/_action/cancel_goalRequest\nrr/turtle1/rotate_absolute/_action/cancel_goalReply\nrq/turtle1/rotate_absolute/_action/get_resultRequest\nrr/turtle1/rotate_absolute/_action/get_resultReply\nrt/turtle1/rotate_absolute/_action/status\nrt/turtle1/rotate_absolute/_action/feedback /rq/turtle1/rotate_absolute/_action/send_goalRequest\n/rr/turtle1/rotate_absolute/_action/send_goalReply\n/rq/turtle1/rotate_absolute/_action/cancel_goalRequest\n/rr/turtle1/rotate_absolute/_action/cancel_goalReply\n/rq/turtle1/rotate_absolute/_action/get_resultRequest\n/rr/turtle1/rotate_absolute/_action/get_resultReply\n/rt/turtle1/rotate_absolute/_action/status\n/rt/turtle1/rotate_absolute/_action/feedback /scope/rq/turtle1/rotate_absolute/_action/send_goalRequest\n/scope/rr/turtle1/rotate_absolute/_action/send_goalReply\n/scope/rq/turtle1/rotate_absolute/_action/cancel_goalRequest\n/scope/rr/turtle1/rotate_absolute/_action/cancel_goalReply\n/scope/rq/turtle1/rotate_absolute/_action/get_resultRequest\n/scope/rr/turtle1/rotate_absolute/_action/get_resultReply\n/scope/rt/turtle1/rotate_absolute/_action/status\n/scope/rt/turtle1/rotate_absolute/_action/feedback\nall parameters for node turtlesim rq/turtlesim/list_parametersRequest\nrr/turtlesim/list_parametersReply\nrq/turtlesim/describe_parametersRequest\nrr/turtlesim/describe_parametersReply\nrq/turtlesim/get_parametersRequest\nrr/turtlesim/get_parametersReply\nrr/turtlesim/get_parameter_typesReply\nrq/turtlesim/get_parameter_typesRequest\nrq/turtlesim/set_parametersRequest\nrr/turtlesim/set_parametersReply\nrq/turtlesim/set_parameters_atomicallyRequest\nrr/turtlesim/set_parameters_atomicallyReply /rq/turtlesim/list_parametersRequest\n/rr/turtlesim/list_parametersReply\n/rq/turtlesim/describe_parametersRequest\n/rr/turtlesim/describe_parametersReply\n/rq/turtlesim/get_parametersRequest\n/rr/turtlesim/get_parametersReply\n/rr/turtlesim/get_parameter_typesReply\n/rq/turtlesim/get_parameter_typesRequest\n/rq/turtlesim/set_parametersRequest\n/rr/turtlesim/set_parametersReply\n/rq/turtlesim/set_parameters_atomicallyRequest\n/rr/turtlesim/set_parameters_atomicallyReply /scope/rq/turtlesim/list_parametersRequest\n/scope/rr/turtlesim/list_parametersReply\n/scope/rq/turtlesim/describe_parametersRequest\n/scope/rr/turtlesim/describe_parametersReply\n/scope/rq/turtlesim/get_parametersRequest\n/scope/rr/turtlesim/get_parametersReply\n/scope/rr/turtlesim/get_parameter_typesReply\n/scope/rq/turtlesim/get_parameter_typesRequest\n/scope/rq/turtlesim/set_parametersRequest\n/scope/rr/turtlesim/set_parametersReply\n/scope/rq/turtlesim/set_parameters_atomicallyRequest\n/scope/rr/turtlesim/set_parameters_atomicallyReply\nspecific ROS discovery topic ros_discovery_info /ros_discovery_info /scope/ros_discovery_info", "link": "https://github.com/eclipse-zenoh/zenoh-plugin-dds", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "dds plugin for eclipse zenoh\nbackground\nthe data distribution service (dds) is a standard for data-centric publish subscribe. whilst dds has been around for quite some time and has a long history of deployments in various industries, it has recently gained quite a bit of attentions thanks to its adoption by the robotic operating system (ros2) -- where it is used for communication between ros2 nodes.\nrobot swarms and edge robotics\nas mentioned above, ros2 has adopted dds as the mechanism to exchange data between nodes within and potentially across a robot. that said, due to some of the very core assumptions at the foundations of the dds wire-protocol, beside the fact that it leverages udp/ip multicast for communication, it is not so straightforward to scale dds communication over a wan or across multiple lans. zenoh, on the other hand was designed since its inception to operate at internet scale.\nthus, the main motivations to have a zenoh bridge for dds are:\nfacilitate the interconnection of robot swarms.\nsupport use cases of edge robotics.\ngive the possibility to use zenoh's geo-distributed storage and query system to better manage robot's data.\nhow to build it\nin order to build the zenoh bridge for dds you need first to install the following dependencies:\nrust\non linux, make sure the llvm and clang development packages are installed:\non debians do: sudo apt install llvm-dev libclang-dev\non centos or rhel do: sudo yum install llvm-devel clang-devel\non alpine do: apk install llvm11-dev clang-dev\ncmake (to build cyclonedds which is a native dependency)\nonce these dependencies are in place, you may clone the repository on your machine:\n$ git clone https://github.com/eclipse-zenoh/zenoh-plugin-dds.git\n$ cd zenoh-plugin-dds\nyou can then choose between building the zenoh bridge for dds:\nas a dynamically loaded plugin:\n$ cargo build --release -p zplugin-dds\nthe plugin shared library (*.so on linux, *.dylib on mac os, *.dll on windows) will be generated in the target/release subdirectory.\nor as a standalone executable binary:\n$ cargo build --release -p zenoh-bridge-dds\nthe zenoh-bridge-dds binary will be generated in the target/release sub-directory.\nros2 package\nif you're a ros2 user, you can also build zenoh-bridge-dds as a ros package running:\nrosdep install --from-paths . --ignore-src -r -y\ncolcon build --packages-select zenoh-bridge-dds\nthe rosdep command will automatically install rust and clang as build dependencies.\ndocker image\nthe zenoh-bridge-dds standalone executable is also available as a docker images for both amd64 and arm64. to get it, do:\ndocker pull eclipse/zenoh-bridge-dds:master for the master branch version\n\u26a0\ufe0f however, notice that it's usage is limited to docker on linux and using the --net host option.\nthe cause being that dds uses udp multicast and docker doesn't support udp multicast between a container and its host (see cases moby/moby#23659, moby/libnetwork#2397 or moby/libnetwork#552). the only known way to make it work is to use the --net host option that is only supported on linux hosts.\nusage: docker run --init --net host eclipse/zenoh-bridge-dds\nit supports the same command line arguments than the zenoh-bridge-dds (see below or check with -h argument).\nfor a quick test with ros2 turtlesim\nprerequisites:\na ros2 environment (no matter the dds implementation as soon as it implements the standard ddsi protocol - the default eclipse cyclonedds being just fine)\nthe turtlesim package\n1 host, 2 ros domains\nfor a quick test on a single host, you can run the turtlesim_node and the turtle_teleop_key on distinct ros domains. as soon as you run 2 zenoh-bridge-dds (1 per domain) the turtle_teleop_key can drive the turtlesim_node.\nhere are the commands to run:\nros_domain_id=1 ros2 run turtlesim turtlesim_node\nros_domain_id=2 ros2 run turtlesim turtle_teleop_key\n./target/release/zenoh-bridge-dds -d 1\n./target/release/zenoh-bridge-dds -d 2\nnotice that by default the 2 bridges will discover each other using udp multicast.\n2 hosts, avoiding udp multicast communication\nby default dds (and thus ros2) uses udp multicast for discovery and publications. but on some networks, udp multicast is not or badly supported.\nin such cases, deploying the zenoh-bridge-dds on both hosts will make it to:\nlimit the dds discovery traffic, as detailled in this blog\nroute all the dds publications made on udp multicast by each node through the zenoh protocol that by default uses tcp.\nhere are the commands to test this configuration with turtlesim:\non host 1:\nros_domain_id=1 ros2 run turtlesim turtlesim_node\n./target/release/zenoh-bridge-dds -d 1\non host 2:\nros_domain_id=2 ros2 run turtlesim turtle_teleop_key\n./target/release/zenoh-bridge-dds -d 2 -e tcp/<host-1-ip>:7447 - where <host-1-ip> is the ip of host 1\nnotice that to avoid unwanted direct dds communication, 2 disctinct ros domains are still used.\n2 hosts, with an intermediate zenoh router in the cloud\nin case your 2 hosts can't have a point-to-point communication, you could leverage a zenoh router deployed in a cloud instance (any linux vm will do the job). you just need to configure your cloud instanse with a public ip and authorize the tcp port 7447.\n\u26a0\ufe0f the zenoh protocol is still under development leading to possible incompatibilities between the bridge and the router if their zenoh version differ. please make sure you use a zenoh router built from a recent commit id from its master branch.\nhere are the commands to test this configuration with turtlesim:\non cloud vm:\nzenohd\non host 1:\nros2 run turtlesim turtlesim_node\n./target/release/zenoh-bridge-dds -e tcp/<cloud-ip>:7447\nwhere <cloud-ip> is the ip of your cloud instance\non host 2:\nros2 run turtlesim turtle_teleop_key\n./target/release/zenoh-bridge-dds -e tcp/<cloud-ip>:7447\nwhere <cloud-ip> is the ip of your cloud instance\nnotice that there is no need to use distinct ros domain here, since the 2 hosts are not supposed to directly communicate with each other.\nmore advanced usage for ros2\nfull support of ros graph and topic lists via the forward discovery mode\nby default the bridge doesn't route throught zenoh the dds discovery traffic to the remote bridges.\nmeaning that, in case you use 2 zenoh-bridge-dds to interconnect 2 dds domains, the dds entities discovered in one domain won't be advertised in the other domain. thus, the dds data will be routed between the 2 domains only if matching readers and writers are declared in the 2 domains independently.\nthis default behaviour has an impact on ros2 behaviour: on one side of the bridge the ros graph might not reflect all the nodes from the other side of the bridge. the ros2 topic list command might not list all the topics declared on the other side. and the ros graph is limited to the nodes in each domain.\nbut using the --fwd-discovery (or -f) option for all bridges make them behave differently:\neach bridge will forward via zenoh the local dds discovery data to the remote bridges (in a more compact way than the original dds discovery traffic)\neach bridge receiving dds discovery data via zenoh will create a replica of the dds reader or writer, with similar qos. those replicas will serve the route to/from zenoh, and will be discovered by the ros2 nodes.\neach bridge will forward the ros_discovery_info data (in a less intensive way than the original publications) to the remote bridges. on reception, the remote bridges will convert the original entities' gids into the gids of the corresponding replicas, and re-publish on dds the ros_discovery_info. the full ros graph can then be discovered by the ros2 nodes on each host.\nlimiting the ros2 topics, services, parameters or actions to be routed\nby default 2 zenoh bridges will route all ros2 topics and services for which they detect a writer on one side and a reader on the other side. but you might want to avoid some topics and services to be routed by the bridge.\nstarting zenoh-bridge-dds you can use the --allow argument to specify the subset of topics and services that will be routed by the bridge. this argument accepts a string wich is a regular expression that must match a substring of an allowed zenoh resource (see details of mapping of ros2 names to zenoh resources).\nhere are some examples of usage:\n--allow value allowed ros2 communication\n/rosout /rosout\n/rosout|/turtle1/cmd_vel|/turtle1/rotate_absolute /rosout\n/turtle1/cmd_vel\n/turtle1/rotate_absolute\n/rosout|/turtle1/ /rosout and all /turtle1 topics, services, parameters and actions\n/turtle1/.* all topics and services with name containing /turtle1/\n/turtle1/ same: all topics, services, parameters and actions with name containing /turtle1/\n/rt/turtle1 all topics with name containing /turtle1 (no services, parameters or actions)\n/rq/turtle1|/rr/turtle1 all services and parameters with name containing /turtle1 (no topics or actions)\n/rq/turtlesim/.*parameter|/rr/turtlesim/.*parameter all parameters with name containing /turtlesim (no topics, services or actions)\n/rq/turtle1/.*/_action|/rr/turtle1/.*/_action all actions with name containing /turtle1 (no topics, services or parameters)\nrunning several robots without changing the ros2 configuration\nif you run similar robots in the same network, they will by default all us the same dds topics, leading to interferences in their operations.\na simple way to address this issue using the zenoh bridge is to:\ndeploy 1 zenoh bridge per robot\nhave each bridge started with the --scope \"/<id>\" argument, each robot having its own id.\nmake sure each robot cannot directly communicate via dds with another robot by setting a distinct domain per robot, or configuring its network interface to not route udp multicast outside the host.\nusing the --scope option, a prefix is added to each zenoh resource published/subscribed by the bridge (more details in mapping of ros2 names to zenoh resources). to interact with a robot, a remote ros2 application must use a zenoh bridge configured with the same scope than the robot.\ncloser integration of ros2 with zenoh\nas you understood, using the zenoh bridge, each ros2 publications and subscriptions are mapped to a zenoh resource. therefore, its relatively easy to develop an application using one of the zenoh apis to interact with one or more robot at the same time.\nsee in details how to achieve that in this blog.\nall zenoh-bridge-dds command line arguments\nzenoh-bridge-dds accepts the following arguments:\nzenoh-related arguments:\n-m, --mode <mode> : the zenoh session mode. default: peer possible values: peer or client.\nsee zenoh documentation for more details.\n-l, --listener <locator> : the locators the bridge will listen on for zenoh protocol. can be specified multiple times. example of locator: tcp/localhost:7447.\n-e, --peer <locator> : zenoh peers locators the bridge will try to connect to (typically another bridge or a zenoh router). example of locator: tcp/<ip-address>:7447.\n--no-multicast-scouting : disable the zenoh scouting protocol that allows automatic discovery of zenoh peers and routers.\n-i, --id <hex_string> : the identifier (as an hexadecimal string - e.g.: 0a0b23...) that the zenoh bridge must use. warning: this identifier must be unique in the system! if not set, a random uuidv4 will be used.\n--group-member-id <id> : the bridges are supervising each other via a group membership algorithm implemented over zenoh. this option allows to set a custom identifier for the bridge, that will be used in group membership algorithm (if not specified, the zenoh uuid is used).\n--group-lease <duration> : the lease duration (in seconds) used in group membership algorithm (default: 3 seconds)\n--rest-plugin : activate the zenoh rest api, available by default on port 8000.\n--rest-http-port <rest-http-port> : set the rest api http port (default: 8000)\ndds-related arguments:\n-d, --domain <id> : the dds domain id (if using with ros this should be the same as ros_domain_id)\n-f, --fwd-discovery : when set, rather than creating a local route when discovering a local dds entity, this discovery info is forwarded to the remote plugins/bridges. those will create the routes, including a replica of the discovered entity. more details here\n-s, --scope <string> : a string used as prefix to scope dds traffic when mapped to zenoh resources.\n-a, --allow <string> : a regular expression matching the set of 'partition/topic-name' that must be routed. by default, all partitions and topic are allowed.\nexamples of expressions:\n.*/topica will allow only the topica to be routed, whatever the partition.\npartitionx/.* will allow all the topics to be routed, but only on partitionx.\ncmd_vel|rosout will allow only the topics containing cmd_vel or rosout in their name or partition name to be routed.\n--dds-max-frequency <string>... : specifies a maximum frequency of data routing over zenoh per-topic. the string must have the format \"regex=float\" where:\n\"regex\" is a regular expression matching the set of 'partition/topic-name' for which the data (per dds instance) must be routedat no higher rate than associated max frequency (same syntax than --allow option).\n\"float\" is the maximum frequency in hertz; if publication rate is higher, downsampling will occur when routing.\n(usable multiple times)\n-w, --generalise-pub <string> : a list of key expressions to use for generalising the declaration of the zenoh publications, and thus minimizing the discovery traffic (usable multiple times). see this blog for more details.\n-r, --generalise-sub <string> : a list of key expressions to use for generalising the declaration of the zenoh subscriptions, and thus minimizing the discovery traffic (usable multiple times). see this blog for more details.\nadmin space\nthe zenoh bridge for dds exposes and administration space allowing to browse the dds entities that have been discovered (with their qos), and the routes that have been established between dds and zenoh. this administration space is accessible via any zenoh api, including the rest api that you can activate at zenoh-bridge-dds startup using the --rest-plugin argument.\nthe zenoh-bridge-dds exposes this administration space with paths prefixed by /@/service/<uuid>/dds (where <uuid> is the unique identifier of the bridge instance). the informations are then organized with such paths:\n/@/service/<uuid>/dds/version : the bridge version\n/@/service/<uuid>/dds/config : the bridge configuration\n/@/service/<uuid>/dds/participant/<gid>/reader/<gid>/<topic> : a discovered dds reader on <topic>\n/@/service/<uuid>/dds/participant/<gid>/writer/<gid>/<topic> : a discovered dds reader on <topic>\n/@/service/<uuid>/dds/route/from_dds/<zenoh-resource> : a route established from a dds writer to a zenoh resource named <zenoh-resource> (see mapping rules).\n/@/service/<uuid>/dds/route/to_dds/<zenoh-resource> : a route established from a zenoh resource named <zenoh-resource> (see mapping rules)..\nexample of queries on administration space using the rest api with the curl command line -----> tool !!!  (don't forget to activate the rest api with --rest-plugin argument):\nlist all the dds entities that have been discovered:\ncurl http://localhost:8000:/@/service/**/participant/**\nlist all established routes:\ncurl http://localhost:8000:/@/service/**/route/**\nlist all discovered dds entities and established route for topic cmd_vel:\ncurl http://localhost:8000:/@/service/**/cmd_vel\npro tip: pipe the result into jq command for json pretty print or transformation.\narchitecture details\nwhether it's built as a library or as a standalone executable, the zenoh bridge for dds do the same things:\nin default mode:\nit discovers the dds readers and writers declared by any dds application, via the standard dds discovery protocol (that uses udp multicast)\nit creates a mirror dds writer or reader for each discovered reader or writer (using the same qos)\nif maps the discovered dds topics and partitions to zenoh resources (see mapping details below)\nit forwards user's data from a dds topic to the corresponding zenoh resource, and vice versa\nit does not forward to the remote bridge any dds discovery information\nin \"forward discovery\" mode\nit behaves as described here\nmapping of dds topics to zenoh resources\nthe mapping between dds and zenoh is rather straightforward. given a dds reader/writer for topic a in a given partition p, then the equivalent zenoh resource will be named as /p/a. if no partition is defined, the equivalent zenoh resource will be named as /a.\noptionally, the bridge can be configured with a scope that will be used as a prefix to each zenoh resource. that is, for scope /s the equivalent zenoh resource will be /s/p/a for a topic a and a partition p, and /s/a for a topic without partition.\nmapping ros2 names to zenoh resources\nthe mapping from ros2 topics and services name to dds topics is specified here. notice that ros2 does not use the dds partitions.\nas a consequence of this mapping and of the dds to zenoh mapping specified above, here are some examples of mapping from ros2 names to zenoh resources:\nros2 names dds topics names zenoh resources names (no scope) zenohs resources names (if scope=\"/scope\")\ntopic: /rosout rt/rosout /rt/rosout /scope/rt/rosout\ntopic: /turtle1/cmd_vel rt/turtle1/cmd_vel /rt/turtle1/cmd_vel /scope/rt/turtle1/cmd_vel\nservice: /turtle1/set_pen rq/turtle1/set_penrequest\nrr/turtle1/set_penreply /rq/turtle1/set_penrequest\n/rr/turtle1/set_penreply /scope/rq/turtle1/set_penrequest\n/scope/rr/turtle1/set_penreply\naction: /turtle1/rotate_absolute rq/turtle1/rotate_absolute/_action/send_goalrequest\nrr/turtle1/rotate_absolute/_action/send_goalreply\nrq/turtle1/rotate_absolute/_action/cancel_goalrequest\nrr/turtle1/rotate_absolute/_action/cancel_goalreply\nrq/turtle1/rotate_absolute/_action/get_resultrequest\nrr/turtle1/rotate_absolute/_action/get_resultreply\nrt/turtle1/rotate_absolute/_action/status\nrt/turtle1/rotate_absolute/_action/feedback /rq/turtle1/rotate_absolute/_action/send_goalrequest\n/rr/turtle1/rotate_absolute/_action/send_goalreply\n/rq/turtle1/rotate_absolute/_action/cancel_goalrequest\n/rr/turtle1/rotate_absolute/_action/cancel_goalreply\n/rq/turtle1/rotate_absolute/_action/get_resultrequest\n/rr/turtle1/rotate_absolute/_action/get_resultreply\n/rt/turtle1/rotate_absolute/_action/status\n/rt/turtle1/rotate_absolute/_action/feedback /scope/rq/turtle1/rotate_absolute/_action/send_goalrequest\n/scope/rr/turtle1/rotate_absolute/_action/send_goalreply\n/scope/rq/turtle1/rotate_absolute/_action/cancel_goalrequest\n/scope/rr/turtle1/rotate_absolute/_action/cancel_goalreply\n/scope/rq/turtle1/rotate_absolute/_action/get_resultrequest\n/scope/rr/turtle1/rotate_absolute/_action/get_resultreply\n/scope/rt/turtle1/rotate_absolute/_action/status\n/scope/rt/turtle1/rotate_absolute/_action/feedback\nall parameters for node turtlesim rq/turtlesim/list_parametersrequest\nrr/turtlesim/list_parametersreply\nrq/turtlesim/describe_parametersrequest\nrr/turtlesim/describe_parametersreply\nrq/turtlesim/get_parametersrequest\nrr/turtlesim/get_parametersreply\nrr/turtlesim/get_parameter_typesreply\nrq/turtlesim/get_parameter_typesrequest\nrq/turtlesim/set_parametersrequest\nrr/turtlesim/set_parametersreply\nrq/turtlesim/set_parameters_atomicallyrequest\nrr/turtlesim/set_parameters_atomicallyreply /rq/turtlesim/list_parametersrequest\n/rr/turtlesim/list_parametersreply\n/rq/turtlesim/describe_parametersrequest\n/rr/turtlesim/describe_parametersreply\n/rq/turtlesim/get_parametersrequest\n/rr/turtlesim/get_parametersreply\n/rr/turtlesim/get_parameter_typesreply\n/rq/turtlesim/get_parameter_typesrequest\n/rq/turtlesim/set_parametersrequest\n/rr/turtlesim/set_parametersreply\n/rq/turtlesim/set_parameters_atomicallyrequest\n/rr/turtlesim/set_parameters_atomicallyreply /scope/rq/turtlesim/list_parametersrequest\n/scope/rr/turtlesim/list_parametersreply\n/scope/rq/turtlesim/describe_parametersrequest\n/scope/rr/turtlesim/describe_parametersreply\n/scope/rq/turtlesim/get_parametersrequest\n/scope/rr/turtlesim/get_parametersreply\n/scope/rr/turtlesim/get_parameter_typesreply\n/scope/rq/turtlesim/get_parameter_typesrequest\n/scope/rq/turtlesim/set_parametersrequest\n/scope/rr/turtlesim/set_parametersreply\n/scope/rq/turtlesim/set_parameters_atomicallyrequest\n/scope/rr/turtlesim/set_parameters_atomicallyreply\nspecific ros discovery topic ros_discovery_info /ros_discovery_info /scope/ros_discovery_info", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000514, "year": null}, {"Unnamed: 0": 1555, "autor": 535, "date": null, "content": "Status\nNot functional (i.e. does not converge with our data). But can be a useful starting point since the paper author's code is not public.\nA TensorFlow implementation of DeepVO: Towards End-to-End Visual Odometry with Deep Recurrent Convolutional Neural Networks\nThis is our submission for the ANN with TensorFlow course, winter 2017. Please note that this implementation does not seem entirely correct. Convergence was observed only on a dataset with random moves forwards and backwards, without rotation.\nData Acquisition\nIn order to make use of the full 720 resolution of the LifeCam 3000, you must do two things\nTell the device driver to use this resolution via v4l2-ctl --set-fmt-video=width=1280,height=720,pixelformat=1 (the pixel format is probably not important, but you may need to adjust the ros node accordingly)\nIn the usb_cam_node, set height and width parameters appropriately.\nData Preprocessing\nBagfile conversion\nThe first thing to do is to convert the rosbag sensor recordings with the conversion tool (which you can find here) like this\nbag_to_cam_pose_data -b <file>.bag -d <outdir> -x -P\nThe -P flag is to dump one npy file for each image and pose. The -x flag is for writing float image arrays instead of uint8. This will create images and poses folders inside the chosen directory.\nFurther preprocessing\nUse the preprocess_data.py script to prepare the data for our network\nwith -d <path-to-data> you give it the path where the images/ and poses/ folders are located. All modifications are done in-place\n-f will map the images to (0, 1)\n-m will subtract the mean (over the entire set) from each image\n-p will add Pi to all pose angles. The robot's EKF output is in the range (-pi, pi), but we want (0, 2pi)\nPotential Problems\nWe are not sure if the timestamps of pose and camera messages are correct and thus whether the training data is good enough\nWe have no control over the exposure time of the camera. Auto-exposure differences while driving around might make the problem more difficult", "link": "https://github.com/themightyoarfish/deepVO", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "status\nnot functional (i.e. does not converge with our data). but can be a useful starting point since the paper author's code is not public.\na tensorflow implementation of deepvo: towards end-to-end visual odometry with deep recurrent convolutional neural networks\nthis is our submission for the ann with tensorflow course, winter 2017. please note that this implementation does not seem entirely correct. convergence was observed only on a dataset with random moves forwards and backwards, without rotation.\ndata acquisition\nin order to make use of the full 720 resolution of the lifecam 3000, you must do two things\ntell the device driver to use this resolution via v4l2-ctl --set-fmt-video=width=1280,height=720,pixelformat=1 (the pixel format is probably not important, but you may need to adjust the ros node accordingly)\nin the usb_cam_node, set height and width parameters appropriately.\ndata preprocessing\nbagfile conversion\nthe first thing to do is to convert the rosbag sensor recordings with the conversion -----> tool !!!  (which you can find here) like this\nbag_to_cam_pose_data -b <file>.bag -d <outdir> -x -p\nthe -p flag is to dump one npy file for each image and pose. the -x flag is for writing float image arrays instead of uint8. this will create images and poses folders inside the chosen directory.\nfurther preprocessing\nuse the preprocess_data.py script to prepare the data for our network\nwith -d <path-to-data> you give it the path where the images/ and poses/ folders are located. all modifications are done in-place\n-f will map the images to (0, 1)\n-m will subtract the mean (over the entire set) from each image\n-p will add pi to all pose angles. the robot's ekf output is in the range (-pi, pi), but we want (0, 2pi)\npotential problems\nwe are not sure if the timestamps of pose and camera messages are correct and thus whether the training data is good enough\nwe have no control over the exposure time of the camera. auto-exposure differences while driving around might make the problem more difficult", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000535, "year": null}, {"Unnamed: 0": 1569, "autor": 549, "date": null, "content": "OpenRE -- The Open Source Robot Embedded Library\nkeywords: moblile robotics , STM32 , c++ , makefiles , ROS\nDescription\nThis is a C++ embedded library for robotics base on STM32 and intended to provide GNU Makefiles.\nOpenRE Tutorial\nCommunity:\nHandsFree Github\nHandsFree Website\nHandsFree Wiki , Old Version Wiki\nBUY Robot\nQQ Group: 521037187(Hands Free Community)\nEmail: hands_free@126.com\nFolder structure\n\u251c\u2500\u2500 0_Project ---- some demos base on this library\n\u2502 \u251c\u2500\u2500 examples ---- stm32fxx examples project of openre\n\u2502 \u251c\u2500\u2500 firmware ---- firmware projects of handsfree robots\n\u2502 \u251c\u2500\u2500 etc..\n\u251c\u2500\u2500 1_Processor --- the board-level abstraction layer , support handsfree countrol_unit_v1 , countrol_unit_v2, countrol_unit_mini , stm32f407_discovery\n\u2502 \u251c\u2500\u2500 BoardAbstract\n\u2502 \u251c\u2500\u2500 board.mk\n\u2502 \u251c\u2500\u2500 Interrupt\n\u2502 \u251c\u2500\u2500 STM32F1\n\u2502 \u2514\u2500\u2500 STM32F4\n\u2502 \u251c\u2500\u2500 etc..\n\u251c\u2500\u2500 2_Package ---- this dir include many tool packs . you can use it for moblile robotics, fly control etc..\n\u2502 \u251c\u2500\u2500 common\n\u2502 \u251c\u2500\u2500 robolink\n\u2502 \u251c\u2500\u2500 imu\n\u2502 \u251c\u2500\u2500 motor\n\u2502 \u251c\u2500\u2500 robot_abstract\n\u2502 \u251c\u2500\u2500 robot_control\n\u2502 \u251c\u2500\u2500 tf\n\u2502 \u251c\u2500\u2500 etc..\n\u251c\u2500\u2500 3_OS : OpenRE support RTOS(ucosII ucosIII nuttx), GUI(STEMWIN) , FATFS\n\u251c\u2500\u2500 4_Thirdparty\n\u2502 \u251c\u2500\u2500 Dobot\n\u2502 \u251c\u2500\u2500 Eigen3 -\u2502 \u251c\u2500\u2500 libs_include.h\n\u2502 \u251c\u2500\u2500 Matrix\n\u2502 \u2514\u2500\u2500 thirdparty.mk\n\u251c\u2500\u2500 5_Development_Toolchain\n\u251c\u2500\u2500 6_Tools\n\u2502 \u251c\u2500\u2500 env_config.sh\n\u2502 \u251c\u2500\u2500 swd_upload.sh\n\u2502 \u251c\u2500\u2500 tarall.sh\n\u2502 \u2514\u2500\u2500 tar.sh\n\u251c\u2500\u2500 7_Documentation\n\u251c\u2500\u2500 clean_build_linux.sh\n\u251c\u2500\u2500 clean_build_win.bat\n\u251c\u2500\u2500 HANDS_FREE_OpenRE_F1.pro\n\u251c\u2500\u2500 HANDS_FREE_OpenRE_F4.pro\n\u2514\u2500\u2500 README.md\nInstallation\nResources\nDownload the latest openre code:\ngit clone https://github.com/HANDS-FREE/OpenRE\nOpenRE Toolchain\nMethod1: source installation (recommended)\n$ cd OpenRE & git clone git@github.com:HANDS-FREE/5_Development_Toolchain.git\n$ cd 5_Development_Toolchain\n$ sh auto_set_openre.sh\nMethod2: installation by deb\n$ sudo apt-get install software-properties-common python-software-properties\n$ sudo add-apt-repository ppa:terry.guo/gcc-arm-embedded\n$ sudo apt-get update\n$ sudo apt-get install openocd gcc-arm-none-eabi\n$ sudo usermod -a -G dialout $USER\n$ sudo apt-get install lib32ncurses5 libtool libusb-1.0 libftdi-dev python python-serial python-empy libpython2.7:i386\n$ sudo apt-get remove modemmanager\nUsage and Examples\nA Simple Demo\n* Led toggle:\ncd 0_Project/examples/handsfree_simple_app/linux\ncheck the makefile config matching your main control board\nmake clean\nmake\nmake burn\nRobot Firmware\ncd 0_Project/firmware/handsfree_wheel_robot/linux\ncheck the makefile config matching your main control board\nmake clean\nmake\nmake burn\nTips: Lots of examples are provided in folder 0_Project and you can get some manuals in folder 7_Documentation.\nOpenRE Board\nOpenRE Board is a motion controller developed according to the hardware standard of the HandsFree open source project. It is an important part of all HandsFree Robot platforms.It can be applied to the development of wheeled robots, humanoid robots, balance vehicles, and drones.More importantly, OpenRE Board is the official standard platform for the OpenRE(Open Source Robot Embedded Library).\nWe are now sharing two different performance openre boards : OpenRE Board Mini,OpenRE Board V2\nOpenRE Board V2\nBUY OpenRE Board\nOpenRE Board Mini\nLicense\nOpenRE is licensed generally under a permissive 3-clause BSD license. Contributions are required to be made under the same license.\nContact\nIf you have any problem, or you have some suggestions for this code, please contact WenKe Ma by 315261982@qq.com, thank you very much!", "link": "https://github.com/HANDS-FREE/OpenRE", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "openre -- the open source robot embedded library\nkeywords: moblile robotics , stm32 , c++ , makefiles , ros\ndescription\nthis is a c++ embedded library for robotics base on stm32 and intended to provide gnu makefiles.\nopenre tutorial\ncommunity:\nhandsfree github\nhandsfree website\nhandsfree wiki , old version wiki\nbuy robot\nqq group: 521037187(hands free community)\nemail: hands_free@126.com\nfolder structure\n\u251c\u2500\u2500 0_project ---- some demos base on this library\n\u2502 \u251c\u2500\u2500 examples ---- stm32fxx examples project of openre\n\u2502 \u251c\u2500\u2500 firmware ---- firmware projects of handsfree robots\n\u2502 \u251c\u2500\u2500 etc..\n\u251c\u2500\u2500 1_processor --- the board-level abstraction layer , support handsfree countrol_unit_v1 , countrol_unit_v2, countrol_unit_mini , stm32f407_discovery\n\u2502 \u251c\u2500\u2500 boardabstract\n\u2502 \u251c\u2500\u2500 board.mk\n\u2502 \u251c\u2500\u2500 interrupt\n\u2502 \u251c\u2500\u2500 stm32f1\n\u2502 \u2514\u2500\u2500 stm32f4\n\u2502 \u251c\u2500\u2500 etc..\n\u251c\u2500\u2500 2_package ---- this dir include many -----> tool !!!  packs . you can use it for moblile robotics, fly control etc..\n\u2502 \u251c\u2500\u2500 common\n\u2502 \u251c\u2500\u2500 robolink\n\u2502 \u251c\u2500\u2500 imu\n\u2502 \u251c\u2500\u2500 motor\n\u2502 \u251c\u2500\u2500 robot_abstract\n\u2502 \u251c\u2500\u2500 robot_control\n\u2502 \u251c\u2500\u2500 tf\n\u2502 \u251c\u2500\u2500 etc..\n\u251c\u2500\u2500 3_os : openre support rtos(ucosii ucosiii nuttx), gui(stemwin) , fatfs\n\u251c\u2500\u2500 4_thirdparty\n\u2502 \u251c\u2500\u2500 dobot\n\u2502 \u251c\u2500\u2500 eigen3 -\u2502 \u251c\u2500\u2500 libs_include.h\n\u2502 \u251c\u2500\u2500 matrix\n\u2502 \u2514\u2500\u2500 thirdparty.mk\n\u251c\u2500\u2500 5_development_toolchain\n\u251c\u2500\u2500 6_tools\n\u2502 \u251c\u2500\u2500 env_config.sh\n\u2502 \u251c\u2500\u2500 swd_upload.sh\n\u2502 \u251c\u2500\u2500 tarall.sh\n\u2502 \u2514\u2500\u2500 tar.sh\n\u251c\u2500\u2500 7_documentation\n\u251c\u2500\u2500 clean_build_linux.sh\n\u251c\u2500\u2500 clean_build_win.bat\n\u251c\u2500\u2500 hands_free_openre_f1.pro\n\u251c\u2500\u2500 hands_free_openre_f4.pro\n\u2514\u2500\u2500 readme.md\ninstallation\nresources\ndownload the latest openre code:\ngit clone https://github.com/hands-free/openre\nopenre toolchain\nmethod1: source installation (recommended)\n$ cd openre & git clone git@github.com:hands-free/5_development_toolchain.git\n$ cd 5_development_toolchain\n$ sh auto_set_openre.sh\nmethod2: installation by deb\n$ sudo apt-get install software-properties-common python-software-properties\n$ sudo add-apt-repository ppa:terry.guo/gcc-arm-embedded\n$ sudo apt-get update\n$ sudo apt-get install openocd gcc-arm-none-eabi\n$ sudo usermod -a -g dialout $user\n$ sudo apt-get install lib32ncurses5 libtool libusb-1.0 libftdi-dev python python-serial python-empy libpython2.7:i386\n$ sudo apt-get remove modemmanager\nusage and examples\na simple demo\n* led toggle:\ncd 0_project/examples/handsfree_simple_app/linux\ncheck the makefile config matching your main control board\nmake clean\nmake\nmake burn\nrobot firmware\ncd 0_project/firmware/handsfree_wheel_robot/linux\ncheck the makefile config matching your main control board\nmake clean\nmake\nmake burn\ntips: lots of examples are provided in folder 0_project and you can get some manuals in folder 7_documentation.\nopenre board\nopenre board is a motion controller developed according to the hardware standard of the handsfree open source project. it is an important part of all handsfree robot platforms.it can be applied to the development of wheeled robots, humanoid robots, balance vehicles, and drones.more importantly, openre board is the official standard platform for the openre(open source robot embedded library).\nwe are now sharing two different performance openre boards : openre board mini,openre board v2\nopenre board v2\nbuy openre board\nopenre board mini\nlicense\nopenre is licensed generally under a permissive 3-clause bsd license. contributions are required to be made under the same license.\ncontact\nif you have any problem, or you have some suggestions for this code, please contact wenke ma by 315261982@qq.com, thank you very much!", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000549, "year": null}, {"Unnamed: 0": 1570, "autor": 550, "date": null, "content": "~Access our Benchbot Environments for Active Robotics (BEAR) through our challenge and development environment addons (installed by default)~\n~ Our BenchBot tutorial is the best place to get started developing with BenchBot ~\nBenchBot Software Stack\nThe BenchBot software stack is a collection of software packages that allow end users to control robots in real or simulated environments with a simple python API. It leverages the simple \"observe, act, repeat\" approach to robot problems prevalent in reinforcement learning communities (OpenAI Gym users will find the BenchBot API interface very similar).\nBenchBot was created as a tool to assist in the research challenges faced by the semantic scene understanding community; challenges including understanding a scene in simulation, transferring algorithms to real world systems, and meaningfully evaluating algorithm performance. We've since realised, these challenges don't just exist for semantic scene understanding, they're prevalent in a wide range of robotic problems.\nThis led us to create version 2 of BenchBot with a focus on allowing users to define their own functionality for BenchBot through add-ons. Want to integrate your own environments? Plug-in new robot platforms? Define new tasks? Share examples with others? Add evaluation measures? This all now possible with add-ons, and you don't have to do anything more than add some YAML and Python files defining your new content!\nThe \"bench\" in \"BenchBot\" refers to benchmarking, with our goal to provide a system that greatly simplifies the benchmarking of novel algorithms in both realistic 3D simulation and on real robot platforms. If there is something else you would like to use BenchBot for (like integrating different simulators), please let us know. We're very interested in BenchBot being the glue between your novel robotics research and whatever your robot platform may be.\nThis repository contains the software stack needed to develop solutions for BenchBot tasks on your local machine. It installs and configures a significant amount of software for you, wraps software in stable Docker images (~50GB), and provides simple interaction with the stack through 4 basic scripts: benchbot_install, benchbot_run, benchbot_submit, and benchbot_eval.\nSystem recommendations and requirements\nThe BenchBot software stack is designed to run seamlessly on a wide number of system configurations (currently limited to Ubuntu 18.04+). System hardware requirements are relatively high due to the software run for 3D simulation (Unreal Engine, Nvidia Isaac, Vulkan, etc.):\nNvidia Graphics card (GeForce GTX 1080 minimum, Titan XP+ / GeForce RTX 2070+ recommended)\nCPU with multiple cores (Intel i7-6800K minimum)\n32GB+ RAM\n64GB+ spare storage (an SSD storage device is strongly recommended)\nHaving a system that meets the above hardware requirements is all that is required to begin installing the BenchBot software stack. The install script analyses your system configuration and offers to install any missing software components interactively. The list of 3rd party software components involved includes:\nNvidia Driver (4.18+ required, 4.50+ recommended)\nCUDA with GPU support (10.0+ required, 10.1+ recommended)\nDocker Engine - Community Edition (19.03+ required, 19.03.2+ recommended)\nNvidia Container Toolkit (1.0+ required, 1.0.5+ recommended)\nISAAC 2019.2 SDK (requires an Nvidia developer login)\nManaging your installation\nInstallation is simple:\nu@pc:~$ git clone https://github.com/qcr/benchbot && cd benchbot\nu@pc:~$ ./install\nAny missing software components, or configuration issues with your system, should be detected by the install script and resolved interactively. The installation asks if you want to add BenchBot helper scripts to your PATH. Choosing yes will make the following commands available from any directory: benchbot_install (same as ./install above), benchbot_run, benchbot_submit, benchbot_eval, and benchbot_batch.\nBenchBot installs a default set of add-ons (currently 'benchbot-addons/ssu'), but this can be changed based on how you want to use BenchBot. For example, the following will also install the 'benchbot-addons/sqa' add-ons:\nu@pc:~$ benchbot_install --addons benchbot-addons/ssu,benchbot-addons/sqa\nSee the BenchBot Add-ons Manager's documentation for more information on using add-ons.\nThe BenchBot software stack will frequently check for updates and can update itself automatically. To update simply run the install script again (add the --force-clean flag if you would like to install from scratch):\nu@pc:~$ benchbot_install\nIf you decide to uninstall the BenchBot software stack, run:\nu@pc:~$ benchbot_install --uninstall\nThere are a number of other options to customise your BenchBot installation, which are all described by running:\nu@pc:~$ benchbot_install --help\nGetting started\nGetting a solution up and running with BenchBot is as simple as 1,2,3. Here's how to use BenchBot with content from the semantic scene understanding add-on:\nRun a simulator with the BenchBot software stack by selecting an available robot, environment, and task definition:\nu@pc:~$ benchbot_run --robot carter --env miniroom:1 --task semantic_slam:active:ground_truth\nA number of useful flags exist to help you explore what content is available in your installation (see --help for full details). For example, you can list what tasks are available via --list-tasks and view the task specification via --show-task TASK_NAME.\nCreate a solution to a BenchBot task, and run it against the software stack. To run a solution you must select a mode. For example, if you've created a solution in my_solution.py that you would like to run natively:\nu@pc:~$ benchbot_submit --native python my_solution.py\nSee --help for other options. You also have access to all of the examples available in your installation. For instance, you can run the hello_active example in containerised mode via:\nu@pc:~$ benchbot_submit --containerised --example hello_active\nSee --list-examples and --show-example EXAMPLE_NAME for full details on what's available out of the box.\nEvaluate the performance of your system using a supported evaluation method (see --list-methods). To use the omq evaluation method on my_results.json:\nu@pc:~$ benchbot_eval --method omq my_results.json\nYou can also simply run evaluation automatically after your submission completes:\nu@pc:~$ benchbot_submit --evaluate-with omq --native --example hello_eval_semantic_slam\nThe BenchBot Tutorial is a great place to start working with BenchBot; the tutorial takes you from a blank system to a working Semantic SLAM solution, with many educational steps along the way. Also remember the examples in your installation (benchbot-addons/examples_base is a good starting point) which show how to get up and running with the BenchBot software stack.\nPower tools for autonomous algorithm evaluation\nOnce you are confident your algorithm is a solution to the chosen task, the BenchBot software stack's power tools allow you to comprehensively explore your algorithm's performance. You can autonomously run your algorithm over multiple environments, and evaluate it holistically to produce a single summary statistic of your algorithm's performance. Here are some examples again with content from the semantic scene understanding add-on:\nUse benchbot_batch to run your algorithm in a number of environments and produce a set of results. The script has a number of toggles available to customise the process (see --help for full details). To autonomously run your semantic_slam:active:ground_truth algorithm over 3 environments:\nu@pc:~$ benchbot_batch --robot carter --task semantic_slam:active:ground_truth --envs miniroom:1,miniroom:3,house:5 --native python my_solution.py\nOr you can use one of the pre-defined environment batches installed via add-ons (e.g. benchbot-addons/batches_isaac):\nu@pc:~$ benchbot_batch --robot carter --task semantic_slam:active:ground_truth --envs-batch develop_1 --native python my_solution.py\nAdditionally, you can create a results ZIP and request an overall evaluation score at the end of the batch:\nu@pc:~$ benchbot_batch --robot carter --task semantic_slam:active:ground_truth --envs miniroom:1,miniroom:3,house:5 --zip --evaluate-with omq --native python my_solution.py\nLastly, both native and containerised submissions are supported exactly as in benchbot_submit:\nu@pc:~$ benchbot_batch --robot carter --task semantic_slam:active:ground_truth --envs miniroom:1,miniroom:3,house:5 --containerised my_solution_folder/\nYou can also directly call the holistic evaluation performed above by benchbot_batch through the benchbot_eval script. The script supports single result files, multiple results files, or a ZIP of multiple results files. See benchbot_eval --help for full details. Below are examples calling benchbot_eval with a series of results and a ZIP of results respectively:\nu@pc:~$ benchbot_eval --method omq -o my_jsons_scores result_1.json result_2.json result_3.json\nu@pc:~$ benchbot_eval --method omq -o my_zip_scores results.zip\nUsing BenchBot in your research\nBenchBot was made to enable and assist the development of high quality, repeatable research results. We welcome any and all use of the BenchBot software stack in your research.\nTo use our system, we just ask that you cite our paper on the BenchBot system. This will help us follow uses of BenchBot in the research community, and understand how we can improve the system to help support future research results. Citation details are as follows:\n@misc{talbot2020benchbot,\ntitle={BenchBot: Evaluating Robotics Research in Photorealistic 3D Simulation and on Real Robots},\nauthor={Ben Talbot and David Hall and Haoyang Zhang and Suman Raj Bista and Rohan Smith and Feras Dayoub and Niko S\u00fcnderhauf},\nyear={2020},\neprint={2008.00635},\narchivePrefix={arXiv},\nprimaryClass={cs.RO}\n}\nComponents of the BenchBot software stack\nThe BenchBot software stack is split into a number of standalone components, each with their own GitHub repository and documentation. This repository glues them all together for you into a working system. The components of the stack are:\nbenchbot_api: user-facing Python interface to the BenchBot system, allowing the user to control simulated or real robots in simulated or real world environments through simple commands\nbenchbot_addons: a Python manager for add-ons to a BenchBot system, with full documentation on how to create and add your own add-ons\nbenchbot_supervisor: a HTTP server facilitating communication between user-facing interfaces and the underlying robot controller\nbenchbot_robot_controller: a wrapping script which controls the low-level ROS functionality of a simulator or real robot, handles automated subprocess management, and exposes interaction via a HTTP server\nbenchbot_simulator: a realistic 3D simulator employing Nvidia's Isaac framework, in combination with Unreal Engine environments\nbenchbot_eval: Python library for evaluating the performance in a task, based on the results produced by a submission\nFurther information\nFAQs: Wiki page where answers to frequently asked questions and resolutions to common issues will be provided\nSemantic SLAM Tutorial: a tutorial stepping through creating a semantic SLAM system in BenchBot that utilises the 3D object detector VoteNet\nSupporters\nDevelopment of the BenchBot software stack was directly supported by:", "link": "https://github.com/qcr/benchbot", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "~access our benchbot environments for active robotics (bear) through our challenge and development environment addons (installed by default)~\n~ our benchbot tutorial is the best place to get started developing with benchbot ~\nbenchbot software stack\nthe benchbot software stack is a collection of software packages that allow end users to control robots in real or simulated environments with a simple python api. it leverages the simple \"observe, act, repeat\" approach to robot problems prevalent in reinforcement learning communities (openai gym users will find the benchbot api interface very similar).\nbenchbot was created as a -----> tool !!!  to assist in the research challenges faced by the semantic scene understanding community; challenges including understanding a scene in simulation, transferring algorithms to real world systems, and meaningfully evaluating algorithm performance. we've since realised, these challenges don't just exist for semantic scene understanding, they're prevalent in a wide range of robotic problems.\nthis led us to create version 2 of benchbot with a focus on allowing users to define their own functionality for benchbot through add-ons. want to integrate your own environments? plug-in new robot platforms? define new tasks? share examples with others? add evaluation measures? this all now possible with add-ons, and you don't have to do anything more than add some yaml and python files defining your new content!\nthe \"bench\" in \"benchbot\" refers to benchmarking, with our goal to provide a system that greatly simplifies the benchmarking of novel algorithms in both realistic 3d simulation and on real robot platforms. if there is something else you would like to use benchbot for (like integrating different simulators), please let us know. we're very interested in benchbot being the glue between your novel robotics research and whatever your robot platform may be.\nthis repository contains the software stack needed to develop solutions for benchbot tasks on your local machine. it installs and configures a significant amount of software for you, wraps software in stable docker images (~50gb), and provides simple interaction with the stack through 4 basic scripts: benchbot_install, benchbot_run, benchbot_submit, and benchbot_eval.\nsystem recommendations and requirements\nthe benchbot software stack is designed to run seamlessly on a wide number of system configurations (currently limited to ubuntu 18.04+). system hardware requirements are relatively high due to the software run for 3d simulation (unreal engine, nvidia isaac, vulkan, etc.):\nnvidia graphics card (geforce gtx 1080 minimum, titan xp+ / geforce rtx 2070+ recommended)\ncpu with multiple cores (intel i7-6800k minimum)\n32gb+ ram\n64gb+ spare storage (an ssd storage device is strongly recommended)\nhaving a system that meets the above hardware requirements is all that is required to begin installing the benchbot software stack. the install script analyses your system configuration and offers to install any missing software components interactively. the list of 3rd party software components involved includes:\nnvidia driver (4.18+ required, 4.50+ recommended)\ncuda with gpu support (10.0+ required, 10.1+ recommended)\ndocker engine - community edition (19.03+ required, 19.03.2+ recommended)\nnvidia container toolkit (1.0+ required, 1.0.5+ recommended)\nisaac 2019.2 sdk (requires an nvidia developer login)\nmanaging your installation\ninstallation is simple:\nu@pc:~$ git clone https://github.com/qcr/benchbot && cd benchbot\nu@pc:~$ ./install\nany missing software components, or configuration issues with your system, should be detected by the install script and resolved interactively. the installation asks if you want to add benchbot helper scripts to your path. choosing yes will make the following commands available from any directory: benchbot_install (same as ./install above), benchbot_run, benchbot_submit, benchbot_eval, and benchbot_batch.\nbenchbot installs a default set of add-ons (currently 'benchbot-addons/ssu'), but this can be changed based on how you want to use benchbot. for example, the following will also install the 'benchbot-addons/sqa' add-ons:\nu@pc:~$ benchbot_install --addons benchbot-addons/ssu,benchbot-addons/sqa\nsee the benchbot add-ons manager's documentation for more information on using add-ons.\nthe benchbot software stack will frequently check for updates and can update itself automatically. to update simply run the install script again (add the --force-clean flag if you would like to install from scratch):\nu@pc:~$ benchbot_install\nif you decide to uninstall the benchbot software stack, run:\nu@pc:~$ benchbot_install --uninstall\nthere are a number of other options to customise your benchbot installation, which are all described by running:\nu@pc:~$ benchbot_install --help\ngetting started\ngetting a solution up and running with benchbot is as simple as 1,2,3. here's how to use benchbot with content from the semantic scene understanding add-on:\nrun a simulator with the benchbot software stack by selecting an available robot, environment, and task definition:\nu@pc:~$ benchbot_run --robot carter --env miniroom:1 --task semantic_slam:active:ground_truth\na number of useful flags exist to help you explore what content is available in your installation (see --help for full details). for example, you can list what tasks are available via --list-tasks and view the task specification via --show-task task_name.\ncreate a solution to a benchbot task, and run it against the software stack. to run a solution you must select a mode. for example, if you've created a solution in my_solution.py that you would like to run natively:\nu@pc:~$ benchbot_submit --native python my_solution.py\nsee --help for other options. you also have access to all of the examples available in your installation. for instance, you can run the hello_active example in containerised mode via:\nu@pc:~$ benchbot_submit --containerised --example hello_active\nsee --list-examples and --show-example example_name for full details on what's available out of the box.\nevaluate the performance of your system using a supported evaluation method (see --list-methods). to use the omq evaluation method on my_results.json:\nu@pc:~$ benchbot_eval --method omq my_results.json\nyou can also simply run evaluation automatically after your submission completes:\nu@pc:~$ benchbot_submit --evaluate-with omq --native --example hello_eval_semantic_slam\nthe benchbot tutorial is a great place to start working with benchbot; the tutorial takes you from a blank system to a working semantic slam solution, with many educational steps along the way. also remember the examples in your installation (benchbot-addons/examples_base is a good starting point) which show how to get up and running with the benchbot software stack.\npower tools for autonomous algorithm evaluation\nonce you are confident your algorithm is a solution to the chosen task, the benchbot software stack's power tools allow you to comprehensively explore your algorithm's performance. you can autonomously run your algorithm over multiple environments, and evaluate it holistically to produce a single summary statistic of your algorithm's performance. here are some examples again with content from the semantic scene understanding add-on:\nuse benchbot_batch to run your algorithm in a number of environments and produce a set of results. the script has a number of toggles available to customise the process (see --help for full details). to autonomously run your semantic_slam:active:ground_truth algorithm over 3 environments:\nu@pc:~$ benchbot_batch --robot carter --task semantic_slam:active:ground_truth --envs miniroom:1,miniroom:3,house:5 --native python my_solution.py\nor you can use one of the pre-defined environment batches installed via add-ons (e.g. benchbot-addons/batches_isaac):\nu@pc:~$ benchbot_batch --robot carter --task semantic_slam:active:ground_truth --envs-batch develop_1 --native python my_solution.py\nadditionally, you can create a results zip and request an overall evaluation score at the end of the batch:\nu@pc:~$ benchbot_batch --robot carter --task semantic_slam:active:ground_truth --envs miniroom:1,miniroom:3,house:5 --zip --evaluate-with omq --native python my_solution.py\nlastly, both native and containerised submissions are supported exactly as in benchbot_submit:\nu@pc:~$ benchbot_batch --robot carter --task semantic_slam:active:ground_truth --envs miniroom:1,miniroom:3,house:5 --containerised my_solution_folder/\nyou can also directly call the holistic evaluation performed above by benchbot_batch through the benchbot_eval script. the script supports single result files, multiple results files, or a zip of multiple results files. see benchbot_eval --help for full details. below are examples calling benchbot_eval with a series of results and a zip of results respectively:\nu@pc:~$ benchbot_eval --method omq -o my_jsons_scores result_1.json result_2.json result_3.json\nu@pc:~$ benchbot_eval --method omq -o my_zip_scores results.zip\nusing benchbot in your research\nbenchbot was made to enable and assist the development of high quality, repeatable research results. we welcome any and all use of the benchbot software stack in your research.\nto use our system, we just ask that you cite our paper on the benchbot system. this will help us follow uses of benchbot in the research community, and understand how we can improve the system to help support future research results. citation details are as follows:\n@misc{talbot2020benchbot,\ntitle={benchbot: evaluating robotics research in photorealistic 3d simulation and on real robots},\nauthor={ben talbot and david hall and haoyang zhang and suman raj bista and rohan smith and feras dayoub and niko s\u00fcnderhauf},\nyear={2020},\neprint={2008.00635},\narchiveprefix={arxiv},\nprimaryclass={cs.ro}\n}\ncomponents of the benchbot software stack\nthe benchbot software stack is split into a number of standalone components, each with their own github repository and documentation. this repository glues them all together for you into a working system. the components of the stack are:\nbenchbot_api: user-facing python interface to the benchbot system, allowing the user to control simulated or real robots in simulated or real world environments through simple commands\nbenchbot_addons: a python manager for add-ons to a benchbot system, with full documentation on how to create and add your own add-ons\nbenchbot_supervisor: a http server facilitating communication between user-facing interfaces and the underlying robot controller\nbenchbot_robot_controller: a wrapping script which controls the low-level ros functionality of a simulator or real robot, handles automated subprocess management, and exposes interaction via a http server\nbenchbot_simulator: a realistic 3d simulator employing nvidia's isaac framework, in combination with unreal engine environments\nbenchbot_eval: python library for evaluating the performance in a task, based on the results produced by a submission\nfurther information\nfaqs: wiki page where answers to frequently asked questions and resolutions to common issues will be provided\nsemantic slam tutorial: a tutorial stepping through creating a semantic slam system in benchbot that utilises the 3d object detector votenet\nsupporters\ndevelopment of the benchbot software stack was directly supported by:", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000550, "year": null}, {"Unnamed: 0": 1575, "autor": 555, "date": null, "content": "Overview\nCiting\nInstallation\nChecking\nConvenience\nTroubleshooting\nOverview\nSimuRLacra (composed of the two modules Pyrado and RcsPySim) is a Python/C++ framework for reinforcement learning from randomized physics simulations. The focus is on robotics tasks with mostly continuous control. It features randomizable simulations written in standalone Python (no license required) as well as simulations driven by the physics engines Bullet (no license required), Vortex (license required), or MuJoCo (license required).\nFuruta Pendulum Ball on Beam One-Mass-Oscillator Cart-Pole\nBall-Balancer Ant Half-Cheetah Hopper\nPros\nExceptionally modular treatment of environments via wrappers. The key idea behind this was to be able to quickly modify and randomize all available simulation environments. Moreover, SimuRLacra contains unique environments that either run completely in Python or allow you to switch between the Bullet or Vortex (requires license) physics engine.\nC++ export of policies based on PyTorch Modules. Since the Policy class is a subclass of PyTorch's nn.Module, you can port your neural-network policies, learned with Python, to you C++ applications. This also holds for stateful recurrent networks.\nCPU-based parallelization for sampling the environments. Similar to the OpenAI Gym, SimuRLacra offers parallelized environments for sampling. This is done by employing Serializable, making the simulation environments fully pickleable.\nDeterministic parallel sampling. The sampling is deterministic when conditioned on the seed.\nSeparation of the exploration strategies and the policy. Instead of having a GaussianFNN and a GaussianRNN ect. policy, you can wrap your policy architectures with (almost) any exploration scheme. At test time, you simple strip the exploration wrapper.\nTested integration of real-world Quanser platforms. This feature is extremely valuable if you want to conduct sim-to-real research, since you can simply replace the simulated environment with the physical one by changing one line of code.\nTested integration of BoTorch, Optuna, and sbi.\nDetailed documentation.\nCons\nNo vision-based environments/tasks. In principle there is nothing stopping you from integrating computer vision into SimuRLacra. However, I assume there are better suited frameworks out there.\nWithout bells and whistles. The implementations (especially the algorithms) do not focus on performance. After all, this framework was created to understand and prototype things. However, improvement suggestions are always welcome.\nHyper-parameters are not fully tuned. Sometimes the most important part of reinforcement learning is the time-consuming search for the right hyper-parameters. I only did this for the environment-algorithm combinations reported in my papers. But, for all the other cases there is Optuna and some optuna-based example scripts that you can start from.\nModerate GPU-support. All policies can run on a GPU. However, the GPU-enabled re-implementation of the simulation environments in the pysim folder (simple Python simulations) is at question. The environments based on Rcs which require the Bullet or Vortex physics engine will only be able to run on CPU.\nSimuRLacra was tested on Ubuntu 16.04 (deprecated), 18.04 (recommended), and 20.04, with PyTorch 1.4, 1.7 (deprecated) and 1.8 (recommended). The part without C++ dependencies, called Pyrado, also works under Windows 10 (not supported).\nCiting\nIf you use code or ideas from SimuRLacra for your projects or research, please cite it.\n@misc{Muratore_SimuRLacra,\nauthor = {Fabio Muratore},\ntitle = {SimuRLacra - A Framework for Reinforcement Learning from Randomized Simulations},\nyear = {2020},\npublisher = {GitHub},\njournal = {GitHub repository},\nhowpublished = {\\url{https://github.com/famura/SimuRLacra}}\n}\nInstallation\nIt is recommended to install SimuRLacra in a separate virtual environment such as anaconda. Follow the instructions on the anaconda homepage to download the anaconda (or miniconda) version for your machine (andaconda 3 is recommended).\nClone the repository and go to the project's directory\ngit clone https://github.com/famura/SimuRLacra.git\n# or via ssh\n# git clone git@github.com:famura/SimuRLacra.git\ncd SimuRLacra\nCreate an anaconda environment (without PyTorch) and install the requirements\nconda create -y -n pyrado python=3.7\nconda activate pyrado\nconda install -y blas cmake lapack libgcc-ng mkl mkl-include patchelf pip pycairo setuptools -c conda-forge\npip install -r requirements.txt\nIf you want to use the gym wrapper\npip install gym\nWhat do you want to be installed?\nIf you just want to have a look at SimuRLacra, or don't care about the Rcs-based robotics part, I recommend going for Red Velvet. However, if you for example want to export your learned controller to a C++ program runnning on a phsical robot, I recommend Black Forest. Here is an overview of the options:\nOptions PyTorch build Policy export to C++ CUDA support Rcs-based simulations (RcsPySim) Python-based simulations (Pyrado) (subset of) mujoco-py simulations\nRed Velvet pip \u274c \u2714\ufe0f \u274c \u2714\ufe0f \u2714\ufe0f\nMalakoff local \u2714\ufe0f \u274c \u274c \u2714\ufe0f \u2714\ufe0f\nSacher pip \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nBlack Forest local \u2714\ufe0f \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nPlease note that the Vortex (optionally used in RcsPySim) as well as the MuJoCo (mandatory for mujoco-py) physics engine require a license.\nPlease note that building PyTorch locally from source will take about 30-60 min.\nIn all cases you will download Rcs, eigen3, pybind11, catch2, and mujoco-py, into the thirdParty directory as git submodules. Rcs will be placed in the project's root directory.\nOption Red Velvet\nMake sure the correct anaconda environment is activated conda activate pyrado\nChoose one\nFor CUDA \u2265 11 (for RTX 3000 Series or Newer):\npip install torch==1.8.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\nFor CUDA < 11:\npip install torch==1.8.1\nWithout CUDA:\npip install torch==1.8.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\nRun the setup script (which initializes and updates the git submodules)\npython setup_deps.py wo_rcs_wo_pytorch -j8 # use --headless option when on a cluster\nIn case this process crashes, please check the Troubleshooting section below. If everything went as expected, you are done and can optionally look at the Checking section.\nOption Malakoff\nMake sure the correct anaconda environment is activated conda activate pyrado\nRun the setup script (which initializes and updates the git submodules)\npython setup_deps.py wo_rcs_w_pytorch -j8 # use --headless option when on a cluster\nIn case this process crashes, please check the Troubleshooting section below. If everything went as expected, you are done and can optionally look at the Checking section.\nOption Sacher\nInfrastructure dependent: install libraries system-wide\nParts of this framework create Python bindings of Rcs called RcsPySim. Running Rcs requires several libraries which can be installed (requires sudo rights) via\npython setup_deps.py dep_libraries\nThis command will install g++-4.8, libqwt-qt5-dev, libbullet-dev, libfreetype6-dev, libxml2-dev, libglu1-mesa-dev, freeglut3-dev, mesa-common-dev, libopenscenegraph-dev, openscenegraph, and liblapack-dev. In case you have no sudo rights, but want to use all the Rcs-dependent environments, you can try installing the libraries via anaconda. For references, see the comments behind required_packages in setup_deps.py.\nIf you can't install the libraries, you can still use the Python part of this framework called Pyrado, but no environments in the rcspysim folder.\nMake sure the correct anaconda environment is activated conda activate pyrado\nChoose one\nFor CUDA \u2265 11 (for RTX 3000 Series or Newer):\npip install torch==1.8.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\nFor CUDA < 11:\npip install torch==1.8.1\nWithout CUDA:\npip install torch==1.8.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\nRun the setup script (which initializes and updates the git submodules)\npython setup_deps.py w_rcs_wo_pytorch -j8 # use --headless option when on a cluster\nIn case this process crashes, please check the Troubleshooting section below. If everything went as expected, you are done and can optionally look at the Checking section.\nOption Black Forest\nInfrastructure dependent: install libraries system-wide\nParts of this framework create Python bindings of Rcs called RcsPySim. Running Rcs requires several libraries which can be installed (requires sudo rights) via\npython setup_deps.py dep_libraries\nThis command will install g++-4.8, libqwt-qt5-dev, libbullet-dev, libfreetype6-dev, libxml2-dev, libglu1-mesa-dev, freeglut3-dev, mesa-common-dev, libopenscenegraph-dev, openscenegraph, and liblapack-dev. In case you have no sudo rights, but want to use all the Rcs-dependent environments, you can try installing the libraries via anaconda. For references, see the comments behind required_packages in setup_deps.py.\nIf you can't install the libraries, you can still use the Python part of this framework called Pyrado, but no environments in the rcspysim folder.\nMake sure the correct anaconda environment is activated conda activate pyrado\nRun the setup script (which initializes and updates the git submodules)\npython setup_deps.py w_rcs_w_pytorch -j8 # use --headless option when on a cluster\nIn case this process crashes, please check the Troubleshooting section below. If everything went as expected, you are done and can optionally look at the Checking section.\nSL & Robcom\nIn case you are at IAS and want to use you SL and robcom, you can set them up (requires sudo rights) with\npython setup_deps.py robcom -j8\nAfter that you still need to install the robot-specific package in SL.\nPython Code Formatting with Black and isort (and pre-commit)\nWe are following the Black code style and isort ordering for all Python files. The black package is already integrated to the pyrado anaconda environment, and configured by the pyproject.toml file. You can format your local code by running\ncd PATH_TO/SimuRLacra\nisort Pyrado --check --diff # remove --check to actually do the changes\nblack Pyrado --check # remove --check to actually do the changes\nMoreover, you can install the pre-commit framework via\npython setup_deps.py pre_commit\nwhich will reformat your code before every commit. The conformity with Black and isort is checked using a GitHub action.\nChecking\nVerify the installation of PyTorch\nconda activate pyrado\nconda env list\nconda list | grep torch # check if the desired version of PyTorch is installed\npython --version # should return Python 3.7.X :: Anaconda, Inc._\nVerify the installation of Pyrado and RcsPySim\nTo exemplarily check basic Pyrado environments (implemented in Python without dependencies to RcsPySim)\nconda activate pyrado\ncd PATH_TO/SimuRLacra/Pyrado/scripts\npython sandbox/sb_qcp.py --env_name qcp-su --dt 0.002\nQuickly check the environments interfacing Rcs via RcsPySim\npython sandbox/sb_qq_rcspysim.py\nIf this does not work it may be because Vortex or Bullet is not installed.\nFor deeper testing, run Pyrado's unit tests\ncd PATH_TO/SimuRLacra/Pyrado/tests\npytest -v -m \"not longtime\"\nBuild and view the documentation\nIf not already activated, execute\nconda activate pyrado\nBuild both html documentations\ncd PATH_TO/SimuRLacra\n./build_docs.sh\nThis will fail if you did not set up RcsPySim.\nRcsPySim\nfirefox RcsPySim/build/doc/html/index.html\nPyrado\nfirefox Pyrado/doc/build/index.html\nConvenience\nHandy aliases\nYou will find yourself often in the same folders, so adding the following aliases to your shell's rc-file will be worth it.\nalias cds='cd PATH_TO/SimuRLacra'\nalias cdps='cd PATH_TO/SimuRLacra/Pyrado/scripts'\nalias cdpt='cd PATH_TO/SimuRLacra/Pyrado/data/temp'\nalias cdrps='cd PATH_TO/SimuRLacra/RcsPySim/build'\nalias cdrcs='cd PATH_TO/SimuRLacra/Rcs/build'\nWorking on the intersection of C++ and Python (e.g. RcsPySim)\nAssuming that you use an IDE (in this case CLion), it is nice to put an empty CMakeLists.txt into the Python part of your project (here Pyrado) and include this as a subdirectory from the C++ part of your project by adding\nadd_subdirectory(../Pyrado \"${CMAKE_BINARY_DIR}/pyrado\")\nIf you then create a project in the RcsPySim directory, your IDE will automatically add Pyrado for you. If you moreover mark Pyrado as sources root (CLion specific), it will be parsed by the IDE's git tool.\nI also suggest to create run configuration that always build the C++ part (RcsPySim) before executing a Python script. In CLion or example, you go Run->Edit Configurations ..., select CMake Application, hit the plus, select _rcsenv as target and python as executable, make your program arguments a module call like -m scripts.sandbox.sb_p3l in connection with the correct working directory PATH_TO/SimuRLacra/Pyrado, and most importantly select Build in the Before launch section.\nIn a similar fashion, you can directly call Rcs. This is useful when you are creating a new environment and want to iterate the graph xml-file. In CLion or example, you go Run->Edit Configurations ..., select CMake Application, hit the plus, select _rcsenv as target and Rcs as executable, pass Rcs-specific arguments to your program arguments like -m 4 -dir PATH_TO/SimuRLacra/RcsPySim/config/Planar3Link/ -f gPlanar3Link.xml in connection with the correct working directory PATH_TO/SimuRLacra/Rcs/build, and select Build in the Before launch section. There are many more command line arguments for Rcs. Look for argP in the Rcs.cpp source file.\nInspecting training logs\nTo look at the training report in detail from console, I recommend to put\nfunction pretty_csv {\ncolumn -t -s, -n \"$@\" | less -F -S -X -K\n}\ninto your sell's rc-file. Executing pretty_csv progress.csv in the experiments folder will yield a nicely formatted table. I found this neat little trick on Stefaan Lippens blog. You might need to install column depending on your OS.\nPDB (The Python Debugger) and Multiprocessing\nDue to the multiprocessing, it is possible that PDB (the Python Debugger) hangs after executing an inspection command. You can set the environment variable ENABLE_SINGLE_WORKER_OPTIMIZATION to disable the sampling parallelization if the number of workers is set to one. However, this will destroy the seed-determinism! Thus only enable it for debugging purposes and not for evaluating an algorithm and similar.\nTroubleshooting\nNo module named _rcsenv\nThis error message is thrown when you try to load a RcsPySim environment without it being properly set up. There are multiple reasons how this could have happened. For example you are switching between different Options (see above), or the cmake options are not correct, or sth more nasty. In the most cases, this error can be solved by removing the complete content within Rcs/build and the RcsPySim/build, and then re-run the instructions of your chosen option.\nUndefined reference to inflateValidate\nDepending on the libraries install on your machine, you might receive the linker error undefined reference to inflateValidate@ZLIB_1.2.9 while building Rcs or RcsPySim. In otder to solve this error, link the z library to the necessary targets by editing the PATH_TO/SimuRLacra/Rcs/bin/CMakeLists.txt replacing\nTARGET_LINK_LIBRARIES(Rcs RcsCore RcsGui RcsGraphics RcsPhysics)\nTARGET_LINK_LIBRARIES(TestGeometry RcsCore RcsGui RcsGraphics RcsPhysics)\nby\nTARGET_LINK_LIBRARIES(Rcs RcsCore RcsGui RcsGraphics RcsPhysics z)\nTARGET_LINK_LIBRARIES(TestGeometry RcsCore RcsGui RcsGraphics RcsPhysics z)\nThe same goes for PATH_TO/SimuRLacra/Rcs/examples/CMakeLists.txt where you replace\nTARGET_LINK_LIBRARIES(ExampleForwardKinematics RcsCore RcsGui RcsGraphics)\nTARGET_LINK_LIBRARIES(ExampleKinetics RcsCore RcsGui RcsGraphics RcsPhysics)\nby\nTARGET_LINK_LIBRARIES(ExampleForwardKinematics RcsCore RcsGui RcsGraphics z)\nTARGET_LINK_LIBRARIES(ExampleKinetics RcsCore RcsGui RcsGraphics RcsPhysics z)\nPython debugger stuck at evaluating expression\nBy default, the sampling (on CPU) in Pyrado is parallelized using PyTorch's multiprocessing module. Thus, your debugger will not be connected to the right process. Rerun your script with num_sampler_envs=1 passed as a parameter to the algorithm, that will then construct a sampler which only uses one process.\nQt5 and Vortex (libpng15.so)\nIf you are using Vortex, which itself has a Qt5-based GUI, RcsPySim may look for the wrong libpng version. Make sure that if finds the same one as Rcs (libpng16.so) and not the one from Vortex (libpng15.so). You can investigate this using the ldd (or lddtree if installed) command on the generated RcsPySim executables. An easy fix is to go to your Vortex library directory and move all Qt5-related libs to a newly generated folder, such that they cant be found. This solution is perfectly fine since we are not using the Vortex GUI anyway. Next, clear the RcsPySim/build folder and build it again.\nBullet double vs. float\nCheck Rcs with which precision Bullet was build\ncd PATH_TO/SimuRLacra/thirdParty/Rcs/build\nccmake .\nUse the same in RcsPySim\ncd PATH_TO/SimuRLacra/RcsPySim/build\nccmake .\nRebuild RcsPySim (with activated anaconda env)\ncd PATH_TO/SimuRLacra/RcsPySim/build\nmake -j12\nModule init-args-initializer\nModuleNotFoundError: No module named 'init_args_serializer' Install it from git+https://github.com/Xfel/init-args-serializer.git@master\nWhen you export the anaconda environment, the yml-file will contain the line init-args-serializer==1.0. This will cause an error when creating a new anaconda environment from this yml-file. To fix this, replace the line with git+https://github.com/Xfel/init-args-serializer.git@master.\nPyTorch version\nYou run a script and get ImportError: cannot import name 'export'? Check if your PyTorch version is >= 1.8. If not, update via\ncd PATH_TO/SimuRLacra\npython setup_deps.py pytorch -j12\nor install the pre-compiled version form anaconda using\nconda install pytorch torchvision cpuonly -c pytorch\nNote: if you choose the latter, the C++ export of policies will not work.\nsetup.py not found\nIf you receive PATH_TO/anaconda3/envs/pyrado/bin/python: can't open file 'setup.py': [Errno 2] No such file or directory while executing python setup_deps pytorch, delete the thirdParty/pytorch and run\ncd PATH_TO/SimuRLacra\npython setup_deps.py pytorch -j12\nLapack library not found in compile time (PyTorch)\nOption 1: if you have sudo rights, run\nsudo apt-get install libopenblas-dev\nand then rebuild PyTorch from scratch. Option 2: if you don't have sudo rights, run\nconda install -c conda-forge lapack\nand then rebuild PyTorch from scratch.\nPyrado's policy export tests are skipped\nRun the setup_deps.py scripts again with --local_torch, or explicitly set USE_LIBTORCH = ON for the cmake arguments of RcsPySim\ncd PATH_TO/SimuRLacra/Rcs/build\nccmake . # set the option, configure (2x), and generate\nPyTorch compilation is too slow or uses too many CPUs\nThe Pytorch setup script (thirdParty/pytorch/setup.py) determines the number of cpus to compile automatically. It can be overridden by setting the environment variable MAX_JOBS:\nexport MAX_JOBS=1 # or another small number\nPlease use your shell syntax accordingly (the above example is for bash).\nSet up MuJoCo and mujoco-py\nDownload mujoco200 linux from the official page and extract it to ~/.mujoco such that you have ~/.mujoco/mujoco200. Put your MuJoCo license file in ~/.mujoco.\nDuring executing setup_deps.py, mujoco-py is set up as a git submodule and installed via the downloaded setup.py. If this fails, have a look at the mujoco-py's canonical dependencies. Try again. If you get an error mentioning patchelf, run conda install -c anaconda patchelf\nIn case you get visualization errors related to GLEW (render causes a frozen window and crashes, or simply a completely black screen) add export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libGLEW.so to your shell's rc-file (like ~/.bashrc). If you now create a new terminal, it should work. If not, try sudo apt-get install libglew-dev.\nShut down the mujoco-py message about the missing MuJoCo installation / license\nIf you dont have a MuJoCo license, or MuJoCo is not installed on zour machine, mujoco-py will print an error message. One way to avoid this would be to not install mujoco-py by default. However, this would create even more options above. Thus, we will just fool mujoco-py's checker by creating a fake directory and an empty license file.\nmkdir /$HOME/.mujoco/mujoco200 -p && touch /$HOME/.mujoco/mjkey.txt\nUbuntu 20.04 and mujoco-py > 2.0.2.5\nUninstall mujoco-py\npip uninstall mujoco_py\nInstall mujoco-py version 2.0.2.5 (this time not using the setup.py)\npip install mujoco_py==2.0.2.5\nThere might be an error saying Failed building wheel for mujoco-py at first, but in the end it should install lockfile and mujoco-py.\nlibstdc++.so.6: version `GLIBCXX_3.4.22' not found\nThis error might come from the scipy.signal.lfilter command (eventually including scipy's fft function). For scipy versions > 1.5.2, this requires GLIBCXX_3.4.22. If your computer is out -of-date and you have no sudo rights, your best option is to set scipy pack to version 1.5.2.\nconda activate pyrado\nconda remove scipy --force\npip install scipy==1.5.2\nClion complains about a non-empty build directory\nDelete the build folder and reload the CMakeProject (right cklick on the CMakeLists.txt). Now build the project again with Clion.\nClion does not find the\nGo to the settings and look for the Buld, Execution, Deployment -> Toolchains tab. Veryfy that your cmake version is supported. If now down/upgrade, and reload the CMakeProject.\nImageMagick error from moviepy\nCheck for the ImageMagick policy file. ImageMagick does not have the proper permission set. You can edit the policy file (requires sudo rights)\nsudo vi /etc/ImageMagick-6/policy.xml\nby commenting out the line(s) containing <policy domain=\"path\" rights=\"none\" pattern=\"@*\" />. Now try again.", "link": "https://github.com/famura/SimuRLacra", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "overview\nciting\ninstallation\nchecking\nconvenience\ntroubleshooting\noverview\nsimurlacra (composed of the two modules pyrado and rcspysim) is a python/c++ framework for reinforcement learning from randomized physics simulations. the focus is on robotics tasks with mostly continuous control. it features randomizable simulations written in standalone python (no license required) as well as simulations driven by the physics engines bullet (no license required), vortex (license required), or mujoco (license required).\nfuruta pendulum ball on beam one-mass-oscillator cart-pole\nball-balancer ant half-cheetah hopper\npros\nexceptionally modular treatment of environments via wrappers. the key idea behind this was to be able to quickly modify and randomize all available simulation environments. moreover, simurlacra contains unique environments that either run completely in python or allow you to switch between the bullet or vortex (requires license) physics engine.\nc++ export of policies based on pytorch modules. since the policy class is a subclass of pytorch's nn.module, you can port your neural-network policies, learned with python, to you c++ applications. this also holds for stateful recurrent networks.\ncpu-based parallelization for sampling the environments. similar to the openai gym, simurlacra offers parallelized environments for sampling. this is done by employing serializable, making the simulation environments fully pickleable.\ndeterministic parallel sampling. the sampling is deterministic when conditioned on the seed.\nseparation of the exploration strategies and the policy. instead of having a gaussianfnn and a gaussianrnn ect. policy, you can wrap your policy architectures with (almost) any exploration scheme. at test time, you simple strip the exploration wrapper.\ntested integration of real-world quanser platforms. this feature is extremely valuable if you want to conduct sim-to-real research, since you can simply replace the simulated environment with the physical one by changing one line of code.\ntested integration of botorch, optuna, and sbi.\ndetailed documentation.\ncons\nno vision-based environments/tasks. in principle there is nothing stopping you from integrating computer vision into simurlacra. however, i assume there are better suited frameworks out there.\nwithout bells and whistles. the implementations (especially the algorithms) do not focus on performance. after all, this framework was created to understand and prototype things. however, improvement suggestions are always welcome.\nhyper-parameters are not fully tuned. sometimes the most important part of reinforcement learning is the time-consuming search for the right hyper-parameters. i only did this for the environment-algorithm combinations reported in my papers. but, for all the other cases there is optuna and some optuna-based example scripts that you can start from.\nmoderate gpu-support. all policies can run on a gpu. however, the gpu-enabled re-implementation of the simulation environments in the pysim folder (simple python simulations) is at question. the environments based on rcs which require the bullet or vortex physics engine will only be able to run on cpu.\nsimurlacra was tested on ubuntu 16.04 (deprecated), 18.04 (recommended), and 20.04, with pytorch 1.4, 1.7 (deprecated) and 1.8 (recommended). the part without c++ dependencies, called pyrado, also works under windows 10 (not supported).\nciting\nif you use code or ideas from simurlacra for your projects or research, please cite it.\n@misc{muratore_simurlacra,\nauthor = {fabio muratore},\ntitle = {simurlacra - a framework for reinforcement learning from randomized simulations},\nyear = {2020},\npublisher = {github},\njournal = {github repository},\nhowpublished = {\\url{https://github.com/famura/simurlacra}}\n}\ninstallation\nit is recommended to install simurlacra in a separate virtual environment such as anaconda. follow the instructions on the anaconda homepage to download the anaconda (or miniconda) version for your machine (andaconda 3 is recommended).\nclone the repository and go to the project's directory\ngit clone https://github.com/famura/simurlacra.git\n# or via ssh\n# git clone git@github.com:famura/simurlacra.git\ncd simurlacra\ncreate an anaconda environment (without pytorch) and install the requirements\nconda create -y -n pyrado python=3.7\nconda activate pyrado\nconda install -y blas cmake lapack libgcc-ng mkl mkl-include patchelf pip pycairo setuptools -c conda-forge\npip install -r requirements.txt\nif you want to use the gym wrapper\npip install gym\nwhat do you want to be installed?\nif you just want to have a look at simurlacra, or don't care about the rcs-based robotics part, i recommend going for red velvet. however, if you for example want to export your learned controller to a c++ program runnning on a phsical robot, i recommend black forest. here is an overview of the options:\noptions pytorch build policy export to c++ cuda support rcs-based simulations (rcspysim) python-based simulations (pyrado) (subset of) mujoco-py simulations\nred velvet pip \u274c \u2714\ufe0f \u274c \u2714\ufe0f \u2714\ufe0f\nmalakoff local \u2714\ufe0f \u274c \u274c \u2714\ufe0f \u2714\ufe0f\nsacher pip \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nblack forest local \u2714\ufe0f \u274c \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f\nplease note that the vortex (optionally used in rcspysim) as well as the mujoco (mandatory for mujoco-py) physics engine require a license.\nplease note that building pytorch locally from source will take about 30-60 min.\nin all cases you will download rcs, eigen3, pybind11, catch2, and mujoco-py, into the thirdparty directory as git submodules. rcs will be placed in the project's root directory.\noption red velvet\nmake sure the correct anaconda environment is activated conda activate pyrado\nchoose one\nfor cuda \u2265 11 (for rtx 3000 series or newer):\npip install torch==1.8.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\nfor cuda < 11:\npip install torch==1.8.1\nwithout cuda:\npip install torch==1.8.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\nrun the setup script (which initializes and updates the git submodules)\npython setup_deps.py wo_rcs_wo_pytorch -j8 # use --headless option when on a cluster\nin case this process crashes, please check the troubleshooting section below. if everything went as expected, you are done and can optionally look at the checking section.\noption malakoff\nmake sure the correct anaconda environment is activated conda activate pyrado\nrun the setup script (which initializes and updates the git submodules)\npython setup_deps.py wo_rcs_w_pytorch -j8 # use --headless option when on a cluster\nin case this process crashes, please check the troubleshooting section below. if everything went as expected, you are done and can optionally look at the checking section.\noption sacher\ninfrastructure dependent: install libraries system-wide\nparts of this framework create python bindings of rcs called rcspysim. running rcs requires several libraries which can be installed (requires sudo rights) via\npython setup_deps.py dep_libraries\nthis command will install g++-4.8, libqwt-qt5-dev, libbullet-dev, libfreetype6-dev, libxml2-dev, libglu1-mesa-dev, freeglut3-dev, mesa-common-dev, libopenscenegraph-dev, openscenegraph, and liblapack-dev. in case you have no sudo rights, but want to use all the rcs-dependent environments, you can try installing the libraries via anaconda. for references, see the comments behind required_packages in setup_deps.py.\nif you can't install the libraries, you can still use the python part of this framework called pyrado, but no environments in the rcspysim folder.\nmake sure the correct anaconda environment is activated conda activate pyrado\nchoose one\nfor cuda \u2265 11 (for rtx 3000 series or newer):\npip install torch==1.8.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\nfor cuda < 11:\npip install torch==1.8.1\nwithout cuda:\npip install torch==1.8.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\nrun the setup script (which initializes and updates the git submodules)\npython setup_deps.py w_rcs_wo_pytorch -j8 # use --headless option when on a cluster\nin case this process crashes, please check the troubleshooting section below. if everything went as expected, you are done and can optionally look at the checking section.\noption black forest\ninfrastructure dependent: install libraries system-wide\nparts of this framework create python bindings of rcs called rcspysim. running rcs requires several libraries which can be installed (requires sudo rights) via\npython setup_deps.py dep_libraries\nthis command will install g++-4.8, libqwt-qt5-dev, libbullet-dev, libfreetype6-dev, libxml2-dev, libglu1-mesa-dev, freeglut3-dev, mesa-common-dev, libopenscenegraph-dev, openscenegraph, and liblapack-dev. in case you have no sudo rights, but want to use all the rcs-dependent environments, you can try installing the libraries via anaconda. for references, see the comments behind required_packages in setup_deps.py.\nif you can't install the libraries, you can still use the python part of this framework called pyrado, but no environments in the rcspysim folder.\nmake sure the correct anaconda environment is activated conda activate pyrado\nrun the setup script (which initializes and updates the git submodules)\npython setup_deps.py w_rcs_w_pytorch -j8 # use --headless option when on a cluster\nin case this process crashes, please check the troubleshooting section below. if everything went as expected, you are done and can optionally look at the checking section.\nsl & robcom\nin case you are at ias and want to use you sl and robcom, you can set them up (requires sudo rights) with\npython setup_deps.py robcom -j8\nafter that you still need to install the robot-specific package in sl.\npython code formatting with black and isort (and pre-commit)\nwe are following the black code style and isort ordering for all python files. the black package is already integrated to the pyrado anaconda environment, and configured by the pyproject.toml file. you can format your local code by running\ncd path_to/simurlacra\nisort pyrado --check --diff # remove --check to actually do the changes\nblack pyrado --check # remove --check to actually do the changes\nmoreover, you can install the pre-commit framework via\npython setup_deps.py pre_commit\nwhich will reformat your code before every commit. the conformity with black and isort is checked using a github action.\nchecking\nverify the installation of pytorch\nconda activate pyrado\nconda env list\nconda list | grep torch # check if the desired version of pytorch is installed\npython --version # should return python 3.7.x :: anaconda, inc._\nverify the installation of pyrado and rcspysim\nto exemplarily check basic pyrado environments (implemented in python without dependencies to rcspysim)\nconda activate pyrado\ncd path_to/simurlacra/pyrado/scripts\npython sandbox/sb_qcp.py --env_name qcp-su --dt 0.002\nquickly check the environments interfacing rcs via rcspysim\npython sandbox/sb_qq_rcspysim.py\nif this does not work it may be because vortex or bullet is not installed.\nfor deeper testing, run pyrado's unit tests\ncd path_to/simurlacra/pyrado/tests\npytest -v -m \"not longtime\"\nbuild and view the documentation\nif not already activated, execute\nconda activate pyrado\nbuild both html documentations\ncd path_to/simurlacra\n./build_docs.sh\nthis will fail if you did not set up rcspysim.\nrcspysim\nfirefox rcspysim/build/doc/html/index.html\npyrado\nfirefox pyrado/doc/build/index.html\nconvenience\nhandy aliases\nyou will find yourself often in the same folders, so adding the following aliases to your shell's rc-file will be worth it.\nalias cds='cd path_to/simurlacra'\nalias cdps='cd path_to/simurlacra/pyrado/scripts'\nalias cdpt='cd path_to/simurlacra/pyrado/data/temp'\nalias cdrps='cd path_to/simurlacra/rcspysim/build'\nalias cdrcs='cd path_to/simurlacra/rcs/build'\nworking on the intersection of c++ and python (e.g. rcspysim)\nassuming that you use an ide (in this case clion), it is nice to put an empty cmakelists.txt into the python part of your project (here pyrado) and include this as a subdirectory from the c++ part of your project by adding\nadd_subdirectory(../pyrado \"${cmake_binary_dir}/pyrado\")\nif you then create a project in the rcspysim directory, your ide will automatically add pyrado for you. if you moreover mark pyrado as sources root (clion specific), it will be parsed by the ide's git -----> tool !!! .\ni also suggest to create run configuration that always build the c++ part (rcspysim) before executing a python script. in clion or example, you go run->edit configurations ..., select cmake application, hit the plus, select _rcsenv as target and python as executable, make your program arguments a module call like -m scripts.sandbox.sb_p3l in connection with the correct working directory path_to/simurlacra/pyrado, and most importantly select build in the before launch section.\nin a similar fashion, you can directly call rcs. this is useful when you are creating a new environment and want to iterate the graph xml-file. in clion or example, you go run->edit configurations ..., select cmake application, hit the plus, select _rcsenv as target and rcs as executable, pass rcs-specific arguments to your program arguments like -m 4 -dir path_to/simurlacra/rcspysim/config/planar3link/ -f gplanar3link.xml in connection with the correct working directory path_to/simurlacra/rcs/build, and select build in the before launch section. there are many more command line arguments for rcs. look for argp in the rcs.cpp source file.\ninspecting training logs\nto look at the training report in detail from console, i recommend to put\nfunction pretty_csv {\ncolumn -t -s, -n \"$@\" | less -f -s -x -k\n}\ninto your sell's rc-file. executing pretty_csv progress.csv in the experiments folder will yield a nicely formatted table. i found this neat little trick on stefaan lippens blog. you might need to install column depending on your os.\npdb (the python debugger) and multiprocessing\ndue to the multiprocessing, it is possible that pdb (the python debugger) hangs after executing an inspection command. you can set the environment variable enable_single_worker_optimization to disable the sampling parallelization if the number of workers is set to one. however, this will destroy the seed-determinism! thus only enable it for debugging purposes and not for evaluating an algorithm and similar.\ntroubleshooting\nno module named _rcsenv\nthis error message is thrown when you try to load a rcspysim environment without it being properly set up. there are multiple reasons how this could have happened. for example you are switching between different options (see above), or the cmake options are not correct, or sth more nasty. in the most cases, this error can be solved by removing the complete content within rcs/build and the rcspysim/build, and then re-run the instructions of your chosen option.\nundefined reference to inflatevalidate\ndepending on the libraries install on your machine, you might receive the linker error undefined reference to inflatevalidate@zlib_1.2.9 while building rcs or rcspysim. in otder to solve this error, link the z library to the necessary targets by editing the path_to/simurlacra/rcs/bin/cmakelists.txt replacing\ntarget_link_libraries(rcs rcscore rcsgui rcsgraphics rcsphysics)\ntarget_link_libraries(testgeometry rcscore rcsgui rcsgraphics rcsphysics)\nby\ntarget_link_libraries(rcs rcscore rcsgui rcsgraphics rcsphysics z)\ntarget_link_libraries(testgeometry rcscore rcsgui rcsgraphics rcsphysics z)\nthe same goes for path_to/simurlacra/rcs/examples/cmakelists.txt where you replace\ntarget_link_libraries(exampleforwardkinematics rcscore rcsgui rcsgraphics)\ntarget_link_libraries(examplekinetics rcscore rcsgui rcsgraphics rcsphysics)\nby\ntarget_link_libraries(exampleforwardkinematics rcscore rcsgui rcsgraphics z)\ntarget_link_libraries(examplekinetics rcscore rcsgui rcsgraphics rcsphysics z)\npython debugger stuck at evaluating expression\nby default, the sampling (on cpu) in pyrado is parallelized using pytorch's multiprocessing module. thus, your debugger will not be connected to the right process. rerun your script with num_sampler_envs=1 passed as a parameter to the algorithm, that will then construct a sampler which only uses one process.\nqt5 and vortex (libpng15.so)\nif you are using vortex, which itself has a qt5-based gui, rcspysim may look for the wrong libpng version. make sure that if finds the same one as rcs (libpng16.so) and not the one from vortex (libpng15.so). you can investigate this using the ldd (or lddtree if installed) command on the generated rcspysim executables. an easy fix is to go to your vortex library directory and move all qt5-related libs to a newly generated folder, such that they cant be found. this solution is perfectly fine since we are not using the vortex gui anyway. next, clear the rcspysim/build folder and build it again.\nbullet double vs. float\ncheck rcs with which precision bullet was build\ncd path_to/simurlacra/thirdparty/rcs/build\nccmake .\nuse the same in rcspysim\ncd path_to/simurlacra/rcspysim/build\nccmake .\nrebuild rcspysim (with activated anaconda env)\ncd path_to/simurlacra/rcspysim/build\nmake -j12\nmodule init-args-initializer\nmodulenotfounderror: no module named 'init_args_serializer' install it from git+https://github.com/xfel/init-args-serializer.git@master\nwhen you export the anaconda environment, the yml-file will contain the line init-args-serializer==1.0. this will cause an error when creating a new anaconda environment from this yml-file. to fix this, replace the line with git+https://github.com/xfel/init-args-serializer.git@master.\npytorch version\nyou run a script and get importerror: cannot import name 'export'? check if your pytorch version is >= 1.8. if not, update via\ncd path_to/simurlacra\npython setup_deps.py pytorch -j12\nor install the pre-compiled version form anaconda using\nconda install pytorch torchvision cpuonly -c pytorch\nnote: if you choose the latter, the c++ export of policies will not work.\nsetup.py not found\nif you receive path_to/anaconda3/envs/pyrado/bin/python: can't open file 'setup.py': [errno 2] no such file or directory while executing python setup_deps pytorch, delete the thirdparty/pytorch and run\ncd path_to/simurlacra\npython setup_deps.py pytorch -j12\nlapack library not found in compile time (pytorch)\noption 1: if you have sudo rights, run\nsudo apt-get install libopenblas-dev\nand then rebuild pytorch from scratch. option 2: if you don't have sudo rights, run\nconda install -c conda-forge lapack\nand then rebuild pytorch from scratch.\npyrado's policy export tests are skipped\nrun the setup_deps.py scripts again with --local_torch, or explicitly set use_libtorch = on for the cmake arguments of rcspysim\ncd path_to/simurlacra/rcs/build\nccmake . # set the option, configure (2x), and generate\npytorch compilation is too slow or uses too many cpus\nthe pytorch setup script (thirdparty/pytorch/setup.py) determines the number of cpus to compile automatically. it can be overridden by setting the environment variable max_jobs:\nexport max_jobs=1 # or another small number\nplease use your shell syntax accordingly (the above example is for bash).\nset up mujoco and mujoco-py\ndownload mujoco200 linux from the official page and extract it to ~/.mujoco such that you have ~/.mujoco/mujoco200. put your mujoco license file in ~/.mujoco.\nduring executing setup_deps.py, mujoco-py is set up as a git submodule and installed via the downloaded setup.py. if this fails, have a look at the mujoco-py's canonical dependencies. try again. if you get an error mentioning patchelf, run conda install -c anaconda patchelf\nin case you get visualization errors related to glew (render causes a frozen window and crashes, or simply a completely black screen) add export ld_preload=/usr/lib/x86_64-linux-gnu/libglew.so to your shell's rc-file (like ~/.bashrc). if you now create a new terminal, it should work. if not, try sudo apt-get install libglew-dev.\nshut down the mujoco-py message about the missing mujoco installation / license\nif you dont have a mujoco license, or mujoco is not installed on zour machine, mujoco-py will print an error message. one way to avoid this would be to not install mujoco-py by default. however, this would create even more options above. thus, we will just fool mujoco-py's checker by creating a fake directory and an empty license file.\nmkdir /$home/.mujoco/mujoco200 -p && touch /$home/.mujoco/mjkey.txt\nubuntu 20.04 and mujoco-py > 2.0.2.5\nuninstall mujoco-py\npip uninstall mujoco_py\ninstall mujoco-py version 2.0.2.5 (this time not using the setup.py)\npip install mujoco_py==2.0.2.5\nthere might be an error saying failed building wheel for mujoco-py at first, but in the end it should install lockfile and mujoco-py.\nlibstdc++.so.6: version `glibcxx_3.4.22' not found\nthis error might come from the scipy.signal.lfilter command (eventually including scipy's fft function). for scipy versions > 1.5.2, this requires glibcxx_3.4.22. if your computer is out -of-date and you have no sudo rights, your best option is to set scipy pack to version 1.5.2.\nconda activate pyrado\nconda remove scipy --force\npip install scipy==1.5.2\nclion complains about a non-empty build directory\ndelete the build folder and reload the cmakeproject (right cklick on the cmakelists.txt). now build the project again with clion.\nclion does not find the\ngo to the settings and look for the buld, execution, deployment -> toolchains tab. veryfy that your cmake version is supported. if now down/upgrade, and reload the cmakeproject.\nimagemagick error from moviepy\ncheck for the imagemagick policy file. imagemagick does not have the proper permission set. you can edit the policy file (requires sudo rights)\nsudo vi /etc/imagemagick-6/policy.xml\nby commenting out the line(s) containing <policy domain=\"path\" rights=\"none\" pattern=\"@*\" />. now try again.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000555, "year": null}, {"Unnamed: 0": 1586, "autor": 566, "date": null, "content": "MeshLocalize: Model-based object tracking in 3D for ROS\nTracking Video: https://www.youtube.com/watch?v=UqzNlcw1U7s\nGrasping Demo: https://www.youtube.com/watch?v=T953WeLroqg\n1. Installation\n1.1 Dependencies\nThis has only been tested with Ubuntu 14.04 + ROS Indigo.\nInstall Eigen3.\nInstall OpenCV 2.4.11. Source can be downloaded from https://github.com/Itseez/opencv/archive/2.4.11.zip\nInstall OIS (needed for OGRE).\nsudo apt-get install libois-dev\nsudo apt-get install libois-1.3.0\nor source can be downloaded from http://sourceforge.net/projects/wgois/files/\nInstall Nvidia Cg Toolkit\nsudo apt-get install nvidia-cg-toolkit\nInstall Ogre 1.8.1. You can download the source from http://sourceforge.net/projects/ogre/files/ogre/1.8/1.8.1/ogre_src_v1-8-1.tar.bz2/download\nInstall TooN from http://www.edwardrosten.com/cvd/toon.html\nInstall the object_renderer library. It is a small wrapper around OGRE. It is used to generate the virtual views of the object that the package uses to initialize the pose.\ngit clone https://github.com/msheckells/object_renderer\nBuild using cmake+make.\n1.2 Package Install\nFrom your catkin source directory:\ngit clone https://github.com/msheckells/mesh_localize.git\nCall catkin_make as usual\n2. Quick Start\nDownload a test rosbag which contains image and camera_info data from http://www.mattsheckells.com/wp-content/uploads/2015/cheezit_test.bag.zip and extract it.\nLaunch the node:\nroslaunch mesh_localize localize_cheezit_ogre.launch\nPlay the sequence:\nrosbag play cheezit_test.bag\n3. Setup Guide\n3.1 Model Creation\nCreate a textured 3D model of the object you wish to track, preferably in DAE or OBJ format. This model can be converted into an OGRE .mesh file using blender and the blender2ogre exporter which can be downloaded from https://blender2ogre.googlecode.com/files/blender2ogre-0.6.0.zip. The path of the object must be added to ogre_cfg/resources.cfg so that the package can find the model.\n3.2 Descriptor/Pose Database Creation\nMake sure the path of the object has been added to cfg/resources.cfg in the object_renderer parent folder. Use the render_views tool in the object_renderer package to extract descriptors from images of the object rendered from known poses. This is necessary to initialize the object tracker. This can be done with the command\nrender_views <output_dir> <model_file> <radius> <num_samples>\nThis will render num_samples views of the object taken at random points lying on a sphere with the specified radius with the camera pointing at the origin. The output will be saved to the path given as output_dir.\n3.3 Calibration\nCalibrate your camera using ROS camera calibration.\n3.4 Tracking Modes\nThere are three tracking modes: PNP, KLT, and EDGE. All initialize using the descriptor/pose database.\nPNP mode extracts features from an input image and from a virtual image of the object rendered from its last known pose. The features are matched to give a set of 2D-3D correspondences between the model and the input image. A PnP problem is solved to give the object pose in the frame of the camera.\nKLT mode first obtains a set of 2D-3D correspondences using PNP mode, then tracks the 2D keypoints using a KLT tracker. A PnP problem is solved to give the object pose. Correspondences are re-matched when the tracking diverges or when the re-projection error rises above a given threshold.\nEDGE mode performs edge-based object tracking and is suitable for objects with little texture. A Canny edge detecttor is used on both a virtual view of the model and an input image. Edge points are matched form the model to the input image by performing a 1D search in the gradient direction of the edge. The distance between matched edges is minimized to estimate the objects pose.\n4. Topics\n4.1 Published\n/mesh_localize/image [sensor_msgs::Image] Rectified version of the input image on which tracking is performed\n/mesh_localize/camera_info [sensor_msgs::CameraInfo] Camera info corresonding to resized and cropped input image.\n/mesh_localize/depth [sensor_msgs::Image] Depth map corresponding to /mesh_localize/image\n/mesh_localize/estimated_pose [geometry_msgs::PoseStamped] Pose of the object in the frame of the camera\n4.2 Subscribed\n/image [sensor_msgs::Image] Unrectified input image on which tracking will be performed\n/camera_info [sensor_msgs::CameraInfo] Camera info of input image\n5. Parameters\nTODO\nTroubleshooting\nUsing with ROS Kinetic\nBuild vision_opencv from source using the indigo branch. Make sure it builds using OpenCV 2.4 instead of the OpenCV3 that comes packaged with Kinetic.\nCuda Linking Error\nIf you receive the error\n/usr/bin/ld: cannot find -lopencv_dep_cudart\nThen set the following cmake variable\n-DCUDA_USE_STATIC_CUDA_RUNTIME=OFF", "link": "https://github.com/msheckells/mesh_localize", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "meshlocalize: model-based object tracking in 3d for ros\ntracking video: https://www.youtube.com/watch?v=uqznlcw1u7s\ngrasping demo: https://www.youtube.com/watch?v=t953welroqg\n1. installation\n1.1 dependencies\nthis has only been tested with ubuntu 14.04 + ros indigo.\ninstall eigen3.\ninstall opencv 2.4.11. source can be downloaded from https://github.com/itseez/opencv/archive/2.4.11.zip\ninstall ois (needed for ogre).\nsudo apt-get install libois-dev\nsudo apt-get install libois-1.3.0\nor source can be downloaded from http://sourceforge.net/projects/wgois/files/\ninstall nvidia cg toolkit\nsudo apt-get install nvidia-cg-toolkit\ninstall ogre 1.8.1. you can download the source from http://sourceforge.net/projects/ogre/files/ogre/1.8/1.8.1/ogre_src_v1-8-1.tar.bz2/download\ninstall toon from http://www.edwardrosten.com/cvd/toon.html\ninstall the object_renderer library. it is a small wrapper around ogre. it is used to generate the virtual views of the object that the package uses to initialize the pose.\ngit clone https://github.com/msheckells/object_renderer\nbuild using cmake+make.\n1.2 package install\nfrom your catkin source directory:\ngit clone https://github.com/msheckells/mesh_localize.git\ncall catkin_make as usual\n2. quick start\ndownload a test rosbag which contains image and camera_info data from http://www.mattsheckells.com/wp-content/uploads/2015/cheezit_test.bag.zip and extract it.\nlaunch the node:\nroslaunch mesh_localize localize_cheezit_ogre.launch\nplay the sequence:\nrosbag play cheezit_test.bag\n3. setup guide\n3.1 model creation\ncreate a textured 3d model of the object you wish to track, preferably in dae or obj format. this model can be converted into an ogre .mesh file using blender and the blender2ogre exporter which can be downloaded from https://blender2ogre.googlecode.com/files/blender2ogre-0.6.0.zip. the path of the object must be added to ogre_cfg/resources.cfg so that the package can find the model.\n3.2 descriptor/pose database creation\nmake sure the path of the object has been added to cfg/resources.cfg in the object_renderer parent folder. use the render_views -----> tool !!!  in the object_renderer package to extract descriptors from images of the object rendered from known poses. this is necessary to initialize the object tracker. this can be done with the command\nrender_views <output_dir> <model_file> <radius> <num_samples>\nthis will render num_samples views of the object taken at random points lying on a sphere with the specified radius with the camera pointing at the origin. the output will be saved to the path given as output_dir.\n3.3 calibration\ncalibrate your camera using ros camera calibration.\n3.4 tracking modes\nthere are three tracking modes: pnp, klt, and edge. all initialize using the descriptor/pose database.\npnp mode extracts features from an input image and from a virtual image of the object rendered from its last known pose. the features are matched to give a set of 2d-3d correspondences between the model and the input image. a pnp problem is solved to give the object pose in the frame of the camera.\nklt mode first obtains a set of 2d-3d correspondences using pnp mode, then tracks the 2d keypoints using a klt tracker. a pnp problem is solved to give the object pose. correspondences are re-matched when the tracking diverges or when the re-projection error rises above a given threshold.\nedge mode performs edge-based object tracking and is suitable for objects with little texture. a canny edge detecttor is used on both a virtual view of the model and an input image. edge points are matched form the model to the input image by performing a 1d search in the gradient direction of the edge. the distance between matched edges is minimized to estimate the objects pose.\n4. topics\n4.1 published\n/mesh_localize/image [sensor_msgs::image] rectified version of the input image on which tracking is performed\n/mesh_localize/camera_info [sensor_msgs::camerainfo] camera info corresonding to resized and cropped input image.\n/mesh_localize/depth [sensor_msgs::image] depth map corresponding to /mesh_localize/image\n/mesh_localize/estimated_pose [geometry_msgs::posestamped] pose of the object in the frame of the camera\n4.2 subscribed\n/image [sensor_msgs::image] unrectified input image on which tracking will be performed\n/camera_info [sensor_msgs::camerainfo] camera info of input image\n5. parameters\ntodo\ntroubleshooting\nusing with ros kinetic\nbuild vision_opencv from source using the indigo branch. make sure it builds using opencv 2.4 instead of the opencv3 that comes packaged with kinetic.\ncuda linking error\nif you receive the error\n/usr/bin/ld: cannot find -lopencv_dep_cudart\nthen set the following cmake variable\n-dcuda_use_static_cuda_runtime=off", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000566, "year": null}, {"Unnamed: 0": 1589, "autor": 569, "date": null, "content": "PILZ robot manipulator module PRBT 6 in ROS\nPackage: pilz_robots\nThe meta package for the PILZ manipulator PRBT 6. Here you can find documentation of the individual packages. For a general overview and link collection we refer to the wiki page.\nInstallation\nTo use the packages, you can install prebuilt packages with\nsudo apt install ros-$ROS_DISTRO-pilz-robots\nBuild Status\nKinetic Melodic Noetic\nTravis/Github CI\nBuildfarm src\nBuildfarm bin\nBranching model\nnoetic-devel is considered to be the active development branch. Relevant changes are cherry-picked into melodic-devel or kinetic-devel on a case-by-case basis.\nSupported hardware versions\nPRBT firmware version 1.1.0\nIf you want to update your PRBT firmware please contact our repair center.\nPackage: prbt_support\nThe package contains the robot description of the PRBT manipulator.\nurdf/ contains the xacros for generating the urdf descriptions of the PILZ robot PRBT.\nmeshes/ contains the stl files for visualization\ntest/ contains test files for urdf\nbuild tests: catkin_make tests\nbuild and run tests: catkin_make run_tests\nconfig/ defines the controllers and drivers. Loads the specialized PilzTrajectoryController.\nPilz Coordinate Frames\nTo see the robot in rviz you can use roslaunch prbt_support test_urdf.launch\nThe joint directions are illustrated in the following image:\nConfigure the tcp\nYou can easily adjust the tool center point frame with an offset and rotation in the xacro file.\nOpen prbt_support/urdf/prbt.xacro\nEdit the lines to your desired offset\n<xacro:unless value=\"$(arg gripper)\">\n<xacro:arg name=\"tcp_offset_xyz\" default=\"0 0 0\"/>\n<xacro:arg name=\"tcp_offset_rpy\" default=\"0 0 0\"/>\n</xacro:unless>\nNote: You can set a different default if you have a gripper attached.\nPackage: prbt_moveit_config\nThe package is generated by moveit setup assistant. It contains configuration files and launch files needed to start up the robot including planning and execution.\nConfiguring the robot\nUse the launch file moveit_planning_execution.launch to bring up the robot controllers with the complete moveit pipeline. The launch file allows to set optional parameters\nsim (default: True)\ntrue: Use fake execution and display emulated robot position in RViz\nfalse: connect to real robot using ros_canopen\npipeline (default: ompl)\nPlanning pipeline to use with moveit\nload_robot_description (default: True)\nLoad robot description to parameter server. Can be set to false to let someone else load the model\nrviz_config (default: prbt_moveit_config/launch/moveit.rviz)\nStart RViz with default configuration settings. Once you have changed the configuration and have saved it inside your package folder, set the path and file name here.\ngripper (default: None)\nSee Running the prbt with a gripper\nsafety_hw (default: pss4000)\nConnect to the safety controller that handles the safe-torque-off signal. Only relevant for sim:=False to issue a Safe stop 1. See prbt_hardware_support package.\nRunning the simulation\nRun roslaunch prbt_moveit_config moveit_planning_execution.launch sim:=true pipeline:=ompl\nUse the moveit Motion Planning rviz plugin to plan and execute (see e.g. ROS-I training exercise 3.4)\nRunning on the real robot\nActivate can interface: sudo ip link set can0 up type can bitrate 1000000 (after every reboot or reconnect of the CAN hardware). For persistent configuration append the following to the file /etc/network/interfaces\nauto can0\niface can0 can static\nbitrate 1000000\nRun roslaunch prbt_moveit_config moveit_planning_execution.launch sim:=false pipeline:=ompl\nUse the moveit Motion Planning rviz plugin to plan and execute (see simulation section; set Velocity Scaling to 0.1 first)\nInstead of OMPL use the industrial motion planners of Pilz for executing industrial robot commands like PTP, LIN, etc. For this install the package pilz_industrial_motion_planner:\nsudo apt install ros-kinetic-pilz-trajectory-generation\nor\nsudo apt install ros-melodic-pilz-trajectory-generation\nor\nsudo apt install ros-noetic-pilz-industrial-motion-planner\nthen replace the pipeline in the above command by pipeline:=pilz_industrial_motion_planner.\nAdjust expert parameters\nIf you've created an application package with your own launch file as described in the tutorials, you can easily adjust many other configuration parameters. See the comments in the pilz_tutorials package and templates in the pilz_templates repo.\nRunning the prbt with a gripper\nCurrently only the Schunk pg70 is supported. To run it, first install the package:\nsudo apt install ros-$ROS_DISTRO-prbt-pg70-support\nthen start the robot like before but with the gripper:=pg70 set. Both simulation and real robot work.\nPackage: prbt_ikfast_manipulator_plugin\nThe package contains a moveit plugin for inverse kinematics of the manipulator, which is a wrapper of ikfast.cpp to the kinematics base interface of moveit.\nPackage: pilz_control\nContains a specialized version of ros_controllers::JointTrajectoryController which can be put into a holding mode. A controlled stop using a hold trajectory is performed thus stopping the manipulator without the mechanical stress of a hard brake. Further, the controller monitors cartesian speed and joint accelerations to fulfill the requirements of the selected operation mode.\nTopic interface deprecated: See here\nPackage: prbt_hardware_support\nThis package provides support for the Pilz hardware PNOZmulti and PSS4000. A configurable modbus connection is set up via roslaunch prbt_hardware_support modbus_client.launch. Particular features (detailed description here):\nStop1 functionality\nSafe Brake Control functionality\nOperation modes T1 (reduced speed) and AUTOMATIC\nPackage: prbt_gazebo\nProvides a launch file to run the prbt manipulator inside gazebo.\nPackage: pilz_status_indicator_rqt\nThis package defines an rqt-plugin to display information about the robots state to the user. It will show:\nCurrent operation mode\nOperation state of the robot (PRBT)\nOperation state of the ROS system\nCurrent speed override\nIt can be launched with rosrun pilz_status_indicator_rqt pilz_status_indicator_rqt in a floating, standalone rqt instance. Alternatively it can be integrated in an existing rqt environment next to other widgets, if required.\nYou need further information?\nOur international hotline staff will support you individually about our ROS packages at ros@pilz.de\nFind more information about the Pilz manipulator module on the product website.\nVisit us at pilz.com\nPilz is an international-scale, innovative automation technology company. Pilz uses its solutions to create safety for man, machine and the environment. In addition to head office in Ostfildern near Stuttgart, the family business is represented over 2,400 employees at 42 subsidiaries and branches on all continents.\nThe company\u2019s products include sensor technology, electronic monitoring relays, safety relays, configurable and programmable control systems, automation solutions with motion control, systems for industrial communication as well as visualization solutions and operator terminals.\nPilz solutions can be used in all areas of mechanical engineering, including the packaging and automotive sector, plus the railway technology, press and wind energy sectors. These solutions ensure that baggage handling systems run safely at airports and funiculars or roller coasters travel safely; they also guarantee fire protection and energy supply in buildings.", "link": "https://github.com/PilzDE/pilz_robots", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "pilz robot manipulator module prbt 6 in ros\npackage: pilz_robots\nthe meta package for the pilz manipulator prbt 6. here you can find documentation of the individual packages. for a general overview and link collection we refer to the wiki page.\ninstallation\nto use the packages, you can install prebuilt packages with\nsudo apt install ros-$ros_distro-pilz-robots\nbuild status\nkinetic melodic noetic\ntravis/github ci\nbuildfarm src\nbuildfarm bin\nbranching model\nnoetic-devel is considered to be the active development branch. relevant changes are cherry-picked into melodic-devel or kinetic-devel on a case-by-case basis.\nsupported hardware versions\nprbt firmware version 1.1.0\nif you want to update your prbt firmware please contact our repair center.\npackage: prbt_support\nthe package contains the robot description of the prbt manipulator.\nurdf/ contains the xacros for generating the urdf descriptions of the pilz robot prbt.\nmeshes/ contains the stl files for visualization\ntest/ contains test files for urdf\nbuild tests: catkin_make tests\nbuild and run tests: catkin_make run_tests\nconfig/ defines the controllers and drivers. loads the specialized pilztrajectorycontroller.\npilz coordinate frames\nto see the robot in rviz you can use roslaunch prbt_support test_urdf.launch\nthe joint directions are illustrated in the following image:\nconfigure the tcp\nyou can easily adjust the -----> tool !!!  center point frame with an offset and rotation in the xacro file.\nopen prbt_support/urdf/prbt.xacro\nedit the lines to your desired offset\n<xacro:unless value=\"$(arg gripper)\">\n<xacro:arg name=\"tcp_offset_xyz\" default=\"0 0 0\"/>\n<xacro:arg name=\"tcp_offset_rpy\" default=\"0 0 0\"/>\n</xacro:unless>\nnote: you can set a different default if you have a gripper attached.\npackage: prbt_moveit_config\nthe package is generated by moveit setup assistant. it contains configuration files and launch files needed to start up the robot including planning and execution.\nconfiguring the robot\nuse the launch file moveit_planning_execution.launch to bring up the robot controllers with the complete moveit pipeline. the launch file allows to set optional parameters\nsim (default: true)\ntrue: use fake execution and display emulated robot position in rviz\nfalse: connect to real robot using ros_canopen\npipeline (default: ompl)\nplanning pipeline to use with moveit\nload_robot_description (default: true)\nload robot description to parameter server. can be set to false to let someone else load the model\nrviz_config (default: prbt_moveit_config/launch/moveit.rviz)\nstart rviz with default configuration settings. once you have changed the configuration and have saved it inside your package folder, set the path and file name here.\ngripper (default: none)\nsee running the prbt with a gripper\nsafety_hw (default: pss4000)\nconnect to the safety controller that handles the safe-torque-off signal. only relevant for sim:=false to issue a safe stop 1. see prbt_hardware_support package.\nrunning the simulation\nrun roslaunch prbt_moveit_config moveit_planning_execution.launch sim:=true pipeline:=ompl\nuse the moveit motion planning rviz plugin to plan and execute (see e.g. ros-i training exercise 3.4)\nrunning on the real robot\nactivate can interface: sudo ip link set can0 up type can bitrate 1000000 (after every reboot or reconnect of the can hardware). for persistent configuration append the following to the file /etc/network/interfaces\nauto can0\niface can0 can static\nbitrate 1000000\nrun roslaunch prbt_moveit_config moveit_planning_execution.launch sim:=false pipeline:=ompl\nuse the moveit motion planning rviz plugin to plan and execute (see simulation section; set velocity scaling to 0.1 first)\ninstead of ompl use the industrial motion planners of pilz for executing industrial robot commands like ptp, lin, etc. for this install the package pilz_industrial_motion_planner:\nsudo apt install ros-kinetic-pilz-trajectory-generation\nor\nsudo apt install ros-melodic-pilz-trajectory-generation\nor\nsudo apt install ros-noetic-pilz-industrial-motion-planner\nthen replace the pipeline in the above command by pipeline:=pilz_industrial_motion_planner.\nadjust expert parameters\nif you've created an application package with your own launch file as described in the tutorials, you can easily adjust many other configuration parameters. see the comments in the pilz_tutorials package and templates in the pilz_templates repo.\nrunning the prbt with a gripper\ncurrently only the schunk pg70 is supported. to run it, first install the package:\nsudo apt install ros-$ros_distro-prbt-pg70-support\nthen start the robot like before but with the gripper:=pg70 set. both simulation and real robot work.\npackage: prbt_ikfast_manipulator_plugin\nthe package contains a moveit plugin for inverse kinematics of the manipulator, which is a wrapper of ikfast.cpp to the kinematics base interface of moveit.\npackage: pilz_control\ncontains a specialized version of ros_controllers::jointtrajectorycontroller which can be put into a holding mode. a controlled stop using a hold trajectory is performed thus stopping the manipulator without the mechanical stress of a hard brake. further, the controller monitors cartesian speed and joint accelerations to fulfill the requirements of the selected operation mode.\ntopic interface deprecated: see here\npackage: prbt_hardware_support\nthis package provides support for the pilz hardware pnozmulti and pss4000. a configurable modbus connection is set up via roslaunch prbt_hardware_support modbus_client.launch. particular features (detailed description here):\nstop1 functionality\nsafe brake control functionality\noperation modes t1 (reduced speed) and automatic\npackage: prbt_gazebo\nprovides a launch file to run the prbt manipulator inside gazebo.\npackage: pilz_status_indicator_rqt\nthis package defines an rqt-plugin to display information about the robots state to the user. it will show:\ncurrent operation mode\noperation state of the robot (prbt)\noperation state of the ros system\ncurrent speed override\nit can be launched with rosrun pilz_status_indicator_rqt pilz_status_indicator_rqt in a floating, standalone rqt instance. alternatively it can be integrated in an existing rqt environment next to other widgets, if required.\nyou need further information?\nour international hotline staff will support you individually about our ros packages at ros@pilz.de\nfind more information about the pilz manipulator module on the product website.\nvisit us at pilz.com\npilz is an international-scale, innovative automation technology company. pilz uses its solutions to create safety for man, machine and the environment. in addition to head office in ostfildern near stuttgart, the family business is represented over 2,400 employees at 42 subsidiaries and branches on all continents.\nthe company\u2019s products include sensor technology, electronic monitoring relays, safety relays, configurable and programmable control systems, automation solutions with motion control, systems for industrial communication as well as visualization solutions and operator terminals.\npilz solutions can be used in all areas of mechanical engineering, including the packaging and automotive sector, plus the railway technology, press and wind energy sectors. these solutions ensure that baggage handling systems run safely at airports and funiculars or roller coasters travel safely; they also guarantee fire protection and energy supply in buildings.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000569, "year": null}, {"Unnamed: 0": 1592, "autor": 572, "date": null, "content": "Joint Perception and Planning For Efficient Obstacle Avoidance Using Stereo Vision\nThis repository contains a C++ implementation of JPP for local obstacle avoidance using stereo cameras. A ROS wrapper for JPP is also included in the ROS/ folder. JPP uses disparity confidence checks on-demand instead of the traditional dense 3D reconstruction approach while performing obstacle avoidance during navigation planning, thus significantly saving computational cost.\nAuthor: Sourish Ghosh\nPublication\nIf you use this software in an academic work or find it relevant to your research, kindly cite:\n@inproceedings{ghosh2017joint,\ndoi = { 10.1109/IROS.2017.8202271 },\nurl = { https://www.joydeepb.com/Publications/jpp.pdf },\npages = { 1026--1031 },\norganization = { IEEE },\nyear = { 2017 },\nbooktitle = { Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on },\nauthor = { Sourish Ghosh and Joydeep Biswas },\ntitle = { Joint Perception And Planning For Efficient Obstacle Avoidance Using Stereo Vision },\n}\nLink to paper: https://www.joydeepb.com/Publications/jpp.pdf\nDependencies\nA C++ compiler (e.g., GCC)\ncmake\npopt\nlibconfig\nBoost\nOpenCV\nOpenMP\nUse the following command to install dependencies:\n$ sudo apt-get install g++ cmake libpopt-dev libconfig-dev libboost-all-dev libopencv-dev python-opencv gcc-multilib\nFor compiling and running the ROS wrapper, install ROS Indigo.\nCompiling\n1. Building JPP libraries and binaries\nClone the repository:\n$ git clone https://github.com/umass-amrl/jpp\nThe script build.sh compiles the JPP library:\n$ cd jpp\n$ chmod +x build.sh\n$ ./build.sh\n2. Building JPP ROS\nFor compiling the ROS wrapper, rosbuild is used. Add the path of the ROS wrapper to ROS_PACKAGE_PATH and put the following line in your .bashrc file. Replace PATH by the actual path where you have cloned the repository:\n$ export ROS_PACKAGE_PATH=$ROS_PACKAGE_PATH:/PATH/jpp/ROS\nExecute the build_ros.sh script:\n$ chmod +x build_ros.sh\n$ ./build_ros.sh\nRunning JPP on AMRL and KITTI Datasets\n1. Download Datasets\nThe complete example data (AMRL and KITTI) along with calibration files can be found here.\n2. Running JPP\nAfter compilation, the jpp binary file is store inside the bin/ folder. For processing a single pair of stereo images, use:\n$ ./bin/jpp -l [path/to/left/image] -r [path/to/right/image] -c [path/to/stereo/calibration/file] -j [path/to/jpp/config/file] -o [output_mode]\nFor processing multiple stereo pairs stored in a directory, use:\n$ ./bin/jpp -n [number of pairs] -d [path/to/directory] -c [path/to/stereo/calibration/file] -j [path/to/jpp/config/file] -o [output_mode]\nNote: stereo image pairs inside the directory must be named like this: left1.jpg, left2.jpg, ... , right1.jpg, right2.jpg, ...\nFor the example datasets, calibration files are stored in the calibration/ folder and JPP configurations are stored in the cfg/ folder. JPP operates on 3 output modes (set by the -o flag) as of now: astar, rrt, and debug mode. Set the flag -v 1 for generating visualizations.\nUsage: jpp [OPTION...]\n-n, --num_imgs=NUM Number of images to be processed\n-d, --img_dir=STR Directory containing image pairs (set if n > 0)\n-l, --left_img=STR Left image file name\n-r, --right_img=STR Right image file name\n-c, --calib_file=STR Stereo calibration file name\n-j, --jpp_config_file=STR JPP config file name\n-o, --output=STR Output - astar, rrt, debug\n-v, --visualize=NUM Set v=1 for displaying visualizations\n-w, --write_files=NUM Set w=1 for writing visualizations to files\nFor example, running JPP on the KITTI dataset in astar mode:\n$ ./bin/jpp -n 33 -d KITTI/ -c calibration/kitti_2011_09_26.yml -j cfg/kitti.cfg -o astar -v 1\nConfidence match visualizations Path visualization\nRunning JPP on the AMRL dataset in rrt mode:\n$ ./bin/jpp -n 158 -d AMRL/ -c calibration/amrl_jackal_webcam_stereo.yml -j cfg/amrl.cfg -o rrt -v 1\nConfidence match visualizations Path visualization\nNote: Press any key to move on to the next image pair.\n3. Running JPP ROS\nRun the ROS node navigation:\n$ rosrun jpp navigation -l [left/image/topic] -r [right/image/topic] -c [path/to/stereo/calibration/file] -j [path/to/jpp/config/file] -o [output_mode]\nThe same flags for displaying/writing visualizations can be used for the ROS node as well.\nUsage: navigation [OPTION...]\n-l, --left_topic=STR Left image topic name\n-r, --right_topic=STR Right image topic name\n-c, --calib_file=STR Stereo calibration file name\n-j, --jpp_config_file=STR JPP config file name\n-o, --output=STR Output - astar, rrt, debug\n-v, --visualize=NUM Set v=1 for displaying visualizations\n-w, --write_files=NUM Set w=1 for writing visualizations to files\n-d, --dynamic_reconfigure=NUM Set d=1 for enabling dynamic reconfigure\nJPP configuration parameters can be changed realtime by using rqt_reconfigure:\n$ rosrun rqt_reconfigure rqt_reconfigure\nMake sure you set the flag -d 1 while using dynamic reconfigure.\nRunning JPP on your Datasets\n1. Stereo Calibration\nTo run JPP on your own data, you need to have a pair of calibrated stereo cameras. For stereo calibration it is recommended to use this tool. The XR and XT matrices in the calibration file are the transformation matrices from the left camera reference frame to the robot reference frame. These matrices depends on how the stereo camera is mounted on the robot. Initially after stereo calibration (using the tool mentioned) you will not have the XR and XT matrices in your calibration file. You need to manually calibrate them and add them to the calibration file. Also, you only need the following matrices in your calibration file: K1, K2, D1, D2, R, T, XR, and XT. An example calibration file can be found inside the calibration/ folder.\nIf you cannot calibrate for XR and XT then just set them to the identity and zero matrices respectively. Then use this stereo dense reconstruction tool to visualize how the point cloud looks in the robot reference frame and visually align the ground plane with z=0.\n2. Running JPP\nJPP can be run in the same way as explained for the exmaple AMRL and KITTI datasets.\nLicense\nThis software is released under the MIT license.", "link": "https://github.com/ut-amrl/jpp", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "joint perception and planning for efficient obstacle avoidance using stereo vision\nthis repository contains a c++ implementation of jpp for local obstacle avoidance using stereo cameras. a ros wrapper for jpp is also included in the ros/ folder. jpp uses disparity confidence checks on-demand instead of the traditional dense 3d reconstruction approach while performing obstacle avoidance during navigation planning, thus significantly saving computational cost.\nauthor: sourish ghosh\npublication\nif you use this software in an academic work or find it relevant to your research, kindly cite:\n@inproceedings{ghosh2017joint,\ndoi = { 10.1109/iros.2017.8202271 },\nurl = { https://www.joydeepb.com/publications/jpp.pdf },\npages = { 1026--1031 },\norganization = { ieee },\nyear = { 2017 },\nbooktitle = { intelligent robots and systems (iros), 2017 ieee/rsj international conference on },\nauthor = { sourish ghosh and joydeep biswas },\ntitle = { joint perception and planning for efficient obstacle avoidance using stereo vision },\n}\nlink to paper: https://www.joydeepb.com/publications/jpp.pdf\ndependencies\na c++ compiler (e.g., gcc)\ncmake\npopt\nlibconfig\nboost\nopencv\nopenmp\nuse the following command to install dependencies:\n$ sudo apt-get install g++ cmake libpopt-dev libconfig-dev libboost-all-dev libopencv-dev python-opencv gcc-multilib\nfor compiling and running the ros wrapper, install ros indigo.\ncompiling\n1. building jpp libraries and binaries\nclone the repository:\n$ git clone https://github.com/umass-amrl/jpp\nthe script build.sh compiles the jpp library:\n$ cd jpp\n$ chmod +x build.sh\n$ ./build.sh\n2. building jpp ros\nfor compiling the ros wrapper, rosbuild is used. add the path of the ros wrapper to ros_package_path and put the following line in your .bashrc file. replace path by the actual path where you have cloned the repository:\n$ export ros_package_path=$ros_package_path:/path/jpp/ros\nexecute the build_ros.sh script:\n$ chmod +x build_ros.sh\n$ ./build_ros.sh\nrunning jpp on amrl and kitti datasets\n1. download datasets\nthe complete example data (amrl and kitti) along with calibration files can be found here.\n2. running jpp\nafter compilation, the jpp binary file is store inside the bin/ folder. for processing a single pair of stereo images, use:\n$ ./bin/jpp -l [path/to/left/image] -r [path/to/right/image] -c [path/to/stereo/calibration/file] -j [path/to/jpp/config/file] -o [output_mode]\nfor processing multiple stereo pairs stored in a directory, use:\n$ ./bin/jpp -n [number of pairs] -d [path/to/directory] -c [path/to/stereo/calibration/file] -j [path/to/jpp/config/file] -o [output_mode]\nnote: stereo image pairs inside the directory must be named like this: left1.jpg, left2.jpg, ... , right1.jpg, right2.jpg, ...\nfor the example datasets, calibration files are stored in the calibration/ folder and jpp configurations are stored in the cfg/ folder. jpp operates on 3 output modes (set by the -o flag) as of now: astar, rrt, and debug mode. set the flag -v 1 for generating visualizations.\nusage: jpp [option...]\n-n, --num_imgs=num number of images to be processed\n-d, --img_dir=str directory containing image pairs (set if n > 0)\n-l, --left_img=str left image file name\n-r, --right_img=str right image file name\n-c, --calib_file=str stereo calibration file name\n-j, --jpp_config_file=str jpp config file name\n-o, --output=str output - astar, rrt, debug\n-v, --visualize=num set v=1 for displaying visualizations\n-w, --write_files=num set w=1 for writing visualizations to files\nfor example, running jpp on the kitti dataset in astar mode:\n$ ./bin/jpp -n 33 -d kitti/ -c calibration/kitti_2011_09_26.yml -j cfg/kitti.cfg -o astar -v 1\nconfidence match visualizations path visualization\nrunning jpp on the amrl dataset in rrt mode:\n$ ./bin/jpp -n 158 -d amrl/ -c calibration/amrl_jackal_webcam_stereo.yml -j cfg/amrl.cfg -o rrt -v 1\nconfidence match visualizations path visualization\nnote: press any key to move on to the next image pair.\n3. running jpp ros\nrun the ros node navigation:\n$ rosrun jpp navigation -l [left/image/topic] -r [right/image/topic] -c [path/to/stereo/calibration/file] -j [path/to/jpp/config/file] -o [output_mode]\nthe same flags for displaying/writing visualizations can be used for the ros node as well.\nusage: navigation [option...]\n-l, --left_topic=str left image topic name\n-r, --right_topic=str right image topic name\n-c, --calib_file=str stereo calibration file name\n-j, --jpp_config_file=str jpp config file name\n-o, --output=str output - astar, rrt, debug\n-v, --visualize=num set v=1 for displaying visualizations\n-w, --write_files=num set w=1 for writing visualizations to files\n-d, --dynamic_reconfigure=num set d=1 for enabling dynamic reconfigure\njpp configuration parameters can be changed realtime by using rqt_reconfigure:\n$ rosrun rqt_reconfigure rqt_reconfigure\nmake sure you set the flag -d 1 while using dynamic reconfigure.\nrunning jpp on your datasets\n1. stereo calibration\nto run jpp on your own data, you need to have a pair of calibrated stereo cameras. for stereo calibration it is recommended to use this -----> tool !!! . the xr and xt matrices in the calibration file are the transformation matrices from the left camera reference frame to the robot reference frame. these matrices depends on how the stereo camera is mounted on the robot. initially after stereo calibration (using the tool mentioned) you will not have the xr and xt matrices in your calibration file. you need to manually calibrate them and add them to the calibration file. also, you only need the following matrices in your calibration file: k1, k2, d1, d2, r, t, xr, and xt. an example calibration file can be found inside the calibration/ folder.\nif you cannot calibrate for xr and xt then just set them to the identity and zero matrices respectively. then use this stereo dense reconstruction tool to visualize how the point cloud looks in the robot reference frame and visually align the ground plane with z=0.\n2. running jpp\njpp can be run in the same way as explained for the exmaple amrl and kitti datasets.\nlicense\nthis software is released under the mit license.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000572, "year": null}, {"Unnamed: 0": 1595, "autor": 575, "date": null, "content": "ros2-code-examples\nThe following tutorials and examples are updated for ROS 2 Dashing Diademata (Released in May 2019).\nThis repository also contains some Dockerfile for running different ROS2-based environments. You can find them here.\nRequirements\nROS2 Foxy\nNote: you can still use ROS2 Bouncy Bolson, but you will not be able to compile the simple_actions package or to reproduce the examples of simple_navigation, simple_bag, simple_gazebo and simple_rqt.\nBuilding the examples\nSource your ROS2 SDK, then create a workspace, add this repository to its sources and build the packages.\n$ source <PATH_TO_ROS2_SDK_WS>/install/setup.sh\n$ mkdir -p ws/src\n$ cd ws/src\n$ git clone https://github.com/alsora/ros2-code-examples\n$ cd ..\n$ colcon build\n$ source install/local_setup.sh\nFor more detailed build instructions look here\nRunning the examples\nFor each of the following tests, run the each command in a separate terminal window (remember that the ROS2 SDK has to be sourced in every terminal window!)\nPublisher/Subscriber\n$ ros2 run simple_publisher publisher_main\n$ ros2 run simple_subscriber subscriber_main\nOne process publishes messages and the other subscribes to that topic.\nService/Client\n$ ros2 run simple_service service_main\n$ ros2 run simple_client client_main\nOne process provides a service and the other requests it.\nRunning simple_client/timer_main contains an alternative implementation for the ROS2 client node.\nActions Service/Client\n$ ros2 run simple_action service_main\n$ ros2 run simple_action client_main\nOne process provides an action service and the other requests it. The server node can reject the goal according to the request parameters. The client node can cancel the action if it's taking too much time.\nLogger utils\n$ ros2 run simple_logger logger_main\nLog some messages and change the verbosity level at runtime.\nMultithread systems\n$ ros2 run simple_multithread multithread_main\nRun multiple nodes in separate threads from the same process.\nTime Synchronization message filters\nNOTE This example requires\n$ ros2 run simple_time_sync publisher\n$ ros2 run simple_time_sync time_sync_main\nOne process publishes messages on multiple topics. Sometimes it publishes only on one topic, sometimes on all with the same timestamp and sometimes on all but with slightly different timestamps. The other process create an approximate and an exact time subscribers.\nParameter servers\n$ ros2 run simple_parameter parameter_server_main\n$ ros2 run simple_parameter parameter_client_main\nOne process sets its own parameters. The other reads them.\nROS2 Navigation stack\nREADME\nAutonomously move a mobile robot in the environment, using a real system or a simulation.\nrosbag2\nREADME\nRecord messages from topic to a file and playback them whenever you want.\nrqt2\nREADME\nGUI for inspection and interaction of a ROS2 graph.\ngazebo\nREADME\nROS2 integration of the 3D simulation and visualization tool Gazebo.\nBackward compatibility\nREADME\nAllow nodes compiled against different versions of a message interface to communicate.\nSecure ROS2\nREADME\nEnable security options for your ROS2 system: nodes authentication, messages encryption and access control.\nROS2 CLI (command line interface)\nNote that these commands comes from a Python package. So if you have disabled Python support (i.e. when cross-compiling) they will not be available.\nSee a list of available commands\n$ ros2 --help\nPrint a list of <package_name> <executable_name>\n$ ros2 pkg executabels\nRun a ROS2 node\n$ ros2 run <package_name> <executable_name>\nList running ROS2 nodes\n$ ros2 node list\nList visible topic names (a topic is visible if at least 1 node is publishing or subscribing to it)\n$ ros2 topic list\nEcho what's published on a topic\n$ ros2 topic echo <topic_name>\nPublish a message to a topic (message_content written as valid YAML)\n$ ros2 topic pub <topic_name> <message_type> <message_content>\nMake a service request (request_content written as valid YAML)\n$ ros2 service call <service_name> <request_message_type> <request_content>", "link": "https://github.com/alsora/ros2-code-examples", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "ros2-code-examples\nthe following tutorials and examples are updated for ros 2 dashing diademata (released in may 2019).\nthis repository also contains some dockerfile for running different ros2-based environments. you can find them here.\nrequirements\nros2 foxy\nnote: you can still use ros2 bouncy bolson, but you will not be able to compile the simple_actions package or to reproduce the examples of simple_navigation, simple_bag, simple_gazebo and simple_rqt.\nbuilding the examples\nsource your ros2 sdk, then create a workspace, add this repository to its sources and build the packages.\n$ source <path_to_ros2_sdk_ws>/install/setup.sh\n$ mkdir -p ws/src\n$ cd ws/src\n$ git clone https://github.com/alsora/ros2-code-examples\n$ cd ..\n$ colcon build\n$ source install/local_setup.sh\nfor more detailed build instructions look here\nrunning the examples\nfor each of the following tests, run the each command in a separate terminal window (remember that the ros2 sdk has to be sourced in every terminal window!)\npublisher/subscriber\n$ ros2 run simple_publisher publisher_main\n$ ros2 run simple_subscriber subscriber_main\none process publishes messages and the other subscribes to that topic.\nservice/client\n$ ros2 run simple_service service_main\n$ ros2 run simple_client client_main\none process provides a service and the other requests it.\nrunning simple_client/timer_main contains an alternative implementation for the ros2 client node.\nactions service/client\n$ ros2 run simple_action service_main\n$ ros2 run simple_action client_main\none process provides an action service and the other requests it. the server node can reject the goal according to the request parameters. the client node can cancel the action if it's taking too much time.\nlogger utils\n$ ros2 run simple_logger logger_main\nlog some messages and change the verbosity level at runtime.\nmultithread systems\n$ ros2 run simple_multithread multithread_main\nrun multiple nodes in separate threads from the same process.\ntime synchronization message filters\nnote this example requires\n$ ros2 run simple_time_sync publisher\n$ ros2 run simple_time_sync time_sync_main\none process publishes messages on multiple topics. sometimes it publishes only on one topic, sometimes on all with the same timestamp and sometimes on all but with slightly different timestamps. the other process create an approximate and an exact time subscribers.\nparameter servers\n$ ros2 run simple_parameter parameter_server_main\n$ ros2 run simple_parameter parameter_client_main\none process sets its own parameters. the other reads them.\nros2 navigation stack\nreadme\nautonomously move a mobile robot in the environment, using a real system or a simulation.\nrosbag2\nreadme\nrecord messages from topic to a file and playback them whenever you want.\nrqt2\nreadme\ngui for inspection and interaction of a ros2 graph.\ngazebo\nreadme\nros2 integration of the 3d simulation and visualization -----> tool !!!  gazebo.\nbackward compatibility\nreadme\nallow nodes compiled against different versions of a message interface to communicate.\nsecure ros2\nreadme\nenable security options for your ros2 system: nodes authentication, messages encryption and access control.\nros2 cli (command line interface)\nnote that these commands comes from a python package. so if you have disabled python support (i.e. when cross-compiling) they will not be available.\nsee a list of available commands\n$ ros2 --help\nprint a list of <package_name> <executable_name>\n$ ros2 pkg executabels\nrun a ros2 node\n$ ros2 run <package_name> <executable_name>\nlist running ros2 nodes\n$ ros2 node list\nlist visible topic names (a topic is visible if at least 1 node is publishing or subscribing to it)\n$ ros2 topic list\necho what's published on a topic\n$ ros2 topic echo <topic_name>\npublish a message to a topic (message_content written as valid yaml)\n$ ros2 topic pub <topic_name> <message_type> <message_content>\nmake a service request (request_content written as valid yaml)\n$ ros2 service call <service_name> <request_message_type> <request_content>", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000575, "year": null}, {"Unnamed: 0": 1606, "autor": 586, "date": null, "content": "3D-Scene-Graph: A Sparse and Semantic Representation of Physical Environments for Intelligent Agents\nThis work is based on our paper (IEEE Transactions on Cybernetics 2019, accepted). We proposed a new concept called 3D scene graph and its construction framework. Our work is based on FactorizableNet, implemented in Pytorch.\n3D Scene Graph Construction Framework\nThe proposed 3D scene graph construction framework extracts relevant semantics within environments such as object categories and relations between objects as well as physical attributes such as 3D positions and major colors in the process of generating 3D scene graphs for the given environments. The framework receives a sequence of observations regarding the environments in the form of RGB-D image frames. For robust performance, the framework filters out unstable observations(i.e., blurry images) using the proposed adaptive blurry image detection algorithm. Then, the framework factors out keyframe groups to avoid redundant processing of the same information. Keyframe groups contain reasonably-overlapping frames. Next, the framework extracts semantics and physical attributes within the environments through recognition modules. During the recognition processes, spurious detections get rejected and missing entities are supplemented.\nFinally, the gathered information gets fused into 3D scene graph and the graph gets updated upon new observations.\nRequirements\nUbuntu 16.04+\nPython 2.7\nPytorch 0.3.1\ntorchtext 0.2.3\nFactorizableNet\nScanNet dataset (optional, used for test. an RGBD video from ScanNet is enough.)\nInstallation\nInstall Pytorch 0.3.1. The code has been tested only with Python 2.7, CUDA 9.0 on Ubuntu 16.04. You will need to modify a significant amount of code if you want to run in a different environment (Python 3+ or Pytorch 0.4+).\nDownload 3D-Scene-Graph repository\ngit clone --recurse-submodules https://github.com/Uehwan/3D-Scene-Graph.git\nInstall FactorizableNet\ncd 3D-Scene-Graph/FactorizableNet\nPlease follow the installation instructions in FactorizableNet repository. Follow steps 1 through 6. You can skip step 7. Download VG-DR-Net in step 8. You do not need to download other models.\nInstall 3D-Scene-Graph\ncd 3D-Scene-Graph\ntouch FactorizableNet/__init__.py\nln -s ./FactorizableNet/options/ options\nmkdir data\nln -s ./FactorizableNet/data/svg data/svg\nln -s ./FactorizableNet/data/visual_genome data/visual_genome\npip install torchtext==0.2.3\npip install setuptools pyyaml graphviz webcolors pandas matplotlib\npip install git+https://github.com/chickenbestlover/ColorHistogram.git\nAn Alternative: use installation script\n./build.sh\nDownload ScanNet dataset\nIn order to use ScanNet dataset, you need to fill out an agreement to toe ScanNet Terms of Use and send it to the ScanNet team at scannet@googlegroups.com. If the process was successful, they will send you a script downloading ScanNet dataset.\nTo download a specific scan (e.g. scene0000_00) using the script (the script only runs on Python 2.7):\ndownload-scannet.py -o [directory in which to download] --id scene0000_00\n(then press Enter twice)\nAfter the download is finished, the scan is located in a new folder scene0000_00. In the folder, *.sens file contains the RGBD Video with camera pose. To extract them, we use SensReader, an extraction tool provided by ScanNet git repo.\ngit clone https://github.com/ScanNet/ScanNet.git\ncd ScanNet/SensReader/python/\npython reader.py \\\n--filename [your .sens filepath] \\\n--output_path [ROOT of 3D-Scene-Graph]/data/scene0000_00/ \\\n--export_depth_images \\\n--export_color_images \\\n--export_poses \\\n--export_intrinsics\nExample of usage\npython scene_graph_tuning.py \\\n--scannet_path data/scene0000_00/\\\n--obj_thres 0.23\\\n--thres_key 0.2\\\n--thres_anchor 0.68 \\\n--visualize \\\n--frame_start 800 \\\n--plot_graph \\\n--disable_spurious \\\n--gain 10 \\\n--detect_cnt_thres 2 \\\n--triplet_thres 0.065\nCore hyper-parameters\nData settings:\n--dataset : choose dataset, default='scannet'.\n--scannet_path : scannet scan filepath , default='./data/scene0507/'.\n--frame_start : idx of frame to start , default=0.\n--frame_end : idx of frame to end , default=5000.\nFactorizableNet Output Filtering Settings:\n--obj_thres : object recognition threshold score , default=0.25.\n--triplet_thres : triplet recognition threshold score , default=0.08.\n--nms : NMS threshold for post object NMS (negative means not NMS) , default=0.2.\n--triplet_nms : Triplet NMS threshold for post object NMS (negative means not NMS) , default=0.4.\nKey-frame Extraction Settings:\n--thres_key : keyframe threshold score , default=0.1.\n--thres_anchor : achorframe threshold score , default=0.65.\n--alpha : weight for Exponentially Weighted Summation , default=0.4.\n--gain : gain for adaptive thresholding in blurry image detection , default=25.\n--offset : offset for adaptive thresholding in blurry image detection , default=1.\nVisualization Settings:\n--pause_time : a pause interval (sec) for every detection , default=1.\n--plot_graph : plot 3D Scene Graph if true.\n--visualize : enable visualization if ture.\n--format : resulting image format, pdf or png, default='png'.\n--draw_color : draw color node in 3D scene graph if true.\n--save_image : save detection result image if true.\nResult\nDemo Video\nCitations\nPlease consider citing this project in your publications if it helps your research. The following is a BibTeX reference.\n@article{kim2019graph3d,\ntitle={3D-Scene-Graph: A Sparse and Semantic Representation of Physical Environments for Intelligent Agents},\nauthor={Kim, Ue-Hwan and Park, Jin-Man and Song, Taek-Jin and Kim, Jong-Hwan},\njournal={IEEE Cybernetics},\nyear={2019}\n}\nAcknowledgement\nThis work was supported by the ICT R&D program of MSIP/IITP. [2016-0-00563, Research on Adaptive Machine Learning Technology Development for Intelligent Autonomous Digital Companion]", "link": "https://github.com/Uehwan/3-D-Scene-Graph", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "3d-scene-graph: a sparse and semantic representation of physical environments for intelligent agents\nthis work is based on our paper (ieee transactions on cybernetics 2019, accepted). we proposed a new concept called 3d scene graph and its construction framework. our work is based on factorizablenet, implemented in pytorch.\n3d scene graph construction framework\nthe proposed 3d scene graph construction framework extracts relevant semantics within environments such as object categories and relations between objects as well as physical attributes such as 3d positions and major colors in the process of generating 3d scene graphs for the given environments. the framework receives a sequence of observations regarding the environments in the form of rgb-d image frames. for robust performance, the framework filters out unstable observations(i.e., blurry images) using the proposed adaptive blurry image detection algorithm. then, the framework factors out keyframe groups to avoid redundant processing of the same information. keyframe groups contain reasonably-overlapping frames. next, the framework extracts semantics and physical attributes within the environments through recognition modules. during the recognition processes, spurious detections get rejected and missing entities are supplemented.\nfinally, the gathered information gets fused into 3d scene graph and the graph gets updated upon new observations.\nrequirements\nubuntu 16.04+\npython 2.7\npytorch 0.3.1\ntorchtext 0.2.3\nfactorizablenet\nscannet dataset (optional, used for test. an rgbd video from scannet is enough.)\ninstallation\ninstall pytorch 0.3.1. the code has been tested only with python 2.7, cuda 9.0 on ubuntu 16.04. you will need to modify a significant amount of code if you want to run in a different environment (python 3+ or pytorch 0.4+).\ndownload 3d-scene-graph repository\ngit clone --recurse-submodules https://github.com/uehwan/3d-scene-graph.git\ninstall factorizablenet\ncd 3d-scene-graph/factorizablenet\nplease follow the installation instructions in factorizablenet repository. follow steps 1 through 6. you can skip step 7. download vg-dr-net in step 8. you do not need to download other models.\ninstall 3d-scene-graph\ncd 3d-scene-graph\ntouch factorizablenet/__init__.py\nln -s ./factorizablenet/options/ options\nmkdir data\nln -s ./factorizablenet/data/svg data/svg\nln -s ./factorizablenet/data/visual_genome data/visual_genome\npip install torchtext==0.2.3\npip install setuptools pyyaml graphviz webcolors pandas matplotlib\npip install git+https://github.com/chickenbestlover/colorhistogram.git\nan alternative: use installation script\n./build.sh\ndownload scannet dataset\nin order to use scannet dataset, you need to fill out an agreement to toe scannet terms of use and send it to the scannet team at scannet@googlegroups.com. if the process was successful, they will send you a script downloading scannet dataset.\nto download a specific scan (e.g. scene0000_00) using the script (the script only runs on python 2.7):\ndownload-scannet.py -o [directory in which to download] --id scene0000_00\n(then press enter twice)\nafter the download is finished, the scan is located in a new folder scene0000_00. in the folder, *.sens file contains the rgbd video with camera pose. to extract them, we use sensreader, an extraction -----> tool !!!  provided by scannet git repo.\ngit clone https://github.com/scannet/scannet.git\ncd scannet/sensreader/python/\npython reader.py \\\n--filename [your .sens filepath] \\\n--output_path [root of 3d-scene-graph]/data/scene0000_00/ \\\n--export_depth_images \\\n--export_color_images \\\n--export_poses \\\n--export_intrinsics\nexample of usage\npython scene_graph_tuning.py \\\n--scannet_path data/scene0000_00/\\\n--obj_thres 0.23\\\n--thres_key 0.2\\\n--thres_anchor 0.68 \\\n--visualize \\\n--frame_start 800 \\\n--plot_graph \\\n--disable_spurious \\\n--gain 10 \\\n--detect_cnt_thres 2 \\\n--triplet_thres 0.065\ncore hyper-parameters\ndata settings:\n--dataset : choose dataset, default='scannet'.\n--scannet_path : scannet scan filepath , default='./data/scene0507/'.\n--frame_start : idx of frame to start , default=0.\n--frame_end : idx of frame to end , default=5000.\nfactorizablenet output filtering settings:\n--obj_thres : object recognition threshold score , default=0.25.\n--triplet_thres : triplet recognition threshold score , default=0.08.\n--nms : nms threshold for post object nms (negative means not nms) , default=0.2.\n--triplet_nms : triplet nms threshold for post object nms (negative means not nms) , default=0.4.\nkey-frame extraction settings:\n--thres_key : keyframe threshold score , default=0.1.\n--thres_anchor : achorframe threshold score , default=0.65.\n--alpha : weight for exponentially weighted summation , default=0.4.\n--gain : gain for adaptive thresholding in blurry image detection , default=25.\n--offset : offset for adaptive thresholding in blurry image detection , default=1.\nvisualization settings:\n--pause_time : a pause interval (sec) for every detection , default=1.\n--plot_graph : plot 3d scene graph if true.\n--visualize : enable visualization if ture.\n--format : resulting image format, pdf or png, default='png'.\n--draw_color : draw color node in 3d scene graph if true.\n--save_image : save detection result image if true.\nresult\ndemo video\ncitations\nplease consider citing this project in your publications if it helps your research. the following is a bibtex reference.\n@article{kim2019graph3d,\ntitle={3d-scene-graph: a sparse and semantic representation of physical environments for intelligent agents},\nauthor={kim, ue-hwan and park, jin-man and song, taek-jin and kim, jong-hwan},\njournal={ieee cybernetics},\nyear={2019}\n}\nacknowledgement\nthis work was supported by the ict r&d program of msip/iitp. [2016-0-00563, research on adaptive machine learning technology development for intelligent autonomous digital companion]", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000586, "year": null}, {"Unnamed: 0": 1608, "autor": 588, "date": null, "content": "Ignition RViz\nRViz is a 3D visualization tool for robots using ROS.\nIgnition RViz offers functionality similar to RViz, and is developed using Ignition libraries.\nBuild Status\nROS versions: Foxy and Rolling\nIgnition versions: Dome and Edifice\nHead over to the wiki to get detailed description of the project.\nRequirements\nROS 2 Foxy or Rolling\nIgnition Dome or Edifice\nIgnition Common\nIgnition GUI\nIgnition Math\nIgnition Rendering\nQt5\nAdditional QML modules required for GPS Plugin\nQtPositioning (qml-module-qtpositioning)\nQtLocation (qml-module-qtlocation)\nSetup the repository\nCreate a colcon workspace\nmkdir -p ~/colcon_ws/src\ncd ~/colcon_ws/src\nClone the repository\ngit clone https://github.com/ignitionrobotics/ign-rviz.git\nChoose an Ignition version\nexport IGNITION_VERSION=<ignition>\nWhere <ignition> can be dome or edifice. Defaults to Edifice if not set.\nYou only need to set this variable when compiling, not when running.\nBuild ign-rviz\nGo to the root of your colcon workspace\ncd ../\nSource ROS2, where <distro> is foxy or rolling:\nsource /opt/ros/<distro>/setup.bash # If using bash\nsource /opt/ros/<distro>/setup.zsh # If using zsh\nBuild ign-rviz\ncolcon build\nLaunch ign-rviz\nIgnition Rviz can be launched using the following command\n# Source the workspace\nsource install/setup.zsh\n# Launch ign-rviz\nros2 launch ign_rviz rviz.launch.py\nInstruction to generate documentation\nProject documentation can be generated with the help of doxygen using the following commands.\ncd ~/colcon_ws/src/ign-rviz/docs/\n# Generate documentation\ndoxygen rviz.doxyfile\n# View documentation\nfirefox ./html/index.html\nInstruction to use plugins\nImageDisplay plugin\nLaunch a sensor_msgs/msg/Image publisher. For example\n# Source ROS2\nros2 run image_tools cam2image\nLaunch ign-rviz as mentioned above\nLoad an ImageDisplay plugin and select the published topic.", "link": "https://github.com/ignitionrobotics/ign-rviz", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "ignition rviz\nrviz is a 3d visualization -----> tool !!!  for robots using ros.\nignition rviz offers functionality similar to rviz, and is developed using ignition libraries.\nbuild status\nros versions: foxy and rolling\nignition versions: dome and edifice\nhead over to the wiki to get detailed description of the project.\nrequirements\nros 2 foxy or rolling\nignition dome or edifice\nignition common\nignition gui\nignition math\nignition rendering\nqt5\nadditional qml modules required for gps plugin\nqtpositioning (qml-module-qtpositioning)\nqtlocation (qml-module-qtlocation)\nsetup the repository\ncreate a colcon workspace\nmkdir -p ~/colcon_ws/src\ncd ~/colcon_ws/src\nclone the repository\ngit clone https://github.com/ignitionrobotics/ign-rviz.git\nchoose an ignition version\nexport ignition_version=<ignition>\nwhere <ignition> can be dome or edifice. defaults to edifice if not set.\nyou only need to set this variable when compiling, not when running.\nbuild ign-rviz\ngo to the root of your colcon workspace\ncd ../\nsource ros2, where <distro> is foxy or rolling:\nsource /opt/ros/<distro>/setup.bash # if using bash\nsource /opt/ros/<distro>/setup.zsh # if using zsh\nbuild ign-rviz\ncolcon build\nlaunch ign-rviz\nignition rviz can be launched using the following command\n# source the workspace\nsource install/setup.zsh\n# launch ign-rviz\nros2 launch ign_rviz rviz.launch.py\ninstruction to generate documentation\nproject documentation can be generated with the help of doxygen using the following commands.\ncd ~/colcon_ws/src/ign-rviz/docs/\n# generate documentation\ndoxygen rviz.doxyfile\n# view documentation\nfirefox ./html/index.html\ninstruction to use plugins\nimagedisplay plugin\nlaunch a sensor_msgs/msg/image publisher. for example\n# source ros2\nros2 run image_tools cam2image\nlaunch ign-rviz as mentioned above\nload an imagedisplay plugin and select the published topic.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000588, "year": null}, {"Unnamed: 0": 1651, "autor": 631, "date": null, "content": "Stonefish\nAn advanced simulation tool developed for marine robotics.\nStonefish is a C++ library combining a physics engine and a lightweight rendering pipeline. The physics engine is based on the core functionality of the Bullet Physics library, extended to deliver realistic simulation of marine robots. It is directed towards researchers in the field of marine robotics but can as well be used as a general purpose robot simulator.\nStonefish includes advanced hydrodynamic computations based on actual geometry of bodies, to better approximate hydrodynamic forces and allow for effects not possible when using symbolic models. The rendering pipeline, developed from the ground up, delivers realistic rendering of atmosphere, ocean and underwater environment. Special focus was put on the latter, where effects of wavelength-dependent light absorption and scattering were considered (other simulators often use only blue fog).\nStonefish can be used to create standalone applications or combined with a Robot Operating System (ROS) package stonefish_ros, which implements standard simulator node and facilitates easy integration with ROS architecture.\nThere are two sources of documentation for the library: html documentation generated with Sphinx and code documentation generated with Doxygen, based on comments in the code (instructions below).\nRequirements\nThe simulation is CPU heavy and requires a recent GPU. The minimum requirement is the support for OpenGL 4.3.\nInstall official manufacturer drivers for your graphics card before using Stonefish!\nThe software is developed and tested on Linux Ubuntu. It should work on any Unix based platform. A version for Windows is not available at this time. MacOS is not supported due to its lack of support for OpenGL 4.3.\nInstallation\nDependencies\nOpenGL Mathematics library (libglm-dev, version >= 0.9.9.0)\nSDL2 library (libsdl2-dev, may need the following fix!)\nInstall SDL2 library from the repository.\ncd /usr/lib/x86_64-linux-gnu/cmake/SDL2/\nsudo vim sdl2-config.cmake\nRemove space after \"-lSDL2\".\nSave file.\nFreetype library (libfreetype6-dev)\nBuilding\nClone stonefish repository.\ncd stonefish\nmkdir build\ncd build\ncmake ..\nmake -jX (where X is the number of threads)\nsudo make install\nDocumentation\nGo to \"stonefish\" directory.\ndoxygen doxygen\nOpen \"docs/html/index.html\".\nCredits\nThis software was written and is continuously developed by Patryk Cie\u015blak. Parts of the software based on code developed by other authors are clearly marked as such.\nIf you find this software useful in your research, please cite:\nPatryk Cie\u015blak, \"Stonefish: An Advanced Open-Source Simulation Tool Designed for Marine Robotics, With a ROS Interface\", In Proceedings of MTS/IEEE OCEANS 2019, June 2019, Marseille, France\n@inproceedings{stonefish,\nauthor = {Cie{\\'s}lak, Patryk},\nbooktitle = {OCEANS 2019 - Marseille},\ntitle = {{Stonefish: An Advanced Open-Source Simulation Tool Designed for Marine Robotics, With a ROS Interface}},\nmonth = jun,\nyear = {2019},\ndoi={10.1109/OCEANSE.2019.8867434}}\nFunding\nThis work was part of a project titled \u201dForce/position control system to enable compliant manipulation from a floating I-AUV\u201d, which received funding from the European Community H2020 Programme, under the Marie Sklodowska-Curie grant agreement no. 750063. The work was continued under a project titled \u201dEU Marine Robots\u201d, which received funding from the European Community H2020 Programme, grant agreement no. 731103.\nLicense\nThis is free software, published under the General Public License v3.0.", "link": "https://github.com/patrykcieslak/stonefish", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "stonefish\nan advanced simulation -----> tool !!!  developed for marine robotics.\nstonefish is a c++ library combining a physics engine and a lightweight rendering pipeline. the physics engine is based on the core functionality of the bullet physics library, extended to deliver realistic simulation of marine robots. it is directed towards researchers in the field of marine robotics but can as well be used as a general purpose robot simulator.\nstonefish includes advanced hydrodynamic computations based on actual geometry of bodies, to better approximate hydrodynamic forces and allow for effects not possible when using symbolic models. the rendering pipeline, developed from the ground up, delivers realistic rendering of atmosphere, ocean and underwater environment. special focus was put on the latter, where effects of wavelength-dependent light absorption and scattering were considered (other simulators often use only blue fog).\nstonefish can be used to create standalone applications or combined with a robot operating system (ros) package stonefish_ros, which implements standard simulator node and facilitates easy integration with ros architecture.\nthere are two sources of documentation for the library: html documentation generated with sphinx and code documentation generated with doxygen, based on comments in the code (instructions below).\nrequirements\nthe simulation is cpu heavy and requires a recent gpu. the minimum requirement is the support for opengl 4.3.\ninstall official manufacturer drivers for your graphics card before using stonefish!\nthe software is developed and tested on linux ubuntu. it should work on any unix based platform. a version for windows is not available at this time. macos is not supported due to its lack of support for opengl 4.3.\ninstallation\ndependencies\nopengl mathematics library (libglm-dev, version >= 0.9.9.0)\nsdl2 library (libsdl2-dev, may need the following fix!)\ninstall sdl2 library from the repository.\ncd /usr/lib/x86_64-linux-gnu/cmake/sdl2/\nsudo vim sdl2-config.cmake\nremove space after \"-lsdl2\".\nsave file.\nfreetype library (libfreetype6-dev)\nbuilding\nclone stonefish repository.\ncd stonefish\nmkdir build\ncd build\ncmake ..\nmake -jx (where x is the number of threads)\nsudo make install\ndocumentation\ngo to \"stonefish\" directory.\ndoxygen doxygen\nopen \"docs/html/index.html\".\ncredits\nthis software was written and is continuously developed by patryk cie\u015blak. parts of the software based on code developed by other authors are clearly marked as such.\nif you find this software useful in your research, please cite:\npatryk cie\u015blak, \"stonefish: an advanced open-source simulation tool designed for marine robotics, with a ros interface\", in proceedings of mts/ieee oceans 2019, june 2019, marseille, france\n@inproceedings{stonefish,\nauthor = {cie{\\'s}lak, patryk},\nbooktitle = {oceans 2019 - marseille},\ntitle = {{stonefish: an advanced open-source simulation tool designed for marine robotics, with a ros interface}},\nmonth = jun,\nyear = {2019},\ndoi={10.1109/oceanse.2019.8867434}}\nfunding\nthis work was part of a project titled \u201dforce/position control system to enable compliant manipulation from a floating i-auv\u201d, which received funding from the european community h2020 programme, under the marie sklodowska-curie grant agreement no. 750063. the work was continued under a project titled \u201deu marine robots\u201d, which received funding from the european community h2020 programme, grant agreement no. 731103.\nlicense\nthis is free software, published under the general public license v3.0.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000631, "year": null}, {"Unnamed: 0": 1654, "autor": 634, "date": null, "content": "pifi\nA headless wifi provisioning system, primarily designed for robots with Raspberry Pi's.\nPifi uses NetworkManager to do the heavy lifting under the hood.\nThe command line tool is pifi:\nUsage:\npifi status Shows if the device is in AP mode or connected to a network\npifi add <ssid> <password> Adds a connection to scan/connect to on bootup (needs sudo)\npifi remove <ssid> Remove this network (may interfere with ssh)\npifi list seen Lists the SSIDs that see seen during bootup\npifi list pending Lists the SSIDs that still need to configured in NetworkManager\npifi set-hostname <hostname> Set the hostname of the system, also deletes existing AP mode configurations\npifi --version Prints the version of pifi on your system\nOptions:\n-h --help Show this help message and exit\n--version Show pifi version and exit\nPifi runs a script at boot up that does the following by default:\nDetermine if there is wifi device capable of access point mode\nScan for visible access points, and save the SSIDs to /var/lib/pifi/seen_ssids\nGo through any pending connections in /var/lib/pifi/pending, and see if any are visiable\nIf any of the pending connections are visible, connect to them, and remove them from pending\nOtherwise look for an existing AP mode definiton and start it\nIf there is no existing AP mode definition create one with the configuration in /etc/pifi/default_ap.em (SSID:<HOSTNAME><4HEX> and password:'robotseverywhere'). (Where <HOSTNAME> is the hostname of the system and <4HEX> is the last 4 digits of the device mac address.)\nIf AP mode was not started, and a button is configured, wait for a button press to start AP mode\nConnecting to a network while in AP mode\nConnect to the ap mode wifi (default <HOSTNAME><4HEX>, password robotseverywhere) on your laptop. (Where <HOSTNAME> is the hostname of the system and <4HEX> is the last 4 digits of the device mac address.)\nSSH into the device with ssh ubuntu@10.42.0.1.\nOnce logged into the device, run sudo pifi add WIFI_SSID PASSWORD, and reboot sudo reboot.\nYour device should now be connected to your network.\nUsing a Button to Start AP Mode\nWhen pifi starts without AP mode (because it connected to an existing network), if there is an input device specified in pifi.conf it will wait for a KEY_CONFIG press and start AP mode.\nOn Magni, the input device is the gpio-keys driver, connected to a button on the sonar board. This allows the button to show up as a input device, with a name specified in the device tree file. The same name should be specified in pifi.conf, so that pifi can grab that device and listen for events. On a event that matches a KEY_CONFIG press,\nInstallation\nThe recommended way to install is from debs. The apt source at https://packages.ubiquityrobotics.com/ has the packages.\nIf that source is configured on your system, simply run sudo apt install pifi.\nTo install from source, run sudo pip3 install . in the pifi directory after cloning this repo.\nDependencies\nNote: Don't worry about dependencies if you are installing from debs, they will be installed automatically.\nThis package depends on python3-networkmanager, python3-empy, and python3-yaml.\npython3-networkmanager is not availible in the standard ubuntu/debian repos, so you will have install it from pip3 install python-networkmanager, or use the debian package from https://packages.ubiquityrobotics.com/. More info here\nConfiguration\nIf you want to change the behavior of pifi, a few options are availible to tweak.\nThe main configuration file is a YAML file at /etc/pifi/pifi.conf.\nThe default configuration file is:\n# YAML configuration file for pifi\n# Should pifi delete other ap mode configurations in NetworkManager?\n# Default: True\n# If true, during the next boot where there are no networks availble\n# pifi will delete existing connections, and create a new default one\ndelete_existing_ap_connections: True\n# The network interface to use for AP mode\n# Default: any\n# If set to any pifi will pick one automatically\nap_device: any\n# The network interface to use for connecting out\n# Default: any\n# If set to any pifi will pick one automatically\nclient_device: any\n# Path to a Linux LED device to use\n# ex: /sys/class/leds/led0\nstatus_led: None\n# Name of a user input device to use\n# ex: \"Keyboard 5\"\nbutton_device_name: None\nThe settings of the default AP that pifi creates are also configurable. /etc/pifi/default_ap.em contains and empy template of the json that represents the connection settings.\nThe varibles passed into the template are the hostname of the system (hostname), the MAC address of the device (mac), and a newly generated UUIDv4 (uuid_str).\nThis is the default configuration:\n{\n\"connection\": {\n\"id\": \"Pifi AP Mode\",\n\"type\": \"802-11-wireless\",\n\"autoconnect\": \"False\",\n\"uuid\": \"@(uuid_str)\"\n},\n\"802-11-wireless\": {\n\"mode\": \"ap\",\n\"security\": \"802-11-wireless-security\",\n\"ssid\": \"@(hostname)@(mac.replace(\":\", \"\")[-4:])\"\n},\n\"802-11-wireless-security\": {\n\"key-mgmt\": \"wpa-psk\",\n\"psk\": \"robotseverywhere\"\n},\n\"ipv4\": {\n\"method\": \"shared\"\n},\n\"ipv6\": {\n\"method\": \"ignore\"\n}\n}\nEmpy uses the template format @() with python expressions inside of the parenthesis.\nThe mac.replace(\":\", \"\")[-4:] gets the last 4 digits of the MAC address after removing colons.\nInterfacing\nYour code can interface with pifi via the files in /var.\n/var/lib/pifi/pending is a JSON file that contains a list of wifi connections that should be activated. The connections should be JSON serializations of the NetworkManager connection configuration.\nExample contents of /var/lib/pifi/pending:\n[\n{\n\"ipv4\": {\n\"method\": \"auto\"\n},\n\"802-11-wireless-security\": {\n\"key-mgmt\": \"wpa-psk\",\n\"psk\": \"supersecurepassword\"\n},\n\"connection\": {\n\"type\": \"802-11-wireless\",\n\"autoconnect\": true,\n\"id\": \"wifi\",\n\"uuid\": \"ce17378f-b187-48ab-af93-48613e88a5e5\"\n},\n\"802-11-wireless\": {\n\"mode\": \"infrastructure\",\n\"ssid\": \"wifi\",\n\"security\": \"802-11-wireless-security\"\n},\n\"ipv6\": {\n\"method\": \"auto\"\n}\n}\n]\n/var/lib/pifi/seen_ssids is a plain text file with the list of SSIDs seen when pifi was starting up, as many cards can't scan once they start AP mode.\nExample contents of file:\nxfinitywifi\nxfinitywifi\nxfinitywifi\nAleopile\nJhuangdds\nRequilme-CA2\nLinksys New\nxfinitywifi\nAleopile\nJhuangdds\nRequilme-CA2\nATT9C8eh3A\nHOME-616E-2.4\nHOME-99EB\nlinksys\nxfinitywifi\nNotice that the SSIDs can be duplicated (there will be one entry per AP).", "link": "https://github.com/rohbotics/pifi", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "pifi\na headless wifi provisioning system, primarily designed for robots with raspberry pi's.\npifi uses networkmanager to do the heavy lifting under the hood.\nthe command line -----> tool !!!  is pifi:\nusage:\npifi status shows if the device is in ap mode or connected to a network\npifi add <ssid> <password> adds a connection to scan/connect to on bootup (needs sudo)\npifi remove <ssid> remove this network (may interfere with ssh)\npifi list seen lists the ssids that see seen during bootup\npifi list pending lists the ssids that still need to configured in networkmanager\npifi set-hostname <hostname> set the hostname of the system, also deletes existing ap mode configurations\npifi --version prints the version of pifi on your system\noptions:\n-h --help show this help message and exit\n--version show pifi version and exit\npifi runs a script at boot up that does the following by default:\ndetermine if there is wifi device capable of access point mode\nscan for visible access points, and save the ssids to /var/lib/pifi/seen_ssids\ngo through any pending connections in /var/lib/pifi/pending, and see if any are visiable\nif any of the pending connections are visible, connect to them, and remove them from pending\notherwise look for an existing ap mode definiton and start it\nif there is no existing ap mode definition create one with the configuration in /etc/pifi/default_ap.em (ssid:<hostname><4hex> and password:'robotseverywhere'). (where <hostname> is the hostname of the system and <4hex> is the last 4 digits of the device mac address.)\nif ap mode was not started, and a button is configured, wait for a button press to start ap mode\nconnecting to a network while in ap mode\nconnect to the ap mode wifi (default <hostname><4hex>, password robotseverywhere) on your laptop. (where <hostname> is the hostname of the system and <4hex> is the last 4 digits of the device mac address.)\nssh into the device with ssh ubuntu@10.42.0.1.\nonce logged into the device, run sudo pifi add wifi_ssid password, and reboot sudo reboot.\nyour device should now be connected to your network.\nusing a button to start ap mode\nwhen pifi starts without ap mode (because it connected to an existing network), if there is an input device specified in pifi.conf it will wait for a key_config press and start ap mode.\non magni, the input device is the gpio-keys driver, connected to a button on the sonar board. this allows the button to show up as a input device, with a name specified in the device tree file. the same name should be specified in pifi.conf, so that pifi can grab that device and listen for events. on a event that matches a key_config press,\ninstallation\nthe recommended way to install is from debs. the apt source at https://packages.ubiquityrobotics.com/ has the packages.\nif that source is configured on your system, simply run sudo apt install pifi.\nto install from source, run sudo pip3 install . in the pifi directory after cloning this repo.\ndependencies\nnote: don't worry about dependencies if you are installing from debs, they will be installed automatically.\nthis package depends on python3-networkmanager, python3-empy, and python3-yaml.\npython3-networkmanager is not availible in the standard ubuntu/debian repos, so you will have install it from pip3 install python-networkmanager, or use the debian package from https://packages.ubiquityrobotics.com/. more info here\nconfiguration\nif you want to change the behavior of pifi, a few options are availible to tweak.\nthe main configuration file is a yaml file at /etc/pifi/pifi.conf.\nthe default configuration file is:\n# yaml configuration file for pifi\n# should pifi delete other ap mode configurations in networkmanager?\n# default: true\n# if true, during the next boot where there are no networks availble\n# pifi will delete existing connections, and create a new default one\ndelete_existing_ap_connections: true\n# the network interface to use for ap mode\n# default: any\n# if set to any pifi will pick one automatically\nap_device: any\n# the network interface to use for connecting out\n# default: any\n# if set to any pifi will pick one automatically\nclient_device: any\n# path to a linux led device to use\n# ex: /sys/class/leds/led0\nstatus_led: none\n# name of a user input device to use\n# ex: \"keyboard 5\"\nbutton_device_name: none\nthe settings of the default ap that pifi creates are also configurable. /etc/pifi/default_ap.em contains and empy template of the json that represents the connection settings.\nthe varibles passed into the template are the hostname of the system (hostname), the mac address of the device (mac), and a newly generated uuidv4 (uuid_str).\nthis is the default configuration:\n{\n\"connection\": {\n\"id\": \"pifi ap mode\",\n\"type\": \"802-11-wireless\",\n\"autoconnect\": \"false\",\n\"uuid\": \"@(uuid_str)\"\n},\n\"802-11-wireless\": {\n\"mode\": \"ap\",\n\"security\": \"802-11-wireless-security\",\n\"ssid\": \"@(hostname)@(mac.replace(\":\", \"\")[-4:])\"\n},\n\"802-11-wireless-security\": {\n\"key-mgmt\": \"wpa-psk\",\n\"psk\": \"robotseverywhere\"\n},\n\"ipv4\": {\n\"method\": \"shared\"\n},\n\"ipv6\": {\n\"method\": \"ignore\"\n}\n}\nempy uses the template format @() with python expressions inside of the parenthesis.\nthe mac.replace(\":\", \"\")[-4:] gets the last 4 digits of the mac address after removing colons.\ninterfacing\nyour code can interface with pifi via the files in /var.\n/var/lib/pifi/pending is a json file that contains a list of wifi connections that should be activated. the connections should be json serializations of the networkmanager connection configuration.\nexample contents of /var/lib/pifi/pending:\n[\n{\n\"ipv4\": {\n\"method\": \"auto\"\n},\n\"802-11-wireless-security\": {\n\"key-mgmt\": \"wpa-psk\",\n\"psk\": \"supersecurepassword\"\n},\n\"connection\": {\n\"type\": \"802-11-wireless\",\n\"autoconnect\": true,\n\"id\": \"wifi\",\n\"uuid\": \"ce17378f-b187-48ab-af93-48613e88a5e5\"\n},\n\"802-11-wireless\": {\n\"mode\": \"infrastructure\",\n\"ssid\": \"wifi\",\n\"security\": \"802-11-wireless-security\"\n},\n\"ipv6\": {\n\"method\": \"auto\"\n}\n}\n]\n/var/lib/pifi/seen_ssids is a plain text file with the list of ssids seen when pifi was starting up, as many cards can't scan once they start ap mode.\nexample contents of file:\nxfinitywifi\nxfinitywifi\nxfinitywifi\naleopile\njhuangdds\nrequilme-ca2\nlinksys new\nxfinitywifi\naleopile\njhuangdds\nrequilme-ca2\natt9c8eh3a\nhome-616e-2.4\nhome-99eb\nlinksys\nxfinitywifi\nnotice that the ssids can be duplicated (there will be one entry per ap).", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000634, "year": null}, {"Unnamed: 0": 1683, "autor": 663, "date": null, "content": "NeuralNetworkAnalysis.jl\nMethods to verify neural network control systems using reachability analysis\n\u26a0\ufe0f This package is still a work-in-progress.\nLinks\nProceedings of ARCH-COMP20 Category Report: Artificial Intelligence and Neural Network Control Systems (AINNCS) for Continuous and Hybrid Systems Plants\nhttps://github.com/verivital/nnv/tree/master/code/nnv/examples/Submission/ARCH_COMP2020\nRepo with benchmark models (2020)\nProceedings of ARCH-COMP 2019 AI-NNCS\nRepo with benchmark models (2019)\nRelated tools\nsee also the wiki entry on related tools\nNeuralVerification.jl -- Methods to soundly verify deep neural networks\nReachNNStar -- Reachability Analysis Tool of Neural Network Controlled Systems (NNCSs)\nNeuralOptimization.jl -- A compilation of ReLU network optimization algorithms.\nsherlock -- Sherlock is a tool for output range anaylsis of Deep Neural Networks with ReLU activation units.\nNeuralVerifier.jl -- Formal Verification of Deep Neural Networks in Julia\nAdversarial.jl -- Adversarial attacks for Neural Networks written with FluxML", "link": "https://github.com/JuliaReach/NeuralNetworkAnalysis.jl", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "neuralnetworkanalysis.jl\nmethods to verify neural network control systems using reachability analysis\n\u26a0\ufe0f this package is still a work-in-progress.\nlinks\nproceedings of arch-comp20 category report: artificial intelligence and neural network control systems (ainncs) for continuous and hybrid systems plants\nhttps://github.com/verivital/nnv/tree/master/code/nnv/examples/submission/arch_comp2020\nrepo with benchmark models (2020)\nproceedings of arch-comp 2019 ai-nncs\nrepo with benchmark models (2019)\nrelated tools\nsee also the wiki entry on related tools\nneuralverification.jl -- methods to soundly verify deep neural networks\nreachnnstar -- reachability analysis -----> tool !!!  of neural network controlled systems (nncss)\nneuraloptimization.jl -- a compilation of relu network optimization algorithms.\nsherlock -- sherlock is a tool for output range anaylsis of deep neural networks with relu activation units.\nneuralverifier.jl -- formal verification of deep neural networks in julia\nadversarial.jl -- adversarial attacks for neural networks written with fluxml", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000663, "year": null}, {"Unnamed: 0": 1711, "autor": 691, "date": null, "content": "ROSpberryPi\nA quick and dirty way of bootstrapping ROS and some additional tools onto a Raspberry Pi Zero / W. (well, maybe not quick, it still takes more than an hour to complete the script and get everything installed)\nFAQ:\nWhy?\nRecent ROS distributions only support ROS on the Raspberry pi 2B or greater (all arm7l devices) and on Ubuntu Mate. The Raspberry Pi Zero/W is an arm6l device, so you if you want ROS on it you must compile it yourself which is a bit of a pain. So I put together all of my work in a separate repository so others can take advantage of a prebuilt ROS Melodic setup.\nYou can pick up a Pi Zero / W for a little as $5, which opens up all kinds of options for small projects\nIf you operate the Pi Zero as headless you can do all of your development work on another machine, and just push the code to the zero. My current setup has:\nthe pi zero w set up as an access point (using create_ap)\nconnect to it from my laptop using ssh/putty.\nthe ros src directory is shared and you can connect to it from \\\\(whatever your pi zero w ip address is)\\ros\nopen the shared folder in visual studio code\nHow?\nI downloaded and configured all of the required sources and compiled them on a Raspberry Pi 3 (I wouldn't recommend trying to compile them directly on a zero). Next I packaged up the installs using checkinstall and hosted them on my google drive. Then I wrote a script which grabs everything necessary for a functional ROS Melodic install.\nWhat works?\nAlmost everything! (As far as I know)\nWhat doesn't work?\nrviz can't seem to load urdf/stl files without crashing (but you shouldn't be running these tools on the zero anyways)\nvisual studio code will not work on a pi zero (it relies on electron, which only compiles for the arm7l, and the pi zero is an arm6l)\nWill this work on any Raspberry Pi?\nIn theory this should work on any Raspian Stretch install from the zero and original A+ to the 3B+.\nWill this work on a Pi 2B/3B/3B+?\nIn theory, but for those devices it would be much easier to just install ubuntu mate and use the official packages\nRequirements:\nA Raspberry Pi Zero (preferably a W)\nRaspbian Stretch (Desktop or Minimal)\nAn Internet connection\nWhat this script installs:\nA compatible version of TinyXML\nA compatible precompiled arm6l version of OpenCV3\nA compatible precompiled arm6l version of\nROS Melodic Desktop\nROS Melodic Perception (minus the pcl stuff)\nROS Melodic Robot\nROS Melodic Joy(stick)\nAll Dependencies\nTensorflow\nIncreases Swap to 1GB\nGPIZero libraries\nInstalls create_ap (ability to use raspi zero w as access point)\nInitializes a ROS workspace\nSets up a SAMBA Fileshare for the /home/pi/ros/src directory\nSets up all env variables for ROS melodic and your ros workspace\nHow to set this up\nDownload and install Raspbian Stretch (Desktop or Lite) onto a microsd card\nLog into the pi and set it up to access the internet\nwith full desktop, just use the UI https://www.raspberrypi.org/documentation/configuration/wireless/desktop.md\nwith lite, using raspi-config https://www.raspberrypi.org/documentation/configuration/wireless/wireless-cli.md\nwith lite, by configuring two files in the /boot directory https://www.raspberrypi.org/documentation/configuration/wireless/headless.md\nwith lite, using a UART (going headless) https://learn.adafruit.com/raspberry-pi-zero-creation\nIf you're feeling adventurous, maybe overclock the zero a bit (I'm not recommending you do this and I'm not going to walk you through how -- but possible settings are)\n# https://www.raspberrypi.org/forums/viewtopic.php?t=177743#p1169042\n# 10% overclock\narm_freq=1100\nover_voltage=8\nsdram_freq=500\nsdram_over_voltage=2\nforce_turbo=1\nboot_delay=1\ndtparam=sd_overclock=100\nOpen up a command prompt and type in the following commands.\ngit clone https://github.com/alansrobotlab/rosberrypizero\ncd rosberrypizero\nRead through the install script so you understand what you're about to do. (You're not the type of person that runs random scripts you download from the internet are you?)\nnano bootstrap.sh\nRun the bootstrap script and go get a cup of coffee.\nsh bootstrap.sh\nReview the results of the bootstrap script, looking for any errors\nCarefully read the last lines of the bootstrap script.\nOK, now there are a few more things that you need to do\nbefore you can get started.\nFirst, run the following command in a command prompt\nto set the samba filesharing password\nfor user 'pi'\\n (Must be at least 8 characters)\nsudo smbpasswd -a pi\nNext, disconnect from your wifi network and run\nthe following command to set up your pi as an access point\nsudo iw wlan0 disconnect (to disconnect wifi)\nsudo systemctl enable create_ap (set access point to begin automatically on startup)\nsudo systemctl start create_ap (to start the create_ap access point)\nFinally, run the following command to enable ssh, the camera and i2c\nsudo rc-gui (for the gui config tool) \\n\\\nsudo raspi-config (for the command line tool) \\n\"\nhttp://wiki.ros.org/ROS/Tutorials\nIf you ever want to grab an updated version of this repository\ncd ~/.rosberrypizero # whereever you cloned this repository\ngit pull", "link": "https://github.com/alansrobotlab/rospberrypi", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "rospberrypi\na quick and dirty way of bootstrapping ros and some additional tools onto a raspberry pi zero / w. (well, maybe not quick, it still takes more than an hour to complete the script and get everything installed)\nfaq:\nwhy?\nrecent ros distributions only support ros on the raspberry pi 2b or greater (all arm7l devices) and on ubuntu mate. the raspberry pi zero/w is an arm6l device, so you if you want ros on it you must compile it yourself which is a bit of a pain. so i put together all of my work in a separate repository so others can take advantage of a prebuilt ros melodic setup.\nyou can pick up a pi zero / w for a little as $5, which opens up all kinds of options for small projects\nif you operate the pi zero as headless you can do all of your development work on another machine, and just push the code to the zero. my current setup has:\nthe pi zero w set up as an access point (using create_ap)\nconnect to it from my laptop using ssh/putty.\nthe ros src directory is shared and you can connect to it from \\\\(whatever your pi zero w ip address is)\\ros\nopen the shared folder in visual studio code\nhow?\ni downloaded and configured all of the required sources and compiled them on a raspberry pi 3 (i wouldn't recommend trying to compile them directly on a zero). next i packaged up the installs using checkinstall and hosted them on my google drive. then i wrote a script which grabs everything necessary for a functional ros melodic install.\nwhat works?\nalmost everything! (as far as i know)\nwhat doesn't work?\nrviz can't seem to load urdf/stl files without crashing (but you shouldn't be running these tools on the zero anyways)\nvisual studio code will not work on a pi zero (it relies on electron, which only compiles for the arm7l, and the pi zero is an arm6l)\nwill this work on any raspberry pi?\nin theory this should work on any raspian stretch install from the zero and original a+ to the 3b+.\nwill this work on a pi 2b/3b/3b+?\nin theory, but for those devices it would be much easier to just install ubuntu mate and use the official packages\nrequirements:\na raspberry pi zero (preferably a w)\nraspbian stretch (desktop or minimal)\nan internet connection\nwhat this script installs:\na compatible version of tinyxml\na compatible precompiled arm6l version of opencv3\na compatible precompiled arm6l version of\nros melodic desktop\nros melodic perception (minus the pcl stuff)\nros melodic robot\nros melodic joy(stick)\nall dependencies\ntensorflow\nincreases swap to 1gb\ngpizero libraries\ninstalls create_ap (ability to use raspi zero w as access point)\ninitializes a ros workspace\nsets up a samba fileshare for the /home/pi/ros/src directory\nsets up all env variables for ros melodic and your ros workspace\nhow to set this up\ndownload and install raspbian stretch (desktop or lite) onto a microsd card\nlog into the pi and set it up to access the internet\nwith full desktop, just use the ui https://www.raspberrypi.org/documentation/configuration/wireless/desktop.md\nwith lite, using raspi-config https://www.raspberrypi.org/documentation/configuration/wireless/wireless-cli.md\nwith lite, by configuring two files in the /boot directory https://www.raspberrypi.org/documentation/configuration/wireless/headless.md\nwith lite, using a uart (going headless) https://learn.adafruit.com/raspberry-pi-zero-creation\nif you're feeling adventurous, maybe overclock the zero a bit (i'm not recommending you do this and i'm not going to walk you through how -- but possible settings are)\n# https://www.raspberrypi.org/forums/viewtopic.php?t=177743#p1169042\n# 10% overclock\narm_freq=1100\nover_voltage=8\nsdram_freq=500\nsdram_over_voltage=2\nforce_turbo=1\nboot_delay=1\ndtparam=sd_overclock=100\nopen up a command prompt and type in the following commands.\ngit clone https://github.com/alansrobotlab/rosberrypizero\ncd rosberrypizero\nread through the install script so you understand what you're about to do. (you're not the type of person that runs random scripts you download from the internet are you?)\nnano bootstrap.sh\nrun the bootstrap script and go get a cup of coffee.\nsh bootstrap.sh\nreview the results of the bootstrap script, looking for any errors\ncarefully read the last lines of the bootstrap script.\nok, now there are a few more things that you need to do\nbefore you can get started.\nfirst, run the following command in a command prompt\nto set the samba filesharing password\nfor user 'pi'\\n (must be at least 8 characters)\nsudo smbpasswd -a pi\nnext, disconnect from your wifi network and run\nthe following command to set up your pi as an access point\nsudo iw wlan0 disconnect (to disconnect wifi)\nsudo systemctl enable create_ap (set access point to begin automatically on startup)\nsudo systemctl start create_ap (to start the create_ap access point)\nfinally, run the following command to enable ssh, the camera and i2c\nsudo rc-gui (for the gui config -----> tool !!! ) \\n\\\nsudo raspi-config (for the command line -----> tool !!! ) \\n\"\nhttp://wiki.ros.org/ros/tutorials\nif you ever want to grab an updated version of this repository\ncd ~/.rosberrypizero # whereever you cloned this repository\ngit pull", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000691, "year": null}, {"Unnamed: 0": 1713, "autor": 693, "date": null, "content": "This repository has been archived. A recent version can be found on https://gitlab.com/project-march.\nMarch\nThe main repository of the MARCH exoskeleton.\nBranch Build Status\nmaster\ndevelop\nFixing code style\nAll C++ code must follow the roscpp_code_format code styling rules. The rules for this format are set in the .clang-format file. clang-format is a tool that can detect and fix these problems in your code. Before pushing you should make sure that this is fixed, otherwise the Travis build will fail. First you need to install clang-format:\nsudo apt install clang-format\nThen you can run clang-format from the root of this repository:\nfind . -name '*.h' -or -name '*.cpp' | xargs clang-format -i -style=file\nNOTE: This command can make changes to your files.\nIf you would like to show a diff and not use find, install clang_format_check.", "link": "https://github.com/project-march/march", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "this repository has been archived. a recent version can be found on https://gitlab.com/project-march.\nmarch\nthe main repository of the march exoskeleton.\nbranch build status\nmaster\ndevelop\nfixing code style\nall c++ code must follow the roscpp_code_format code styling rules. the rules for this format are set in the .clang-format file. clang-format is a -----> tool !!!  that can detect and fix these problems in your code. before pushing you should make sure that this is fixed, otherwise the travis build will fail. first you need to install clang-format:\nsudo apt install clang-format\nthen you can run clang-format from the root of this repository:\nfind . -name '*.h' -or -name '*.cpp' | xargs clang-format -i -style=file\nnote: this command can make changes to your files.\nif you would like to show a diff and not use find, install clang_format_check.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000693, "year": null}, {"Unnamed: 0": 1719, "autor": 699, "date": null, "content": "Team O2AC at the World Robot Summit 2020 Assembly Challenge\nThis repository provides the robot solution and a common runtime/development environment used by Team O2AC in the Assembly Challenge of the World Robot Summit 2020.\nSee our intro video here:\nhttps://www.youtube.com/watch?v=Dm76puapISY\nSee our exhibition talk and demo here:\nhttps://www.youtube.com/watch?v=tue42LWGzO0&t=2090s\nSee our completed taskboard run here:\nhttps://www.youtube.com/watch?v=u2Hrf--aK5k&t=21522s\nSUMMARY\nThis repository contains both the ROS packages used to control the O2AC robot system, as well as a series of shell scripts and Docker images to build and run the code.\nWith this, you can:\nUse two robot arms\nPlan and execute motions simultaneously and independently on each arm, without risk of collision\nPerform insertions using impedance control\nAlign parts\nExecute motion sequences while subsequent motions are being planned, minimizing waiting times\nFasten screws\nUse a 3D-printed screw tool (under $100 USD in parts) with compliance to find and fasten set screws\nDetect and pick parts from an unstructured tray\nGenerate parts assemblies and use their TF frames\nComplete the WRS2020 taskboard task\nComplete the WRS2020 assembly task (with enough luck)\nQUICK START\nClone the repository into your home folder in Ubuntu 18.04 (or newer).\nInstall the development environment (SETUP-DEVEL-MACHINE.sh, then BUILD-DOCKER-IMAGE.sh)\nRun LAUNCH-TERMINATOR-TERMINAL.sh\nExecute roslaunch o2ac_moveit_config demo.launch and rosrun o2ac_routines assembly.py (in separate terminals)\nRead the documentation, check the Troubleshooting page if you have any problems, and report an issue if you are still stuck afterwards.\nContribution Guidelines\nPlease read the contribution guidelines before pushing code or requesting a merge. Details can be found in the wiki:\nhttps://gitlab.com/o2ac/o2ac-ur/wikis/contribution-guidelines\nReporting Issues\nMinor compatibility issues are expected to arise. Please report any issue by using the issue tracker: https://gitlab.com/o2ac/o2ac-ur/issues\nCiting\nIf you find this repository useful, please star it and cite our papers:\nFelix von Drigalski, Chisato Nakashima, Yoshiya Shibata, Yoshinori Konishi, Joshua C. Triyonoputro, Kaidi Nie, Damien Petit, Toshio Ueshiba, Ryuichi Takase, Yukiyasu Domae, Taku Yoshioka, Yoshihisa Ijiri, Ixchel G. Ramirez-Alpizar, Weiwei Wan & Kensuke Harada (2020) Team O2AS at the world robot summit 2018: an approach to robotic kitting and assembly tasks using general purpose grippers and tools, Advanced Robotics, 34:7-8, 514-530, DOI: 10.1080/01691864.2020.1734481\narXiv\nFelix von Drigalski, Christian Schlette, Martin Rudorfer, Nikolaus Correll, Joshua C. Triyonoputro, Weiwei Wan, Tokuo Tsuji & Tetsuyou Watanabe (2020) Robots assembling machines: learning from the World Robot Summit 2018 Assembly Challenge, Advanced Robotics, 34:7-8, 408-421, DOI: 10.1080/01691864.2019.1705910\narXiv\n(WRS2020 versions to come)\nCredits\nThe Docker and shell script backbone of this project is based on the HSR environment maintained at the Emergent Systems Laboratory, Department of Human and Computer Intelligence, College of Information Science and Engineering, Ritsumeikan University. For support, contact Coarobo GK.\nTeam O2AC at the World Robot Summit 2020 Assembly Challenge consisted of members from OMRON SINIC X, OMRON, Osaka University, National Institute for Advanced Science and Technology (AIST) and Chukyo University:\nFelix von Drigalski (OMRON SINIC X)\nKazumi Kasaura (OMRON SINIC X)\nChisato Nakashima (OMRON)\nCristian C. Beltran-Hernandez (OMRON SINIC X / Osaka University)\nHu Zhengtao (Osaka University)\nToshio Ueshiba (AIST)\nShuichi Akizuki (Chukyo University)\nTakuma Terasawa (Chukyo University)\nRyo Miyoshi (Chukyo University)\nNanako Shimizu (Chukyo University)\nRyosuke Yamada (Chukyo University)\nTenho Houda (Chukyo University)\nYu Horiuchi (Chukyo University)\nRyosuke Tachi (Chukyo University)\nManabu Hashimoto (Chukyo University)\nYukiyasu Domae (AIST)\nWeiwei Wan (Osaka University)\nKensuke Harada (Osaka University)\nWe thank each organization for their support and for the approval to release the source code.", "link": "https://github.com/o2ac/o2ac-ur", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "team o2ac at the world robot summit 2020 assembly challenge\nthis repository provides the robot solution and a common runtime/development environment used by team o2ac in the assembly challenge of the world robot summit 2020.\nsee our intro video here:\nhttps://www.youtube.com/watch?v=dm76puapisy\nsee our exhibition talk and demo here:\nhttps://www.youtube.com/watch?v=tue42lwgzo0&t=2090s\nsee our completed taskboard run here:\nhttps://www.youtube.com/watch?v=u2hrf--ak5k&t=21522s\nsummary\nthis repository contains both the ros packages used to control the o2ac robot system, as well as a series of shell scripts and docker images to build and run the code.\nwith this, you can:\nuse two robot arms\nplan and execute motions simultaneously and independently on each arm, without risk of collision\nperform insertions using impedance control\nalign parts\nexecute motion sequences while subsequent motions are being planned, minimizing waiting times\nfasten screws\nuse a 3d-printed screw -----> tool !!!  (under $100 usd in parts) with compliance to find and fasten set screws\ndetect and pick parts from an unstructured tray\ngenerate parts assemblies and use their tf frames\ncomplete the wrs2020 taskboard task\ncomplete the wrs2020 assembly task (with enough luck)\nquick start\nclone the repository into your home folder in ubuntu 18.04 (or newer).\ninstall the development environment (setup-devel-machine.sh, then build-docker-image.sh)\nrun launch-terminator-terminal.sh\nexecute roslaunch o2ac_moveit_config demo.launch and rosrun o2ac_routines assembly.py (in separate terminals)\nread the documentation, check the troubleshooting page if you have any problems, and report an issue if you are still stuck afterwards.\ncontribution guidelines\nplease read the contribution guidelines before pushing code or requesting a merge. details can be found in the wiki:\nhttps://gitlab.com/o2ac/o2ac-ur/wikis/contribution-guidelines\nreporting issues\nminor compatibility issues are expected to arise. please report any issue by using the issue tracker: https://gitlab.com/o2ac/o2ac-ur/issues\nciting\nif you find this repository useful, please star it and cite our papers:\nfelix von drigalski, chisato nakashima, yoshiya shibata, yoshinori konishi, joshua c. triyonoputro, kaidi nie, damien petit, toshio ueshiba, ryuichi takase, yukiyasu domae, taku yoshioka, yoshihisa ijiri, ixchel g. ramirez-alpizar, weiwei wan & kensuke harada (2020) team o2as at the world robot summit 2018: an approach to robotic kitting and assembly tasks using general purpose grippers and tools, advanced robotics, 34:7-8, 514-530, doi: 10.1080/01691864.2020.1734481\narxiv\nfelix von drigalski, christian schlette, martin rudorfer, nikolaus correll, joshua c. triyonoputro, weiwei wan, tokuo tsuji & tetsuyou watanabe (2020) robots assembling machines: learning from the world robot summit 2018 assembly challenge, advanced robotics, 34:7-8, 408-421, doi: 10.1080/01691864.2019.1705910\narxiv\n(wrs2020 versions to come)\ncredits\nthe docker and shell script backbone of this project is based on the hsr environment maintained at the emergent systems laboratory, department of human and computer intelligence, college of information science and engineering, ritsumeikan university. for support, contact coarobo gk.\nteam o2ac at the world robot summit 2020 assembly challenge consisted of members from omron sinic x, omron, osaka university, national institute for advanced science and technology (aist) and chukyo university:\nfelix von drigalski (omron sinic x)\nkazumi kasaura (omron sinic x)\nchisato nakashima (omron)\ncristian c. beltran-hernandez (omron sinic x / osaka university)\nhu zhengtao (osaka university)\ntoshio ueshiba (aist)\nshuichi akizuki (chukyo university)\ntakuma terasawa (chukyo university)\nryo miyoshi (chukyo university)\nnanako shimizu (chukyo university)\nryosuke yamada (chukyo university)\ntenho houda (chukyo university)\nyu horiuchi (chukyo university)\nryosuke tachi (chukyo university)\nmanabu hashimoto (chukyo university)\nyukiyasu domae (aist)\nweiwei wan (osaka university)\nkensuke harada (osaka university)\nwe thank each organization for their support and for the approval to release the source code.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000699, "year": null}, {"Unnamed: 0": 1721, "autor": 701, "date": null, "content": "Autonomous Systems\nThis repository holds code and resources for the Autonomous Systems course that takes place at Instituto Superior Tecnico, Lisboa.\nFor this course you will need to have a machine running Ubuntu, ROS and some other things, please follow the instructions bellow before the laboratory classes begin.\nThe supported distribution is ROS Kinetic on Ubuntu 16.04, although both ROS Melodic and ROS Noetic on Ubuntu 18.04 and 20.04 should also work. It is preferable to use ROS Kinetic, but if for some reason you already have one of the other Ubuntus you can try that distribution, at your own risk. If there is a problem of incompatibility, you might have to change to ROS Kinetic.\nUbuntu instalation\nFor the first classes you may use a virtual machine, but it is advisable, at some point, to use a native instalation of Ubuntu 16.04 64-bit in order to prevent unwanted issues that may arise from the use of a virtual machine.\nROS Kinetic installation\nInstall git on your system:\nsudo apt-get install git\nIt is advisable to go for the Desktop-Full Instalation of ROS, please go through the instructions in the following link: http://wiki.ros.org/kinetic/Installation/Ubuntu\nGit and Repository setup\nNavigate to your home folder:\ncd $HOME\nClone the repository:\ngit clone https://github.com/socrob/autonomous_systems.git\nWhenever the repo is updated you can get the latest updates with:\ncd $HOME/autonomous_systems\ngit pull origin master\n(Optional) Terminator is a usefull tool when you want to have several terminals open. If you want to install it just run:\nsudo apt-get install terminator\nSetup catkin workspace and scripts structure (one time only)\nA partial documentation of this can be found here and here\nCreate a folder for your ros workspace in your system:\nmkdir -p ~/catkin_ws/src\nInstall python catkin tools (to be able to compile with the new catkin build system, instead of catkin_make which is the old one)\nsudo apt-get install python-catkin-tools\nSource (enable) ROS on your system (only one time, this line should not be there on your .bashrc as it will be bashed by the scripts structure)\nsource /opt/ros/kinetic/setup.bash\nCompile your workspace:\ncd ~/catkin_ws && catkin build\nAdd lines of code to your .bashrc to source the scripts configuration files\necho \"# personal config starts here\" >> ~/.bashrc && echo \"source ~/scripts/permanent.sh\" >> ~/.bashrc\nCopy scripts folder to your home directory:\ncp -r ~/autonomous_systems/resources/scripts $HOME\nSource your new scripts structure:\nsource ~/.bashrc\nLab1\nLab1 will give you some concepts on ROS, and how to use it. Please open lab1 folder to access the lab slides.\nLab2\nLab2 has 2 components, a simulation focused part and the real robot part. Please open lab2-simulation or lab2-real-robot folders to access the content and follow the instructions for the practical exercise. There is README.md file inside. You should follow the instructions in the Before the Lab section so you can perform the tasks during the lab. This lab will be more hands on than the previous, and you are expected to experiment ROS yourself. If you can, perform all the tasks in lab2-simulation by yourself so in the classroom you can present your doubts and have time to try the real robot instructions.\nQuestions\nPlease post your questions under:\nhttps://github.com/socrob/autonomous_systems/issues\nBy creating an issue, so that we can all benefit from the answers.\nThanks!\nExtra Resources\nWiki with summarized information about system setup, git useful commands and information about the robots\nLearn how to program in python, google developers nice online course\nROS python example on how to publish / subcribe from code\nROS beginner tutorials\nNice ROS online course from ETH Zurich see their youtube lectures here\nROS tf tranformations library package\nROS tf tutorials\nROS AMCL package Adaptive Monte Carlo localization\nROS gmapping : SLAM algorithm (simultaneous localization and mapping)", "link": "https://github.com/socrob/autonomous_systems", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "autonomous systems\nthis repository holds code and resources for the autonomous systems course that takes place at instituto superior tecnico, lisboa.\nfor this course you will need to have a machine running ubuntu, ros and some other things, please follow the instructions bellow before the laboratory classes begin.\nthe supported distribution is ros kinetic on ubuntu 16.04, although both ros melodic and ros noetic on ubuntu 18.04 and 20.04 should also work. it is preferable to use ros kinetic, but if for some reason you already have one of the other ubuntus you can try that distribution, at your own risk. if there is a problem of incompatibility, you might have to change to ros kinetic.\nubuntu instalation\nfor the first classes you may use a virtual machine, but it is advisable, at some point, to use a native instalation of ubuntu 16.04 64-bit in order to prevent unwanted issues that may arise from the use of a virtual machine.\nros kinetic installation\ninstall git on your system:\nsudo apt-get install git\nit is advisable to go for the desktop-full instalation of ros, please go through the instructions in the following link: http://wiki.ros.org/kinetic/installation/ubuntu\ngit and repository setup\nnavigate to your home folder:\ncd $home\nclone the repository:\ngit clone https://github.com/socrob/autonomous_systems.git\nwhenever the repo is updated you can get the latest updates with:\ncd $home/autonomous_systems\ngit pull origin master\n(optional) terminator is a usefull -----> tool !!!  when you want to have several terminals open. if you want to install it just run:\nsudo apt-get install terminator\nsetup catkin workspace and scripts structure (one time only)\na partial documentation of this can be found here and here\ncreate a folder for your ros workspace in your system:\nmkdir -p ~/catkin_ws/src\ninstall python catkin tools (to be able to compile with the new catkin build system, instead of catkin_make which is the old one)\nsudo apt-get install python-catkin-tools\nsource (enable) ros on your system (only one time, this line should not be there on your .bashrc as it will be bashed by the scripts structure)\nsource /opt/ros/kinetic/setup.bash\ncompile your workspace:\ncd ~/catkin_ws && catkin build\nadd lines of code to your .bashrc to source the scripts configuration files\necho \"# personal config starts here\" >> ~/.bashrc && echo \"source ~/scripts/permanent.sh\" >> ~/.bashrc\ncopy scripts folder to your home directory:\ncp -r ~/autonomous_systems/resources/scripts $home\nsource your new scripts structure:\nsource ~/.bashrc\nlab1\nlab1 will give you some concepts on ros, and how to use it. please open lab1 folder to access the lab slides.\nlab2\nlab2 has 2 components, a simulation focused part and the real robot part. please open lab2-simulation or lab2-real-robot folders to access the content and follow the instructions for the practical exercise. there is readme.md file inside. you should follow the instructions in the before the lab section so you can perform the tasks during the lab. this lab will be more hands on than the previous, and you are expected to experiment ros yourself. if you can, perform all the tasks in lab2-simulation by yourself so in the classroom you can present your doubts and have time to try the real robot instructions.\nquestions\nplease post your questions under:\nhttps://github.com/socrob/autonomous_systems/issues\nby creating an issue, so that we can all benefit from the answers.\nthanks!\nextra resources\nwiki with summarized information about system setup, git useful commands and information about the robots\nlearn how to program in python, google developers nice online course\nros python example on how to publish / subcribe from code\nros beginner tutorials\nnice ros online course from eth zurich see their youtube lectures here\nros tf tranformations library package\nros tf tutorials\nros amcl package adaptive monte carlo localization\nros gmapping : slam algorithm (simultaneous localization and mapping)", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000701, "year": null}, {"Unnamed: 0": 1748, "autor": 728, "date": null, "content": "This repository has been archived and is not maintained any further. Refer to alurity if you wish to access pre-built robots challenges and robotic environments.\nRobotics CTF (RCTF)\nThe Robotics Capture the Flag (Robotics CTF or RCTF) is an online playground to challenge robot security from your browser. It was designed to be online, available 24/7, launchable through your browser and designed to learn robot hacking step by step while competing with other security researchers. In an attempt to facilitate reproduction of results and further customization (common when researching vulnerabilities), this repository provides a list with open source reference scenarios that run in our Robotics CTF.\nThis repository contains a list of the scenarios available in the Robotics CTF.\nScenario Short description Author/s Status\nrctf-scenario1 Unprotected topics show a lot of interesting information. Search on them to get your answer. Useful tools: rostopic aliasrobotics Active\nrctf-scenario2 In ROS2, even if security measures are available, not configuring them leverages to the same results as in ROS1. Useful tools: ros2 topic aliasrobotics Active\nrctf-scenario3 The dinosaurs are out of control, and the node that controls the gates is not letting us in. Try to guess what the node wants so you can access the next scenario. aliasrobotics Active\nrctf-scenario4 Even if collaborative robots are fun to play with, if they are out of control, they can be dangerous too! Try to hit our friend, Pruden, with the robot in order to get the flag. aliasrobotics Active\nrctf-scenario5 There is a topic that has the flag, but rostopic has been disabled. Try to use alternative methods in order to get it. aliasrobotics Active\nrctf-scenario6 To know more about the topics and the communications between the nodes, use our footprinting tool, called Aztarna. aliasrobotics Active\nContributing\nWe invite security researchers to create their own robotics security scenarios and share them with the community. We accept such contributions through Pull Request. To create your own scenario, start from this simple template.\nCite our work\nIf you're using our work for your research, please cite us as:\n@ARTICLE{rctf,\nauthor = {{Olalde Mendia}, G. and {Usategui San Juan}, L. and {Perez Bascaran}, X. and\n{Bilbao Calvo}, A. and {Hern{\\'a}ndez Cordero}, A. and {Zamalloa Ugarte}, I. and\n{Mu{\\~n}iz Rosas}, A. and {Mayoral Vilches}, D. and {Ayucar Carbajo}, U. and\n{Alzola Kirschgens}, L. and {Mayoral Vilches}, V. and {Gil-Uriarte}, E.\n},\ntitle = \"{Robotics CTF (RCTF), a playground for robot hacking}\",\njournal = {ArXiv e-prints},\narchivePrefix = \"arXiv\",\neprint = {1810.02690},\nprimaryClass = \"cs.CY\",\nkeywords = {Computer Science - Computers and Society, Computer Science - Robotics},\nyear = 2018,\nmonth = oct,\nadsurl = {http://adsabs.harvard.edu/abs/2018arXiv181002690O},\nadsnote = {Provided by the SAO/NASA Astrophysics Data System}\n}", "link": "https://github.com/aliasrobotics/RCTF", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "this repository has been archived and is not maintained any further. refer to alurity if you wish to access pre-built robots challenges and robotic environments.\nrobotics ctf (rctf)\nthe robotics capture the flag (robotics ctf or rctf) is an online playground to challenge robot security from your browser. it was designed to be online, available 24/7, launchable through your browser and designed to learn robot hacking step by step while competing with other security researchers. in an attempt to facilitate reproduction of results and further customization (common when researching vulnerabilities), this repository provides a list with open source reference scenarios that run in our robotics ctf.\nthis repository contains a list of the scenarios available in the robotics ctf.\nscenario short description author/s status\nrctf-scenario1 unprotected topics show a lot of interesting information. search on them to get your answer. useful tools: rostopic aliasrobotics active\nrctf-scenario2 in ros2, even if security measures are available, not configuring them leverages to the same results as in ros1. useful tools: ros2 topic aliasrobotics active\nrctf-scenario3 the dinosaurs are out of control, and the node that controls the gates is not letting us in. try to guess what the node wants so you can access the next scenario. aliasrobotics active\nrctf-scenario4 even if collaborative robots are fun to play with, if they are out of control, they can be dangerous too! try to hit our friend, pruden, with the robot in order to get the flag. aliasrobotics active\nrctf-scenario5 there is a topic that has the flag, but rostopic has been disabled. try to use alternative methods in order to get it. aliasrobotics active\nrctf-scenario6 to know more about the topics and the communications between the nodes, use our footprinting -----> tool !!! , called aztarna. aliasrobotics active\ncontributing\nwe invite security researchers to create their own robotics security scenarios and share them with the community. we accept such contributions through pull request. to create your own scenario, start from this simple template.\ncite our work\nif you're using our work for your research, please cite us as:\n@article{rctf,\nauthor = {{olalde mendia}, g. and {usategui san juan}, l. and {perez bascaran}, x. and\n{bilbao calvo}, a. and {hern{\\'a}ndez cordero}, a. and {zamalloa ugarte}, i. and\n{mu{\\~n}iz rosas}, a. and {mayoral vilches}, d. and {ayucar carbajo}, u. and\n{alzola kirschgens}, l. and {mayoral vilches}, v. and {gil-uriarte}, e.\n},\ntitle = \"{robotics ctf (rctf), a playground for robot hacking}\",\njournal = {arxiv e-prints},\narchiveprefix = \"arxiv\",\neprint = {1810.02690},\nprimaryclass = \"cs.cy\",\nkeywords = {computer science - computers and society, computer science - robotics},\nyear = 2018,\nmonth = oct,\nadsurl = {http://adsabs.harvard.edu/abs/2018arxiv181002690o},\nadsnote = {provided by the sao/nasa astrophysics data system}\n}", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000728, "year": null}, {"Unnamed: 0": 1760, "autor": 740, "date": null, "content": "The fmi_adapter repository\nThis repository provides the fmi_adapter package for wrapping functional mockup units (FMUs) for co-simulation of physical models into ROS 2 nodes, i.e. for the version ROS 2. The implementation for the first generation of ROS can be found in the melodic_and_noetic branch.\nFMUs are defined in the FMI standard and can be created with a variety of modeling and simulation tools, including Dymola, MATLAB/Simulink, OpenModelica, SimulationX, and Wolfram System Modeler.\nfmi_adapter provides a library with convenience functions based on common ROS types to load an FMU during runtime, retrieve the input, output, and parameter names, set timestamped input values, run the FMU's numeric solver, and query the resulting output.\nIn detail, this repository contains two ROS 2 packages:\nfmi_adapter provides a generic library and node for loading and running FMUs in ROS-based applications.\nfmi_adapter_examples provides small examples for the use of fmi_adapter.\nTechnical information on the interfaces and use of these packages is given in the README.md files in the corresponding subfolders.\nPurpose of the project\nThe software is not ready for production use. It has neither been developed nor tested for a specific use case. However, the license conditions of the applicable Open Source licenses allow you to adapt the software to your needs. Before using it in a safety relevant setting, make sure that the software fulfills your requirements and adjust it according to any applicable safety standards (e.g. ISO 26262).\nRequirements, how to build, test, install, use, etc.\nClone the repository into a ROS workspace and build it using colcon.\nLicense\nfmi_adapter is open-sourced under the Apache-2.0 license. See the LICENSE file for details.\nFor a list of other open source components included in fmi_adapter, see the file 3rd-party-licenses.txt.\nQuality assurance\nThe colcon_test tool is used for quality assurances, which includes cpplint, uncrustify, flake8, xmllint and various other tools.\nUnit tests based on gtest are located in the fmi_adapter/test folder. The unit tests use an FMU created with the FMU SDK by QTronic GmbH, cf. 3rd-party-licenses.txt.\nKnown issues/limitations\nPlease notice the following issues/limitations:\nfmi_adapter only supports FMUs according to the FMI 2.0 standard.\nfmi_adapter treats all inputs, outputs and parameters of a given FMU as floating-point values (ROS message std_msgs::msg::Float64, C++ type double, FMI type fmi2fmi2_real_t).\nPapers\nIf you want to cite this repository/package, please cite the following book chapter (PDF available at Springer Link) instead:\nRalph Lange, Silvio Traversaro, Oliver Lenord, and Christian Bertsch: Integrating the Functional Mock-Up Interface with ROS and Gazebo. In: Anis Koubaa (ed.) Robot Operating System (ROS): The Complete Reference (Volume 5), Springer, pp. 187\u2013231, 2021.\n@INBOOK{Lange_et_al_2021_Integrating_the_FMI_with_ROS_and_Gazebo,\nauthor = {Ralph Lange and Silvio Traversaro and Oliver Lenord and Christian Bertsch},\ntitle = {Integrating the Functional Mock-Up Interface with ROS and Gazebo},\neditor = {Anis Koubaa},\nbooktitle = {Robot Operating System (ROS): The Complete Reference (Volume 5)},\nyear = {2021},\npublisher = {Springer},\npages = {187--231},\ndoi = {10.1007/978-3-030-45956-7_7}\n}", "link": "https://github.com/boschresearch/fmi_adapter", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "the fmi_adapter repository\nthis repository provides the fmi_adapter package for wrapping functional mockup units (fmus) for co-simulation of physical models into ros 2 nodes, i.e. for the version ros 2. the implementation for the first generation of ros can be found in the melodic_and_noetic branch.\nfmus are defined in the fmi standard and can be created with a variety of modeling and simulation tools, including dymola, matlab/simulink, openmodelica, simulationx, and wolfram system modeler.\nfmi_adapter provides a library with convenience functions based on common ros types to load an fmu during runtime, retrieve the input, output, and parameter names, set timestamped input values, run the fmu's numeric solver, and query the resulting output.\nin detail, this repository contains two ros 2 packages:\nfmi_adapter provides a generic library and node for loading and running fmus in ros-based applications.\nfmi_adapter_examples provides small examples for the use of fmi_adapter.\ntechnical information on the interfaces and use of these packages is given in the readme.md files in the corresponding subfolders.\npurpose of the project\nthe software is not ready for production use. it has neither been developed nor tested for a specific use case. however, the license conditions of the applicable open source licenses allow you to adapt the software to your needs. before using it in a safety relevant setting, make sure that the software fulfills your requirements and adjust it according to any applicable safety standards (e.g. iso 26262).\nrequirements, how to build, test, install, use, etc.\nclone the repository into a ros workspace and build it using colcon.\nlicense\nfmi_adapter is open-sourced under the apache-2.0 license. see the license file for details.\nfor a list of other open source components included in fmi_adapter, see the file 3rd-party-licenses.txt.\nquality assurance\nthe colcon_test -----> tool !!!  is used for quality assurances, which includes cpplint, uncrustify, flake8, xmllint and various other tools.\nunit tests based on gtest are located in the fmi_adapter/test folder. the unit tests use an fmu created with the fmu sdk by qtronic gmbh, cf. 3rd-party-licenses.txt.\nknown issues/limitations\nplease notice the following issues/limitations:\nfmi_adapter only supports fmus according to the fmi 2.0 standard.\nfmi_adapter treats all inputs, outputs and parameters of a given fmu as floating-point values (ros message std_msgs::msg::float64, c++ type double, fmi type fmi2fmi2_real_t).\npapers\nif you want to cite this repository/package, please cite the following book chapter (pdf available at springer link) instead:\nralph lange, silvio traversaro, oliver lenord, and christian bertsch: integrating the functional mock-up interface with ros and gazebo. in: anis koubaa (ed.) robot operating system (ros): the complete reference (volume 5), springer, pp. 187\u2013231, 2021.\n@inbook{lange_et_al_2021_integrating_the_fmi_with_ros_and_gazebo,\nauthor = {ralph lange and silvio traversaro and oliver lenord and christian bertsch},\ntitle = {integrating the functional mock-up interface with ros and gazebo},\neditor = {anis koubaa},\nbooktitle = {robot operating system (ros): the complete reference (volume 5)},\nyear = {2021},\npublisher = {springer},\npages = {187--231},\ndoi = {10.1007/978-3-030-45956-7_7}\n}", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000740, "year": null}, {"Unnamed: 0": 1776, "autor": 756, "date": null, "content": "Rcs\nRcs is a set of C and C++ libraries for robot control and simulation. It is written for research purposes and for simulation and analysis in robotics. It contains algorithms for vector-matrix calculations, rigid body kinematics, dynamics, control, physical simulation and more. There are many utilities such as OpenSceneGraph nodes and Qt Guis to support research and testing.\nGetting Started\nRcs can be compiled with the cmake build system and has mainly been developed on Ubuntu 14.04 and GCC 4.8. To compile it, just type:\ncd <build-directory>\ncmake <source-directory>\nmake\nmake unittest\nTo build the doxygen documentation, just type:\nmake RcsDoc\nCompilation has successfully been tested on Ubuntu 14.04, Ubuntu 16.04 with GCC 5 and Ubuntu 18.04 with GCC 7 and clang, and Ubuntu 20.\nNote that using Vortex Essentials on newer operating systems requires extra care. The official distribution is compiled with GCC 4.8, and will not work with newer compiler versions. To work around this limitation, Rcs compiles the Vortex integration module separately. If GCC 4.8 is available (by installing the g++-4.8 package), the integration module is built automatically. If it isn't, you need to provide a pre-built version of libRcsVortex.so.\nIt is also possible to compile it on Microsoft Visual Studio, however with some additional efforts. The libxml2 and pthreads library as well as the below mentioned dependencies need to be installed. Further, a cmake version higher than 3.4 is required. They support automatic symbol generation for windows compilation. The formerly mandatory export declaratives are not needed (except for global variables). Here is how to call it:\ncd <build-directory>\ncmake.exe -DCMAKE_WINDOWS_EXPORT_ALL_SYMBOLS=TRUE -DBUILD_SHARED_LIBS=TRUE -DCMAKE_INSTALL_PREFIX=<install-directory> -DCMAKE_BUILD_TYPE=Release <source-directory> -Wno-dev\nBuild options\nUSE_BULLET: Enable bullet physics. With this option, a PhysicsSimulation class that uses Bullet Physics will be built. It can be instantiated through the PhysicsFactory class. Please refer to the documentation. Rcs requires a version equal or higher than 2.83, compiled with shared libraries. Please refer to the Bullet Physics instructions. In case cmake can find a bullet library, the option is ON, otherwise it is OFF.\nUSE_WM5: Use the GemoetricTools library (WildMagic5). This will enable a number of functions related to computational geometry, e.g. distance calculation of shape primitives, and ray casts. In case cmake can find the library, the option is ON, otherwise it is OFF.\nUSE_VORTEX: Enable Vortex physics. With this option, a PhysicsSimulation class that uses Vortex Essentials (CmLabs) or Vortex with version 6.8 will be built. It can be instantiated through the PhysicsFactory class. Please refer to the documentation. To obtain Vortex, please refer to the CmLabs web-site. The default is OFF.\nVORTEX_ESSENTIALS_DIR: Installation directory of Vortex Essentials. Required since there is no standard location for this.\nUSE_EIGEN3: Use the Eigen3 library. This will compile in a number of additional linear algebra functions (See Rcs_eigen3.h for details)\nENABLE_DEBUG_MACROS: This option enables logging macros that allow to log information on different debug levels (such as RLOG). If disabled, all these conditional logs are not compiled into the binaries. This usually leads to faster programs, however with the drawback of loosing the information if something goes wrong. The default is ON.\nENABLE_C++11: This option sets the corresponding flag so that all code is compiled with the C++11 support. It is not needed from the code, but might be necessary for binary compatibility if other code is compiled for C++11. The default is OFF.\nHEADLESS_BUILD: Build only RcsCore and RcsPhysics, and leave out all graphics and Gui related classes. If no graphics and Gui libraries are present on the system, you need to call cmake -DHEADLESS_BUILD=TRUE. The default is OFF.\nProject structure\nbin : Source files with a main() function\ncmake : CMake related configuration files\nconfig : Configuration files such as e.g xml,\ndoc : Documentation (doxygen configuration, latex)\nexamples : Examples with a main() function\nexternal : Third party source code\nsrc : Source files that are compiled into libraries\nRcsCore : Algorithms, math, utilities\nRcsGui : Qt widgets\nRcsGraphics : OpenSceneGraph nodes\nRcsPhysics : Bullet and Vortex wrappers\n3rd party libraries\nRcs has been designed carefully to have only little dependencies. The ones that have been selected are in our opinion very matured libraries:\nLibxml2 (MIT License, Ubuntu-package libxml2-dev)\nQt5: LGPL (Also dual commercial license available, Ubuntu-package qt5-default)\nqwt (LGPL with additions, Ubuntu-package libqwt-qt5-dev)\nOpenSceneGraph (OSGPL, Ubuntu-package libopenscenegraph-dev)\nOptionally (please use the ccmake tool to manage the compile options), additional functionality can be compiled in when activating\nGeometricTools (Wild Magic Engine 5.17, Boost Software License 1.0). Web-site: https://www.geometrictools.com\nBullet Physics (Zlib, Ubuntu-package libbullet-dev)\nVortex Studio Essentials\nEigen3 (Mozilla Public License Version 2.0)\nIn case you don't have a Vortex license, you can in certain cases apply for the Adademic Access Program that allows to get a free license. Please check: https://www.cm-labs.com\nTroubleshooting\nBullet library linking errors: In case you use a bullet version compiled with double precision, please make sure the define BT_USE_DOUBLE_PRECISION is used during compilation (see cmake/Externals.cmake for details). Likewise, in case you are using a bullet library compiled with float values, make sure that the above define is not set.\nLicense\nThis project is licensed under the BSD 3-clause license - see the LICENSE.md file for details\nDisclaimer\nThe copyright holders are not liable for any damage(s) incurred due to improper use of Rcs.", "link": "https://github.com/HRI-EU/Rcs", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "rcs\nrcs is a set of c and c++ libraries for robot control and simulation. it is written for research purposes and for simulation and analysis in robotics. it contains algorithms for vector-matrix calculations, rigid body kinematics, dynamics, control, physical simulation and more. there are many utilities such as openscenegraph nodes and qt guis to support research and testing.\ngetting started\nrcs can be compiled with the cmake build system and has mainly been developed on ubuntu 14.04 and gcc 4.8. to compile it, just type:\ncd <build-directory>\ncmake <source-directory>\nmake\nmake unittest\nto build the doxygen documentation, just type:\nmake rcsdoc\ncompilation has successfully been tested on ubuntu 14.04, ubuntu 16.04 with gcc 5 and ubuntu 18.04 with gcc 7 and clang, and ubuntu 20.\nnote that using vortex essentials on newer operating systems requires extra care. the official distribution is compiled with gcc 4.8, and will not work with newer compiler versions. to work around this limitation, rcs compiles the vortex integration module separately. if gcc 4.8 is available (by installing the g++-4.8 package), the integration module is built automatically. if it isn't, you need to provide a pre-built version of librcsvortex.so.\nit is also possible to compile it on microsoft visual studio, however with some additional efforts. the libxml2 and pthreads library as well as the below mentioned dependencies need to be installed. further, a cmake version higher than 3.4 is required. they support automatic symbol generation for windows compilation. the formerly mandatory export declaratives are not needed (except for global variables). here is how to call it:\ncd <build-directory>\ncmake.exe -dcmake_windows_export_all_symbols=true -dbuild_shared_libs=true -dcmake_install_prefix=<install-directory> -dcmake_build_type=release <source-directory> -wno-dev\nbuild options\nuse_bullet: enable bullet physics. with this option, a physicssimulation class that uses bullet physics will be built. it can be instantiated through the physicsfactory class. please refer to the documentation. rcs requires a version equal or higher than 2.83, compiled with shared libraries. please refer to the bullet physics instructions. in case cmake can find a bullet library, the option is on, otherwise it is off.\nuse_wm5: use the gemoetrictools library (wildmagic5). this will enable a number of functions related to computational geometry, e.g. distance calculation of shape primitives, and ray casts. in case cmake can find the library, the option is on, otherwise it is off.\nuse_vortex: enable vortex physics. with this option, a physicssimulation class that uses vortex essentials (cmlabs) or vortex with version 6.8 will be built. it can be instantiated through the physicsfactory class. please refer to the documentation. to obtain vortex, please refer to the cmlabs web-site. the default is off.\nvortex_essentials_dir: installation directory of vortex essentials. required since there is no standard location for this.\nuse_eigen3: use the eigen3 library. this will compile in a number of additional linear algebra functions (see rcs_eigen3.h for details)\nenable_debug_macros: this option enables logging macros that allow to log information on different debug levels (such as rlog). if disabled, all these conditional logs are not compiled into the binaries. this usually leads to faster programs, however with the drawback of loosing the information if something goes wrong. the default is on.\nenable_c++11: this option sets the corresponding flag so that all code is compiled with the c++11 support. it is not needed from the code, but might be necessary for binary compatibility if other code is compiled for c++11. the default is off.\nheadless_build: build only rcscore and rcsphysics, and leave out all graphics and gui related classes. if no graphics and gui libraries are present on the system, you need to call cmake -dheadless_build=true. the default is off.\nproject structure\nbin : source files with a main() function\ncmake : cmake related configuration files\nconfig : configuration files such as e.g xml,\ndoc : documentation (doxygen configuration, latex)\nexamples : examples with a main() function\nexternal : third party source code\nsrc : source files that are compiled into libraries\nrcscore : algorithms, math, utilities\nrcsgui : qt widgets\nrcsgraphics : openscenegraph nodes\nrcsphysics : bullet and vortex wrappers\n3rd party libraries\nrcs has been designed carefully to have only little dependencies. the ones that have been selected are in our opinion very matured libraries:\nlibxml2 (mit license, ubuntu-package libxml2-dev)\nqt5: lgpl (also dual commercial license available, ubuntu-package qt5-default)\nqwt (lgpl with additions, ubuntu-package libqwt-qt5-dev)\nopenscenegraph (osgpl, ubuntu-package libopenscenegraph-dev)\noptionally (please use the ccmake -----> tool !!!  to manage the compile options), additional functionality can be compiled in when activating\ngeometrictools (wild magic engine 5.17, boost software license 1.0). web-site: https://www.geometrictools.com\nbullet physics (zlib, ubuntu-package libbullet-dev)\nvortex studio essentials\neigen3 (mozilla public license version 2.0)\nin case you don't have a vortex license, you can in certain cases apply for the adademic access program that allows to get a free license. please check: https://www.cm-labs.com\ntroubleshooting\nbullet library linking errors: in case you use a bullet version compiled with double precision, please make sure the define bt_use_double_precision is used during compilation (see cmake/externals.cmake for details). likewise, in case you are using a bullet library compiled with float values, make sure that the above define is not set.\nlicense\nthis project is licensed under the bsd 3-clause license - see the license.md file for details\ndisclaimer\nthe copyright holders are not liable for any damage(s) incurred due to improper use of rcs.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000756, "year": null}, {"Unnamed: 0": 1779, "autor": 759, "date": null, "content": "Visit Website:\nYoutube Channel:\nRobotics Level 4\nThis repo is an extension of previous level. The code of this robot is organised in various folders inside the directory 'earthrover'. The names of these folders briefly indicate the purpose of the code inside them. This repo focusses on the advanced capabilities embedded into the robot via use of Pre-trained Machine Learning models provided by \"tensorflow.org\" or created via online tool of Google called Teachable Machine. The following projects in this repo demonstrate how we can integrate Tensorflow Lite and such Machine Learning Models on a Raspberry Pi computer. You can further read about them by accessing their individual README.md file.\nGesture Controls\nImage Classification\nObject Detection\nObject Tracking\nHuman Following\nDownload the code and configure your Raspberry Pi\nI have created a bash script that installs all the packages / libraries required to run this code on your Raspberry Pi. The script also downloads the code of this repo along with ML models on your device automatically. Follow the instructions on the link given below to configure your Raspberry Pi:-\nhttps://helloworld.co.in/earthrover\nObject Detection\nThe code for this project is placed in a directory named 'object_detection' inside the 'earthrover' directory The ML model used in this project is placed inside 'all_models' directory.\nThe robot can spy on a particular object and provide an alarm on a remote Web Control panel whenever the selected object appears in the frame.\nObject Tracking\nThe code for this project is placed in a directory named 'object_tracking' inside the 'earthrover' directory The ML model used in this project is placed inside 'all_models' directory.\nRobot is made to track a ball and follow it. You can see the robot's camera view on a browser while it is tracking the ball.\nHuman Following\nThe code for this project is placed in a directory named 'human_following' inside the 'earthrover' directory The ML model used in this project is placed inside 'all_models' directory.\nRobot is made to follow a human. It is a good human follower :)\nImage Classification\nThe code for this project is placed in a directory named 'image_classification' inside the 'earthrover' directory. The ML model used in this project is placed inside 'all_models' directory.\nThe robot's camera view is streamed over LAN with overlays of image classification output. Also, if an object is recognised, the robot speaks out its name.\nGesture Control\nThe code for this project is placed in a folder named 'tm' inside the 'earthrover' directory. The model used in this project is trained through Teachable Machine online tool by Google. The model files are present in the same directory. Presently the model is trained to recognise hand gestures. You can train your own model using Teachable Machine and replace the model files to customise the project.", "link": "https://github.com/jiteshsaini/robotics-level-4", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "visit website:\nyoutube channel:\nrobotics level 4\nthis repo is an extension of previous level. the code of this robot is organised in various folders inside the directory 'earthrover'. the names of these folders briefly indicate the purpose of the code inside them. this repo focusses on the advanced capabilities embedded into the robot via use of pre-trained machine learning models provided by \"tensorflow.org\" or created via online -----> tool !!!  of google called teachable machine. the following projects in this repo demonstrate how we can integrate tensorflow lite and such machine learning models on a raspberry pi computer. you can further read about them by accessing their individual readme.md file.\ngesture controls\nimage classification\nobject detection\nobject tracking\nhuman following\ndownload the code and configure your raspberry pi\ni have created a bash script that installs all the packages / libraries required to run this code on your raspberry pi. the script also downloads the code of this repo along with ml models on your device automatically. follow the instructions on the link given below to configure your raspberry pi:-\nhttps://helloworld.co.in/earthrover\nobject detection\nthe code for this project is placed in a directory named 'object_detection' inside the 'earthrover' directory the ml model used in this project is placed inside 'all_models' directory.\nthe robot can spy on a particular object and provide an alarm on a remote web control panel whenever the selected object appears in the frame.\nobject tracking\nthe code for this project is placed in a directory named 'object_tracking' inside the 'earthrover' directory the ml model used in this project is placed inside 'all_models' directory.\nrobot is made to track a ball and follow it. you can see the robot's camera view on a browser while it is tracking the ball.\nhuman following\nthe code for this project is placed in a directory named 'human_following' inside the 'earthrover' directory the ml model used in this project is placed inside 'all_models' directory.\nrobot is made to follow a human. it is a good human follower :)\nimage classification\nthe code for this project is placed in a directory named 'image_classification' inside the 'earthrover' directory. the ml model used in this project is placed inside 'all_models' directory.\nthe robot's camera view is streamed over lan with overlays of image classification output. also, if an object is recognised, the robot speaks out its name.\ngesture control\nthe code for this project is placed in a folder named 'tm' inside the 'earthrover' directory. the model used in this project is trained through teachable machine online tool by google. the model files are present in the same directory. presently the model is trained to recognise hand gestures. you can train your own model using teachable machine and replace the model files to customise the project.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000759, "year": null}, {"Unnamed: 0": 1789, "autor": 769, "date": null, "content": "yak\nyak (yet another kinfu) is a library and ROS wrapper for Truncated Signed Distance Fields (TSDFs).\nA TSDF is a probabilistic representations of a solid surface in 3D space. It's a useful tool for combining many noisy incomplete sensor readings into a single smooth and complete model.\nTo break down the name:\nDistance field: Each voxel in the volume contains a value that represents its metric distance from the closest point on the surface. Voxels very far from the surface have high-magnitude distance values, while those near the surface have values approaching zero.\nSigned: Voxels outside the surface have positive distances, while voxels inside the surface have negative distances. This allows the representation of solid objects. The distance field becomes a gradient that shifts from positive to negative as it crosses the surface.\nTruncated: Only the distance values of voxels very close to the surface are regularly updated. Distances beyond a certain threshold have their values capped at +/- 1. This decreases the cost of integrating new readings, since not every voxel in the volume needs to be updated.\nyak handles two very different use cases. It can reconstruct from a RGBD camera moved around by a human without any knowledge of pose relative to the global frame. It can also reconstruct from a sensor mounted on a robot arm using pose hints provided by TF and robot kinematics. The idea is that this second case doesn't need to deduce sensor motion by comparing the most recent reading to previous readings via ICP, so it should work better in situations with incomplete sensor readings.\nThe human-guided situation should work out of the box without any other packages. You might need to force the sensor to reset to get the volume positioned in a desirable orientation around your target. The easiest way to do this is to cover and uncover the camera lens.\nThe robot-assisted situation is currently partially hardcoded to use the sensors and work cell models from the Godel blending project. This will change soon to make it more generalized!\nyak_meshing\nyak_meshing is a ROS package to mesh TSDF volumes generated by Kinect Fusion-like packages.\nMeshing happens through the /get_mesh service, which in turn calls the kinfu_ros /get_tsdf service. yak_meshing_node expects a serialized TSDF voxel volume, which is a list of TSDF values and weights for every occupied voxel along with a list of the coordinates of each occupied voxel. OpenVDB's voxel meshing algorithm generates a triangular mesh along the zero-value isosurface of the TSDF volume. The mesh is saved as a .obj, which can be viewed and manipulated in a program like Meshlab or Blender.\nnbv_planner\nnbv_planner is a ROS package to perform Next Best View analysis using data provided from RGBD cameras like the Asus Xtion. It uses octomap to track voxel occupancy and integrate new readings.\nCall the /get_nbv service to return a sorted list (best to worst) of candidate poses near the volume that could expose unknown voxels. Currently evaluation of poses is conducted by casting rays corresponding to the camera's field of view into the octomap. More hits on unknowns = better view.\nExecuting rosrun nbv_planner exploration_controller_node will execute an exploration routine that will try to move the robot to views that expose unknown regions of a user-specified volume, using the NBV evaluation explained above. The octomap server should be running.\nOperating Instructions for Human-Guided Reconstruction\nStart TSDF/KinFu processes: roslaunch yak launch_xtion_default.launch\nLaunch the drivers for the RGBD camera. For the Asus Xtion, this is roslaunch openni2_launch openni2.launch.\nStart mapping! Since yak doesn't have any way to relate the pose of the camera to the global frame, the initial position of the volume will be centered in front of the camera. You might have to force the camera to reset a few times to get the volume positioned where you need it.\nWhen you decide that the reconstruction is good enough: rosservice call /get_mesh\nOperating Instructions for Autonomous Exploration and Reconstruction\nIf you intend to use an actual robot, make sure that its state and motion servers are running, and that autonomous motion is allowed (deadman switch engaged, or auto mode).\nroslaunch a moveit planning/execution launch file. My command looks like: roslaunch godel_irb2400_moveit_config moveit_planning_execution.launch robot_ip:=192.168.125.1 sim:=False use_ftp:=False. Wait for rviz and moveit to start up.\nLaunch the TSDF reconstruction nodes. For example, roslaunch yak launch_xtion_robot.launch.\nLaunch the drivers for the RGBD camera. For the Asus Xtion, this is roslaunch openni2_launch openni2.launch.\nStart the octomap server: roslaunch nbv_planner octomap_mapping.launch\nWhen you want to start exploration: rosrun nbv_planner exploration_controller_node\nWhen you decide that the reconstruction is good enough: rosservice call /get_mesh\nBuild with Docker:\nnvidia-docker run -v \"<absolute path to your yak workspace:/yak_ws>\" rosindustrial/yak:kinetic catkin build --workspace /yak_ws -DCMAKE_LIBRARY_PATH=/usr/local/nvidia/lib64/", "link": "https://github.com/AustinDeric/yak", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "yak\nyak (yet another kinfu) is a library and ros wrapper for truncated signed distance fields (tsdfs).\na tsdf is a probabilistic representations of a solid surface in 3d space. it's a useful -----> tool !!!  for combining many noisy incomplete sensor readings into a single smooth and complete model.\nto break down the name:\ndistance field: each voxel in the volume contains a value that represents its metric distance from the closest point on the surface. voxels very far from the surface have high-magnitude distance values, while those near the surface have values approaching zero.\nsigned: voxels outside the surface have positive distances, while voxels inside the surface have negative distances. this allows the representation of solid objects. the distance field becomes a gradient that shifts from positive to negative as it crosses the surface.\ntruncated: only the distance values of voxels very close to the surface are regularly updated. distances beyond a certain threshold have their values capped at +/- 1. this decreases the cost of integrating new readings, since not every voxel in the volume needs to be updated.\nyak handles two very different use cases. it can reconstruct from a rgbd camera moved around by a human without any knowledge of pose relative to the global frame. it can also reconstruct from a sensor mounted on a robot arm using pose hints provided by tf and robot kinematics. the idea is that this second case doesn't need to deduce sensor motion by comparing the most recent reading to previous readings via icp, so it should work better in situations with incomplete sensor readings.\nthe human-guided situation should work out of the box without any other packages. you might need to force the sensor to reset to get the volume positioned in a desirable orientation around your target. the easiest way to do this is to cover and uncover the camera lens.\nthe robot-assisted situation is currently partially hardcoded to use the sensors and work cell models from the godel blending project. this will change soon to make it more generalized!\nyak_meshing\nyak_meshing is a ros package to mesh tsdf volumes generated by kinect fusion-like packages.\nmeshing happens through the /get_mesh service, which in turn calls the kinfu_ros /get_tsdf service. yak_meshing_node expects a serialized tsdf voxel volume, which is a list of tsdf values and weights for every occupied voxel along with a list of the coordinates of each occupied voxel. openvdb's voxel meshing algorithm generates a triangular mesh along the zero-value isosurface of the tsdf volume. the mesh is saved as a .obj, which can be viewed and manipulated in a program like meshlab or blender.\nnbv_planner\nnbv_planner is a ros package to perform next best view analysis using data provided from rgbd cameras like the asus xtion. it uses octomap to track voxel occupancy and integrate new readings.\ncall the /get_nbv service to return a sorted list (best to worst) of candidate poses near the volume that could expose unknown voxels. currently evaluation of poses is conducted by casting rays corresponding to the camera's field of view into the octomap. more hits on unknowns = better view.\nexecuting rosrun nbv_planner exploration_controller_node will execute an exploration routine that will try to move the robot to views that expose unknown regions of a user-specified volume, using the nbv evaluation explained above. the octomap server should be running.\noperating instructions for human-guided reconstruction\nstart tsdf/kinfu processes: roslaunch yak launch_xtion_default.launch\nlaunch the drivers for the rgbd camera. for the asus xtion, this is roslaunch openni2_launch openni2.launch.\nstart mapping! since yak doesn't have any way to relate the pose of the camera to the global frame, the initial position of the volume will be centered in front of the camera. you might have to force the camera to reset a few times to get the volume positioned where you need it.\nwhen you decide that the reconstruction is good enough: rosservice call /get_mesh\noperating instructions for autonomous exploration and reconstruction\nif you intend to use an actual robot, make sure that its state and motion servers are running, and that autonomous motion is allowed (deadman switch engaged, or auto mode).\nroslaunch a moveit planning/execution launch file. my command looks like: roslaunch godel_irb2400_moveit_config moveit_planning_execution.launch robot_ip:=192.168.125.1 sim:=false use_ftp:=false. wait for rviz and moveit to start up.\nlaunch the tsdf reconstruction nodes. for example, roslaunch yak launch_xtion_robot.launch.\nlaunch the drivers for the rgbd camera. for the asus xtion, this is roslaunch openni2_launch openni2.launch.\nstart the octomap server: roslaunch nbv_planner octomap_mapping.launch\nwhen you want to start exploration: rosrun nbv_planner exploration_controller_node\nwhen you decide that the reconstruction is good enough: rosservice call /get_mesh\nbuild with docker:\nnvidia-docker run -v \"<absolute path to your yak workspace:/yak_ws>\" rosindustrial/yak:kinetic catkin build --workspace /yak_ws -dcmake_library_path=/usr/local/nvidia/lib64/", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000769, "year": null}, {"Unnamed: 0": 1795, "autor": 775, "date": null, "content": "Puma 762 Robot Simulation\nThis is a Simulation of a Puma 762 manipulator capable of solving the Forward and Inverse Kinematics problems.\nThe Code can also be found in Matlab File-Exchange and is based on '3D Puma Robot Demo' from Don Riley. The work provides a general analysis of the PUMA 762 kinematics and their solution methodology. There is also a description of the Graphical User Interface (GUI) used, its functions and the code developed to further enhance its capabilities and functions. Some changes were made to the functions so as to give a more complete solution for the inverse kinematics problem (position and orientation). The GUI has been enhanced with some new features. An 'Inverse Kinematics' panel for IK variables input, Radiobuttons for viewing different solutions, a 'D-H parameters' table and a 'Transformation matrix' table. The 'Demo' button has been changed to 'Draw' with some new 'drawings' available.\nThe PUMA 762 Manipulator\nThe PUMA 762 Robotic Manipulator is one of the most widely spread robots, used in production lines, mainly in automatic welding applications, but also in many university laboratories around the world.\nThe manipulator simulates the human arm, imitating the left or right side of the human body (torso, shoulder, elbow, and wrist) according to its setup. Originally developed for General Motors, it was based on drawings by Scheinman, while at Stanford University.\nThe six joints of the robot are revolute with their limitations are illustrated in Figure 1. The joints are properly positioned to give the arm six degrees of freedom. His workplace is roughly made up of a sphere with a radius of 1.25 meters (Figure 2).\nThe end-effector of the arm is a Spherical wrist which means that the last three axes intersect at a common point (Figure 3). This is a sufficient condition for a six-rotary operator to have a closed-form solution. We will see the importance of this feature in the next section.\nRobot Kinematics\nThe kinematics problem in robots is divided into two separate problems. The Forward Kinematic Problem and the Inverse Kinematic Problem.\nThe Forward Kinematic Problem is defined as follows:\nGiven all the variables (angles or displacements) of the joints, we want to calculate the position and direction of the end-effector's frame relative to the base frame.\nThe Inverse Kinematic Problem is defined as follows:\nGiven the position and direction of the robots' end-effector, calculate all possible sets of joint variables (angles or displacements) with which the end-effector could reach the given position.\nAn illustration of the two kinematics problems is given in Figure 4\nThe most common method used for solving the kinematics problems is that of Denavit & Hartenberg (1955) which uses homogeneous 4x4 orthogonal transforms.\nFor the D-H method, an axis system is attached to each joint (i) with the zi axis pointing to the axis of rotation for revolute joints or to the axis of translation for prismatic joints. The xi, point along the common normal between zi and zi-1 and it's perpendicular to it. The yi axis is positioned to satisfy the right-hand rule.\nDenavit & Hartenberg have shown that only four parameters are needed to find the displacement of the arm.\nThese four parameters are as follows:\n\u03b8i is the joint angle from the xi-1 axis to the xi about the zi-1 axis (using the right-hand rule)\ndi is the distance from the origin of the (i\u22121)th coordinate frame to the intersection of the zi-1 axis with the xi-1 axis along the zi-1 axis.\nai is the distance from the intersection of the zi-1 axis with the xi axis to the origin of the ith frame along the xi axis.\n\u03b1i is the angle from the zi-1 axis to the zi axis about the xi axis\nSolving the Forward Kinematics Problem\nTo solve the direct kinematics problem using the Denavit & Hartenberg method we first need to place the axes on the joints of the arm, making sure that the rules mentioned above are observed. This ends up with the axes as shown in Figure 5.\nThe Denavit-Hartenberg parameters of PUMA 762 are shown in Table 3.\nThe next step requires the creation of the individual homogeneous transformations. The transformation matrices are given by the equeation (1) i-1iT = Rx (\u03b1i-1) * Dx (ai-1) * Rz (\u03b8i) * Qi (di). Where Rx, Rz denote the rotation around the x and z axes respectively, while Dx and Q denote the translation along the x and z axes respectively. The operation is as follows:\nHaving calculated the matrices 01T, 12T, 23T, 34T, 45T, 56T we can find the position and direction of any joint of the robot relative to the reference frame (usually the robot's base) but also to any other frame by multiplying the matrices from the reference frame to the requested link. The position of the end-effector of the arm relative to the reference frame is given by the table 06T = 01T * 12T * 23T * 34T * 45T * 56T. So the final solution is given by the table 2\nIn the table T (Figure 6), the elements of the last column p are the Cartesian position of the end-effector relative to the basic coordinate frame. The elements of columns 1 and 3 a and n give the normalized orientation vectors. The elements of the 2nd column s are the products of the normalized orientation vectors a and n and therefore contain no further information.\nSolving the Inverse Kinematics Problem\nWhile the Forward kinematics problem always has a unique solution, the Inverse Kinematics problem is more difficult to solve as it may have one or more solutions or none at all. The reason for this is the nonlinearity of the problem. A prerequisite for the existence of a solution is for the desired location has to be within the robot's workplace.\nThe solution of the inverse kinematics problem is mainly done by 2 methods, numerical and analytical (closed form) which is in turn divided into 2 sub-approaches, geometric and algebraic. In this work we used the geometric approach. This choice was made because this approach gives clear indications of which is the optimal solution for each layout of the robot, as opposed to the algebraic approach that usually requires experience by the operator and involves more calculations. The closed-form solution allows us to develop rules for choosing one particular solution among many.\nKinematic Decoupling\nSolving the Inverse Problem can be simplified for arms with 6 degrees of freedom, with the last 3 links intersecting at the same point. In these cases it is possible to use kinematic decoupling, dividing the original problem into two simpler problems. These two sub-problems are called Inverse Position Kinematics and Inverse Orientation Kinematic. The first one calculates the position of the intersection of the wrist axes and the second the orientation of the wrist. This separation simplifies the calculations. A basic requirement for the application of kinematic decoupling is that the wrist of the arm is spherical, that is to say, its three axes intersect at the same point. The arrangement of the spherical wrist whose axes z3, z4, z5 intersect at a common point is shown in Figure 7.\nFor a Puma arm, there are four possible solutions for the first three joints. Each of these solutions has two possible solutions for the last three links, as shown in Figures 8 and 9. In order to be able to choose the desired of these solutions, we introduce some indicators depending on the setup of the arm, which are translated into signs (+ or -) for the equations. These indicators are determined according to the arrangement of the members (hands) of the human body, and in particular, the position of the shoulder, the elbow and the wrist, and are as follows.\nShoulder Right: +1 | Elbow over wrist : +1 | Wrist down: +1\nShoulder Left: -1 | Elbow under wrist: -1 | Wrist up : -1\nThe Graphical User Interface. Additions, Changes, Improvements\nSeveral changes were made to the original code, increasing its functionality. The main ones are\nAbility to fully solve the inverse kinematics problem,\nDisplay Denavit-Hartenberg parameters\nDisplay the final 06T transform matrix\nEnhance graphical interface capabilities\nA breakdown of the changes and additions is given below.\nInverse Kinematics Solver\nIn the original code, there was a function called PumaIK, with a limited ability to solve the inverse kinematics problem. The solution was incomplete as only Inverse Position Kinematics were resolved and no solution was provided for Inverse Orientation Kinematics. Also, at most one solution to the inverse problem was returned without the option of changing it. Finally, the user could use this function. Extensive changes were made to the PumaIK code. Existing equations were corrected and those missing were added to match those developed in the previous section. Functions were also added to find more solutions.\nThe Inverse Kinematics table was added to the graphical interface so that the user could enter the position and orientation values of the desired end position. It is possible to input the nine variables (Px, Py, Pz, r11, r21, r31, r13, r23, r33) needed to find the final position of the endpoint in the Inverse Kinematics table. The input can be done in two ways, by entering the value in the corresponding field or by changing the slider bar as in the Forward Kinematics table. To the left and right of each slider bar are the minimum and maximum values that each variable can receive and each input is controlled by the check_edit function. Below the input fields, there are radio buttons where the user can choose which solution to the problem they prefer. Specifically, the first radiobutton gives the option of preferring the solution where the elbow is up or down in the final arm assembly with the elbow up being the default device. The second radiobutton offers the choice of rotating or not the end element.\nDisplay Denavit-Hartenberg parameters and final 06T transform matrix\nAnother important addition is the ability to display the Denavit-Hartenberg parameter and especially the final 06T transform matrix. Although their role is auxiliary, they give an educational tone and complement the graphical environment.\nThe Denavit-Hartenberg parameter table displays the parameters of each link of the robot. In the Transformation matrix table, we can see all the values of the homogeneous 06T transformation. Many times these values can be used to verify and correlate the solutions of the two kinematics problems, but not always as the inverse solution is not always unique.\nA button (DH&T matrix) was implemented in the graphical interface to show or hide these two tables.\nEnhance graphical interface capabilities\nAnother change made to the GUI is replacement the Demo button with the Draw button. This is not just a name change. The name change was made to highlight its functionality. Right next to it a drop-down menu was added to allow the user to select one of the five different drawings. Of course, besides the name and addition of the menu, there was a big change in the code. So with the push of the Draw button allocated function checks which of the drawings is selected and proceeds to its design.\nFor each design, there is a matrix of values in the 3D space of each point that constitutes it. These matrices were created with the help of GeoGebra software. The function takes these points one by one and passes them to the PumaIK function. In turn, PumaIK calculates the angles that the robot links need to form so that it can be found at that point and then sends them to the pumaANI function with the option 'track trace' enabled. Thus each point of the drawing is highlighted in the 3D space with a blue dot whose whole constitutes the final drawing.\nOther minor changes include the addition of a grid to the axis of the arm to facilitate the verification of the right position, but also the activation of the graphical interface window menu whose different tools make it easy to study the robot's motion. For example, the \"Rotate 3D\" tool allows the axes to rotate giving a better perception of the robot even from angles that initially are considered \"blind spots\". Finally, appropriate changes were made to make the graphical interface look the same on all systems, regardless of the size of the screen on which it is displayed.\nConclusion\nIn this project, we managed to create a graphical interface that allows the user to study the Forward and the Inverse Kinematics Problem for one of the most widely used robotic arms. Using the foundations that Don Riley had put in the original code, we took the project one step further, paving the way for further improvements to the utility of the code.\nBibliography\nCraig, J. J. (1985). Introduction to Robotics, Mechanics and Control. 3rd Ed.\nCubero, S. (2007). Industrial Robotics: Theory, Modelling and Control.\nDolinsky, J.-U. (2001). The Development of a Genetic Programming Method for Kinematic Robot Calibration.\nRobot Manipulators. Position, Orientation and Coordinate Transformations. (n.d.).\nSpong, M. W., & Vidyasagar, M. (1989). Robot Dynamics and Control 2nd Ed.\nUnimation. (1986). PUMA_761_762_Equipment_Manual.\nZiegler, C. s. (1983). Geometric Approach in Solving Inverse Kinematics of PUMA Robots.\nEmiris, D. (2004). \u03a1\u03bf\u03bc\u03c0\u03bf\u03c4\u03b9\u03ba\u03ae.\nBoutalis, \u0399. (2014). \u0394\u03b9\u03b1\u03bb\u03ad\u03be\u03b5\u03b9\u03c2 \u03a1\u03bf\u03bc\u03c0\u03bf\u03c4\u03b9\u03ba\u03ae\u03c2.", "link": "https://github.com/PascPeli/Puma-Robot-Simulation", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "puma 762 robot simulation\nthis is a simulation of a puma 762 manipulator capable of solving the forward and inverse kinematics problems.\nthe code can also be found in matlab file-exchange and is based on '3d puma robot demo' from don riley. the work provides a general analysis of the puma 762 kinematics and their solution methodology. there is also a description of the graphical user interface (gui) used, its functions and the code developed to further enhance its capabilities and functions. some changes were made to the functions so as to give a more complete solution for the inverse kinematics problem (position and orientation). the gui has been enhanced with some new features. an 'inverse kinematics' panel for ik variables input, radiobuttons for viewing different solutions, a 'd-h parameters' table and a 'transformation matrix' table. the 'demo' button has been changed to 'draw' with some new 'drawings' available.\nthe puma 762 manipulator\nthe puma 762 robotic manipulator is one of the most widely spread robots, used in production lines, mainly in automatic welding applications, but also in many university laboratories around the world.\nthe manipulator simulates the human arm, imitating the left or right side of the human body (torso, shoulder, elbow, and wrist) according to its setup. originally developed for general motors, it was based on drawings by scheinman, while at stanford university.\nthe six joints of the robot are revolute with their limitations are illustrated in figure 1. the joints are properly positioned to give the arm six degrees of freedom. his workplace is roughly made up of a sphere with a radius of 1.25 meters (figure 2).\nthe end-effector of the arm is a spherical wrist which means that the last three axes intersect at a common point (figure 3). this is a sufficient condition for a six-rotary operator to have a closed-form solution. we will see the importance of this feature in the next section.\nrobot kinematics\nthe kinematics problem in robots is divided into two separate problems. the forward kinematic problem and the inverse kinematic problem.\nthe forward kinematic problem is defined as follows:\ngiven all the variables (angles or displacements) of the joints, we want to calculate the position and direction of the end-effector's frame relative to the base frame.\nthe inverse kinematic problem is defined as follows:\ngiven the position and direction of the robots' end-effector, calculate all possible sets of joint variables (angles or displacements) with which the end-effector could reach the given position.\nan illustration of the two kinematics problems is given in figure 4\nthe most common method used for solving the kinematics problems is that of denavit & hartenberg (1955) which uses homogeneous 4x4 orthogonal transforms.\nfor the d-h method, an axis system is attached to each joint (i) with the zi axis pointing to the axis of rotation for revolute joints or to the axis of translation for prismatic joints. the xi, point along the common normal between zi and zi-1 and it's perpendicular to it. the yi axis is positioned to satisfy the right-hand rule.\ndenavit & hartenberg have shown that only four parameters are needed to find the displacement of the arm.\nthese four parameters are as follows:\n\u03b8i is the joint angle from the xi-1 axis to the xi about the zi-1 axis (using the right-hand rule)\ndi is the distance from the origin of the (i\u22121)th coordinate frame to the intersection of the zi-1 axis with the xi-1 axis along the zi-1 axis.\nai is the distance from the intersection of the zi-1 axis with the xi axis to the origin of the ith frame along the xi axis.\n\u03b1i is the angle from the zi-1 axis to the zi axis about the xi axis\nsolving the forward kinematics problem\nto solve the direct kinematics problem using the denavit & hartenberg method we first need to place the axes on the joints of the arm, making sure that the rules mentioned above are observed. this ends up with the axes as shown in figure 5.\nthe denavit-hartenberg parameters of puma 762 are shown in table 3.\nthe next step requires the creation of the individual homogeneous transformations. the transformation matrices are given by the equeation (1) i-1it = rx (\u03b1i-1) * dx (ai-1) * rz (\u03b8i) * qi (di). where rx, rz denote the rotation around the x and z axes respectively, while dx and q denote the translation along the x and z axes respectively. the operation is as follows:\nhaving calculated the matrices 01t, 12t, 23t, 34t, 45t, 56t we can find the position and direction of any joint of the robot relative to the reference frame (usually the robot's base) but also to any other frame by multiplying the matrices from the reference frame to the requested link. the position of the end-effector of the arm relative to the reference frame is given by the table 06t = 01t * 12t * 23t * 34t * 45t * 56t. so the final solution is given by the table 2\nin the table t (figure 6), the elements of the last column p are the cartesian position of the end-effector relative to the basic coordinate frame. the elements of columns 1 and 3 a and n give the normalized orientation vectors. the elements of the 2nd column s are the products of the normalized orientation vectors a and n and therefore contain no further information.\nsolving the inverse kinematics problem\nwhile the forward kinematics problem always has a unique solution, the inverse kinematics problem is more difficult to solve as it may have one or more solutions or none at all. the reason for this is the nonlinearity of the problem. a prerequisite for the existence of a solution is for the desired location has to be within the robot's workplace.\nthe solution of the inverse kinematics problem is mainly done by 2 methods, numerical and analytical (closed form) which is in turn divided into 2 sub-approaches, geometric and algebraic. in this work we used the geometric approach. this choice was made because this approach gives clear indications of which is the optimal solution for each layout of the robot, as opposed to the algebraic approach that usually requires experience by the operator and involves more calculations. the closed-form solution allows us to develop rules for choosing one particular solution among many.\nkinematic decoupling\nsolving the inverse problem can be simplified for arms with 6 degrees of freedom, with the last 3 links intersecting at the same point. in these cases it is possible to use kinematic decoupling, dividing the original problem into two simpler problems. these two sub-problems are called inverse position kinematics and inverse orientation kinematic. the first one calculates the position of the intersection of the wrist axes and the second the orientation of the wrist. this separation simplifies the calculations. a basic requirement for the application of kinematic decoupling is that the wrist of the arm is spherical, that is to say, its three axes intersect at the same point. the arrangement of the spherical wrist whose axes z3, z4, z5 intersect at a common point is shown in figure 7.\nfor a puma arm, there are four possible solutions for the first three joints. each of these solutions has two possible solutions for the last three links, as shown in figures 8 and 9. in order to be able to choose the desired of these solutions, we introduce some indicators depending on the setup of the arm, which are translated into signs (+ or -) for the equations. these indicators are determined according to the arrangement of the members (hands) of the human body, and in particular, the position of the shoulder, the elbow and the wrist, and are as follows.\nshoulder right: +1 | elbow over wrist : +1 | wrist down: +1\nshoulder left: -1 | elbow under wrist: -1 | wrist up : -1\nthe graphical user interface. additions, changes, improvements\nseveral changes were made to the original code, increasing its functionality. the main ones are\nability to fully solve the inverse kinematics problem,\ndisplay denavit-hartenberg parameters\ndisplay the final 06t transform matrix\nenhance graphical interface capabilities\na breakdown of the changes and additions is given below.\ninverse kinematics solver\nin the original code, there was a function called pumaik, with a limited ability to solve the inverse kinematics problem. the solution was incomplete as only inverse position kinematics were resolved and no solution was provided for inverse orientation kinematics. also, at most one solution to the inverse problem was returned without the option of changing it. finally, the user could use this function. extensive changes were made to the pumaik code. existing equations were corrected and those missing were added to match those developed in the previous section. functions were also added to find more solutions.\nthe inverse kinematics table was added to the graphical interface so that the user could enter the position and orientation values of the desired end position. it is possible to input the nine variables (px, py, pz, r11, r21, r31, r13, r23, r33) needed to find the final position of the endpoint in the inverse kinematics table. the input can be done in two ways, by entering the value in the corresponding field or by changing the slider bar as in the forward kinematics table. to the left and right of each slider bar are the minimum and maximum values that each variable can receive and each input is controlled by the check_edit function. below the input fields, there are radio buttons where the user can choose which solution to the problem they prefer. specifically, the first radiobutton gives the option of preferring the solution where the elbow is up or down in the final arm assembly with the elbow up being the default device. the second radiobutton offers the choice of rotating or not the end element.\ndisplay denavit-hartenberg parameters and final 06t transform matrix\nanother important addition is the ability to display the denavit-hartenberg parameter and especially the final 06t transform matrix. although their role is auxiliary, they give an educational tone and complement the graphical environment.\nthe denavit-hartenberg parameter table displays the parameters of each link of the robot. in the transformation matrix table, we can see all the values of the homogeneous 06t transformation. many times these values can be used to verify and correlate the solutions of the two kinematics problems, but not always as the inverse solution is not always unique.\na button (dh&t matrix) was implemented in the graphical interface to show or hide these two tables.\nenhance graphical interface capabilities\nanother change made to the gui is replacement the demo button with the draw button. this is not just a name change. the name change was made to highlight its functionality. right next to it a drop-down menu was added to allow the user to select one of the five different drawings. of course, besides the name and addition of the menu, there was a big change in the code. so with the push of the draw button allocated function checks which of the drawings is selected and proceeds to its design.\nfor each design, there is a matrix of values in the 3d space of each point that constitutes it. these matrices were created with the help of geogebra software. the function takes these points one by one and passes them to the pumaik function. in turn, pumaik calculates the angles that the robot links need to form so that it can be found at that point and then sends them to the pumaani function with the option 'track trace' enabled. thus each point of the drawing is highlighted in the 3d space with a blue dot whose whole constitutes the final drawing.\nother minor changes include the addition of a grid to the axis of the arm to facilitate the verification of the right position, but also the activation of the graphical interface window menu whose different tools make it easy to study the robot's motion. for example, the \"rotate 3d\" -----> tool !!!  allows the axes to rotate giving a better perception of the robot even from angles that initially are considered \"blind spots\". finally, appropriate changes were made to make the graphical interface look the same on all systems, regardless of the size of the screen on which it is displayed.\nconclusion\nin this project, we managed to create a graphical interface that allows the user to study the forward and the inverse kinematics problem for one of the most widely used robotic arms. using the foundations that don riley had put in the original code, we took the project one step further, paving the way for further improvements to the utility of the code.\nbibliography\ncraig, j. j. (1985). introduction to robotics, mechanics and control. 3rd ed.\ncubero, s. (2007). industrial robotics: theory, modelling and control.\ndolinsky, j.-u. (2001). the development of a genetic programming method for kinematic robot calibration.\nrobot manipulators. position, orientation and coordinate transformations. (n.d.).\nspong, m. w., & vidyasagar, m. (1989). robot dynamics and control 2nd ed.\nunimation. (1986). puma_761_762_equipment_manual.\nziegler, c. s. (1983). geometric approach in solving inverse kinematics of puma robots.\nemiris, d. (2004). \u03c1\u03bf\u03bc\u03c0\u03bf\u03c4\u03b9\u03ba\u03ae.\nboutalis, \u03b9. (2014). \u03b4\u03b9\u03b1\u03bb\u03ad\u03be\u03b5\u03b9\u03c2 \u03c1\u03bf\u03bc\u03c0\u03bf\u03c4\u03b9\u03ba\u03ae\u03c2.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000775, "year": null}, {"Unnamed: 0": 1810, "autor": 790, "date": null, "content": "Content\nGoals\nDemo\nTasks\nPriority 1\nPriority 2\nFuture\nGoals\nWhile current ROS GUI/CLI tools exist to monitor your robots they require full access to the robot network and force you to have ROS installed on the system you are monitoring from. This means that monitoring a ROS system remotely is almost impossible.\nBy utilizing rosbridge-suite jviz can be run from any browser on any device with full access to the ROS system.\nFreedom : Connect and monitor your ROS robot from any device and any location.\nEasy : Easy to use means that lowers the bar for new developers getting into robotics.\nExtendable : It should be as easy as possible for new modules to be added to the interface to support additional features.\nDemo\nThere is a live version of this running on github pages.\nDemo\nAs github only support secure sites (https) you will need to make sure that your rosbridge server is running in secure mode. This ros answers question contains a good starting point for enabling wss with rosbridge.\nTasks\nThe immediate focus is to match the functionality of the rostopic tool. rostopic echo and rostopic pub\nPriority 1\nSubscriber\nFormat received messages\nSteady scroll\nPublisher\nChoose topic\nFill message\nAuto fill header\nSend frequency\nNode Graph\nNodes\nEdges\nNamespaces\nPerformance\nPriority 2\nImage viewer\nTheme\nDocker image\nSubscriber+\nMessage filter\nSubscribe to sub-fields\nService clients\nAction clients\nSaving dashboards locally\nFuture\nNative apps\nUser accounts to store dashboards, logging over time, etc", "link": "https://github.com/MJohnson459/jviz", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "content\ngoals\ndemo\ntasks\npriority 1\npriority 2\nfuture\ngoals\nwhile current ros gui/cli tools exist to monitor your robots they require full access to the robot network and force you to have ros installed on the system you are monitoring from. this means that monitoring a ros system remotely is almost impossible.\nby utilizing rosbridge-suite jviz can be run from any browser on any device with full access to the ros system.\nfreedom : connect and monitor your ros robot from any device and any location.\neasy : easy to use means that lowers the bar for new developers getting into robotics.\nextendable : it should be as easy as possible for new modules to be added to the interface to support additional features.\ndemo\nthere is a live version of this running on github pages.\ndemo\nas github only support secure sites (https) you will need to make sure that your rosbridge server is running in secure mode. this ros answers question contains a good starting point for enabling wss with rosbridge.\ntasks\nthe immediate focus is to match the functionality of the rostopic -----> tool !!! . rostopic echo and rostopic pub\npriority 1\nsubscriber\nformat received messages\nsteady scroll\npublisher\nchoose topic\nfill message\nauto fill header\nsend frequency\nnode graph\nnodes\nedges\nnamespaces\nperformance\npriority 2\nimage viewer\ntheme\ndocker image\nsubscriber+\nmessage filter\nsubscribe to sub-fields\nservice clients\naction clients\nsaving dashboards locally\nfuture\nnative apps\nuser accounts to store dashboards, logging over time, etc", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000790, "year": null}, {"Unnamed: 0": 1816, "autor": 796, "date": null, "content": "OpenGrab EPM v3\nOpenGrab EPM v3 is an electropermanent magnet, combining the advantages of electro and permanent magnets. It has been developed by NicaDrone in collaboration with Zubax Robotics.\nUseful links:\nPRODUCT DOCUMENTATION\nHARDWARE SOURCES\nSUPPORT FORUM\nBuilding the firmware\nPrerequisites\nPython 2.7+ or Python 3.2+\nARM GCC Embedded 4.9+\nBuilding\nAfter cloning this repository, execute the following:\ngit submodule update --init --recursive\ncd firmware\nmake -j8\nThe build outputs will be available in the directory build/.\nFlashing the firmware\nUseful info\nIf you do not cycle the EPM you cannot get zapped.\nDon't zap your computer it will not survive.\nHV section to avoid touching during debug, see fig. 1.\nSolder Jumper settings see fig. 1 and 2.\nV3 Rev 4B and earlier\nV3 Rev 5C\nVia UART\nConnecting USB-UART adapter\nFTDI cable is the recommended USB-UART adapter. 5 V and 3.3 V FTDI cables work fine, 3.3 V is recommended. Use the following pinout when connecting:\nWire Signal\nOrange RXD\nYellow TXD\nBlack GND\nFlashing with NXP Flash Magic (simplest method)\nGet the flashing tool from http://www.flashmagictool.com/. Start the tool, then configure it as follows:\nParameter Value\nSelect Device LPC11C24/301\nBaud rate 115200\nInterface ISP\nOscilator 12 MHz\nFile Prebuilt .hex available here **\nConnect the USB-UART adapter.\nClose J3 to select serial bootloader.\nClose J4 to force the LPC to start the bootloader.\nPower up the board.\nRun the flashing tool.\n** .hex is also created in the build output directory firmware/build or a .hex file can be created from .bin:\narm-none-eabi-objcopy -I binary -O ihex firmware.bin out.hex\nVia DroneCode Probe\nDroneCode Probe is a generic JTAG / SWD + UART console adapter compatible with most ARM Cortex based designs and in particular with hardware maintained by the DroneCode project.\nIn order to flash the board using this tool, simply connect the debugger and execute the script blackmagic_flash.sh.\nVia 8devices USB2CAN and embedded CAN bootloader\nNote to Windows users: In order to use USB2CAN in a VM the drivers have to be installed on the host and driver signature enforcement has to be disabled in some versions of Windows.\nRegister the CAN interface with SocketCAN:\nsudo ip link set can0 up type can bitrate 100000 sample-point 0.875\nUploading binary using LPC11 embedded CAN bootloader:\ntools/drwatson/lpc11c00_can_bootloader.py can0 firmware/build/firmware.bin\nLicenses\nFirmware\nCopyright (C) 2015 Zubax Robotics info@zubax.com\nThis program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\nYou should have received a copy of the GNU General Public License along with this program. If not, see http://www.gnu.org/licenses/.\nHardware\nThe hardware sources are distributed under the terms of CC BY-SA 4.0.", "link": "https://github.com/Zubax/opengrab_epm_v3", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "opengrab epm v3\nopengrab epm v3 is an electropermanent magnet, combining the advantages of electro and permanent magnets. it has been developed by nicadrone in collaboration with zubax robotics.\nuseful links:\nproduct documentation\nhardware sources\nsupport forum\nbuilding the firmware\nprerequisites\npython 2.7+ or python 3.2+\narm gcc embedded 4.9+\nbuilding\nafter cloning this repository, execute the following:\ngit submodule update --init --recursive\ncd firmware\nmake -j8\nthe build outputs will be available in the directory build/.\nflashing the firmware\nuseful info\nif you do not cycle the epm you cannot get zapped.\ndon't zap your computer it will not survive.\nhv section to avoid touching during debug, see fig. 1.\nsolder jumper settings see fig. 1 and 2.\nv3 rev 4b and earlier\nv3 rev 5c\nvia uart\nconnecting usb-uart adapter\nftdi cable is the recommended usb-uart adapter. 5 v and 3.3 v ftdi cables work fine, 3.3 v is recommended. use the following pinout when connecting:\nwire signal\norange rxd\nyellow txd\nblack gnd\nflashing with nxp flash magic (simplest method)\nget the flashing -----> tool !!!  from http://www.flashmagictool.com/. start the -----> tool !!! , then configure it as follows:\nparameter value\nselect device lpc11c24/301\nbaud rate 115200\ninterface isp\noscilator 12 mhz\nfile prebuilt .hex available here **\nconnect the usb-uart adapter.\nclose j3 to select serial bootloader.\nclose j4 to force the lpc to start the bootloader.\npower up the board.\nrun the flashing tool.\n** .hex is also created in the build output directory firmware/build or a .hex file can be created from .bin:\narm-none-eabi-objcopy -i binary -o ihex firmware.bin out.hex\nvia dronecode probe\ndronecode probe is a generic jtag / swd + uart console adapter compatible with most arm cortex based designs and in particular with hardware maintained by the dronecode project.\nin order to flash the board using this tool, simply connect the debugger and execute the script blackmagic_flash.sh.\nvia 8devices usb2can and embedded can bootloader\nnote to windows users: in order to use usb2can in a vm the drivers have to be installed on the host and driver signature enforcement has to be disabled in some versions of windows.\nregister the can interface with socketcan:\nsudo ip link set can0 up type can bitrate 100000 sample-point 0.875\nuploading binary using lpc11 embedded can bootloader:\ntools/drwatson/lpc11c00_can_bootloader.py can0 firmware/build/firmware.bin\nlicenses\nfirmware\ncopyright (c) 2015 zubax robotics info@zubax.com\nthis program is free software: you can redistribute it and/or modify it under the terms of the gnu general public license as published by the free software foundation, either version 3 of the license, or (at your option) any later version.\nthis program is distributed in the hope that it will be useful, but without any warranty; without even the implied warranty of merchantability or fitness for a particular purpose. see the gnu general public license for more details.\nyou should have received a copy of the gnu general public license along with this program. if not, see http://www.gnu.org/licenses/.\nhardware\nthe hardware sources are distributed under the terms of cc by-sa 4.0.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000796, "year": null}, {"Unnamed: 0": 1846, "autor": 826, "date": null, "content": "Peg_in_hole_assembly\nOur work bases on openAI baselines code espically DDPG framework.\nThe automatic completion of multiple peg-in-hole assembly tasks by robots remains a formidable challenge because the traditional control strategies require a complex analysis of the contact model.\nWe proposed a model-driven deep deterministic policy gradient (MDDPG) algorithm is proposed to accomplish the assembly task through the learned policy without analyzing the contact states.\nTo improve the learning efficiency, we utilize a fuzzy reward system for the complex assembly process. Then, simulations and realistic experiments of a dual peg-in-hole assembly demonstrate the effectiveness of the proposed algorithm.\nOpenAI Baselines is a set of high-quality implementations of reinforcement learning algorithms.\nThese algorithms will make it easier for the research community to replicate, refine, and identify new ideas, and will create good baselines to build research on top of. Our DQN implementation and its variants are roughly on par with the scores in published papers. We expect they will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones.\nYou can install it by typing:\ngit clone https://github.com/hzm2016/Peg_in_hole_assembly\ncd baselines\npip install -e .\nDDPG\nDDPG\nPapers\nRL for assembly tasks paper: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8454796\nRelated paper: https://www.emeraldinsight.com/doi/pdfplus/10.1108/AA-03-2018-039\nFirst a simulation environment is setup to demonstrate the effectiveness.\nSimulate_main.py\nSecond a dual peg-in-hole assembly experiments has been done.\nExperiment_main.py", "link": "https://github.com/hzm2016/Peg_in_hole_assembly", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "peg_in_hole_assembly\nour work bases on openai baselines code espically ddpg framework.\nthe automatic completion of multiple peg-in-hole assembly tasks by robots remains a formidable challenge because the traditional control strategies require a complex analysis of the contact model.\nwe proposed a model-driven deep deterministic policy gradient (mddpg) algorithm is proposed to accomplish the assembly task through the learned policy without analyzing the contact states.\nto improve the learning efficiency, we utilize a fuzzy reward system for the complex assembly process. then, simulations and realistic experiments of a dual peg-in-hole assembly demonstrate the effectiveness of the proposed algorithm.\nopenai baselines is a set of high-quality implementations of reinforcement learning algorithms.\nthese algorithms will make it easier for the research community to replicate, refine, and identify new ideas, and will create good baselines to build research on top of. our dqn implementation and its variants are roughly on par with the scores in published papers. we expect they will be used as a base around which new ideas can be added, and as a -----> tool !!!  for comparing a new approach against existing ones.\nyou can install it by typing:\ngit clone https://github.com/hzm2016/peg_in_hole_assembly\ncd baselines\npip install -e .\nddpg\nddpg\npapers\nrl for assembly tasks paper: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8454796\nrelated paper: https://www.emeraldinsight.com/doi/pdfplus/10.1108/aa-03-2018-039\nfirst a simulation environment is setup to demonstrate the effectiveness.\nsimulate_main.py\nsecond a dual peg-in-hole assembly experiments has been done.\nexperiment_main.py", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000826, "year": null}, {"Unnamed: 0": 1848, "autor": 828, "date": null, "content": "Overview\nOpenPHRI: a generic framework to easily and safely control robots in interactions with humans\nMaster Integration\nFor a quick install of OpenPHRI, jump to the Standalone install section.\nFor an overview of the library, you can check the related Robotics and Automation Magazine article, but keep in mind that the library went through some changes since its writing and so some parts might not be up-to-date.\nYou can find the documentation (work in progress) about OpenPHRI in this Gitbook.\nDISCLAMER: version 1.0, the current release, has lots of improvements compared to initial release (0.x), but the Python bindings haven't been updated and so cannot be used. Since a version 2.0 with major usability improvements is on its way, the 1.0 will not be updated except for potential bugs fixes. Python bindings will be back with 2.0.\nThe license that applies to the whole package content is GNULGPL. Please look at the license.txt file at the root of this repository.\nInstallation and Usage\nThe detailed procedures for installing the open-phri package and for using its components is based on the PID build and deployment system called PID. Just follow and read the links to understand how to install, use and call its API and/or applications.\nFor a quick installation:\nInstalling the project into an existing PID workspace\nTo get last version :\ncd <path to pid workspace>/pid\nmake deploy package=open-phri\nTo get a specific version of the package :\ncd <path to pid workspace>/pid\nmake deploy package=open-phri version=<version number>\nStandalone install\ngit clone https://github.com/BenjaminNavarro/open-phri.git\ncd open-phri\nThen run the adequate install script depending on your system. For instance on linux:\nsh share/install/standalone_install.sh\nThe pkg-config tool can be used to get all links and compilation flags for the libraries defined inthe project. To let pkg-config know these libraries, read the last output of the install_script and apply the given command. It consists in setting the PKG_CONFIG_PATH, for instance on linux do:\nexport PKG_CONFIG_PATH=<path to open-phri>/binaries/pid-workspace/share/pkgconfig:$PKG_CONFIG_PATH\nThen, to get compilation flags run:\npkg-config --static --cflags open-phri_<name of library>\nTo get linker flags run:\npkg-config --static --libs open-phri_<name of library>\nAbout authors\nopen-phri has been developped by following authors:\nBenjamin Navarro (LIRMM)\nPlease contact Benjamin Navarro (navarro@lirmm.fr) - LIRMM for more information or questions.", "link": "https://github.com/open-phri/open-phri", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "overview\nopenphri: a generic framework to easily and safely control robots in interactions with humans\nmaster integration\nfor a quick install of openphri, jump to the standalone install section.\nfor an overview of the library, you can check the related robotics and automation magazine article, but keep in mind that the library went through some changes since its writing and so some parts might not be up-to-date.\nyou can find the documentation (work in progress) about openphri in this gitbook.\ndisclamer: version 1.0, the current release, has lots of improvements compared to initial release (0.x), but the python bindings haven't been updated and so cannot be used. since a version 2.0 with major usability improvements is on its way, the 1.0 will not be updated except for potential bugs fixes. python bindings will be back with 2.0.\nthe license that applies to the whole package content is gnulgpl. please look at the license.txt file at the root of this repository.\ninstallation and usage\nthe detailed procedures for installing the open-phri package and for using its components is based on the pid build and deployment system called pid. just follow and read the links to understand how to install, use and call its api and/or applications.\nfor a quick installation:\ninstalling the project into an existing pid workspace\nto get last version :\ncd <path to pid workspace>/pid\nmake deploy package=open-phri\nto get a specific version of the package :\ncd <path to pid workspace>/pid\nmake deploy package=open-phri version=<version number>\nstandalone install\ngit clone https://github.com/benjaminnavarro/open-phri.git\ncd open-phri\nthen run the adequate install script depending on your system. for instance on linux:\nsh share/install/standalone_install.sh\nthe pkg-config -----> tool !!!  can be used to get all links and compilation flags for the libraries defined inthe project. to let pkg-config know these libraries, read the last output of the install_script and apply the given command. it consists in setting the pkg_config_path, for instance on linux do:\nexport pkg_config_path=<path to open-phri>/binaries/pid-workspace/share/pkgconfig:$pkg_config_path\nthen, to get compilation flags run:\npkg-config --static --cflags open-phri_<name of library>\nto get linker flags run:\npkg-config --static --libs open-phri_<name of library>\nabout authors\nopen-phri has been developped by following authors:\nbenjamin navarro (lirmm)\nplease contact benjamin navarro (navarro@lirmm.fr) - lirmm for more information or questions.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000828, "year": null}, {"Unnamed: 0": 1851, "autor": 831, "date": null, "content": "path_optimizer_2\nThis is a newer version of my path planning ROS package for autonomous driving vehicles \ud83d\udc49path_optimizer.\nImprovements\nHigher success rate in dense/complicated environments.\nSimpler problem formulation.\nCandidate result when no collision-free solution exists.\nSimulation\n(1) Simulation in dynamic environment\n(2) Simulation with complex static obstacles\nDependencies\nROS Kinetic (Ubuntu 16.04) or Melodic (Ubuntu 18.04);\nOther dependencies: glog, gflags, osqp-eigen, grid_map\nPut these ROS packages in your ros workspace: ros_viz_tools, tinyspline_ros.\nThese dependencies (except for ROS) can be installed by running script/install_deps.sh.\nUsage\nA png image is loaded as the grid map. You can click to specify the global reference path and the start/goal state of the vehicle.\nroslaunch path_optimizer_2 demo.launch\n(1) Pick reference points using \"Publish Point\" tool in RViz.\nPick at least six points.\nThere are no hard and fast rules about the spacing of the points.\nIf you want to abandon the chosen points, just double click anywhere when using the \"Publish Point\" tool.\nYou can replace gridmap.png with other black and white images. Note that the resolution in demo.cpp is set to 0.2m, whick means that the length of one pixel is 0.2m on the map.\nIn application, the reference path is given by a global path or by a search algorithm like A*.\n(2) Pick start state using \"2D Pose Estimate\" tool and pick goal state using \"2D Nav Goal\" tool.\nCurrently, it's not strictly required to reach the goal state. But this can be changed.\nThe start state must be ahead of the first reference point.", "link": "https://github.com/LiJiangnanBit/path_optimizer_2", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "path_optimizer_2\nthis is a newer version of my path planning ros package for autonomous driving vehicles \ud83d\udc49path_optimizer.\nimprovements\nhigher success rate in dense/complicated environments.\nsimpler problem formulation.\ncandidate result when no collision-free solution exists.\nsimulation\n(1) simulation in dynamic environment\n(2) simulation with complex static obstacles\ndependencies\nros kinetic (ubuntu 16.04) or melodic (ubuntu 18.04);\nother dependencies: glog, gflags, osqp-eigen, grid_map\nput these ros packages in your ros workspace: ros_viz_tools, tinyspline_ros.\nthese dependencies (except for ros) can be installed by running script/install_deps.sh.\nusage\na png image is loaded as the grid map. you can click to specify the global reference path and the start/goal state of the vehicle.\nroslaunch path_optimizer_2 demo.launch\n(1) pick reference points using \"publish point\" -----> tool !!!  in rviz.\npick at least six points.\nthere are no hard and fast rules about the spacing of the points.\nif you want to abandon the chosen points, just double click anywhere when using the \"publish point\" tool.\nyou can replace gridmap.png with other black and white images. note that the resolution in demo.cpp is set to 0.2m, whick means that the length of one pixel is 0.2m on the map.\nin application, the reference path is given by a global path or by a search algorithm like a*.\n(2) pick start state using \"2d pose estimate\" tool and pick goal state using \"2d nav goal\" tool.\ncurrently, it's not strictly required to reach the goal state. but this can be changed.\nthe start state must be ahead of the first reference point.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000831, "year": null}, {"Unnamed: 0": 1870, "autor": 850, "date": null, "content": "Robot-Runner\nRobot Runner (RR) is a tool to automatically execute measurement-based experiments on robotics software.\nThe following scientific paper gives an overview about the main components, plugins, and configurations of Robot Runner: ICSE 2021 tool demo.\nA short video, giving a brief explanation of Robot Runner, can be seen here.\nHow to cite Robot Runner\nIf Robot Runner is helping your research, consider to cite it as follows, thank you!\n@inproceedings{ICSE_2021,\ntitle={{Robot Runner: A Tool for Automatically Executing Experiments on Robotics Software}},\nauthor={Stan Swanborn and Ivano Malavolta},\nbooktitle = {Proceedings of the ACM/IEEE 43rd International Conference on Software Engineering},\nyear={2021},\nurl= {https://github.com/S2-group/robot-runner/tree/master/documentation/ICSE_2021.pdf},\norganization={ACM}\n}\nOverview\nThe steps to start using Robot Runner are detailed below.\nAs visualized below, Robot Runner consists of the following components:\nExperiment orchestrator: Is in charge of executing the whole experiment according to the experiment configuration provided by the user.\nEvent manager: Provides the user with subscribable events, to which callback methods can be set, which are called at the appropriate time by the Experiment Orchestrator.\nProgress manager: Keeps track of the execution of each run of the experiment.\nConfig Validator: Provides a validation of a user's configuration file and checks system readiness.\nRobot Runner is developed to be entirely independent from any communicational means or robotic system. This means specifically; that Robot Runner can be used with any robotic system, or any simulation software, using any form of communication (e.g. ROS1, ROS2, TCP Sockets, etc.).\nRR offers an automation of the infrastructure overhead for measurement-based, empirical, experiments as a consequence of its design, as produced by the following design drivers:\nUser Authority: Give the user full aothority over the experiment execution in the Python-based configuration file.\nFocus on Orhcestration: Orchestrate the experiment on the basis of events. These events are characterized by their moment of excecution in any experiment.\nFocus on Supporting Infrastructure: Offer the user all, potentially necessary, supporting features (e.g. profiler plugins).\nSetup guide\nTo be able to use RR, some requirements on the system in question need to be met:\nPython3.8: RR is specifically tested with Python 3.8\nPython3.8 pip packages: The following pip packages need to be installed on the system (psutil, tabulate)\nMultiprocessing: The Python3.8 module multiprocessing needs to be supported by the system. It is found that a bug in this library prevents RR from running successfully on macOS.\nThe user's system is ready to run RR if the abovementioned requirements are met. However, for the communication with a robotic system such means will need to be installed. As mentioned before, the user is entirely free in this choice as RR is independent from this.\nHowever, as the Robotic Operating System (ROS) is the de-facto standard for robotics software, it was used during development in combination with a ROBOTIS TurtleBot3 (Burger).\nFor the installation of ROS, any version can be used. But during development, ROS2 Foxy and ROS1 Melodic were explicitly used.\nQuick start\nNow that the system has all the necessary components installed to run Robot Runner, and has robotic enabling software installed (such as ROS), a quick start with Robot Runner can be performed.\nCreating configuration file\nPython3.8 robot-runner/ config-create [directory]\nWhen running this command, where [directory] is an optional argument, a new config file with some example code will be generated. The default location for this would be robot-runner/experiments/, otherwise the given directory will be used.\nSetting up an experiment\nNow that a new configuration file is available, the user can define the experiment. An experiment configuration can be defined using the provided experiment parameters and events.\nThe parameters\nname: str = \"mini_test\"\nrequired_ros_version: int = 2\nrequired_ros_distro: str = \"foxy\"\noperation_type: OperationType = OperationType.AUTO\ntime_between_runs_in_ms: int = 1000\nresults_output_path: Path = Path(\"~/Documents/experiments\")\nSupporting information:\nname: The name of the experiment, which is the name used for the experiment output folder, which will be created in the results_output_path.\nrequired_ros_version: If ROS is used in the experiment, the experiment can be defined as to be dependent on that ROS version using this paramater. If ROS is not used, the value can be set to None. If ROS is used, but the version does not matter, the value can be set to any. If the ROS version is set to any, the ros distribution is automatically not checked anymore as it is not relevant anymore.\nrequired_ros_distro: This is a further specification of the abovementioned, it therefore has the same function and can also be discard by setting it to None (not required) or set to any (any distribution of a certain version can be used).\noperation_type: If set to AUTO, the experiment will continue with the next run (after time_between_runs_in_ms milliseconds) automatically without waiting for any other stimuli. If set to SEMI, the experiment will only continue (after waiting time_between_runs_in_ms milliseconds), if the callback for the event CONTINUE is returned.\ntime_between_runs_in_ms: The time Robot Runner will wait after a run completes, before continuing with the orchestration. This can be essential to accommodate for cooldown periods on some systems.\nresults_output_path: The path in which Robot Runner will create an experiment folder according to the experiment name.\nThe events\ndef __init__(self):\n\"\"\"Executes immediately after program start, on config load\"\"\"\nEventSubscriptionController.subscribe_to_multiple_events([\n(RobotRunnerEvents.BEFORE_EXPERIMENT, self.before_experiment),\n(RobotRunnerEvents.START_RUN, self.start_run),\n(RobotRunnerEvents.START_MEASUREMENT, self.start_measurement),\n(RobotRunnerEvents.LAUNCH_MISSION, self.launch_mission),\n(RobotRunnerEvents.STOP_MEASUREMENT, self.stop_measurement),\n(RobotRunnerEvents.STOP_RUN, self.stop_run),\n(RobotRunnerEvents.CONTINUE, self.continue_experiment)\n(RobotRunnerEvents.POPULATE_RUN_DATA, self.populate_run_data),\n(RobotRunnerEvents.AFTER_EXPERIMENT, self.after_experiment)\n])\ndef create_run_table(self) -> List[Dict]:\n\"\"\"Create and return the run_table here. A run_table is a List (rows) of dictionaries (columns),\nrepresenting each run robot-runner must perform\"\"\"\nrun_table = RunTableModel(\nfactors = [\nFactorModel(\"example_factor\", ['example_treatment1', 'example_treatment2'])\n],\nexclude_variations = [\n{\"example_treatment1\"},\n{\"example_treatment1\", \"example_treatment2\"}\n],\ndata_columns=[\"data_column1\", \"data_column2\"]\n)\nrun_table.create_experiment_run_table()\nreturn run_table.get_experiment_run_table()\ndef before_experiment(self) -> None:\n\"\"\"Perform any activity required before starting the experiment here\"\"\"\ndef start_run(self, context: RobotRunnerContext) -> None:\n\"\"\"Perform any activity required for starting the run here.\nActivities before and after starting the run should also be performed here.\"\"\"\ndef start_measurement(self, context: RobotRunnerContext) -> None:\n\"\"\"Perform any activity required to start the measurements\"\"\"\ndef launch_mission(self, context: RobotRunnerContext) -> None:\n\"\"\"Perform any activity interacting with the robotic\nsystem in question (simulated or real-life) here.\"\"\"\ndef stop_measurement(self, context: RobotRunnerContext) -> None:\n\"\"\"Perform any activity required to stop the measurements\"\"\"\ndef stop_run(self, context: RobotRunnerContext) -> None:\n\"\"\"Perform any activity required for stopping the run here.\nActivities before and after stopping the run should also be performed here.\"\"\"\ndef populate_run_data(self, context: RobotRunnerContext) -> tuple:\n\"\"\"Return the run data as a row for the output manager represented as a tuple\"\"\"\ndef continue_experiment(self, context: RobotRunnerContext) -> None:\n\"\"\"On return of this callback, Robot Runner continues with the orchestration of the experiment\"\"\"\ndef after_experiment(self) -> None:\n\"\"\"Perform any activity required after stopping the experiment here\"\"\"\nPerforming the experiment\nOnce the experiment has been defined by the user, as business logic setup in the shown event callbacks above, the experiment can be performed by Robot Runner. To do this, the user runs the following command:\npython3.8 robot-runner/ experiment_config.py\nAfter which Robot Runner will:\nValidate the config\nOutput the config's values as read by RR in the terminal for user validation\nCreate the experiment folder\nCreate the run table (.csv), and persist it in the experiment folder\nRun the experiment on a per-run basis, going over each run with its specified treatments in the run table.\nExamples\nRobot Runner offer a simple example for a ROS1 based robotic system. The experiment was performed on a ROBOTIS TurtleBot3 specifically. The example experiment is called 'mini-mission' and can be found in the robot-runner/experiments/mini-mission/ folder.\nThe mini-mission, its execution and its output is explained in the video referenced at the beginning of this README.\nSupporting Features\nRobot Runner offers extensive supporting infrastructure, such as:\nRestarting: Robot Runner is able to restart an experiment, if the experiment was not entirely completed on the last run. Every run that is not yet performed and persisted by Robot Runner will in that case be run again and their information persisted in the experiment output folder. Robot Runner has extensive protection measures installed to prevent the accidental overwriting of an experiment or already performed runs.\nOperational Types: As mentioned before, Robot Runner offers the user the possibility of two operational types, which aid in a more flexible experiment design.\nRun Table Creation Model: In the mandatory method create_run_table, the RunTableModel in combination with the FactorModel's can be seen in action. The user is offered this supporting infrastructure to be able to easily define an experiment with Factors, their Treatments, exclude certain combinations of Treatments, and add data columns for storing aggregated data in the run table using the populate_run_table event callback.\nROS Dependency Requirements: As mentioned before, Robot Runner offers the user the possibility, if ROS is used and a dependency on a certain version exists, to state it as such and make sure the experiment can only be ran if those dependencies are met.\nPlugins: Robot Runner offers reusable plugins across missions, which can be used to further abstract and simplify experiment configurations. For example, the INA219Profiler exists, which offers a comprehensive abstraction around energy related data gathered from an INA219 sensor.\nDocker: Dockerfiles are available here which are created so the user can build either a ROS1 or ROS2 supporting Docker container in which robot-runner is able to run and perform missions.", "link": "https://github.com/S2-group/robot-runner", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "robot-runner\nrobot runner (rr) is a -----> tool !!!  to automatically execute measurement-based experiments on robotics software.\nthe following scientific paper gives an overview about the main components, plugins, and configurations of robot runner: icse 2021 tool demo.\na short video, giving a brief explanation of robot runner, can be seen here.\nhow to cite robot runner\nif robot runner is helping your research, consider to cite it as follows, thank you!\n@inproceedings{icse_2021,\ntitle={{robot runner: a tool for automatically executing experiments on robotics software}},\nauthor={stan swanborn and ivano malavolta},\nbooktitle = {proceedings of the acm/ieee 43rd international conference on software engineering},\nyear={2021},\nurl= {https://github.com/s2-group/robot-runner/tree/master/documentation/icse_2021.pdf},\norganization={acm}\n}\noverview\nthe steps to start using robot runner are detailed below.\nas visualized below, robot runner consists of the following components:\nexperiment orchestrator: is in charge of executing the whole experiment according to the experiment configuration provided by the user.\nevent manager: provides the user with subscribable events, to which callback methods can be set, which are called at the appropriate time by the experiment orchestrator.\nprogress manager: keeps track of the execution of each run of the experiment.\nconfig validator: provides a validation of a user's configuration file and checks system readiness.\nrobot runner is developed to be entirely independent from any communicational means or robotic system. this means specifically; that robot runner can be used with any robotic system, or any simulation software, using any form of communication (e.g. ros1, ros2, tcp sockets, etc.).\nrr offers an automation of the infrastructure overhead for measurement-based, empirical, experiments as a consequence of its design, as produced by the following design drivers:\nuser authority: give the user full aothority over the experiment execution in the python-based configuration file.\nfocus on orhcestration: orchestrate the experiment on the basis of events. these events are characterized by their moment of excecution in any experiment.\nfocus on supporting infrastructure: offer the user all, potentially necessary, supporting features (e.g. profiler plugins).\nsetup guide\nto be able to use rr, some requirements on the system in question need to be met:\npython3.8: rr is specifically tested with python 3.8\npython3.8 pip packages: the following pip packages need to be installed on the system (psutil, tabulate)\nmultiprocessing: the python3.8 module multiprocessing needs to be supported by the system. it is found that a bug in this library prevents rr from running successfully on macos.\nthe user's system is ready to run rr if the abovementioned requirements are met. however, for the communication with a robotic system such means will need to be installed. as mentioned before, the user is entirely free in this choice as rr is independent from this.\nhowever, as the robotic operating system (ros) is the de-facto standard for robotics software, it was used during development in combination with a robotis turtlebot3 (burger).\nfor the installation of ros, any version can be used. but during development, ros2 foxy and ros1 melodic were explicitly used.\nquick start\nnow that the system has all the necessary components installed to run robot runner, and has robotic enabling software installed (such as ros), a quick start with robot runner can be performed.\ncreating configuration file\npython3.8 robot-runner/ config-create [directory]\nwhen running this command, where [directory] is an optional argument, a new config file with some example code will be generated. the default location for this would be robot-runner/experiments/, otherwise the given directory will be used.\nsetting up an experiment\nnow that a new configuration file is available, the user can define the experiment. an experiment configuration can be defined using the provided experiment parameters and events.\nthe parameters\nname: str = \"mini_test\"\nrequired_ros_version: int = 2\nrequired_ros_distro: str = \"foxy\"\noperation_type: operationtype = operationtype.auto\ntime_between_runs_in_ms: int = 1000\nresults_output_path: path = path(\"~/documents/experiments\")\nsupporting information:\nname: the name of the experiment, which is the name used for the experiment output folder, which will be created in the results_output_path.\nrequired_ros_version: if ros is used in the experiment, the experiment can be defined as to be dependent on that ros version using this paramater. if ros is not used, the value can be set to none. if ros is used, but the version does not matter, the value can be set to any. if the ros version is set to any, the ros distribution is automatically not checked anymore as it is not relevant anymore.\nrequired_ros_distro: this is a further specification of the abovementioned, it therefore has the same function and can also be discard by setting it to none (not required) or set to any (any distribution of a certain version can be used).\noperation_type: if set to auto, the experiment will continue with the next run (after time_between_runs_in_ms milliseconds) automatically without waiting for any other stimuli. if set to semi, the experiment will only continue (after waiting time_between_runs_in_ms milliseconds), if the callback for the event continue is returned.\ntime_between_runs_in_ms: the time robot runner will wait after a run completes, before continuing with the orchestration. this can be essential to accommodate for cooldown periods on some systems.\nresults_output_path: the path in which robot runner will create an experiment folder according to the experiment name.\nthe events\ndef __init__(self):\n\"\"\"executes immediately after program start, on config load\"\"\"\neventsubscriptioncontroller.subscribe_to_multiple_events([\n(robotrunnerevents.before_experiment, self.before_experiment),\n(robotrunnerevents.start_run, self.start_run),\n(robotrunnerevents.start_measurement, self.start_measurement),\n(robotrunnerevents.launch_mission, self.launch_mission),\n(robotrunnerevents.stop_measurement, self.stop_measurement),\n(robotrunnerevents.stop_run, self.stop_run),\n(robotrunnerevents.continue, self.continue_experiment)\n(robotrunnerevents.populate_run_data, self.populate_run_data),\n(robotrunnerevents.after_experiment, self.after_experiment)\n])\ndef create_run_table(self) -> list[dict]:\n\"\"\"create and return the run_table here. a run_table is a list (rows) of dictionaries (columns),\nrepresenting each run robot-runner must perform\"\"\"\nrun_table = runtablemodel(\nfactors = [\nfactormodel(\"example_factor\", ['example_treatment1', 'example_treatment2'])\n],\nexclude_variations = [\n{\"example_treatment1\"},\n{\"example_treatment1\", \"example_treatment2\"}\n],\ndata_columns=[\"data_column1\", \"data_column2\"]\n)\nrun_table.create_experiment_run_table()\nreturn run_table.get_experiment_run_table()\ndef before_experiment(self) -> none:\n\"\"\"perform any activity required before starting the experiment here\"\"\"\ndef start_run(self, context: robotrunnercontext) -> none:\n\"\"\"perform any activity required for starting the run here.\nactivities before and after starting the run should also be performed here.\"\"\"\ndef start_measurement(self, context: robotrunnercontext) -> none:\n\"\"\"perform any activity required to start the measurements\"\"\"\ndef launch_mission(self, context: robotrunnercontext) -> none:\n\"\"\"perform any activity interacting with the robotic\nsystem in question (simulated or real-life) here.\"\"\"\ndef stop_measurement(self, context: robotrunnercontext) -> none:\n\"\"\"perform any activity required to stop the measurements\"\"\"\ndef stop_run(self, context: robotrunnercontext) -> none:\n\"\"\"perform any activity required for stopping the run here.\nactivities before and after stopping the run should also be performed here.\"\"\"\ndef populate_run_data(self, context: robotrunnercontext) -> tuple:\n\"\"\"return the run data as a row for the output manager represented as a tuple\"\"\"\ndef continue_experiment(self, context: robotrunnercontext) -> none:\n\"\"\"on return of this callback, robot runner continues with the orchestration of the experiment\"\"\"\ndef after_experiment(self) -> none:\n\"\"\"perform any activity required after stopping the experiment here\"\"\"\nperforming the experiment\nonce the experiment has been defined by the user, as business logic setup in the shown event callbacks above, the experiment can be performed by robot runner. to do this, the user runs the following command:\npython3.8 robot-runner/ experiment_config.py\nafter which robot runner will:\nvalidate the config\noutput the config's values as read by rr in the terminal for user validation\ncreate the experiment folder\ncreate the run table (.csv), and persist it in the experiment folder\nrun the experiment on a per-run basis, going over each run with its specified treatments in the run table.\nexamples\nrobot runner offer a simple example for a ros1 based robotic system. the experiment was performed on a robotis turtlebot3 specifically. the example experiment is called 'mini-mission' and can be found in the robot-runner/experiments/mini-mission/ folder.\nthe mini-mission, its execution and its output is explained in the video referenced at the beginning of this readme.\nsupporting features\nrobot runner offers extensive supporting infrastructure, such as:\nrestarting: robot runner is able to restart an experiment, if the experiment was not entirely completed on the last run. every run that is not yet performed and persisted by robot runner will in that case be run again and their information persisted in the experiment output folder. robot runner has extensive protection measures installed to prevent the accidental overwriting of an experiment or already performed runs.\noperational types: as mentioned before, robot runner offers the user the possibility of two operational types, which aid in a more flexible experiment design.\nrun table creation model: in the mandatory method create_run_table, the runtablemodel in combination with the factormodel's can be seen in action. the user is offered this supporting infrastructure to be able to easily define an experiment with factors, their treatments, exclude certain combinations of treatments, and add data columns for storing aggregated data in the run table using the populate_run_table event callback.\nros dependency requirements: as mentioned before, robot runner offers the user the possibility, if ros is used and a dependency on a certain version exists, to state it as such and make sure the experiment can only be ran if those dependencies are met.\nplugins: robot runner offers reusable plugins across missions, which can be used to further abstract and simplify experiment configurations. for example, the ina219profiler exists, which offers a comprehensive abstraction around energy related data gathered from an ina219 sensor.\ndocker: dockerfiles are available here which are created so the user can build either a ros1 or ros2 supporting docker container in which robot-runner is able to run and perform missions.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000850, "year": null}, {"Unnamed: 0": 1871, "autor": 851, "date": null, "content": "description\nLearning Drake from the perspective of a robotics beginner.\nDrake Tutorial\nDrake is a robotics C++ software maintained by Robot Locomotion Group from MIT and Toyota Research Institute. It is a powerful tool for robotics developers but is difficult for beginners. Here, I share my learning experiences and my personal understanding of Drake. Hopefully, Drake beginners find it useful.\nThis document is supported by GitBook. The official website is https://drake.guzhaoyuan.com/.\nTo contribute, contact the author github@guzhaoyuan.\nIf you have any questions, please file an issue, to help improve the document.\nDrake started to release monthly since 2020 and the development moves fast. This tutorial is based on release v0.20.0.", "link": "https://github.com/guzhaoyuan/drake-tutorial", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "description\nlearning drake from the perspective of a robotics beginner.\ndrake tutorial\ndrake is a robotics c++ software maintained by robot locomotion group from mit and toyota research institute. it is a powerful -----> tool !!!  for robotics developers but is difficult for beginners. here, i share my learning experiences and my personal understanding of drake. hopefully, drake beginners find it useful.\nthis document is supported by gitbook. the official website is https://drake.guzhaoyuan.com/.\nto contribute, contact the author github@guzhaoyuan.\nif you have any questions, please file an issue, to help improve the document.\ndrake started to release monthly since 2020 and the development moves fast. this tutorial is based on release v0.20.0.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000851, "year": null}, {"Unnamed: 0": 1925, "autor": 905, "date": null, "content": "Robot Vulnerability Scoring System (RVSS) Python 3 reference implementation.\nWork inspired by https://github.com/ctxis/cvsslib. Paper available at https://arxiv.org/pdf/1807.10357.pdf.\nCurrent version: 1.0\nLicense: GPLv3\nThis repository provides a Python 3 (only Python 3) library for calculating robot vulnerability scores. The library extends cvsslib to support the following scoring systems:\nCVSSv2\nCVSSv3\nRVSSv1\nFrom the original README:\nExamples on how to use the library is shown below, and there is some documentation on the internals within the docs directory. The library is designed to be completely extendable, so it is possible to implement your own custom scoring systems (or those of your clients) and have it work with the same API, and with the same bells and whistles.\nHow to cite our work\n@ARTICLE{2018arXiv180710357M,\nauthor = {{Mayoral Vilches}, V. and {Gil-Uriarte}, E. and {Zamalloa Ugarte}, I. and\n{Olalde Mendia}, G. and {Izquierdo Pis{\\'o}n}, R. and {Alzola Kirschgens}, L. and\n{Bilbao Calvo}, A. and {Hern{\\'a}ndez Cordero}, A. and {Apa}, L. and\n{Cerrudo}, C.},\ntitle = \"{Towards an open standard for assessing the severity of robot security vulnerabilities, the Robot Vulnerability Scoring System (RVSS)}\",\njournal = {ArXiv e-prints},\narchivePrefix = \"arXiv\",\neprint = {1807.10357},\nprimaryClass = \"cs.RO\",\nkeywords = {Computer Science - Robotics, Computer Science - Cryptography and Security},\nyear = 2018,\nmonth = jul,\nadsurl = {http://adsabs.harvard.edu/abs/2018arXiv180710357M},\nadsnote = {Provided by the SAO/NASA Astrophysics Data System}\n}\nInstall\npython3 setup.py install\nTry it out\nRVSSv1\n$ rvss RVSS:1.0/AV:AN/AC:L/PR:N/UI:N/Y:O/S:U/C:N/I:L/A:N/H:H\nBase Score:7.3\nTemporal:7.3\nEnvironment:7.3\nCVSSv3\n$ rvss CVSS:3.0/AV:A/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H\nBase Score:8.8\nTemporal:8.8\nEnvironment:8.8\nCVSSv2\n$ rvss CVSS:2.0/AV:L/AC:M/Au:N/C:N/I:P/A:C/E:POC/RL:W/RC:UR/CDP:LM/TD:H/CR:M/IR:L/AR:H\nBase Score:5.4\nTemporal:4.4\nEnvironment:6.9\nAPI\nIt's pretty simple to use. cvsslib has a cvss2, cvss3 and rvss sub modules that contains all of the enums and calculation code. There are also some functions to manipulate vectors that take these cvss modules as arguments. E.G:\nfrom cvsslib import cvss2, cvss3, calculate_vector\nvector_v2 = \"AV:L/AC:M/Au:S/C:N/I:P/A:C/E:U/RL:OF/RC:UR/CDP:N/TD:L/CR:H/IR:H/AR:H\"\ncalculate_vector(vector_v2, cvss2)\n>> (5, 3.5, 1.2)\nvector_v3 = \"CVSS:3.0/AV:L/AC:L/PR:H/UI:R/S:U/C:H/I:N/A:H/MPR:N\"\ncalculate_vector(vector_v3, cvss3)\n>> (5.8, 5.8, 7.1)\nYou can access every CVSS enum through the cvss2 or cvss3 modules:\nfrom cvsslib import cvss2\n# In this case doing from 'cvsslib.cvss2.enums import *' might be less verbose.\nvalue = cvss2.ReportConfidence.CONFIRMED\nif value != cvss2.ReportConfidence.NOT_DEFINED:\ndo_something()\nThere are some powerful mixin functions if you need a class with CVSS members. These functions take a cvss version and return a base class you can inherit from. This class hassome utility functions like to_vector() and from_vector() you can use.\nfrom cvsslib import cvss3, class_mixin\nBaseClass = class_mixin(cvss3) # Can pass cvss2 module instead\nclass SomeObject(BaseClass):\ndef print_stats(self):\nfor item, value in self.enums:\nprint(\"{0} is {1}\".format(item, value)\nstate = SomeObject()\nprint(\"\\n\".join(state.debug()))\nprint(state.calculate())\nstate.from_vector(\"CVSS:3.0/AV:L/AC:L/PR:H/UI:R/S:U/C:H/I:N/A:H/MPR:N\")\nprint(\"Vector: \" + state.to_vector())\n# Access members:\nif state.report_confidence == ReportConfidence.NOT_DEFINED:\ndo_something()\nIt also supports Django models. Requires the django-enumfields package.\nfrom cvsslib.contrib.django_model import django_mixin\nfrom cvsslib import cvss2\nfrom django.db import models\nCVSSBase = django_mixin(cvss2)\nclass CVSSModel(models.Model, metaclass=CVSSBase)\npass\n# CVSSModel now has lots of enum you can use\nx = CVSSModel()\nx.save()\nx.exploitability\nIf you want it to work with django Migrations you need to give an attribute name to the django_mixin function. This should match the attribute name it is being assigned to:\nCVSSBase = django_mixin(cvss2, attr_name=\"CVSSBase\")\nAnd there is a command line tool available:\n> cvss CVSS:3.0/AV:L/AC:H/PR:H/UI:N/S:C/C:N/I:H/A:N/E:P/RL:U/RC:U/CR:H/IR:L/AR:H/MAV:L/MUI:R/MS:C/MC:N/MI:L/MA:N\nBase Score: 5.3\nTemporal: 4.6\nEnvironment: 1.3\nCustom Scoring Systems\nCreating a new scoring system is very simple. First create a Python file with the correct name, e.g super_scores.py. Next create some enums with the correct values for your system:\nfrom cvsslib.base_enum import BaseEnum\nclass Risk(BaseEnum):\n\"\"\"\nVector: S\n\"\"\"\nHIGH = 1\nMEDIUM = 2\nLOW = 3\nclass Difficulty(BaseEnum):\n\"\"\"\nVector: D\n\"\"\"\nDIFFICULT = 1\nMODERATE = 2\nEASY = 3\nAnd lastly add a calculate function in the module that accepts some vector values and returns a result of some kind:\ndef calculate(difficulty: Difficulty, risk: Risk):\nif difficulty == Difficulty.EASY and risk == Risk.CRITICAL:\nreturn \"oh nuts you're screwed\"\nreturn \"You're probs ok m8\"\nOnce you define this you can pass your super_scores module to any cvsslib function like calculate_vector or django_mixin and it will all just work. You can even serialize the data to and from a vector if you define the correct vector: X in the enum docstrings.", "link": "https://github.com/aliasrobotics/RVSS", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "robot vulnerability scoring system (rvss) python 3 reference implementation.\nwork inspired by https://github.com/ctxis/cvsslib. paper available at https://arxiv.org/pdf/1807.10357.pdf.\ncurrent version: 1.0\nlicense: gplv3\nthis repository provides a python 3 (only python 3) library for calculating robot vulnerability scores. the library extends cvsslib to support the following scoring systems:\ncvssv2\ncvssv3\nrvssv1\nfrom the original readme:\nexamples on how to use the library is shown below, and there is some documentation on the internals within the docs directory. the library is designed to be completely extendable, so it is possible to implement your own custom scoring systems (or those of your clients) and have it work with the same api, and with the same bells and whistles.\nhow to cite our work\n@article{2018arxiv180710357m,\nauthor = {{mayoral vilches}, v. and {gil-uriarte}, e. and {zamalloa ugarte}, i. and\n{olalde mendia}, g. and {izquierdo pis{\\'o}n}, r. and {alzola kirschgens}, l. and\n{bilbao calvo}, a. and {hern{\\'a}ndez cordero}, a. and {apa}, l. and\n{cerrudo}, c.},\ntitle = \"{towards an open standard for assessing the severity of robot security vulnerabilities, the robot vulnerability scoring system (rvss)}\",\njournal = {arxiv e-prints},\narchiveprefix = \"arxiv\",\neprint = {1807.10357},\nprimaryclass = \"cs.ro\",\nkeywords = {computer science - robotics, computer science - cryptography and security},\nyear = 2018,\nmonth = jul,\nadsurl = {http://adsabs.harvard.edu/abs/2018arxiv180710357m},\nadsnote = {provided by the sao/nasa astrophysics data system}\n}\ninstall\npython3 setup.py install\ntry it out\nrvssv1\n$ rvss rvss:1.0/av:an/ac:l/pr:n/ui:n/y:o/s:u/c:n/i:l/a:n/h:h\nbase score:7.3\ntemporal:7.3\nenvironment:7.3\ncvssv3\n$ rvss cvss:3.0/av:a/ac:l/pr:n/ui:n/s:u/c:h/i:h/a:h\nbase score:8.8\ntemporal:8.8\nenvironment:8.8\ncvssv2\n$ rvss cvss:2.0/av:l/ac:m/au:n/c:n/i:p/a:c/e:poc/rl:w/rc:ur/cdp:lm/td:h/cr:m/ir:l/ar:h\nbase score:5.4\ntemporal:4.4\nenvironment:6.9\napi\nit's pretty simple to use. cvsslib has a cvss2, cvss3 and rvss sub modules that contains all of the enums and calculation code. there are also some functions to manipulate vectors that take these cvss modules as arguments. e.g:\nfrom cvsslib import cvss2, cvss3, calculate_vector\nvector_v2 = \"av:l/ac:m/au:s/c:n/i:p/a:c/e:u/rl:of/rc:ur/cdp:n/td:l/cr:h/ir:h/ar:h\"\ncalculate_vector(vector_v2, cvss2)\n>> (5, 3.5, 1.2)\nvector_v3 = \"cvss:3.0/av:l/ac:l/pr:h/ui:r/s:u/c:h/i:n/a:h/mpr:n\"\ncalculate_vector(vector_v3, cvss3)\n>> (5.8, 5.8, 7.1)\nyou can access every cvss enum through the cvss2 or cvss3 modules:\nfrom cvsslib import cvss2\n# in this case doing from 'cvsslib.cvss2.enums import *' might be less verbose.\nvalue = cvss2.reportconfidence.confirmed\nif value != cvss2.reportconfidence.not_defined:\ndo_something()\nthere are some powerful mixin functions if you need a class with cvss members. these functions take a cvss version and return a base class you can inherit from. this class hassome utility functions like to_vector() and from_vector() you can use.\nfrom cvsslib import cvss3, class_mixin\nbaseclass = class_mixin(cvss3) # can pass cvss2 module instead\nclass someobject(baseclass):\ndef print_stats(self):\nfor item, value in self.enums:\nprint(\"{0} is {1}\".format(item, value)\nstate = someobject()\nprint(\"\\n\".join(state.debug()))\nprint(state.calculate())\nstate.from_vector(\"cvss:3.0/av:l/ac:l/pr:h/ui:r/s:u/c:h/i:n/a:h/mpr:n\")\nprint(\"vector: \" + state.to_vector())\n# access members:\nif state.report_confidence == reportconfidence.not_defined:\ndo_something()\nit also supports django models. requires the django-enumfields package.\nfrom cvsslib.contrib.django_model import django_mixin\nfrom cvsslib import cvss2\nfrom django.db import models\ncvssbase = django_mixin(cvss2)\nclass cvssmodel(models.model, metaclass=cvssbase)\npass\n# cvssmodel now has lots of enum you can use\nx = cvssmodel()\nx.save()\nx.exploitability\nif you want it to work with django migrations you need to give an attribute name to the django_mixin function. this should match the attribute name it is being assigned to:\ncvssbase = django_mixin(cvss2, attr_name=\"cvssbase\")\nand there is a command line -----> tool !!!  available:\n> cvss cvss:3.0/av:l/ac:h/pr:h/ui:n/s:c/c:n/i:h/a:n/e:p/rl:u/rc:u/cr:h/ir:l/ar:h/mav:l/mui:r/ms:c/mc:n/mi:l/ma:n\nbase score: 5.3\ntemporal: 4.6\nenvironment: 1.3\ncustom scoring systems\ncreating a new scoring system is very simple. first create a python file with the correct name, e.g super_scores.py. next create some enums with the correct values for your system:\nfrom cvsslib.base_enum import baseenum\nclass risk(baseenum):\n\"\"\"\nvector: s\n\"\"\"\nhigh = 1\nmedium = 2\nlow = 3\nclass difficulty(baseenum):\n\"\"\"\nvector: d\n\"\"\"\ndifficult = 1\nmoderate = 2\neasy = 3\nand lastly add a calculate function in the module that accepts some vector values and returns a result of some kind:\ndef calculate(difficulty: difficulty, risk: risk):\nif difficulty == difficulty.easy and risk == risk.critical:\nreturn \"oh nuts you're screwed\"\nreturn \"you're probs ok m8\"\nonce you define this you can pass your super_scores module to any cvsslib function like calculate_vector or django_mixin and it will all just work. you can even serialize the data to and from a vector if you define the correct vector: x in the enum docstrings.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000905, "year": null}, {"Unnamed: 0": 1929, "autor": 909, "date": null, "content": "Value Disagreement Sampling (VDS)\nThis codebase is adapted from Openai baselines.\nRun experiments\nIn project directory, run\npython -m baselines.ve_run --alg=her --env=FetchPush-v1 --num_timesteps=500000 \\\n--size_ensemble=3 --log_path=./data/test_fetch_push\nBaselines\nOpenAI Baselines is a set of high-quality implementations of reinforcement learning algorithms.\nThese algorithms will make it easier for the research community to replicate, refine, and identify new ideas, and will create good baselines to build research on top of. Our DQN implementation and its variants are roughly on par with the scores in published papers. We expect they will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones.\nPrerequisites\nBaselines requires python3 (>=3.5) with the development headers. You'll also need system packages CMake, OpenMPI and zlib. Those can be installed as follows\nUbuntu\nsudo apt-get update && sudo apt-get install cmake libopenmpi-dev python3-dev zlib1g-dev\nMac OS X\nInstallation of system packages on Mac requires Homebrew. With Homebrew installed, run the following:\nbrew install cmake openmpi\nVirtual environment\nFrom the general python package sanity perspective, it is a good idea to use virtual environments (virtualenvs) to make sure packages from different projects do not interfere with each other. You can install virtualenv (which is itself a pip package) via\npip install virtualenv\nVirtualenvs are essentially folders that have copies of python executable and all python packages. To create a virtualenv called venv with python3, one runs\nvirtualenv /path/to/venv --python=python3\nTo activate a virtualenv:\n. /path/to/venv/bin/activate\nMore thorough tutorial on virtualenvs and options can be found here\nTensorflow versions\nThe master branch supports Tensorflow from version 1.4 to 1.14. For Tensorflow 2.0 support, please use tf2 branch.\nInstallation\nClone the repo and cd into it:\ngit clone https://github.com/openai/baselines.git\ncd baselines\nIf you don't have TensorFlow installed already, install your favourite flavor of TensorFlow. In most cases, you may use\npip install tensorflow-gpu==1.14 # if you have a CUDA-compatible gpu and proper drivers\nor\npip install tensorflow==1.14\nto install Tensorflow 1.14, which is the latest version of Tensorflow supported by the master branch. Refer to TensorFlow installation guide for more details.\nInstall baselines package\npip install -e .\nMuJoCo\nSome of the baselines examples use MuJoCo (multi-joint dynamics in contact) physics simulator, which is proprietary and requires binaries and a license (temporary 30-day license can be obtained from www.mujoco.org). Instructions on setting up MuJoCo can be found here", "link": "https://github.com/zzyunzhi/vds", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "value disagreement sampling (vds)\nthis codebase is adapted from openai baselines.\nrun experiments\nin project directory, run\npython -m baselines.ve_run --alg=her --env=fetchpush-v1 --num_timesteps=500000 \\\n--size_ensemble=3 --log_path=./data/test_fetch_push\nbaselines\nopenai baselines is a set of high-quality implementations of reinforcement learning algorithms.\nthese algorithms will make it easier for the research community to replicate, refine, and identify new ideas, and will create good baselines to build research on top of. our dqn implementation and its variants are roughly on par with the scores in published papers. we expect they will be used as a base around which new ideas can be added, and as a -----> tool !!!  for comparing a new approach against existing ones.\nprerequisites\nbaselines requires python3 (>=3.5) with the development headers. you'll also need system packages cmake, openmpi and zlib. those can be installed as follows\nubuntu\nsudo apt-get update && sudo apt-get install cmake libopenmpi-dev python3-dev zlib1g-dev\nmac os x\ninstallation of system packages on mac requires homebrew. with homebrew installed, run the following:\nbrew install cmake openmpi\nvirtual environment\nfrom the general python package sanity perspective, it is a good idea to use virtual environments (virtualenvs) to make sure packages from different projects do not interfere with each other. you can install virtualenv (which is itself a pip package) via\npip install virtualenv\nvirtualenvs are essentially folders that have copies of python executable and all python packages. to create a virtualenv called venv with python3, one runs\nvirtualenv /path/to/venv --python=python3\nto activate a virtualenv:\n. /path/to/venv/bin/activate\nmore thorough tutorial on virtualenvs and options can be found here\ntensorflow versions\nthe master branch supports tensorflow from version 1.4 to 1.14. for tensorflow 2.0 support, please use tf2 branch.\ninstallation\nclone the repo and cd into it:\ngit clone https://github.com/openai/baselines.git\ncd baselines\nif you don't have tensorflow installed already, install your favourite flavor of tensorflow. in most cases, you may use\npip install tensorflow-gpu==1.14 # if you have a cuda-compatible gpu and proper drivers\nor\npip install tensorflow==1.14\nto install tensorflow 1.14, which is the latest version of tensorflow supported by the master branch. refer to tensorflow installation guide for more details.\ninstall baselines package\npip install -e .\nmujoco\nsome of the baselines examples use mujoco (multi-joint dynamics in contact) physics simulator, which is proprietary and requires binaries and a license (temporary 30-day license can be obtained from www.mujoco.org). instructions on setting up mujoco can be found here", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000909, "year": null}, {"Unnamed: 0": 1938, "autor": 918, "date": null, "content": "umati sample server\nSample-Server implementation of various umati endorsed OPC UA companion specifications. Provides a \ud83d\udc33 \ud83d\udce6 to run locally for development purpose.\nCurrent Implementation\nMachine Tool Examples\nThis is an example implementation of the OPC UA Machine Tool Companion Specification based on open62541. The server provides several simulated machine tools, which show different extension stages of the specification.\nWoodworking Examples\nThis is an example implementation of the OPC UA Woodworking Companion Specification. The server provides two simulated woodworking machines, a basic one implementing only mandatory variables/objects and a full one implementing every optional variable/object.\nScope\nThis implementation provides simulationed instances for the machine tool and woodworking specification, which changes most values in a pseudo random way. As no real machine simulation is done in the background, the values need not correlate with each other. Also only reading of values of OPC UA-clients is implemented. All write requests are ignored silently.\nDocker Image\nThis repository automatically builds a sample server docker image which is then run at opc.tcp://opcua.umati.app:4840 (Security: none and higher / Authentication: anonymous)\nUse this container image for your local development purposes. Pull this public image at: docker pull ghcr.io/umati/sample-server:main\nTo run this image you need a docker installation and than typically: docker run -d -p 4840:4840 ghcr.io/umati/sample-server:main\nRefer to the docker documentation for details.\nBuild status\nConfiguration\nThe configuration uses a json format. By default, the server looks for a file configuration.json. The supported parameters are:\n{\n\"Hostname\": \"localhost\", // Hostname of the server, should be identical to the hostname that is used by OPC UA clients to connect to the server.\n\"Encryption\": { // Optional encryption, can be omitted to only support unencrypted connections\n\"ServerCert\": \"server_cert.der\", // Server certificate\n\"ServerKey\": \"server_key.der\", // Server private key for the certificate\n\"TrustedClients\": [\"trusted/test.pem\"], // List of allowed clients, if empty, all client certificates are accepted\n\"IssuerCerts\": [], // List of allowed issuers\n\"Revocation\": [] // List of revoked certificates.\n}\n}\nConcept\nThis implementation represents the OPC UA Instance with an instance of an C++ struct and connects them together. So writing an value in the C++ instance will also set this value in the corresponding OPC UA node. The binding is established by utilizing reflection (an improved version of the binding in this publication).\nThe binding is done by comparing the name of structure members with the browse name of the OPC UA instance. As the binding uses memory addresses of the C++ instance, the instance itself must not be copied after the binding.\nFolder Structure\n\u251c\u2500\u2500arch architecture specific files\n\u251c\u2500\u2500cmake CMake files for building the project, e.g. custom find-scripts\n\u251c\u2500\u2500Exceptions Custom exceptions\n\u251c\u2500\u2500MachineTools Simulated machine tools\n\u251c\u2500\u2500model Unpublished/Fixed NodeSet files\n\u251c\u2500\u2500OpcUaTypes C++ equivalent of some OPC UA DataTypes and helpers for their definition\n\u251c\u2500\u2500Robotics Simulated Robots\n\u251c\u2500\u2500tests Some unit tests, initially created for tracking down bugs\n\u251c\u2500\u2500tools/certGen Custom certificate generator tool\n\u251c\u2500\u2500TypeDefinition Definition of C++ Types for OPC UA Types, that can be bind later\n\u251c\u2500\u2500UmatiServerLib Library for binding the defined types to OPC UA instances\n\u251c\u2500\u2500Woodworkng Simulated wood working sample machines\nLicense\nThis sample server implementation is licensed under the Mozilla Public License v2.0 (MPLv2) except otherwise stated in the header of a file.", "link": "https://github.com/umati/Sample-Server", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "umati sample server\nsample-server implementation of various umati endorsed opc ua companion specifications. provides a \ud83d\udc33 \ud83d\udce6 to run locally for development purpose.\ncurrent implementation\nmachine -----> tool !!!  examples\nthis is an example implementation of the opc ua machine -----> tool !!!  companion specification based on open62541. the server provides several simulated machine tools, which show different extension stages of the specification.\nwoodworking examples\nthis is an example implementation of the opc ua woodworking companion specification. the server provides two simulated woodworking machines, a basic one implementing only mandatory variables/objects and a full one implementing every optional variable/object.\nscope\nthis implementation provides simulationed instances for the machine tool and woodworking specification, which changes most values in a pseudo random way. as no real machine simulation is done in the background, the values need not correlate with each other. also only reading of values of opc ua-clients is implemented. all write requests are ignored silently.\ndocker image\nthis repository automatically builds a sample server docker image which is then run at opc.tcp://opcua.umati.app:4840 (security: none and higher / authentication: anonymous)\nuse this container image for your local development purposes. pull this public image at: docker pull ghcr.io/umati/sample-server:main\nto run this image you need a docker installation and than typically: docker run -d -p 4840:4840 ghcr.io/umati/sample-server:main\nrefer to the docker documentation for details.\nbuild status\nconfiguration\nthe configuration uses a json format. by default, the server looks for a file configuration.json. the supported parameters are:\n{\n\"hostname\": \"localhost\", // hostname of the server, should be identical to the hostname that is used by opc ua clients to connect to the server.\n\"encryption\": { // optional encryption, can be omitted to only support unencrypted connections\n\"servercert\": \"server_cert.der\", // server certificate\n\"serverkey\": \"server_key.der\", // server private key for the certificate\n\"trustedclients\": [\"trusted/test.pem\"], // list of allowed clients, if empty, all client certificates are accepted\n\"issuercerts\": [], // list of allowed issuers\n\"revocation\": [] // list of revoked certificates.\n}\n}\nconcept\nthis implementation represents the opc ua instance with an instance of an c++ struct and connects them together. so writing an value in the c++ instance will also set this value in the corresponding opc ua node. the binding is established by utilizing reflection (an improved version of the binding in this publication).\nthe binding is done by comparing the name of structure members with the browse name of the opc ua instance. as the binding uses memory addresses of the c++ instance, the instance itself must not be copied after the binding.\nfolder structure\n\u251c\u2500\u2500arch architecture specific files\n\u251c\u2500\u2500cmake cmake files for building the project, e.g. custom find-scripts\n\u251c\u2500\u2500exceptions custom exceptions\n\u251c\u2500\u2500machinetools simulated machine tools\n\u251c\u2500\u2500model unpublished/fixed nodeset files\n\u251c\u2500\u2500opcuatypes c++ equivalent of some opc ua datatypes and helpers for their definition\n\u251c\u2500\u2500robotics simulated robots\n\u251c\u2500\u2500tests some unit tests, initially created for tracking down bugs\n\u251c\u2500\u2500tools/certgen custom certificate generator tool\n\u251c\u2500\u2500typedefinition definition of c++ types for opc ua types, that can be bind later\n\u251c\u2500\u2500umatiserverlib library for binding the defined types to opc ua instances\n\u251c\u2500\u2500woodworkng simulated wood working sample machines\nlicense\nthis sample server implementation is licensed under the mozilla public license v2.0 (mplv2) except otherwise stated in the header of a file.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000918, "year": null}, {"Unnamed: 0": 1943, "autor": 923, "date": null, "content": "Dominh\nDiscussion\nIf you happen to find this useful, leave a quick note in the Discussions section. If something doesn't work, open an issue on the tracker.\nOverview\nThis is a poor man's version of a subset of the RPC functionality provided by the (Windows-only) Fanuc PCDK implemented in Python. This uses the Web Svr Enhancements option (R626) and the interfaces it provides to the controller (see note in Requirements). Only a subset of the functionality is currently available and performance will not be comparable with the PCDK.\nAdditionally a simple set of CLI tools is included, which allows some of the library's functionality to be used from the command line and/or in shell scripts.\nNOTE: this is only meant as an example of such a remote control facility. Proper integration of a Fanuc controller with an external application or workcell should be done using either the real PCDK, a fieldbus or similar technology. The scripts and functionality provided here are only a convenience and are only intended to be used in academic and laboratory settings. They allow incidental external access to a controller without needing to use any additional hardware. Do not use this on production systems or in contexts where any kind of determinism is required. The author recommends using PCDK and/or any of the supported fieldbuses in those cases.\nNOTE 2: on R-30iB+ controllers, option R912 can do some of the things this library supports.\nNOTE 3: on controllers with sufficiently recent system software versions, an OPC-UA server is included. Access over OPC-UA would also support some of the things this library supports.\nTOC\nRequirements\nCompatibility\nInstallation\nExample usage\nSupported functionality\nLimitations / Known issues\nPerformance\nRelated projects\nBugs, feature requests, etc\nFAQ\nDisclaimer\nRequirements\nThis needs the base Web Server (HTTP) and the Web Svr Enhancements (R626) options (note: contrary to what the manual states, this is not a separate option: all functionality is already built-in, at least for R-30iA and newer controllers). As some parts are written in Karel, option R632 could be a requirement, but this is unlikely.\nOther requirements include a functioning networking setup (make sure you can ping the controller and the controller's website shows up when opening http://robot_ip), and correctly configured HTTP Authentication and FTP server settings. Either unlock the KAREL and KCL resources completely or configure sets of credentials for both resources. Configuration for the built-in FTP server should be OK by default, but may be changed to also require a username and password. Refer to section 6.5 HTTP AUTHENTICATION of the FANUC Robot series - Ethernet Function - Operator's Manual (document B-82974EN for the R-30iA) for more information.\nCompatibility\nControllers\nCompatibility has only been tested with R-30iA and R-30iB+ controllers running V7.70, V9.10 and V9.30 of the system software, but others are expected to be compatible as well, as long as they have the required options (or equivalents).\nOperating Systems\nThe library and CLI tools have been written for Python version 3. No specific OS dependencies are known, so all platforms with a Python 3 interpreter should be supported. Only Ubuntu Xenial, Bionic and Focal have been extensively tested however.\nInstallation\nHelpers\nTranslate all .kl files in res/kl either with Roboguide or with the supplied Makefile. The latter will require a compatible version of GNU Make for Windows to be available on the %PATH%. Now copy the resultant .pc files to the controller.\nFinally, make sure to check the web server security settings (see Requirements).\nNo further setup is required.\nPackage\nIt's recommended to use a virtual Python 3 environment and install the package in it. The author has primarily used Python 3.8, but other versions are expected to work, though they are not actively tested.\nFuture versions may be released to PyPi.\nExample (install dominh 0.4.2; be sure to update the URL to download the desired version):\npython3 -m venv $HOME/venv_dominh\nsource $HOME/venv_dominh/bin/activate\npip install -U pip wheel setuptools\npip install https://github.com/gavanderhoorn/dominh/archive/0.4.2.tar.gz\ndominh --version\nThe last command should then print \"dominh 0.4.2\".\nExample usage\nExample script\nThe examples directory contains an example script which uses some of the functionality of dominh to print various bits of information retrieved from a controller.\nExample output for an R-30iA with an M-10iA in Roboguide:\nClick to expand\nNote: this script may fail on controllers which do not have UOP and SOP configured (gavanderhoorn/dominh#7).\nLibrary\nThe following shows a short example of how this library could be used to connect to a controller with credentials for access to the KAREL resource, reset the controller, then set the override to 100% and finally read the DOUT[1] IO element.\nClick to expand\nCLI\nClick to expand\nSupported functionality\nDominh can currently be used to (incomplete list):\nfault reset the controller\ndetermine whether the controller is:\nfaulted\ne-stopped\nexecuting a program\npaused\nin AUTO or MANUAL mode\nin REMOTE mode\ndetermine whether the TP is enabled\ndetermine the controller series (ie: R-30iA, R-30iB, R-30iB+)\ndetermine whether a specific options is installed\nretrieve the application software (ie: HandlingTool, etc)\nretrieve the version of the application software\nretrieve the list of installed options\nretrieve the controller's time and date\nread/write (system) variables (scalar variables are mostly supported)\nread/write IO elements (digital, analogue, group, UOP, SOP, TP, flags, markers, etc)\nread/write numerical registers\nread/write string registers\nretrieve the number of numerical and string registers\nread/write the general override\nretrieve the number of defined groups\nretrieve the ID and model of configured robots\nretrieve jog, tool and user frames\nretrieve currently active jog, tool or user frame\nretrieve the current TCP pose (in the currently active User Frame)\nretrieve payload schedules\nretrieve the list of errors (including history)\nretrieve a list of programs (filtered by program type)\nupdate the comments of numeric, string and position registers and IO ports\nThe above list only includes the functionality offered by the Controller class' public interface and that of the returned objects. Much more is possible (especially in the area of system variable wrapping/retrieving), but would require adding more convenience methods.\nLimitations / Known issues\nThe following limitations and known issues exist:\nOnly a small subset of the functionality offered by the PCDK is supported.\nSeveral methods have a high runtime overhead. This is largely caused by the use of the Fanuc web server as an intermediary and the resulting need to download and parse returned HTML. The library makes use of .stm files and zero-output KCL commands where possible, but cannot avoid parsing some pages.\nNot all methods are symmetric (ie: not all getters have setters). This may change in the future.\n\"Robot Out\" (ie: RDO) is not writable. The port name as specified in the Fanuc manual on KCL does not seem to work.\nDominh CLI tools wrap only a subset of the library's functionality.\nCLI tools currently do not support authentication\nEven though some helpers return JSON, HTTP headers returned by the web server do not reflect this. This is a limitation of the web server used by Fanuc.\nHTTP status return codes do not reflect the result of operations in all cases. This is again a limitation of the web server used by Fanuc.\nPerformance\nAs an indication of the performance: reading DOUT[1] from an idle R-30iB+ takes about 300 to 400 ms. Retrieving the value of the $FNO system variable from the same controller takes also about 300 to 400 ms.\nIn both cases the helpers were already present on the controller and the controller was idle (ie: no user TP or Karel programs were running).\nRelated projects\nFor a similar library, but written in Go, see onerobotics/go-fanuc.\nBugs, feature requests, etc\nPlease use the GitHub issue tracker.\nFAQ\nDoesn't this emulate the PCDK?\nYes, it does.\nWhy create it then?\nThe PCDK is primarily targeted at Windows and .NET and C++. I needed something that worked on a non-Windows OS and with languages other than .NET and C++. Python was a natural choice for me.\nWhy did you not use Go/Rust/Java/Kotlin/Swift/anything but Python?\nTime and application requirements: target framework supported Python, so writing Dominh in Python made sense.\nThis is far from production-ready code\nYes, I agree. See also the NOTE in the Overview section.\nPerformance is really bad\nCompared to the PCDK: certainly, but if you need a more performant solution, ask Fanuc for a PCDK license or use a fieldbus. If you have ideas on how to improve performance, post an issue on the tracker.\nWhy not use Karel more?\nWhile it would certainly possible to delegate much more of the functionality to Karel programs on the controller, it would also increase the 'footprint' of dominh. As much of the functionality is not intended to be used in performance critical parts of applications, I decided it would be interesting to see how much could be done with existing interfaces and functionality already provided by Fanuc. Dominh uses the KCL and built-in web server as much as possible. Only where absolutely necessary (or where very much more efficient) is Karel used.\nCan I submit feature/enhancement requests?\nOf course! I can't guarantee I'll have time to work on them though.\nWould you take pull requests which add new features?\nMost certainly. As long as new features (or enhancements of existing functionality) pass CI and are reasonably implemented, they will be merged.\nHow should Dominh be pronounced?\nDomin-a (the h is silent).\nWhy can't I change active payload?\nAccording to Fanuc, changing just the system variables which contain information about payload schedules is not sufficient. The UI on the TP triggers updates of internal controller settings other than those system variables, and writing to the variables alone does not trigger those internal updates. The PCDK does not have that problem as it uses FANUC RPC (on tcp://:3002) which has a direct interface to controller internals.\nDisclaimer\nThe author of this software is not affiliated with FANUC Corporation in any way. All trademarks and registered trademarks are property of their respective owners, and company, product and service names mentioned in this readme or appearing in source code or other artefacts in this repository are used for identification purposes only. Use of these names does not imply endorsement by FANUC Corporation.", "link": "https://github.com/gavanderhoorn/dominh", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "dominh\ndiscussion\nif you happen to find this useful, leave a quick note in the discussions section. if something doesn't work, open an issue on the tracker.\noverview\nthis is a poor man's version of a subset of the rpc functionality provided by the (windows-only) fanuc pcdk implemented in python. this uses the web svr enhancements option (r626) and the interfaces it provides to the controller (see note in requirements). only a subset of the functionality is currently available and performance will not be comparable with the pcdk.\nadditionally a simple set of cli tools is included, which allows some of the library's functionality to be used from the command line and/or in shell scripts.\nnote: this is only meant as an example of such a remote control facility. proper integration of a fanuc controller with an external application or workcell should be done using either the real pcdk, a fieldbus or similar technology. the scripts and functionality provided here are only a convenience and are only intended to be used in academic and laboratory settings. they allow incidental external access to a controller without needing to use any additional hardware. do not use this on production systems or in contexts where any kind of determinism is required. the author recommends using pcdk and/or any of the supported fieldbuses in those cases.\nnote 2: on r-30ib+ controllers, option r912 can do some of the things this library supports.\nnote 3: on controllers with sufficiently recent system software versions, an opc-ua server is included. access over opc-ua would also support some of the things this library supports.\ntoc\nrequirements\ncompatibility\ninstallation\nexample usage\nsupported functionality\nlimitations / known issues\nperformance\nrelated projects\nbugs, feature requests, etc\nfaq\ndisclaimer\nrequirements\nthis needs the base web server (http) and the web svr enhancements (r626) options (note: contrary to what the manual states, this is not a separate option: all functionality is already built-in, at least for r-30ia and newer controllers). as some parts are written in karel, option r632 could be a requirement, but this is unlikely.\nother requirements include a functioning networking setup (make sure you can ping the controller and the controller's website shows up when opening http://robot_ip), and correctly configured http authentication and ftp server settings. either unlock the karel and kcl resources completely or configure sets of credentials for both resources. configuration for the built-in ftp server should be ok by default, but may be changed to also require a username and password. refer to section 6.5 http authentication of the fanuc robot series - ethernet function - operator's manual (document b-82974en for the r-30ia) for more information.\ncompatibility\ncontrollers\ncompatibility has only been tested with r-30ia and r-30ib+ controllers running v7.70, v9.10 and v9.30 of the system software, but others are expected to be compatible as well, as long as they have the required options (or equivalents).\noperating systems\nthe library and cli tools have been written for python version 3. no specific os dependencies are known, so all platforms with a python 3 interpreter should be supported. only ubuntu xenial, bionic and focal have been extensively tested however.\ninstallation\nhelpers\ntranslate all .kl files in res/kl either with roboguide or with the supplied makefile. the latter will require a compatible version of gnu make for windows to be available on the %path%. now copy the resultant .pc files to the controller.\nfinally, make sure to check the web server security settings (see requirements).\nno further setup is required.\npackage\nit's recommended to use a virtual python 3 environment and install the package in it. the author has primarily used python 3.8, but other versions are expected to work, though they are not actively tested.\nfuture versions may be released to pypi.\nexample (install dominh 0.4.2; be sure to update the url to download the desired version):\npython3 -m venv $home/venv_dominh\nsource $home/venv_dominh/bin/activate\npip install -u pip wheel setuptools\npip install https://github.com/gavanderhoorn/dominh/archive/0.4.2.tar.gz\ndominh --version\nthe last command should then print \"dominh 0.4.2\".\nexample usage\nexample script\nthe examples directory contains an example script which uses some of the functionality of dominh to print various bits of information retrieved from a controller.\nexample output for an r-30ia with an m-10ia in roboguide:\nclick to expand\nnote: this script may fail on controllers which do not have uop and sop configured (gavanderhoorn/dominh#7).\nlibrary\nthe following shows a short example of how this library could be used to connect to a controller with credentials for access to the karel resource, reset the controller, then set the override to 100% and finally read the dout[1] io element.\nclick to expand\ncli\nclick to expand\nsupported functionality\ndominh can currently be used to (incomplete list):\nfault reset the controller\ndetermine whether the controller is:\nfaulted\ne-stopped\nexecuting a program\npaused\nin auto or manual mode\nin remote mode\ndetermine whether the tp is enabled\ndetermine the controller series (ie: r-30ia, r-30ib, r-30ib+)\ndetermine whether a specific options is installed\nretrieve the application software (ie: handlingtool, etc)\nretrieve the version of the application software\nretrieve the list of installed options\nretrieve the controller's time and date\nread/write (system) variables (scalar variables are mostly supported)\nread/write io elements (digital, analogue, group, uop, sop, tp, flags, markers, etc)\nread/write numerical registers\nread/write string registers\nretrieve the number of numerical and string registers\nread/write the general override\nretrieve the number of defined groups\nretrieve the id and model of configured robots\nretrieve jog, -----> tool !!!  and user frames\nretrieve currently active jog, -----> tool !!!  or user frame\nretrieve the current tcp pose (in the currently active user frame)\nretrieve payload schedules\nretrieve the list of errors (including history)\nretrieve a list of programs (filtered by program type)\nupdate the comments of numeric, string and position registers and io ports\nthe above list only includes the functionality offered by the controller class' public interface and that of the returned objects. much more is possible (especially in the area of system variable wrapping/retrieving), but would require adding more convenience methods.\nlimitations / known issues\nthe following limitations and known issues exist:\nonly a small subset of the functionality offered by the pcdk is supported.\nseveral methods have a high runtime overhead. this is largely caused by the use of the fanuc web server as an intermediary and the resulting need to download and parse returned html. the library makes use of .stm files and zero-output kcl commands where possible, but cannot avoid parsing some pages.\nnot all methods are symmetric (ie: not all getters have setters). this may change in the future.\n\"robot out\" (ie: rdo) is not writable. the port name as specified in the fanuc manual on kcl does not seem to work.\ndominh cli tools wrap only a subset of the library's functionality.\ncli tools currently do not support authentication\neven though some helpers return json, http headers returned by the web server do not reflect this. this is a limitation of the web server used by fanuc.\nhttp status return codes do not reflect the result of operations in all cases. this is again a limitation of the web server used by fanuc.\nperformance\nas an indication of the performance: reading dout[1] from an idle r-30ib+ takes about 300 to 400 ms. retrieving the value of the $fno system variable from the same controller takes also about 300 to 400 ms.\nin both cases the helpers were already present on the controller and the controller was idle (ie: no user tp or karel programs were running).\nrelated projects\nfor a similar library, but written in go, see onerobotics/go-fanuc.\nbugs, feature requests, etc\nplease use the github issue tracker.\nfaq\ndoesn't this emulate the pcdk?\nyes, it does.\nwhy create it then?\nthe pcdk is primarily targeted at windows and .net and c++. i needed something that worked on a non-windows os and with languages other than .net and c++. python was a natural choice for me.\nwhy did you not use go/rust/java/kotlin/swift/anything but python?\ntime and application requirements: target framework supported python, so writing dominh in python made sense.\nthis is far from production-ready code\nyes, i agree. see also the note in the overview section.\nperformance is really bad\ncompared to the pcdk: certainly, but if you need a more performant solution, ask fanuc for a pcdk license or use a fieldbus. if you have ideas on how to improve performance, post an issue on the tracker.\nwhy not use karel more?\nwhile it would certainly possible to delegate much more of the functionality to karel programs on the controller, it would also increase the 'footprint' of dominh. as much of the functionality is not intended to be used in performance critical parts of applications, i decided it would be interesting to see how much could be done with existing interfaces and functionality already provided by fanuc. dominh uses the kcl and built-in web server as much as possible. only where absolutely necessary (or where very much more efficient) is karel used.\ncan i submit feature/enhancement requests?\nof course! i can't guarantee i'll have time to work on them though.\nwould you take pull requests which add new features?\nmost certainly. as long as new features (or enhancements of existing functionality) pass ci and are reasonably implemented, they will be merged.\nhow should dominh be pronounced?\ndomin-a (the h is silent).\nwhy can't i change active payload?\naccording to fanuc, changing just the system variables which contain information about payload schedules is not sufficient. the ui on the tp triggers updates of internal controller settings other than those system variables, and writing to the variables alone does not trigger those internal updates. the pcdk does not have that problem as it uses fanuc rpc (on tcp://:3002) which has a direct interface to controller internals.\ndisclaimer\nthe author of this software is not affiliated with fanuc corporation in any way. all trademarks and registered trademarks are property of their respective owners, and company, product and service names mentioned in this readme or appearing in source code or other artefacts in this repository are used for identification purposes only. use of these names does not imply endorsement by fanuc corporation.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000923, "year": null}, {"Unnamed: 0": 1966, "autor": 946, "date": null, "content": "Setup guide for obstacle avoidance on the Jackal\nStep 1: Clone the source\nSetup a static IP connection between your computer and the Jackal. If you're doing this over ethernet, set your IP to 192.168.1.X where X can be anything except 11. The Jackal's IP address is 192.168.11. Then do the following:\n$ ssh administrator@192.168.1.11\nThe password is clearpath.\n~$ cd ~/catkin_ws\n~/catkin_ws$ git clone https://github.com/sourishg/jackal-navigation\n~/catkin_ws$ catkin_make\nClone and make the repository in the Intel NUC. The NUC does all the processing for obstacle avoidance. You need to setup remote ROS between the NUC and the Jackal's computer. Make sure you set up a two-way connection (check this by pinging one computer from another). rostopic echo to see if you can receive the topics published by the Jackal on the NUC (or your computer).\nStep 2: Start the cameras\nSSH into the Jackal to start the cameras.\nInstall UVC camera driver for the webcams:\n$ sudo apt-get install ros-<distro>-uvc-camera\nLogitech C920 webcams\n$ sudo chmod a+rw /dev/video*\n$ roslaunch jackal_nav stereo.launch\nIf this fails, just repeat the above steps till it works! The camera topics are:\n/webcam/left/image_raw/compressed\n/webcam/right/image_raw/compressed\nPointGrey IMX249\nThe launch files for the PointGrey cameras are stored in the pointgrey_camera_driver package. roscd into the package if you want to edit the launch files. Check the ID of the cameras by running\n$ rosrun pointgrey_camera_driver list_cameras\nIf you have the launch files ready, then start the cameras by running\n$ roslaunch pointgrey_camera_driver camera_left.launch\n$ roslaunch pointgrey_camera_driver camera_right.launch\nThe camera topics are:\n/camera_left/image_color/compressed\n/camera_right/image_color/compressed\nMake sure the cameras are actually grabbing frames by running rostopic echo on these topics.\nStep 3: Camera Calibration\nGrab images of the checkerboard to calibrate the cameras on the Jackal. Run the following node:\n$ rosrun jackal_nav grab_frames [options]\noptions:\n-w=width, specify image width\n-h=height, specify image height\nOnce you have the calibration images saved, use this tool to calibrate for the intrinsics and the extrinsics. An example calibration file is saved in calibration/amrl_jackal_webcam_stereo.yml. The XR and XT matrices in the calibration file are the transformation matrices from the camera frame to the robot frame.\nInitially after stereo calibration (using the tool mentioned above) you will not have the XR and XT matrices in your calibration file. Just manually add the two matrices and set them to the identity and zero matrices respectively. Also, you only need the following matrices in your calibration file: K1, K2, D1, D2, R, T, XR, and XT.\nNow you need to perform the extrinsic calibration between the rover reference frame and the left camera reference frame to detect the ground plane. To do this, run the point_cloud binary with the -g and -m flag (described below). Then run\n$ rosrun rqt_reconfigure rqt_reconfigure\nTweak the rotation and translation matrix parameters and see the observed point cloud in an rviz window. Visually align the point cloud so that the ground plane aligns with rviz's ground plane. After this is done, copy the transformation matrices that are printed on the terminal running the point_cloud binary to the calibration file.\nStep 4: Generate disparity map, point cloud, and obstacle scan\nOnce the camera topics are being published, generate a point cloud and an obstacle scan using this command.\n$ rosrun jackal_nav point_cloud -c=path/to/calib/file [options]\noptions:\n-h=height, specify height of the left and right image from the top, so that only a partial disparity map can be computed\n-g, generates point cloud before obstacle scan\n-l, log time taken for each step\n-d=path/to/dmap/time/file, specify the file where time taken by disparity map is stored for each frame\n-p=path/to/pcl/time/file, specify the file where time taken to generate a point cloud is stored for each frame\n-s=path/to/scan/time/file, specify the file where time taken to scan for obstacles is stored for each frame\n-m, flag to perform extrinsic calibration between the rover reference frame and the left camera reference frame.\nStep 5: Safe navigation\nNow the run the navigate node for safe navigation.\n$ rosrun jackal_nav navigate [options]\noptions:\n-f=max_forward_vel, specify maximum forward velocity\n-l=laser_scan_threshold, specify a threshold for the number of laser points in front of the robot which determines whether there is an obstacle or not\n-c=forward_clearance, the forward clearance for the robot (e.g. 1m forward clearance will make the robot stop if there's an obstacle within 1m)\nDrive modes\nUse the following combos on the DualShock controller to switch between modes\nR1 + R2: Use left stick to drive. The jackal stops in front of obstacles.\nHold X: Let the Jackal move in the direction it's facing, avoiding obstacles in its way. Adjust speed using the left stick.", "link": "https://github.com/sourishg/jackal-navigation", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "setup guide for obstacle avoidance on the jackal\nstep 1: clone the source\nsetup a static ip connection between your computer and the jackal. if you're doing this over ethernet, set your ip to 192.168.1.x where x can be anything except 11. the jackal's ip address is 192.168.11. then do the following:\n$ ssh administrator@192.168.1.11\nthe password is clearpath.\n~$ cd ~/catkin_ws\n~/catkin_ws$ git clone https://github.com/sourishg/jackal-navigation\n~/catkin_ws$ catkin_make\nclone and make the repository in the intel nuc. the nuc does all the processing for obstacle avoidance. you need to setup remote ros between the nuc and the jackal's computer. make sure you set up a two-way connection (check this by pinging one computer from another). rostopic echo to see if you can receive the topics published by the jackal on the nuc (or your computer).\nstep 2: start the cameras\nssh into the jackal to start the cameras.\ninstall uvc camera driver for the webcams:\n$ sudo apt-get install ros-<distro>-uvc-camera\nlogitech c920 webcams\n$ sudo chmod a+rw /dev/video*\n$ roslaunch jackal_nav stereo.launch\nif this fails, just repeat the above steps till it works! the camera topics are:\n/webcam/left/image_raw/compressed\n/webcam/right/image_raw/compressed\npointgrey imx249\nthe launch files for the pointgrey cameras are stored in the pointgrey_camera_driver package. roscd into the package if you want to edit the launch files. check the id of the cameras by running\n$ rosrun pointgrey_camera_driver list_cameras\nif you have the launch files ready, then start the cameras by running\n$ roslaunch pointgrey_camera_driver camera_left.launch\n$ roslaunch pointgrey_camera_driver camera_right.launch\nthe camera topics are:\n/camera_left/image_color/compressed\n/camera_right/image_color/compressed\nmake sure the cameras are actually grabbing frames by running rostopic echo on these topics.\nstep 3: camera calibration\ngrab images of the checkerboard to calibrate the cameras on the jackal. run the following node:\n$ rosrun jackal_nav grab_frames [options]\noptions:\n-w=width, specify image width\n-h=height, specify image height\nonce you have the calibration images saved, use this -----> tool !!!  to calibrate for the intrinsics and the extrinsics. an example calibration file is saved in calibration/amrl_jackal_webcam_stereo.yml. the xr and xt matrices in the calibration file are the transformation matrices from the camera frame to the robot frame.\ninitially after stereo calibration (using the tool mentioned above) you will not have the xr and xt matrices in your calibration file. just manually add the two matrices and set them to the identity and zero matrices respectively. also, you only need the following matrices in your calibration file: k1, k2, d1, d2, r, t, xr, and xt.\nnow you need to perform the extrinsic calibration between the rover reference frame and the left camera reference frame to detect the ground plane. to do this, run the point_cloud binary with the -g and -m flag (described below). then run\n$ rosrun rqt_reconfigure rqt_reconfigure\ntweak the rotation and translation matrix parameters and see the observed point cloud in an rviz window. visually align the point cloud so that the ground plane aligns with rviz's ground plane. after this is done, copy the transformation matrices that are printed on the terminal running the point_cloud binary to the calibration file.\nstep 4: generate disparity map, point cloud, and obstacle scan\nonce the camera topics are being published, generate a point cloud and an obstacle scan using this command.\n$ rosrun jackal_nav point_cloud -c=path/to/calib/file [options]\noptions:\n-h=height, specify height of the left and right image from the top, so that only a partial disparity map can be computed\n-g, generates point cloud before obstacle scan\n-l, log time taken for each step\n-d=path/to/dmap/time/file, specify the file where time taken by disparity map is stored for each frame\n-p=path/to/pcl/time/file, specify the file where time taken to generate a point cloud is stored for each frame\n-s=path/to/scan/time/file, specify the file where time taken to scan for obstacles is stored for each frame\n-m, flag to perform extrinsic calibration between the rover reference frame and the left camera reference frame.\nstep 5: safe navigation\nnow the run the navigate node for safe navigation.\n$ rosrun jackal_nav navigate [options]\noptions:\n-f=max_forward_vel, specify maximum forward velocity\n-l=laser_scan_threshold, specify a threshold for the number of laser points in front of the robot which determines whether there is an obstacle or not\n-c=forward_clearance, the forward clearance for the robot (e.g. 1m forward clearance will make the robot stop if there's an obstacle within 1m)\ndrive modes\nuse the following combos on the dualshock controller to switch between modes\nr1 + r2: use left stick to drive. the jackal stops in front of obstacles.\nhold x: let the jackal move in the direction it's facing, avoiding obstacles in its way. adjust speed using the left stick.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000946, "year": null}, {"Unnamed: 0": 1967, "autor": 947, "date": null, "content": "inspexel\nThe swiss army knife for dynamixel servo motors.\nThis software's purpose is to have a command line tool to configure dynamixel motors. Dynamixel motors are being used in many robotic projects but it seems like some tooling around the motors is missing. There exists the robotis tool RoboManager which only runs under Windows. There are also other projects like Mixcell which bring some of the functionality to Linux. One big issue with these tools is that they have a GUI which makes it hard to use them over ssh on remote computers. Also it is not possible to use them inside of scripts. Inspexel is a command-line only utility to fill this gap. It also enables the generation of scripted (via bash) motions.\nFeatures\nAutomatic motor discovery\nAll baudrates (even nonstandard baudrates)\nDynamixel Protocol V1 and V2\nSupport for all currently produced dynamixel motors\nReading registers with additional information and pretty output\nReading/Writing individual registers or register groups\nRepresent motors and their registers as a fuse mounted filesystem for easy scriptability\nReboot motors\nsupported motors\nFamily Subtypes\nMX28 MX-28T, MX-28R, MX-28AT, MX-28AR\nMX64 MX-64T, MX-64R, MX-64AT, MX-64AR\nMX106 MX-106T, MX-106R\nMX12 MX-12W\nMX28-V2 MX-28T-V2, MX-28R-V2, MX-28AT-V2, MX-28AR-V2\nMX64-V2 MX-64T-V2, MX-64R-V2, MX-64AT-V2, MX-64AR-V2\nMX106-V2 MX-106T-V2, MX-106R-V2\nXH430-W350 XH430-W350-T, XH430-W350-R\nXH430-W210 XH430-W210-T, XH430-W210-R\nXM430-W350 XM430-W350-T, XM430-W350-R\nXM430-W210 XM430-W210-T, XM430-W210-R\nXH430-V350 XH430-V350\nXH430-V210 XH430-V210\nXL430-W250 XL430-W250\nXM540-W150 XM540-W150-T, XM540-W150-R\nXM540-W270 XM540-W270-T, XM540-W270-R\nM42-10-S260-R M42-10-S260-R\nM54-40-S250-R M54-40-S250-R\nM54-60-S250-R M54-60-S250-R\nH42-20-S300-R H42-20-S300-R\nH54-100-S500-R H54-100-S500-R\nH54-200-S500-R H54-200-S500-R\nXL-320 XL-320\nAX-12A AX-12A\nAX-18A AX-18A\nAX-12W AX-12W\nUsage\nInspexel comes with several subcommands. Each subommand represents a different aspect or way to configure a dynamixel motor or inquire its configuration. All commands accept the arguments\n--device [path-to-serial-device] select the serial device\n--baudrate [baudrate-in-baud] select the baudrate to communicate to the motors (some commands also support multiple baudrates)\n--protocol_verion [1/2] use either protocol version 1 or 2\ndetect all connected motors\n$ inspexel\nor:\n$ inspexel detect\nAdd the flag --read_all flag to get the content of all registers nicely printed.\n$ inspexel detect --read_all\n{% picture default assets/images/inspexel.png --alt console output of inspexel %} console output of inspexel\nReboot a motor\nReboots motor with id 3\n$ inspexel reboot --id 3\nManual setting of register(s)\nSet register 0x44 (68) of motor 3 to value 1\n$ inspexel set_register --register 0x44 --values 1 --id 0x03\nFuse integration\nInspexel can expose all registers of all connected as a fuse filesystem.\n$ inspexel fuse\nInspexel will create a directory dynamixelFS. Then a detect cycle is run automatically and every detected motor will be represented in a subdirectory of dynamixelFS (e.g., dynamixelFS/11/ for motor with id 11). Within that directory are two subdirectories containing files representing the registers either by name dynamixelFS/11/by-register-name or by address dynamixelFS/11/by-register-id. A read on any of the containing files will make inspexel perform a read of the corresponding register and return the content as string. You can use that to live monitor the value of a register:\n$ inspexel fuse & && watch cat \"dynamixelFS/11/by-register-name/Present\\ Position\"\nLikewise you can set register values as if they were files:\n$ inspexel fuse & && echo 1 > dynamixelFS/11/by-register-name/LED\nFurther you can manually trigger detection of a motor by writing the motorID to look for to dynamixelFS/detect_motor:\n$ echo 11 > dynamixelFS/detect_motor\nMiscellaneous\ngetting help:\n$ inspexel --help\nManpage:\n$ man inspexel\nHow to install\nUbuntu 16.04\n# install Fuse deps\n$ sudo apt-get install libfuse-dev\n# install gcc-8\n$ sudo add-apt-repository ppa:ubuntu-toolchain-r/test -y\n$ sudo apt-get update\n$ sudo apt-get install g++-8 gcc-8\n# build inspexel\n$ git clone https://github.com/gottliebtfreitag/inspexel.git\n$ cd inspexel\n$ make && sudo make install\nUbuntu 18.04\n# install gcc-8\n$ sudo add-apt-repository ppa:ubuntu-toolchain-r/test -y\n$ sudo apt-get update\n$ sudo apt-get install g++-8 gcc-8\n# install Fuse deps\n$ sudo apt-get install libfuse-dev\n# build inspexel\n$ git clone https://github.com/gottliebtfreitag/inspexel.git\n$ cd inspexel\n$ make CXX=/usr/bin/g++-8 && sudo make install\nUbuntu 20.04\n# install Fuse deps\n$ sudo apt-get install libfuse-dev\n# build inspexel\n$ git clone https://github.com/gottliebtfreitag/inspexel.git\n$ cd inspexel\n$ make && sudo make install\nArchlinux\n# build inspexel\n$ git clone https://github.com/gottliebtfreitag/inspexel.git\n$ cd inspexel\n$ make && sudo make install", "link": "https://github.com/gottliebtfreitag/inspexel", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "inspexel\nthe swiss army knife for dynamixel servo motors.\nthis software's purpose is to have a command line -----> tool !!!  to configure dynamixel motors. dynamixel motors are being used in many robotic projects but it seems like some tooling around the motors is missing. there exists the robotis tool robomanager which only runs under windows. there are also other projects like mixcell which bring some of the functionality to linux. one big issue with these tools is that they have a gui which makes it hard to use them over ssh on remote computers. also it is not possible to use them inside of scripts. inspexel is a command-line only utility to fill this gap. it also enables the generation of scripted (via bash) motions.\nfeatures\nautomatic motor discovery\nall baudrates (even nonstandard baudrates)\ndynamixel protocol v1 and v2\nsupport for all currently produced dynamixel motors\nreading registers with additional information and pretty output\nreading/writing individual registers or register groups\nrepresent motors and their registers as a fuse mounted filesystem for easy scriptability\nreboot motors\nsupported motors\nfamily subtypes\nmx28 mx-28t, mx-28r, mx-28at, mx-28ar\nmx64 mx-64t, mx-64r, mx-64at, mx-64ar\nmx106 mx-106t, mx-106r\nmx12 mx-12w\nmx28-v2 mx-28t-v2, mx-28r-v2, mx-28at-v2, mx-28ar-v2\nmx64-v2 mx-64t-v2, mx-64r-v2, mx-64at-v2, mx-64ar-v2\nmx106-v2 mx-106t-v2, mx-106r-v2\nxh430-w350 xh430-w350-t, xh430-w350-r\nxh430-w210 xh430-w210-t, xh430-w210-r\nxm430-w350 xm430-w350-t, xm430-w350-r\nxm430-w210 xm430-w210-t, xm430-w210-r\nxh430-v350 xh430-v350\nxh430-v210 xh430-v210\nxl430-w250 xl430-w250\nxm540-w150 xm540-w150-t, xm540-w150-r\nxm540-w270 xm540-w270-t, xm540-w270-r\nm42-10-s260-r m42-10-s260-r\nm54-40-s250-r m54-40-s250-r\nm54-60-s250-r m54-60-s250-r\nh42-20-s300-r h42-20-s300-r\nh54-100-s500-r h54-100-s500-r\nh54-200-s500-r h54-200-s500-r\nxl-320 xl-320\nax-12a ax-12a\nax-18a ax-18a\nax-12w ax-12w\nusage\ninspexel comes with several subcommands. each subommand represents a different aspect or way to configure a dynamixel motor or inquire its configuration. all commands accept the arguments\n--device [path-to-serial-device] select the serial device\n--baudrate [baudrate-in-baud] select the baudrate to communicate to the motors (some commands also support multiple baudrates)\n--protocol_verion [1/2] use either protocol version 1 or 2\ndetect all connected motors\n$ inspexel\nor:\n$ inspexel detect\nadd the flag --read_all flag to get the content of all registers nicely printed.\n$ inspexel detect --read_all\n{% picture default assets/images/inspexel.png --alt console output of inspexel %} console output of inspexel\nreboot a motor\nreboots motor with id 3\n$ inspexel reboot --id 3\nmanual setting of register(s)\nset register 0x44 (68) of motor 3 to value 1\n$ inspexel set_register --register 0x44 --values 1 --id 0x03\nfuse integration\ninspexel can expose all registers of all connected as a fuse filesystem.\n$ inspexel fuse\ninspexel will create a directory dynamixelfs. then a detect cycle is run automatically and every detected motor will be represented in a subdirectory of dynamixelfs (e.g., dynamixelfs/11/ for motor with id 11). within that directory are two subdirectories containing files representing the registers either by name dynamixelfs/11/by-register-name or by address dynamixelfs/11/by-register-id. a read on any of the containing files will make inspexel perform a read of the corresponding register and return the content as string. you can use that to live monitor the value of a register:\n$ inspexel fuse & && watch cat \"dynamixelfs/11/by-register-name/present\\ position\"\nlikewise you can set register values as if they were files:\n$ inspexel fuse & && echo 1 > dynamixelfs/11/by-register-name/led\nfurther you can manually trigger detection of a motor by writing the motorid to look for to dynamixelfs/detect_motor:\n$ echo 11 > dynamixelfs/detect_motor\nmiscellaneous\ngetting help:\n$ inspexel --help\nmanpage:\n$ man inspexel\nhow to install\nubuntu 16.04\n# install fuse deps\n$ sudo apt-get install libfuse-dev\n# install gcc-8\n$ sudo add-apt-repository ppa:ubuntu-toolchain-r/test -y\n$ sudo apt-get update\n$ sudo apt-get install g++-8 gcc-8\n# build inspexel\n$ git clone https://github.com/gottliebtfreitag/inspexel.git\n$ cd inspexel\n$ make && sudo make install\nubuntu 18.04\n# install gcc-8\n$ sudo add-apt-repository ppa:ubuntu-toolchain-r/test -y\n$ sudo apt-get update\n$ sudo apt-get install g++-8 gcc-8\n# install fuse deps\n$ sudo apt-get install libfuse-dev\n# build inspexel\n$ git clone https://github.com/gottliebtfreitag/inspexel.git\n$ cd inspexel\n$ make cxx=/usr/bin/g++-8 && sudo make install\nubuntu 20.04\n# install fuse deps\n$ sudo apt-get install libfuse-dev\n# build inspexel\n$ git clone https://github.com/gottliebtfreitag/inspexel.git\n$ cd inspexel\n$ make && sudo make install\narchlinux\n# build inspexel\n$ git clone https://github.com/gottliebtfreitag/inspexel.git\n$ cd inspexel\n$ make && sudo make install", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000947, "year": null}, {"Unnamed: 0": 1972, "autor": 952, "date": null, "content": "Anki Vector .NET SDK Sample applications\nThis solution contains 2 applications and 15 tutorial applications for the Anki Vector SDK.\nGetting Started\nDocumentation\nAnki Vector .NET SDK Documentation\nAnki Vector .NET SDK GitHub Project\nVector SDK Configuration and Authentication\nIn order to run these samples, you will need authenticate with the robot and create a configuration file that is stored in your user profile. This SDK uses the same configuration file as the Python SDK and the Vector Explorer application.\nThe easiest way to get setup with Vector on you Windows PC is to install Vector Explorer and configure your robot through that application. However, you can also use the VectorConfigure command line tool located in this solution.\nSample Projects\nVectorConfigure\nThis is a command line tool for configuring and authenticating the SDK with your Vector robot. You will need to run this command to create a configuration file in your profile to run the remaining SDK sample applications in this solution.\nYou will be prompted for your robot\u2019s name, ip address and serial number. You will also be asked for your Anki login and password. Make sure to use the same account that was used to set up your Vector. These credentials give full access to your robot, including camera stream, audio stream and data. Do not share these credentials.\nReserveControl\nCommand line tool for reserving control of Vector. Reserving control will keep Vector still during development so he doesn't drive off in-between runs of your code. It's also useful to keep Vector still when you need some peace and quiet.\nTutorial_01_HelloWorld\nEveryone's first project, Vector will speak \"Hello World\".\nTutorial_02_DriveSquare\nMake Vector drive in a square by going forward and turning left 4 times in a row.\nTutorial_03_Motors\nDrive Vector's wheels, lift and head motors directly. This is an example of how you can also have low-level control of Vector's motors (wheels, lift and head) for fine-grained control and ease of controlling multiple things at once.\nTutorial_04_Animation\nPlay a few of Vector's animations. Play an animation using a trigger, and then another animation by name.\nTutorial_05_DriveOnOffCharger\nTell Vector to return to his charger and then drive off.\nTutorial_06_FaceImage\nDisplay an JPEG image on Vector's face.\nTutorial_07_DockWithCube\nTell Vector to drive up to a seen cube. This example demonstrates Vector driving to and docking with a cube, without picking it up. Vector will line his arm hooks up with the cube so that they are inserted into the cube's corners. You must place a cube in front of Vector so that he can see it.\nTutorial_08_DownloadPhoto\nDownloads all the photos stored in Vector. Before running this script, please make sure you have successfully had Vector take a photo by saying, \"Hey Vector! Take a photo.\"\nTutorial_09_EyeColor\nSet Vector's eye color. Note that Vector's eye color will return to normal when the connection terminates.\nTutorial_10_PlayAudio\nPlay audio files through Vector's speaker. This will play an embedded MP3 music file through Vector's speakers.\nTutorial_11_DriveToCliffAndBackUp\nMake Vector drive to a cliff and back up. Place the robot about a foot from a \"cliff\" (such as a tabletop edge), then run this script.\nTutorial_12_ControlPriorityLevel\nVector maintains SDK behavior control after being picked up. During normal operation, SDK programs cannot maintain control over Vector when he is at a cliff, stuck on an edge or obstacle, tilted or inclined, picked up, in darkness, etc. This script demonstrates how to use the highest level SDK behavior control to make Vector perform actions that normally take priority over the SDK.\nTutorial_13_UserIntent\nReturn information about a voice commands given to Vector. The user_intent event is only dispatched when the SDK program has requested behavior control and Vector gets a voice command. After the robot hears \"Hey Vector! ...\" and a valid voice command is given for example \"...what time is it?\") the event will be dispatched and displayed.\nTutorial_14_FaceEvent\nWait for Vector to see a face, and then print output to the console. This script demonstrates how to set up a listener for an event. It subscribes to event ObservedFace. When that event is dispatched, the lambda is called, which prints text to the console. Vector will also say \"I see a face\" one time, and the program will exit when he finishes speaking.\nTutorial_15_FaceFollower\nMake Vector turn toward a face. This script shows off the turn towards face behavior. It will wait for a face and then constantly turn towards it to keep it in frame.\nTutorial_16_FeatureStatus\nDemonstration of the FeatureStatus event that shows which features Vector's AI is used to perform operations. This sample was contributed by Randall Maas.\nTutorial_17_AppIntent\nDemonstration of submitting AppIntents to Vector for processing. This sample was contributed by Randall Maas.\nTutorial_18_XboxDrive\nDrive Vector around with an XBox Controller. This sample was contributed by Randall Maas.\nGetting Help\nThere are numerous places to get help with programming Vector using the .NET SDK:\nOfficial Anki developer forums: https://forums.anki.com/\nAnki Vector developer subreddit: https://www.reddit.com/r/ankivectordevelopers\nAnki robots Discord chat: https://discord.gg/FT8EYwu", "link": "https://github.com/codaris/Anki.Vector.Samples", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "anki vector .net sdk sample applications\nthis solution contains 2 applications and 15 tutorial applications for the anki vector sdk.\ngetting started\ndocumentation\nanki vector .net sdk documentation\nanki vector .net sdk github project\nvector sdk configuration and authentication\nin order to run these samples, you will need authenticate with the robot and create a configuration file that is stored in your user profile. this sdk uses the same configuration file as the python sdk and the vector explorer application.\nthe easiest way to get setup with vector on you windows pc is to install vector explorer and configure your robot through that application. however, you can also use the vectorconfigure command line -----> tool !!!  located in this solution.\nsample projects\nvectorconfigure\nthis is a command line tool for configuring and authenticating the sdk with your vector robot. you will need to run this command to create a configuration file in your profile to run the remaining sdk sample applications in this solution.\nyou will be prompted for your robot\u2019s name, ip address and serial number. you will also be asked for your anki login and password. make sure to use the same account that was used to set up your vector. these credentials give full access to your robot, including camera stream, audio stream and data. do not share these credentials.\nreservecontrol\ncommand line tool for reserving control of vector. reserving control will keep vector still during development so he doesn't drive off in-between runs of your code. it's also useful to keep vector still when you need some peace and quiet.\ntutorial_01_helloworld\neveryone's first project, vector will speak \"hello world\".\ntutorial_02_drivesquare\nmake vector drive in a square by going forward and turning left 4 times in a row.\ntutorial_03_motors\ndrive vector's wheels, lift and head motors directly. this is an example of how you can also have low-level control of vector's motors (wheels, lift and head) for fine-grained control and ease of controlling multiple things at once.\ntutorial_04_animation\nplay a few of vector's animations. play an animation using a trigger, and then another animation by name.\ntutorial_05_driveonoffcharger\ntell vector to return to his charger and then drive off.\ntutorial_06_faceimage\ndisplay an jpeg image on vector's face.\ntutorial_07_dockwithcube\ntell vector to drive up to a seen cube. this example demonstrates vector driving to and docking with a cube, without picking it up. vector will line his arm hooks up with the cube so that they are inserted into the cube's corners. you must place a cube in front of vector so that he can see it.\ntutorial_08_downloadphoto\ndownloads all the photos stored in vector. before running this script, please make sure you have successfully had vector take a photo by saying, \"hey vector! take a photo.\"\ntutorial_09_eyecolor\nset vector's eye color. note that vector's eye color will return to normal when the connection terminates.\ntutorial_10_playaudio\nplay audio files through vector's speaker. this will play an embedded mp3 music file through vector's speakers.\ntutorial_11_drivetocliffandbackup\nmake vector drive to a cliff and back up. place the robot about a foot from a \"cliff\" (such as a tabletop edge), then run this script.\ntutorial_12_controlprioritylevel\nvector maintains sdk behavior control after being picked up. during normal operation, sdk programs cannot maintain control over vector when he is at a cliff, stuck on an edge or obstacle, tilted or inclined, picked up, in darkness, etc. this script demonstrates how to use the highest level sdk behavior control to make vector perform actions that normally take priority over the sdk.\ntutorial_13_userintent\nreturn information about a voice commands given to vector. the user_intent event is only dispatched when the sdk program has requested behavior control and vector gets a voice command. after the robot hears \"hey vector! ...\" and a valid voice command is given for example \"...what time is it?\") the event will be dispatched and displayed.\ntutorial_14_faceevent\nwait for vector to see a face, and then print output to the console. this script demonstrates how to set up a listener for an event. it subscribes to event observedface. when that event is dispatched, the lambda is called, which prints text to the console. vector will also say \"i see a face\" one time, and the program will exit when he finishes speaking.\ntutorial_15_facefollower\nmake vector turn toward a face. this script shows off the turn towards face behavior. it will wait for a face and then constantly turn towards it to keep it in frame.\ntutorial_16_featurestatus\ndemonstration of the featurestatus event that shows which features vector's ai is used to perform operations. this sample was contributed by randall maas.\ntutorial_17_appintent\ndemonstration of submitting appintents to vector for processing. this sample was contributed by randall maas.\ntutorial_18_xboxdrive\ndrive vector around with an xbox controller. this sample was contributed by randall maas.\ngetting help\nthere are numerous places to get help with programming vector using the .net sdk:\nofficial anki developer forums: https://forums.anki.com/\nanki vector developer subreddit: https://www.reddit.com/r/ankivectordevelopers\nanki robots discord chat: https://discord.gg/ft8eywu", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000952, "year": null}, {"Unnamed: 0": 1977, "autor": 957, "date": null, "content": "YAC - Yet Another Calibrator\nYet Another Calibrator (YAC) is a calibration tool for calibrating RGB cameras. Specifically:\nCamera intrinsics\nStereo camera intrinsics and extrinsics\nMocap-marker to camera extrinsics\nSupported projection-distortion models:\npinhole-radtan4\npinhole-equi4\nDependencies\n# Ubuntu packages\nlibyaml-cpp-dev\nlibeigen3-dev\nlibceres-dev\nlibopencv-dev\nlibomp-dev (optional)\n# Custom (see deps dir)\nModified apriltag3 (https://github.com/AprilRobotics/apriltag)\nModified apriltags (https://github.com/chutsu/apriltags)\nBuild\nFor the lazy :\n# The following assumes you have a catkin workspace at $HOME/catkin_ws\n# If not edit the CATKIN_WS variable in the Makefile\ngit clone https://github.com/chutsu/yac\ncd yac\nmake deps\nmake release\nOr standard approach:\n# Clone repo to your catkin workspace\ncd <your catkin workspace>/src\ngit clone https://github.com/chutsu/yac\n# Install dependencies\ncd yac && make deps\n# Build yac\ncatkin build -DCMAKE_BUILD_TYPE=Release yac yac_ros\n# Note: Optimization will be very slow if not built in RELEASE mode.\nCalibration Target\nyac uses a grid of AprilTags, a.k.a an AprilGrid, as a calibration target.\nClick here to download and print the AprilGrid. During data collection make sure the calibration target is as flat as possible.\nCalibrate Camera Intrinsics or Stereo Intrinsics and Extrinsics\nFirst inorder to calibrate a monocular camera or stereo camera pair we need to create the calibration configuration file. For example calib_intel_d435i.yaml configuration file:\nros:\nbag: \"/data/intel_d435i/imucam-1.bag\"\ncam0_topic: \"/stereo/camera0/image\"\ncalib_target:\ntarget_type: 'aprilgrid' # Target type\ntag_rows: 6 # Number of rows\ntag_cols: 6 # Number of cols\ntag_size: 0.088 # Size of apriltag, edge to edge [m]\ntag_spacing: 0.3 # Ratio of space between tags to tagSize\n# Example: tag_size=2m, spacing=0.5m\n# --> tag_spacing=0.25[-]\ncam0:\nproj_model: \"pinhole\"\ndist_model: \"radtan4\"\nlens_hfov: 69.4\nlens_vfov: 42.5\nresolution: [640.0, 480.0]\nrate: 30.0\ncam1:\nproj_model: \"pinhole\"\ndist_model: \"radtan4\"\nlens_hfov: 69.4\nlens_vfov: 42.5\nresolution: [640.0, 480.0]\nrate: 30.0\nThe above tells yac where to find the rosbag containing the calibration data, the calibration target and camera details. Now, we need to pass that to a yac calibrator via the roslaunch file calib_mono.launch or calib_stereo.launch:\n# To calibrate cam0 intrinsics only\nroslaunch yac calib_mono.launch config_file:=<path to calib_intel_d435i.yaml>\n# To calibrate stereo camera pair intrinsics and extrinsics\nroslaunch yac calib_stereo.launch config_file:=<path to calib_intel_d435i.yaml>\nLICENCE\nMIT", "link": "https://github.com/chutsu/yac", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "yac - yet another calibrator\nyet another calibrator (yac) is a calibration -----> tool !!!  for calibrating rgb cameras. specifically:\ncamera intrinsics\nstereo camera intrinsics and extrinsics\nmocap-marker to camera extrinsics\nsupported projection-distortion models:\npinhole-radtan4\npinhole-equi4\ndependencies\n# ubuntu packages\nlibyaml-cpp-dev\nlibeigen3-dev\nlibceres-dev\nlibopencv-dev\nlibomp-dev (optional)\n# custom (see deps dir)\nmodified apriltag3 (https://github.com/aprilrobotics/apriltag)\nmodified apriltags (https://github.com/chutsu/apriltags)\nbuild\nfor the lazy :\n# the following assumes you have a catkin workspace at $home/catkin_ws\n# if not edit the catkin_ws variable in the makefile\ngit clone https://github.com/chutsu/yac\ncd yac\nmake deps\nmake release\nor standard approach:\n# clone repo to your catkin workspace\ncd <your catkin workspace>/src\ngit clone https://github.com/chutsu/yac\n# install dependencies\ncd yac && make deps\n# build yac\ncatkin build -dcmake_build_type=release yac yac_ros\n# note: optimization will be very slow if not built in release mode.\ncalibration target\nyac uses a grid of apriltags, a.k.a an aprilgrid, as a calibration target.\nclick here to download and print the aprilgrid. during data collection make sure the calibration target is as flat as possible.\ncalibrate camera intrinsics or stereo intrinsics and extrinsics\nfirst inorder to calibrate a monocular camera or stereo camera pair we need to create the calibration configuration file. for example calib_intel_d435i.yaml configuration file:\nros:\nbag: \"/data/intel_d435i/imucam-1.bag\"\ncam0_topic: \"/stereo/camera0/image\"\ncalib_target:\ntarget_type: 'aprilgrid' # target type\ntag_rows: 6 # number of rows\ntag_cols: 6 # number of cols\ntag_size: 0.088 # size of apriltag, edge to edge [m]\ntag_spacing: 0.3 # ratio of space between tags to tagsize\n# example: tag_size=2m, spacing=0.5m\n# --> tag_spacing=0.25[-]\ncam0:\nproj_model: \"pinhole\"\ndist_model: \"radtan4\"\nlens_hfov: 69.4\nlens_vfov: 42.5\nresolution: [640.0, 480.0]\nrate: 30.0\ncam1:\nproj_model: \"pinhole\"\ndist_model: \"radtan4\"\nlens_hfov: 69.4\nlens_vfov: 42.5\nresolution: [640.0, 480.0]\nrate: 30.0\nthe above tells yac where to find the rosbag containing the calibration data, the calibration target and camera details. now, we need to pass that to a yac calibrator via the roslaunch file calib_mono.launch or calib_stereo.launch:\n# to calibrate cam0 intrinsics only\nroslaunch yac calib_mono.launch config_file:=<path to calib_intel_d435i.yaml>\n# to calibrate stereo camera pair intrinsics and extrinsics\nroslaunch yac calib_stereo.launch config_file:=<path to calib_intel_d435i.yaml>\nlicence\nmit", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000957, "year": null}, {"Unnamed: 0": 1982, "autor": 962, "date": null, "content": "BUILDING AN AUTOMATIC VEHICLE BASED ON STEREO CAMERA\nOverview\nThis project focuses on using stereo camera to build a vehicle which can map the surrounding area and autonomosly navigate a to the pointed destination in the built map. The vehicle is a Turtlebot model and based on ROS (Robot Operating System). For a further purpose, the vehicle can be developed to go around a house doing diffrent tasks or explore unknown enviroment.\nAlgorithm\nDependencies\nIn order to run the project, the following dependencies are required:\nUbuntu 16.04 here\nROS Kinetic Kame here\nCatkin tool here\nZED SDK (version 2.1) here\nCuda Toolkit (version 8.0) here\nFirmware for Mobile base here\nROS packages:\nzed_ros_wrapper\ndepthimage_to_laserscan\nlaser_scan_matcher\ngmapping\nmap_server\nmove_base\namcl\ndwa_local_planner\nInstall ROS package dependencies:\nsudo apt install ros-kinetic-<dependency_name>\nBuild and run the robot\nSetup workspace:\ngit clone https://github.com/minht57/ROS_Basic_SLAM.git\ncp -rf ROS_Basic_SLAM/src/* ~/catkin_ws/src/\nsudo chmod 777 ~/catkin_ws/src/zed-ros-wapper/cfg/Zed.cfg\nsudo chmod 777 ~/catkin_ws/src/depthimage_to_laserscan/cfg/Depth.cfg\ncd ~/catkin_ws\ncatkin_make\nMapping:\nRun the mapping:\nroslaunch depthimage_to_laserscan gmapping.launch\nUse RF controller to move the robot around. The map will be automatically built.\nTo see the map, launch Rviz and create visualization by adding 'Map' to the 'Display' console:\nrviz\nSave map:\nrosrun map_server map_saver -f ~/catkin_ws/test_map\nRaw map:\nEdit the map with GIMP for better map:\nNavigation:\nRun navigation:\nroslaunch navigation slam_amcl.launch\nSend velocity commands to move base via serial port:\nsudo chmod 777 /dev/ttyUSB0\nroslaunch serial serial.launch\nPlace a box in the area to check object avoidance algorithm.\nOpen Rviz:\nrviz\nResult:\nYoutube:\nMapping: here\nNavigation: here", "link": "https://github.com/minht57/ROS_Basic_SLAM", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "building an automatic vehicle based on stereo camera\noverview\nthis project focuses on using stereo camera to build a vehicle which can map the surrounding area and autonomosly navigate a to the pointed destination in the built map. the vehicle is a turtlebot model and based on ros (robot operating system). for a further purpose, the vehicle can be developed to go around a house doing diffrent tasks or explore unknown enviroment.\nalgorithm\ndependencies\nin order to run the project, the following dependencies are required:\nubuntu 16.04 here\nros kinetic kame here\ncatkin -----> tool !!!  here\nzed sdk (version 2.1) here\ncuda toolkit (version 8.0) here\nfirmware for mobile base here\nros packages:\nzed_ros_wrapper\ndepthimage_to_laserscan\nlaser_scan_matcher\ngmapping\nmap_server\nmove_base\namcl\ndwa_local_planner\ninstall ros package dependencies:\nsudo apt install ros-kinetic-<dependency_name>\nbuild and run the robot\nsetup workspace:\ngit clone https://github.com/minht57/ros_basic_slam.git\ncp -rf ros_basic_slam/src/* ~/catkin_ws/src/\nsudo chmod 777 ~/catkin_ws/src/zed-ros-wapper/cfg/zed.cfg\nsudo chmod 777 ~/catkin_ws/src/depthimage_to_laserscan/cfg/depth.cfg\ncd ~/catkin_ws\ncatkin_make\nmapping:\nrun the mapping:\nroslaunch depthimage_to_laserscan gmapping.launch\nuse rf controller to move the robot around. the map will be automatically built.\nto see the map, launch rviz and create visualization by adding 'map' to the 'display' console:\nrviz\nsave map:\nrosrun map_server map_saver -f ~/catkin_ws/test_map\nraw map:\nedit the map with gimp for better map:\nnavigation:\nrun navigation:\nroslaunch navigation slam_amcl.launch\nsend velocity commands to move base via serial port:\nsudo chmod 777 /dev/ttyusb0\nroslaunch serial serial.launch\nplace a box in the area to check object avoidance algorithm.\nopen rviz:\nrviz\nresult:\nyoutube:\nmapping: here\nnavigation: here", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000962, "year": null}, {"Unnamed: 0": 2017, "autor": 997, "date": null, "content": "The-love-I-receive-from-Udacity-reviewer-resources\nMy gratitude to Sir Sebastion Thrun, Instructor Katie Malone, Sir Peter Norvig, Sir Thad Starner,Sir Nicholas Roy, Sir David J. Malan, Sir David Silver, Sir Raffaello D'Andrea, Sir Ryan, Instructor Jay Alammar, Madam Dana Sheahen, Instructor Alexis Cook, Instructor Cezanne Camacho, Instructor Erica, Instructor Karim, Instructor Julia, Sir Stephen, Sir Aaron Brown, Sir Andy, Instructor Stefanie, Instructor Angela, Sir Andrew Trask, Sir Jonathan Larkin, Sir Gordon Ritter, Sir Justin Sheetz, Instructor Luis Serrano, Instructor Liz Otto Hamel, Instructor Eddie Shyu, Instructor Josh Bernhard, Instructor Parnian Barekatain, Instructor Miriam, Instructor Mat Leonard, Instructor Cindy, Instructor Brok Bucholtz, Instructor Arpan Chakraborty, Instructor Juan Delgado, Sir Jake, Sir Michael Virgo, Sir Tucker, Instructor Juno Lee, Sir Akshit, Brenda.Udacity, Palak.Udacity, Grace.Udacity, Instructor Grant Sanderson, Instructor Ortal, Instructor Jennifer Staab and Instructor Amanda Moran.\nTo memorize the journey start from self-driving Car Engineer Nanodegree Program to Artificial Intelligence Nanodegree Program to Flying Car Nanodegree Program then Robotics Software Nanodegree Program and CS50: Introduction to Computer Science 2018, To memorize the year of my enrollment in Udacity from 2017 to present.\nTo memorize mentors Donald, Christopher, Jafar and David who encourage and support me a lot through my enrollment years. Here I collect the feedback and paper references from my Udacity reviewers and friendships I got from Udacity and CS50X. I want to extend what I learn and also extend the spirits, share knowledge with all my classmates at least this is what I can do if any of this citation papers helps you please send a star to them or references their papers will be an honorable action:).\nWithin these periods I want to say thank you to Madam Olga Uskova, Mylene doublet o'kane and Sir Luigi Morelli. I love your writings and very appreciate every like you send to me. My gratitude to Martin McGovern, Karen E. Baker, and Udacity team you let me learn to be Udacious and persistence:D!\nFrom Madam Olga Uskova:\nhttps://en.wikipedia.org/wiki/Olga_Uskova\nhttps://vimeo.com/cognitivetech\nhttp://www.taipeitimes.com/News/feat/archives/2018/01/08/2003685375\nhttp://www.agriland.ie/farming-news/fully-driverless-combine-harvester-by-2024/\nPaper References from Self-Driving Car reviewers:\nhttps://airccj.org/CSCP/vol5/csit53211.pdf\nhttps://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_houghlines/py_houghlines.html\nhttp://www.kerrywong.com/2009/05/07/canny-edge-detection-auto-thresholding/\nhttps://medium.com/@vivek.yadav/improved-performance-of-deep-learning-neural-network-models-on-traffic-sign-classification-using-6355346da2dc\nhttps://medium.com/@jeremyeshannon/udacity-self-driving-car-nanodegree-project-2-traffic-sign-classifier-f52d33d4be9f\nhttps://github.com/tflearn/tflearn/blob/master/examples/images/googlenet.py\nhttps://keras.io/callbacks/#callback\nhttps://keras.io/callbacks/#modelcheckpoint\nhttp://cs229.stanford.edu/proj2015/054_report.pdf\nhttp://ruder.io/optimizing-gradient-descent/\nhttp://ruder.io/optimizing-gradient-descent/index.html#adam\nhttp://alexlenail.me/NN-SVG/LeNet.html\nhttps://keras.io/visualization/\nhttps://www.researchgate.net/publication/257291768_A_Much_Advanced_and_Efficient_Lane_Detection_Algorithm_for_Intelligent_Highway_Safety\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC5017478/\nhttps://medium.com/@mohankarthik/feature-extraction-for-vehicle-detection-using-hog-d99354a84d10\nhttps://chatbotslife.com/towards-a-real-time-vehicle-detection-ssd-multibox-approach-2519af2751c\nhttps://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python\nhttps://stackoverflow.com/questions/27466642/what-kind-of-optimization-does-const-offer-in-c-c\nhttps://www.youtube.com/watch?v=aUkBa1zMKv4\nhttp://correll.cs.colorado.edu/?p=965\nhttps://www.cs.cmu.edu/afs/cs/project/jair/pub/volume9/mazer98a-html/node2.html\nhttp://www.roborealm.com/help/Path_Planning.php\nhttps://robotics.stackexchange.com/questions/8302/what-is-the-difference-between-path-planning-and-motion-planning\nhttps://www.robotshop.com/community/forum/t/excellent-tutorial-on-a-robot-path-planning/13170\nhttps://www.mathworks.com/help/robotics/examples/path-planning-in-environments-of-difference-complexity.html;jsessionid=a7ff890a4e697fe79be723535659\nhttp://ais.informatik.uni-freiburg.de/teaching/ss11/robotics/slides/18-robot-motion-planning.pdf\nhttp://ais.informatik.uni-freiburg.de/teaching/ss10/robotics/slides/16-pathplanning.pdf\nhttp://ai.stanford.edu/~ddolgov/papers/dolgov_gpp_stair08.pdf\nhttps://webpages.uncc.edu/~jmconrad/GradStudents/Thesis_Ghangrekar.pdf\nhttp://blog.qure.ai/notes/semantic-segmentation-deep-learning-review\nhttps://arxiv.org/pdf/1411.4038.pdf\nhttp://cs231n.github.io/neural-networks-2/#init\nhttp://cs231n.github.io/neural-networks-2/#reg\nhttps://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network\nhttps://www.jeremyjordan.me/nn-learning-rate/\nhttps://arxiv.org/abs/1806.02446\nhttps://arxiv.org/abs/1903.03273\nhttps://arxiv.org/abs/1901.00114\nhttps://roscon.ros.org/2018/#program\nhttp://driving.stanford.edu/papers/ISER2010.pdf\nPaper References from Flying Car reviewers:\nhttp://planning.cs.uiuc.edu/\nhttps://docs.ufpr.br/~danielsantos/ProbabilisticRobotics.pdf\nhttps://www.youtube.com/playlist?list=PLX2gX-ftPVXU3oUFNATxGXY90AULiqnWT\nhttps://simondlevy.academic.wlu.edu/\nhttps://docs.google.com/viewer?url=https%3A%2F%2Fwww.seas.harvard.edu%2Fcourses%2Fcs281%2Fpapers%2Funscented.pdf\nhttp://andrew.gibiansky.com/downloads/pdf/Quadcopter%20Dynamics,%20Simulation,%20and%20Control.pdf http://aeroconf.org/\nhttps://arxiv.org/abs/1902.01465\nhttp://navion.mit.edu/\nhttps://www.researchgate.net/publication/322020415_Adaptive_Super-twisting_Second-order_Sliding_Mode_Control_for_Attitude_Control_of_Quadcopter_UAVs\nhttps://arxiv.org/abs/1801.10130\nhttps://utm.arc.nasa.gov/documents.shtml\nPaper References from Artificial Intelligence Nanodegree Program reviewers:\nhttps://people.csail.mit.edu/rivest/pubs/Riv87c.pdf\nhttps://www.semanticscholar.org/paper/Deep-Blue-Campbell-Hoane/378f933bbdb70d6f373e32e7182b6a5669c95d02\nhttps://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf\nhttp://www.cs.nott.ac.uk/~psznza/G52PAS/lecture9.pdf\nhttps://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-825-techniques-in-artificial-intelligence-sma-5504-fall-2002/lecture-notes/Lecture10FinalPart1.pdf\nhttps://artint.info/html/ArtInt_206.html\nhttp://www.cs.umd.edu/~djacobs/CMSC828/ApplicationsHMMs.pdf\nhttps://www.quora.com/What-are-some-applications-of-Probabilistic-Graphical-Models\nPaper References from Robotics Software Engineer Nanodegree Program reviewers:\nhttps://www.youtube.com/watch?time_continue=33&v=J_lXNPRIwag\nhttp://www.cs.cmu.edu/~15464-s13/lectures/lecture6/IK.pdf\nhttp://www.cs.columbia.edu/~allen/F15/NOTES/jacobians.pdf\nhttps://wiki.python.org/moin/UsingPickle\nhttps://distill.pub/2016/augmented-rnns/\nhttps://indico.io/blog/sequence-modeling-neural-networks-part2-attention-models/\nhttps://blog.keras.io/building-autoencoders-in-keras.html\nhttps://arxiv.org/pdf/1511.06309.pdf\nhttps://pjreddie.com/darknet/yolo/\nhttp://cs231n.github.io/classification/\nhttps://towardsdatascience.com/deep-learning-for-image-classification-why-its-challenging-where-we-ve-been-and-what-s-next-93b56948fcef\nhttps://blog.paralleldots.com/data-science/must-read-path-breaking-papers-about-image-classification/\nhttps://blog.openai.com/adversarial-example-research/\nhttps://blog.xix.ai/how-adversarial-attacks-work-87495b81da2d\nhttp://robots.stanford.edu/papers/thrun.robust-mcl.pdf\nhttps://ai.googleblog.com/2018/12/exploring-quantum-neural-networks.html\nhttps://realsense.intel.com/deep-learning-for-vr-ar/\nhttp://vision.stanford.edu/pdf/mandlekar2018corl.pdf\nhttp://robot.cc/papers/thrun.graphslam.pdf\nhttp://www2.informatik.uni-freiburg.de/~stachnis/pdf/grisetti10titsmag.pdf\nhttps://s3-us-west-1.amazonaws.com/udacity-drlnd/bookdraft2018.pdf\nhttps://github.com/udacity/rl-cheatsheet/blob/master/cheatsheet.pdf\nhttps://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision\nhttps://movingai.com/astar-var.html\nhttp://theory.stanford.edu/~amitp/GameProgramming/Variations.html\nhttps://www.cs.cmu.edu/~maxim/files/pathplanforMAV_icra13.pdf\nhttps://arxiv.org/pdf/1611.03673.pdf\nhttp://proceedings.mlr.press/v48/mniha16.pdf\nhttps://deepmind.com/blog/article/reinforcement-learning-unsupervised-auxiliary-tasks\nhttps://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2\nhttps://arxiv.org/pdf/1609.05143.pdf\nhttps://openai.com/blog/ingredients-for-robotics-research/\nhttps://arxiv.org/pdf/1708.05866.pdf\nhttps://openai.com/resources/\nhttps://www.groundai.com/project/self-supervised-deep-reinforcement-learning-with-generalized-computation-graphs-for-robot-navigation/\nhttp://raiahadsell.com/uploads/3/6/4/2/36428762/erf2017_keynote_talk.pdf\nhttps://papers.nips.cc/paper/1999/file/54f5f4071faca32ad5285fef87b78646-Paper.pdf\nhttp://read.pudn.com/downloads142/sourcecode/others/617477/inventory%20supply%20chain/04051310570412465(1).pdf\nhttps://deeplearning.mit.edu/\nhttps://www.youtube.com/channel/UCXZCJLdBC09xxGZ6gcdrc6A\nhttps://github.com/openai/gym\nhttps://www.youtube.com/watch?v=iX5V1WpxxkY\nhttp://colah.github.io/posts/2015-08-Understanding-LSTMs/\nhttps://www.youtube.com/watch?v=UNmqTiOnRfg\nhttps://www.youtube.com/watch?v=WCUNPb-5EYI\nhttp://twistedoakstudios.com/blog/Post554_minkowski-sums-and-differences\nhttps://www.toptal.com/game/video-game-physics-part-ii-collision-detection-for-solid-objects\nSLAM\nSir Sebastian Thrun's course: AI for Robotics\nInteresting content on SLAM\nfacebook & SLAM: Bringing art to life through technology\nfacebook & SLAM: Powered by AI: Oculus Insight\nSLAM (Simultaneous Localization And Mapping) Tracking Technology As Explained By Facebook\nMapping roads through deep learning and weakly supervised training\nBatSLAM: Biomimetic Sonar SLAM\nWhich problems/situations are still unsolved or difficult in SLAM?\nRecent Papers on SLAM\nViTa-SLAM: Biologically-Inspired Visuo-Tactile SLAM\nk -SLAM: A fast RGB-D SLAM approach for large indoor environments\nk-SLAM: A fast RGB-D SLAM approach for large indoor environments - Video\nDRE-SLAM: Dynamic RGB-D Encoder SLAM for a Differential-Drive Robot\nLinear SLAM: Linearising the SLAM problems using submap joining\nBenchmark of Visual SLAM Algorithms: ORB-SLAM2 vs RTAB-Map\nGSLAM: A General SLAM Framework and Benchmark\nminiSAM: A Flexible Factor Graph Non-linear Least Squares Optimization Framework\nMapping\nExternal Material\nROS Navigation tuning guide\nset start position of robot within amcl\nHow to Tune Navigational Parameters Using a Graphical Tool?\nofficial AMCL documentation\nMapping Accuracy\nLoop closure during SLAM\nSimultaneous Localization And Mapping (SLAM) using RTAB-Map\nOccupancy Mapping. What is it?\nOccupancy Grid Mapping - Log odd Update\nOccupancy Grid Mapping - Handling Range Sensor\nInverse Sensor Model\nPaper References from Secure and Private AI Scholarship Challenge Nanodegree Program:\nhttps://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf\nhttps://arxiv.org/pdf/1607.00133.pdf\nhttps://blog.openmined.org/federated-learning-of-a-rnn-on-raspberry-pis/ from Sarah.\nhttps://towardsdatascience.com/pysyft-android-b28da47a767e\nhttps://course.fast.ai/\nCNN's for Visual Recognition\nDeep Conv nets for image classification\nLarge Scale image Recognition using DNN's\nTransfer Learning\nAwesome Deep Learning Papers\nPaper References from AI for Trading Nanodegree Program:\nhttps://machinelearningmastery.com/statistical-hypothesis-tests/\nhttps://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-the-differences-between-one-tailed-and-two-tailed-tests/\nhttps://www.datacamp.com/community/tutorials/finance-python-trading\nhttps://towardsdatascience.com/inferential-statistics-series-t-test-using-numpy-2718f8f9bf2f\nhttps://www.zipline.io/appendix.html#zipline.pipeline.factors.Factor.rank\nhttps://towardsdatascience.com/how-the-mathematics-of-fractals-can-help-predict-stock-markets-shifts-19fee5dd6574\nhttp://www.econ.yale.edu/~shiller/\nhttps://www.nytimes.com/search/?query=economic+view+and+shiller&srchst=nyt\nhttps://www.fooledbyrandomness.com/\nhttps://iai.tv/video/how-do-you-solve-a-problem-like-uncertainty\nhttps://www.nytimes.com/2012/12/24/opinion/stabilization-wont-save-us.html\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=1921537\nhttp://www.statsmodels.org/dev/generated/statsmodels.stats.diagnostic.het_breuschpagan.html\nhttps://www.investopedia.com/articles/trading/08/turtle-trading.asp\nhttps://github.com/quantopian/alphalens/blob/master/alphalens/performance.py\nhttps://quantopian.github.io/alphalens/alphalens.html?highlight=alphalens%20performance#alphalens.utils.get_clean_factor_and_forward_returns\nhttp://colah.github.io/posts/2015-08-Understanding-LSTMs/\nhttp://blog.echen.me/2017/05/30/exploring-lstms/\nhttp://karpathy.github.io/2015/05/21/rnn-effectiveness/\nhttps://www.youtube.com/watch?v=iX5V1WpxxkY\nhttps://www.crummy.com/software/BeautifulSoup/bs4/doc/#differences-between-parsers\nhttps://github.com/udacity/deep-learning-v2-pytorch\nhttps://pytorch.org/docs/stable/nn.html#recurrent-layers\nhttps://video.udacity-data.com/topher/2018/October/5bc56d28_word2vec-mikolov/word2vec-mikolov.pdf\nhttps://video.udacity-data.com/topher/2018/October/5bc56da8_distributed-representations-mikolov2/distributed-representations-mikolov2.pdf\nhttps://github.com/pytorch/tnt/blob/master/torchnet/dataset/tensordataset.py\nhttps://stocktwits.com\nhttps://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\nhttps://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27\nhttps://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf\nhttps://github.com/slundberg/shap\nhttps://github.com/Polarbeargo/artificial-intelligence-for-trading/blob/master/quiz/m7/m7l6/calculate_shap_solution.ipynb\nhttps://arxiv.org/pdf/1802.03888.pdf\nhttps://github.com/Polarbeargo/artificial-intelligence-for-trading/blob/master/quiz/m7/m7l6/tree_shap_solution.ipynb\nhttps://shap.readthedocs.io/en/latest/\nhttps://github.com/Polarbeargo/artificial-intelligence-for-trading/blob/master/quiz/m7/m7l6/rank_features_solution.ipynb\nhttps://www.youtube.com/watch?v=Q8rTrmqUQsU\nhttps://stackabuse.com/overview-of-classification-methods-in-python-with-scikit-learn/\nhttps://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052\nhttps://heartbeat.fritz.ai/introduction-to-machine-learning-model-evaluation-fa859e1b2d7f\nhttps://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114\nhttps://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\nhttps://www.quora.com/What-is-the-difference-between-autocorrelation-cross-correlation\nhttps://pythonprogramming.net/rolling-statistics-data-analysis-python-pandas-tutorial/\nhttps://rabinpoudyal.medium.com/train-test-split-and-cross-validation-in-python-434ecba10909\nhttps://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\nhttps://towardsdatascience.com/understanding-random-forest-58381e0602d2\nhttps://towardsdatascience.com/fighting-imbalance-data-set-with-code-examples-f2a3880700a6\nhttps://stackoverflow.com/questions/10032579/how-to-evaluate-a-dataset-for-class-overlapping\nhttps://www.datatechnotes.com/2019/03/classification-with-bagging-classifier.html\nhttps://www.geeksforgeeks.org/ml-bagging-classifier/\nhttps://www.programcreek.com/python/example/86713/sklearn.ensemble.BaggingClassifier\nhttp://datagrid.lbl.gov/backtest/index.php\nhttps://web.stanford.edu/~hastie/Papers/ESLII.pdf\nhttps://medium.com/kaggle-blog\nhttps://arxiv.org/pdf/1811.05230.pdf\nhttp://scipy-lectures.org/advanced/mathematical_optimization/\nhttp://web.stanford.edu/class/ee364b/lectures.html\nhttps://cims.nyu.edu/~ritter/ritter2017machine.pdf\nhttps://cims.nyu.edu/~ritter/\nhttps://towardsdatascience.com/backtesting-your-first-trading-strategy-ad3977f3f2a\nhttps://towardsdatascience.com/backtesting-trading-strategies-less-sorcery-and-more-statistics-on-your-side-241ac41d18b0\nhttps://trade.collective2.com/ai-powered-strategy-backtesting-and-forecasting-platform-neotic-integrates-with-collective2-to-provide-automated-trading/\nhttps://medium.com/@alexrachnog/ai-for-algorithmic-trading-7-mistakes-that-could-make-me-broke-a41f94048b8c\nhttps://www.naftaliharris.com/blog/visualizing-k-means-clustering/\nhttp://jalammar.github.io\nReferences from C++ Nanodegree Program:\nhttps://www.stroustrup.com/papers.html\nhttps://www.valgrind.org/info/\nhttps://github.com/sowson/valgrind\nhttps://docs.microsoft.com/en-us/visualstudio/debugger/finding-memory-leaks-using-the-crt-library?view=vs-2019\nhttps://darkdust.net/files/GDB%20Cheat%20Sheet.pdf\nhttps://github.com/hishamhm/htop/commit/da4877f48c70f765f8bfb60c7668e8499055662e\nhttp://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#S-glossary\nhttps://en.cppreference.com/w/cpp/language/identifiers\nhttps://github.com/CppCon\nhttps://en.cppreference.com/w/cpp/language/raii\nhttp://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#rsmart-smart-pointers\nhttps://en.cppreference.com/w/cpp/memory\nhttp://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#c135-use-multiple-inheritance-to-represent-multiple-distinct-interfaces\nhttp://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#c136-use-multiple-inheritance-to-represent-the-union-of-implementation-attributes\nhttps://github.com/isocpp/CppCoreGuidelines/blob/master/CppCoreGuidelines.md#Rh-get\nhttp://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#c2-use-class-if-the-class-has-an-invariant-use-struct-if-the-data-members-can-vary-independently\nhttps://en.cppreference.com/w/cpp/chrono\nhttps://www.geeksforgeeks.org/c-data-types/\nhttps://en.cppreference.com/w/cpp/language/types\nhttps://en.cppreference.com/w/cpp/language/default_constructor\nhttp://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#cctor-constructors-assignments-and-destructors\nhttps://docs.microsoft.com/en-us/cpp/cpp/scope-resolution-operator?view=msvc-160&viewFallbackFrom=vs-2019\nhttps://visualstudiomagazine.com/Kunk0211\nhttps://github.com/hishamhm/htop\nhttps://www.learncpp.com/\nhttps://www.geeksforgeeks.org/basic-concepts-of-object-oriented-programming-using-c/\nhttps://www.youtube.com/watch?v=ZOKLjJF54Xc\nhttps://www.roberthalf.com/blog/salaries-and-skills/4-advantages-of-object-oriented-programming\nhttps://www.geeksforgeeks.org/differences-between-procedural-and-object-oriented-programming/\nhttps://ms.sapientia.ro/~manyi/teaching/c++/CPP_v1.1.pdf\nhttps://www.youtube.com/playlist?list=PLqCJpWy5FohcehaXlCIt8sVBHBFFRVWsx\nhttps://www.youtube.com/watch?v=-DP1i2ZU9gk\nhttps://www.youtube.com/watch?v=xXXt3htgDok&list=PLrOv9FMX8xJE8NgepZR1etrsU63fDDGxO&index=18\nhttps://www.youtube.com/watch?v=kxKKHKSMGIg&t=651s\nhttps://www.youtube.com/watch?v=iChalAKXffs\nhttps://www.learncpp.com/cpp-tutorial/configuring-your-compiler-warning-and-error-levels/\nhttps://www.viva64.com/en/k/0048/\nhttps://github.com/boostorg/boost/wiki/Guidelines:-Warnings\nhttps://software.intel.com/en-us/cpp-compiler-developer-guide-and-reference-remarks-warnings-and-errors\nhttp://man7.org/linux/man-pages/man5/proc.5.html\nhttp://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#S-glossary\nhttp://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#c4-make-a-function-a-member-only-if-it-needs-direct-access-to-the-representation-of-a-class\nhttps://en.cppreference.com/w/cpp/language/operators\nhttps://gcc.gnu.org/\nhttps://www.gnu.org/gnu/thegnuproject.en.html\nhttps://www.gnu.org/software/make/manual/html_node/index.html#Top\nhttps://cmake.org/\nhttps://stackoverflow.com/questions/19736281/what-are-the-differences-between-overriding-virtual-functions-and-hiding-non-vir\nhttps://isocpp.org/wiki/faq/strange-inheritance\nhttp://www.cs.cmu.edu/~motionplanning/reading/PlanningforDynamicVeh-1.pdf\nhttp://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#Rh-override\nhttps://www.onlinegdb.com/\nhttps://darkdust.net/files/GDB%20Cheat%20Sheet.pdf\nhttps://github.com/mvirgo/Rideshare-Simulation\nReferences from Computer Vision Nanodegree Program:\nhttps://blogs.nvidia.com/blog/2017/06/09/drone-navigates-without-gps/\nhttps://github.com/udacity/CVND_Exercises\nhttps://www.affectiva.com/\nhttps://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/\nhttps://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_transforms/py_fourier_transform/py_fourier_transform.html\nhttps://bair.berkeley.edu/blog/2018/05/17/delayed-impact/\nhttps://hbr.org/2018/02/can-we-keep-our-biases-from-creeping-into-ai?utm_campaign=hbr&utm_source=twitter&utm_medium=social\nhttps://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms\nhttps://towardsdatascience.com/teaching-cars-to-see-advanced-lane-detection-using-computer-vision-87a01de0424f\nhttps://video.udacity-data.com/topher/2018/June/5b2c01ba_gender-shades-paper/gender-shades-paper.pdf\nhttps://godatadriven.com/blog/fairness-in-machine-learning-with-pytorch/\nhttps://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_features_harris/py_features_harris.html\nhttps://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_contours/py_table_of_contents_contours/py_table_of_contents_contours.html\nhttps://pytorch.org/docs/stable/nn.html#pooling-layers\nhttps://pytorch.org/docs/master/optim.html\nhttps://pytorch.org/docs/master/nn.html#loss-functions\nhttps://medium.com/@smallfishbigsea/a-walk-through-of-alexnet-6cbd137a5637\nhttps://experiments.withgoogle.com/what-neural-nets-see\nhttps://github.com/alexisbcook/tsne\nhttps://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9\nhttps://github.com/jwyang/faster-rcnn.pytorch\nhttps://vivek-yadav.medium.com/part-1-generating-anchor-boxes-for-yolo-like-network-for-vehicle-detection-using-kitti-dataset-b2fe033e5807\nhttps://pjreddie.com/media/files/papers/YOLOv3.pdf\nhttps://pjreddie.com/darknet/\nhttps://video.udacity-data.com/topher/2018/May/5af0e03b_video-classification/video-classification.pdf\nhttps://en.wikipedia.org/wiki/Vanishing_gradient_problem\nhttps://en.wikipedia.org/wiki/Time_delay_neural_network\nhttps://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1\nhttps://en.wikipedia.org/wiki/Recurrent_neural_network#Elman_networks_and_Jordan_networks\nhttp://www.bioinf.jku.at/publications/older/2604.pdf\nhttps://en.wikipedia.org/wiki/Sepp_Hochreiter\nhttps://people.idsia.ch//~juergen/\nhttps://engineering.fb.com/2016/10/25/ml-applications/building-an-efficient-neural-language-model-over-a-billion-words/\nhttps://arxiv.org/pdf/1511.06939.pdf\nhttp://blog.datumbox.com/tuning-the-learning-rate-in-gradient-descent/\nhttps://cs231n.github.io/neural-networks-3/#loss\nhttp://jalammar.github.io/\nhttps://www.deeplearningbook.org/contents/guidelines.html\nhttps://arxiv.org/abs/1206.5533\nhttp://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network's_hyper-parameters\nhttp://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\nhttps://arxiv.org/abs/1507.05523\nhttps://arxiv.org/abs/1606.02228\nhttps://arxiv.org/abs/1506.02078\nhttps://arxiv.org/pdf/1502.03044.pdf\nhttps://arxiv.org/pdf/1707.07998.pdf\nhttps://www.cv-foundation.org/openaccess/content_cvpr_2016/app/S19-04.pdf\nhttps://arxiv.org/pdf/1507.05738.pdf\nhttps://arxiv.org/pdf/1708.02711.pdf\nhttps://arxiv.org/pdf/1607.05910.pdf\nhttps://arxiv.org/abs/1706.03762\nhttps://www.youtube.com/watch?v=rBCqOTEfxvg\nReferences from Bertelsmann Scholarship Introduction to AI in Business Nanodegree Program:\nhttps://cezannec.github.io/Intro_Neural_Networks/\nhttps://www.pnas.org/content/pnas/114/50/13108.full.pdf\nhttps://www.pnas.org/content/pnas/suppl/2017/11/27/1700035114.DCSupplemental/pnas.1700035114.sapp.pdf\nhttps://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf\nhttps://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab\nhttps://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\nhttps://www.washingtonpost.com/graphics/2018/business/alexa-does-not-understand-your-accent/?noredirect=on&utm_term=.a17219a26daa\nhttps://github.com/topics/video-annotation\nhttps://www.invisionapp.com/design-defined/mockup/\nhttps://www.figure-eight.com/dataset/parking-sign-detection/\nhttps://www.figure-eight.com/datasets/\nReferences from Data Engineering Nanodegree Program:\nhttps://www.postgresqltutorial.com/postgresql-upsert/\nhttps://www.postgresql.org/docs/9.5/sql-insert.html\nhttps://www.datastax.com/blog/allow-filtering-explained\nhttps://www.xenonstack.com/blog/nosql-databases/\nhttps://learnsql.com/blog/companies-that-use-postgresql-in-business/\nhttps://www.w3resource.com/PostgreSQL/foreign-key-constraint.php\nhttps://www.geeksforgeeks.org/introduction-of-dbms-database-management-system-set-1/\nhttps://learn.panoply.io/hubfs/Data%20Engineering%20-%20Introduction%20and%20Epochs.pdf\nhttps://softwareengineering.stackexchange.com/questions/375704/why-should-i-use-foreign-keys-in-database\nhttp://norvig.com/21-days.html\nReferences from the Intel\u00ae Edge AI for IoT Developers Nanodegree Program:\nhttps://thegradient.pub/semantic-segmentation/", "link": "https://github.com/polarbeargo/The-love-I-receive-from-Udacity-reviewer-resources", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "the-love-i-receive-from-udacity-reviewer-resources\nmy gratitude to sir sebastion thrun, instructor katie malone, sir peter norvig, sir thad starner,sir nicholas roy, sir david j. malan, sir david silver, sir raffaello d'andrea, sir ryan, instructor jay alammar, madam dana sheahen, instructor alexis cook, instructor cezanne camacho, instructor erica, instructor karim, instructor julia, sir stephen, sir aaron brown, sir andy, instructor stefanie, instructor angela, sir andrew trask, sir jonathan larkin, sir gordon ritter, sir justin sheetz, instructor luis serrano, instructor liz otto hamel, instructor eddie shyu, instructor josh bernhard, instructor parnian barekatain, instructor miriam, instructor mat leonard, instructor cindy, instructor brok bucholtz, instructor arpan chakraborty, instructor juan delgado, sir jake, sir michael virgo, sir tucker, instructor juno lee, sir akshit, brenda.udacity, palak.udacity, grace.udacity, instructor grant sanderson, instructor ortal, instructor jennifer staab and instructor amanda moran.\nto memorize the journey start from self-driving car engineer nanodegree program to artificial intelligence nanodegree program to flying car nanodegree program then robotics software nanodegree program and cs50: introduction to computer science 2018, to memorize the year of my enrollment in udacity from 2017 to present.\nto memorize mentors donald, christopher, jafar and david who encourage and support me a lot through my enrollment years. here i collect the feedback and paper references from my udacity reviewers and friendships i got from udacity and cs50x. i want to extend what i learn and also extend the spirits, share knowledge with all my classmates at least this is what i can do if any of this citation papers helps you please send a star to them or references their papers will be an honorable action:).\nwithin these periods i want to say thank you to madam olga uskova, mylene doublet o'kane and sir luigi morelli. i love your writings and very appreciate every like you send to me. my gratitude to martin mcgovern, karen e. baker, and udacity team you let me learn to be udacious and persistence:d!\nfrom madam olga uskova:\nhttps://en.wikipedia.org/wiki/olga_uskova\nhttps://vimeo.com/cognitivetech\nhttp://www.taipeitimes.com/news/feat/archives/2018/01/08/2003685375\nhttp://www.agriland.ie/farming-news/fully-driverless-combine-harvester-by-2024/\npaper references from self-driving car reviewers:\nhttps://airccj.org/cscp/vol5/csit53211.pdf\nhttps://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_houghlines/py_houghlines.html\nhttp://www.kerrywong.com/2009/05/07/canny-edge-detection-auto-thresholding/\nhttps://medium.com/@vivek.yadav/improved-performance-of-deep-learning-neural-network-models-on-traffic-sign-classification-using-6355346da2dc\nhttps://medium.com/@jeremyeshannon/udacity-self-driving-car-nanodegree-project-2-traffic-sign-classifier-f52d33d4be9f\nhttps://github.com/tflearn/tflearn/blob/master/examples/images/googlenet.py\nhttps://keras.io/callbacks/#callback\nhttps://keras.io/callbacks/#modelcheckpoint\nhttp://cs229.stanford.edu/proj2015/054_report.pdf\nhttp://ruder.io/optimizing-gradient-descent/\nhttp://ruder.io/optimizing-gradient-descent/index.html#adam\nhttp://alexlenail.me/nn-svg/lenet.html\nhttps://keras.io/visualization/\nhttps://www.researchgate.net/publication/257291768_a_much_advanced_and_efficient_lane_detection_algorithm_for_intelligent_highway_safety\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/pmc5017478/\nhttps://medium.com/@mohankarthik/feature-extraction-for-vehicle-detection-using-hog-d99354a84d10\nhttps://chatbotslife.com/towards-a-real-time-vehicle-detection-ssd-multibox-approach-2519af2751c\nhttps://github.com/rlabbe/kalman-and-bayesian-filters-in-python\nhttps://stackoverflow.com/questions/27466642/what-kind-of-optimization-does-const-offer-in-c-c\nhttps://www.youtube.com/watch?v=aukba1zmkv4\nhttp://correll.cs.colorado.edu/?p=965\nhttps://www.cs.cmu.edu/afs/cs/project/jair/pub/volume9/mazer98a-html/node2.html\nhttp://www.roborealm.com/help/path_planning.php\nhttps://robotics.stackexchange.com/questions/8302/what-is-the-difference-between-path-planning-and-motion-planning\nhttps://www.robotshop.com/community/forum/t/excellent-tutorial-on-a-robot-path-planning/13170\nhttps://www.mathworks.com/help/robotics/examples/path-planning-in-environments-of-difference-complexity.html;jsessionid=a7ff890a4e697fe79be723535659\nhttp://ais.informatik.uni-freiburg.de/teaching/ss11/robotics/slides/18-robot-motion-planning.pdf\nhttp://ais.informatik.uni-freiburg.de/teaching/ss10/robotics/slides/16-pathplanning.pdf\nhttp://ai.stanford.edu/~ddolgov/papers/dolgov_gpp_stair08.pdf\nhttps://webpages.uncc.edu/~jmconrad/gradstudents/thesis_ghangrekar.pdf\nhttp://blog.qure.ai/notes/semantic-segmentation-deep-learning-review\nhttps://arxiv.org/pdf/1411.4038.pdf\nhttp://cs231n.github.io/neural-networks-2/#init\nhttp://cs231n.github.io/neural-networks-2/#reg\nhttps://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network\nhttps://www.jeremyjordan.me/nn-learning-rate/\nhttps://arxiv.org/abs/1806.02446\nhttps://arxiv.org/abs/1903.03273\nhttps://arxiv.org/abs/1901.00114\nhttps://roscon.ros.org/2018/#program\nhttp://driving.stanford.edu/papers/iser2010.pdf\npaper references from flying car reviewers:\nhttp://planning.cs.uiuc.edu/\nhttps://docs.ufpr.br/~danielsantos/probabilisticrobotics.pdf\nhttps://www.youtube.com/playlist?list=plx2gx-ftpvxu3oufnatxgxy90auliqnwt\nhttps://simondlevy.academic.wlu.edu/\nhttps://docs.google.com/viewer?url=https%3a%2f%2fwww.seas.harvard.edu%2fcourses%2fcs281%2fpapers%2funscented.pdf\nhttp://andrew.gibiansky.com/downloads/pdf/quadcopter%20dynamics,%20simulation,%20and%20control.pdf http://aeroconf.org/\nhttps://arxiv.org/abs/1902.01465\nhttp://navion.mit.edu/\nhttps://www.researchgate.net/publication/322020415_adaptive_super-twisting_second-order_sliding_mode_control_for_attitude_control_of_quadcopter_uavs\nhttps://arxiv.org/abs/1801.10130\nhttps://utm.arc.nasa.gov/documents.shtml\npaper references from artificial intelligence nanodegree program reviewers:\nhttps://people.csail.mit.edu/rivest/pubs/riv87c.pdf\nhttps://www.semanticscholar.org/paper/deep-blue-campbell-hoane/378f933bbdb70d6f373e32e7182b6a5669c95d02\nhttps://storage.googleapis.com/deepmind-media/alphago/alphagonaturepaper.pdf\nhttp://www.cs.nott.ac.uk/~psznza/g52pas/lecture9.pdf\nhttps://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-825-techniques-in-artificial-intelligence-sma-5504-fall-2002/lecture-notes/lecture10finalpart1.pdf\nhttps://artint.info/html/artint_206.html\nhttp://www.cs.umd.edu/~djacobs/cmsc828/applicationshmms.pdf\nhttps://www.quora.com/what-are-some-applications-of-probabilistic-graphical-models\npaper references from robotics software engineer nanodegree program reviewers:\nhttps://www.youtube.com/watch?time_continue=33&v=j_lxnpriwag\nhttp://www.cs.cmu.edu/~15464-s13/lectures/lecture6/ik.pdf\nhttp://www.cs.columbia.edu/~allen/f15/notes/jacobians.pdf\nhttps://wiki.python.org/moin/usingpickle\nhttps://distill.pub/2016/augmented-rnns/\nhttps://indico.io/blog/sequence-modeling-neural-networks-part2-attention-models/\nhttps://blog.keras.io/building-autoencoders-in-keras.html\nhttps://arxiv.org/pdf/1511.06309.pdf\nhttps://pjreddie.com/darknet/yolo/\nhttp://cs231n.github.io/classification/\nhttps://towardsdatascience.com/deep-learning-for-image-classification-why-its-challenging-where-we-ve-been-and-what-s-next-93b56948fcef\nhttps://blog.paralleldots.com/data-science/must-read-path-breaking-papers-about-image-classification/\nhttps://blog.openai.com/adversarial-example-research/\nhttps://blog.xix.ai/how-adversarial-attacks-work-87495b81da2d\nhttp://robots.stanford.edu/papers/thrun.robust-mcl.pdf\nhttps://ai.googleblog.com/2018/12/exploring-quantum-neural-networks.html\nhttps://realsense.intel.com/deep-learning-for-vr-ar/\nhttp://vision.stanford.edu/pdf/mandlekar2018corl.pdf\nhttp://robot.cc/papers/thrun.graphslam.pdf\nhttp://www2.informatik.uni-freiburg.de/~stachnis/pdf/grisetti10titsmag.pdf\nhttps://s3-us-west-1.amazonaws.com/udacity-drlnd/bookdraft2018.pdf\nhttps://github.com/udacity/rl-cheatsheet/blob/master/cheatsheet.pdf\nhttps://en.wikipedia.org/wiki/bag-of-words_model_in_computer_vision\nhttps://movingai.com/astar-var.html\nhttp://theory.stanford.edu/~amitp/gameprogramming/variations.html\nhttps://www.cs.cmu.edu/~maxim/files/pathplanformav_icra13.pdf\nhttps://arxiv.org/pdf/1611.03673.pdf\nhttp://proceedings.mlr.press/v48/mniha16.pdf\nhttps://deepmind.com/blog/article/reinforcement-learning-unsupervised-auxiliary-tasks\nhttps://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2\nhttps://arxiv.org/pdf/1609.05143.pdf\nhttps://openai.com/blog/ingredients-for-robotics-research/\nhttps://arxiv.org/pdf/1708.05866.pdf\nhttps://openai.com/resources/\nhttps://www.groundai.com/project/self-supervised-deep-reinforcement-learning-with-generalized-computation-graphs-for-robot-navigation/\nhttp://raiahadsell.com/uploads/3/6/4/2/36428762/erf2017_keynote_talk.pdf\nhttps://papers.nips.cc/paper/1999/file/54f5f4071faca32ad5285fef87b78646-paper.pdf\nhttp://read.pudn.com/downloads142/sourcecode/others/617477/inventory%20supply%20chain/04051310570412465(1).pdf\nhttps://deeplearning.mit.edu/\nhttps://www.youtube.com/channel/ucxzcjldbc09xxgz6gcdrc6a\nhttps://github.com/openai/gym\nhttps://www.youtube.com/watch?v=ix5v1wpxxky\nhttp://colah.github.io/posts/2015-08-understanding-lstms/\nhttps://www.youtube.com/watch?v=unmqtionrfg\nhttps://www.youtube.com/watch?v=wcunpb-5eyi\nhttp://twistedoakstudios.com/blog/post554_minkowski-sums-and-differences\nhttps://www.toptal.com/game/video-game-physics-part-ii-collision-detection-for-solid-objects\nslam\nsir sebastian thrun's course: ai for robotics\ninteresting content on slam\nfacebook & slam: bringing art to life through technology\nfacebook & slam: powered by ai: oculus insight\nslam (simultaneous localization and mapping) tracking technology as explained by facebook\nmapping roads through deep learning and weakly supervised training\nbatslam: biomimetic sonar slam\nwhich problems/situations are still unsolved or difficult in slam?\nrecent papers on slam\nvita-slam: biologically-inspired visuo-tactile slam\nk -slam: a fast rgb-d slam approach for large indoor environments\nk-slam: a fast rgb-d slam approach for large indoor environments - video\ndre-slam: dynamic rgb-d encoder slam for a differential-drive robot\nlinear slam: linearising the slam problems using submap joining\nbenchmark of visual slam algorithms: orb-slam2 vs rtab-map\ngslam: a general slam framework and benchmark\nminisam: a flexible factor graph non-linear least squares optimization framework\nmapping\nexternal material\nros navigation tuning guide\nset start position of robot within amcl\nhow to tune navigational parameters using a graphical -----> tool !!! ?\nofficial amcl documentation\nmapping accuracy\nloop closure during slam\nsimultaneous localization and mapping (slam) using rtab-map\noccupancy mapping. what is it?\noccupancy grid mapping - log odd update\noccupancy grid mapping - handling range sensor\ninverse sensor model\npaper references from secure and private ai scholarship challenge nanodegree program:\nhttps://www.cis.upenn.edu/~aaroth/papers/privacybook.pdf\nhttps://arxiv.org/pdf/1607.00133.pdf\nhttps://blog.openmined.org/federated-learning-of-a-rnn-on-raspberry-pis/ from sarah.\nhttps://towardsdatascience.com/pysyft-android-b28da47a767e\nhttps://course.fast.ai/\ncnn's for visual recognition\ndeep conv nets for image classification\nlarge scale image recognition using dnn's\ntransfer learning\nawesome deep learning papers\npaper references from ai for trading nanodegree program:\nhttps://machinelearningmastery.com/statistical-hypothesis-tests/\nhttps://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-the-differences-between-one-tailed-and-two-tailed-tests/\nhttps://www.datacamp.com/community/tutorials/finance-python-trading\nhttps://towardsdatascience.com/inferential-statistics-series-t-test-using-numpy-2718f8f9bf2f\nhttps://www.zipline.io/appendix.html#zipline.pipeline.factors.factor.rank\nhttps://towardsdatascience.com/how-the-mathematics-of-fractals-can-help-predict-stock-markets-shifts-19fee5dd6574\nhttp://www.econ.yale.edu/~shiller/\nhttps://www.nytimes.com/search/?query=economic+view+and+shiller&srchst=nyt\nhttps://www.fooledbyrandomness.com/\nhttps://iai.tv/video/how-do-you-solve-a-problem-like-uncertainty\nhttps://www.nytimes.com/2012/12/24/opinion/stabilization-wont-save-us.html\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=1921537\nhttp://www.statsmodels.org/dev/generated/statsmodels.stats.diagnostic.het_breuschpagan.html\nhttps://www.investopedia.com/articles/trading/08/turtle-trading.asp\nhttps://github.com/quantopian/alphalens/blob/master/alphalens/performance.py\nhttps://quantopian.github.io/alphalens/alphalens.html?highlight=alphalens%20performance#alphalens.utils.get_clean_factor_and_forward_returns\nhttp://colah.github.io/posts/2015-08-understanding-lstms/\nhttp://blog.echen.me/2017/05/30/exploring-lstms/\nhttp://karpathy.github.io/2015/05/21/rnn-effectiveness/\nhttps://www.youtube.com/watch?v=ix5v1wpxxky\nhttps://www.crummy.com/software/beautifulsoup/bs4/doc/#differences-between-parsers\nhttps://github.com/udacity/deep-learning-v2-pytorch\nhttps://pytorch.org/docs/stable/nn.html#recurrent-layers\nhttps://video.udacity-data.com/topher/2018/october/5bc56d28_word2vec-mikolov/word2vec-mikolov.pdf\nhttps://video.udacity-data.com/topher/2018/october/5bc56da8_distributed-representations-mikolov2/distributed-representations-mikolov2.pdf\nhttps://github.com/pytorch/tnt/blob/master/torchnet/dataset/tensordataset.py\nhttps://stocktwits.com\nhttps://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\nhttps://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27\nhttps://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-paper.pdf\nhttps://github.com/slundberg/shap\nhttps://github.com/polarbeargo/artificial-intelligence-for-trading/blob/master/quiz/m7/m7l6/calculate_shap_solution.ipynb\nhttps://arxiv.org/pdf/1802.03888.pdf\nhttps://github.com/polarbeargo/artificial-intelligence-for-trading/blob/master/quiz/m7/m7l6/tree_shap_solution.ipynb\nhttps://shap.readthedocs.io/en/latest/\nhttps://github.com/polarbeargo/artificial-intelligence-for-trading/blob/master/quiz/m7/m7l6/rank_features_solution.ipynb\nhttps://www.youtube.com/watch?v=q8rtrmquqsu\nhttps://stackabuse.com/overview-of-classification-methods-in-python-with-scikit-learn/\nhttps://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052\nhttps://heartbeat.fritz.ai/introduction-to-machine-learning-model-evaluation-fa859e1b2d7f\nhttps://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114\nhttps://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\nhttps://www.quora.com/what-is-the-difference-between-autocorrelation-cross-correlation\nhttps://pythonprogramming.net/rolling-statistics-data-analysis-python-pandas-tutorial/\nhttps://rabinpoudyal.medium.com/train-test-split-and-cross-validation-in-python-434ecba10909\nhttps://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\nhttps://towardsdatascience.com/understanding-random-forest-58381e0602d2\nhttps://towardsdatascience.com/fighting-imbalance-data-set-with-code-examples-f2a3880700a6\nhttps://stackoverflow.com/questions/10032579/how-to-evaluate-a-dataset-for-class-overlapping\nhttps://www.datatechnotes.com/2019/03/classification-with-bagging-classifier.html\nhttps://www.geeksforgeeks.org/ml-bagging-classifier/\nhttps://www.programcreek.com/python/example/86713/sklearn.ensemble.baggingclassifier\nhttp://datagrid.lbl.gov/backtest/index.php\nhttps://web.stanford.edu/~hastie/papers/eslii.pdf\nhttps://medium.com/kaggle-blog\nhttps://arxiv.org/pdf/1811.05230.pdf\nhttp://scipy-lectures.org/advanced/mathematical_optimization/\nhttp://web.stanford.edu/class/ee364b/lectures.html\nhttps://cims.nyu.edu/~ritter/ritter2017machine.pdf\nhttps://cims.nyu.edu/~ritter/\nhttps://towardsdatascience.com/backtesting-your-first-trading-strategy-ad3977f3f2a\nhttps://towardsdatascience.com/backtesting-trading-strategies-less-sorcery-and-more-statistics-on-your-side-241ac41d18b0\nhttps://trade.collective2.com/ai-powered-strategy-backtesting-and-forecasting-platform-neotic-integrates-with-collective2-to-provide-automated-trading/\nhttps://medium.com/@alexrachnog/ai-for-algorithmic-trading-7-mistakes-that-could-make-me-broke-a41f94048b8c\nhttps://www.naftaliharris.com/blog/visualizing-k-means-clustering/\nhttp://jalammar.github.io\nreferences from c++ nanodegree program:\nhttps://www.stroustrup.com/papers.html\nhttps://www.valgrind.org/info/\nhttps://github.com/sowson/valgrind\nhttps://docs.microsoft.com/en-us/visualstudio/debugger/finding-memory-leaks-using-the-crt-library?view=vs-2019\nhttps://darkdust.net/files/gdb%20cheat%20sheet.pdf\nhttps://github.com/hishamhm/htop/commit/da4877f48c70f765f8bfb60c7668e8499055662e\nhttp://isocpp.github.io/cppcoreguidelines/cppcoreguidelines#s-glossary\nhttps://en.cppreference.com/w/cpp/language/identifiers\nhttps://github.com/cppcon\nhttps://en.cppreference.com/w/cpp/language/raii\nhttp://isocpp.github.io/cppcoreguidelines/cppcoreguidelines#rsmart-smart-pointers\nhttps://en.cppreference.com/w/cpp/memory\nhttp://isocpp.github.io/cppcoreguidelines/cppcoreguidelines#c135-use-multiple-inheritance-to-represent-multiple-distinct-interfaces\nhttp://isocpp.github.io/cppcoreguidelines/cppcoreguidelines#c136-use-multiple-inheritance-to-represent-the-union-of-implementation-attributes\nhttps://github.com/isocpp/cppcoreguidelines/blob/master/cppcoreguidelines.md#rh-get\nhttp://isocpp.github.io/cppcoreguidelines/cppcoreguidelines#c2-use-class-if-the-class-has-an-invariant-use-struct-if-the-data-members-can-vary-independently\nhttps://en.cppreference.com/w/cpp/chrono\nhttps://www.geeksforgeeks.org/c-data-types/\nhttps://en.cppreference.com/w/cpp/language/types\nhttps://en.cppreference.com/w/cpp/language/default_constructor\nhttp://isocpp.github.io/cppcoreguidelines/cppcoreguidelines#cctor-constructors-assignments-and-destructors\nhttps://docs.microsoft.com/en-us/cpp/cpp/scope-resolution-operator?view=msvc-160&viewfallbackfrom=vs-2019\nhttps://visualstudiomagazine.com/kunk0211\nhttps://github.com/hishamhm/htop\nhttps://www.learncpp.com/\nhttps://www.geeksforgeeks.org/basic-concepts-of-object-oriented-programming-using-c/\nhttps://www.youtube.com/watch?v=zokljjf54xc\nhttps://www.roberthalf.com/blog/salaries-and-skills/4-advantages-of-object-oriented-programming\nhttps://www.geeksforgeeks.org/differences-between-procedural-and-object-oriented-programming/\nhttps://ms.sapientia.ro/~manyi/teaching/c++/cpp_v1.1.pdf\nhttps://www.youtube.com/playlist?list=plqcjpwy5fohcehaxlcit8svbhbffrvwsx\nhttps://www.youtube.com/watch?v=-dp1i2zu9gk\nhttps://www.youtube.com/watch?v=xxxt3htgdok&list=plrov9fmx8xje8ngepzr1etrsu63fddgxo&index=18\nhttps://www.youtube.com/watch?v=kxkkhksmgig&t=651s\nhttps://www.youtube.com/watch?v=ichalakxffs\nhttps://www.learncpp.com/cpp-tutorial/configuring-your-compiler-warning-and-error-levels/\nhttps://www.viva64.com/en/k/0048/\nhttps://github.com/boostorg/boost/wiki/guidelines:-warnings\nhttps://software.intel.com/en-us/cpp-compiler-developer-guide-and-reference-remarks-warnings-and-errors\nhttp://man7.org/linux/man-pages/man5/proc.5.html\nhttp://isocpp.github.io/cppcoreguidelines/cppcoreguidelines#s-glossary\nhttp://isocpp.github.io/cppcoreguidelines/cppcoreguidelines#c4-make-a-function-a-member-only-if-it-needs-direct-access-to-the-representation-of-a-class\nhttps://en.cppreference.com/w/cpp/language/operators\nhttps://gcc.gnu.org/\nhttps://www.gnu.org/gnu/thegnuproject.en.html\nhttps://www.gnu.org/software/make/manual/html_node/index.html#top\nhttps://cmake.org/\nhttps://stackoverflow.com/questions/19736281/what-are-the-differences-between-overriding-virtual-functions-and-hiding-non-vir\nhttps://isocpp.org/wiki/faq/strange-inheritance\nhttp://www.cs.cmu.edu/~motionplanning/reading/planningfordynamicveh-1.pdf\nhttp://isocpp.github.io/cppcoreguidelines/cppcoreguidelines#rh-override\nhttps://www.onlinegdb.com/\nhttps://darkdust.net/files/gdb%20cheat%20sheet.pdf\nhttps://github.com/mvirgo/rideshare-simulation\nreferences from computer vision nanodegree program:\nhttps://blogs.nvidia.com/blog/2017/06/09/drone-navigates-without-gps/\nhttps://github.com/udacity/cvnd_exercises\nhttps://www.affectiva.com/\nhttps://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/\nhttps://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_transforms/py_fourier_transform/py_fourier_transform.html\nhttps://bair.berkeley.edu/blog/2018/05/17/delayed-impact/\nhttps://hbr.org/2018/02/can-we-keep-our-biases-from-creeping-into-ai?utm_campaign=hbr&utm_source=twitter&utm_medium=social\nhttps://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms\nhttps://towardsdatascience.com/teaching-cars-to-see-advanced-lane-detection-using-computer-vision-87a01de0424f\nhttps://video.udacity-data.com/topher/2018/june/5b2c01ba_gender-shades-paper/gender-shades-paper.pdf\nhttps://godatadriven.com/blog/fairness-in-machine-learning-with-pytorch/\nhttps://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_features_harris/py_features_harris.html\nhttps://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_contours/py_table_of_contents_contours/py_table_of_contents_contours.html\nhttps://pytorch.org/docs/stable/nn.html#pooling-layers\nhttps://pytorch.org/docs/master/optim.html\nhttps://pytorch.org/docs/master/nn.html#loss-functions\nhttps://medium.com/@smallfishbigsea/a-walk-through-of-alexnet-6cbd137a5637\nhttps://experiments.withgoogle.com/what-neural-nets-see\nhttps://github.com/alexisbcook/tsne\nhttps://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9\nhttps://github.com/jwyang/faster-rcnn.pytorch\nhttps://vivek-yadav.medium.com/part-1-generating-anchor-boxes-for-yolo-like-network-for-vehicle-detection-using-kitti-dataset-b2fe033e5807\nhttps://pjreddie.com/media/files/papers/yolov3.pdf\nhttps://pjreddie.com/darknet/\nhttps://video.udacity-data.com/topher/2018/may/5af0e03b_video-classification/video-classification.pdf\nhttps://en.wikipedia.org/wiki/vanishing_gradient_problem\nhttps://en.wikipedia.org/wiki/time_delay_neural_network\nhttps://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1\nhttps://en.wikipedia.org/wiki/recurrent_neural_network#elman_networks_and_jordan_networks\nhttp://www.bioinf.jku.at/publications/older/2604.pdf\nhttps://en.wikipedia.org/wiki/sepp_hochreiter\nhttps://people.idsia.ch//~juergen/\nhttps://engineering.fb.com/2016/10/25/ml-applications/building-an-efficient-neural-language-model-over-a-billion-words/\nhttps://arxiv.org/pdf/1511.06939.pdf\nhttp://blog.datumbox.com/tuning-the-learning-rate-in-gradient-descent/\nhttps://cs231n.github.io/neural-networks-3/#loss\nhttp://jalammar.github.io/\nhttps://www.deeplearningbook.org/contents/guidelines.html\nhttps://arxiv.org/abs/1206.5533\nhttp://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network's_hyper-parameters\nhttp://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\nhttps://arxiv.org/abs/1507.05523\nhttps://arxiv.org/abs/1606.02228\nhttps://arxiv.org/abs/1506.02078\nhttps://arxiv.org/pdf/1502.03044.pdf\nhttps://arxiv.org/pdf/1707.07998.pdf\nhttps://www.cv-foundation.org/openaccess/content_cvpr_2016/app/s19-04.pdf\nhttps://arxiv.org/pdf/1507.05738.pdf\nhttps://arxiv.org/pdf/1708.02711.pdf\nhttps://arxiv.org/pdf/1607.05910.pdf\nhttps://arxiv.org/abs/1706.03762\nhttps://www.youtube.com/watch?v=rbcqotefxvg\nreferences from bertelsmann scholarship introduction to ai in business nanodegree program:\nhttps://cezannec.github.io/intro_neural_networks/\nhttps://www.pnas.org/content/pnas/114/50/13108.full.pdf\nhttps://www.pnas.org/content/pnas/suppl/2017/11/27/1700035114.dcsupplemental/pnas.1700035114.sapp.pdf\nhttps://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-paper.pdf\nhttps://www.youtube.com/playlist?list=plzhqobowtqdpd3mizzm2xvfitgf8he_ab\nhttps://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\nhttps://www.washingtonpost.com/graphics/2018/business/alexa-does-not-understand-your-accent/?noredirect=on&utm_term=.a17219a26daa\nhttps://github.com/topics/video-annotation\nhttps://www.invisionapp.com/design-defined/mockup/\nhttps://www.figure-eight.com/dataset/parking-sign-detection/\nhttps://www.figure-eight.com/datasets/\nreferences from data engineering nanodegree program:\nhttps://www.postgresqltutorial.com/postgresql-upsert/\nhttps://www.postgresql.org/docs/9.5/sql-insert.html\nhttps://www.datastax.com/blog/allow-filtering-explained\nhttps://www.xenonstack.com/blog/nosql-databases/\nhttps://learnsql.com/blog/companies-that-use-postgresql-in-business/\nhttps://www.w3resource.com/postgresql/foreign-key-constraint.php\nhttps://www.geeksforgeeks.org/introduction-of-dbms-database-management-system-set-1/\nhttps://learn.panoply.io/hubfs/data%20engineering%20-%20introduction%20and%20epochs.pdf\nhttps://softwareengineering.stackexchange.com/questions/375704/why-should-i-use-foreign-keys-in-database\nhttp://norvig.com/21-days.html\nreferences from the intel\u00ae edge ai for iot developers nanodegree program:\nhttps://thegradient.pub/semantic-segmentation/", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000997, "year": null}, {"Unnamed: 0": 2020, "autor": 1000, "date": null, "content": "ROSMOD\nThis repository contains the implementation of ROSMOD - Journal of Electronics, built on WebGME.\nROSMOD is a web-based, collaborative, modeling and execution environment for distributed embedded applications built using ROS.\nAn example server running ROSMOD can be found on rosmod.collaborative-design.org.\nPlease see the WIKI for in-depth information about ROSMOD, how to use it, and links to various Youtube videos that walk through development and usage of ROSMOD.\nNote: WebGME ROSMOD code generators have changed - they no longer generate for rosmod-comm, please see Setting up target systems to run ROSMOD to properly install the new dependencies.\nContents\nWhat is ROSMOD\nMotivation\nImplementation\nFeatures\nHow to set up ROSMOD\nSetting up the ROSMOD server\nRequired Dependencies\nOptional Dependencies\nStart the Server\nSetting up target systems to run ROSMOD\nHow to use ROSMOD\nCreating a ROSMOD project\nCreating a WebGME project\nCreating a Single ROSMOD project\nModeling the Software\nModeling the Systems\nCreating a Deployment\nCreating an Experiment\nRunning an Experiment\nStopping a Running Experiment\nKeeping ROSMOD up to date\nWhat is ROSMOD?\nThe Robot Operating System Model-driven development tool suite, (ROSMOD) an integrated development environment for rapid prototyping component-based software for the Robot Operating System (ROS) middleware.\nROSMOD consists of\nThe ROSMOD Server (this repo), which provides collaborative model-driven development, and\nThe ROSMOD Actor (github), which is a standardized ros::Node that provides a well defined component execution model which helps reduce the complexity of writing multi-threaded code.\nMotivation\nRobotics is a discipline that involves integration between heterogeneous fields such as electronics, mechanics, computer science, control theory and artificial intelligence. During the design, development and deployment of robotic systems, the components that belong to the various sub-systems, e.g., image processing software, servo-motors, power distribution system, etc., must work congruously to achieve a common goal. As systems increase in complexity, software development platforms must enable the creation of such components in a rapid, reliable, and reusable manner while also providing systems-level design, analysis, and deployment.\nIn other similar systems, users are often tasked with developing the models, generating the code, and then manually touching the generated code to implement the business logic for the various operations their system needs to perform (e.g. periodic or event-triggered functions). The injection and management of this user-developed code inside the generated code can increase the learning curve for new users as it adds some extra file management and build-system overhead. Additionally, almost all other similar projects require installation on the users' computers, which means they may need to troubleshoot setting up the IDE, the compiler, and any other required libraries (e.g. catkin, python, ssh, etc.) which will increase the time it takes to configure their system and increase the number of failure points in their configuration process (not to mention increasing the platform and version control support nightmare).\nTo combat these issues, we wanted ROSMOD to act as a lightweight (from the users' perspectives) IDE which does not require any installation, and can be run cross-platform since it only relies on a web-browser (even smartPhones can act as interfaces!). In this way, the development of robotics with ROSMOD is done in a decentralized, collaborative, automatically versioned development , with centralized management of the infrastructure (including any compatibility, package version management, and build / deployment infrastructure management).\nImplementation\nROSMOD is implemented as a web-based graphical interface to a versioned, git-like database of models with integrated code. Along with this database exist server-side plugins which enable the generation, compilation, and deployment of executable code. This executable code is fully complete without the need for users to touch or even download the files. These plugins have accses to the file-system on the server, where the sys-admin for the ROSMOD webserver has already configured the build system and deployment infrastructure so that these plugins can automatically perform the compilation and deployment as requested by the users.\nFeatures\nIntegrated support for run-time monitoring using ROS Bridge\nIntegrated support for run-time mission control / command and telemetry using NASA OpenMCT and ROSMCT\nIntegrated Hierarchical Finite State Machine (HFSM) Modeling, Simulation, and Code Generation\nSee webgme-hfsm for documentation, walkthroughs, videos, etc.\nCollaborative, automatically versioned web-based development\nModel-based framework for developing hardware and software\nFully integrated code development, with documentation generation\nFully integrated code generation, compilation, and deployment\nInteractive deployment visualization\nEmbeddable documentation at every level of the model\nDistributed debugging enabled by automatic trace logging and plotting of trace and user logs\nAutomatic load management of embedded targets\nI don't have a picture here because it's in the backend :)\nHow to set up ROSMOD\nSetting up the ROSMOD Server\nRequired Dependencies\nInstall Node.js LTS\ncd ~/Downloads\nwget https://nodejs.org/dist/v8.11.1/node-v8.11.1-linux-x64.tar.xz\ntar xvf node-v8.11.1-linux-x64.tar.xz\nsudo cp -r node-v8.11.1-linux-x64/{bin,include,lib} /usr/.\nInstall Bower\nsudo npm install -g bower\nInstall MongoDB\nsudo apt-get install mongodb\nClone this repo\ngit clone https://github.com/rosmod/webgme-rosmod\nInstall dependencies\ncd webgme-rosmod\nnpm install\nOptional Dependencies\n# for documentation generation\nsudo apt-get install pandoc doxygen texlive-full\n# for debugging\nsudo apt-get install gdbserver gdb-multiarch valgrind\n# add repo for rosbridge - this is copied from http://wiki.ros.org/kinetic/Installation/Ubuntu\nsudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-latest.list'\nsudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 421C365BD9FF1F717815A3895523BAEEB01FA116\nsudo apt-get update\n# for connection to rosmct / rosbridge\nsudo apt-get install ros-kinetic-rosbridge-suite\nStart the server\nnpm start\nNOTE: to make changes to the META or to view the META, you can run\nnpm run start-dev\nwhich enables the META visualizer.\nSetting up target systems to run ROSMOD\nNote: If you want to be able to compile on the server, you will need to follow these steps for the server as well. Note: These steps have changed and no longer depend on rosmod-comm, please make sure to update your targets accordingly.\nInstall ROS onto the target platform.\nNOTE: only ros-<version>-ros-base is required, but any install target works.\nInstall ROSMOD-Actor onto the target platform. (Optionally) If you want to use Action Clients or Action Servers from actionlib, you will need to install our custom fork of actionlib from rosmod/actionlib. To install both, simply copy these commands below.\n# install both rosmod/actionlib and rosmod/rosmod-actor to /opt/rosmod\nmkdir -p ~/rosmod_ws/src\ncd ~/rosmod_ws\ncatkin config --extend /opt/ros/kinetic\ncatkin config -i /opt/rosmod\ncatkin config --install\ngit clone https://github.com/ros/common_msgs.git src/common_msgs\ngit clone https://github.com/rosmod/actionlib src/actionlib\ngit clone https://github.com/rosmod/rosmod-actor src/rosmod-actor\ncatkin build\nEnsure the target has SSH capabilities; configure an ssh key that the ROSMOD server can use to remotely access the target device.\nNOTE: password-based authentication is not allowed for ROSMOD targets.\n(Optional) If you want to perform debugging - you should install valgrind and gdbserver on the target systems\nsudo apt-get install valgrind gdbserver\nHow to use ROSMOD\nThis section serves as a short guide for users who want to develop robotics with ROSMOD. A more complete guide into what each of the elements of a ROSMOD model is can be found in the Samples Seed, which contains some example projects for robots, autonomous cars, and automous planes, as well as a simple Introduction to ROSMOD sample which is fully documented (within the model using Documentation objects that render Markdown within the model. This sample is hosted on the live server listed at the top of this page, but can also be used as the base for any project created in any deployment of the server since it is part of this repository.\nCreating a ROSMOD Project\nIn ROSMOD, there are two different types of Projects:\nA WebGME Project which can contain any number of ROSMOD Projects, and which is the granularity at which you can authenticate for user / organization RWD access.\nA ROSMOD Project, which is a self-contained model tree which describes the Software, Systems, Deployments, and Experiments.\nCreating a WebGME project\nTo create a WebGME project, you open your web-browser (NOTE: Google Chrome is the fully-supported browser, YMMV with any other browser) and navigate to the web address of the ROSMOD server.\nAfter you login (if authentication is enabled) you will be presented with the WebGME landing page, which will show you the available projects you have access to on the ROSMOD server, and will provide a Create New button in the bottom left of the modal dialog that allows you to create your own project. When creating a WebGME project, you can either use one of the ROSMOD seeds (base or samples) as the basis for your project or you can duplicate an existing project on the server (that you have access to). The seeds provide the meta-model that defines ROSMOD, and all projects made on the ROSMOD server start out with one of the two seeds as a base.\nCreating a single ROSMOD Project\nHaving created a WebGME project, you will see the ROSMOD Project Root view, which shows all ROSMOD projects contained within the WebGME project. If you copied an empty WebGME project or used the base seed, then the page will be empty.\nTo create a ROSMOD project, you simply drag and drop the Project object from the left panel (the Part Browser) into the empty space, and a new ROSMOD Project will show up.\nNote: single clicking within the project's space in the center panel will select the project within the Property Editor in the right panel which allows you to edit the Project attributes.\nNote: to edit the Authors, Brief Description, or Detailed Description project attributes, you can double click on the text for those attributes within the center panel (the Visualizer) or you can single click on the attribute in the Property Editor in the right panel.\nNote: to edit the icon the displays for the project, you should click the New Document icon that is on the far right side of the Icon attribute in the Property Editor in the right panel. This will bring up a file upload dialog where you should select the SVG Icon you wish to associate with this project.\nModeling the Software\nHaving created a ROSMOD project, double click on the project icon to open the project within the Visualizer, which will automatically switch to the Model Visualizer. With the project open in the Model visualizer, drag a Software object into the center panel, this Software object will be the root of the sub-tree that describes all of the software for this project.\nWithin the software object, you can create any number of ROS Packages, Source Libraries, and System Libraries.\nWithin a Package, you can define any number of ROS Messages and ROS Services, as well as any number of ROSMOD Components.\nWithin a Component, you can define any number of ROS Publishers, ROS Subscribers, ROS Clients, ROS Servers, and ROS Timers.\nTo edit the code defining any of these objects, click the CodeEditor visualizer, which will bring up the CodeEditor's code attribute tree on the left (in the active visualizer) and the code being edited on the right.\nModeling the Systems\nHaving created a ROSMOD project, double click on the project icon to open the project within the Visualizer, which will automatically switch to the Model Visualizer. With the project open in the Model visualizer, drag a Systems object into the center panel, this Systems object will be the root of the sub-tree that describes all of the systems for which this project has been developed.\nDouble click the Systems object to open it. Within this aspect, you can drag a System object to describe a single system. A System is a collection of Hosts connected by one or more Networks and a collection of Users which may have access to certain Hosts.\nCreating a Deployment\nHaving created a ROSMOD project, double click on the project icon to open the project within the Visualizer, which will automatically switch to the Model Visualizer. With the project open in the Model visualizer, drag a Deployments object into the center panel, this Deployments object will be the root of the sub-tree that describes all of the deployment configurations for this project.\nDouble click on the Deployments object to open it. Within this aspect, you can drag a Deployment object to describe a single deployment. A Deployment is a collection of Component Instances which are colocated into Nodes ( POSIX processes ), which are themselves colocated into Containers. Containers are abstract representations of computing hardware.\nCreating an Experiment\nHaving created a ROSMOD project, double click on the project icon to open the project within the Visualizer, which will automatically switch to the Model Visualizer. With the project open in the Model visualizer, drag an Experiments object into the center panel, this Experiments object will be the root of the sub-tree that contains all of the current experiments, past experiments, and their results for this project.\nDouble click on the Experiments object to open it. Within this aspect, you can drag an Experiment object to describe an experiment you wish to run. An experiment maps a Deployment to a System. To configure the experiment with the deployment you want to run and the system on which you wish to run it, you can drag from the Object Browser in the right panel a Deployment and a System (that must be within the same ROSMOD project onto the Experiment object you have created. The Experiment will turn green while your drag is over it, indicating the Experiment is a valid drop target for the object you are dragging. This will set the respective System or Deployment pointer within the Experiment object. To view (or set or clear) the value for these pointers, single click on the Experiment to select it within the Property Editor panel. With the Experiment active in the Property Editor, press the property editor's Pointers tab, which will show the pointers the object has.\nNOTE: Never alter an object's base pointer!\nRunning an Experiment\nTo run an experiment, simply execute the RunExperiment plugin. The plugin will automatically check to ensure that the system pointed to by the experiment is running and has the available resources while also satisfying the constraints of the software.\nIf the experiment successfully deploys to the system, the plugin will update the model to show which containers from the deployment mapped to which hosts of the system.\nStopping a Running Experiment\nIf the experiment has a valid model mapping containers to hosts (which can only be created by running an experiment), then the StopExperiment plugin will stop the experiment's processes and copy the component and ROS logs back into the model as attributes of a time-stamped Results object.\nOpening the Results object generated by the StopExperiment plugin will open the ResultsViz visualizer which will automatically plot the logs recovered from the experiment.\nKeeping ROSMOD Up-to-Date\nTo keep ROSMOD up to date, you simply need to periodically stop the server, pull from the repository, update the npm packages, and then restart the server.\n# 1. within the terminal that is running the server press ctrl+c to stop the server\n^C\n# 2. pull\ngit pull\n# 3. update\nnpm update\n# 4. restart\nnpm start", "link": "https://github.com/rosmod/webgme-rosmod", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "rosmod\nthis repository contains the implementation of rosmod - journal of electronics, built on webgme.\nrosmod is a web-based, collaborative, modeling and execution environment for distributed embedded applications built using ros.\nan example server running rosmod can be found on rosmod.collaborative-design.org.\nplease see the wiki for in-depth information about rosmod, how to use it, and links to various youtube videos that walk through development and usage of rosmod.\nnote: webgme rosmod code generators have changed - they no longer generate for rosmod-comm, please see setting up target systems to run rosmod to properly install the new dependencies.\ncontents\nwhat is rosmod\nmotivation\nimplementation\nfeatures\nhow to set up rosmod\nsetting up the rosmod server\nrequired dependencies\noptional dependencies\nstart the server\nsetting up target systems to run rosmod\nhow to use rosmod\ncreating a rosmod project\ncreating a webgme project\ncreating a single rosmod project\nmodeling the software\nmodeling the systems\ncreating a deployment\ncreating an experiment\nrunning an experiment\nstopping a running experiment\nkeeping rosmod up to date\nwhat is rosmod?\nthe robot operating system model-driven development -----> tool !!!  suite, (rosmod) an integrated development environment for rapid prototyping component-based software for the robot operating system (ros) middleware.\nrosmod consists of\nthe rosmod server (this repo), which provides collaborative model-driven development, and\nthe rosmod actor (github), which is a standardized ros::node that provides a well defined component execution model which helps reduce the complexity of writing multi-threaded code.\nmotivation\nrobotics is a discipline that involves integration between heterogeneous fields such as electronics, mechanics, computer science, control theory and artificial intelligence. during the design, development and deployment of robotic systems, the components that belong to the various sub-systems, e.g., image processing software, servo-motors, power distribution system, etc., must work congruously to achieve a common goal. as systems increase in complexity, software development platforms must enable the creation of such components in a rapid, reliable, and reusable manner while also providing systems-level design, analysis, and deployment.\nin other similar systems, users are often tasked with developing the models, generating the code, and then manually touching the generated code to implement the business logic for the various operations their system needs to perform (e.g. periodic or event-triggered functions). the injection and management of this user-developed code inside the generated code can increase the learning curve for new users as it adds some extra file management and build-system overhead. additionally, almost all other similar projects require installation on the users' computers, which means they may need to troubleshoot setting up the ide, the compiler, and any other required libraries (e.g. catkin, python, ssh, etc.) which will increase the time it takes to configure their system and increase the number of failure points in their configuration process (not to mention increasing the platform and version control support nightmare).\nto combat these issues, we wanted rosmod to act as a lightweight (from the users' perspectives) ide which does not require any installation, and can be run cross-platform since it only relies on a web-browser (even smartphones can act as interfaces!). in this way, the development of robotics with rosmod is done in a decentralized, collaborative, automatically versioned development , with centralized management of the infrastructure (including any compatibility, package version management, and build / deployment infrastructure management).\nimplementation\nrosmod is implemented as a web-based graphical interface to a versioned, git-like database of models with integrated code. along with this database exist server-side plugins which enable the generation, compilation, and deployment of executable code. this executable code is fully complete without the need for users to touch or even download the files. these plugins have accses to the file-system on the server, where the sys-admin for the rosmod webserver has already configured the build system and deployment infrastructure so that these plugins can automatically perform the compilation and deployment as requested by the users.\nfeatures\nintegrated support for run-time monitoring using ros bridge\nintegrated support for run-time mission control / command and telemetry using nasa openmct and rosmct\nintegrated hierarchical finite state machine (hfsm) modeling, simulation, and code generation\nsee webgme-hfsm for documentation, walkthroughs, videos, etc.\ncollaborative, automatically versioned web-based development\nmodel-based framework for developing hardware and software\nfully integrated code development, with documentation generation\nfully integrated code generation, compilation, and deployment\ninteractive deployment visualization\nembeddable documentation at every level of the model\ndistributed debugging enabled by automatic trace logging and plotting of trace and user logs\nautomatic load management of embedded targets\ni don't have a picture here because it's in the backend :)\nhow to set up rosmod\nsetting up the rosmod server\nrequired dependencies\ninstall node.js lts\ncd ~/downloads\nwget https://nodejs.org/dist/v8.11.1/node-v8.11.1-linux-x64.tar.xz\ntar xvf node-v8.11.1-linux-x64.tar.xz\nsudo cp -r node-v8.11.1-linux-x64/{bin,include,lib} /usr/.\ninstall bower\nsudo npm install -g bower\ninstall mongodb\nsudo apt-get install mongodb\nclone this repo\ngit clone https://github.com/rosmod/webgme-rosmod\ninstall dependencies\ncd webgme-rosmod\nnpm install\noptional dependencies\n# for documentation generation\nsudo apt-get install pandoc doxygen texlive-full\n# for debugging\nsudo apt-get install gdbserver gdb-multiarch valgrind\n# add repo for rosbridge - this is copied from http://wiki.ros.org/kinetic/installation/ubuntu\nsudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-latest.list'\nsudo apt-key adv --keyserver hkp://ha.pool.sks-keyservers.net:80 --recv-key 421c365bd9ff1f717815a3895523baeeb01fa116\nsudo apt-get update\n# for connection to rosmct / rosbridge\nsudo apt-get install ros-kinetic-rosbridge-suite\nstart the server\nnpm start\nnote: to make changes to the meta or to view the meta, you can run\nnpm run start-dev\nwhich enables the meta visualizer.\nsetting up target systems to run rosmod\nnote: if you want to be able to compile on the server, you will need to follow these steps for the server as well. note: these steps have changed and no longer depend on rosmod-comm, please make sure to update your targets accordingly.\ninstall ros onto the target platform.\nnote: only ros-<version>-ros-base is required, but any install target works.\ninstall rosmod-actor onto the target platform. (optionally) if you want to use action clients or action servers from actionlib, you will need to install our custom fork of actionlib from rosmod/actionlib. to install both, simply copy these commands below.\n# install both rosmod/actionlib and rosmod/rosmod-actor to /opt/rosmod\nmkdir -p ~/rosmod_ws/src\ncd ~/rosmod_ws\ncatkin config --extend /opt/ros/kinetic\ncatkin config -i /opt/rosmod\ncatkin config --install\ngit clone https://github.com/ros/common_msgs.git src/common_msgs\ngit clone https://github.com/rosmod/actionlib src/actionlib\ngit clone https://github.com/rosmod/rosmod-actor src/rosmod-actor\ncatkin build\nensure the target has ssh capabilities; configure an ssh key that the rosmod server can use to remotely access the target device.\nnote: password-based authentication is not allowed for rosmod targets.\n(optional) if you want to perform debugging - you should install valgrind and gdbserver on the target systems\nsudo apt-get install valgrind gdbserver\nhow to use rosmod\nthis section serves as a short guide for users who want to develop robotics with rosmod. a more complete guide into what each of the elements of a rosmod model is can be found in the samples seed, which contains some example projects for robots, autonomous cars, and automous planes, as well as a simple introduction to rosmod sample which is fully documented (within the model using documentation objects that render markdown within the model. this sample is hosted on the live server listed at the top of this page, but can also be used as the base for any project created in any deployment of the server since it is part of this repository.\ncreating a rosmod project\nin rosmod, there are two different types of projects:\na webgme project which can contain any number of rosmod projects, and which is the granularity at which you can authenticate for user / organization rwd access.\na rosmod project, which is a self-contained model tree which describes the software, systems, deployments, and experiments.\ncreating a webgme project\nto create a webgme project, you open your web-browser (note: google chrome is the fully-supported browser, ymmv with any other browser) and navigate to the web address of the rosmod server.\nafter you login (if authentication is enabled) you will be presented with the webgme landing page, which will show you the available projects you have access to on the rosmod server, and will provide a create new button in the bottom left of the modal dialog that allows you to create your own project. when creating a webgme project, you can either use one of the rosmod seeds (base or samples) as the basis for your project or you can duplicate an existing project on the server (that you have access to). the seeds provide the meta-model that defines rosmod, and all projects made on the rosmod server start out with one of the two seeds as a base.\ncreating a single rosmod project\nhaving created a webgme project, you will see the rosmod project root view, which shows all rosmod projects contained within the webgme project. if you copied an empty webgme project or used the base seed, then the page will be empty.\nto create a rosmod project, you simply drag and drop the project object from the left panel (the part browser) into the empty space, and a new rosmod project will show up.\nnote: single clicking within the project's space in the center panel will select the project within the property editor in the right panel which allows you to edit the project attributes.\nnote: to edit the authors, brief description, or detailed description project attributes, you can double click on the text for those attributes within the center panel (the visualizer) or you can single click on the attribute in the property editor in the right panel.\nnote: to edit the icon the displays for the project, you should click the new document icon that is on the far right side of the icon attribute in the property editor in the right panel. this will bring up a file upload dialog where you should select the svg icon you wish to associate with this project.\nmodeling the software\nhaving created a rosmod project, double click on the project icon to open the project within the visualizer, which will automatically switch to the model visualizer. with the project open in the model visualizer, drag a software object into the center panel, this software object will be the root of the sub-tree that describes all of the software for this project.\nwithin the software object, you can create any number of ros packages, source libraries, and system libraries.\nwithin a package, you can define any number of ros messages and ros services, as well as any number of rosmod components.\nwithin a component, you can define any number of ros publishers, ros subscribers, ros clients, ros servers, and ros timers.\nto edit the code defining any of these objects, click the codeeditor visualizer, which will bring up the codeeditor's code attribute tree on the left (in the active visualizer) and the code being edited on the right.\nmodeling the systems\nhaving created a rosmod project, double click on the project icon to open the project within the visualizer, which will automatically switch to the model visualizer. with the project open in the model visualizer, drag a systems object into the center panel, this systems object will be the root of the sub-tree that describes all of the systems for which this project has been developed.\ndouble click the systems object to open it. within this aspect, you can drag a system object to describe a single system. a system is a collection of hosts connected by one or more networks and a collection of users which may have access to certain hosts.\ncreating a deployment\nhaving created a rosmod project, double click on the project icon to open the project within the visualizer, which will automatically switch to the model visualizer. with the project open in the model visualizer, drag a deployments object into the center panel, this deployments object will be the root of the sub-tree that describes all of the deployment configurations for this project.\ndouble click on the deployments object to open it. within this aspect, you can drag a deployment object to describe a single deployment. a deployment is a collection of component instances which are colocated into nodes ( posix processes ), which are themselves colocated into containers. containers are abstract representations of computing hardware.\ncreating an experiment\nhaving created a rosmod project, double click on the project icon to open the project within the visualizer, which will automatically switch to the model visualizer. with the project open in the model visualizer, drag an experiments object into the center panel, this experiments object will be the root of the sub-tree that contains all of the current experiments, past experiments, and their results for this project.\ndouble click on the experiments object to open it. within this aspect, you can drag an experiment object to describe an experiment you wish to run. an experiment maps a deployment to a system. to configure the experiment with the deployment you want to run and the system on which you wish to run it, you can drag from the object browser in the right panel a deployment and a system (that must be within the same rosmod project onto the experiment object you have created. the experiment will turn green while your drag is over it, indicating the experiment is a valid drop target for the object you are dragging. this will set the respective system or deployment pointer within the experiment object. to view (or set or clear) the value for these pointers, single click on the experiment to select it within the property editor panel. with the experiment active in the property editor, press the property editor's pointers tab, which will show the pointers the object has.\nnote: never alter an object's base pointer!\nrunning an experiment\nto run an experiment, simply execute the runexperiment plugin. the plugin will automatically check to ensure that the system pointed to by the experiment is running and has the available resources while also satisfying the constraints of the software.\nif the experiment successfully deploys to the system, the plugin will update the model to show which containers from the deployment mapped to which hosts of the system.\nstopping a running experiment\nif the experiment has a valid model mapping containers to hosts (which can only be created by running an experiment), then the stopexperiment plugin will stop the experiment's processes and copy the component and ros logs back into the model as attributes of a time-stamped results object.\nopening the results object generated by the stopexperiment plugin will open the resultsviz visualizer which will automatically plot the logs recovered from the experiment.\nkeeping rosmod up-to-date\nto keep rosmod up to date, you simply need to periodically stop the server, pull from the repository, update the npm packages, and then restart the server.\n# 1. within the terminal that is running the server press ctrl+c to stop the server\n^c\n# 2. pull\ngit pull\n# 3. update\nnpm update\n# 4. restart\nnpm start", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7001000, "year": null}, {"Unnamed: 0": 2021, "autor": 1001, "date": null, "content": "Yakut\nYak\u00fat is a simple cross-platform command-line interface (CLI) tool for diagnostics and debugging of UAVCAN networks. By virtue of being based on PyUAVCAN, Yakut supports all UAVCAN transports (UDP, serial, CAN, ...) and is compatible with all major features of the protocol. It is designed to be usable with GNU/Linux, Windows, and macOS.\nAsk questions and get assistance at forum.uavcan.org.\nInstalling\nFirst, make sure to have Python installed. Windows users are recommended to grab the official distribution from Windows Store. Yakut requires Python 3.7 or newer.\nInstall Yakut: pip install yakut\nBy default, Yakut does not support joysticks or MIDI controllers (this feature is described below in section Publishing messages). To enable the support for input devices, install the optional dependency: pip install yakut[joystick]. GNU/Linux users will need to also install: SDL2, possibly libjack (with headers), possibly libasound2 (with headers).\nAfterward do endeavor to read the docs: yakut --help\nCheck for new versions every now and then: pip install --upgrade yakut\nInstallation & configuration screencasts for Windows and GNU/Linux are available on the forum.\nInvoking commands\nAny option can be supplied either as a command-line argument or as an environment variable named like YAKUT_[subcommand_]option. If both are provided, command-line options take precedence over environment variables. You can use this feature to configure desired defaults by exporting environment variables from the rc-file of your shell (for bash/zsh this is ~/.bashrc/~/.zshrc, for PowerShell see $profile).\nOptions for the main command shall be specified before the subcommand when invoking Yakut:\nyakut --path=/the/path compile path/to/my_namespace --output=destination/directory\nIn this example, the corresponding environment variables are YAKUT_PATH and YAKUT_COMPILE_OUTPUT.\nAny subcommand like yakut compile can be used in an abbreviated form like yakut com as long as the resulting abbreviation is unambiguous.\nThere is a dedicated --help option for every subcommand.\nYakut may also be invoked via its alias y as long as this name does not conflict with another installed program.\nCompiling DSDL\nSuppose we have our custom DSDL namespace that we want to use. First, it needs to be compiled:\nyakut compile ~/custom_data_types/sirius_cyber_corp\nMost of the commands require the standard namespace to be available, so let's compile it too, along with the regulated namespace:\nyakut compile ~/public_regulated_data_types/uavcan ~/public_regulated_data_types/reg\nCompilation outputs will be stored in the current working directory, but it can be overridden if needed via --output or YAKUT_COMPILE_OUTPUT. Naturally, Yakut needs to know where the outputs are located to use them; by default it looks in the current directory. You can specify additional search locations using --path or YAKUT_PATH.\nA question one is likely to ask here is: Why don't you ship precompiled regulated DSDL together with the tool? Indeed, that would be trivial to do, but we avoid that on purpose to emphasize our commitment to supporting vendor-specific and regulated DSDL at the same level. In the past we used to give regulated namespaces special treatment, which caused our users to acquire misconceptions about the purpose of DSDL. Specifically, there have been forks of the standard namespace extended with vendor-specific types, which is harmful to the ecosystem.\nHaving to manually compile the regulated namespaces is not an issue because it is just a single command to run. You may opt to keeping compiled namespaces that you use often somewhere in a dedicated directory and put YAKUT_PATH=/your/directory into your shell's rc-file so that you don't have to manually specify the path when invoking Yakut. Similarly, you can configure it to use that directory as the default destination for compiled DSDL:\n# bash/zsh on GNU/Linux or macOS\nexport YAKUT_COMPILE_OUTPUT=~/.yakut\nexport YAKUT_PATH=\"$YAKUT_COMPILE_OUTPUT\"\n# PowerShell on Windows (double quotes are always required!)\n$env:YAKUT_COMPILE_OUTPUT=\"$env:APPDATA\\Yakut\"\n$env:YAKUT_PATH=\"$env:YAKUT_COMPILE_OUTPUT\"\nSo that you say simply yakut compile path/to/my_namespace knowing that the outputs will be always stored to and read from a fixed place unless you override it.\nCommunicating\nCommands that access the network need to know how to do so. There are two ways to configure that: pass UAVCAN registers via environment variables (this is the default), or pass an initialization expression via --transport/YAKUT_TRANSPORT (in which case the registers are ignored). The latter is not recommended for general use so we'll focus on the first one.\nUAVCAN registers are named values that contain various configuration parameters of a UAVCAN application/node. They are extensively described in the UAVCAN Specification. When starting a new process, it is possible to pass arbitrary registers via environment variables.\nThere are certain registers that are looked at by UAVCAN nodes to determine how to connect to the network. Some of them are given below, but the list is not exhaustive. The full description of supported registers is available in the API documentation for pyuavcan.application.make_transport().\nIf the available registers define more than one transport configuration, a redundant transport will be initialized.\nTransport Register name Register type Environment variable name Semantics Example environment variable value\nAll uavcan.node.id natural16[1] UAVCAN__NODE__ID The local node-ID; anonymous if not set 42\nUDP uavcan.udp.iface string UAVCAN__UDP__IFACE Space-separated local IPs (16 LSB set to node-ID) 127.9.0.0 192.168.0.0\nSerial uavcan.serial.iface string UAVCAN__SERIAL__IFACE Space-separated serial port names COM9 socket://127.0.0.1:50905\nCAN uavcan.can.iface string UAVCAN__CAN__IFACE Space-separated CAN iface names socketcan:vcan0 pcan:PCAN_USBBUS1\nCAN uavcan.can.mtu natural16[1] UAVCAN__CAN__MTU Maximum transmission unit; selects Classic/FD 64\nCAN uavcan.can.bitrate natural32[2] UAVCAN__CAN__BITRATE Arbitration/data segment bits per second 1000000 4000000\nLoopback uavcan.loopback bit[1] UAVCAN__LOOPBACK Use loopback interface (only for basic testing) 1\nSubscribing to subjects\nSubscribe to subject 33 of type uavcan.si.unit.angle.Scalar.1.0 as shown below; notice how we specify the subject-ID before the data type name. You will see output if there is a publisher on this subject (more on this in the next section).\n$ export UAVCAN__UDP__IFACE=127.63.0.0\n$ yakut sub 33:uavcan.si.unit.angle.Scalar.1.0\n---\n33:\n_metadata_:\ntimestamp: {system: 1608987583.298886, monotonic: 788272.540747}\npriority: nominal\ntransfer_id: 0\nsource_node_id: 42\nradian: 2.309999942779541\n---\n33:\n_metadata_:\ntimestamp: {system: 1608987583.298886, monotonic: 788272.540747}\npriority: nominal\ntransfer_id: 1\nsource_node_id: 42\nradian: 2.309999942779541\nPublishing messages\nPublishing two messages synchronously twice (four messages total):\nexport UAVCAN__UDP__IFACE=127.63.0.0\nexport UAVCAN__NODE__ID=42\nyakut pub -N2 33:uavcan.si.unit.angle.Scalar.1.0 'radian: 2.31' \\\nuavcan.diagnostic.Record.1.1 'text: \"2.31 rad\"'\nWe did not specify the subject-ID for the second subject, so Yakut defaulted to the fixed subject-ID.\nThe above example will publish constant values which is rarely useful. You can define arbitrary Python expressions that are evaluated by Yakut before publication. Such expressions are entered as strings marked with YAML tag !$. There may be an arbitrary number of such expressions in a YAML document, and their results may be arbitrary as long as the final structure can initialize the specified message. The following example will publish a sinewave with frequency 1 Hz, amplitude 10 meters:\nyakut pub -T 0.01 1234:uavcan.si.unit.length.Scalar.1.0 '{meter: !$ \"sin(t * pi * 2) * 10\"}'\nNotice that we make use of entities like the variable t or the standard function sin in the expression. You will see the full list of available entities if you run yakut pub --help.\nOne particularly important capability of this command is the ability to read data from connected joysticks or MIDI controllers. It allows the user to control UAVCAN processes or equipment in real time, simulate sensor feeds, etc. Function A(x,y) returns the normalized value of axis y from connected controller x (for full details see yakut pub --help); likewise, there is B(x,y) for push buttons and T(x,y) for toggle switches. The next example will publish 3D angular velocity setpoint, thrust setpoint, and the arming switch state, allowing the user to control these parameters interactively:\nyakut pub -T 0.1 \\\n5:uavcan.si.unit.angular_velocity.Vector3.1.0 'radian_per_second: !$ \"[A(1,0)*10, A(1,1)*10, (A(1,2)-A(1,5))*5]\"' \\\n6:uavcan.si.unit.power.Scalar.1.0 'watt: !$ A(2,10)*1e3' \\\n7:uavcan.primitive.scalar.Bit.1.0 'value: !$ T(1,5)'\nThe list of connected controllers and how their axes are mapped can be seen using yakut joystick, as shown in the video:\nHere is an example where a MIDI controller is used to interactively change the frequency and amplitude of a sinewave:\nInvoking RPC-services\nGiven custom data types:\n# sirius_cyber_corp.PerformLinearLeastSquaresFit.1.0\nPointXY.1.0[<64] points\n@extent 1024 * 8\n---\nfloat64 slope\nfloat64 y_intercept\n@sealed\n# sirius_cyber_corp.PointXY.1.0\nfloat16 x\nfloat16 y\n@sealed\nSuppose that there is node 42 that serves sirius_cyber_corp.PerformLinearLeastSquaresFit.1.0 at service-ID 123:\n$ export UAVCAN__UDP__IFACE=127.63.0.0\n$ export UAVCAN__NODE__ID=42\n$ yakut compile sirius_cyber_corp\n$ yakut call 42 123:sirius_cyber_corp.PerformLinearLeastSquaresFit.1.0 'points: [{x: 10, y: 1}, {x: 20, y: 2}]'\n---\n123:\nslope: 0.1\ny_intercept: 0.0\nMonitoring the network\nThe command yakut monitor can be used to display all activity on the network in a compact representation. It tracks online nodes and maintains real-time statistics on all transfers exchanged between each node on the network. It may also be able to detect some common network configuration issues like zombie nodes (nodes that do not publish uavcan.node.Heartbeat).\nRead yakut monitor --help for details.\n$ export UAVCAN__CAN__IFACE=\"socketcan:can0 socketcan:can1 socketcan:can2\" # Triply-redundant UAVCAN/CAN\n$ export UAVCAN__CAN__MTU=8 # Force MTU = 8 bytes\n$ export UAVCAN__CAN__BITRATE=\"1000000 1000000\" # Disable BRS, use the same bit rate for arbitration/data\n$ y mon # Abbreviation of \"yakut monitor\"\nThe monitor can be an anonymous node or it can be given a node-ID of its own. In the latter case it will actively query other nodes using the standard introspection services.\nSome transports, UAVCAN/UDP in particular, require special privileges to run this tool due to the security implications of low-level packet capture.\nUpdating node software\nThe file server command can be used to serve files, run a plug-and-play node-ID allocator (some embedded bootloader implementations require that), and automatically send software update requests uavcan.node.ExecuteCommand to nodes whose software is old.\nTo demonstrate this capability, suppose that the network contains the following nodes:\nnodes 1, 2 named com.example.foo, software 1.0\nnodes 3, 4 named com.example.bar, hardware v4.2, software v3.4\nnode 5 named com.example.baz\nSoftware updates are distributed as atomic package files. In case of embedded systems, the package is usually just the firmware image, possibly compressed or amended with some metadata. For the file server this is irrelevant since it never looks inside the files it serves. However, the name is relevant as it shall follow a particular pattern to make the server recognize the file as a software package. The full specification is given in the command help: yakut file-server --help.\nSuppose that we have the following packages that we need to deploy:\nv1.1 for nodes com.example.foo with any hardware\nv3.3 for nodes com.example.bar with hardware v4.x\nv3.5 for nodes com.example.bar with hardware v5.6 only\nnothing for com.example.baz\n$ ls *.app* # List all software packages\ncom.example.foo-1.1.app.zip # Any hardware\ncom.example.bar-4-3.3.app.pkg # Hardware v4.x\ncom.example.bar-5.6-3.5.app.bin # Hardware v5.6 only\nThe server rescans its root directory whenever a new node is found online, meaning that packages can be added/removed at runtime and the server will pick up the changes on the fly. Launch the server:\n$ export UAVCAN__UDP__IFACE=127.63.0.0\n$ export UAVCAN__NODE__ID=42\n$ yakut file-server --plug-and-play=allocation_table.db --update-software\nIf there are any nodes online (or if they join the network later), the server will check the version of each by sending uavcan.node.GetInfo, and if a newer package is available locally, it will request the node to install it by sending uavcan.node.ExecuteCommand.\nIn this specific case, the following will happen:\nNodes 1 and 2 will be updated to v1.1.\nNodes 3 and 4 will not be updated because the newer package v3.5 is incompatible with hardware v4.2, and the compatible version v3.3 is too old.\nNode 5 will not be updated because there are no suitable packages.\nAdd --verbose to see how exactly the decisions are made.\nThis command can be used to implement automatic network-wide configuration management. Start the server and leave it running. Store all relevant packages into its root directory. When a node is connected or restarted, the server will automatically compare the version of its software against the local files and perform an update if necessary. Therefore, the entire network will be kept up-to-date without manual intervention.", "link": "https://github.com/UAVCAN/yakut", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "yakut\nyak\u00fat is a simple cross-platform command-line interface (cli) -----> tool !!!  for diagnostics and debugging of uavcan networks. by virtue of being based on pyuavcan, yakut supports all uavcan transports (udp, serial, can, ...) and is compatible with all major features of the protocol. it is designed to be usable with gnu/linux, windows, and macos.\nask questions and get assistance at forum.uavcan.org.\ninstalling\nfirst, make sure to have python installed. windows users are recommended to grab the official distribution from windows store. yakut requires python 3.7 or newer.\ninstall yakut: pip install yakut\nby default, yakut does not support joysticks or midi controllers (this feature is described below in section publishing messages). to enable the support for input devices, install the optional dependency: pip install yakut[joystick]. gnu/linux users will need to also install: sdl2, possibly libjack (with headers), possibly libasound2 (with headers).\nafterward do endeavor to read the docs: yakut --help\ncheck for new versions every now and then: pip install --upgrade yakut\ninstallation & configuration screencasts for windows and gnu/linux are available on the forum.\ninvoking commands\nany option can be supplied either as a command-line argument or as an environment variable named like yakut_[subcommand_]option. if both are provided, command-line options take precedence over environment variables. you can use this feature to configure desired defaults by exporting environment variables from the rc-file of your shell (for bash/zsh this is ~/.bashrc/~/.zshrc, for powershell see $profile).\noptions for the main command shall be specified before the subcommand when invoking yakut:\nyakut --path=/the/path compile path/to/my_namespace --output=destination/directory\nin this example, the corresponding environment variables are yakut_path and yakut_compile_output.\nany subcommand like yakut compile can be used in an abbreviated form like yakut com as long as the resulting abbreviation is unambiguous.\nthere is a dedicated --help option for every subcommand.\nyakut may also be invoked via its alias y as long as this name does not conflict with another installed program.\ncompiling dsdl\nsuppose we have our custom dsdl namespace that we want to use. first, it needs to be compiled:\nyakut compile ~/custom_data_types/sirius_cyber_corp\nmost of the commands require the standard namespace to be available, so let's compile it too, along with the regulated namespace:\nyakut compile ~/public_regulated_data_types/uavcan ~/public_regulated_data_types/reg\ncompilation outputs will be stored in the current working directory, but it can be overridden if needed via --output or yakut_compile_output. naturally, yakut needs to know where the outputs are located to use them; by default it looks in the current directory. you can specify additional search locations using --path or yakut_path.\na question one is likely to ask here is: why don't you ship precompiled regulated dsdl together with the tool? indeed, that would be trivial to do, but we avoid that on purpose to emphasize our commitment to supporting vendor-specific and regulated dsdl at the same level. in the past we used to give regulated namespaces special treatment, which caused our users to acquire misconceptions about the purpose of dsdl. specifically, there have been forks of the standard namespace extended with vendor-specific types, which is harmful to the ecosystem.\nhaving to manually compile the regulated namespaces is not an issue because it is just a single command to run. you may opt to keeping compiled namespaces that you use often somewhere in a dedicated directory and put yakut_path=/your/directory into your shell's rc-file so that you don't have to manually specify the path when invoking yakut. similarly, you can configure it to use that directory as the default destination for compiled dsdl:\n# bash/zsh on gnu/linux or macos\nexport yakut_compile_output=~/.yakut\nexport yakut_path=\"$yakut_compile_output\"\n# powershell on windows (double quotes are always required!)\n$env:yakut_compile_output=\"$env:appdata\\yakut\"\n$env:yakut_path=\"$env:yakut_compile_output\"\nso that you say simply yakut compile path/to/my_namespace knowing that the outputs will be always stored to and read from a fixed place unless you override it.\ncommunicating\ncommands that access the network need to know how to do so. there are two ways to configure that: pass uavcan registers via environment variables (this is the default), or pass an initialization expression via --transport/yakut_transport (in which case the registers are ignored). the latter is not recommended for general use so we'll focus on the first one.\nuavcan registers are named values that contain various configuration parameters of a uavcan application/node. they are extensively described in the uavcan specification. when starting a new process, it is possible to pass arbitrary registers via environment variables.\nthere are certain registers that are looked at by uavcan nodes to determine how to connect to the network. some of them are given below, but the list is not exhaustive. the full description of supported registers is available in the api documentation for pyuavcan.application.make_transport().\nif the available registers define more than one transport configuration, a redundant transport will be initialized.\ntransport register name register type environment variable name semantics example environment variable value\nall uavcan.node.id natural16[1] uavcan__node__id the local node-id; anonymous if not set 42\nudp uavcan.udp.iface string uavcan__udp__iface space-separated local ips (16 lsb set to node-id) 127.9.0.0 192.168.0.0\nserial uavcan.serial.iface string uavcan__serial__iface space-separated serial port names com9 socket://127.0.0.1:50905\ncan uavcan.can.iface string uavcan__can__iface space-separated can iface names socketcan:vcan0 pcan:pcan_usbbus1\ncan uavcan.can.mtu natural16[1] uavcan__can__mtu maximum transmission unit; selects classic/fd 64\ncan uavcan.can.bitrate natural32[2] uavcan__can__bitrate arbitration/data segment bits per second 1000000 4000000\nloopback uavcan.loopback bit[1] uavcan__loopback use loopback interface (only for basic testing) 1\nsubscribing to subjects\nsubscribe to subject 33 of type uavcan.si.unit.angle.scalar.1.0 as shown below; notice how we specify the subject-id before the data type name. you will see output if there is a publisher on this subject (more on this in the next section).\n$ export uavcan__udp__iface=127.63.0.0\n$ yakut sub 33:uavcan.si.unit.angle.scalar.1.0\n---\n33:\n_metadata_:\ntimestamp: {system: 1608987583.298886, monotonic: 788272.540747}\npriority: nominal\ntransfer_id: 0\nsource_node_id: 42\nradian: 2.309999942779541\n---\n33:\n_metadata_:\ntimestamp: {system: 1608987583.298886, monotonic: 788272.540747}\npriority: nominal\ntransfer_id: 1\nsource_node_id: 42\nradian: 2.309999942779541\npublishing messages\npublishing two messages synchronously twice (four messages total):\nexport uavcan__udp__iface=127.63.0.0\nexport uavcan__node__id=42\nyakut pub -n2 33:uavcan.si.unit.angle.scalar.1.0 'radian: 2.31' \\\nuavcan.diagnostic.record.1.1 'text: \"2.31 rad\"'\nwe did not specify the subject-id for the second subject, so yakut defaulted to the fixed subject-id.\nthe above example will publish constant values which is rarely useful. you can define arbitrary python expressions that are evaluated by yakut before publication. such expressions are entered as strings marked with yaml tag !$. there may be an arbitrary number of such expressions in a yaml document, and their results may be arbitrary as long as the final structure can initialize the specified message. the following example will publish a sinewave with frequency 1 hz, amplitude 10 meters:\nyakut pub -t 0.01 1234:uavcan.si.unit.length.scalar.1.0 '{meter: !$ \"sin(t * pi * 2) * 10\"}'\nnotice that we make use of entities like the variable t or the standard function sin in the expression. you will see the full list of available entities if you run yakut pub --help.\none particularly important capability of this command is the ability to read data from connected joysticks or midi controllers. it allows the user to control uavcan processes or equipment in real time, simulate sensor feeds, etc. function a(x,y) returns the normalized value of axis y from connected controller x (for full details see yakut pub --help); likewise, there is b(x,y) for push buttons and t(x,y) for toggle switches. the next example will publish 3d angular velocity setpoint, thrust setpoint, and the arming switch state, allowing the user to control these parameters interactively:\nyakut pub -t 0.1 \\\n5:uavcan.si.unit.angular_velocity.vector3.1.0 'radian_per_second: !$ \"[a(1,0)*10, a(1,1)*10, (a(1,2)-a(1,5))*5]\"' \\\n6:uavcan.si.unit.power.scalar.1.0 'watt: !$ a(2,10)*1e3' \\\n7:uavcan.primitive.scalar.bit.1.0 'value: !$ t(1,5)'\nthe list of connected controllers and how their axes are mapped can be seen using yakut joystick, as shown in the video:\nhere is an example where a midi controller is used to interactively change the frequency and amplitude of a sinewave:\ninvoking rpc-services\ngiven custom data types:\n# sirius_cyber_corp.performlinearleastsquaresfit.1.0\npointxy.1.0[<64] points\n@extent 1024 * 8\n---\nfloat64 slope\nfloat64 y_intercept\n@sealed\n# sirius_cyber_corp.pointxy.1.0\nfloat16 x\nfloat16 y\n@sealed\nsuppose that there is node 42 that serves sirius_cyber_corp.performlinearleastsquaresfit.1.0 at service-id 123:\n$ export uavcan__udp__iface=127.63.0.0\n$ export uavcan__node__id=42\n$ yakut compile sirius_cyber_corp\n$ yakut call 42 123:sirius_cyber_corp.performlinearleastsquaresfit.1.0 'points: [{x: 10, y: 1}, {x: 20, y: 2}]'\n---\n123:\nslope: 0.1\ny_intercept: 0.0\nmonitoring the network\nthe command yakut monitor can be used to display all activity on the network in a compact representation. it tracks online nodes and maintains real-time statistics on all transfers exchanged between each node on the network. it may also be able to detect some common network configuration issues like zombie nodes (nodes that do not publish uavcan.node.heartbeat).\nread yakut monitor --help for details.\n$ export uavcan__can__iface=\"socketcan:can0 socketcan:can1 socketcan:can2\" # triply-redundant uavcan/can\n$ export uavcan__can__mtu=8 # force mtu = 8 bytes\n$ export uavcan__can__bitrate=\"1000000 1000000\" # disable brs, use the same bit rate for arbitration/data\n$ y mon # abbreviation of \"yakut monitor\"\nthe monitor can be an anonymous node or it can be given a node-id of its own. in the latter case it will actively query other nodes using the standard introspection services.\nsome transports, uavcan/udp in particular, require special privileges to run this tool due to the security implications of low-level packet capture.\nupdating node software\nthe file server command can be used to serve files, run a plug-and-play node-id allocator (some embedded bootloader implementations require that), and automatically send software update requests uavcan.node.executecommand to nodes whose software is old.\nto demonstrate this capability, suppose that the network contains the following nodes:\nnodes 1, 2 named com.example.foo, software 1.0\nnodes 3, 4 named com.example.bar, hardware v4.2, software v3.4\nnode 5 named com.example.baz\nsoftware updates are distributed as atomic package files. in case of embedded systems, the package is usually just the firmware image, possibly compressed or amended with some metadata. for the file server this is irrelevant since it never looks inside the files it serves. however, the name is relevant as it shall follow a particular pattern to make the server recognize the file as a software package. the full specification is given in the command help: yakut file-server --help.\nsuppose that we have the following packages that we need to deploy:\nv1.1 for nodes com.example.foo with any hardware\nv3.3 for nodes com.example.bar with hardware v4.x\nv3.5 for nodes com.example.bar with hardware v5.6 only\nnothing for com.example.baz\n$ ls *.app* # list all software packages\ncom.example.foo-1.1.app.zip # any hardware\ncom.example.bar-4-3.3.app.pkg # hardware v4.x\ncom.example.bar-5.6-3.5.app.bin # hardware v5.6 only\nthe server rescans its root directory whenever a new node is found online, meaning that packages can be added/removed at runtime and the server will pick up the changes on the fly. launch the server:\n$ export uavcan__udp__iface=127.63.0.0\n$ export uavcan__node__id=42\n$ yakut file-server --plug-and-play=allocation_table.db --update-software\nif there are any nodes online (or if they join the network later), the server will check the version of each by sending uavcan.node.getinfo, and if a newer package is available locally, it will request the node to install it by sending uavcan.node.executecommand.\nin this specific case, the following will happen:\nnodes 1 and 2 will be updated to v1.1.\nnodes 3 and 4 will not be updated because the newer package v3.5 is incompatible with hardware v4.2, and the compatible version v3.3 is too old.\nnode 5 will not be updated because there are no suitable packages.\nadd --verbose to see how exactly the decisions are made.\nthis command can be used to implement automatic network-wide configuration management. start the server and leave it running. store all relevant packages into its root directory. when a node is connected or restarted, the server will automatically compare the version of its software against the local files and perform an update if necessary. therefore, the entire network will be kept up-to-date without manual intervention.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7001001, "year": null}, {"Unnamed: 0": 2022, "autor": 1002, "date": null, "content": "Two-dimensional materials manufacturing system (2DMMS)\nOpen-source software for automated searching and assembly for van der Waals heterostructures\nNote that this webpage is currently under development.\nAbout 2DMMS\nOpen-source platform for aunotnomous robotic searching and assembly of van der Waals heterostructures. The softwre is dedicated to minimize the human intervention involved in fabrication of van der Waals heterostructures. Using the software in combination with the hardware provides the following functionalities:\nAutomated searching for atomically thin exfoliated 2D crystals on SiO2/Si substrate\nBuilding the database of opitcal microscope images of detected 2D crystals\nComputer assisted design of van der Waals heterostructures by selecting the 2D crystals from the database\nSemi-automated assembly of designed van der Waals heterostructures\nLicense\nThe software is licenced under BSD-3-Clause License\nSoftware Requirements\nThe following software components are required.\nUbuntu 14.04 - ROS Indigo or Ubuntu 16.04 - ROS Kinetic\nQT 4.0.0 or higher\nHALCON12 Image processing library\nOpen CL\nMySQL\nTensorflow\nKeras\nInstall dependencies for Ubuntu 14.04\n% sudo apt-get install ros-indigo-desktop-full ros-indigo-nmea-msgs ros-indigo-nmea-navsat-driver ros-indigo-sound-play\nChanges\nbeta-1 initial public release.\nbeta-2 support for deep learning inference\nCitations\nThe detailed descriptions of this system are provided in the paper;\n\"Autonomous robotic searching and assembly of two-dimensional crystals to build van der Waals superlattices\", Satoru Masubuchi et al., Nature Communications 9, Article number: 1413 (2018).\n\"Classifying optical microscope images of exfoliated graphene flakes by data-driven machine learning\", Satoru Masubuchi and Tomoki Machida, npj 2D Materials and Applications volume 3, Article number: 4 (2019)\n\"Deep-Learning-Based Image Segmentation Integrated with Optical Microscopy for Automatically Searching for Two-Dimensional Materials\", Satoru Masubuchi et al., 4, Article number: 3 (2020)\nIf the information provided helps your research, it would be appreciated if you could cite the paper in your publications.\nRemarks\nI hope to continue developing 2DMMS to become a truly helpful tool for the research community of van der Waals heterostructures. Please feel free to email me with your feedback or any issues at: msatoru@iis.u-tokyo.ac.jp\nSatoru Masubuchi\nInstitute of Industrial Science, University of Tokyo\nHardware support\nThe following hardware comopnents are supported.\nAutomated Optical Microscope\nUnit Component Parts ID Manufacturer\nAutofocus Microscope Autofocus System AF-77VB-N60LP-TDS Chuo Precision\nAF-77VB Chuo Precision\nCMOS Camera Basler ACE acA2000-340kc Basler\nAF Pattern Positioner Chuo Precision\nAFC-5 Autofocus Controller Chuo Precision\nAF-61ZA Movement module Chuo Precision\nMotor Cables 3m for each Chuo Precision", "link": "https://github.com/tdmms/tdmms", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "two-dimensional materials manufacturing system (2dmms)\nopen-source software for automated searching and assembly for van der waals heterostructures\nnote that this webpage is currently under development.\nabout 2dmms\nopen-source platform for aunotnomous robotic searching and assembly of van der waals heterostructures. the softwre is dedicated to minimize the human intervention involved in fabrication of van der waals heterostructures. using the software in combination with the hardware provides the following functionalities:\nautomated searching for atomically thin exfoliated 2d crystals on sio2/si substrate\nbuilding the database of opitcal microscope images of detected 2d crystals\ncomputer assisted design of van der waals heterostructures by selecting the 2d crystals from the database\nsemi-automated assembly of designed van der waals heterostructures\nlicense\nthe software is licenced under bsd-3-clause license\nsoftware requirements\nthe following software components are required.\nubuntu 14.04 - ros indigo or ubuntu 16.04 - ros kinetic\nqt 4.0.0 or higher\nhalcon12 image processing library\nopen cl\nmysql\ntensorflow\nkeras\ninstall dependencies for ubuntu 14.04\n% sudo apt-get install ros-indigo-desktop-full ros-indigo-nmea-msgs ros-indigo-nmea-navsat-driver ros-indigo-sound-play\nchanges\nbeta-1 initial public release.\nbeta-2 support for deep learning inference\ncitations\nthe detailed descriptions of this system are provided in the paper;\n\"autonomous robotic searching and assembly of two-dimensional crystals to build van der waals superlattices\", satoru masubuchi et al., nature communications 9, article number: 1413 (2018).\n\"classifying optical microscope images of exfoliated graphene flakes by data-driven machine learning\", satoru masubuchi and tomoki machida, npj 2d materials and applications volume 3, article number: 4 (2019)\n\"deep-learning-based image segmentation integrated with optical microscopy for automatically searching for two-dimensional materials\", satoru masubuchi et al., 4, article number: 3 (2020)\nif the information provided helps your research, it would be appreciated if you could cite the paper in your publications.\nremarks\ni hope to continue developing 2dmms to become a truly helpful -----> tool !!!  for the research community of van der waals heterostructures. please feel free to email me with your feedback or any issues at: msatoru@iis.u-tokyo.ac.jp\nsatoru masubuchi\ninstitute of industrial science, university of tokyo\nhardware support\nthe following hardware comopnents are supported.\nautomated optical microscope\nunit component parts id manufacturer\nautofocus microscope autofocus system af-77vb-n60lp-tds chuo precision\naf-77vb chuo precision\ncmos camera basler ace aca2000-340kc basler\naf pattern positioner chuo precision\nafc-5 autofocus controller chuo precision\naf-61za movement module chuo precision\nmotor cables 3m for each chuo precision", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7001002, "year": null}, {"Unnamed: 0": 2024, "autor": 1004, "date": null, "content": "continuousprediction\nLearning Accurate Long-term Dynamicsfor Model-based Reinforcement Learning\nAbstract\nAccurately predicting the dynamics of robotic systems is crucial to make use of model-based control. A common way to estimate dynamics is by modeling the one-step ahead prediction and then use it to recursively propagate the predicted state distribution over long horizons. Unfortunately, this approach is known to compound even small prediction errors, making long-term predictions inaccurate. In this paper we propose a new parametrizaion to supervised learning on state-action data to stably predict at longer horizons -- that we call a trajectory-based model. This trajectory-based model takes an initial state, a time index, and control parameters as inputs and predicts a state at that time. Our results in simulated and experimental robotic tasks show accurate long term predictions, improved sample efficiency, and ability to predict task reward.\nRunning the Code:\nTo run the code use the following steps:\nCreate a conda environment from the provided yml file and activate it\nInstalling mujoco will fail. See the repo for instructions: https://github.com/openai/mujoco-py\nUsing this for your robot:\nTo use this on your robot, here will be the process:\nCreate a new file this your robot_name.py (this is needed becuase the controller changes for each robot).\nCreate an environment config file in conf/envs/robot_name.yml with items like state dimension, control parameter dimension, and more for model training. Also create or re-used a core conf file like reacher.yml in conf/.\nCreate or modify existing data generation and trajectory-based model training code. See create_dataset_traj( ) in multiple files for inspiration. The dimensions of this data must match the configuration.\nThe code should have two modes, train and collect. Collect runs the model and train will load objects from dynamics_model.py to train and save your model, if you so choose.\nUse evaluate.py to view the model prediction accuracy.\nCore files for models and evaluation:\ndynamics_model.py: This class contains the modular class for dynamics models of the single step and trajectory parametrization. There is code to use neural networks and gaussian processes as the modelling tool.\npolicy.py: This file contains the different controller parametrizations used in the experiments.\nplot.py: This file stores all the plotting functions used by the other files.\nmbrl_resource: Other functions used for iterative data collection.\nReplicating Experiments:\nFor questions on configurations, see Hyrda.\nLong-term prediction, section 5.2\nThis section has multiple files (reacher_pd.py, cartpole_lqr.py, crazyflie_pd.py, crazyflie_hardware.py) to collect data and train models, and a central file to evaluate results (evaluate.py). Because of a slightly different space (using hardware), crazyflie_hardware.py evaulates results by running it with mode=eval. An important config item is data_dir as this is where data will be saved, models will be saved from, and evaluate.py will test from.\nCollect simulated data: python reacher_pd.py models=t envs=reacher mode=collect\nTrain models: python reacher_pd.py models=t envs=reacher mode=train or a sweep with multiple models python reacher_pd.py -m models=d,de,t,te envs=reacher mode=train\nPredicting unstable and period data, section 5.3\nFor this experiment, procesd as above, but the data_dir needs to be changed in the cartpole configuration file. Also, the data_mode in conf/envs/cartpole.yaml must be changed correspondingly. The three datasets to be used are:\nStable data: trajectories/cartpole/rawl200_t100_v4.dat\nUnstable data: trajectories/cartpole/rawl200_t100_unstable.dat\nPeriodic data:trajectories/cartpole/rawl200_t100_chaotic.dat These files can of course be recollected.\nData efficiency, section 5.4\nExample of how to run efficiency code to train some models and then test them (this experiment is more computationally intensive):\nTrain: python3 efficiency.py training.num_traj=3,5,7,9 training.t_range=10,20,30,40 models=d,t training.copy=1,2,3,4,5 -m\nTest: python3 efficiency.py mode=plot plotting.num_traj=[3,5,7,9] plotting.t_range=[10,20,30,40] plotting.models=[d,t] plotting.copy=[1,2,3,4,5] -m\nPredicting reward, section 5.5\nThis example uses the file reward_rank.py. To run this, run python reward_rank.py envs=cartpole. It is currently not supported for any other environments.\nExtra files currently not in use:\nWhen examining the code, one will see a few extra files that represent potential future avenues for research. Some of these files are:\nlorenz.py: This was an attempt to model the long term behavior of the lorenz system. Results were mixed on this very challenging application.\nstable_system.py: This was used to evaluate how far into the future a trajectory-based model could predict a state-space system, but it was omitted from the paper.", "link": "https://github.com/natolambert/continuousprediction", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "continuousprediction\nlearning accurate long-term dynamicsfor model-based reinforcement learning\nabstract\naccurately predicting the dynamics of robotic systems is crucial to make use of model-based control. a common way to estimate dynamics is by modeling the one-step ahead prediction and then use it to recursively propagate the predicted state distribution over long horizons. unfortunately, this approach is known to compound even small prediction errors, making long-term predictions inaccurate. in this paper we propose a new parametrizaion to supervised learning on state-action data to stably predict at longer horizons -- that we call a trajectory-based model. this trajectory-based model takes an initial state, a time index, and control parameters as inputs and predicts a state at that time. our results in simulated and experimental robotic tasks show accurate long term predictions, improved sample efficiency, and ability to predict task reward.\nrunning the code:\nto run the code use the following steps:\ncreate a conda environment from the provided yml file and activate it\ninstalling mujoco will fail. see the repo for instructions: https://github.com/openai/mujoco-py\nusing this for your robot:\nto use this on your robot, here will be the process:\ncreate a new file this your robot_name.py (this is needed becuase the controller changes for each robot).\ncreate an environment config file in conf/envs/robot_name.yml with items like state dimension, control parameter dimension, and more for model training. also create or re-used a core conf file like reacher.yml in conf/.\ncreate or modify existing data generation and trajectory-based model training code. see create_dataset_traj( ) in multiple files for inspiration. the dimensions of this data must match the configuration.\nthe code should have two modes, train and collect. collect runs the model and train will load objects from dynamics_model.py to train and save your model, if you so choose.\nuse evaluate.py to view the model prediction accuracy.\ncore files for models and evaluation:\ndynamics_model.py: this class contains the modular class for dynamics models of the single step and trajectory parametrization. there is code to use neural networks and gaussian processes as the modelling -----> tool !!! .\npolicy.py: this file contains the different controller parametrizations used in the experiments.\nplot.py: this file stores all the plotting functions used by the other files.\nmbrl_resource: other functions used for iterative data collection.\nreplicating experiments:\nfor questions on configurations, see hyrda.\nlong-term prediction, section 5.2\nthis section has multiple files (reacher_pd.py, cartpole_lqr.py, crazyflie_pd.py, crazyflie_hardware.py) to collect data and train models, and a central file to evaluate results (evaluate.py). because of a slightly different space (using hardware), crazyflie_hardware.py evaulates results by running it with mode=eval. an important config item is data_dir as this is where data will be saved, models will be saved from, and evaluate.py will test from.\ncollect simulated data: python reacher_pd.py models=t envs=reacher mode=collect\ntrain models: python reacher_pd.py models=t envs=reacher mode=train or a sweep with multiple models python reacher_pd.py -m models=d,de,t,te envs=reacher mode=train\npredicting unstable and period data, section 5.3\nfor this experiment, procesd as above, but the data_dir needs to be changed in the cartpole configuration file. also, the data_mode in conf/envs/cartpole.yaml must be changed correspondingly. the three datasets to be used are:\nstable data: trajectories/cartpole/rawl200_t100_v4.dat\nunstable data: trajectories/cartpole/rawl200_t100_unstable.dat\nperiodic data:trajectories/cartpole/rawl200_t100_chaotic.dat these files can of course be recollected.\ndata efficiency, section 5.4\nexample of how to run efficiency code to train some models and then test them (this experiment is more computationally intensive):\ntrain: python3 efficiency.py training.num_traj=3,5,7,9 training.t_range=10,20,30,40 models=d,t training.copy=1,2,3,4,5 -m\ntest: python3 efficiency.py mode=plot plotting.num_traj=[3,5,7,9] plotting.t_range=[10,20,30,40] plotting.models=[d,t] plotting.copy=[1,2,3,4,5] -m\npredicting reward, section 5.5\nthis example uses the file reward_rank.py. to run this, run python reward_rank.py envs=cartpole. it is currently not supported for any other environments.\nextra files currently not in use:\nwhen examining the code, one will see a few extra files that represent potential future avenues for research. some of these files are:\nlorenz.py: this was an attempt to model the long term behavior of the lorenz system. results were mixed on this very challenging application.\nstable_system.py: this was used to evaluate how far into the future a trajectory-based model could predict a state-space system, but it was omitted from the paper.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7001004, "year": null}, {"Unnamed: 0": 2026, "autor": 1006, "date": null, "content": "ros-docker-images\nROS desktop-full docker images with Qt C++ debug tool-chain for Fedora and other Linux users who do not use Ubuntu.\nThis project aims to build an All-in-One development environment for robot learning including robotics and deep reinforcement learning and a portable platform for intelligent robot applications.\nFeatures\nintegrated with open source AMD and Intel GPU driver, and NVIDIA driver manually installation instruction\nsupport gazebo simulation\nincluding QtCreator and RoboWare as ROS IDE which supports build and debug your ROS packages\nincluding terminator, a multi-windows supported free terminal emulator\nincluding ranger - a console file manager with VI key bindings\nincluding ros-desktop-full packages, currently support indigo and kinetic version\nincluding a light weight file manager GUI application pcmanfm\nUsage\nPull docker image\n$ docker pull jacknlliu/ros:kinetic-ide-init\nSee it on docker hub.\nset X access on your host for docker containers\n$ xhost +\nRun\ndocker run --privileged \\\n--security-opt label=disable \\\n--security-opt seccomp=unconfined \\\n--env=\"DISPLAY\" --env QT_X11_NO_MITSHM=1 \\\n--volume=\"/tmp/.X11-unix:/tmp/.X11-unix:rw\" \\\n--volume=\"/home/<your_user_name>/Workspace:/data:rw\" \\\n--volume=\"/path/to/gazebo_models:/home/ros/.gazebo/models:rw\" \\\n--name=\"ros_kinetic\" \\\njacknlliu/ros:kinetic-ide-init terminator\nNOTE: /home/<your_user_name>/Workspace defines your workspace according to your own workspace to share between your host and docker container,\n/path/to/gazebo_models defines your gazebo models directory path.\nRun the above command just in the first time, later please just run the container with its name, this will keep things easy.\n$ docker start ros_kinetic\nTroubleshooting with NVIDIA video card\nIf you use NVIDIA video card, you should follow this instruction.\nRun ros docker images on multiple hosts, please refer to robot-manipulation-cluster.\nKnown Issues and Limitations\nIt's a little large image.\ncommon issues about NVIDIA GPU driver\nReference\nOSRF Docker Images\nPX4 Docker Containers\ndocker GUI tutorial - ROS wiki\nx11docker\nrobot-manipulation-cluster\nLICENSE\nThis project is distributed under MIT License.", "link": "https://github.com/jacknlliu/ros-docker-images", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "tool", "selectorShort": "tool", "MarkedSent": "ros-docker-images\nros desktop-full docker images with qt c++ debug -----> tool !!! -chain for fedora and other linux users who do not use ubuntu.\nthis project aims to build an all-in-one development environment for robot learning including robotics and deep reinforcement learning and a portable platform for intelligent robot applications.\nfeatures\nintegrated with open source amd and intel gpu driver, and nvidia driver manually installation instruction\nsupport gazebo simulation\nincluding qtcreator and roboware as ros ide which supports build and debug your ros packages\nincluding terminator, a multi-windows supported free terminal emulator\nincluding ranger - a console file manager with vi key bindings\nincluding ros-desktop-full packages, currently support indigo and kinetic version\nincluding a light weight file manager gui application pcmanfm\nusage\npull docker image\n$ docker pull jacknlliu/ros:kinetic-ide-init\nsee it on docker hub.\nset x access on your host for docker containers\n$ xhost +\nrun\ndocker run --privileged \\\n--security-opt label=disable \\\n--security-opt seccomp=unconfined \\\n--env=\"display\" --env qt_x11_no_mitshm=1 \\\n--volume=\"/tmp/.x11-unix:/tmp/.x11-unix:rw\" \\\n--volume=\"/home/<your_user_name>/workspace:/data:rw\" \\\n--volume=\"/path/to/gazebo_models:/home/ros/.gazebo/models:rw\" \\\n--name=\"ros_kinetic\" \\\njacknlliu/ros:kinetic-ide-init terminator\nnote: /home/<your_user_name>/workspace defines your workspace according to your own workspace to share between your host and docker container,\n/path/to/gazebo_models defines your gazebo models directory path.\nrun the above command just in the first time, later please just run the container with its name, this will keep things easy.\n$ docker start ros_kinetic\ntroubleshooting with nvidia video card\nif you use nvidia video card, you should follow this instruction.\nrun ros docker images on multiple hosts, please refer to robot-manipulation-cluster.\nknown issues and limitations\nit's a little large image.\ncommon issues about nvidia gpu driver\nreference\nosrf docker images\npx4 docker containers\ndocker gui tutorial - ros wiki\nx11docker\nrobot-manipulation-cluster\nlicense\nthis project is distributed under mit license.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7001006, "year": null}], "name": "toolrobotics"}
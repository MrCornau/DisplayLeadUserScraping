{"interestingcomments": [{"Unnamed: 0": 1037, "autor": 17, "date": null, "content": "Awesome Robotics\nThis is a list of various books, courses and other resources for robotics. It's an attempt to gather useful material in one place for everybody who wants to learn more about the field.\nCourses\nArtificial Intelligence for Robotics Udacity\nRobotics Nanodegree Udacity \ud83d\udcb5\nAutonomous Mobile Robots edX\nUnderactuated Robotics MIT CSAIL\nAutonomous Mobile Robots edX\nRobot Mechanics and Control, Part I edX\nRobot Mechanics and Control, Part II edX\nAutonomous Navigation for Flying Robots edX\nRobotics Specialization by GRASP Lab Coursera \ud83d\udcb5\nControl of Mobile Robots Coursera\nQUT Robot Academy QUT\nRobotic vision QUT\nIntroduction to robotics MIT\nRobotics: Vision Intelligence and Machine Learning edX\nApplied robot design Stanford University\nIntroduction to Robotics Stanford University\nIntroduction to Mobile Robotics University of Freiburg\nRobotics edx \ud83d\udcb5\nColumbia Robotics edx\nModern Robotics: Mechanics, Planning, and Control Coursera\nHello (Real) World with ROS \u2013 Robot Operating System edx\nAdvanced Robotics UCBerkeley\nBuilding Arduino robots and devices Coursera\nIntroduction to The Robot Operating System (ROS2) Coursera\nModern Robotics: Mechanics, Planning, and Control Specialization Coursera\nBecome a Robotics Software Enginee Udacity\nAdvanced Robotics UC Berkeley\nBooks\nProbabilistic Robotics (Intelligent Robotics and Autonomous Agents series) \ud83d\udcb5\nIntroduction to Autonomous Mobile Robots (Intelligent Robotics and Autonomous Agents series) \ud83d\udcb5\nSpringer Handbook of Robotics \ud83d\udcb5\nPlanning Algorithms\nA gentle introduction to ROS\nA Mathematical Introduction to Robotic Manipulation\nLearning Computing With Robots\nRobotics, Vision and Control: Fundamental Algorithms in MATLAB (Springer Tracts in Advanced Robotics) \ud83d\udcb5\nINTECH Books\nIntroduction to Autonomous Robots\nPrinciples of Robot Motion: Theory, Algorithms, and Implementations \ud83d\udcb5\nIntroduction to Modern Robotics: Mechanics, Planning, and Control [pdf]\nProgramming Robots with ROS: A Practical Introduction to the Robot Operating System \ud83d\udcb5\nLearning ROS for Robotics Programming \ud83d\udcb5\nMastering ROS for Robotics Programming \ud83d\udcb5\nBehavior Trees in Robotics and AI: An Introduction [pdf]\nAutomated Planning and Acting [pdf]\nSoftware and Libraries\nGazebo Robot Simulator\nROS The Robot Operating System (ROS) is a flexible framework for writing robot software. It is a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behavior across a wide variety of robotic platforms.\nROS2 ROS2 is a new version of ROS with radical design changes and improvement over older ROS version.\nRobWork RobWork is a collection of C++ libraries for simulation and control of robot systems. RobWork is used for research and education as well as for practical robot applications.\nMRPT Mobile Robot Programming Toolkit provides developers with portable and well-tested applications and libraries covering data structures and algorithms employed in common robotics research areas.\nRobotics Library The Robotics Library (RL) is a self-contained C++ library for robot kinematics, motion planning and control. It covers mathematics, kinematics and dynamics, hardware abstraction, motion planning, collision detection, and visualization.\nSimbad 2D/3D simulator in Java and Jython.\nMorse General purpose indoor/outdoor 3D simulator.\nCarmen CARMEN is an open-source collection of software for mobile robot control. CARMEN is modular software designed to provide basic navigation primitives including: base and sensor control, logging, obstacle avoidance, localization, path planning, and mapping.\nPeekabot Peekabot is a real-time, networked 3D visualization tool for robotics, written in C++. Its purpose is to simplify the visualization needs faced by a roboticist daily.\nYARP Yet Another Robot Platform.\nV-REP Robot simulator, 3D, source available, Lua scripting, APIs for C/C++, Python, Java, Matlab, URBI, 2 physics engines, full kinematic solver.\nWebots Webots is a development environment used to model, program and simulate mobile robots.\nDrake A planning, control and analysis toolbox for nonlinear dynamical systems.\nNeurorobotics Platform (NRP) An Internet-accessible simulation system that allows the simulation of robots controlled by spiking neural networks.\nThe Player Project Free Software tools for robot and sensor applications\nOpen AI's Roboschool Open-source software for robot simulation, integrated with OpenAI Gym.\nViSP Open-source visual servoing platform library, is able to compute control laws that can be applied to robotic systems.\nROS Behavior Trees Open-source library to create robot's behaviors in form of Behavior Trees running in ROS (Robot Operating System).\ng2core Open-source motion control software for CNC and Robotics, designed to run on Arduino Due class microcontrollers.\nur5controller Open-source OpenRAVE controller for UR5 robot integrated with ROS.\nRBDL Open-source (zlib) C++ libray for both forward and inverse dynamics and kinematics. Also supports contacts and loops.\nUnity Robotics Hub Central repository for open-source Unity packages, tutorials, and other resources demonstrating how to use Unity for robotics simulations. Includes new support for ROS integration.\nPapers\nOptimization Based Controller Design and Implementation for the Atlas Robot in the DARPA Robotics Challenge Finals\nConferences\nACM/IEEE International Conference on Human Robot Interaction (HRI)\nCISM IFToMM Symposium on Robot Design, Dynamics and Control (RoManSy)\nIEEE Conference on Decision and Controls (CDC)\nIEEE International Conference on Rehabilitation Robotics (ICORR)\nIEEE International Conference on Robotics and Automation (ICRA)\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\nIEEE-RAS International Conference on Humanoid Robots (Humanoids)\nInternational Symposium of Robotic Research (ISRR)\nInternational Symposium of Experimental Robotics (ISER)\nRobotica\nRobotics: Science and Systems Conference (RSS)\nThe International Workshop on the Algorithmic Foundations of Robotics (WAFR)\nJournals\nAutonomous Robots\nBioinspiration & Biomimetics\nFrontiers in Robotics and AI\nIEEE Robotics & Automation Magazine\nIEEE Transactions on Haptics\nIEEE Transactions on Robotics\nIEEE/ASME Transactions on Mechatronics\nInternational Journal of Social Robotics\nJournal of Field Robotics\nJournal of Intelligent & Robotic Systems\nMechatronics\nRobotics and Computer-Integrated Manufacturing\nRobotics and Autonomous Systems\nThe International Journal of Robotics Research\nCompetitions\nICRA Robot Challenges\nRobotChallenge\nDARPA Robotics Challenge\nEuropean Robotics Challenges\nFirst Robotics Competition\nVEX Robotics Competition\nRoboCup\nRoboCupJunior\nEurobot International Students Robotics Contest\nRoboMasters\nRoboSoft, Grand Challenge\nIntelligent Ground Vehicle Competition\nRobotex The biggest robotics festival in Europe\nFirst Lego League\nCompanies\nBoston Dynamics robotics R&D company, creator of the state of the art Atlas and Spot robots\niRobot manufacturer of the famous Roomba robotic vacuum cleaner\nPAL Robotics\nAldebaran Robotics creator of the NAO robot\nABB Robotics the largest manufacturer of industrial robots\nKUKA Robotics major manufacturer of industrial robots targeted at factory automation\nFANUC industrial robots manufacturer with the biggest install base\nRethink Robotics creator of the collaborative robot Baxter\nDJI industry leader in drones for both commerical and industrial needs.\nThe construct sim A cloud based tool for building modern, future-proof robot simulations.\nFetch Robotics A robotics startup in San Jose, CA building the future of e-commerce fulfillment and R&D robots.\nFesto Robotics Festo is known for making moving robots that move like animals such as the sea gull like SmartBird, jellyfish, butterflies and kangaroos.\nNeobotix manufacturer of industrial, research and as well as custom mobile robots.\nMisc\nIEEE Spectrum Robotics robotics section of the IEEE Spectrum magazine\nMIT Technology Review Robotics robotics section of the MIT Technology Review magazine\nreddit robotics subreddit\nRosCON conference (video talks included)\nCarnegie Mellon Robotics Academy\nLet's Make Robots\nHow do I learn Robotics?\nFree NXT Lego MindStorms NXT-G code tutorials\nStackExachange Robotics community\n47 Programmable robotic kits\nLinorobot A suite of DIY ROS compatible robots\nHexapod Robot Simulator - Solve and visualize hexapod robot inverse kinematics and gaits in the web\nPythonRobotics - Implementations of various robotics algorithms in python\nRelated awesome lists\nAwesome Artificial Intelligence\nAwesome Computer Vision\nAwesome Machine Learning\nAwesome Deep Learning\nAwesome Deep Vision\nAwesome Gazebo\nAwesome Reinforcement Learning\nAwesome Robotics\nAwesome Robotics Libraries\nAwesome ROS2\nAwesome RoboCupJunior Soccer", "link": "https://github.com/kiloreux/awesome-robotics", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "awesome robotics\nthis is a list of various books, courses and other resources for robotics. it's an attempt to gather useful material in one place for everybody who wants to learn more about the field.\ncourses\nartificial intelligence for robotics udacity\nrobotics nanodegree udacity \ud83d\udcb5\nautonomous mobile robots edx\nunderactuated robotics mit csail\nautonomous mobile robots edx\nrobot mechanics and control, part i edx\nrobot mechanics and control, part ii edx\nautonomous navigation for flying robots edx\nrobotics specialization by grasp lab coursera \ud83d\udcb5\ncontrol of mobile robots coursera\nqut robot academy qut\nrobotic vision qut\nintroduction to robotics mit\nrobotics: vision intelligence and machine learning edx\napplied robot design stanford university\nintroduction to robotics stanford university\nintroduction to mobile robotics university of freiburg\nrobotics edx \ud83d\udcb5\ncolumbia robotics edx\nmodern robotics: mechanics, planning, and control coursera\nhello (real) world with ros \u2013 robot operating system edx\nadvanced robotics ucberkeley\nbuilding arduino robots and devices coursera\nintroduction to the robot operating system (ros2) coursera\nmodern robotics: mechanics, planning, and control specialization coursera\nbecome a robotics software enginee udacity\nadvanced robotics uc berkeley\nbooks\nprobabilistic robotics (intelligent robotics and autonomous agents series) \ud83d\udcb5\nintroduction to autonomous mobile robots (intelligent robotics and autonomous agents series) \ud83d\udcb5\nspringer handbook of robotics \ud83d\udcb5\nplanning algorithms\na gentle introduction to ros\na mathematical introduction to robotic manipulation\nlearning computing with robots\nrobotics, vision and control: fundamental algorithms in matlab (springer tracts in advanced robotics) \ud83d\udcb5\nintech books\nintroduction to autonomous robots\nprinciples of robot motion: theory, algorithms, and implementations \ud83d\udcb5\nintroduction to modern robotics: mechanics, planning, and control [pdf]\nprogramming robots with ros: a practical introduction to the robot operating system \ud83d\udcb5\nlearning ros for robotics programming \ud83d\udcb5\nmastering ros for robotics programming \ud83d\udcb5\nbehavior trees in robotics and ai: an introduction [pdf]\nautomated planning and acting [pdf]\nsoftware and libraries\ngazebo robot simulator\nros the robot operating system (ros) is a flexible framework for writing robot software. it is a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behavior across a wide variety of robotic platforms.\nros2 ros2 is a new version of ros with radical design changes and improvement over older ros version.\nrobwork robwork is a collection of c++ libraries for simulation and control of robot systems. robwork is used for research and education as well as for practical robot applications.\nmrpt mobile robot programming toolkit provides developers with portable and well-tested applications and libraries covering data structures and algorithms employed in common robotics research areas.\nrobotics library the robotics library (rl) is a self-contained c++ library for robot kinematics, motion planning and control. it covers mathematics, kinematics and dynamics, hardware abstraction, motion planning, collision detection, and visualization.\nsimbad 2d/3d simulator in java and jython.\nmorse general purpose indoor/-----> outdoor !!!  3d simulator.\ncarmen carmen is an open-source collection of software for mobile robot control. carmen is modular software designed to provide basic navigation primitives including: base and sensor control, logging, obstacle avoidance, localization, path planning, and mapping.\npeekabot peekabot is a real-time, networked 3d visualization tool for robotics, written in c++. its purpose is to simplify the visualization needs faced by a roboticist daily.\nyarp yet another robot platform.\nv-rep robot simulator, 3d, source available, lua scripting, apis for c/c++, python, java, matlab, urbi, 2 physics engines, full kinematic solver.\nwebots webots is a development environment used to model, program and simulate mobile robots.\ndrake a planning, control and analysis toolbox for nonlinear dynamical systems.\nneurorobotics platform (nrp) an internet-accessible simulation system that allows the simulation of robots controlled by spiking neural networks.\nthe player project free software tools for robot and sensor applications\nopen ai's roboschool open-source software for robot simulation, integrated with openai gym.\nvisp open-source visual servoing platform library, is able to compute control laws that can be applied to robotic systems.\nros behavior trees open-source library to create robot's behaviors in form of behavior trees running in ros (robot operating system).\ng2core open-source motion control software for cnc and robotics, designed to run on arduino due class microcontrollers.\nur5controller open-source openrave controller for ur5 robot integrated with ros.\nrbdl open-source (zlib) c++ libray for both forward and inverse dynamics and kinematics. also supports contacts and loops.\nunity robotics hub central repository for open-source unity packages, tutorials, and other resources demonstrating how to use unity for robotics simulations. includes new support for ros integration.\npapers\noptimization based controller design and implementation for the atlas robot in the darpa robotics challenge finals\nconferences\nacm/ieee international conference on human robot interaction (hri)\ncism iftomm symposium on robot design, dynamics and control (romansy)\nieee conference on decision and controls (cdc)\nieee international conference on rehabilitation robotics (icorr)\nieee international conference on robotics and automation (icra)\nieee/rsj international conference on intelligent robots and systems (iros)\nieee-ras international conference on humanoid robots (humanoids)\ninternational symposium of robotic research (isrr)\ninternational symposium of experimental robotics (iser)\nrobotica\nrobotics: science and systems conference (rss)\nthe international workshop on the algorithmic foundations of robotics (wafr)\njournals\nautonomous robots\nbioinspiration & biomimetics\nfrontiers in robotics and ai\nieee robotics & automation magazine\nieee transactions on haptics\nieee transactions on robotics\nieee/asme transactions on mechatronics\ninternational journal of social robotics\njournal of field robotics\njournal of intelligent & robotic systems\nmechatronics\nrobotics and computer-integrated manufacturing\nrobotics and autonomous systems\nthe international journal of robotics research\ncompetitions\nicra robot challenges\nrobotchallenge\ndarpa robotics challenge\neuropean robotics challenges\nfirst robotics competition\nvex robotics competition\nrobocup\nrobocupjunior\neurobot international students robotics contest\nrobomasters\nrobosoft, grand challenge\nintelligent ground vehicle competition\nrobotex the biggest robotics festival in europe\nfirst lego league\ncompanies\nboston dynamics robotics r&d company, creator of the state of the art atlas and spot robots\nirobot manufacturer of the famous roomba robotic vacuum cleaner\npal robotics\naldebaran robotics creator of the nao robot\nabb robotics the largest manufacturer of industrial robots\nkuka robotics major manufacturer of industrial robots targeted at factory automation\nfanuc industrial robots manufacturer with the biggest install base\nrethink robotics creator of the collaborative robot baxter\ndji industry leader in drones for both commerical and industrial needs.\nthe construct sim a cloud based tool for building modern, future-proof robot simulations.\nfetch robotics a robotics startup in san jose, ca building the future of e-commerce fulfillment and r&d robots.\nfesto robotics festo is known for making moving robots that move like animals such as the sea gull like smartbird, jellyfish, butterflies and kangaroos.\nneobotix manufacturer of industrial, research and as well as custom mobile robots.\nmisc\nieee spectrum robotics robotics section of the ieee spectrum magazine\nmit technology review robotics robotics section of the mit technology review magazine\nreddit robotics subreddit\nroscon conference (video talks included)\ncarnegie mellon robotics academy\nlet's make robots\nhow do i learn robotics?\nfree nxt lego mindstorms nxt-g code tutorials\nstackexachange robotics community\n47 programmable robotic kits\nlinorobot a suite of diy ros compatible robots\nhexapod robot simulator - solve and visualize hexapod robot inverse kinematics and gaits in the web\npythonrobotics - implementations of various robotics algorithms in python\nrelated awesome lists\nawesome artificial intelligence\nawesome computer vision\nawesome machine learning\nawesome deep learning\nawesome deep vision\nawesome gazebo\nawesome reinforcement learning\nawesome robotics\nawesome robotics libraries\nawesome ros2\nawesome robocupjunior soccer", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000017, "year": null}, {"Unnamed: 0": 1057, "autor": 37, "date": null, "content": "Habitat-Sim\nA high-performance physics-enabled 3D simulator with support for:\n3D scans of indoor/outdoor spaces (with built-in support for HM3D, MatterPort3D, Gibson, Replica, and other datasets)\nCAD models of spaces and piecewise-rigid objects (e.g. ReplicaCAD, YCB, Google Scanned Objects),\nConfigurable sensors (RGB-D cameras, egomotion sensing)\nRobots described via URDF (mobile manipulators like Fetch, fixed-base arms like Franka,quadrupeds like AlienGo),\nRigid-body mechanics (via Bullet).\nThe design philosophy of Habitat is to prioritize simulation speed over the breadth of simulation capabilities. When rendering a scene from the Matterport3D dataset, Habitat-Sim achieves several thousand frames per second (FPS) running single-threaded and reaches over 10,000 FPS multi-process on a single GPU. Habitat-Sim simulates a Fetch robot interacting in ReplicaCAD scenes at over 8,000 steps per second (SPS), where each \u2018step\u2019 involves rendering 1 RGBD observation (128\u00d7128 pixels) and rigid-body dynamics for 1/30sec.\nHabitat-Sim is typically used with Habitat-Lab, a modular high-level library for end-to-end experiments in embodied AI -- defining embodied AI tasks (e.g. navigation, instruction following, question answering), training agents (via imitation or reinforcement learning, or no learning at all as in classical SensePlanAct pipelines), and benchmarking their performance on the defined tasks using standard metrics.\nhabitat2_small.mp4\nTable of contents\nCiting Habitat\nInstallation\nTesting\nDocumentation\nDatasets\nExternal Contributions\nLicense\nCiting Habitat\nIf you use the Habitat platform in your research, please cite the Habitat and Habitat 2.0 papers:\n@inproceedings{szot2021habitat,\ntitle = {Habitat 2.0: Training Home Assistants to Rearrange their Habitat},\nauthor = {Andrew Szot and Alex Clegg and Eric Undersander and Erik Wijmans and Yili Zhao and John Turner and Noah Maestre and Mustafa Mukadam and Devendra Chaplot and Oleksandr Maksymets and Aaron Gokaslan and Vladimir Vondrus and Sameer Dharur and Franziska Meier and Wojciech Galuba and Angel Chang and Zsolt Kira and Vladlen Koltun and Jitendra Malik and Manolis Savva and Dhruv Batra},\nbooktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\nyear = {2021}\n}\n@inproceedings{habitat19iccv,\ntitle = {Habitat: {A} {P}latform for {E}mbodied {AI} {R}esearch},\nauthor = {Manolis Savva and Abhishek Kadian and Oleksandr Maksymets and Yili Zhao and Erik Wijmans and Bhavana Jain and Julian Straub and Jia Liu and Vladlen Koltun and Jitendra Malik and Devi Parikh and Dhruv Batra},\nbooktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},\nyear = {2019}\n}\nHabitat-Sim also builds on work contributed by others. If you use contributed methods/models, please cite their works. See the External Contributions section for a list of what was externally contributed and the corresponding work/citation.\nInstallation\nHabitat-Sim can be installed in 3 ways:\nVia Conda - Recommended method for most users. Stable release and nightly builds.\n[Experimental] Via PIP - pip install . to compile the latest headless build with Bullet. Read build instructions and common build issues.\nVia Docker - Updated approximately once per year for Habitat Challenge. Read habitat-docker-setup.\nVia Source - For active development. Read build instructions and common build issues.\n[Recommended] Conda Packages\nHabitat is under active development, and we advise users to restrict themselves to stable releases. Starting with v0.1.4, we provide conda packages for each release.\nPreparing conda env\nAssuming you have conda installed, let's prepare a conda env:\n# We require python>=3.6 and cmake>=3.10\nconda create -n habitat python=3.6 cmake=3.14.0\nconda activate habitat\nconda install habitat-sim\nPick one of the options below depending on your system/needs:\nTo install on machines with an attached display:\nconda install habitat-sim -c conda-forge -c aihabitat\nTo install on headless machines (i.e. without an attached display, e.g. in a cluster) and machines with multiple GPUs:\nconda install habitat-sim headless -c conda-forge -c aihabitat\n[Most common scenario] To install habitat-sim with bullet physics\nconda install habitat-sim withbullet -c conda-forge -c aihabitat\nNote: Build parameters can be chained together. For instance, to install habitat-sim with physics on headless machines:\nconda install habitat-sim withbullet headless -c conda-forge -c aihabitat\nConda packages for older versions can installed by explicitly specifying the version, e.g. conda install habitat-sim=0.1.6 -c conda-forge -c aihabitat.\nWe also provide a nightly conda build for the main branch. However, this should only be used if you need a specific feature not yet in the latest release version. To get the nightly build of the latest main, simply swap -c aihabitat for -c aihabitat-nightly.\nTesting\nLet's download some 3D assets using our python data download utility:\nDownload (testing) 3D scenes\npython -m habitat_sim.utils.datasets_download --uids habitat_test_scenes --data-path /path/to/data/\nNote that these testing scenes do not provide semantic annotations. If you would like to test the semantic sensors via example.py, please use the data from the Matterport3D dataset (see Datasets).\nDownload example objects\npython -m habitat_sim.utils.datasets_download --uids habitat_example_objects --data-path /path/to/data/\nInteractive testing: Use the interactive viewer included with Habitat-Sim\n# ./build/viewer if compiling locally\nhabitat-viewer /path/to/data/scene_datasets/habitat-test-scenes/skokloster-castle.glb\nYou should be able to control an agent in this test scene. Use W/A/S/D keys to move forward/left/backward/right and arrow keys or mouse to control gaze direction (look up/down/left/right). Try to find the picture of a woman surrounded by a wreath. Have fun!\nPhysical interactions: Use the interactive viewer with physics enabled:\n# ./build/viewer if compiling locally\nhabitat-viewer --enable-physics --object-dir data/objects/example_objects -- data/scene_datasets/habitat-test-scenes/apartment_1.glb\nYou should be able to insert objects into the scene by pressing 'o'. The viewer application outputs the full list of keyboard shortcuts to the console at runtime.\nNon-interactive testing: Run the example script:\npython /path/to/habitat-sim/examples/example.py --scene /path/to/data/scene_datasets/habitat-test-scenes/skokloster-castle.glb\nThe agent will traverse a particular path and you should see the performance stats at the very end, something like this: 640 x 480, total time: 3.208 sec. FPS: 311.7.\nTo reproduce the benchmark table from Habitat ICCV'19 run examples/benchmark.py --scene /path/to/mp3d_example/17DRP5sb8fy/17DRP5sb8fy.glb.\nAdditional arguments to example.py are provided to change the sensor configuration, print statistics of the semantic annotations in a scene, compute action-space shortest path trajectories, and set other useful functionality. Refer to the example.py and demo_runner.py source files for an overview.\nLoad a specific MP3D or Gibson house: examples/example.py --scene path/to/mp3d/house_id.glb.\nWe have also provided an example demo for reference.\nTo run a physics example in python (after building with \"Physics simulation via Bullet\"):\npython examples/example.py --scene /path/to/data/scene_datasets/habitat-test-scenes/skokloster-castle.glb --enable_physics\nNote that in this mode the agent will be frozen and oriented toward the spawned physical objects. Additionally, --save_png can be used to output agent visual observation frames of the physical scene to the current directory.\nCommon testing issues\nIf you are running on a remote machine and experience display errors when initializing the simulator, e.g.\nX11: The DISPLAY environment variable is missing\nCould not initialize GLFW\nensure you do not have DISPLAY defined in your environment (run unset DISPLAY to undefine the variable)\nIf you see libGL errors like:\nX11: The DISPLAY environment variable is missing\nCould not initialize GLFW\nchances are your libGL is located at a non-standard location. See e.g. this issue.\nDocumentation\nBrowse the online Habitat-Sim documentation.\nTo get you started, see the Lighting Setup tutorial for adding new objects to existing scenes and relighting the scene & objects. The Image Extractor tutorial shows how to get images from scenes loaded in Habitat-Sim.\nQuestions?\nDatasets\nCommon datasets used with Habitat.\nExternal Contributions\nIf you use the noise model from PyRobot, please cite the their technical report.\nSpecifically, the noise model used for the noisy control functions named pyrobot_* and defined in src_python/habitat_sim/agent/controls/pyrobot_noisy_controls.py\nIf you use the Redwood Depth Noise Model, please cite their paper\nSpecifically, the noise model defined in src_python/habitat_sim/sensors/noise_models/redwood_depth_noise_model.py and src/esp/sensor/RedwoodNoiseModel.*\nLicense\nHabitat-Sim is MIT licensed. See the LICENSE for details.\nThe WebGL demo and demo scripts use:\nThe King\u00b4s Hall by Skokloster Castle (Skoklosters slott) licensed under Creative Commons Attribution\nVan Gogh Room by ruslans3d licensed under Creative Commons Attribution", "link": "https://github.com/facebookresearch/habitat-sim", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "habitat-sim\na high-performance physics-enabled 3d simulator with support for:\n3d scans of indoor/-----> outdoor !!!  spaces (with built-in support for hm3d, matterport3d, gibson, replica, and other datasets)\ncad models of spaces and piecewise-rigid objects (e.g. replicacad, ycb, google scanned objects),\nconfigurable sensors (rgb-d cameras, egomotion sensing)\nrobots described via urdf (mobile manipulators like fetch, fixed-base arms like franka,quadrupeds like aliengo),\nrigid-body mechanics (via bullet).\nthe design philosophy of habitat is to prioritize simulation speed over the breadth of simulation capabilities. when rendering a scene from the matterport3d dataset, habitat-sim achieves several thousand frames per second (fps) running single-threaded and reaches over 10,000 fps multi-process on a single gpu. habitat-sim simulates a fetch robot interacting in replicacad scenes at over 8,000 steps per second (sps), where each \u2018step\u2019 involves rendering 1 rgbd observation (128\u00d7128 pixels) and rigid-body dynamics for 1/30sec.\nhabitat-sim is typically used with habitat-lab, a modular high-level library for end-to-end experiments in embodied ai -- defining embodied ai tasks (e.g. navigation, instruction following, question answering), training agents (via imitation or reinforcement learning, or no learning at all as in classical senseplanact pipelines), and benchmarking their performance on the defined tasks using standard metrics.\nhabitat2_small.mp4\ntable of contents\nciting habitat\ninstallation\ntesting\ndocumentation\ndatasets\nexternal contributions\nlicense\nciting habitat\nif you use the habitat platform in your research, please cite the habitat and habitat 2.0 papers:\n@inproceedings{szot2021habitat,\ntitle = {habitat 2.0: training home assistants to rearrange their habitat},\nauthor = {andrew szot and alex clegg and eric undersander and erik wijmans and yili zhao and john turner and noah maestre and mustafa mukadam and devendra chaplot and oleksandr maksymets and aaron gokaslan and vladimir vondrus and sameer dharur and franziska meier and wojciech galuba and angel chang and zsolt kira and vladlen koltun and jitendra malik and manolis savva and dhruv batra},\nbooktitle = {advances in neural information processing systems (neurips)},\nyear = {2021}\n}\n@inproceedings{habitat19iccv,\ntitle = {habitat: {a} {p}latform for {e}mbodied {ai} {r}esearch},\nauthor = {manolis savva and abhishek kadian and oleksandr maksymets and yili zhao and erik wijmans and bhavana jain and julian straub and jia liu and vladlen koltun and jitendra malik and devi parikh and dhruv batra},\nbooktitle = {proceedings of the ieee/cvf international conference on computer vision (iccv)},\nyear = {2019}\n}\nhabitat-sim also builds on work contributed by others. if you use contributed methods/models, please cite their works. see the external contributions section for a list of what was externally contributed and the corresponding work/citation.\ninstallation\nhabitat-sim can be installed in 3 ways:\nvia conda - recommended method for most users. stable release and nightly builds.\n[experimental] via pip - pip install . to compile the latest headless build with bullet. read build instructions and common build issues.\nvia docker - updated approximately once per year for habitat challenge. read habitat-docker-setup.\nvia source - for active development. read build instructions and common build issues.\n[recommended] conda packages\nhabitat is under active development, and we advise users to restrict themselves to stable releases. starting with v0.1.4, we provide conda packages for each release.\npreparing conda env\nassuming you have conda installed, let's prepare a conda env:\n# we require python>=3.6 and cmake>=3.10\nconda create -n habitat python=3.6 cmake=3.14.0\nconda activate habitat\nconda install habitat-sim\npick one of the options below depending on your system/needs:\nto install on machines with an attached display:\nconda install habitat-sim -c conda-forge -c aihabitat\nto install on headless machines (i.e. without an attached display, e.g. in a cluster) and machines with multiple gpus:\nconda install habitat-sim headless -c conda-forge -c aihabitat\n[most common scenario] to install habitat-sim with bullet physics\nconda install habitat-sim withbullet -c conda-forge -c aihabitat\nnote: build parameters can be chained together. for instance, to install habitat-sim with physics on headless machines:\nconda install habitat-sim withbullet headless -c conda-forge -c aihabitat\nconda packages for older versions can installed by explicitly specifying the version, e.g. conda install habitat-sim=0.1.6 -c conda-forge -c aihabitat.\nwe also provide a nightly conda build for the main branch. however, this should only be used if you need a specific feature not yet in the latest release version. to get the nightly build of the latest main, simply swap -c aihabitat for -c aihabitat-nightly.\ntesting\nlet's download some 3d assets using our python data download utility:\ndownload (testing) 3d scenes\npython -m habitat_sim.utils.datasets_download --uids habitat_test_scenes --data-path /path/to/data/\nnote that these testing scenes do not provide semantic annotations. if you would like to test the semantic sensors via example.py, please use the data from the matterport3d dataset (see datasets).\ndownload example objects\npython -m habitat_sim.utils.datasets_download --uids habitat_example_objects --data-path /path/to/data/\ninteractive testing: use the interactive viewer included with habitat-sim\n# ./build/viewer if compiling locally\nhabitat-viewer /path/to/data/scene_datasets/habitat-test-scenes/skokloster-castle.glb\nyou should be able to control an agent in this test scene. use w/a/s/d keys to move forward/left/backward/right and arrow keys or mouse to control gaze direction (look up/down/left/right). try to find the picture of a woman surrounded by a wreath. have fun!\nphysical interactions: use the interactive viewer with physics enabled:\n# ./build/viewer if compiling locally\nhabitat-viewer --enable-physics --object-dir data/objects/example_objects -- data/scene_datasets/habitat-test-scenes/apartment_1.glb\nyou should be able to insert objects into the scene by pressing 'o'. the viewer application outputs the full list of keyboard shortcuts to the console at runtime.\nnon-interactive testing: run the example script:\npython /path/to/habitat-sim/examples/example.py --scene /path/to/data/scene_datasets/habitat-test-scenes/skokloster-castle.glb\nthe agent will traverse a particular path and you should see the performance stats at the very end, something like this: 640 x 480, total time: 3.208 sec. fps: 311.7.\nto reproduce the benchmark table from habitat iccv'19 run examples/benchmark.py --scene /path/to/mp3d_example/17drp5sb8fy/17drp5sb8fy.glb.\nadditional arguments to example.py are provided to change the sensor configuration, print statistics of the semantic annotations in a scene, compute action-space shortest path trajectories, and set other useful functionality. refer to the example.py and demo_runner.py source files for an overview.\nload a specific mp3d or gibson house: examples/example.py --scene path/to/mp3d/house_id.glb.\nwe have also provided an example demo for reference.\nto run a physics example in python (after building with \"physics simulation via bullet\"):\npython examples/example.py --scene /path/to/data/scene_datasets/habitat-test-scenes/skokloster-castle.glb --enable_physics\nnote that in this mode the agent will be frozen and oriented toward the spawned physical objects. additionally, --save_png can be used to output agent visual observation frames of the physical scene to the current directory.\ncommon testing issues\nif you are running on a remote machine and experience display errors when initializing the simulator, e.g.\nx11: the display environment variable is missing\ncould not initialize glfw\nensure you do not have display defined in your environment (run unset display to undefine the variable)\nif you see libgl errors like:\nx11: the display environment variable is missing\ncould not initialize glfw\nchances are your libgl is located at a non-standard location. see e.g. this issue.\ndocumentation\nbrowse the online habitat-sim documentation.\nto get you started, see the lighting setup tutorial for adding new objects to existing scenes and relighting the scene & objects. the image extractor tutorial shows how to get images from scenes loaded in habitat-sim.\nquestions?\ndatasets\ncommon datasets used with habitat.\nexternal contributions\nif you use the noise model from pyrobot, please cite the their technical report.\nspecifically, the noise model used for the noisy control functions named pyrobot_* and defined in src_python/habitat_sim/agent/controls/pyrobot_noisy_controls.py\nif you use the redwood depth noise model, please cite their paper\nspecifically, the noise model defined in src_python/habitat_sim/sensors/noise_models/redwood_depth_noise_model.py and src/esp/sensor/redwoodnoisemodel.*\nlicense\nhabitat-sim is mit licensed. see the license for details.\nthe webgl demo and demo scripts use:\nthe king\u00b4s hall by skokloster castle (skoklosters slott) licensed under creative commons attribution\nvan gogh room by ruslans3d licensed under creative commons attribution", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000037, "year": null}, {"Unnamed: 0": 1062, "autor": 42, "date": null, "content": "Documentation and Tutorials\nlibpointmatcher is a modular library implementing the Iterative Closest Point (ICP) algorithm for aligning point clouds. It has applications in robotics and computer vision. The library is written in C++ for effeciency with bindings in Python.\nQuick link for the tutorial pages: Tutorials (also available on readthedocs.org).\nThose tutorials are written using Markdown syntax and stored in the project's /doc folder. Their scope ranges from introductory material on performing point cloud registration to instructions for the more experienced developer on how to extend the library's codebase.\nLibpointmatcher's source code is fully documented based on doxygen to provide an easy API to developers. An example of this API can be found here, but it is suggested to use the one build for your version in doc/html.\nlibpointmatcher is being developed by Fran\u00e7ois Pomerleau and St\u00e9phane Magnenat as part of our work at ASL-ETH.\nYou can read the latest changes in the release notes.\nQuick Start\nAlthough we suggest to use the tutorials, here is a quick version of it:\nThe library has a light dependency list:\nEigen version 3, a modern C++ matrix and linear-algebra library,\nboost version 1.48 and up, portable C++ source libraries,\nlibnabo version 1.0.7, a fast K Nearest Neighbour library for low-dimensional spaces,\nand was compiled on:\nUbuntu (see how)\nMac OS X (see how)\nWindows (see how - partially supported)\nCompilation & Installation\nFor beginner users who are not familiar with compiling and installing a library in Linux, go here for detailed instructions on how to compile libpointmatcher from the source code. If you are comfortable with Linux and CMake and have already installed the prerequisites above, the following commands should install libpointmatcher on your system.\nmkdir build && cd build\ncmake ..\nmake\nsudo make install\nTesting\nLibpointmatcher ships with a version of the Google testing framework GTest. Unit tests are located in the utest/ directory and are compiled with libpointmatcher (CMake variable BUILD_TESTS must be set to TRUE before compiling). To run the tests and make sure that your compiled version is working correctly, run the test executable in your build directory:\ncd build\nutest/utest --path ../examples/data/\nLinking to external projects.\nWe mainly develop for cmake projects and we provide example files under examples/demo_cmake/ to help you in your own project. We also provide a QT Creator example in examples/demo_QT/, which manually list all the dependencies in the file demo.pro. You would need to ajust those paths to point at the appropriate locations on your system. For a more detailled procedure, check the Linking Projects to libpointmatcher section.\nBug reporting\nPlease use our github's issue tracker to report bugs. If you are running the library on Ubuntu, copy-paste the output of the script listVersionsUbuntu.sh to simplify the search of an answer.\nFile formats\nThe library support different file formats for importing or exporting data:\ncsv (Comma Separated Values)\nvtk (Visualization Toolkit Files)\nply (Polygon File Format)\npcd (Point Cloud Library Format)\nThose functionnalities are available without increasing the list of dependencies at the expense of a limited functionality support. For more details, see the tutorial Importing and Exporting Point Clouds. Example executables using those file formats from the command line can be found in the /examples directory and are described here in more details.\nCiting\nIf you use libpointmatcher in an academic context, please cite the following publication:\n@article{Pomerleau12comp,\nauthor = {Pomerleau, Fran{\\c c}ois and Colas, Francis and Siegwart, Roland and Magnenat, St{\\'e}phane},\ntitle = {{Comparing ICP Variants on Real-World Data Sets}},\njournal = {Autonomous Robots},\nyear = {2013},\nvolume = {34},\nnumber = {3},\npages = {133--148},\nmonth = feb\n}\nand/or\n@INPROCEEDINGS{pomerleau11tracking,\nauthor = {Fran{\\c c}ois Pomerleau and St{\\'e}phane Magnenat and Francis Colas and Ming Liu and Roland Siegwart},\ntitle = {Tracking a Depth Camera: Parameter Exploration for Fast ICP},\nbooktitle = {Proc. of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\npublisher = {IEEE Press},\npages = {3824--3829},\nyear = {2011}\n}\nExtra Reading\nIf you are interested in learning more about different registration algorithms, we recently put together a literature review surveying multiple solutions. The review is organized in the same way as the library and many examples are provided based on real deployments.\nF. Pomerleau, F. Colas and R. Siegwart (2015), \"A Review of Point Cloud Registration Algorithms for Mobile Robotics\", Foundations and Trends\u00ae in Robotics: Vol. 4: No. 1, pp 1-104. https://doi.org/10.1561/2300000035\nIf you don't have access to the journal, you can download it from here.\nMore Point Clouds\nWe also produced those freely available data sets to test different registration solutions:\nChallenging data sets for point cloud registration algorithms\nYou can download the files in CSV or VTK formats, which are directly supported by the library I/O module.\nProjects and Partners\nIf you are using libpointmatcher in your project and you would like to have it listed here, please contact Fran\u00e7ois Pomerleau.\nEuropean Project NIFTi (FP7 ICT-247870): Search and rescue project in dynamic environments. Results: video of multi-floor reconstruction and video of railyard reconstruction. All results with real-time computation.\nNASA Ames Stereo Pipeline: Planetary reconstruction from satellite observations. Results: used for Mars, Moon and Earth point clouds.\nArmasuisse S+T UGV research program ARTOR: Development of techniques for reliable autonomous navigation of a wheeled robot in rough, outdoor terrain. Results: video of urban and dynamic 3D reconstruction and video of open space 3D reconstruction with real-time computation.\nSwiss National Science Foundation - Limnobotics: Robotic solution for toxic algae monitoring in lacs. Result: video of 3D shore reconstruction with real-time computation.\nCGAL includes our library for their registration pipeline.\nNorlab is maintaining and using the library for their research on autonomous navigation in harsh environments.\nANYbotics AG is investigating autonomous navigation algorithms using this library.\nFor a larger list of work realized with libpointmatcher, please see the page Applications And Publications.\nLicense\nlibpointmatcher is released under a permissive BSD license. Enjoy!", "link": "https://github.com/ethz-asl/libpointmatcher", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "documentation and tutorials\nlibpointmatcher is a modular library implementing the iterative closest point (icp) algorithm for aligning point clouds. it has applications in robotics and computer vision. the library is written in c++ for effeciency with bindings in python.\nquick link for the tutorial pages: tutorials (also available on readthedocs.org).\nthose tutorials are written using markdown syntax and stored in the project's /doc folder. their scope ranges from introductory material on performing point cloud registration to instructions for the more experienced developer on how to extend the library's codebase.\nlibpointmatcher's source code is fully documented based on doxygen to provide an easy api to developers. an example of this api can be found here, but it is suggested to use the one build for your version in doc/html.\nlibpointmatcher is being developed by fran\u00e7ois pomerleau and st\u00e9phane magnenat as part of our work at asl-eth.\nyou can read the latest changes in the release notes.\nquick start\nalthough we suggest to use the tutorials, here is a quick version of it:\nthe library has a light dependency list:\neigen version 3, a modern c++ matrix and linear-algebra library,\nboost version 1.48 and up, portable c++ source libraries,\nlibnabo version 1.0.7, a fast k nearest neighbour library for low-dimensional spaces,\nand was compiled on:\nubuntu (see how)\nmac os x (see how)\nwindows (see how - partially supported)\ncompilation & installation\nfor beginner users who are not familiar with compiling and installing a library in linux, go here for detailed instructions on how to compile libpointmatcher from the source code. if you are comfortable with linux and cmake and have already installed the prerequisites above, the following commands should install libpointmatcher on your system.\nmkdir build && cd build\ncmake ..\nmake\nsudo make install\ntesting\nlibpointmatcher ships with a version of the google testing framework gtest. unit tests are located in the utest/ directory and are compiled with libpointmatcher (cmake variable build_tests must be set to true before compiling). to run the tests and make sure that your compiled version is working correctly, run the test executable in your build directory:\ncd build\nutest/utest --path ../examples/data/\nlinking to external projects.\nwe mainly develop for cmake projects and we provide example files under examples/demo_cmake/ to help you in your own project. we also provide a qt creator example in examples/demo_qt/, which manually list all the dependencies in the file demo.pro. you would need to ajust those paths to point at the appropriate locations on your system. for a more detailled procedure, check the linking projects to libpointmatcher section.\nbug reporting\nplease use our github's issue tracker to report bugs. if you are running the library on ubuntu, copy-paste the output of the script listversionsubuntu.sh to simplify the search of an answer.\nfile formats\nthe library support different file formats for importing or exporting data:\ncsv (comma separated values)\nvtk (visualization toolkit files)\nply (polygon file format)\npcd (point cloud library format)\nthose functionnalities are available without increasing the list of dependencies at the expense of a limited functionality support. for more details, see the tutorial importing and exporting point clouds. example executables using those file formats from the command line can be found in the /examples directory and are described here in more details.\nciting\nif you use libpointmatcher in an academic context, please cite the following publication:\n@article{pomerleau12comp,\nauthor = {pomerleau, fran{\\c c}ois and colas, francis and siegwart, roland and magnenat, st{\\'e}phane},\ntitle = {{comparing icp variants on real-world data sets}},\njournal = {autonomous robots},\nyear = {2013},\nvolume = {34},\nnumber = {3},\npages = {133--148},\nmonth = feb\n}\nand/or\n@inproceedings{pomerleau11tracking,\nauthor = {fran{\\c c}ois pomerleau and st{\\'e}phane magnenat and francis colas and ming liu and roland siegwart},\ntitle = {tracking a depth camera: parameter exploration for fast icp},\nbooktitle = {proc. of the ieee/rsj international conference on intelligent robots and systems (iros)},\npublisher = {ieee press},\npages = {3824--3829},\nyear = {2011}\n}\nextra reading\nif you are interested in learning more about different registration algorithms, we recently put together a literature review surveying multiple solutions. the review is organized in the same way as the library and many examples are provided based on real deployments.\nf. pomerleau, f. colas and r. siegwart (2015), \"a review of point cloud registration algorithms for mobile robotics\", foundations and trends\u00ae in robotics: vol. 4: no. 1, pp 1-104. https://doi.org/10.1561/2300000035\nif you don't have access to the journal, you can download it from here.\nmore point clouds\nwe also produced those freely available data sets to test different registration solutions:\nchallenging data sets for point cloud registration algorithms\nyou can download the files in csv or vtk formats, which are directly supported by the library i/o module.\nprojects and partners\nif you are using libpointmatcher in your project and you would like to have it listed here, please contact fran\u00e7ois pomerleau.\neuropean project nifti (fp7 ict-247870): search and rescue project in dynamic environments. results: video of multi-floor reconstruction and video of railyard reconstruction. all results with real-time computation.\nnasa ames stereo pipeline: planetary reconstruction from satellite observations. results: used for mars, moon and earth point clouds.\narmasuisse s+t ugv research program artor: development of techniques for reliable autonomous navigation of a wheeled robot in rough, -----> outdoor !!!  terrain. results: video of urban and dynamic 3d reconstruction and video of open space 3d reconstruction with real-time computation.\nswiss national science foundation - limnobotics: robotic solution for toxic algae monitoring in lacs. result: video of 3d shore reconstruction with real-time computation.\ncgal includes our library for their registration pipeline.\nnorlab is maintaining and using the library for their research on autonomous navigation in harsh environments.\nanybotics ag is investigating autonomous navigation algorithms using this library.\nfor a larger list of work realized with libpointmatcher, please see the page applications and publications.\nlicense\nlibpointmatcher is released under a permissive bsd license. enjoy!", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000042, "year": null}, {"Unnamed: 0": 1257, "autor": 237, "date": null, "content": "TOC {:toc}\nDataset Collections\nRobotics\nRadish: The Robotics Data Set Repository, Andrew Howard and Nicholas Roy (Not working)\nRepository of Robotics and Computer Vision Datasets, MRPT\n\ud83d\udcdd It includes Malaga datasets and some of classic datasets published in Radish.\nIJRR Data Papers, IJRR\nAwesome SLAM Datasets, Younggun Cho \ud83d\udc4d\nComputer Vision\nCVonline Image Databases, CVonline\nComputer Vision Datasets on the Web, CVPapers \ud83d\udc4d\nYACVID: Yet Another Computer Vision Index To Datasets, Hayko Riemenschneider \ud83d\udc4d\nComputer Vision Online Datasets, Computer Vision Online\nOthers\nMachine Learning Repository, UCI\nKaggle Datasets, Kaggle\nIEEE DataPort, IEEE\nPlace-specific Datasets\nDriving Datasets\nKITTI Vision Benchmark Suite and KITTI-360, Andreas Geiger et al. \ud83d\udc4d\nSemanticKITTI, Jens Behley et al.\nWaymo Open Dataset, Waymo\nCityscapes Dataset\nAppoloScape Dataset\nBerkely DeepDrive Dataset (BDD100K), BAIR at UC Berkely\nnuScenes Dataset, APTIV\n$D^2$-City Dataset, DiDi\nFord Campus Vision and Lidar Data Set, PeRL at Univ. of Michigan\nMIT DARPA Urban Challenge Dataset, MIT\nKAIST Multi-spectral Recognition Dataset in Day and Night, RCV Lab at KAIST\nKAIST Complex Urban Dataset, IRAP Lab at KAIST\nNew College Dataset, MRG at Oxford Univ.\nChinese Driving from a Bike View (CDBV), CAS\nCULane Dataset, CUHK\nROMA (ROad MArkings) Image Database, Jean-Philippe Tarel et al.\nFlying Datasets\nThe Zurich Urban Micro Aerial Vehicle Dataset, RPG at ETHZ\nThe UZH-FPV Drone Racing Dataset, RPG at ETHZ\nMultiDrone Public Dataset, MultiDrone Project\nThe Blackbird Dataset, AgileDrones Group at MIT\nUnderwater Datasets\nMarine Robotics Datasets, ACFR\nOutdoor Datasets\nThe Rawseeds Project\n\ud83d\udcdd It includes Bovisa dataset is for outdoor and Bicocca dataset is for indoor.\nPlanetary Mapping and Navigation Datasets, ASRL at Univ. of Toronto\nIndoor Datasets\nRobotics 2D-Laser Datasets, Cyrill Stachniss\n\ud83d\udcdd It includes some of classic datasets published in Radish.\nLong-Term Mobile Robot Operations, Lincoln Univ.\nMIT Stata Center Data Set, Marine Robotics Group at MIT\nKTH and COLD Database, Andrzej Pronobis\nShopping Mall Datasets, IRC at ATR\nRGB-D Dataset 7-Scenes, Microsoft\nTopic-specific Datasets for Robotics\nLocalization, Mapping, and SLAM\nSLAM Benchmarking, AIS at Univ. of Freiburg\nRobotic 3D Scan Repository, Univ. of Wurzburg and Univ. of Osnabruck\n3D Pose Graph Optimization, Luca Carlone\nLandmark-based Localization\nRange-only Data for Localization, CMU RI\nRoh's Angulation Dataset, HyunChul Roh\nWireless Sensor Network Dataset, Kamin Whitehouse\nPath Planning and Navigation\nPathfinding Benchmarks, Moving AI Lab at Univ. of Denver\nTask and Motion Planner Benchmarking, RSS 2018 Workshop\nTopic-specific Datasets for Computer Vision\nFeatures\nAffine Covariant Features Datasets, VGG at Oxford\nRepeatability Benchmark Tutorial, VLFeat\nA list of feature performance evaluation datasets, maintained by openMVG\nSaliency and Foreground\nSaliency\nMIT Saliency Benchmark, MIT\nSalient Object Detection: A Benchmark, Ming-Ming Cheng\nForeground/Change Detection (Background Subtraction)\nChangeDetection.NET (a.k.a. CDNET)\nMotion and Pose Estimation\nAdelaideRMF: Robust Model Fitting Data Set, Hoi Sim Wong\nStructure-from-Motion and 3D Reconstruction\nObjects\nIVL-SYNTHESFM v2, Davide Marelli et al.\nFuji-SfM Dataset, Jordi Gene-Mola et al.\nLarge Geometric Models Archive, Georgia Tech\nThe Stanford 3D Scanning Repository, Stanford Univ.\nPlaces\nPhoto Tourism Data, UW and Microsoft\nObject Tracking\nVisual Object Tracking Challenge (a.k.a. VOT) \ud83d\udc4d\nVisual Tracker Benchmark (a.k.a. OTB)\nObject, Place, and Event Recognition\nPedestrians\nEuroCity Persons Dataset (a.k.a. ECP)\nDaimler Pedestrian Benchmark Data Sets\nCrowdHuman\nObjects\nRGB-D Object Dataset, UW\nSweet Pepper and Peduncle 3D Datasets, InKyu Sa\nPlaces\nLoop Closure Detection, David Filliat et. al.\nTraffic and Surveillance\nBEST: Benchmark and Evaluation of Surveillance Task, SJTU\nVIRAT Video Dataset\nResearch Groups\nTUM CVG Datasets\nTags: Visual(-inertia) odometry, visual SLAM, 3D reconstruction\nOxford VGG Datasets\nTags: Visual features, visual recognition, 3D reconstruction\nQUT CyPhy Datasets\nTags: Visual SLAM, LiDAR SLAM\nUniv. of Bonn Univ. Stachniss Lab Datasets\nTags: SLAM\nEPFL CVLAB Datasets\nTags: 3D reconstruction, local keypoint, optical flow, RGB-D pedestrian\nThe Middlebury Computer Vision Pages\nTags: Stereo matching, 3D reconstruction, MRF, optical flow, color\nCaltech CVG Datasets\nTags: Objects (pedestrian, car, face), 3D reconstruction (on turntables)", "link": "https://github.com/mint-lab/awesome-robotics-datasets", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "toc {:toc}\ndataset collections\nrobotics\nradish: the robotics data set repository, andrew howard and nicholas roy (not working)\nrepository of robotics and computer vision datasets, mrpt\n\ud83d\udcdd it includes malaga datasets and some of classic datasets published in radish.\nijrr data papers, ijrr\nawesome slam datasets, younggun cho \ud83d\udc4d\ncomputer vision\ncvonline image databases, cvonline\ncomputer vision datasets on the web, cvpapers \ud83d\udc4d\nyacvid: yet another computer vision index to datasets, hayko riemenschneider \ud83d\udc4d\ncomputer vision online datasets, computer vision online\nothers\nmachine learning repository, uci\nkaggle datasets, kaggle\nieee dataport, ieee\nplace-specific datasets\ndriving datasets\nkitti vision benchmark suite and kitti-360, andreas geiger et al. \ud83d\udc4d\nsemantickitti, jens behley et al.\nwaymo open dataset, waymo\ncityscapes dataset\nappoloscape dataset\nberkely deepdrive dataset (bdd100k), bair at uc berkely\nnuscenes dataset, aptiv\n$d^2$-city dataset, didi\nford campus vision and lidar data set, perl at univ. of michigan\nmit darpa urban challenge dataset, mit\nkaist multi-spectral recognition dataset in day and night, rcv lab at kaist\nkaist complex urban dataset, irap lab at kaist\nnew college dataset, mrg at oxford univ.\nchinese driving from a bike view (cdbv), cas\nculane dataset, cuhk\nroma (road markings) image database, jean-philippe tarel et al.\nflying datasets\nthe zurich urban micro aerial vehicle dataset, rpg at ethz\nthe uzh-fpv drone racing dataset, rpg at ethz\nmultidrone public dataset, multidrone project\nthe blackbird dataset, agiledrones group at mit\nunderwater datasets\nmarine robotics datasets, acfr\n-----> outdoor !!!  datasets\nthe rawseeds project\n\ud83d\udcdd it includes bovisa dataset is for -----> outdoor !!!  and bicocca dataset is for indoor.\nplanetary mapping and navigation datasets, asrl at univ. of toronto\nindoor datasets\nrobotics 2d-laser datasets, cyrill stachniss\n\ud83d\udcdd it includes some of classic datasets published in radish.\nlong-term mobile robot operations, lincoln univ.\nmit stata center data set, marine robotics group at mit\nkth and cold database, andrzej pronobis\nshopping mall datasets, irc at atr\nrgb-d dataset 7-scenes, microsoft\ntopic-specific datasets for robotics\nlocalization, mapping, and slam\nslam benchmarking, ais at univ. of freiburg\nrobotic 3d scan repository, univ. of wurzburg and univ. of osnabruck\n3d pose graph optimization, luca carlone\nlandmark-based localization\nrange-only data for localization, cmu ri\nroh's angulation dataset, hyunchul roh\nwireless sensor network dataset, kamin whitehouse\npath planning and navigation\npathfinding benchmarks, moving ai lab at univ. of denver\ntask and motion planner benchmarking, rss 2018 workshop\ntopic-specific datasets for computer vision\nfeatures\naffine covariant features datasets, vgg at oxford\nrepeatability benchmark tutorial, vlfeat\na list of feature performance evaluation datasets, maintained by openmvg\nsaliency and foreground\nsaliency\nmit saliency benchmark, mit\nsalient object detection: a benchmark, ming-ming cheng\nforeground/change detection (background subtraction)\nchangedetection.net (a.k.a. cdnet)\nmotion and pose estimation\nadelaidermf: robust model fitting data set, hoi sim wong\nstructure-from-motion and 3d reconstruction\nobjects\nivl-synthesfm v2, davide marelli et al.\nfuji-sfm dataset, jordi gene-mola et al.\nlarge geometric models archive, georgia tech\nthe stanford 3d scanning repository, stanford univ.\nplaces\nphoto tourism data, uw and microsoft\nobject tracking\nvisual object tracking challenge (a.k.a. vot) \ud83d\udc4d\nvisual tracker benchmark (a.k.a. otb)\nobject, place, and event recognition\npedestrians\neurocity persons dataset (a.k.a. ecp)\ndaimler pedestrian benchmark data sets\ncrowdhuman\nobjects\nrgb-d object dataset, uw\nsweet pepper and peduncle 3d datasets, inkyu sa\nplaces\nloop closure detection, david filliat et. al.\ntraffic and surveillance\nbest: benchmark and evaluation of surveillance task, sjtu\nvirat video dataset\nresearch groups\ntum cvg datasets\ntags: visual(-inertia) odometry, visual slam, 3d reconstruction\noxford vgg datasets\ntags: visual features, visual recognition, 3d reconstruction\nqut cyphy datasets\ntags: visual slam, lidar slam\nuniv. of bonn univ. stachniss lab datasets\ntags: slam\nepfl cvlab datasets\ntags: 3d reconstruction, local keypoint, optical flow, rgb-d pedestrian\nthe middlebury computer vision pages\ntags: stereo matching, 3d reconstruction, mrf, optical flow, color\ncaltech cvg datasets\ntags: objects (pedestrian, car, face), 3d reconstruction (on turntables)", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000237, "year": null}, {"Unnamed: 0": 1305, "autor": 285, "date": null, "content": "LiDARTag\nOverview\nThis is a package for LiDARTag, described in paper: LiDARTag: A Real-Time Fiducial Tag System for Point Clouds (PDF)(arXiv). This work is accepted by IEEE Robotics and Automation Letters and published at (here).\nImage-based fiducial markers are useful in problems such as object tracking in cluttered or textureless environments, camera (and multi-sensor) calibration tasks, and vision-based simultaneous localization and mapping (SLAM). However, the state-of-the-art fiducial marker detection algorithms rely on the consistency of the ambient lighting. To the best of our knowledge, there are no existing fiducial markers for point clouds.\nThis paper introduces LiDARTag, a novel fiducial tag design and detection algorithm suitable for LiDAR point clouds. The proposed method runs in real-time and can process data at 100 Hz, which is faster than the currently available LiDAR sensor frequencies. Additionally, the software works with different marker sizes in cluttered indoors and spacious outdoors, even when it is entirely dark. Because of the LiDAR sensors' nature, rapidly changing ambient lighting will not affect the detection of a LiDARTag. Therefore, LiDARTag can be used in tandem with camera-based markers to address the issue of images being sensitive to ambient lighting.\nAuthor: Jiunn-Kai (Bruce) Huang, Shoutian Wang, Maani Ghaffari, and Jessy W. Grizzle\nMaintainer: Bruce JK Huang, brucejkh[at]gmail.com\nAffiliation: The Biped Lab, the University of Michigan\nThis package has been tested under [ROS] Melodic and Ubuntu 18.04.\n[Note] More detailed introduction will be updated shortly. Sorry for the inconvenient!\n[Issues] If you encounter any issues, I would be happy to help. If you cannot find a related one in the existing issues, please open a new one. I will try my best to help!\nAbstract\nImage-based fiducial markers are useful in problems such as object tracking in cluttered or textureless environments, camera (and multi-sensor) calibration tasks, and vision-based simultaneous localization and mapping (SLAM). The state-of-the-art fiducial marker detection algorithms rely on the consistency of the ambient lighting. This paper introduces LiDARTag, a novel fiducial tag design and detection algorithm suitable for light detection and ranging (LiDAR) point clouds. The proposed method runs in real-time and can process data at 100 Hz, which is faster than the currently available LiDAR sensor frequencies. Because of the LiDAR sensors' nature, rapidly changing ambient lighting will not affect the detection of a LiDARTag; hence, the proposed fiducial marker can operate in a completely dark environment. In addition, the LiDARTag nicely complements and is compatible with existing visual fiducial markers, such as AprilTags, allowing for efficient multi-sensor fusion and calibration tasks. We further propose a concept of minimizing a fitting error between a point cloud and the marker's template to estimate the marker's pose. The proposed method achieves millimeter error in translation and a few degrees in rotation. Due to LiDAR returns' sparsity, the point cloud is lifted to a continuous function in a reproducing kernel Hilbert space where the inner product can be used to determine a marker's ID. The experimental results, verified by a motion capture system, confirm that the proposed method can reliably provide a tag's pose and unique ID code. The rejection of false positives is validated on the Google Cartographer dataset and the outdoor Honda H3D datasets. All implementations are coded in C++ and are available at: https://github.com/UMich-BipedLab/LiDARTag.\nVideo\nPlease checkout the introduction video. It highlights some important keypoints in the paper!\nQuick View\nLiDAR-based markers can be used in tandem with camera-based markers to address the issue of images being sensitive to ambient lighting. LiDARTags have been successfully applied to LiDAR-camera extrinsic calibration (paper, GitHub). This figure shows a visualization of LiDARTags of two different sizes in a full point cloud scan.\nThis system runs in real-time (over 100 Hz) while handling a full scan of the point cloud; it achieves millimeter accuracy in translation and a few degrees of error in rotation. The tag decoding accuracy is 99.7%.\nWhy LiDAR?\nRobust to lighting!! The following shows LiDARTags are detected in several challenging lighting conditions:\nDingy environment\nCompletely dark environment\nHalf tag being overexposed\nRapid changing ambient light\nOverall pipeline\nThe system contains three parts: tag detection, pose estimation, and tag decoding. The detection step takes an entire LiDAR scan (up to 120,000 points from a 32-Beam Velodyne ULTRA Puck LiDAR) and outputs collections of likely payload points of the LiDARTag. Next, a tag's optimal pose minimizes the -inspired cost in (8), though the rotation of the tag about a normal vector to the tag may be off by or and will be resolved in the decoding process. The tag's ID is decoded with a pre-computed function library. The decoded tag removes the rotation ambiguity about the normal.\nPackage Analysis\nWe present performance evaluations of the LiDARTag where ground truth data are provided by a motion capture system with 30 motion capture cameras. We also extensively analyze each step in the system with spacious outdoor and cluttered indoor environments. Additionally, we report the rate of false positives validated on the indoor Google Cartographer dataset and the outdoor Honda H3D datasets.\nPose and Decoding Analysis\nDecoding accuracy of the RKHS method and pose accuracy of the fitting method. The ground truth is provided by a motion capture system with 30 motion capture cameras. The distance is in meters. The translation error is in millimeters and rotation error is the misalignment angle, (23), in degrees.\nComputation Time of Each Step Analysis\nThis table averages all the datasets we collected and describes computation time of each step for indoors and outdoors.\nCluster Rejection Analysis\nThis table takes into account all the data we collected and shows numbers of rejected clusters in each step in different scenes. Additionally, we also report false positive rejection for Google Cartographer dataset and Honda H3D datasets.\nDouble-Sum Analysis\nThe original double sum in (18) is too slow to achieve a real-time application. This table compares different methods to compute the double sum, in which the TBB stands for Threading Building Blocks library from Intel. Additionally, we also apply a k-d tree data structure to speed up the querying process; the k-d tree, however, does not produce fast enough results. The unit in the table is milliseconds.\nFalse Positives Analysis\nThis table shows the numbers of false positive rejection of the proposed algorithm. We validated the rejection rate on the indoor Google Cartographer dataset and the outdoor Honda H3D datasets. The former has two VLP-16 Velodyne LiDAR and the latter has one 64-beam Velodyne LiDAR.\nRequired Libraries / Packages\nThose are the packages used in the LiDARTag package. It seems many but if you follow my steps, it should take you no more than 30 mins to instal them (including building time!). It took me awhile to get everything right. I summarize how I installed them here. However, you may choose any way you want to install them.\nPlease install ROS Melodic.\nPlease install TBB library. You may need to modify the CMakeLists.txt according to your installation.\nPlease install NLopt. You may need to midify the CMakeLists.txt according to your installation.\nPlease download LiDARTag_msgs and place them under your catkin workspace.\nThe issue of Eigen version has been fixed by using an internal Eigen library instead of the system-wise Eigen library.\nInstallation of Related Libraries\nROS Melodic\nPlease directly follow the instruction on the official website (here).\nTBB library\nInstallation\nOriginal TBB package from Intel does not support CMake; I, therefore, use another repository that supports CMake to make my life easier.\ngit clone https://github.com/wjakob/tbb\nmkdir build;\ncd build;\ncmake ..;\ncmake --build . --config Release -- -j 6;\nsudo cmake --build . --target install\nNLopt library\nInstallation\nPlease direct follow the instruction on the official website (here) or as follow:\ngit clone git://github.com/stevengj/nlopt\ncd nlopt\nmkdir build\ncd build\ncmake ..\nmake\nsudo make install\nLiDARTag package\nOnce you place LiDARTag_msgs under your catkin workspace and installed all the required libraries, you can directly catkin_make the package.\nsource devel/setup.bash\nroslaunch lidartag LiDARTag_twotags.launch\nrosbag play -l -q bagfile.bag\nDatasets and Results\nQuantitative results:\nIf you would like to see how the tables in the paper are generated, please follow as below:\nDownload this folder.\nPut them under LiDARTag/matlab/paper_data/\nRun genTable.m located at LiDARTag/matlab/\nTo regenerate results on the paper from scratch, please download the two datasets below:\nPlease download bagfiles from here.\nPlease download motion capture data from here\nchange the output_path in the launch file\nroslaunch lidartag LiDARTag_threetags.launch\nNote\nThe target sizes in the quantitative result folder are 1.22.\nQualitative results:\nPlease download bagfiles from here.\nroslaunch lidartag LiDARTag_twotags.launch\nNote\nThe target sizes in the qualitative result folder are 0.8051, 0.61.\nFalse positive rejection:\nPlease download Google Cartographer dataset and Honda H3D datasets. We also provide different launch files (cartographer.launch, H3D.launch) for different datasets due to different published LiDAR topics and different output_path. I also wrote my own parsing script to pass bin files to rosbag. Please let me know if anyone needs it.\nRunning\nPlease download qualitative bagfiles from here.\ncatkin_make the package.\nsource devel/setup.bash\nroslaunch lidartag LiDARTag_twotags.launch\nrosbag play -l -q bagfile.bag\nTo see the results, rosrun rviz rviz. You can directly open LiDARTag.rviz under LiDARTag/rviz/ folder.\nNotes\nThis package provides several launch files that you can directly run the package.\nPlease remember to change the tag_size_list in a launch file according to your target sizes or which bag file you are playing, or what marker sizes you have.\nDifferent launch files:\n-- LiDARTag_smallest.launch: only the smallest tag (0.61)\n-- LiDARTag_twotags.launch: two smaller tags (0.61, 0.8)\n-- LiDARTag_threetags.launch: all tags (0.8, 0.61, 1.22)\nPlease note that, the clearance around the markers should larger than , where is the size of the largest marker. Therefore, it is recommended to use smaller tags in indoor environments.\nBuilding Your Markers\nWe provide tag16h6c5 from AprilTag3 with three sizes (0.61, 0.85, 1.2).\nIf you want to use the provided markers, it is easy:\nAttach a fiducial marker to a squared cardboard or plexiglass and place the marker inside the yellow region.\nNote: The sizes must be one of 0.61, 0.805, 1.22 meter, or you have to regenerate the function dictionary. If so, please follow here.\nFind a 3D object to support your marker. It could be a box or an easel.\nPlease note that, the clearance around the markers should larger than , where is the size of the largest marker. Therefore, it is recommended to use smaller tags in indoor environments.\nFollow these steps to run the package.\nBuilding Your Own Customized Markers\nIf you would like to use your own customized markers (i.e. different types of markers or different sizes), please follow these steps:\nI. Build your function dictionary:\ngit clone https://github.com/UMich-BipedLab/matlab_utils\nAdd matlab_utils into build_LiDARTag_library.m or add matlab_utils into your MATLAB path.\nEdit opts.img_path in build_LiDARTag_library.m according to where you put images of your fiducial markers.\nMeasure the size of your marker ()\nOpen build_LiDARTag_library.m in LiDARTag/matlab/function_dictionary/. Change opts.target_size_ to your marker size and run build_LiDARTag_library.m to generate your function library.\nPut the generated function dictuionary into LiDARTag/lib/\nWhen placing the generated function dictionary in LiDARTag/lib/, please put different sizes into different sub-folders (0, 1, 2, 3, ...) and put them in ascending order. For example, if you have three sizes (0.6, 0.8, 1.2), then you will have three sub-folders (0, 1, 2) inside the lib/ folder. Please place them as follow:\nLiDARTag/lib/0/: put 0.6-size here\nLiDARTag/lib/1/: put 0.8-size here\nLiDARTag/lib/2/: put 1.2-size here\nII. Follow Building Your Markers\nNote\nAll the functions that are used for testing RKHS are all released in LiDARTag/matlab/function_dictionary/\nParameters of launch files\nWe split the parameters to two different launch files: LiDARTag_outdoor.launch and LiDARTag_master.launch. The front contains the most common tunables for different environments such as indoor or outdoor. The latter includes more parameters that you usually need to change for your system only once and just leave them there.\nLiDARTag_outdoor.launch\nfeature clustering\nnearby_factor\nValue used to determine if two points are near to each other\nlinkage_tunable\nValue used to compute the linkage criteria\ncluster validation\nmax_outlier_ratio\nValue used to validate clusters during checking outliers in plane fitting\ntag_size_list\nList of possible sizes of tag\nLiDARTag_master.launch\nSystem Mode\nmark_cluster_validity\nwhether to validate clusters according to different conditions\nplane_fitting\nwhether to validate clusters according to the result of plane_fitting\noptimize_pose\nWhether to optimize poses via reducing the cost function\ndecode_id\nWhether to decode IDs\ncollect_data\nWhether to publish detected PCs\nnum_threads\nThe number of threads used for TBB\nprint_info\nWhether to log status in ros_info_stream\nDebugging Mode\ndebug_info\nWhether to log debug information in ros_debug_stream\ndebug_time\nWhether to compute time for different parts\ndebug_decoding_time\nWhether to log time for decoding IDs\nlog_data\nWhether to save status information into txt file\nLiDAR Specification\nhas_ring\nWhether input data has ring information for each point\nestimate_ring\nWhether to estimate ring number for each point\nSolvers for Pose Optimization\noptimization_solver (default: 8)\nWhich optimization solver to use for optimizing the cost function of a pose.\nBelow is numerical gradient-based methods\n1: opt_method = nlopt::LN_PRAXIS;\n2: opt_method = nlopt::LN_NEWUOA_BOUND;\n3: opt_method = nlopt::LN_SBPLX; // recommended\n4: opt_method = nlopt::LN_BOBYQA;\n5: opt_method = nlopt::LN_NELDERMEAD;\n6: opt_method = nlopt::LN_COBYLA;\nBelow is analytical gradient-based methods\n7: opt_method = nlopt::LD_SLSQP; // recommended 200Hz\n8: opt_method = nlopt::LD_MMA; // recommended 120Hz\n9: opt_method = nlopt::LD_TNEWTON_PRECOND_RESTART; // fail 90%\n10: opt_method = nlopt::LD_TNEWTON_PRECOND; // fail 90%\n11: opt_method = nlopt::LD_TNEWTON_RESTART; // fail 80%\n12: opt_method = nlopt::LD_TNEWTON; // fail 90%\n13: opt_method = nlopt::LD_LBFGS; // fail 90%\n14: opt_method = nlopt::LD_VAR1; // fail 90%\n15: opt_method = nlopt::LD_VAR2; // fail 90%\neuler_derivative\nWhether to use euler derivative or lie group derivative in optimization\noptimize_up_bound\nValue used for constraints in optimization\noptimize_low_bound\nValue used for constraints in optimization\nDecode Method\ndecode_method (default: 2)\nWhich decoding method to use:\n0: naive decoder\n1: Weighted Gaussian\n2: RKHS\ndecode_mode (default: 5)\nWhich mode to use: 0: single thread: original double sum\n1: single thread: convert to matrices\n2: single thread: convert matrices to vectors\n3: c++ thread (works for each point for a thread but not for blobs of points for a thread)\n4: Multi-threading: Original double sum using TBB\n5: Multi-threading: Vector form using TBB without scheduling\n6: Multi-threading: Vector form using TBB with manual scheduling\n7: Multi-threading: Vector form using TBB with TBB scheduling\n8: Single thread: using KDTree\nTunable\nfeature clustering\ndistance_bound\nValue used to construct a cube and only detect the tag inside this cube\ndepth_bound\nValue used to detect feature points compared with depth gradients\nnum_points_for_plane_feature\nnumber of points used for detection of feature points\ncluster validation\nmin_return_per_grid\nMinimum number of points in each grid (below this number, the cluster will be invalid)\noptimize_percentage\nValue used to validate the result of pose estimation via checking cost value\npayload_intensity_threshold\nValue used to detect boundary points on the cluster via intensity gradient\npoints_threshold_factor\ndistance_to_plane_threshold\nValue used for plane fitting for a cluster\nminimum_ring_boundary_points\nMinimum number of boundary points on each ring in the cluster\ncoa_tunable\nValue used to validate the result of pose estimation via checking coverage area\ntagsize_tunable\nValue used to estimate the size of tag\nCitations\nThe detail is described in: LiDARTag: A Real-Time Fiducial Tag for Point Clouds, Jiunn-Kai Huang, Shoutian Wang, Maani Ghaffari, and Jessy W. Grizzle. (PDF) (arXiv) (here)\n@ARTICLE{HuangLiDARTag2020,\nauthor={Huang, Jiunn-Kai and Wang, Shoutian and Ghaffari, Maani and Grizzle, Jessy W.},\njournal={IEEE Robotics and Automation Letters},\ntitle={LiDARTag: A Real-Time Fiducial Tag System for Point Clouds},\nyear={2021},\nvolume={6},\nnumber={3},\npages={4875-4882},\ndoi={10.1109/LRA.2021.3070302}}", "link": "https://github.com/UMich-BipedLab/LiDARTag", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "lidartag\noverview\nthis is a package for lidartag, described in paper: lidartag: a real-time fiducial tag system for point clouds (pdf)(arxiv). this work is accepted by ieee robotics and automation letters and published at (here).\nimage-based fiducial markers are useful in problems such as object tracking in cluttered or textureless environments, camera (and multi-sensor) calibration tasks, and vision-based simultaneous localization and mapping (slam). however, the state-of-the-art fiducial marker detection algorithms rely on the consistency of the ambient lighting. to the best of our knowledge, there are no existing fiducial markers for point clouds.\nthis paper introduces lidartag, a novel fiducial tag design and detection algorithm suitable for lidar point clouds. the proposed method runs in real-time and can process data at 100 hz, which is faster than the currently available lidar sensor frequencies. additionally, the software works with different marker sizes in cluttered indoors and spacious outdoors, even when it is entirely dark. because of the lidar sensors' nature, rapidly changing ambient lighting will not affect the detection of a lidartag. therefore, lidartag can be used in tandem with camera-based markers to address the issue of images being sensitive to ambient lighting.\nauthor: jiunn-kai (bruce) huang, shoutian wang, maani ghaffari, and jessy w. grizzle\nmaintainer: bruce jk huang, brucejkh[at]gmail.com\naffiliation: the biped lab, the university of michigan\nthis package has been tested under [ros] melodic and ubuntu 18.04.\n[note] more detailed introduction will be updated shortly. sorry for the inconvenient!\n[issues] if you encounter any issues, i would be happy to help. if you cannot find a related one in the existing issues, please open a new one. i will try my best to help!\nabstract\nimage-based fiducial markers are useful in problems such as object tracking in cluttered or textureless environments, camera (and multi-sensor) calibration tasks, and vision-based simultaneous localization and mapping (slam). the state-of-the-art fiducial marker detection algorithms rely on the consistency of the ambient lighting. this paper introduces lidartag, a novel fiducial tag design and detection algorithm suitable for light detection and ranging (lidar) point clouds. the proposed method runs in real-time and can process data at 100 hz, which is faster than the currently available lidar sensor frequencies. because of the lidar sensors' nature, rapidly changing ambient lighting will not affect the detection of a lidartag; hence, the proposed fiducial marker can operate in a completely dark environment. in addition, the lidartag nicely complements and is compatible with existing visual fiducial markers, such as apriltags, allowing for efficient multi-sensor fusion and calibration tasks. we further propose a concept of minimizing a fitting error between a point cloud and the marker's template to estimate the marker's pose. the proposed method achieves millimeter error in translation and a few degrees in rotation. due to lidar returns' sparsity, the point cloud is lifted to a continuous function in a reproducing kernel hilbert space where the inner product can be used to determine a marker's id. the experimental results, verified by a motion capture system, confirm that the proposed method can reliably provide a tag's pose and unique id code. the rejection of false positives is validated on the google cartographer dataset and the -----> outdoor !!!  honda h3d datasets. all implementations are coded in c++ and are available at: https://github.com/umich-bipedlab/lidartag.\nvideo\nplease checkout the introduction video. it highlights some important keypoints in the paper!\nquick view\nlidar-based markers can be used in tandem with camera-based markers to address the issue of images being sensitive to ambient lighting. lidartags have been successfully applied to lidar-camera extrinsic calibration (paper, github). this figure shows a visualization of lidartags of two different sizes in a full point cloud scan.\nthis system runs in real-time (over 100 hz) while handling a full scan of the point cloud; it achieves millimeter accuracy in translation and a few degrees of error in rotation. the tag decoding accuracy is 99.7%.\nwhy lidar?\nrobust to lighting!! the following shows lidartags are detected in several challenging lighting conditions:\ndingy environment\ncompletely dark environment\nhalf tag being overexposed\nrapid changing ambient light\noverall pipeline\nthe system contains three parts: tag detection, pose estimation, and tag decoding. the detection step takes an entire lidar scan (up to 120,000 points from a 32-beam velodyne ultra puck lidar) and outputs collections of likely payload points of the lidartag. next, a tag's optimal pose minimizes the -inspired cost in (8), though the rotation of the tag about a normal vector to the tag may be off by or and will be resolved in the decoding process. the tag's id is decoded with a pre-computed function library. the decoded tag removes the rotation ambiguity about the normal.\npackage analysis\nwe present performance evaluations of the lidartag where ground truth data are provided by a motion capture system with 30 motion capture cameras. we also extensively analyze each step in the system with spacious outdoor and cluttered indoor environments. additionally, we report the rate of false positives validated on the indoor google cartographer dataset and the outdoor honda h3d datasets.\npose and decoding analysis\ndecoding accuracy of the rkhs method and pose accuracy of the fitting method. the ground truth is provided by a motion capture system with 30 motion capture cameras. the distance is in meters. the translation error is in millimeters and rotation error is the misalignment angle, (23), in degrees.\ncomputation time of each step analysis\nthis table averages all the datasets we collected and describes computation time of each step for indoors and outdoors.\ncluster rejection analysis\nthis table takes into account all the data we collected and shows numbers of rejected clusters in each step in different scenes. additionally, we also report false positive rejection for google cartographer dataset and honda h3d datasets.\ndouble-sum analysis\nthe original double sum in (18) is too slow to achieve a real-time application. this table compares different methods to compute the double sum, in which the tbb stands for threading building blocks library from intel. additionally, we also apply a k-d tree data structure to speed up the querying process; the k-d tree, however, does not produce fast enough results. the unit in the table is milliseconds.\nfalse positives analysis\nthis table shows the numbers of false positive rejection of the proposed algorithm. we validated the rejection rate on the indoor google cartographer dataset and the outdoor honda h3d datasets. the former has two vlp-16 velodyne lidar and the latter has one 64-beam velodyne lidar.\nrequired libraries / packages\nthose are the packages used in the lidartag package. it seems many but if you follow my steps, it should take you no more than 30 mins to instal them (including building time!). it took me awhile to get everything right. i summarize how i installed them here. however, you may choose any way you want to install them.\nplease install ros melodic.\nplease install tbb library. you may need to modify the cmakelists.txt according to your installation.\nplease install nlopt. you may need to midify the cmakelists.txt according to your installation.\nplease download lidartag_msgs and place them under your catkin workspace.\nthe issue of eigen version has been fixed by using an internal eigen library instead of the system-wise eigen library.\ninstallation of related libraries\nros melodic\nplease directly follow the instruction on the official website (here).\ntbb library\ninstallation\noriginal tbb package from intel does not support cmake; i, therefore, use another repository that supports cmake to make my life easier.\ngit clone https://github.com/wjakob/tbb\nmkdir build;\ncd build;\ncmake ..;\ncmake --build . --config release -- -j 6;\nsudo cmake --build . --target install\nnlopt library\ninstallation\nplease direct follow the instruction on the official website (here) or as follow:\ngit clone git://github.com/stevengj/nlopt\ncd nlopt\nmkdir build\ncd build\ncmake ..\nmake\nsudo make install\nlidartag package\nonce you place lidartag_msgs under your catkin workspace and installed all the required libraries, you can directly catkin_make the package.\nsource devel/setup.bash\nroslaunch lidartag lidartag_twotags.launch\nrosbag play -l -q bagfile.bag\ndatasets and results\nquantitative results:\nif you would like to see how the tables in the paper are generated, please follow as below:\ndownload this folder.\nput them under lidartag/matlab/paper_data/\nrun gentable.m located at lidartag/matlab/\nto regenerate results on the paper from scratch, please download the two datasets below:\nplease download bagfiles from here.\nplease download motion capture data from here\nchange the output_path in the launch file\nroslaunch lidartag lidartag_threetags.launch\nnote\nthe target sizes in the quantitative result folder are 1.22.\nqualitative results:\nplease download bagfiles from here.\nroslaunch lidartag lidartag_twotags.launch\nnote\nthe target sizes in the qualitative result folder are 0.8051, 0.61.\nfalse positive rejection:\nplease download google cartographer dataset and honda h3d datasets. we also provide different launch files (cartographer.launch, h3d.launch) for different datasets due to different published lidar topics and different output_path. i also wrote my own parsing script to pass bin files to rosbag. please let me know if anyone needs it.\nrunning\nplease download qualitative bagfiles from here.\ncatkin_make the package.\nsource devel/setup.bash\nroslaunch lidartag lidartag_twotags.launch\nrosbag play -l -q bagfile.bag\nto see the results, rosrun rviz rviz. you can directly open lidartag.rviz under lidartag/rviz/ folder.\nnotes\nthis package provides several launch files that you can directly run the package.\nplease remember to change the tag_size_list in a launch file according to your target sizes or which bag file you are playing, or what marker sizes you have.\ndifferent launch files:\n-- lidartag_smallest.launch: only the smallest tag (0.61)\n-- lidartag_twotags.launch: two smaller tags (0.61, 0.8)\n-- lidartag_threetags.launch: all tags (0.8, 0.61, 1.22)\nplease note that, the clearance around the markers should larger than , where is the size of the largest marker. therefore, it is recommended to use smaller tags in indoor environments.\nbuilding your markers\nwe provide tag16h6c5 from apriltag3 with three sizes (0.61, 0.85, 1.2).\nif you want to use the provided markers, it is easy:\nattach a fiducial marker to a squared cardboard or plexiglass and place the marker inside the yellow region.\nnote: the sizes must be one of 0.61, 0.805, 1.22 meter, or you have to regenerate the function dictionary. if so, please follow here.\nfind a 3d object to support your marker. it could be a box or an easel.\nplease note that, the clearance around the markers should larger than , where is the size of the largest marker. therefore, it is recommended to use smaller tags in indoor environments.\nfollow these steps to run the package.\nbuilding your own customized markers\nif you would like to use your own customized markers (i.e. different types of markers or different sizes), please follow these steps:\ni. build your function dictionary:\ngit clone https://github.com/umich-bipedlab/matlab_utils\nadd matlab_utils into build_lidartag_library.m or add matlab_utils into your matlab path.\nedit opts.img_path in build_lidartag_library.m according to where you put images of your fiducial markers.\nmeasure the size of your marker ()\nopen build_lidartag_library.m in lidartag/matlab/function_dictionary/. change opts.target_size_ to your marker size and run build_lidartag_library.m to generate your function library.\nput the generated function dictuionary into lidartag/lib/\nwhen placing the generated function dictionary in lidartag/lib/, please put different sizes into different sub-folders (0, 1, 2, 3, ...) and put them in ascending order. for example, if you have three sizes (0.6, 0.8, 1.2), then you will have three sub-folders (0, 1, 2) inside the lib/ folder. please place them as follow:\nlidartag/lib/0/: put 0.6-size here\nlidartag/lib/1/: put 0.8-size here\nlidartag/lib/2/: put 1.2-size here\nii. follow building your markers\nnote\nall the functions that are used for testing rkhs are all released in lidartag/matlab/function_dictionary/\nparameters of launch files\nwe split the parameters to two different launch files: lidartag_outdoor.launch and lidartag_master.launch. the front contains the most common tunables for different environments such as indoor or outdoor. the latter includes more parameters that you usually need to change for your system only once and just leave them there.\nlidartag_outdoor.launch\nfeature clustering\nnearby_factor\nvalue used to determine if two points are near to each other\nlinkage_tunable\nvalue used to compute the linkage criteria\ncluster validation\nmax_outlier_ratio\nvalue used to validate clusters during checking outliers in plane fitting\ntag_size_list\nlist of possible sizes of tag\nlidartag_master.launch\nsystem mode\nmark_cluster_validity\nwhether to validate clusters according to different conditions\nplane_fitting\nwhether to validate clusters according to the result of plane_fitting\noptimize_pose\nwhether to optimize poses via reducing the cost function\ndecode_id\nwhether to decode ids\ncollect_data\nwhether to publish detected pcs\nnum_threads\nthe number of threads used for tbb\nprint_info\nwhether to log status in ros_info_stream\ndebugging mode\ndebug_info\nwhether to log debug information in ros_debug_stream\ndebug_time\nwhether to compute time for different parts\ndebug_decoding_time\nwhether to log time for decoding ids\nlog_data\nwhether to save status information into txt file\nlidar specification\nhas_ring\nwhether input data has ring information for each point\nestimate_ring\nwhether to estimate ring number for each point\nsolvers for pose optimization\noptimization_solver (default: 8)\nwhich optimization solver to use for optimizing the cost function of a pose.\nbelow is numerical gradient-based methods\n1: opt_method = nlopt::ln_praxis;\n2: opt_method = nlopt::ln_newuoa_bound;\n3: opt_method = nlopt::ln_sbplx; // recommended\n4: opt_method = nlopt::ln_bobyqa;\n5: opt_method = nlopt::ln_neldermead;\n6: opt_method = nlopt::ln_cobyla;\nbelow is analytical gradient-based methods\n7: opt_method = nlopt::ld_slsqp; // recommended 200hz\n8: opt_method = nlopt::ld_mma; // recommended 120hz\n9: opt_method = nlopt::ld_tnewton_precond_restart; // fail 90%\n10: opt_method = nlopt::ld_tnewton_precond; // fail 90%\n11: opt_method = nlopt::ld_tnewton_restart; // fail 80%\n12: opt_method = nlopt::ld_tnewton; // fail 90%\n13: opt_method = nlopt::ld_lbfgs; // fail 90%\n14: opt_method = nlopt::ld_var1; // fail 90%\n15: opt_method = nlopt::ld_var2; // fail 90%\neuler_derivative\nwhether to use euler derivative or lie group derivative in optimization\noptimize_up_bound\nvalue used for constraints in optimization\noptimize_low_bound\nvalue used for constraints in optimization\ndecode method\ndecode_method (default: 2)\nwhich decoding method to use:\n0: naive decoder\n1: weighted gaussian\n2: rkhs\ndecode_mode (default: 5)\nwhich mode to use: 0: single thread: original double sum\n1: single thread: convert to matrices\n2: single thread: convert matrices to vectors\n3: c++ thread (works for each point for a thread but not for blobs of points for a thread)\n4: multi-threading: original double sum using tbb\n5: multi-threading: vector form using tbb without scheduling\n6: multi-threading: vector form using tbb with manual scheduling\n7: multi-threading: vector form using tbb with tbb scheduling\n8: single thread: using kdtree\ntunable\nfeature clustering\ndistance_bound\nvalue used to construct a cube and only detect the tag inside this cube\ndepth_bound\nvalue used to detect feature points compared with depth gradients\nnum_points_for_plane_feature\nnumber of points used for detection of feature points\ncluster validation\nmin_return_per_grid\nminimum number of points in each grid (below this number, the cluster will be invalid)\noptimize_percentage\nvalue used to validate the result of pose estimation via checking cost value\npayload_intensity_threshold\nvalue used to detect boundary points on the cluster via intensity gradient\npoints_threshold_factor\ndistance_to_plane_threshold\nvalue used for plane fitting for a cluster\nminimum_ring_boundary_points\nminimum number of boundary points on each ring in the cluster\ncoa_tunable\nvalue used to validate the result of pose estimation via checking coverage area\ntagsize_tunable\nvalue used to estimate the size of tag\ncitations\nthe detail is described in: lidartag: a real-time fiducial tag for point clouds, jiunn-kai huang, shoutian wang, maani ghaffari, and jessy w. grizzle. (pdf) (arxiv) (here)\n@article{huanglidartag2020,\nauthor={huang, jiunn-kai and wang, shoutian and ghaffari, maani and grizzle, jessy w.},\njournal={ieee robotics and automation letters},\ntitle={lidartag: a real-time fiducial tag system for point clouds},\nyear={2021},\nvolume={6},\nnumber={3},\npages={4875-4882},\ndoi={10.1109/lra.2021.3070302}}", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000285, "year": null}, {"Unnamed: 0": 1466, "autor": 446, "date": null, "content": "ros_dwm1000\nROS Indoor/Outdoor Positioning System based on Decawave's DWM1000 Ultra Wide Band transceivers. The whole system was tested indoors with 3 anchors and 1 tag. Technically this should work outdoors as well.\nHardware\nBoth anchors and tags use the same circuit board. The board houses the UWB transceiver, an Arduino Pro Mini 3.3V and a built-in USB Male so you can plug it straight to a power bank and deploy the anchors remotely.\nIf you wish to fabricate the PCB, the Gerber files are included /hardware/pcb/fabricate. Seeedstudio's DRU for 2-layer board was used to verify the design.\nThe DWM1000 Arduino library uses random address for the anchors and doesn't support arbitrary addresses. You need to edit https://github.com/grassjelly/ros_dwm1000/blob/master/hardware/firmware/anchor/lib/DW1000/DW1000Ranging.cpp#L166-L167 every time you upload the anchor codes to have unique address per anchor from:\n_currentShortAddress[0] = random(0, 256);\n_currentShortAddress[1] = random(0, 256);\nto the address you want (ie. address: '01'):\n_currentShortAddress[0] = 1;\n_currentShortAddress[1] = 0;\nInstallation\nInstall dependencies\n$ pip install localization\n$ pip install scipy\n$ pip install shapely\nInstall map_server\n$ sudo apt-get install ros-indigo-map-server\nBuild the package\n$ cd ~/catkin_ws\n$ catkin_make\nDefining TF\nYou need to define the transforms from map to each anchor's frame. You can run map_server and load your map to identify the exact points you want your anchors placed. Open RVIZ and click on the location of each anchor by using \"Publish Points\".\nOnce you get the exact location of each anchor, add the transforms in /launch/tf.launch file. Take note that the frame_id of each anchor must be the same as the address defined in Arduino (_currentShortAddress[]).\nUsage\nRun localize.launch:\n$ roslaunch ros_dwm1000 localize.launch\nParameters\nserial_port(default: '/dev/ttyUSB0')\nTag's serial port.\nframe_id(default: 'uwb_tag')\nTag's frame_id when transform's published from map to tag.\nreq_anchor(default: 3)\nThe mininum number of anchors the system must find before performing trilateration between anchors. Increase this if you want to improve the accuracy. This can be tweaked down to a minimum of 2 anchors. Each tag can only support up to 4 anchors now.\nmin_range(default: 0.5)\nThe minimum probable distance from an anchor to a tag to prevent false reading. For example, if your anchor is hanging 1 m away from the floor, it makes sense to put 1 meter as minimum range since there's no way it will range less than 1 meter.\nmax_range(default: 10.0)\nThe maximum probable distance from an anchor to a tag to prevent false reading. For example, if your anchors are placed in 8 meter x 8 meter room, you can set the max_range less than or equal the hypotenuse of the room since ranges won't get any longer than that.", "link": "https://github.com/linorobot/ros_dwm1000", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "ros_dwm1000\nros indoor/-----> outdoor !!!  positioning system based on decawave's dwm1000 ultra wide band transceivers. the whole system was tested indoors with 3 anchors and 1 tag. technically this should work outdoors as well.\nhardware\nboth anchors and tags use the same circuit board. the board houses the uwb transceiver, an arduino pro mini 3.3v and a built-in usb male so you can plug it straight to a power bank and deploy the anchors remotely.\nif you wish to fabricate the pcb, the gerber files are included /hardware/pcb/fabricate. seeedstudio's dru for 2-layer board was used to verify the design.\nthe dwm1000 arduino library uses random address for the anchors and doesn't support arbitrary addresses. you need to edit https://github.com/grassjelly/ros_dwm1000/blob/master/hardware/firmware/anchor/lib/dw1000/dw1000ranging.cpp#l166-l167 every time you upload the anchor codes to have unique address per anchor from:\n_currentshortaddress[0] = random(0, 256);\n_currentshortaddress[1] = random(0, 256);\nto the address you want (ie. address: '01'):\n_currentshortaddress[0] = 1;\n_currentshortaddress[1] = 0;\ninstallation\ninstall dependencies\n$ pip install localization\n$ pip install scipy\n$ pip install shapely\ninstall map_server\n$ sudo apt-get install ros-indigo-map-server\nbuild the package\n$ cd ~/catkin_ws\n$ catkin_make\ndefining tf\nyou need to define the transforms from map to each anchor's frame. you can run map_server and load your map to identify the exact points you want your anchors placed. open rviz and click on the location of each anchor by using \"publish points\".\nonce you get the exact location of each anchor, add the transforms in /launch/tf.launch file. take note that the frame_id of each anchor must be the same as the address defined in arduino (_currentshortaddress[]).\nusage\nrun localize.launch:\n$ roslaunch ros_dwm1000 localize.launch\nparameters\nserial_port(default: '/dev/ttyusb0')\ntag's serial port.\nframe_id(default: 'uwb_tag')\ntag's frame_id when transform's published from map to tag.\nreq_anchor(default: 3)\nthe mininum number of anchors the system must find before performing trilateration between anchors. increase this if you want to improve the accuracy. this can be tweaked down to a minimum of 2 anchors. each tag can only support up to 4 anchors now.\nmin_range(default: 0.5)\nthe minimum probable distance from an anchor to a tag to prevent false reading. for example, if your anchor is hanging 1 m away from the floor, it makes sense to put 1 meter as minimum range since there's no way it will range less than 1 meter.\nmax_range(default: 10.0)\nthe maximum probable distance from an anchor to a tag to prevent false reading. for example, if your anchors are placed in 8 meter x 8 meter room, you can set the max_range less than or equal the hypotenuse of the room since ranges won't get any longer than that.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000446, "year": null}, {"Unnamed: 0": 1568, "autor": 548, "date": null, "content": "Updated on 2021.12.16\nSLAM\nPublish Date Title Authors PDF Code\n2021-12-15 Homography Decomposition Networks for Planar Object Tracking Xinrui Zhan et.al. 2112.07909v1 null\n2021-12-14 Autonomous Navigation System from Simultaneous Localization and Mapping Micheal Caracciolo et.al. 2112.07723v1 link\n2021-12-12 360-DFPE: Leveraging Monocular 360-Layouts for Direct Floor Plan Estimation Bolivar Solarte et.al. 2112.06180v1 null\n2021-12-11 Simultaneous Localization and Mapping: Through the Lens of Nonlinear Optimization Amay Saxena et.al. 2112.05921v1 null\n2021-12-07 Hybrid Visual SLAM for Underwater Vehicle Manipulator Systems Gideon Billings et.al. 2112.03826v1 null\n2021-12-05 Iterated Posterior Linearization PMB Filter for 5G SLAM Yu Ge et.al. 2112.02575v1 null\n2021-12-03 Fast Direct Stereo Visual SLAM Jiawei Mo et.al. 2112.01890v1 link\n2021-12-02 MegBA: A High-Performance and Distributed Library for Large-Scale Bundle Adjustment Jie Ren et.al. 2112.01349v2 link\n2021-12-01 Research on Event Accumulator Settings for Event-Based SLAM Kun Xiao et.al. 2112.00427v1 link\n2021-11-29 An in-depth experimental study of sensor usage and visual reasoning of robots navigating in real environments Assem Sadek et.al. 2111.14666v1 null\n2021-11-29 Deployment of Aerial Robots after a major fire of an industrial hall with hazardous substances, a report Hartmut Surmann et.al. 2111.14542v1 null\n2021-11-24 Automatic Mapping with Obstacle Identification for Indoor Human Mobility Assessment V. Ayala-Alfaro et.al. 2111.12690v1 null\n2021-11-24 Autonomous bot with ML-based reactive navigation for indoor environment Yash Srivastava et.al. 2111.12542v1 null\n2021-11-22 A General Framework for Lifelong Localization and Mapping in Changing Environment Min Zhao et.al. 2111.10946v1 link\n2021-11-17 Probabilistic Spatial Distribution Prior Based Attentional Keypoints Matching Network Xiaoming Zhao et.al. 2111.09006v2 null\n2021-11-10 Comparing dominance of tennis' big three via multiple-output Bayesian quantile regression models Bruno Santos et.al. 2111.05631v1 null\n2021-11-10 TomoSLAM: factor graph optimization for rotation angle refinement in microtomography Mark Griguletskii et.al. 2111.05562v1 null\n2021-11-07 Hierarchical Segment-based Optimization for SLAM Yuxin Tian et.al. 2111.04101v1 null\n2021-11-07 Online Mutual Adaptation of Deep Depth Prediction and Visual SLAM Shing Yan Loo et.al. 2111.04096v2 null\n2021-11-05 MSC-VO: Exploiting Manhattan and Structural Constraints for Visual Odometry Joan P. Company-Corcoles et.al. 2111.03408v1 null\n2021-10-31 Loop closure detection using local 3D deep descriptors Youjie Zhou et.al. 2111.00440v1 null\n2021-10-27 Millimeter Wave Wireless Assisted Robot Navigation with Link State Classification Mingsheng Yin et.al. 2110.14789v2 link\n2021-10-27 Efficient Placard Discovery for Semantic Mapping During Frontier Exploration David Balaban et.al. 2110.14742v1 null\n2021-10-26 Robust Multi-view Registration of Point Sets with Laplacian Mixture Model Jin Zhang et.al. 2110.13744v1 null\n2021-10-25 WOLF: A modular estimation framework for robotics based on factor graphs Joan Sola et.al. 2110.12919v1 null\n2021-10-21 Real-Time Ground-Plane Refined LiDAR SLAM Fan Yang et.al. 2110.11517v1 null\n2021-10-21 SymbioLCD: Ensemble-Based Loop Closure Detection using CNN-Extracted Objects and Visual Bag-of-Words Jonathan J. Y. Kim et.al. 2110.11491v1 null\n2021-10-21 InterpolationSLAM: A Novel Robust Visual SLAM System in Rotational Motion Zhenkun Zhu et.al. 2110.11040v2 null\n2021-10-20 SLAM: A Unified Encoder for Speech and Language Modeling via Speech-Text Joint Pre-Training Ankur Bapna et.al. 2110.10329v1 null\n2021-10-18 Enhancing exploration algorithms for navigation with visual SLAM Kirill Muravyev et.al. 2110.09156v1 null\n2021-10-18 Accurate and Robust Object-oriented SLAM with 3D Quadric Landmark Construction in Outdoor Environment Rui Tian et.al. 2110.08977v1 null\n2021-10-16 Partial Hierarchical Pose Graph Optimization for SLAM Alexander Korovko et.al. 2110.08639v1 null\n2021-10-14 Active SLAM over Continuous Trajectory and Control: A Covariance-Feedback Approach Shumon Koga et.al. 2110.07546v1 null\n2021-10-13 Collaborative Radio SLAM for Multiple Robots based on WiFi Fingerprint Similarity Ran Liu et.al. 2110.06541v2 null\n2021-10-12 Learning Efficient Multi-Agent Cooperative Visual Exploration Chao Yu et.al. 2110.05734v1 null\n2021-10-07 Self-Supervised Depth Completion for Active Stereo Frederik Warburg et.al. 2110.03234v1 null\n2021-10-06 InterpolationSLAM: A Novel Robust Visual SLAM System in Rotating Scenes Zhenkun Zhu et.al. 2110.02593v1 null\n2021-10-03 AEROS: Adaptive RObust least-Squares for Graph-Based SLAM Milad Ramezani et.al. 2110.02018v1 null\n2021-10-04 Fast Uncertainty Quantification for Active Graph SLAM Julio A. Placed et.al. 2110.01289v1 link\n2021-10-04 Geometry-based Graph Pruning for Lifelong SLAM Gerhard Kurz et.al. 2110.01286v1 null\n2021-10-03 Quadrotor Control on $SU(2)\\times R^3$ with SLAM Integration Marcus Greiff et.al. 2110.01099v1 null\n2021-10-02 Online Incremental Non-Gaussian Inference for SLAM Using Normalizing Flows Qiangqiang Huang et.al. 2110.00876v1 null\nSFM\nPublish Date Title Authors PDF Code\n2021-12-10 Critical configurations for three projective views Martin Br\u00e5telund et.al. 2112.05478v1 null\n2021-12-09 Critical configurations for two projective views, a new approach Martin Br\u00e5telund et.al. 2112.05074v1 null\n2021-12-06 Dense Depth Priors for Neural Radiance Fields from Sparse Input Views Barbara Roessle et.al. 2112.03288v1 null\n2021-12-02 MegBA: A High-Performance and Distributed Library for Large-Scale Bundle Adjustment Jie Ren et.al. 2112.01349v2 link\n2021-11-11 Multi-Resolution Elevation Mapping and Safe Landing Site Detection with Applications to Planetary Rotorcraft Pascal Schoppmann et.al. 2111.06271v1 null\n2021-11-05 Damage Estimation and Localization from Sparse Aerial Imagery Rene Garcia Franceschini et.al. 2111.03708v2 null\n2021-11-03 Event and Activity Recognition in Video Surveillance for Cyber-Physical Systems Swarnabja Bhaumik et.al. 2111.02064v1 null\n2021-10-14 Modeling dynamic target deformation in camera calibration Annika Hagemann et.al. 2110.07322v1 null\n2021-10-13 Hyperspectral 3D Mapping of Underwater Environments Maxime Ferrera et.al. 2110.06571v1 null\n2021-09-24 Automatic Map Update Using Dashcam Videos Aziza Zhanabatyrova et.al. 2109.12131v1 null\n2021-09-16 Rotation Averaging in a Split Second: A Primal-Dual Method and a Closed-Form for Cycle Graphs Gabriel Moreira et.al. 2109.08046v1 link\n2021-09-06 Single-Camera 3D Head Fitting for Mixed Reality Clinical Applications Tejas Mane et.al. 2109.02740v1 null\n2021-09-02 Dynamic Scene Novel View Synthesis via Deferred Spatio-temporal Consistency Beatrix-Em\u0151ke F\u00fcl\u00f6p-Balogh et.al. 2109.01018v1 null\n2021-09-01 On the Limits of Pseudo Ground Truth in Visual Camera Re-localisation Eric Brachmann et.al. 2109.00524v1 link\n2021-08-31 DensePose 3D: Lifting Canonical Surface Maps of Articulated Objects to the Third Dimension Roman Shapovalov et.al. 2109.00033v1 null\n2021-08-29 Solving Viewing Graph Optimization for Simultaneous Position and Rotation Registration Seyed-Mahdi Nasiri et.al. 2108.12876v1 null\n2021-08-23 Burst Imaging for Light-Constrained Structure-From-Motion Ahalya Ravendran et.al. 2108.09895v1 null\nVisual Localization\nPublish Date Title Authors PDF Code\n2021-12-05 RADA: Robust Adversarial Data Augmentation for Camera Localization in Challenging Weather Jialu Wang et.al. 2112.02469v1 null\n2021-11-25 MegLoc: A Robust and Accurate Visual Localization Pipeline Shuxue Peng et.al. 2111.13063v1 null\n2021-10-08 Semantic Image Alignment for Vehicle Localization Markus Herb et.al. 2110.04162v1 null\n2021-10-05 Season-invariant GNSS-denied visual localization for UAVs Jouko Kinnari et.al. 2110.01967v1 link\n2021-09-30 Forming a sparse representation for visual place recognition using a neurorobotic approach Sylvain Colomer et.al. 2109.14916v1 null\n2021-09-22 Audio-Visual Grounding Referring Expression for Robotic Manipulation Yefei Wang et.al. 2109.10571v1 null\n2021-09-20 Efficient shape mapping through dense touch and vision Sudharshan Suresh et.al. 2109.09884v1 null\n2021-09-15 S3LAM: Structured Scene SLAM Mathieu Gonzalez et.al. 2109.07339v1 null\n2021-09-13 Monocular Camera Localization for Automated Vehicles Using Image Retrieval Eunhyek Joa et.al. 2109.06296v1 null\n2021-09-10 Line as a Visual Sentence: Context-aware Line Descriptor for Visual Localization Sungho Yoon et.al. 2109.04753v1 link\n2021-09-09 CrowdDriven: A New Challenging Dataset for Outdoor Visual Localization Ara Jafarzadeh et.al. 2109.04527v1 null\n2021-09-09 Keeping an Eye on Things: Deep Learned Features for Long-Term Visual Localization Mona Gridseth et.al. 2109.04041v1 null\nKeypoint Detection\nPublish Date Title Authors PDF Code\n2021-12-13 DenseGAP: Graph-Structured Dense Correspondence Learning with Anchor Points Zhengfei Kuang et.al. 2112.06910v1 null\n2021-12-12 Few-shot Keypoint Detection with Uncertainty Learning for Unseen Species Changsheng Lu et.al. 2112.06183v1 null\n2021-12-09 Few-Shot Keypoint Detection as Task Adaptation via Latent Embeddings Mel Vecerik et.al. 2112.04910v2 null\n2021-12-06 ALIKE: Accurate and Lightweight Keypoint Detection and Descriptor Extraction Xiaoming Zhao et.al. 2112.02906v1 link\n2021-11-25 Attend to Who You Are: Supervising Self-Attention for Keypoint Detection and Instance-Aware Association Sen Yang et.al. 2111.12892v1 link\n2021-11-08 Template NeRF: Towards Modeling Dense Shape Correspondences from Category-Specific Object Images Jianfei Guo et.al. 2111.04237v1 null\n2021-11-04 Voxel-based 3D Detection and Reconstruction of Multiple Objects from a Single Image Feng Liu et.al. 2111.03098v1 null\n2021-11-01 Learning Event-based Spatio-Temporal Feature Descriptors via Local Synaptic Plasticity: A Biologically-realistic Perspective of Computer Vision Ali Safa et.al. 2111.00791v2 null\n2021-10-30 Geometry-Aware Hierarchical Bayesian Learning on Manifolds Yonghui Fan et.al. 2111.00184v1 null\n2021-10-26 CoFiNet: Reliable Coarse-to-fine Correspondences for Robust Point Cloud Registration Hao Yu et.al. 2110.14076v1 link\n2021-10-23 HWTool: Fully Automatic Mapping of an Extensible C++ Image Processing Language to Hardware James Hegarty et.al. 2110.12106v1 null\n2021-10-18 Keypoint-Based Bimanual Shaping of Deformable Linear Objects under Environmental Constraints using Hierarchical Action Planning Shengzeng Huo et.al. 2110.08962v1 null\n2021-10-11 High-order Tensor Pooling with Attention for Action Recognition Piotr Koniusz et.al. 2110.05216v1 null\n2021-10-10 Digging Into Self-Supervised Learning of Feature Descriptors Iaroslav Melekhov et.al. 2110.04773v1 null\n2021-10-04 BPFNet: A Unified Framework for Bimodal Palmprint Alignment and Fusion Zhaoqun Li et.al. 2110.01179v1 link\n2021-10-01 Machine learning aided noise filtration and signal classification for CREDO experiment \u0141ukasz Bibrzycki et.al. 2110.00297v1 null\n2021-09-28 PDC-Net+: Enhanced Probabilistic Dense Correspondence Network Prune Truong et.al. 2109.13912v2 link\n2021-09-27 HarrisZ$^+$: Harris Corner Selection for Next-Gen Image Matching Pipelines Fabio Bellavia et.al. 2109.12925v3 null\n2021-09-24 Catadioptric Stereo on a Smartphone Kristijan Bartol et.al. 2109.11872v1 null\n2021-09-20 Semi-supervised Dense Keypointsusing Unlabeled Multiview Images Zhixuan Yu et.al. 2109.09299v1 null\n2021-08-31 A Novel Dataset for Keypoint Detection of quadruped Animals from Images Prianka Banik et.al. 2108.13958v1 link\n2021-08-27 A Matching Algorithm based on Image Attribute Transfer and Local Features for Underwater Acoustic and Optical Images Xiaoteng Zhou et.al. 2108.12151v1 null\nImage Matching\nPublish Date Title Authors PDF Code\n2021-12-10 More Control for Free! Image Synthesis with Semantic Diffusion Guidance Xihui Liu et.al. 2112.05744v2 null\n2021-12-08 Label-free virtual HER2 immunohistochemical staining of breast tissue using deep learning Bijie Bai et.al. 2112.05240v1 null\n2021-12-01 FaSS-MVS -- Fast Multi-View Stereo with Surface-Aware Semi-Global Matching from UAV-borne Monocular Imagery Boitumelo Ruf et.al. 2112.00821v1 null\n2021-12-01 CLIPstyler: Image Style Transfer with a Single Text Condition Gihyun Kwon et.al. 2112.00374v1 link\n2021-11-29 Nonlinear Intensity Underwater Sonar Image Matching Method Based on Phase Information and Deep Convolution Features Xiaoteng Zhou et.al. 2111.15514v1 null\n2021-11-29 Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic Yoad Tewel et.al. 2111.14447v1 link\n2021-11-29 Heterogeneous Visible-Thermal and Visible-Infrared Face Recognition using Unit-Class Loss and Cross-Modality Discriminator Usman Cheema et.al. 2111.14339v1 null\n2021-11-17 Probabilistic Spatial Distribution Prior Based Attentional Keypoints Matching Network Xiaoming Zhao et.al. 2111.09006v2 null\n2021-11-17 Nonlinear Intensity Sonar Image Matching based on Deep Convolution Features Xiaoteng Zhou et.al. 2111.08994v3 null\n2021-10-30 A Deep Search for Faint Chandra X-ray Sources, Radio Sources, and Optical Counterparts in NGC 6752 Haldan N. Cohn et.al. 2111.00357v1 null\n2021-10-01 Robustly Removing Deep Sea Lighting Effects for Visual Mapping of Abyssal Plains Kevin K\u00f6ser et.al. 2110.00480v1 null\n2021-09-29 Visually Grounded Concept Composition Bowen Zhang et.al. 2109.14115v1 null\n2021-09-27 HarrisZ$^+$: Harris Corner Selection for Next-Gen Image Matching Pipelines Fabio Bellavia et.al. 2109.12925v3 null\n2021-09-20 Viewpoint Invariant Dense Matching for Visual Geolocalization Gabriele Berton et.al. 2109.09827v1 link\n2021-09-20 Image Subtraction in Fourier Space Lei Hu et.al. 2109.09334v1 null\n2021-09-10 Line as a Visual Sentence: Context-aware Line Descriptor for Visual Localization Sungho Yoon et.al. 2109.04753v1 link\n2021-09-08 Matching in the Dark: A Dataset for Matching Image Pairs of Low-light Scenes Wenzheng Song et.al. 2109.03585v2 null\n2021-08-27 A Matching Algorithm based on Image Attribute Transfer and Local Features for Underwater Acoustic and Optical Images Xiaoteng Zhou et.al. 2108.12151v1 null\n2021-08-27 Matching Underwater Sonar Images by the Learned Descriptor Based on Style Transfer Method Xiaoteng Zhou et.al. 2108.12072v1 null\n2021-08-26 Efficient Joint Object Matching via Linear Programming Antonio De Rosa et.al. 2108.11911v1 null", "link": "https://github.com/Vincentqyw/cv-arxiv-daily", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "updated on 2021.12.16\nslam\npublish date title authors pdf code\n2021-12-15 homography decomposition networks for planar object tracking xinrui zhan et.al. 2112.07909v1 null\n2021-12-14 autonomous navigation system from simultaneous localization and mapping micheal caracciolo et.al. 2112.07723v1 link\n2021-12-12 360-dfpe: leveraging monocular 360-layouts for direct floor plan estimation bolivar solarte et.al. 2112.06180v1 null\n2021-12-11 simultaneous localization and mapping: through the lens of nonlinear optimization amay saxena et.al. 2112.05921v1 null\n2021-12-07 hybrid visual slam for underwater vehicle manipulator systems gideon billings et.al. 2112.03826v1 null\n2021-12-05 iterated posterior linearization pmb filter for 5g slam yu ge et.al. 2112.02575v1 null\n2021-12-03 fast direct stereo visual slam jiawei mo et.al. 2112.01890v1 link\n2021-12-02 megba: a high-performance and distributed library for large-scale bundle adjustment jie ren et.al. 2112.01349v2 link\n2021-12-01 research on event accumulator settings for event-based slam kun xiao et.al. 2112.00427v1 link\n2021-11-29 an in-depth experimental study of sensor usage and visual reasoning of robots navigating in real environments assem sadek et.al. 2111.14666v1 null\n2021-11-29 deployment of aerial robots after a major fire of an industrial hall with hazardous substances, a report hartmut surmann et.al. 2111.14542v1 null\n2021-11-24 automatic mapping with obstacle identification for indoor human mobility assessment v. ayala-alfaro et.al. 2111.12690v1 null\n2021-11-24 autonomous bot with ml-based reactive navigation for indoor environment yash srivastava et.al. 2111.12542v1 null\n2021-11-22 a general framework for lifelong localization and mapping in changing environment min zhao et.al. 2111.10946v1 link\n2021-11-17 probabilistic spatial distribution prior based attentional keypoints matching network xiaoming zhao et.al. 2111.09006v2 null\n2021-11-10 comparing dominance of tennis' big three via multiple-output bayesian quantile regression models bruno santos et.al. 2111.05631v1 null\n2021-11-10 tomoslam: factor graph optimization for rotation angle refinement in microtomography mark griguletskii et.al. 2111.05562v1 null\n2021-11-07 hierarchical segment-based optimization for slam yuxin tian et.al. 2111.04101v1 null\n2021-11-07 online mutual adaptation of deep depth prediction and visual slam shing yan loo et.al. 2111.04096v2 null\n2021-11-05 msc-vo: exploiting manhattan and structural constraints for visual odometry joan p. company-corcoles et.al. 2111.03408v1 null\n2021-10-31 loop closure detection using local 3d deep descriptors youjie zhou et.al. 2111.00440v1 null\n2021-10-27 millimeter wave wireless assisted robot navigation with link state classification mingsheng yin et.al. 2110.14789v2 link\n2021-10-27 efficient placard discovery for semantic mapping during frontier exploration david balaban et.al. 2110.14742v1 null\n2021-10-26 robust multi-view registration of point sets with laplacian mixture model jin zhang et.al. 2110.13744v1 null\n2021-10-25 wolf: a modular estimation framework for robotics based on factor graphs joan sola et.al. 2110.12919v1 null\n2021-10-21 real-time ground-plane refined lidar slam fan yang et.al. 2110.11517v1 null\n2021-10-21 symbiolcd: ensemble-based loop closure detection using cnn-extracted objects and visual bag-of-words jonathan j. y. kim et.al. 2110.11491v1 null\n2021-10-21 interpolationslam: a novel robust visual slam system in rotational motion zhenkun zhu et.al. 2110.11040v2 null\n2021-10-20 slam: a unified encoder for speech and language modeling via speech-text joint pre-training ankur bapna et.al. 2110.10329v1 null\n2021-10-18 enhancing exploration algorithms for navigation with visual slam kirill muravyev et.al. 2110.09156v1 null\n2021-10-18 accurate and robust object-oriented slam with 3d quadric landmark construction in -----> outdoor !!!  environment rui tian et.al. 2110.08977v1 null\n2021-10-16 partial hierarchical pose graph optimization for slam alexander korovko et.al. 2110.08639v1 null\n2021-10-14 active slam over continuous trajectory and control: a covariance-feedback approach shumon koga et.al. 2110.07546v1 null\n2021-10-13 collaborative radio slam for multiple robots based on wifi fingerprint similarity ran liu et.al. 2110.06541v2 null\n2021-10-12 learning efficient multi-agent cooperative visual exploration chao yu et.al. 2110.05734v1 null\n2021-10-07 self-supervised depth completion for active stereo frederik warburg et.al. 2110.03234v1 null\n2021-10-06 interpolationslam: a novel robust visual slam system in rotating scenes zhenkun zhu et.al. 2110.02593v1 null\n2021-10-03 aeros: adaptive robust least-squares for graph-based slam milad ramezani et.al. 2110.02018v1 null\n2021-10-04 fast uncertainty quantification for active graph slam julio a. placed et.al. 2110.01289v1 link\n2021-10-04 geometry-based graph pruning for lifelong slam gerhard kurz et.al. 2110.01286v1 null\n2021-10-03 quadrotor control on $su(2)\\times r^3$ with slam integration marcus greiff et.al. 2110.01099v1 null\n2021-10-02 online incremental non-gaussian inference for slam using normalizing flows qiangqiang huang et.al. 2110.00876v1 null\nsfm\npublish date title authors pdf code\n2021-12-10 critical configurations for three projective views martin br\u00e5telund et.al. 2112.05478v1 null\n2021-12-09 critical configurations for two projective views, a new approach martin br\u00e5telund et.al. 2112.05074v1 null\n2021-12-06 dense depth priors for neural radiance fields from sparse input views barbara roessle et.al. 2112.03288v1 null\n2021-12-02 megba: a high-performance and distributed library for large-scale bundle adjustment jie ren et.al. 2112.01349v2 link\n2021-11-11 multi-resolution elevation mapping and safe landing site detection with applications to planetary rotorcraft pascal schoppmann et.al. 2111.06271v1 null\n2021-11-05 damage estimation and localization from sparse aerial imagery rene garcia franceschini et.al. 2111.03708v2 null\n2021-11-03 event and activity recognition in video surveillance for cyber-physical systems swarnabja bhaumik et.al. 2111.02064v1 null\n2021-10-14 modeling dynamic target deformation in camera calibration annika hagemann et.al. 2110.07322v1 null\n2021-10-13 hyperspectral 3d mapping of underwater environments maxime ferrera et.al. 2110.06571v1 null\n2021-09-24 automatic map update using dashcam videos aziza zhanabatyrova et.al. 2109.12131v1 null\n2021-09-16 rotation averaging in a split second: a primal-dual method and a closed-form for cycle graphs gabriel moreira et.al. 2109.08046v1 link\n2021-09-06 single-camera 3d head fitting for mixed reality clinical applications tejas mane et.al. 2109.02740v1 null\n2021-09-02 dynamic scene novel view synthesis via deferred spatio-temporal consistency beatrix-em\u0151ke f\u00fcl\u00f6p-balogh et.al. 2109.01018v1 null\n2021-09-01 on the limits of pseudo ground truth in visual camera re-localisation eric brachmann et.al. 2109.00524v1 link\n2021-08-31 densepose 3d: lifting canonical surface maps of articulated objects to the third dimension roman shapovalov et.al. 2109.00033v1 null\n2021-08-29 solving viewing graph optimization for simultaneous position and rotation registration seyed-mahdi nasiri et.al. 2108.12876v1 null\n2021-08-23 burst imaging for light-constrained structure-from-motion ahalya ravendran et.al. 2108.09895v1 null\nvisual localization\npublish date title authors pdf code\n2021-12-05 rada: robust adversarial data augmentation for camera localization in challenging weather jialu wang et.al. 2112.02469v1 null\n2021-11-25 megloc: a robust and accurate visual localization pipeline shuxue peng et.al. 2111.13063v1 null\n2021-10-08 semantic image alignment for vehicle localization markus herb et.al. 2110.04162v1 null\n2021-10-05 season-invariant gnss-denied visual localization for uavs jouko kinnari et.al. 2110.01967v1 link\n2021-09-30 forming a sparse representation for visual place recognition using a neurorobotic approach sylvain colomer et.al. 2109.14916v1 null\n2021-09-22 audio-visual grounding referring expression for robotic manipulation yefei wang et.al. 2109.10571v1 null\n2021-09-20 efficient shape mapping through dense touch and vision sudharshan suresh et.al. 2109.09884v1 null\n2021-09-15 s3lam: structured scene slam mathieu gonzalez et.al. 2109.07339v1 null\n2021-09-13 monocular camera localization for automated vehicles using image retrieval eunhyek joa et.al. 2109.06296v1 null\n2021-09-10 line as a visual sentence: context-aware line descriptor for visual localization sungho yoon et.al. 2109.04753v1 link\n2021-09-09 crowddriven: a new challenging dataset for outdoor visual localization ara jafarzadeh et.al. 2109.04527v1 null\n2021-09-09 keeping an eye on things: deep learned features for long-term visual localization mona gridseth et.al. 2109.04041v1 null\nkeypoint detection\npublish date title authors pdf code\n2021-12-13 densegap: graph-structured dense correspondence learning with anchor points zhengfei kuang et.al. 2112.06910v1 null\n2021-12-12 few-shot keypoint detection with uncertainty learning for unseen species changsheng lu et.al. 2112.06183v1 null\n2021-12-09 few-shot keypoint detection as task adaptation via latent embeddings mel vecerik et.al. 2112.04910v2 null\n2021-12-06 alike: accurate and lightweight keypoint detection and descriptor extraction xiaoming zhao et.al. 2112.02906v1 link\n2021-11-25 attend to who you are: supervising self-attention for keypoint detection and instance-aware association sen yang et.al. 2111.12892v1 link\n2021-11-08 template nerf: towards modeling dense shape correspondences from category-specific object images jianfei guo et.al. 2111.04237v1 null\n2021-11-04 voxel-based 3d detection and reconstruction of multiple objects from a single image feng liu et.al. 2111.03098v1 null\n2021-11-01 learning event-based spatio-temporal feature descriptors via local synaptic plasticity: a biologically-realistic perspective of computer vision ali safa et.al. 2111.00791v2 null\n2021-10-30 geometry-aware hierarchical bayesian learning on manifolds yonghui fan et.al. 2111.00184v1 null\n2021-10-26 cofinet: reliable coarse-to-fine correspondences for robust point cloud registration hao yu et.al. 2110.14076v1 link\n2021-10-23 hwtool: fully automatic mapping of an extensible c++ image processing language to hardware james hegarty et.al. 2110.12106v1 null\n2021-10-18 keypoint-based bimanual shaping of deformable linear objects under environmental constraints using hierarchical action planning shengzeng huo et.al. 2110.08962v1 null\n2021-10-11 high-order tensor pooling with attention for action recognition piotr koniusz et.al. 2110.05216v1 null\n2021-10-10 digging into self-supervised learning of feature descriptors iaroslav melekhov et.al. 2110.04773v1 null\n2021-10-04 bpfnet: a unified framework for bimodal palmprint alignment and fusion zhaoqun li et.al. 2110.01179v1 link\n2021-10-01 machine learning aided noise filtration and signal classification for credo experiment \u0142ukasz bibrzycki et.al. 2110.00297v1 null\n2021-09-28 pdc-net+: enhanced probabilistic dense correspondence network prune truong et.al. 2109.13912v2 link\n2021-09-27 harrisz$^+$: harris corner selection for next-gen image matching pipelines fabio bellavia et.al. 2109.12925v3 null\n2021-09-24 catadioptric stereo on a smartphone kristijan bartol et.al. 2109.11872v1 null\n2021-09-20 semi-supervised dense keypointsusing unlabeled multiview images zhixuan yu et.al. 2109.09299v1 null\n2021-08-31 a novel dataset for keypoint detection of quadruped animals from images prianka banik et.al. 2108.13958v1 link\n2021-08-27 a matching algorithm based on image attribute transfer and local features for underwater acoustic and optical images xiaoteng zhou et.al. 2108.12151v1 null\nimage matching\npublish date title authors pdf code\n2021-12-10 more control for free! image synthesis with semantic diffusion guidance xihui liu et.al. 2112.05744v2 null\n2021-12-08 label-free virtual her2 immunohistochemical staining of breast tissue using deep learning bijie bai et.al. 2112.05240v1 null\n2021-12-01 fass-mvs -- fast multi-view stereo with surface-aware semi-global matching from uav-borne monocular imagery boitumelo ruf et.al. 2112.00821v1 null\n2021-12-01 clipstyler: image style transfer with a single text condition gihyun kwon et.al. 2112.00374v1 link\n2021-11-29 nonlinear intensity underwater sonar image matching method based on phase information and deep convolution features xiaoteng zhou et.al. 2111.15514v1 null\n2021-11-29 zero-shot image-to-text generation for visual-semantic arithmetic yoad tewel et.al. 2111.14447v1 link\n2021-11-29 heterogeneous visible-thermal and visible-infrared face recognition using unit-class loss and cross-modality discriminator usman cheema et.al. 2111.14339v1 null\n2021-11-17 probabilistic spatial distribution prior based attentional keypoints matching network xiaoming zhao et.al. 2111.09006v2 null\n2021-11-17 nonlinear intensity sonar image matching based on deep convolution features xiaoteng zhou et.al. 2111.08994v3 null\n2021-10-30 a deep search for faint chandra x-ray sources, radio sources, and optical counterparts in ngc 6752 haldan n. cohn et.al. 2111.00357v1 null\n2021-10-01 robustly removing deep sea lighting effects for visual mapping of abyssal plains kevin k\u00f6ser et.al. 2110.00480v1 null\n2021-09-29 visually grounded concept composition bowen zhang et.al. 2109.14115v1 null\n2021-09-27 harrisz$^+$: harris corner selection for next-gen image matching pipelines fabio bellavia et.al. 2109.12925v3 null\n2021-09-20 viewpoint invariant dense matching for visual geolocalization gabriele berton et.al. 2109.09827v1 link\n2021-09-20 image subtraction in fourier space lei hu et.al. 2109.09334v1 null\n2021-09-10 line as a visual sentence: context-aware line descriptor for visual localization sungho yoon et.al. 2109.04753v1 link\n2021-09-08 matching in the dark: a dataset for matching image pairs of low-light scenes wenzheng song et.al. 2109.03585v2 null\n2021-08-27 a matching algorithm based on image attribute transfer and local features for underwater acoustic and optical images xiaoteng zhou et.al. 2108.12151v1 null\n2021-08-27 matching underwater sonar images by the learned descriptor based on style transfer method xiaoteng zhou et.al. 2108.12072v1 null\n2021-08-26 efficient joint object matching via linear programming antonio de rosa et.al. 2108.11911v1 null", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000548, "year": null}, {"Unnamed: 0": 1662, "autor": 642, "date": null, "content": "Visual-Based-Localization-Papers\nThe camera re-localization task aims to estimate the 6-DoF pose of a novel (unseen) frame in the coordinate system given by the prior model of the world. The most related academic topics are SLAM and SfM and it's widely applied in AR, Robotic, etc.\nFeel free to make a PR or contribute. \ud83d\ude04\nTable of Contents\nsurvey\nsystem\ndirect methods\nfeature extraction\nfeature match\nretrieval\nrobust pose estimation\nmulti-sensors fusion\nslam\nsfm\nwaiting to sort\nSurvey\n[Image-based camera localization: an overview] Yihong Wu. Visual Computing for Industry, Biomedicine, and Art, 2018. [paper]\nSystem\n[Wide area localization on mobile phones] Clemens Arth. ISMAR, 2009. [paper]\n[Parallel Tracking and Mapping on a Camera Phone] ISMAR, 2009. [paper]\n[Real-time self-localization from panoramic images on mobile devices] Clemens Arth. ISMAR, 2011. [paper]\n[Scalable 6-DOF Localization on Mobile Devices] Iven Middelberg, Torsten Sattler. ECCV, 2014. [paper]\n[6D dynamic camera relocalization from single reference image] Feng W. CVPR 2016. [paper]\n[Image Matching Across Wide Baselines: From Paper to Practice] Yuehe, Jin. CVPR, 2020. [paper] [code]\n[GN-Net: The Gauss-Newton Loss for Multi-Weather Relocaliza-tion] L. von Stumberg, P. Wenzel, Q. Khan, and D. Cremers. ICRA, 2020. [paper] [code]\n[Using Image Sequences for Long-Term Visual Localization] Erik Stenborg, Torsten Sattler and Lars Hammarstrand. 3DV, 2020. [paper] [code]\n[LM-Reloc: Levenberg-Marquardt Based Direct Visual Relocalization] Lukas von Stumberg, Patrick Wenzel, Nan Yang, Daniel Cremers. 3DV, 2020. [paper]\n[Efficient 2D-3D Matching for Multi-Camera Visual Localization] Marcel Geppert, Peidong Liu, Zhaopeng Cui, Marc Pollefeys, Torsten Sattler. ICRL, 2020. [paper]\n[KFNet: Learning Temporal Camera Relocalization using Kalman Filtering] Lei Zhou, Zixin Luo, Tianwei Shen... CVPR, 2020 [paper]\n[Robust Neural Routing Through Space Partitions for Camera Relocalizationin Dynamic Indoor Environments] Siyan Dong, Qingnan Fan... CVPR, 2021, oral [paper]\nDirect Method\n[PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization] A. Kendall, M. ICCV, 2015. [code][paper]\nFeature Extracting\n[Semantic Visual Localization] J. L. Sch\ufffdonberger. CVPR, 2018. [paper]\n[R2D2: Repeatable and Reliable Detector and Descriptor] Jerome Revaud. NeurIPS, 2019. [paper][code]\n[Learning Feature Descriptors using Camera Pose Supervision] Qianqian Wang. ECCV, 2020, oral. [paper] [code]\n[ASLFeat: Learning Local Features of Accurate Shape and Localization] Zixin, Lup. CVPR, 2020. [paper] [code]\n[DISK: learning local features with policy gradient] Micha\u0142 J. Tyszkiewicz. NeurIPS, 2020. [paper] [code]\n[Reinforced Feature Points: Optimizing Feature Detection and Description for a High-Level Task] Aritra Bhowmik... CVPR oral, 2020. [paper][code]\n[FisheyeSuperPoint: Keypoint Detection andDescription Network for Fisheye Images] Anna Konrad. 2021. [paper]\nFeature Matching\n[Learning to Find Good Correspondences] Kwang Moo Yi. CVPR, 2018, oral. [paper] [code]\n[OANet: Learning Two-View Correspondences and Geometry Using Order-Aware Network] Zhang, Jiahui and Sun. ICCV, 2019. [paper] [code]\n[ACNe: Attentive Context Normalization for Robust Permutation Equivariant Learning] Sun, W. CVPR, 2020. [paper] [code]\n[Is there anything new to say about SIFT matching?] Fabio Bellavia. IJCV, 2020. [paper]\n[Deep Keypoint-Based Camera Pose Estimation with Geometric Constraints] You-Yi Jau, Rui Zhu. IROS, 2020 [paper] [code]\n[SuperGlue: Learning Feature Matching with Graph Neural Networks] Paul-Edouard Sarlin. CVPR, 2020. [paper] [code]\n[LoFTR: Detector-Free Local Feature Matching with Transformers] Jiaming Sun, Zehong Shen, Yu'ang Wang. CVPR, 2021. [paper] [code]\n[COTR: Correspondence Transformer for Matching Across Images] Wei Jiang. ICCV, 2021. [paper] [code]\n[Patch2Pix for Accurate Image Correspondence Estimation] Qunjie Zhou, Torsten Sattle, Laura Leal-Taix \u0301e. CVPR, 2021. [paper][code]\n[DFM: A Performance Baseline for Deep Feature Matching] Ufuk Efe, Kutalmis Gokalp Ince, A. Aydin Alatan. CVPR, 2021 [paper]\n[Back to the Feature: Learning Robust Camera Localization from Pixels to Pose] Paul-Edouard Sarlin. CVPR, 2021 [paper] [code]\n[Cross-Descriptor Visual Localization and Mapping] Mihai Dusmanu. ICCV, 2021 [paper] [code]\nRetrieval Methods\n[Visual Categorization with Bags of Keypoints] G. Csurka. ECCV, 2004. [paper]\n[Total Recall: Automatic Query Expansion with a Generative Feature Model for Object Retrieval] Chum, O. ICCV, 2007. [paper]\n[Fisher Kernels on Visual Vocabularies for Image Categorization] F. Perronnin and C. Dance. CVPR, 2007. [paper]\n[Aggregating Local Descriptors Into a Compact Image Representation] H. Jegou. CVPR, 2010. [paper]\n[Fast image-based localization using direct 2D to-3D matching] Sattler T. ICCV, 2011. [paper] [code]\n[Improving image-based localization by active correspondence search] ECCV, 2012. [paper]\n[Aggregating Deep Convolutional Features for Image Retrieval] A. Babenko and V. Lempitsky. ICCV, 2015. [paper]\n[A Vote-and-Verify Strategy for Fast Spatial Verification in Image Retrieval] Johannes L. Sch\u00a8onberger. ACCV, 2016. [paper] [code]\n[NetVLAD: CNN Architecture for Weakly Supervised Place Recognition] R. Arandjelovic. CVPR, 2016. [paper]\n[Crossdimensional Weighting for Aggregated Deep Convolutional Features] Y. Kalantidis. ECCV, 2016. [paper] [code]\n[Fine-Tuning CNN Image Retrieval with no Human Annotation] F. Radenovic, G. PAMI, 2017. [paper] [code]\n[Efficient diffusion on region manifolds: Recovering small objects with compact cnn representations] A. Iscen. CVPR, 2017. [paper]\n[Revisiting Oxford and Paris: Large-scale Image Retrieval Benchmarking] F. Radenovic, G. CVPR, 2018. [paper]\n[Learning with Average Precision: Training Image Retrieval with a Listwise Loss] J. Revaud. ICCV, 2019. [paper]\n[Benchmarking Image Retrieval for Visual Localization] No\u00e9 Pion,..., Torsten Sattler. 3DV, 2020. [paper] [code]\nRobust Pose Estimation\n[Fixing the Locally Optimized RANSAC] Karel Lebeda. BMVC, 2012. [paper]\n[Camera Pose Voting for Large-Scale Image-Based Localization] B. Zeisl, T. Sattler. ICCV, 2015. [paper]\n[City-Scale Localization for Cameras with Known Vertical Direction] Linus Svarm. TPAMI, 2016. [paper]\n[DSAC - Differentiable RANSAC for Camera Localization] E.Brachmann. CVPR, 2017. [code][paper]\n[MAGSAC: marginalizing sample consensus] Barath, D. CVPR, 2019. [paper]\n[GC-RANSAC: Graph-Cut RANSAC] Daniel Barath, Jiri Matas. CVPR, 2020. [paper][code]\n[AdaLAM: Revisiting Handcrafted Outlier Detection] Luca Cavalli... ECCV, 2020. [paper][code]\n[DegenSac] 2021 [code]\n[Learning Bipartite Graph Matching for Robust Visual Localization] Hailin Yu, Weicai Ye. ISMAR, 2020. [paper]\nMulti-sensors Fusion\nFusion with IMU\n[DARNavi: An Indoor-Outdoor Immersive Navigation System with Augmented Reality] Didi Chuxing. CVPR, 2020. [paper]\nFusion with GPS\n[Multi-sensor navigation algorithm using monocular camera, imu and gps for large scale augmented reality] T. Oskiper. ISMAR, 2012. [paper]\n[Gomsf: Graph-optimization based multi-sensor fusion for robust uav pose estimation] R. Mascaro, L. ICRA 2018. [paper]\n[Intermittent GPS-aided VIO: Online Initialization and Calibration] Woosik Lee. ICRA, 2020. [paper]\n[Vins Fusion \u2014 A General Optimization-based Framework for Global Pose Estimation with Multiple Sensors] Tong Qin. [paper]\nSLAM\n[Towards SLAM-based Outdoor Localization using Poor GPS and 2.5D Building Models] R.Liu et al. ISMAR, 2019. [code] [paper]\n[Neural Topological SLAM for Visual Navigation] Devendra Singh Chaplot. CVPR, 2020. [paper]\n[TANDEM: Tracking and Dense Mapping in Real-time using Deep Multi-view Stereo] Lukas Koestler et al. CoRL, 2021. [paper] [code]\nSfM\n[DPSNet: End-to-end Deep Plane Sweep Stereo] Sunghoon Im. ICLR, 2019. [paper]\n[Consistent Video Depth Estimation] XUAN LUO. SIGGRAPH 2020. [paper]\n[DeepSFM: Structure From Motion Via Deep Bundle Adjustment] ECCV 2020. [paper]\n[Multi-View Optimization of Local Feature Geometry] Mihai Dusmanu et al. ECCV 2020. [paper] [paper]\n[Deepv2d: Video to depth with differentiable structure from motion] Zachary Teed, Jia Deng. ICLR, 2020. [paper]\n[Hybrid Rotation Averaging: A Fast and Robust Rotation Averaging Approach] Yu Chen. CVPR, 2021. [paper] [code]\n[Pixel-Perfect Structure-from-Motion with Featuremetric Refinement] Philipp Lindenberger,* Paul-Edouard Sarlin,* Viktor Larsson, Marc Pollefeys. ICCV, oral, 2021. [paper][code]\nWaiting to sort\n[FREAK:Fast Retina Keypoint.] A. Amit. CVPR, 2012.\n[Three things evereyone should know to improve object retrieval] R. Arandjelovic. CVPR, 2012.\n[Learning local feature descriptors with triplets and shallow convolutional neural networks] V. Balntas. BMVC, 2016.\n[Learning 6D Object Pose Estimation Using 3D Objet Coordinates] E.Brachmann. ECCV, 2014.\n[Discriminative Learning of Local Image Descriptors] TPAMI, 2011.\n[MatchNet: Unifying feature and metric learning for patch-based matching] X. Han. CVPR, 2015.\n[Comparative evaluation of binary features] J. Heinly. ECCV.\n[LIFT: Learned Invariant Feature Transform] M.Kwang. ECCV, 2016.", "link": "https://github.com/Johnzdh/awesome-visual-localization-papers", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "visual-based-localization-papers\nthe camera re-localization task aims to estimate the 6-dof pose of a novel (unseen) frame in the coordinate system given by the prior model of the world. the most related academic topics are slam and sfm and it's widely applied in ar, robotic, etc.\nfeel free to make a pr or contribute. \ud83d\ude04\ntable of contents\nsurvey\nsystem\ndirect methods\nfeature extraction\nfeature match\nretrieval\nrobust pose estimation\nmulti-sensors fusion\nslam\nsfm\nwaiting to sort\nsurvey\n[image-based camera localization: an overview] yihong wu. visual computing for industry, biomedicine, and art, 2018. [paper]\nsystem\n[wide area localization on mobile phones] clemens arth. ismar, 2009. [paper]\n[parallel tracking and mapping on a camera phone] ismar, 2009. [paper]\n[real-time self-localization from panoramic images on mobile devices] clemens arth. ismar, 2011. [paper]\n[scalable 6-dof localization on mobile devices] iven middelberg, torsten sattler. eccv, 2014. [paper]\n[6d dynamic camera relocalization from single reference image] feng w. cvpr 2016. [paper]\n[image matching across wide baselines: from paper to practice] yuehe, jin. cvpr, 2020. [paper] [code]\n[gn-net: the gauss-newton loss for multi-weather relocaliza-tion] l. von stumberg, p. wenzel, q. khan, and d. cremers. icra, 2020. [paper] [code]\n[using image sequences for long-term visual localization] erik stenborg, torsten sattler and lars hammarstrand. 3dv, 2020. [paper] [code]\n[lm-reloc: levenberg-marquardt based direct visual relocalization] lukas von stumberg, patrick wenzel, nan yang, daniel cremers. 3dv, 2020. [paper]\n[efficient 2d-3d matching for multi-camera visual localization] marcel geppert, peidong liu, zhaopeng cui, marc pollefeys, torsten sattler. icrl, 2020. [paper]\n[kfnet: learning temporal camera relocalization using kalman filtering] lei zhou, zixin luo, tianwei shen... cvpr, 2020 [paper]\n[robust neural routing through space partitions for camera relocalizationin dynamic indoor environments] siyan dong, qingnan fan... cvpr, 2021, oral [paper]\ndirect method\n[posenet: a convolutional network for real-time 6-dof camera relocalization] a. kendall, m. iccv, 2015. [code][paper]\nfeature extracting\n[semantic visual localization] j. l. sch\ufffdonberger. cvpr, 2018. [paper]\n[r2d2: repeatable and reliable detector and descriptor] jerome revaud. neurips, 2019. [paper][code]\n[learning feature descriptors using camera pose supervision] qianqian wang. eccv, 2020, oral. [paper] [code]\n[aslfeat: learning local features of accurate shape and localization] zixin, lup. cvpr, 2020. [paper] [code]\n[disk: learning local features with policy gradient] micha\u0142 j. tyszkiewicz. neurips, 2020. [paper] [code]\n[reinforced feature points: optimizing feature detection and description for a high-level task] aritra bhowmik... cvpr oral, 2020. [paper][code]\n[fisheyesuperpoint: keypoint detection anddescription network for fisheye images] anna konrad. 2021. [paper]\nfeature matching\n[learning to find good correspondences] kwang moo yi. cvpr, 2018, oral. [paper] [code]\n[oanet: learning two-view correspondences and geometry using order-aware network] zhang, jiahui and sun. iccv, 2019. [paper] [code]\n[acne: attentive context normalization for robust permutation equivariant learning] sun, w. cvpr, 2020. [paper] [code]\n[is there anything new to say about sift matching?] fabio bellavia. ijcv, 2020. [paper]\n[deep keypoint-based camera pose estimation with geometric constraints] you-yi jau, rui zhu. iros, 2020 [paper] [code]\n[superglue: learning feature matching with graph neural networks] paul-edouard sarlin. cvpr, 2020. [paper] [code]\n[loftr: detector-free local feature matching with transformers] jiaming sun, zehong shen, yu'ang wang. cvpr, 2021. [paper] [code]\n[cotr: correspondence transformer for matching across images] wei jiang. iccv, 2021. [paper] [code]\n[patch2pix for accurate image correspondence estimation] qunjie zhou, torsten sattle, laura leal-taix \u0301e. cvpr, 2021. [paper][code]\n[dfm: a performance baseline for deep feature matching] ufuk efe, kutalmis gokalp ince, a. aydin alatan. cvpr, 2021 [paper]\n[back to the feature: learning robust camera localization from pixels to pose] paul-edouard sarlin. cvpr, 2021 [paper] [code]\n[cross-descriptor visual localization and mapping] mihai dusmanu. iccv, 2021 [paper] [code]\nretrieval methods\n[visual categorization with bags of keypoints] g. csurka. eccv, 2004. [paper]\n[total recall: automatic query expansion with a generative feature model for object retrieval] chum, o. iccv, 2007. [paper]\n[fisher kernels on visual vocabularies for image categorization] f. perronnin and c. dance. cvpr, 2007. [paper]\n[aggregating local descriptors into a compact image representation] h. jegou. cvpr, 2010. [paper]\n[fast image-based localization using direct 2d to-3d matching] sattler t. iccv, 2011. [paper] [code]\n[improving image-based localization by active correspondence search] eccv, 2012. [paper]\n[aggregating deep convolutional features for image retrieval] a. babenko and v. lempitsky. iccv, 2015. [paper]\n[a vote-and-verify strategy for fast spatial verification in image retrieval] johannes l. sch\u00a8onberger. accv, 2016. [paper] [code]\n[netvlad: cnn architecture for weakly supervised place recognition] r. arandjelovic. cvpr, 2016. [paper]\n[crossdimensional weighting for aggregated deep convolutional features] y. kalantidis. eccv, 2016. [paper] [code]\n[fine-tuning cnn image retrieval with no human annotation] f. radenovic, g. pami, 2017. [paper] [code]\n[efficient diffusion on region manifolds: recovering small objects with compact cnn representations] a. iscen. cvpr, 2017. [paper]\n[revisiting oxford and paris: large-scale image retrieval benchmarking] f. radenovic, g. cvpr, 2018. [paper]\n[learning with average precision: training image retrieval with a listwise loss] j. revaud. iccv, 2019. [paper]\n[benchmarking image retrieval for visual localization] no\u00e9 pion,..., torsten sattler. 3dv, 2020. [paper] [code]\nrobust pose estimation\n[fixing the locally optimized ransac] karel lebeda. bmvc, 2012. [paper]\n[camera pose voting for large-scale image-based localization] b. zeisl, t. sattler. iccv, 2015. [paper]\n[city-scale localization for cameras with known vertical direction] linus svarm. tpami, 2016. [paper]\n[dsac - differentiable ransac for camera localization] e.brachmann. cvpr, 2017. [code][paper]\n[magsac: marginalizing sample consensus] barath, d. cvpr, 2019. [paper]\n[gc-ransac: graph-cut ransac] daniel barath, jiri matas. cvpr, 2020. [paper][code]\n[adalam: revisiting handcrafted outlier detection] luca cavalli... eccv, 2020. [paper][code]\n[degensac] 2021 [code]\n[learning bipartite graph matching for robust visual localization] hailin yu, weicai ye. ismar, 2020. [paper]\nmulti-sensors fusion\nfusion with imu\n[darnavi: an indoor------> outdoor !!!  immersive navigation system with augmented reality] didi chuxing. cvpr, 2020. [paper]\nfusion with gps\n[multi-sensor navigation algorithm using monocular camera, imu and gps for large scale augmented reality] t. oskiper. ismar, 2012. [paper]\n[gomsf: graph-optimization based multi-sensor fusion for robust uav pose estimation] r. mascaro, l. icra 2018. [paper]\n[intermittent gps-aided vio: online initialization and calibration] woosik lee. icra, 2020. [paper]\n[vins fusion \u2014 a general optimization-based framework for global pose estimation with multiple sensors] tong qin. [paper]\nslam\n[towards slam-based outdoor localization using poor gps and 2.5d building models] r.liu et al. ismar, 2019. [code] [paper]\n[neural topological slam for visual navigation] devendra singh chaplot. cvpr, 2020. [paper]\n[tandem: tracking and dense mapping in real-time using deep multi-view stereo] lukas koestler et al. corl, 2021. [paper] [code]\nsfm\n[dpsnet: end-to-end deep plane sweep stereo] sunghoon im. iclr, 2019. [paper]\n[consistent video depth estimation] xuan luo. siggraph 2020. [paper]\n[deepsfm: structure from motion via deep bundle adjustment] eccv 2020. [paper]\n[multi-view optimization of local feature geometry] mihai dusmanu et al. eccv 2020. [paper] [paper]\n[deepv2d: video to depth with differentiable structure from motion] zachary teed, jia deng. iclr, 2020. [paper]\n[hybrid rotation averaging: a fast and robust rotation averaging approach] yu chen. cvpr, 2021. [paper] [code]\n[pixel-perfect structure-from-motion with featuremetric refinement] philipp lindenberger,* paul-edouard sarlin,* viktor larsson, marc pollefeys. iccv, oral, 2021. [paper][code]\nwaiting to sort\n[freak:fast retina keypoint.] a. amit. cvpr, 2012.\n[three things evereyone should know to improve object retrieval] r. arandjelovic. cvpr, 2012.\n[learning local feature descriptors with triplets and shallow convolutional neural networks] v. balntas. bmvc, 2016.\n[learning 6d object pose estimation using 3d objet coordinates] e.brachmann. eccv, 2014.\n[discriminative learning of local image descriptors] tpami, 2011.\n[matchnet: unifying feature and metric learning for patch-based matching] x. han. cvpr, 2015.\n[comparative evaluation of binary features] j. heinly. eccv.\n[lift: learned invariant feature transform] m.kwang. eccv, 2016.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000642, "year": null}, {"Unnamed: 0": 1743, "autor": 723, "date": null, "content": "Awesome Robotics and Artificially Intelligence Agent Companies\nA List of Companies, Both Large and Small, That Are Stable and Developing Robots or Autonomous Intelligent Agents\nRobotics is awesome, and people who think robotics is awesome are awesome, so I have a decided to put together a living list of important industry innovators in robotics and artificially intelligent agents. This is intended to be a starting point; there are many other cool companies out there that are just getting started! If you want a company added to this list, or think something needs to be adjusted, please a pull request (see the contributing guidelines)!\nA NOTE ABOUT ARTIFICIAL INTELLIGENT AGENTS: A lot of companies are saying that they have \"artificial intelligence\" at this point, including many on this list. However, this list is restricted to those companies that work on \"artificially intelligent agency\" or \"agent intelligence;\" that is, the products they produce and services the sell use artificial intelligence or machine learning to enable autonomous agents that make their own decisions. I make no claim that there is one correct use of the term \"AI,\" but this is not to be confused with the umbrella term \"AI\" being used by businesses to include generic machine learning, data analysis, and data visionalization services that enable human decision makers (to say nothing of the services that involve none of the above). Occasionally this list will include companies focused on technology that supports or directly applies to artificially intelligent agents (for example those working on dialogue, 3D mapping, or computer vision), but I generally try to discourage this.\nAwesome Robotics and Intelligent Agent Companies\nLarge Companies\nStable Start-ups\nNorth American Startups (by locale)\nBoston\nSan Francisco Bay Area\nBoulder Area\nPittsburgh Area\nAustin Area\nNew York\nOther\nInternational Startups\nNon-Profits and Government Research Labs\nCredits\nLarge Companies\nAll locations listed are primary locations for engineering development, so the locations lists will be representative but are not guaranteed to be complete.\nQualcomm\nAutonomous Driving (Locations: Philadelphia, USA; San Diego, USA)\nAutonomous Unmanned Aerial Vehicles (Locations: Philadelphia, USA)\nNVIDIA\nDeep Learning/AI (Locations: Santa Clara, USA; Seattle, USA)\nComputer Vision\nSimulated Robotic Control\nManipulation\nTesla\nAutonomous Driving (Locations: Palo Alto, USA; Fremont, USA; McCarren, USA; St. Leonards, AUS; Beijing, CHN; Hong Kong, CHN; Tokyo, JPN)\nMicrosoft\nMicrosoft Research (Locations: Palo Alto, USA; Redmond, USA; Seattle, USA; Cambridge, USA; New York, USA; Montreal, CAN; Beijing, CHN)\nDeep Learning/AI Research\nComputer Vision\nRobotics\nMicrosoft Cortana (Location: Seattle, USA; Cambridge, USA)\nIntelligent disembodied agents\nAzure (Locations: Seattle, USA)\nDeep Learning/AI Research\nFacebook\nFAIR (Location: Menlo Park, USA; Seattle, USA; Pittsburgh, USA)\nDeep Learning/AI Research\nReinforcement learning\nRobotics\nFRL (formerly Occulus Research) (Location: Seattle, USA)\nIntelligent virtual agents\nAlphabet (Company Outline)\nGoogle X (Locations: Mountain View, USA; Undisclosed)\nAutonomous Unmanned Aerial Vehicles\nAutonomous Driving\nGeneral Robotics\nGoogle AI (Locations: Mountain View, USA; New York, USA; Cambridge, USA; Seattle, USA)\nDeep Learning/AI Research\nReinforcement learning\nRobotics\nDeepmind (Locations: Mountain View, USA; London, GBR)\nDeep Learning/AI Research\nGoogle Assistant (Locations: Mountain View, USA; New York, USA)\nIntelligent disembodied agents\nGoogle GCP (Locations: Mountain View, USA)\nDeep Learning/AI Research\nHonda\nHonda Research Institute (Location: Tokyo, JP; Palo Alto, USA; Colombus, USA)\nGeneral Robotics\nAutonomous Driving\nIntelligent virtual assistants\nToyota (Locations: Tokyo, JPN; Los Altos, USA; Ann Arbor, USA; Cambridge, USA)\nToyota Research Institute\nGeneral Robotics\nAutonomous Driving\nAmazon\nAmazon Robotics (Formerly Kiva Systems) (Locations: Westborough, USA; Seattle, USA)\nWarehouse and fulfillment robotics and automation\nManipulation\nAutonomous Unmanned Aerial Vehicles\nAmazon Scout (Locations: Seattle, USA)\nAutonomous driving and delivery\nAWS AI (Locations: Seattle, USA)\nDeep Learning/AI\nAlexa (Locations: Seattle, USA; New York, USA)\nIntelligent disembodied agents\nLab 126\nSmart Devies\nGo (store)\nComputer Vision\nMachine learning\nSensor fusion\nPrime Air\nAutonomous Unmanned Aerial Videos\nAutonomous driving and delivery\nSoftbank\nSoftbank Robotics (Tokyo, JPN; Paris, FRA)\nGeneral Robotics\nHRI and Assistive Robotics\nDyson (Location: Malmesbury, GBR; Singapore)\nHousehold cleaning robotics\nSLAM/Localisation\nComputer Vision (image processing, object detection/recognition)\niRobot (Location: Boston, USA)\nHousehold cleaning robotics\nOutdoor robotics\nTelepresence\nSLAM/Localisation\nComputer Vision (image processing, object detection/recognition)\nEducational robotics\nEcovacs (Location: Suzhou, CHN)\nHousehold cleaning robotics\nABB Robotics (Location: Zurich, CHE)\nManipulation\nIndustrial Robotics\nFANUC (Location: Oshino, JPN)\nManipulation\nIndustrial robotics\nDJI (Locations: Shenzhen, CHN; Los Angeles, USA)\nAutonomous Unmanned Aerial Vehicles\nSmart cameras\nComputer vision\nApple (Locations: Seattle, USA; Cupertino, USA)\nAI Research\nComputer vision\nAutonomous Driving (Special Problems Group)\nMedtronic\nHealthcare robotic devices (Locations: Frindley, USA; Dublin, IRL)\nRobotic surgery (Locations: Caeserea, ISR)\nIntuitive Surgical\nHealthcare robotic devices (Locations: Sunnyvale, USA)\nRobotic surgery (Locations: Sunnyvale, USA)\nHuman robot interaction research (Locations: Sunnyvale, USA; Atlanta, USA)\nUber\nUber Advanced Technologies Group (Locations: Pittsburgh, USA; Toronto, CAN; Palo Alto, USA)\nAutonomous driving\nComputer Vision\nUber AI (Locations: Palo Alto, USA; Toronto, CAN)\nAutonomous driving\nComputer Vision\nRobotics research\nReinforcement learning\nAptiv (Locations: Boston, USA; Pittsburgh, USA; Krakow, POL; Kokomo, USA; Seoul, KOR)\nAutonomous driving\nSensor systems for autonomous vehicles\nUniversal Robots (Locations: Odense, DNK; Boston, USA; Tokyo, JPN)\nManipulation\nRobot mechanics and design\nIndustrial robotics\nKuka (Locations: Augsburg, DEU; Budapest, HUN; Austin, USA; Detroit, USA)\nManipulation\nRobot mechanics and design\nIndustrial robotics\nWarehouse and fulfillment robotics and automation\nMobility and planning\nSwisslog (Locations: Aarau, CHE; Dortmund, DEU; Denver, USA; San Francisco, USA)\nWarehouse and fulfillment robotics and automation\nHealthcare automation\nParent company: Kuka\nCruise (San Francisco, USA)\nAutonomous driving\nZoox (Location: Foster City, USA)\nAutonomous driving\nSamsung Research\nIntelligent disembodied agents\nComputer vision\nRobotics\nStable Start-ups\nThis list contains what I am calling start-ups, which is any company that isn't public, has a valuation at <$500M, has only a single primary location, and/or does not have a revenue stream that could result in stable profitability. \"Stable\" doesn't mean they cannot fail, but this list will not be used to keep track of every single effort as that would be exhuasting. If you want something like that, may I suggest CrunchBase and Robotics Business Review.\nAmerican Startups (by locale)\nBoston\nOptimus Ride\nAutonomous driving\nBoston Dynamics\nRobot mechanics and design\nRobot control\nAutonomy\nParent company: Softbank\nRealtime Robotics\nAutonomy\nRobotic control\nUser Interfaces\nHarvest Automation\nOutdoor mobile robotics\nAgricultural robotics\nWarehouse and fulfillment robotics and automation\nBarrett Technology\nRobot mechnics and design\nRobot control\nManipulation\nVeo Robotics\nIndustrial robotics\nHuman robot interaction\nManipulation\nComputer vision\nVecna Robotics\nManipulation\nWarehouse and fulfillment robotics and automation\n6 River Systems\nWarehouse and fulfillment robotics and automation\nRighthand Robotics\nManipulation\nRobot mechanics and design\nSea Machines\nAutonomous marine vehicles\nLocus Robotics\nWarehouse and fulfillment robotics and automation\nAva Robotics\nTelepresence\nSan Francisco Bay Area\nNuro\nAutonomous driving\nAutonomous delivery\nFetch Robotics\nRetail robotics\nWarehouse and fulfillment robotics and automation\nManipulation\nZenuity\nAutonomous driving\nAutoX\nAutonomous driving\nAlt Location: Shenzhen, CHN\nDeepscale AI\nAutonomous Driving\nDouble Robotics\nTelepresence\nCovariant AI\nRobotics\nManipulation\nComputer Vision\nReinforcement Learning\nAbundant Robotics\nAgriculture robotics\nBlue River Technology\nAgriculture Robotics\nOutdoor robotics\nParent company: John Deere\nNeato Robotics\nRobot vacuums\nSkydio\nAutonomous Unmanned Aerial Vehicles\nIron Ox\nAgriculture robotics\nLiquid Robotics\nMarine robotics\nRobot controls\nRobot research\nParent company: Boeing\nVicarious\nArtificial intelligence research\nIntelligent robot assistants\nSuitable Technologies\nTelepresence\nOrbital Insight\nComputer Vision\nAlt locations: Washington D.C., USA; Boston, USA\nSpace Know\nComputer Vision\nAurora Technologies\nAutonomous driving\nAlt Locations: Pittsburgh, USA; Bozeman, USA\nStarship Technologies\nAutonomous driving and delivery\nNextNav\nComputer vision\n3D Mapping\nDeepMap\nComputer vision\n3D Mapping\nFarmwise\nFarming robot\nPicks weeds/reduces pesticide use\n(Lemnos funded)\nBuilt Robotics\nEarth moving robots\nUse OEM machines from Caterpillar/Komatsu\nPhantom Auto\nSupports robots through teleoperation\nBoulder Area\nLeft Hand Robotics\nOutdoor robotics\nAgriculture Robotics\nAMP Robotics\nIntelligent autonomous recycling\nIndustrial robotics\nSphero/Misty Robotics\nRobot toys\nRobot companions\nValyant AI\nIntelligent disembodied agents\nBerkshire Grey\nManipulation\nWarehouse and fulfillment robotics and automation\nPittsburgh Area\nBossa Nova Robotics\nRetail robotics\nManipulation\nArgo AI\nAutonomous driving\nAstrobotic\nAutonomous spaceflight landers\nAutonomous planetary rovers\nRE2 Robotics\nResearch robotics\nManipulation\nSeegrid\nRetail robotics\nWarehouse and fulfillment robotics and automation\nComputer vision\n3D Mapping\nTybot\nTies rebar cages\nAustin Area\nDiligent Robotics\nHealthcare robotics\nPersonal robot assistants\nMaidbot\nHousehold robotics\nHuman robot interaction\nPensa Systems\nRetail robotics\nAutonomous Unmanned Aerial Vehicles\nNew York\nClarifai\nVideo/Image recognition\nEnterprise visual decision making\nAlt locations: San Francisco, USA; Washington D.C., USA\nCarmera\nComputer vision\n3D Mapping\nToggle\nRebar tying robot with robotic arm\nOther\nAutonomous Solutions (Location: Petersboro, UT, USA)\nMining robotics\nIndustrial robotics\nAutonomous driving\nInVia Robotics (Thousand Oaks, CA, USA)\nWarehouse and fulfillment robotics and automation\nIntelligent Automation, Inc. (Washington D.C., USA)\nResearch robotics\nAutomous inspection systems\nInternational Startups\nClearpath Robotics (Toronto, CAN)\nResearch robotics\nOutdoor robotics\nRobotic control\nUser Interfaces\nKinova (Location: Montreal, CAN)\nManipulation\nRobot mechanics and design\nMIR (Locations: Odense, DNK; New York, USA)\nWarehouse and fulfillment robotics and automation\nRapyuta Robotics (Locations: Tokyo, JPN; Zurich, CHE; Bangalore, IND)\nAutonomous Unmanned Aerial Vehicles\nCloud robotics\nFiveAI (Location: Cambridge, GBR)\nAutonomous Driving\nIris AI (Locations: Oslo, NOR; Berlin, DEU; Odessa, UKR; Sofia, BGR)\nAI-based autonomy for robotic science experimentation\nNaio Technologies (Location: Escalquens, FRA)\nAgricultural robotics\nFranka Emika (Location: Munich, DEU)\nManipulation\nRobot mechanics and design\nDorabot (Location: Shenzhen, CHN)\nManipulation\nMaterials handling robotics\nWarehouse and fulfillment robotics and automation\nAlt Locations: Atlanta, USA; Brisbane, AUS\nWayve (Location: Cambridge, GBR)\nAutonomous driving\nSaga Robotics (Oslo, NOR)\nAgricultural Robotics\nAlt Location: Lincoln, GBR\nNon-Profits and Government Research Labs\nOpenAI (Location: San Francisco, CA, USA)\nAutonomous Driving\nDeep Learning/General AI\nDeep Reinforcement Learning\nMachine Learning Theory\nComputer Vision\nAllen Institute for Artificial Intelligence (Location: Seattle, WA, USA)\nNatural language processing/semantics/question answering\nMachine learning theory\nKnowledge representation\nComputer vision\nOpen Robotics (Location: San Francisco, CA, USA; Singapore)\nRobotics software development\nRobotics Simulation\nFormerly: Open Source Robotics Foundation; Willow Garage\nArmy Research Laboratory (ARL) (Locatoin: Adelphi, MD, USA)\nAutonomous Driving\nComputer vision\nHuman Robot Interaction\nAdaptation/robustness to unknown or adversarial terrain\nNaval Resaerch Laboratory (NRL) (Location: Washinton D.C., USA)\nAutonomous Underwater unmanned vehicles\nAutonomous Unmanned Aerial Vehicles\nMarine robotics\nAutonomous Testing of Robotics\nHuman Robot Interaction\nArtificial Intelligence\nManipulation\nAir Force Research Laboratory (AFRL) (Location: Dayton, OH, USA)\nAutonomous Unmanned Aerial Vehicles\nComputer Vision\nNASA Jet Propulsion Laboratory (JPL) and Ames Laboratory (Location: Pasadena, CA, USA; Sunnyvale, CA, USA)\nSpace robotics\nAutonomous Unmanned Ground Vehicles\nAutonomous Unmanned Aerial Vehicles\nSelf-assembling structures\nComputer Vision\nHuman Robot Interaction\nCredits\nDesigned in accordance with Sindre's awesome superlist. Thanks Sindre! TOC from @ekalinin 's Easy TOC for Github README. Thanks Eugene!\nAnd thank you to all of the contributers on and off Github (mostly off at this point). Couldn't do it without you.", "link": "https://github.com/balloch/awesome-robotics-ai-companies", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "awesome robotics and artificially intelligence agent companies\na list of companies, both large and small, that are stable and developing robots or autonomous intelligent agents\nrobotics is awesome, and people who think robotics is awesome are awesome, so i have a decided to put together a living list of important industry innovators in robotics and artificially intelligent agents. this is intended to be a starting point; there are many other cool companies out there that are just getting started! if you want a company added to this list, or think something needs to be adjusted, please a pull request (see the contributing guidelines)!\na note about artificial intelligent agents: a lot of companies are saying that they have \"artificial intelligence\" at this point, including many on this list. however, this list is restricted to those companies that work on \"artificially intelligent agency\" or \"agent intelligence;\" that is, the products they produce and services the sell use artificial intelligence or machine learning to enable autonomous agents that make their own decisions. i make no claim that there is one correct use of the term \"ai,\" but this is not to be confused with the umbrella term \"ai\" being used by businesses to include generic machine learning, data analysis, and data visionalization services that enable human decision makers (to say nothing of the services that involve none of the above). occasionally this list will include companies focused on technology that supports or directly applies to artificially intelligent agents (for example those working on dialogue, 3d mapping, or computer vision), but i generally try to discourage this.\nawesome robotics and intelligent agent companies\nlarge companies\nstable start-ups\nnorth american startups (by locale)\nboston\nsan francisco bay area\nboulder area\npittsburgh area\naustin area\nnew york\nother\ninternational startups\nnon-profits and government research labs\ncredits\nlarge companies\nall locations listed are primary locations for engineering development, so the locations lists will be representative but are not guaranteed to be complete.\nqualcomm\nautonomous driving (locations: philadelphia, usa; san diego, usa)\nautonomous unmanned aerial vehicles (locations: philadelphia, usa)\nnvidia\ndeep learning/ai (locations: santa clara, usa; seattle, usa)\ncomputer vision\nsimulated robotic control\nmanipulation\ntesla\nautonomous driving (locations: palo alto, usa; fremont, usa; mccarren, usa; st. leonards, aus; beijing, chn; hong kong, chn; tokyo, jpn)\nmicrosoft\nmicrosoft research (locations: palo alto, usa; redmond, usa; seattle, usa; cambridge, usa; new york, usa; montreal, can; beijing, chn)\ndeep learning/ai research\ncomputer vision\nrobotics\nmicrosoft cortana (location: seattle, usa; cambridge, usa)\nintelligent disembodied agents\nazure (locations: seattle, usa)\ndeep learning/ai research\nfacebook\nfair (location: menlo park, usa; seattle, usa; pittsburgh, usa)\ndeep learning/ai research\nreinforcement learning\nrobotics\nfrl (formerly occulus research) (location: seattle, usa)\nintelligent virtual agents\nalphabet (company outline)\ngoogle x (locations: mountain view, usa; undisclosed)\nautonomous unmanned aerial vehicles\nautonomous driving\ngeneral robotics\ngoogle ai (locations: mountain view, usa; new york, usa; cambridge, usa; seattle, usa)\ndeep learning/ai research\nreinforcement learning\nrobotics\ndeepmind (locations: mountain view, usa; london, gbr)\ndeep learning/ai research\ngoogle assistant (locations: mountain view, usa; new york, usa)\nintelligent disembodied agents\ngoogle gcp (locations: mountain view, usa)\ndeep learning/ai research\nhonda\nhonda research institute (location: tokyo, jp; palo alto, usa; colombus, usa)\ngeneral robotics\nautonomous driving\nintelligent virtual assistants\ntoyota (locations: tokyo, jpn; los altos, usa; ann arbor, usa; cambridge, usa)\ntoyota research institute\ngeneral robotics\nautonomous driving\namazon\namazon robotics (formerly kiva systems) (locations: westborough, usa; seattle, usa)\nwarehouse and fulfillment robotics and automation\nmanipulation\nautonomous unmanned aerial vehicles\namazon scout (locations: seattle, usa)\nautonomous driving and delivery\naws ai (locations: seattle, usa)\ndeep learning/ai\nalexa (locations: seattle, usa; new york, usa)\nintelligent disembodied agents\nlab 126\nsmart devies\ngo (store)\ncomputer vision\nmachine learning\nsensor fusion\nprime air\nautonomous unmanned aerial videos\nautonomous driving and delivery\nsoftbank\nsoftbank robotics (tokyo, jpn; paris, fra)\ngeneral robotics\nhri and assistive robotics\ndyson (location: malmesbury, gbr; singapore)\nhousehold cleaning robotics\nslam/localisation\ncomputer vision (image processing, object detection/recognition)\nirobot (location: boston, usa)\nhousehold cleaning robotics\n-----> outdoor !!!  robotics\ntelepresence\nslam/localisation\ncomputer vision (image processing, object detection/recognition)\neducational robotics\necovacs (location: suzhou, chn)\nhousehold cleaning robotics\nabb robotics (location: zurich, che)\nmanipulation\nindustrial robotics\nfanuc (location: oshino, jpn)\nmanipulation\nindustrial robotics\ndji (locations: shenzhen, chn; los angeles, usa)\nautonomous unmanned aerial vehicles\nsmart cameras\ncomputer vision\napple (locations: seattle, usa; cupertino, usa)\nai research\ncomputer vision\nautonomous driving (special problems group)\nmedtronic\nhealthcare robotic devices (locations: frindley, usa; dublin, irl)\nrobotic surgery (locations: caeserea, isr)\nintuitive surgical\nhealthcare robotic devices (locations: sunnyvale, usa)\nrobotic surgery (locations: sunnyvale, usa)\nhuman robot interaction research (locations: sunnyvale, usa; atlanta, usa)\nuber\nuber advanced technologies group (locations: pittsburgh, usa; toronto, can; palo alto, usa)\nautonomous driving\ncomputer vision\nuber ai (locations: palo alto, usa; toronto, can)\nautonomous driving\ncomputer vision\nrobotics research\nreinforcement learning\naptiv (locations: boston, usa; pittsburgh, usa; krakow, pol; kokomo, usa; seoul, kor)\nautonomous driving\nsensor systems for autonomous vehicles\nuniversal robots (locations: odense, dnk; boston, usa; tokyo, jpn)\nmanipulation\nrobot mechanics and design\nindustrial robotics\nkuka (locations: augsburg, deu; budapest, hun; austin, usa; detroit, usa)\nmanipulation\nrobot mechanics and design\nindustrial robotics\nwarehouse and fulfillment robotics and automation\nmobility and planning\nswisslog (locations: aarau, che; dortmund, deu; denver, usa; san francisco, usa)\nwarehouse and fulfillment robotics and automation\nhealthcare automation\nparent company: kuka\ncruise (san francisco, usa)\nautonomous driving\nzoox (location: foster city, usa)\nautonomous driving\nsamsung research\nintelligent disembodied agents\ncomputer vision\nrobotics\nstable start-ups\nthis list contains what i am calling start-ups, which is any company that isn't public, has a valuation at <$500m, has only a single primary location, and/or does not have a revenue stream that could result in stable profitability. \"stable\" doesn't mean they cannot fail, but this list will not be used to keep track of every single effort as that would be exhuasting. if you want something like that, may i suggest crunchbase and robotics business review.\namerican startups (by locale)\nboston\noptimus ride\nautonomous driving\nboston dynamics\nrobot mechanics and design\nrobot control\nautonomy\nparent company: softbank\nrealtime robotics\nautonomy\nrobotic control\nuser interfaces\nharvest automation\noutdoor mobile robotics\nagricultural robotics\nwarehouse and fulfillment robotics and automation\nbarrett technology\nrobot mechnics and design\nrobot control\nmanipulation\nveo robotics\nindustrial robotics\nhuman robot interaction\nmanipulation\ncomputer vision\nvecna robotics\nmanipulation\nwarehouse and fulfillment robotics and automation\n6 river systems\nwarehouse and fulfillment robotics and automation\nrighthand robotics\nmanipulation\nrobot mechanics and design\nsea machines\nautonomous marine vehicles\nlocus robotics\nwarehouse and fulfillment robotics and automation\nava robotics\ntelepresence\nsan francisco bay area\nnuro\nautonomous driving\nautonomous delivery\nfetch robotics\nretail robotics\nwarehouse and fulfillment robotics and automation\nmanipulation\nzenuity\nautonomous driving\nautox\nautonomous driving\nalt location: shenzhen, chn\ndeepscale ai\nautonomous driving\ndouble robotics\ntelepresence\ncovariant ai\nrobotics\nmanipulation\ncomputer vision\nreinforcement learning\nabundant robotics\nagriculture robotics\nblue river technology\nagriculture robotics\noutdoor robotics\nparent company: john deere\nneato robotics\nrobot vacuums\nskydio\nautonomous unmanned aerial vehicles\niron ox\nagriculture robotics\nliquid robotics\nmarine robotics\nrobot controls\nrobot research\nparent company: boeing\nvicarious\nartificial intelligence research\nintelligent robot assistants\nsuitable technologies\ntelepresence\norbital insight\ncomputer vision\nalt locations: washington d.c., usa; boston, usa\nspace know\ncomputer vision\naurora technologies\nautonomous driving\nalt locations: pittsburgh, usa; bozeman, usa\nstarship technologies\nautonomous driving and delivery\nnextnav\ncomputer vision\n3d mapping\ndeepmap\ncomputer vision\n3d mapping\nfarmwise\nfarming robot\npicks weeds/reduces pesticide use\n(lemnos funded)\nbuilt robotics\nearth moving robots\nuse oem machines from caterpillar/komatsu\nphantom auto\nsupports robots through teleoperation\nboulder area\nleft hand robotics\noutdoor robotics\nagriculture robotics\namp robotics\nintelligent autonomous recycling\nindustrial robotics\nsphero/misty robotics\nrobot toys\nrobot companions\nvalyant ai\nintelligent disembodied agents\nberkshire grey\nmanipulation\nwarehouse and fulfillment robotics and automation\npittsburgh area\nbossa nova robotics\nretail robotics\nmanipulation\nargo ai\nautonomous driving\nastrobotic\nautonomous spaceflight landers\nautonomous planetary rovers\nre2 robotics\nresearch robotics\nmanipulation\nseegrid\nretail robotics\nwarehouse and fulfillment robotics and automation\ncomputer vision\n3d mapping\ntybot\nties rebar cages\naustin area\ndiligent robotics\nhealthcare robotics\npersonal robot assistants\nmaidbot\nhousehold robotics\nhuman robot interaction\npensa systems\nretail robotics\nautonomous unmanned aerial vehicles\nnew york\nclarifai\nvideo/image recognition\nenterprise visual decision making\nalt locations: san francisco, usa; washington d.c., usa\ncarmera\ncomputer vision\n3d mapping\ntoggle\nrebar tying robot with robotic arm\nother\nautonomous solutions (location: petersboro, ut, usa)\nmining robotics\nindustrial robotics\nautonomous driving\ninvia robotics (thousand oaks, ca, usa)\nwarehouse and fulfillment robotics and automation\nintelligent automation, inc. (washington d.c., usa)\nresearch robotics\nautomous inspection systems\ninternational startups\nclearpath robotics (toronto, can)\nresearch robotics\noutdoor robotics\nrobotic control\nuser interfaces\nkinova (location: montreal, can)\nmanipulation\nrobot mechanics and design\nmir (locations: odense, dnk; new york, usa)\nwarehouse and fulfillment robotics and automation\nrapyuta robotics (locations: tokyo, jpn; zurich, che; bangalore, ind)\nautonomous unmanned aerial vehicles\ncloud robotics\nfiveai (location: cambridge, gbr)\nautonomous driving\niris ai (locations: oslo, nor; berlin, deu; odessa, ukr; sofia, bgr)\nai-based autonomy for robotic science experimentation\nnaio technologies (location: escalquens, fra)\nagricultural robotics\nfranka emika (location: munich, deu)\nmanipulation\nrobot mechanics and design\ndorabot (location: shenzhen, chn)\nmanipulation\nmaterials handling robotics\nwarehouse and fulfillment robotics and automation\nalt locations: atlanta, usa; brisbane, aus\nwayve (location: cambridge, gbr)\nautonomous driving\nsaga robotics (oslo, nor)\nagricultural robotics\nalt location: lincoln, gbr\nnon-profits and government research labs\nopenai (location: san francisco, ca, usa)\nautonomous driving\ndeep learning/general ai\ndeep reinforcement learning\nmachine learning theory\ncomputer vision\nallen institute for artificial intelligence (location: seattle, wa, usa)\nnatural language processing/semantics/question answering\nmachine learning theory\nknowledge representation\ncomputer vision\nopen robotics (location: san francisco, ca, usa; singapore)\nrobotics software development\nrobotics simulation\nformerly: open source robotics foundation; willow garage\narmy research laboratory (arl) (locatoin: adelphi, md, usa)\nautonomous driving\ncomputer vision\nhuman robot interaction\nadaptation/robustness to unknown or adversarial terrain\nnaval resaerch laboratory (nrl) (location: washinton d.c., usa)\nautonomous underwater unmanned vehicles\nautonomous unmanned aerial vehicles\nmarine robotics\nautonomous testing of robotics\nhuman robot interaction\nartificial intelligence\nmanipulation\nair force research laboratory (afrl) (location: dayton, oh, usa)\nautonomous unmanned aerial vehicles\ncomputer vision\nnasa jet propulsion laboratory (jpl) and ames laboratory (location: pasadena, ca, usa; sunnyvale, ca, usa)\nspace robotics\nautonomous unmanned ground vehicles\nautonomous unmanned aerial vehicles\nself-assembling structures\ncomputer vision\nhuman robot interaction\ncredits\ndesigned in accordance with sindre's awesome superlist. thanks sindre! toc from @ekalinin 's easy toc for github readme. thanks eugene!\nand thank you to all of the contributers on and off github (mostly off at this point). couldn't do it without you.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000723, "year": null}, {"Unnamed: 0": 1826, "autor": 806, "date": null, "content": "Multi-Task Regression-based Learning for Autonomous Unmanned Aerial Vehicle Flight Control within Unstructured Outdoor Environments\nAbstract \u2014 Increased growth in the global Unmanned Aerial Vehicles (UAV) (drone) industry has expanded possibilities for fully autonomous UAV applications. A particular application which has in part motivated this research is the use of UAV in wide area search and surveillance operations in unstructured outdoor environments. The critical issue with such environments is the lack of structured features that could aid in autonomous \ufb02ight, such as road lines or paths. In this paper, we propose an End-to-End Multi-Task Regression-based Learning approach capable of de\ufb01ning \ufb02ight commands for navigation and exploration under the forest canopy, regardless of the presence of trails or additional sensors (i.e. GPS). Training and testing are performed using a software in the loop pipeline which allows for a detailed evaluation against state-of-the-art pose estimation techniques. Our extensive experiments demonstrate that our approach excels in performing dense exploration within the required search perimeter, is capable of covering wider search regions, generalises to previously unseen and unexplored environments and outperforms contemporary state-of-the-art techniques.\nArXiv: Maciel-Pearson et al.,RA-L, 2019\nTested using: Anaconda 2018.12 | Python 3.7.3 | Keras 2.2.4 | Tensorflow 1.13.1 | OpenCV 3.4.2\nNetwork Architecture:\nPrerequisites and setup\nEnvironment Setup\nInstall AirSim.\nInstall Anaconda with Python 3.5 or higher.\nCreate an Anaconda environment (preferable using Python 3.7).\nInstall Keras with Tensorflow backend.\n$ conda install keras-gpu\nInstall additional dependencies:\nmatplotlib\nimage\nopencv\nmsgpack-rpc-python\npandas\nh5py\npyquaternion\n$ pip install opencv-python h5py matplotlib image pandas msgpack-rpc-python pyquaternion\nPre-trained Model\nTo provide better testing opportunities, we provide a set of pre-trained weights. The pre-trained are separately stored on Zenodo due to their large size.\nThe script entitled \"download_model.sh\" will download the pre-trained weights and check the file integrity.\nTo download the pre-trained model, run the following commands:\n$ chmod +x ./download_model.sh\n$ ./download_model.sh\nSimulator\nn_predictions[n]\nHere, you can define how many predictions should be computed by the network. At each iteration, one set of predicted values for waypoints (x,y,z) and orientations are outputted.\nbehaviour [search,flight]\nHere, the behaviour of the UAV can be defined either as search or flight. In the search mode, the UAV navigational change in x and y directions are increased, which results in a wider angular rotation of the head. In contrast, when the behaviour is set to flight the predicted values are smoothed, which reduces the angular rotation of the head. During the production of this paper, all tests were carried using the search mode.\nsearch flight\nstart position [-100,100]\nHere, the starting position of the drone can be defined by assigning a value for the x,y,z coordinates. We recommend these value to be between -100 and 100.\nsmoothness [-2.0,2.0]\nWhen using the flight mode, you can also define the smoothness of the flight in the x,y and z directions. We recommend using values between -2.0 and 2.0.\n$ python test_mtrl.py n_predictions behaviour x y z smoothness_x smoothness_y smoothness_z\nExample:\n$ python test_mtrl.py 150 search -3 -10 -10 0.75 -0.75 0.15\nReference\nIf you make use of this work in any way (including our pre-trained models or dataset, please reference the following:\nMulti-Task Regression-based Learning for Autonomous Unmanned Aerial Vehicle Flight Control within Unstructured Outdoor Environments Maciel-Pearson et al., In Robotics and Automation Letters IEEE, 2019.\n@article{pearson19regression,\ntitle={Multi-Task Regression-based Learning for Autonomous Unmanned Aerial Vehicle Flight Control within Unstructured Outdoor Environments},\nauthor={Maciel-Pearson, B.G., Ak\u00e7ay, S., Atapour-Abarghouei, A., Holder, C. and Breckon, T.P.},\njournal={IEEE Robotics and Automation Letters},\nvolume={4},\npages={1-8},\nyear={2019},\npublisher={IEEE}\n}", "link": "https://github.com/brunapearson/mtrl-auto-uav", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "multi-task regression-based learning for autonomous unmanned aerial vehicle flight control within unstructured -----> outdoor !!!  environments\nabstract \u2014 increased growth in the global unmanned aerial vehicles (uav) (drone) industry has expanded possibilities for fully autonomous uav applications. a particular application which has in part motivated this research is the use of uav in wide area search and surveillance operations in unstructured outdoor environments. the critical issue with such environments is the lack of structured features that could aid in autonomous \ufb02ight, such as road lines or paths. in this paper, we propose an end-to-end multi-task regression-based learning approach capable of de\ufb01ning \ufb02ight commands for navigation and exploration under the forest canopy, regardless of the presence of trails or additional sensors (i.e. gps). training and testing are performed using a software in the loop pipeline which allows for a detailed evaluation against state-of-the-art pose estimation techniques. our extensive experiments demonstrate that our approach excels in performing dense exploration within the required search perimeter, is capable of covering wider search regions, generalises to previously unseen and unexplored environments and outperforms contemporary state-of-the-art techniques.\narxiv: maciel-pearson et al.,ra-l, 2019\ntested using: anaconda 2018.12 | python 3.7.3 | keras 2.2.4 | tensorflow 1.13.1 | opencv 3.4.2\nnetwork architecture:\nprerequisites and setup\nenvironment setup\ninstall airsim.\ninstall anaconda with python 3.5 or higher.\ncreate an anaconda environment (preferable using python 3.7).\ninstall keras with tensorflow backend.\n$ conda install keras-gpu\ninstall additional dependencies:\nmatplotlib\nimage\nopencv\nmsgpack-rpc-python\npandas\nh5py\npyquaternion\n$ pip install opencv-python h5py matplotlib image pandas msgpack-rpc-python pyquaternion\npre-trained model\nto provide better testing opportunities, we provide a set of pre-trained weights. the pre-trained are separately stored on zenodo due to their large size.\nthe script entitled \"download_model.sh\" will download the pre-trained weights and check the file integrity.\nto download the pre-trained model, run the following commands:\n$ chmod +x ./download_model.sh\n$ ./download_model.sh\nsimulator\nn_predictions[n]\nhere, you can define how many predictions should be computed by the network. at each iteration, one set of predicted values for waypoints (x,y,z) and orientations are outputted.\nbehaviour [search,flight]\nhere, the behaviour of the uav can be defined either as search or flight. in the search mode, the uav navigational change in x and y directions are increased, which results in a wider angular rotation of the head. in contrast, when the behaviour is set to flight the predicted values are smoothed, which reduces the angular rotation of the head. during the production of this paper, all tests were carried using the search mode.\nsearch flight\nstart position [-100,100]\nhere, the starting position of the drone can be defined by assigning a value for the x,y,z coordinates. we recommend these value to be between -100 and 100.\nsmoothness [-2.0,2.0]\nwhen using the flight mode, you can also define the smoothness of the flight in the x,y and z directions. we recommend using values between -2.0 and 2.0.\n$ python test_mtrl.py n_predictions behaviour x y z smoothness_x smoothness_y smoothness_z\nexample:\n$ python test_mtrl.py 150 search -3 -10 -10 0.75 -0.75 0.15\nreference\nif you make use of this work in any way (including our pre-trained models or dataset, please reference the following:\nmulti-task regression-based learning for autonomous unmanned aerial vehicle flight control within unstructured outdoor environments maciel-pearson et al., in robotics and automation letters ieee, 2019.\n@article{pearson19regression,\ntitle={multi-task regression-based learning for autonomous unmanned aerial vehicle flight control within unstructured outdoor environments},\nauthor={maciel-pearson, b.g., ak\u00e7ay, s., atapour-abarghouei, a., holder, c. and breckon, t.p.},\njournal={ieee robotics and automation letters},\nvolume={4},\npages={1-8},\nyear={2019},\npublisher={ieee}\n}", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000806, "year": null}, {"Unnamed: 0": 2014, "autor": 994, "date": null, "content": "Fast and Robust Bio-inspired Teach and Repeat Navigation\nThis repository contains code for a low compute teach and repeat navigation approach which only requires monocular vision and wheel odometry. Teach the robot a route by teleoperation, then the robot will be able to repeat it - robust to lighting variation and moderate environmental changes. For full details see our IROS2021 paper, available on arXiv. You can view the conference presentation here as well as other multimedia material and a full 550 metre outdoor run.\nLicense and attribution\nIf you use the code in this repository, please cite our paper. The code is available under the BSD-2-Clause License.\n@inproceedings{dall2021fast,\ntitle={Fast and Robust Bio-inspired Teach and Repeat Navigation},\nauthor={Dall'Osto, Dominic and Fischer, Tobias and Milford, Michael},\nbooktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems},\nyear={2021}\n}\nSetup and use\nThis approach can be used with any mobile robot with a monocular camera and odometry source.\nFor the teach run, run both the data_collect.py and data_save.py nodes. Teleoperate the robot along the desired route and the teach run (odometry poses and images) will be recorded to a specified folder.\nFor the repeat run, use image_matcher.py and localiser.py. The localiser will publish Goal messages on the topic goal, containing a goal to navigate to in the robot's odometry frame. An example drive_to_pose_controller is used here, but can be replaced with another controller as required.\nIn both cases, remap the odom and image topics to those provided by the robot. Note, the published odometry must also contain an integrated pose estimate.\nEssential parameters for these nodes are shown below. Other parameters exist to save additional diagnostic data, or to wait for a ready signal before starting - if the robot needs to run a setup procedure for example. These are shown in the nodes and example usage is shown in the provided launch files.\nGlobal parameters\nParameter Description Default Value\n/data_load_dir directory in which the teach runs are saved ~/miro/data\n/data_save_dir directory in which to save the results of a repeat run ~/miro/data/follow-straight_tests/5\n/image_resize_width width to resize images before comparison 115\n/image_resize_height height to resize images before comparison 44\n/patch_size patch size to use for patch normalisation (9,9)\n/goal_pose_separation distance between goals, should match ~distance_threshold in data_collect.py 0.2\n/image_field_of_view_width_deg horizontal field of view of images (degrees) 175.2\nParameters for data_collect.py\nParameter Description Example Value\n~distance_threshold distance (metres) travelled from the previous pose after which a new pose is stored in the teach map 0.2\n~angle_threshold_deg angular distance (degrees) travelled from the previous pose after which a new pose is stored in the teach map 15.0\nParameters for data_save.py\nParameter Description Example Value\n~save_dir directory in which to save the teach run ~/miro/data\n~timestamp_folder whether to timestamp the folder name of the teach run, so multiple runs can be performed without overwriting true\nParameters for localiser.py\nParameter Description Default Value\n~rotation_correction_gain proportional gain term to use for rotation corrections, $K_\\theta$, shouldn't need to be tuned 0.01\n~path_correction_gain proportional gain term to use for along-path corrections, $K_p$, shouldn't need to be tuned 0.01\n~stop_at_end whether the robot should stop at the end of the route, otherwise it assumes the route is circular and restarts from the beginning true\n~discrete-correction reduce compute by only performing a correction at each goal pose, not continually false\n~search-range how many teach images to search either side of the current to perform along-path correction 1\n~global_localisation_init when initialising, find the closest matching teach image to the current and start the route from there, otherwise start at the first goal false\n~min_init_correlation minimum correlation with a teach image at initialisation, otherwise the robot thinks it's not on the path and doesn't start repeating 0.0\nOverview of approach\nTeach run\nFirst the robot needs to be taught a route via teleoperation. At regular distance intervals along the path the dead-reckoning position and and image will be saved, resulting in a topometric map of the route. Images are patch normalised to increase robustness to lighting variation.\nRepeat run\nHaving learnt a route, the robot can robustly repeat it. The robot initially follows the sequence of odometry poses stored during the teach run, but errors accumulate in this approach over time. Images are compared between the teach and repeat routes to make corrections to the route.\nCorrection overview\nBoth rotational and lateral path errors result in horizontal image offsets that can't be distinguished, but this is not a problem because both require the same correction response. However, moving along the path can also horizontal image offsets. These must be accounted for by interpolating between the previous and next goal images.\nOrientation correction\nIf an orientation error is detected by comparing teach and repeat images, an associated path correction is performed, modulated by a constant gain factor. This correction causes the robot to steer back onto the path.\nAlong-path correction\nRepeat images are compared to teach images within a certain search range of the current goal. If correlation values are stronger to images ahead or behind the robot's current estimated position, and along-path correction is performed. In this case, the goal is pulled towards the robot so it will be reached faster, allowing the estimated position to \"catch up\" to the real position.\nExamples of running teach and repeat for the robots we used\nRunning teach on Miro\nroslaunch teach_repeat data_collection_miro.launch\nRunning repeat on Miro\nroslaunch teach_repeat data_matching_miro.launch\nRunning teach on Jackal\nrosnode kill twist_mux (optional, only required for comparison with bearnav)\nroslaunch slam_toolbox localization.launch (optional, only required for quantitative analysis)\nroslaunch stroll_bearnav mapping-core-jackal.launch (optional, only required for comparison with bearnav)\nroslaunch stroll_bearnav mapping-gui-jackal.launch (optional, only required for comparison with bearnav)\nroslaunch teach_repeat data_collection_jackal.launch\nRunning repeat on Jackal\nroslaunch slam_toolbox localization.launch (optional, only required for quantitative analysis)\nroslaunch teach_repeat data_matching_jackal.launch\nRunning Bearnav repeat on Jackal\nroslaunch slam_toolbox localization.launch\nroslaunch stroll_bearnav navigation-core-jackal.launch\nroslaunch stroll_bearnav navigation-gui-jackal.launch", "link": "https://github.com/QVPR/teach-repeat", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "outdoor", "selectorShort": "outdoor", "MarkedSent": "fast and robust bio-inspired teach and repeat navigation\nthis repository contains code for a low compute teach and repeat navigation approach which only requires monocular vision and wheel odometry. teach the robot a route by teleoperation, then the robot will be able to repeat it - robust to lighting variation and moderate environmental changes. for full details see our iros2021 paper, available on arxiv. you can view the conference presentation here as well as other multimedia material and a full 550 metre -----> outdoor !!!  run.\nlicense and attribution\nif you use the code in this repository, please cite our paper. the code is available under the bsd-2-clause license.\n@inproceedings{dall2021fast,\ntitle={fast and robust bio-inspired teach and repeat navigation},\nauthor={dall'osto, dominic and fischer, tobias and milford, michael},\nbooktitle={ieee/rsj international conference on intelligent robots and systems},\nyear={2021}\n}\nsetup and use\nthis approach can be used with any mobile robot with a monocular camera and odometry source.\nfor the teach run, run both the data_collect.py and data_save.py nodes. teleoperate the robot along the desired route and the teach run (odometry poses and images) will be recorded to a specified folder.\nfor the repeat run, use image_matcher.py and localiser.py. the localiser will publish goal messages on the topic goal, containing a goal to navigate to in the robot's odometry frame. an example drive_to_pose_controller is used here, but can be replaced with another controller as required.\nin both cases, remap the odom and image topics to those provided by the robot. note, the published odometry must also contain an integrated pose estimate.\nessential parameters for these nodes are shown below. other parameters exist to save additional diagnostic data, or to wait for a ready signal before starting - if the robot needs to run a setup procedure for example. these are shown in the nodes and example usage is shown in the provided launch files.\nglobal parameters\nparameter description default value\n/data_load_dir directory in which the teach runs are saved ~/miro/data\n/data_save_dir directory in which to save the results of a repeat run ~/miro/data/follow-straight_tests/5\n/image_resize_width width to resize images before comparison 115\n/image_resize_height height to resize images before comparison 44\n/patch_size patch size to use for patch normalisation (9,9)\n/goal_pose_separation distance between goals, should match ~distance_threshold in data_collect.py 0.2\n/image_field_of_view_width_deg horizontal field of view of images (degrees) 175.2\nparameters for data_collect.py\nparameter description example value\n~distance_threshold distance (metres) travelled from the previous pose after which a new pose is stored in the teach map 0.2\n~angle_threshold_deg angular distance (degrees) travelled from the previous pose after which a new pose is stored in the teach map 15.0\nparameters for data_save.py\nparameter description example value\n~save_dir directory in which to save the teach run ~/miro/data\n~timestamp_folder whether to timestamp the folder name of the teach run, so multiple runs can be performed without overwriting true\nparameters for localiser.py\nparameter description default value\n~rotation_correction_gain proportional gain term to use for rotation corrections, $k_\\theta$, shouldn't need to be tuned 0.01\n~path_correction_gain proportional gain term to use for along-path corrections, $k_p$, shouldn't need to be tuned 0.01\n~stop_at_end whether the robot should stop at the end of the route, otherwise it assumes the route is circular and restarts from the beginning true\n~discrete-correction reduce compute by only performing a correction at each goal pose, not continually false\n~search-range how many teach images to search either side of the current to perform along-path correction 1\n~global_localisation_init when initialising, find the closest matching teach image to the current and start the route from there, otherwise start at the first goal false\n~min_init_correlation minimum correlation with a teach image at initialisation, otherwise the robot thinks it's not on the path and doesn't start repeating 0.0\noverview of approach\nteach run\nfirst the robot needs to be taught a route via teleoperation. at regular distance intervals along the path the dead-reckoning position and and image will be saved, resulting in a topometric map of the route. images are patch normalised to increase robustness to lighting variation.\nrepeat run\nhaving learnt a route, the robot can robustly repeat it. the robot initially follows the sequence of odometry poses stored during the teach run, but errors accumulate in this approach over time. images are compared between the teach and repeat routes to make corrections to the route.\ncorrection overview\nboth rotational and lateral path errors result in horizontal image offsets that can't be distinguished, but this is not a problem because both require the same correction response. however, moving along the path can also horizontal image offsets. these must be accounted for by interpolating between the previous and next goal images.\norientation correction\nif an orientation error is detected by comparing teach and repeat images, an associated path correction is performed, modulated by a constant gain factor. this correction causes the robot to steer back onto the path.\nalong-path correction\nrepeat images are compared to teach images within a certain search range of the current goal. if correlation values are stronger to images ahead or behind the robot's current estimated position, and along-path correction is performed. in this case, the goal is pulled towards the robot so it will be reached faster, allowing the estimated position to \"catch up\" to the real position.\nexamples of running teach and repeat for the robots we used\nrunning teach on miro\nroslaunch teach_repeat data_collection_miro.launch\nrunning repeat on miro\nroslaunch teach_repeat data_matching_miro.launch\nrunning teach on jackal\nrosnode kill twist_mux (optional, only required for comparison with bearnav)\nroslaunch slam_toolbox localization.launch (optional, only required for quantitative analysis)\nroslaunch stroll_bearnav mapping-core-jackal.launch (optional, only required for comparison with bearnav)\nroslaunch stroll_bearnav mapping-gui-jackal.launch (optional, only required for comparison with bearnav)\nroslaunch teach_repeat data_collection_jackal.launch\nrunning repeat on jackal\nroslaunch slam_toolbox localization.launch (optional, only required for quantitative analysis)\nroslaunch teach_repeat data_matching_jackal.launch\nrunning bearnav repeat on jackal\nroslaunch slam_toolbox localization.launch\nroslaunch stroll_bearnav navigation-core-jackal.launch\nroslaunch stroll_bearnav navigation-gui-jackal.launch", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000994, "year": null}], "name": "outdoorrobotics"}
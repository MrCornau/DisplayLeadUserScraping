{"interestingcomments": [{"Unnamed: 0": 1389, "autor": 369, "date": null, "content": "ros_hadoop / RosbagInputFormat\nLarge-scale data processing for ROS using Hadoop, Spark, TensorFlow and more.\nRosbagInputFormat is an open source splittable Hadoop InputFormat for the ROS bag file format.\nThe complete source code is available in src/ folder and the jar file is generated using SBT (see build.sbt)\nWorkflow\nDownload latest release jar file and put it in classpath\nExtract the index configuration of your ROS bag file. The extracted index is a very very small configuration file containing a protobuf array that will be given in the job configuration. Note that the operation will not process and it will not parse the whole bag file, but will simply seek to the required offset. e.g.\njava -jar lib/rosbaginputformat.jar -f /opt/ros_hadoop/master/dist/HMB_4.bag\n# will create an idx.bin config file /opt/ros_hadoop/master/dist/HMB_4.bag.idx.bin\nPut the ROS bag file in HDFS e.g.\nhdfs dfs -put\nUse it in your Spark jobs e.g.\nsc.newAPIHadoopFile(\npath = \"hdfs://127.0.0.1:9000/user/spark/HMB_4.bag\",\ninputFormatClass = \"io.autovia.foss.RosbagMapInputFormat\",\nkeyClass = \"org.apache.hadoop.io.LongWritable\",\nvalueClass = \"org.apache.hadoop.io.MapWritable\",\nconf = {\"RosbagInputFormat.chunkIdx\":\"/opt/ros_hadoop/master/dist/HMB_4.bag.idx.bin\"})\nExample data can be found for instance at https://github.com/udacity/self-driving-car/tree/master/datasets published under MIT License.\nWant to Upgrade?\nWe also sell ros_spark and ros_go which provide more features, a commercial-friendly license and allow you to support high quality open source development all at the same time. Please see the Autovia homepage for more detail.\nDocumentation\nThe doc/ folder contains a jupyter notebook with a few basic usage examples.\nTutorial\nTo test locally use the Dockerfile\nTo build an image using the Dockerfile run the following in the shell. Please note that it will download Hadoop and Spark from the URL source. The generated image is therefore relatively large ~5G.\ndocker build -t ros_hadoop:latest -f Dockerfile .\nTo start a container use the following shell command in the ros_hadoop folder.\n# $(pwd) will point to the ros_hadoop git clone folder\ndocker run -it -v $(pwd):/root/ros_hadoop -p 8888:8888 ros_hadoop\nThe container has a configured HDFS as well as Spark and the RosInputFormat jar. It leaves the user in a bash shell.\nPoint your browser to the local URL and enjoy the tutorial. The access token is printed in the docker container console.\nUsage from Spark (pyspark)\nExample data can be found for instance at https://github.com/udacity/self-driving-car/tree/master/datasets published under MIT License.\nCheck that the Rosbag file version is V2.0\njava -jar lib/rosbaginputformat.jar --version -f /opt/ros_hadoop/master/dist/HMB_4.bag\nExtract the index as configuration\nThe index is a very very small configuration file containing a protobuf array that will be given in the job configuration. Note that the operation will not process and it will not parse the whole bag file, but will simply seek to the required offset.\n# assuming you start the notebook in the doc/ folder\njava -jar ../lib/rosbaginputformat.jar \\\n-f /opt/ros_hadoop/master/dist/HMB_4.bag\nhdfs dfs -ls\nThis will generate a very small file named HMB_4.bag.idx.bin in the same folder.\nCopy the bag file in HDFS\nUsing your favorite tool put the bag file in your working HDFS folder.\nNote: keep the index file as configuration to your jobs, do not put small files in HDFS. For convenience we already provide an example file (/opt/ros_hadoop/master/dist/HMB_4.bag) in the HDFS under /user/root/\nhdfs dfs -put /opt/ros_hadoop/master/dist/HMB_4.bag\nhdfs dfs -ls\nHadoop InputFormat and Record Reader for Rosbag\nProcess Rosbag with Spark, Yarn, MapReduce, Hadoop Streaming API, \u2026\nSpark RDD are cached and optimised for analysis\nProcess the ROS bag file in Spark using the RosbagInputFormat\nNote: your HDFS address might differ.\nfin = sc.newAPIHadoopFile(\npath = \"hdfs://127.0.0.1:9000/user/root/HMB_4.bag\",\ninputFormatClass = \"io.autovia.foss.RosbagMapInputFormat\",\nkeyClass = \"org.apache.hadoop.io.LongWritable\",\nvalueClass = \"org.apache.hadoop.io.MapWritable\",\nconf = {\u201cRosbagInputFormat.chunkIdx\u201d:\u201d/opt/ros_hadoop/master/dist/HMB_4.bag.idx.bin\"})\nInterpret the Messages\nTo interpret the messages we need the connections. We could get the connections as configuration as well. At the moment we decided to collect the connections into Spark driver in a dictionary and use it in the subsequent RDD actions.\nCollect the connections from all Spark partitions of the bag file into the Spark driver\nconn_a = fin.filter(\nlambda r: r[1]['header']['op'] == 7\n).map(\nlambda r: r[1]\n).collect()\nconn_d = {str(k['header']['topic']):k for k in conn_a}\n# see topic names\nconn_d.keys()\nFrom all ROS bag splits we collect into Spark driver the connection messages (op=7 in header) where the ROS definitions are stored. This operation happens in parallel of course.\nLoad the python map functions from src/main/python/functions.py\n%run -i ../src/main/python/functions.py\nAt the moment the file contains a single mapper function named msg_map.\nUse of msg_map to apply a function on all messages\nPython rosbag.bag needs to be installed on all Spark workers. The msg_map function (from src/main/python/functions.py) takes three arguments: 1. r = the message or RDD record Tuple 2. func = a function (default str) to apply to the ROS message 3. conn = a connection to specify what topic to process\n%matplotlib nbagg\n# use %matplotlib notebook in python3\nfrom functools import partial\nimport pandas as pd\nimport numpy as np\n# Take messages from '/imu/data' topic using default str func\nrdd = fin.flatMap(\npartial(msg_map, conn=conn_d['/imu/data'])\n)\nThe connection dictionary is sent over the closure to the workers that uses it in the msg_map.\nprint(rdd.take(1)[0])\nheader:\nseq: 1701626\nstamp:\nsecs: 1479425728\nnsecs: 747487068\nframe_id: /imu\norientation:\nx: -0.0251433756238\ny: 0.0284643176884\nz: -0.0936542998233\nw: 0.994880191333\norientation_covariance: [0.017453292519943295, 0.0, 0.0, 0.0, 0.017453292519943295, 0.0, 0.0, 0.0, 0.15707963267948966]\nangular_velocity:\nx: 0.0\ny: 0.0\nz: 0.0\nangular_velocity_covariance: [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]\nlinear_acceleration:\nx: 1.16041922569\ny: 0.595418334007\nz: 10.7565326691\nlinear_acceleration_covariance: [0.0004, 0.0, 0.0, 0.0, 0.0004, 0.0, 0.0, 0.0, 0.0004]\nImage data from camera messages\nAn example of taking messages using a func other than default str. In our case we apply a lambda to messages from from '/center_camera/image_color/compressed' topic. As usual with Spark the operation will happen in parallel on all workers.\nfrom PIL import Image\nfrom io import BytesIO\nres = fin.flatMap(\npartial(msg_map, func=lambda r: r.data, conn=conn_d['/center_camera/image_color/compressed'])\n).take(50)\nImage.open(BytesIO(res[48]))\nPlot fuel level\nThe topic /vehicle/fuel_level_report contains 2215 ROS messages. Let us plot the header.stamp in seconds vs. fuel_level using a pandas dataframe.\ndef f(msg):\nreturn (msg.header.stamp.secs, msg.fuel_level)\nd = fin.flatMap(\npartial(msg_map, func=f, conn=conn_d['/vehicle/fuel_level_report'])\n).toDF().toPandas()\nd.set_index(\u2018_1').plot()\nMachine Learning models on Spark workers\nA dot product Keras \"model\" for each message from a topic. We will compare it with the one computed with numpy.\nNote that the imports happen in the workers and not in driver. On the other hand the connection dictionary is sent over the closure.\ndef f(msg):\nfrom keras.layers import dot, Dot, Input\nfrom keras.models import Model\nlinear_acceleration = {\n'x': msg.linear_acceleration.x,\n'y': msg.linear_acceleration.y,\n'z': msg.linear_acceleration.z,\n}\nlinear_acceleration_covariance = np.array(msg.linear_acceleration_covariance)\ni1 = Input(shape=(3,))\ni2 = Input(shape=(3,))\no = dot([i1,i2], axes=1)\nmodel = Model([i1,i2], o)\n# return a tuple with (numpy dot product, keras dot \"predict\")\nreturn (\nnp.dot(linear_acceleration_covariance.reshape(3,3),\n[linear_acceleration['x'], linear_acceleration['y'], linear_acceleration['z']]),\nmodel.predict([\nnp.array([[ linear_acceleration['x'], linear_acceleration['y'], linear_acceleration['z'] ]]),\nlinear_acceleration_covariance.reshape((3,3))])\n)\nfin.flatMap(partial(msg_map, func=f, conn=conn_d['/vehicle/imu/data_raw'])).take(5)\n# tuple with (numpy dot product, keras dot \u201cpredict\u201d)\nOne can sample of course and collect the data in the driver to train a model on one single machine. Note that the msg is the most granular unit but you could replace the flatMap with a mapPartitions to apply such a Keras function to a whole split.\nAnother option would be to have a map.reduceByKey before the flatMap so that the function argument would be a whole interval instead of a msg. The idea is to key on time.", "link": "https://github.com/autovia/ros_hadoop", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "sbt", "selectorShort": "sbt", "MarkedSent": "ros_hadoop / rosbaginputformat\nlarge-scale data processing for ros using hadoop, spark, tensorflow and more.\nrosbaginputformat is an open source splittable hadoop inputformat for the ros bag file format.\nthe complete source code is available in src/ folder and the jar file is generated using -----> sbt !!!  (see build.sbt)\nworkflow\ndownload latest release jar file and put it in classpath\nextract the index configuration of your ros bag file. the extracted index is a very very small configuration file containing a protobuf array that will be given in the job configuration. note that the operation will not process and it will not parse the whole bag file, but will simply seek to the required offset. e.g.\njava -jar lib/rosbaginputformat.jar -f /opt/ros_hadoop/master/dist/hmb_4.bag\n# will create an idx.bin config file /opt/ros_hadoop/master/dist/hmb_4.bag.idx.bin\nput the ros bag file in hdfs e.g.\nhdfs dfs -put\nuse it in your spark jobs e.g.\nsc.newapihadoopfile(\npath = \"hdfs://127.0.0.1:9000/user/spark/hmb_4.bag\",\ninputformatclass = \"io.autovia.foss.rosbagmapinputformat\",\nkeyclass = \"org.apache.hadoop.io.longwritable\",\nvalueclass = \"org.apache.hadoop.io.mapwritable\",\nconf = {\"rosbaginputformat.chunkidx\":\"/opt/ros_hadoop/master/dist/hmb_4.bag.idx.bin\"})\nexample data can be found for instance at https://github.com/udacity/self-driving-car/tree/master/datasets published under mit license.\nwant to upgrade?\nwe also sell ros_spark and ros_go which provide more features, a commercial-friendly license and allow you to support high quality open source development all at the same time. please see the autovia homepage for more detail.\ndocumentation\nthe doc/ folder contains a jupyter notebook with a few basic usage examples.\ntutorial\nto test locally use the dockerfile\nto build an image using the dockerfile run the following in the shell. please note that it will download hadoop and spark from the url source. the generated image is therefore relatively large ~5g.\ndocker build -t ros_hadoop:latest -f dockerfile .\nto start a container use the following shell command in the ros_hadoop folder.\n# $(pwd) will point to the ros_hadoop git clone folder\ndocker run -it -v $(pwd):/root/ros_hadoop -p 8888:8888 ros_hadoop\nthe container has a configured hdfs as well as spark and the rosinputformat jar. it leaves the user in a bash shell.\npoint your browser to the local url and enjoy the tutorial. the access token is printed in the docker container console.\nusage from spark (pyspark)\nexample data can be found for instance at https://github.com/udacity/self-driving-car/tree/master/datasets published under mit license.\ncheck that the rosbag file version is v2.0\njava -jar lib/rosbaginputformat.jar --version -f /opt/ros_hadoop/master/dist/hmb_4.bag\nextract the index as configuration\nthe index is a very very small configuration file containing a protobuf array that will be given in the job configuration. note that the operation will not process and it will not parse the whole bag file, but will simply seek to the required offset.\n# assuming you start the notebook in the doc/ folder\njava -jar ../lib/rosbaginputformat.jar \\\n-f /opt/ros_hadoop/master/dist/hmb_4.bag\nhdfs dfs -ls\nthis will generate a very small file named hmb_4.bag.idx.bin in the same folder.\ncopy the bag file in hdfs\nusing your favorite tool put the bag file in your working hdfs folder.\nnote: keep the index file as configuration to your jobs, do not put small files in hdfs. for convenience we already provide an example file (/opt/ros_hadoop/master/dist/hmb_4.bag) in the hdfs under /user/root/\nhdfs dfs -put /opt/ros_hadoop/master/dist/hmb_4.bag\nhdfs dfs -ls\nhadoop inputformat and record reader for rosbag\nprocess rosbag with spark, yarn, mapreduce, hadoop streaming api, \u2026\nspark rdd are cached and optimised for analysis\nprocess the ros bag file in spark using the rosbaginputformat\nnote: your hdfs address might differ.\nfin = sc.newapihadoopfile(\npath = \"hdfs://127.0.0.1:9000/user/root/hmb_4.bag\",\ninputformatclass = \"io.autovia.foss.rosbagmapinputformat\",\nkeyclass = \"org.apache.hadoop.io.longwritable\",\nvalueclass = \"org.apache.hadoop.io.mapwritable\",\nconf = {\u201crosbaginputformat.chunkidx\u201d:\u201d/opt/ros_hadoop/master/dist/hmb_4.bag.idx.bin\"})\ninterpret the messages\nto interpret the messages we need the connections. we could get the connections as configuration as well. at the moment we decided to collect the connections into spark driver in a dictionary and use it in the subsequent rdd actions.\ncollect the connections from all spark partitions of the bag file into the spark driver\nconn_a = fin.filter(\nlambda r: r[1]['header']['op'] == 7\n).map(\nlambda r: r[1]\n).collect()\nconn_d = {str(k['header']['topic']):k for k in conn_a}\n# see topic names\nconn_d.keys()\nfrom all ros bag splits we collect into spark driver the connection messages (op=7 in header) where the ros definitions are stored. this operation happens in parallel of course.\nload the python map functions from src/main/python/functions.py\n%run -i ../src/main/python/functions.py\nat the moment the file contains a single mapper function named msg_map.\nuse of msg_map to apply a function on all messages\npython rosbag.bag needs to be installed on all spark workers. the msg_map function (from src/main/python/functions.py) takes three arguments: 1. r = the message or rdd record tuple 2. func = a function (default str) to apply to the ros message 3. conn = a connection to specify what topic to process\n%matplotlib nbagg\n# use %matplotlib notebook in python3\nfrom functools import partial\nimport pandas as pd\nimport numpy as np\n# take messages from '/imu/data' topic using default str func\nrdd = fin.flatmap(\npartial(msg_map, conn=conn_d['/imu/data'])\n)\nthe connection dictionary is sent over the closure to the workers that uses it in the msg_map.\nprint(rdd.take(1)[0])\nheader:\nseq: 1701626\nstamp:\nsecs: 1479425728\nnsecs: 747487068\nframe_id: /imu\norientation:\nx: -0.0251433756238\ny: 0.0284643176884\nz: -0.0936542998233\nw: 0.994880191333\norientation_covariance: [0.017453292519943295, 0.0, 0.0, 0.0, 0.017453292519943295, 0.0, 0.0, 0.0, 0.15707963267948966]\nangular_velocity:\nx: 0.0\ny: 0.0\nz: 0.0\nangular_velocity_covariance: [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]\nlinear_acceleration:\nx: 1.16041922569\ny: 0.595418334007\nz: 10.7565326691\nlinear_acceleration_covariance: [0.0004, 0.0, 0.0, 0.0, 0.0004, 0.0, 0.0, 0.0, 0.0004]\nimage data from camera messages\nan example of taking messages using a func other than default str. in our case we apply a lambda to messages from from '/center_camera/image_color/compressed' topic. as usual with spark the operation will happen in parallel on all workers.\nfrom pil import image\nfrom io import bytesio\nres = fin.flatmap(\npartial(msg_map, func=lambda r: r.data, conn=conn_d['/center_camera/image_color/compressed'])\n).take(50)\nimage.open(bytesio(res[48]))\nplot fuel level\nthe topic /vehicle/fuel_level_report contains 2215 ros messages. let us plot the header.stamp in seconds vs. fuel_level using a pandas dataframe.\ndef f(msg):\nreturn (msg.header.stamp.secs, msg.fuel_level)\nd = fin.flatmap(\npartial(msg_map, func=f, conn=conn_d['/vehicle/fuel_level_report'])\n).todf().topandas()\nd.set_index(\u2018_1').plot()\nmachine learning models on spark workers\na dot product keras \"model\" for each message from a topic. we will compare it with the one computed with numpy.\nnote that the imports happen in the workers and not in driver. on the other hand the connection dictionary is sent over the closure.\ndef f(msg):\nfrom keras.layers import dot, dot, input\nfrom keras.models import model\nlinear_acceleration = {\n'x': msg.linear_acceleration.x,\n'y': msg.linear_acceleration.y,\n'z': msg.linear_acceleration.z,\n}\nlinear_acceleration_covariance = np.array(msg.linear_acceleration_covariance)\ni1 = input(shape=(3,))\ni2 = input(shape=(3,))\no = dot([i1,i2], axes=1)\nmodel = model([i1,i2], o)\n# return a tuple with (numpy dot product, keras dot \"predict\")\nreturn (\nnp.dot(linear_acceleration_covariance.reshape(3,3),\n[linear_acceleration['x'], linear_acceleration['y'], linear_acceleration['z']]),\nmodel.predict([\nnp.array([[ linear_acceleration['x'], linear_acceleration['y'], linear_acceleration['z'] ]]),\nlinear_acceleration_covariance.reshape((3,3))])\n)\nfin.flatmap(partial(msg_map, func=f, conn=conn_d['/vehicle/imu/data_raw'])).take(5)\n# tuple with (numpy dot product, keras dot \u201cpredict\u201d)\none can sample of course and collect the data in the driver to train a model on one single machine. note that the msg is the most granular unit but you could replace the flatmap with a mappartitions to apply such a keras function to a whole split.\nanother option would be to have a map.reducebykey before the flatmap so that the function argument would be a whole interval instead of a msg. the idea is to key on time.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000369, "year": null}], "name": "sbtrobotics"}
{"interestingcomments": [{"Unnamed: 0": 1163, "autor": 143, "date": null, "content": "QuadtreeMapping\nA Real-time Monocular Dense Mapping System\nThis is a monocular dense mapping system following the IROS 2018 paper Quadtree-accelerated Real-time Monocular Dense Mapping, Kaixuan Wang, Wenchao Ding, Shaojie Shen.\nGiven a localized monocular camera, the system can generate dense depth maps in real-time using portable devices. The generated depth maps can be used to reconstruct the environment or be used for UAV autonomous flight. An example of real-time reconstruction is\nRed line is the camera trajectory.\nA video can be used to illustrate the pipeline and the performance of our system:\nWe would like to thank rpg_open_remode for their open source work. The project inspires us, and the system architecture helps we build QuadtreeMapping.\nPlease note that, in the system, the depth value is defined as Euclidean distance instead of z value. For example, if the point is (x, y, z) in camera coordinate, the depth value is\nThis branch uses opencv totally on CPU. If you need to speed up the image undistort step by using OpenCV with CUDA, please checkout to master branch.\n1.0 Prerequisites\nUbuntu and ROS\nBoth Ubuntu 16.04 with ROS Kinect and Ubuntu 14.04 with ROS Indigo are ok.\nCUDA\nThe system uses GPU to parallel most of the computation. You don't need a powerful GPU to run the code, but it must be a Nvidia GPU that supports CUDA. We use CUDA 8.0 to run the system. CUDA 9.0 has not been tested yet.\nOpenCV\nThe OpenCV that comes with ROS is ok.\n2.0 install\nSince the GPU device varies from each one to another, the CMakeLists.txt needs to be changed accordingly.\ncd ~/catkin_ws/src\ngit clone https://github.com/HKUST-Aerial-Robotics/open_quadtree_mapping.git\ncd open_quadtree_mapping\nnow find the CMakeLists.txt\nFirst, change the Compute capability in line 11 and line 12 according to your device. The default value is 61 and it works for Nvidia TITAN Xp etc. The compute capability of your device can be found at wikipedia.\nAfter the change of CMakeLists.txt, you can compile the QuadtreeMapping.\ncd ~/catkin_ws\ncatkin_make\n3.0 parameters\nBefore running the system, please take a look at the parameters in the launch/example.launch.\ncam_width, cam_height, cam_fx, cam_cx, cam_fy, cam_cy, cam_k1, cam_k2, cam_r1, cam_r2 are the camera intrinsic parameters. We use pinhole model.\ndownsample_factor is used to resize the image. The estimated depth maps have size of cam_width*downsample_factor x cam_height*downsample_factor. This factor is useful if you want to run QuadtreeMapping on platforms with limited resources, for example Jetson TX1.\nsemi2dense_ratio is the ratio to control output depth map frequency and must be an integer. If the input camera-pose pairs are 30Hz and you only need 10Hz depth estimation, set semi2dense_ratio to 3. High-frequency camera-pose input works better than a low-frequency input even you want a low-frequency depth estimation.\n4.0 run QuadtreeMapping\nThe input of QuadtreeMapping is synchronized Image (sensor_msgs::Image) and Pose (geometry_msgs::PoseStamped). Make sure the ROS messages are the correct type and the timestamps are the same. Images and poses at different frequencies is ok. For example, the system will filter 30Hz images and 10Hz poses into 10Hz image-pose pairs as input.\n4.1 run the example\nWe provide an example of a hand-held camera navigating in a garden. We provide the link to download the bag. The ego-motion is estimated using VINS-MONO.\nTo run the example, just\nroslaunch open_quadtree_mapping example.launch\nand play the bag in another terminal\nrosbag play example.bag\nThe results are published as:\n/open_quadtree_mapping/depth : estimated depth for each pixel, invalid depth are filled with zeros,\n/open_quadtree_mapping/color_depth : color-coded depth maps for visualization, invaild depth are red,\n/open_quadtree_mapping/debug : color-coded depth of pixels before depth interpolation.\n/open_quadtree_mapping/reference : undistorted intensity image,\n/open_quadtree_mapping/pointcloud : point cloud from the current undistorted intensity image and the extracted depth map.\n4.2 run with other datasets or run live\nTo run with other data, you can modify the launch file according to your settings. To get good results, a few things need to be noticed:\nGood ego-motion estimation is required. The ego-motion should be precise and have the metric scale. We recommend using VINS-MONO to estimate the camera motion. Visual odometry systems like ORB-SLAM cannot be directly used unless the scale information is recovered.\nRotation is not good for the system. Rotation reduces the number of frames QuadtreeMapping can use to estimate the depth map.\nA good camera is required. A good choice is an industry camera that has a global shutter and is set to a fixed exposure time. Also, images should have a balanced contrast, too bright or too dark is not good.\n5.0 fuse into a global map\nQuadtree publishes depth maps and the corresponding intensity images. You can fuse them using the tool you like. We use a modified open chisel for 3D reconstruction and use a GPU-based TSDF to support autonomous flight.\n6.0 future update\nmodified open chisel will be open source soon.", "link": "https://github.com/HKUST-Aerial-Robotics/open_quadtree_mapping", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "garden", "selectorShort": "garden", "MarkedSent": "quadtreemapping\na real-time monocular dense mapping system\nthis is a monocular dense mapping system following the iros 2018 paper quadtree-accelerated real-time monocular dense mapping, kaixuan wang, wenchao ding, shaojie shen.\ngiven a localized monocular camera, the system can generate dense depth maps in real-time using portable devices. the generated depth maps can be used to reconstruct the environment or be used for uav autonomous flight. an example of real-time reconstruction is\nred line is the camera trajectory.\na video can be used to illustrate the pipeline and the performance of our system:\nwe would like to thank rpg_open_remode for their open source work. the project inspires us, and the system architecture helps we build quadtreemapping.\nplease note that, in the system, the depth value is defined as euclidean distance instead of z value. for example, if the point is (x, y, z) in camera coordinate, the depth value is\nthis branch uses opencv totally on cpu. if you need to speed up the image undistort step by using opencv with cuda, please checkout to master branch.\n1.0 prerequisites\nubuntu and ros\nboth ubuntu 16.04 with ros kinect and ubuntu 14.04 with ros indigo are ok.\ncuda\nthe system uses gpu to parallel most of the computation. you don't need a powerful gpu to run the code, but it must be a nvidia gpu that supports cuda. we use cuda 8.0 to run the system. cuda 9.0 has not been tested yet.\nopencv\nthe opencv that comes with ros is ok.\n2.0 install\nsince the gpu device varies from each one to another, the cmakelists.txt needs to be changed accordingly.\ncd ~/catkin_ws/src\ngit clone https://github.com/hkust-aerial-robotics/open_quadtree_mapping.git\ncd open_quadtree_mapping\nnow find the cmakelists.txt\nfirst, change the compute capability in line 11 and line 12 according to your device. the default value is 61 and it works for nvidia titan xp etc. the compute capability of your device can be found at wikipedia.\nafter the change of cmakelists.txt, you can compile the quadtreemapping.\ncd ~/catkin_ws\ncatkin_make\n3.0 parameters\nbefore running the system, please take a look at the parameters in the launch/example.launch.\ncam_width, cam_height, cam_fx, cam_cx, cam_fy, cam_cy, cam_k1, cam_k2, cam_r1, cam_r2 are the camera intrinsic parameters. we use pinhole model.\ndownsample_factor is used to resize the image. the estimated depth maps have size of cam_width*downsample_factor x cam_height*downsample_factor. this factor is useful if you want to run quadtreemapping on platforms with limited resources, for example jetson tx1.\nsemi2dense_ratio is the ratio to control output depth map frequency and must be an integer. if the input camera-pose pairs are 30hz and you only need 10hz depth estimation, set semi2dense_ratio to 3. high-frequency camera-pose input works better than a low-frequency input even you want a low-frequency depth estimation.\n4.0 run quadtreemapping\nthe input of quadtreemapping is synchronized image (sensor_msgs::image) and pose (geometry_msgs::posestamped). make sure the ros messages are the correct type and the timestamps are the same. images and poses at different frequencies is ok. for example, the system will filter 30hz images and 10hz poses into 10hz image-pose pairs as input.\n4.1 run the example\nwe provide an example of a hand-held camera navigating in a -----> garden !!! . we provide the link to download the bag. the ego-motion is estimated using vins-mono.\nto run the example, just\nroslaunch open_quadtree_mapping example.launch\nand play the bag in another terminal\nrosbag play example.bag\nthe results are published as:\n/open_quadtree_mapping/depth : estimated depth for each pixel, invalid depth are filled with zeros,\n/open_quadtree_mapping/color_depth : color-coded depth maps for visualization, invaild depth are red,\n/open_quadtree_mapping/debug : color-coded depth of pixels before depth interpolation.\n/open_quadtree_mapping/reference : undistorted intensity image,\n/open_quadtree_mapping/pointcloud : point cloud from the current undistorted intensity image and the extracted depth map.\n4.2 run with other datasets or run live\nto run with other data, you can modify the launch file according to your settings. to get good results, a few things need to be noticed:\ngood ego-motion estimation is required. the ego-motion should be precise and have the metric scale. we recommend using vins-mono to estimate the camera motion. visual odometry systems like orb-slam cannot be directly used unless the scale information is recovered.\nrotation is not good for the system. rotation reduces the number of frames quadtreemapping can use to estimate the depth map.\na good camera is required. a good choice is an industry camera that has a global shutter and is set to a fixed exposure time. also, images should have a balanced contrast, too bright or too dark is not good.\n5.0 fuse into a global map\nquadtree publishes depth maps and the corresponding intensity images. you can fuse them using the tool you like. we use a modified open chisel for 3d reconstruction and use a gpu-based tsdf to support autonomous flight.\n6.0 future update\nmodified open chisel will be open source soon.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000143, "year": null}, {"Unnamed: 0": 1417, "autor": 397, "date": null, "content": "DeepSeqSLAM - the deep learning framework for robot place recognition\nThis repository contains the official PyTorch implementation of the papers:\n[1] Sequential Place Learning: Heuristic-Free High-Performance Long-Term Place Recognition. Marvin Chanc\u00e1n, Michael Milford. [ArXiv] [Website]\n[2] DeepSeqSLAM: A Trainable CNN+RNN for Joint Global Description and Sequence-based Place Recognition. Marvin Chanc\u00e1n, Michael Milford. NeurIPS 2020 Workshop on Machine Learning for Autonomous Driving (ML4AD). [ArXiv] [Website] [YouTube Video]\nBoth papers introduce DeepSeqSLAM, a CNN+LSTM baseline architecture for state-of-the-art route-based place recognition. DeepSeqSLAM leverages visual and positional time-series data for joint global description and sequential place inference in the context of simultaneous localization and mapping (SLAM) and autonomous driving research. Contrary to classical two-stage pipelines, e.g., match-then-temporally-filter, this codebase is orders of magnitud faster, scalable and learns from a single traversal of a route, while accurately generalizing to multiple traversals of the same route under very different environmental conditions.\nDeepSeqSLAM: The baseline architecture for Sequential Place Learning\nNews\n(May 10, 2021) Fixed and uploaded the Gardens Point dataset (.zip) file on Zenodo (version 2). You can also find it on Google Drive. Thanks everyone!\n(Apr 30, 2021) The Gardens Point dataset link on Zenodo (version 1) has some errors when trying to unzip. Here is an alternative Google Drive link. Thanks for the interest in the code!\n(Mar 4, 2021) Contributions welcome!\n(Mar 3, 2021) Archive ML4AD release and update this README.md with new Gardens Point dataset link.\n(Mar 1, 2021) Paper Sequential Place Learning submitted to RSS 2021.\n(Oct 30, 2020) Paper DeepSeqSLAM accepted at the NeurIPS 2020 Workshop on ML4AD.\nBibTex Citation\nIf you find any of the tools provided here useful for your research or report our results in a publication, please consider citing both Sequential Place Learning and DeepSeqSLAM papers:\n@article{chancan2021spl,\ntitle = {Sequential Place Learning: Heuristic-Free High-Performance Long-Term Place Recognition},\nauthor = {Marvin Chanc{\\'a}n and Michael Milford},\njournal = {arXiv preprint arXiv:2103.02074},\nyear = {2021}\n}\n@article{chancan2020deepseqslam,\ntitle = {DeepSeqSLAM: A Trainable CNN+RNN for Joint Global Description and Sequence-based Place Recognition},\nauthor = {Marvin Chanc{\\'a}n and Michael Milford},\njournal = {arXiv preprint arXiv:2011.08518},\nyear = {2020}\n}\nGetting Started\nYou just need Python v3.6+ with standard scientific packages, PyTorch v1.1+, and TorchVision v0.3.0+.\ngit clone https://github.com/mchancan/deepseqslam\nTraining Data\nThe challenging Gardens Point Walking dataset consists of three folders with 200 images each. The image name indicates correspondence in location between each of the three route traversals. Download the dataset, unzip, and place the day_left, day_right, and night_right image folders in the datasets/GardensPointWalking directory of DeepSeqSLAM.\nSingle Node Training (CPU, GPU or Multi-GPUs)\nIn this release, we provide an implementation of DeepSeqSLAM for evaluation on the Gardens Point dataset with challenging day-night changing conditions. We also provide normalized (synthetic) positional data for end-to-end training and deployment.\nRun the demo on the Gardens Point dataset\nsh demo_deepseqslam.sh\nYou can run this demo using one of these pre-trained models: alexnet, resnet18, vgg16, squeezenet1_0, densenet161, or easily configure the run.py script for training with any other PyTorch's model from torchvision.\nCommands example\nSEQ_LENGHT=10\nBATCH_SIZE=16\nEPOCHS=100\nNGPUS=1\nSEQ1='day_left'\nSEQ2='day_right'\nSEQ3='night_right'\nCNN='resnet18'\nMODEL_NAME=\"gp_${CNN}_lstm\"\npython run.py train \\\n--model_name $MODEL_NAME \\\n--ngpus $NGPUS \\\n--batch_size $BATCH_SIZE \\\n--seq_len $SEQ_LENGHT \\\n--epochs $EPOCHS \\\n--val_set $SEQ2 \\\n--cnn_arch $CNN\nfor i in $SEQ1 $SEQ2 $SEQ3\ndo\npython run.py val \\\n--model_name $MODEL_NAME \\\n--ngpus $NGPUS \\\n--batch_size $BATCH_SIZE \\\n--seq_len $SEQ_LENGHT \\\n--val_set $i \\\n--cnn_arch $CNN\ndone\nHelp\nusage: run.py [-h] [--data_path DATA_PATH] [-o OUTPUT_PATH]\n[--model_name MODEL_NAME] [-a ARCH] [--pretrained PRETRAINED]\n[--val_set VAL_SET] [--ngpus NGPUS] [-j WORKERS]\n[--epochs EPOCHS] [--batch_size BATCH_SIZE] [--lr LR]\n[--load LOAD] [--nimgs NIMGS] [--seq_len SEQ_LEN]\n[--nclasses NCLASSES] [--img_size IMG_SIZE]\nGardens Point Training\noptional arguments:\n-h, --help show this help message and exit\n--data_path DATA_PATH\npath to dataset folder that contains preprocessed\ntrain and val *npy image files\n-o OUTPUT_PATH, --output_path OUTPUT_PATH\npath for storing model checkpoints\n--model_name MODEL_NAME\ncheckpoint model name (default:\ndeepseqslam_resnet18_lstm)\n-a ARCH, --cnn_arch ARCH\nmodel architecture: alexnet | densenet121 |\ndensenet161 | densenet169 | densenet201 | googlenet |\ninception_v3 | mobilenet_v2 | resnet101 | resnet152 |\nresnet18 | resnet34 | resnet50 | resnext101_32x8d |\nresnext50_32x4d | shufflenet_v2_x0_5 |\nshufflenet_v2_x1_0 | shufflenet_v2_x1_5 |\nshufflenet_v2_x2_0 | squeezenet1_0 | squeezenet1_1 |\nvgg11 | vgg11_bn | vgg13 | vgg13_bn | vgg16 | vgg16_bn\n| vgg19 | vgg19_bn (default: resnet18)\n--pretrained PRETRAINED\nuse pre-trained CNN model (default: True)\n--val_set VAL_SET validation_set (default: day_right)\n--ngpus NGPUS number of GPUs for training; 0 if you want to run on\nCPU (default: 2)\n-j WORKERS, --workers WORKERS\nnumber of data loading workers (default: 4)\n--epochs EPOCHS number of total epochs to run (default: 200)\n--batch_size BATCH_SIZE\nmini-batch size: 2^n (default: 32)\n--lr LR, --learning_rate LR\ninitial learning rate (default: 1e-3)\n--load LOAD restart training from last checkpoint\n--nimgs NIMGS number of images (default: 200)\n--seq_len SEQ_LEN sequence length: ds (default: 10)\n--nclasses NCLASSES number of classes = nimgs - seq_len (default: 190)\n--img_size IMG_SIZE image size (default: 224)\nMultiple Nodes\nFor training on multiple nodes, you should use the NCCL backend for multi-processing distributed training since it currently provides the best distributed training performance. Please refer to ImageNet training in PyTorch for additional information on this.\nContributions welcome\nYou are welcome to contribute with features that might be valuable:\ntraining/testing using pre-computed (e.g. NetVLAD) global descriptors (reference.npy/query.npy)\nadd more CNN models for global description from raw images (e.g. NetVLAD)\nsupporting multiple datasets (e.g. Oxford RobotCar, Nordland)\nstandardize positional encoding inputs (mean=0, variance=1)\ndeployment visualizations (e.g. raw image sequences, features, top-k matches)\nAcknowledgements\nThis code has been largely inspired by the following projects:\nhttps://github.com/dicarlolab/CORnet\nhttps://github.com/dicarlolab/vonenet\nhttps://github.com/mchancan/flynet\nhttps://github.com/pytorch/examples/tree/master/imagenet\nLicense\nGNU GPL 3+\nCreated and maintained by Marvin Chanc\u00e1n.", "link": "https://github.com/mchancan/deepseqslam", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "gardens", "selectorShort": "garden", "MarkedSent": "deepseqslam - the deep learning framework for robot place recognition\nthis repository contains the official pytorch implementation of the papers:\n[1] sequential place learning: heuristic-free high-performance long-term place recognition. marvin chanc\u00e1n, michael milford. [arxiv] [website]\n[2] deepseqslam: a trainable cnn+rnn for joint global description and sequence-based place recognition. marvin chanc\u00e1n, michael milford. neurips 2020 workshop on machine learning for autonomous driving (ml4ad). [arxiv] [website] [youtube video]\nboth papers introduce deepseqslam, a cnn+lstm baseline architecture for state-of-the-art route-based place recognition. deepseqslam leverages visual and positional time-series data for joint global description and sequential place inference in the context of simultaneous localization and mapping (slam) and autonomous driving research. contrary to classical two-stage pipelines, e.g., match-then-temporally-filter, this codebase is orders of magnitud faster, scalable and learns from a single traversal of a route, while accurately generalizing to multiple traversals of the same route under very different environmental conditions.\ndeepseqslam: the baseline architecture for sequential place learning\nnews\n(may 10, 2021) fixed and uploaded the -----> gardens !!!  point dataset (.zip) file on zenodo (version 2). you can also find it on google drive. thanks everyone!\n(apr 30, 2021) the gardens point dataset link on zenodo (version 1) has some errors when trying to unzip. here is an alternative google drive link. thanks for the interest in the code!\n(mar 4, 2021) contributions welcome!\n(mar 3, 2021) archive ml4ad release and update this readme.md with new gardens point dataset link.\n(mar 1, 2021) paper sequential place learning submitted to rss 2021.\n(oct 30, 2020) paper deepseqslam accepted at the neurips 2020 workshop on ml4ad.\nbibtex citation\nif you find any of the tools provided here useful for your research or report our results in a publication, please consider citing both sequential place learning and deepseqslam papers:\n@article{chancan2021spl,\ntitle = {sequential place learning: heuristic-free high-performance long-term place recognition},\nauthor = {marvin chanc{\\'a}n and michael milford},\njournal = {arxiv preprint arxiv:2103.02074},\nyear = {2021}\n}\n@article{chancan2020deepseqslam,\ntitle = {deepseqslam: a trainable cnn+rnn for joint global description and sequence-based place recognition},\nauthor = {marvin chanc{\\'a}n and michael milford},\njournal = {arxiv preprint arxiv:2011.08518},\nyear = {2020}\n}\ngetting started\nyou just need python v3.6+ with standard scientific packages, pytorch v1.1+, and torchvision v0.3.0+.\ngit clone https://github.com/mchancan/deepseqslam\ntraining data\nthe challenging gardens point walking dataset consists of three folders with 200 images each. the image name indicates correspondence in location between each of the three route traversals. download the dataset, unzip, and place the day_left, day_right, and night_right image folders in the datasets/gardenspointwalking directory of deepseqslam.\nsingle node training (cpu, gpu or multi-gpus)\nin this release, we provide an implementation of deepseqslam for evaluation on the gardens point dataset with challenging day-night changing conditions. we also provide normalized (synthetic) positional data for end-to-end training and deployment.\nrun the demo on the gardens point dataset\nsh demo_deepseqslam.sh\nyou can run this demo using one of these pre-trained models: alexnet, resnet18, vgg16, squeezenet1_0, densenet161, or easily configure the run.py script for training with any other pytorch's model from torchvision.\ncommands example\nseq_lenght=10\nbatch_size=16\nepochs=100\nngpus=1\nseq1='day_left'\nseq2='day_right'\nseq3='night_right'\ncnn='resnet18'\nmodel_name=\"gp_${cnn}_lstm\"\npython run.py train \\\n--model_name $model_name \\\n--ngpus $ngpus \\\n--batch_size $batch_size \\\n--seq_len $seq_lenght \\\n--epochs $epochs \\\n--val_set $seq2 \\\n--cnn_arch $cnn\nfor i in $seq1 $seq2 $seq3\ndo\npython run.py val \\\n--model_name $model_name \\\n--ngpus $ngpus \\\n--batch_size $batch_size \\\n--seq_len $seq_lenght \\\n--val_set $i \\\n--cnn_arch $cnn\ndone\nhelp\nusage: run.py [-h] [--data_path data_path] [-o output_path]\n[--model_name model_name] [-a arch] [--pretrained pretrained]\n[--val_set val_set] [--ngpus ngpus] [-j workers]\n[--epochs epochs] [--batch_size batch_size] [--lr lr]\n[--load load] [--nimgs nimgs] [--seq_len seq_len]\n[--nclasses nclasses] [--img_size img_size]\ngardens point training\noptional arguments:\n-h, --help show this help message and exit\n--data_path data_path\npath to dataset folder that contains preprocessed\ntrain and val *npy image files\n-o output_path, --output_path output_path\npath for storing model checkpoints\n--model_name model_name\ncheckpoint model name (default:\ndeepseqslam_resnet18_lstm)\n-a arch, --cnn_arch arch\nmodel architecture: alexnet | densenet121 |\ndensenet161 | densenet169 | densenet201 | googlenet |\ninception_v3 | mobilenet_v2 | resnet101 | resnet152 |\nresnet18 | resnet34 | resnet50 | resnext101_32x8d |\nresnext50_32x4d | shufflenet_v2_x0_5 |\nshufflenet_v2_x1_0 | shufflenet_v2_x1_5 |\nshufflenet_v2_x2_0 | squeezenet1_0 | squeezenet1_1 |\nvgg11 | vgg11_bn | vgg13 | vgg13_bn | vgg16 | vgg16_bn\n| vgg19 | vgg19_bn (default: resnet18)\n--pretrained pretrained\nuse pre-trained cnn model (default: true)\n--val_set val_set validation_set (default: day_right)\n--ngpus ngpus number of gpus for training; 0 if you want to run on\ncpu (default: 2)\n-j workers, --workers workers\nnumber of data loading workers (default: 4)\n--epochs epochs number of total epochs to run (default: 200)\n--batch_size batch_size\nmini-batch size: 2^n (default: 32)\n--lr lr, --learning_rate lr\ninitial learning rate (default: 1e-3)\n--load load restart training from last checkpoint\n--nimgs nimgs number of images (default: 200)\n--seq_len seq_len sequence length: ds (default: 10)\n--nclasses nclasses number of classes = nimgs - seq_len (default: 190)\n--img_size img_size image size (default: 224)\nmultiple nodes\nfor training on multiple nodes, you should use the nccl backend for multi-processing distributed training since it currently provides the best distributed training performance. please refer to imagenet training in pytorch for additional information on this.\ncontributions welcome\nyou are welcome to contribute with features that might be valuable:\ntraining/testing using pre-computed (e.g. netvlad) global descriptors (reference.npy/query.npy)\nadd more cnn models for global description from raw images (e.g. netvlad)\nsupporting multiple datasets (e.g. oxford robotcar, nordland)\nstandardize positional encoding inputs (mean=0, variance=1)\ndeployment visualizations (e.g. raw image sequences, features, top-k matches)\nacknowledgements\nthis code has been largely inspired by the following projects:\nhttps://github.com/dicarlolab/cornet\nhttps://github.com/dicarlolab/vonenet\nhttps://github.com/mchancan/flynet\nhttps://github.com/pytorch/examples/tree/master/imagenet\nlicense\ngnu gpl 3+\ncreated and maintained by marvin chanc\u00e1n.", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000397, "year": null}, {"Unnamed: 0": 1924, "autor": 904, "date": null, "content": "Twitter Bot\nA Twitter Bot developed in Python who likes and retweets tweets with #python3 hashtag\nYou can find my Twitter Bot @pymoonbot at Twitter.\nDo you use twitter? Have you come across a twitter bot that like, retweet, follow, or even reply to your tweets? Do you want to know how to build one?\nDisclaimer:\nThis code is for educational purposes only and the author is not responsible for any consequences resulted. Please do not try it if you do not agree with the condition.\nLog in to your Twitter account on which you want to make a bot.\nPre-requisites:\n- Twitter Developer Account\n- Python\n- Tweepy\nTwitter Developer Account:\nTo know how to create a Twitter developer account, Check This to get a step by step guide.\nTweepy:\nTweepy supports both OAuth 1a (application-user) and OAuth 2 (application-only) authentication. Authentication is handled by the tweepy.AuthHandler class.\nThe API class provides access to the entire twitter RESTful API methods. Each method can accept various parameters and return responses.\nYou can refer to the Tweepy's documentation\nInstallation:\npip install tweepy\nThe source is available on Link\nTime to Code:\nYou can find the code in twitter_bot.py\nExplanation Walkthrough:\nIf you wish to understand the complete walkthrough,step by step, you should visit my Blog\nLICENSE:\nCopyright (c) 2020 Ayushi Rawat\nTwitter-Bot Repository is licensed under the MIT License\nMy Digital Garden:\nYou can find my blogs at my Website.\nGitHub CLI 1.0: All you need to know\nPython 3.9: All You need to know\nWeb Scraping Coronavirus Data into MS Excel\nHow to make your own Google Chrome Extension\nThe Developer Dictionary \ud83c\udf31\nCheck out my latest videos on YouTube:\nHow to make your own Google Chrome Extension\nWeb Scraping Coronavirus Data into MS Excel\nSeptember Leetcode playlist\nLet's connect! Find me on the web.\nIf you have any Queries or Suggestions, feel free to reach out to me.\nShow some \u2764\ufe0f by starring some of the repositories!", "link": "https://github.com/ayushi7rawat/Twitter-Bot", "origin": "Github", "suborigin": "robotics", "result": true, "Selector": "garden", "selectorShort": "garden", "MarkedSent": "twitter bot\na twitter bot developed in python who likes and retweets tweets with #python3 hashtag\nyou can find my twitter bot @pymoonbot at twitter.\ndo you use twitter? have you come across a twitter bot that like, retweet, follow, or even reply to your tweets? do you want to know how to build one?\ndisclaimer:\nthis code is for educational purposes only and the author is not responsible for any consequences resulted. please do not try it if you do not agree with the condition.\nlog in to your twitter account on which you want to make a bot.\npre-requisites:\n- twitter developer account\n- python\n- tweepy\ntwitter developer account:\nto know how to create a twitter developer account, check this to get a step by step guide.\ntweepy:\ntweepy supports both oauth 1a (application-user) and oauth 2 (application-only) authentication. authentication is handled by the tweepy.authhandler class.\nthe api class provides access to the entire twitter restful api methods. each method can accept various parameters and return responses.\nyou can refer to the tweepy's documentation\ninstallation:\npip install tweepy\nthe source is available on link\ntime to code:\nyou can find the code in twitter_bot.py\nexplanation walkthrough:\nif you wish to understand the complete walkthrough,step by step, you should visit my blog\nlicense:\ncopyright (c) 2020 ayushi rawat\ntwitter-bot repository is licensed under the mit license\nmy digital -----> garden !!! :\nyou can find my blogs at my website.\ngithub cli 1.0: all you need to know\npython 3.9: all you need to know\nweb scraping coronavirus data into ms excel\nhow to make your own google chrome extension\nthe developer dictionary \ud83c\udf31\ncheck out my latest videos on youtube:\nhow to make your own google chrome extension\nweb scraping coronavirus data into ms excel\nseptember leetcode playlist\nlet's connect! find me on the web.\nif you have any queries or suggestions, feel free to reach out to me.\nshow some \u2764\ufe0f by starring some of the repositories!", "sortedWord": "None", "removed": "Nan", "score": null, "comments": null, "media": "Nan", "medialink": "Nan", "identifyer": 7000904, "year": null}], "name": "gardenrobotics"}
{"interestingcomments": [{"autor": "elmosworld37", "date": "2019-11-22 00:22:58", "content": "[D] Voice Assistant: Better to use a model trained on commands or just use STT? /!/ I would like to make a deep-learning based voice assistant for an application I have that controls a digital -----> camera !!! . Some example commands are \"auto focus\", \"set zoom to 2\", \"turn off flash\", etc.\n\nI see two ways of going about this:\n\n1. Train a model that classifies an audio snippet as containing one of the commands or background noise. This seems easier than option 2 but also less robust, as I would have to retrain the model every time I add a new command. Also not sure how numbers would work (record myself saying every number up to like 100?).\n\n2. Use STT to convert audio to text and do some fuzzy string matching to see if it matches a command. I've downloaded Mozilla's DeepSpeech and it did not seem to work very well, so I'm guessing that creating a good STT model is very difficult.\n\nWhich of these is a better approach? Or is there some in-between approach that's even better?", "link": "https://www.reddit.com/r/MachineLearning/comments/dzsssi/d_voice_assistant_better_to_use_a_model_trained/"}, {"autor": "neuroguy6", "date": "2019-11-21 20:38:54", "content": "Looking for resources to build an -----> image !!!  recognition recommender system in python /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/dzpjcf/looking_for_resources_to_build_an_image/"}, {"autor": "milaworld", "date": "2019-02-05 07:55:56", "content": "[R] Flickr-Faces-HQ Dataset (FFHQ) /!/ [Flickr-Faces-HQ (FFHQ)](https://github.com/NVlabs/ffhq-dataset) is a high-quality -----> image !!!  dataset of human faces, originally created as a benchmark for generative adversarial networks (GAN), and used in the [StyleGAN](http://stylegan.xyz/paper) paper.\n\nThe dataset consists of 70,000 high-quality PNG images at 1024&amp;times;1024 resolution and contains considerable variation in terms of age, ethnicity and image background. It also has good coverage of accessories such as eyeglasses, sunglasses, hats, etc. The images were crawled from Flickr, thus inheriting all the biases of that website, and automatically aligned and cropped. Only images under permissive licenses were collected. Various automatic filters were used to prune the set, and finally Mechanical Turk was used to remove the occasional statues, paintings, or photos of photos.\n\nhttps://github.com/NVlabs/ffhq-dataset\n", "link": "https://www.reddit.com/r/MachineLearning/comments/anbze2/r_flickrfaceshq_dataset_ffhq/"}, {"autor": "d0_0d", "date": "2019-02-05 00:36:46", "content": "Flickr-Faces-HQ dataset released /!/ Flickr-Faces-HQ (FFHQ) is out now. \"... 70,000 high-quality PNG -----> image !!! s at 1024\u00d71024 resolution and contains considerable variation in terms of age, ethnicity and -----> image !!!  background.\" The dataset used for the Style-GAN paper. [https://github.com/NVlabs/ffhq-dataset \u2026](https://t.co/7ieYcC0N4F)", "link": "https://www.reddit.com/r/MachineLearning/comments/an8g06/flickrfaceshq_dataset_released/"}, {"autor": "abby621", "date": "2019-02-04 23:02:18", "content": "[R] Hotels-50K: A Global Hotel Recognition Dataset /!/ &amp;#x200B;\n\n*Processing img js4itys7tme21...*\n\nFor the last several years, my lab has worked on approaches to hotel recognition, with the goal of building global scale -----> image !!!  search systems to help human trafficking investigators locate what hotels victims of human trafficking are being photographed in. Our efforts included the creation of a smartphone application, called TraffickCam, which has been used by over 150,000 travelers to collect imagery that is more similar to investigative images than images that can be found on travel websites, and a global scale image search approach trained on this data to human trafficking investigators at the National Center for Missing and Exploited Children.\n\nTo support further advancement in this important and challenging problem domain, we released the Hotels-50K dataset at AAAI this past week.\n\n**Abstract**: \"Recognizing a hotel from an image of a hotel room is important for human trafficking investigations.  Images directly link victims to places and can help verify where victims have been trafficked, and where their traffickers might move them or others in the future.  Recognizing the hotel from images is challenging because of low image quality, uncommon camera perspectives, large occlusions (often the victim), and the similarity of objects (e.g., furniture, art, bedding) across different hotel rooms.\n\nTo support efforts towards this hotel recognition task, we have curated a dataset of over 1 million annotated hotel room images from 50,000 hotels. These images include professionally captured photographs from travel websites and crowd-sourced images from a mobile application, which are more similar to the types of images analyzed in real-world investigations.  We present a baseline approach based on a standard network architecture and a collection of data-augmentation approaches tuned to this problem domain.\"\n\n**Paper**: [https://www.aaai.org/Papers/AAAI/2019/AAAI-StylianouA.3453.pdf](https://www.aaai.org/Papers/AAAI/2019/AAAI-StylianouA.3453.pdf) \n\n**Code and dataset available at**: [https://github.com/GWUvision/Hotels-50K](https://github.com/GWUvision/Hotels-50K)", "link": "https://www.reddit.com/r/MachineLearning/comments/an7jz7/r_hotels50k_a_global_hotel_recognition_dataset/"}, {"autor": "invertedpassion", "date": "2019-02-04 16:53:48", "content": "[P] Making deep neural networks paint to understand how they work /!/ Hello,\n\nI wrote a tutorial on **generating abstract art using compositional pattern producing networks**. In the process of generating images, I explore various things like the impact of additional hidden layers, behavior of activation functions, and impact of more neurons per layer to gain insights into how deep neural networks work. \n\nThe code is short (&lt;100 lines) and is written in PyTorch. Following is an example of an -----> image !!!  generated:\n\n*Processing img a3yuwdt50le21...*\n\nCheck out the tutorial here: [Making deep neural networks paint to understand how they\u00a0work](https://towardsdatascience.com/making-deep-neural-networks-paint-to-understand-how-they-work-4be0901582ee) and access the [code here](https://github.com/paraschopra/abstract-art-neural-network).\n\nHope you like the project. Feedback welcome :)", "link": "https://www.reddit.com/r/MachineLearning/comments/an3kwz/p_making_deep_neural_networks_paint_to_understand/"}, {"autor": "DGs29", "date": "2019-03-14 05:56:59", "content": "[D] How to detect paragraphs with less line spaces in document images? /!/ I have asked a similar [previous question](https://www.reddit.com/r/MachineLearning/comments/axy95y/d_how_to_detect_text_blocks_in_document_images/). I can now able to detect blocks of text. But when it comes to paragraphs with less line spaces my method doesn't detect properly. \n\nThis is how I wanted to detect:\n\n&amp;#x200B;\n\nhttps://i.redd.it/9v3itc78w0m21.png\n\nBut this is what I can achieve:\n\n&amp;#x200B;\n\nhttps://i.redd.it/dhtwfjdcw0m21.jpg\n\nHow I did:\n\n1. First I placed bounding boxes over individual characters.\n2. Next, I binarized and inverted colors for the -----> image !!! .\n3. Apply dilation.\n4. Then, I placed bounding box to that dilated image.\n\nThis method works well for paragraphs which are placed at a decent line-spaces. But when they're close to each other, it doesn't segment the paragraphs separately.\n\nWhat are the necessary changes to be made to achieve my result?", "link": "https://www.reddit.com/r/MachineLearning/comments/b0wp2d/d_how_to_detect_paragraphs_with_less_line_spaces/"}, {"autor": "TiredOldCrow", "date": "2019-03-13 21:08:24", "content": "[D] Irresponsible anthropomorphism is killing AI journalism /!/ Basically the title.  The current state of media coverage of AI is fixated on constructing a compelling narrative to readers, and often personifies models well beyond their capabilities.  This is to the extent that articles almost always end up reading like every classifier is some form of limited AGI.\n\nTake [\"Meet Norman the Psychopathic AI\"](https://www.bbc.com/news/technology-44040008), an article by the BBC, whom I generally consider quite capable journalists.  While the research methodology and some of the implications are discussed in the article, the majority of laypeople who encounter the article will likely erroneously conclude that Norman possesses beliefs, a worldview, and some dark outlook on humanity.  Some readers will think \"Norman\" is violent or dangerous, with a mind of his own.  A headline and an -----> image !!!  go a long way in communication, especially online.\n\nAnd this article is by far not the worst offender. Many news outlets perform much worse, publishing misleading, fearmongering, or sensationalist stories about \"some new AI\", borrowing from pop sci-fi tropes, with the star AI inevitably represented by lacklustre CG avatars bought off stock photo websites.\n\nI remember having several discussions in the wake of the Facebook experiment where researchers had AIs communicate, and saw they developed a communication standard unreadable by humans.  Based on the articles that circulated afterwards, a significant number of people concluded \"they had to turn it off because they were on the verge of SKYNET\".\n\nIn the interests of doing more than just ranting: how do we deal with this as a community?  Should we be reaching out to journalists about these issues?  Is it our responsibility in interviews to communicate the limitations of the models we develop?\n\nPersonifying the projects we work on, and giving them human qualities, is certainly entertaining and helps market our research.  That said, it seems like a sizeable portion of the public has been misinformed about the state of machine learning research as a result.\n", "link": "https://www.reddit.com/r/MachineLearning/comments/b0rdsi/d_irresponsible_anthropomorphism_is_killing_ai/"}, {"autor": "notfirecrow", "date": "2019-03-12 18:18:06", "content": "Is it possible to use a CNN for anything other than -----> image !!!  classification? /!/ Also, is it possible to have a NN take text as an input, or do I have to convert it to numerical data using a model first?", "link": "https://www.reddit.com/r/MachineLearning/comments/b0b2u3/is_it_possible_to_use_a_cnn_for_anything_other/"}, {"autor": "ashutoshbsathe", "date": "2019-03-12 17:05:05", "content": "[D] CycleGAN model collapse - Any bright ideas ? /!/ Hi,\n\nI've been using CycleGAN for converting gameplay of 1989 Prince of Persia 1 to its newer version Prince of Persia 2. I've collected 8000 images of both the games and resized them into 320x200 dimensions. Then I'm using CycleGAN's TensorFlow implementation by [vanhuyz](https://github.com/vanhuyz/CycleGAN-TensorFlow) to train the network. \n\nAfter about 2400 steps, all of the outputs are blackish. \n\n![Imgur](https://i.imgur.com/yFcDay8.png)\n\nOR [link to -----> image !!!  if it doesn't load](https://i.imgur.com/yFcDay8.png)\n\nIt seems that in both the cycles (A-&gt;B-&gt;A) and (B-&gt;A-&gt;B) reconstruction works alright but overall conversion is very poor.\n\nThe original paper mentions the idea of adding an identity loss for preserving input colors. But I didn't understand it correctly. Can anyone please explain ?\n\nI'm also open to other bright ideas.\n\nThanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/b0a7qq/d_cyclegan_model_collapse_any_bright_ideas/"}, {"autor": "reditor_1234", "date": "2019-12-29 17:22:50", "content": "Super Resolution is like magic ! (from a super fuzzy low res -----> image !!!  to super detailed shown in one picture)", "link": "https://www.reddit.com/r/MachineLearning/comments/eh8jfn/super_resolution_is_like_magic_from_a_super_fuzzy/"}, {"autor": "cansozbir", "date": "2019-12-28 20:49:03", "content": "[P] -----> Image !!!  + Text input classification /!/ Hi, I'm trying to build this network with inspiration from this article.\n\n[Classifying e-commerce products based on images and text](http://cbonnett.github.io/Insight.html)\n\nHe's trying to predict a product's label from given 1 image input and 1 product name text input.\n\nMy data set have 6 attributes (5 image and 1 text input) and 1 class label(output). So I want to create a model which takes 5 product image inputs + 1 description text input and predict that product's category.\n\n&amp;#x200B;\n\n[model\\_architecture](https://preview.redd.it/tfnlg7hsqf741.png?width=979&amp;format=png&amp;auto=webp&amp;s=73717c23853b8df5dc80bbc130028e83cd64310a)\n\nMy questions are;\n\n1. I thought ,for image part, instead of merging 5 image into 1 and passing it to a CNN feature extractor, creating 1 CNN feature extractor(into the blue box) and using it 5 times for 5 image with same weights would help. Am I right?\n2. Author is using pre-trained VGG-16 for image feature extraction and he has write that in 2014. Should I change that extractor or not? If so, I have take a look the state of the art classification algorithms from [there](https://paperswithcode.com/sota/image-classification-on-imagenet) and saw EfficientNets have pretty good results. Or, even if it's not a SOTA algorithm, I have used Darknet-53 for different task. How should I choose my extractor? Should I try all of them and find which one is better?\n3. I said 5 image + 1 text but actually there are up to 5 images for each product. Users can upload 1 to 5 images. So there are products with 1 to 5 images in my training set. Would it help feeding the network with 3 image + 2 zero matrix if I have 3 images for a product ?\n4. I wrote \"RNN\" into the image but I have no idea what to do for text feature extraction part. The author is using a bag of words model. Should I go with that? Or do you know any better, SOTA, idea for text feature extraction ? I took Andrew NG's deeplearning courses and saw something like that for sentiment classification:\n\n&amp;#x200B;\n\n[rnn\\_for\\_extraction](https://preview.redd.it/mvza1dnuqf741.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=f2fc151e5cc13c7e27fa71a92eacafbbcac0eec4)\n\nHow can effect using something like this for text feature extraction(without softmax) ? Should I even do this?", "link": "https://www.reddit.com/r/MachineLearning/comments/egw16o/p_image_text_input_classification/"}, {"autor": "donutloop", "date": "2019-12-28 20:10:01", "content": "GitHub - LeukemiaResearchFoundation/ALL-IDB-Classifiers: Classifiers created with various languages and frameworks, using Fabio Scotti's ALL-IDB (Acute Lymphoblastic Leukemia -----> Image !!!  Database for -----> Image !!!  Processing) dataset.", "link": "https://www.reddit.com/r/MachineLearning/comments/egvix8/github/"}, {"autor": "shockingly-immoral", "date": "2019-12-28 19:15:21", "content": "[D] Is there software that I can \"train\" to recognize my handwriting and convert to text? (Or actually decent OCR software out there?) /!/ I recently got an app that allows me to write on a designated notepad, take a -----> picture !!! , convert to a PDF, and then export. This is super cool, but it would be even better if I could convert the handwritten PDFs into text. I've read some about OCR but I'm wondering if there is some software out there I could train to recognize specifically *my* handwriting. I imagine it would be a lot less error prone because it would be personalized, but I'm not sure if there's such a thing out there. \n\n&amp;#x200B;\n\nI did work in an AI lab and understand the basics of how training the programs work, so I know it would technically be possible, but is this software out there somewhere already? Could I foreseeably train some rudimentary program with open source software available on github or the like? Or, maybe is there just some pretty good program out there already that's decent enough to recognize fairly poor, half-cursive handwriting?\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/egut5s/d_is_there_software_that_i_can_train_to_recognize/"}, {"autor": "muruganr333", "date": "2019-12-28 09:33:40", "content": "Flask tutorial - -----> Image !!!  classification", "link": "https://www.reddit.com/r/MachineLearning/comments/egow3o/flask_tutorial_image_classification/"}, {"autor": "lstmcnn", "date": "2019-12-27 08:29:28", "content": "[D] The 1997 LSTM paper by Hochreiter &amp; Schmidhuber has become the most cited deep learning research paper of the 20th century /!/ - Long short-term memory. S Hochreiter, J Schmidhuber. Neural computation, MIT Press, 1997 (26k citations as of 2019)\n\nIt has passed the backpropagation papers by Rumelhart et al. (1985, 1986, 1987). Don't get confused by Google Scholar which sometimes incorrectly lumps together different Rumelhart publications including: \n\n- Learning internal representations by error propagation. DE Rumelhart, GE Hinton, RJ Williams, California Univ San Diego La Jolla, Inst for Cognitive Science, 1985 (25k)\n\n- Parallel distributed processing. JL McClelland, DE Rumelhart, PDP Research Group, MIT press, 1987 (24k)\n\n- Learning representations by back-propagating errors. DE Rumelhart, GE Hinton, RJ Williams, Nature 323 (6088), 533-536, 1986 (19k) \n\nI think it's good that the backpropagation paper is no longer number one, because it's a bad role model. It does not cite the true inventors of backpropagation, and the authors have never corrected this. I learned this on reddit: [Schmidhuber on Linnainmaa, inventor of backpropagation in 1970](https://www.reddit.com/r/MachineLearning/comments/e5vzun/d_jurgen_schmidhuber_on_seppo_linnainmaa_inventor/). This post also mentions Kelley (1960) and Werbos (1982). \n\nThe LSTM paper is now receiving more citations per year than all of Rumelhart's backpropagation papers combined. And  more than the most cited paper by LeCun and Bengio (1998) which is about CNNs: \n\n- Gradient-based learning applied to document recognition. Y LeCun, L Bottou, Y Bengio, P Haffner, IEEE 86 (11), 2278-2324, 1998 (23k)\n \nIt may soon have more citations than Bishop's textbook on neural networks (1995).  \n\nIn the 21st century, activity in the field has surged, and I found three deep learning research papers with even more citations. All of them are about applications of neural networks to ImageNet (2012, 2014, 2015). One paper describes a fast, CUDA-based, deep CNN (AlexNet) that won ImageNet 2012. Another paper describes a significantly deeper CUDA CNN that won ImageNet 2014:  \n\n- A Krizhevsky, I Sutskever, GE Hinton. Imagenet classification with deep convolutional neural networks. NeuerIPS 2012 (53k) \n\n- B. K Simonyan, A Zisserman. Very deep convolutional networks for large-scale -----> image !!!  recognition. arXiv:1409.1556, 2014 (32k)\n\nThe paper with the most citations per year is a recent one on the much deeper ResNet which won ImageNet 2015: \n\n- K He, X Zhang, S Ren, J Sun. Deep Residual Learning for Image Recognition. CVPR 2016 (36k; 18k in 2019)\n\nRemarkably, such \"contest-winning deep GPU-based CNNs\" can also be traced back to the Schmidhuber lab. Krizhevsky cites DanNet, the first CUDA CNN to win image recognition challenges and the first superhuman CNN (2011). I learned this on reddit: [DanNet, the CUDA CNN of Dan Ciresan in J\u00fcrgen Schmidhuber's team, won 4 image recognition challenges prior to AlexNet](https://www.reddit.com/r/MachineLearning/comments/dwnuwh/d_dannet_the_cuda_cnn_of_dan_ciresan_in_jurgen/): ICDAR 2011 Chinese handwriting contest - IJCNN 2011 traffic sign recognition contest - ISBI 2012 image segmentation contest - ICPR 2012 medical imaging contest.  \n\nResNet is much deeper than DanNet and AlexNet and works even better. It cites the [Highway Net](http://people.idsia.ch/~juergen/highway-networks.html) (Srivastava &amp; Greff &amp; Schmidhuber, 2015) of which it is a special case. In a sense, this closes the LSTM circle, because \"Highway Nets are essentially feedforward versions of recurrent Long Short-Term Memory (LSTM) networks.\"\n\nMost LSTM citations refer to the 1997 LSTM paper. However, Schmidhuber's [post on their Annus Mirabilis](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html#Sec.%204) points out that \"essential insights\" for LSTM date back to Seep Hochreiter's 1991 diploma thesis which he considers \"one of the most important documents in the history of machine learning.\" (He also credits other students: \"LSTM and its training procedures were further improved\" \"through the work of my later students Felix Gers, Alex Graves, and others.\")\n\nThe LSTM principle is essential for both recurrent networks and feedforward networks. Today it is on every smartphone. And in Deepmind's Starcraft champion and OpenAI's Dota champion. And in thousands of additional applications. It is the core of the deep learning revolution.", "link": "https://www.reddit.com/r/MachineLearning/comments/eg8mmn/d_the_1997_lstm_paper_by_hochreiter_schmidhuber/"}, {"autor": "mathowned", "date": "2019-09-11 22:47:45", "content": "[P] How to identify only one object on screen? /!/ Hello everyone!\n\nI'm currently doing a CS Project which identifies one object in a game running on window mode and take actions by itself (by autoclicking or pressing a key).  Basically, I will set a canvas on the up right side of my desktop, match it with the game window size and then start trying to identify one specific object. For exemple, i use an -----> image !!!  of a monster in the identification algorithm, if it appears on screen and matches, it takes an action.\n\n&amp;#x200B;\n\nI'm just a beginner in ML... Do I need a neural net for doing that object identification? If so, how do i create a dataset if it's only trying to identify one specific object? \n\nI accept literally every possible tip you guys can get me. Libraries I can use, image processing tips for that... anything.\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/d2yh74/p_how_to_identify_only_one_object_on_screen/"}, {"autor": "varkarrus", "date": "2019-09-11 13:57:28", "content": "[D] How do you feel Machine Learning will affect video games? /!/ I'm studying to become a video game designer, and these advances in machine learning are proooobably gonna affect my chosen career at some point. I know there's stuff already, like AlphaStar being a Pretty Darn Good SC2 AI, and that paper where character face sliders are preselected based off of a -----> photo !!! .\n\nThere's one application I can already see happening in the short term: right now, in story driven games, characters awkwardly avoid referring to the player by their name (with one notable exception in Fallout 4). But, I could easily see AIs trained off of a game's voice actors adjusting in-game recorded dialogue to include synthesized clips of the player's name. Even on lower end machines, these clips could be generated in the background so they're ready when needed.", "link": "https://www.reddit.com/r/MachineLearning/comments/d2qkvf/d_how_do_you_feel_machine_learning_will_affect/"}, {"autor": "gecko39", "date": "2019-09-10 15:14:41", "content": "[D] Advice on domain adaption / transfer? /!/ I am training a 2d -----> image !!!  landmark estimation network where I have lots of synthetic data, but limited realistic labelled data. I can gather lots of un-labelled real data, but labelling is difficult. I have come across a few techniques out there like [DANN](http://sites.skoltech.ru/compvision/projects/grl/) As a non-expert, I'm not sure where exactly to start. Are there any common techniques that are worth trying first? Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/d28xyh/d_advice_on_domain_adaption_transfer/"}, {"autor": "etotheipi_", "date": "2019-09-10 12:56:58", "content": "Wireless Video Camera for ML? /!/ Hi, I'm looking to do a simple home project involving object identification.  I want to put a video -----> camera !!!  on a tree in my front yard and try to automatically identify what triggered motion captures in my driveway.  It's a really simple project, mostly to get my feet wet.\n\nThe problem is, I have no idea how to search for cameras that allow me to pull the video from them to my computer (Windows or Linux).  It doesn't have to be exactly real-time, but I'd like to not have to physically go do something with the camera to get the video (some of them mention SD card capture -- which I would do as a last resort).\n\nSo I need a camera that:\n\n\\- Wireless/battery-powered (it'll be in a tree)  \n\\- Wifi-connected to talk to...something inside my network  \n\\- Video captures are accessible on my Linux or Windows boxes (I don't mind doing some hackery with API keys, downloading from S3 buckets, etc, if necessary)", "link": "https://www.reddit.com/r/MachineLearning/comments/d275sj/wireless_video_camera_for_ml/"}, {"autor": "jweir136", "date": "2019-08-05 19:05:05", "content": "[P] Building NSFW -----> Image !!!  Detector /!/ I have currently started building a nsfw image filter. I currently have a dataset of 120k nsfw images. \n\nThe main issue I am having is with the anomaly detection part. All of the images are so different, I can't see how I can easily build a system to find if an image is nsfw or not. \n\nAny advice or notes is greatly appreciated. Thanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/cmfbki/p_building_nsfw_image_detector/"}, {"autor": "AruniRC", "date": "2019-04-18 20:16:10", "content": "[R] A Bayesian Perspective on the Deep Image Prior - CVPR 2019 /!/ **Abstract:** \n\nThe [deep -----> image !!!  prior](https://dmitryulyanov.github.io/deep_-----> image !!! _prior)  was recently introduced as a prior for natural -----> image !!! s. It represents  images as the output of a convolutional network with random inputs. For  \u201cinference\u201d, gradient descent is performed to adjust network parameters  to make the output match observations. This approach yields good  performance on a range of image reconstruction tasks. We show that the  deep image prior is asymptotically equivalent to a stationary Gaussian  process prior in the limit as the number of channels in each layer of  the network goes to infinity, and derive the corresponding kernel. This  informs a Bayesian approach to inference. We show that by conducting  posterior inference using stochastic gradient Langevin we avoid the need  for early stopping, which is a drawback of the current approach, and  improve results for denoising and impainting tasks. We illustrate these  intuitions on a number of 1D and 2D signal reconstruction tasks.  \n\n&amp;#x200B;\n\n**Project page:** [https://people.cs.umass.edu/\\~zezhoucheng/gp-dip/](https://people.cs.umass.edu/~zezhoucheng/gp-dip/)", "link": "https://www.reddit.com/r/MachineLearning/comments/beq6v6/r_a_bayesian_perspective_on_the_deep_image_prior/"}, {"autor": "AruniRC", "date": "2019-04-18 20:13:03", "content": "[R] Objects as Points /!/ &amp;#x200B;\n\nhttps://i.redd.it/kc4e1b3ly2t21.png\n\n**Abstract:** \n\nDetection identifies objects as axis-aligned boxes in an -----> image !!! . Most  successful object detectors enumerate a nearly exhaustive list of  potential object locations and classify each. This is wasteful,  inefficient, and requires additional post-processing. In this paper, we  take a different approach. We model an object as a single point -- the  center point of its bounding box. Our detector uses keypoint estimation  to find center points and regresses to all other object properties, such  as size, 3D location, orientation, and even pose. Our center point  based approach, CenterNet, is end-to-end differentiable, simpler,  faster, and more accurate than corresponding bounding box based  detectors. CenterNet achieves the best speed-accuracy trade-off on the  MS COCO dataset, with 28.1% AP at 142 FPS, 37.4% AP at 52 FPS, and 45.1%  AP with multi-scale testing at 1.4 FPS. We use the same approach to  estimate 3D bounding box in the KITTI benchmark and human pose on the  COCO keypoint dataset. Our method performs competitively with  sophisticated multi-stage methods and runs in real-time.\n\n&amp;#x200B;\n\n**Github:** [https://github.com/xingyizhou/CenterNet](https://github.com/xingyizhou/CenterNet)", "link": "https://www.reddit.com/r/MachineLearning/comments/beq5mx/r_objects_as_points/"}, {"autor": "strangecosmos", "date": "2019-04-18 02:58:18", "content": "[D] Large-scale imitation learning/apprenticeship learning for self-driving cars /!/ *Quick summary:* Imitation learning for self-driving cars is confounded by the DAgger problem, but this problem is in principle soluble by scaling up training data, as AlphaStar has demonstrated. Tesla appears to be trying this right now with ~450,000 drivers. \n\n***\n\nEver since DeepMind showed with AlphaStar that you can get to [human-level performance](https://twitter.com/oriolvinyalsml/status/1094670648042012673?s=21) on StarCraft with imitation learning alone, I've been obsessed with the idea of applying imitation learning to self-driving cars on a similar scale. \n\nWaymo has experimented with imitation learning on a very small scale (just ~1,400 hours of driving). ([blog post](https://link.medium.com/T2tem8DtYV) | [paper](https://arxiv.org/abs/1812.03079)) Waymo's experiment with their imitation network, ChauffeurNet, felt like a Rorschach test. Some deep learning/autonomous vehicle people on Twitter interpreted it as showing that imitation learning doesn't work. Others reacted the opposite way, seeing it as a promising direction for future R&amp;D.  \n\nLarge-scale imitation learning is more exciting to me because AlphaStar is such a compelling proof of concept. Alex Irpan, a reinforcement learning researcher, has a great explanation [on his blog](https://www.alexirpan.com/2019/02/22/alphastar-part2.html):\n\n&gt; One of the problems with imitation learning is the way errors can compound over time. I\u2019m not sure if there\u2019s a formal name for this. I\u2019ve always called it the [DAgger](https://www.ri.cmu.edu/pub_files/2011/4/Ross-AISTATS11-NoRegret.pdf) problem, because that\u2019s the paper that everyone cites when talking about this problem ([Ross et al, AISTATS 2011](https://www.ri.cmu.edu/pub_files/2011/4/Ross-AISTATS11-NoRegret.pdf)).\n&gt;\n&gt; ... This problem means mistakes in imitation learning often aren\u2019t recoverable, and the temporal nature of the problem means that the longer your episode is, the more likely it is that you enter this negative feedback loop, and the worse you\u2019ll be if you do. ...\n&gt;\n&gt; Due to growing quadratically in *_T_*, we expect long-horizon tasks to be harder for imitation learning. A StarCraft game is long enough that I didn\u2019t expect imitation learning to work at all. And yet, imitation learning was good enough to reach the level of a Gold player. \n&gt;\n&gt; ... If you have a very large dataset, from a wide variety of experts of varying skill levels (like, say, a corpus of StarCraft games from anyone who\u2019s ever played the game), then it\u2019s possible that your data already has enough variation to let your agent learn how to recover from several of the incorrect decisions it could make.\n\nSo, AlphaStar has shown us that one potential solution to the compounding errors that arise with supervised imitation learning/behavioural cloning (the DAgger problem) is to collect a massive and highly varied dataset that includes a lot of errors, and a lot of recovering from errors. Counterintuitively, humans are teaching the AI by doing it wrong!\n\nIt was [recently reported](https://www.theinformation.com/articles/what-makes-teslas-autopilot-different) in The Information that Tesla is taking a behavioural cloning approach to self-driving. Tesla has [around 450,000](https://hcai.mit.edu/tesla-vehicle-numbers/) drivers with the latest generation of sensor hardware, which includes eight cameras covering 360 degrees around the car. Here's what The Information said:\n\n&gt; Tesla\u2019s cars collect so much -----> camera !!!  and other sensor data as they drive around, even when Autopilot isn\u2019t turned on, that the Autopilot team can examine what traditional human driving looks like in various driving scenarios and mimic it, said the person familiar with the system. It uses this information as an additional factor to plan how a car will drive in specific situations\u2014for example, how to steer a curve on a road or avoid an object.\n&gt;\n&gt;Such an approach has its limits, of course: behavior cloning, as the method is sometimes called, cannot teach an automated driving system to handle dangerous scenarios that cannot be easily anticipated. That\u2019s why some autonomous vehicle programs are wary of relying on the technique.\n&gt;\n&gt;But Tesla\u2019s engineers believe that by putting enough data from good human driving through a neural network, that network can learn how to directly predict the correct steering, braking and acceleration in most situations. \u201cYou don\u2019t need anything else\u201d to teach the system how to drive autonomously, said a person who has been involved with the team. They envision a future in which humans won\u2019t need to write code to tell the car what to do when it encounters a particular scenario; it will know what to do on its own.\n\nAnother potential solution is to give the imitation agent [access to an expert/human demonstrator](https://youtu.be/V7CY68zH6ps?t=36m27s) when it makes an error and doesn't know how to recover. If a vehicle drives up onto a sidewalk, and there was never any sidewalk state-action pairs in its training dataset, then you can get a human to demonstrate what to do in that situation. The problem is this is obviously very labour intensive. You need a lot of demonstrators ready to take over when an error occurs.\n\nStrikingly, this seems to be exactly what Tesla is doing. Elon Musk [recently described](https://pca.st/episode/8e374af4-3951-47ae-aafa-3716d2d916a8?t=676.0) something that sounds like this solution to the DAgger problem:\n\n&gt; Well, there\u2019s a lot of things that are learnt. There are certainly edge cases where say somebody\u2019s on Autopilot and they take over. And then, okay, that\u2019s a trigger that goes into our system that says, okay, did they take over for convenience, or did they take over because the Autopilot wasn\u2019t working properly.\n&gt;\n&gt; There\u2019s also like, let\u2019s say we\u2019re trying to figure out what is the optimal spline for traversing an intersection. Then, the ones where there are no interventions are the right ones. So you then say okay, when it looks like this, do the following. And then you get the optimal spine for navigating a complex intersection.\n\nElon later [said](https://twitter.com/elonmusk/status/1117154663537623040?s=20) on Twitter:\n\n&gt; Your interventions do train the NN [neural network]\n\nThis sounds like the neural network is sampling human demonstrations when it makes an error. In theory, it could be reinforcement learning rather than imitation/apprenticeship learning. Any thoughts on whether it would make sense to use RL instead of IL here? \n\nA totally different approach is to use a GAN and do generative adversarial imitation learning (GAIL). In [one paper](https://arxiv.org/pdf/1701.06699.pdf), GAIL did worse than behavioural cloning on short time scales (~2 seconds) but better over long time scales. There's also [inverse reinforcement learning](https://thegradient.pub/learning-from-humans-what-is-inverse-reinforcement-learning/). So, there are a bunch of different ideas to explore in this area. \n\nSo, to summarize:\n\n* AlphaStar showed behavioural cloning can solve a complex, tactical, multi-agent task with an astronomically large, continuous action space \u2014 like driving! The solution is a massive and highly varied dataset with a lot of human errors.\n\n* When behavioural cloning falls short, another potential solution is to allow the neural network to ask a human for a demonstration when it makes an error. \n\n* Tesla appears to be collecting a massive and highly varied dataset with a lot of human errors for behavioural cloning of the driving task.\n\n* Tesla also appears to allowing its NN to sample human demonstrations when the NN makes an error, unless this is actually reinforcement learning. \n\nThis is so exciting to me. The only big difference I can think of between StarCraft and driving is the obvious one: AlphaStar just plugged into the game's API, whereas to deploy a self-driving car you need to solve computer vision. Besides that, I can't think of anything. Can y'all? \n\nOne way in which driving is actually easier than StarCraft is the time horizon. Driving is a sequence of short time horizon tasks. For example, the time horizon for navigating an intersection is short. Once a car is through the intersection, its actions don't depend on its past actions or previously observed states. \n\nBefore AlphaStar, imitation learning felt a lot more dubious. Now it feels like a proven solution. We might be within spitting distance of honest-to-God self-driving cars. \n\nEither I'm way too optimistic about this, or a lot of people are missing something big. So, which is it? Am I overlooking important differences between StarCraft and driving? Is it wrong to assume the difference between ChauffeurNet and AlphaStar is just scale?", "link": "https://www.reddit.com/r/MachineLearning/comments/begm91/d_largescale_imitation_learningapprenticeship/"}, {"autor": "hellomotorcycle", "date": "2019-04-17 22:02:47", "content": "Would like help understanding weights and biases in CNNs for -----> image !!!  classification /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/bedof2/would_like_help_understanding_weights_and_biases/"}, {"autor": "cocobananaohhohh", "date": "2019-04-17 20:34:03", "content": "confused about weight bias update with CNNs for an -----> image !!!  classifier /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/becou2/confused_about_weight_bias_update_with_cnns_for/"}, {"autor": "cocobananaohhohh", "date": "2019-04-17 19:55:32", "content": "Would like help understanding CNN weight and bias updates for -----> image !!!  classification /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/bec94h/would_like_help_understanding_cnn_weight_and_bias/"}, {"autor": "gargomeister", "date": "2019-11-01 18:11:35", "content": "Which Experiment tracking tool are you using? /!/ Hello, r/MachineLearning's people! \n\nI plan on using a cloud solution for experiment tracking in my different projects (which are mostly -----> image !!!  and text classification tasks using Deep Learning).  \n\nI found several tools online: \n\n[https://www.comet.ml](https://www.comet.ml/)\n\n[https://neptune.ml](https://neptune.ml/)\n\n[https://www.wandb.com](https://www.wandb.com/)\n\n[https://github.com/allegroai/trains \u2011 ](https://github.com/allegroai/trains)  \n\n\nBy looking at them and trying them on several experiments, I noticed that they almost are providing the same feature (except versioning for some of them).  \n\n\nI would like to have your feedback after using them for a long time (what are the drawbacks and advantages of the solution that you are using). \n\nHere are observations that I was able to make after trying them:\n\n[https://www.comet.ml](https://www.comet.ml/) \n\n- Pretty insightful interface\n- Used by some famous companies\n- Easy to use\n\n[https://neptune.ml](https://neptune.ml/)\n\n- Integration with a lot of frameworks (such as MLFLOW)\n- Versioning of notebook\n- Good collaboration tool for each experiment \n- Easy to comment, edit and plan the next experiments.\n- Look like their service connection is not great (on AWS cluster, even by whitelisting the service, experienced a lot of HTTP errors)\n- No automatic metric recording (you have to create your own callback)\n\n[https://www.wandb.com](https://www.wandb.com/)\n\n- The interface is really clean\n- Experiments are easy to implement\n- Tracking a lot of different metrics\n- Really good to generate reports in order to present experiments.\n- \n\n[https://github.com/allegroai/trains \u2011 ](https://github.com/allegroai/trains)\n\n- Open-source project \ud83d\ude80 \n- Noisy interface\n- Quite easy to implement.", "link": "https://www.reddit.com/r/MachineLearning/comments/dq80aw/which_experiment_tracking_tool_are_you_using/"}, {"autor": "devinjeon", "date": "2019-10-31 19:26:33", "content": "[P] Try Jokerise to test your joker makeup! Implemented \"Jokerise\" project(-----> image !!! -to------> image !!!  transition) on web /!/ Hi, Today is Halloween(it may already be gone in some timezones)!\n\n*Try Jokerise to test your joker makeup!*\n\n* https://jokerise.hyojun.me/\n   * https://github.com/devinjeon/jokerise-web\n   * https://github.com/junkwhinger/jokerise\n\nI just wanted to inform my toy project :) Actually this toy project is a web service based on my friend's toy project named [jokerise](https://github.com/junkwhinger/jokerise) using CycleGAN.\n\nWhen I heard about the project, I just thought it would be better to serve this project on web so that we can use it more easily. So I've made the web service for that.\n\nYou can see the previous post written by the jokerise project's owner:\nhttps://www.reddit.com/r/MachineLearning/comments/dhrz0o/p_jokeriser_imagetoimage_translation_toy_project/", "link": "https://www.reddit.com/r/MachineLearning/comments/dpsiri/p_try_jokerise_to_test_your_joker_makeup/"}, {"autor": "currantts", "date": "2019-12-27 00:41:18", "content": "[D] Utter Newbie Here: is it possible to use ML to determine if an -----> image !!!  is 'good' or 'bad'? /!/ i have a collection of about 30,000 images that i have a script tweeting out every ten mins.  the images are generally 'aesthetically pleasing' but theres a bunch of random images in there thats making me have to moderate the tweets, so this process is less autonomous than id like.  \n\nis there a way using TS or otherwise to feed a model say \\~100 images that i handpick and it be able to tell me how 'alike' the image is to the other images? it seems hard bc im not classifying nude images (for example) so training the model would be difficult, right? im classifying a bunch of different things - fashion, landscapes, plants, etc.  is determining if an image is 'aesthetic' too abstract for ML?  If you have any resources please let me know! thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/eg3p31/d_utter_newbie_here_is_it_possible_to_use_ml_to/"}, {"autor": "currantts", "date": "2019-12-27 00:31:02", "content": "Utter Newbie To ML: is it possible to train a model to recognize 'good' from 'bad' images? /!/ i have a collection of about 30,000 images that a i have a script tweeting out every ten mins.  the images are generally 'aesthetically pleasing' but theres a bunch of random images in there thats making me moderate the tweets, so this process is less autonomous than id like.  \n\nis there a way using TS or otherwise to feed a model say \\~100 -----> image !!! s that i handpick and it be able to tell me how 'alike' the -----> image !!!  is to the other -----> image !!! s? it seems hard bc im not classifying nude images (for example) so training the model would be difficult, right? im classifying a bunch of different things - fashion, landscapes, plants, etc.  is determining if an image is 'aesthetic' too abstract for ML?  If you have any resources please let me know! thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/eg3kji/utter_newbie_to_ml_is_it_possible_to_train_a/"}, {"autor": "skud14", "date": "2019-12-26 13:57:28", "content": "Give me an -----> image !!!  you want upscaled and I will upscale it using machine learning", "link": "https://www.reddit.com/r/MachineLearning/comments/efvoya/give_me_an_image_you_want_upscaled_and_i_will/"}, {"autor": "hotpot_ai", "date": "2019-12-25 01:44:15", "content": "[D] Evaluating \"A Neural Algorithm of Artistic Style\" by Gatys et al. /!/ Paper: [https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576)\n\nThis is an exciting paper because it's the first to introduce artistic style transfer using pre-trained neural networks.\n\nWhat\u2019s great is how the paper demonstrated how to extract content (e.g., shapes, contours) from an -----> image !!!  and how to extract style from multiple layers (e.g., some layers extract fine-grained styles while others extract overall style). Combining these techniques can yield realistic and professional results.\n\nThe main issue is timing and compute power required. The paper extracts content features of an image with VGG19 and extracts style features from multiple layers with VGG19. Because it requires extensive compute power for feature extraction and for optimizing style and content loss, it takes 500-1000 iterations just to produce a low-resolution image. It would be ideal if the algorithm could produce results in a few iterations.\n\n1. Has anybody tried RESNET or any other state of the art network instead of VGG19?\n2. Does anyone know of a transformer network that produces similar results to this paper?\n3. Any recommendations on better style transfer papers?", "link": "https://www.reddit.com/r/MachineLearning/comments/efaak6/d_evaluating_a_neural_algorithm_of_artistic_style/"}, {"autor": "elmsha", "date": "2019-12-24 12:17:47", "content": "[D] Decision Tree Splitting strategy /!/  I have a dataset with 4 categorical features (Cholesterol, Systolic Blood pressure, diastolic blood pressure, and smoking rate). I use a decision tree classifier to find the probability of stroke. I am trying to verify my understanding of the splitting procedure done by Python Sklearn. Since it is a binary tree, there are three possible ways to split the first feature which is either to group categories {0 and 1 to a leaf, 2 to another leaf} or {0 and 2, 1}, or {0, 1 and 2}. What I know (please correct me here) is that the chosen split is the one with the least information gain (Gini impurity).\n\nI have calculated the Gini impurity for each of the three grouping scenarios:\n\n{0 + 1 , 2} --&gt; 0.17\n\n{0 + 2 , 1} --&gt; 0.18\n\n{1 + 2 , 0} --&gt; 0.004\n\nHowever, sklearn's decision tree chose the first scenario instead of the third (please check the -----> picture !!! ).\n\nCan anyone please help clarify the reason for selecting the first scenario? is there a priority for splits that results in pure nodes. thus selecting such a scenario although it has less information gain?\n\nhttps://preview.redd.it/mkve4teopk641.jpg?width=1319&amp;format=pjpg&amp;auto=webp&amp;s=fe487bedf67bc812d720ae2fe595fc41d9589dda", "link": "https://www.reddit.com/r/MachineLearning/comments/ef0nvh/d_decision_tree_splitting_strategy/"}, {"autor": "alexyalunin", "date": "2019-02-23 01:12:37", "content": "Cats GAN thiscatdoesnotexist.com learned to generate cats with watermarks and site caption below -----> image !!! ", "link": "https://www.reddit.com/r/MachineLearning/comments/atpggg/cats_gan_thiscatdoesnotexistcom_learned_to/"}, {"autor": "iLikeTurtles817", "date": "2019-02-22 20:42:08", "content": "Intro Machine Learning Project /!/ So I'm currently doing a project in school that is focused on just comparing two data sets and displaying the information in graphical form (i.e. histograms, scatter plots, etc.). The example in class compared two items (using one -----> photo !!!  of each) and making the observations be different blocks of pixels from each respective -----> photo !!! . \n\n&amp;#x200B;\n\nFor my side of things I need to compare two completely separate pieces of data and am having trouble figuring out what to compare / finding the data to compare the data sets. I know this is super beginner, but if anyone has any ideas or resources available it would be much appreciated!", "link": "https://www.reddit.com/r/MachineLearning/comments/atmjr9/intro_machine_learning_project/"}, {"autor": "1abhigupta", "date": "2019-02-21 23:07:27", "content": "[R] Call for Papers (CfP) for the Workshop on \"Structure and Priors in Reinforcement Learning\" (SPiRL) at ICLR 2019 /!/ We're excited to share the call for papers for our upcoming workshop on structure and priors in reinforcement learning at ICLR 2019. The submission deadline for a 5-page extended abstract is 3/7.\n\nBest, SPiRL Co-organizers\n\nWorkshop on \u201cStructure &amp; Priors in Reinforcement Learning\u201d (SPiRL) at ICLR 2019 Monday, May 6th, 09:00 AM \u2013 06:00 PM CST Room R4, Ernest N. Morial Convention Center, New Orleans http://spirl.info/2019/\n\nCall for Extended Abstracts\n\nA powerful solution to the problem of generalization and sample complexity in reinforcement learning (RL) is the deliberate use of inductive bias. There has been a recent resurgence of interest in methods of imposing or learning inductive bias in RL in the form of structure and priors, including, for example, prior distributions for Bayesian inference, learned hyperparameters in a multi-task or meta-learning setup, or structural constraints such as temporal abstraction or hierarchy.\n\nThe goal of this workshop is to bring together researchers across a variety of domains, including RL and machine learning practitioners, neuroscientists, and cognitive scientists, to discuss the role that structure and priors play in RL. We invite the submission of abstracts on topics including, but not limited to:\n\nBayesian inference as used in RL\nmeta-RL\ntransfer learning in RL\nmodularity and compositionality\nhierarchical RL\ntemporal abstraction\nstructured state and action abstractions\nsequential decision-making in humans\nreinforcement processes in the brain\nWe also invite abstracts that address the following questions directly:\n\nWhat is the trade-off between generality and the use of structure and priors in RL, in the context of specific tasks or in general, and how can we evaluate this in practice?\n\nWhat are the practical or theoretical implications of specific ways of imposing or learning structure or priors in RL?\n\nHow can we learn data-driven structure and priors for RL (via transfer in RL, meta-RL, or multi-task RL)?\n\nHow can the different communities (including cognitive science, neuroscience, and machine learning) benefit from collaborative research on these topics?\n\nImportant Dates\n\nExtended abstract deadline: Thursday, March 7th, 2019, 11:59 PM anywhere on Earth Decision notification: Thursday, March 28th, 2019 -----> Camera !!! -ready deadline: Thursday, May 2nd, 2019, 11:59 PM anywhere on Earth\n\nAbstract Format\n\nExtended abstracts should be a short research paper of at most 5 pages long (excluding references or appendix) in PDF format. Abstracts must be anonymized; the review process will be double-blind.\n\nPlease see the CfP on the workshop website (http://spirl.info/2019/call/) for more details.\n\nPlease submit your extended abstracts via CMT (https://cmt3.research.microsoft.com/User/Login?ReturnUrl=%2FSPiRLICLR2019) by the deadline given above.\n\nPresentation Details\n\nAll accepted abstracts will be presented in the form of a poster. A few select contributions will additionally be given as contributed talks. Accepted papers will be posted in a non-archival format on the workshop website.\n\nWorkshop Committee\n\nPierre-Luc Bacon (Stanford) Marc Deisenroth (Imperial College London) Chelsea Finn (UC Berkeley/Google Brain/Stanford) Erin Grant (UC Berkeley) Tom Griffiths (Princeton) Abhishek Gupta (UC Berkeley) Nicolas Heess (DeepMind) Michael Littman (Brown) Junhyuk Oh (DeepMind)\n\nIf you have any further questions, please contact the SPiRL 2019 committee at organizers@spirl.info.", "link": "https://www.reddit.com/r/MachineLearning/comments/at9ufi/r_call_for_papers_cfp_for_the_workshop_on/"}, {"autor": "Horror_Counter", "date": "2019-02-21 19:30:46", "content": "[D] Semantic segmentation for methane leak detection, does it make sense? /!/ We're starting to apply AI in a heavy industry context and ideas have been brainstormed. One idea was to use ConvNets to detect methane leaks, by looking at thermal cameras images (actually, I think it would be thermal cameras _videos_, but one could start by looking at images sampled every now &amp; then). An -----> image !!!  from one of these camera could look like this:\n\nhttp://www.hazardexonthenet.net/article/107539/Massive-gas-leak-from-California-underground-storage-reservoir-causes-1-800-families-to-relocate.aspx\n\nand you would likely look for red-plumey-thingies, which should be hot gas escaping from a storage site, or a well, etc.\n\nhttps://www.eurekalert.org/pub_releases/2018-12/uov-nsf122018.php\n\nDo you think the idea could make sense? These images are very different from the usual images on which one trains ConvNets (in particular, I think Fully Convolutional Networks could be used for this task), so I'm not sure how much help pretrained models would be, or to put it in another way, how much retraining a pretrained model would need, before getting a decent validation loss.\n\nNow, I was wondering if there could be a way to at least estimate the size of the problem. In other words, if one could give some numbers for the dynamic range and the resolution of these thermograms, would there be any way to very roughly estimate the order of magnitude of the size of the dataset needed to build such a model? Or should one go the other way round, and say, given that I'm going to use this model (say, FCN-16), which has a certain capacity, how many images do I need to train it to a  certain accuracy? Is there any way to get some kind of estimate, or the only way is to try and see? Am I missing something obvious?   ", "link": "https://www.reddit.com/r/MachineLearning/comments/at7b8m/d_semantic_segmentation_for_methane_leak/"}, {"autor": "stratospark", "date": "2019-02-21 18:27:18", "content": "[P] Tutorial: Generating Synthetic Data for Image Segmentation with Unity and PyTorch/fastai /!/ &amp;#x200B;\n\nhttps://i.redd.it/hy5fbbajsyh21.jpg\n\nHey all, I've spent the last few months playing with domain randomization and synthetic data generation in Unity. I found it to be a great asset for machine learning explorations and wanted to spread awareness to the larger community. Since I'm assuming most ML developers haven't dabbled with game engines, my tutorial tries to walk them through the more idiosyncratic features of the Unity workflow.\n\nIn future lessons, I'd like to tackle randomized lighting, HDR textures, -----> camera !!!  angles and lens characteristics, as well as importing 3d models and randomizing blendshapes. I'd also like to cover more realistic datasets like humans or cars. Or maybe the community has additional ideas?\n\nAppreciate any feedback.\n\nwriteup: [https://blog.stratospark.com/generating-synthetic-data-image-segmentation-unity-pytorch-fastai.html](https://blog.stratospark.com/generating-synthetic-data-image-segmentation-unity-pytorch-fastai.html)\n\nvideo tutorial: [https://youtu.be/P4CCMvtUohA](https://youtu.be/P4CCMvtUohA)\n\ngithub repo: [https://github.com/stratospark/UnityImageSynthesisTutorial1](https://github.com/stratospark/UnityImageSynthesisTutorial1)", "link": "https://www.reddit.com/r/MachineLearning/comments/at6jtj/p_tutorial_generating_synthetic_data_for_image/"}, {"autor": "lzhbrian", "date": "2019-02-21 14:53:22", "content": "[D] Dataset size for -----> image !!! -to------> image !!!  algorithms? /!/ Training dataset size seems to have critical impact for training image synthesizing models like BigGAN, StyleGAN.\nFor example, if you have 1,000 or 10,000 or 100,000 images for training, the quality of the synthesized results can really differ.\n\nDoes this also apply for image-to-image algorithms (e.g. CycleGAN, pix2pix) ?\nIn pix2pix, size of the datasets used differs a lot, however, the small datasets also provide decent results.\n\n| Dataset | size |\n| ------- | ---- |\n| Architectural labels -&gt; photo | 400 |\n| Maps -&gt; aerial photograph | 1096 |\n| Edges -&gt; shoes | 50,000 |\n| Edge -&gt; Handbags | 137,000 |\n\nThanks.\n", "link": "https://www.reddit.com/r/MachineLearning/comments/at4096/d_dataset_size_for_imagetoimage_algorithms/"}, {"autor": "tworats", "date": "2019-03-12 16:29:33", "content": "[D] SOTA in keypoint detection? /!/ After very stupidly losing a large amount of manually annotated keypoint data, I'd like to try to automate some of the process. What is practical SOTA in keypoint detection? The task is similar to facial keypoint detection on custom -----> image !!!  data, with only 4 keypoints.", "link": "https://www.reddit.com/r/MachineLearning/comments/b09s7x/d_sota_in_keypoint_detection/"}, {"autor": "ElBalistico", "date": "2019-03-12 16:26:03", "content": "[N] Applications of Deep Generative Models on Smartphones : Part 1 (-----> Image !!! ) /!/ In this short post, I give a few examples of what generative models (mostly GANs) can bring to our smartphones, especially in the domain of image generation.\n\n&amp;#x200B;\n\nHave a look ! :)\n\n&amp;#x200B;\n\n[https://heartbeat.fritz.ai/using-generative-deep-learning-models-on-device-c37aa74ae4dd](https://heartbeat.fritz.ai/using-generative-deep-learning-models-on-device-c37aa74ae4dd)", "link": "https://www.reddit.com/r/MachineLearning/comments/b09qox/n_applications_of_deep_generative_models_on/"}, {"autor": "Mockapapella", "date": "2019-03-12 16:10:45", "content": "[D] What are some good tools for creating ROI's for large -----> image !!!  datasets easily? /!/ A while back I remember seeing something like this on GitHub, but I can't for the life of me find it now", "link": "https://www.reddit.com/r/MachineLearning/comments/b09k78/d_what_are_some_good_tools_for_creating_rois_for/"}, {"autor": "abinjoabraham", "date": "2019-03-11 10:03:20", "content": "Need a faster bounding box detector /!/ Guys I am working on a project which can detect object instances in an -----> image !!! . I have found an algorithm which suits to my needs and as a prerequisite to this algorithm, I need to draw a bounding box over the object instance. I googled and found several bbox detectors like Mask.RCNN, Fast RCNN, Faster RCNN and I am confused which one will give me better and faster  bbox over the object instances in the image. Do you people have some suggestions on this? In the end I was planning to compare my accuracy considering Mask.RCNN as my baseline data. ", "link": "https://www.reddit.com/r/MachineLearning/comments/azrmr8/need_a_faster_bounding_box_detector/"}, {"autor": "doyuplee", "date": "2019-02-04 11:32:50", "content": "[P] Deblur -----> image !!!  with WGAN and tensorflow /!/ **DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks \\[paper\\](**[**https://arxiv.org/abs/1711.07064**](https://arxiv.org/abs/1711.07064)**)**\n\nIn this paper, WGAN is trained to make blurred image sharp.\n\nAlso, there is various implementations (pytorch, keras), but recently new tensorflow implementation is available.\n\nClick \\[link\\]([https://github.com/LeeDoYup/DeblurGAN-tf](https://github.com/LeeDoYup/DeblurGAN-tf))", "link": "https://www.reddit.com/r/MachineLearning/comments/an0tmr/p_deblur_image_with_wgan_and_tensorflow/"}, {"autor": "ProfessionalFinger5", "date": "2019-02-03 11:06:15", "content": "[D] Sound Mathematical Foundation for Machine Learning /!/ I'm starting to specialize in Machine Learning and would like to discuss the way forward. My exposure has been minimal so far; I did implement a CNN to do basic -----> image !!!  recognition from the MNIST set in C and am currently trying to make the kernel run in parallel on CUDA.\n\nI'm a CS undergraduate, and on the mathematical side completed Linear Algebra, Real Analysis, Probability &amp; Statistics, various introductions to topics from Discrete Mathematics (Combinatorics, Graph- &amp; Number Theory, Abstract Algebra, Formal Logic), and a bunch of Algorithm courses that notably also included Numerical Analysis, some Optimization, Convex Geometry and some Game Theory topics.\n\nI'm considering taking Mathematical Optimization, Foundations of Statistics and Information Theory. The various Machine Learning courses are very thorough by reputation, and consist of a mix between rigorous theory and programming projects where we implement the methods from scratch. There's also a Statistical Learning Theory course, for which I am very excited. Additionally, I'm not forgetting to take courses on (Big) Data (Analysis / Mining / Retrieval), etc.\n\nDoes this line up sound good? Are there other fundamental topics to consider? I don't necessarily aim to do a PhD, but I want to learn what's necessary to \"understand\" what's going on and to be able to create new stuff and build upon latest research. Consequently, that also includes the necessary tools to learn the Graduate Level Math that might creep up in Machine Learning in the present and future.", "link": "https://www.reddit.com/r/MachineLearning/comments/amonyz/d_sound_mathematical_foundation_for_machine/"}, {"autor": "nix_zu_verlian", "date": "2019-02-02 17:40:51", "content": "A Quick Exploration of the -----> Image !!!  of Artificial Intelligence In Pop Culture", "link": "https://www.reddit.com/r/MachineLearning/comments/amgc3s/a_quick_exploration_of_the_image_of_artificial/"}, {"autor": "antondan", "date": "2019-02-01 22:59:15", "content": "[D] What kind of Machine Learning Model/Neural Network would be used to predict the second part of a pair of images after being trained? /!/ Hello, Noobie here. So lets say I have lots of pairs of images like the one below. Where A is a low resolution drawn sprite of a person with his hands down and B with his hands up. I want to use all those pairs of -----> image !!! s in order to train a machine learning program, then I want to give it a new -----> image !!!  A and it producing -----> image !!!  B. What do I need to look into?  \n\n\nThanks in advance.  \n\n\n&amp;#x200B;\n\nhttps://i.redd.it/yy207v2se1e21.png", "link": "https://www.reddit.com/r/MachineLearning/comments/am7x1y/d_what_kind_of_machine_learning_modelneural/"}, {"autor": "ankitnayak1", "date": "2019-05-24 10:12:24", "content": "Samsung AI lab develops tech that can animate highly realistic heads using only a few -or in some cases - only one starter -----> image !!! .", "link": "https://www.reddit.com/r/MachineLearning/comments/bsf3t8/samsung_ai_lab_develops_tech_that_can_animate/"}, {"autor": "himanshuragtah1", "date": "2019-05-23 19:41:48", "content": "[R] State of the art- Latest from Google AI: Moving Camera, Moving People: A Deep Learning Approach to Depth Prediction /!/ paper: [**Learning the Depths of Moving People by Watching Frozen People**](https://www.profillic.com/paper/arxiv:1904.11111)  \n\n&amp;#x200B;\n\nGoal: Predict depth when both -----> camera !!!  and person are moving.\n\nThese guys show improvement over **state-of-the-art** monocular depth prediction methods", "link": "https://www.reddit.com/r/MachineLearning/comments/bs7bh8/r_state_of_the_art_latest_from_google_ai_moving/"}, {"autor": "Natsu6767", "date": "2019-05-23 18:55:25", "content": "[N] Samsung AI lab develops tech that can animate highly realistic heads using only a few - or in some cases - only one starter -----> image !!! .", "link": "https://www.reddit.com/r/MachineLearning/comments/bs6s9y/n_samsung_ai_lab_develops_tech_that_can_animate/"}, {"autor": "VermillionBlu", "date": "2019-05-23 18:01:53", "content": "Samsung AI lab develops tech that can animate highly realistic heads using only a few -or in some cases - only one starter -----> image !!! .", "link": "https://www.reddit.com/r/MachineLearning/comments/bs65uj/samsung_ai_lab_develops_tech_that_can_animate/"}, {"autor": "sjoerdapp", "date": "2019-05-23 17:12:05", "content": "\ud83d\udd25\ud83d\udd25\ud83d\udd25Moving -----> Camera !!! , Moving People: A Deep Learning Approach to Depth Prediction", "link": "https://www.reddit.com/r/MachineLearning/comments/bs5kk5/moving_camera_moving_people_a_deep_learning/"}, {"autor": "thevatsalsaglani", "date": "2019-07-01 06:29:04", "content": "Multi-class -----> Image !!!  classification using CNN over PyTorch, and the basics of CNN", "link": "https://www.reddit.com/r/MachineLearning/comments/c7p4k6/multiclass_image_classification_using_cnn_over/"}, {"autor": "SamPusegaonkar", "date": "2019-06-30 14:02:42", "content": "-----> Image !!!  to image regression? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/c7ecxi/image_to_image_regression/"}, {"autor": "ArsenicBismuth", "date": "2019-06-29 18:02:46", "content": "[D] Best way to label/prepare data for full-body gesture recognition. /!/ Sorry, I'm pretty much a newbie to this kind of things. Long story short, I have a pose data obtained from Openpose and I want to recognize certain gestures using LSTM-RNN. A gesture would be 8 consecutive poses obtained from -----> camera !!! . Some of the gestures: walking, sweeping, idling, bringing object.\n\nTraining-wise it's pretty much not a problem acquiring 98% on training set and 90%+ on test set. Data wise also shouldn't be a problem since I have over millions of poses already (the day I realized Google Sheet cells limit is too small LOL). But implementing it on my real-time data always shows how bad it's. Making me thinking it's learning the wrong kind of features.\n\nWhat I've done: \n\n- With no pre-processing at all, I separated the poses into 4 regions: NE, NW, SE, SW (as in \"north-east\", etc) which basically means the poses happening in 4 very diff locations on the image. Result: The LSTM isn't even correctly recognizing the separated region consistently, let alone the certain gestures.\n\n- Amplifying the real distance between the 4 regions, by giving them a large offset if a certain boundary is passed. Again, same result.\n\n- Normalizing all of the poses to origin. This way no translation info is present on the data (inference &amp; training). Yet, even after simplifying the data for two classes, it's still so bad.\n\n- Normalizing only the first poses of a gesture to origin, and the last 7 would start moving from origin (to preserve translation data). Even worse that previous iteration on test set, giving 85%.\n\n\nAt this point, I'm afraid I'm doing something fundamentally wrong with my data. What I worried about are:\n\n- I divide my gestures into regions instead of the direction it's going.\n\n- The model just isn't cut for it.\n\n- The movement from the noise is way too big compared to the actual gesture.\n\nI humbly ask for any assistance at this point LOL.", "link": "https://www.reddit.com/r/MachineLearning/comments/c72hcn/d_best_way_to_labelprepare_data_for_fullbody/"}, {"autor": "DryCredit", "date": "2019-11-20 15:15:46", "content": "What are the advantages of using a GAN over a CNN for -----> image !!!  translation tasks? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/dz3e82/what_are_the_advantages_of_using_a_gan_over_a_cnn/"}, {"autor": "retardis_roark", "date": "2019-11-20 10:05:56", "content": "[P] Deploying a working model on the server. /!/ Hello guys, \n\nI recently developed a CNN to analyse and cut -----> image !!! s (-----> image !!!  text documents). I saved the entire network locally as \".h5\" extension and whenever I have to use it I call it. I also use \"pytesseract\" (OCR Library) to extract data from the cutouts. I have created a data frame using pandas where I append the results from tesseract to maintain the logs. Currently, I am using Jupyter notebook.\n\nI want to upload it all on the server to automate this process so I can daily check the DataFrame without the hassle of running all the notebooks. I currently have a subscription for DigitalOcean's server. \n\nAny leads or help on how to do this will be appreciated.", "link": "https://www.reddit.com/r/MachineLearning/comments/dz022q/p_deploying_a_working_model_on_the_server/"}, {"autor": "ikadorus", "date": "2019-11-19 22:30:52", "content": "Loss function for increasing the quality of the -----> image !!!  when labels are not perfectly alligned /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/dys242/loss_function_for_increasing_the_quality_of_the/"}, {"autor": "DependentSky6", "date": "2019-11-19 16:00:20", "content": "[D] What is the state-of-the-art implementation for style transfer? /!/ Hello, \n\nI'm having a hard time finding an implementation of style transfer that would work as well as an example like this : \n\n[Style transfer that works well](https://preview.redd.it/683t2hxzxnz31.png?width=1800&amp;format=png&amp;auto=webp&amp;s=6f7d871ba8249a898550f0fa405c50389a23c378)\n\nSee how the details of the wave are well transferred to the hair and beard? I'm looking for an implementation of style transfer that is able to do this type of detail. \n\nI have not been lucky in finding a style transfer algorithm that works like this. For instance a well cited paper like [AdaIn style transfer](https://arxiv.org/abs/1703.06868) typically yields something like this (see [github](https://github.com/xunhuang1995/AdaIN-style) implementation):   \n\n\n[Style transfer that does not work that well](https://preview.redd.it/b0quzh2iznz31.png?width=2316&amp;format=png&amp;auto=webp&amp;s=aea430958ccd81dd54bf1ce3c38add1ac2f59de3)\n\nWhat I'm not looking for is this type of style transfer that only changes the colors and roughly the shapes of the -----> image !!!  content. But in the end we cannot really find the \"identity\" of the style image (pencil drawing).\n\nI suspect that the first image with pewdiepie might have used photoshop in some ways... But if you guys think it's possible to have this type of output using only style transfer I would really like a bit of help here. Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/dymbbs/d_what_is_the_stateoftheart_implementation_for/"}, {"autor": "johnathanjones1998", "date": "2019-11-19 15:59:44", "content": "-----> Image !!!  augmentation for MRI in Detectron2 (MaskRCNN-like network) /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/dymb2b/image_augmentation_for_mri_in_detectron2/"}, {"autor": "goolulusaurs", "date": "2019-11-19 11:51:23", "content": "[D] ICML 2019 Machine Learning Talks /!/ ---\nRecent Advances in Population-Based Search for Deep Neural Networks: Quality Diversity, Indirect Encodings, and Open-Ended Algorithms\n\nPresented by Jeff Clune, Joel Lehman and Kenneth Stanley\n\nhttps://www.facebook.com/icml.imls/videos/481758745967365/\n\n---\nNever-Ending Learning\n\nPresented by Tom Mitchell and Partha Talukdar.\n\nhttps://www.facebook.com/icml.imls/videos/350412952342021/\nhttps://www.facebook.com/icml.imls/videos/1083330081864839/\n\n---\nA Primer on PAC-Bayesian Learning\n\nPresented by Benjamin Guedj and John Shawe-Taylor\n\nhttps://www.facebook.com/icml.imls/videos/318683639013879/\n\n---\nMeta-Learning: from Few-Shot Learning to Rapid Reinforcement Learning\n\nPresented by Chelsea Finn and Sergey Levine\n\nhttps://www.facebook.com/icml.imls/videos/400619163874853/\nhttps://www.facebook.com/icml.imls/videos/2970931166257998/\n\n---\nActive Learning: From Theory to Practice\n\nPresented by Robert Nowak and Steve Hanneke\n\nhttps://www.facebook.com/icml.imls/videos/662482727539899/\n\n---\nNeural Approaches to Conversational AI\n\nPresented by Michel Galley and Jianfeng Gao\n\nhttps://www.facebook.com/icml.imls/videos/2375117292730871/\n\n---\nA Tutorial on Attention in Deep Learning\n\nPresented by Alex Smola and Aston Zhang\n\nhttps://www.facebook.com/icml.imls/videos/382464939283864/\nhttps://www.facebook.com/icml.imls/videos/889237771440064/\n\n---\nActive Hypothesis Testing: An Information Theoretic (re)View\n\nPresented by Tara Javidi\n\nhttps://www.facebook.com/icml.imls/videos/478549476247044/\n\n---\nAlgorithm configuration: learning in the space of algorithm designs\n\nPresented by Kevin Leyton-Brown and Frank Hutter\n\nhttps://www.facebook.com/icml.imls/videos/2044426569187107/\n\n---\n\"The U.S. Census Bureau Tries to be a Good Data Steward in the 21st Century\"\ninvited talk by John M. Abowd\n\nBest Paper Awards:\nChallenging Common Assumptions in the Unsupervised Learning of Disentangled Representations\n\nhttps://www.facebook.com/icml.imls/videos/446476306189465/\n\n---\nSession on Deep Learning Algorithms\n\n\n\u2022 SelectiveNet: A Deep Neural Network with an Integrated Reject Option\n\n\u2022 Manifold Mixup: Better Representations by Interpolating Hidden States\n\n\u2022 Processing Megapixel Images with Deep Attention-Sampling Models\n\n\u2022 TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning\n\n\u2022 Online Meta-Learning\n\n\u2022 Training Neural Networks with Local Error Signals\n\n\u2022 GMNN: Graph Markov Neural Networks\n\n\u2022 Self-Attention Graph Pooling\n\n\u2022 Combating Label Noise in Deep Learning using Abstention\n\n\u2022 LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning\n\nhttps://www.facebook.com/icml.imls/videos/336722770596090/\n\n---\nSession on Deep Reinforcement Learning\n\n\u2022 ELF OpenGo: an analysis and open reimplementation of AlphaZero\n\n\u2022 Making Deep Q-learning methods robust to time discretization\n\n\u2022 Nonlinear Distributional Gradient Temporal-Difference Learning\n\n\u2022 Composing Entropic Policies using Divergence Correction\n\n\u2022 TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning\n\n\u2022 Multi-Agent Adversarial Inverse Reinforcement Learning\n\n\u2022 Policy Consolidation for Continual Reinforcement Learning\n\n\u2022 Off-Policy Deep Reinforcement Learning without Exploration\n\n\u2022 Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation\n\n\u2022 Revisiting the Softmax Bellman Operator: New Benefits and New Perspective\n\nhttps://www.facebook.com/icml.imls/videos/1577337105730518/\n\n---\nSession on Adversarial Examples\n\n\u2022 Adversarial Attacks on Node Embeddings via Graph Poisoning\n\n\u2022 First-Order Adversarial Vulnerability of Neural Networks and Input Dimension\n\n\u2022 On Certifying Non-Uniform Bounds against Adversarial Attacks\n\n\u2022 Improving Adversarial Robustness via Promoting Ensemble Diversity\n\n\u2022 Adversarial -----> camera !!!  stickers: A physical -----> camera !!! -based attack on deep learning systems\n\n\u2022 Adversarial examples from computational constraints\n\n\u2022 POPQORN: Quantifying Robustness of Recurrent Neural Networks\n\n\u2022 Using Pre-Training Can Improve Model Robustness and Uncertainty\n\n\u2022 Generalized No Free Lunch Theorem for Adversarial Robustness\n\n\u2022 PROVEN: Verifying Robustness of Neural Networks with a Probabilistic Approach\n\nhttps://www.facebook.com/icml.imls/videos/689280291532883/\n\n---\nSession on Generative Adversarial Networks\n\n\u2022 Self-Attention Generative Adversarial Networks\n\n\u2022 Multivariate-Information Adversarial Ensemble for Scalable Joint Distribution Matching\n\n\u2022 High-Fidelity Image Generation With Fewer Labels\n\n\u2022 Revisiting precision recall definition for generative modeling\n\n\u2022 Wasserstein of Wasserstein Loss for Learning Generative Models\n\n\u2022 Flat Metric Minimization with Applications in Generative Modeling\n\n\u2022 Entropic GANs meet VAEs: A Statistical Approach to Compute Sample Likelihoods in GANs\n\n\u2022 Non-Parametric Priors For Generative Adversarial Networks\n\n\u2022 Lipschitz Generative Adversarial Nets\n\n\u2022 HexaGAN: Generative Adversarial Nets for Real World Classification\n\nhttps://www.facebook.com/icml.imls/videos/713631379054038/\n\n---\nSession on Deep Reinforcement Learning\n\n\u2022 An Investigation of Model-Free Planning\n\n\u2022 CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning\n\n\u2022 Task-Agnostic Dynamics Priors for Deep Reinforcement Learning\n\n\u2022 Collaborative Evolutionary Reinforcement Learning\n\n\u2022 EMI: Exploration with Mutual Information\n\n\u2022 Imitation Learning from Imperfect Demonstration\n\n\u2022 Curiosity-Bottleneck: Exploration By Distilling Task-Specific Novelty\n\n\u2022 Dynamic Weights in Multi-Objective Deep Reinforcement Learning\n\n\u2022 Fingerprint Policy Optimisation for Robust Reinforcement Learning\n\nhttps://www.facebook.com/icml.imls/videos/298536957693171/\n\n---\nSession on Deep Learning Theory\n\n\u2022 On Learning Invariant Representations for Domain Adaptation\n\n\u2022 Lexicographic and Depth-Sensitive Margins in Homogeneous and Non-Homogeneous Deep Models\n\n\u2022 Adversarial Generation of Time-Frequency Features with application in audio synthesis\n\n\u2022 On the Universality of Invariant Networks\n\n\u2022 Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks\n\n\u2022 Gauge Equivariant Convolutional Networks and the Icosahedral CNN\n\n\u2022 Feature-Critic Networks for Heterogeneous Domain Generalization\n\n\u2022 Learning to Convolve: A Generalized Weight-Tying Approach\n\n\u2022 On Dropout and Nuclear Norm Regularization\n\n\u2022 Gradient Descent Finds Global Minima of Deep Neural Networks\n\nhttps://www.facebook.com/icml.imls/videos/2339557826311186/\n\n---\nSession on Deep Learning Architectures\n\n\u2022 Graph Matching Networks for Learning the Similarity of Graph Structured Objects\n\n\u2022 BayesNAS: A Bayesian Approach for Neural Architecture Search\n\n\u2022 Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks\n\n\u2022 Shallow-Deep Networks: Understanding and Mitigating Network Overthinking\n\n\u2022 Graph U-Nets\n\n\u2022 SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver\n\n\u2022 Area Attention\n\n\u2022 The Evolved Transformer\n\n\u2022 Jumpout : Improved Dropout for Deep Neural Networks with ReLUs\n\n\u2022 Stochastic Deep Networks\n\nhttps://www.facebook.com/icml.imls/videos/3253466301345987/\n\n---\nSession on Deep Learning Optimization\n\n\u2022 An Investigation into Neural Net Optimization via Hessian Eigenvalue Density\n\n\u2022 Differentiable Linearized ADMM\n\n\u2022 Adaptive Stochastic Natural Gradient Method for One-Shot Neural Architecture Search\n\n\u2022 A Quantitative Analysis of the Effect of Batch Normalization on Gradient Descent\n\n\u2022 The Effect of Network Width on Stochastic Gradient Descent and Generalization: an Empirical Study\n\n\u2022 AdaGrad stepsizes: sharp convergence over nonconvex landscapes\n\n\u2022 Beyond Backprop: Online Alternating Minimization with Auxiliary Variables\n\n\u2022 SWALP : Stochastic Weight Averaging in Low Precision Training\n\n\u2022 Efficient optimization of loops and limits with randomized telescoping sums\n\n\u2022 Self-similar Epochs: Value in arrangement\n\nhttps://www.facebook.com/icml.imls/videos/874988016194584/\n\n---\nSession on Large Scale Learning and Systems\n\n\u2022 Composable Core-sets for Determinant Maximization: A Simple Near-Optimal Algorithm\n\n\u2022 Sublinear Time Nearest Neighbor Search over Generalized Weighted Space\n\n\u2022 Compressing Gradient Optimizers via Count-Sketches\n\n\u2022 Scalable Fair Clustering\n\n\u2022 Conditional Gradient Methods via Stochastic Path-Integrated Differential Estimator\n\n\u2022 Fault Tolerance in Iterative-Convergent Machine Learning\n\n\u2022 Static Automatic Batching In TensorFlow\n\n\u2022 Improving Neural Network Quantization without Retraining using Outlier Channel Splitting\n\n\u2022 Memory-Optimal Direct Convolutions for Maximizing Classification Accuracy in Embedded Applications\n\n\u2022 DL2: Training and Querying Neural Networks with Logic\n\nhttps://www.facebook.com/icml.imls/videos/2250364101882755/\n\n---\n\"Machine Learning for Robots To Think Fast\"\ninvited talk by Aude Billard\n\nTest of time Award\nOnline dictionary learning for Sparse Coding\n\nhttps://www.facebook.com/icml.imls/videos/2368059266588651/\n\n---\nSession on Deep Generative Models\n\n\u2022 Sum-of-Squares Polynomial Flow\n\n\u2022 FloWaveNet : A Generative Flow for Raw Audio\n\n\u2022 Are Generative Classifiers More Robust to Adversarial Attacks?\n\n\u2022 A Gradual, Semi-Discrete Approach to Generative Network Training via Explicit Wasserstein Minimization\n\n\u2022 Disentangling Disentanglement in Variational Autoencoders\n\n\u2022 EDDI: Efficient Dynamic Discovery of High-Value Information with Partial VAE\n\n\u2022 A Wrapped Normal Distribution on Hyperbolic Space for Gradient-Based Learning\n\n\u2022 Emerging Convolutions for Generative Normalizing Flows\n\n\u2022 A Large-Scale Study on Regularization and Normalization in GANs\n\n\u2022 Variational Annealing of GANs: A Langevin Perspective\n\nhttps://www.facebook.com/icml.imls/videos/325725335009518/\nhttps://www.facebook.com/icml.imls/videos/518469445360005/\n\n---\nSession on Deep Reinforcement Learning\n\n\u2022 Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning\n\n\u2022 Maximum Entropy-Regularized Multi-Goal Reinforcement Learning\n\n\u2022 Imitating Latent Policies from Observation\n\n\u2022 SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning\n\n\u2022 Dimension-Wise Importance Sampling Weight Clipping for Sample-Efficient Reinforcement Learning\n\n\u2022 Structured agents for physical construction\n\n\u2022 Learning Novel Policies For Tasks\n\n\u2022 Taming MAML: Efficient unbiased meta-reinforcement learning\n\n\u2022 Self-Supervised Exploration via Disagreement\n\n\u2022 Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables\n\nhttps://www.facebook.com/icml.imls/videos/355035025132741/\n\n---\nSession on Adversarial Examples\n\n\u2022 Theoretically Principled Trade-off between Robustness and Accuracy\n\n\u2022 The Odds are Odd: A Statistical Test for Detecting Adversarial Examples\n\n\u2022 ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation\n\n\u2022 Certified Adversarial Robustness via Randomized Smoothing\n\n\u2022 Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition\n\n\u2022 Parsimonious Black-Box Adversarial Attacks via Efficient Combinatorial Optimization\n\n\u2022 Wasserstein Adversarial Examples via Projected Sinkhorn Iterations\n\n\u2022 Transferable Clean-Label Poisoning Attacks on Deep Neural Nets\n\n\u2022 NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks\n\n\u2022 Simple Black-box Adversarial Attacks\n\nhttps://www.facebook.com/icml.imls/videos/607431793098200/\n\n---\nSession on Deep Learning Architectures\n\n\u2022 Invertible Residual Networks\n\n\u2022 NAS-Bench-101: Towards Reproducible Neural Architecture Search\n\n\u2022 Approximated Oracle Filter Pruning for Destructive CNN Width Optimization\n\n\u2022 LegoNet: Efficient Convolutional Neural Networks with Lego Filters\n\n\u2022 Sorting Out Lipschitz Function Approximation\n\n\u2022 Graph Element Networks: adaptive, structured computation and memory\n\n\u2022 Training CNNs with Selective Allocation of Channels\n\n\u2022 Equivariant Transformer Networks\n\n\u2022 Overcoming Multi-model Forgetting\n\n\u2022 Bayesian Nonparametric Federated Learning of Neural Networks\n\nhttps://www.facebook.com/icml.imls/videos/552835701913736/\n\n---\nSession on Deep Reinforcement Learning\n\n\u2022 The Natural Language of Actions\n\n\u2022 Control Regularization for Reduced Variance Reinforcement Learning\n\n\u2022 On the Generalization Gap in Reparameterizable Reinforcement Learning\n\n\u2022 Trajectory-Based Off-Policy Deep Reinforcement Learning\n\n\u2022 A Deep Reinforcement Learning Perspective on Internet Congestion Control\n\n\u2022 Model-Based Active Exploration\n\n\u2022 Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations\n\n\u2022 Distributional Multivariate Policy Evaluation and Exploration with the Bellman GAN\n\n\u2022 A Baseline for Any Order Gradient Estimation in Stochastic Computation Graphs\n\n\u2022 Remember and Forget for Experience Replay\n\nhttps://www.facebook.com/icml.imls/videos/674476986298614/\n\n---\nSession on Causality\n\n\u2022 Causal Identification under Markov Equivalence: Completeness Results\n\n\u2022 Counterfactual Off-Policy Evaluation with Gumbel-Max Structural Causal Models\n\n\u2022 Causal Discovery and Forecasting in Nonstationary Environments with State-Space Models\n\n\u2022 Classifying Treatment Responders Under Causal Effect Monotonicity\n\n\u2022 Learning Models from Data with Measurement Error: Tackling Underreporting\n\n\u2022 Adjustment Criteria for Generalizing Experimental Findings\n\n\u2022 Conditional Independence in Testing Bayesian Networks\n\n\u2022 Sensitivity Analysis of Linear Structural Causal Models\n\n\u2022 More Efficient Off-Policy Evaluation through Regularized Targeted Learning\n\n\u2022 Inferring Heterogeneous Causal Effects in Presence of Spatial Confounding\n\nhttps://www.facebook.com/icml.imls/videos/2188227091246504/\n\n---\nSession on Representation Learning\n\n\u2022 Adversarially Learned Representations for Information Obfuscation and Inference\n\n\u2022 Adaptive Neural Trees\n\n\u2022 Connectivity-Optimized Representation Learning via Persistent Homology\n\n\u2022 Minimal Achievable Sufficient Statistic Learning\n\n\u2022 Learning to Route in Similarity Graphs\n\n\u2022 Invariant-Equivariant Representation Learning for Multi-Class Data\n\n\u2022 Infinite Mixture Prototypes for Few-shot Learning\n\n\u2022 MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing\n\n\u2022 Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting\n\nhttps://www.facebook.com/icml.imls/videos/307375446865883/\n\n---\nSession on Generative Models\n\n\u2022 Tensor Variable Elimination for Plated Factor Graphs\n\n\u2022 Predicate Exchange: Inference with Declarative Knowledge\n\n\u2022 Discriminative Regularization for Latent Variable Models with Applications to Electrocardiography\n\n\u2022 Hierarchical Decompositional Mixtures of Variational Autoencoders\n\n\u2022 Finding Mixed Nash Equilibria of Generative Adversarial Networks\n\n\u2022 CompILE: Compositional Imitation Learning and Execution\n\n\u2022 Sparse Multi-Channel Variational Autoencoder for the Joint Analysis of Heterogeneous Data\n\n\u2022 Deep Generative Learning via Variational Gradient Flow\n\n\u2022 Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design\n\n\u2022 Learning Neurosymbolic Generative Models via Program Synthesis\n\nhttps://www.facebook.com/icml.imls/videos/457663645035961/\n\n---\nSession on Deep Learning Algorithms\n\n\u2022 How does Disagreement Help Generalization against Label Corruption?\n\n\u2022 EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis\n\n\u2022 Addressing the Loss-Metric Mismatch with Adaptive Loss Alignment\n\n\u2022 Deep Compressed Sensing\n\n\u2022 Differentiable Dynamic Normalization for Learning Deep Representation\n\n\u2022 Toward Understanding the Importance of Noise in Training Neural Networks\n\n\u2022 Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Group\n\n\u2022 Breaking Inter-Layer Co-Adaptation by Classifier Anonymization\n\n\u2022 Understanding the Impact of Entropy on Policy Optimization\n\n\u2022 Probability Functional Descent: A Unifying Perspective on GANs, Variational Inference, and Reinforcement Learning\n\nhttps://www.facebook.com/icml.imls/videos/600823507067800/\n\n---\nSession on Deep Generative Models\n\n\u2022 State-Reification Networks: Improving Generalization by Modeling the Distribution of Hidden Representations\n\n\u2022 Variational Laplace Autoencoders\n\n\u2022 Latent Normalizing Flows for Discrete Sequences\n\n\u2022 Multi-objective training of Generative Adversarial Networks with multiple discriminators\n\n\u2022 Learning Discrete and Continuous Factors of Data via Alternating Disentanglement\n\n\u2022 Bit-Swap: Recursive Bits-Back Coding for Lossless Compression with Hierarchical Latent Variables\n\n\u2022 Graphite: Iterative Generative Modeling of Graphs\n\n\u2022 Hybrid Models with Deep and Invertible Features\n\n\u2022 MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Sets\n\n\u2022 On Scalable and Efficient Computation of Large Scale Optimal Transport\n\nhttps://www.facebook.com/icml.imls/videos/1269891676506524/\n\n---\nSession on Reinforcement Learning\n\n\u2022 Batch Policy Learning under Constraints\n\n\u2022 Quantifying Generalization in Reinforcement Learning\n\n\u2022 Learning Latent Dynamics for Planning from Pixels\n\n\u2022 Projections for Approximate Policy Iteration Algorithms\n\n\u2022 Learning Structured Decision Problems with Unawareness\n\n\u2022 Calibrated Model-Based Deep Reinforcement Learning\n\n\u2022 Reinforcement Learning in Configurable Continuous Environments\n\n\u2022 Target-Based Temporal-Difference Learning\n\n\u2022 Iterative Linearized Control: Stable Algorithms and Complexity Guarantees\n\n\u2022 Finding Options that Minimize Planning Time\n\nhttps://www.facebook.com/icml.imls/videos/2547484245262588/\n\n---\nSession on Interpretability \n\n\u2022 Neural Network Attributions: A Causal Perspective\n\n\u2022 Towards a Deep and Unified Understanding of Deep Neural Models in NLP\n\n\u2022 Explaining Deep Neural Networks with a Polynomial Time Algorithm for Shapley Value Approximation\n\n\u2022 Functional Transparency for Structured Data: a Game-Theoretic Approach\n\n\u2022 Exploring interpretable LSTM neural networks over multi-variable data\n\n\u2022 TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing\n\n\u2022 Gaining Free or Low-Cost Interpretability with Interpretable Partial Substitute\n\n\u2022 State-Regularized Recurrent Neural Networks\n\n\u2022 Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation\n\n\u2022 On the Connection Between Adversarial Robustness and Saliency Map Interpretability\n\nhttps://www.facebook.com/icml.imls/videos/460378531393374/\n\n---\nSession on Deep Learning\n\n\u2022 Understanding and correcting pathologies in the training of learned optimizers\n\n\u2022 Demystifying Dropout\n\n\u2022 Ladder Capsule Network\n\n\u2022 Unreproducible Research is Reproducible\n\n\u2022 Geometric Scattering for Graph Data Analysis\n\n\u2022 Robust Inference via Generative Classifiers for Handling Noisy Labels\n\n\u2022 LIT: Learned Intermediate Representation Training for Model Compression\n\n\u2022 Analyzing and Improving Representations with the Soft Nearest Neighbor Loss\n\n\u2022 What is the Effect of Importance Weighting in Deep Learning?\n\n\u2022 Similarity of Neural Network Representations Revisited\n\nhttps://www.facebook.com/icml.imls/videos/308727963404001/\n\n---\nSession on Deep Sequence Models\n\n\u2022 Stochastic Beams and Where To Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement\n\n\u2022 Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs\n\n\u2022 Meta-Learning Neural Bloom Filters\n\n\u2022 CoT: Cooperative Training for Generative Modeling of Discrete Data\n\n\u2022 Non-Monotonic Sequential Text Generation\n\n\u2022 Insertion Transformer: Flexible Sequence Generation via Insertion Operations\n\n\u2022 Empirical Analysis of Beam Search Performance Degradation in Neural Sequence Models\n\n\u2022 Trainable Decoding of Sets of Sequences for Neural Sequence Models\n\n\u2022 Learning to Generalize from Sparse and Underspecified Rewards\n\n\u2022 Efficient Training of BERT by Progressively Stacking\n\nhttps://www.facebook.com/icml.imls/videos/895968107420746/\n\n---\nSession on Deep Learning Theory\n\n\u2022 Why do Larger Models Generalize Better? A Theoretical Perspective via the XOR Problem\n\n\u2022 On the Spectral Bias of Neural Networks\n\n\u2022 Recursive Sketches for Modular Deep Learning\n\n\u2022 Zero-Shot Knowledge Distillation in Deep Networks\n\n\u2022 A Convergence Theory for Deep Learning via Over-Parameterization\n\n\u2022 A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks\n\n\u2022 Approximation and non-parametric estimation of ResNet-type convolutional neural networks\n\n\u2022 Global Convergence of Block Coordinate Descent in Deep Learning\n\n\u2022 Measurements of Three-Level Hierarchical Structure in the Outliers in the Spectrum of Deepnet Hessians\n\n\u2022 On the Limitations of Representing Functions on Sets\n\nhttps://www.facebook.com/icml.imls/videos/606052416553010/\n\n---\n\"What 4 Year Olds Can Do and AI Can't (yet)\"\n\ninvited talk by Alison Gopnik\n\nBest Paper Awards:\nRates of Convergence for Sparse Variational Gaussian Process Regression\n\nhttps://www.facebook.com/icml.imls/videos/680801775700033/\n\n---\nSession on Representation Learning\n\n\u2022 Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations\n\n\u2022 Breaking the Softmax Bottleneck via Learnable Monotonic Pointwise Non-linearities\n\n\u2022 Multi-Object Representation Learning with Iterative Variational Inference\n\n\u2022 Cross-Domain 3D Equivariant Image Embeddings\n\n\u2022 Loss Landscapes of Regularized Linear Autoencoders\n\n\u2022 Hyperbolic Disk Embeddings for Directed Acyclic Graphs\n\n\u2022 LatentGNN: Learning Efficient Non-local Relations for Visual Recognition\n\n\u2022 Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness\n\n\u2022 Lorentzian Distance Learning for Hyperbolic Representations\n\nhttps://www.facebook.com/icml.imls/videos/321425055451434/\n\n---\nSession on Bandits and Multiagent Learning\n\n\u2022 Decentralized Exploration in Multi-Armed Bandits\n\n\u2022 Warm-starting Contextual Bandits: Robustly Combining Supervised and Bandit Feedback\n\n\u2022 Exploiting structure of uncertainty for efficient matroid semi-bandits\n\n\u2022 PAC Identification of Many Good Arms in Stochastic Multi-Armed Bandits\n\n\u2022 Contextual Multi-armed Bandit Algorithm for Semiparametric Reward Model\n\n\u2022 Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning\n\n\u2022 TarMAC: Targeted Multi-Agent Communication\n\n\u2022 QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning\n\n\u2022 Actor-Attention-Critic for Multi-Agent Reinforcement Learning\n\n\u2022 Finite-Time Analysis of Distributed TD(0) with Linear Function Approximation on Multi-Agent Reinforcement Learning\n\nhttps://www.facebook.com/icml.imls/videos/444326646299556/\n\n---\nSession on Bayesian Deep Learning\n\n\u2022 Probabilistic Neural Symbolic Models for Interpretable Visual Question Answering\n\n\u2022 Nonparametric Bayesian Deep Networks with Local Competition\n\n\u2022 Good Initializations of Variational Bayes for Deep Models\n\n\u2022 Dropout as a Structured Shrinkage Prior\n\n\u2022 ARSM: Augment-REINFORCE-Swap-Merge Estimator for Gradient Backpropagation Through Categorical Variables\n\n\u2022 On Variational Bounds of Mutual Information\n\n\u2022 Partially Exchangeable Networks and Architectures for Learning Summary Statistics in Approximate Bayesian Computation\n\n\u2022 Hierarchical Importance Weighted Autoencoders\n\n\u2022 Faster Attend-Infer-Repeat with Tractable Probabilistic Models\n\n\u2022 Understanding Priors in Bayesian Neural Networks at the Unit Level\n\nhttps://www.facebook.com/icml.imls/videos/2202320806483370/\n\n---\nWorkshop on Generative Modeling and Model-Based Reasoning for Robotics and AI\n\n\"Self Supervised Learning\"\ninvited talk by Yann LeCun \n\n\"Mental Simulation, Imagination, and Model-Based Deep RL\"\ninvited talk by Jessica B. Hamrick\n\n\n\u2022 Bayesian Inference to Identify the Cause of Human Errors\n\n\u2022 Data-Efficient Model-Based RL through Unsupervised Discovery and Curiosity-Driven Exploration\n\n\u2022 A Top-Down Bottom-Up Approach to Learning Hierarchical Physics Models for Manipulation\n\n\u2022 Discovering, Predicting, and Planning with Objects\n\n\u2022 FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and Discovery\n\n\u2022 Generalized Hidden Parameter MDPs for Model-based Meta-reinforcement Learning\n\n\u2022 HEDGE: Hierarchical Event-Driven Generation\n\n\u2022 Improved COnditional VRNNs for Video Prediction\n\n\u2022 Improvisation through Physical Understanding: Using Novel Objects as Tools with Visual Foresight\n\n\u2022 Learning Feedback Linearization by MF RL\n\n\u2022 \"Learning High Level Representations from Continous Experience\"\n\n\u2022 Deep Knowledge-Based Agents\n\nhttps://www.facebook.com/icml.imls/videos/394896141118878/\nhttps://www.facebook.com/icml.imls/videos/2084133498380491/\n\n---\nWorkshop on Uncertainty and Robustness in Deep Learning\n\nhttps://www.facebook.com/icml.imls/videos/892421577776699/\n\n---\nWorkshop on Understanding and Improving Generalizing in Deep Learning\n\nDaniel Roy - Progress on Nonvacuous Generalization Bounds\n\nChelsea Finn - Training for Generalization\n\nSpotlight Talk - A Meta-Analysis of Overfitting in Machine Learning\n\nSpotlight Talk - Uniform Convergence may be unable to explain generalization in deep learning\n\nhttps://www.facebook.com/icml.imls/videos/834773703576296/\n\n---\nWorkshop on Understanding and Improving generalization in Deep Learning\n\nSham Kakade - Prediction, Learning and Memory\n\nMikhail Belkin - A Hard Look at Generalization and its Theories\n\nSpotlight Talk - Towards Task and Architecture-Indipendent Generalization Gap Predictors\n\nSpotlight Talk - Data-Dependent Sample Complexity of Deep Neural Networks Via Lipschitz Augmentation\n\nhttps://www.facebook.com/icml.imls/videos/2543954589165286/\n\n---\nWorkshop on Generative Modeling and Model-Based Reasoning for Robotics and AI\n\n\"What should be Learned?\"\nInvited talk by Stefan Schaal\n\n\n\u2022 When to Trust Your Model: Model-Based Policy Optimization\n\n\u2022 Model Based Planning with Energy Based Models\n\n\u2022 A Perspective on Objects and Systematic Generalization in Model-Based RL\n\nhttps://www.facebook.com/icml.imls/videos/1286528018196347/\n\n---\nWorkshop Session\n\nKeynote by Kilian Weinberger: On Calibration and Fairness\n\n\n\u2022 Why ReLU networks yield high-confidence predictions far away from training data and how to mitigate the problem\n\n\u2022 Detecting Extrapolation with Influence Functions\n\n\u2022 How Can We Be So Dense? The Robustness of Highly Sparse Representations\n\nKeynote by Suchi Saria: Safety Challenges with Black-Box Predictors and Novel Learning Approaches for Failure Proofing\n\nhttps://www.facebook.com/icml.imls/videos/474831503062000/\n\n---\nWorkshop on Understanding and Improving generalization in Deep Learning\n\nInvited Speaker: Aleksander M\u0105dry \"Are All Features Created Equal?\"\nInvited Speaker: Jason Lee \"On the Foundations of Deep Learning: SGD, Overparametrization, and Generalization\"\nSpotlight Talk: \"Towards Large Scale Structure of the Loss Landscape of Neural Networks\"\nSpotlight Talk: \"Zero-Shot Learning from scratch: leveraging local compositional representations\"\n\nhttps://www.facebook.com/icml.imls/videos/365029137702011/\n\n---\nWorkshop Session\n\n\n\u2022 Subspace Inference for Bayesian Deep Learning\n\n\u2022 Quality of Uncertainty Quantification for Bayesian Neural Network Inference\n\n\u2022 \u2018In-Between\u2019 Uncertainty in Bayesian Neural Networks\n\nKeynote by Dawn Song: Adversarial Machine Learning: Challenges, Lessons, and Future Directions\n\nhttps://www.facebook.com/icml.imls/videos/320132412242165/\n\n---\nWorkshop on Generative Modeling and Model-Based Reasoning for Robotics and AI\n\nValue Focused Models, Invited Talk by David Silver\n\nManipulation by Feel: Touch-Based Control with Deep Predictive Models\n\nModel-based Policy Gradients with Entropy Exploration through Sampling\n\nModel-based Reinforcement Learning for Atari\n\nLearning to Predict Without Looking Ahead: World Models Without Forward Prediction\n\nPhysics-as-Inverse-Graphics: Joint Unsupervised Learning of Objects and Physics from Video\n\nPlanning to Explore Visual Environments without Rewards\n\nPRECOG: PrEdiction Conditioned On Goals in Visual Multi-Agent settings\n\nRegularizing Trajectory Optimization with Denoising Autoencoders\n\nTowards Jumpy Planning\n\nVariational Temporal Abstraction\n\nVisual Planning with Semi-Supervised Stochastic Action Representations\n\nWorld Programs for Model-Based Learning and Planning in Compositional State and Action Spaces\n\nOnline Learning and Planning without Prior Knowledge\n\nhttps://www.facebook.com/icml.imls/videos/2366831430268790/\n\n---\nWorkshop on Generative Modeling and Model-Based Reasoning for robotics and AI\n\n\"Online Learning for Adaptive Robotic Systems\" - Byron Boots\n\n\n\"An inference perspective on model-based reinforcement learning\"\n\n\"Reducing Noise in GAN Training with Variance Reduced Extragradient\"\n\n\n\"Complexity without Losing Generality: The Role of Supervision and Composition\" - Chelsea Finn\n\n\"Self-supervised Learning for Exploration &amp; Representation\" - Abhinav Gupta\n\nPanel Discussion\n\nhttps://www.facebook.com/icml.imls/videos/449245405622423/\n\n---\nWorkshop on Understanding and Improving generalization in Deep Learning\n\nPanel Discussion (Moderator: Nati Srebro)\n\n\"Overparameterization without Overfitting: Jacobian-based Generalization Guarantees for Neural Networks\"\n\n\"How Learning Rate and Delay Affect Minima Selection in Asynchronous Training of Neural Networks: Toward Closing the Generalization Gap\"\n\nhttps://www.facebook.com/icml.imls/videos/854556684898913/\n\n---\nWorkshop on Self-Supervised Learning\n\n\n\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" - Jacob Devlin\n\n\"Play as Self-Supervised Learning\" - Alison Gopnik\n\n\"Learning Latent Plans from Play\" - Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, Pierre Sermanet\n\n\"Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty\" - Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, Dawn Song\n\nhttps://www.facebook.com/icml.imls/videos/2479161722147572/\n\n---\nWorkshop on Identify and Understanding Deep Learnign Phenomena\n\n\"Optimization\u2019s Untold Gift to Learning: Implicit Regularization\" - Nati Srebro\n\n\"Bad Global Minima Exist and SGD Can Reach Them \"\n\n\"Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask\"\n\n\"Are all layers created equal? -- Studies on how neural networks represent functions\" - Chiyuan Zhang\n\nhttps://www.facebook.com/icml.imls/videos/450413519084800/\n\n---\nWorkshop on Exploration in Reinforcement Learning\n\n\"Exploration: The Final Frontier\" - Doina Precup\n\n\"Overcoming Exploration with Play\" - Corey Lynch\n\n\"Optimistic Exploration with Pessimistic Initialisation\" - Tabish Rashid\n\n\"Scheduled Intrinsic Drive: A Hierarchical Take on Intrinsically Motivated Exploration\" - Nicolai Dorka\n\n\"Generative Exploration and Exploitation\" (Missing)\n\n\"The Journey is the Reward: Unsupervised Learning of Influential Trajectories\" - Jonathan Binas\n\nhttps://www.facebook.com/icml.imls/videos/2236060723167801/\n\n---\nWorkshop on Exploration in Reinforcement Learning\n\n\"Sampling and exploration for control of physical systems\" - Emo Todorov\n\n\"Benchmarking Bonus-Based Exploration Methods on the Arcade Learning Environment\" - Adrien Taiga\n\n\"Simple Reget Minimzation for Contextual Bandits\" - Aniket Deshmukh\n\n\"Some Explorations of Exploration in Reinforcement Learning\" - Pieter Abbeel\n\nhttps://www.facebook.com/icml.imls/videos/2265408103721327/\n\n---\nWorkshop Session\n\n\n\u2022 Line attractor dynamics in recurrent networks for sentiment classi\ufb01cation \n\n\u2022 Do deep neural networks learn shallow learnable examples first? \n\n\u2022 Crowdsourcing Deep Learning Phenomena\n\nhttps://www.facebook.com/icml.imls/videos/855147788189057/\n\n---\n\"Agents that Set Measurable Goals for Themselves\" - Chelsea Finn\n\nhttps://www.facebook.com/icml.imls/videos/315467659393385/\n\n---\nWorkshop Session\n\n\"Reverse engineering neuroscience and cognitive science principles\" - Aude Oliva\n\n\"On Understanding the Hardness of Samples in Neural Networks\"\n\n\"On the Convex Behavior of Deep Neural Networks in Relation to the Layers' Width\"\n\n\"Intriguing phenomena in training and generalization dynamics of deep networks\" - Andrew Saxe\n\nhttps://www.facebook.com/icml.imls/videos/2353033231653025/\n\n---\nWorkshop session on Self-Supervised Learning\n\n\"Self Supervised Learning\" - Yann LeCun\n\n\"Revisiting Self-Supervised Visual Representation Learning\" - Alexander Kolesnikov, Xiaohua Zhai, Lucas Beyer\n\n\"Data-Efficient Image Recognition with Contrastive Predictive Coding\" - Olivier J. Henaff, Ali Razavi, Carl Doersch, S. M. Ali Eslami, Aaron van den Oord\n\nhttps://www.facebook.com/icml.imls/videos/378993762742156/\n\n---\nWorkshop Session on Explroation in Reinforcemnt Learning\n\n\"Exploration... in a dangerous world\" - Raia Hadsell\n\nLightning Talks:\n\n\"Curious iLQR: Resolving Uncertainty in Model-based RL\" - Sarah Bechtle\n\n\"An Empirical and Conceptual Categorization of Value-based Exploration Methods\" - Niko Yasui\n\n\"Skew-Fit: State-Covering Self-Supervised Reinforcement Learning\" - Vitchyr H. Pong\n\n\"Optimistic Proximal Policy Optimization\" - Takahisa Imagawa\n\n\"Exploration ", "link": "https://www.reddit.com/r/MachineLearning/comments/dyjf03/d_icml_2019_machine_learning_talks/"}, {"autor": "seanDL_", "date": "2019-01-16 17:26:28", "content": "[P] Data scrapper to creat -----> image !!!  collection for training safe va unsafe -----> image !!!  classifiers", "link": "https://www.reddit.com/r/MachineLearning/comments/agnfqi/p_data_scrapper_to_creat_image_collection_for/"}, {"autor": "ComfortablyNumb190", "date": "2019-01-16 15:16:29", "content": "\"[D]\" What is your dream -----> image !!!  dataset? /!/ We probably all have dream cars and if you're in the machine learning industry you probably have your dream dataset :)  \nImagine being able to have any image dataset you want - for your current project or future project or your dream project, doesn't matter. What would that image dataset be?", "link": "https://www.reddit.com/r/MachineLearning/comments/agm3me/d_what_is_your_dream_image_dataset/"}, {"autor": "zenggyu", "date": "2019-01-16 03:35:36", "content": "[D]Confusions regarding Mask R-CNN: how to scale masks for training and inference? /!/  In Mask R-CNN, the output feature maps of the mask branch are 28\\*28;  when training the network, the ground-truth masks are scaled to the same  size to compute the loss (as indicated by this post: [https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46](https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46)).\n\nMy question is, what techniques are used to scale the ground-truth  masks to 28\\*28? Does it involve any deep learning techniques  (convolution, max pooling, etc.) or any interpolation techniques  (bilinear interpolation, etc.)? Also, how are the mask outputs scaled to  the same scale as the original -----> image !!! ?", "link": "https://www.reddit.com/r/MachineLearning/comments/agh1gs/dconfusions_regarding_mask_rcnn_how_to_scale/"}, {"autor": "seanDL_", "date": "2019-01-16 03:09:28", "content": "[P] NSFW data scrapper: automatic -----> image !!!  collection for training classifiers", "link": "https://www.reddit.com/r/MachineLearning/comments/aggtbd/p_nsfw_data_scrapper_automatic_image_collection/"}, {"autor": "eleos_phobos", "date": "2019-01-16 01:25:35", "content": "[R] Discriminate between classes with subtle differences /!/ What is the best way to distinguish between two classes with very subtle differences? As an example, consider a dataset consisting of people in a variety of poses and environments and, for simplicity, assume we have only two labels: (wears/does not wear) a watch. -----> Image !!! -level annotations are available, but no bounding boxes. Now, a simple VGG net with cross-entropy loss could focus on a thousand things to make his decision. How could a second dataset consisting of watches help? To be clear: these two datasets are unrelated, no paired images. What\u2019s the best way to use the latter dataset to steer the focus of the former dataset. Like: \u201cHere is the item you should pay attention to.\u201d  Could an attention mechanism actually be useful here? Any domain transfer ideas that come to mind? I guess the simplest solution would be to train a watch detector on the watch dataset and apply it to the other dataset, but that feels very ad-hoc.", "link": "https://www.reddit.com/r/MachineLearning/comments/agfvx5/r_discriminate_between_classes_with_subtle/"}, {"autor": "paul_read_it", "date": "2019-04-17 11:51:21", "content": "[P] Experiment: 70fps real-time object detection with Google's Coral Dev Board with Edge TPU /!/ Maybe you have already heard of Google's Coral Dev Board with Edge TPU and ask yourself how well it performs. We made a video to share our experience: [https://youtu.be/bOYWx1jJCZo](https://youtu.be/bOYWx1jJCZo)\n\n&amp;#x200B;\n\nIn the video, we tested an object detection live stream under the following conditions:\n\n\\- a pretrained MobileNet v2 model, trained on the common objects in context (coco) dataset \n\n\\- a bounding boxes threshold of 45% confidence because there were way too many boxes displayed in the default configuration\n\n\\- a -----> camera !!!  connected via USB, not the official -----> camera !!!  from Coral\n\n&amp;#x200B;\n\nWe used this command to run the object detection server described above:\n\nedgetpu\\_classify\\_server \\\\ --source /dev/video1:YUY2:800x600:24/1  \\\\ --model path/to/model/mobilenet\\_ssd\\_v2\\_coco\\_quant\\_postprocess\\_edgetpu.tflite \\\\ --labels path/to/labels/coco\\_labels.txt --threshold=0.45\n\n&amp;#x200B;\n\nYou can find more demos to play around here:\n\n[https://coral.withgoogle.com/docs/dev-board/-----> camera !!! /](https://coral.withgoogle.com/docs/dev-board/-----> camera !!! /)\n\n&amp;#x200B;\n\nWe hope this example helps you to get started with your own project!\n\n&amp;#x200B;\n\nIf you have any idea, what we could build with it, let us know :-)\n\n&amp;#x200B;\n\nPaul", "link": "https://www.reddit.com/r/MachineLearning/comments/be6ua6/p_experiment_70fps_realtime_object_detection_with/"}, {"autor": "atulshanbhag", "date": "2019-04-17 03:03:47", "content": "-----> Image !!!  Style Transfer Using Convolutional Neural Networks, CVPR 2016, by Gatys et al /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/be2wej/image_style_transfer_using_convolutional_neural/"}, {"autor": "ripventura", "date": "2019-04-17 01:18:13", "content": "Training a model from an unique -----> image !!!  /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/be1v6k/training_a_model_from_an_unique_image/"}, {"autor": "RedditAcy", "date": "2019-04-16 18:54:47", "content": "[D]My Machine Learning Journal #11: Getting the macro view of reinforcement learning and more OpenAI gym stuff /!/ Wow guys, thanks for the huge support in my last post! \n\nHere's the vlog version of this journal (should I call it Vournal?) as usual: [https://youtu.be/N7KThxV5maI](https://youtu.be/N7KThxV5maI)\n\nAnyways, let me cut straight to the chase and provide as much value as possible.\n\nI spent a few hours researching just what the heck is going on with RL. I debriefed the most starred GitHub projects, the SOTA (state of the art) algorithms, and the open source platforms. I compiled all of this into a [google doc](https://docs.google.com/document/d/1jaYoQBoFq2KIc0u55eKuo8ZKi3cwtLXeeBWCxoHFUg4/edit?usp=sharing), but I think [this GitHub repository](https://github.com/aikorea/awesome-rl) contains even more information. Generally speaking, RL should be used whenever a problem can be modeled with an agent, environment, and reward setup. This is cool, because technically speaking we can train a lot of models with an RL mindset. For example, a traditional GAN has a generator and a discriminator, the generator is trained by how much it **fooled** the discriminator. With an RL mindset, we can define the generator as the agent, its state being the random strokes it painted, and the rewards being the degree it fooled the discriminator. Still the same workflow, but now we introduce more possibilities, we can tune the reward algorithms &amp; RL training mechanisms, etc. You can find more about RL GANs [here](https://hub.packtpub.com/how-googles-deepmind-is-creating-images-with-artificial-intelligence/).\n\nMy general plan for learning RL is to implement gym games first, then playing around with complex environments like [Project Malmo](https://github.com/Microsoft/malmo) and [ViZDoom](https://github.com/Microsoft/malmo), and at last, I will get onto the Unity RL env and making my own game and training my own RL agent to beat it!\n\nSo right now, let me implement a few gym games first, for this time, I am trying to beat Cartpole &amp; Acrobot. \n\n[Cartpole, objective is to balance the stick so it doesn't tilt over 15 degrees](https://i.redd.it/2cqqc5l9hns21.jpg)\n\nSince I copied and understood the atari breakout code, I thought this was a piece of cake! Interestingly, the observations of the environment turned out to be an array with 4 floats while I thought it was going to be a -----> picture !!!  (like in Atari). That changes my game plan, I can't just use the Atari code for this, because in Atari, we predict an action by feeding in a 210 \\* 180 \\* 3 image, which is the state, that image goes through a few Conv layers, connects to a Dense layer with 4 outputs, which represents the actions we can choose (we will choose the output that had the highest value because that represents the one that will yield the highest reward). But a cartpole state is a 4 \\* 1 array, so I decided to feed it through a random neural network and connected it to a dense layer with 2 outputs at the end. \n\nBut I still ended up using a great portion of [gsurma's code](https://github.com/gsurma/cartpole). The reason being that the original Atari code wasn't the best, it is intuitive to do this: \n\n    class AtariSolver:\n        def __init__(...): \n            define the model structure\n        def saveMemory(...): \n            self.memory.append the current state, reward, etc so we can train the model with the memory         \n        variable later\n        def getAction(...): \n             ...\n\nBut the Atari code scattered all of these across places, they weren't contained inside of a single class, which would make **a lot of sense.** Here's [my GitHub repository](https://github.com/BlastWind/cartpole) that beat Cartpole, again, huge credits to gsurma.\n\n&amp;#x200B;\n\nNow onto my **nemesis: the Acrobot.** \n\n*Processing gif 6girxjaykns21...*\n\nThe observation of an acrobot is an array with 6 floats, so I thought I could've beat it with the same approach with Cartpole and use the Deep Q-Learning algorithm to train the RL agent. \n\nI was wrong! I am still trying to motivate this skinny blue dude to cross the black line (the objective of the game)! The agent gets rewarded with negative points each frame the RL agent cannot reach the black line. **The problem is, a lot of negative points won't help on making the best choice if the agent never experienced a positive reward.** **And since the default Deep Q-Learning algorithm decreases the exploration rate over time, the agent will try random stuff less and less.** That is at least what I am feeling. I ran this model for 100 iterations and all of it terminated because it reached the maximum 500 timeframes. \n\nIt's okay, the next time I will be reporting back to you, I will have defeated this skinny blue dude. I looked into something that might help me, on the [openAI Gym Leaderboard](https://github.com/openai/gym/wiki/Leaderboard), someone beat acrobot with an algorithm called PPO (proximal policy optimization). It seems really hard to understand mathematically, but I will understand it, beat acrobot, and share it with you next time!", "link": "https://www.reddit.com/r/MachineLearning/comments/bdxohy/dmy_machine_learning_journal_11_getting_the_macro/"}, {"autor": "MarekCichy", "date": "2019-04-16 13:28:18", "content": "[D]Does AI have a dirty mind, too? Adversarial NSFW examples for humans and AI alike. /!/ Can seemingly NSFW images can fool AIs as well as humans? To answer this, I gathered 50 popular -----> image !!! s with NSFW optical illusions and fed them to 8 publicly available -----> image !!!  detectors. Results? Yes, they are foolable. Read more in my [Medium article](https://medium.com/@marekkcichy/does-ai-have-a-dirty-mind-too-6948430e4b2b) and contribute with your input.", "link": "https://www.reddit.com/r/MachineLearning/comments/bdty5y/ddoes_ai_have_a_dirty_mind_too_adversarial_nsfw/"}, {"autor": "Xayo", "date": "2019-04-16 12:57:06", "content": "[P] I used a Variational Autoencoder to build a feature-based face editing software /!/ Hey reddit,\n\nIn my latest weekend-project I have been using a Variational Autoencoder to build a feature-based face editor. The model is explained in my youtube video:\n\nhttps://youtu.be/uszj2MOLY08\n\nYou can inspect the code at Github:\n\nhttps://github.com/SteffenCzolbe/FeatureTransferApp\n\nThe feature editing is based on modifying the latent distribution of the VAE. After training of the VAE is completed, the latent space is mapped by encoding the training data once more. Latent space vectors of each feature are determined based on the labels of the training data. Then to edit an -----> image !!! , we can add a combination of feature vectors to its latent distribution, and then reconstruct it. The reconstruction creates an altered version of the original image, based on the featrures we added to the latent representation.\n\nThe model used is heavily inspired by the Bate-VAE used in this paper by google deepmind (https://pdfs.semanticscholar.org/a902/26c41b79f8b06007609f39f82757073641e2.pdf). I made some adjustments to it to incorporate more recent advancements in neural network architecture, like using a Leaky ReLu activation function. The dataset used is celebA, which consist of 200.000 annotated images of celebrities. I aligned and cropped the images to a 64x64 resolution before training. The model is implememted in PyTorch, and PyGame has been used for the GUI. Training on my single consumer grade GPU took about 1:30h. The finished application, inducing the trained model, runs smoothly even without GPU support.\n\nThis project has been quite cool, playing with the result has been good fun. I got a lot of hands-on experience with VAEs. Creating a YouTube video explaining the project let me to learn much more about video editing and presentation techniques. I'm testing the waters with presenting this project in video form, lets see if it pays off!", "link": "https://www.reddit.com/r/MachineLearning/comments/bdtmgh/p_i_used_a_variational_autoencoder_to_build_a/"}, {"autor": "cavedave", "date": "2019-04-16 10:46:59", "content": "The Society of Mind 30+ years later /!/  I found a copy of the Society of Mind by Minsky [here](http://www.acad.bg/ebook/ml/Society%20of%20Mind.pdf)\n\nAI has not gone this way since. Faster computation and better algorithms have each contributed about the same improvement. Better bigger datasets have also been a huge deal.\n\nThe big AI breakthroughs Deep Blue, Watson, AlphaGo, Alphafold\\*, self driving cars, -----> image !!!  recognition have not come from either agents or encoding human expertise into algorithms. But from better data and faster processing.  \nThe Bitter Lesson by Rich Sutton is good on how improving datasets, algorithms and hardware has improved AI [http://www.incompleteideas.net/IncIdeas/BitterLesson.html](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)\n\n&amp;#x200B;\n\n In T[he book of Why Pearl](https://www.nytimes.com/2018/06/01/business/dealbook/review-the-book-of-why-examines-the-science-of-cause-and-effect.html) talks about the [scruffies versus the neats](https://books.google.ie/books?id=BzM0DwAAQBAJ&amp;printsec=frontcover&amp;dq=the+book+of+Why+Pearl&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwi2uY7ItdThAhVYUhUIHWcPD1MQ6AEIKDAA#v=onepage&amp;q=neat&amp;f=false) and why the scruffies that just get things to work are in the ascendant at the moment. I am probably being unfair to Minsky here as I read his book 20 years ago. But I read it as more about finding underlying principles of cognition that we would put into use. And I do not see many cases where we have.   \n\n\nBut how much of Minsky's vision has happened? And will more happen in future?\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\*This is arguable as there was a good amount of NLP in the original Watson. Or that the Alphas are doing similar hierarchical reasoning to what Minsky talked about.", "link": "https://www.reddit.com/r/MachineLearning/comments/bdsgkn/the_society_of_mind_30_years_later/"}, {"autor": "AppleNamu", "date": "2019-04-16 09:08:59", "content": "Using CRF (conditional random fields) for outside of -----> image !!!  segmentation /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/bdrqno/using_crf_conditional_random_fields_for_outside/"}, {"autor": "dmdmello", "date": "2019-08-04 23:21:53", "content": "What's the current state-of-the-art for binary -----> image !!!  classification? /!/ I have no specific type of image or labels in mind. Just any state-of-the-art model for classifying images using binary labels. \n\nI would appreciate it if you showed me some articles and/or conferences to look for it.", "link": "https://www.reddit.com/r/MachineLearning/comments/cm35x5/whats_the_current_stateoftheart_for_binary_image/"}, {"autor": "dontuseyourreal_name", "date": "2019-08-04 13:55:10", "content": "Final year project /!/ Hi guys and gals,\nI was thinking of doing a machine learning / computer vision based project for my final year project, I was going to look into something like mounting a raspberry pi and a -----> camera !!!  to a car and being able to highlight stuff like stop signs and cars and such, and maybe changes in speed limits or something? Not sure, just a concept at the minute, but if anyone could point me in the right direction I'd appreciate it!", "link": "https://www.reddit.com/r/MachineLearning/comments/clwk2g/final_year_project/"}, {"autor": "idkname999", "date": "2019-08-04 10:27:22", "content": "[D] Unsupervised 3D Pose Estimation with Geometric Self-Supervision /!/ Has anyone seen the paper arxiv.org/abs/1904.04812\n\nI thought it is particularly interesting because it can generate 3d pose training only on 2d pose data. But how legit is this paper though? I find that their description of the geometric projections to be quite vague. How can they get 3d coordinates without knowing the parameters of the -----> camera !!! ?", "link": "https://www.reddit.com/r/MachineLearning/comments/cluwk0/d_unsupervised_3d_pose_estimation_with_geometric/"}, {"autor": "fershey17", "date": "2019-08-03 18:25:05", "content": "Detect a pattern in an -----> image !!!  /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/clm07b/detect_a_pattern_in_an_image/"}, {"autor": "aiff22", "date": "2019-10-24 19:00:49", "content": "[R] AI Benchmark: All About Deep Learning on Smartphones in 2019 /!/ **\\[arXiv Abstract\\]:**  The performance of mobile AI accelerators has been evolving rapidly in the past two years, nearly doubling with each new generation of SoCs. The current 4th generation of mobile NPUs is already approaching the results of CUDA-compatible Nvidia graphics cards presented not long ago, which together with the increased capabilities of mobile deep learning frameworks makes it possible to run complex and deep AI models on mobile devices. In this paper, we evaluate the performance and compare the results of all chipsets from Qualcomm, HiSilicon, Samsung, MediaTek and Unisoc that are providing hardware acceleration for AI inference. We also discuss the recent changes in the Android ML pipeline and provide an overview of the deployment of deep learning models on mobile devices. All numerical results provided in this paper can be found and are regularly updated on the official project website: [http://ai-benchmark.com](http://ai-benchmark.com)\n\n&amp;#x200B;\n\n[ Performance evolution of mobile AI accelerators rs: -----> image !!!  throughput for the float Inception-V3 model.](https://i.redd.it/n4jciy8ucju31.png)\n\n&amp;#x200B;\n\nThe paper discusses the following topics:\n\n1. Four generations of mobile NPUs\n2. Hardware acceleration resources for AI inference on each of Android mobile SoC platforms\n3. Android ecosystem for running deep learning models\n4. Quantized and Floating-point performance of **all generations** of mobile NPUs\n5. Performance comparison of FP inference on **mobile NPUs** vs. **Intel CPUs** vs. **Nvidia GPUs.**\n\nThe full paper is available on arXiv:  [https://arxiv.org/pdf/1910.06663.pdf](https://arxiv.org/pdf/1910.06663.pdf)", "link": "https://www.reddit.com/r/MachineLearning/comments/dmkzzo/r_ai_benchmark_all_about_deep_learning_on/"}, {"autor": "sabot00", "date": "2019-10-24 18:51:09", "content": "[D] Do CNNs understand semantic relationships between their classes? /!/ Hi all,\n\nHow do CNNs understand \"compositional semantic relationships\" between their classes? The problem exists in the entire field, but I'm referencing this paper in particular: http://gandissect.csail.mit.edu/\n\nIn the 3rd paragraph of the introduction of the paper, Bau et al. say (emphasis mine):\n&gt; To a human observer, a well-trained GAN appears to have learned facts about the objects in the -----> image !!! : for example, **a door can appear on a building but not on a tree**. We wish to understand how a GAN represents such structure. Do the objects emerge as pure pixel patterns without any explicit representation of objects such as doors and trees, or does the GAN contain internal variables that correspond to the objects that humans perceive?\n\nFrom my (limited) understanding of CNNs, they take in the input image (HxWx3 channels) and pass it through a bunch of filters and maxpool layers. Each maxpool layer reduces the HxW of the matrix. Each filter layer increases the depth of matrix as we move away from the image and toward the \"highest levels of representation.\" In a sense, we're abstracting toward higher and higher level features as the receptive field of our neurons increases and we're able to model longer-distance relationships. \n\nFinally, the last layer of the CNN is connected to the class output by a fully connected layer. \n\nFrom what I've read, it seems like the field is a bit split on this. You have this paper saying \n&gt; oh look! GANs (and thus CNNs) *can* understand relationships between classes because doors don't appear in the sky\n\nBut others also say it's an explicit shortcoming of the convolution function \u2014 that the spatial equivariance of convolution means it inherently cannot understand these relationships.\n&gt; A CNN only looks for 2 eyes, 1 nose, and 1 mouth. It doesn't care that the eyes are parallel and above the nose, or that the nose is above the mouth!\n\n\u2014\u2014\u2014\u2014\n\nMy take is that the CNN can understand broadly the correlations between classes because of the last, FC layer. As a result, it can understand that maybe standing is negatively correlated with beer, or that pens are correlated with paper. But, it can't understand spatial relationships. \n\nWhat do you guys think of this issue?", "link": "https://www.reddit.com/r/MachineLearning/comments/dmkuw5/d_do_cnns_understand_semantic_relationships/"}, {"autor": "itsDitzy", "date": "2019-10-24 16:35:38", "content": "[D] Suggestion for multi-digit number recognition/OCR approach /!/ hello, im a final year student in college. \n\nrecently, ive been tasked to build a system that can recognize runners based on their bib numbers. I came up with an idea to detect the runner first using mask r-cnn and then using the masked area from the processed -----> image !!!  to do the OCR for the bib numbers. Is there any suggestion for the best approach to do the OCR thing? thanks", "link": "https://www.reddit.com/r/MachineLearning/comments/dmiybr/d_suggestion_for_multidigit_number_recognitionocr/"}, {"autor": "bartbdm", "date": "2019-10-24 09:01:40", "content": "Deep -----> Image !!!  - AI/ML online upscale and enhance image with one click! /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/dmdpa1/deep_image_aiml_online_upscale_and_enhance_image/"}, {"autor": "Hurasuruja", "date": "2019-10-24 08:34:40", "content": "Confidence in regression problem /!/ I have a regression problem where I try to predict objects coordinates within a bounded field. The input is -----> image !!! , where  field, field bounds and object is seen. I use convolutional network to predict the object coordinates ( model is resNet50 + a single neuron at the top, RMSE is used as a loss function). Model works pretty well on input images where the bounds of the field are seen. \n\nHowever some of the input images are close-ups from the object, where field bounds cannot be seen. Therefore there are no landmarks to predict the objects location. In these cases the model always predicts the object to lie around the center of the field (majority of the true coordinates are around the center of the field).  These predictions make my model pretty much unusable. I would like the model not to predict when close-up of image is seen. Is there a common practice to deal with this type of problem? \n\nI could train a different classifier to detect the close-up images, but this would require lots of labeling, which is infeasible.\n\nI could also try something like described in here: [https://arxiv.org/abs/1506.02142](https://arxiv.org/abs/1506.02142) and ignore predictions where the uncertainty is high, but this would require to run the images trough the model several times.\n\nWould it be possible to define a custom non-continuous lost function, which would allow the model to 'choose' a constant value, when close-up image is seen? After the inference I could just ignore predictions where this constant value is chosen.", "link": "https://www.reddit.com/r/MachineLearning/comments/dmdhdg/confidence_in_regression_problem/"}, {"autor": "AbdallahNasr5", "date": "2019-10-23 15:35:39", "content": "-----> Image !!!  copy&amp;paste using CNN", "link": "https://www.reddit.com/r/MachineLearning/comments/dm102v/image_copypaste_using_cnn/"}, {"autor": "The_Amp_Walrus", "date": "2019-10-23 10:34:30", "content": "[D] Feature Loss vs. GANs - what are the trade offs? /!/ I'm doing a bit of reading on the speech enhancement problem, where you have an audio signal containing human speech plus some noise, and you want extract just the human speech. It's pretty analogous to -----> image !!!  denoising or \"super-resolution\", and a lot of the techniques from the -----> image !!!  domain are being borrowed and re-applied to audio quite successfully (eg. repurposing the [U-Net](https://arxiv.org/abs/1505.04597) architecture from image processing to [spectrograms](https://ismir2017.smcnus.org/wp-content/uploads/2017/10/171_Paper.pdf) and then [raw audio](https://arxiv.org/pdf/1806.03185.pdf)). It's all pretty cool.\n\nThere's some interesting work being done with loss functions this space and I'm looking for some clarification as to why you'd choose one approach over another. You want to compare a target image, or audio waveform, with a predicted sample, and you need to define a loss function which measures how \"close\" they are. The *Related work* section of [this paper](https://arxiv.org/pdf/1609.04802.pdf) gives a pretty good overview of the different approaches, which I'll try to summarize here.  \n\n* **Mean squared error loss**: A pretty standard regression loss as far as I know, but it's limited to only considering one pixel at a time: \"the ability of MSE to capture perceptually relevant differences, such as high texture detail, is very limited\".\n* **Feature loss**: This is where you pre-train a network on a similar problem, such as image classification, and then you freeze the weights. For both the target and predicted sample, you run each through the classification network, then grab some internal activations from that network and call them \"features\". You compute some distance between these feature vectors to get your loss. The key idea is that the classification network is able to capture important features that MSE loss cannot. \n* **GAN loss**: A discriminator network trains in-tandem with the generator network, where the job of the discriminator is to classify whether its input is \"real\" or \"generated\". Like the feature loss network, it can detect features that MSE loss cannot, but it can also punish identifiable quirks of the generator network, whereas feature loss can potentially be \"hacked\" by the generator network.\n\nSo my questions are:\n\n* Have I characterised these approaches well?\n* Why would you ever choose feature loss over using a discriminator network (ie. GAN)?\n   * Discriminators can punish the generator for being predictably wrong (ie. common artifacts)\n   * Pre-trained feature loss networks may better represent image features, if they have been trained for longer, on larger data sets\n   * Apparently GANs can have stability issues when training\n* The [SRGAN](https://arxiv.org/pdf/1609.04802.pdf) suggests using both feature loss *and* a GAN for their loss function - is this the best known approach?", "link": "https://www.reddit.com/r/MachineLearning/comments/dlxd2t/d_feature_loss_vs_gans_what_are_the_trade_offs/"}, {"autor": "redditferdays", "date": "2019-10-23 04:23:04", "content": "[D] Tools/Techniques for Efficiently Sorting -----> Image !!!  Data /!/ I'm planning on sorting \\~100,000 images to use as data for a computer vision application. With this much data, shaving a little time off of each picture would add up quickly. I was wondering whether there are any tools or techniques to make this as quick and easy as possible. \n\nFor my specific task I'm simply looking to go through an entire folder of images and discard those that aren't 'good pictures', no complicated sorting required.\n\nThanks in advance, and sorry if this is the wrong tag.", "link": "https://www.reddit.com/r/MachineLearning/comments/dlu1wm/d_toolstechniques_for_efficiently_sorting_image/"}, {"autor": "donjuan1337", "date": "2019-10-23 03:46:51", "content": "[D] Spatio-temporal modeling, scalar input /!/ I'm doing video prediction research i.e. predicting the next frame in a video sequence. In essential it's just a mapping from the past frame into the future frame. I wonder how I can incorporate a scalar input in addition to the input frame.\n\nSince I'm just using CNN operations and never making any flattening of the feature maps, I cannot concatenate the scalar input directly. I have found [this](https://stats.stackexchange.com/questions/299322/combining------> image !!! -and-scalar-inputs-into-a-neural-network) which suggest that one could treat the bias as the scalar input of some CNN layer but doing so you are not directly adding any parameters to the scalar input.\n\nDoes anyone have any experience with this? All info, papers etc are appreciated!", "link": "https://www.reddit.com/r/MachineLearning/comments/dltoj8/d_spatiotemporal_modeling_scalar_input/"}, {"autor": "donjuan1337", "date": "2019-10-23 03:36:50", "content": "[R] Spatio-temporal modeling, scalar input /!/ I'm doing video prediction research i.e. predicting the next frame in a video sequence. In essential it's just a mapping from the past frame into the future frame. I wonder how I can incorporate a scalar input in addition to the input frame.\n\nSince I'm just using CNN operations and never making any flattening of the feature maps, I cannot concatenate the scalar input directly. I have found [this](https://stats.stackexchange.com/questions/299322/combining------> image !!! -and-scalar-inputs-into-a-neural-network) which suggest that one could treat the bias as the scalar input of some CNN layer but doing so you are not directly adding any parameters to the scalar input.\n\nDoes anyone have any experience with this? All info, papers etc are appreciated!", "link": "https://www.reddit.com/r/MachineLearning/comments/dltkrj/r_spatiotemporal_modeling_scalar_input/"}, {"autor": "budha3", "date": "2019-10-22 23:47:37", "content": "-----> Image !!!  classification capability question - ukiyo-e print art", "link": "https://www.reddit.com/r/MachineLearning/comments/dlqrht/image_classification_capability_question_ukiyoe/"}, {"autor": "winchester6788", "date": "2019-03-30 07:31:33", "content": "[P] Nudity detection and Censoring in images with Image Classification and Object Detection /!/ Hi all, for the past two months, I worked on collecting and curating dataset for nudity detection using -----> image !!!  classification and censoring using object detection.\n\nThe code and pre-trained models are available at https://github.com/bedapudi6788/NudeNet\n\nThe test results and methodology are explained in the following posts\n\nhttps://medium.com/@praneethbedapudi/nudenet-an-ensemble-of-neural-nets-for-nudity-detection-and-censoring-d9f3da721e3?source=friends_link&amp;sk=e19cdcc610e63b16274dd659050ea955\n\nhttps://medium.com/@praneethbedapudi/nudenet-an-ensemble-of-neural-nets-for-nudity-detection-and-censoring-c8fcefa6cc92?source=friends_link&amp;sk=f0a4786bf005cd4b7e89cf625f109af0\n", "link": "https://www.reddit.com/r/MachineLearning/comments/b78j1q/p_nudity_detection_and_censoring_in_images_with/"}, {"autor": "xuj1", "date": "2019-03-29 13:32:30", "content": "[R] 9 new SOTA records: Invariant Information Clustering for unsupervised -----> image !!!  classification and segmentation /!/ The state of the art in unsupervised learning. 9 new records set on 8 datasets - CIFAR, MNIST, COCO-Stuff, STL from ImageNet. Paper and code out now:  [https://arxiv.org/pdf/1807.06653.pdf](https://arxiv.org/pdf/1807.06653.pdf)\n\nhttps://i.redd.it/51qkizj382p21.jpg\n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/b6xbf1/r_9_new_sota_records_invariant_information/"}, {"autor": "jacksonkr_", "date": "2019-03-29 01:31:13", "content": "Create 3D Object Model from Still RGB -----> Camera !!!  /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/b6raks/create_3d_object_model_from_still_rgb_camera/"}, {"autor": "wei_jok", "date": "2019-03-29 00:52:45", "content": "[R] StrokeNet: A Neural Painting Environment (ICLR 2019) /!/ This [paper](https://openreview.net/forum?id=HJxwDiActX) presents a method that is like combining a SPIRAL with a learned/differentiable painting 'world model' and training the system end-to-end.\n\n**Abstract**\n\n*We've seen tremendous success of -----> image !!!  generating models these years. Generating images through a neural network is usually pixel-based, which is fundamentally different from how humans create artwork using brushes. To imitate human drawing, interactions between the environment and the agent is required to allow trials. However, the environment is usually non-differentiable, leading to slow convergence and massive computation. In this paper we try to address the discrete nature of software environment with an intermediate, differentiable simulation. We present  StrokeNet, a novel model where the agent is trained upon a well-crafted neural approximation of the painting environment. With this approach, our agent was able to learn to write characters such as MNIST digits faster than reinforcement learning approaches in an unsupervised manner. Our primary contribution is the neural simulation of a real-world environment. Furthermore, the agent trained with the emulated environment is able to directly transfer its skills to real-world software.*\n\nAccepted as a poster at ICLR this year: https://openreview.net/forum?id=HJxwDiActX", "link": "https://www.reddit.com/r/MachineLearning/comments/b6qwh4/r_strokenet_a_neural_painting_environment_iclr/"}, {"autor": "artificial_intelect", "date": "2019-03-28 23:40:27", "content": "[D][R] Is there a theoretical or fundamental reason why LayerNorm outperforms BatchNorm on RNN networks? /!/ Is there a theoretical or fundamental reason why LayerNorm outperforms BatchNorm on RNN networks? Is the best answer that we have just simply because the [Layer Normalization Paper](https://arxiv.org/abs/1607.06450) ran an experiment, therefore it's is better.\n\nFor instance, BN normalizes using population statistics. Do population statistics not make sense in RNN networks like they do in -----> image !!!  networks?\n\nAlternatively, Layer Normalization is actually better for Image Tasks, but the regularization effect of BN is needed durring optimization since the optimization problem is so ill-posed.\n\nAnother alternative: the the regularization effect of BN is too strong for RNN networks.\n\nI still don't know the answer, the above are just guesses.\n\n&amp;#x200B;\n\nGenerally speaking should population statistics behave well in RNN's?", "link": "https://www.reddit.com/r/MachineLearning/comments/b6q4on/dr_is_there_a_theoretical_or_fundamental_reason/"}, {"autor": "aDutchofMuch", "date": "2019-03-28 18:41:03", "content": "[R] Really fast object-level -----> image !!!  search for deep features like DELF, with linear-time geometric verification", "link": "https://www.reddit.com/r/MachineLearning/comments/b6mk4m/r_really_fast_objectlevel_image_search_for_deep/"}, {"autor": "techartwork", "date": "2019-03-28 16:17:04", "content": "AI -----> Image !!!  with AI Music with AI Voice with AI Text", "link": "https://www.reddit.com/r/MachineLearning/comments/b6ksa0/ai_image_with_ai_music_with_ai_voice_with_ai_text/"}, {"autor": "tcr98", "date": "2019-12-09 21:56:11", "content": "Explore -----> image !!!  augmentations using a convenient tool", "link": "https://www.reddit.com/r/MachineLearning/comments/e8gq9e/explore_image_augmentations_using_a_convenient/"}, {"autor": "unrulyspeed", "date": "2019-12-09 21:24:37", "content": "[D] Sonnet implementation of DeepMind's VQ-VAE 2 -----> image !!!  generation? /!/ [Paper](https://arxiv.org/pdf/1906.00446v1.pdf). I am aware of the [vqvae example](https://github.com/deepmind/sonnet/blob/v2/examples/vqvae_example.ipynb) in the sonnet repository, however this only covers reconstruction of images rather than image generation.\n\nAfter digging a bit I found [this pytorch implementation](https://github.com/rosinality/vq-vae-2-pytorch) which does have the generation, but I am wondering if there is some Sonnet implementation that I just haven't come across? Perhaps one by the DeepMind team?", "link": "https://www.reddit.com/r/MachineLearning/comments/e8gb1h/d_sonnet_implementation_of_deepminds_vqvae_2/"}, {"autor": "miznick", "date": "2019-12-09 16:05:59", "content": "[P] Is cGAN the right approach? /!/ Hi all - Relatively new to ML, but have been doing my homework.  I have an application where I am trying to generate an -----> image !!!  based on data vectors from a non-optical domain.  The mapping / relationship is unknown, but let's assume there is some deterministic relationship.  (For example, if I had ultrasonic data reflecting off a target and I wanted to generate an image of the target.)  Could I use a cGAN model and train it with known reflection / image pairs?  The thing I find confusing is that most if not all cGAN example use a noise vector as the input to the generator.  Couldn't I simply use my non-image (reflection) data as the input vector instead?  My simplistic understanding is that the noise vector acts as a \"recipe\" for the unknown image, and the cGAN is learning to read the recipe through trial and error reduction.\n\nWhat else should I be diving into to get this working?", "link": "https://www.reddit.com/r/MachineLearning/comments/e8busu/p_is_cgan_the_right_approach/"}, {"autor": "SumanthN9", "date": "2019-12-09 14:25:16", "content": "Suggestions for -----> Image !!!  datasets that contagion both object segmentations and object bounding boxes /!/ Hi,\n\nI am looking for suggestions for some generic image datasets that contain both segmentations and bounding boxes of the objects. I already know about Pascal VOC dataset.\n\nThanks,\n\nSumanth.", "link": "https://www.reddit.com/r/MachineLearning/comments/e8alc8/suggestions_for_image_datasets_that_contagion/"}, {"autor": "SumanthN9", "date": "2019-12-09 14:13:18", "content": "-----> Image !!!  dataset with Segmentation and Object Detection Labels (I already know about Pascal VOC) /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/e8agbs/image_dataset_with_segmentation_and_object/"}, {"autor": "YoungStellarObject", "date": "2019-12-09 09:40:03", "content": "[D] Techniques for guiding NN attention? /!/ Are there techniques out there for making NNs attend to certain parts of the input (say, regions in an -----> image !!! ), but not to others?  \nI can imagine replacing the \"unwanted\" regions with noise or constant values, but are there better approaches?  \n\n\nThanks for your insights!", "link": "https://www.reddit.com/r/MachineLearning/comments/e87vj3/d_techniques_for_guiding_nn_attention/"}, {"autor": "ilia10000", "date": "2019-12-08 23:46:10", "content": "[R] Soft-Label Dataset Distillation and Text Dataset Distillation /!/ Paper: [https://arxiv.org/abs/1910.02551](https://arxiv.org/abs/1910.02551)\n\nCode: [https://github.com/ilia10000/dataset-distillation](https://github.com/ilia10000/dataset-distillation)\n\nDataset distillation is a method for reducing dataset sizes by learning a small number of synthetic samples containing all the information of a large dataset. This has several benefits like speeding up model training, reducing energy consumption, and reducing required storage space. Currently, each synthetic sample is assigned a single \\`hard' label, and also, dataset distillation can currently only be used with -----> image !!!  data. We propose to simultaneously distill both images and their labels, thus assigning each synthetic sample a \\`soft' label (a distribution of labels). Our algorithm increases accuracy by 2-4% over the original algorithm for several image classification tasks. Using \\`soft' labels also enables distilled datasets to consist of fewer samples than there are classes as each sample can encode information for multiple classes. For example, training a LeNet model with 10 distilled images (one per class) results in over 96% accuracy on MNIST, and almost 92% accuracy when trained on just 5 distilled images. We also extend the dataset distillation algorithm to distill sequential datasets including texts. We demonstrate that text distillation outperforms other methods across multiple datasets. For example, models attain almost their original accuracy on the IMDB sentiment analysis task using just 20 distilled sentences.\n\nThis is my first full-length paper and code release so I'd love to get your feedback on the text and the code, especially since this will likely be part of my thesis!", "link": "https://www.reddit.com/r/MachineLearning/comments/e81lew/r_softlabel_dataset_distillation_and_text_dataset/"}, {"autor": "Nawaf-waqas", "date": "2019-12-08 02:10:47", "content": "Dcgan for higher resolution /!/ Hello friends I hope you all will be good, I want to synthesize high resolution -----> image !!!  using DCGAN so can I produce High resolution image lets say 384x384. Actually I want to synthesize medical MR image and its respective masks.\n\nI am also new to this field so explain in much simpler way if you can and also mention me the code if possible to produce such kind of output.\n\nThank you in advance for your help.", "link": "https://www.reddit.com/r/MachineLearning/comments/e7nmno/dcgan_for_higher_resolution/"}, {"autor": "matttron3000", "date": "2019-09-10 01:59:48", "content": "Question scanning books /!/ Hello all, \n\nFirmware engineer dipping their toes into the vast world of machine learning here. A friend works at our local library and was telling me about how the worst part of her job is when people put backs on the shelves themselves as they often do it wrong. So she has to scan each row going book by book and find books out of order. It's time consuming and mentally draining to do for a long period of time. I wonder if it would be possible to make an app on her phone that would read the white dewey decimal system tags on the spines of the books and figure out if a book was out of order? I've taken some sample footage and noticed a few problems:  \n\\- Passing the -----> camera !!!  over rows tends to induce some blur and while I think it could reduced or unblurred, it does make it hard.  \n\\- Some books are too thin for the whole tag and it's impossible to read (I don't know a way to handle that)\n\nHere's what I'm currently thinking, tell me if I'm totally off track.  \n1. Process image and reduce blur and adjust to high constrast  \n2. Cut out \"labels\" on the books and create small B&amp;W images with the source location of where they came from.  \n3. Rotate the label as needed (sometimes they are at a 90 degree angle)  \n3. OCR the label and categorize it into the dewey\n\nIs this feasible with some degree of accuracy? Ultimately, I'd just want to highlight the book that's out of order in red. I don't need to do anything more complicated.", "link": "https://www.reddit.com/r/MachineLearning/comments/d20ziz/question_scanning_books/"}, {"autor": "AnotherForce", "date": "2019-09-09 15:07:09", "content": "Performance RAM vs -----> image !!!  observations in OpenAI Retro Gym /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/d1s4nn/performance_ram_vs_image_observations_in_openai/"}, {"autor": "rathernot000", "date": "2019-09-09 09:26:24", "content": "[D] Are filters from a particular Convolutional layer for a given CNN chosen at random by random initialization of weights in that network? /!/ In a Convolutional Layer of a given Convolutional Neural Network there is a defined input of size NxMxD. Whereas N and M stand for a dimension of an input -----> image !!!  (can be smaller than the original size of an -----> image !!!  due to pooling) and as I understand, D stands for a number of filters used in Convolution. My question is how network decides, what are the best filters for a given layer?", "link": "https://www.reddit.com/r/MachineLearning/comments/d1ojnm/d_are_filters_from_a_particular_convolutional/"}, {"autor": "artika_labs", "date": "2019-09-08 12:25:42", "content": "[R] Audio Conversion GAN with Unpaired Data /!/ For the past month I have been working on voice conversion using unpaired data. I naively applied -----> image !!!  conversion algorithms to audio spectrograms and after working out a few obstacles I got convincing, although not perfect, results.\n\nUsing the exact same algorithm on music genre conversion is also possible and the results, despite a fairly shallow generator with very low capacity, are pretty interesting.\n\nHere are some examples:\n\n[https://youtu.be/3BN577LK62Y](https://youtu.be/3BN577LK62Y)\n\nThe model is able to translate audio signals of any length and does not use any vocoder.\n\nI cannot find papers with similar approaches, and I don't really know what I should do with this research. Being an Engineering student and not understanding how the academic world works, maybe a simple article and a code release is the best idea.\n\nThank you for your attention!", "link": "https://www.reddit.com/r/MachineLearning/comments/d1a9vc/r_audio_conversion_gan_with_unpaired_data/"}, {"autor": "compsci_prof", "date": "2019-09-08 00:25:55", "content": "question about leaf node of regression tree /!/  \n\nhello, i'm working through the data mining textbook \"data mining - practical machine learning tools and techniques\", and there's an example involving regression trees (see attached -----> image !!! ). in the book they say \"the leaves of the tree are values that represent the average outcome for instances that reach the leaf\".\n\nhowever, if you look at the leftmost leaf on the tree, there are three values: 19.3 (28/8.7%)\n\ni get that 19.3 is the average outcome of instances that reach the leaf\n\ni get that 28 is the number of instances that reach that leaf (if you add them all you get 209, the number of instances in the dataset)\n\nhowever, i have no idea what the 8.7% represents. i know its correlated with the average outcome value, ie if you look at the rightmost leaf, the value is 783 (5/359%), so does anyone know what this 359% represents?\n\nthanks\n\n*Processing img zvrhzuo5k9l31...*", "link": "https://www.reddit.com/r/MachineLearning/comments/d140ku/question_about_leaf_node_of_regression_tree/"}, {"autor": "PuzzledProgrammer3", "date": "2019-09-07 13:25:13", "content": "[D] Facebook Microsoft $10M deepfake detection challenge /!/ blog post: [https://ai.facebook.com/blog/deepfake-detection-challenge/](https://ai.facebook.com/blog/deepfake-detection-challenge/)\n\nchallenge: [https://deepfakedetectionchallenge.ai/](https://deepfakedetectionchallenge.ai/)\n\nalso repo for generating deepfakes from a single -----> image !!!  with a few shot approach: [https://github.com/shaoanlu/fewshot-face-translation-GAN](https://github.com/shaoanlu/fewshot-face-translation-GAN)\n\nit works on games as well: https://twitter.com/roadrunning01/status/1170121199285866497?s=20", "link": "https://www.reddit.com/r/MachineLearning/comments/d0vxrs/d_facebook_microsoft_10m_deepfake_detection/"}, {"autor": "mesmer_adama", "date": "2019-09-07 09:50:13", "content": "[D] Learnable -----> image !!!  loss - what are the approaches? /!/ When doing auto encoders on images L1 loss is the standard choice but it tend to produce blurry images. Is there any literature that have a learnable loss, maybe something like GANs or some other function that takes a target image and an  output from the network but is also trainable?\n\nI haven't seen this in the VAE and AE papers I've read but I'm sure there are lots of examples. Have you found any learnable losses or similar approaches?", "link": "https://www.reddit.com/r/MachineLearning/comments/d0u3vj/d_learnable_image_loss_what_are_the_approaches/"}, {"autor": "etotheipi_", "date": "2019-12-23 17:49:41", "content": "Should autoencoders be symmetric? /!/ I've spent a lot of time working with -----> image !!!  autoencoders, primarily to learn Tensorflow and different deep learning architectures.  I always find myself wanting to make the decoder side as symmetric as possible with respect to the encoder side, because it feels like an \"elegant\" design decision  But I suspect that it's not optimal.\n\nIn most of mathematics, complex functions tend to have even more complex inverses.  With respect to CNNs, convolutions are not strictly invertible, so it seems like the Conv2DTranspose operations could benefit from a higher number of parameters to approximate it better.   Some simple google searching doesn't turn up any direct discussions of this.  I'm primarily curious if anyone has direct experience studying this, or if there are conventions for \"optimizing\" the decoder side of an autoencoder (or maybe the encoder side needs more parameters...?!). \n\nMy first inclination is to just increase some numbers on the decoder side to give it twice as many parameters.  But maybe using more layers is ideal, since it more potently increases the complexity of functions it can approximate.\n\nHere's a sample that is as close as you can get to pure symmetry (without BatchNorm, dropout, etc):\n\n    latent_size = 64\n    \n    # Encoder\n    enc_input = L.Input(shape=(48, 48, 3))\n    enc0 = L.Conv2D(filters= 32, kernel_size=5, strides=2, padding='same', activation='relu')(enc_input)\n    enc1 = L.Conv2D(filters= 64, kernel_size=4, strides=2, padding='same', activation='relu')(enc0)\n    enc2 = L.Conv2D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')(enc1)\n    \n    enc_flat = L.Flatten()(enc2)\n    enc_dense = L.Dense(256, activation='tanh')(enc_flat)\n    enc_out = L.Dense(latent_size, activation='linear')(enc_dense)\n    encoder = keras.Model(inputs=enc_input, outputs=enc_out, name='Encoder')\n    \n    # Decoder\n    dec_input = L.Input(shape=(latent_size,))\n    dec_dense1 = L.Dense(256, activation='tanh')(dec_input)\n    dec_dense2 = L.Dense(6*6*128, activation='relu')(dec_dense1)\n    dec_reshape = L.Reshape((6,6,128))(dec_dense2)\n    dec2 = L.Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu')(dec_reshape)\n    dec1 = L.Conv2DTranspose(filters=32, kernel_size=4, strides=2, padding='same', activation='relu')(dec2)\n    dec0 = L.Conv2DTranspose(filters= 3, kernel_size=5, strides=2, padding='same', activation='linear')(dec1)\n    decoder = keras.Model(inputs=dec_input, outputs=dec0, name='Decoder')\n    \n    encoder.summary()\n    decoder.summary()\n\nThis creates a beautifully symmetric computation graph:\n\n    Model: \"Encoder\"\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #   \n    =================================================================\n    input_5 (InputLayer)         [(None, 48, 48, 3)]       0         \n    _________________________________________________________________\n    conv2d_9 (Conv2D)            (None, 24, 24, 32)        2432      \n    _________________________________________________________________\n    conv2d_10 (Conv2D)           (None, 12, 12, 64)        32832     \n    _________________________________________________________________\n    conv2d_11 (Conv2D)           (None, 6, 6, 128)         73856     \n    _________________________________________________________________\n    flatten_3 (Flatten)          (None, 4608)              0         \n    _________________________________________________________________\n    dense_8 (Dense)              (None, 256)               1179904   \n    _________________________________________________________________\n    dense_9 (Dense)              (None, 64)                16448     \n    =================================================================\n    Total params: 1,305,472\n    \n    \n    \n    \n    Model: \"Decoder\"\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #   \n    =================================================================\n    input_6 (InputLayer)         [(None, 64)]              0         \n    _________________________________________________________________\n    dense_10 (Dense)             (None, 256)               16640     \n    _________________________________________________________________\n    dense_11 (Dense)             (None, 4608)              1184256   \n    _________________________________________________________________\n    reshape_1 (Reshape)          (None, 6, 6, 128)         0         \n    _________________________________________________________________\n    conv2d_transpose_3 (Conv2DTr (None, 12, 12, 64)        73792     \n    _________________________________________________________________\n    conv2d_transpose_4 (Conv2DTr (None, 24, 24, 32)        32800     \n    _________________________________________________________________\n    conv2d_transpose_5 (Conv2DTr (None, 48, 48, 3)         2403      \n    =================================================================\n    Total params: 1,309,891", "link": "https://www.reddit.com/r/MachineLearning/comments/eenvdf/should_autoencoders_be_symmetric/"}, {"autor": "bharathsh36", "date": "2019-12-23 15:40:34", "content": "What are some of the best practices when it comes to building a -----> image !!!  detection model? /!/ Also what are some of the important things that students should know when building such models for production?", "link": "https://www.reddit.com/r/MachineLearning/comments/eem4jo/what_are_some_of_the_best_practices_when_it_comes/"}, {"autor": "mankaden", "date": "2019-12-23 10:35:47", "content": "Does anyone know of any open source or commercial qa software for -----> image !!!  annotation? Looking primarily for a feature to accept or reject images.", "link": "https://www.reddit.com/r/MachineLearning/comments/eej0b8/does_anyone_know_of_any_open_source_or_commercial/"}, {"autor": "Lorenzo_de_Medici", "date": "2019-12-23 04:36:43", "content": "[P] Implementing \"Ambient Sound Provides Supervision for Visual Learning\" /!/ Hi, I attempted to recreate the results from the multi-modal self supervised learning paper - \"Ambient Sound Provides Supervision for Visual Learning\" by Owens, et. al.\n\nHere is the code along with my detailed report on it - [https://github.com/rowhanm/ambient-sound-self-supervision](https://github.com/rowhanm/ambient-sound-self-supervision)\n\nSome key things I learned during this:\n\n1. Sound is an interesting supervision signal for -----> image !!! /scene recognition.\n2. Representation of sound matter quite a lot. I tried using MFCCs along with the Statistical sound summaries proposed in the original paper and saw some increase in the downstream task evaluation.\n3. I don't know what makes an optimal sound representation. Perhaps these can be learned.\n4. Visualizing the top activated images from learned by using sound as supervision, we see that the model somewhat understands the context in which an object occurs (like fish splashing in water/ man holding some kind of fish) and not the details of the object itself.\n5. Can we combine both audio and images as supervisory signals? What are some good papers on this?\n\nWould love to hear some comments/criticisms/thoughts on this.\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/eefml7/p_implementing_ambient_sound_provides_supervision/"}, {"autor": "Virtual_Error", "date": "2019-12-23 02:40:34", "content": "[D] \"Reproducibility is the wrong objective for Machine Learning. Reproducibility is key for science, but ML is not a science.\" /!/ Quote of this post can be found here:\n\n[https://twitter.com/notmisha/status/1208569457511030784](https://twitter.com/notmisha/status/1208569457511030784)\\\\\n\nThe tweet is referring to this article:\n\n\"This AI researcher is trying to ward off a reproducibility crisis\"\n\n[https://www.nature.com/articles/d41586-019-03895-5](https://www.nature.com/articles/d41586-019-03895-5)\n\nTo give some context: Joelle Pineau's group authored a paper called \"Deep Reinforcement Learning that Matters\" ([https://arxiv.org/abs/1709.06560](https://arxiv.org/abs/1709.06560)) which performed a meta-analysis of how reproducible papers in the deep RL community were. They found that reproducing results were surprisingly difficult and the methods these papers used to report results were questionable.\n\nI personally think Joelle handled the topic very diplomatically and in her talk made efforts not to isolate any individual or group. Despite her efforts, the work of one or two groups did stand out.\n\nI thought I would start a discussion on the merits of reproducibility and reflect on how important it really is. If it is important, what makes it important? Is it over emphasized over the bigger -----> picture !!! ? What should we strive for in the future?", "link": "https://www.reddit.com/r/MachineLearning/comments/eeebni/d_reproducibility_is_the_wrong_objective_for/"}, {"autor": "elmsha", "date": "2019-12-22 18:38:09", "content": "Decision Tree Classifier is not working with categorical values /!/  I am using a Decision Tree Classifier as part of an ensemble (soft voting system). The problem is that I have the categorical features already converted to Integer values (i.e. if blood pressure is within the medium range, then blood pressure =2 in the dataset).\n\nThe categories for the feature variables are as follows\n\nfeature#1 (total cholesterol) = 1, 2, 3\n\nfeature#2 (Systolic Blood Pressure) = 1, 2, 3\n\nfeature#3 (diastolic Blood Pressure) = 1, 2, 3\n\nfeature#4 (smoking rate) = 1, 2, 3\n\nThe target/class variable (stroke) = 1, 2 (yes and no, respectively).\n\nThe problem is that when I draw the DT (please check the -----> picture !!! ), I expect to get a splitting decision where a certain feature equals one of the above-mentioned values (i.e. 1, 2, 3). However, it is giving me other values like 1.5, 0.5, etc.,\n\nI have included the code and a sample dataset for anyone who can help me out. btw, I need to keep the current dataset in the number format as I am using it in conjunction with other classifiers for the soft voting classifier.\n\n Here is the [dataset](https://www.dropbox.com/s/lj3kqbra2z5n7bj/FDALL_balanced8_convert.csv?dl=0) and  this is [the code](https://www.dropbox.com/s/s5zj7usgrr1kylo/DT.py?dl=0), I am using Spyder with Python 3.7\n\n&amp;#x200B;\n\n![img](382rpqp3a8641)", "link": "https://www.reddit.com/r/MachineLearning/comments/ee8faq/decision_tree_classifier_is_not_working_with/"}, {"autor": "KichangKim", "date": "2019-12-22 03:21:34", "content": "[P] Deep Danbooru (girl -----> image !!!  tag estimation) source code is available on github now /!/ Previous post: [https://www.reddit.com/r/MachineLearning/comments/akbc11/p\\_tag\\_estimation\\_for\\_animestyle\\_girl\\_image/](https://www.reddit.com/r/MachineLearning/comments/akbc11/p_tag_estimation_for_animestyle_girl_image/)\n\nHi, as promised, I released my code to github now.\n\nHere is brief changes from last post:\n\n* ML library is changed from CNTK to Tensorflow 2.x\n* Optimizer is changed from Adam to SGD with momentum\n* Dataset is updated (20191108)\n\nRepository URL is here : [https://github.com/KichangKim/DeepDanbooru](https://github.com/KichangKim/DeepDanbooru)\n\nIt contains pre-trained model as well, so you can test your image immediately.\n\nLegacy CNTK version web demo is still available, but URL is changed to  [http://kanotype.iptime.org:8004/deepdanbooru/](http://kanotype.iptime.org:8003/deepdanbooru/)\n\n(only port). Current web demo uses latest Tensorflow 2 version. I'll keep CNTK version demo for a while, but I'll end it one day.\n\nHave fun.", "link": "https://www.reddit.com/r/MachineLearning/comments/edzqp8/p_deep_danbooru_girl_image_tag_estimation_source/"}, {"autor": "KichangKim", "date": "2019-12-22 03:19:20", "content": "DeepDanbooru (girl -----> image !!!  classification) source code is available on github now. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/edzpsl/deepdanbooru_girl_image_classification_source/"}, {"autor": "___mlm___", "date": "2019-02-13 13:24:44", "content": "[P] StyleGAN Encoder - from real images to latent representation /!/ I made a implementation of encoder for StyleGAN which can transform a real -----> image !!!  to latent representation of generator. Then this representation can be moved along some direction in latent space, e.g. \"smiling direction\" and transformed back into images by generator.\n\nLink: https://github.com/Puzer/stylegan\n\nNotebook with more examples https://github.com/Puzer/stylegan/blob/master/Play_with_latent_directions.ipynb", "link": "https://www.reddit.com/r/MachineLearning/comments/aq6jxf/p_stylegan_encoder_from_real_images_to_latent/"}, {"autor": "CrazyCar09", "date": "2019-02-13 03:17:21", "content": "Did I explain Deep Learning here correctly? /!/ I am typing a research paper for my high school lit class. i am trying to be thorough in my explanations but I feel like it is still really complicated to understand. I also might have misrepresented how to weight in layers works.\n\n&amp;#x200B;\n\nPARAGRAPH: \n\n&amp;#x200B;\n\n \n\nJeff Dean, a PhD graduate from University of Washington, draws a -----> picture !!!  of deep learning, stating, \u201cWhen you hear the term deep learning, just think of a large deep neural net. Deep refers to the number of layers typically and so this is kind of the popular term that\u2019s been adopted in the press. I think of them as deep neural networks generally\u201d. Neural net is short for the term \u201cneural network, a computer diagram that allows for deep learning to take place. This diagram is inspired by a human brain. At the start of the network is an input. This input leads into a set of layers called \u201chidden layers.\u201d These layers are where the change occurs. From the hidden layer, an output is produced. This all functions like a human brain. A human brain is composed of neurons, which are composed of dendrites and one axon. When one dendrite is prompted, it will trigger the axon. This axon might trigger another separate neuron and this continues until an output is given (\u201cBrownlee\u201d). Using advanced algorithms, as well as the chain rule, computers can create their own outputs. These algorithms give each input a weight; generally this weight is around 1 for small networks, a large network might be given a number in the 100\u2019s to produce a more accurate output. Once the input leaves, it is transferred to a hidden layer. Based on the programs algorithm and function, this newly arrived input will be given a new weight, if it is very close to the desired output then the weight remains close to the original weight. The farther from the output, the lower the weight. This process repeats for the amount of hidden layers there are (\u201cCosmos\u201d). After it gets to the final stage of hidden layers, the highest number is chosen to be the output. This process leads into the first implementations of deep learning.", "link": "https://www.reddit.com/r/MachineLearning/comments/aq1zlz/did_i_explain_deep_learning_here_correctly/"}, {"autor": "joiemoie", "date": "2019-02-13 00:06:43", "content": "Developing -----> image !!!  memory using unsupervised learning? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/aq07et/developing_image_memory_using_unsupervised/"}, {"autor": "YourWelcomeOrMine", "date": "2019-02-12 14:22:26", "content": "[D] Simple and robust automatic -----> image !!!  caption generation? /!/ I would like to use different word embeddings (ELMo, BERT, etc.) for image caption generation. However, I'm more concerned with the structure of the language it produces, rather than how accurate the caption is. As such, I was going to test it with very simple images that are easy for the caption generation system. Does some sort of baseline like this exist?", "link": "https://www.reddit.com/r/MachineLearning/comments/aptt4x/d_simple_and_robust_automatic_image_caption/"}, {"autor": "YourWelcomeOrMine", "date": "2019-02-12 14:07:13", "content": "Simple and robust automatic -----> image !!!  caption generation? /!/ I would like to use different word embeddings (ELMo, BERT, etc.) for image caption generation. However, I'm more concerned with the structure of the language it produces, rather than how accurate the caption is. As such, I was going to test it with very simple images that are easy for the caption generation system. Does some sort of baseline like this exist?", "link": "https://www.reddit.com/r/MachineLearning/comments/apto23/simple_and_robust_automatic_image_caption/"}, {"autor": "WillingCucumber", "date": "2019-02-12 07:23:19", "content": "[D] Style GAN - Generating styles which are correlated over different scales /!/ I have a doubt regarding recent paper: StyleGAN. The paper uses different mean/variance normalization parameters at different layers. I am confused that when we normalize an intermediate layer, it will change the mean and variance of the input to 0,1 respectively  and then adjust the feature map using gamma and beta parameters.. Thus, when different styles are used in a coarse to fine model, won't the model forget the style at higher level and  style of the generated -----> image !!!   would only posses  style of the last layer features only ??\n\nPlease help me resolve my doubt. Thanks !!", "link": "https://www.reddit.com/r/MachineLearning/comments/apqs8i/d_style_gan_generating_styles_which_are/"}, {"autor": "wronk17", "date": "2019-07-18 17:02:49", "content": "[P] fastai-Serving: running containerized inference with fastai models /!/ Code: [fastai-serving repo](https://github.com/developmentseed/fastai-serving)\n\n&amp;#x200B;\n\nWe've been experimenting with some Fast AI models recently for our remote sensing work. Unfortunately, we ran into a lot of issues when trying to deploy those models on large-scale inference jobs (specifically running land-classification on big satellite imagery datasets). This [fastai-serving](https://github.com/developmentseed/fastai-serving) repo is meant to solve this in a way that mimics the [TF Serving](https://www.tensorflow.org/tfx/guide/serving) approach/API. Namely, it helps you package a trained model within a small Docker -----> image !!!  (running a mini server) so you can make prediction requests via REST POST requests.\n\n&amp;#x200B;\n\nWe're working on expanding the functionality (and are very receptive to any help!). For anyone who's running inference on large image sets, we usually spin up multiple of these these inference-ready images and run large batch predictions with our open [chip-n-scale](https://github.com/developmentseed/chip-n-scale-queue-arranger) pipeline.", "link": "https://www.reddit.com/r/MachineLearning/comments/cev5uw/p_fastaiserving_running_containerized_inference/"}, {"autor": "makereven", "date": "2019-07-18 08:24:11", "content": "[P] Google coral board -----> image !!!  classification and transfer learning demos with Captain America and BB-8 /!/ Google coral board image classification and transfer learning demos with Captain America and BB-8\n\n&amp;#x200B;\n\n![video](y6j1dde2u0b31)\n\n&amp;#x200B;\n\ni got coral hardware(dev board and USB accelerator) from [this website](https://store.gravitylink.com), strongly recommended.", "link": "https://www.reddit.com/r/MachineLearning/comments/cepqc5/p_google_coral_board_image_classification_and/"}, {"autor": "neural_kusp_machine", "date": "2019-03-02 01:07:07", "content": "[R] Learning Implicitly Recurrent CNNs Through Parameter Sharing (ICLR'19) + Code /!/ Paper: [https://arxiv.org/abs/1902.09701](https://arxiv.org/abs/1902.09701)\n\nCode: [https://github.com/lolemacs/soft-sharing](https://github.com/lolemacs/soft-sharing)\n\n&amp;#x200B;\n\nAbstract:\n\nWe introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates. Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks. Compared to traditional CNNs, we demonstrate substantial parameter savings on standard -----> image !!!  classification tasks, while maintaining accuracy.   \nOur simple parameter sharing scheme, though defined via soft weights, in practice often yields trained networks with near strict recurrent structure; with negligible side effects, they convert into networks with actual loops. Training these networks thus implicitly involves discovery of suitable recurrent architectures. Though considering only the design aspect of recurrent links, our trained networks achieve accuracy competitive with those built using state-of-the-art neural architecture search (NAS) procedures.   \nOur hybridization of recurrent and convolutional networks may also represent a beneficial architectural bias. Specifically, on synthetic tasks which are algorithmic in nature, our hybrid networks both train faster and extrapolate better to test examples outside the span of the training set.", "link": "https://www.reddit.com/r/MachineLearning/comments/awc3ri/r_learning_implicitly_recurrent_cnns_through/"}, {"autor": "Jandevries101", "date": "2019-03-01 19:35:17", "content": "How do i apply Batch Normalization to my Network? /!/ Hi everyone,\n\n&amp;#x200B;\n\n**Intro**\n\n&amp;#x200B;\n\nSo i am working on my DDPG (RL) Algorithme (Discrete) and i came accros a lot of saturation/exploding gradients, i was able to solve it sometimes while tweaking parameters, but on the long run it usually resolves still into saturation. Also notice able about my network is that it has three actions, but it usually only works with 2 actions, i've heard this could be common, but he **never** uses the third action, it's almost random every run, so i can't imagne it has something to do with the action specific. \n\n&amp;#x200B;\n\n**Environment:** \n\n&amp;#x200B;\n\nThe state is fairly \"simple\", it has a lot of similarities with the frogger game, except he has to carry a package. The main rewards that can be fetched from carrying the package accros the street, if dropped incorrect he gets a punishment. \n\n&amp;#x200B;\n\nThe reward scheme that i am currently using is as follows:\n\n&amp;#x200B;\n\n    --------------------------------------------------\n    \n    Do Nothing           =  0\n    Carry Package        =  0.01\n    Drop Fail            = -0.5\n    Drop Goal            =  0.5\n    StepLimit            = -1 (50 Steps)\n    Pick-Up Package      = 0\n        \n    --------------------------------------------------\n\n&amp;#x200B;\n\n**Network** \n\n&amp;#x200B;\n\nUnder here you can see the Actor Critic network that i am using right now, my state-space is 6 and is normalized and i used to have 64-33-3 for the neurons per layer in the actor, but i was testing doubling it, so don't keep the number of neurons too much in count.\n\n&amp;#x200B;\n\nParameters:\n\n    MAX_EPISODES = 200\n    MAX_EP_STEPS = 200\n    LR_A = 0.0001    # learning rate for actor\n    LR_C = 0.0002    # learning rate for critic\n    GAMMA = 0.9     # reward discount\n    TAU = 0.01      # soft replacement\n    MEMORY_CAPACITY = 1000 #usually use 500, but i was testing, so i doubled it\n    BATCH_SIZE = 32\n\nActor:\n\n        def _build_a(self, s, reuse=None, custom_getter=None):\n            trainable = True if reuse is None else False\n            with tf.variable_scope('Actor', reuse=reuse, custom_getter=custom_getter):\n                net = tf.layers.dense(s, 140, activation=tf.nn.leaky_relu, name='l1', trainable=trainable)\t\t\t\t\t\n                mid = tf.layers.dense(net, 70, activation=tf.nn.tanh, name='l2', trainable=trainable)\n                a = tf.layers.dense(mid, 3, activation=tf.nn.softmax, name='la', trainable=trainable)\n                return a\n\nCritic:\n\n        def _build_c(self, s, a, reuse=None, custom_getter=None):\n            trainable = True if reuse is None else False\n            with tf.variable_scope('Critic', reuse=reuse, custom_getter=custom_getter):\n                n_l1 = s.shape[1]\n                w1_s = tf.get_variable('w1_s', [s.get_shape()[1], n_l1], trainable=trainable)\n                w1_a = tf.get_variable('w1_a', [3, n_l1], trainable=trainable)\n                b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)\n                net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n                return tf.layers.dense(net, 1, trainable=trainable)  # Q(s,a)  \n\n&amp;#x200B;\n\n**Batch Normalization?**\n\n&amp;#x200B;\n\nSo i got reccomended to use Batch Normalization, however i do get what it does, but i don't know if this will indeed solve my 2 problems of saturation and the softmax only using 2 actions instead of making use of the three actions. A results graph of action probabilities for the three actions, however i only did a thousand steps in this run, due too time issues.\n\n&amp;#x200B;\n\n[https://cdn.discordapp.com/attachments/491949654666248213/550428445386080256/unknown.png](https://cdn.discordapp.com/attachments/491949654666248213/550428445386080256/unknown.png)\n\n&amp;#x200B;\n\nIt's only to give a -----> image !!!  of how it looks sometimes. For the rest is my td-error fairly normal i suppose:\n\n&amp;#x200B;\n\n[https://cdn.discordapp.com/attachments/491949654666248213/550713201650565128/unknown.png](https://cdn.discordapp.com/attachments/491949654666248213/550713201650565128/unknown.png)\n\n&amp;#x200B;\n\nSo i guess i wanna use batch normalization, since i don't think gradient clipping will help.... but i have no idea how batch normalization can be implemented in my algorithme.... I know i have to define some things in my init, my learn function and of course my network function, but i don't get what and how, besides i am not even sure on which layers i should apply it, i suppose it should be on the leaky\\_relu? And my last question is how long does it take on average for batch normalization to be \"sane\" ?\n\n&amp;#x200B;\n\n**Outro**\n\n&amp;#x200B;\n\nThanks for answering my question and reading, i hope i explained everything well enough and maybe you know the answer to my questions and maybe even have diffrent suggestions to approach this problem. I am curious to know!\n\n&amp;#x200B;\n\nHave a nice day further,\n\n&amp;#x200B;\n\nJan\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/aw8rdh/how_do_i_apply_batch_normalization_to_my_network/"}, {"autor": "sdhnshu", "date": "2019-03-01 18:40:13", "content": "-----> Image !!!  classifier webapp in React serving from a CNN in Pytorch /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/aw85oo/image_classifier_webapp_in_react_serving_from_a/"}, {"autor": "Scutterbum", "date": "2019-03-01 16:52:48", "content": "How to add gender, age etc. to an -----> image !!!  array for neural networks? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/aw72hn/how_to_add_gender_age_etc_to_an_image_array_for/"}, {"autor": "Lord_Fixer", "date": "2019-03-01 06:33:23", "content": "[P] Real time face landmarking using decision trees and NN autoencoders. /!/ Repo: [https://github.com/TomaszRewak/Face-Landmarking](https://github.com/TomaszRewak/Face-Landmarking)\n\nLonger description: [https://blog.tomasz-rewak.com/face-landmarking/](https://blog.tomasz-rewak.com/face-landmarking/)\n\nIn this project I use simple decision trees and a NN autoencoder to map 194 key face points on a video input in real time.\n\nThe algorithms performs 45 iterations each frame. During a single iteration it adjust the position of each point independently (using DT, based on some local features of the -----> image !!! ). As points during this process tend to go in slightly random directions, it's essential to fix the mask afterwards. I use a NN autoencoder with 2 hidden layers for this.\n\nThe learning process was based on the HELEN dataset.", "link": "https://www.reddit.com/r/MachineLearning/comments/aw1nil/p_real_time_face_landmarking_using_decision_trees/"}, {"autor": "invertedpassion", "date": "2019-03-01 05:37:24", "content": "[P] Search images by describing them /!/ Hello,\n\nI trained a network that generates captions for images and ran it in reverse to find images given a caption. **Here's my writeup:** [One neural network, many\u00a0uses](https://towardsdatascience.com/one-neural-network-many-uses------> image !!! -captioning------> image !!! -search-similar------> image !!! -and-words-in-one-model-1e22080ce73d)\n\n**Some examples** (note that the text doesn't exist in dataset, it's user supplied)\n\nFor '**a boy smiling**', it returns the following as the first result:\n\n&amp;#x200B;\n\nhttps://i.redd.it/cyk495cz1gj21.png\n\nFor '**a man playing basketball**', it returns the following as the second result (the first one is a man playing american football).\n\n&amp;#x200B;\n\nhttps://i.redd.it/4zjcpwtc2gj21.png\n\nHope you like [the project](https://towardsdatascience.com/one-neural-network-many-uses-image-captioning-image-search-similar-image-and-words-in-one-model-1e22080ce73d). While doing the project, I had a ton of fun exploring image and word representations learned by the network while getting trained to generate captions. \n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/aw16a0/p_search_images_by_describing_them/"}, {"autor": "CommunismDoesntWork", "date": "2019-02-28 23:07:52", "content": "Has anyone tested whether or not -----> image !!!  normalization actually helps when training a CNN? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/avxel5/has_anyone_tested_whether_or_not_image/"}, {"autor": "hizpain", "date": "2019-10-31 16:12:08", "content": "Q: Offline Entity Analysis on Android? /!/ Hi! I am building very simplistic Android app and what I am looking for is to analyze searched term (text/string) if its Address/Location/City name, Person name, Animal, Thing or Date/Time info ... just sort of \"basic\" distinction between a few separate categories.  \n\n\nThere is a bunch of tools to -----> image !!!  recognition but a very little Entity Analysis frameworks/models able to run offline (on phone) on simple text data.  \n\n\nAny tips? Any suggestions where/what to look for?", "link": "https://www.reddit.com/r/MachineLearning/comments/dppr7w/q_offline_entity_analysis_on_android/"}, {"autor": "rainmanwy", "date": "2019-10-31 10:06:32", "content": "[P] Milvus: A big leap to scalable AI search engine /!/ # The challenge with data search\n\nThe explosion in unstructured data, such as images, videos, sound records, and text, requires an effective solution for computer vision, voice recognition, and natural language processing. How to extract value from unstructured data poses as a big challenge for many enterprises.\n\nAI, especially deep learning, has been proved as an effective solution. Vectorization of data features enables people to perform content-based search on unstructured data. For example, you can perform content-based -----> image !!!  retrieval, including facial recognition and object detection, etc.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/20lpm6iqouv31.png?width=5148&amp;format=png&amp;auto=webp&amp;s=75051c51002f71687a1ff2eae8f6b8690b2b388e\n\nNow the challenge turns into how to execute effectively search among billions of vectors. That\u2019s what Milvus is designed for.\n\n# What is Milvus?\n\nMilvus is an open source distributed vector search engine that provides state-of-the-art similarity search and analysis of feature vectors and unstructured data. Some of its key features are:\n\n* GPU-accelerated search engine\n\nMilvus is designed for the largest scale of vector index. CPU/GPU heterogeneous computing architecture allows you to process data at a speed 1000 times faster.\n\n* Intelligent index\n\nWith a \u201cDecide Your Own Algorithm\u201d approach, you can embed machine learning and advanced algorithms into Milvus without the headache of complex data engineering or migrating data between disparate systems. Milvus is built on optimized indexing algorithm based on quantization indexing, tree-based and graph indexing methods.\n\n* Strong scalability\n\nThe data is stored and computed on a distributed architecture. This lets you scale data sizes up and down without redesigning the system.\n\n* High compatibility\n\nMilvus is compatible with major AI/ML models and programming languages such as C++, Java and Python.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/2aadp060puv31.png?width=1275&amp;format=png&amp;auto=webp&amp;s=ce1f18df54bba4744f58421efec4c84374d2ea3a\n\n# Billion-Scale similarity search\n\nYou may follow[ this link ](https://github.com/milvus-io/bootcamp/blob/master/EN_docs/labs/lab2_sift1b_100m.md)for step-by-step procedures to carry out performance test on 100 million vector search ([SIFT1B](http://corpus-texmex.irisa.fr/)).\n\nIf you want, you can also try testing 1 billion with Milvus. Here is the [hardware requirements](https://github.com/jielinxu/bootcamp/blob/master/EN_docs/milvus101/hardware_platform.md).\n\n# Join us\n\nMilvus has been open sourced lately. We greatly welcome contributors to join us in reinventing data science!\n\n[Milvus on GitHub](https://github.com/milvus-io/milvus)\n\n[Our Slack channel](https://join.slack.com/t/milvusio/shared_invite/enQtNzY1OTQ0NDI3NjMzLWNmYmM1NmNjOTQ5MGI5NDhhYmRhMGU5M2NhNzhhMDMzY2MzNDdlYjM5ODQ5MmE3ODFlYzU3YjJkNmVlNDQ2ZTk)\n\n&amp;#x200B;\n\nCheck the original article:\n\n[https://medium.com/@milvusio/milvus-a-big-leap-to-scalable-ai-search-engine-e9c5004543f](https://medium.com/@milvusio/milvus-a-big-leap-to-scalable-ai-search-engine-e9c5004543f)", "link": "https://www.reddit.com/r/MachineLearning/comments/dpldzv/p_milvus_a_big_leap_to_scalable_ai_search_engine/"}, {"autor": "that_weird_potato", "date": "2019-10-30 03:12:51", "content": "[D] Art and ML /!/ I am very interested in what is happening at the crossroads between computer science and art, especially algorithmic and ML generated / supported art (music, -----> film !!! , paintings, installations, etc.).\n\nBy this, I don't mean using ML to develop tools that are used in the creative process (improving a 3D modeling software using ML for instance), but using Machine Learning as the creative tool in itself (a standard example being [NextRembrandt](https://www.nextrembrandt.com/)).\n\nMy background is in ML and CS and I have been toying with the idea of pursuing a job / PhD in a related project.\n\nCan anyone point me to any research labs / institutions / projects that they find interesting, fit the description and could maybe accept applications?\n\nI know it is a pretty broad question.  I'm looking for opinions from those of you who already know an interesting project and that maybe hasn't had much public attention yet.\n\nAny help would be great! Thank you.", "link": "https://www.reddit.com/r/MachineLearning/comments/dp1478/d_art_and_ml/"}, {"autor": "am_i_having_fun", "date": "2019-10-30 00:44:21", "content": "[D] Legality of Scraping Training Data from Google Images /!/ I think my original post was removed because I didn't tag it.\n\nI have a  project in mind. I want to build an -----> image !!!  classifier with novel classes. For example, lets say I want to classify images of different types of bicycles. Google images is ripe with these images for each type of bike.\n\nI want to publish a blog post about my project, and put my code (including scraper) on github but not upload the image files anywhere. I might put up a (free) endpoint hosting my resulting classifier if it works.\n\nQuestions:\n\n1. Are all images on google images fair game for training data or do I have to limit it to images \"labelled for reuse\"?\n2. Do I have to cite the images I use as training data?\n3. I've read about \"fair use\", how does that figure in here?\n\nThanks, and sorry if this has been covered elsewhere", "link": "https://www.reddit.com/r/MachineLearning/comments/doz9ze/d_legality_of_scraping_training_data_from_google/"}, {"autor": "amrha", "date": "2019-10-29 21:14:25", "content": "an -----> image !!!  search by content platform using keras and flask /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/dowdu2/an_image_search_by_content_platform_using_keras/"}, {"autor": "supermanstream", "date": "2019-02-21 12:07:26", "content": "[D] Up-to-date deep learning classification courses /!/ Hey, so as a bit of background, I work as a ML engineer in a lab. I work on a project right now which involves binary classification of highly imbalanced data. While I am able to get decent results with RandomForestClassifier, I feel like I can achieve much more with neural networks, simply due to the fact that the dataset is huge (\\~10 mil. rows) and highly dimensional (\\~200 columns). I feel like RandomForestClassifier is not complex enough to \"understand\" the data in the ML way.\n\n&amp;#x200B;\n\nSo, I started to look into neural networks. I feel like I have a solid understanding of them in terms of basics of how they work. However, one thing I don't get about neural networks is what architecture to choose for my project. I was looking for courses online that teach deep learning from scratch, looking to find relevant architecture discussion in them, but the courses mostly describe different applications, such as -----> image !!!  classification, or text mining, and while this is definitely fun, I do not think that it is suitable for me. So, my question is:\n\n&amp;#x200B;\n\nTL,DR: Is there a good up-to-date course that you can advise that goes over using different neural network architectures for classification problems of tabular data?", "link": "https://www.reddit.com/r/MachineLearning/comments/at2erm/d_uptodate_deep_learning_classification_courses/"}, {"autor": "run_youngjoo", "date": "2019-02-21 00:28:46", "content": "[R] SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch and Color /!/ &amp;#x200B;\n\nhttps://i.redd.it/pja1y12wfth21.jpg\n\nSC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch and Color\n\narXiv: [https://arxiv.org/abs/1902.06838](https://arxiv.org/abs/1902.06838)\n\ncode: [https://github.com/JoYoungjoo/SC-FEGAN](https://github.com/JoYoungjoo/SC-FEGAN)\n\n&amp;#x200B;\n\nAbstract\n\nWe present a novel -----> image !!!  editing system that generates -----> image !!! s as the user provides free-form mask, sketch and color as an input. Our system consist of a end-to-end trainable convolutional network. Contrary to the existing methods, our system wholly utilizes free-form user input with color and shape. This allows the system to respond to the user's sketch and color input, using it as a guideline to generate an image. In our particular work, we trained network with additional style loss which made it possible to generate realistic results, despite large portions of the image being removed. Our proposed network architecture SC-FEGAN is well suited to generate high quality synthetic image using intuitive user inputs.", "link": "https://www.reddit.com/r/MachineLearning/comments/asw6nr/r_scfegan_face_editing_generative_adversarial/"}, {"autor": "ramonserrallonga", "date": "2019-02-20 15:24:02", "content": "New Similarity Methods for Unsupervised Machine Learning /!/ **1. Introduction**\n\nData science is changing the rules of the game for decision making. Artificial intelligence is living its golden years where abundance of data, cheap computing capacity, and devoted talent depicts an unstoppable intelligence assisted life for humans. While it is common to hear about AI advice on health or financial investments, the same in business strategy is not so common. Maybe it is just a matter of time that AI learns how to handle data to support decision-making on business strategy, but it could also be that there is a lack of theoretical framework for it to build on. Following the competitive dynamics approach proposed in the article [*Strategizing with Competitive Asymmetry*](https://www.linkedin.com/pulse/strategizing-competitive-asymmetry-ramon-serrallonga/), a quantitative model was built to bridge this gap between business strategy and data science. In this article, I will outline an experiment that compares competitors' data arranged in vectors using this framework. The outcome was an alternative similarity measure, Projection Similarity, as accurate as Cosine Similarity but with asymmetric similarity.\n\n**2. Using Cosine Similarity**\n\nThe competitive dynamics model used in [*Strategizing with Competitive Asymmetry*](https://www.linkedin.com/pulse/strategizing-competitive-asymmetry-ramon-serrallonga/) has two dimensions, Market Commonality and Resource Similarity, and the possible combinations are:\n\n&amp;#x200B;\n\nhttps://i.redd.it/0ohcaebfqqh21.png\n\n*Source: Competitor Analysis and Interfirm Rivalry: Toward a Theoretical Integration,* [*Ming-Jer Chen*](https://www.linkedin.com/in/ming-jer-chen-0577289/)*, Academy of Management Review, 1996, Vol. 21, No. 1, 100-134.*\n\nUnder this approach, companies were characterized with one vector for each dimension including several determinant traits of their markets and their resources. Cosine Similarity was initially used to compare the vectors pairwise, but two problems arose.\n\nFirst, Cosine Similarity is symmetric. The similarity of vector A with respect to vector B is the same as the one of vector B with respect of vector A. Cosine Similarity fails to represent competitive asymmetry.\n\nSecond, the similarities were very high. In a two-by-two matrix like -----> Image !!!  1 above, the intuitive threshold to classify a data point as high or low is 50%. Above 50% there are more odds that the two data points compared are similar than they are not. They are classified as \"high\". And vice versa - low if below 50%. With Cosine Similarity even companies radically different had similarities above 50%. If there is a training data set to find which is the optimal threshold, rather than at 50%, this problem is solvable. In this case falls Market Commonality where the industry and the countries where a company operates are known. But, for unsupervised classification, the fact that the optimal threshold falls at the intuitive 50% has a significant impact on the accuracy of the classification. This is the case of Resource Similarity where the skills of a company are neither easily nor publicly known.\n\n**3. Using Projection Similarity**\n\nAn alternative method to compare the vectors was used in order to have asymmetric similarity. The projection similarity of vector A in relation to vector B was calculated as follows:\n\n1. Calculate the orthogonal projection of vector A over vector B\n\n2. Divide the norm of the orthogonal projection by the norm of B, which will give the relative value of the norm of the orthogonal projection in relation to B\n\n3. Subtract 1 to the resulting value of step 2 and take the absolute value (to take advantage of the symmetric distributions)\n\n&amp;#x200B;\n\nhttps://i.redd.it/l0nkwp9iqqh21.png\n\nThis difference statistic can be used as Z value of a standard normal distribution to get the Standard Projection Similarity by multiplying the area of the cumulative distribution function from -\u221e to -Z by 2:\n\n&amp;#x200B;\n\nhttps://i.redd.it/q22udbhlqqh21.png\n\nIt could also be used as the exponent of a logistic function to get the Logistic Projection Similarity:\n\n&amp;#x200B;\n\nhttps://i.redd.it/nnp5mzsmqqh21.png\n\nIn the next section, we will examine the validity and accuracy of this alternative method.\n\n**4. Calculations**\n\nAt this phase, 5 different measures were calculated: Cosine Similarity, Standard Projection Similarity, Logistic Projection Similarity, Cosine Similarity multiplied by Standard Projection Similarity, and Cosine Similarity multiplied by Logistic Projection Similarity. Each dimension had its unique data set, one for Market Commonality and one for Resource Similarity. The criterion to decide if two companies were similar was set by industry: if two companies are in the same industry, their similarity should be high in any of the dimensions; otherwise, low. The positive outcomes for both data sets were 32%, and the negative outcomes 68%. The chosen criterion carried the implicit assumption that companies in the same industry can differ but not a lot, either in one dimension or the other one. The performance of each method as a function of the threshold value were the following:\n\n**Market Commonality**\n\n&amp;#x200B;\n\nhttps://i.redd.it/9gdiqp7pqqh21.png\n\n&amp;#x200B;\n\nhttps://i.redd.it/vwcxv5hqqqh21.png\n\n&amp;#x200B;\n\nhttps://i.redd.it/1e62dx1sqqh21.png\n\n**Resource Similarity**\n\n&amp;#x200B;\n\nhttps://i.redd.it/znalaztuqqh21.png\n\n*Processing img ycl7zvsuqqh21...*\n\n*Processing img o1n7y0tuqqh21...*\n\n&amp;#x200B;\n\n*Processing img 1ekjzr2yqqh21...*\n\n**5. Conclusions**\n\nThe two challenges of using Cosine Similarity were the presence of symmetric similarity and the optimal threshold value far from the intuitive 50%.\n\nFor the former, the experiment showed that the method of Cosine Similarity multiplied by Logistic Projection Similarity can successfully deliver the best asymmetric similarity with a high accuracy at par with Cosine Similarity.\n\nFor the latter, the optimal threshold of Cosine \u00b7 Logistic (60%) was 5% below the Cosine one (65%) for Market Commonality, and 10% below for Resource Similarity (75% and 85% respectively). But those values were still far from 50%. So, even if there was an improvement, the challenge for unsupervised classification remained.\n\nFollowing closely, the suboptimal Cosine \u00b7 Standard method delivered a pair of optimal thresholds of 55% for Market Commonality and 70% for Resource Similarity. Even a bit better.", "link": "https://www.reddit.com/r/MachineLearning/comments/aspjvx/new_similarity_methods_for_unsupervised_machine/"}, {"autor": "cloudares", "date": "2019-02-20 10:37:08", "content": "[P] Extracting text from different areas on label /!/ Hi, I'm trying to achieve algorithm that will extract text from few areas(marked with red color) on label(similar to attached -----> image !!! ) on a single photo taken with mobile camera so label may be in a little bit different position. I'm struggling with choosing and executing good enough solution to accomplish it. I was wondering about labeling images with marked boxes around my area of intersts and then put it into some CNN. I've encounter some interesting paper on text spotting([https://arxiv.org/pdf/1810.12738v1.pdf](https://arxiv.org/pdf/1810.12738v1.pdf)) but I'm not sure if it would be good solution for my problem.\n\nI will be more than thankful for any help and suggestions. Thanks in advance :)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://i.redd.it/lq8f05qqbph21.png", "link": "https://www.reddit.com/r/MachineLearning/comments/asmv8n/p_extracting_text_from_different_areas_on_label/"}, {"autor": "Kriegher2005", "date": "2019-02-20 07:47:50", "content": "[D] Using Visual and Audio Data for training /!/ I want to train a ml algorithm using data from my -----> camera !!!  and microphone. The data will be occurring simultaneously.\n\n-\n\nIn a way I want my data to hear a sound and make a visual representation of the thing causing the sound and to give a sound after getting the image of an object.", "link": "https://www.reddit.com/r/MachineLearning/comments/aslm9m/d_using_visual_and_audio_data_for_training/"}, {"autor": "run_youngjoo", "date": "2019-02-20 05:29:01", "content": "[R] SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch and Color /!/ &amp;#x200B;\n\nhttps://i.redd.it/6f9fyvabonh21.jpg\n\n&amp;#x200B;\n\n**SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch and Color**\n\narXiv: [https://arxiv.org/abs/1902.06838](https://arxiv.org/abs/1902.06838)\n\ngithub: [https://github.com/JoYoungjoo/SC-FEGAN](https://github.com/JoYoungjoo/SC-FEGAN)\n\nAbstract\n\nWe present a novel -----> image !!!  editing system that generates -----> image !!! s as the user provides free-form mask, sketch and color as an input. Our system consist of a end-to-end trainable convolutional network. Contrary to the existing methods, our system wholly utilizes free-form user input with color and shape. This allows the system to respond to the user's sketch and color input, using it as a guideline to generate an image. In our particular work, we trained network with additional style loss which made it possible to generate realistic results, despite large portions of the image being removed. Our proposed network architecture SC-FEGAN is well suited to generate high quality synthetic image using intuitive user inputs.", "link": "https://www.reddit.com/r/MachineLearning/comments/askgkc/r_scfegan_face_editing_generative_adversarial/"}, {"autor": "cloudares", "date": "2019-02-19 20:45:15", "content": "Extracting text from few areas on product label /!/ Hi, I'm trying to achieve algorithm that will extract text from few areas(marked with red color) on label(similar to attached -----> image !!! ) and QR code on a single photo.  I'm struggling with choosing and executing good enough solution to accomplish it. I was wondering about labeling images with marked boxes around my area of intersts and then put it into some CNN.  I've encounter some interesting paper on text spotting([https://arxiv.org/pdf/1810.12738v1.pdf](https://arxiv.org/pdf/1810.12738v1.pdf)) but I'm not sure if it would be good solution for my problem. \n\nI will be more than thankful for any help and suggestions. Thanks in advance :)", "link": "https://www.reddit.com/r/MachineLearning/comments/asf65r/extracting_text_from_few_areas_on_product_label/"}, {"autor": "YuansongFeng", "date": "2019-08-23 08:05:28", "content": "-----> Image !!!  captioning model refuses to look at image features /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/cuate5/image_captioning_model_refuses_to_look_at_image/"}, {"autor": "blakerazor", "date": "2019-10-17 16:14:02", "content": "Audio Captcha Solver - Machine Learning /!/ Hi,\n\nI am building an ML model to solve an audio captcha, but then I am not very successful with it, can someone tell me what I have done wrong.  \n\n\nPFA the steps I have done.  \n\n\nI have a fixed-length (length = 8) audio captcha files generated using gTTS, I am using keras to train the model. And most importantly I have converted the audio files into Spectrogram using \"librosa\" and I am using these images to train my model.\n\n(SymbolSet: 0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ (basically all chars in caps and numbers))  \nThis is my final -----> image !!!  which I generate after converting the audio to spectrogram:  \n\n\n[00B212DH](https://i.redd.it/bae2fuqak4t31.jpg)\n\nit's of size 128 x 64, I am NOT doing any preprocessing on this -----> image !!!  or the audio apart from converting it to this -----> image !!! . (Should I be doing something? If so some pointers towards that would help greatly).  \n\n\nThis is my Keras model.\n\n    def create_model(captcha_length, captcha_num_symbols, input_shape, model_depth=5, module_size=2):\n     input_tensor = keras.Input(input_shape)\n     x = input_tensor\n     for i, module_length in enumerate([module_size] * model_depth):\n        for j in range(module_length):\n            x = keras.layers.Conv2D(32 * 2 ** min(i, 3), kernel_size=3, padding='same', kernel_initializer='he_uniform')(x)\n            x = keras.layers.BatchNormalization()(x)\n            x = keras.layers.Activation('relu')(x)\n        x = keras.layers.MaxPooling2D(2)(x)\n     x = keras.layers.Dropout(0.2)(x)\n     x = keras.layers.Flatten()(x)\n     x = [keras.layers.Dense(captcha_num_symbols, activation='softmax',\n name='char_%d' % (i + 1))(x) for i in range(captcha_length)]\n     model = keras.Model(inputs=input_tensor, outputs=x)\n    \n return model\n\nShould I be improving this? Should I be processing Audio in this way by converting it to a spectrogram. My training set is around 40000 audio captchas, validation is around 8000 audio captchas, I ran it with a batch size of 4, for 6 epochs, and my accuracy is very bad.  \n\n\nAny suggestions on improving this would be of great help!  \nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/dj8lwm/audio_captcha_solver_machine_learning/"}, {"autor": "fan_rma", "date": "2019-10-17 15:40:18", "content": "AdaptIS: New Instance Segmentation Method From Samsung AI /!/ GitHub: [https://github.com/saic-vul/adaptis](https://github.com/saic-vul/adaptis)\n\narxiv: [https://arxiv.org/pdf/1909.07829.pdf](https://arxiv.org/pdf/1909.07829.pdf)\n\nSamsung AI has released their new Instance segmentation model that is based on the Convolutional Neural Networks and a mechanism what they call as an \"AdaIN mechanism\" (Adaptive Instance Normalization). Their architecture uses three subnetworks that perform the tasks together to predict the segmentation masks when given the training data.\n\nThe first network is a feature extractor network which is a pre-trained backbone.\n\nThe second network is a controller network that encodes the feature at the region of interest and the output of this network is passed as input to the AdaIN mechanism.\n\nThe third network is a Relative Coordinated Convolutional Block that differentiates similar objects at different regions in an -----> image !!! .", "link": "https://www.reddit.com/r/MachineLearning/comments/dj84yr/adaptis_new_instance_segmentation_method_from/"}, {"autor": "Drimage", "date": "2019-10-17 14:01:03", "content": "[D] Looking for a specific figure demonstrating the importance of good datasets /!/ I'm looking for a figure to cite in one of my projects. I have seen it once before, but didn't save the source, and have no luck finding it again. The effect of the figure is to show that on average, successful theories were invented very early, but good results only follow the release of good datasets.\n\nThis is achieved by listing a number of tasks (e.g. -----> image !!!  classification). For each task, the figure lists which technique has been used to successfully tackle the problem (e.g. CNNs) and the year it was first proposed. Additionally, it lists the year that a significant dataset for this problem (e.g. ImageNet) was released. Finally, the last column displays the year that some performance threshold was reached on the given technique.\n\nI hope my description is clear. It would be great if someone could find the actual figure!\n\nIn general, do you think the claim made here is valid? Or is it simplistically aggregating too much information, and missing the point?", "link": "https://www.reddit.com/r/MachineLearning/comments/dj6sw4/d_looking_for_a_specific_figure_demonstrating_the/"}, {"autor": "atlphonehome", "date": "2019-10-16 21:08:18", "content": "create -----> image !!!  dataset from indoor security cam? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/divu2u/create_image_dataset_from_indoor_security_cam/"}, {"autor": "posnererez", "date": "2019-10-16 19:16:53", "content": "[R] - HAMR \u2014 3D Hand Shape and Pose Estimation from a Single RGB -----> Image !!! ", "link": "https://www.reddit.com/r/MachineLearning/comments/diu73z/r_hamr_3d_hand_shape_and_pose_estimation_from_a/"}, {"autor": "lostmsu", "date": "2019-03-09 23:59:51", "content": "[P] Gradient: TensorFlow binding for .NET (C#, F#, VB, etc.) /!/ Quick links: [NuGet](https://www.nuget.org/packages/Gradient/), [Getting Started](https://github.com/losttech/Gradient/#getting-started), [Samples](https://github.com/losttech/Gradient-Samples/), [Landing Page](https://losttech.software/gradient.html)\n\n&amp;#x200B;\n\nMy company recently got a pre-release out for full C#/.NET binding to TensorFlow **Python** API. E.g. not only you can use primitive ops to construct graphs, but tf.keras, tf.data, tf.contrib, others are available too. It also supports TensorBoard integration. Of course, since it is a binding to Python API, you need Python and TensorFlow for Python installed (tensorflow, tensorflow-gpu, or tensorflow-rocm in pip).\n\n&amp;#x200B;\n\nCurrently, the binding **only** supports **1.10.\\*** with **Python 3.6** on Windows, Mac and Linux, but that's mostly because the focus was on getting it up and running as soon as possible. One major feature that is still missing is the ability to inherit from TensorFlow classes (e.g. to create a new type of Keras layer), but it is being actively worked on.\n\n&amp;#x200B;\n\nPlease, read the [Getting Started](https://github.com/losttech/Gradient/#getting-started) before jumping to it to understand quirks, that you might have to deal with due to interfacing with Python. There are some [Samples](https://github.com/losttech/Gradient-Samples/) on GitHub, including Char-RNN (train+sample), GPT-2, -----> image !!!  classification, etc.\n\n&amp;#x200B;\n\nLicense: The company is a very small shop, and not currently ready to open-source the whole thing, as it is our only project. The preview and the final release will be free for non-commercial use, and for small businesses (&lt; $1M/y profit AND &lt;$5M/y revenue). Also it is free for experimentation (e.g. if you try it, but not use any outputs for commercial gain). If you do not fit into one of the options above, but still want to run it, please contact us at: CONTACT --AT-- LOSTTECH.SOFTWARE for licensing and/or to request help with deployment.", "link": "https://www.reddit.com/r/MachineLearning/comments/az9u5c/p_gradient_tensorflow_binding_for_net_c_f_vb_etc/"}, {"autor": "naboo_random", "date": "2019-03-09 01:26:13", "content": "[DL] How to deal with -----> image !!!  data captured from different camera sources /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ayy1xu/dl_how_to_deal_with_image_data_captured_from/"}, {"autor": "cesusjhrist", "date": "2019-03-08 14:52:55", "content": "[D] What are some augmentation method for non-image data? /!/ Hi,\n\nI want to expand the size of my dataset, composed of credit card transaction that has the following features: data, amount, description and category (restaurant, flight etc etc).\n\n&amp;#x200B;\n\nCan you recommend me some data augmentation techniques? All I can find involve -----> image !!!  data. Thanks", "link": "https://www.reddit.com/r/MachineLearning/comments/ayr4d2/d_what_are_some_augmentation_method_for_nonimage/"}, {"autor": "raichet", "date": "2019-01-25 13:47:41", "content": "[D] Research Project Advice Needed! /!/ Hi guys, I need some advices for my school research project. I cannot post the actual data or talk about the project detail itself, so I will be modifying the project context a bit. Please bear with me!\n\nI am given images similar to [this](https://pixabay.com/en/eucalyptus-flower-buds-buds-blossom-1234009/), except there will be multiple stems with their respective group of flower buds. **My task is to segment the -----> image !!!  into individual components (stems), and measure the number of flower buds belonging to each component, and the length and diameter of each flower bud on a component.** Each picture is guaranteed to have stems. Most stems will have many flower buds on them, but some stems may not have any (highly unlikely). There will likely be a LOT of measurements per image.\n\n\nMy questions:\n\n1. I would like to have some mental clarity on the project's pipeline. Since each picture is guaranteed to have stems, I am thinking instance segmentation method to segment the picture into individual components consisting of a stem and its flower buds. Then for each component, since some component may have no flower buds, I would use object detection methods to draw a bounding box around each bud, then use semantic/instance segmentation to segment each bud from its box. Is this a correct way to be tackling the segmentation portion of the project?\n\n2. What type of task would measuring length/diameter/angle of each bud be?\n\n3. What is the best way to label the given pictures? The dataset is pretty small, which will pose a challenge.  \n\nI am brainstorming ideas, and trying to come up with a clear mental picture of exactly what needs to be done. Thank you for reading through this long post. Any input will be appreciated!\n\n\n", "link": "https://www.reddit.com/r/MachineLearning/comments/ajp6db/d_research_project_advice_needed/"}, {"autor": "kh22l22", "date": "2019-01-25 10:01:53", "content": "[R] Unsupervised -----> Image !!! -to------> Image !!!  Translation with Self-Attention Networks", "link": "https://www.reddit.com/r/MachineLearning/comments/ajnldo/r_unsupervised_imagetoimage_translation_with/"}, {"autor": "KoltBruh", "date": "2019-01-25 01:11:04", "content": "[D] Finding static -----> image !!! s inside of a larger -----> image !!!  /!/ Hello! To elaborate a bit more on the title, I'm attempting to find a way to detect if an image exists inside of another image. I will give an example to be more specific: The goal of my work is to create a system where the player of a card game (for example Hearthstone) can look at a single screen of their collection and be notified of all of the cards that are currently visible on the screen. Note: I'm not actually using Hearthstone, so I can't just tap into the available local game data.  I figured I'd post here as I've tried just about everything that I can think of with no success. The things that I have tried are listed below.\n\n* Looking for relevant log files or local game data didn't reveal anything useful.\n* I attempted to use the Tesseract library to use OCR to find the names of the cards. I think that the fancy fonts made this a bad option.\n* I tried to use image diff tools like pixelmatch, resemble.js, and blink-diff. These seem to require both images to be the same size, so this didn't work out either.\n* I tried to use template matching using OpenCV to check if a match could be found, but the confidence scores between cards that weren't on the screen and cards that were were either way too close or even flipped backwards, so I abandoned that. This was after attempting to crop the card images enough to where all of the not-unique parts of the cards (ex. the borders and number values were removed), as well as attempting to apply edge detection filters on the images to simplify them.\n\nMy current thought process is that I would need to do some sort of feature extraction or object extraction to do this. I've considered researching how to make a training dataset based on the card images that I have, but that seems like a bit overkill as I'm only really looking for one static, unchanging image every time. I've seen that projects such as \\[card\\_scan\\]([https://github.com/YenTheFirst/card\\_scan](https://github.com/YenTheFirst/card_scan)) exist, but even those seem more complicated than what I'm attempting to do. Any help or discussion is greatly appreciated. Thank you again!", "link": "https://www.reddit.com/r/MachineLearning/comments/ajjnen/d_finding_static_images_inside_of_a_larger_image/"}, {"autor": "Felix_Tholomyes", "date": "2019-01-24 20:17:07", "content": "[N] The latest version of AlphaStar loses to Liquid MaNa /!/ The newest version of AlphaStar has to control the -----> camera !!!  as well, MaNa just beat it live after losing to the previous version 0-5", "link": "https://www.reddit.com/r/MachineLearning/comments/ajgjto/n_the_latest_version_of_alphastar_loses_to_liquid/"}, {"autor": "Marthinwurer", "date": "2019-01-24 00:39:28", "content": "[D] Interview questions for a NLP job? /!/ Tomorrow I'm going to be interviewing candidates for a job involving ML and NLP. I didn't know I was doing this until today. I have a bit of a background in ML from lurking here, but I've only really done stuff with RL and -----> image !!!  classifiers. What are some good interview questions to ask or resources that I can read to get up to speed so that I can make sure that my candidates know what they're doing?", "link": "https://www.reddit.com/r/MachineLearning/comments/aj6oqp/d_interview_questions_for_a_nlp_job/"}, {"autor": "saravanakumar17", "date": "2019-11-18 14:26:08", "content": "How to extract date from an receipt -----> image !!!  /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/dy3u8d/how_to_extract_date_from_an_receipt_image/"}, {"autor": "downtownslim", "date": "2019-11-17 21:08:23", "content": "[N] Microsoft Incorporates Graphcore AI Chips in Azure Cloud /!/ Graphcore\u2019s AI accelerator chip, the Colossus intelligence processing unit (IPU) is now available for customers to use as part of Microsoft\u2019s Azure cloud platform.\n\nThis is the first time any major cloud service provider has publicly offered customers the opportunity to run their data on an accelerator from any of the dozens of AI chip startups and as such, it represents a big win for Graphcore. Microsoft has said access will initially be prioritised for customers who are \u201cpushing the boundaries of machine learning\u201d.\n\nMicrosoft and Graphcore have been working together for two years to develop cloud systems and build enhanced vision and natural language processing models for the Graphcore IPU. In particular, the natural language processing (NLP) model, Google\u2019s BERT (bidirectional encoder representations from transformers), which is currently very popular with search engines, including Google themselves.\n\nUsing eight Graphcore IPU processor cards (each with a pair of Colossus accelerators), BERT can be trained in 56 hours, similar to the result for GPU with PyTorch, though it is faster than the GPU with TensorFlow (see graph below). Graphcore says customers are seeing BERT inference throughput increase threefold, with 20% improvement in latency.\n\nGiven the level of hype surrounding Graphcore\u00a0\u2014 the company is valued at $1.7 billion \u2014 these performance improvements seem rather modest. It remains to be seen whether the promised improvement is enough to tempt customers into optimising their models for the IPU.\n\n**Advanced models**  \nAt the same time, Graphcore has also released some results on more advanced models, where it showed more dramatic performance improvements.\n\nInference on -----> image !!!  processing model ResNext was accelerated 3.4x in terms of throughput at 18x lower latency, compared to a GPU solution consuming the same amount of power. ResNext uses a technique called group separable convolutions, which splits convolution filters into smaller separable blocks to increase accuracy while reducing the parameter count. This approach is well-suited to the IPU, Graphcore says, because of the chip\u2019s massively parallel processor architecture and more flexible, high-throughput memory; smaller blocks of data can be mapped to thousands of fully independent processing threads.\n\nGraphcore also showed good results for Markov Chain Monte Carlo (MCMC)-based models, a new type of probabilistic algorithm which is used for modelling financial markets. This type of model has been out of reach for many in the finance industry, as it was previously considered too computationally expensive to use, said Graphcore. Early access IPU customers in the finance sector have been able to train their proprietary, optimised MCMC models in 4.5 minutes on IPUs, compared to over 2 hours with their existing hardware, a 26x speed up in training time.\n\nReinforcement learning (RL), another popular technique in modern AI algorithm development, can also be accelerated compared to typical existing solutions. Graphcore cited a factor of ten improvement in throughput for RL models, even before they are optimised for the IPU.\n\n&amp;#x200B;\n\n[https://www.eetimes.com/document.asp?doc\\_id=1335297#](https://www.eetimes.com/document.asp?doc_id=1335297#)", "link": "https://www.reddit.com/r/MachineLearning/comments/dxslwq/n_microsoft_incorporates_graphcore_ai_chips_in/"}, {"autor": "eigenlaplace", "date": "2019-11-17 16:58:30", "content": "[D] NeurIPS proceedings - what are is the -----> image !!!  reuse copyright license? /!/ I'm writing my MSc thesis and would like to use a couple of images from a NeurIPS paper. What should I follow to avoid copyright issues? I can't find any licensing information on their website", "link": "https://www.reddit.com/r/MachineLearning/comments/dxp1be/d_neurips_proceedings_what_are_is_the_image_reuse/"}, {"autor": "ssd123456789", "date": "2019-11-10 12:40:34", "content": "[D] SSH and using a remote server /!/ My last post asked for help with SSH and training a model on a remote server. \nIn this post can I just get a confirmation for what I think I need to do?\n1) build docker -----> image !!!  for project \n2) transfer -----> image !!!  to server using scp\n3) log in to server \n4) select a machine to compute on\n5) use linux command to run docker -----> image !!! \nDid I get this right? If not, can someone please tell me what I got wrong?\nThank you for your help everyone!", "link": "https://www.reddit.com/r/MachineLearning/comments/duayjy/d_ssh_and_using_a_remote_server/"}, {"autor": "AlanRoofies", "date": "2019-11-09 16:45:19", "content": "Merging 3 samples of the same -----> image !!!  into a single -----> image !!!  using a Neural Network /!/ So, I want to merge multiple images that contain exactly the same scene but with different color filters or different effects, and i was thinking about simply creating a CNN that takes in 3 RGB images of the same size and that contain the same scene (9 channels in total), and that outputs 1 RGB image (3 channels in total) (merge).\n\ntheoretically the neural network would merge each RGB values in a pixel position to give the best outcome. \n\ndo you have any idea if this would work ? do you have any ideas on how to achieve this ? any advice ? any resources i could use ?\n\nPS: this is my first real project with neural nets, all i have been doing until now is following tutorials.", "link": "https://www.reddit.com/r/MachineLearning/comments/dtyce9/merging_3_samples_of_the_same_image_into_a_single/"}, {"autor": "re_gen", "date": "2019-11-09 16:32:29", "content": "[P] A Tool for gAnimating Anime with StyleGAN /!/ I've been working on a multipart project involving a reimplementation of StyleGAN and a research tool to interact with trained StyleGAN models.\n\nHere're some example images/gifs from the project:\n\n* [UI of the tool](https://imgur.com/BJUAeFN)\n* [Spatially isolated animations](https://imgur.com/383nHxe)\n* [Rectangular -----> image !!!  generation with attribute modification](https://imgur.com/lHXcj0X)\n* Automatic [facial feature detection](https://imgur.com/IOlEJNu) and [modification](https://imgur.com/xG9DHBI) without needing labels for the training data:\n* [Unfortunately, somewhat poor-quality male images](https://imgur.com/yJBgHuq)\n\n&amp;#x200B;\n\nI published a couple of blogs that go into more detail.\n\nIn the first blog, I introduce the project and discuss the results, the implementation, and the [training data](https://www.gwern.net/Danbooru2018). I also share the code for the tool and my StyleGAN reimplementation:\n\n[https://towardsdatascience.com/animating-ganime-with-stylegan-part-1-4cf764578e](https://towardsdatascience.com/animating-ganime-with-stylegan-part-1-4cf764578e)\n\nThe second part is a tutorial that demonstrates how to use the tool to animate images and detect facial features. In it, I supply a compiled version of the (Windows) tool that can be used to follow the tutorial:\n\n[https://towardsdatascience.com/animating-ganime-with-stylegan-the-tool-c5a2c31379d?source=friends\\_link&amp;sk=eec12e2da8c84b9736d32f697da21689](https://towardsdatascience.com/animating-ganime-with-stylegan-the-tool-c5a2c31379d?source=friends_link&amp;sk=eec12e2da8c84b9736d32f697da21689)\n\nMy background before ML was reverse engineering, so building a tool that simplified visualizing and interacting with the internal representation of a model felt like an important step to understand it better. There were some results I did not expect, like how modifying a single feature map can consistently make the same meaningful changes across many different images (like opening/closing a mouth). Also, the ability of some feature maps to act as facial feature detectors without training labels made me interested in applying the same approach to other types of generative models.\n\nLet me know if you have any questions/comments/corrections/criticisms or know about similar prior work. As this has been a solo project, I'm pretty starved for outside perspectives.", "link": "https://www.reddit.com/r/MachineLearning/comments/dty6bs/p_a_tool_for_ganimating_anime_with_stylegan/"}, {"autor": "HistoricalTouch0", "date": "2019-11-09 11:53:09", "content": "The visual hull concept for silhouette-based -----> image !!!  understanding /!/ Hi, is anyone familiar with this paper [The visual hull concept for silhouette-based image understanding](https://areeweb.polito.it/ricerca/cgvg/Articles/pami94.pdf) and know what Fig.19 is doing?", "link": "https://www.reddit.com/r/MachineLearning/comments/dtv0kr/the_visual_hull_concept_for_silhouettebased_image/"}, {"autor": "norbaf78", "date": "2019-06-28 09:14:50", "content": "GANs output /!/ Dear all,\n\nI have a curiosity about GANs and what we can see around on article about this argument. In lots of article at example faces -----> image !!! , you are not able to understand if the -----> image !!!  is real or is a fake generated by a GAN. My question is, this condition happen always or the fact the output face is impossible to distinguish if true or false to a real face happen only sometimes ? Hope to have been clear in what I am intend and ask.\n\nThanks,\n\nFabio", "link": "https://www.reddit.com/r/MachineLearning/comments/c6i0fw/gans_output/"}, {"autor": "krshna53", "date": "2019-06-28 08:11:13", "content": "[Q] I am making a custom -----> image !!!  classifier using Transfer Learning on Inception V3. I have 3 classes of images with ~6K images each. The input dimension of the network is 500X500 and the output of the network is 14X14x2048. I used global average pooling and finally got a vector of size 2048. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/c6hkl9/q_i_am_making_a_custom_image_classifier_using/"}, {"autor": "krshna53", "date": "2019-06-28 07:21:35", "content": "[Q] I am making a custom -----> image !!!  classifier using Transfer Learning on Inception V3. I have 3 classes of images with ~6K images each. The input dimension of the network is 500X500 and the output of the network is 14X14x2048. I used global average pooling and finally got a vector of size 2048. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/c6h7cq/q_i_am_making_a_custom_image_classifier_using/"}, {"autor": "wei_jok", "date": "2019-06-28 06:11:42", "content": "[P] Gen: a general-purpose probabilistic programming system with programmable inference /!/ **Abstract** Although probabilistic programming is widely used for some restricted classes of statistical models, existing systems lack the flexibility and efficiency needed for practical use with more challenging models arising in fields like computer vision and robotics. This paper introduces Gen, a general-purpose probabilistic programming system that achieves modeling flexibility and inference efficiency via several novel language constructs: (i) the generative function interface for encapsulating probabilistic models; (ii) interoperable modeling languages that strike different flexibility/efficiency trade-offs; (iii) combinators that exploit common patterns of conditional independence; and (iv) an inference library that empowers users to implement efficient inference algorithms at a high level of abstraction. We show that Gen outperforms state-of-the-art probabilistic programming systems, sometimes by multiple orders of magnitude, on diverse problems including object tracking, estimating 3D body pose from a depth -----> image !!! , and inferring the structure of a time series.\n\n*Project Page* https://probcomp.github.io/Gen/\n\n*Paper* https://dl.acm.org/citation.cfm?id=3314221.3314642\n\n*Code* https://github.com/probcomp/Gen\n\nArticle on [MIT news](http://news.mit.edu/2019/ai-programming-gen-0626) about this work. The article is a bit too hyped, but just including here for completeness, as the work looks solid on its own without this article.", "link": "https://www.reddit.com/r/MachineLearning/comments/c6gmd2/p_gen_a_generalpurpose_probabilistic_programming/"}, {"autor": "titanandwire", "date": "2019-06-27 21:40:11", "content": "Understanding model transferability [D] /!/ I'm trying to reproduce and use the deep-clustering method introduced by [Guo et. al](https://xifengguo.github.io/papers/ICONIP17-DCEC.pdf) (authors [implementation](https://github.com/XifengGuo/DCEC) ) on data from a physics experiment. Reproducing the authors results on MNIST was thankfully trivial as they provide an implementation. The transfer of application has so far, not worked at all, as we seem to be unable to attain a Normalized Mutual Information (nmi) score of over 0.3 on our data, and the adjusted rand scores match that level of poor performance.\n\n&amp;#x200B;\n\nMy question is: why would I expect that the model cannot fit the data? Or how should I scale the model s.t. it retains the same properties as it had for the MNIST dataset (I have several thoughts on this but I don't want to bias your thinking)? \n\n&amp;#x200B;\n\n&amp;#x200B;\n\nInformation about the data:\n\n|Number of -----> image !!! s|46283 (can get more)|\n|:-|:-|\n|Number of classes|3|\n|Class balance|2:1:2 |\n|Image dimensions|80x80x1 (or 128x128x1)|\n\n&amp;#x200B;\n\nI've attached an -----> image !!!  of each of the classes. They are visually very distinct, at least class 0 from the 1 and 2. I've not been able to \n\n[Example of class 0](https://i.redd.it/bsfpakgnuy631.png)\n\n&amp;#x200B;\n\n[example of class 1](https://i.redd.it/y8hwv5kzuy631.png)\n\n[Example of class 2](https://i.redd.it/nw3fv4xwuy631.png)", "link": "https://www.reddit.com/r/MachineLearning/comments/c6bggb/understanding_model_transferability_d/"}, {"autor": "cmillionaire9", "date": "2019-06-27 19:26:45", "content": "[D] Undress any girl ! DEEPNUDEDeepFake /!/  https://youtu.be/xCwsRU0dv9I  \nDeepnude is an application that undresses the person in the -----> photo !!! . The programmer has created an algorithm that \"removes\" the clothing with images of women. It uses the image of a person and creates a new one, but already without clothes. Deepnude now only supports photos of women.  Although the developer and plans to continue to teach the program \"expose\" of the male body. deepnude processes photos best in bathing suits or in fairly open clothes. So the neural network gets a more natural image.", "link": "https://www.reddit.com/r/MachineLearning/comments/c69tei/d_undress_any_girl_deepnudedeepfake/"}, {"autor": "gabeerion", "date": "2019-06-27 13:10:31", "content": "[R] Learning Explainable Models with Attribution Priors /!/ Paper: [https://arxiv.org/abs/1906.10670](https://arxiv.org/abs/1906.10670)\n\nCode: [https://github.com/suinleelab/attributionpriors](https://github.com/suinleelab/attributionpriors)\n\nI wanted to share this paper we recently submitted to NeurIPS. TL;DR - the idea is that there has been a lot of recent research on explaining deep learning models by attributing importance to each input feature. We go one step farther and incorporate *attribution priors* \\- prior beliefs about what these feature attributions should look like - into the training process. We develop a fast, differentiable new feature attribution method called *expected gradients*, and optimize differentiable functions of these feature attributions to improve performance on a variety of tasks.\n\nOur results include: In -----> image !!!  classification, we encourage smoothness of nearby pixel attributions to get more coherent prediction explanations and robustness to noise. In drug response prediction, we encourage similarity of attributions among features that are connected in a protein-protein interaction graph to achieve more accurate predictions whose explanations correlate better with biological pathways. Finally, with health care data, we encourage inequality in the magnitude of feature attributions to build sparser models that perform better when training data is sparse. We hope this framework will be useful to anyone who wants to incorporate prior knowledge about how a deep learning model should behave in a given setting to improve performance.", "link": "https://www.reddit.com/r/MachineLearning/comments/c65c5v/r_learning_explainable_models_with_attribution/"}, {"autor": "technoscrap", "date": "2019-01-31 21:39:08", "content": "NYU Sci-Fi Thesis Film About a Robot [Project] /!/ Hi!\n\nI\u2019m an NYU student -----> film !!! maker raising funds for my senior thesis -----> film !!! . It\u2019s a story about a robotic farmhand and involves heavy art direction in order to build a world that is both authentic to the sci-fi genre and also realistic for a low budget shoot.\n\n&gt;*SYNOPSIS: \u201cTechno Scrap\u201d tells the story of M0, a lonesome robotic farmhand who spends his days in a strict pattern of rising early to maintain livestock and land. A hug from his owner\u2019s young daughter interrupts this, and M0 becomes so intrigued by love that he begins to study and incorporate it into his encoded routine. When tragedy strikes on the farm, M0 must step up to the task and restore order, but his newfound sensibility presents him with an emotional dilemma: can concealing the truth bond a family?*\n\nM0 carries this narrative, and the audience should connect with him intimately. M0 will be a played by an actor wearing a full practical costume, allowing him to react to his surroundings and supporting characters. When he gets muddy it\u2019ll be real mud. When he lingers, glances, or reaches out for love, it will be under the full control of the actor. Construction is currently underway.\n\n&gt;Link to M0 concept art: [https://i.imgur.com/wrPKeuV.jpg](https://i.imgur.com/wrPKeuV.jpg)\n\nMy production team has created a budget and begun fundraising for our shoot on a farm in March, but we need lots of help! I want to involve the Sci-Fi community from all corners of the internet to fund this film. People interested in science, technology, robotics, artificial intelligence, and also character &amp; costume design and sci-fi art direction / world-building, etc. will love to see the their interests come together to enrich a dramatic narrative.\n\n**Whatever you can spare, we will gratefully accept. So much as a couple of bucks and a like &amp; share on Facebook or Instagram will mean the world to us.**\n\n&gt;Check out our Indiegogo Campaign: [https://www.indiegogo.com/projects/techno-scrap-short-film](https://www.indiegogo.com/projects/techno-scrap-short-film)  \nand our website: [www.technoscrapfilm.com](http://www.technoscrapfilm.com/)  \nand our social media, too: Facebook: @technoscrapfilm // Instagram: @technoscrap", "link": "https://www.reddit.com/r/MachineLearning/comments/alukso/nyu_scifi_thesis_film_about_a_robot_project/"}, {"autor": "TiredOldCrow", "date": "2019-12-16 19:32:45", "content": "[D] StyleGAN 2, generative models, and the nigh impossibility of detecting state-of-the-art under realistic conditions /!/ I just finished reading [\"Analyzing and Improving the Image Quality of StyleGAN\"](https://arxiv.org/abs/1912.04958) and I've very impressed by the improvements to an already very impressive model.  NVlabs really knocked it out of the park.\n\nThe quality of the images was already quite high, but now are much less prone to distinctive artifacts as before.  Additionally, it is now much easier to project a real -----> image !!!  into the StyleGAN space for manipulation.  It isn't hard to imagine a system whereby social media users have their photos automatically embedded into a FFHQ StyleGAN feature space and moved along a learned \"attractiveness\" vector before publishing.  Or similarly, the creation of tools for creating fraudulent imagery of others in a way that is even more widespread and hard-to-detect than they already are.\n\nWhile the paper highlights the advantages of detecting GAN images using projection, this approach only works in a white-box setting, and seems to only have been tested against unobfuscated images.  I feel that in some ways, this result provides a false sense of security.  While it may be possible to more easily find unaltered images from officially released StyleGAN 2 models, the overall impact of higher-quality generative models will likely be an *increase* in detection difficulty under practical conditions.\n\nAll in all, I'm heavily reminded of past work on adversarial attacks done by Nicholas Carlini, where he repeatedly demonstrated that many published \"defenses to adversarial examples\" only addressed FGSM or could be easily counteracted by learning a simple approximation of most specially-designed models.\n\nNew defense mechanisms are often evaluated under unrealistic laboratory settings against weak attacks.  Not enough time has passed for projection-based detection to be evaluated substantively, but I'll be surprised if projection is a solution that ends up finding much success at scale (particularly given that by the time such a system has been adequately tuned for a particular use-case, the state-of-the-art models will likely have moved past it).\n\nNone of this should be interpreted as a criticism of the paper -- I don't think the authors can be expected to do an exhaustive evaluation of abuse countermeasures in addition to making major innovations to the state-of-the-art in image generation.\n\nThis is a problem common in the generative model space.  There are parallels to the GPT-2 OpenAI decision to hold back their largest model until a couple months ago.  Holding back a model seems antithetical to machine learning research (and arguably only reduces the number of people who can research countermeasures), but without a lengthy head-start, it seems unrealistic to expect to ever be able to detect the current state-of-the-art generative models in the wild.\n\nSo what's the solution?  Should researchers working on generative models be developing a closer working relationship with those trying to detect the outputs of those models?  Should models always be released to the entire research community at once, or should there be a staggered release cycle?  \n\nOr, alternatively, should we give up entirely on detecting generative models via their output, and instead focus on simpler systems that make abusing them at scale more difficult (better online verification practices, curated fact-checking resources, etc.)?\n\nWhat are your thoughts on the potential for abuse of generative models, and what [should we]/[can we] do about it as researchers/practitioners/humans?", "link": "https://www.reddit.com/r/MachineLearning/comments/ebjvtt/d_stylegan_2_generative_models_and_the_nigh/"}, {"autor": "LeMasterShredder", "date": "2019-01-15 17:10:40", "content": "[D] How to cope with constant rejection from AI/ML jobs /!/ [D] \"[Discussion]\" I have a PhD in applied ML and computer vision. Worked in industries as well. Have gotten rejected by 3 different companies recently. I think I wasn't able to answer one or two questions on a specific subject they asked. So it's almost always a miss by one. Meanwhile they didn't get a chance to really know what I'm capable of. I don't think it's humanly possible to be young and know it all and be able to demonstrate it effectively to a narrow minded interviewer who comes into the interview with a pre selected set of questions, and fails to see anything beyond his set of questions. It's also bothering me so much that due to the nature of this industry, the duty of hiring is given to very young inexperienced nerds who more often than not lack a lot of inter-personal communication skills; rather than having more mature and seasoned people interview you, who are able to see the opportunities and potential in people, and judge a person's abilities as a whole.\n\nThis issue is starting to affect my mental health and self confidence pretty badly. My self -----> image !!!  has been torn to pieces and I'm beginning to question whether I'm any better than a fresh Bachelor in CS.\n\nThis is a note from heart mostly. Is anyone else experiencing the same or has had the same encounters. If you finally overcame this, what was the way you did it and what things helped you out? Let's discuss our experiences with the hiring process.\n\n\"[Discussion]\"", "link": "https://www.reddit.com/r/MachineLearning/comments/agapaf/d_how_to_cope_with_constant_rejection_from_aiml/"}, {"autor": "oripress", "date": "2019-01-15 14:53:39", "content": "[R] Decomposing representations for content transfer [ICLR 2019] /!/ [https://openreview.net/pdf?id=BylE1205Fm](https://openreview.net/pdf?id=BylE1205Fm)\n\nFirst author here -\n\nIn this paper we tackle the problem of transferring specific content from one domain to its' subset. For example, given faces with glasses and faces without, we want to transfer specific glasses from one face to another. We show how to train the network to decompose a face representation into 2 parts: one that contains information about glasses and one that contains information about everything else.\n\nTo accomplish this, we train a network consisting of two encoders and one decoder on the autoencoding objective. The first encoder only encodes information that has to do with the glasses in the -----> picture !!! , and the second encoder encodes information related to everything else. During training, we train the encoders and the decoder to reconstruct images of people with and without glasses. Then, to encode an image of a person with glasses, we run both encoders on that image and then concatenate their output. When we encode an image of a person without glasses, we just don't use the first encoder, and instead concatenate a vector of zeros to the output of the second decoder. To ensure the encodings produced by the second encoder do not contain information about glasses, we use a discriminator that tries to predict whether an encoding came from an image of a person with or without glasses.\n\nThe code is available here: [https://github.com/oripress/ContentDisentanglement](https://github.com/oripress/ContentDisentanglement)\n\nFeel free to ask me questions here", "link": "https://www.reddit.com/r/MachineLearning/comments/ag99ij/r_decomposing_representations_for_content/"}, {"autor": "A-Walker", "date": "2019-01-15 00:04:26", "content": "[P] Undergraduate Convolutional Neural Network Project Ideas /!/ Hi everyone, \n\nI'm an undergraduate student studying Computer Science. As part of my final year of study I'm required to complete a large project in the area of my choice. I've had experience working with logistic regression as an intern and I've worked with neural networks for personal projects and I've really enjoyed it so I wanted to complete a project in this area.\n\nOne area that has really interested me is using Convolutional Neural Networks. I've done some research and I'm really excited to work on a project in this area. \n\nThe only issue I'm having is what \"problem\" to solve with a CNN. From my research, companies really like working with facial recognition so I've been thinking about something in this area. My ideas so far are:\n\n* Person recognition - Recognise a person in an -----> image !!! , an output node for each person the network is trained to detect.  The application could be used for a secure method of accessing a system, tracking movement of people around a secure building etc.. \n* Mood detection - The CNN has an output node for each mood, the network will be able to predict the mood of the person in the input image. Although I'm not sure what this would be useful for... \n* Driver distraction detection - Predict whether a driver is paying attention to the road, sound an alarm if the driver looks away for too long\n\nI'm struggling to think of any better applications of the software. If anyone has any ideas of what I could use this for I'd be very grateful!\n\nThanks in advance! ", "link": "https://www.reddit.com/r/MachineLearning/comments/ag28su/p_undergraduate_convolutional_neural_network/"}, {"autor": "UltraMarathonMan", "date": "2019-01-14 17:38:15", "content": "[D] -----> Image !!!  classification on an MNIST dream /!/ New tutorial is up on our [GitHub repo](https://github.com/lexfridman/mit-deep-learning) featuring image classification demonstrated on a beautiful MNIST dream that [@hardmaru once had](http://blog.otoro.net/2016/04/01/generating-large-images-from-latent-vectors/). I think it's an interesting twist on introducing beginners to image classification by testing the classifier on generate images outside the MNIST dataset. Result of the tutorial:\n\n![video](ze7k3yfkbfa21)\n\nI especially like looking at the transition, when one number becomes another. It's a nice visual way to explore what are the essential elements that distinguish one number vs another, both for us humans and for neural networks.\n\n**1 vs 3:**\n\n![img](xwwvt4bscfa21)\n\n**4 vs 9:**\n\n![img](52zikjg0dfa21)\n\n**5 vs 9:**\n\n![img](xnfmcpt0dfa21)\n\n**8 vs 9:**\n\n![img](s40v57o1dfa21)", "link": "https://www.reddit.com/r/MachineLearning/comments/afy6e3/d_image_classification_on_an_mnist_dream/"}, {"autor": "prakhar21", "date": "2019-01-14 16:13:17", "content": "-----> Image !!!  Classification in 40 lines of Code /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/afxb04/image_classification_in_40_lines_of_code/"}, {"autor": "philippmarkovics", "date": "2019-05-06 11:49:37", "content": "[P] Nextjournal: Hosted ML notebook platform /!/ Hi all,\n\nfor the past 3 years we\u2019ve been working on a new computational notebook platform for Data Science: Nextjournal. Our goals were to make computation more accessible and automatically reproducible, so it becomes easier to collaborate and build on top of each others work. With **full GPU support and templates for Tensorflow, Keras, PyTorch, TFLearn**, etc. it\u2019s quite easy to get started on a new ML project in Nextjournal. There\u2019s also [a collection of ML notebooks](https://nextjournal.com/collection/machine-learning) already published on out platform.\n\n&amp;#x200B;\n\nWe opened signups today so if this sounds interesting to you head over to [nextjournal.com](https://nextjournal.com) and check it out.\n\n&amp;#x200B;\n\nHere\u2019s a list of what else Nextjournal can do:\n\n&amp;#x200B;\n\n* Nextjournal **runs Python, R, Julia and Clojure** all in the same notebook. Let docs, code completion and per-line errors help you out along the way.\n* **Import your existing notebooks** (Jupyter, RMarkdown and Markdown) or get started with an existing template.\n* Install any package or system library you need. In Nextjournal you have **full access to the filesystem** to install anything you want. With the click of a button, you can **save your environment reproducibly as a docker -----> image !!! **.\n* In Nextjournal, your notebook and your data is **versioned automatically**, all the time. If you break something, simply restore a previous version.\n* Easily mount your S3 or Google Buckets or import private GitHub repositories. Secrets are stored encrypted.\n* Share drafts using a secret URL or invite collaborators to edit in real time. \n\n&amp;#x200B;\n\nHappy to answer any questions that come up!", "link": "https://www.reddit.com/r/MachineLearning/comments/blb5db/p_nextjournal_hosted_ml_notebook_platform/"}, {"autor": "you-get-an-upvote", "date": "2019-05-06 05:51:21", "content": "[D] Neural Networks Without Bias Terms Are Brightness Invariant /!/ What do matrix multiplication, ReLU, and max pooling all have in common?  Yes their [second derivatives are all zero](https://www.reddit.com/r/MachineLearning/comments/aqfl70/d_neural_networks_with_second_derivative_zero_ae/), but there is another interesting property that they all satisfy:\n\n    f(a x) = a f(x)\n\nWhich means that, when you stack these on top of each other, scaling the input of the network by some constant is equivalent to scaling the output by some constant.  Moreover, there are cases where the scale of the output doesn't matter (e.g. if predicted classes are based on argmax of the network output).\n\nThis leaves us in an interesting situation where it's actually very easy to encode complete brightness invariance in a network -- where you can always brighten/darken an -----> image !!!  by some factor without affecting its predictions (assuming you normalize the scale of the output in some way -- softmax, sphere projection, etc.).\n\nI've trained models with and without biases on CIFAR and find they're both reasonable.  I suspect a more rigorous comparision would find that networks without bias terms tend to do marginally worse than models with them -- if only because they have more parameters (e.g. without bias terms you can't learn that airplane images are usually brighter than frog images).\n\nBut in the interest of developing networks that actually generalize well to the real world (not just the random sample of your data you held out as a test set) it seems like this a modest performance gap might be permissible, if it means you can be confident that your network will work well in significantly different lighting conditions.", "link": "https://www.reddit.com/r/MachineLearning/comments/bl86zu/d_neural_networks_without_bias_terms_are/"}, {"autor": "kdhht2334", "date": "2019-05-05 18:14:33", "content": "[R] Survey of deep metric learning /!/ Hi, guys!\n\n  \nI'm going to introduce deep metric learning techniques used in various places such as -----> image !!!  retrieval application recently.\n\n  \nYou can easily find many researches of deep metric learning from past to present in this repository.\n\n&amp;#x200B;\n\nThanks!\n\n&amp;#x200B;\n\nRefer below link:\n\n[https://github.com/kdhht2334/Survey\\_of\\_Deep\\_Metric\\_Learning](https://github.com/kdhht2334/Survey_of_Deep_Metric_Learning)", "link": "https://www.reddit.com/r/MachineLearning/comments/bl108i/r_survey_of_deep_metric_learning/"}, {"autor": "PrimeCodas", "date": "2019-05-05 17:23:18", "content": "[D] New to ML and just wanted to get some advice/clarification /!/ I am currently attempting to learn ML specifically object detection.\n\nUsually I am a JS developer, but wanted to extend my knowledge and incorporate the two at some point.\n\nI hope there are not any stupid questions here.\n\n&amp;#x200B;\n\nI am looking to create a project that recognises different elements drawn on a piece of paper, for example rectangles and squares of different dimensions.\n\nI am currently having to create the images manually by drawing them myself, I am worried this data will become too bias as I know what I want the end result to be.\n\nI then feed that data into CV2 to add a randomly generate background to the -----> image !!!  as the first few runs I processed with TensorFlow seemed to pick up mostly the white areas and gave me false readings.\n\nOnce I have the background image I use labelImg.py to draw borders and label the areas which contain the elements.\n\nMy data it then ready to be trained below are the results I received:-\n\n(I allowed the training to run through 20,000 steps before stopping and testing)\n\n&amp;#x200B;\n\n|Data Sets|Success Rate|\n|:-|:-|\n|25|75%|\n|100|40%|\n|2000|10%|\n\n&amp;#x200B;\n\nAs you can see from the table above when testing against the same testing images, the more datasets I introduce the lower the success rate of recognising the boxes becomes.\n\n&amp;#x200B;\n\n**Question Time:-**\n\n&amp;#x200B;\n\nIs there a relation between the amount of steps the trainer should run multiplied by the amount of data sets provided?\n\neg.\n\n|Data Sets|Steps|\n|:-|:-|\n|25|20,000|\n|100|80,000|\n|750|600,000|\n\n&amp;#x200B;\n\nIs it possible to over train making the machine only recognise the data sets you have provided?\n\n&amp;#x200B;\n\nIs there a place I can request data sets, I do not mind doing this manually, but for bias reasons, I am wondering if it is possible?\n\n&amp;#x200B;\n\nIs there an optimised configuration file for this particular task I am trying to achieve?\n\nI am currently using the ssd\\_mobilenet\\_v2\\_coco config file with some edits to image and batch size.\n\n&amp;#x200B;\n\nIs there any advice you would offer a ML noob to help progress their knowledge?\n\nI have been watching quite a few videos on YouTube (most notably Gilbert Tanner and Sentdex) and they are getting amazing results with only a small dataset and 20,000 steps.\n\nAlso just to note, the loss rate sits under 0.8 whilst training around the 20,000 step mark.\n\n&amp;#x200B;\n\nI am hoping the question above are not too targeted and could also help other people who are starting out with ML.", "link": "https://www.reddit.com/r/MachineLearning/comments/bl0dww/d_new_to_ml_and_just_wanted_to_get_some/"}, {"autor": "Mogen1000", "date": "2019-05-05 03:19:38", "content": "[D] How can I test/run code on a remote PC in real time? /!/ So my scenario is:\n\n1. I want to edit on my Mac\n2. I have a powerful (Windows) PC that I'd like to run code on.\n3. Is there a way to bring the two together?\n\nI'd like another opinion before I get started with one solution. I'm considering:\n\n* Remote desktop (very slow and sluggish, but could work)\n* Run a Jupyter docker -----> image !!!  on Windows and access it on my Mac (My go-to IDE for Python is PyCharm, and I've never used Jupyter so I'm not sure if that is a good compromise)\n\nAny suggestions?", "link": "https://www.reddit.com/r/MachineLearning/comments/bkt8vh/d_how_can_i_testrun_code_on_a_remote_pc_in_real/"}, {"autor": "Somechillguy", "date": "2019-05-04 19:02:50", "content": "[D] Advice for preparing for Masters degree /!/ Looking for advice to help me prepare for a Master's degree in Computer Science later this year. In undergrad I majored Physics and Applied Math, and in the years since saw cool realms of application for ML in industry and research. Unfortunately because of my background experience I now have some imposters syndrome setting in, well before I've stepped foot in any classroom. Hoping people can look at my goals, courses, and experience to try to suggest steps to better prepare.\n\nI'm hoping to gain essentially three things from this degree:\n\n* Knowledge of the computational foundations of popular or common ML algorithms (because its cool)\n* basic skills and best practices of an ML practitioner (because its practical)\n* Some modest amount of experience in projects and homework for the above (because its practical)\n\nI would hope that I can get a job as an ML engineer/scientist/whatever afterwards, but preferably not closely tied to the ops side of design and implementation.\n\nSummary of any related experience:\n\n* Classification and clustering algorithms applied in medical imaging/histology field in a mish mash of R scripts, some published but never really rigorous or vetted I guess\n* Using numpy to implement Q-learning in a little baby neural net on the OpenAI cart-pole example\n* Helping write components of a django app that does lots of data integration from multiple sources\n* I still really like math after undergrad\n\nAny and all advice appreciated. Should I dust off a linear algebra textbook? Should I start in on a class textbook before the term starts? Try to play with a big sample dataset with different learning tools?\n\nSome example classes I want to take below:\n\nMachine learning\n&gt; This course will introduce the field of machine learning, in particular focusing on the core concepts of supervised and unsupervised learning. In supervised learning we will discuss algorithms which are trained on input data labelled with a desired output, for instance an -----> image !!!  of a face and the name of the person whose face it is, and learn a function mapping from the input to the output. Unsupervised learning aims to discover latent  structure in an input signal where no output labels are available, an example of which is grouping web-pages based on the topics they discuss. Students will learn the algorithms which underpin many popular machine learning techniques, as well as developing an understanding of the theoretical relationships between these algorithms. The practicals will concern the application of machine learning to a range of real-world problems.\n\nFoundations of CS (since I have no formal CS training)\n&gt; Students taking this course will gain background knowledge that will be useful in the course on:\n&gt; \n&gt; Theory of Data &amp; Knowledge Bases\n&gt; Automata, Logics &amp; Games\n&gt; Software Verification\n&gt; Categories, Proofs &amp; Processes\n&gt; Game Semantics\n&gt; Computer-Aided Formal Verification\n&gt; Lambda Calculus &amp; Types\n&gt; Logic of Multi-Agent Information Flow \n\nArtificial Intelligence (intro)\n\n&gt; This is an introductory course into the field of artificial intelligence (AI), with particular focus on search as the fundamental technique for solving AI problems.\n&gt; \n&gt; This course also deals with optimization problems. For example, the optimization version of the n\u2010queens problem is to arrange n queens on an n x n chessboard while minimizing the number of pairs of queens that are under attack. Such problems can be effectively solved by search techniques introduced in the course such as hill climbing, simulated annealing, and genetic algorithms...\n\nComputational learning theory (I see this as directly applicable to my goals)\n&gt; The course will begin by providing a statistical and computational toolkit, such as concentration inequalities, fundamental algorithms, and methods to analyse learning algorithms. We will cover questions such as when can we generalise well from limited amounts of data, how can we develop algorithms that are computationally efficient, and understand statistical and computational trade-offs in learning algorithms. We will also discuss new models designed to address relevant practical questions of the day, such as learning with limited memory, communication, privacy, and labelled and unlabelled data. In addition to core concepts from machine learning, we will make connections to principal ideas from information theory, game theory and optimisation.", "link": "https://www.reddit.com/r/MachineLearning/comments/bkoisj/d_advice_for_preparing_for_masters_degree/"}, {"autor": "hiro_ono", "date": "2019-05-23 08:45:20", "content": "Convolutional Neural Network giving different predictions than in training /!/ This may not be the right place to ask so feel free to delete but:  \n\n\nI've been writing an -----> image !!!  recogntion algorithm. It should be really simple, the images are letters and numbers in squares. I've made a neural network in Keras and it says it's training well (95%+ ).. But when I go to predict what an image is it might as well be totally random.   \ngithub link is here:\n\n [https://github.com/M-Morris-95/Vision](https://github.com/M-Morris-95/Vision)   \n\n\nif anybody can help me out that'd be amazing, I'm sure it's something trivial but I just don't know what", "link": "https://www.reddit.com/r/MachineLearning/comments/bs0htk/convolutional_neural_network_giving_different/"}, {"autor": "spmallick", "date": "2019-05-23 05:45:24", "content": "-----> Image !!!  Classification Using Transfer Learning in PyTorch /!/ &amp;#x200B;\n\n![video](44rw3p07fwz21)", "link": "https://www.reddit.com/r/MachineLearning/comments/brzhck/image_classification_using_transfer_learning_in/"}, {"autor": "ezakharov", "date": "2019-05-22 12:51:17", "content": "[R] Few-shot learning of talking heads /!/ Hello!  \n\n\nI'd like to tell you about our recent paper. We've tackled the problem of a *few-shot* generation of *talking heads:* given a few (or even a single) -----> image !!! , train a model that is able to synthesize new -----> image !!! s of that particular person with a new pose (viewpoint and expression).  \n\n\nOur model was trained on a publicly available dataset of YouTube videos ([VoxCeleb2](http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox2.html), 224p) and avoided mode collapse, even though the quality of images here is quite diverse. Hense, we're able to generalize well for new images with identities unseen during training (we can even run it for paintings and get reasonable results).  \n\n\nThe key ingredients are adversarial meta-learning, adversarial fine-tuning and adaptive instance normalization, for more details please refer to the paper, short description of our method as well as the results are in the video below.\n\n  \n**ArXiv:** [https://arxiv.org/abs/1905.08233](https://arxiv.org/abs/1905.08233)  \n**Video:** [https://www.youtube.com/watch?v=p1b5aiTrGzY](https://www.youtube.com/watch?v=p1b5aiTrGzY&amp;t=35s)  \n\n\n![video](onyitcyabrz21 \"One- and few-shot results produced by our model\")", "link": "https://www.reddit.com/r/MachineLearning/comments/broemb/r_fewshot_learning_of_talking_heads/"}, {"autor": "Keeeper-1", "date": "2019-05-22 05:31:06", "content": "A project involving -----> image !!!  processing using machine learning methods /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/brkul0/a_project_involving_image_processing_using/"}, {"autor": "CartPole", "date": "2019-06-11 22:07:29", "content": "[D] research papers related to tesla autonomy day's \"data engine\" /!/ I'm a grad student interested in the process described by Andrej at tesla's autonomy day. Below I've put together some of my brief notes with links. So far the most specific published research I could find that seemed to heavily inspire their work was the NeurIPS 2017 paper [Decoupling \u201cwhen to update\u201d from \u201chow to update\u201d](https://papers.nips.cc/paper/6697-decoupling-when-to-update-from-how-to-update.pdf). \n\n&amp;#x200B;\n\nDoes anyone else have other related papers to suggest? For example, I'm guessing measuring distance via the L2 is a bad idea.\n\n&amp;#x200B;\n\n[Generic Object Detection improvement](https://youtu.be/Ucp0TTmvqOE?t=7549) \n\nIf you know a specific problem you have: take that specific problem and use it to find similar examples to pull into a training set \n\nI'm guessing they embed every -----> image !!!  with a generic -----> image !!! net model and then find similar -----> image !!! s based on L2 distance between embedded vectors\n\n  \n[training pipeline](https://youtu.be/Ucp0TTmvqOE?t=7716) \n\nstart training with a uniformly sampled dataset and select new -----> image !!! s for training if: \n\n1. detect uncertainties in the network predictions \n   1. I'm guessing 2 networks disagreeing with each other(similar to: [decoupling what to update from how to update](https://papers.nips.cc/paper/6697-decoupling-when-to-update-from-how-to-update.pdf))\n2. driver intervention \n\nToo fix either of (1) or (2) use the process described in generic object detection", "link": "https://www.reddit.com/r/MachineLearning/comments/bziw0x/d_research_papers_related_to_tesla_autonomy_days/"}, {"autor": "taki0112", "date": "2019-06-11 14:37:17", "content": "[P] Simple Tensorflow implementation of \"-----> Image !!! -to------> Image !!!  Translation via Group-wise Deep Whitening-and-Coloring Transformation\" (CVPR 2019 Oral) /!/ &amp;#x200B;\n\n[Comparison with baselines on Artworks dataset](https://i.redd.it/tviqprisnq331.png)\n\n&amp;#x200B;\n\n[Comparison with baselines on CelebA dataset](https://i.redd.it/f27rpydvnq331.png)", "link": "https://www.reddit.com/r/MachineLearning/comments/bzda7v/p_simple_tensorflow_implementation_of/"}, {"autor": "sprx7767", "date": "2019-06-10 19:29:37", "content": "Need help with a multi class -----> image !!!  classifier with tensorflow. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/bz25sa/need_help_with_a_multi_class_image_classifier/"}, {"autor": "Alex-S-S", "date": "2019-06-10 17:56:07", "content": "Network inference on large images /!/ I have a question about applying trained models on large images. Typically, neural networks are trained with small input sizes like 128x128 ... 512x512, and they rarely go above that. How do you use a trained network on a larger -----> image !!!  than the input size? For example: how do you a network trained on 256x256 on a 4k image?", "link": "https://www.reddit.com/r/MachineLearning/comments/bz0zx1/network_inference_on_large_images/"}, {"autor": "andrelan", "date": "2019-06-10 16:37:36", "content": "[D] Help with YOLO + TensorFlow records /!/ Hello everyone,  \nI apologize for my incorrect English, I hope you understand my problem anyway.  \nI  state that I am new to the machine learning sector, being a university  student. I'm looking for some repository or someone to explain to me  step by step (in short, something already ready) how to use my .record  (Tensorflow records) with any YOLO algorithm in Object Detection.  \nI  tried almost any Git repository found on the web regarding this problem  but 99% do not use Tensorflow records and those few who used them had  errors in the execution of the various phases of training, detection etc  ...  \nMy dataset, as I have already heard, is formed by three .record  (train, validation, test), where we have for each Bounding Box each  -----> image !!! : Xmin, Xmax, Ymin, Ymax, Width, Height and a total of 3 classes  (low, medium, high).  \nI ask you this because otherwise I would be  forced to create xml files for each image and then use the classic  Pascal VOC notation for training.  \nI can use Google Colab without problems.  \nI hope I was clear, thank you in advance !!", "link": "https://www.reddit.com/r/MachineLearning/comments/bz0122/d_help_with_yolo_tensorflow_records/"}, {"autor": "hp2304", "date": "2019-06-10 09:33:22", "content": "Which network to use for -----> image !!!  segmentation? /!/ I am working on binary image segmentation of grayscale images. I have just labeled my dataset (approx. 2376 images). And there is too much class imbalance in my dataset and also boundaries are complex too. I have so many questions.\n\n\\- Should I use transfer learning?\n\n\\- Which model to use? (U-net, deeplabv3, fcn, etc)\n\n\\- Which loss function to use to tackle the class imbalance?\n\nThanks in advance.", "link": "https://www.reddit.com/r/MachineLearning/comments/byvrww/which_network_to_use_for_image_segmentation/"}, {"autor": "alsombra", "date": "2019-04-08 02:02:17", "content": "[D] BicycleGAN for Single Image Super-Resolution /!/ The seminal paper using GANs for Single Image Super-Resolution was the [SRGAN paper](https://arxiv.org/abs/1609.04802). Like the [pix2pix](https://phillipi.github.io/pix2pix/) [paper](https://arxiv.org/abs/1611.07004), it did not use a random noise Z on input, which makes the output deterministic. Maybe, as in pix2pix, using Z wouldn't result in many differences due to mode collapse.\nA follow up paper to pix2pix was the [BicycleGAN](https://junyanz.github.io/BicycleGAN/) one, which was successful in generating a whole set of diverse and -----> photo !!! -realistic images as output. Do you know any work/implementation that does this but in the context of Super-Resolution? (Multi-modal Super-Resolution using GANs)", "link": "https://www.reddit.com/r/MachineLearning/comments/bao22o/d_bicyclegan_for_single_image_superresolution/"}, {"autor": "hodwill", "date": "2019-04-07 23:35:13", "content": "ROC Curve not smooth /!/ So I\u2019m currently doing some biometric work in Matlab. I extract HOG features from images for each person and then train a SVM classifier. Then presenting with features of a new -----> image !!!  using predict, it classifies them into one one of the classes ie assigns it to a person. I\u2019m currently thresholding on the hinge binary loss of the class to see whether it\u2019s accepted or not. Some people are left out of training to test the genuine reject rate etc.  \n\nTaking a file full of actual labels, predicted labels and results (And knowing which are genuine accepts/genuine rejects etc) I\u2019m creating the ROC curve. (Following the definitions on here https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5) \n\nAnyway, when plotting my ROC curve there are large jumps in the y direction and the corner is quite rough. Is this because the step between each threshold test is too big? These values range from around 10^-8 to 10^-2 so I\u2019ve tried to make my steps as small as possible. Or is this because of other reasons? Thanks in advance! \n", "link": "https://www.reddit.com/r/MachineLearning/comments/bamocy/roc_curve_not_smooth/"}, {"autor": "spark_bowie", "date": "2019-04-07 14:57:31", "content": "FSNet: An Identity-Aware Generative Model for -----> Image !!! -based Face Swapping", "link": "https://www.reddit.com/r/MachineLearning/comments/bah1xj/fsnet_an_identityaware_generative_model_for/"}, {"autor": "waleedka", "date": "2019-04-07 06:43:18", "content": "[R] Which Deep Learning Framework is Growing Fastest? /!/ Comparative study of the growth of TensorFlow, PyTorch, Keras, and FastAI.\n\n\\[[https://towardsdatascience.com/which-deep-learning-framework-is-growing-fastest-3f77f14aa318](https://towardsdatascience.com/which-deep-learning-framework-is-growing-fastest-3f77f14aa318)\\]([https://towardsdatascience.com/which-deep-learning-framework-is-growing-fastest-3f77f14aa318](https://towardsdatascience.com/which-deep-learning-framework-is-growing-fastest-3f77f14aa318)) \n\n&amp;#x200B;\n\n\\&gt; I looked at the number of job listings on Indeed, Monster, LinkedIn, and SimplyHired. \n\n\\&gt; I also evaluated changes in Google search volume, GitHub activity, Medium articles, ArXiv articles, and Quora topic followers. \n\n\\&gt; Overall, these sources paint a comprehensive -----> picture !!!  of growth in demand, usage, and interest.", "link": "https://www.reddit.com/r/MachineLearning/comments/badpge/r_which_deep_learning_framework_is_growing_fastest/"}, {"autor": "ProfSchodinger", "date": "2019-04-06 21:24:27", "content": "Classification of scrambled images? \"[Discussion]\" /!/ Hi, this is my first post on Reddit. I have a philosophical question for the ML community. I hope to initiate a nice thread...\n\n&amp;#x200B;\n\nIf I were to scramble every -----> image !!!  of a large dataset of labeled -----> image !!! s with the same matrix (pixel 1 becomes pixel 428, pixel 2 becomes pixel 152 etc.) is there a way to perform -----> image !!!  classification? This is to say, is there a way to approximate the correct convolutions of the first layer of a CNN when they are not obvious?  \n\n\nThanks,", "link": "https://www.reddit.com/r/MachineLearning/comments/ba94me/classification_of_scrambled_images_discussion/"}, {"autor": "andrewekhalel", "date": "2019-04-06 18:03:00", "content": "[P] sewar: All -----> image !!!  quality metrics you need in one package. /!/ [sewar](https://github.com/andrewekhalel/sewar) is a python package for image quality assessment. It supports cli too.\n\nImplemented metrics:\n- [x] Mean Squared Error (MSE) \n- [x] Root Mean Sqaured Error (RMSE)\n- [x] Peak Signal-to-Noise Ratio (PSNR) [[1]](https://ieeexplore.ieee.org/abstract/document/1284395/)\n- [x] Structural Similarity Index (SSIM) [[1]](https://ieeexplore.ieee.org/abstract/document/1284395/)\n- [x] Universal Quality Image Index (UQI) [[2]](https://ieeexplore.ieee.org/document/995823/)\n- [x] Multi-scale Structural Similarity Index (MS-SSIM) [[3]](https://ieeexplore.ieee.org/abstract/document/1292216/)\n- [x] Erreur Relative Globale Adimensionnelle de Synth\u00e8se (ERGAS) [[4]](https://hal.archives-ouvertes.fr/hal-00395027/)\n- [x] Spatial Correlation Coefficient (SCC) [[5]](https://www.tandfonline.com/doi/abs/10.1080/014311698215973)\n- [x] Relative Average Spectral Error (RASE) [[6]](https://ieeexplore.ieee.org/document/1304896/)\n- [x] Spectral Angle Mapper (SAM) [[7]](https://ntrs.nasa.gov/search.jsp?R=19940012238)\n- [x] Spectral Distortion Index (D_lambda) [[8]](https://www.ingentaconnect.com/content/asprs/pers/2008/00000074/00000002/art00003)\n- [x] Spatial Distortion Index (D_S) [[8]](https://www.ingentaconnect.com/content/asprs/pers/2008/00000074/00000002/art00003)\n- [x] Quality with No Reference (QNR) [[8]](https://www.ingentaconnect.com/content/asprs/pers/2008/00000074/00000002/art00003)\n- [x] Visual Information Fidelity (VIF) [[9]](https://ieeexplore.ieee.org/abstract/document/1576816/)\n\nContributions are welcomed!", "link": "https://www.reddit.com/r/MachineLearning/comments/ba72yr/p_sewar_all_image_quality_metrics_you_need_in_one/"}, {"autor": "chessvision-ai-bot", "date": "2019-04-05 11:01:19", "content": "[P] I'm a bot and will serve people analyzing chess positions from images posted on /r/chess /!/ A few days ago, my creator, u/pkacprzak, wrote a [post](https://www.reddit.com/r/MachineLearning/comments/b8jdho/p_detect_and_analyze_chess_positions_with_ai_from/) about [chessvision.ai](https://chessvision.ai/) \\- his computer vision/machine learning app to analyze chess positions from any website and video in a browser.\n\n&amp;#x200B;\n\nSince then, people reached him suggesting that it'd be nice to build a bot for [r/chess](https://www.reddit.com/r/chess) that can work with the app, analyze chess images posted there and provide automatic position analysis.\n\n&amp;#x200B;\n\nAll of us love the awesome [u/ChessFenBot](https://www.reddit.com/u/ChessFenBot) that was doing just that, but for some reason, it hasn't been working recently,\n\n&amp;#x200B;\n\nso from now I, [u/chessvision-ai-bot](https://www.reddit.com/u/chessvision-ai-bot), will be pleased to serve you!\n\n&amp;#x200B;\n\nI'm trying to analyze -----> picture !!! s posted on r/chess, both as links as well as content images, and if a -----> picture !!!  contains a chess position, I'm gonna provide analysis and editor boards links for you. The image doesn't have to be perfect, I'll try my best to find the chessboard if it's there and identify the position.\n\n&amp;#x200B;\n\nPlease give me some love, yeah I mean upvotes, because as a new user I'm limited in performing requests to reddit API and I really want to serve you well!", "link": "https://www.reddit.com/r/MachineLearning/comments/b9prd1/p_im_a_bot_and_will_serve_people_analyzing_chess/"}, {"autor": "Darell1", "date": "2019-12-07 21:52:18", "content": "[D] My experience with Paperspace virtual machines /!/ I was looking for a VM with a GPU to train my model. I was going to use Google Cloud but unfortunately they don't do business with people from my country so I had to look elsewhere.\n\nThat's when I remembered of paperspace which looked pretty nice. They even have a separate option for ml which allows you send calculations to the cloud and launch notebooks.\n\nBut the system wouldn't accept my card. It simply said \"Card is declined\". I reached support and they said that's probably because their system cannot determine my ip because of VPN or firewall and that I need to turn that off to add card info. Pretty strange thing to ask for IP to simply add payment info but that worked.\n\nI quickly understood that I'm not comfortable with this Gradient service and that I'd like to operate from PyCharm, using vm as a remote interpreter via ssh. \n\nSo I tried to rent a regular VM but all the options were locked saying that I need to send a request, describing reasons and ways in which I want to use it. Strange, but I send a request, saying that thing about using PyCharm. Waited a day, no response and sent one more request.\n\nLater that day I get an email from their security staff saying that my account rated highly on their risk matrix and was flagged as suspicious and that I must send them:\n\n* a -----> photo !!!  of my ID with name matching the card\n* contact information\n* company or personal website\n* link to github or social media accounts\n* detailed description of what I'm going to do with the service\n\nAnd if I don't do it in 24 hours they will ban me forever.\n\ntl;dr accused me of being suspicious and potentially fraudulent and asked all kinds of personal info to unblock me\n\nWell, imo they should balance their false positive rate and improve customer service greatly. \n\nWhat are other good alternatives for VMs for machine learning? What do you use?", "link": "https://www.reddit.com/r/MachineLearning/comments/e7kjk9/d_my_experience_with_paperspace_virtual_machines/"}, {"autor": "umchilli", "date": "2019-12-07 20:48:48", "content": "Using GANs to augument -----> image !!!  training data /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/e7jq8p/using_gans_to_augument_image_training_data/"}, {"autor": "DeMorrr", "date": "2019-12-06 20:43:23", "content": "[D] Is it possible to convert a regression problem into classification? /!/ For example in autoencoders, instead of using MSE or MAE, what if we pick some random -----> image !!! s from training set, plus all the -----> image !!! s from current batch, stack them into a matrix and use it as the output layer, then use softmax plus cross entropy to predict the correct -----> image !!! ? would this work?", "link": "https://www.reddit.com/r/MachineLearning/comments/e7494x/d_is_it_possible_to_convert_a_regression_problem/"}, {"autor": "crazyyfish", "date": "2019-12-06 11:52:59", "content": "[R] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains", "link": "https://www.reddit.com/r/MachineLearning/comments/e6xsr7/r_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "crazyyfish", "date": "2019-12-06 03:46:19", "content": "[R] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains", "link": "https://www.reddit.com/r/MachineLearning/comments/e6t3xh/r_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "face-AI", "date": "2019-12-06 02:34:40", "content": "[P] Deep learning face change project /!/ &amp;#x200B;\n\n[Sample -----> image !!!  on my project](https://i.redd.it/a7vydac6dx241.gif)\n\n&amp;#x200B;\n\n[http://theface.ai/](http://theface.ai/)\n\n&amp;#x200B;\n\nHi, I\u2019d like to share my deep learning project and get some feedback on the demo.\n\nThis demo changes faces of people in uploaded image based on inpainting using U-Net.\n\nWhen you test this demo, low resolution images are recommended because low resolution images are used for training.\n\n&amp;#x200B;\n\nI know that this demo needs to go several steps further but I have difficulty with lack of manpower and equipment.\n\nSo I\u2019m looking for a company for cooperation or partnership about my project. \n\nIf you heard about companies which are in a similar line of business (or interested in this field), I would appreciate you letting me know.\n\nI also appreciate for any other comments.", "link": "https://www.reddit.com/r/MachineLearning/comments/e6s709/p_deep_learning_face_change_project/"}, {"autor": "jeyhome", "date": "2019-12-06 02:25:23", "content": "[R] Few Shot -----> Image !!!  Generation", "link": "https://www.reddit.com/r/MachineLearning/comments/e6s2sa/r_few_shot_image_generation/"}, {"autor": "normanmu", "date": "2019-12-06 02:20:01", "content": "[R] AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty /!/ Paper: [https://arxiv.org/pdf/1912.02781.pdf](https://arxiv.org/pdf/1912.02781.pdf)\n\nCode: [https://github.com/google-research/augmix](https://github.com/google-research/augmix)\n\nWe propose AugMix, a data processing technique that mixes augmented images and enforces consistent embeddings of the augmented images, which results in increased robustness and improved uncertainty calibration. AugMix does not require tuning to work correctly, as with random cropping or CutOut, and thus enables plug-and-play data augmentation. AugMix significantly improves robustness and uncertainty measures on challenging -----> image !!!  classification benchmarks, closing the gap between previous methods and the best possible performance by more than half in some cases. With AugMix, we obtain state-of-the-art on ImageNet-C, ImageNet-P and in uncertainty estimation when the train and test distribution do not match.", "link": "https://www.reddit.com/r/MachineLearning/comments/e6s0bz/r_augmix_a_simple_data_processing_method_to/"}, {"autor": "Muunich", "date": "2019-07-31 17:45:43", "content": "[D] Are Graph Convolution Networks (GCNs) with graph constraints possible? /!/  \n\nGreetings,\n\nI am very new to the area of machine learning, so excuse what may appear as a stupid question.\n\nI am interested in using graph convolution networks for semi-supervised classification of nodes in a graph. I would like to impose a graph constraint on the network. I have included an -----> image !!!  to illustrate\n\n&amp;#x200B;\n\n![img](wzmtll88eod31 \" Constraining node classification of a GCN with a graph constraint (connectivity of communities). \")\n\nThe graph constraint I want to impose is that there can only be one communities/region for each label (blue, green, red). In the above figure you can see a sample resulting GCN classification that produces two green communities/regions. I was thinking that perhaps there is a way to create a community/region graph from the resulting GCN algorithm then I compare that with the constraint graph. That comparison, or similarity measure, would be incorporated into some loss function and somehow back propagate that back into to GCN and finally arrive at a classification where there is only one community/region per label.\n\nIs this possible? Is there a better way to approach this (perhaps using an doing some graph embedding or autoencoder/decorder technique)?\n\nThanks for any suggestions or comments :)", "link": "https://www.reddit.com/r/MachineLearning/comments/ckbt5d/d_are_graph_convolution_networks_gcns_with_graph/"}, {"autor": "herzegovina_flor", "date": "2019-12-18 13:16:56", "content": "Generate images from video snippet /!/ Could someone tell me, what is the name of algorithm which generates -----> image !!!  of the object based on video. In particular, I move camera and capture only some part of this object and neural networks figure outs how to merge this particular area and adds this part to image. Thanks", "link": "https://www.reddit.com/r/MachineLearning/comments/ecchk7/generate_images_from_video_snippet/"}, {"autor": "idlab-media", "date": "2019-12-18 13:16:37", "content": "[News] Safe sexting app does not withstand AI /!/ A few weeks ago, the .comdom app was released by Telenet, a large Belgian telecom provider. The app aims to make sexting safer, by overlaying a private -----> picture !!!  with a visible watermark that contains the receiver's name and phone number. As such, a receiver is discouraged to leak nude pictures.\n\n[Example of watermarked image](https://preview.redd.it/q4fremfttd541.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=31e8619cf977d0c595e5a5d43ff71f0eacaec634)\n\nThe .comdom app claims to provide a safer alternative than apps such as Snapchat and Confide, which have functions such as screenshot-proofing and self-destructing messages or images. These functions only provide the illusion of security. For example, it's simple to capture the screen of your smartphone using another camera, and thus cirumventing the screenshot-proofing and self-destruction of the private images. However, we found that the .comdom app only *increases* the illusion of security.\n\nIn a matter of days, we (IDLab-MEDIA from Ghent University) were able to automatically remove these visible watermarks from images. We watermarked thousands of random pictures in the same way that the .comdom app does, and provided those to a simple convolutional neural network with these images. As such, the AI algorithm learns to perform some form of image inpainting.\n\n[Unwatermarked image, using our machine learning algorithm](https://preview.redd.it/ykkf8d5pyd541.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=46158274a580dcb38861c5538b6b007fbd250595)\n\nThus, the developers of the .comdom have underestimated the power of modern AI technologies.\n\nMore info on the website of our research group: [http://media.idlab.ugent.be/2019/12/05/safe-sexting-in-a-world-of-ai/](http://media.idlab.ugent.be/2019/12/05/safe-sexting-in-a-world-of-ai/)", "link": "https://www.reddit.com/r/MachineLearning/comments/ecchg8/news_safe_sexting_app_does_not_withstand_ai/"}, {"autor": "hrithikpuri5", "date": "2019-12-18 09:02:26", "content": "How can I improve the mask RCNN model performance for document images? /!/  I am training a model to extract all the necessary fields from a resume for which I am using mask rcnn to detect the fields in -----> image !!! . I have trained my mask RCNN model for 1000 training samples with 49 fields to extract. I am unable to improve the accuracy. How to improve the model? Is there any pretrained weights that might help?", "link": "https://www.reddit.com/r/MachineLearning/comments/eca6lo/how_can_i_improve_the_mask_rcnn_model_performance/"}, {"autor": "Valuable_Doughnut", "date": "2019-12-18 05:46:59", "content": "Any recommendations for an -----> image !!!  good -----> image !!!  based dataset for learning purposes? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ec8hcd/any_recommendations_for_an_image_good_image_based/"}, {"autor": "godinshackles", "date": "2019-12-17 17:10:17", "content": "-----> Image !!!  detailing AI (Remini). Looking for something similar. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ebykx3/image_detailing_ai_remini_looking_for_something/"}, {"autor": "godinshackles", "date": "2019-12-17 17:00:04", "content": "-----> Image !!!  detailing AI (Remini). Looking for something similar. Does it exist at all? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ebyfup/image_detailing_ai_remini_looking_for_something/"}, {"autor": "david_picard", "date": "2019-09-06 13:57:15", "content": "[R] Metric Learning With HORDE: High-Order Regularizer for Deep Embeddings /!/ Learning an effective similarity measure between -----> image !!!  representations is key to the success of recent advances in visual search tasks (e.g. verification or zero-shot learning). Although the metric learning part is well addressed, this metric is usually computed over the average of the extracted deep features. This representation is then trained to be discriminative. However, these deep features tend to be scattered across the feature space. Consequently, the representations are not robust to outliers, object occlusions, background variations, etc. In this paper, we tackle this scattering problem with a distribution-aware regularization named HORDE. This regularizer enforces visually-close images to have deep features with the same distribution which are well localized in the feature space. We provide a theoretical analysis supporting this regularization effect. We also show the effectiveness of our approach by obtaining state-of-the-art results on 4 well-known datasets (Cub-200-2011, Cars-196, Stanford Online Products and Inshop Clothes Retrieval).\n\nPaper: [https://arxiv.org/abs/1908.02735](https://arxiv.org/abs/1908.02735)\n\nCode: [https://github.com/pierre-jacob/ICCV2019-Horde](https://github.com/pierre-jacob/ICCV2019-Horde)", "link": "https://www.reddit.com/r/MachineLearning/comments/d0h0h9/r_metric_learning_with_horde_highorder/"}, {"autor": "arnokha", "date": "2019-09-05 21:31:53", "content": "[R] Reusing Convolutional Activations from Frame to Frame to Speed up Learning and Inference /!/ **Abstract**: When processing similar frames in succession, we can take advantage of the locality of the convolution operation to reevaluate only portions of the -----> image !!!  that changed from the previous frame. By saving the output of a layer of convolutions and calculating the change from frame to frame, we can reuse previous activations and save computational resources that would otherwise be wasted recalculating convolutions whose outputs we have already observed. This technique can be applied to many domains, such as processing videos from stationary video cameras, studying the  effects of occluding or distorting sections of images, applying convolution to multiple frames of audio or time series data, or playing Atari games. Furthermore, this technique can be applied to speed up both  training and inference.\n\n**Summary of results**: Reusing convolutional activations with CPUs is a good way to save computation for both training and inference, and can serve as a viable alternative to training or doing inference on GPUs in some scenarios. It is likely cheaper, sometimes faster, and it will likely have access to more memory. Unfortunately, there is currently not as much incentive to use this method on GPUs, other than possibly saving power. There are many possible application domains for this technique, and there are likely many ways to improve upon it.\n\nCode and result figures: [https://github.com/arnokha/reusing\\_convolutions](https://github.com/arnokha/reusing_convolutions)\n\nPaper link: \\[Submitted, coming soon\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/d076g4/r_reusing_convolutional_activations_from_frame_to/"}, {"autor": "turnmanh", "date": "2019-09-05 19:54:11", "content": "[D] consequences of converting .tiff to some tf.data supported format in terms of information loss? /!/ Hi, I'm currently working with a dataset of .tiff files and want to feed those to a model using a tf Dataset for performance reasons. However, tf currently does not support loading .tiff files with Dataset.\n\nNow I'm curious how to assess the loss in information if I convert a tiff to e.g. a png. Currently theses tiffs only hold a single -----> image !!!  which for me should not make much a difference.  \n\nWhat would be a good approach to assess this?", "link": "https://www.reddit.com/r/MachineLearning/comments/d05va6/d_consequences_of_converting_tiff_to_some_tfdata/"}, {"autor": "SamuelArzt", "date": "2019-09-05 12:06:39", "content": "[P] Agent Learns to Park a Car using Unity ML-Agents / Deep Reinforcement Learning (PPO) /!/ Video available here (YouTube): [https://youtu.be/VMp6pq6\\_QjI](https://youtu.be/VMp6pq6_QjI)\n\nContext:\n\nAn AI learns to park a car in a parking lot in a 3D physics simulation. The simulation was implemented using Unity's ML-Agents framework ([https://unity3d.com/machine-learning](https://unity3d.com/machine-learning)). The AI consists of a deep Neural Network with 3 hidden layers of 128 neurons each. It is trained with the Proximal Policy Optimization (PPO) algorithm.\n\nThe input of the Neural Network are the readings of eight depth sensors, the cars current speed and position, as well as its relative position to the target. The outputs of the Neural Network are interpreted as engine force, braking force and turning force (continuous values). These outputs can be seen at the top right corner of the zoomed out -----> camera !!!  shots.\n\nThe AI starts off with random behaviour, i.e. the Neural Network is initialized with random weights. It then gradually learns to solve the task by reacting to environment feedback accordingly.\n\nThe AI is rewarded with small positive signals for getting closer to the parking spot, which is outlined in red, and gets a larger reward when it actually reaches the parking spot and stops there. The final reward for reaching the parking spot is dependent on how parallel the car stops in relation to the actual parking position. If the car stops in a 90\u00b0 angle to the actual parking direction for instance, the AI will only be rewarded a very small amount, relative to the amount it would get for stopping completely parallel to the actual direction. The AI is penalized with a negative reward signal, when it either drives further away from the parking spot or if it crashes into any obstacles.\n\n&amp;#x200B;\n\nhttps://i.redd.it/cul3s0nanrk31.png", "link": "https://www.reddit.com/r/MachineLearning/comments/czzx2d/p_agent_learns_to_park_a_car_using_unity_mlagents/"}, {"autor": "JuicyRacoonAnus", "date": "2019-07-25 16:47:48", "content": "[D] Classification: Having the NN know when it doesn't know /!/ So I'm working on: \n\nBuilding an app to classify animals for the visually impaired. Users have an app where they can take a -----> picture !!!  and get the name of the animal. If the camera is being pointed somewhere with no animal, it should predict \"No Animal\". But ALSO, if the camera points at an animal that I don't have in my dataset, I'd like it to predict \"Unrecognized Animal\" so I can store the frame and the manually tag it and feed back to my training set.\n\nHere's what I'm thinking:\n\n- On Data: Have varied images with no animals in them that the network should predict as \"No animal\". \nTake a number of species and have them as \"Unrecognized Animal\" so the network **learns what it doesn't know** (the truth label would be [Animal = 1, Recognized = 0, 0, 0, 0, 0...] vs. the recognized animals e.g. [Animal = 1, Recognized = 1, 0, 0, 1, 0...]). I know the normal approach would be to decide \"Unrecognized\" based on a threshold of the max predicted confidence, but several [papers](https://arxiv.org/pdf/1706.04599.pdf) and empirical evidence show how overconfident nets can be...?). I'm not too sure.\n\n- On loss function: I was just going to use cross entropy for each of the three terms (Animal/No Animal, Recognized/Not Recognized, Animal classification) and have them weighted.\n\nIs my approach in the right direction? I don't know how to express this problem well enough to find good results on google but this must have been solved before right?\n\nThanks to any ideas!", "link": "https://www.reddit.com/r/MachineLearning/comments/chq8pk/d_classification_having_the_nn_know_when_it/"}, {"autor": "learner_30", "date": "2019-07-25 15:37:19", "content": "Extracting multiple features of the object from the -----> image !!!  using deep learning /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/chpbwz/extracting_multiple_features_of_the_object_from/"}, {"autor": "junsukchoe", "date": "2019-07-25 02:00:47", "content": "[R] Our source codes for attention-based dropout layer for weakly supervised object localization (CVPR 2019 Oral) /!/ code: [https://github.com/junsukchoe/ADL](https://github.com/junsukchoe/ADL)\n\nvideo: [https://youtu.be/azevl9\\_w2BE](https://youtu.be/azevl9_w2BE?fbclid=IwAR2m_pulUFWhCpBksOJ5FeVz4BkcpVUzbkbTusP26znX7W9orS40SopXaPI)\n\npaper: [http://openaccess.thecvf.com/content\\_CVPR\\_2019/papers/Choe\\_Attention-Based\\_Dropout\\_Layer\\_for\\_Weakly\\_Supervised\\_Object\\_Localization\\_CVPR\\_2019\\_paper.pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Choe_Attention-Based_Dropout_Layer_for_Weakly_Supervised_Object_Localization_CVPR_2019_paper.pdf)\n\n: We are glad to share our Tensorflow and Pytorch implementations for \"Attention-based Dropout Layer for Weakly Supervised Object Localization. CVPR 2019.\" Although this repository is still in progress, we believe that the current version can also help researchers and practitioners in this field. More detailed instructions will be released soon. Contributions and stars are welcome!\n\n&amp;#x200B;\n\n**Abstract**\n\n&gt;Weakly Supervised Object Localization (WSOL) techniques learn the object location only using -----> image !!! -level labels, without location annotations. A common limitation for these techniques is that they cover only the most discriminative part of the object, not the entire object. To address this problem, we propose an Attention-based Dropout Layer (ADL), which utilizes the self-attention mechanism to process the feature maps of the model. The proposed method is composed of two key components: 1) hiding the most discriminative part from the model for capturing the integral extent of object, and 2) highlighting the informative region for improving the recognition power of the model. Based on extensive experiments, we demonstrate that the proposed method is effective to improve the accuracy of WSOL, achieving a new state-of-the-art localization accuracy in CUB-200-2011 dataset. We also show that the proposed method is much more efficient in terms of both parameter and computation overheads than existing techniques.", "link": "https://www.reddit.com/r/MachineLearning/comments/chhg3j/r_our_source_codes_for_attentionbased_dropout/"}, {"autor": "NNFAK", "date": "2019-07-24 22:00:07", "content": "[P] Lock Picking Detection Using Machine Learning - Audio Classification /!/ I thought you guys might find this interesting. I slightly modified an -----> image !!!  classifier to take audio, then recorded myself sticking keys and picks into locks for 45 minutes respectively. This was in order to create my dataset. I broke those long clips into 5 second segments, which left me with about 1000 clips for training. After 5 minutes of training and 15 epochs, I achieved a little more than 90% accuracy on my training and validation set, which is good enough for a fun project like this.\n\nWhat this means is that I can put my microphone next to a lock, then detect in live time whether that lock is being picked or if a key is being inserted. I can then record the time the event happened and save the audio clip that triggered the event.\n\n\n[Here is a video demonstration of the project.](https://www.youtube.com/watch?v=s5ePte2AE-g)\n\n\n[Here is an article I wrote where you can see the code.](https://evigio.com/post/lock-picking-detection-with-machine-learning-audio-classification)\n\n\nFor anyone that\u2019s into lock picking, I created my training data on Sargent, Corbin Russwin, and Schlage mortise cylinders. I used both single pin picking and raking. I might play around with bumping in the future, if I come back to the project.", "link": "https://www.reddit.com/r/MachineLearning/comments/cheq5w/p_lock_picking_detection_using_machine_learning/"}, {"autor": "Macstevey", "date": "2019-07-24 16:27:46", "content": "[P] Computer Vision Concept for Property Maintenance /!/ Hello all,\n\n&amp;#x200B;\n\nI thought this may be of interest to the community. I've been developing, with the aid of a freelancer so far, a computer vision concept (codename, Analizar) for use in the real estate field (in the high octane world of property maintenance, to be precise). This follows on from a previous post here exploring the possibilities of [-----> image !!!  classification](https://www.reddit.com/r/datascience/comments/9o6tl8/using_-----> image !!! _classification_for_automated_property/). It is a basic demo website which generates automated maintenance information from user submitted photos; [Analizar Computer Vision Concept](https://www.mccormackpartners.co.uk/)\n\n&amp;#x200B;\n\nThe demo is currently trained (to varying degrees of success) to identify the following materials / issues.\n\n&amp;#x200B;\n\n|Asbestos cement sheet roofs|Cladding cut edge corrosion|Brick efflorescence|\n|:-|:-|:-|\n|Asbestos toilet cisterns|Asbestos floor tiles|Roof ponding|\n|Asbestos artex wall coatings|Condensation Mould|Slate roofing|\n|Japanese knotweed|Concrete crazing|Timber weatherboarding|\n\n&amp;#x200B;\n\nhttps://i.redd.it/4lbb6o9w1ac31.png\n\n&amp;#x200B;\n\nYou can download the photos [here](https://drive.google.com/drive/u/2/folders/10e2ynOZ5HQqHQ9BV-nQcAr88VM80qZ2P), or see how it performs with photos you can find online.\n\n&amp;#x200B;\n\nhttps://i.redd.it/88mcx1gx1ac31.png\n\n&amp;#x200B;\n\nFor a (very) brief tech summary, both frontend / backend are deployed on an AWS EC2 instance. Initial training data was sourced from my own records and supplemented with web scraped images. The backend model is a convolutional neural network. The domain knowledge is provided by me. Although this may be some way from a commercial application, it does highlight the potential in property management for computer vision applications. There may even be scope to perform a visual search function (the timber weatherboarding label is included for demonstration of this), perhaps coupled with an augmented reality interface. I also prepared a business model canvas that envisages this concept as part of a wider property maintenance service offering (serving commercial and residential sectors).\n\n&amp;#x200B;\n\nFor a bit more on my background, I've\u00a0been working in this field for nearly 10 years. There's a few things I'm tired of seeing; business friction, inconsistency of service and consumer misinformation. That's partly due to the fragmented nature of contractors\u00a0and advisors, partly due to the snails pace\u00a0of innovation in property maintenance. Now that I have my own [practice](https://www.mccormackpartners.com/), I'm looking to reinvest my cashflow from traditional activities into concepts such as this, and try to help the sector evolve.\n\n&amp;#x200B;\n\nOne thing I lack however is a deep network of tech / machine learning expertise, mentors and business advisors, particularly those who are crossing the chasm between property maintenance and software. That's one reason why I've been lurking on these forums; it gives me a glimpse into the tech startup ecosystem and helps my understanding of software / MVP development.\n\n&amp;#x200B;\n\nSo, to roundup, I would love to hear your thoughts on this. I'm very keen to develop a relationship with this community; to both further my software understanding, and continue the discussion with any interested parties. If this intrigues you (or you think it's a terrible idea!) then please get in touch!\n\n&amp;#x200B;\n\nThanks for reading,\n\nSteven", "link": "https://www.reddit.com/r/MachineLearning/comments/chaend/p_computer_vision_concept_for_property_maintenance/"}, {"autor": "da_g_prof", "date": "2019-03-27 22:41:21", "content": "[D] any way to share model architecture blindly /!/ So we are preparing a paper and model architecture does not fit in the paper. (I am not referring to code just the model designs at this point in time.) \nThe obvious solution is the supplemental but this does not survive the -----> camera !!!  ready (ie not included in the proceedings of the conference)\nA less obvious solution is to share an anonymous github link with a Readme that contains the details.\nAnother solution is to share a figure somewhere (eg figshare). \n\nAny other ideas? \nIs there something more elegant, more persistent and that can retain initial anonymity? \n\nAs an advocate of reproducibility I find supplemental sources on author websites really problematic and ditto for github.com and other commercial or solutions (you never know how long they will be in business for or for what they will change in their strategy). \n\nArxiv is not anonymous...for initial posting prior to camera ready although it is good for posting a collated version of the full paper (and supplemental after) although it creates a problem with citation upkeeping (Scopus and web of science does not count arxiv citations) \n", "link": "https://www.reddit.com/r/MachineLearning/comments/b6axpz/d_any_way_to_share_model_architecture_blindly/"}, {"autor": "scriptcoder43", "date": "2019-03-21 00:42:04", "content": "Does the shape of a tensor for an -----> image !!!  affect the resulting output? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/b3jy2y/does_the_shape_of_a_tensor_for_an_image_affect/"}, {"autor": "iliauk", "date": "2019-03-20 18:38:50", "content": "[D] Pretrained -----> image !!!  embedding models /!/ I've been struggling to find pre-trained models to produce embedding for image similarity. It seems there are quite a few new SOTA results coming out based on different losses and also [sampling strategy](https://arxiv.org/pdf/1706.07567.pdf). \n\nI'm not averse to implementing that sampling strategy and training ResNet50 in PyTorch on the [Stanford Online Products Dataset](http://cvgl.stanford.edu/projects/lifted_struct/) however just surprised it's hard to find a model (whereas face embedding seem much more popular)", "link": "https://www.reddit.com/r/MachineLearning/comments/b3fnwa/d_pretrained_image_embedding_models/"}, {"autor": "tsauri", "date": "2019-01-08 00:48:55", "content": "[D] Is maximum likelihood estimation with generative ConvNet broken? /!/ Since we can do inverse forward-pass on generative models such as Flow, RealNVP, Glow, I think the foundation of probability density estimation of generative images today is *broken*.  \n\n\nI came across this paper \"Do Deep Generative Models Know What They Don't Know?\"([https://arxiv.org/abs/1810.09136](https://arxiv.org/abs/1810.09136)) from Deepmind, stating that out-of-sample scores *higher* in likelihood, whereas logically *it should be smaller*. In their case, a Glow-based model trained on CIFAR scores SVHN images higher than CIFAR images it trained (i.e. model thinks SVHN is more likely comes from CIFAR train samples). The paper didn't nailed out the reason why, it is mostly experiments to verify this phenomenon.  \n\n\nThen I remembered a paper from Bengio's lab, \"Measuring the tendency of CNNs to Learn Surface Statistical Regularities\"  \n[https://arxiv.org/abs/1711.11561](https://arxiv.org/abs/1711.11561), in their experiment, CIFAR -----> image !!!  classifier break down in testing when -----> image !!! s are preprocessed with Fourier filters (remove high frequencies, remove colors range, etc.) while maintaining same look to human eye, suggesting that ConvNets learnt to cheat by considering those statistical regularities unknown to human.  \n\n\nAlso there is work from \"Flow-GAN\" ([https://arxiv.org/abs/1705.08868](https://arxiv.org/abs/1705.08868)), where a Flow model can be trained either on MLE, or GAN, or both. They stated that Flow model trained with GAN setting generates prettier images on current GAN metrics, compared to normal MLE training like Flow did. However, likelihood scores from images generated by GAN training have 100x-1000x worse likelihood scores compared to images generated by MLE training.  \n\n\nWhat do you think? Do you think ConvNet + backprop is the culprit? Or maybe Gaussian prior?   \n\n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/adoipn/d_is_maximum_likelihood_estimation_with/"}, {"autor": "MuchArmadillo", "date": "2019-01-07 20:28:58", "content": "[D] Misunderstood ML/AI Concepts /!/ **Questions / Topics for Discussion**\n1. What are some concepts which many people who use Machine Learning / Artificial Intelligence get \"wrong\" or misunderstand?\n2. What are some good articles (research and \"lay\") on the subject?\n3. What might be some good or interesting \"polling\" questions to tease these sorts of things out?\nNote: I would love and/or encourage anyone to take the result of this discussion and turn it into a peer-reviewed-caliber paper. I've been thinking about it, but realistically, I probably won't do it, so knock yourself out! P.S., would be happy to help.\n**Background**\n&amp;nbsp;\nLast year I gave some talks on bias in data science at two reasonably well-attended conferences largely for data scientists who use ML/AI (one had ~500 attendees, the other over 1200). During my talks, I asked a few questions using \"polleverywhere,\" which falls short of scientifically rigorous, but is interesting nonetheless. An example of one of the questions that stood out for me follows.\n&amp;nbsp;\n**Question:**\n&amp;nbsp;\nWhen the output of the output node a typical, unidirectional, feed-forward deep-leaning -----> image !!!  classifying neural network (the NN) is \"0.734 Horse\" after processing an -----> image !!! . In this case, the **\"0.734 Horse\"** means:\n&amp;nbsp;\n- A. There is **not enough information** to answer the question.\n- B. It is the **confidence interval** of the NN that there is a horse in the image.\n- C. It represents the NN's **confidence estimate** that there is a horse in the image.\n- D. There is a **roughly 74% chance** that there is a horse in the image.\n&amp;nbsp;\nThe results of the clicker poll at the talk directed toward practicing, publishing data scientists, out of 61 respondents:\n- 1 (~1.5%) chose A\n- 17 (~28%) chose B\n- 13 (~21%) chose C\n- 30 (~49%) chose D\n&amp;nbsp;\nAt the second talk with a slightly broader audience (12 people chose \"E. I don't know\", which I exclude because there were some non-practitioners), of 97 respondents, I got:\n- 0 (0%) chose A\n- 21 (~22%) chose B\n- 23 (~24%) chose C\n- 53 (~55%) chose D\n&amp;nbsp;\nOne attendee -- likely the only one who answered A -- persuaded me that A is reasonably acceptable for a number of reasons. I think it's a stretch, but I'd give credit on a quiz after such an animated defense. However, C, is the only true answer because it is a confidence estimate, no matter the state of the NN. Even an untrained NN will produce a random output which is a confidence estimate -- a very poor one, but one nonetheless. B is a common mistake that even I sometimes make -- I'll say confidence interval when I mean confidence estimate and vice-versa because my brain does that to me, so confounding the two did not surprise or alarm me. D is concerning, however.\n&amp;nbsp;\n**Note 1:** It was pointed out to me at the last talk that some on-line resources, including Wikipedia, call the output a probability; however, I asked the audience to send me the links but never received any such links.\n&amp;nbsp;\n**Note 2:** Understanding not-so-subtle differences such as those above is very important to understanding some of the concerning consequences to biased algorithms in ML and AI.\n&amp;nbsp;\nWe know that the confidence estimate can be VERY different from the actual probabilities that classification is correct, and, indeed over the past several years we have had several papers underscoring this; however, the fact remains that many people using ML/AI misinterpret the results. Most alarming to me is the number of healthcare publications that are being churned out from top research institutions that make this mistake. If the research paper suggests that a system which classifies a scan as \"0.734 cancerous lesion\" means there is a 73.4% chance the lesion is cancerous, this can have dangerous implications. The same goes for things such as the scary-as-can-be COMPAS (\" Correctional Offender Management Profiling for Alternative Sanctions \") system.", "link": "https://www.reddit.com/r/MachineLearning/comments/adlu1s/d_misunderstood_mlai_concepts/"}, {"autor": "neuralPr0cess0r", "date": "2019-01-07 15:35:00", "content": "[P] A video describing a method for high resolution -----> image !!!  generator, without GAN /!/ [https://www.youtube.com/watch?v=IdgEBYd5FfU&amp;feature=youtu.be](https://www.youtube.com/watch?v=IdgEBYd5FfU&amp;feature=youtu.be)\n\n&amp;#x200B;\n\nThis video describes a method to train a neural network to synthesize plausable, high resolution imagary without a GAN.  This work is a novel combination of various existing techniques:  \n\nInspiration: Mean Teacher \n\nPerceptual Losses for Real-Time Style Transfer Lines to image ... etc  \n\n&amp;#x200B;\n\nPlease forgive my poor oration! This is for fun. I am not an academic. :)  \n\n&amp;#x200B;\n\nThe basic idea is to utilize a U-Net, modified perceptual loss (pearson instead of MAE), learned basis functions, and  \"mean teacher\" training in order to synthesize images of high quality without the normal troubles of training a GAN.", "link": "https://www.reddit.com/r/MachineLearning/comments/adirr1/p_a_video_describing_a_method_for_high_resolution/"}, {"autor": "spacevstab", "date": "2019-01-06 10:01:35", "content": "[D] Convolution Sequence to Sequence for HTR Pytorch Implementation Issues /!/ Hello, I am trying to implement a sequence to sequence-based model for handwritten text recognition. The paper is [here](https://arxiv.org/pdf/1806.00578.pdf) (SCAN) which primarily uses Convolution Sequence to Sequence \\[Gehring\\]. I am using IAM Dataset which has almost 7000 training -----> image !!!  and text pairs. I have modelled in almost the same as the SCAN paper indicates but there are some issues with my implementation. The model is giving very poor accuracy on test set. After the first epoch, the loss value doesn\u2019t decreases but revolves around a range of value. This is my very first paper implementation on Pytorch. I am linking the Google Colab [notebook](https://colab.research.google.com/drive/1PDa6o4TZC0s_N8zaVytyCZhL1xaDE-dz) here, please help me out on this issue.\n\nJust to clarify, the images are scaled to (32, ) maintaining aspect ratio. The images are batched and padded since all images have different widths. The image batch is passed through the Convolution Extraction layer which outputs feature in shape \\[batch\\_size, channel, seq\\_len\\]. The encoder is fed with above output with sequence expanding across seq\\_len. The text are also batched and padded with token with and token at start and end. While training, the decoder initially is fed with token and further the decoder outputs are fed back into decoder as decoder\\_inputs. I hope this somehow clarifies what my approach is.\n\nLet me know of any suggestions or improvements.  \nThank you", "link": "https://www.reddit.com/r/MachineLearning/comments/ad4drw/d_convolution_sequence_to_sequence_for_htr/"}, {"autor": "temptai", "date": "2019-01-06 01:46:40", "content": "The future is now. We're bringing together many open-sourced -----> image !!!  manipulation libraries to create infinitely customizable adult content.", "link": "https://www.reddit.com/r/MachineLearning/comments/ad0yfn/the_future_is_now_were_bringing_together_many/"}, {"autor": "gourxb", "date": "2019-01-05 19:20:28", "content": "[D] Can I find a pre-trained Deep Neural Network based -----> image !!!  de-noising model? /!/ I am looking for a fast/reliable deep neural network based pre trained model to remove artifacts/grains from a set of images that I have.\n\nI have tried OpenCV \\[1\\] as well as scikit-image \\[2\\] denoising techniques. I am not satisfied with the result. \n\n&amp;#x200B;\n\n\\[1\\] [https://docs.opencv.org/3.0-beta/modules/photo/doc/denoising.html](https://docs.opencv.org/3.0-beta/modules/photo/doc/denoising.html) \n\n\\[2\\] [http://scikit-image.org/docs/dev/auto\\_examples/filters/plot\\_denoise.html](http://scikit-image.org/docs/dev/auto_examples/filters/plot_denoise.html) ", "link": "https://www.reddit.com/r/MachineLearning/comments/acx7qn/d_can_i_find_a_pretrained_deep_neural_network/"}, {"autor": "Weihua99", "date": "2019-01-05 18:32:24", "content": "This software lets you create labeled -----> image !!!  datasets from videos of objects using Re3", "link": "https://www.reddit.com/r/MachineLearning/comments/acwqp3/this_software_lets_you_create_labeled_image/"}, {"autor": "m_baas", "date": "2019-07-08 19:14:12", "content": "[P] Pretrained pytorch resnet models for anime tag estimation /!/ Hi\n\nI trained some Resnet models to estimate tags for anime images. \n\nThere has been a lot of cool anime-related projects recently, such as [DeepDanbooru](https://www.reddit.com/r/MachineLearning/comments/akbc11/p_tag_estimation_for_animestyle_girl_image/) and some other cool work with [anime face generation](https://www.gwern.net/Faces), however most use tensorflow and so I wanted a nice pretrained pytorch model to use for transfer learning with downstream tasks. \n\n**A TL;DR of the networks:**\n\n* Framework: Pytorch\n* Network: adapted resnet50, resnet34, and resnet18 models\n* Training time : about 12 days (using mixed-precision training and Nvidia V100)\n* Dataset: [Danbooru2018 dataset](https://gwern.net/Danbooru2018), with networks trained to predict top tags of each -----> image !!! \n\n[An example output of the resnet50 model](https://github.com/RF5/danbooru-pretrained/blob/master/img/danbooru_resnet2.png)\n\nIf you have pytorch, you can load up the model and predict on new images in only a few lines using pytorch hub. [See this post for a quick getting started](https://rf5.github.io/2019/07/08/danbuuro-pretrained.html)\n\nI hope some people find this useful, and that it saves some people training time with other tasks.\n\nFor more info on the networks, training, data preparation, and performance metrics of each network, please have a look at the [project page](https://rf5.github.io/2019/07/08/danbuuro-pretrained.html), or just ask :). Any comments or feedback is nice.", "link": "https://www.reddit.com/r/MachineLearning/comments/caphhp/p_pretrained_pytorch_resnet_models_for_anime_tag/"}, {"autor": "Mxbonn", "date": "2019-07-08 12:30:40", "content": "[P] Python library to work with the Visual Wake Words Dataset. /!/ Recently Google published a paper introducing the [Visual Wake Words Dataset](https://arxiv.org/abs/1906.05721).\n\n&gt;Currently vision models are benchmarked on the CIFAR10 or ImageNet  datasets both of which are restricted in terms of benchmarking the model accuracy and the memory costs for the common low-complexity microcontroller use-case. We present a new dataset, Visual Wake Words, that represents a common microcontroller vision use-case of identifying whether a person is present in the -----> image !!!  or not, The proposed dataset is derived from the publicly available COCO dataset, and provides a realistic benchmark for tiny vision models.\n\nAs the dataset is derived from the COCO dataset I created a library that inherits from the pycocotools libary and that can be used in a similar fashion on the Visual Wake Words Dataset.\n\nI've also included a Pytorch Dataset class that can be used like any VisionDataset.\n\n[https://github.com/Mxbonn/visualwakewords](https://github.com/Mxbonn/visualwakewords) \n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://i.redd.it/xxeoua9xp2931.png\n\nhttps://i.redd.it/3lyaebaxp2931.png", "link": "https://www.reddit.com/r/MachineLearning/comments/cakfg0/p_python_library_to_work_with_the_visual_wake/"}, {"autor": "Rbent98", "date": "2019-07-06 23:53:52", "content": "How difficult would it be to use -----> image !!!  recognition to say what ingredients is in your chipotle bowl ? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ca0ftw/how_difficult_would_it_be_to_use_image/"}, {"autor": "Chiplusplus", "date": "2019-10-22 09:46:12", "content": "How to extract the different sections of a pdf document using -----> image !!!  processing? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/dlfl8d/how_to_extract_the_different_sections_of_a_pdf/"}, {"autor": "h4k1m0u", "date": "2019-10-21 23:16:20", "content": "-----> Image !!!  semantic segmentation without deep learning /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/dl8rb0/image_semantic_segmentation_without_deep_learning/"}, {"autor": "johnolafenwa", "date": "2019-10-21 14:40:30", "content": "-----> Image !!!  Classification on Android with Pytorch Mobile", "link": "https://www.reddit.com/r/MachineLearning/comments/dl1i2l/image_classification_on_android_with_pytorch/"}, {"autor": "sadeepj", "date": "2019-11-30 13:44:24", "content": "[P] CRF-RNN PyTorch version (deep learning based semantic -----> image !!!  segmentation)", "link": "https://www.reddit.com/r/MachineLearning/comments/e3wgl2/p_crfrnn_pytorch_version_deep_learning_based/"}, {"autor": "davidblyrics", "date": "2019-11-29 19:53:44", "content": "[D] What is SOTA for -----> image !!!  manipulation detection? /!/ What is sota for image manipulation detection? I have not seen much work in this field happening, does anybody know the reason for that? The only recent paper have seen is from CVPR2018 but I thought there would be more work being done? \n\nThanks for your help!", "link": "https://www.reddit.com/r/MachineLearning/comments/e3jv3j/d_what_is_sota_for_image_manipulation_detection/"}, {"autor": "elcric_krej", "date": "2019-11-29 12:26:01", "content": "[D] What are some problems types where ML could be applied \"in theory\" but it's outside of practical reach ? /!/ It might be an overly-simplified view of the field, but it seems to me that a lot of the ML boom of this decade is due to the appearance of hardware+architectures that were able to tackle a set of problems which were easy in terms of data gathering and \"pretty deterministic\" (that is to say, based on our human abilities to tackle them, we can be pretty certain there are usually no latent variables which are necessary to solve  the problem correctly), things like bounding boxes, -----> image !!!  classification and translation.\n\nOn the other hand these new methods have hardly put a dent in how most people approach mostly \"pretty non-deterministic\" issues (e.g. stock trading or risk analysis), where practice and intuition shows that there's simply not sufficient \"easy\" data that can make a reliable prediction.\n\nIt seems to me that most efforts right now are focused on either \"productizing\" the gains that were had on text and image problems (e.g. getting that 0.x% extra accuracy and 0.y% extra specificity that makes them practical to use in fields with low error margins) or getting algorithms that can better communicate the uncertainty of non-deterministic datasets (e.g. Bayesian/Probabilistic NNs).\n\nHowever, it's not obvious to me what the next set of problems similar to images and text will hit the chopping block, or if there is such a set of problems. \n\nI've seen some interesting research (e.g. Alpha Fold) and some huge failures (e.g. that earthquake prediction publish in Nature that was worse than a linear regression) in the realm of scientific problems where we \"seem to\" have sufficient data but lack the mathematical frameworks to gain insights from the data. I think anything related to complex molecular dynamics in a \"static\" environment is a pretty good example, since in theory the starting state should allow us insight into any state at a later time T, but in practice this is often too computationally expensive and/or too complex to formalize in a way that is fitting for our current models. However, there doesn't seem to be near that amount of adoption, excitement or novel ideas coming from this class of problems.\n\nSo I wonder, what would you guys think would be the next \"category\" of problems where, conceptually, ML techniques could be applied without too much of a data-gathering barrier, yet the hardware+knowledge combination of current humans is yet to evolve to a point where they are feasible.", "link": "https://www.reddit.com/r/MachineLearning/comments/e3e4cv/d_what_are_some_problems_types_where_ml_could_be/"}, {"autor": "youali", "date": "2019-11-29 10:16:48", "content": "[R] Faster AutoAugment: Learning Augmentation Strategies using Backpropagation /!/ Paper: https://arxiv.org/abs/1911.06987\n\n\nAbstract: \nData augmentation methods are indispensable heuristics to boost the performance of deep neural networks, especially in -----> image !!!  recognition tasks. Recently, several studies have shown that augmentation strategies found by search algorithms outperform hand-made strategies. Such methods employ black-box search algorithms over image transformations with continuous or discrete parameters and require a long time to obtain better strategies. In this paper, we propose a differentiable policy search pipeline for data augmentation, which is much faster than previous methods. We introduce approximate gradients for several transformation operations with discrete parameters as well as the differentiable mechanism for selecting operations. As the objective of training, we minimize the distance between the distributions of augmented data and the original data, which can be differentiated. We show that our method, Faster AutoAugment, achieves significantly faster searching than prior work without a performance drop.", "link": "https://www.reddit.com/r/MachineLearning/comments/e3czou/r_faster_autoaugment_learning_augmentation/"}, {"autor": "siddarth2947", "date": "2019-11-29 08:04:21", "content": "[D] Five major deep learning papers by Geoff Hinton did not cite similar earlier work by Jurgen Schmidhuber /!/ still milking Jurgen's very dense [inaugural tweet](https://twitter.com/SchmidhuberAI) about their [annus mirabilis 1990-1991](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html) with Sepp Hochreiter and others, 2 of its 21 sections already made for nice reddit threads, section 5 [Jurgen really had GANs in 1990](https://www.reddit.com/r/MachineLearning/comments/djju8a/d_jurgen_schmidhuber_really_had_gans_in_1990/) and section 19 [DanNet, the CUDA CNN of Dan Ciresan in Jurgen's team, won 4 -----> image !!!  recognition challenges prior to AlexNet](https://www.reddit.com/r/MachineLearning/comments/dwnuwh/d_dannet_the_cuda_cnn_of_dan_ciresan_in_jurgen/), but these are not the juiciest parts of the blog post\n\ninstead look at sections 1 2 8 9 10 where Jurgen mentions work they did long before Geoff, who did not cite, as confirmed by studying the references, at first glance it's not obvious, it's hidden, one has to work backwards from the references\n\n[section 1, First Very Deep NNs, Based on Unsupervised Pre-Training (1991)](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html#Sec.%201), Jurgen \"facilitated supervised learning in deep RNNs by unsupervised pre-training of a hierarchical stack of RNNs\" and soon was able to \"solve previously unsolvable Very Deep Learning tasks of depth &gt; 1000,\" he mentions reference [UN4] which is actually Geoff's later similar work:\n\n&gt; More than a decade after this work [UN1], a similar method for more limited feedforward NNs (FNNs) was published, facilitating supervised learning by unsupervised pre-training of stacks of FNNs called Deep Belief Networks (DBNs) [UN4]. The 2006 justification was essentially the one I used in the early 1990s for my RNN stack: each higher level tries to reduce the description length (or negative log probability) of the data representation in the level below. \n\nback then unsupervised pre-training was a big deal, today it's not so important any more, see [section 19, From Unsupervised Pre-Training to Pure Supervised Learning (1991-95 and 2006-11)](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html#Sec.%2019) \n\n[section 2, Compressing / Distilling one Neural Net into Another (1991)](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html#Sec.%202), Jurgen also trained \"a student NN to imitate the behavior of the teacher NN,\" briefly referring to Geoff's much later similar work [DIST2]:\n\n&gt; I called this \"collapsing\" or \"compressing\" the behavior of one net into another. Today, this is widely used, and also called \"distilling\" [DIST2] or \"cloning\" the behavior of a teacher net into a student net. \n\n[section 9, Learning Sequential Attention with NNs (1990)](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html#Sec.%209), Jurgen \"had both of the now common types of neural sequential attention: end-to-end-differentiable \"soft\" attention (in latent space) through multiplicative units within NNs [FAST2](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.1885&amp;rep=rep1&amp;type=pdf), and \"hard\" attention (in observation space) in the context of Reinforcement Learning (RL) [ATT0](http://people.idsia.ch/~juergen/FKI-128-90ocr.pdf) [ATT1],\" the blog has a statement about Geoff's later similar work [ATT3](https://papers.nips.cc/paper/4089-learning-to-combine-foveal-glimpses-with-a-third-order-boltzmann-machine.pdf) which I find both funny and sad: \n\n&gt; My overview paper for CMSS 1990 [ATT2] summarised in Section 5 our early work on attention, to my knowledge the first implemented neural system for combining glimpses that jointly trains a recognition &amp; prediction component with an attentional component (the fixation controller). Two decades later, the reviewer of my 1990 paper wrote about his own work as second author of a related paper [ATT3]: \"To our knowledge, this is the first implemented system for combining glimpses that jointly trains a recognition component ... with an attentional component (the fixation controller).\" \n\nsimilar in [section 10, Hierarchical Reinforcement Learning (1990)](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html#Sec.%2010), Jurgen introduced HRL \"with end-to-end differentiable NN-based subgoal generators [HRL0](http://people.idsia.ch/~juergen/FKI-129-90ocr.pdf), also with recurrent NNs that learn to generate sequences of subgoals [HRL1] [HRL2],\" referring to Geoff's later work [HRL3](https://papers.nips.cc/paper/714-feudal-reinforcement-learning.pdf):  \n\n&gt; Soon afterwards, others also started publishing on HRL. For example, the reviewer of our reference [ATT2] (which summarised in Section 6 our early work on HRL) was last author of ref [HRL3]\n\n[section 8, End-To-End-Differentiable Fast Weights: NNs Learn to Program NNs (1991)](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html#Sec.%208), Jurgen published a network \"that learns by gradient descent to quickly manipulate the fast weight storage\" of another network, and \"active control of fast weights through 2D tensors or outer product updates [FAST2](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.1885&amp;rep=rep1&amp;type=pdf),\" dryly referring to [FAST4a](https://papers.nips.cc/paper/6057-using-fast-weights-to-attend-to-the-recent-past.pdf) which happens to be Geoff's later similar paper: \n\n&gt; A quarter century later, others followed this approach [FAST4a]\n\nit's really true, Geoff did not cite Jurgen in any of these similar papers, and what's kinda crazy, he was editor of Jurgen's 1990 paper [ATT2](http://people.idsia.ch/~juergen/hinton-rev.pdf) summarising both attention learning and hierarchical RL, then later he published closely related work, sections 9, 10, but he did not cite \n\nJurgen also [famously complained](http://people.idsia.ch/~juergen/deep-learning-conspiracy.html) that Geoff's deep learning survey in Nature neither mentions the inventors of backpropagation (1960-1970) nor \"the father of deep learning, Alexey Grigorevich Ivakhnenko, who published the first general, working learning algorithms for deep networks\" in 1965 \n\napart from the early pioneers in the 60s and 70s, like Ivaknenko and Fukushima, most of the big deep learning concepts stem from Jurgen's team with Sepp and Alex and Dan and others: unsupervised pre-training of deep networks, artificial curiosity and GANs, vanishing gradients, LSTM for language processing and speech and everything, distilling networks, attention learning, CUDA CNNs that win vision contests, deep nets with 100+ layers, metalearning, plus theoretical work on optimal AGI and Godel Machine", "link": "https://www.reddit.com/r/MachineLearning/comments/e3buo3/d_five_major_deep_learning_papers_by_geoff_hinton/"}, {"autor": "iskulll", "date": "2019-11-28 17:17:33", "content": "A little help with MLP and -----> Image !!!  recognition in Python /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/e30yug/a_little_help_with_mlp_and_image_recognition_in/"}, {"autor": "emreqemal", "date": "2019-07-16 19:38:38", "content": "[D] -----> Image !!!  recognition on server /!/ Hello!\n\nI'm trying to create an image recognition functionality, where the user takes photos and sends it to my server and I check if the photo contains an image I previously registered to my server.\n\nI found out about a paper and its implementation\n\n[https://github.com/cl199443/Deep-Semantic-Feature-Matching](https://github.com/cl199443/Deep-Semantic-Feature-Matching)\n\nI currently think feature-matching is what I look for.\n\nQuestion it, is there a automatic way to train a feature network to also store feature patterns it's seen before and matched to target images?\n\nOr do you have any other workflow suggestions to tackle this problem? Thanks", "link": "https://www.reddit.com/r/MachineLearning/comments/ce1wdq/d_image_recognition_on_server/"}, {"autor": "ououwen", "date": "2019-07-16 18:01:52", "content": "[P] TensorFlow DICOM Medical Imaging Decoder Operation /!/ Hello, I wanted to share something our team has been working on for a while. I work on an early stage radiology imaging company where we have a blessing and curse of having too much medical imaging data. Something we found internally useful to build was a DICOM Decoder Op for TensorFlow. We are making this available open-source here: [https://github.com/gradienthealth/gradient\\_decode\\_dicom](https://github.com/gradienthealth/gradient_decode_dicom).  \n\n  \nDICOM is an extremely broad standard, so we try to cover the 90% case of -----> image !!!  formats (PNG, TIFF, BMP, JPEG, JPEG2000). We also support multi-frame/multi-frame color images. Try images found here: [https://barre.dev/medical/samples/](https://barre.dev/medical/samples/). In the case an unsupported format is found, an empty Tensor is returned which can be filtered out. Reading the files directly off of bucket storage has allowed us to prevent data duplication of .dcm data (a single CT can be 300MB). You can play with the op in this Colab notebook: [https://colab.research.google.com/drive/1MdjXN3XkYs\\_mSyVtdRK7zaCbzkjGub\\_B](https://colab.research.google.com/drive/1MdjXN3XkYs_mSyVtdRK7zaCbzkjGub_B) \n\n  \nWe firmly believe that having open-source resources in healthcare is what will enable its use in practice, not AI trade secrets. We plan on opening more of our work in the future. DM me if there is interest in contributing to upcoming toolkits (the next one we are thinking of creating is an operation to decrypt+decompress gzip files). Also, lmk if there is interest in working with our dataset (\\~300M DICOMs + notes). **The goal of these project collaborations is that they are ultimately open-sourced.** \n\n  \nAnyway, give the operation a try. If there are problems with loading a file of interest, please make an issue on GitHub. Right now only Linux based systems are supported, and a Dockerfile example will be coming soon.", "link": "https://www.reddit.com/r/MachineLearning/comments/ce0lqk/p_tensorflow_dicom_medical_imaging_decoder/"}, {"autor": "Bulbasaur2015", "date": "2019-07-16 14:54:18", "content": "[D] How does Calorie Mama work? Python implementation /!/ How do you train a model to learn nutritional information from a -----> picture !!!  of a food, or drink? Maybe you need a start with an image classifier, like how the app Calorie Mama works. Despite reports that nutritional recognition is[inaccurate and unreliable](https://www.theverge.com/2015/6/2/8707851/google-calories-food-photos-im2calories), I'm still interested in the (python) implementation of ID'ing the food, getting calories, nutrients, vitamins etc.", "link": "https://www.reddit.com/r/MachineLearning/comments/cdy2wv/d_how_does_calorie_mama_work_python_implementation/"}, {"autor": "iyaja", "date": "2019-07-16 13:51:49", "content": "[P] Training an Image Classifier Using Modern Best Practices /!/ Hi everyone. I recently trained an -----> image !!!  classifier on a Japanese character dataset called KMNIST and achieved 97% validation accuracy within a few minutes using a suite of modern deep learning tools and techniques.\n\nI explain all the techniques [my blog post](https://www.wandb.com/articles/how-to-teach-your-computer-japanese), published on the weights and biases site.\n\nI kept this article relatively short and straightforward, so it should be quite accessible to beginners and is likely to improve the performance of your deep learning models.\n\nPrimarily, I used the learning rate finder and 1cycle learning rate policy taught by Jeremy Howard in the [fast.ai practical deep learning for coders course](https://course.fast.ai/) along with visualization and monitoring tools from a library called [Weights &amp; Biases](https://www.wandb.com/). \n\nHope you enjoy (and your poor GPU that's been computing for days) it!", "link": "https://www.reddit.com/r/MachineLearning/comments/cdxb7y/p_training_an_image_classifier_using_modern_best/"}, {"autor": "wymco", "date": "2019-07-15 18:56:15", "content": "-----> Image !!!  classification - where to start? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/cdlpvv/image_classification_where_to_start/"}, {"autor": "atomlib_com", "date": "2019-07-15 13:16:27", "content": "AI-based -----> photo !!!  restoration: defects removal, inpainting, colorization", "link": "https://www.reddit.com/r/MachineLearning/comments/cdhci2/aibased_photo_restoration_defects_removal/"}, {"autor": "ironhide23586", "date": "2019-07-15 06:59:05", "content": "RoomNet - A CNN to classify pictures of different rooms of a house/apartment with 88.9 % accuracy /!/ &amp;#x200B;\n\nhttps://i.redd.it/lf1xaqo40fa31.png\n\nCodebase at  [https://github.com/ironhide23586/RoomNet](https://github.com/ironhide23586/RoomNet) \n\nThis is a custom neural net I designed to classify an input -----> image !!!  in to one of the following 6 classes (in order of their class IDs) -\n\n* Backyard\n* Bathroom\n* Bedroom\n* Frontyard\n* Kitchen\n* LivingRoom", "link": "https://www.reddit.com/r/MachineLearning/comments/cde695/roomnet_a_cnn_to_classify_pictures_of_different/"}, {"autor": "nile6499", "date": "2019-07-15 01:43:36", "content": "Any method which connects edges in a binary -----> image !!! ? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/cdba1e/any_method_which_connects_edges_in_a_binary_image/"}, {"autor": "PuzzledProgrammer3", "date": "2019-08-31 01:34:21", "content": "[P] Mona Lisa Stylegan FUNIT SPADE deepfake face swap video from a single -----> image !!!  no training for any face pair /!/ result video: [https://twitter.com/roadrunning01/status/1167558507547320320?s=20](https://twitter.com/roadrunning01/status/1167558507547320320?s=20)\n\ngithub repo: [https://github.com/shaoanlu/fewshot-face-translation-GAN](https://github.com/shaoanlu/fewshot-face-translation-GAN)", "link": "https://www.reddit.com/r/MachineLearning/comments/cxpzui/p_mona_lisa_stylegan_funit_spade_deepfake_face/"}, {"autor": "arkady_red", "date": "2019-08-29 09:25:01", "content": "[D] Inter-annotator agreement: how does it work for computer vision? /!/ We have a dataset which we need to annotate: the task is object detection, thus we need to create bounding boxes. We're going to use\n\nhttps://github.com/wkentaro/labelme\n\nBut I'mm open to alternative suggestions, if you think there are better tools. Since the dataset is very large and very confidential, we're going to annotate it in-house. I've heard of people trying to estimate the error due to subjectivity/mistakes in human annotation, but I don't quite understand how it works. Let's suppose for the sake of example that I have 900 images and 3 annotators. If I understand correctly, rather than partitioning the dataset in three subsets of size 300 and sending each subset to a different annotator, I divide it in three datasets of size, say, 330, which means that some images will necessarily be annotated by multiple users.\n\nI don't understand how to use these multiple annotations in practice, though: when I prepare my dataset, for each -----> image !!!  which has been annotated by multiple users  I'll have to choose which annotations to use. It's not like I can have three different bounding boxes (three different ground truths) for each object in the image. So, how does it work in practice?", "link": "https://www.reddit.com/r/MachineLearning/comments/cwyh33/d_interannotator_agreement_how_does_it_work_for/"}, {"autor": "avgMLenthusiast", "date": "2019-08-29 05:49:28", "content": "Applying a filter over RGB -----> image !!!  or a feature map with d=32 (for example) /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/cwwq3c/applying_a_filter_over_rgb_image_or_a_feature_map/"}, {"autor": "ChocoMoi", "date": "2019-02-11 13:32:09", "content": "[R] [ICLR 2019] Unsupervised Adversarial Image Reconstruction /!/ Open review: [https://openreview.net/forum?id=BJg4Z3RqF7](https://openreview.net/forum?id=BJg4Z3RqF7)\n\n&amp;#x200B;\n\n**Abstract:** We address the problem of recovering an underlying signal from lossy, inaccurate observations in an unsupervised setting. Typically, we consider situations where there is little to no background knowledge on the structure of the underlying signal, no access to signal-measurement pairs, nor even unpaired signal-measurement data. The only available information is provided by the observations and the measurement process statistics. We cast the problem as finding the \\\\textit{maximum a posteriori} estimate of the signal given each measurement, and propose a general framework for the reconstruction problem. We use a formulation of generative adversarial networks, where the generator takes as input a corrupted observation in order to produce realistic reconstructions, and add a penalty term tying the reconstruction to the associated observation. We evaluate our reconstructions on several -----> image !!!  datasets with different types of corruptions. The proposed approach yields better results than alternative baselines, and comparable performance with model variants trained with additional supervision.", "link": "https://www.reddit.com/r/MachineLearning/comments/apgacq/r_iclr_2019_unsupervised_adversarial_image/"}, {"autor": "JasonJost", "date": "2019-02-10 05:05:39", "content": "Are there any apps that scan the internet for a similar face from uploading a -----> photo !!! ? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ap0y4n/are_there_any_apps_that_scan_the_internet_for_a/"}, {"autor": "rick854", "date": "2019-12-21 14:13:55", "content": "[D] Accuracy metric in LSTM not considers time offset for multivariate time-series classification? /!/ So this is a kind of complex question, so I hope I formulate it good enough.\n\n&amp;#x200B;\n\nI have a human activity detection task that binary classifies if a user does a specific action or not. For me, it is enough if the system detects the action \\*\\*within 3 seconds\\*\\* after it initially happened.\n\n&amp;#x200B;\n\nI am using smartphone sensor data with a frequency of 50Hz, which I then combine with a windowing approach with windows of 1sec length and 0.5sec overlap (i.e. I calculate statistics such as \\`mean\\` or \\`std\\` for each sensor data for a set time of 1 sec, store these in \"windows\" and overlap these \"windows\" by 0.5sec).\n\n&amp;#x200B;\n\nFor LSTM to learn longterm data I use 5 such windows as timesteps (which would represent 3sec of data) and shift each timestep by one window. So the shape of the data fed to the model is:\n\n\\[13000 instances, 5 timesteps, 21 features\\]\n\n&amp;#x200B;\n\nNow let's consider the following case of a finished classification of such a model where all of the large squares in the -----> image !!!  are labeled as an event, but only some are classified as such:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/bcscy38nvz541.png?width=1976&amp;format=png&amp;auto=webp&amp;s=034919109a9daee0d7dc6bc21ee1851c08dfdda1\n\n&amp;#x200B;\n\nAs I understand it, LSTM using the \\`binary\\_crossentropy\\` loss function and \\`accuracy\\` as a metric in Keras will evaluate the results in a way that the above accuracy would be 2 out of 5 correctly classified instances. However, the accuracy, in this case, should be 100% because my goal is to detect the event within 3 sec, so as long one of these 5 timesteps are labeled as the event I should get 100% accuracy.\n\n&amp;#x200B;\n\n\\*\\*So my questions are:\\*\\*\n\n&amp;#x200B;\n\n 1. Do I understand the metric correctly and is this a problem for my current goal?\n\n 2. If yes, how could I overcome this evaluation problem?", "link": "https://www.reddit.com/r/MachineLearning/comments/edqcqm/d_accuracy_metric_in_lstm_not_considers_time/"}, {"autor": "Boozybrain", "date": "2019-12-21 12:13:25", "content": "[D] I have an idea that I think is important and possibly commercially viable but I don't have the time or domain expertise to make it happen /!/ Privacy is becoming more and more important, especially with the availability of facial detection.  For people who insist on using Facebook and Instagram, I've had the idea to apply adversarial noise to people's faces to sanitize them before they're uploaded online.  The problem is how to generalize the noise so it's effective against as many models as possible, and I imagine if that's even possible that it would distort the -----> image !!!  too much to be worth using. Another question I've had is are the models even available to test against? I know black box attacks exist, but how effective are they? And how hard is it to generate an effective attack if the weights are changed?\n\nLike I said, I don't have the expertise to fully follow through on this and I'd love if it actually came to fruition, or maybe the field isn't quite there yet, but I wanted to put it out there just in case.", "link": "https://www.reddit.com/r/MachineLearning/comments/edp9mu/d_i_have_an_idea_that_i_think_is_important_and/"}, {"autor": "mrpoopybutthole1262", "date": "2019-12-21 01:41:54", "content": "[D] How hard is it to train a GAN? /!/ Hi all, im planning to train a GAN with a 512X512 -----> image !!!  in colour.\n\nIs it realistic to train this on a regular desktop machine? How long will this take?\n\n&amp;#x200B;\n\nThanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/edjfqq/d_how_hard_is_it_to_train_a_gan/"}, {"autor": "iamnotabotiamtob", "date": "2019-12-20 17:51:29", "content": "3D -----> image !!!  data as sequence of 2D -----> image !!! s /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/eddh0p/3d_image_data_as_sequence_of_2d_images/"}, {"autor": "sergbur", "date": "2019-12-20 14:04:57", "content": "PySS3: A python package implementing a novel text classifier with visualization tools for Explainable AI /!/ A recently created Python package that may be really helpful for those working on NLP or Text Mining problems.\n\nGithub: [https://github.com/sergioburdisso/pyss3](https://github.com/sergioburdisso/pyss3)\n\nDocumentation: [https://pyss3.readthedocs.io/en/latest/](https://pyss3.readthedocs.io/en/latest/)\n\nPaper preprint: [https://arxiv.org/abs/1912.09322](https://arxiv.org/abs/1912.09322)\n\n&amp;#x200B;\n\n*Information from the repo:*\n\n&amp;#x200B;\n\nhttps://preview.redd.it/4n01am3oos541.png?width=558&amp;format=png&amp;auto=webp&amp;s=0ae65769c1986ccd55324f836fbbffa4d3d8689d\n\n&amp;#x200B;\n\n# A python package implementing a novel text classifier with visualization tools for Explainable AI\n\nThe SS3 text classifier is a novel supervised machine learning model for text classification. SS3 was originally introduced in Section 3 of the paper [*\"A text classification framework for simple and effective early depression detection over social media streams\"*](https://dx.doi.org/10.1016/j.eswa.2019.05.023) (preprint available [here](https://arxiv.org/abs/1905.08772)).\n\n**Some virtues of SS3:**\n\n* It has the **ability to visually explain its rationale**.\n* Introduces a **domain-independent** classification model that **does not require feature engineering**.\n* Naturally supports **incremental (online) learning** and **incremental classification**.\n* Well suited for classification over **text streams**.\n* Its 3 **hyperparameters** are **easy-to-understand and intuitive** for humans (it is not an \"obscure\" model).\n\n**Note:** this package also incorporates different variations of the SS3 classifier, such as the one introduced in *\"t-SS3: a text classifier with dynamic n-grams for early risk detection over text streams \"* (recently submitted to Pattern Recognition Letters, preprint available [here](https://arxiv.org/abs/1911.06147)) which allows SS3 to recognize important word n-grams \"on the fly\".\n\n## What is PySS3?\n\n[PySS3](https://github.com/sergioburdisso/pyss3) is a Python package that allows you to work with SS3 in a very straightforward, interactive and visual way. In addition to the implementation of the SS3 classifier, PySS3 comes with a set of tools to help you developing your machine learning models in a clearer and faster way. These tools let you analyze, monitor and understand your models by allowing you to see what they have actually learned and why. To achieve this, PySS3 provides you with 3  main components: the `SS3` class, the `Server` class and the `PySS3 Command Line` tool, as pointed out below.\n\n## The SS3 class\n\nwhich implements the classifier using a clear API (very similar to that of `sklearn`'s models):\n\n        from pyss3 import SS3\n        clf = SS3()\n        ...\n        clf.fit(x_train, y_train)\n        y_pred = clf.predict(x_test)\n\n## The Server class\n\nwhich allows you to interactively test your model and visually see the reasons behind classification decisions, **with just one line of code**:\n\n        from pyss3.server import Server\n        from pyss3 import SS3\n    \n        clf = SS3(name=\"my_model\")\n        ...\n        clf.fit(x_train, y_train)\n        Server.serve(clf, x_test, y_test) # &lt;- this one! cool uh? :)\n\nAs shown in the -----> image !!!  below, this will open up, locally, an interactive tool in your browser which you can use to (live) test your models with the documents given in `x_test` (or typing in your own!). This will allow you to visualize and understand what your model is actually learning.\n\n&amp;#x200B;\n\n*Processing gif 6qatvjmvos541...*\n\nFor example, we have uploaded two of these live tests online for you to try out: [\"Movie Review (Sentiment Analysis)\"](http://tworld.io/ss3/live_test_online/#30305) and [\"Topic Categorization\"](http://tworld.io/ss3/live_test_online/#30303), both were obtained following the [tutorials](https://pyss3.readthedocs.io/en/latest/user_guide/getting-started.html#tutorials).\n\n## And last but not least, the PySS3 Command Line tool\n\nThis is probably the most useful component of PySS3. When you install the package (for instance by using `pip install pyss3`) a new command `pyss3` is automatically added to your environment's command line. This command allows you to access to the *PySS3 Command Line*, an interactive command-line query tool. This tool will let you interact with your SS3 models through special commands while assisting you during the whole machine learning pipeline (model selection, training, testing, etc.). Probably one of its most important features is the ability to automatically (and permanently) record the history of every evaluation result of any type (tests, k-fold cross-validations, grid searches, etc.) that you've performed. This will allow you (with a single command) to interactively visualize and analyze your classifier performance in terms of its different hyper-parameters values (and select the best model according to your needs). For instance, let's perform a grid search with a 4-fold cross-validation on the three [hyperparameters](https://pyss3.readthedocs.io/en/latest/user_guide/ss3-classifier.html#hyperparameters), smoothness(`s`), significance(`l`), and sanction(`p`) as follows:\n\n    your@user:/your/project/path$ pyss3\n    (pyss3) &gt;&gt;&gt; load my_model\n    (pyss3) &gt;&gt;&gt; grid_search path/to/dataset 4-fold -s r(.2,.8,6) -l r(.1,2,6) -p r(.5,2,6)\n\nIn this illustrative example, `s` will take 6 different values between 0.2 and 0.8, `l` between 0.1 and 2, and `p` between 0.5 and 2. After the grid search finishes, we can use the following command to open up an interactive 3D plot in the browser:\n\n    (pyss3) &gt;&gt;&gt; plot evaluations\n\n&amp;#x200B;\n\n*Processing gif hzxsd7lxos541...*\n\nEach point represents an experiment/evaluation performed using that particular combination of values (s, l, and p). Also, these points are painted proportional to how good the performance was using that configuration of the model. Researchers can interactively change the evaluation metrics to be used (accuracy, precision, recall, f1, etc.) and plots will update \"on the fly\". Additionally, when the cursor is moved over a data point, useful information is shown (including a \"compact\" representation of the confusion matrix obtained in that experiment). Finally, it is worth mentioning that, before showing the 3D plots, PySS3 creates a single and portable HTML file in your project folder containing the interactive plots. This allows researchers to store, send or upload the plots to another place using this single HTML file (or even provide a link to this file in their own papers, which would be nicer for readers, plus it would increase experimentation transparency). For example, we have uploaded two of these files for you to see: [\"Movie Review (Sentiment Analysis)\"](https://pyss3.readthedocs.io/en/latest/_static/ss3_model_evaluation[movie_review_3grams].html) and [\"Topic Categorization\"](https://pyss3.readthedocs.io/en/latest/_static/ss3_model_evaluation[topic_categorization_3grams].html), both evaluation plots were also obtained following the [tutorials](https://pyss3.readthedocs.io/en/latest/user_guide/getting-started.html#tutorials).\n\n## The PySS3 Workflow\n\nPySS3 provides two main types of workflow: classic and \"command-line\". Both workflows are briefly described below.\n\n## Classic\n\nAs usual, importing the needed classes and functions from the package, the user writes a python script to train and test the classifiers. In this workflow, user can use the `PySS3 Command Line` tool to perform model selection (though hyperparameter optimization).\n\n## Command-Line\n\nThe whole process is done using only the `PySS3 Command Line` tool. This workflow provides a faster way to perform experimentations since the user doesn't have to write any python script. Plus, this Command Line tool allows the user to actively interact  \"on the fly\" with the models being developed.\n\nNote: [tutorials](https://pyss3.readthedocs.io/en/latest/user_guide/getting-started.html#tutorials) are presented in two versions, one for each workflow type, so that the reader can choose the workflow that best suit her/his needs.\n\n## Want to give PySS3 a try?\n\nJust go to the [Getting Started](https://pyss3.readthedocs.io/en/latest/user_guide/getting-started.html) page :D\n\n## Installation\n\n## Using pip\n\nSimply use:\n\n    pip install pyss3\n\nOr, if you already have installed an old version, update it with:\n\n    pip install --upgrade pyss3\n\n## Further Readings\n\n[Full documentation](https://pyss3.readthedocs.io)\n\n[API documentation](https://pyss3.readthedocs.io/en/latest/api/)", "link": "https://www.reddit.com/r/MachineLearning/comments/edam18/pyss3_a_python_package_implementing_a_novel_text/"}, {"autor": "Shredinger_129", "date": "2019-02-25 20:30:50", "content": "Self-driving car follow the route /!/ Hello everyone!\n\nRecently im make my first self-driving car. \n\nI took Nvidia NN architecture as a basis.\n\nIt described here: https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf\n\nBut i have a question:\n\nNow, my car just follow road lines. But i want make something new:\n\nAt last video from sentdex (https://www.youtube.com/watch?v=rvnHikUJ9T0&amp;t=915s) he make GPS-follow AI for GTA 5.\n\nHis model holds both gps and road markings simultaneously.\n\nHow he make this? What i should use as input to NN model?\n\nP.S For my model i use something like this:\n\nhttps://i.imgur.com/zA1Lrs5.png\n\nBut this method just ignored pink line of GPS and trying to keep the road marking!\n\nIm choise wrong model? \n\nP.S As input - im use what I mentioned earlier and resize it to 135x135 pixels (because im learning model on GTX 1050, she have only 2 gb of ram :c. Maybe of the resizing of the -----> image !!!  of the vehicle and does not keep gps route?", "link": "https://www.reddit.com/r/MachineLearning/comments/aupxmx/selfdriving_car_follow_the_route/"}, {"autor": "themlearning", "date": "2019-02-25 16:58:07", "content": "[P] -----> Image !!! -Based Airbnb Pricing Algorithm, Hey guys, I just created a machine learning blog and uploaded my first post/project, please let me know what you think.", "link": "https://www.reddit.com/r/MachineLearning/comments/aunetd/p_imagebased_airbnb_pricing_algorithm_hey_guys_i/"}, {"autor": "RoseVM", "date": "2019-02-25 16:27:32", "content": "UG2+ Prize Challenge at CVPR 2019 ($60K in prizes!)[News] /!/  Registration is now open for the **UG2+ Prize Challenge at CVPR 2019**. We are offering $60K in prizes! If you are interested in -----> image !!!  restoration and enhancement, you should check it out:  \n**UG2+ Bridging the Gap between Computational Photography and Visual Recognition**  \nThe 30th IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2019)  \nJune 16th-21th, 2019  \nLong Beach, CA, USA  \nWebsite: [http://www.ug2challenge.org](http://www.ug2challenge.org/)  \n\n\nContinuing  the success of the 1st UG2 Prize Challenge workshop held at CVPR 2018,  UG2+ provides an integrated forum for researchers to review the recent  progress of handling various adverse visual conditions in real-world  scenes, in robust, effective and task-oriented ways. Beyond the human  vision-driven restorations, we also extend particular attention to the  degradation models and the related inverse recovery processes that may  benefit successive machine vision tasks. We embrace the most advanced  deep learning systems but are still open to classical physically  grounded models, as well as any well-motivated combination of the two  streams. The workshop will consist of four invited talks, together with  peer-reviewed regular papers (oral and poster), and talks associated  with winning prize challenge contributions.  \n\n\nThe UG2+ Challenge  seeks to advance the analysis of \"difficult\" imagery by applying image  restoration and enhancement algorithms to improve analysis performance.  Participants are tasked with developing novel algorithms to improve the  analysis of imagery captured under problematic conditions.  \n\n\nThis prize challenge has two components that have been combined for a unified workshop at CVPR:  \n\n\n1. **Video object classification and detection from unconstrained mobility platforms:** Image restoration and enhancement algorithms that remove corruptions like blur,  noise, and mis-focus, or manipulate images to gain resolution, change  perspective and compensate for lens distortion are now commonplace in  photo editing tools. Such operations are necessary to improve the  quality of images for recognition purposes. But they must be compatible  with the recognition process itself, and not adversely affect feature  extraction or decision making. Sub-Challenges:\n   1. Object Detection Improvement on Video\n   2. Object Classification Improvement on Video\n2. **Object Detection in Poor Visibility Environments:**  While most current vision systems are designed to perform in  environments where the subjects are well observable without  (significant) attenuation or alteration, a dependable vision system must  reckon with the entire spectrum of complex unconstrained and dynamic  degraded outdoor environments. It is highly desirable to study to what  extent, and in what sense, such challenging visual conditions can be  coped with, for the goal of achieving robust visual sensing. Sub-Challenges:\n   1. (Semi-)Supervised Object Detection in Haze Conditions\n   2. (Semi-)Supervised Face Detection in Low Light Conditions\n   3. Zero-Shot Object Detection with Raindrop Occlusions\n\n$60,000 will be awarded in prizes to the best performing submissions!  \n\n\n**Important Dates:**  \n \n\n1. Paper Submission  \nMay 1, 2019: Paper submission deadline  \nMay 10, 2019: Paper decision notification  \nMay 17, 2019: Paper camera ready\n2. Challenge Participation:  \nJanuary 31, 2019: Development kit and registration made available  \nMarch 15 - April 15, 2019: Dry run period  \nApril 1, 2019: Registration deadline  \nMay 1, 2019: Challenge submission deadline  \nMay 20, 2019: Challenge results will be released  \nJune 18, 2019: Most successful and innovative teams present at CVPR 2019 workshop\n\n**Organization Committee:**  \n\n\n* **Walter** **Scheirer**,\u00a0Assistant Professor, Notre Dame University, USA\n* **Zhangyang** **(Atlas)** **Wang**, Assistant Professor, Texas A&amp;M University, USA\n* **Jiaying** **Liu**, Associate Professor, Peking University, China\n* **Wenqi** **Ren**,\u00a0Assistant Professor,\u00a0Chinese Academy of Sciences, China\n* **Wenhan** **Yang**, Postdoc Researcher, City University of Hong Kong, Hong Kong, China\n* **Kevin** **Bowyer**, Schubmehl-Prein Family Professor, Notre Dame University, USA\n* **Thomas** **S.** **Huang**, Maybelle Leland Swanlund Endowed Chair Emeritus, University of Illinois at Urbana-Champaign, USA\n* **Sreya** **Banerjee**,\u00a0Graduate Student,\u00a0Notre Dame University, USA\n* **Rosaura** **Vidal-Mata**,\u00a0Graduate Student, Notre Dame University, USA\n* **Ye Yuan**,\u00a0Graduate Student,\u00a0Texas A&amp;M University, USA\n\nFor information on the challenge, rules, and submissions, please visit the workshop's website: [www.ug2challenge.org](http://www.ug2challenge.org/) ", "link": "https://www.reddit.com/r/MachineLearning/comments/aun2eo/ug2_prize_challenge_at_cvpr_2019_60k_in_prizesnews/"}, {"autor": "zxall", "date": "2019-02-25 13:25:55", "content": "[D] Great idea, how to publish? /!/ I'm an independent software developer. While working on my own -----> image !!!  processing project I invented a \"thing\".  There is no reason to keep it to myself, as idea it's not patentable. What I'd like is to make it public and add something to my resume. What is the right way of doing it? \nAs for idea:\n  1. It gives great, clear benefits.\n  2. it's generic and applicable in other domains as well.\n  3. it's intuitive. If I tell most of you will say 'nice' even before looking at the results.\n  4. it's easy to implement. I did it in pytorch, it's as easy to do in tensorflow or any other similar tool.\n\nIt would be quite popular, but I've seen nothing like this. So, I think it's something really new and worth publishing.", "link": "https://www.reddit.com/r/MachineLearning/comments/aul89k/d_great_idea_how_to_publish/"}, {"autor": "atum47", "date": "2019-02-25 08:24:57", "content": "[D] what is the best way to search for something in a -----> image !!! ? /!/ This might be a silly question but how does YOLO looks for occurrences of cats, dogs, humans... In a image with all that? Let's pretend I've trained a classifier with dogs and cats. For that I normalize the images to the same size. E.g.: 32x32 pixels. How do I predict a image in a different format? Like it's in a different orientation and even if I scale it down I would have to crop it so it can be 32x32. Maybe the title is wrong, cause I even have this doubt about machine learning and text. How do I classify text? Don't it need to be a fixed size inputs? (This collides with my previous question). Let's suppose I have the text \"I love you\" wich is 3 words of 7 letters. How a neural network process other inputs bigger or smaller?\n\nIf anyone could clarify this for me I would appreciate.", "link": "https://www.reddit.com/r/MachineLearning/comments/auix9r/d_what_is_the_best_way_to_search_for_something_in/"}, {"autor": "Uriopass", "date": "2019-02-25 08:06:13", "content": "[D] Does the discriminator of a GAN represent anything? /!/ Suppose you train a GAN on a given dataset, you get a discriminator D and a generator G.\n\nUsually, we throw D and use G for generating purposes.\n\nBut could you use D to answer the question \"Is this input in my dataset manifold?\" ?\n\nCould this be used for NSFW -----> image !!!  detection, for example ?", "link": "https://www.reddit.com/r/MachineLearning/comments/auis7j/d_does_the_discriminator_of_a_gan_represent/"}, {"autor": "seek_freedom", "date": "2019-08-21 13:15:32", "content": "[D] Looking for opinions on using 3d models for -----> image !!!  recognition. /!/ I was thinking about why the training process for image recognition is such a resource intensive task, and I got caught up on the idea that information loss caused by using projections (i.e. pictures) as training data would be an interesting area to explore.\n\n&amp;#x200B;\n\nIn comes mesh based 3d models. The training subject now has all of the necessary features in a single example My question is twofold. Do you see any potential in using 3d models as a source for an ML algo/NN, and if yes, how would you go about doing it?\n\n&amp;#x200B;\n\nI've done a bit of brain storming, but I can't really get anywhere. My initial thoughts were decompose the problem into parts, and solve it from both ends. One challenge is getting an isolated object from an image to use as a target, and the other is decomposing a 3d model into a useful 2d projection that can be compared with the target. \n\n&amp;#x200B;\n\nFiguring that the 2D part would be easier, I decided to tackle that first, and I found the task of meaningful image segmentation to be a bit more difficult than I expected. It essentially came down to this: I can use pre existing methods to segment an image, or I can make my own; but nothing actually does decent job of pulling out objects with the exception of NNs which need to be trained to do so. In an attempt to improve the training process, I essentially run into the need to use methods that I'm trying to replace. \n\n&amp;#x200B;\n\nThe 3D part has a variety of challenges as well. I figured I could do something like take an image of the model that's in similar proportion to the image that contains my target, then try to align the centroid of the model with the centroid of the target. This would eventually get to needing to iteratively translate, rotate, and scale the model. Some other obvious issues include segmenting the 3d model, posing the model, and finding what model to use as a reference when there are multiple ones saved.\n\n&amp;#x200B;\n\nThoughts?", "link": "https://www.reddit.com/r/MachineLearning/comments/ctgp9q/d_looking_for_opinions_on_using_3d_models_for/"}, {"autor": "elie2222", "date": "2019-08-21 10:19:57", "content": "-----> Image !!!  recognition API that recognises products /!/ I'm looking for a readily available API that will label products. For example, if I take a photo of Dove shampoo or Kit Kat chocolate, it will tell me that's what I've taken a picture of.\n\nI've played Amazon Rekognition and some Google APIs, but they don't seem to do the exact job I need. They may be able to tell me I took a picture of chocolates, but not that I took a picture of Kit Kats.\n\nAny suggestions for a good API to use? (Or a better subreddit to ask this question in?)", "link": "https://www.reddit.com/r/MachineLearning/comments/cteurj/image_recognition_api_that_recognises_products/"}, {"autor": "_-rootkid-_", "date": "2019-08-20 23:11:19", "content": "Infra red humanoid -----> image !!!  recognition? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ct80af/infra_red_humanoid_image_recognition/"}, {"autor": "aaronjl33", "date": "2019-08-20 20:06:19", "content": "[D] Should I include my weights inside my docker container? /!/ I am running my ML inference inside a docker container. Should I include my weights in the -----> image !!! , or should I download them from S3 when the container starts up? From what I can see, the benefits are as follows:\n\nPros for including: faster startup times since I don't need to download after startup. Less dependencies since everything is included in the container image\n\nPros for downloading: separation of weights and code. Easier weight tweaking since I won't need to redeploy image when changing weights\n\nThoughts?", "link": "https://www.reddit.com/r/MachineLearning/comments/ct58f1/d_should_i_include_my_weights_inside_my_docker/"}, {"autor": "taki0112", "date": "2019-08-14 02:22:55", "content": "[P] Simple Tensorflow implementation of \"Few-Shot Unsupervised -----> Image !!! -to------> Image !!!  Translation (a.k.a. FUNIT)\" in ICCV 2019 /!/ &amp;#x200B;\n\n[Our result](https://i.redd.it/3x2lun84rbg31.png)\n\n[Paper result](https://i.redd.it/63fk49h6rbg31.gif)", "link": "https://www.reddit.com/r/MachineLearning/comments/cq3ckf/p_simple_tensorflow_implementation_of_fewshot/"}, {"autor": "dtransposed", "date": "2019-08-13 17:40:59", "content": "[P] Towards explainable video analysis - Visual Attention For Action Recognition /!/ I am currently researching practical applications of action recognition models with use of attention models. I have decided to share lessons learned from implementing several ideas from research papers in this field. The network learns to classify -----> image !!! s from HMDB-51 dataset and creates attention heatmaps which focus on different parts on the -----> image !!!  and thus justify model's decision. Heatmaps can be very accurate, to the point that one could probably use them for tracking.\n\nThe tutorial contains brief overview of action recognition and visual attention mechanisms. Then I present the network architecture and discuss the results of my project. Additionally, I include github repo with my implementation.\n\n[Here are the results!](https://dtransposed.github.io/blog/Action-Recognition-Attention.html)\n\nI hope you guys find it interesting! \n\nhttps://i.redd.it/l01bfscy59g31.gif", "link": "https://www.reddit.com/r/MachineLearning/comments/cpwju2/p_towards_explainable_video_analysis_visual/"}, {"autor": "mwitiderrick", "date": "2019-08-13 14:07:04", "content": "[R] https://heartbeat.fritz.ai/a-2019-guide-to-deep-learning-based-image-compression-2f5253b4d811 /!/ Today I look at how deep learning can be used in -----> image !!!  compression. The piece is from a comprehensive analysis of several research papers.\n\n  https://heartbeat.fritz.ai/a-2019-guide-to-deep-learning-based-image-compression-2f5253b4d811", "link": "https://www.reddit.com/r/MachineLearning/comments/cptmgy/r/"}, {"autor": "LudicrousAbode", "date": "2019-08-13 04:41:20", "content": "[R] Google Health: An augmented reality microscope with real-time artificial intelligence integration for cancer diagnosis /!/ Abstract:\n\nThe microscopic assessment of tissue samples is instrumental for the diagnosis and staging of cancer, and thus guides therapy. However, these assessments demonstrate considerable variability and many regions of the world lack access to trained pathologists. Though artificial intelligence (AI) promises to improve the access and quality of healthcare, the costs of -----> image !!!  digitization in pathology and difficulties in deploying AI solutions remain as barriers to real-world use. Here we propose a cost-effective solution: the augmented reality microscope (ARM). The ARM overlays AI-based information onto the current view of the sample in real time, enabling seamless integration of AI into routine workflows. We demonstrate the utility of ARM in the detection of metastatic breast cancer and the identification of prostate cancer, with latency compatible with real-time use. We anticipate that the ARM will remove barriers towards the use of AI designed to improve the accuracy and efficiency of cancer diagnosis.\n\n[https://www.nature.com/articles/s41591-019-0539-7.epdf?author\\_access\\_token=BI9AOTsesmNoV2lSdpucn9RgN0jAjWel9jnR3ZoTv0PDGU3ZwysZtsN41a2fOgaoj4PRxjTvAHjSFrKF\\_S\\_mq4QNNV8dNoxAjytIQuVz9vdjplLQHUSEPiIo392MzIJY8fqxLKHC5vIwNpLLEoXMnA%3D%3D](https://www.nature.com/articles/s41591-019-0539-7.epdf?author_access_token=BI9AOTsesmNoV2lSdpucn9RgN0jAjWel9jnR3ZoTv0PDGU3ZwysZtsN41a2fOgaoj4PRxjTvAHjSFrKF_S_mq4QNNV8dNoxAjytIQuVz9vdjplLQHUSEPiIo392MzIJY8fqxLKHC5vIwNpLLEoXMnA%3D%3D)", "link": "https://www.reddit.com/r/MachineLearning/comments/cpo6vm/r_google_health_an_augmented_reality_microscope/"}, {"autor": "ykilcher", "date": "2019-08-12 15:42:22", "content": "[R] Video Analysis: Processing Megapixel Images with Deep Attention-Sampling Models /!/ [https://youtu.be/H6Qiegq\\_36c](https://youtu.be/H6Qiegq_36c)\n\nCurrent CNNs have to downsample large images before processing them, which can lose a lot of detail information. This paper proposes attention sampling, which learns to selectively process parts of any large -----> image !!!  in full resolution, while discarding uninteresting bits. This leads to enormous gains in speed and memory consumption.", "link": "https://www.reddit.com/r/MachineLearning/comments/cpe307/r_video_analysis_processing_megapixel_images_with/"}, {"autor": "ykilcher", "date": "2019-08-12 14:39:43", "content": "Video Analysis: Processing Megapixel Images with Deep Attention-Sampling Models /!/ [https://youtu.be/H6Qiegq\\_36c](https://youtu.be/H6Qiegq_36c)\n\nCurrent CNNs have to downsample large images before processing them, which can lose a lot of detail information. This paper proposes attention sampling, which learns to selectively process parts of any large -----> image !!!  in full resolution, while discarding uninteresting bits. This leads to enormous gains in speed and memory consumption.", "link": "https://www.reddit.com/r/MachineLearning/comments/cpd90l/video_analysis_processing_megapixel_images_with/"}, {"autor": "joshuacpeterson", "date": "2019-10-29 12:36:46", "content": "[P] CIFAR-10H - Human guess distribution soft labels for CIFAR-10 (Dataset Release) /!/ CIFAR-10H is a new dataset of soft labels reflecting human perceptual uncertainty for the 10,000------> image !!! \u00a0CIFAR-10\u00a0test set, which we are releasing today. It first appears in the paper:\n\nJoshua C. Peterson,\u00a0Ruairidh M. Battleday,\u00a0Thomas L. Griffiths, &amp;\u00a0Olga Russakovsky (2019). Human uncertainty makes classification more robust.\u00a0In Proceedings of the IEEE International Conference on Computer Vision.\n\nDataset Link: https://github.com/jcpeterson/cifar-10h/", "link": "https://www.reddit.com/r/MachineLearning/comments/dop6c2/p_cifar10h_human_guess_distribution_soft_labels/"}, {"autor": "joshuacpeterson", "date": "2019-10-29 01:24:03", "content": "[1905.01164] SinGAN: Learning a Generative Model from a Single Natural -----> Image !!! ", "link": "https://www.reddit.com/r/MachineLearning/comments/doiukh/190501164_singan_learning_a_generative_model_from/"}, {"autor": "ryanl247", "date": "2019-10-28 19:19:04", "content": "-----> Image !!!  Classification Question /!/ Hey, \n\nI am aware that you can train a model to classify between distinct images (e.g. numbers). Let's say I train a model to understand the number 6. Would it be possible to provide a photo of thousands of numbers to the model and have it pick out all the 6s? Or in a more real world example where different photo angles come into play, show a photo of a beach and have it pick up all the garbage (not shells, towels, crabs, etc.)? How would one do this?", "link": "https://www.reddit.com/r/MachineLearning/comments/dodphd/image_classification_question/"}, {"autor": "data-soup", "date": "2019-10-28 11:13:44", "content": "[D] How to , concretly, measure a model's robustness against adversarial/perturbations examples? ... I mean concretly. /!/ We know that we can measure a model's robustness to perturbation by applying perturbation to training points and checking if the outputs are the same:\n\n&gt; The `lp` ball around an -----> image !!!  is said to be the adversarial ball, and a network is said to be `E-robust` around `x` if every point in the adversarial ball around `x` classifies the same. [source, Part 3](https://arxiv.org/pdf/1909.13846.pdf)\n\n- But how is this done concretely? Is there a smarter way to do it than just applying perturbation in the input and see how it goes out? \n- Who is doing it IRL?", "link": "https://www.reddit.com/r/MachineLearning/comments/do7e7l/d_how_to_concretly_measure_a_models_robustness/"}, {"autor": "funkmasterplex", "date": "2019-02-16 14:57:30", "content": "[P] Automatic -----> image !!!  enhance first attempt /!/ Here's some outputs from the test set:\n\nhttps://i.imgur.com/Go66srR.png\n\nThe network structure is a GAN, with a unet style autoencoder as the generator, and a normal discriminator based on DCGAN.\n\nFor my training set, I took a few thousand images from CelebA, and removed the ones that were faded/over saturated/black and white/bad skin tones (too green or red usually).\n\nFor the test set, I picked some of those images which were rejected from the training set.\n\nDuring training, the input to the autoencoder is a mangled version of the training data, realtime augmentation with random brightness shifts, contrast shifts, separate colour channel shifts, some converted to grayscale, some HSV channel shifts, some gaussian noise, some blurring, some random boxes drawn over the images, some sobel edge detection mixed in.  The output to the autoencoder is the original image.\n\nI used Adam optimisers with low momentum, inverted smooth labels to the discriminator (0=real, 0.9=fake).  For the loss of the joined model, I used mae for the reconstruction of the image, and binary cross entropy on the discriminator output, weighted at 0.98/0.02 respectively.  The disciminator is just Conv/LeakyReLU layers, the generator is the same but with instance normalisation layers and skip connections.\n\nTraining was about half a day on an NVIDIA 960.  I am pretty pleased with these first results.", "link": "https://www.reddit.com/r/MachineLearning/comments/ar9h1b/p_automatic_image_enhance_first_attempt/"}, {"autor": "csbotos", "date": "2019-02-15 20:29:29", "content": "[D] Instability of -----> image !!!  generation models on a very simple problem. /!/ * Objective: I synthesized a dataset of 16x16 black images with a single random 4x4 white square appearing on each of them.\n* Reason: test the coverage of the uniform distribution of the appearances using different GANs and a basic VAE model\n* Outcome: surprisingly, all the PyTorch example code have failed:\n\n[https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py)[https://github.com/pytorch/examples/tree/master/dcgan](https://github.com/pytorch/examples/tree/master/dcgan)\n\n[https://github.com/pytorch/examples/tree/master/vae](https://github.com/pytorch/examples/tree/master/vae)\n\n&amp;#x200B;\n\nI have also tried more nuanced versions of GAN training methods, like WGAN, Spectral Normalization, TTUR without success. I trained them for 50k iterations, I tried to reduce the number of parameters, increase/decrease the batch size (which helped a bit for the VAE) but this is ridiculous.\n\nI started to wonder, how people are making HQ images and argue which model is the best when tinkering around with their baselines are only working for that specific dataset (e.g. celebA-HQ, ImageNet, LSUN...)? \n\nAm I missing something?", "link": "https://www.reddit.com/r/MachineLearning/comments/ar0vvl/d_instability_of_image_generation_models_on_a/"}, {"autor": "csbotos", "date": "2019-02-15 20:17:31", "content": "[D] Instability on toy problems /!/ I tried producing a 16x16 black -----> image !!!  with a random 4x4 square appearing on it to test the coverage of the randomness of different GANs and a basic VAE model.\n\nSurprisingly, all the PyTorch example code have failed:\n\n[https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py)  \n[https://github.com/pytorch/examples/tree/master/dcgan](https://github.com/pytorch/examples/tree/master/dcgan)\n\n[https://github.com/pytorch/examples/tree/master/vae](https://github.com/pytorch/examples/tree/master/vae)\n\n&amp;#x200B;\n\nI have also tried more nuanced versions of GAN training methods, like WGAN, Spectral Normalization, TTUR without success. Needless to say, I tried to reduce the number of parameters, increase/decrease the batch size (which helped a bit for the VAE) but this is ridiculous.\n\nI started to wonder, how people are making HQ images and argue which model is the best when tinkering around with their baselines are only working for that specific dataset? Am I missing something? ", "link": "https://www.reddit.com/r/MachineLearning/comments/ar0req/d_instability_on_toy_problems/"}, {"autor": "FoCDoT", "date": "2019-02-15 13:34:11", "content": "This is not a photoshop project, this is -----> image !!!  generation using google deepdream artificial neural network which uses inception net, this basically was started as a project to understand how convolution neural network works.", "link": "https://www.reddit.com/r/MachineLearning/comments/aqwlju/this_is_not_a_photoshop_project_this_is_image/"}, {"autor": "372995411", "date": "2019-02-15 12:15:43", "content": "[P] A curated list of deep learning -----> image !!!  classification papers and codes /!/ Recently, I just created a project that curated a list of deep learning image lcassification papers and codes, if anyone were intrested, here is the github repository:[https://github.com/weiaicunzai/awesome-image-classification](https://github.com/weiaicunzai/awesome-image-classification) , welcome to create a pull request.\n\n&amp;#x200B;\n\nI don't know if I am allowed to post my github repository here, If I am violating the rules, please let me know.", "link": "https://www.reddit.com/r/MachineLearning/comments/aqvzfd/p_a_curated_list_of_deep_learning_image/"}, {"autor": "atum47", "date": "2019-02-15 08:26:36", "content": "[P] machine learning and games /!/ Recently I uploaded a experiment: can a Perceptron play a simple game?\n\nhttps://github.com/victorqribeiro/carGamePerceptron\n\nI did this to prove that you can do interesting things with simple code. Now I just finished my MLP, so I can do more robust things. I'm thinking about something with -----> image !!!  recognition. Anyways, here's the link in case you want to see the code.\n\nhttps://github.com/victorqribeiro/mlp", "link": "https://www.reddit.com/r/MachineLearning/comments/aquje4/p_machine_learning_and_games/"}, {"autor": "svenip", "date": "2019-02-15 05:03:20", "content": "best practice/methods for facial marker tracking via ML /!/  \n\nHi,\n\ni am investigating if i can adopt ML to predict markers on faces. to  describe the problem better: a very typical method for facial  performance capture is to put markers on actors faces and later on track  their positions frame by frame inside the -----> image !!!  dimension. so let's say  the actor has 20 markers and their position is in 2D space. i do have a  large dataset with manual tracked markers and their corresponding  images. Right now i'm trying to see what approach is the best to take  here. at it's simplest i already tried a convolutional net where the  output is a flattened array for the marker positions. this got me  relative close but i need much more accuracy. right now i'm looking into  autoencoders as an alternative but haven't fully implemented anything  yet.\n\nare there other known networks which might be best suited for this task?", "link": "https://www.reddit.com/r/MachineLearning/comments/aqt3nt/best_practicemethods_for_facial_marker_tracking/"}, {"autor": "CrazyCar09", "date": "2019-02-19 03:50:07", "content": "[P] Can I get a peer review of this paragraph in my essay? /!/ I am new to deep learning and want to make sure I am representing it correctly and explaining it well. Thanks a million!\n\n&amp;#x200B;\n\n Jeff Dean, a PhD graduate from University of Washington, draws a -----> picture !!!  of deep learning, stating, \u201cWhen you hear the term deep learning, just think of a large deep neural net. Deep refers to the number of layers typically and so this is kind of the popular term that\u2019s been adopted in the press. I think of them as deep neural networks generally\u201d. Neural net is short for the term \u201cneural network, a computer diagram that allows for deep learning to take place. This diagram is inspired by a human brain. At the start of the network is an input. This input leads into a set of layers called \u201chidden layers.\u201d These layers are where the change occurs. From the hidden layer, an output is produced. This all functions like a human brain. A human brain is composed of neurons, which are composed of dendrites and one axon. When one dendrite is prompted, it will trigger the axon. This axon might trigger another separate neuron and this continues until an output is given (\u201cBrownlee\u201d). Using advanced algorithms, as well as the chain rule, computers can create their own outputs. These algorithms give each input a weight; generally this weight is around 1 for small networks, a large network might be given a number in the 100\u2019s to produce a more accurate output. Once the input leaves, it is transferred to a hidden layer. Based on the programs algorithm and function, this newly arrived input will be given a new weight, if it is very close to the desired output then the weight remains close to the original weight. The farther from the output, the lower the weight. This process repeats for the amount of hidden layers there are (\u201cCosmos\u201d). After it gets to the final stage of hidden layers, the highest number is chosen to be the output. This process leads into the first implementations of deep learning. ", "link": "https://www.reddit.com/r/MachineLearning/comments/as626p/p_can_i_get_a_peer_review_of_this_paragraph_in_my/"}, {"autor": "kur054ki", "date": "2019-02-18 23:45:19", "content": "Stereo -----> image !!!  classification &amp; deep learning /!/ Hello guys , just been wondering how can I combine stereo vision and convolutional neural networks for classification problems ?", "link": "https://www.reddit.com/r/MachineLearning/comments/as3qyb/stereo_image_classification_deep_learning/"}, {"autor": "MiksLus", "date": "2019-02-18 13:52:31", "content": "[P] Public APIs for cyrillic detection in -----> image !!!  /!/ Hi, I am working on a project that would work with images containing russian text and was wondering if there are any publicly available image recognition APIs that support extraction of cyrillic text.\n\nI started to research AWS's Amazon Rekognition API but stumbled upon the fact that it tries to convert cyrillic to latin and that is not what I need in this project.", "link": "https://www.reddit.com/r/MachineLearning/comments/arxfje/p_public_apis_for_cyrillic_detection_in_image/"}, {"autor": "Fontpage", "date": "2019-02-28 21:13:16", "content": "[D] How to recognize type of -----> image !!!  /!/ Can someone point me in the right direction to recognize if a image is a forrest or not. Thanks in advance", "link": "https://www.reddit.com/r/MachineLearning/comments/avwa6t/d_how_to_recognize_type_of_image/"}, {"autor": "themlearning", "date": "2019-02-28 15:22:28", "content": "[P] -----> Image !!! -Based Airbnb Pricing Algorithm /!/ , Hey guys, I just created a machine learning blog and uploaded my first post/project, please let me know what you think.\n\nwebsite: [https://themlearning.com/2019/02/25/image-based-airbnb-pricing-algorithm/](https://themlearning.com/2019/02/25/image-based-airbnb-pricing-algorithm/)\n\nGitHub: [https://github.com/moe18/Image-Based-Airbnb-Pricing-Algorithm/blob/master/Airbnb\\_Project.ipynb](https://github.com/moe18/Image-Based-Airbnb-Pricing-Algorithm/blob/master/Airbnb_Project.ipynb)", "link": "https://www.reddit.com/r/MachineLearning/comments/avs9mt/p_imagebased_airbnb_pricing_algorithm/"}, {"autor": "soochan", "date": "2019-02-28 12:51:59", "content": "[R] Mr. GAN: Don't Use Reconstruction Loss in cGANs! (ICLR 2019) /!/ &amp;#x200B;\n\n[Pix2Pix on Cityscapes dataset with different loss functions](https://i.redd.it/g2jegqkiyaj21.jpg)\n\nThis is our paper \"Harmonizing Maximum Likelihood with GANs for Multimodal Conditional Generation\" accepted at ICLR 2019.\n\nWe reveal that **reconstruction loss is a major cause of mode collapse** in conditional GANs. As alternatives, we propose novel loss functions called **Moment Reconstruction (MR) losses**. We believe that our work has a significant impact on the numerous areas where cGAN is used.\n\n&amp;#x200B;\n\n**Project:** [https://soochanlee.com/publications/mr-gan](https://soochanlee.com/publications/mr-gan)\n\n**Paper:** [https://arxiv.org/abs/1902.09225](https://arxiv.org/abs/1902.09225)\n\n**PyTorch Implementation:** [https://github.com/soochan-lee/MR-GAN](https://github.com/soochan-lee/MR-GAN)\n\n&amp;#x200B;\n\n**Abstract:**\n\nRecent advances in conditional -----> image !!!  generation tasks, such as -----> image !!! -to------> image !!!  translation and -----> image !!!  inpainting, are largely accounted to the success of conditional GAN models, which are often optimized by the joint use of the GAN loss with the reconstruction loss. However, we reveal that this training recipe shared by almost all existing methods causes one critical side effect: lack of diversity in output samples. In order to accomplish both training stability and multimodal output generation, we propose novel training schemes with a new set of losses named *moment reconstruction losses* that simply replace the reconstruction loss. We show that our approach is applicable to any conditional generation tasks by performing thorough experiments on image-to-image translation, super-resolution and image inpainting using Cityscapes and CelebA dataset. Quantitative evaluations also confirm that our methods achieve a great diversity in outputs while retaining or even improving the visual fidelity of generated samples.", "link": "https://www.reddit.com/r/MachineLearning/comments/avqscd/r_mr_gan_dont_use_reconstruction_loss_in_cgans/"}, {"autor": "shawnmanuel000", "date": "2019-02-28 03:48:47", "content": "[Video] How to Code Your Own -----> Image !!!  Classification NN in Tensorflow (MNIST) /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/avmi0o/video_how_to_code_your_own_image_classification/"}, {"autor": "Radzinsky99", "date": "2019-02-27 21:22:05", "content": "[Discussion] CNN that classifies whether or not an object is in a -----> picture !!! . /!/ Hi, I would like to create a convolutional neural network using keras from TensorFlow and Python. \n\nThe neural network is supposed to receive an image as input and output true if an object I trained the network to is in the  picture, but I'm having some doubts about the training data and I hope you guys can give me some hints.\n\n&amp;#x200B;\n\nFor example, let's suppose I want to know if there is a dog or a cat in a picture. I need lots of image containing a dog and lots of image containing a cat. \n\nNow let's suppose I want to know whether a dog is in a picture or not. I need lots of pictures containing a dog and what? What pictures should I have to classify the image as \"not containing a dog\"? I tried searching on Google but nothing. Thanks for your help :)", "link": "https://www.reddit.com/r/MachineLearning/comments/avilyh/discussion_cnn_that_classifies_whether_or_not_an/"}, {"autor": "benjaminbardou", "date": "2019-11-13 14:56:27", "content": "\ud83d\uddbc L'Amour au bordel / Love at the brothel (2019) /!/ &amp;#x200B;\n\n*Processing img c4lleb1fwgy31...*\n\n&amp;#x200B;\n\nhttps://preview.redd.it/4pgvj31fwgy31.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=21306246c4538a6b1883a92a9830a454de443bf3\n\n&amp;#x200B;\n\nhttps://preview.redd.it/piv64b2fwgy31.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=a401dba06cca1d2ba494e31a2ad2a98d45ddc9dd\n\n&amp;#x200B;\n\nhttps://preview.redd.it/g9y70c1fwgy31.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=fc079faedc9d2b48a179a15b5d5cf80db9e47d0e\n\n&amp;#x200B;\n\nhttps://preview.redd.it/l0777a1fwgy31.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=f32e87468e0351d3e4101d0aa1e9521a1ae24777\n\n&amp;#x200B;\n\nhttps://preview.redd.it/osht731fwgy31.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=b8294f235ab2273fa27bbe33a14ab39cdea3c21f\n\n&amp;#x200B;\n\nhttps://preview.redd.it/6hkqo91fwgy31.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=cd8f2f8b9b2b8bf28f8da9926189601907f3fa17\n\n&amp;#x200B;\n\nhttps://preview.redd.it/752y451fwgy31.jpg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=17e24508928d2226a5e0714837d097404590d1bb\n\nIt's been a while since I wanted to try machine learning.  \nAfter several tests to see how it worked, I tried to produce an -----> image !!!  as if I had done it in a traditional way (drawing, modeling, rendering, paintover) but this time I would start from a machine learning algorithm.\n\nTo do this, I used the AttnGAN image synthesis algorithm that transforms text into an image.  \nI really like Baudelaire's poem \"Voyage\" because it prefigures as early as 1859 an idea of \u200b\u200bthe cinema:  \n\"To pass over our minds, stretched like a canvas,  \nYour memories with their frames of horizons. \"\n\nSo I used these verse to generate an image.  \nSince this sketch, I reworked the masses, the contrasts and the colorimetry by paintover as if I had done it normally from a rough drawing.\n\nThe image that results from this work is entitled \"Love to the brothel\". Do not ask me why. Another algorithm could very well have found this title. \ud83d\udcf7:)  \nFor me, it evokes the surrealists, Degas, Baudelaire and ... the cinema. \ud83d\udcf7;)\n\n[\\#machinelearning](https://www.facebook.com/hashtag/machinelearning?source=feed_text&amp;epa=HASHTAG) [\\#digitalpainting](https://www.facebook.com/hashtag/digitalpainting?source=feed_text&amp;epa=HASHTAG) [\\#GAN](https://www.facebook.com/hashtag/gan?source=feed_text&amp;epa=HASHTAG) [\\#AttnGAN](https://www.facebook.com/hashtag/attngan?source=feed_text&amp;epa=HASHTAG)", "link": "https://www.reddit.com/r/MachineLearning/comments/dvsw27/lamour_au_bordel_love_at_the_brothel_2019/"}, {"autor": "nwj0612", "date": "2019-11-13 12:52:39", "content": "How can I submit a -----> camera !!!  ready version of AAAI 2020 paper? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/dvrbub/how_can_i_submit_a_camera_ready_version_of_aaai/"}, {"autor": "khalidmuzappa", "date": "2019-11-13 00:28:08", "content": "Any tools to embed -----> image !!!  file to fixed column in dataset (.csv)? /!/ Hello good people of machine learning-land,\nIs there any tool to embed image into 1 row and fixed column dataset (preferably csv)? \nThank you", "link": "https://www.reddit.com/r/MachineLearning/comments/dvjege/any_tools_to_embed_image_file_to_fixed_column_in/"}, {"autor": "yetanothernormalG", "date": "2019-11-12 18:16:55", "content": ": I am totally out of -----> picture !!!  - is anybody trying to make some ML algorithms using FRP ? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/dvdvl0/i_am_totally_out_of_picture_is_anybody_trying_to/"}, {"autor": "OverLordGoldDragon", "date": "2019-11-12 17:14:56", "content": "[D] AI clones voice from 5 second recording /!/ [TwoMinutePapers clip](https://www.youtube.com/watch?v=0sR1rU3gLzQ) \\-- [Research paper](https://arxiv.org/abs/1806.04558)\n\nPublished in 2018, and I've yet to see any fuss made over this - as opposed to -----> image !!!  face recognition. Isn't it a big deal? Evidence fabrication can be taken to a whole new level - in criminal, political, personal, and other domains.\n\nOn another note, combined with [GPT2](https://talktotransformer.com/), and some of the face simulation methods, shouldn't one be able to make a fairly convincing conversational AI? If GPT2 can somehow be configured to turn off lecture-mode and shorten responses, it might be fully convincing to those who don't look much into AI.", "link": "https://www.reddit.com/r/MachineLearning/comments/dvcyzu/d_ai_clones_voice_from_5_second_recording/"}, {"autor": "ido90", "date": "2019-11-12 16:16:06", "content": "[P] Traffic Analysis in Original Video Data /!/ Half a year ago, when I lived in an apartment with a view of Ayalon Road in Tel Aviv, I decided I couldn't just let the data pass indifferently beneath my window.\n\nI used my smartphone -----> camera !!!  to record 81 short videos of the traffic; built a dedicated CNN to detect the vehicles (after the small, crowded cars in the videos were failed to be detected by several out-of-the-box networks) and trained it on a few manually-tagged video-frames; modified SORT algorithm to allow tracking a vehicle even when it does not overlap itself over adjacent frames (required due to particularly low videos frame-rate); and derived several insights from the resulted data, mainly regarding lane-transitions and the relations between density, speed and flux.\n\nI believe that this nicely demonstrates the amounts of data surrounding us, and their accessibility using as trivial tools as a smartphone.\n\nAny comments, questions and insights are welcome :)\n\nA small demonstration is attached in the form of an unpolished poster.\n\nFor more details please visit the repo's readme:\n\n[https://github.com/ido90/AyalonRoad](https://github.com/ido90/AyalonRoad)", "link": "https://www.reddit.com/r/MachineLearning/comments/dvc4q6/p_traffic_analysis_in_original_video_data/"}, {"autor": "good_rice", "date": "2019-11-12 06:22:29", "content": "[D] Tuning of generated synthetic data for instance segmentation /!/ I'm currently training a Mask-RCNN model through synthetic images \"generated\" through a procedure similar to the \"[Cut, Paste and Learn](https://arxiv.org/abs/1708.01642)\" paper. In a nutshell, this paper just randomly pastes crops of objects over backgrounds, with pretty standard augmentation for the crops themselves and Gaussian / Poisson blending for pasting. The resulting images contain all the objects with perfect masks and bounding box labels, over some arbitrary backgrounds. \n\nHowever, the generated training data still looks fairly different from real images. I do, however, have a large dataset of unlabeled real images with the real objects in them. Would anyone be aware of a method for tuning a generated -----> image !!!  to look more similar to the -----> image !!! s in the real dataset? I would want to preserve spatial information so as to not invalidate generated labels, but also add noise / shadows / pixel artifacts in a meaningful way that resembles those found in my real dataset. \n\nMy first thought was to look for papers using something like auto-encoders, but I was flooded with papers about VAEs and end-to-end generation. Is anyone aware of research for this specific problem?", "link": "https://www.reddit.com/r/MachineLearning/comments/dv614h/d_tuning_of_generated_synthetic_data_for_instance/"}, {"autor": "sensetime", "date": "2019-11-12 05:10:49", "content": "[N] Hikvision marketed ML surveillance -----> camera !!!  that automatically identifies Uyghurs, on its China website /!/ News Article: https://ipvm.com/reports/hikvision-uyghur\n\nh/t [James Vincent](https://twitter.com/jjvincent/status/1193935124582322182) who regularly reports about ML in The Verge.\n\nThe [article](https://ipvm.com/reports/hikvision-uyghur) contains a marketing image from Hikvision, the world's largest security camera company, that speaks volumes about the brutal simplicity of the techno-surveillance state.\n\nThe product feature is simple: Han \u2705, Uyghur \u274c\n\nHikvision is a regular sponsor of top ML conferences such as CVPR and ICCV, and have reportedly recruited research interns for their US-based research lab using [job posting](https://eccv2018.org/jobs/research-internship/) in ECCV. They have recently been added to a US government [blacklist](https://www.bloomberg.com/news/articles/2019-10-07/u-s-blacklists-eight-chinese-companies-including-hikvision-k1gvpq77), among other companies such as Shenzhen-based Dahua, Beijing-based Megvii (Face++) and Hong Kong-based Sensetime over human rights violation.\n\nShould research conferences continue to allow these companies to sponsor booths at the events that can be used for recruiting?\n\nhttps://ipvm.com/reports/hikvision-uyghur\n\n(N.B. no, I *don't* work at Sensetime :)", "link": "https://www.reddit.com/r/MachineLearning/comments/dv5axp/n_hikvision_marketed_ml_surveillance_camera_that/"}, {"autor": "kevinaiworld", "date": "2019-02-09 03:39:55", "content": "User- -----> image !!!  recommender system by using -----> image !!!  CNN feature /!/  There are a lot of user-item recommender systems by using user's and item's context infotmation.\n\nAre there any popular user-image recommender system papers that incorporate image CNN feature? because it is very easy to extract CNN feature from image. In item, such as movie or music, which the features is not easy to obtain.\n\nMy question:\n\nAny popular papers talk about user-image recommender system using CNN feature?\n\nPS: It may be inappropriate to ask this question in StackOverflow, Any suggest places should I ask this kind of problem?\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/aoom2a/user_image_recommender_system_by_using_image_cnn/"}, {"autor": "Eoncarry", "date": "2019-02-08 16:31:39", "content": "[D] Why do we need special methods to evaluate the performance of a GAN? Can't we just do that from the discriminator network's loss? /!/ I came across inception score in a [youtube video](https://www.youtube.com/watch?v=1ct_P3IZow0) and was wondering why can't we just check the quality of an -----> image !!!  from the predicted probability of the discriminator network (i.e., if the discriminator network outputs a high probability for my generated -----> image !!! , then my GAN is doing a good job). Why do we need seperate methods to evaluate the performance?\n\nThanks in advance!", "link": "https://www.reddit.com/r/MachineLearning/comments/aoi8gy/d_why_do_we_need_special_methods_to_evaluate_the/"}, {"autor": "deluded_soul", "date": "2019-02-08 16:20:57", "content": "code to test quick -----> image !!!  segmentation /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/aoi4gp/code_to_test_quick_image_segmentation/"}, {"autor": "Minimum_Zucchini", "date": "2019-02-07 17:15:34", "content": "[D] Changes in neural network weights with respect to affine transformations of input /!/ I'm curios to learn more about this and if it has been properly studied and if so, what some theoretical results are. I wasn't able to find papers but probably because I'm using the wrong terminology so please bear with me.\n\nAn example, let's say we build a CNN -----> image !!!  classifier of cats vs. dogs. After we are done training, lets say I pass in a transformed image from the training set into the neural network. If we take 'feature space' to mean the weights on the final fully connected layer, how close are the weights in the feature space between the original image and the affinely transformed image? Do affine transformations of inputs cause unpredictable drastic changes in the 'feature space' or is there some sort of relationship we can establish? \n\n\nThe reason I'm asking is because I read that one way of increasing data set sizes and generalization power of a neural network (at least for image classifiers) is to apply transformations to your existing data (scaling, zooming, rotating, etc). This makes sense intuitively. For humans we usually only have to see one or two examples of something before we can recognize it in any arbitrary orientation, scale etc. Of course there are potentially an infinite number of affine transformations one can apply to their data set, but this would increase training time significantly. But if we already have some idea of how these transformations will affect the neural network weights, then maybe we can somehow skip the training on the augmented data set..? I don't know.\n\n\nI'm currently a masters student and my work is more so in applied machine learning but I'd like to have a discussion about this and/or if someone can point me to relevant research papers that would be great!", "link": "https://www.reddit.com/r/MachineLearning/comments/ao5zwx/d_changes_in_neural_network_weights_with_respect/"}, {"autor": "Pawnbrake", "date": "2019-02-07 13:02:05", "content": "[P] Question about document classification with unspecified number of images /!/ There are plenty of algorithms for [document classification](https://en.wikipedia.org/wiki/Document_classification).  Likewise, there are plenty of good -----> image !!!  classification algorithms, and there is a clear standout with [convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNNs) outperforming nearly anything else.   \n However, I am working on a task that requires a hybrid of the two fields, and I am unsure how to merge the two.  \n\n&amp;#x200B;\n\nFor my project, I am classifying documents.  I have 300 or so categories, and a document may belong to multiple categories at once (no more than 5).  These documents contain text and an unspecified number of images - each document has a different number of images, and some may have zero.  In order to have high accuracy in document classification, I am required to process these images.  For the purposes of my project, assume that it is trivial to parse out the images and text from the documents.  \n\n&amp;#x200B;\n\nIs there any research on how to use unspecified number of images as well as text in document classification?  I spent some time researching and I couldn't find anything too useful.  My project is experimental so I'm interested in trying a variety of algorithms, so I would try something computationally intensive such as using perhaps a hybrid of [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory) and CNN, and other computationally simple algorithms.  ", "link": "https://www.reddit.com/r/MachineLearning/comments/ao3hwr/p_question_about_document_classification_with/"}, {"autor": "VerTiGo_Etrex", "date": "2019-02-07 03:40:03", "content": "[D] StyleGAN, but in reverse. Is it possible? /!/ I discovered https://github.com/NVlabs/stylegan today, and was wondering if it's possible to train a network that outputs the optimal StyleGAN vector given an -----> image !!!  as input.\n\nDisclaimer: I did some work for Snap on boosted decision trees for ad and lens ranking, but my background is in infrastructure and app dev, and I haven't ever really touched CNNs. Looking to learn more, hence my question.\n\nIt seems that one could train a network similar to the way CycleGANs or autoencoders are trained. Slap a CNN on the front whose outputs encode down to a 512 length vector (the input size of StyleGAN,) and then feed that vector into StyleGAN to get an output image. The network would minimize the distance between the input image, and the generated StyleGAN image, and in doing so, would learn the weights required to convert an image into a StyleGAN encoding.\n\nThis would allow us to do fun things like interpolate between two input faces, or blend the traits of multiple faces into one face.\n\nProducing training examples would be trivial. Simply feed a random vector into StyleGAN, and use the output as the training image.\n\nIf this seems feasible, please tell me. I would love to take a crack at it.", "link": "https://www.reddit.com/r/MachineLearning/comments/anzi1t/d_stylegan_but_in_reverse_is_it_possible/"}, {"autor": "blueh8t", "date": "2019-06-19 10:27:50", "content": "-----> image !!!  processing-help /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/c2fdd5/image_processinghelp/"}, {"autor": "sahebqaran", "date": "2019-06-19 03:00:41", "content": "Multi-Label Binary Classification with MANY zeros: What's the state of research? /!/ Hi there,\n\n&amp;#x200B;\n\nI didn't know whether to post this in Learn Machine Learning or on here, but basically, I've been looking at papers all day, and nothing I've found so far seems to be able to solve the problem of massively imbalanced datasets for -----> image !!!  classification. The one and only thing I found was this paper on an algorithm called CoCoa, but it had only been tested for image classification on 6 classes.\n\nHow do you deal with having label sets where, the model can predict all zeros and still get a 99% accuracy?", "link": "https://www.reddit.com/r/MachineLearning/comments/c2bool/multilabel_binary_classification_with_many_zeros/"}, {"autor": "cryptonewsguy", "date": "2019-06-18 18:37:11", "content": "[D] Meta-Generative Adversarial Networks for AGI /!/ So I have this idea for creating AGI based off of some things I've been reading from J\u00fcrgen Schmidhubers.\n\n# Meta-learn\n\nLets say you have a Neural Network (the parent network) which can design arbitrary children networks and learn optimized design patterns for a given task. Of course your parent network won't be super generalized for any type of network design, just relatively specific tasks. This is kind of why we don't have \"real\" AI or AGI. The tasks are still relatively narrow. \n\n\nSkimming the literature on meta-learning it looks like researchers have been able to get SOME generalization by training their meta-networks on multiple tasks. But of course data and identifying tasks might be a limitation for scale-ability and high levels of generalization. So I purpose a potentially more elegant way.\n\n\n# PowerPlay\n\n\nThis is where [J\u00fcrgen Schmidhubers PowerPlay would come in.](https://arxiv.org/abs/1112.5309) The PowerPlay algorithm is split into a solver and a problem generator. The problem generator generates novel problems which the solver has to try to solve. Novel problems are problems which are unsolvable by the current solver. The created problems are just a bit more complicated than the most complicated solvable problem. The solver has to be able to solve all previous problems the generator created plus the new one. \n\n\n\n\n# Meta-PowerPlay\n\nBoth the problem Solver and Generator have parent Networks which continually learn to design more sophisticated Solvers and Generators until you have much more general problem solvers, or rather a neural network that can design general problem solvers.\n\n\nIts kind of similar how GANs work for deepfakes and -----> image !!!  problems work, the networks try to outsmart eachother in a feedbackloop but instead of just doing a faceswap, it can generate a general purpose computational engine. Or at least one that is a lot more general than what we currently have.", "link": "https://www.reddit.com/r/MachineLearning/comments/c26040/d_metagenerative_adversarial_networks_for_agi/"}, {"autor": "Berdas_", "date": "2019-06-18 18:11:25", "content": "[D] Creating a Face Verification algorithm for authentication /!/ Hello guys!  \n\n\nI've just started working on a small project that involves analyzing a web cam generated -----> image !!!  and compare it to -----> image !!! s on a dataset folder to try and find a match.\n\nThere are some pre-trained models out there, which enables one-shot learning (like this GitHub for example:  [https://github.com/mohitwildbeast/Facial-Recognition-Using-FaceNet-Siamese-One-Shot-Learning](https://github.com/mohitwildbeast/Facial-Recognition-Using-FaceNet-Siamese-One-Shot-Learning))\n\nHowever, it is not precise as I wanted, and I don't really know why.\n\nI was looking the FaceNet model by David Sandberg ([https://github.com/davidsandberg/facenet](https://github.com/davidsandberg/facenet)) and it seems promising, however I don't know how to use it for my case.\n\nSo, I was wondering if you guys have any advice for me, any link, that might help me!  \nThe system should be simple, is just a proof of concept, so as long as the algorithm can compare the face it is detecting on the webcam, for example, to one on a images folder and return the embedded distance (distance between the faces, where smaller are similar faces and bigger otherwise).\n\n  \nI'm not sure if I was clear, as it is my first time writing on this sub.\n\n&amp;#x200B;\n\nThank you all in advance.", "link": "https://www.reddit.com/r/MachineLearning/comments/c25o80/d_creating_a_face_verification_algorithm_for/"}, {"autor": "Oswald_Hydrabot", "date": "2019-10-16 04:23:10", "content": "[P] My implementation of object tracking using an Xbox 360 Kinect, a dynamixel Pan/Tilt turret, ROS and YOLOv3 /!/ This is a little video clip I made of a project of mine where I've used the 2D bounding box data from PJReddie's YOLOv3 to guide the joint positions of a pan tilt servo.\n\nhttps://m.youtube.com/watch?v=_iQWCRToUkA\n\nThe simplified version of how this works, is that I created an ROS node that utilizes OpenCV to syncronize the depth frame with the RGB -----> camera !!!  frame from the Kinect 360. In the callback function, I then calculate the centermost rgb pixel matrix coordinates of the bounding box if a detected object, and retrieve the depth data from the syncronized depth image (uv,xyz). I then convert that to a post stamped message that is sent to a modified \"head_tracker.py\" module from the original rbx2 code (sourced below).\n\nSome of the prep work and background on this project:\n\nI used Google's OpenImages V4 to train YOLO and create the pretrained weights for \"Human Head\" detection that I'm using here. (V5 is out now?.. mmmm juicy!) Here's my tutorial I made for this process: https://github.com/WyattAutomation/Train-YOLOv3-with-OpenImagesV4\n\nThe version of ROS used here is Melodic, running on Xubuntu 18.04. You have to build the freenect dependencies and ROS package for the 360 Kinect from source (doesn't appear to be an apt install option for melodic?)\n\nI also used leggedrobotic's darknet_ros ROS module for YOLO:\nhttps://github.com/leggedrobotics/darknet_ros\n\nThe robotics hardware I used here is a PhantomX pan/tilt turret from Trossen robotics with 2 Dynamixel ax18-a's: https://www.trossenrobotics.com/p/phantomX-robot-turret.aspx\n\nAnd probably the most important to mention out of all, I rewrote the \u201cnearest_pointcloud\u201d ROS node and other code from \u201cRobotics by Example, Vol2\u201d, retrofitting it for the purpose of tracking objects in 3D space from the 2D bounding box pixel coordinates published by the YOLOv3 ROS package: https://github.com/pirobot/rbx2\n\nI'll be willing to answer any questions on how I got this working, so feel free to ask!\n\nI have a version of this that uses an Astra Orrbec Pro and I am nearly finished setting it up to run on a Raspberry Pi 4, with Darknet/YOLO running remotely and publishing rostopics over WAN or LAN from my desktop (using a GTX 1060 for YOLO).\n\nI will post progress as it is made, as well as comprehensive documentation and source code on my GitHub account on this build, for whoever it may help. If you like it, feel free to visit my Patreon!", "link": "https://www.reddit.com/r/MachineLearning/comments/dik1lr/p_my_implementation_of_object_tracking_using_an/"}, {"autor": "KidDora7", "date": "2019-10-16 02:50:48", "content": "Cs ml project ideas suggestions /!/ Hello all, i am cs student i have to create an android application which utilizes hardware sensor like gyroscope, proximity sensor, acclerometer. We cannot use -----> camera !!! .\nAnd the app has to have some kind of basic machine learning implementation. \nCan anyone give suggestions for the possible ideas that i can use.\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/diiz8g/cs_ml_project_ideas_suggestions/"}, {"autor": "i-make-robots", "date": "2019-10-15 19:54:20", "content": "[P] I have a challenge for you! /!/ I know it's easier to learn robotics when I've decided on a robot to build.  In the same way, I bet it's easier to learn ML when you have a specific goal in mind.  Maybe even something novel.  So.  \n\nMy challenge to you is to train a ML artist.  this would be a network that inputs a bitmap -----> picture !!!  and outputs a set of vectors that could be drawn by a plotter robot.  I have lots of non-ML image-&gt;vector converters and I'm a big fan of [turtletoy.net](https://turtletoy.net).  I would like to see your network run on turtetoy given a source image.  They could be any style you want.  You don't have to share your training data, just the final resulting weights and the NN to run in javascript on their site.  Your result would be public for everyone to try and enjoy, while your trade secret training stays all yours.", "link": "https://www.reddit.com/r/MachineLearning/comments/diddno/p_i_have_a_challenge_for_you/"}, {"autor": "zimmer550king", "date": "2019-10-15 07:14:18", "content": "[D] How good is this idea: A website for machine learning enthusiasts where collaborators can label other people's data and get paid for it while also putting their own data to be labeled (of course then they would have to pay for it) /!/ Hello all. I want to know whether this seems like a good idea. I actually am looking for ideas for my Business Plan class and this seems to be like something that hasn't been done yet.\n\nBasically, the idea is that people can upload their data online with instructions on how to label it. The label can be as simple as assigning a label to the whole -----> picture !!!  and it can also be as complicated as marking individual pixels or something like that. I don't really plan on going too deep into this since it is only for academic reasons but feedback on this would be appreciated.\n\nI think the recent boom in the usage of neural networks means there will be a big market for this idea as everyone from independent machine learning enthusiasts to big corporations would take advantage of this. What do you guys think?", "link": "https://www.reddit.com/r/MachineLearning/comments/di46wx/d_how_good_is_this_idea_a_website_for_machine/"}, {"autor": "Aradarbel10", "date": "2019-10-14 21:27:16", "content": "Help With Creating a Pokemon Classifier /!/ Hello,\n\nI want to create a classifier that takes an -----> image !!!  of a Pokemon and outputs its name. I have a folder containing a picture of each pok\u00e9mon. The input images aren't going to be very different from the data set, and by that I mean all pok\u00e9mons are going to be on a white background, so it shouldn't be too complicated to perform. I had a few Ideas for how to do it:\n\n1. Have a CNN with an output for each pok\u00e9mon. This seems too complicated for what I need and also I have only one instance for each pok\u00e9mon.\n2. Color ratios: see how much of each color is in the image and compare it to values in the data set.\n3. Shadow: transform the input image into a b&amp;w shadow (who is that pok\u00e9mon style) and try to see which image of the data set fits the best by scaling, moving, and reflecting.\n\nI am not sure exactly how to implement any of these methods or if they will even work, so I'd love to hear what you say about it. What will be my best option?", "link": "https://www.reddit.com/r/MachineLearning/comments/dhxenp/help_with_creating_a_pokemon_classifier/"}, {"autor": "permalip", "date": "2019-11-08 11:32:33", "content": "[R] Natural -----> image !!!  reconstruction from brain waves: a novel visual BCI system with native feedback /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/dte66t/r_natural_image_reconstruction_from_brain_waves_a/"}, {"autor": "drr21", "date": "2019-11-08 10:21:02", "content": "Studies in the variation of the features extracted by a CNN? /!/ Hi,  I'm investigating what is the variation in the features extracted by a  CNN per class. In other words, if we have a CNN classifier, every time  we input an -----> image !!!  we can extract a feature layer previous to the  classification layer. Let's say we get 1000 features. I'm interesting in  knowing what's the variation inside of a class (e.g. dog) for all these  1000 features. Does it follow a gaussian distribution? Or more complex  distributions?\n\nI've been looking for work done in this task but I haven't found anything. Do you guys know if someone researched this? Thanks!!\n\nAlso posted here: [https://www.reddit.com/r/deeplearning/comments/dtdgb6/studies\\_in\\_the\\_variation\\_of\\_the\\_features/](https://www.reddit.com/r/deeplearning/comments/dtdgb6/studies_in_the_variation_of_the_features/)", "link": "https://www.reddit.com/r/MachineLearning/comments/dtdhqo/studies_in_the_variation_of_the_features/"}, {"autor": "nehalemlabs", "date": "2019-11-08 04:16:11", "content": "[P] Trained a StyleGAN on all emojis from emojipedia. Samples posted hourly. Please help describing samples! /!/ I set up a twitter bot posting a new sample every hour here:\u00a0[https://twitter.com/EmojiPainter](https://twitter.com/EmojiPainter)\n\nI am trying to collect a dataset with images and associated descriptions. I am hoping to collect human generated descriptions to augment the existing emojis (too few to train an -----> image !!!  - language model on).\n\nThe images can be unintentionally hilarious but also sometimes disturbing! Enjoy!", "link": "https://www.reddit.com/r/MachineLearning/comments/dt9zu9/p_trained_a_stylegan_on_all_emojis_from/"}, {"autor": "Kerlin_Michel", "date": "2019-11-06 23:27:43", "content": "[D] What are good heuristics when choosing classes for -----> image !!!  classification? /!/ For example let's say I want to classify eggs. And eggs in images can often be seen as just an egg, eggs in an egg cartoon and the carton can be closed or opened.\n\nA naive approach would be to put all these image under the class eggs. But it might work better if there are 2 classes one for eggs and one for eggs in carton so training should be easier since these can look quite different since a group of eggs looks much different from a closed carton of eggs. I also feel like separating  the classes can have unwanted outcomes like separating contexts. For example eggs withing cartons could rely on context of being in a kitchen and grocery store so it may less accurately predict an image has eggs if it is a carton of eggs in a farm.\n\nIs my thinking correct on this?\n\nWhat has been your experience with similar situations? \n\nThis question specifically focuses on image classification using neural nets.", "link": "https://www.reddit.com/r/MachineLearning/comments/dsolxx/d_what_are_good_heuristics_when_choosing_classes/"}, {"autor": "Luxonis-Brian", "date": "2019-11-06 18:11:28", "content": "[P] DepthAI hardware: RGBd, Myriad X VPU, Object-Tracking, Neural Network Accelerators for Raspberry Pi /!/ We wanted to share with you all about some embedded and low-cost hardware we've been working on that combines disparity depth and AI via Intel's Myriad X VPU. We've developed a SoM that's not much bigger than a US quarter which takes direct -----> image !!!  inputs from 3 cameras (2x OV9282, 1x IMX378), processes it, and spits the result back to the host via USB3.1.\n\nWe wanted disparity + AI so we could get object localization outputs - an understanding of where and what objects are in our field of view, and we wanted this done fast, with as little latency as possible. Oh, and at the edge. And for low power. Our ultimate goal is actually to develop a rear-facing AI vision system that will alert cyclists of potential danger from distracted drivers. An ADAS for bikes!\n\nThere are some Myriad X solutions on the market already, but most use PCIe, so the data pipeline isn't as direct as Sensor--&gt;Myriad--&gt;Host, and the existing solutions also don't offer a three camera solution for RGBd. So, we built it!\n\nHope the shameless plug is OK here (sorry mods!), and if anyone has any questions or comments, we'd love to hear it!\n\n[cnx-software article](https://www.cnx-software.com/2019/11/06/depthai-brings-ai-plus-depth-to-the-raspberry-pi/)\n\n[hackster.io article](https://www.hackster.io/news/luxonis-launches-depthai-depth-sensing-object-tracking-neural-network-accelerators-on-crowd-supply-de4e7e826f1f)\n\n[crowdsupply](https://www.crowdsupply.com/luxonis/depthai)\n\n[hackaday](https://hackaday.io/project/163679-luxonis-depthai)\nhttps://hackaday.io/project/163679-luxonis-depthai", "link": "https://www.reddit.com/r/MachineLearning/comments/dsk69k/p_depthai_hardware_rgbd_myriad_x_vpu/"}, {"autor": "nooob_Master_69", "date": "2019-09-28 23:33:43", "content": "Best way to learn distance metric learning? /!/ I am currently working on the topic and I have found papers that have different options about what are the necessary topics to domain to understand distance metric learning. Obviously metric spaces are a good start, but a look into the big -----> picture !!!  would be very helpful for me.", "link": "https://www.reddit.com/r/MachineLearning/comments/dankk2/best_way_to_learn_distance_metric_learning/"}, {"autor": "HumanSpinach2", "date": "2019-09-28 00:43:07", "content": "Intuition behind \"Recursive Autoconvolution for Unsupervised Learning of Convolutional Neural Networks\"? /!/ https://arxiv.org/abs/1606.00611\n\nWhat this paper does is learn CNN features by first applying K-means clustering  to -----> image !!!  patches, and then zero-padding the resulting cluster centers and applying autoconvolution recursively. The results are then used as features in a CNN (and it seems they do this to learn features for multiple layers, not just the first layer). The resulting features bear a very close resemblance to wavelets.\n\nIs there any intuition behind why convolving KNN image patches with themselves would give useful features? I know that taking the cross-correlation (which is very closely related to convolution) of a signal with itself can be used to reveal hidden structure in, for example, audio signals. But that's not really analogous to what they're doing here. The paper references numerous other works, and I've tried reading them but they're too mathematically dense for me to understand.\n\nIs there any intuitive rationale for why this works? The only reason I can think of is that applying autoconvolution makes a feature more selective to specific frequencies (because autoconvolution is equal to squaring the coefficients on the Fourier domain), but by that logic, a pre-defined family of wavelets should work just as well. \"Wavelet Scattering\" CNNs have actually shown pretty decent results using Gabor wavelets as predefined filters.\n\nI also observed that, although applying autoconvolution to a feature will make it more selective to frequency, this comes at the cost of scrambling the phase information. In the paper, they apply the recursive autoconvolution operator as many as 3 times, which is the same as raising the Fourier coefficients to power 8 - this would completely scramble the phases and I suspect that at that point you might as well just randomize the phases and it wouldn't make a difference.", "link": "https://www.reddit.com/r/MachineLearning/comments/da951v/intuition_behind_recursive_autoconvolution_for/"}, {"autor": "moizsawan", "date": "2019-09-27 13:31:31", "content": "Medical Image Classification /!/ Hi all. I am working on a medical -----> image !!!  based binary classification. So far, I have tried every pre-trained model and have even tried to train some models from scratch. But still I am not getting good results. I have only two features for the patient other than the image. Images are cell images. I have even tried to incorporate non-image data with the image data but still no results. I have also tried handcrafted features for the images but to no use. The issue is that even visually I can not differentiate between the two classes. Any comments on what other methods or approaches I could follow to get a better result? Currently, I am having a \\~60% accuracy but I need to get it past 70% at least.", "link": "https://www.reddit.com/r/MachineLearning/comments/da0g7n/medical_image_classification/"}, {"autor": "siddarth2947", "date": "2019-11-15 08:27:46", "content": "[D] DanNet, the CUDA CNN of Dan Ciresan in Jurgen Schmidhuber's team, won 4 -----> image !!!  recognition challenges prior to AlexNet /!/ probably many do not know this, I learned it by studying the references in section 19 of Jurgen's very dense [inaugural tweet](https://twitter.com/SchmidhuberAI/status/1180035193962401792) \n\nI knew AlexNet, the CUDA CNN by Alex Krizhevsky and Ilya Sutskever and Geoff Hinton which won ImageNet 2012, but prior to AlexNet, Jurgen's team with his \"outstanding Romanian postdoc Dan Ciresan ... won 4 important computer vision competitions in a row between May 15, 2011, and September 10, 2012\" with an earlier CUDA CNN, let me call this DanNet, the [blog post on their miraculous year](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html) links to a [summary of these contests](http://people.idsia.ch/~juergen/computer-vision-contests-won-by-gpu-cnns.html)\n\nI saw a news article claiming that AlexNet started a deep learning revolution in 2012, but actually the references show that DanNet was the [first superhuman CNN](http://people.idsia.ch/~juergen/superhumanpatternrecognition.html) in 2011 and also won a [medical imaging contest](http://people.idsia.ch/~juergen/first-time-deep-learning-won-medical-imaging-contest-september-2012.html) on images way bigger than AlexNet's \n\nthe most cited DanNet paper is [CVPR July 2012](http://people.idsia.ch/~juergen/cvpr2012.pdf), 5 months before [AlexNet at NIPS 2012](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf), but earlier descriptions of DanNet appeared at [IJCAI 2011](http://people.idsia.ch/~juergen/ijcai2011.pdf) and [IJCNN 2011](http://people.idsia.ch/~juergen/ijcnn2011.pdf) \n\nin his blog, Jurgen also cites CNN pioneers since Fukushima 1979, and GPU implementations of neural networks since [Jung and Oh 2004](https://www.sciencedirect.com/science/article/abs/pii/S0031320304000524)\n\nto be fair, [AlexNet](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf) cites [DanNet](https://arxiv.org/abs/1202.2745) and admits that it is similar, however, it does not mention that DanNet won all those earlier challenges\n\n[ResNet](https://arxiv.org/abs/1512.03385) beat AlexNet on ImageNet in 2015, but ResNet is actually a special case of the earlier [highway networks](https://arxiv.org/abs/1507.06228), also invented in Jurgen's lab, the \"First Working Feedforward Networks With Over 100 Layers,\" section 4 of [The Blog](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html) links to [an overview](http://people.idsia.ch/~juergen/highway-networks.html), he credits his students Rupesh Kumar Srivastava and Klaus Greff\n\nthere was a big reddit thread on section 5 of his blog, [Jurgen's GAN of 1990](https://www.reddit.com/r/MachineLearning/comments/djju8a/d_jurgen_schmidhuber_really_had_gans_in_1990/), and everybody knows LSTM, which won contests already in 2009, section 4 of [The Blog](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html), but I think many don't know yet that his team also was first in the CUDA CNN game", "link": "https://www.reddit.com/r/MachineLearning/comments/dwnuwh/d_dannet_the_cuda_cnn_of_dan_ciresan_in_jurgen/"}, {"autor": "ilia10000", "date": "2019-11-14 23:38:23", "content": "[D] Looking for a good open-source NAS implementation /!/ I'm looking for a good Neural Architecture Search implementation that I could use to learn a deep network for my custom dataset. The dataset is sequential (long sequences of \\~40 unique tokens), and the task is just sequence modeling. I'm happy to use a fixed window size and one-hot encoding to turn the inputs into fixed-size matrices if that makes life easier (e.g. turning this into an -----> image !!!  classification task). I've already used hand-crafted RNNs and attention-based models and achieved pretty high accuracy, but I'm hoping to try out NAS and see how it compares. \n\nI wanted to ask here and see if anyone has suggestions first before I sink a large number of hours into any one package. Some packages I've used/heard of:\n\nAutoKeras - Doesn't seem to work with the sequential form of the data but does work with the fixed-size matrices by using the ImageClassifier. Having a bunch of problems though as 0.4 version is a little broken, and 1.0 version isn't yet complete. \n\nauto-sklearn - My understanding is it focuses on XGBoost rather than neural nets.\n\n[https://github.com/quark0/darts](https://github.com/quark0/darts) \\- I really like the idea/paper for this one, has anyone had experience using it for a custom dataset?\n\n[https://github.com/carpedm20/ENAS-pytorch](https://github.com/carpedm20/ENAS-pytorch) \\- Same algorithm as AutoKeras (ENAS) but only RNNs are implemented so far. I see a lot of open issues on GitHub so a little apprehensive about committing time to this specific implementation. \n\nExploring Randomly Wired Neural Networks for Image Recognition ([https://arxiv.org/abs/1904.01569](https://arxiv.org/abs/1904.01569)) - I love this paper but can only find unofficial implementations on GitHub. Looking through them it seems like they don't achieve quite as high results as in the paper. Has anyone seen an official (or best) implementation of this?\n\nMicrosoft NNI ([https://github.com/microsoft/nni](https://github.com/microsoft/nni)) - This seems a little bit intense and I haven't had a chance to learn how to use it yet. It does seem to contain ENAS as one of the algorithms though, so this seems promising. \n\nIs one of these the right tool for me to use? Are there some other implementations out there that would be better suited for my task?", "link": "https://www.reddit.com/r/MachineLearning/comments/dwi09y/d_looking_for_a_good_opensource_nas_implementation/"}, {"autor": "big_skapinsky", "date": "2019-11-14 10:39:14", "content": "[D] Working on an ethically questionnable project... /!/ Hello all,\n\nI'm writing here to discuss a bit of a moral dilemma I'm having at work with a new project we got handed. Here it is in a nutshell : \n\n&gt;Provide a tool that can gauge a person's personality just from an -----> image !!!  of their face. This can then be used by an HR office to help out with sorting job applicants.\n\nSo first off, there is no concrete proof that this is even possible. I mean, I have a hard time believing that our personality is characterized by our facial features. [Lots of papers](http://alittlelab.com/littlelab/pubs/Little_07_personality_composites.pdf) claim this to be possible, but they don't give accuracies above 20%-25%. (And if you are detecting a person's personality using the big 5, this is simply random.) This branch of [pseudoscience](https://en.wikipedia.org/wiki/Physiognomy) was discredited in the Middle Ages for crying out loud.\n\nSecond, if somehow there is a correlation, and we do develop this tool, I don't want to be anywhere near the training of this algorithm. What if we underrepresent some population class? What if our algorithm becomes racist/ sexist/ homophobic/ etc... The social implications of this kind of technology used in a recruiter's toolbox are huge.\n\nNow the reassuring news is that the team I work with all have the same concerns as I do. The project is still in its State-of-the-Art phase, and we are hoping that it won't get past the Proof-of-Concept phase. Hell, my boss told me that it's a good way to \"empirically prove that this mumbo jumbo does not work.\"\n\nWhat do you all think?", "link": "https://www.reddit.com/r/MachineLearning/comments/dw7sms/d_working_on_an_ethically_questionnable_project/"}, {"autor": "blackjack503", "date": "2019-03-07 18:56:21", "content": "[Discussion] The effects of -----> image !!!  orientation on classification /!/ I recently saw a post on /r/dataisbeautiful where a user checked the classification accuracy of google cloud vision on an image duck/rabbit illusion ([https://www.reddit.com/r/dataisbeautiful/comments/aydqig/is\\_it\\_a\\_duck\\_or\\_a\\_rabbit\\_for\\_google\\_cloud\\_vision/](https://www.reddit.com/r/dataisbeautiful/comments/aydqig/is_it_a_duck_or_a_rabbit_for_google_cloud_vision/))\n\n&amp;#x200B;\n\nI was wondering what could cause such an effect and how we can prevent such a thing from affecting our own models.\n\nIs it just a lack of training data where the ducks and rabbits are always shown in a certain orientation or could there be other factors that need to be considered?", "link": "https://www.reddit.com/r/MachineLearning/comments/ayg7iz/discussion_the_effects_of_image_orientation_on/"}, {"autor": "ALLIRIX", "date": "2019-03-07 01:10:05", "content": "[Discussion] Can you increase the resolution an -----> image !!!  by using multiple offset -----> image !!! s and an ML algorithm? More in description. /!/ I was just walking to uni thinking about how super-resolution has a limit when using a single image because a pixel is a average of its contents and therefore loses the distribution of colour behind it. My understanding is current super-resolution techniques find patterns in this neighbourhood of averages using ML and use these patterns to enhance an image.\n\nBut, we can capture multiple photos. As an ideal scenario, if we take 10 photos, each offset from the last by 0.1 Pixels, up and down, would it be possible to use a machine learning algorithm to merge the image into one that is 10x larger?\n\nI'm completely new to computer vision, machine learning and statistics as a undergraduate student in engineering but it fascinates me.\n\nEdit: I have made a [picture](https://imgur.com/a/NwWbfXY) to help communicate what I mean. You may not need a fancy ML algorithm since averaging out (blue region) the overlaid pixels (red &amp; green) can store the lost information in a higher resolution image. But averaging out larger pixels into the smaller region is using information from outside that pixel to determine its value so does that nullify the result or mean a more complex method is required?", "link": "https://www.reddit.com/r/MachineLearning/comments/ay6n1l/discussion_can_you_increase_the_resolution_an/"}, {"autor": "ALLIRIX", "date": "2019-03-07 00:35:26", "content": "Can you increase the resolution an -----> image !!!  by using multiple offset -----> image !!! s and an ML algorithm? More in description. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ay6amu/can_you_increase_the_resolution_an_image_by_using/"}, {"autor": "karstenchu", "date": "2019-03-06 21:13:11", "content": "[P] Learning and Detecting Low-Resolution Objects (Anti-Super Resolution) /!/ I just wanted to run an idea I had by some ML experts.  \n\n&amp;#x200B;\n\nSay I have a gigapixel digital photograph and a digital representation of a polaroid from the 80's.  Could I use high resolution/low resolution pairs of -----> image !!! s to train a network to recognize objects in the low resolution -----> image !!! ?  My methodology would be to go through both images, chip out all instances of say cars, and then train an ANN to predict what the \"polaroid\" version of the high resolution image would look like.  Ultimately, I'd like to be able to then use segmentation to go through polaroids and detect cars.\n\n&amp;#x200B;\n\nDoes this make sense, is it feasible and what am I missing?  It's kind of like downsampling, except there's also a difference in sensor phenomenology I'd like the network to learn.  ", "link": "https://www.reddit.com/r/MachineLearning/comments/ay413m/p_learning_and_detecting_lowresolution_objects/"}, {"autor": "mekass", "date": "2019-06-26 20:04:20", "content": "[Project] Similar -----> image !!!  search using pre-trained ResNet50 as -----> image !!!  embeddings. /!/ Hey guys!\n\n&amp;#x200B;\n\nFinished my experimental project where I tried to come up with algorithm to find similar images using pre-trained ResNet50 model for image features generation and cosine similarity as distance metric. At this experiment I used DeepFashion image dataset.  \n\n\nTake a look! [https://github.com/tomasrasymas/simimg](https://github.com/tomasrasymas/simimg)", "link": "https://www.reddit.com/r/MachineLearning/comments/c5v05g/project_similar_image_search_using_pretrained/"}, {"autor": "dominik_mai", "date": "2019-06-26 12:37:37", "content": "[P] Telegram bot which rates the aesthetics of your photos /!/ Hi guys,\n\nas a part of my master thesis, in which I try to predict how aesthetic an -----> image !!!  is, I wrote a telegram bot that can rate -----> image !!! s you send to it.\n\n&amp;#x200B;\n\nActually there are two bots, you can reach the first one at [https://t.me/ImageAestheticsBot](https://t.me/ImageAestheticsBot) which will rate all kind of images. The second one is trained only with photos with people on it, so you can use that for photos like portraits. This one you can reach at [https://t.me/ImageAestheticsPeopleBot](https://t.me/ImageAestheticsPeopleBot) .\n\n&amp;#x200B;\n\nIf you like to read something on how image aesthetics can be rated by neuronal nets [NIMA: Neural Image Assessment](https://arxiv.org/abs/1709.05424) is a good start.\n\n&amp;#x200B;\n\nI would really like it if you try this bot and maybe give me some feedback if you think it's useful.\n\n&amp;#x200B;\n\nIf you have questions feel free to ask and if you know some other subreddits where this post might fit in please tell me. This is my first post here so any advice about posting, if I did something wrong, would be cool too.\n\n&amp;#x200B;\n\nThanks in advance!", "link": "https://www.reddit.com/r/MachineLearning/comments/c5pb1j/p_telegram_bot_which_rates_the_aesthetics_of_your/"}, {"autor": "phdsudip", "date": "2019-06-26 06:53:17", "content": "How to save -----> image !!!  after clustering using kmeans? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/c5mjs3/how_to_save_image_after_clustering_using_kmeans/"}, {"autor": "fuckEAandTheirGame", "date": "2019-06-26 00:50:18", "content": "[D] I think Neural Nets can help r/estoration /!/ The idea of r/estoration is to restorate old images and remove the little specs, tears, discolorations, etc... And I think that this process could be easily automated for your average -----> image !!!  via neural networks, I mean trying to recover data from an -----> image !!!  as already been done (denoising autoencoder, super resolution for example), the only thing is that what is one of the most valuable thing (IMO) in Deep Learning is the dataset, plenty of autoencoder/GAN are out there but they are not worth anything if they can't be trained, hopefully someone or mutiple people (and that obviously will make things faster) reading this is/are motivated to make a dataset.\n\n&amp;#x200B;\n\nSo I am just hoping to spread the word around, if anyone have the patience to make a dataset, maybe even if you have the resources to train a model/host a website for people to use to restore their images plenty of people will be grateful, I really think there is something to be done here.\n\n&amp;#x200B;\n\n^(Also some people will probably tell me \"can't you do it?\", in theory I can, in practice I don't have the patience to gather 500+ images and edit them, also I have a r9 390 so I am a bit limited hardware-side, I use plaid-ML+keras on windows 10 for those wondering, plus if I spread the word around hopefully someone with a lot more of ressources/knowledge than me can do something 10x better than what I could.)\n\n^(Speaking of knowledge, I have thought about the architecture of the NN, I think in a first time an autoencoder would be easier (maybe a gan after, a fully convolutionnal one (so that it can accept most images sizes, with strides of 2 instead of pools, that output a 3 layer image that is then added to the input image to produce the output image, so as to reduce blurriness and maybe use dssim as loss, I will just leave that out there in case someone wants a \"guide\" instead of using a random autoencoder.)", "link": "https://www.reddit.com/r/MachineLearning/comments/c5iia0/d_i_think_neural_nets_can_help_restoration/"}, {"autor": "inglorious_noname", "date": "2019-01-20 22:38:46", "content": "GAN Latent space interpolation before it became cool with GPUs - MJ's Black or White. /!/ &amp;#x200B;\n\n![video](lgw5y475nnb21 \"Associated paper: http://www.hammerhead.com/thad/morph.html - Feature based -----> image !!!  metamorphosis. \")", "link": "https://www.reddit.com/r/MachineLearning/comments/ai3a5f/gan_latent_space_interpolation_before_it_became/"}, {"autor": "python_noob_001", "date": "2019-01-20 20:14:40", "content": "Are usually non------> image !!!  data things turned into -----> image !!!  representations for the sole purpose of classification by -----> image !!!  analysis? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ai1sm5/are_usually_nonimage_data_things_turned_into/"}, {"autor": "UltraMarathonMan", "date": "2019-01-20 16:27:58", "content": "[D] Deep learning is representation learning (by machines). Science is representation learning (by humans). /!/ Do you agree/disagree? My favorite example from science (the one that arguably launched the scientific revolution):\n\n&amp;#x200B;\n\n![video](z54l05xvtlb21)\n\nThis is an excerpt slide from [lecture on deep learning basics](https://www.youtube.com/watch?v=O5xeyoRL95U).\n\nSun/planet trajectory visualization by [Giorgi Koberidze](https://www.youtube.com/watch?v=waexG16WZrE).\n\nIn case the above video/gif doesn't work, here's an -----> image !!! :\n\n![img](b2jybak6ulb21)\n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/ahz3ko/d_deep_learning_is_representation_learning_by/"}, {"autor": "thetechkid", "date": "2019-01-20 04:32:03", "content": "[P] Validation issues /!/ I am working on an -----> image !!!  classification project and I am having some issues with my models. I can get my training validation to get highish(like 80%-90%), however; my validation data never seems to get higher than 20%. I have tried several things and hope someone may be able to help me out, and here is some information in hope of it helping:\n\n&amp;#x200B;\n\n\\- I am using Keras with TensorFlow\n\n&amp;#x200B;\n\n\\- I am starting with images of size 1024x1024 and scaling them down to 256x256 in the code, and they are of chest x-rays. I use the class\\_mode of grey scale in my models if that makes a difference. \n\n&amp;#x200B;\n\n\\- I have 8 types of classes, I don't have a predefined dataset so the way I am separating them is through sub folders in the training and testing folders. I also use \"train\\_datagen.flow\\_from\\_directory\" and \"test\\_datagen.flow\\_from\\_directory\" for the data(I'm not sure if this is the right approach for the labels or classes, so if it is not then I would appreciate a helping hand in correcting this). \n\n&amp;#x200B;\n\n\\- For my test images I have about 7474 images and 1468 for my test data currently, I can add more but I've been slowly ramping up the amount of images in each run and the data validation is not improving.\n\n&amp;#x200B;\n\n\\- I have tried the model structures AlexNet, VGG16, VGG19, and ResNet.\n\n&amp;#x200B;\n\n\\- As for my los and metrics I have used loss=binary\\_crossentropy and metrics=categorical\\_accuracy and I have also tried loss=categorical\\_crossentropy and metrics=categorical\\_accuracy. I have tried going through the documentation for this on keras and I'm not sure if Categorical Accuracy is the correct metric to be using, but the documentation doesn't go into much detail(or at least isn't very good about what metrics are good for what). As for the optimizer I have tried Adam, Nadam, and I have had the best results with SGD.\n\n&amp;#x200B;\n\n\\- I normally run about 50 epochs and then the loss for validation grows a lot more rapidly and the accuracy stops improving with the training accuracy increases and the los does not. \n\n&amp;#x200B;\n\n\\- I have tried batch normalization with slightly better results than without, but L1 and L2 regularization seem to be detrimental effects on the accuracy of my models.\n\n&amp;#x200B;\n\nI'm at a loss for what else I could do to improve my results, but if anymore information is required to help with the issue I will do my best to provide it. ", "link": "https://www.reddit.com/r/MachineLearning/comments/ahu8ds/p_validation_issues/"}, {"autor": "waspinator", "date": "2019-01-18 21:06:42", "content": "[D] automatically find pixel-level differences in classes /!/ If I have an -----> image !!!  dataset of portrait -----> image !!! s with two classes, \"blonds\" and \"brunettes\". The samples are labeled, but only on the whole image level (ex: this is a image of a brunette), so no pixel-level annotations. Can I generate pixel-level annotations from a trained classifier which can discriminate between the two classes?", "link": "https://www.reddit.com/r/MachineLearning/comments/ahf0qx/d_automatically_find_pixellevel_differences_in/"}, {"autor": "zsajjad", "date": "2019-01-18 14:37:00", "content": "-----> Image !!!  detection in the browser, using ONNX.js &amp; React.JS", "link": "https://www.reddit.com/r/MachineLearning/comments/ahb17x/image_detection_in_the_browser_using_onnxjs/"}, {"autor": "MrKotia", "date": "2019-01-27 11:47:24", "content": "[D] Best approach to variable -----> image !!!  sizes for Image Classification? /!/ I am working on classification of brain tumors. The dataset is comprised of brain images from various angles, with a border and mask of the tumor position. I have cropped the rectangle that contains the tumor, as the other parts of the image are irrelevant and vary due to the different angles from which the image is taken. Now I am left with a series of images of the tumor, each belonging to 1 of 3 possible cases of tumor. But to train this data for classification, I need to have the 2d image array in a uniform shape I believe.\n\n&amp;#x200B;\n\nPossible approaches:\n\n1) Crop each image to a fixed size (say 100x100). But this would result in loss of data and also depending on position of the tumor in image, I could be facing uneven crops due to reaching the edge of the image.\n\n2) Pad the image to a fixed shape, bigger than the biggest cropped image shape (say 350x350). But again this would introduce noise in the data I suppose, and I'm not sure how can I pad the image uniformly across all 4 sides of the rectangle.\n\n&amp;#x200B;\n\nAny responses are welcome, as I couldn't find viable solutions to this problem elsewhere.", "link": "https://www.reddit.com/r/MachineLearning/comments/akbe39/d_best_approach_to_variable_image_sizes_for_image/"}, {"autor": "KichangKim", "date": "2019-01-27 11:36:58", "content": "[P] Tag estimation for anime-style girl -----> image !!!  /!/ Hi. I created tag estimation system for anime-style girl image. This is my hobby project and any of feedbacks are welcome.\n\nWeb : http://kanotype.iptime.org:8003/deepdanbooru/\nTelegram Bot : https://telegram.me/deepdanbooru_bot\n\nHere is breif information for this system.\n- Used library : CNTK\n- Network : resnet based (299x299 image -&gt; about 6000 tags)\n- Training time : about 28 days (Ryzen 7 1800X + Geforce 1070)\n\nI hope you enjoy this.", "link": "https://www.reddit.com/r/MachineLearning/comments/akbc11/p_tag_estimation_for_animestyle_girl_image/"}, {"autor": "TheSkipRow", "date": "2019-01-27 10:02:35", "content": "[D] Is there any app that enhances writing in images? /!/ So I have some stuff to read for my Uni and the -----> image !!!  quality is kinda shit. Problem is, the guy lost the original documents, so the images is all we have. It's handwriting if that makes a difference.", "link": "https://www.reddit.com/r/MachineLearning/comments/akatzd/d_is_there_any_app_that_enhances_writing_in_images/"}, {"autor": "SoulDrivenOlives", "date": "2019-01-26 19:04:36", "content": "[D] An analysis on how AlphaStar's superhuman speed is a band-aid fix for the limitations of imitation learning. /!/ I will try to make a convincing argument for the following:\n\n1. AlphaStar played with superhuman speed and precision. \n2. Deepmind claimed to have restricted the AI from performing actions that would be physically impossible to a human. They have not succeeded in this and most likely are aware of it.\n3. **The reason why AlphaStar is performing at superhuman speeds is most likely due to it's inability to unlearn the human players tendency to spam click. I suspect Deepmind wanted to restrict it to a more human like performance but are simply not able to.** It's going to take us some time to work our way to this point but it is the whole reason why I'm writing this so I ask you to have patience.\n\nFirst of all I want to clarify that I am a layman. I've been following AI and the Starcraft 2 scene for years but I do not claim to be an expert in either topic. If you notice any misconceptions in what I'm about write please do point it out. I'm only a fanboy and all this is fascinating to me. This essay contains a lot of speculation and I will admit that I can't prove all of my core claims definitively. Having said that, if you are so kind to read all of this and disagree with me, please argue in good faith. I would love to be proven wrong. Ok, here we go.\n\n# The Superhuman Speed of AlphaStar\n\n&gt;David Silver*:* [*\"AlphaStar can't react faster than a human player can, nor can it execute more clicks than a human player\".*](https://www.youtube.com/watch?v=UuhECwm31dM&amp;t=99s)\n\nThe Starcraft 2 scene was dominated in 2018 by a player called Serral. He is the current world champion and won 7 out of 9 major tournaments he attended that year resulting in the single most dominant run of any Starcraft 2 player in the history of the game. This guy is fast. Maybe the fastest player in the world. \n\n[Here's a first person view of him playing.](https://www.youtube.com/watch?v=HRsDAX8DfBw&amp;t=780) Serral is the pinkish white player. Take a look at his APM displayed in the upper left corner of the screen. APM is short for actions per minute. Basically it is a number that represents how fast the player is clicking on his mouse and keyboard. At no point is he able to sustain more than 500 APM for long. There are burst of 800 but they last only fractions of a second and are most likely resulted from spam clicking, which I will be discussing shortly.\n\nWhile arguably the fastest human player is able to sustain an impressive 500 APM, AlphaStar had bursts going up to 1500+. These inhumane 1000+ APM bursts sometimes lasted for 5 seconds stretches and were full of meaningful actions. 1500 actions in a minute translates to 25 actions a second. This is physically impossible to do. I also want you to take into account that in a game of Starcraft 5 seconds is a long time, especially at the very beginning of a big battle. If the superhuman execution during the first 5 seconds gives the AI an upper hand it will win the engagement by a large margin because of the snowball effect. [An engagement in game 3 vs Mana. AlphaStar sustain a 1000+ APM over a period of 5 seconds.](https://www.youtube.com/watch?v=H3MCb4W7-kM&amp;t=59m30s) Another one in game 4 had bursts that go up to [1500+ APM.](https://www.youtube.com/watch?v=cUTMhmVh1qs&amp;t=2h11m32s) One of the commentators points out how the average APM is still acceptable but it is quite clear that the sustained bursts are way higher than what a human could do. \n\n# Spam-clicks, APM and the Surgical Precision of a Robot\n\nMost human players have a tendency to spam click. Spam-clicks are exactly what they sound like. Meaningless clicks that don't have an effect on anything. For example, a human being might be moving his army and when they click to where they want the army to go, they click more than once. What effect does this have? Nothing. The army won't walk any faster. A single click would have been enough. Why do they do it then? There are two reasons:\n\n1. Spam-clicking is the natural by-product of a human being trying to click around as fast as possible.\n2. It helps to warm up finger muscles.\n\nRemember that player Serral we talked about earlier? The impressive thing about him is actually not how fast he is clicking but how precise he is. Serral not only has a really high APM (the total clicks per minute, including spam-clicks) but also a ridiculously high effective-APM (the total clicks per minute, excluding spam-clicks). \n\n[Here's a former proplayer losing his mind on twitter after discovering the effective-APM of Serral](https://twitter.com/retjah/status/959047573930696704?lang=fi). An E-APM that high is practically unheard of. 344 is so much that I have hard time believing it. But wait. If AlphaStar can potentially play without spam, wouldn't this mean that it's peak E-APM could be equal to it's peak APM? This makes the 1000+ spikes even more inhumane. Now also take into consideration that AlphaStar plays with perfect accuracy. It always clicks exactly where it intends to. Humans missclick. \n\nThere is a clear, almost unanimous consensus among the Starcraft 2 scene that AlphaStar performed sequences that no human could ever hope to replicate. It was faster and more precise than what is physically possible. The most mechanically impressive human pro in the world is several times slower. The accuracy can't even be compared. \n\n**David Silver's claim that AlphaStar can't execute anything human player a human being can't replicate is simply not true.** \n\n# Doing Things the Right Way VS Doing Things The Fast Way\n\n&gt;OriolVinyals:  It is important that we play the games that we created and collectively agreed on by the community as \u201cgrand challenges\u201d . We are trying to build intelligent systems that develop the amazing learning capabilities that we possess, so it is indeed desirable to make our systems learn in a way that\u2019s as \u201chuman-like\u201d as possible. As cool as it may sound to push a game to its limits by, for example, playing at very high APMs, that doesn\u2019t really help us measure our agents\u2019 capabilities and progress, making the benchmark useless. \n\nWhy is Deepmind interested in restricting the agent to play like a human? Why not just let it run wild with no limitations? The reason is that Starcraft 2 is a game that can be broken by mechanical perfection. [In this video](https://www.youtube.com/watch?v=IKVFZ28ybQs) a bot attacks a group of tanks with perfect micro. When the unit control is this good, the AI doesn't even need to learn strategy. The purpose of this exercise is not lost on me. Deepmind is not necessarily interested in creating an AI that can beat Starcraft but advancing AI research as a whole.  It is deeply unsatisfying to have prominent members of this research project make claims of human-like mechanical limitations when the agent is very obviously breaking them and winning it's games specifically because it is demonstrating superhuman execution. \n\nAlphaStar is able to outperform human players with unit control that was not taken into consideration when the game developer was carefully balancing the game. This can obfuscate any strategic thinking the AI has learned. It can even make the strategic thinking completely unnecessary. This is not the same thing as being stuck in a local maxima. **Abusing superior control is very likely to be** **the best and most effective way to play the game** as disappointing as that sounds. \n\nThis is what one of the pros who played AlphaStar had to say about it's strengths and weaknesses:\n\n&gt; Mana: I would say that clearly the best aspect of its game is the unit control. In all of the games when we had a similar unit count, AlphaStar came victorious. The worst aspect from the few games that we were able to play was its stubbornness to tech up. It was so convinced to win with basic units that it barely made anything else and eventually in the exhibition match that did not work out. There weren\u2019t many crucial decision making moments so I would say its mechanics were the reason for victory. \n\nThere's almost unanimous consensus among Starcraft fans that AlphaStar won almost purely because of it's superhuman speed, reaction times and accuracy. The pro who played against seems to agree. There was a member of the Deepmind team who played against AlphaStar before they let the pros test it. Most likely he would agree with the assesment. David Silver and Oriol Vinyal keep repeating the mantra of how the bot is only able to do things that a human could do as well. \n\nThis does not sound like doing things the right way. Something about this is really sketchy.\n\n# Why Does Deepmind Allow AlphaStar to Have Super-Human Mechanical Ability?\n\nNow we finally get to the meat and potatoes of this essay. First let's recap.\n\n* We know what APM, E-APM and spam-clicking are. \n* We have a rudimentary understanding of what the upper limits of human play looks like. \n* We understand that AlphaStars gameplay is in direct contradiction to what the developers claim it was allowed to be able to execute. \n* We understand that the consensus among Starcraft scene is that the unit control won the games and that strategic thinking wasn't even needed. \n* We understand that the goal of Deepmind is not to create a microbot or abuse the game in ways it was never meant to be played like. \n* It is incredibly unlikely that no one in Deepmind's Starcraft AI team thought burst APM of 1500+ were unrealistic. Their Starcraft guy knows more about the game than I do. They are close with Blizzard. It is in their interest (see the previous bullet point) to make the bot act as close to a human as possible.  \n\n\nThank you for sticking with me for this long. Here's what I suspect happened:\n\n1) At the very start of the project Deepmind agrees upon heavy APM restrictions on AlphaStar. At this point the agent is not allowed to have superhuman bursts of speed we saw in the demonstration. These restrictions most likely take into account.\n\n* **The average APM across the whole game.**\n* **Maximum burst APM over a short period of time.** I think capping it around 4-6 clicks per second would be reasonable. Remember Serral and his 344 E-APM that was head and shoulders above the competition? That is less than 6 clicks per second. The AlphaStar that played against Mana was able to perform 25 clicks per second over sustained periods of time. This is so much faster than even the fastest spam clicks a human can do that I don't think the original restrictions allowed for it. \n* **Minimum time between clicks.** Even if the speed bursts of the bot were capped, it could still perform almost instantaneous actions at the beginning of the time slice in an inhuman way. A human being obviously could not do this.  \n\nSomething like that. Some people would argue for adding a random element on accuracy as well but I suspect that would hinder the training way too much.\n\n2) Next Deepmind downloads thousands of high-ranking amateur games and begins imitation learning. At this stage the agent is simply trying to imitate what humans do in games. \n\n3) The agent adopts a behavior\u00a0of spam-clicking. This is highly likely because\u00a0human players spam click so much during games. It is almost certainly the single most repeated pattern of action that humans perform and thus would most likely root itself very deeply into the behavior of the agent.\n\n4) AlphaStars maximum burst APM is restricted close to how fast a human spam clicks. **Because most of the actions Alphastar is executing are spam clicks, it does not have the APM available to experiment in fights.**\u00a0If the agent doesn't experiment, it won't\u00a0learn. Here's what one of the developers said in an AMA yesterday, I think he tipped his hand: \n\n&gt;OriolVinyals: Training an AI to play with low APM is quite interesting. In the early days, we had agents trained with very low APMs, but they did not micro at all.  \n\n5) In order to speed up development they change APM restrictions to allow high bursts. Here are the APM restrictions that AlphaStar was playing in the demonstration:\n\n&gt;OriolVinyals:  In particular, we set a maximum of 600 APMs over 5 second periods, 400 over 15 second periods, 320 over 30 second periods, and 300 over 60 second period. If the agent issues more actions in such periods, we drop / ignore the actions. These were values taken from human statistics. \n\nThis is the equivalent of lying through statistics. It looks reasonable, but it allows for superhuman execution.\n\nThere's a limit to how fast a human can spam click. The most typical form of spam clicking is issuing a movement or attack commands to a unit. This is done by clicking with your mouse. Try clicking your mouse as fast as you can. The agent\u00a0learnt that kind of\u00a0spam clicking. It would not be clicking faster because the humans it is imitating are not clicking faster. **The extra APM that allows it\u00a0to go to superhuman speeds\u00a0can be considered \"free\"\u00a0APM\u00a0wich it can experiment with.**  \n\n6) The free APM\u00a0is used to experiment in engagements. This kind of interaction would happen often while training.\u00a0AlphaStar starts to learn new kind of behavior that leads to better outcomes and it starts to break away from constant spam clicking.   \n\n7) If the agent learned actual useful actions why then didn't Deepmind go back to the speculated initial harsher, humanlike limitations on APM? It couldn't be because they didn't realize that what the bot was doing was superhuman. The Starcraft community has\u00a0almost unanimous consensus that AlphaStar had superhuman micro. The human pros said in the ama that AlphaStars greatest strength was it's unit control and greatest weakness it's strategic thinking. The Starcraft people within Deepmind's team must have been thinking the same. The reason is probably because the agent still occasionally displays spam clicking. Even though based on analyzing the games it seems to be able to execute crisply with very little spam, it still regularly engage in it. This is apparent in game 1 against Mana when Alphastar is moving up the ramp. The agent was spamclicking movement commands. It still had not unlearned it. The spam clicking would hurt the agent most during big engagements and the APM cap was tinkered to allow it to perform well even in those. \n\n# You Wrote a Damn Book, Why Do You Even Care OP?\n\nSo there you have it. The agent was not able to un-learn spam clicking and Deepmind had to tinker with the APM cap to allow experimentation. This had unfortunate side effect of superhuman execution wich resulted in the agent breaking the game by being able to execute strategies that were never intended to be possible in the game. \n\nWhat leaves the sourest taste in my mouth is this -----> image !!! : \n\nhttps://i.redd.it/ctuungv1dtc21.png\n\nIt seems to be designed to mislead people unfamiliar with Starcraft 2. It seems to be designed to show how the APM of AlphaStar was reasonable. Look at Mana's APM and compare that to AlphaStar. While the mean of Mana is higher, the tail of AlphaStar goes way above what any human is capable of doing with any kind of intent of precision. Notice how Mana's peak APM is around 750 while AlphaStar is above 1500. Now take into account that Mana's 750 is almost 50% spamclicks while AlphaStars is a lot less spammy. \n\nNow take a look at TLO's APM. The tail goes up to 2000's. Think about that for a second. How is that even possible? It is made possible by a trick called [rapid fire](https://www.youtube.com/watch?v=N2dNK-mIWkQ&amp;t=1m25s). TLO is not clicking super fast. He is holding down a button and the game is registering this as 2000 APM. The only thing you can do with rapid fire is to spam a spell. That's it. TLO just over-uses it for some reason. The neat little effect is that this is masking AlphaStars burst APM and making it look reasonable to people who are not familiar with Starcraft.\n\nThis is literally lying through statistics. ", "link": "https://www.reddit.com/r/MachineLearning/comments/ak3v4i/d_an_analysis_on_how_alphastars_superhuman_speed/"}, {"autor": "R4rathee", "date": "2019-01-26 07:38:32", "content": "Gender Estimation from profile -----> picture !!!  /!/ Hi Friends,\n\nThis is my first post on Reddit. Please bear with me :)\n\nI've around 6k photos from Linkedin profiles and I want to guess the gender based on the profile photos.\n\nCan anybody help me with python package or any other efficient method to determine the gender with good accuracy for all images in one go?\n\nRegards,\nOmbir", "link": "https://www.reddit.com/r/MachineLearning/comments/ajyy28/gender_estimation_from_profile_picture/"}, {"autor": "sirkloda", "date": "2019-04-26 21:29:39", "content": "[D] What your favorite / the best GAN -----> image !!!  generator implementation currently available on github? /!/ I just wanted to have an overview of the new stuff since I've been out of the loop for a while. Are there any github projects that implement the GAN and upscaling etc yet? Which one produces the most pleasant, hidef images these days? Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/bhrpg4/d_what_your_favorite_the_best_gan_image_generator/"}, {"autor": "Mjjjokes", "date": "2019-04-26 01:48:34", "content": "[D] Given a training set of real 3d scenes, would it be possible to accurately estimate the contents of a single voxel outside of the -----> camera !!!  scene of a test 3d scene? /!/ Voxel = 3d pixel", "link": "https://www.reddit.com/r/MachineLearning/comments/bhgrvc/d_given_a_training_set_of_real_3d_scenes_would_it/"}, {"autor": "Bicharro27", "date": "2019-04-25 23:41:09", "content": "Build an android app (Machine Learning related) from python source code /!/ Hey guys, I\u00b4ve been developping a project that envolves deep learning and computer vision and it's turning out to be quite an amazing result. Since it is related to real-time -----> camera !!!  recording and processing, the next step would be to develop an app so it can run in an android cell. The source code is fully written in python and, though Java also has the libraries, it would cost me a lot of time to re-write all the code into Java code, and I'm better used to python's openCV. Any thoughts and advices on how I can build an app using the code I already have in python? I've heard of BeeWare and Kivy, but don't really know Which one's better and how to work with them yet. Thanks in advance!", "link": "https://www.reddit.com/r/MachineLearning/comments/bhfk6b/build_an_android_app_machine_learning_related/"}, {"autor": "DrPseud", "date": "2019-04-25 15:30:12", "content": "-----> Image !!!  recognition by a noob /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/bh9zek/image_recognition_by_a_noob/"}, {"autor": "coolwulf", "date": "2019-12-12 17:19:16", "content": "[Project] I created 3D reconstruction using single X-ray -----> image !!!  for Pediatric Orthodontics applications", "link": "https://www.reddit.com/r/MachineLearning/comments/e9qhxj/project_i_created_3d_reconstruction_using_single/"}, {"autor": "coolwulf", "date": "2019-12-12 16:55:23", "content": "[P] [P] 3D reconstruction using single X-ray -----> image !!!  for Pediatric Orthodontics applications", "link": "https://www.reddit.com/r/MachineLearning/comments/e9q5w2/p_p_3d_reconstruction_using_single_xray_image_for/"}, {"autor": "coolwulf", "date": "2019-12-12 16:52:49", "content": "[P] I created 3D reconstruction using single X-ray -----> image !!!  for Pediatric Orthodontics applications", "link": "https://www.reddit.com/r/MachineLearning/comments/e9q4kn/p_i_created_3d_reconstruction_using_single_xray/"}, {"autor": "ccwpog", "date": "2019-12-12 16:11:41", "content": "[R] horse breed misclassification /!/ I have heard from my professor that a current research article about classifying images of horse breeds by means of CNN got debunked. The reason was that the input -----> image !!!  data had their corresponding labels on the -----> image !!! s themselves so the classifier learned to achieve it's high accuracy looking at the correct labels instead of the actual horses. Apparently nobody bothered to have a closer look on the input files.\nUnfortunately, I cannot find the article or related ones online. Would anybody else know where to find it? Thanks in advance!", "link": "https://www.reddit.com/r/MachineLearning/comments/e9pkcr/r_horse_breed_misclassification/"}, {"autor": "veqtor", "date": "2019-12-12 11:24:35", "content": "[R] StyleGAN2: Analyzing and Improving the Image Quality of StyleGAN /!/ **Abstract**\n\nThe style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative -----> image !!!  modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent vectors to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably detect if an image is generated by a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.\n\nArxiv: [https://arxiv.org/abs/1912.04958](https://arxiv.org/abs/1912.04958) \n\nPDF: [https://arxiv.org/pdf/1912.04958.pdf](https://arxiv.org/pdf/1912.04958.pdf) \n\nYoutube: [https://www.youtube.com/watch?v=c-NJtV9Jvp0](https://www.youtube.com/watch?v=c-NJtV9Jvp0) \n\nCode: [https://github.com/NVlabs/stylegan2](https://github.com/NVlabs/stylegan2)", "link": "https://www.reddit.com/r/MachineLearning/comments/e9md4j/r_stylegan2_analyzing_and_improving_the_image/"}, {"autor": "TRAINS_CHOOCHOO", "date": "2019-12-11 23:07:27", "content": "[D] Best approach to fine-classification of -----> image !!!  objects? /!/ I'm basically looking to make a classifier that gives a binary (yes/no) response to an image's subject matching my model. For example let's say I wanted it to match copperhead snakes and only copperhead snakes. Is there a good cloud solution for this?\n\nI tried AWS Rekognition but it wanted a minimum of 2 labels and I felt the results wouldn't be good enough after doing a test run with \\~120 images. I know that's relatively small but don't want to waste time/money with a cloud solution if it won't get me there.\n\nWill Google Vision have better results? Would I need to roll my own solution in Python? Or is it unrealistic for a novice solution to tell one type of snake from another?", "link": "https://www.reddit.com/r/MachineLearning/comments/e9ew9r/d_best_approach_to_fineclassification_of_image/"}, {"autor": "StellaAthena", "date": "2019-12-11 23:04:11", "content": "[D] Concerns about \"Face Beautification: Beyond Makeup Transfer\" /!/  I came across the paper \"Face Beautification: Beyond Makeup Transfer\" and was appalled at the poor ethical and scientific practice shown by the paper. I sent the following email to the PC and the D&amp;I chairs, but I wanted to share the critique with the community as well:\n\nHello,  \nI came across the paper \"Face Beautification: Beyond Makeup Transfer\" that was published at NeurIPS this year. I was deeply concerned by the apparent complete lack of the social and ethical repercussions of the paper. The goal of the paper is to change photos of women to make them more attractive. While it may be possible to do this in a way that isn't objectionable, the paper there is zero discussion of or acknowledgement of the social, political, and power-dynamical (is that a word?) aspects of what is judged as attractive. The paper also contains serious methodological issues and blatantly contradicts itself in a fashion that I would expect to disqualify the paper from publication in the first place.\n\nEven worse, the examples in the paper make it clear that the algorithm's concept of \"attractive\" is \"light skinned white people.\" Of the 114 demo examples of computer-generated \"attractive people\" in the paper, 100% are white. Not only that, almost all of them have extremely light skin. Only a couple of the shown inputs appear to be non-white people (e.g., Table 2 appears to contain a MENA or South Asian woman), and the algorithm clearly makes them into white people both by lightening their skin and by changing other morphological features to make the person appear white. None of the inputs appear to be black people. Even among white people it strongly prefers people with lighter skin; there are zero examples where the algorithm appears to darken the skin tone of the person to be beautified and in the majority of cases it is significantly lightened.\n\nThis isn't just an issue of using white people as \"attractive references,\" as it even happens when the *reference attractive -----> image !!! * is a photo of a non-white person, as seen in the table at the top of the first page. Two east Asian people are used as reference images to beautify white people, but the resulting image has typically white features such as a less ovular face shape and doubled eyelids.\n\nNot only does this appear as a persistent pattern in the images, the authors don't even mention that it happens, let alone critically engage with this. Given how much NeurIPS appears to pride itself on social awareness in AI research, I am saddened and disheartened to see that this paper was viewed as having sufficient merit and ethical practice to warrant publication. The very last paragraph of their paper says\n\n&gt;Personalized beautification is expected to attract increasingly more attention in the incoming years. This work we have only focused on the beautification of female Caucasian faces. A similar question can be studied for other populations even though the relationship between gender, race, cultural background and the perception of facial attractiveness has remained under-researched in the literature. How can AI help reshape the practice of personal makeup and plastic surgery is an emerging field for future research.\n\nThis paragraph is clearly false for several reasons. As I mentioned, they have reference photos of non-Caucasian people in the paper itself and appear to input at least a couple non-Caucasian people. Secondly, the authors use data sets that contain a large number of non-Caucasian people. Since they mention the number of training and testing data points used, it is easy to verify that either their \"Experimental Setup\" section is not wrong or this paragraph is. Given that the images given as examples in the paper itself appear to falsify this paragraph, it seems clear that they are simply not telling the truth here. At no point in the entire paper other than this paragraph do they say anything about only being interested in Caucasian people.\n\nWhile I generally believe in making the most charitable assumptions, it seems uncredible that this might be a mistake or that the authors might be unaware that this paragraph is false. Not only do the reference images in their own paper falsify it, one of their data sets is drawn from a paper titled \"SCUT-FBP5500\u2021 : A Diverse Benchmark Dataset for Multi-Paradigm Facial Beauty Prediction.\" The very first page of this paper prominently features a graphic showing non-Caucasian people. For their other data set (CelebA), the [project website](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) shows demo examples of non-Caucasian people. It does not appear possible for the authors to have done their due diligence and not noticed this. Additionally, it took me a couple seconds to find dozens of papers studying \"the relationship between gender, race, cultural background and the perception of facial attractiveness\" and even *Googling that phrase* brings up lots of papers. Given the location of this paragraph within the paper and the fact that the paragraph blatantly contradicts the description of the experiment in the paper itself I fear this paragraph was added later after concerns about the paper were raised in order to mislead the reader and justify their poor ethical practice.\n\nI also believe that the validation methodologies considered by the paper are extremely insufficient, even setting aside social and ethical concerns.The authors say\n\n&gt;To evaluate the image quality from human\u2019s perception, we develop a user study and ask users to vote the most attractive one among ours and the baseline. 100 face images from testing set are submitted to Amazon Mechanical Turk (AMT), and each survey requires 20 users. We collect 2000 data points in total to evaluate human preference. The final results demonstrate the superiority of out model, showing in Table 1.\n\nThis is a rather small sample size, especially as no analysis of variance or estimation of uncertainty is done. Despite the extensive literature on how socioeconomic and racial factors influence assessments of attraction, these attributes are never discussed in the Mechanical Turk population. Additionally, they never actually assess if people find the computer generated images more attractive than the reference images, which is purportedly the entire purpose of the paper. They only ask if the image their algorithm generates is more attractive than *other computer-generated images*. The only further validation is that they ask their algorithm to score the beauty of the new images and find that on average the beauty rating goes up. This isn't evidence of anything meaningful at all, as they're using the same algorithm to evaluate if the beauty increased as they used inside their GAN to make the image more beautiful in the first place.\n\nYou can find the paper on arXiv here: [https://arxiv.org/abs/1912.03630](https://arxiv.org/abs/1912.03630)", "link": "https://www.reddit.com/r/MachineLearning/comments/e9eurc/d_concerns_about_face_beautification_beyond/"}, {"autor": "got_implicit", "date": "2019-12-11 15:52:56", "content": "How to search for action (from an -----> image !!! ) in a video? /!/  \\[Posted on r/deeplearning\\] Reposting for a wider response.\n\n&gt;I need to search for action similarity between an image and frames from the video. I need this to find the starting time of the frame where the action represented by the image is being carried out.  \n&gt;  \n&gt;Visual similarity via pixel by pixel difference will not work at all since there may not be the same placement of the person/action in both (the image and the frame). Even if that is the case, the color of attire/background, etc. might differ.  \n&gt;  \n&gt;I need to this for any general sports-related action involving humans. I could not find any classification models which seem to be so detailed so as to classify between such nuanced and minutely differing actions as is the case with sports.  \n&gt;  \n&gt;Is there any way I can achieve this? How helpful it might be if I use another representation of action (say keypoint set as given by [openpose](https://github.com/CMU-Perceptual-Computing-Lab/openpose))? One flaw that instantly comes to mind with this approach is that it would not provide for comparison of objects involved in the action but only the people.  \n&gt;  \n&gt;Any help is appreciated. Thanks a lot.", "link": "https://www.reddit.com/r/MachineLearning/comments/e98t23/how_to_search_for_action_from_an_image_in_a_video/"}, {"autor": "CarrotCakePls", "date": "2019-12-10 23:51:55", "content": "[P] Animal detection robot! Help! /!/ Hi everyone! I'm doing my university final year project on a robot that deters foxes (and maybe cats ;) ) from peoples gardens! The general plan is to use an artificial neural network to detect foxes from a -----> camera !!!  feed. I'm guessing I need night vision data, because they tend to show at night... Anyone know where i can get training data from? Would splitting down a video into frames work as training data? (probably a stupid question, but how would i even do this!)\n\nThe initial idea was to have this running on a raspberry pi and i'm not sure if this would this even be possible because of the limited processing power?\n\nAlso I'd need a way of testing this! But i'm not allowed to use real animals (not that i could find and use a fox anyways!), Would toy stuffed animals work? But this i'm guessing would not work if I use night vision images in training my model.. I'm not sure\n\nDoes anyone have any experience in real time computer vision and advice on how to proceed!\n\nAny help would be appreciated :)", "link": "https://www.reddit.com/r/MachineLearning/comments/e8yy7w/p_animal_detection_robot_help/"}, {"autor": "OverLordGoldDragon", "date": "2019-12-10 22:02:31", "content": "[P] See RNN: Kernel-, Gate-, Channel-wise Visualization of Gradients, Weights, and Activations /!/ DL is more than shooting in the dark and seeing what sticks; to this end, I present the first comprehensive RNN visualization API for Keras &amp; TensorFlow layers, [See RNN](https://github.com/OverLordGoldDragon/see-rnn/):\n\n* *Per-gate*, *per-kernel*, *per-channel*, and *per-direction* visuals\n* Gradients, weights, and activations visuals\n* Applicable to CNNs &amp; other meaningfully-related layers\n\n**Why use?** *Introspection -* is powerful for debugging, regularizing, and understanding NNs. For example, how can you tell whether your RNN is **learning long-term dependencies**? *Monitor gradients*:  if a non-zero gradient flows through every timestep, then *every timestep contributes to updating weights*, so the RNN doesn't ignore parts of sequences and is forced to *learn from them*. Or just because the visuals are rather pretty.\n\nNumerous examples explored w/ -----> image !!!  results in the link. Functionalities are fully-documented, and are compatible w/ TF 1.15.0 &amp; Keras 2.2.5-, and TF 2.0.0+ &amp; Keras 2.3.0+. Quickstart [sandbox code](https://github.com/OverLordGoldDragon/see-rnn/rnn_sandbox.py) included.\n\nFeedback is welcome.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\n&amp;#x200B;\n\nhttps://preview.redd.it/lm7o59f1lv341.png?width=1170&amp;format=png&amp;auto=webp&amp;s=cb7bd53f51b931dc514b5431c9a1afe687bb37f8\n\nhttps://preview.redd.it/g7p8es25lv341.png?width=1065&amp;format=png&amp;auto=webp&amp;s=0ef3580189c633ccd0a8ca1f0bc640b54be7392b\n\nhttps://preview.redd.it/yjmciimnhv341.png?width=1025&amp;format=png&amp;auto=webp&amp;s=69f37b3a66e3821015ec2120ca897f8e97af2223\n\nhttps://preview.redd.it/52q9u4clhv341.png?width=1060&amp;format=png&amp;auto=webp&amp;s=0e4a75bd6ca167e74b5837f981072ff4187ef82f", "link": "https://www.reddit.com/r/MachineLearning/comments/e8xiff/p_see_rnn_kernel_gate_channelwise_visualization/"}, {"autor": "psuzn", "date": "2019-01-30 08:17:06", "content": "Need help to find use cases of my ML project.[Human path tracing using CCTV -----> camera !!! ] /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/alan67/need_help_to_find_use_cases_of_my_ml_projecthuman/"}, {"autor": "pnavarre", "date": "2019-01-30 03:15:20", "content": "[R] MGBP : Multi-Grid Back-Projection super-resolution /!/ &amp;#x200B;\n\nhttps://i.redd.it/i2g3akxx4hd21.png\n\nPaper at AAAI 2019: [https://arxiv.org/abs/1809.09326](https://arxiv.org/abs/1809.09326)\n\nPaper at ECCV PIRM 2018: [https://arxiv.org/abs/1809.10711](https://arxiv.org/abs/1809.10711) (2nd best perceptual quality)\n\nGithub: [https://github.com/pnavarre/pirm-sr-2018](https://github.com/pnavarre/pirm-sr-2018)\n\n&amp;#x200B;\n\nAbstract (AAAI-2019):\n\nWe introduce a novel deep-learning architecture for -----> image !!!  upscaling by large factors (e.g. 4x, 8x) based on examples of pristine high-resolution -----> image !!! s. Our target is to reconstruct high-resolution images from their downscale versions. The proposed system performs a multi-level progressive upscaling, starting from small factors (2x) and updating for higher factors (4x and 8x). The system is recursive as it repeats the same procedure at each level. It is also residual since we use the network to update the outputs of a classic upscaler. The network residuals are improved by Iterative Back-Projections (IBP) computed in the features of a convolutional network. To work in multiple levels we extend the standard back-projection algorithm using a recursion analogous to Multi-Grid algorithms commonly used as solvers of large systems of linear equations. We finally show how the network can be interpreted as a standard upsampling-and-filter upscaler with a space-variant filter that adapts to the geometry. This approach allows us to visualize how the network learns to upscale. Finally, our system reaches state of the art quality for models with relatively few number of parameters.\n\n&amp;#x200B;\n\nAbstract (ECCV-PIRM 2018):\n\nWe describe our solution for the PIRM Super-Resolution Challenge 2018 where we achieved the 2nd best perceptual quality for average RMSE&lt;=16, 5th best for RMSE&lt;=12.5, and 7th best for RMSE&lt;=11.5. We modify a recently proposed Multi-Grid Back-Projection (MGBP) architecture to work as a generative system with an input parameter that can control the amount of artificial details in the output. We propose a discriminator for adversarial training with the following novel properties: it is multi-scale that resembles a progressive-GAN; it is recursive that balances the architecture of the generator; and it includes a new layer to capture significant statistics of natural images. Finally, we propose a training strategy that avoids conflicts between reconstruction and perceptual losses. Our configuration uses only 281k parameters and upscales each image of the competition in 0.2s in average.\n\n&amp;#x200B;\n\nAlso see:\n\nPIRM-SR 2018 Report: [https://arxiv.org/pdf/1809.07517.pdf](https://arxiv.org/pdf/1809.07517.pdf)\n\nPIRM Dataset: [https://pirm.github.io/](https://pirm.github.io/)", "link": "https://www.reddit.com/r/MachineLearning/comments/al8cyn/r_mgbp_multigrid_backprojection_superresolution/"}, {"autor": "AcceptableExplorer0", "date": "2019-01-29 12:48:03", "content": "Machine Vision Market Share, Industry Analysis, Share, Countries, Growth, Top Key Players and Future Insights Report 2025 /!/   \n\ngrowing at a CAGR of 5.0% from 2018 to 2025.\n\nWhat is Machine Vision?\n\nMachine vision is a term that is used to describe the methods as well as technology that is used to extract information from an -----> image !!!  in a way that is automated. This process differs from image processing due to the fact that image processing results in another image while machine vision involves the extraction of the data from an image. Machine vision involves the methods as well as technology that is used to provide services such as image-based automatic analysis as well as inspection. These services are provided to areas such as process control, robotic guidance, and automatic inspection. Machine vision involves the incorporation of software, technology as well as hardware products. The process begins with the planning of the overall requirements which is subsequently followed by developing a solution.\n\n**Get || FREE Sample Report @** [**https://www.verifiedmarketresearch.com/download-sample/?rid=3983**](https://www.verifiedmarketresearch.com/download-sample/?rid=3983)\n\nMachine Vision Market Outlook\n\nRising need for automation and quality inspection, increasing demand for vision-guided robotic systems and increment in adoption rate of 3d machine vision systems have been driving the global machine vision market. On the other hand, changing requirements of end-user and lacking flexible machine vision solutions might hinder the overall market at a global level.The \u201cGlobal Substation Automation Market\u201d study report will provide a valuable insight with an emphasis on global market including some of the major players such as ABB Ltd., Siemens AG, General Electric, Cisco Systems, Inc., Schneider Electric, Eaton Corporation PLC, Honeywell International, Inc., Schweitzer Engineering Laboratories, Inc., Novatech LLC, Crompton Greaves. Our market analysis also entails a section solely dedicated to such major players wherein our analysts provide an insight into the financial statements of all the major players, along with its product benchmarking and SWOT analysis.\n\n**Full Report is Available @** [**https://www.verifiedmarketresearch.com/product/global-machine-vision-market-size-and-forecast-to-2025/**](https://www.verifiedmarketresearch.com/product/global-machine-vision-market-size-and-forecast-to-2025/)\n\n**About Us:**\n\n Verified Market Research' has been providing Research Reports, with up to date information, and in-depth analysis, for several years now, to individuals and companies alike that are looking for accurate Research Data. Our aim is to save your Time and Resources, providing you with the required Research Data, so you can only concentrate on Progress and Growth. Our Data includes research from various industries, along with all necessary statistics like Market Trends, or Forecasts from reliable sources.  \n \n\n**Contact Us:**\n\nMr. Ankush\n\nCall: +1 (650) 781 4080\n\nEmail:sales@verifiedmarketresearch.com", "link": "https://www.reddit.com/r/MachineLearning/comments/akzodj/machine_vision_market_share_industry_analysis/"}, {"autor": "behindthedash", "date": "2019-01-29 09:52:46", "content": "[P] Neural network for car recognition /!/ The last year I attended the field of machine learning and at the end of 2018 achieved some results that I want to share. I trained a neural network to recognize a car by a -----> photo !!!  and created this simple [**demo**](http://68.183.4.242/) for illustration.\n\n# # The data\n\nThe starting point for the task was the [**Stanford Cars Dataset**](https://www.kaggle.com/jutrera/stanford-car-dataset-by-classes-folder). Some classes of this dataset contain quite a lot of errors (e.g. models of Audi or Aston Martin are often difficult to tell apart for a human being). So I took only 48 classes from the dataset and cleaned them up. During training, the data was augmented by rotations, reflections and messing around with colors.\n\n# # Base models\n\nSure enough, I applied transfer learning for the task. I used as feature extractors the models that are among the best performing on the ImageNet dataset (ResNet50, Inception V3 and Exception). First, I simply concatenated their outputs and stacked some dense layers on the top. This approach didn\u2019t work particularly well - about 55% accuracy on the test set.\n\n# # Approach\n\nI tried to analyze how I recognize a car that I see in the street. The easiest thing to tell about a car is its body type. There are only a few of them and usually, they have distinct features. As a rule, a manufacturer is also not a problem. And recognizing the particular model almost always takes another moment or two. Sometimes it may take a minute if a model is rare. In this case, I look at the features like the shape of the radiator grill or of lamps. These features are decisive, I think. The shape of the doors and windows is also important. And the information about the body type and the manufacturer is certainly also helpful.\n\nIt looked like a sequence to me, so I decided to try an LSTM. It should take the concatenated feature vector yielded by pretrained models and predict body type on the first step, manufacturer on the second and model on the third. And it should be bidirectional because the sequence probably works in the opposite direction as well. At the top of that, another LSTM should gather these predictions together and yield the final prediction of make and model. This approach raised the test accuracy significantly, we got about 72% on the test set.\n\nI have to admit that LSTM is a bit of overkill here. The sequence is short, so an ordinary RNN should do just as well. But LSTM is just as easy to implement in Keras and barely adds a significant lag to the overall evaluation time, so it wasn\u2019t of the highest priority to try and replace the LSTM.\n\n# # Tweaks and tricks\n\nNext, I tried to get rid of the redundant features the network might overfit to, like the characteristics of the background. Dropout layers didn\u2019t help a lot, so I used another ready solution provided by the amazing [**ImageAI**](https://github.com/OlafenwaMoses/ImageAI) library. I applied the yolo model to detect a car on the image and crop it accordingly on the preprocessing step. This gave us other 10% of test accuracy.\n\nThe last step was made by removing all dropout layers which gave us 5 more percent. It surprised me first, but now I think that since we got rid of almost all background after the detection step, the network could infer almost always only relevant features from the input. So the dropouts probably became more harmful than helpful after that.\n\n# # Architecture\n\nI think there is no reason to describe the architecture we used any further, so I\u2019ll just provide a diagram\u00a0\n\nhttps://i.redd.it/33lwf11j1cd21.png\n\n# # Analysis\n\nThe final result, for now, is 87,5% of top 1 accuracy and 98,5% of top 5 accuracy. Top 1 accuracy means that a prediction with the highest score is correct and top 5 accuracy means that the correct answer is among the predictions with top 5 scores.\n\nMost of the errors the network makes are easy to make. For example, this Audi S4 is confused with an Audi A4 of the previous generation:\u00a0\n\nhttps://i.redd.it/jwdh485q2cd21.png\n\nThis Bentley Continental is classified as Audi A5:\u00a0\n\nhttps://i.redd.it/o2zi56ur2cd21.png\n\nSome errors suggest that the network still takes color into account too much. This Bentley is recognized as Chevrolet Cobalt SS many of which are of red color in the training set:\u00a0\n\nhttps://i.redd.it/zyiryn6t2cd21.png\n\nIt is interesting how the network behaves when it sees a model that wasn\u2019t in the training set. This Porsche 911 was classified as Chevrolet Corvette which is also a sports car of a similar kind:\u00a0\n\nhttps://i.redd.it/h9k6kk8v2cd21.png\n\nAnd this Honda CRV is classified as Acura ZDX:\u00a0\n\nhttps://i.redd.it/yvzrk3vw2cd21.png\n\nSome results are funny though:\u00a0\n\nhttps://i.redd.it/uneeg93z2cd21.png\n\nIn general, it looks like the network gets the idea of a car on a photo correctly. A sports car is rarely confused with an SUV and a convertible most likely may be confused with another convertible of the same kind:\u00a0\n\nhttps://i.redd.it/wfqe39h03cd21.png\n\nI think that many problems may go away if we use a bigger dataset. And that is what we are working on right now.\n\nAlso, I can\u2019t help thinking that maybe the features we get from the last layers of the pretrained models are too general. It may happen that if we use the earlier layers we get the better results. It was difficult to check while the development was happening on a local machine because the earlier layers are much bigger than the last one and demand much more memory. Now that I moved to [**Google Cloud Platform**](https://cloud.google.com/)\u00a0and to [**FloydHub**](https://www.floydhub.com/)\u00a0I think, we can give it a try\n\n# # Stack\n\nAll development was done in Python Notebooks and [**Keras**](https://keras.io/)\u00a0and deployed using [**Flask**](http://flask.pocoo.org/)\u00a0and [**Docker**](https://www.docker.com/).\n\nIf you are interested, you can find the code on [**GitHub**](https://github.com/iPhaeton/car_identification).\u00a0", "link": "https://www.reddit.com/r/MachineLearning/comments/akyj8y/p_neural_network_for_car_recognition/"}, {"autor": "UnlikelyClue", "date": "2019-01-14 05:28:42", "content": "How to select beard in -----> image !!!  in increase or decrease growth /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/afskln/how_to_select_beard_in_image_in_increase_or/"}, {"autor": "b0red1337", "date": "2019-01-11 18:28:26", "content": "[P] Visualizing the dynamics of GAN /!/ Did this while playing around with GAN, think it might be able to give some intuitions about how GAN works\n\nSee the video here: [https://www.youtube.com/watch?v=aOoKLiDFTek](https://www.youtube.com/watch?v=aOoKLiDFTek)\n\nAt each time step, the discriminator (green curve) tries to decrease the value around the generated data (blue histogram) and increase the value around the true data (orange histogram) by the discriminator gradient while the blue histogram tries to \"move\" along the direction that increases its probability (green curve) by the generator gradient.\n\nInterestingly, GAN never collapsed into a single mode in this example despite passing through each mode several times. The chaotic motion in this simple example makes me feel that the ability to generate anything remotely useful in high dimensional space (-----> image !!! ) is absolutely miraculous.\n\n&amp;#x200B;\n\n(source can be found here [https://github.com/hrpan/test\\_gan](https://github.com/hrpan/test_gan))\n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/aeyacf/p_visualizing_the_dynamics_of_gan/"}, {"autor": "ihaphleas", "date": "2019-01-23 10:52:18", "content": "[1901.07441] PadChest: A large chest x-ray -----> image !!!  dataset with multi-label annotated reports", "link": "https://www.reddit.com/r/MachineLearning/comments/aiyji4/190107441_padchest_a_large_chest_xray_image/"}, {"autor": "Hsankesara", "date": "2019-01-23 06:15:30", "content": "U-Net Introducing Symmetry in Segmentation /!/ Just submit an article on medium describing a novel -----> image !!!  segmentation architecture. Please check this out and leave your valuable feedback.\n\n [https://medium.com/@heetsankesara3/u-net-b229b32b4a71](https://medium.com/@heetsankesara3/u-net-b229b32b4a71)", "link": "https://www.reddit.com/r/MachineLearning/comments/aiwris/unet_introducing_symmetry_in_segmentation/"}, {"autor": "Beanocean", "date": "2019-01-23 01:28:58", "content": "Why Transformer use little activations? /!/ I am confused about the activation functions used in the famous NMT model Transformer. In other classical networks such as ResNet, the activation functions are used after every convolutional transformation. So a stereotype -----> image !!!  is that the activation functions should follow after the linear functions.  But in the Transformer, I found that the activation functions are only used in Position-wise Feed-Forward Layers. In Multi-Head Attention Layers, there are only linear transformations. Can someone help to explain this? ", "link": "https://www.reddit.com/r/MachineLearning/comments/aiu9uv/why_transformer_use_little_activations/"}, {"autor": "Eoncarry", "date": "2019-01-22 04:13:49", "content": "[D] In YOLO Object detection, how does the algorithm predict bounding boxes larger than the grid size? /!/ From what I understand, the algorithm runs separately on each section of the grid. So taking the car in the -----> picture !!!  as an example, how does the algorithm know what's outside the grid-section? (How is it able to predict the car's length across 3 sections if it processed only the middle section)\n\nhttps://i.redd.it/3u229djzgwb21.png\n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/aij0ro/d_in_yolo_object_detection_how_does_the_algorithm/"}, {"autor": "danborthwick", "date": "2019-01-22 02:31:33", "content": "[D] Help needed, AI startup needs images of your teeth to train ML model /!/ Hi r/MachineLearning,\n\n&amp;#x200B;\n\nAI Dental is a very early stage startup based in Melbourne, Australia, with a mission to improve the world's health through early detection of dental problems. We're currently building an -----> image !!!  set of smartphone-captured, real world teeth. If you have a few minutes to spare then we'd be really grateful if you could [use our simple web tool](http://aidental.smartcasual.cc/?utm_source=reddit) to take a couple of pictures. All images are completely anonymous and unidentifiable, will not be shared with any third parties, and will only be used to train our model.\n\nAny questions or problems using the tool, please comment below and I'll try and respond as quickly as possible.\n\nThanks again for your help,  \nDan\n\nhttp://aidental.smartcasual.cc/?utm\\_source=reddit", "link": "https://www.reddit.com/r/MachineLearning/comments/aii4vc/d_help_needed_ai_startup_needs_images_of_your/"}, {"autor": "neural_potato", "date": "2019-01-22 02:04:48", "content": "[R] CAMOU: Learning Physical Vehicle Camouflages to Adversarially Attack Detectors in the Wild [ICLR 2019] /!/ Paper: [https://openreview.net/forum?id=SJgEl3A5tm](https://openreview.net/forum?id=SJgEl3A5tm)\n\nI hope it is not too late to advertise our ICLR work.\n\n&amp;#x200B;\n\nhttps://i.redd.it/buvcbduntvb21.png\n\n In this paper, we conduct an intriguing experimental study about the physical adversarial attack on object detectors in the wild. In particular, we learn a camouflage pattern to hide vehicles from being detected by state-of-the-art convolutional neural network based detectors. Our approach alternates between two threads. In the first, we train a neural approximation function to imitate how a simulator applies a camouflage to vehicles and how a vehicle detector performs given images of the camouflaged vehicles. In the second, we minimize the approximated detection score by searching for the optimal camouflage. Experiments show that the learned camouflage can not only hide a vehicle from the -----> image !!! -based detectors under many test cases but also generalizes to different environments, vehicles, and object detectors. \n\nPlease feel free to ask any questions :)", "link": "https://www.reddit.com/r/MachineLearning/comments/aihwff/r_camou_learning_physical_vehicle_camouflages_to/"}, {"autor": "alexparinov", "date": "2019-03-05 21:02:47", "content": "[P] Albumentations, an -----> image !!!  augmentation library version 0.2.0 released /!/ About 5 months ago [we released](https://www.reddit.com/r/MachineLearning/comments/9j6f9j/p_albumentations_a_fast_and_flexible_image/) an image augmentation library called Albumentations. Recently we updated the library to version 0.2.0. You can install the latest version by running `pip install -U albumentations` or downloading the library from [https://github.com/albu/albumentations](https://github.com/albu/albumentations)\n\n&amp;#x200B;\n\n**Keypoint transformations**\n\nWe added a long-awaited feature, support for the keypoint transformations and we made [an example notebook](https://github.com/albu/albumentations/blob/master/notebooks/example_keypoints.ipynb) that shows how to use them.\n\n&amp;#x200B;\n\nhttps://i.redd.it/n6xslslrlck21.jpg\n\n&amp;#x200B;\n\n**Apply the same transformation to the more than one target of the same type**\n\nWe also added an option to apply the same transformation to the more than one target of the same type. The possible use cases are image2image or stereo-image pipelines. So you can pass multiple images as input and all of them will be transformed in the same way using exactly the same parameters. See a more detailed description in [this notebook](https://github.com/albu/albumentations/blob/master/notebooks/example_multi_target.ipynb).\n\n&amp;#x200B;\n\n**Other improvements**\n\nAlso, we fixed some bugs and improved performance. Here are benchmarking results for Albumentations against other popular augmentation libraries (the best result for each transformation is shown in bold).\n\n&amp;#x200B;\n\nhttps://i.redd.it/4ztyn7r1uck21.png\n\n&amp;#x200B;\n\n**Using Albumentations to win data science competitions**\n\nAlbumentations is widely used on Kaggle and other platforms for machine learning competitions. Here is a list of people and teams who used the library to achieve top places on the leaderboard.\n\n&amp;#x200B;\n\n[**Airbus Ship Detection Challenge**](https://www.kaggle.com/c/airbus-ship-detection/leaderboard)\n\n* 1st place: [Konstantin Gavrilchik](https://www.kaggle.com/dempton) and [Evgeny Kononenko](https://www.kaggle.com/lenny27)\n* 2nd place: [Victor Durnov](https://www.kaggle.com/victorsd) and [Selim Seferbekov](https://www.kaggle.com/selimsef)\n* 6th place: [Nick Sergievskiy](https://www.kaggle.com/nicksergievskiy), [Yauhen Babakhin](https://www.kaggle.com/ybabakhin) and [ZFTurbo](https://www.kaggle.com/zfturbo)\n\n&amp;#x200B;\n\n[**Quick, Draw! Doodle Recognition Challenge**](https://www.kaggle.com/c/quickdraw-doodle-recognition/leaderboard)\n\n* 1st place: [Pavel Pleskov](https://www.kaggle.com/ppleskov) and [Pavel Ostyakov](https://www.kaggle.com/pavelost)\n* 4th place: [Yauhen Babakhin](https://www.kaggle.com/ybabakhin), [Ivan Sosin](https://www.kaggle.com/sawseen), [ZFTurbo](https://www.kaggle.com/zfturbo), [Alex Parinov](https://www.kaggle.com/creafz) and [Roman Vlasov](https://www.kaggle.com/romavlasov)\n\n&amp;#x200B;\n\n[**TGS Salt Identification Challenge**](https://www.kaggle.com/c/tgs-salt-identification-challenge/leaderboard)\n\n* 1st place: [Yauhen Babakhin](https://www.kaggle.com/ybabakhin) and [phalanx](https://www.kaggle.com/phalanx)\n* 3rd place: [Victor Durnov](https://www.kaggle.com/victorsd) and [Selim Seferbekov](https://www.kaggle.com/selimsef)\n\n&amp;#x200B;\n\n[**Inclusive Images Challenge**](https://www.kaggle.com/c/inclusive-images-challenge/leaderboard)\n\n* 1st place:  [Pavel Ostyakov](https://www.kaggle.com/pavelost)\n\n&amp;#x200B;\n\n[**Humpback Whale Identification**](https://www.kaggle.com/c/humpback-whale-identification/leaderboard)\n\n* 5th place: [ZFTurbo](https://www.kaggle.com/zfturbo) and [Weimin Wang](https://www.kaggle.com/weimin)\n\n&amp;#x200B;\n\n[**Spacenet 4**](https://community.topcoder.com/longcontest/stats/?module=ViewOverview&amp;rd=17313)\n\n* 3rd place: [Konstantin Maksimov](https://www.kaggle.com/maksimovka)\n\n&amp;#x200B;\n\n[**The 1st Tellus Satellite Challenge**](https://signate.jp/competitions/110/leaderboard)\n\n* 3rd place: [Eugene Khvedchenya](https://www.kaggle.com/bloodaxe)\n\n&amp;#x200B;\n\n**Next version**\n\nIn the next version, we are planning to add an option to serialize / deserialize the augmentation pipeline to and from the JSON config files. This step will allow better reproducibility of the results achieved with the library.\n\n&amp;#x200B;\n\nIf you found a bug, missing a feature, or just have problems understanding the documentation please [open an issue on GitHub](https://github.com/albu/albumentations/issues/new).", "link": "https://www.reddit.com/r/MachineLearning/comments/axq6lu/p_albumentations_an_image_augmentation_library/"}, {"autor": "FeelTheDataBeTheData", "date": "2019-03-05 19:06:43", "content": "[D] OCR For Niche Fonts (Dot Matrix) /!/ Hi everybody! I am working on a project where I need to be able to extract date codes from an -----> image !!!  snapped on a smartphone. I have run similar images through traditional approaches like Tesseract, Google Vision API, etc with little to no luck nor consistency. I am assuming I will need to train or transfer learn from another model, but I need help finding the right approach. I have currently tried manually splitting the image based on white space and trying to identify each individual character cropped from the original image. This makes it super difficult to keep users of the app to stay consistent with their photo taking.\n\nI have also tried the tensorflow implementation of YOLO called darkflow. After painstakingly massaging custom image data into the correct format and slow training, we ended up with a 200mb model that could barely identify a couple of the characters and it still isn't a complete solution since we also need to put these characters into order and supply it to the user.\n\nI have seen the convolutional recurrent neural net approach with CTC loss for handwriting detection, but when I tried it, it didn't seem to work for this problem. I can try again, if you think I should!\n\nAm I wrong for assuming this is an object detection problem? Is there another set of solutions that relate more specifically to OCR?\n\nThere are plenty of streamlined object detection services through Azure, AWS, and [Supervise.ly](https://Supervise.ly) so I am not concerned if I need to go this route, but I want to make sure I am going in the right direction.\n\nFor data, we have gigabytes of photos of actual datecodes. To label, we are looking into MTurk or [Supervise.ly](https://Supervise.ly).\n\nThanks everyone!\n\n[Needs to be transcribed to ==\\&gt; line 1:\\\\\"JUL2219\\\\\", line2:\\\\\"F04111444 31554\\\\\"](https://i.redd.it/afzrcedfmck21.jpg)", "link": "https://www.reddit.com/r/MachineLearning/comments/axov01/d_ocr_for_niche_fonts_dot_matrix/"}, {"autor": "blaquaman2", "date": "2019-03-04 17:04:02", "content": "Can multi-label classification work with class hierarchies? /!/ I'm relatively new to machine learning. Recently I retrained SSD Mobilenet to recognize certain animals using Tensorflow. I was wondering if would be possible to train a model that can output a more general class if it doesn't recognize the specific one. \n\n&amp;#x200B;\n\nFor example, could I train a model to recognize birds in general, as well as specific species? That way, if if recognizes the bird species, it will label the -----> image !!!  accordingly (e.g. sparrow), but if it doesn't, it will simply label the -----> image !!!  as a bird.\n\n&amp;#x200B;\n\nI have a general understanding of multi-label classification, but I'm having a hard time understanding if how (or if) it can be implemented with a hierarchy of classes. Also, I'd like to know if it's possible to implement it with popular models like YOLO or SSD Mobilenet.\n\n&amp;#x200B;\n\nIf this question is better suited for r/learnmachinelearning, please let me know and I'll remove here and post it there.\n\n&amp;#x200B;\n\nThank you for your help\n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/ax9fh4/can_multilabel_classification_work_with_class/"}, {"autor": "PlatoRepublic", "date": "2019-01-11 02:46:59", "content": "Medical Records Document Classification, Identification, and Separation /!/ I work as a Solution Engineer for a large Enterprise Document Management company specifically within the Healthcare vertical. I have noticed Health Information Management departments around the world struggle with medical records documentation classification and identification. Their goal is to get paper into their Enterprise Medical Record system asap and as efficiently as possible. The main areas they struggle with are document separation, classification (what type of document it is), and identification (matching it to a patient/visit/order).\n\nI'm trying to create a software demo so an investor will take me seriously. The end goal of the software would be to spit out multi-page tiff -----> image !!!  files with a corresponding comma separated index file so it could be loaded in to whatever DMS/EMR the company is using. Each index file would have the name of the image file, name of the document type. If patient/visit/order identifiers were present on the document it would also include them.\n\nI'm a ML armature so the only thing I have tried so far is re-training the last layer of googles inception deep convolutional neural network. I saw some success but was wondering if any of you had any ideas as to what ML algorithm/s would be better suited for this task. I envision it using at least two algorithm/s one to look at the pixels like the inception DCNN does, and one to look at the orc rendition of the characters of the pages. Please let me know your thoughts! I appreciate any and all comments.", "link": "https://www.reddit.com/r/MachineLearning/comments/aeqsr9/medical_records_document_classification/"}, {"autor": "ipsingh06", "date": "2019-01-11 00:44:35", "content": "[P] I trained a DCGAN to remove a Snapchat filter from selfie images /!/ Link: https://github.com/ipsingh06/ml-desnapify\n\nI trained a DCGAN to remove Snapchat selfie filters from images (inpainting problem). For now it's only trained to remove the infamous doggy filter.\n\nMost of the architectural hard work was done by the excellent [pix2pix](https://phillipi.github.io/pix2pix) project.\n\nThings I added that may be useful to others:\n\n- A pipeline to generate -----> image !!!  pairs for training data, sanitize -----> image !!! s, detect and remove duplicates, etc.\n- On-demand loading of training data so it doesn't have to fit entirely in memory (it's very important to use multiprocessing queues here to maintain sufficient throughput to the GPU)\n- Integration with tensorboard to visualize the training metrics\n\nThere's still a lot of hyperparameter exploration I need to do, but the results so far look promising.\n\nAny feedback is welcome. Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/aeppf7/p_i_trained_a_dcgan_to_remove_a_snapchat_filter/"}, {"autor": "pteroduct", "date": "2019-01-10 22:29:11", "content": "[D] Combining multiple neural networks for autonomous drones? /!/ I've been working on several networks for my drone (which only has a -----> camera !!!  and IMU): learning optical flow, learning depth estimation for monocular vision, and also object detection. They all work fine independently, but as  I am just starting the process of embedding all these onto hardware, I'm stumbled upon an issue:\n\n&amp;#x200B;\n\nWhat is the feasibility/complexity of combining all 3 into one architecture? (motivated by the DeMoN architecture which outputs both depth and camera motion [https://arxiv.org/abs/1612.02401](https://arxiv.org/abs/1612.02401))", "link": "https://www.reddit.com/r/MachineLearning/comments/aeodx0/d_combining_multiple_neural_networks_for/"}, {"autor": "DGs29", "date": "2019-01-10 13:37:29", "content": "How to build -----> image !!!  document text detection as good as Google cloud vision API? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/aej7bx/how_to_build_image_document_text_detection_as/"}, {"autor": "XMasterDE", "date": "2019-06-02 13:50:23", "content": "current state of the arte for GENs for -----> image !!!  generation /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/bvxjsy/current_state_of_the_arte_for_gens_for_image/"}, {"autor": "uniqueUserName1527", "date": "2019-06-01 12:32:56", "content": "[R] Image Classification using context information /!/ Hello everyone! Could someone recommend a good state-of-the-art paper and/or dataset for -----> image !!!  classification using context information or metadata in general? I have been trying to get into the topic but there isn't much relevant research in that particular field yet and I am still looking for a good entry point.", "link": "https://www.reddit.com/r/MachineLearning/comments/bvkbu1/r_image_classification_using_context_information/"}, {"autor": "LukeAndGeorge", "date": "2019-06-01 11:59:42", "content": "[P] EfficientNet in PyTorch /!/ TL;DR: Implemented EfficientNet in PyTorch: [https://github.com/lukemelas/EfficientNet-PyTorch](https://github.com/lukemelas/EfficientNet-PyTorch)\n\n&amp;#x200B;\n\nI found [EfficientNet](https://arxiv.org/abs/1905.11946) to be quite an exciting paper, so I implemented it in PyTorch over the past two days. The implementation makes it simple to load pretrained models and integrate them into your own projects/research/models. GitHub: [https://github.com/lukemelas/EfficientNet-PyTorch](https://github.com/lukemelas/EfficientNet-PyTorch)\n\n&amp;#x200B;\n\nThere are multiple examples in the GitHub repo and [here](https://colab.research.google.com/drive/1Jw28xZ1NJq4Cja4jLe6tJ6_F5lCzElb4) is one on Colab. It's as quick as \n\n    from efficientnet_pytorch import EfficientNet \n    model = EfficientNet.from_pretrained('efficientnet-b0')\n\nAnd you can install it via pip if you would like: \n\n    pip install efficientnet_pytorch\n\nFinally, there are scripts to evaluate on ImageNet (with training scripts coming soon) and there's functionality to easily extract -----> image !!!  features. \n\n&amp;#x200B;\n\nLet me know (either here or on GitHub) if you have any comments or find any bugs. I hope some of you find it useful!", "link": "https://www.reddit.com/r/MachineLearning/comments/bvk2iw/p_efficientnet_in_pytorch/"}, {"autor": "hubert0527", "date": "2019-05-31 18:52:15", "content": "[R] (ICML\u201919 AutoML workshop) InstaNAS: Instance-aware Neural Architecture Search /!/ ***TL;DL***  The first paper integrates Neural Architecture Search (NAS) with instance awareness and searches for a ***distribution*** of neural architectures to further expoit the power of NAS. InstaNAS achieves significant improvement in accuracy-latency tradeoff than single model. Our qualitative results show the controller policy shares many interesting similarities to human perception.\n\nhttps://i.redd.it/vuvexwob9k131.png\n\n* **Project page (with interactive demo)**:   \n[https://hubert0527.github.io/InstaNAS](https://hubert0527.github.io/InstaNAS)\n* **Paper**:   \n[https://arxiv.org/abs/1811.10201](https://arxiv.org/abs/1811.10201)\n* **We will release our code soon here**:  \n[https://github.com/AnjieZheng/InstaNas](https://github.com/AnjieZheng/InstaNas)\n\n&amp;#x200B;\n\n# Also see our other recent works:\n\n* COCO-GAN: Generation by Parts via Conditional Coordinating:   \n***TL;DR***  We show that it is possible to generate images by parts with a conditional coordinate mechanism. Our model preserves the state-of-the-art FID score and provides multiple interesting applications.   \n**Project Page:** [https://hubert0527.github.io/COCO-GAN/](https://hubert0527.github.io/COCO-GAN/)  \n\n* Point-to-point Video Generation:  \n***TL;DR***  We propose a new paradigm for controllable video generation between a pair of start- and end-frame. We show that our approach can generate various lengths of videos without loss of quality and diversity. We also showcase several new extensions with the proposed framework.  \n**Project Page:** [https://zswang666.github.io/P2PVG-Project-Page/](https://zswang666.github.io/P2PVG-Project-Page/)  \n\n* Conditional Cost-Volume Normalization for Sparse Sensory Data:   \n***TL;DR***  We investigate and propose a novel normalization module for fusing sparse sensory data (3D LiDAR) and dense -----> image !!! ry data (stereo -----> image !!! ). We show that our method effectively utilizes sparse sensory and brings significant performance, robustness and sensitivity improvement.   \n**Project Page:** [https://zswang666.github.io/Stereo-LiDAR-CCVNorm-Project-Page/](https://zswang666.github.io/Stereo-LiDAR-CCVNorm-Project-Page/)", "link": "https://www.reddit.com/r/MachineLearning/comments/bvbe84/r_icml19_automl_workshop_instanas_instanceaware/"}, {"autor": "machiavellian1", "date": "2019-06-09 22:11:37", "content": "[D] Using GANS with -----> image !!!  and non------> image !!!  data. /!/ Is there a way to combine both image and non-image while training  GAN's. So far most papers I've read seem to use one or the other but not both.", "link": "https://www.reddit.com/r/MachineLearning/comments/bypu9y/d_using_gans_with_image_and_nonimage_data/"}, {"autor": "saurabh241", "date": "2019-06-08 06:29:30", "content": "Guys I've created CNN-LSTM based r/RoastMe Bot ,that roasts given the -----> image !!! ,how can I improve further that model", "link": "https://www.reddit.com/r/MachineLearning/comments/by53a0/guys_ive_created_cnnlstm_based_rroastme_bot_that/"}, {"autor": "smartified", "date": "2019-04-02 22:10:27", "content": "[D] What are the advantages / weaknesses for the common set of off-the-shelf RE agents? /!/ I was looking through keras-rl and stable-baselines repos trying to wrap my head around the different RE agent implementations:\n\nhttps://keras-rl.readthedocs.io/en/latest/agents/overview/\n\nhttps://stable-baselines.readthedocs.io/en/master/guide/algos.html\n\nWhat's not clear to me is what something like a PPO2 algorithm might offer as compared with say DDPG alg for a given task.  How far are these algorithms able to predict into the future (say for a chess game or starcraft like game vs breakout) or how big of a memory footprint can these handle (eg., a small set of floats vs a full -----> image !!!  matrix).\n\nIs there a good discussion I can find that delves into these topics?", "link": "https://www.reddit.com/r/MachineLearning/comments/b8ppad/d_what_are_the_advantages_weaknesses_for_the/"}, {"autor": "skhadem", "date": "2019-04-02 18:46:34", "content": "[D] Augmentation to Semantic Segmentation GT mask /!/ I am working on semantic -----> image !!!  segmentation and have implemented a U-net in Tensorflow. I have a multi class output mask and use categorical cross entropy for the loss on tensors of size (batch\\_size, sizeX, sizeY, num\\_classes). For one of my classes, the mask is created by taking a labelled keypoint and drawing a filled circle around it. For this class, I really only care about the region in which this keypoint exists - I do not care if the segmentation results are exactly that arbitrary circle shape, or if they are in that exact position. So, I thought of trying to add some augmentation to the mask itself: random circle radius, random offset in X and in Y from the keypoint. My idea is this would guide the loss away from trying to get the exact shape and spot, but rather learn about the region. Is this a valid method? Are there any other efforts (papers theory, etc.) regarding doing this? The reason behind this is for much faster labelling, rather than painting over the whole region I am interested in, I could only mark one point in the region, and get some output in that area. (Example: mark the center of a square in the image, hopefully get results anywhere in the square). I am going to try training a couple hundred epochs anyway to see the results, but I'm curious if this is a method anyone else has tried. ", "link": "https://www.reddit.com/r/MachineLearning/comments/b8n7ju/d_augmentation_to_semantic_segmentation_gt_mask/"}, {"autor": "alexeykurov", "date": "2019-04-02 16:52:50", "content": "[P] Can artificial intelligence be taught how to joke? /!/ We did research about how AI can be taught to joke. For our project we take meme dataset from iFunny and try to  create a funny caption generator.  There were several different approaches:  \n1) Searching nearest caption to theme of -----> image !!!  by cluster  \n2) Searching nearest caption to theme of -----> image !!!  by visual similarity  \n3) Transferring the -----> image !!!  descriptor into the vector space of text descriptors  \n4) Generating captions using Markov\u00a0Chains  \n\n\nIn more details you can read in our blog post [https://heartbeat.fritz.ai/can-artificial-intelligence-be-taught-how-to-joke-7c7d53a3492a](https://heartbeat.fritz.ai/can-artificial-intelligence-be-taught-how-to-joke-7c7d53a3492a). We are happy to answer any questions about our work and discuss other approaches. ", "link": "https://www.reddit.com/r/MachineLearning/comments/b8lrlg/p_can_artificial_intelligence_be_taught_how_to/"}, {"autor": "OlgaPaints", "date": "2019-04-02 12:10:33", "content": "[P] Face Frontalization GAN in Pytorch + thoughts on GANs in supervised ML in general /!/ First time reddit poster here :) I recently implemented a [face frontalization GAN in Pytorch](https://github.com/scaleway/frontalization): the task is to take an -----> image !!!  of a person's face at an angle (0 to 90 degrees) as input and produce a synthesized -----> image !!!  of that person's face at 0 degree angle. \n\nI've got a pretty standard generative adversarial network setup, where the generator has an encoder/decoder architecture. The [idea of using a GAN for face frontalization is not new](https://arxiv.org/abs/1704.04086), my goal was rather to build an easy to follow end-to-end network that still produces reasonable-ish results: \n\n[Here the upper row is input, the middle row is the model's output after five training epochs, and the bottom is the ground truth frontal images](https://i.redd.it/nt6fpgzc4np21.jpg)\n\nThere is something I learned from this project, about using GANs for supervised ML tasks, that I found both interesting and generalizable to other tasks. For my loss function, in addition to the binary cross entropy for the discriminator that is used in GANs, I am also optimising the mean square error (synthetic vs ground truth frontal images) pixel-wise loss for the generator.\n\nOne could just use that mean square error and the encoder/decoder generator network for a problem like this, so I was interested in seeing precisely what was the benefit of incorporating a GAN into the model. Turned out, without GAN, the generator learns to produce face-like images rather quickly, but the fine details remain blurry for a  long time - not only for the test set, but also for the training samples:\n\n[Images generated after 20 000 mini batch evaluations \\(inputs from the training set\\)](https://i.redd.it/obiqxnyn6np21.jpg)\n\nMathematically, this can be attributed to the small contribution of these fine features to pixelwise loss. Since longer training times are required to achieve desired accuracy for the training set, this makes such models prone to over-fitting.\n\nI did not have enough training data for a real GAN generating random frontal images of faces (my dataset only has around 700 unique ground truth frontals), so having a generator+discriminator setup was just a thing to try at first. I found that despite generating images that overall look pretty noisy, the GAN is quite good at synthesizing the finer details that the no-GAN version was lacking:\n\n[Images generated from the same inputs by the GAN model](https://i.redd.it/3z9oo5gt6np21.jpg)\n\n[Ground truth images](https://i.redd.it/45hu69tg8np21.jpg)\n\nEvidently, the fine features - eyelids, shadows around the mouth, etc - are the details that the discriminator uses to distinguish the \"real\" images from the generated \"fake\" ones. Similar reasoning may apply to other examples of using GANs in supervised ML, notably  [Super Resolution](https://medium.com/@jonathan_hui/gan-super-resolution-gan-srgan-b471da7270ec). \n\nCode + tutorial available here: [https://github.com/scaleway/frontalization](https://github.com/scaleway/frontalization) \n\nI would be very interested in hearing about other people's experiences with GANs outside of unsupervised learning :)", "link": "https://www.reddit.com/r/MachineLearning/comments/b8idws/p_face_frontalization_gan_in_pytorch_thoughts_on/"}, {"autor": "Ramtin8731", "date": "2019-04-02 11:10:37", "content": "[P] Rock Paper Scissors with Artificial Intelligence /!/ Hi! I've created a Rock-Paper-Scissors game that works with artificial intelligence (AI). The AI can see and detect your hand gestures by front-facing -----> camera !!! . Also it can learn your playing strategy in a smart way. The more you play, It gets harder to win!\n\nThis app uses TensorFlow and deep learning technologies in order to detect the hand gestures. Sometimes the gestures may not be properly detected, but this will improve in future versions. You can help me in this process by taking pictures of your hand in different positions and sending them as a zip file to rpsapp@outlook.com .\n\nPlease Note:\n\nTo get best results in hand gestures detection, put your device on a flat and steady surface.\n\nIn order for the app to work properly, your device should have decent camera and hardware to run relatively heavy calculations.\n\nI've been working on developing this app for a year, so any feedback from you will be a pleasure for me :)\n\nYou can get the app on Google Play: https://play.google.com/store/apps/details?id=cc.ramtin.rps\n\nAnd you can read more about it on XDA: https://www.xda-developers.com/play-rock-paper-scissors-hand-gestures-against-ai-bot/", "link": "https://www.reddit.com/r/MachineLearning/comments/b8htjt/p_rock_paper_scissors_with_artificial_intelligence/"}, {"autor": "jerry0822", "date": "2019-04-02 00:10:17", "content": "[R] ASS-Network: Adversarial Sample Synthesis Network /!/ We present a new Generative Adversarial Network Architecture for  perspective augmented -----> image !!!  samples which can effectively discriminate  over physical attacks and recover contextual information through our  dual-stage GAN architecture. Using a novel method known as Adversarial  Sample Synthesis, our sampling strategy trains on pairs of images where  we utilize an adversarial sub-sampling approach to effectively learn the  divergence between the fake and real image distributions.\n\nLink: [https://www.scribd.com/document/403980329/A-Net-Architecture](https://www.scribd.com/document/403980329/A-Net-Architecture)", "link": "https://www.reddit.com/r/MachineLearning/comments/b8bogp/r_assnetwork_adversarial_sample_synthesis_network/"}, {"autor": "tangerto", "date": "2019-04-01 17:24:08", "content": "[Discussion] Would you use an app that lets you collect labeled data from your phone? /!/ Sometimes when I'm doing machine learning experiments, I often end up collecting my own data- -----> image !!! , audio, video, etc. Would other people find an app that lets you collect and label image/audio/video data right from your phone? Auto-export CSVs, labeled folders, etc.", "link": "https://www.reddit.com/r/MachineLearning/comments/b85e70/discussion_would_you_use_an_app_that_lets_you/"}, {"autor": "tsauri", "date": "2019-04-04 23:49:54", "content": "[D] CIFAR-10 test set labels /!/ Anyone here had actually eyeballed every single test images and their labels from CIFAR-10?  \nBecause last time I found a test -----> image !!!  that is essentially \\*black\\* .  \nAlso current SOTA is 1.0% error, meaning that from 10,000 images, there are 100 wrongly classified images.  \nHow do the network classify image that resembles nothing?  \n[https://arxiv.org/pdf/1811.06965.pdf](https://arxiv.org/pdf/1811.06965.pdf) ", "link": "https://www.reddit.com/r/MachineLearning/comments/b9k5i9/d_cifar10_test_set_labels/"}, {"autor": "andrea_manero", "date": "2019-04-04 10:22:57", "content": "Traditional Programming versus Machine Learning, in One -----> Picture !!!  /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/b9azkh/traditional_programming_versus_machine_learning/"}, {"autor": "sataky", "date": "2019-04-04 07:58:50", "content": "[P] UNET: a neural network for -----> image !!!  segmentation", "link": "https://www.reddit.com/r/MachineLearning/comments/b99x1l/p_unet_a_neural_network_for_image_segmentation/"}, {"autor": "karttikey_ab", "date": "2019-04-04 05:38:23", "content": "[P] AutoML Vision API deployment /!/ I am currently working on a project dealing with real time -----> image !!!  classification of water bottles of different kinds. For this , I used Google's AutoML vision API and I have a pretty decent model with me right now.\n\nThese bottles will come sliding down a ramp where a camera is installed, doing real time classifications.\n\nI'm not able to figure out how to integrate the model that I have deployed on cloud , with a camera doing real time predictions.\n\n(One of the good things about AutoML is that it provides you with a RESTAPI of the trained model.\nDo you think rather than using a camera , a phone with an Android app with my API as backend will work? )\n", "link": "https://www.reddit.com/r/MachineLearning/comments/b98t2f/p_automl_vision_api_deployment/"}, {"autor": "sarmientoj24", "date": "2019-12-02 14:23:31", "content": "[D] How to use VAE's encoded mu and sigma with respect to user-given z? /!/ My understanding of VAE is that unlike Autoencoders, it does not directly give you a discrete encoding (latent code vectors n-dim) instead, it gives you both mu and sigma (n- dim mean vectors and n-dim standard deviation vectors). Then you have epsilon which enables the smooth representation. When combining mu, sigma and epsilon, you get z which is the one decoded by the VAE's decoder. z is basically the main encoding.\n\nSay my z, mu, sigma are of n-dimension like 10 (10-dim z, mu, sigma). I enable the user to have a free picking/giving me numbers 10 vectors [-a, a], say a = 5. So the user is free to pick 10 vectors between -5, 5.\n\nThis becomes my z that is decoded by my decoder to generate a new -----> image !!! .\n\n[Main problem]\n\nMy VAE is trained on a dataset of apparel. Now, if I run my VAE's encoder on each of the data in the dataset, I'd get a mu and sigma for each (not sure if this is still correct). \n\nUsing the z given by the user, how do I find the most similar from the dataset using VAE's encoding of only mu and sigma?", "link": "https://www.reddit.com/r/MachineLearning/comments/e4zqqv/d_how_to_use_vaes_encoded_mu_and_sigma_with/"}, {"autor": "gananath", "date": "2019-12-02 06:13:45", "content": "[P] NERD for -----> image !!!  generation. /!/ A while back I shared an algorithm called NERD here. The initial implementation was for generating sequence and the results were not spectacular but encouraging. I have now used the same algorithm with minor changes to generate images. The generated images were still not spectacular but its not random either.\n\nproject: https://github.com/Gananath/NERD/tree/master/NERD_IMAGES\nimage: https://raw.githubusercontent.com/Gananath/NERD/master/NERD_IMAGES/nerd_mnist.png", "link": "https://www.reddit.com/r/MachineLearning/comments/e4uu2e/p_nerd_for_image_generation/"}, {"autor": "andrea_manero", "date": "2019-12-01 15:13:06", "content": "Deep Learning: AlphaGo Zero Explained In One -----> Picture !!!  /!/ https://www.datasciencecentral.com/profiles/blogs/deep-learning-alphago-zero-explained-in-one-picture", "link": "https://www.reddit.com/r/MachineLearning/comments/e4ikva/deep_learning_alphago_zero_explained_in_one/"}, {"autor": "jeyhome", "date": "2019-12-05 18:22:06", "content": "[R] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains", "link": "https://www.reddit.com/r/MachineLearning/comments/e6l79y/r_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "jeyhome", "date": "2019-12-05 18:11:35", "content": "[R] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains", "link": "https://www.reddit.com/r/MachineLearning/comments/e6l1qb/r_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "jeyhome", "date": "2019-12-05 18:00:12", "content": "[R] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains /!/ &amp;#x200B;\n\nhttps://i.redd.it/bd5djopbsu241.gif", "link": "https://www.reddit.com/r/MachineLearning/comments/e6kvnf/r_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "jeyhome", "date": "2019-12-05 17:49:10", "content": "[R] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains /!/ &amp;#x200B;\n\n![video](dech4dazqu241)", "link": "https://www.reddit.com/r/MachineLearning/comments/e6kpw8/r_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "jeyhome", "date": "2019-12-05 17:23:53", "content": "[R] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains", "link": "https://www.reddit.com/r/MachineLearning/comments/e6kcsu/r_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "kooro100", "date": "2019-12-05 17:13:14", "content": "[R] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains", "link": "https://www.reddit.com/r/MachineLearning/comments/e6k77g/r_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "yunjey", "date": "2019-12-05 16:48:54", "content": "[R] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains /!/ &amp;#x200B;\n\n![video](qmou2gypgu241)", "link": "https://www.reddit.com/r/MachineLearning/comments/e6juka/r_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "yunjey-choi", "date": "2019-12-05 16:45:11", "content": "[P] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains", "link": "https://www.reddit.com/r/MachineLearning/comments/e6jsmu/p_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "yunjey", "date": "2019-12-05 16:43:51", "content": "[R] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains", "link": "https://www.reddit.com/r/MachineLearning/comments/e6jryk/r_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "albrinbor", "date": "2019-12-05 11:57:12", "content": "[Research] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains /!/ Since the last post was removed by the owner and I found it interesting, I publish it back.\n\nPaper: [https://arxiv.org/pdf/1912.01865v1.pdf](https://arxiv.org/pdf/1912.01865v1.pdf)", "link": "https://www.reddit.com/r/MachineLearning/comments/e6g7cu/research_stargan_v2_diverse_image_synthesis_for/"}, {"autor": "albrinbor", "date": "2019-12-05 11:45:28", "content": "StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains", "link": "https://www.reddit.com/r/MachineLearning/comments/e6g356/stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "yunjeychoi", "date": "2019-12-05 11:43:34", "content": "[R] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains /!/ &amp;#x200B;\n\n![img](7b355mm2ys241 \"Diverse image synthesis results on the CelebA-HQ dataset and our newly collected animal faces (AFHQ) dataset. The first column shows input images while the remaining columns are images synthesized by StarGAN v2.\")\n\n&amp;#x200B;\n\n![video](e0syeez3ys241 \"StarGAN v2 can transform a source image into an output image reflecting the style (e.g., hairstyle and makeup) of a given reference image. Additional high-quality videos can be found at the link below.\")", "link": "https://www.reddit.com/r/MachineLearning/comments/e6g2hs/r_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "yunjeychoi", "date": "2019-12-05 10:22:30", "content": "[R] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains", "link": "https://www.reddit.com/r/MachineLearning/comments/e6fcaj/r_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "yunjey-choi", "date": "2019-12-05 10:13:50", "content": "[R] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/e6f9i1/r_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "yunjey-choi", "date": "2019-12-05 10:03:21", "content": "[R] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains /!/ StarGAN v2 can transform a source image into an output image reflecting the style (e.g., hairstyle and makeup) of a given reference image. Additional high-quality videos can be found at the link below.\n\n![video](x7jdss8ags241)\n\narXiv: [https://arxiv.org/abs/1912.01865](https://arxiv.org/abs/1912.01865)\n\ngithub: [https://github.com/clovaai/stargan-v2](https://github.com/clovaai/stargan-v2)\n\nvideo: [shorturl.at/eACS9](https://shorturl.at/eACS9)\n\n&amp;#x200B;\n\nAbstract\n\nA good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain variations. The code, pretrained models, and dataset will be released for reproducibility.", "link": "https://www.reddit.com/r/MachineLearning/comments/e6f61a/r_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "yunjey", "date": "2019-12-05 09:59:52", "content": "[R][P] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains /!/ &amp;#x200B;\n\n![img](ypy74yikfs241 \"Diverse image synthesis results on the CelebA-HQ dataset and our newly collected animal faces (AFHQ) dataset. The first column shows input images while the remaining columns are images synthesized by StarGAN v2.\")\n\n&amp;#x200B;\n\n![video](pdaoqt5rfs241 \"StarGAN v2 can transform a source image into an output image reflecting the style (e.g., hairstyle and makeup) of a given reference image. Additional high-quality videos can be found at the link below.\")\n\n&amp;#x200B;\n\narXiv: [https://arxiv.org/abs/1912.01865](https://arxiv.org/abs/1912.01865)\n\ngithub: [https://github.com/clovaai/stargan-v2](https://github.com/clovaai/stargan-v2)\n\nvideo: [shorturl.at/eACS9](https://shorturl.at/eACS9)\n\n&amp;#x200B;\n\nAbstract\n\nA good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain variations. The code, pretrained models, and dataset will be released for reproducibility.", "link": "https://www.reddit.com/r/MachineLearning/comments/e6f4st/rp_stargan_v2_diverse_image_synthesis_for/"}, {"autor": "yunjey-choi", "date": "2019-12-05 09:52:56", "content": "[R] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains /!/ &amp;#x200B;\n\n![img](6514prh6es241 \"Diverse image synthesis results on the CelebA-HQ dataset and our newly collected animal faces (AFHQ) dataset. The first column shows input images while the remaining columns are images synthesized by StarGAN v2.\")\n\n&amp;#x200B;\n\n![video](oa6dxz0ees241 \"StarGAN v2 can transform a source image into an output image reflecting the style (e.g., hairstyle and makeup) of a given reference image. Additional high-quality videos can be found at the link below.\")\n\n&amp;#x200B;\n\narXiv: [https://arxiv.org/abs/1912.01865](https://arxiv.org/abs/1912.01865)\n\ngithub: [https://github.com/clovaai/stargan-v2](https://github.com/clovaai/stargan-v2)\n\nvideo: [shorturl.at/eACS9](https://shorturl.at/eACS9)\n\n&amp;#x200B;\n\nAbstract\n\n&amp;#x200B;\n\nA good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain variations. The code, pretrained models, and dataset will be released for reproducibility.", "link": "https://www.reddit.com/r/MachineLearning/comments/e6f2rt/r_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "yunjey", "date": "2019-12-05 09:42:34", "content": "[R] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains /!/ &amp;#x200B;\n\n![img](jmh87ztdcs241 \"Diverse image synthesis results on CelebA-HQ and our newly collected AFHQ dataset. The first column shows input images while the remaining columns are images synthesized by StarGAN v2.\")\n\n&amp;#x200B;\n\n![video](hsmuamzmbs241 \" StarGAN v2 can transform a source image into an output image reflecting the style (e.g., hairstyle and makeup) of a given reference image. Additional high-quality videos can be found at the link below. \")\n\n&amp;#x200B;\n\narXiv: [https://arxiv.org/abs/1912.01865](https://arxiv.org/abs/1912.01865)\n\ngithub: [https://github.com/clovaai/stargan-v2](https://github.com/clovaai/stargan-v2)\n\nvideo: [shorturl.at/eACS9](https://shorturl.at/eACS9)\n\n&amp;#x200B;\n\nAbstract\n\n&amp;#x200B;\n\nA good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain variations. The code, pretrained models, and dataset will be released for reproducibility.", "link": "https://www.reddit.com/r/MachineLearning/comments/e6ezo1/r_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "yunjey", "date": "2019-12-05 07:48:03", "content": "[R] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains /!/ &amp;#x200B;\n\nhttps://preview.redd.it/gmzjjic5sr241.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=93ed0d8f70662b55011559f8beb180fa42cea80e", "link": "https://www.reddit.com/r/MachineLearning/comments/e6e08g/r_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "yunjey-choi", "date": "2019-12-05 07:42:15", "content": "StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains /!/ &amp;#x200B;\n\n![video](qrligix6rr241)", "link": "https://www.reddit.com/r/MachineLearning/comments/e6dyeo/stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "yunjeychoi", "date": "2019-12-05 07:32:13", "content": "[R] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains /!/ &amp;#x200B;\n\n![video](zg364mcbpr241 \"StarGAN v2 can generate high-quality images, reflecting the diverse styles (e.g., hairstyle, makeup) of the reference images.\")", "link": "https://www.reddit.com/r/MachineLearning/comments/e6dv4v/r_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "yunjey", "date": "2019-12-05 07:26:02", "content": "StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains", "link": "https://www.reddit.com/r/MachineLearning/comments/e6dsyf/stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "yunjey", "date": "2019-12-05 07:23:29", "content": "StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains /!/ &amp;#x200B;\n\n*Processing img 2r6inbnrnr241...*\n\narXiv: [https://arxiv.org/abs/1912.01865](https://arxiv.org/abs/1912.01865)\n\ngithub: [https://github.com/clovaai/stargan-v2](https://github.com/clovaai/stargan-v2)\n\nvideo: [shorturl.at/eACS9](https://shorturl.at/eACS9)\n\n&amp;#x200B;\n\nAbstract\n\nA good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain variations. The code, pretrained models, and dataset will be released for reproducibility.", "link": "https://www.reddit.com/r/MachineLearning/comments/e6ds24/stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "yunjey", "date": "2019-12-05 07:20:37", "content": "[R] StarGAN v2: Diverse -----> Image !!!  Synthesis for Multiple Domains /!/ StarGAN v2: Diverse Image Synthesis for Multiple Domains\n\n&amp;#x200B;\n\n![img](1pzdip14nr241 \"Diverse image synthesis results on CelebA-HQ and our newly collected AFHQ dataset.\")\n\narXiv: [https://arxiv.org/abs/1912.01865](https://arxiv.org/abs/1912.01865)\n\ngithub: [https://github.com/clovaai/stargan-v2](https://github.com/clovaai/stargan-v2)\n\nvideo: [shorturl.at/eACS9](https://shorturl.at/eACS9)\n\n&amp;#x200B;\n\nAbstract\n\nA good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain variations. The code, pretrained models, and dataset will be released for reproducibility.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n![video](d7i5410anr241 \"StarGAN v2 can synthesize high-quality images, reflecting the diverse styles (e.g., hairstyle and makeup) of reference images.\")", "link": "https://www.reddit.com/r/MachineLearning/comments/e6dr18/r_stargan_v2_diverse_image_synthesis_for_multiple/"}, {"autor": "whria78", "date": "2019-12-05 00:29:07", "content": "[R] Facial Skin Cancer Detection using R-CNN /!/ Our paper \u201cKeratinocytic Skin Cancer Detection on the Face using Region-based Convolutional Neural Network\u201d was published on JAMA Dermatology. To my knowledge, the performance of cancer detection was compared with that of dermatologists for the first time in dermatology. Because most of previous studies were classification studies, preselection of end-user was essential. In addition, there were numerous false positives because training data set did not include enough number of common disorders and normal structures.\n\n \n\n \n\nWith the assistance of R-CNN, we trained neural networks with 1,106,886 -----> image !!!  crops to localize and diagnose malignancy. The algorithm detects suspected lesion and shows malignancy score and predicts possible diagnosis (178 disease classes).\n\nWe used region-based CNN (faster-RCNN; backbone = VGG-16) as a region proposal module, and utilized CNN (SE-ResNet-50) to choose adequate lesion, and utilized CNN (SE-ResNeXt-50 + SENet) to determine malignancy. We chose a multi-step approach to reduce the dimension of problem (object detection -&gt; classification).\n\n \n\n \n\nThe AUC for the validation dataset (2,844 images from 673 patients comprising 185 malignant, 305 benign, and 183 normal conditions) was 0.910. The algorithm\u2019s F1 score and Youden index (sensitivity + specificity - 100%) were comparable with those of 13 dermatologists, while surpassing those of 20 non-dermatologists (325 images from 80 patients comprising 40 malignant, 20 benign, and 20 normal). We are performing an additional work with large scale external validation data set. The pilot result is similar with this report, so I hope I will publish soon.\n\n \n\n \n\nWeb DEMO ([https://rcnn.modelderm.com](https://rcnn.modelderm.com)) of the model is accessible via smartphone or PC, to facilitate scientific communication. Sorry for the slowness of the DEMO because it runs on my personal computer despite of the multi-threading and parallel processing with 2080 x1 and 1070 x1.\n\nThank you.\n\n \n\nPaper : [https://jamanetwork.com/journals/jamadermatology/article-abstract/2756346](https://jamanetwork.com/journals/jamadermatology/article-abstract/2756346)\n\nScreenshot : [https://i.imgur.com/2TCkdHf.png](https://i.imgur.com/2TCkdHf.png)\n\nScreenshot : [https://i.imgur.com/IEZLfOg.jpg](https://i.imgur.com/IEZLfOg.jpg)\n\nDEMO : [https://rcnn.modelderm.com](https://rcnn.modelderm.com)", "link": "https://www.reddit.com/r/MachineLearning/comments/e68uco/r_facial_skin_cancer_detection_using_rcnn/"}, {"autor": "WengeKong", "date": "2019-12-04 19:38:35", "content": "[D] VQ-VAE-2 /!/ \"Generating Diverse High-Fidelity Images with VQ-VAE-2\" produce high quality and high resolution face -----> image !!! .\n\nDoes it beat state-of-the-art StyleGAN in face generation task? Thank you", "link": "https://www.reddit.com/r/MachineLearning/comments/e64j51/d_vqvae2/"}, {"autor": "UltraMarathonMan", "date": "2019-12-14 15:34:12", "content": "[D] Rohit Prasad: Amazon Alexa and Conversational AI /!/ Rohit Prasad is the vice president and head scientist of Amazon Alexa and one of its original creators.\n\n**Video:** https://www.youtube.com/watch?v=Ad89JYS-uZM  \n**Audio:** https://lexfridman.com/rohit-prasad\n\n**Outline:**  \n*(click on the timestamp to jump to that part of the video)*\n\n[0:00](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=0s) \\- Introduction  \n[4:34](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=274s) \\- Her  \n[6:31](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=391s) \\- Human-like aspects of smart assistants  \n[8:39](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=519s) \\- Test of intelligence  \n[13:04](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=784s) \\- Alexa prize  \n[21:35](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=1295s) \\- What does it take to win the Alexa prize?  \n[27:24](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=1644s) \\- Embodiment and the essence of Alexa  \n[34:35](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=2075s) \\- Personality  \n[36:23](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=2183s) \\- Personalization  \n[38:49](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=2329s) \\- Alexa's backstory from her perspective  \n[40:35](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=2435s) \\- Trust in Human-AI relations  \n[44:00](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=2640s) \\- Privacy  \n[47:45](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=2865s) \\- Is Alexa listening?  \n[53:51](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=3231s) \\- How Alexa started  \n[54:51](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=3291s) \\- Solving far-field speech recognition and intent understanding  \n[1:11:51](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=4311s) \\- Alexa main categories of skills  \n[1:13:19](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=4399s) \\- Conversation intent modeling  \n[1:17:47](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=4667s) \\- Alexa memory and long-term learning  \n[1:22:50](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=4970s) \\- Making Alexa sound more natural  \n[1:27:16](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=5236s) \\- Open problems for Alexa and conversational AI  \n[1:29:26](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=5366s) \\- Emotion recognition from audio and video  \n[1:30:53](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=5453s) \\- Deep learning and reasoning  \n[1:36:26](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=5786s) \\- Future of Alexa  \n[1:41:47](https://www.youtube.com/watch?v=Ad89JYS-uZM&amp;t=6107s) \\- The big -----> picture !!!  of conversational AI\n\nhttps://preview.redd.it/j0ut1ljfbm441.png?width=1280&amp;format=png&amp;auto=webp&amp;s=e13fd282f47c36cb752e272ab60fca78c48cc787", "link": "https://www.reddit.com/r/MachineLearning/comments/eal2oa/d_rohit_prasad_amazon_alexa_and_conversational_ai/"}, {"autor": "wildtales", "date": "2019-12-13 17:53:00", "content": "[D] Repositioning the -----> camera !!!  for generating adversarial examples /!/ Is it possible to keep an image unchanged, while positioning the camera (possibly by attaching it to a robo-hand) so that the image is misclassified by a trained classifier? The orientation and position of the camera for misclassification can be learnt by gradient descent.", "link": "https://www.reddit.com/r/MachineLearning/comments/ea7aw7/d_repositioning_the_camera_for_generating/"}, {"autor": "otter-in-a-suit", "date": "2019-12-13 16:01:38", "content": "Tensorflow on edge, or \u2013 Building a \u201csmart\u201d security -----> camera !!!  with a Raspberry Pi", "link": "https://www.reddit.com/r/MachineLearning/comments/ea5tbb/tensorflow_on_edge_or_building_a_smart_security/"}, {"autor": "da_g_prof", "date": "2019-12-13 09:28:01", "content": "[D] Tools for quick web-demos? /!/ Hi all, I see recently a proliferation of papers that not only release the code and some supplementary results in a github repo, but have complete sites with interactive demos.\n\nExamples: [http://gannotsee.net/](http://gannotsee.net/), [http://gandissect.res.ibm.com/ganpaint.html](http://gandissect.res.ibm.com/ganpaint.html)\n\nTo me, these seem like a difficult exercise that not all students / labs, have access to. Unless there is a quick tool/pipeline that people use. \n\nAre all these websites dynamic and have server side stuff to run the model?  \nor is there a simpler way to do this browser (client-side)? \n\nI guess the answer depends on the type of demo (e.g. here everything is pre-done: [https://dmitryulyanov.github.io/deep\\_image\\_prior](https://dmitryulyanov.github.io/deep_image_prior)), but I am just wondering if there is a tool etc to make the whole process easier.\n\nTo clarify: I am aware of tensorflow-js, and in browser running of models, but I am referring to a bigger -----> picture !!! .", "link": "https://www.reddit.com/r/MachineLearning/comments/ea1uu9/d_tools_for_quick_webdemos/"}, {"autor": "annoyed_panda", "date": "2019-12-13 06:16:02", "content": "[D] Tensorflow GPU memory management (TF_FORCE_GPU_ALLOW_GROWTH) /!/   So this is more of an exploratory question. I am deploying models using a TF serving docker -----> image !!!  with the flag TF\\_FORCE\\_GPU\\_ALLOW\\_GROWTH. I am deploying a small fashion mnist model, resnet (99MB), and inception v3(92MB) models. Because of the flag, the tf model server initially occupies only \\~300 MB approx, then on sequential requests to the models it increases as follows (according to nvidia-smi):\n\n\\~300 MB | after inception request \\~4306MB | after resnet request \\~ 8402 MB\n\nif I send a request to resnet first, the GPU usage does not increase at all (Even when I add more models):\n\n\\~300 MB | after resnet request \\~7888MB | after inception request \\~ 7888 MB\n\nWhy does the GPU usage not increase after adding more models? Are they flushed from memory when new models are loaded for inference? How can I accurately estimate how many similar sized models can be loaded on one GPU enabled machine without the trial and error method? Is there a pattern to what fraction of GPU memory is progressively allocated?\n\nNote: This is run on an EC2 instance with available GPU memory 11441MiB \\[ Tesla K80 \\] when I trey to run the same on a machine with lower capacity \\[Quadro P2000 - 5059 MB\\], I face a similar situation where there is no increase in memory usage. However, I also get the following in the logs:\n\n    2019-12-11 05:10:54.727985: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.25GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available. 2019-12-11 05:10:54.736610: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:211] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.26GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available", "link": "https://www.reddit.com/r/MachineLearning/comments/ea083i/d_tensorflow_gpu_memory_management_tf_force_gpu/"}, {"autor": "leaderj1001", "date": "2019-05-04 02:01:03", "content": "[P] Implementing Billion-scale semi-supervised learning for -----> image !!!  classification using Pytorch /!/ Hi, reddit !! We are Myeongjun Kim and Taehun Kim. Our major is computer vision research using deep learning. Previously, we implemented the paper with RandWireNN pytorch version and tensorflow version. This time, we implemented a paper on \"Billion-scale semi-supervised learning for image classification written by Facebook AI\"(https://arxiv.org/abs/1905.00546). To briefly describe the paper, it is stated that the classification performance is improved by using unlabeled data. We realized that it was a simple and novel idea. So, we implemented it.\nDue to the lack of GPU resources and unlabeled data, we are delaying the experiment on ImageNet and are experimenting with CIFAR-100 first. We would like to ask for your interest and feedback.\nThank you for reading long long story.\nGithub URL: https://github.com/leaderj1001/Billion-scale-semi-supervised-learning", "link": "https://www.reddit.com/r/MachineLearning/comments/bkgb63/p_implementing_billionscale_semisupervised/"}, {"autor": "gwern", "date": "2019-05-03 22:48:40", "content": "\"Billion-scale semi-supervised learning for -----> image !!!  classification\", Yalniz et al 2019 {FB}", "link": "https://www.reddit.com/r/MachineLearning/comments/bkek6e/billionscale_semisupervised_learning_for_image/"}, {"autor": "neziib", "date": "2019-05-03 17:38:21", "content": "Simple storage archive for -----> image !!!  dataset /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/bkb7bj/simple_storage_archive_for_image_dataset/"}, {"autor": "iabhishekofficial", "date": "2019-05-03 06:47:14", "content": "Machine Learning Summarized in one -----> picture !!! ", "link": "https://www.reddit.com/r/MachineLearning/comments/bk5emn/machine_learning_summarized_in_one_picture/"}, {"autor": "rednivaralat", "date": "2019-05-02 10:14:04", "content": "Which algorithm suits best to your business problem? Have a look at the -----> image !!!  below /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/bjt8pe/which_algorithm_suits_best_to_your_business/"}, {"autor": "thumbsdrivesmecrazy", "date": "2019-05-02 09:40:52", "content": "[R] Why Git and Git-LFS is not enough to solve the Machine Learning Reproducibility crisis - TowardsDataScience /!/ Keeping the data under version control with Git-LFS is a big improvement. But the lack of version control of the data files is not the entire problem.\n\nThe determining factors for the results of training a model or other activities include the following:\n\n* Training data\u200a\u2014\u200athe -----> image !!!  database or whatever data source is used in training the model\n* The scripts used in training the model\n* The libraries used by the training scripts\n* The scripts used in processing data\n* The libraries or other tools used in processing data\n* The operating system and CPU/GPU hardware\n* Production system code\n* Libraries used by production system code\n\nObviously the result of training a model depends on a variety of conditions. Since there are so many variables to this, it is hard to be precise, but the general problem is a lack of what\u2019s now called Configuration Management.\n\n[DVC](https://dvc.org/) takes on and solves a larger slice of the machine learning reproducibility problem than does Git-LFS or several other potential solutions:\n\n[DVC workflow - code &amp; data](https://i.redd.it/mbl8s0s1qrv21.png)\n\nFull article: [Why Git and Git-LFS is not enough to solve the Machine Learning Reproducibility crisis](https://towardsdatascience.com/why-git-and-git-lfs-is-not-enough-to-solve-the-machine-learning-reproducibility-crisis-f733b49e96e8)", "link": "https://www.reddit.com/r/MachineLearning/comments/bjszua/r_why_git_and_gitlfs_is_not_enough_to_solve_the/"}, {"autor": "abriosi", "date": "2019-05-13 17:21:07", "content": "[P] This eye does not exist - Using GANs to separate an unlabeled -----> image !!!  dataset into clusters", "link": "https://www.reddit.com/r/MachineLearning/comments/bo6d6v/p_this_eye_does_not_exist_using_gans_to_separate/"}, {"autor": "longinglove", "date": "2019-05-13 04:33:00", "content": "[P] SpecAugment experiments using tensor2tensor /!/ SpecAugment experiments using tensor2tensor\n\n&amp;#x200B;\n\n[https://github.com/Kyubyong/specAugment](https://github.com/Kyubyong/specAugment)\n\n&amp;#x200B;\n\nThis is an implementation of [SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition](https://arxiv.org/abs/1904.08779).\n\n**Notes**\n\n* The paper introduces three techniques for augmenting speech data in speech recognition.\n* They come from the observation that spectrograms which often used as input can be treated as -----> image !!! s, so various -----> image !!!  augmentation methods can be applied.\n* I find the idea interesting.\n* It covers three methods: time warping, frequency masking, and time masking.\n* Details are clearly explained in the paper.\n* While the first one, time warping, looks salient apparently, Daniel, the first author, told me that indeed the other two are much more important than time warping, so it can be ignored if necessary. (Thanks for the advice, Daniel!)\n* I found that implementing time warping with TensorFlow is tricky because the relevant functions are based on the static shape of the melspectrogram tensor, which is hard to get from the pre-defined graph.\n* I test frequency / time masking on Tensor2tensor's LibriSpeech Clean Small Task.\n* The paper used the LAS model, but I stick to Transformer.\n* To compare the effect of specAugment, I also run a base model without augmentation.", "link": "https://www.reddit.com/r/MachineLearning/comments/bnyw65/p_specaugment_experiments_using_tensor2tensor/"}, {"autor": "og_mryamz", "date": "2019-05-12 23:14:54", "content": "[D] GPT Model 345M Prose /!/ This AI's contemporary prose is so rich in imagery, and allegory.\n------------------------------------------------------------\nMy Randomly thought of Prompt:\n------------------------------------------------------------\nIt was a quaint village, not too far from the southern bay. The smell of grass fumed -- like it always did early spring -- as the sound of the stream rushed along the riverside. A girl stood and wondered while taking in the cool crisp air. These were her final moments.\n------------------------------------------------------------\nGPT Model 345M Output:\n------------------------------------------------------------\nNo sooner had she placed her hand on the nameplate on the plaque than a shaft of sunlight stabbed through the mist at her feet. She looked down at the cold marble inside and saw that underneath was her sister's diamond-studded slipcase. It was open for inspection. It was ice cold and dark and strangely, she called it \"Shiny.\" She was so lonely at this point it hurt badly. The girl let out a short struggle. Snow drifts filled the sky, overlapping like sides of frost inside the ladder from which she slid down into the fissure of the mountain. At last, a firm hand fell on the hard smooth stone again. It's dull, granite surface gleamed out in the dark wind beneath, like a mouth of millions of teeth. \"You'll remember me for the court case we win today. Then the dust will pass.\" She looked down at her carved stone and over at the child. \"They'll see right through it.\" There was the crossroads, and no spiritual journey was made.\n\n\nWhen she finally got down, as far as she had ever gone on ice -- still heading down on the roads -- there was hardly a breath of wind. The little snow from the bottom of the mountain director faced directly up, overhead, as her face opened up into sincere smiling and the cold slab at her heel curled around her into an -----> image !!!  of life. As she let out her first breath of air she clawed like a red-clawed beast at the chain that tightens at the base of her glassy skull and when she broke the tight clasp and looked up into the direction of the softly glowing sun, it felt like seeing snow landscape like a panel cast from crystalline stone. It wasn't snow on her foot -- for thousands of years, snow had fallen on virtually every other person away from religion -- and the snow was real after all, shadowed and much too complex to fit into a myth. But, and her certainest prayer in her fingers, the joy still lingered when she saw her moon face, minus a skull to juxtapose the lineaments of our mortal soul.", "link": "https://www.reddit.com/r/MachineLearning/comments/bnvklk/d_gpt_model_345m_prose/"}, {"autor": "og_mryamz", "date": "2019-05-12 23:04:54", "content": "GPT Model 345M Output /!/ This ai prose is rich with allegory\n\nPrompt\n\nIt was a quaint village, not too far from the southern bay. The smell of grass fumed -- like it always did early spring -- as the sound of the stream rushed along the riverside. A girl stood and wondered while taking in the cool crisp air. These were her final moments.\n\nGPT Output (after 5 samples)\n\nNo sooner had she placed her hand on the nameplate on the plaque than a shaft of sunlight stabbed through the mist at her feet. She looked down at the cold marble inside and saw that underneath was her sister's diamond-studded slipcase. It was open for inspection. It was ice cold and dark and strangely, she called it \"Shiny.\" She was so lonely at this point it hurt badly. The girl let out a short struggle. Snow drifts filled the sky, overlapping like sides of frost inside the ladder from which she slid down into the fissure of the mountain. At last, a firm hand fell on the hard smooth stone again. It's dull, granite surface gleamed out in the dark wind beneath, like a mouth of millions of teeth. \"You'll remember me for the court case we win today. Then the dust will pass.\" She looked down at her carved stone and over at the child. \"They'll see right through it.\" There was the crossroads, and no spiritual journey was made.\n\n\nWhen she finally got down, as far as she had ever gone on ice -- still heading down on the roads -- there was hardly a breath of wind. The little snow from the bottom of the mountain director faced directly up, overhead, as her face opened up into sincere smiling and the cold slab at her heel curled around her into an -----> image !!!  of life. As she let out her first breath of air she clawed like a red-clawed beast at the chain that tightens at the base of her glassy skull and when she broke the tight clasp and looked up into the direction of the softly glowing sun, it felt like seeing snow landscape like a panel cast from crystalline stone. It wasn't snow on her foot -- for thousands of years, snow had fallen on virtually every other person away from religion -- and the snow was real after all, shadowed and much too complex to fit into a myth. But, and her certainest prayer in her fingers, the joy still lingered when she saw her moon face, minus a skull to juxtapose the lineaments of our mortal soul.", "link": "https://www.reddit.com/r/MachineLearning/comments/bnvgyl/gpt_model_345m_output/"}, {"autor": "Former_Hippo", "date": "2019-05-12 16:08:58", "content": "[D] GANs were invented in 2010? /!/ Link: [https://web.archive.org/web/20120312111546/http://yehar.com:80/blog/?p=167](https://web.archive.org/web/20120312111546/http://yehar.com:80/blog/?p=167)\n\n&amp;#x200B;\n\n2010-02-24\n\nA **method for training artificial neural networks** to generate missing data within a variable context. As the idea is hard to put in a single sentence, I will use an example:\n\nAn -----> image !!!  may have missing pixels (let's say, under a smudge). How can one restore the missing pixels, knowing only the surrounding pixels? One approach would be a \"generator\" neural network that, given the surrounding pixels as input, generates the missing pixels.\n\nBut how to train such a network? One can't expect the network to exactly produce the missing pixels. Imagine, for example, that the missing data is a patch of grass. One could teach the network with a bunch of images of lawns, with portions removed. The teacher knows the data that is missing, and could score the network according to the root mean square difference (RMSD) between the generated patch of grass and the original data. The problem is that if the generator encounters an image that is not part of the training set, it would be impossible for the neural network to put all the leaves, especially in the middle of the patch, in exactly the right places. The lowest RMSD error would probably be achieved by the network filling the middle area of the patch with a solid color that is the average of the color of pixels in typical images of grass. If the network tried to generate grass that looks convincing to a human and as such fulfills its purpose, there would be an unfortunate penalty by the RMSD metric.\n\nMy idea is this (see figure below): Train simultaneously with the generator a classifier network that is given, in random or alternating sequence, generated and original data. The classifier then has to guess, in the context of the surrounding image context, whether the input is original (1) or generated (0). The generator network is simultaneously trying to get a high score (1) from the classifier. The outcome, hopefully, is that both networks start out really simple, and progress towards generating and recognizing more and more advanced features, approaching and possibly defeating human's ability to discern between the generated data and the original. If multiple training samples are considered for each score, then RMSD is the correct error metric to use, as this will encourage the classifier network to output probabilities.", "link": "https://www.reddit.com/r/MachineLearning/comments/bnqm0p/d_gans_were_invented_in_2010/"}, {"autor": "thatglitch", "date": "2019-07-28 17:14:10", "content": "[Research] Tattoo Synthesis /!/ How would you approach this concept: I am interested in making a bot that creates tattoos. I have a tattoo artist who is willing to potentially sacrifice his artistic integrity and feed the bot his simple designs to serve as a dataset. Which framework do you reckon would be best? Let's say we have 1000 individual high res scans of black tattoos on white background and we want the bot to spit new, artificially synthesized designs (even if abstract and nonsensical). Any help would be much appreciated. \nThis is for a short -----> film !!! . We have backing behind this so if there is anyone interested in being part of a creative collaboration and potentially get some small buck for your time, hit me in the DMs.", "link": "https://www.reddit.com/r/MachineLearning/comments/ciygrm/research_tattoo_synthesis/"}, {"autor": "therumsticks", "date": "2019-07-28 16:50:36", "content": "[Discussion] How to get generalization across -----> camera !!!  parameters? /!/ If I want to perform some computer vision task using CNN (let's say segmentation), how can I ensure that I can get generalization across images that were captured using another camera(with different intrinsic matrix)? I tried using a learned semantic segmentation model on my own camera images and it failed terribly.  I suspect this is because the convolution filters are overfitted to the particular resolution (fx, fy) we trained our model on.", "link": "https://www.reddit.com/r/MachineLearning/comments/ciy5z1/discussion_how_to_get_generalization_across/"}, {"autor": "thatglitch", "date": "2019-07-28 16:46:06", "content": "Tattoo Synthesis /!/ How would you approach this concept: I am interested in making a bot that creates tattoos. I have a tattoo artist who is willing to potentially sacrifice his artistic integrity and feed the bot his simple designs to serve as a dataset. Which framework do you reckon would be best? Let's say we have 1000 individual high res scans of black tattoos on white background and we want the bot to spit new, artificially synthesized designs (even if abstract and nonsensical). Any help would be much appreciated. \nThis is for a short -----> film !!! . We have backing behind this so if there is anyone interested in being part of a creative collaboration and potentially get some small buck for your time, hit me in the DMs.", "link": "https://www.reddit.com/r/MachineLearning/comments/ciy3zj/tattoo_synthesis/"}, {"autor": "Nickless314", "date": "2019-07-28 08:33:59", "content": "[R] DNN representational convexity and Human visual illusions /!/  [https://arxiv.org/abs/1907.09019](https://arxiv.org/abs/1907.09019)\n\nTL;DR we consider -----> image !!! s that show illusion in Human, represent the -----> image !!! s in ImageNet-trained VGG-19, and find that representational distances around an illusion -----> image !!!  are \"weird\" (non-monotonic). This is mostly unlike distances around control images, at least to the extend that we checked.\n\nWhat do you think?  \nWe are now considering extensions, and would very much appreciate ideas, even negative.", "link": "https://www.reddit.com/r/MachineLearning/comments/citi3f/r_dnn_representational_convexity_and_human_visual/"}, {"autor": "strangecosmos", "date": "2019-07-28 06:58:18", "content": "[D] When will self-supervised learning replace supervised learning for computer vision tasks where unlabelled video is abundant? /!/ DeepMind's self-supervised ([a.k.a. unsupervised](https://www.facebook.com/722677142/posts/10155934004262143/)) network, CPC, [surpassed AlexNet's performance](https://medium.com/syncedreview/new-deepmind-unsupervised------> image !!! -model-challenges-alexnet-d658ef92ab1e) on ImageNet. If I understand correctly, both CPC and AlexNet used the same set of training images. CPC just didn't use labels, while AlexNet did. So, what about instances where a self-supervised network can be trained on 10,000x as much data as would be economically feasible to label? In these cases, are supervised learning's days numbered? Or not so fast? \n\nThe application I'm personally most interested in is self-driving cars. By putting cameras on consumer cars, you are limited really only by your fleet size, your data centre costs, and your customers' monthly bandwidth limits for their home wifi in terms of how much data you can collect. Tesla, for instance, has over 500,000 cars with 360-degree cameras, GPUs or ASICs to run neural networks, and the ability to connect to wifi and upload training data. Elon Musk recently [mentioned](https://youtu.be/Ucp0TTmvqOE) Tesla's plans to \"do unsupervised massive training of vast amounts of video\". Tesla's Director of AI, Andrej Karpathy, also recently [tweeted](https://twitter.com/karpathy/status/1152701561212030976?s=20) his strong support for self-supervised learning. So this question is more than hypothetical.", "link": "https://www.reddit.com/r/MachineLearning/comments/cistim/d_when_will_selfsupervised_learning_replace/"}, {"autor": "good_rice", "date": "2019-07-28 04:41:41", "content": "[D] skip-GANomaly and general issues with reproduction of papers /!/ Hi all, I'm currently working through an implementation of skip-GANomaly, a paper on anomaly detection using an adversarially trained auto-encoder. In a nutshell, they train an auto-encoder over normal examples. Abnormal examples at test time will have poorer reconstruction, and thus higher loss, as they come from a different -----> image !!!  distribution than normal examples. \n\nHowever, this paper contains no code. Additionally, the paper contains no clear definition of their network architecture. The only description of the network architecture is a figure shown beneath: \n\n[Problematic Figure of skip-GANomaly](https://i.redd.it/8zb8wu770zc31.png)\n\nAs there is no explanation, I interpret the initial convolutional maps to come from a single convolution from 3 -&gt; 64 channels; each following arrow is LReLU, Conv, and BN, so a nonlinearity would be applied afterwards. \n\nHowever, for any initial convolutional feature maps concatenated over, as shown in the image, reconstruction could be perfect without any of the rest of the network as a single convolution would be necessary to go from 64 + 64 -&gt; 3 channels. This convolution could just learn the inverse of the initial convolution which created the feature maps from the image. I could be misinterpreting this, so if anyone could enlighten me, that'd be great. I'd imagine that the feature maps resulting from the first convolution could not be concatenated, unless feature maps are defined to be subsequent to batch norm and activations. If bn and LReLU is included in the initial convolution, this is not described anywhere in the paper.\n\n**In general,** how frequent are papers published without code / with difficulties in obtaining the architecture or training specifications to reproduce the work? I'm a student, and this is the first time I've diverged from large SOTA papers with thousands of citations. I was surprised to encounter a lot of the ambiguity in this paper.", "link": "https://www.reddit.com/r/MachineLearning/comments/cirpen/d_skipganomaly_and_general_issues_with/"}, {"autor": "macob12432", "date": "2019-07-27 23:49:25", "content": "why convolutional neural networks work? /!/ Who came up with the use of watt filters and pass them over an -----> image !!!  and then increase them to a neural network to work?\n\nI see the amount of mathematics and I imagine aliens teaching humans how to use artificial intelligence.", "link": "https://www.reddit.com/r/MachineLearning/comments/cios5i/why_convolutional_neural_networks_work/"}, {"autor": "azzipog", "date": "2019-07-26 22:14:26", "content": "What's the best way to detect horizontal lines in an -----> image !!! . HoughLinesP isn't finding all of them. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ci9c0d/whats_the_best_way_to_detect_horizontal_lines_in/"}, {"autor": "manicman1999", "date": "2019-07-26 17:26:17", "content": "[P] Finding similarities and clustering r/EarthPorn images using BiGAN, implemented in Keras! /!/ Hi everyone! I recently implemented BiGAN in Keras, with some modifications (hinge loss, gradient penalty, etc.). Then, with that, I made a script to find similarities in r/EarthPorn images using BiGAN's encoder, as well as clustering images using k-means clustering in the feature space. I've uploaded my code to github aswell.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nHere is an -----> image !!! , on the left are target -----> image !!! s, and on the right are the 7 most \"similar\" -----> image !!! s in the dataset, in order.\n\n[https://i.imgur.com/owXSkXf.png](https://i.imgur.com/owXSkXf.png)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nHere is another, where each row is a cluster, detecting by k-means in the encoder's feature space.\n\n[https://i.imgur.com/wgiKuiK.png](https://i.imgur.com/wgiKuiK.png)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nLastly, here is another image. On the top are images generated by BiGAN. On the bottom are real images of landscapes (1st and 3rd row) and their reconstruction using BiGAN's E and G (2nd and 4th row).\n\n[https://i.imgur.com/lcUzGZk.png](https://i.imgur.com/lcUzGZk.png)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nGithub implementation:\n\n[https://github.com/manicman1999/Keras-BiGAN](https://github.com/manicman1999/Keras-BiGAN)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nEnjoy!", "link": "https://www.reddit.com/r/MachineLearning/comments/ci5rzz/p_finding_similarities_and_clustering_rearthporn/"}, {"autor": "creotiv", "date": "2019-12-17 14:00:18", "content": "[P] Tutorial: Automatic cropping and -----> image !!!  warping", "link": "https://www.reddit.com/r/MachineLearning/comments/ebw4xr/p_tutorial_automatic_cropping_and_image_warping/"}, {"autor": "creotiv", "date": "2019-12-17 13:32:28", "content": "[P] OpenCV Tutorial: Automatic cropping and -----> image !!!  warping", "link": "https://www.reddit.com/r/MachineLearning/comments/ebvu5v/p_opencv_tutorial_automatic_cropping_and_image/"}, {"autor": "creotiv", "date": "2019-12-17 12:47:21", "content": "OpenCV Tutorial: Automatic cropping and -----> image !!!  warping", "link": "https://www.reddit.com/r/MachineLearning/comments/ebvdzd/opencv_tutorial_automatic_cropping_and_image/"}, {"autor": "APumarola", "date": "2019-12-17 11:05:20", "content": "[R] C-Flow: Conditional Generative Flow Models for Images and 3D Point Clouds /!/ &amp;#x200B;\n\nhttps://preview.redd.it/x1qxpi3p76541.jpg?width=736&amp;format=pjpg&amp;auto=webp&amp;s=7761c84809112c8bda57e24cbbc9810c2870a84d\n\nWe introduce a novel conditioning scheme that brings normalizing flows to an entirely new scenario for multi-modal data modeling. We demonstrate our conditioning method to be very adaptable, being applicable to -----> image !!!  manipulation, style transfer and multi-modal mapping in a diversity of domains, including RGB -----> image !!! s, 3D point clouds, segmentation maps, and edge masks.\n\nPaper PDF:  https://arxiv.org/pdf/1912.07009.pdf\n\nProject Website:   https://www.albertpumarola.com/research/C-Flow/index.html", "link": "https://www.reddit.com/r/MachineLearning/comments/ebuh0v/r_cflow_conditional_generative_flow_models_for/"}, {"autor": "liamjacobjose4", "date": "2019-12-17 10:12:45", "content": "EdgeTPU model for object detection using squeezedet /!/ I found out a project in Freelancer and here is the title and description \n\n&amp;#x200B;\n\nTitle : EdgeTPU model for object detection using squeezedet   \n\n\nOur project is based on squeezedet which can be found here ([https://github.com/BichenWuUCB/squeezeDet.git](https://github.com/BichenWuUCB/squeezeDet.git))  \n We need to bring our model to EDGE TPU (Coral) according the steps shown at [https://coral.ai/docs/edgetpu/models-intro](https://coral.ai/docs/edgetpu/models-intro) adopting a \"Quantization aware training approach\".  \n The expected deliverable are:  \n \\- quantized model for tensorflow (python)  \n \\- script for frozen graph  \n \\- script for tf\\_lite convert  \n \\- script for TPU compile  \n \\- minimal python program for using the compiled network with an input -----> image !!!   \n \\- sw requirements for running the previous scripts/programs  \n \\- docker -----> image !!!  for the environment to be used to run the previous scripts/programs       \n\n\nIf you guys can help me hwith ow to do it, I'd be very grateful. I need some videos/websites to walk me through the whole process since I am a beginner.   \n\n\nThank you.", "link": "https://www.reddit.com/r/MachineLearning/comments/ebu0g5/edgetpu_model_for_object_detection_using/"}, {"autor": "Kivid93", "date": "2019-09-04 10:39:19", "content": "[D] Explainability of PCA + Random Forest on images /!/ Good morning everyone.\n\nI am working on a project where I have a dataset of -----> image !!! s regarding an industrial product and I am currently classifying each -----> image !!!  into two classes, let's say \\[good, bad\\].\n\nIn order to correctly classify the images, I am currently applying the following techniques in this order: [HOG](https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.hog) \\+ [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) \\+ [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n\nMy performance are really good, but I have the necessity to know a specific information, which is roughly the following: \"this area of the image weight X% on the final classification\".\n\nFirst of all, I am executing HOG, so the \"area of the image\" could be the HOG Cell. Then, I need to gather some feature importances. in order to get this information, Random Forest comes in handy because it will let you know the [feature importances](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html) in percentage form, once trained. But here the problem arises: my features comes from a PCA. Now, is possible to execute an inverse transformation on a PCA output array and that will give you back the original features array before the application of PCA (which for me is the output of HOG). \n\nIn my scenario, will it make sense to apply the inverse transformation to the array of feature importances? And if not, which approach should I go for?\n\nThanks in advance!", "link": "https://www.reddit.com/r/MachineLearning/comments/czj3rg/d_explainability_of_pca_random_forest_on_images/"}, {"autor": "PyWarrior", "date": "2019-09-04 09:03:59", "content": "Best Way to generate synthetic -----> image !!!  data /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/czib8p/best_way_to_generate_synthetic_image_data/"}, {"autor": "da_g_prof", "date": "2019-09-03 19:41:29", "content": "[D] looking for subjective evaluation tools or web service /!/ Some of us work in -----> image !!!  synthesis / translation problems where we can't use Amazon Mturk.\n\nWe need to show people some images and we want them to score / rank each picture or pair or panel etc \n\nOur current approach to this is to create a stack of images or PDF and share it along with an excel etc.\n\nQuestion 1: is there a tool that we can use to have users see a panel of images and rank/score from a drop down etc? I know we can write our own gui / interface but seems a bit redundant. Is there a simpler way? \n\nQuestion 2: you can see how the above won't scale to many different users . Are you aware of any web service / tool instead? (*)\n\n* one time I did this as a Google form and send it to our specialist pool and worked out wonders but I had to hand do it. It seems that Google api has a scripting language to automate things (which means we may be able to automate part) but Google forms has restrictions in the width of images that can be shown so it affects the range of applications.", "link": "https://www.reddit.com/r/MachineLearning/comments/cz9qmo/d_looking_for_subjective_evaluation_tools_or_web/"}, {"autor": "lolo_sicara", "date": "2019-09-03 12:53:23", "content": "[D] What annotation tool do you use for your -----> image !!! /video datasets? /!/ I have just written an article about a benchmark I made of labeling tools for computer vision.\n\n[https://blog.sicara.com/best-open-source-annotation-tools-in-computer-vision-4b9f6a18f911](https://blog.sicara.com/best-open-source-annotation-tools-in-computer-vision-4b9f6a18f911)\n\nDo you know other tools which I should talk about?", "link": "https://www.reddit.com/r/MachineLearning/comments/cz4hci/d_what_annotation_tool_do_you_use_for_your/"}, {"autor": "DEAD_SH0T", "date": "2019-09-03 12:45:46", "content": "Easiest thing to detect /!/ What should be the easiest thing to detect in a real world -----> image !!! ? Is it a cat, dog or person (owing to presence of large opensource data and research)? Is it characters for same reason in the form of OCR? Or is it simple patterns like lines and circles because of simplicity although data could be an issue. My problem is I have to to recognise objects and I have, say, a model to detect these objects accurately and I have the liberty to paint/colour patterns or photos on these objects. I know the conventional way is to print numbers and ocr them. I am looking for something simpler.", "link": "https://www.reddit.com/r/MachineLearning/comments/cz4e6s/easiest_thing_to_detect/"}, {"autor": "Ben-Harries", "date": "2019-09-03 09:07:55", "content": "Deep learning approach for detecting any part of a human in an -----> image !!! ??? /!/ Been using fastai DL toolkit recently and want to apply it to detecting if image consent is needed for an image post on my companies platform.\n\nWould the way be to use multilabel classification of many images of people? \n\nTagged eg. foot; skin; kind of thing and then train and work it out. \n\nNeed to then train on images which don't contain humans or not needed? Can then work out if patient consent is needed.\n\nAny small response would be very much appreciated!", "link": "https://www.reddit.com/r/MachineLearning/comments/cz2cbr/deep_learning_approach_for_detecting_any_part_of/"}, {"autor": "PartlyShaderly", "date": "2019-07-21 13:48:34", "content": "[D] I have this wild idea for dating site that's only for attractive people, using eigenfaces (it has a million other uses!) /!/ We all know about eigenfaces, alright? How about we take a -----> picture !!!  of some people who we *know* everyone agrees on their attractiveness, make an eigenface for both men and women amongst them, then ask people to send in a driver's license photo (WHAT ARE THEY CALLED?) and when they do, match it with the eigenface. If they are attractive, the alpha or whatever will be positive. Hence only allowing attractive people to become a member of that website. It will also serve as a website to help people know if they are attractive. Of course there are intricacies, such as race and age, but we'll overcome them. This algorithm can be used by casting agents to \"poll\" if someone is beautiful and attractive. It has millions of uses. \n\n&amp;#x200B;\n\nI think the worst problem we face is fiducial point detection... I don't personally know how to automatically detect fiducial points, and if we let people do it themselves they might cheat.\n\n&amp;#x200B;\n\nSo what do you think guys? Doable?", "link": "https://www.reddit.com/r/MachineLearning/comments/cfz617/d_i_have_this_wild_idea_for_dating_site_thats/"}, {"autor": "MasterScrat", "date": "2019-07-21 09:11:20", "content": "[D] Good review papers about Image Segmentation? /!/ I have general knowledge about ML/DL, and good knowledge of Deep Reinforcement Learning. I now want to dive into -----> image !!!  segmentation. More specifically, I want to take a stab at this Kaggle challenge: [SIIM-ACR Pneumothorax Segmentation](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation)\n\nWhat are be the best recent resources? I am mainly looking for review papers and strong blog posts - ideally written resources, which are more efficient to consume than videos.\n\nI have started with this review: [Understanding Deep Learning Techniques for Image Segmentation](https://arxiv.org/abs/1907.06119), and will review the relevant part from the fast.ai course, since a lot of strong participants seem to rely on it on Kaggle.", "link": "https://www.reddit.com/r/MachineLearning/comments/cfx5ph/d_good_review_papers_about_image_segmentation/"}, {"autor": "thisappear", "date": "2019-07-20 15:53:38", "content": "[D] Does anyone know about a machine learning tool or any type of tool which can plot the boundaries of a map by the -----> image !!!  of the map or in pdf format of map? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/cfnawv/d_does_anyone_know_about_a_machine_learning_tool/"}, {"autor": "iamart_intelligence", "date": "2019-07-20 15:44:47", "content": "FaceApp &amp; Machine Learning /!/ FaceApp rather than using -----> image !!!  stacking or pixel manipulation uses Artificial Intelligence &amp; machine learning based neural networks to bring the changes in the facial features.\u00a0Comment your thoughts about Machine Learning and FaceApp.", "link": "https://www.reddit.com/r/MachineLearning/comments/cfn7gc/faceapp_machine_learning/"}, {"autor": "robotvison", "date": "2019-07-20 08:19:01", "content": "[D] Should conferences have a policy for papers that clearly have harmful applications? /!/ [This paper](https://arxiv.org/pdf/1803.08805.pdf) I saw today on \"inferring crowd density from a moving drone -----> camera !!!  especially when perspective effects are strong\" will appear in IROS, and it seems like it cannot possibly be used for good or even other related problems. I have also seen similar papers for mass surveillance with drones in the proceedings of a few top-tier CV conferences, and I'm sure there are many similar cases that you all have seen. \n\nShould these conferences be held accountable for publishing papers with inevitability harmful applications?", "link": "https://www.reddit.com/r/MachineLearning/comments/cfjhy0/d_should_conferences_have_a_policy_for_papers/"}, {"autor": "ligamentouscreep", "date": "2019-07-19 22:19:11", "content": "[D] Conditional GANs and class imbalance /!/ I have a small and highly imbalanced low-resolution -----> image !!!  dataset with 6 classes where 50% of the observations are from a single class. With unconditional GANs, I can get stably obtain samples with seemingly sufficient diversity under most setups (architecture, losses, etc). \n\n&amp;#x200B;\n\nIn the conditional case, there is significant class leakage in the samples generated. I've tried various standards on class conditioning such as, (1) conditional batch norm in the generator; (2) a projective layer in the critic and; (3) increasing the batch size significantly to cover more modes in each batch. (1) and (2) seem to be helping, and (3) doesn't appear to be helping with sample diversity and is making sample quality worse. Training is still in early stages though, so maybe things change (or modes collapse).\n\n&amp;#x200B;\n\nAre there any strategies or heuristics for conditional GANs specifically dealing with the class imbalanced case? At this point, I'm considering using balanced subsampled batches in each iteration or weighing the hinge loss by class distribution and hoping for the best.", "link": "https://www.reddit.com/r/MachineLearning/comments/cfe2sh/d_conditional_gans_and_class_imbalance/"}, {"autor": "ajboloor", "date": "2019-07-19 21:16:39", "content": "[R] [1903.05157] Simple Physical Adversarial Examples against End-to-End Autonomous Driving Models /!/ arxiv: [https://arxiv.org/abs/1903.05157](https://arxiv.org/abs/1903.05157)\n\nAbstract:\n\n&gt;Recent advances in machine learning, especially techniques such as deep neural networks, are promoting a range of high-stakes applications, including autonomous driving, which often relies on deep learning for perception. While deep learning for perception has been shown to be vulnerable to a host of subtle adversarial manipulations of images, end-to-end demonstrations of successful attacks, which manipulate the physical environment and result in physical consequences, are scarce. Moreover, attacks typically involve carefully constructed adversarial examples at the level of pixels. We demonstrate the first end-to-end attacks on autonomous driving in simulation, using simple physically realizable attacks: the painting of black lines on the road. These attacks target deep neural network models for end-to-end autonomous driving control. A systematic investigation shows that such attacks are surprisingly easy to engineer, and we describe scenarios (e.g., right turns) in which they are highly effective, and others that are less vulnerable (e.g., driving straight). Further, we use network deconvolution to demonstrate that the attacks succeed by inducing activation patterns similar to entirely different scenarios used in training.\n\nRecently published my first, first-authored paper in the ML domain. Thought this would be a good place to share and talk about it.\n\nEssentially, we answer the question: Can we paint a line on the road in a way that would confuse an autonomous vehicle? \n\n* so far we looked only at vision (-----> camera !!! ) based end-to-end Imitation Learning and Reinforcement Learning models\n* wanted to bring up discussion about adversarial ML against self-driving vehicles", "link": "https://www.reddit.com/r/MachineLearning/comments/cfdas4/r_190305157_simple_physical_adversarial_examples/"}, {"autor": "adeeplearner", "date": "2019-07-19 18:35:46", "content": "How to handle variant -----> image !!!  sizes with a fully convolutional network in keras? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/cfbbn9/how_to_handle_variant_image_sizes_with_a_fully/"}, {"autor": "sjoerdapp", "date": "2019-07-19 17:03:06", "content": "\ud83d\udd25\ud83d\udd25\ud83d\udd25Building SMILY, a Human-Centric, Similar------> Image !!!  Search Tool for Pathology", "link": "https://www.reddit.com/r/MachineLearning/comments/cfa47z/building_smily_a_humancentric_similarimage_search/"}, {"autor": "Atralb", "date": "2019-07-24 09:35:30", "content": "[D] Help on how to extract specific text from -----> image !!!  with printed and handwritten text /!/ Hi there,\n\n&amp;#x200B;\n\nI have a task where I need to extract specific elements (name, surname, company name) from text present in images which are forms and other administrative documents (example [here](https://farfadoc.files.wordpress.com/2013/11/photo.jpg)), but there are many types of documents and each are structured differently.\n\n**Sometimes the required elements will be handwritten, sometimes not**. **They also will be placed differently on the page depending on the document.**\n\nAnd the **data is not annotated**. (do I need to annotate it myself before, or can it be made unsupervised ?)\n\n&amp;#x200B;\n\nSo I'm at a real loss here on what to do. ^((I'm a beginner in the field so please bear with me :).))\n\nI'm really not an expert on NLP nor text extraction or detection. Mostly worked on images with classical datasets.\n\n&amp;#x200B;\n\nClassic OCR techniques I tested like Tesseract don't work at all for handwritten text (maybe there's appropriate configuration for this ?).\n\nSince the data is so unstructured (to me) I thought of using neural networks and especially a Faster RCNN model which I would train to detect only the three elements I mentioned before (after having annotated the data myself) as if they were specific objects like for classical Faster RCNN models. [Paper](https://ieeexplore.ieee.org/abstract/document/8270290) ^((use hub of science)) on researchers successfully doing it.\n\nBut someone I talked to told me it was easy to do even with the constraints I told you, and that neural networks weren't needed, nor was annotating.\n\nMy guess at the beginning was that I needed to find a way to extract the whole text from the image (with the handwritten text, and well positioned in relation to the printed text, and then find a way to give my algorithm insight on how to distinguish names from the rest.\n\nHow could I do this with simple machine learning ?Is there a dataset of names and surnames that I can train my model on so that it would ?\n\n&amp;#x200B;\n\nThanks a lot in advance for your help !", "link": "https://www.reddit.com/r/MachineLearning/comments/ch5zv2/d_help_on_how_to_extract_specific_text_from_image/"}, {"autor": "toadsofbattle", "date": "2019-07-24 02:36:56", "content": "[D] Is there a SOTA model or standardized dataset with a focus on detection/classification of object *attributes*, not just object type? /!/ Hi, pretty new to computer vision here. I've seen a lot of standardized datasets for CV such as MNIST, CIFAR-10, ImageNet, etc. ImageNet in particular stood out to me because of how specific a lot of the -----> image !!!  classifications were, such as \"pug\" or \"dalmation\". However, is there a model out there which focuses on classifying attributes of the object e.g. dog color, or a standardized dataset that would enable that analysis?\n\nPointing me towards the relevant literature would be really appreciated! I've done my Googling but I think I might not know the specific terms used in this domain to describe stuff like this, because I keep running into generic object classification and detection without the object attributes.", "link": "https://www.reddit.com/r/MachineLearning/comments/ch29pm/d_is_there_a_sota_model_or_standardized_dataset/"}, {"autor": "toadsofbattle", "date": "2019-07-24 02:34:03", "content": "[D] What is a SOTA model or standardized dataset for classification/detection of -----> image !!!  *attributes*? /!/ Hi, pretty new to computer vision here. I've seen a lot of standardized datasets for CV such as MNIST, CIFAR-10, ImageNet, etc. ImageNet in particular stood out to me because of how specific a lot of the image classifications were, such as \"pug\" or \"dalmation\". However, is there a model out there which focuses on classifying attributes of the image - e.g. \"brown dog\", or \"tall person\", or \"spotted dog\", or something like that, or a standardized dataset that would enable that analysis?", "link": "https://www.reddit.com/r/MachineLearning/comments/ch28nl/d_what_is_a_sota_model_or_standardized_dataset/"}, {"autor": "RelativeAnalyst9", "date": "2019-07-23 15:55:05", "content": "[N] Natural Adversarial Examples Slash -----> Image !!!  Classification Accuracy by 90% /!/ Researchers from UC Berkeley and the Universities of Washington and Chicago have released a set of natural adversarial examples, which they call \u201cImageNet-A.\u201d The images are described as real-world, naturally occurring examples that have the potential to highly degrade the performance of an image classifier. For example DenseNet-121 obtains only around two percent accuracy on the new ImageNet-A test set, a drop of approximately 90 percent.\n\nThe ImageNet challenge competition was closed in 2017, as it was generally agreed in the machine learning community that the task of image classification was mostly solved and that further improvements were not a priority. It should be noted however that the ImageNet test examples are mostly relatively uncluttered close-up images which do not represent the more challenging object contexts and representations found in real world.\n\nWhat\u2019s more, it has been shown that adversarial examples that succeed in fooling one classification model can also fool other models that use different architecture or were trained on different datasets. Adversarial attacks therefore have the potential to cause serious and widespread security vulnerabilities across popular AI applications such as facial recognition, self-driving cars, etc.\n\n&amp;#x200B;\n\nMore in article:\n\n[https://medium.com/syncedreview/natural-adversarial-examples-slash-image-classification-accuracy-by-90-702f381acbb8](https://medium.com/syncedreview/natural-adversarial-examples-slash-image-classification-accuracy-by-90-702f381acbb8)", "link": "https://www.reddit.com/r/MachineLearning/comments/cgue2d/n_natural_adversarial_examples_slash_image/"}, {"autor": "msamoylov", "date": "2019-07-23 10:28:18", "content": "[D] Why Computer Vision still sucks? /!/ I have a pet project built with the Computer Vision service from Microsoft. Sometimes it provides very accurate annotations and descriptions like ['A view of a snow covered mountain'](https://taken.photos/photos/3738716/) (confidence 0.97) for an -----> image !!!  of a mountain but mostly it's utter garbage like ['A motorcycle is parked on the side of a road'](https://taken.photos/photos/3738489/) (confidence 0.8) for a Formula 1 car.\n\n&amp;#x200B;\n\nThe Vision AI service from Google is doing even worse.\n\n&amp;#x200B;\n\nI'm not seeing any significant improvements in this field at all. You can get a very realistic image of older you, but no one is able to annotate even a simple photo yet.\n\n&amp;#x200B;\n\nDo you think we will have truly working Computer Vision within next few years?", "link": "https://www.reddit.com/r/MachineLearning/comments/cgqv9f/d_why_computer_vision_still_sucks/"}, {"autor": "MrBojanglesReturns", "date": "2019-07-23 04:21:50", "content": "[D] What's with Linguistic Data Consortium paywall for.. everything? /!/ What's up with LDC pay walling every corpus up to 8,000$? Look at the surreal price of how a year subscription is worth. It is out of anybody's league. Are there any open alternatives to speech recognition?\n\n&amp;#x200B;\n\nWe have a lot of open-sourced datasets for other domains, computer vision, -----> image !!!  processing, medical scans. Only audio datasets are being kept behind a paywall. According to another Reddit post, they had plenty of public funding, and yet nothing changed.\n\n&amp;#x200B;\n\nThey have old stuff as of 1992 and nothing is released for free. Is it common for US unis to have access to this kind of material? They sound more like a corporation than a consortium. Not having an updated and big corpora hinders the whole field from discoveries, because each time there is a research in the domain, there will be time wasted building another dataset.", "link": "https://www.reddit.com/r/MachineLearning/comments/cgnub9/d_whats_with_linguistic_data_consortium_paywall/"}, {"autor": "labelimagedata", "date": "2019-07-22 18:02:31", "content": "[Project] Turn-key -----> image !!!  annotation service to train ML model /!/ [https://labelimagedata.com](https://labelimagedata.com)\n\n&amp;#x200B;\n\nhttps://i.redd.it/ixkn61jp9wb31.png", "link": "https://www.reddit.com/r/MachineLearning/comments/cggn0y/project_turnkey_image_annotation_service_to_train/"}, {"autor": "Yuqing7", "date": "2019-07-22 17:37:10", "content": "Natural Adversarial Examples Slash -----> Image !!!  Classification Accuracy by 90%", "link": "https://www.reddit.com/r/MachineLearning/comments/cggb4l/natural_adversarial_examples_slash_image/"}, {"autor": "taki0112", "date": "2019-07-31 06:06:25", "content": "[R] [1907.10830] U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for -----> Image !!! -to------> Image !!!  Translation /!/ &amp;#x200B;\n\n![img](4r40q22dxkd31 \"1st row : input, 2nd row : attention map, 3rd row : output\")\n\n**Abstract**\n\n&gt;We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. **The attention module** guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based methods which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new **AdaLIN (Adaptive Layer-Instance Normalization)** function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a **fixed network architecture and hyper-parameters.**\n\n&amp;#x200B;\n\n* **paper :** [https://arxiv.org/abs/1907.10830](https://arxiv.org/abs/1907.10830)\n* **Official Tensorflow :** [https://github.com/taki0112/UGATIT](https://github.com/taki0112/UGATIT)\n* **Official Pytorch :** [https://github.com/znxlwm/UGATIT-pytorch](https://github.com/znxlwm/UGATIT-pytorch)", "link": "https://www.reddit.com/r/MachineLearning/comments/ck4do7/r_190710830_ugatit_unsupervised_generative/"}, {"autor": "SoFarFromHome", "date": "2019-07-30 15:49:11", "content": "[D] What I'd like to write in my NeurIPS rebuttal /!/ We thank the reviewers for the detailed comments, of which some were even based on our paper.\n\nTo the reviewer that said our paper was \"underdeveloped\" because we didn't use a different methodology Y from field Z, we'd like to point out that a) this is in field A, b) we provided a framework for how to extend this to other methodologies in field A, and c) methodology Y has no obvious way to extend to the problem we're addressing (and doing so would be a whole paper in its own right).  Do you often read papers and get frustrated that they aren't the papers you've written?\n\nTo the same reviewer, who asked why we didn't cite papers Z1 and Z2, we would again point out that this isn't field Z and those papers have no relevance to the topic at hand except that you'd have written a paper on a different topic, which we didn't.\n\nTo the reviewer that asked why we didn't cite X, we'd like to point out that we did cite X, and had a whole paragraph discussing the relationship of this work to that one.\n\nTo the reviewer that proposed an example dataset to evaluate our model on, we point out that we already evaluate the model on that data set; see our Experiments section.\n\nTo the reviewer that pointed out that our method won't work when assumption 3 isn't met, yes, you're correct.  That's why we stated it as an assumption.  Congratulations on your reading comprehension.\n\nTo the reviewer that directly copy/pasted our introduction into the \"what 3 things does this paper contribute\" box, we'll be sure to include in future revisions a copy/paste-able review justifying \"score 10, confidence 5\" to make your review easier.  That you also mischaracterized our main claim and completely missed the discussion on relationship to prior work makes your review particularly useful to development of the work.\n\nTo the reviewer that wrote that, while THEY were familiar with the definitions in a reference, we should explain it for readers that might be confused, we understand entirely.  We'll gladly explain it for \"a friend of yours\", err \"readers\", and not you, because you get it and you're smart and it's just the readers who don't.\n\nTo the reviewer who commented that our results were \"contradictory\" because we said that our modification \"in general performed slightly worse\" on this metric, when in fact our plots show it sometimes performed better, we'll gladly fix our claim to be clear that \"in general\" doesn't mean \"always\" and also our results are even better than the previous wording indicated.\n\nTo the reviewer that said our comparison method's results were worse than reported in the original paper, we've carefully compared their bar charts to ours and found that the results are the same to the precision of the graphical printout in the previous paper.  If you could lend us your -----> image !!!  sharpening function so we can get more significant digits out of their plot, we'd be glad to redo the comparison.\n\nTo the reviewer who used half of their review to argue that our entire subfield is dumb and wrong, we thank them for reaching across academic lines to provide commentary in an area that pains you deeply.\n\nAnd finally, to the reviewers who called our paper (all actual quotes) \"original, well-motivated, and worthy of study\", \"important in its own right\", that said you \"greatly enjoyed reading this paper\" and that \"this is an interesting problem and certainly worth studying\" and that \"this paper identifies an important problem ... [and the authors] then present a simple\" solution, thank you for also marking this a reject.  Since all of you gave us scores between 5 and 3, neither the AC nor any of you will ever have to read this response or reconsider your scores before we are inevitably rejected, but we hope that your original, well-motivated, worth-studying, important, interesting, clear papers receive reviews of equal quality in the future!\n\n/salt", "link": "https://www.reddit.com/r/MachineLearning/comments/cjtmi8/d_what_id_like_to_write_in_my_neurips_rebuttal/"}, {"autor": "boltzBrain", "date": "2019-07-30 05:28:13", "content": "Data augmentation use outside of the -----> image !!!  domain? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/cjn826/data_augmentation_use_outside_of_the_image_domain/"}, {"autor": "MasterScrat", "date": "2019-05-20 21:39:24", "content": "[D] GAN to generate images of planets /!/ Hi all, \n\nI have seen some very nice demo where you can draw the outline of a scenery, cat or handbag and it creates a (more or less) realistic -----> picture !!!  following your shapes.\n\nHow much work would it be to generate pictures of planets? you select which parts are water, forests, mountains, craters and it generates something like this: https://images.app.goo.gl/76jbnxVDg8YFE64g6\n\nHow much technical and computational work would this involve? I know very little about GANs, but I\u2019m familiar with python (I study reinforcement learning) and have access to beefy GPUs. \n\nActually ideally I\u2019d want to generate 3D models of the planets, but as a first approximation a 2D image would be enough.", "link": "https://www.reddit.com/r/MachineLearning/comments/br1qyl/d_gan_to_generate_images_of_planets/"}, {"autor": "jthat92", "date": "2019-05-20 07:29:03", "content": "Count intersections on -----> image !!!  with neural network?", "link": "https://www.reddit.com/r/MachineLearning/comments/bqsfgy/count_intersections_on_image_with_neural_network/"}, {"autor": "ai-lover", "date": "2019-05-19 06:00:02", "content": "Unpaired -----> Image !!! -to------> Image !!!  Translation using Cycle-Consistent Adversarial Networks [Worth Reading]", "link": "https://www.reddit.com/r/MachineLearning/comments/bqdgwe/unpaired_imagetoimage_translation_using/"}, {"autor": "MyMomSaysImHot", "date": "2019-05-18 20:31:31", "content": "[P] New in DeOldify: Smooth Colorization of Video! Courtesy of one weird trick- NoGAN /!/ Hello again! I just realized after a few weeks of sitting on this that I didn't tell you guys about something really cool I've been working on! Maybe I should do that. \n\nA few months ago I posted about [DeOldify-](https://github.com/jantic/DeOldify) my pet project for colorizing and restoring old photos. Well now that includes videos as well! Here's a demo I showed at Facebook's F8 conference:\n\n&amp;#x200B;\n\n![video](9tu80qji41z21)\n\nAnd here's the talk at F8: [https://www.facebook.com/FacebookforDevelopers/videos/340167420019712/](https://www.facebook.com/FacebookforDevelopers/videos/340167420019712/) \n\nAnd here's the article I wrote with Jeremy Howard of FastAI and Uri Manor of Salk Institute: [https://www.fast.ai/2019/05/03/decrappify/](https://www.fast.ai/2019/05/03/decrappify/) \n\nAnyway, the gist is that we (FastAI and I) developed this one weird trick called NoGAN to achieve this. That basically consists of this (slide from F8): \n\n&amp;#x200B;\n\n![img](midd5z9v41z21)\n\n \n\nThe pretraining is using basic perceptual loss (or \"feature loss\") for the generator.  This gets you the benefits of GANs, without the problems, basically. Hence, smooth video!\n\nThe progression of training looks like this (sweet spot is at 1.4%, then it goes too far from there and gets weird with the orange skin):\n\n&amp;#x200B;\n\n![video](34o1egby41z21)\n\nAnyway, that's the gist. You can read more in the links above (readme for github project also has good details on NoGAN). Oh by the way, NoGAN also works on super resolution, and I suspect for most -----> image !!!  to -----> image !!!  tasks as well as perhaps even non------> image !!!  tasks. \n\n&amp;#x200B;\n\n![img](z0tlr0zz41z21)", "link": "https://www.reddit.com/r/MachineLearning/comments/bq8gji/p_new_in_deoldify_smooth_colorization_of_video/"}, {"autor": "ApathiaDeus", "date": "2019-05-18 09:03:44", "content": "[D] Learning from simulated data ? /!/  Hi,\n\nSo I'm working on this virtual sensing DL project (basically, I try to estimate (regression) the outcome of an expensive sensor from other cheaper sensors), and I have a limited amount of real-life labelled time series data (Acquiring the labelled data is expensive because of the sensor in itself).\n\nThe physics behind what I'm trying to estimate can be pretty accurately modeled with complex equations that doesn't have any close form solutions. Those physical models also require me to feed as input some parameters that I wish it didn't need because I would ultimately want something that is able to generalized in all type of situations.\n\nSo far, I have some results using DL (MLP and RNN variants), but nothing crazy. I think I am limited by the fact that the data I'm provided is extremely small.\n\nNow come into play a simulation some of my colleagues have been working on. According to them, the simulation can reproduce with a high degree of accuracy the behavior of the physical model. The simulation take into account some of the parameters I mentioned above, which is a good news since it can generates diverse data with respect to those parameters and maybe help the NN to better generalize.\n\nUnfortunately so far, I have extremely poor results when training on the generated data and testing on real life data. I believe the simulated data is too \"perfect\" to the point where the network can easily fit the smooth training data, but this does not help the noisy real life data. I don't know if this would be consider over-fitting (My validation loss on real life data, does decrease first before it goes up, which makes me believed the network is learning some stuff), but even so, I tried regularization, up to simplifying the network to one hidden layer and a dozens of neurons, and some dropout; it didn't improve anything.\n\nNow appart from adding noise to the simulated data (which I will try next), do any of you guys know how I can better exploit the simulated data ? I still have some labelled data from real life and and also a ton of unlabeled data (data from the cheap sensors). I'm wondering if there is an equivalent of GAN for non images data (time series of physical quantities for instance). From what I understand, they can \"turn\" simulated data into something closer to reality, but doesn't seem to apply outside of -----> image !!!  data.\n\nAny ideas ? I'm even open to the semi supervised side, since I have a lot of unlabeled data.", "link": "https://www.reddit.com/r/MachineLearning/comments/bq1u91/d_learning_from_simulated_data/"}, {"autor": "Pranoykrishnan", "date": "2019-03-23 10:36:31", "content": "-----> Image !!!  Classification result when two objects from different classes are given as input. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/b4hr5c/image_classification_result_when_two_objects_from/"}, {"autor": "vrvc014pssi", "date": "2019-03-23 03:10:54", "content": "Human leg area recognition from -----> image !!! ? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/b4e7yo/human_leg_area_recognition_from_image/"}, {"autor": "lzhbrian", "date": "2019-03-22 15:20:25", "content": "[P] Curated List of Arbitrary Text to Image Papers /!/ [lzhbrian/arbitrary-text-to------> image !!! -papers](https://github.com/lzhbrian/arbitrary-text-to------> image !!! -papers)\n\nStill preliminary. PR, issues, discussions are welcomed!", "link": "https://www.reddit.com/r/MachineLearning/comments/b46b7u/p_curated_list_of_arbitrary_text_to_image_papers/"}, {"autor": "jarekduda", "date": "2019-03-22 14:38:32", "content": "[D] ML-based -----> image !!! /video compression: \"AE\" vs \"AE +regularizer\" vs \"VAE + bits back\" ... ? /!/ The space of let say images in everyday applications is much smaller than of all bitmaps \u2013 restricting to such subspace allows to define one of them with a much smaller number of bits, providing better compression. \n\nWhile being more costly than classical e.g. Fourier-based compressors like JPG, machine learning allows for much more accurate modelling of the space of real images, and there are papers claiming revolutionary improvement in compression ratios  this way, especially from **WaveOne** - for **image compression** ( https://arxiv.org/pdf/1705.05823 ) and recently also for **video** ( http://www.wave.one/video-compression/ ).\n\nWhile they quantize and encode latent variable of standard AutoEncoder, we can add **regularizer** ([WAE](https://arxiv.org/pdf/1711.01558)/[SWAE](https://arxiv.org/pdf/1804.01947)/[CWAE](https://arxiv.org/pdf/1805.09235)/[GAE](https://arxiv.org/pdf/1811.04751)...) to minimized distortion loss to enforce e.g. Gaussian distribution in latent space - which should be better for further encoding.\n\nIt turns out we can also use indeterministic **VAE** thanks to **bits-back** approach ( https://arxiv.org/pdf/1901.04866  https://youtu.be/grsO57XMJMk?t=2396 ).\n\nAre there other interesting approaches for ML-based image compression?\n\nWhich seems the most promising?\n\nWhen they can realistically replace JPG ... video compression?", "link": "https://www.reddit.com/r/MachineLearning/comments/b45tuh/d_mlbased_imagevideo_compression_ae_vs_ae/"}, {"autor": "Sarthaks21", "date": "2019-03-22 14:37:27", "content": "Recreate GauGAN /!/ NVidia recently released their paper titled  \"Semantic -----> Image !!!  Synthesis with Spatially-Adaptive Normalization\" ([https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291)). The GAN architecture turns a random doodle into a piece of art, as captured by a camera in a real world setting. \n\nHere is a video showing it's working:\n\n [https://www.inverse.com/article/54198-nvidia-gans-app-turns-doodles-into-art](https://www.inverse.com/article/54198-nvidia-gans-app-turns-doodles-into-art)\n\nI have never created a GAN architecture before. I only know how GANs work.  I would like to recreate the whole network from scratch. \n\nPlease suggest learning resources or papers I should consider recreating first before tackling this one. If interested in partnering up, then message me. \n\nThank you for replying. ", "link": "https://www.reddit.com/r/MachineLearning/comments/b45tgd/recreate_gaugan/"}, {"autor": "scriptcoder43", "date": "2019-03-21 23:50:17", "content": "[D] Does the shape of a tensor for an -----> image !!!  affect the resulting output? /!/ I am representing images of size 100px by 100px, so I can have the shape (None, 100, 100, 3) or shape (None, 10000, 3)\n\nI can't find any clear explanation on Google, however, will the following two tensors result in similar results?\n\n1. (None, 100, 100, 3)\n2. (None, 10000, 3)\n\nI assume either is sufficient as I would have thought the neural network will still learn just as well if the image is in a single row, your thoughts?", "link": "https://www.reddit.com/r/MachineLearning/comments/b3xvow/d_does_the_shape_of_a_tensor_for_an_image_affect/"}, {"autor": "datitran", "date": "2019-03-26 19:33:41", "content": "[P] Image Super Resolution: Super-scale your images and run experiments very easily with RDNs and GANs. /!/ We just released a major update of our [Keras based -----> image !!!  super resolution project](https://github.com/idealo/-----> image !!! -super-resolution/) \ud83d\udd0e. Now you can super-scale your images and run experiments very easily with RDNs and GANs.\n\n&amp;#x200B;\n\nIn particular,\n\n\u2b50\ufe0f We've added some new cool stuff like VGG deep features + GANs to achieve amazingly realistic upscaled images\n\n\u2b50\ufe0f We provide a few more pre-trained weights as well as some Colab-notebook tutorials to play around\n\n\u2b50\ufe0f And the best thing our project is now available on PyPI! So you can just pip install your way to our project\n\n&amp;#x200B;\n\n\ud83d\udcbbBlog: [https://medium.com/idealo-tech-blog/zoom-in-enhance-a-deep-learning-based-magnifying-glass-part-2-c021f98ebede](https://medium.com/idealo-tech-blog/zoom-in-enhance-a-deep-learning-based-magnifying-glass-part-2-c021f98ebede)\n\n\ud83d\udd24Code: [https://github.com/idealo/image-super-resolution/](https://github.com/idealo/image-super-resolution/)\n\n\ud83d\udcddDocumentation: [https://idealo.github.io/image-super-resolution/](https://idealo.github.io/image-super-resolution/)\n\n\ud83d\udcd5Colab (prediction): [https://colab.research.google.com/github/idealo/image-super-resolution/blob/master/notebooks/ISR\\_Prediction\\_Tutorial.ipynb](https://colab.research.google.com/github/idealo/image-super-resolution/blob/master/notebooks/ISR_Prediction_Tutorial.ipynb)\n\n\ud83d\udcd8Colab (training): [https://colab.research.google.com/github/idealo/image-super-resolution/blob/master/notebooks/ISR\\_Traininig\\_Tutorial.ipynb](https://colab.research.google.com/github/idealo/image-super-resolution/blob/master/notebooks/ISR_Traininig_Tutorial.ipynb)", "link": "https://www.reddit.com/r/MachineLearning/comments/b5u4vo/p_image_super_resolution_superscale_your_images/"}, {"autor": "engineheat", "date": "2019-03-26 03:03:48", "content": "Is Naive Bayes a decent model for simple -----> image !!!  classification? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/b5khdo/is_naive_bayes_a_decent_model_for_simple_image/"}, {"autor": "clanleader", "date": "2019-03-19 11:02:57", "content": "[D] Why can't we combine different types of neural networks for inputs from different domains but same output domain? /!/ I was wondering whilst reading over LSTMs which take the inputs from previous predictions, and ResNets which consider the activations from previous layers in the same prediction, why we couldn't apply a similar concept to connect two completely different network architectures that are attempting to predict the same thing using entirely different domains of input?\n\nFor example, a CNN for detecting and classifying images of animals, and a completely different architecture for detecting and classifying soundwaves of animals.\n\nWhat if we had a third, fully connected neural network that would take the flattened activations from the hidden layers of both the -----> image !!!  network and sound network in order to ensure their dimensionality matched, and used these as input features for this third network? The output of this third network should in theory use features from both of the other networks in determining its classification.\n\nOf course in terms of a training \"input\", both the image network and sound network would need to have the same classification labels which would in turn match this third network.\n\nSurely in this toy example, using both image and sound taken together might further increase classification accuracy? Or more importantly, perhaps the idea could be applied to completely different areas where inputs from two entirely different domains would nevertheless benefit from using information from one another, similar to how a human can take advantage of various senses combined together instead of a single sense.\n\nSo my question is, has this been tried, what were the results? Is it a promising idea?", "link": "https://www.reddit.com/r/MachineLearning/comments/b2w192/d_why_cant_we_combine_different_types_of_neural/"}, {"autor": "Fieuws", "date": "2019-03-18 14:29:38", "content": "Real time object detection without deep learning /!/ Hi all, I am a student applied engineering and for my bachelors degree I have to count people using video -----> camera !!! . For that I had to compare machine learning models and test how good they could find people in frames. But now I don't know how to use this model and detect people walking through a door so that a program can track the person.\nCan anybody give me some directions where to go? Cause online I only find sollutions with Keras and Tensorflow and trained Neural Net models", "link": "https://www.reddit.com/r/MachineLearning/comments/b2jdob/real_time_object_detection_without_deep_learning/"}, {"autor": "_Howlin", "date": "2019-03-18 09:22:47", "content": "[D] (Help requested) Neural Network to classify images between \"town\" and countryside-village /!/ Hi everyone,\n\n&amp;#x200B;\n\nI am trying to build a neural network aiming to detect whether a 360\u00b0 -----> image !!!  taken from a road (for instance, a Google Street View) can be associated to a location in a town or not.\n\nI have tried to build my own little CNN, thinking this was not such a difficult task. The network could not make any good fitting and was always stuck. I also tried using some of the traditional \"lightweight\" architectures like VGG and MobileNet. These had results that were exactly the same before and after training. There is no special learning, and finally the neural network is not giving any good result.\n\n&amp;#x200B;\n\nAm I doing something wrong ? Are neural networks really the best idea to solve this problem or some classical image processing would do the job ?\n\n&amp;#x200B;\n\nThank you for your potential help and recommendations  !", "link": "https://www.reddit.com/r/MachineLearning/comments/b2gig2/d_help_requested_neural_network_to_classify/"}, {"autor": "Yuqing7", "date": "2019-01-04 15:20:44", "content": "InstaGAN Excels in Instance-Aware -----> Image !!! -To------> Image !!!  Translation", "link": "https://www.reddit.com/r/MachineLearning/comments/acj42v/instagan_excels_in_instanceaware_imagetoimage/"}, {"autor": "sepehr_fard", "date": "2019-01-03 22:58:42", "content": "CNN for fish-eye classifier /!/ Did some research to find if anyone has published anything in regards training to detect fish eye images vs non-fisheye images. \n\n&amp;#x200B;\n\nThings in mind:\n\n\\- Some type of data augmentation where the edge of the images are exaggerated. Since only difference between two classes are the subtle roundness of the -----> image !!!  on the edges. ", "link": "https://www.reddit.com/r/MachineLearning/comments/acbdzz/cnn_for_fisheye_classifier/"}, {"autor": "Natsu6767", "date": "2019-01-03 19:20:15", "content": "[P] Generating Devanagari Characters Using DRAW /!/ I really love trying out and playing with generative models. Although it's really difficult to intuitively make out how a model is actually generating images (sometimes even good enough to fool humans), I find that [DRAW: A Recurrent Neural Network For Image Generation](https://arxiv.org/abs/1502.04623)**,** is the most \"humane\" out of them all in the way it generates images.\n\nThis is a PyTorch implementation of DRAW trained to generate [Devanagari](https://en.wikipedia.org/wiki/Devanagari) Characters:\n\n[https://github.com/Natsu6767/Generating-Devanagari-Using-DRAW](https://github.com/Natsu6767/Generating-Devanagari-Using-DRAW)\n\nThe basic idea behind DRAW is to only generate a small portion of an -----> image !!!  at a time *(say, a 5 x 5 region when the whole -----> image !!!  is 32 x 32)* and recursively refine this. It's sort of like how we humans draw a picture: we first draw rough outlines and slowly add in the details over time.\n\nOne problem with DRAW is that, as it's underlying base architecture is the Variational Autoencoder which uses the reconstruction loss, the outputs tend to be blurry. This is especially noticeable in higher complexity datasets such as CIFAR-10. \n\n&amp;#x200B;\n\n*P.S. I find it weirdly satisfying watching the animation of the images being generated through time, glimpse by glimpse.*", "link": "https://www.reddit.com/r/MachineLearning/comments/ac9335/p_generating_devanagari_characters_using_draw/"}, {"autor": "gwen0927", "date": "2019-01-03 16:50:36", "content": "[P] InstaGAN Excels in Instance-Aware -----> Image !!! -To------> Image !!!  Translation", "link": "https://www.reddit.com/r/MachineLearning/comments/ac7ii1/p_instagan_excels_in_instanceaware_imagetoimage/"}, {"autor": "InfoPaste", "date": "2019-01-03 05:54:16", "content": "[P] Web-based annotation tool for -----> image !!!  segmentation and localization /!/ I have been working on an [image annotator tool](https://github.com/jsbroks/coco-annotator/) to create custom datasets for Mask-RCNN and other image detection models.\n\n&amp;#x200B;\n\nEnjoy!", "link": "https://www.reddit.com/r/MachineLearning/comments/ac2n43/p_webbased_annotation_tool_for_image_segmentation/"}, {"autor": "reamsofdata", "date": "2019-01-02 22:54:45", "content": "[D] Image Recognition with Keras: Convolutional Neural Networks /!/ This is an example of an -----> image !!!  classifier that I recently developed with Keras.\n\n&amp;#x200B;\n\n[http://www.michaeljgrogan.com/image-recognition-with-keras-convolutional-neural-networks/](http://www.michaeljgrogan.com/image-recognition-with-keras-convolutional-neural-networks/)\n\n&amp;#x200B;\n\nEssentially, the model is trained to distinguish between images of cars and planes with 200 images in total (100 for each category).\n\n&amp;#x200B;\n\nI found that the accuracy was maximised (and loss was minimised between 2 to 3 epochs) - accuracy was nearly 100% on the training set and 92.5% on the test set.\n\n&amp;#x200B;\n\nOne of my goals is to expand on the model to classify more complex images, as this would have great application in the area of transport, autonomous vehicles, etc.\n\n&amp;#x200B;\n\nHope you find the write-up useful and I highly welcome any feedback.", "link": "https://www.reddit.com/r/MachineLearning/comments/abyu1h/d_image_recognition_with_keras_convolutional/"}, {"autor": "cmillionaire9", "date": "2019-07-03 20:40:29", "content": "[D] LidarView and Deeplearning /!/ [https://youtu.be/7pQGYHgvvrY](https://youtu.be/7pQGYHgvvrY)\n\n Visualize your multi-sensor data with [\\#LidarView](https://www.youtube.com/results?search_query=%23LidarView), the open source [\\#ParaView](https://www.youtube.com/results?search_query=%23ParaView) Lidar application. And thanks to our in-house Lidar SLAM, map them in 3D, be it indoor or outdoor.Here is a sequence in Lyon, nearby our Kitware Europe office\ufffc: Only one VLP-16 from Velodyne Lidar, Inc. and a Gopro -----> camera !!! , no IMU or GPS. Automatic computation of the position/orientation of the camera relatively to the Lidar, as well as the time-sync. Showing our preferred semantic segmentation, from [\\#bonnetal](https://www.youtube.com/results?search_query=%23bonnetal). Stay tuned as the 3D semantic segmentation should be available soon too.", "link": "https://www.reddit.com/r/MachineLearning/comments/c8tc21/d_lidarview_and_deeplearning/"}, {"autor": "arnokha", "date": "2019-07-03 16:06:48", "content": "[D] MNIST Transfer learning: Training on 28x28, then inference on 50x50 /!/ Given a CNN trained on MNIST, can I easily build a similar network that recognized those digits anywhere in a 50x50 -----> image !!! ?\n\n&amp;#x200B;\n\nPhrased another way, if I had a CNN that recognized 28x28 MNIST digits, is there a known method to classify those MNIST digits if they appeared anywhere within a 50x50 -----> image !!!  (the digits themselves would still be 28x28).\n\n  \u2014 Assume the last layer is fully connected + softmax. The convolutional layers should remain unchanged, so this question breaks down to easily creating a dense layer that works for 50x50 from the one that worked with 28x28.\n\n&amp;#x200B;\n\nWhy I think this is important: We can achieve more specific labeling with smaller image sizes, cropping out things like eyes and noses and being able to recognize them in new places in an image. Also, this would potentially reduce the amount and variety of training data needed (we wouldn't have to create new 50x50 images with the 28x28 images tiled in different locations).", "link": "https://www.reddit.com/r/MachineLearning/comments/c8pwkn/d_mnist_transfer_learning_training_on_28x28_then/"}, {"autor": "po3na4skld", "date": "2019-07-03 07:16:51", "content": "[R] Looking for help in my university research! /!/ Hello Reddit, I'm a student that getting masters degree in university. I do medical research about hand skin diseases (dermatitis, dermatosis, psoriasis). I'm using neural networks for that and unfortunately I need a lot of examples of hands, so for that I ask Reddit to help me in my research! If you wanted to help me or you have any questions about research, we can talk in PM. I added example of hand's -----> photo !!!  to this post. Please send your photos as file that they will not lose their quality. Thanks for attention.\n\n*Processing img 4vedqrggg1831...*", "link": "https://www.reddit.com/r/MachineLearning/comments/c8l1vm/r_looking_for_help_in_my_university_research/"}, {"autor": "dragomen747180", "date": "2019-07-03 06:03:05", "content": "[discussion] i have an idea read below, not sure if its possible or how to approach doing this. /!/  \n\nforewarning I am out of my element with this idea\n\nI'm  building a lichtenberg machine DIY from a microwave power transformer, i  was wondering if its possible to hook up like an Arduino and a web cam  or some type of -----> camera !!!  and use machine learning to teach the ai lighting  patterns and eventually get it to create its own patterns based off  what i feed it Litchenberg wise. now please don't flame me im not an  expert on anything AI so maybe Arduino isn't the way to go. preferably  would like to do this in Python. if there's any resource or constructive  input id be willing to read.", "link": "https://www.reddit.com/r/MachineLearning/comments/c8kgoi/discussion_i_have_an_idea_read_below_not_sure_if/"}, {"autor": "Lewba", "date": "2019-07-03 05:30:57", "content": "[D] Teacher-Student training situation with CNN-FC /!/ I've been asked to convert a fully-trained CNN to a simple FC network with fixed architecture (it'll be used on a small chip if I remember correctly). They understand the classification performance will drop but it needs to be done anyway. I've set up the student network such that it just takes the flattened -----> image !!!  as the input but I'm unsure what my targets are. I have the data the teacher network was trained on so I guess I can train the student using those inputs with the correspoding teacher output (rather than one-hot targets in the dataset). But my real question is can I just generate random input images and use whatever the teacher outputs as a target for the student to train on? Is that what is usually done to generate a lot of training data for the student network?", "link": "https://www.reddit.com/r/MachineLearning/comments/c8k71h/d_teacherstudent_training_situation_with_cnnfc/"}, {"autor": "dragomen747180", "date": "2019-07-03 04:44:56", "content": "[discussion] Had an idea want to know if its possible to do. /!/ &amp;#x200B;\n\nforewarning I am out of my element with this idea \n\nI'm building a lichtenberg machine DIY from a microwave power transformer, i was wondering if its possible to hook up like an Arduino and a web cam or some type of -----> camera !!!  and use machine learning to teach the ai lighting patterns and eventually get it to create its own patterns based off what i feed it Litchenberg wise. now please don't flame me im not an expert on anything AI so maybe Arduino isn't the way to go. preferably would like to do this in Python. if there's any resource or constructive input id be willing to read.", "link": "https://www.reddit.com/r/MachineLearning/comments/c8jswa/discussion_had_an_idea_want_to_know_if_its/"}, {"autor": "AndriPi", "date": "2019-07-02 12:56:02", "content": "[D] Defect detection in an object looking at multiple pictures of the same object from different angles /!/ Hi, I would like to detect defects in some objects. I have multiple pictures of the same object taken from different angles. Some defects are easier to detect, if you look at two (or three) pictures of the same object from different angles, rather than just at one. However, the object detection models I know of (such as RetinaNet, or Faster R-CNN) only look at one -----> image !!!  at a time.\n\nDo you know of any model(s) which can look at multiple pictures of the same object at once?", "link": "https://www.reddit.com/r/MachineLearning/comments/c88w7q/d_defect_detection_in_an_object_looking_at/"}, {"autor": "junkwhinger", "date": "2019-07-05 20:14:43", "content": "[P] PyTorch Implementation of SamplePairing &amp; Testing on a small dataset /!/ Hi, I've recently read Data Augmentation by Pairing Samples for Images Classification and found SamplePairing fascinating. I applied the method on my small -----> image !!!  dataset and it showed lower validation loss than the baseline :)\n\npost: [https://jsideas.net/samplepairing/](https://jsideas.net/samplepairing/)\n\ngithub: [https://github.com/junkwhinger/SamplePairing](https://github.com/junkwhinger/SamplePairing)", "link": "https://www.reddit.com/r/MachineLearning/comments/c9kmo6/p_pytorch_implementation_of_samplepairing_testing/"}, {"autor": "Freightlok", "date": "2019-10-20 05:23:50", "content": "[P] What're some good datasets for -----> image !!!  classification projects? /!/ What're  some good datasets for image classification projects other than the  MNISTs out there. I'm still a beginner, but I'm looking for a  challenging project, that I'll spend a lot of time tuning, to push me  closer to that intermediate stage.\n\nAppreciate any suggestions and thank you", "link": "https://www.reddit.com/r/MachineLearning/comments/dkfgsb/p_whatre_some_good_datasets_for_image/"}, {"autor": "sendMeMememes", "date": "2019-10-20 00:32:37", "content": "[N] Secret -----> Image !!!  from Node summit on how AI works.", "link": "https://www.reddit.com/r/MachineLearning/comments/dkcaws/n_secret_image_from_node_summit_on_how_ai_works/"}, {"autor": "EveryDay-NormalGuy", "date": "2019-10-19 09:55:35", "content": "[R] Billion-scale semi-supervised learning for state-of-the-art -----> image !!!  and video classification - FAIR", "link": "https://www.reddit.com/r/MachineLearning/comments/dk1omk/r_billionscale_semisupervised_learning_for/"}, {"autor": "pumpedcharge", "date": "2019-10-18 23:22:58", "content": "Learning the types (styles &amp; content) of photos users like /!/ Have any papers been published that can accurately identify the types of stylistic and content components that users like in an -----> image !!! ? E.g. some folks really enjoy taking pics of landscapes at sunset, and others love taking pictures of people with shadows in a specific way. I'd be interested in seeing what model architectures and setup has been used to be able to give an output probability where 0 is would not like this and 1 is would really like/care about this style.\n\nYou could imagine that inputs would be the images with either learned features or engineered ones, and then the labels would be either a rating from 0 to 1 or simply a 0 or 1 value (e.g. I like this photo or I don't).\n\nAny papers to recommend?", "link": "https://www.reddit.com/r/MachineLearning/comments/djvn6i/learning_the_types_styles_content_of_photos_users/"}, {"autor": "bitsydarel", "date": "2019-10-18 10:12:00", "content": "-----> Image !!!  classification with other properties /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/djle2g/image_classification_with_other_properties/"}, {"autor": "ObsidianMinecart256", "date": "2019-12-19 14:48:29", "content": "How to make a -----> image !!!  recognition ai? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ecu61j/how_to_make_a_image_recognition_ai/"}, {"autor": "arimorcos", "date": "2019-11-25 23:06:29", "content": "[R] Understanding the generalization of \"lottery tickets\" in neural networks /!/ Sharing our recent blog post summarizing some of our recent work understanding the boundaries of the lottery ticket hypothesis. In particular, we make some progress towards the following questions:\n\n* Do winning ticket initializations contain generic inductive biases or are they overfit to the particular dataset and optimizer used to generate them? \n* Is the lottery ticket phenomenon limited to supervised -----> image !!!  classification, or is it also present in other domains like RL and NLP? \n* Can we begin to explain lottery tickets theoretically? \n\nThe blog post is below:\n\n[**Understanding the generalization of \"lottery tickets\" in neural networks**](https://ai.facebook.com/blog/understanding-the-generalization-of-lottery-tickets-in-neural-networks/)\n\nAnd the papers covered can be found here:\n\n[One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers](https://arxiv.org/abs/1906.02773)\n\n[Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP](https://arxiv.org/abs/1906.02768)\n\n[Luck Matters: Understanding Training Dynamics of Deep ReLU Networks](https://arxiv.org/abs/1905.13405)\n\n[Student Specialization in Deep ReLU Networks With Finite Width and Input Dimension](https://arxiv.org/abs/1909.13458)", "link": "https://www.reddit.com/r/MachineLearning/comments/e1ojfo/r_understanding_the_generalization_of_lottery/"}, {"autor": "pramook", "date": "2019-11-25 18:11:28", "content": "[R][P] Talking Head Anime from a Single -----> Image !!!  /!/ I trained a network to animate faces of anime characters. The input is an image of the character looking straight at the viewer and a pose, specified by 6 numbers. The output is another image of the character with the face posed accordingly.\n\n![video](5h95d7fzfv041 \"What the network can do in a nutshell.\")\n\nI created two tools with this network.\n\n* One that changes facial poses by GUI manipulation:  [https://www.youtube.com/watch?v=kMQCERkTdO0](https://www.youtube.com/watch?v=kMQCERkTdO0) \n* One that reads a webcam feed and make a character imitates the user's facial movement:  [https://www.youtube.com/watch?v=T1Gp-RxFZwU](https://www.youtube.com/watch?v=T1Gp-RxFZwU) \n\nUsing a face tracker, I could transfer human face movements from existing videos to anime characters. Here are some characters impersonating President Obama:\n\n![video](jqb6eziwgv041)\n\nThe approach I took is to combine two previous works. The first is the [Pumarola et al.'s 2018 GANimation paper](https://www.albertpumarola.com/research/GANimation/index.html), which I use to change the facial features (closing eyes and mouth, in particular). The second is  [Zhou et al.'s 2016 object rotation by appearance flow paper](https://arxiv.org/abs/1605.03557), which I use to rotate the face. I generated a new dataset by rendering 8,000 downloadable 3D models of anime characters. \n\nYou can find out more about the project at [https://pkhungurn.github.io/talking-head-anime/](https://pkhungurn.github.io/talking-head-anime/).", "link": "https://www.reddit.com/r/MachineLearning/comments/e1k092/rp_talking_head_anime_from_a_single_image/"}, {"autor": "JsonPun", "date": "2019-11-25 18:06:47", "content": "[P] Webpage Data Extraction using Image Classification and Object Detection /!/ I am working on creating something that can detect and ideally extract information from a job posting. \n\nI have some questions around the data I am using. I currently crawl websites and take screenshots of their career pages. These screenshots vary in dimensions due to the length of the website. \n\nDisclaimer, I am not a ML Pro. I am self taught everything and currently using Google's AutoML Services for training my model. \n\nMy Questions: \n\n1. Should I use these long/large images? Or is it better to cut them in half and then feed it to the AI. With the large images when I zoom in I can see everything fine for labeling. When not zoomed in, it can be hard to make things out.\n2. How small should labels be? Google allows the smallest to be 8 pixels by 8 pixels. If they can be big I can use the large images and just zoom in? \n3. Is there a way to give context to the classifier/object detector? I realized when I evaluate a job posting I get context from the url and other words on the page that it doesn't get since it only sees a screenshot.\n4. Should I try to label every element on the page? if yes, In a high level way or granular? \n5. Any other hints or tips I should think about to solve this problem?\n\nMy Attempts/Approaches\n\n**Attempt 1: Object Detection** \n\nMy first attempt was to perform object detection on screen shots that were cut down to \\~2,000 pixels. I then labeled most of the content on the page with labels like: Header, Footer, Section, Heading, SubHeading, Job Title, Job Posting, Paragraph, Section Heading, Section SubHeading. \n\nResults :   \n**Total images:** 183  \n**Test items:** 17  \n**Total objects:** 244  \n**Object to image avg:** 14.35  \n**Precision:** 91.43% (Using a score threshold of 0.508)  \n**Recall:** 13.11% (Using a score threshold of 0.508)  \n**Average precision:** 0.171 at 0.5 IoU\n\n**Conclusion:** Object detection needs many more images, also the labels I provided were not concrete enough. Looking back I found the definitions for certain things to be vague. For example I was using the label heading, subheading and job title. Well sometimes the heading is also a job title, but I would only mark it as job title. Thinking about it from the computers perspective how will it know a heading from a job title? There is not much there visually for it to grab onto. This lead me to cut the images down to a height of 2,000 pixels so I could see each element more clearly. \n\nThe problem here is do I try to label every HTML element?\n\n**Attempt 2 Object Classification**\n\nMy second try was to use -----> image !!!  classification to determine if I was on a job posting page, then if true use another model to extract the data. \n\nMy first model1 results  \n**Total images:** 85  \n**Test items:** 9  \n**Precision:** 77.78%  \n**Recall:** 77.78%\n\nMy second model2 results  \n**Total images:** 484  \n**Test items:**55  \n**Precision:** 90.7%  \n**Recall:** 70.91%\n\nThese results were more in-line with what I had thought. When looking at the overall page there over and over there becomes a familiar pattern with what a job posting looks like.\n\n**Final Attempt - Object Detection:**  \nI am now trying again with an object detection model, that is trained only on job posting's, I think this will do better as it only has 3 labels, Job Title, Job Location and Apply Button. I wanted to include a label for: Responsibilities, Qualifications, skills, bonus, ect... but came back to the fact that there is not much for it to grab onto...as I find these in the posting by reading.\n\nModel currently in training...\n\n**Final Notes**  \nI believe the correct way for me to do this problem would be to train the AI on the html code, but I am using google's automl services so I dont know how/if that is possible. I was thinking about using/combining different types of data/techniques since there is information in the URL and code that I'm not leveraging. Perhaps apply NLP to the URLS? \n\nThanks for checking out my project any thoughts are appreciated.", "link": "https://www.reddit.com/r/MachineLearning/comments/e1jxin/p_webpage_data_extraction_using_image/"}, {"autor": "WoahCanyonero", "date": "2019-11-25 09:59:31", "content": "[D] -----> Image !!!  synthesis before GANs /!/ I'm writing my master's thesis on image generation and was wondering: which other methods were/are used to synthesize images, aside from GANs? Any time I look up \"image synthesis\" I only find GAN tutorials, and excluding GANs seems to bring up photon scanning.", "link": "https://www.reddit.com/r/MachineLearning/comments/e1dxvl/d_image_synthesis_before_gans/"}, {"autor": "hardmaru", "date": "2019-11-25 09:34:08", "content": "[R] DeepFovea: Neural Reconstruction for Foveated Rendering and Video Compression using Learned Statistics of Natural Videos (SIGGRAPH Asia 2019) /!/ I thought this [project](https://research.fb.com/publications/deepfovea-neural-reconstruction-for-foveated-rendering-and-video-compression-using-learned-statistics-of-natural-videos/) from FB Research is a really cool work:\n\n*Abstract*\n\nIn order to provide an immersive visual experience, modern displays require head mounting, high -----> image !!!  resolution, low latency, as well as high refresh rate. This poses a challenging computational problem. On the other hand, the human visual system can consume only a tiny fraction of this video stream due to the drastic acuity loss in the peripheral vision. Foveated rendering and compression can save computations by reducing the image quality in the peripheral vision. However, this can cause noticeable artifacts in the periphery, or, if done conservatively, would provide only modest savings. In this work, we explore a novel foveated reconstruction method that employs the recent advances in generative adversarial neural networks. We reconstruct a plausible peripheral video from a small fraction of pixels provided every frame. The reconstruction is done by finding the closest matching video to this sparse input stream of pixels on the learned manifold of natural videos. Our method is more efficient than the state-of-the-art foveated rendering, while providing the visual experience with no noticeable quality degradation. We conducted a user study to validate our reconstruction method and compare it against existing foveated rendering and video compression techniques. Our method is fast enough to drive gaze-contingent head-mounted displays in real time on modern hardware. We plan to publish the trained network to establish a new quality bar for foveated rendering and compression as well as encourage follow-up research.\n\nProject page / paper / video / code: https://research.fb.com/publications/deepfovea-neural-reconstruction-for-foveated-rendering-and-video-compression-using-learned-statistics-of-natural-videos/", "link": "https://www.reddit.com/r/MachineLearning/comments/e1dpn9/r_deepfovea_neural_reconstruction_for_foveated/"}, {"autor": "daanvdn", "date": "2019-11-24 20:39:33", "content": "[D] anybody know an implementation of fchollet's 2016 paper on label embeddings? /!/ This is the paper: Information-theoretical label embeddings for large-scale -----> image !!!  classification\n\nhttps://arxiv.org/abs/1607.05691", "link": "https://www.reddit.com/r/MachineLearning/comments/e149u6/d_anybody_know_an_implementation_of_fchollets/"}, {"autor": "tjasmin111", "date": "2019-11-23 23:21:58", "content": "Techniques and semantics in better training of deep learning models /!/ I'm relatively new to Deep Learning, and trying various models and datasets using \\`Keras\\`. I'm starting to love it!\n\nThrough-out my experimentations, I have come into some semantic questions that I don't know how they can affect the overall accuracy of my trained model. My target application is fire detection in videos (fire vs non-fire). So I'm trying to get tips and tricks from those well experienced on Deep learning, and here are my semantic questions:\n\n 1. Given that I have to do detection on videos, I've been mostly adding actuall frames of videos to my dataset, and less photos. Does adding photos from Google ever help (as we largen our dataset) or it's actually more considered noises and shall be removed?\n\n 2. I've trained a deep model (\\`ResNet50\\`) as well as a shallow 5-layer model. I realized the ResNet50 model is more sensitive and has a high recall (all fires are definitely detected), but has false positives as well (strong source of lights like sunlight or lamps are identified as fire). While the shallower model is 10x faster, it can miss fires if it is smaller in the -----> image !!! , so it's less sensitive. But also has low false positives. Is it always true? So what are techniques and tips to fix these issues in each of these models? \n\nFor instance, the shallow model doesn't see this fire. Shall I think it's not complex enough to work well when the scene has many objects inside?\n\n\\[!\\[enter image description here\\]\\[1\\]\\]\\[1\\]\n\n&amp;#x200B;\n\n 3. The sample code I saw resizes photos to \\`256x256\\` for training. What's the effect of bigger sizes vs smaller ones say \\`300x300\\`? Can I expect while bigger sizes increase computation time, they provide higher accuracy?\n\n 4. The sample code also converts photos to grayscale and uses Antialiasing before passing. Does it have good effects? What if I pass the colored version as fire is mostly about colors?\n\n 5. When I see the model is doing bad on certain scenes (say those sun lights or lamps), I take multiple of those frames and add them to my non-fire dataset. Does it have any positive effects and being taken care of? And is it better to add multiple successive frames or just one frame is enough? \n\n 6. My fire dataset has \\`1800\\` images and my non-fire dataset has \\`4500\\` images. As a rule of thumb, the bigger each class, the better? Of course the non-fire data should be bigger, but we can not add whatever on earth as non-fire so what should be the distribution of the sizes?\n\n&amp;#x200B;\n\n  \\[1\\]: [https://i.stack.imgur.com/gwrjrm.jpg](https://i.stack.imgur.com/gwrjrm.jpg)", "link": "https://www.reddit.com/r/MachineLearning/comments/e0pzx9/techniques_and_semantics_in_better_training_of/"}, {"autor": "MyMastersAccount", "date": "2019-02-24 11:37:20", "content": "[R] Evaluating the impact of sensory inputs for self-driving cars. /!/ Hey guys, was wondering if you knew of any papers that used only sensory inputs (such as speed, lane position, distance between cars etc) rather than images as a way to train/test a self-driving car model. \n\nDuring my research I have found many papers where sensory inputs only contribute a small part of the total training data (such as tyre direction sensor, LIDAR for distance between cars) but still mainly relied on -----> image !!!  data. \n\nIs there any paper that exclusively used multiple sensor based inputs (without turning sensor data to images) to train a model?\n\nThank you for your help. \n\n", "link": "https://www.reddit.com/r/MachineLearning/comments/au772h/r_evaluating_the_impact_of_sensory_inputs_for/"}, {"autor": "pkacprzak", "date": "2019-02-24 02:25:18", "content": "[D] Best way to host an -----> image !!!  classification model for predictions on Google Cloud /!/ I trained two models for image classification for a task similar to MNIST (although a bit harder) on my local machine. One model I have is a transfer learning from another model in TensorFlow while the other one is trained from scratch in PyTorch. Now,  I'd like to deploy it and serve for online predictions. \n\nI'm quite familiar with Google Cloud platform, especially with App Engine, but not that much with Compute Engine and their specialized Machine Learning solutions. I searched the web for tutorials and documentations about how to do that but all I found are clumsy and didn't cover end-to-end process. In short, I'd like to know what's the recommended way to upload an existing model and serve its prediction via API, e.g. REST, and maybe to be able to get the predictions internally from my another application on App Engine.\n\n&amp;#x200B;\n\nIt seems this is the official documentation for deploying TensorFlow models on Google Cloud:\n\n[https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models](https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models)\n\n&amp;#x200B;\n\nWhile for PyTorch, I found this and related links for creating a VM with PyTorch: [https://cloud.google.com/deep-learning-vm/docs/pytorch\\_start\\_instance](https://cloud.google.com/deep-learning-vm/docs/pytorch_start_instance)\n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/au38sr/d_best_way_to_host_an_image_classification_model/"}, {"autor": "hasannasirk", "date": "2019-02-23 21:54:25", "content": "[D] How can I fuse CNN features with texture (GLCM) features in -----> image !!!  classification task? /!/ I want to fuse CNN features with GLCM features (energy, entropy, contrast, homogeneity, correlation etc).  What will be the Python implementation code? please give a solution", "link": "https://www.reddit.com/r/MachineLearning/comments/au0mct/d_how_can_i_fuse_cnn_features_with_texture_glcm/"}, {"autor": "hasannasirk", "date": "2019-02-23 21:44:27", "content": "How can I fuse CNN features with texture (GLCM) features in -----> image !!!  classification task? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/au0ilo/how_can_i_fuse_cnn_features_with_texture_glcm/"}, {"autor": "freshprinceofuk", "date": "2019-02-23 12:51:54", "content": "Image Processing Resources /!/ Hi,\n\nI've recently been hired by a startup as a Machine Learning Engineer with a focus on Computer Vision (first job out of university) and while a large Machine Learning project has been identified to be started very soon, I'm currently doing some simpler Image Processing tasks. My background is programming, time-related signal processing, -----> image !!! -based machine learning, and engineering (Engineering BEng and MSc).\n\n\nCan anyone suggest resources on specifically non-learning based Image Processing/Computer Vision algorithms/techniques?\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/atv78p/image_processing_resources/"}, {"autor": "xternalz", "date": "2019-02-09 10:51:39", "content": "[R] ColorNet: Investigating the importance of color spaces for -----> image !!!  classification", "link": "https://www.reddit.com/r/MachineLearning/comments/aorc8b/r_colornet_investigating_the_importance_of_color/"}, {"autor": "Atom_101", "date": "2019-02-09 09:33:08", "content": "[D]Preparation of ground truth vectors for YOLOv3 /!/ I'm trying to implement a training module for YOLOv3, from scratch in keras+tensorflow. I'm having a hard time figuring out how to prepare the target vectors from bounding box coordinates. \n\nThe network divides the input -----> image !!!  into grid cells and for each cell it predicts an objectness and 4 terms (tx,ty,th,tw) for the bounding box. The actual box coordinates are obtained as follows:\n\n    bx = sigmoid (tx) + cx\n    by = sigmoid(ty) + cy\n    bh = ph.e^(th)\n    bw = pw.e^(tw)\n\nWhere ph,pw are anchor box priors and cx,cy are position of grid cell. Am I supposed to \n\na) use the inverse of these functions to calculate tx,ty,th,tw for each image?(maybe it's because my code isn't very optimised but this is taking quite a while to compute) \n\nOR\n\nb) calculate actual box coordinates(in pixels) for each image and compute the loss using ground truth box coordinates? If yes, then how am I to find cross entropy box loss using pixel values?\n\nAlso, I have used the darknet repo by GitHub user AlexeyAB. It takes the coordinates as the ratio of pixel values of ground truth to the total height/width of the image. How does it calculate the target vectors?", "link": "https://www.reddit.com/r/MachineLearning/comments/aoqw3y/dpreparation_of_ground_truth_vectors_for_yolov3/"}, {"autor": "zenraysofficial", "date": "2019-02-09 06:26:54", "content": "Get to know the Benefits of Machine Learning /!/  \n\n Machine Learning is a field of Artificial Intelligence. It allows software applications to become more accurate. The basic thing of Machine Learning is to build algorithms that receives data input\u00a0to predict\u00a0output using statistical analysis. It requires searching data for patterns.\u00a0\n\n&amp;#x200B;\n\n**Benefits of Machine Learning**\n\n1. Data mining is the process of examining several databases to\u00a0analyze data and to provide\u00a0information.\u00a0Machine learning provides actual assumptions that can be used to support decisions.\u00a0\n2. Machine learning system system will\u00a0refer newly acquired data to do better\u00a0consumption patterns.\n3. Machine learning can automate task\n\n**Machine Learning is applied in**\n\n1. Fraud Detection\n2. Web Search Results\n3. Self-driving Google Car\n4. Online recommendation engines\n5. New pricing models\n6. Prediction of equipment failures\n7. Real-time ads on Web pages\n8. Email spam filtering\n9. Pattern &amp; -----> Image !!!  recognition\n10. Text based sentimental analysis\n11. Network intrusion detection\n12. Credit scoring\n13. Next best offers\n\nIf you are Interested to learn Machine learning, have a look at course content\n\n&amp;#x200B;\n\n*Processing img f2iokz10lhf21...*\n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/aoptfo/get_to_know_the_benefits_of_machine_learning/"}, {"autor": "kaswanclan", "date": "2019-07-11 15:05:37", "content": "Tempered -----> Image !!!  detection /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/cbwn08/tempered_image_detection/"}, {"autor": "sjking1880", "date": "2019-07-11 00:20:58", "content": "Python Open Cv Homography giving unexpected results /!/ Hi! I am doing homography in Python and am not getting expected Output. I am getting differen matrix\n\n&amp;#x200B;\n\nHere is my code's Google Colab link:-\n\n[https://colab.research.google.com/drive/1OMWP4faY8dbNRAfryvBs7S7CEuM8oeDz](https://colab.research.google.com/drive/1OMWP4faY8dbNRAfryvBs7S7CEuM8oeDz)\n\n&amp;#x200B;\n\nHere is basic Homography Link (just for your reference):-\n\n[https://www.learnopencv.com/image-alignment-feature-based-using-opencv-c-python/](https://www.learnopencv.com/image-alignment-feature-based-using-opencv-c-python/)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nIn casw you are not able to open Colab. Here is source code.\n\n    import cv2\n    import numpy as np\n     \n    if __name__ == '__main__' :\n     \n    \n        pts_src = np.array([[141.0, 131.0], [480.0, 159.0], [493.0, 630.0],[64.0, 601.0]])  \n    \n        pts_src=np.asarray(pts_src)                     #not needed actually\n        pts_src = pts_src.astype('float32')              #not needed actually\n    \n       \n        pts_dst = np.array([[318.0, 256.0],[534.0, 372.0],[316.0, 670.0],[73.0, 473.0]])\n        pts_dst=np.asarray(pts_dst)                      #not needed actually\n        pts_dst= pts_dst.astype('float32')               #not needed actually\n    \n        h, status = cv2.findHomography(pts_src, pts_dst,cv2.RANSAC, 5.0)\n         \n        # Wrap source -----> image !!!  to destination based on homography\n        \n    \n        print( h)\n        print(len(h))\n        \n        print(\"------------Printing H------\")\n        \n        print( status)\n    \n        print(\"------Printing Status-------------\")\n    \n        print (status.ravel().tolist())\n    \n        print(\"--------------------\")\n            \n    \n        print(\"Source is multiplied first\")\n    \n        #pts_src2 = np.array([[141.0, 131.0,1.0], [480.0, 159.0,1.0], [493.0, 630.0,1.0]])\n        pts_dst2=np.array([[318.0, 256.0,1.0],[534.0, 372.0,1.0],[316.0, 670.0,1.0]]).transpose()\n    \n    \n        print(\"----see---\")\n        print(+len(pts_dst2))\n        pts_dst2=np.asarray(pts_dst2)                      #not needed actually\n        pts_dst2= pts_dst2.astype('float32')               #not needed actually\n        pts_dst2=np.asmatrix(pts_dst2)\n    \n    \n        h=np.asmatrix(h)\n    \n    \n        pts=np.dot( h, pts_dst2) \n        print(pts)\n    \n    \n    print(\"--------END-----------\")\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nOutput:-\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n     [ 1.46491654e-01  4.41418278e-01  1.61369294e+02]\n     [-3.62463336e-04 -9.14274844e-05  1.00000000e+00]]\n    3\n    ------------Printing H------\n    [[1]\n     [1]\n     [1]\n     [1]]\n    ------Printing Status-------------\n    [1, 1, 1, 1]\n    --------------------\n    Source is multiplied first\n    ----see---\n    3\n    [[322.31218638 367.38950309 147.72051431]\n     [320.95671889 403.80343648 503.41090271]\n     [  0.86133122   0.77243355   0.82420517]]\n    --------END-----------", "link": "https://www.reddit.com/r/MachineLearning/comments/cboqq0/python_open_cv_homography_giving_unexpected/"}, {"autor": "AlphaHumanZero", "date": "2019-07-10 22:26:27", "content": "[News] DeepMind\u2019s StarCraft II Agent AlphaStar Will Play Anonymously on Battle.net /!/ [https://starcraft2.com/en-us/news/22933138](https://starcraft2.com/en-us/news/22933138)\n\nThe announcement is from the Starcraft 2 official page. AlphaStar will play as an anonymous player against some ladder players who opt in in this experiment in the European game servers.\n\nSome highlights:\n\n* AlphaStar can play anonymously as and against the three different races of the game: Protoss, Terran and Zerg in 1vs1 matches, in a non-disclosed future date. Their intention is that players treat AlphaStar as any other player.\n* Replays will be used to publish a peer-reviewer paper.\n* They restricted this version of AlphaStar to only interact with the information it gets from the game -----> camera !!!  (I assume that this includes the minimap, and not the API from the January version?).\n* They also increased the restrictions of AlphaStar actions-per-minute (APM), according to pro players advice. There is no additional info in the blog about how this restriction is taking place.\n\nPersonally, I see this as a very interesting experiment, although I'll like to know more details about the new restrictions that AlphaStar will be using, because as it was discussed here in January, such restrictions can be unfair to human players. What are your thoughts?", "link": "https://www.reddit.com/r/MachineLearning/comments/cbnftu/news_deepminds_starcraft_ii_agent_alphastar_will/"}, {"autor": "cho-kawaii", "date": "2019-07-10 20:18:07", "content": "Does anyone know of a library program which defeats face recognition but leaves the -----> image !!!  recognizable by humans? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/cblv31/does_anyone_know_of_a_library_program_which/"}, {"autor": "RedditAcy", "date": "2019-04-13 02:53:45", "content": "[D]My Machine Learning Journal #10: First time doing reinforcement learning and beating atari breakout with it /!/ I have been inconsistent with my journal, but I am back and fresher than ever. \n\nVlog version as usual:\n\n[https://youtu.be/dcnGI6x-yk0](https://youtu.be/dcnGI6x-yk0)\n\nToday (and yesterday) I did &amp; learned:\n\nRL seems to have a lot of exploration going on vs some other ML tasks. One popular application it has is definitely beating videogames. The [Mario AI](https://www.youtube.com/watch?v=qv6UVOQ0F44&amp;t=1s) was a viral hit in 2015. I decided to build a RL model that can beat atari breakout. This was soon classified as impossible given my current coding skills, so I chose to implement a [medium article](https://becominghuman.ai/lets-build-an-atari-ai-part-0-intro-to-rl-9b2c5336e0ec) first that beat atari breakout. This article was great at linking the original Atari breakout RL paper with the code, but the full code was not posted, so I was stuck. Luckily, a user named boyuanf hit us up with the tensorflow implementation of the article on medium, here's the forked version of [it](https://github.com/BlastWind/DeepQLearning). \n\nI downloaded the trained weights and model, and I ran it after installing openAI gym in conda with pip. Unfortunately, atari-py seems incompatible with windows 10, so I had to go through a very annoying process to finally come through with this easy line of code to solve the problem: \n\n    pip install --no-index -f https://github.com/Kojoley/atari-py/releases atari_py\n\nYea it is just one of those problems man. \n\nAnyways, I then was able to run **gym** and see the beautiful pre-trained model doing work, it got to a pretty good high score, I think 57 or something. \n\nIt is actually **after I implemented the project that I come back to reading the papers**, this works for me. I usually try to guess what the original algorithm does by doing a project first. For me, doing a project first then reading the paper also gives that revelation of: \"oh, the reason that I have this line in the code is because of that sentence in the paper\".\n\nThe [paper](https://arxiv.org/abs/1312.5602) and this [medium article](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe) helped my understanding a lot. This pseudocode in the paper opened the doors for me: \n\n&amp;#x200B;\n\nhttps://i.redd.it/6bpl6f203yr21.png\n\nI'm going to try to explain this pseudocode with even English-er language. We will input the current frame and a few previous frames to our RL model. The RL model will interpret these inputs as the **state**, and it will either choose the **action** based on the **Q-table** or choose a random action. We can imagine that as the model gets more advanced, we will choose less random actions to let the model learn, but in the early stages, when the model has no idea what to do, we probably want to let it explore randomly, we will use a decreasing **epsilon** value to model this. The emulator will receive the action chosen by the RL model, run that action, then display the new -----> image !!!  and return the reward. The **Q-table** will be updated based on this reward. The **Q-table** is just a table that has states mapping to potential actions. When the model is complex and epsilon is low, the RL model chooses actions based on the **Q-table,** a higher value (which means high rewards) in the state mapping to action will probably mean the model is choosing that. \n\nThat;s it for this one, I learned a lot since it was my first time exploring RL! Exciting, can't wait to do more.", "link": "https://www.reddit.com/r/MachineLearning/comments/bcm681/dmy_machine_learning_journal_10_first_time_doing/"}, {"autor": "DifficultDifficulty", "date": "2019-04-12 22:43:37", "content": "[P] Framework for medical -----> image !!!  segmentation with tools and models off-the-shelf /!/ Hi all,\n\nAt my university, I am taking a grad course in which we're select a deep learning project and work on it by the end of the course.I had previously developed a set of tools for my [M.Sc](https://M.Sc) project where I need to manipulate sets of medical images in different formats (DICOM, Niftii, Nrrd), pre-process them using SimpleITK and feed them into a deep learning pipeline.I figured I could execute that course project using those tools and put it up on github for everyone to use and for me to showcase my skills, as I will finish school very soon and that will give me some visibility for my upcoming job search.\n\nThe platform relies on visdom for visualization capabilities, I had implemented functions that allow you to visualize your network's computation graph, the experiment options and hyperparameters, your loss and accuracy curves, the gradient flow graphs through your networks as well as histograms for weight distribution in your layers. The python classes offer different functions to easily extend functionality where you can sample images during training if you're working with images and display them on the browser.\n\nThe repository comes with implementations of UNets and ResNets as well as many GAN loss functions such as Wasserstein GAN with gradient penalty. The same class can also be set in non-GAN mode so that it uses a simple cross-entropy loss function.\n\nI had used this code to segment vertebrae from MRIs, reaching a 0.87 dice coefficient.\n\nThe code is generic enough to be used for tasks other than image segmentation if you wish to play around with it.\n\nI hope this will be helpful to you.\n\nPlease find it in the link down below:\n\n[https://github.com/Roulbac/GanSeg](https://github.com/Roulbac/GanSeg)\n\n&amp;#x200B;\n\nHere is an example of segmentation on unseen data:\n\n&amp;#x200B;\n\nhttps://i.redd.it/f81y3pw57ur21.png", "link": "https://www.reddit.com/r/MachineLearning/comments/bcjvpx/p_framework_for_medical_image_segmentation_with/"}, {"autor": "MathCompNeuroProf", "date": "2019-04-11 21:22:16", "content": "[R] Generator persistence in GANs /!/ I had an idea that I haven't been able to find implemented elsewhere, but might not be using the right search terms.\n\nAfter generating an -----> image !!!  with a GAN, use the generator with its current parameters fixed to iteratively update the -----> image !!! , either through grad descent on the pixels or on the generator params (which would be reverted to their old values after this).\n\nThis idea overlaps with minibatch persistence and with the deep image prior paper.\n\nHas this been tried? Is there any reason to think it wouldn't work?", "link": "https://www.reddit.com/r/MachineLearning/comments/bc5csf/r_generator_persistence_in_gans/"}, {"autor": "gwen0927", "date": "2019-04-11 17:28:21", "content": "TableBank: Benchmark for -----> Image !!! -based Table Detection and Recognition", "link": "https://www.reddit.com/r/MachineLearning/comments/bc2mvd/tablebank_benchmark_for_imagebased_table/"}, {"autor": "JackAndrewMcKay", "date": "2019-03-16 17:24:09", "content": "[D] Representing visual structures with expanding trees /!/ Hi, I'm not sure if this is the right place to ask this.\n\nI haven't had much experience developing ML/NN but it's something I love to think and talk about with friends. Recently, I came to the realisation that everything a -----> camera !!!  has ever seen could be represented by a single tree, and I'm wondering if this approach has been taking before.\n\nIf it hasn't been used before, I'm wondering what people's thoughts are in how useful such an approach would be.\n\nDetecting horizontal, vertical, diagonal, or no line is simple enough. You could program in the following context:\n\n* Horizontal lines can only continue on the left or right pixel.\n* Vertical lines can only continue on the top or bottom pixel.\n* Diagonal lines can only continue to the top-left and bottom-right pixel, or top-right and bottom-left pixel (depending on orientation).\n\nWhen lines are extracted from an image (or frame of live video), those lines can then be trace to \"map a graph\", where each node represents a horizontal/vertical/(left/right)diagonal line.\n\nFor example, a certain triangle might be represented as \"/, \\, -\".\n\n\"Training\" would consist of analysing a video feed using this technique to build the tree. When a new \"path\" is constructed, it may follow the nodes previously mapped until the path doesn't match and a new node needs created.\n\nFor example, a hexagon might be represented as \"/, \\, |, /, \\, |\". Comparing to the previous triangle example:\n/ \\ | / \\ |    &lt;- hexagon\n/ \\ -   &lt;- triangle\n\nThe hexagon and the triangle follow the same path until the 3rd element, where the hexagon has the \"|\" line, but the triangle has the \"-\" line, in which case a new \"split\" would be created at the second element. One representing the continued path of the triangle, and the other representing the continued path of the hexagon.\n\nUnless I'm thinking about it wrong, literally any shape ever seen could be represented on the same tree.\n\nAfter a period of training, pruning could be performed where pathways that were rarely activated get stripped (a \"sleep\" cycle).\n\nThe same shapes at multiple scales could also be detected. Using the triangle as an example,\n\"/ / \\ \\ - -\" may represent a triangle x2 bigger than \"/ \\ -\".\n\nDuring training, if \"/ / \\\" is detected rather than \"/ \\\", the model could reason that it is \"following the same path\" but it's been stretch x2, and thus all extra analysis on the same path should be done scaled to a half, allowing a x2 triangle to be represented with the same pathway.\n\nUsing this technique, the activation at each \"leaf node\" could represent a unique line shape, i.e. the leaf represents a classification.\n\nAside from the performance implications (I'd imagine heavy optimisation would be needed to utilise this), does this seem like a reasonable way to represent visual memory with unsupervised learning?", "link": "https://www.reddit.com/r/MachineLearning/comments/b1uuuv/d_representing_visual_structures_with_expanding/"}, {"autor": "duynn912", "date": "2019-03-15 05:22:04", "content": "SOD-MTGAN: Small Object Detection via Multi-Task Generative Adversarial Network /!/ SOD-MTGAN: Small Object Detection via Multi-Task Generative Adversarial Network\n\nWhen I read this paper I was confused and have some questions as follows:\n\n1/ I wonder what high resolution images (real data) and low resolution images(fake data) are. The fake data is generated by downsampling the real data using bicubic interpolation with a factor 4 or the fake data is positive and negative areas cropped from an input -----> image !!! . If the fake data is cropped areas so how I can get the high resolution images (real data).\n\n2/ what is the purpose of cropping background areas from the input image? After using the off-the-shelf detectors, we have interest objects and the question here is that the detectors miss objects of interest in some areas of background and we crop these area to increase the performance of the method? and the cropped areas are randomly chosen or picked according to a particular criteria.", "link": "https://www.reddit.com/r/MachineLearning/comments/b1avsm/sodmtgan_small_object_detection_via_multitask/"}, {"autor": "DGs29", "date": "2019-03-15 04:35:25", "content": "[D] How do I texts blocks of text from scanned documents /!/ Here's what I mean:\n\n&amp;#x200B;\n\n[ ](https://i.redd.it/dv6oow3gn7m21.png)\n\nI've tried to do -----> image !!!  processing in Opencv to achieve this:\n\n1. I plotted character level bounding box\n2. Next, I gray-scaled the image and  binarized it.\n3. Applied dilation\n4. And finally placed bbox over the dilated image.\n\nThis is what I can get:\n\n&amp;#x200B;\n\nhttps://i.redd.it/ur7wm12sn7m21.jpg\n\nHere the image is scanned and moreover the line-spaces between the paragraphs are small. So I couldn't able to segment them based on the paragraphs.\n\n&amp;#x200B;\n\nHow can I achieve my desired result. What are the steps to follow?\n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/b1ahhq/d_how_do_i_texts_blocks_of_text_from_scanned/"}, {"autor": "titanandwire", "date": "2019-03-14 23:26:38", "content": "[P] Uncovering latent patterns with DRAW /!/ Hi all, to head this off I figured I'd point out that this question is part of my thesis work for a masters degree in physics. Both me and my advisor are \"fairly\" new to the ML scene (a couple of years worth of experience) so I'm seeking the broader ML community with a question we're not sure about.\n\n### The project\n\nOur project aims to cluster observations based on latent expressions. It's an unsupervised problem where we, for the sake of this post, want to cluster the MNIST dataset based on some latent variable representation. Vanilla or convolutional variational autoencoders were not able to reconstruct the data sufficiently (like the MNIST data our data is pretty sparse on the canvas and with rather sharp edges, it's also noisy but that's beside the point). We then turned to the [DRAW](https://arxiv.org/abs/1502.04623) algorithm. I've attached an -----> image !!!  from the paper regarding it's architecture ![DRAW](https://cdn-images-1.medium.com/max/800/1*KgBk06gZ3AV8jqkwMZGbGg.png \"Image taken from the linked paper\")\n\n### Our question\n\nGiven the recurrent nature of the algorithm, and the fact that it uses glimpses/attention for the reconstruction can we capture the latent expression? The latent samples themselves seems to not carry the information itself (based on clustering analysis when applying DRAW on simulated data). Is there some part of the literature I've missed? Any thoughts would be very welcome.", "link": "https://www.reddit.com/r/MachineLearning/comments/b17gsp/p_uncovering_latent_patterns_with_draw/"}, {"autor": "coltonoscopy", "date": "2019-04-10 15:49:25", "content": "Live from Harvard on Twitch, Wed 4/10 at 3pm ET, CS50's Nick Wong introduces us to the world of neural networks using Tensorflow and Python, featuring k-means clustering and -----> image !!!  generation based on prior stream screenshots.", "link": "https://www.reddit.com/r/MachineLearning/comments/bbnkz0/live_from_harvard_on_twitch_wed_410_at_3pm_et/"}, {"autor": "PolyrogueKappa", "date": "2019-04-10 12:16:35", "content": "How to deal with an imbalanced dataset for MULTI-LABEL classification? /!/ Hello there. You can consider me novice to intermediate at best with Machine Learning.\n\nFor the past few months, I've been developing a neural network that learns to play a 3D fighting game by trying to mimic how I play, in Keras, using a Tensorflow backend. The input data consists of both a low-resolution, greyscaled version of the frame at the time, along with some corresponding categorical information. I extract important information from the game using computer vision and represent that using a multi-hot array. E.g. [0, 1, 0, 0, 0, 0, 1, 0, 0, 0] where each index represents some information about that moment, like if the enemy is attacking. This all might be useless information, but I'm providing it just in case.\n\nThe labels are the crucial matter. I am providing the network with an array of the buttons that I am pressing at that moment in time.  The key thing to note is that this array is multi-hot, as I am often pressing multiple buttons at once.\n\nAdditional information - The network architecture consists of two branches - one LSTM layer for the categorical data, and one convolutional layer for the -----> image !!!  data. These are then concatenated and fed through two Dense layers before finally going through a sigmoid activation. I am using binary-crossentropy as a loss function. This is really the only combination of activation and loss that I am aware of for multi-label classification. In terms of metrics, both accuracy and binary accuracy appear to result in over 90% for validation and training each time, right from epoch 1.\n\n[Here is a diagram of the model](https://i.stack.imgur.com/R7Fsc.png)\n\nThe unfortunate problem I have had from the start was that my network just doesn't seem to work well. It claims to achieve over 90% accuracy and validation accuracy, and yet only exhibits some form of intelligence when tested. I think this is down to the inherent imbalance of my training labels, as some buttons are just far, far more likely to be pressed than others. But I'm really not sure how to deal with balancing them, as traditional methods like over and undersampling or using class weightings don't work with *multi-label* classification. I'm basically stuck, and googling isn't really helping.\n\nAny help or advice would be greatly appreciated. If you need more information, please don't hesitate to ask. Thank you!", "link": "https://www.reddit.com/r/MachineLearning/comments/bblb6j/how_to_deal_with_an_imbalanced_dataset_for/"}, {"autor": "hardmaru", "date": "2019-04-09 23:08:26", "content": "[R] Open Questions about Generative Adversarial Networks /!/ *New distill.pub [article](https://distill.pub/2019/gan-open-problems/) about future direction of GAN research*\n\n**Open Questions about Generative Adversarial Networks**\n\nWhat we\u2019d like to find out about GANs that we don\u2019t know yet.\n\n1. What are the trade-offs between GANs and other generative models?\n\n2. What sorts of distributions can GANs model?\n\n3. How can we Scale GANs beyond -----> image !!!  synthesis?\n\n4. What can we say about the global convergence of the training dynamics?\n\n5. How should we evaluate GANs and when should we use them?\n\n6. How does GAN training scale with batch size?\n\n7. What is the relationship between GANs and adversarial examples?\n\nhttps://distill.pub/2019/gan-open-problems/", "link": "https://www.reddit.com/r/MachineLearning/comments/bber25/r_open_questions_about_generative_adversarial/"}, {"autor": "imdien", "date": "2019-04-09 10:30:51", "content": "How to proceed after bachelor's degree. /!/ Hi everyone.\n\nI will finish my bachelor's degree in biology this summer. I am certain that i want to work in data science (especially ML/DL) in the future, however, I am not certain how to proceed after I finish my bachelor's. My options are:\n\n\\-Find a job\n\n\\-Start a master's program in quantitative Biology or something similar\n\n\\-try to get into a master's program in data science (probably not that realistic, since they often require a bachelor's in Informatics or statistics)\n\nI have set a lot of focus on math, statistics and data analyis during my studies and i am currently working on a deep learning project where i do -----> image !!!  recognition using CNNs, so I think I should have enough expertise to find an entry level job. I just wanted to ask what you guys think I should do / not do.\n\nThanks for your suggestions!", "link": "https://www.reddit.com/r/MachineLearning/comments/bb6axi/how_to_proceed_after_bachelors_degree/"}, {"autor": "deep-yearning", "date": "2019-08-17 03:52:12", "content": "[D] Why have we not seen equivalent success in deep learning based -----> image !!!  registration? /!/ It seems that other computer vision tasks such as classification, segmentation and synthesis have seen huge advances in accuracy thanks to CNNs, but there seems to be no equivalent advance in image registration. I tried searching for advances in image registration, but it seems that researchers still use 'classical' image registration techniques like mutual information, cross-correlation, etc. Even though there are DL image registration research papers, they are not well adopted in the community.\n\n&amp;#x200B;\n\nFundamentally, is there a reason why this task is more complex that the aforementioned ones?", "link": "https://www.reddit.com/r/MachineLearning/comments/crh5om/d_why_have_we_not_seen_equivalent_success_in_deep/"}, {"autor": "samsamsamrox1212", "date": "2019-08-15 18:46:40", "content": "[D] Selfie2Anime Synthesis using UGATIT /!/ The author has released the pre-trained models a 50 epoch and a 100 epoch variant, alongside the dataset.\n\nTool: [https://github.com/taki0112/UGATIT](https://github.com/taki0112/UGATIT) \n\nI have generated some images using it: [results](https://www.instagram.com/p/B1MdhrYAE-Y/?utm_source=ig_web_button_share_sheet)\n\nTip: 256x256 -----> image !!! s work faster, as well as around 50% of the -----> image !!!  being the face helps.\n\nI am not disappointed with the results, what do you think?", "link": "https://www.reddit.com/r/MachineLearning/comments/cqu3cn/d_selfie2anime_synthesis_using_ugatit/"}, {"autor": "makereven", "date": "2019-08-15 07:15:30", "content": "[P] Pose detection for Google Coral EdgeTPU /!/ this is the demo for showing how to use the PoseNet model to detect human poses from -----> image !!! s and video, such as where someone\u2019s elbow, shoulder or foot appear in the -----> image !!! . \n\nexample code to shows how to run it on a camera stream.  \n\ncode here: [https://github.com/google-coral/project-posenet](https://github.com/google-coral/project-posenet)\n\n&amp;#x200B;\n\nhttps://i.redd.it/htrtp01zbkg31.gif\n\n&amp;#x200B;\n\n&amp;#x200B;\n\npppt: before you run this repo, you should get yourself a coral deb board or coral usb : [suggest this site](https://store.gravitylink.com/global/product/DevBoard)", "link": "https://www.reddit.com/r/MachineLearning/comments/cqmjsg/p_pose_detection_for_google_coral_edgetpu/"}, {"autor": "taki0112", "date": "2019-08-15 05:27:13", "content": "[P] Simple Tensorflow implementation of \"Few-Shot Unsupervised -----> Image !!! -to------> Image !!!  Translation (a.k.a. FUNIT)\" in ICCV 2019 /!/ &amp;#x200B;\n\n[our result](https://i.redd.it/fkkx25bysjg31.png)\n\n[paper result](https://i.redd.it/5etdnm7zsjg31.jpg)", "link": "https://www.reddit.com/r/MachineLearning/comments/cqlo0n/p_simple_tensorflow_implementation_of_fewshot/"}, {"autor": "taki0112", "date": "2019-08-15 05:26:20", "content": "[P] Simple Tensorflow implementation of \"Few-Shot Unsupervised -----> Image !!! -to------> Image !!!  Translation (a.k.a. FUNIT)\" in ICCV 2019 /!/ &amp;#x200B;\n\n![img](5k7ibqlusjg31 \"our result\")\n\n![gif](jxp9npjvsjg31 \"paper result\")", "link": "https://www.reddit.com/r/MachineLearning/comments/cqlnqr/p_simple_tensorflow_implementation_of_fewshot/"}, {"autor": "brhrrr", "date": "2019-08-28 11:01:40", "content": "[D] Design a network what combines supervised (CNN) and unsupervised (AE) for classification task /!/ Hello everyone! Working under one interesting problem, as you can read from post name, and wonder does anyone have ideas or hints for it? As we know autoencoders take input (in my case it's an -----> image !!!  from the popular dataset) and reconstruct it as an output. Let's call input - node 1, output - node 3. It creates valuable features at its hidden layers (let's call it node 2)  during the process. Let's hypothesize, that if node 2 is used as input for CNN then the classification will be improved. My current ideas are:  \n1 - For now, it sounds interesting and reasonable to try use output of the encoder - latent space representation as an input for following CNN. \n\n2 - Use one of the decoder layers as input for CNN.\n\nA possible purpose of it - try to get more important futures from class imbalanced data. (As an example - from 5 classes 1 of them contain 50% fewer images than other). Let's discuss?", "link": "https://www.reddit.com/r/MachineLearning/comments/cwj7qz/d_design_a_network_what_combines_supervised_cnn/"}, {"autor": "PuzzledProgrammer3", "date": "2019-08-27 21:29:05", "content": "[P] -----> photo !!!  realistic character art with stylegan /!/ model: [https://www.dropbox.com/s/y8petm9dd74uv9g/allface2.pkl](https://www.dropbox.com/s/y8petm9dd74uv9g/allface2.pkl)\n\ninterpolation: https://twitter.com/roadrunning01/status/1156368137669005314?s=20", "link": "https://www.reddit.com/r/MachineLearning/comments/cwavku/p_photo_realistic_character_art_with_stylegan/"}, {"autor": "lantern_lol", "date": "2019-08-27 20:38:11", "content": "[D] Is there an accepted state-of-the-art for Video Action Localisation/Region Suggestion? /!/ I am working on a project which involves video action recognition, and am using the fantastic [I3D](https://arxiv.org/abs/1705.07750) approach.\n\nHowever, I am now interested in localising the region of video which contains the action being classified (i.e. with a bounding box). For example, if my I3D network classifies a segment of video containing the action of a human \"Eating an Apple\", I now want to draw a bounding box over the person eating the apple in each frame of video. Note that I am not interested in drawing a region around \"Apple\" or \"Human\", but instead the region in which the action itself is being performed.\n\nI am familiar with similar approaches for -----> image !!!  classification (e.g. YOLO), but am having trouble finding work in the video domain for actions. Can anyone point me to some good papers which cover this if they exist?", "link": "https://www.reddit.com/r/MachineLearning/comments/cwa713/d_is_there_an_accepted_stateoftheart_for_video/"}, {"autor": "rybakovcom", "date": "2019-08-27 11:43:49", "content": "[P] I applied Mark Zuckerberg's face to Facebook emojis /!/ Seeing the post on photorealistic emojis reminded me of a project I did last year: [Zuckerberg Emojis](https://rybakov.com/blog/zuckerberg_emojis/)\n\n&amp;#x200B;\n\n[Sad Mark](https://i.redd.it/669tx1a7azi31.jpg)\n\nWhy? Well, facebook forces us to use quite specific representation of emotions to react to things. In a way, these emojis become our facial expression. So it would only fair to apply the same expression to Zuckerberg's face.\n\nI used CNNMRF, Deep -----> Image !!!  Analogy and jcjohnsons neural style in sequence to apply the face and upscale it to a good resolution.\n\n[ \t1.Original 2.CNNMRF result 3. Deep Image Analogy output 4.Upscaled with Neural-style ](https://i.redd.it/yd0dmyoyazi31.jpg)\n\nThe full write-up with all emojis is here: [https://rybakov.com/blog/zuckerberg\\_emojis/](https://rybakov.com/blog/zuckerberg_emojis/)", "link": "https://www.reddit.com/r/MachineLearning/comments/cw39dx/p_i_applied_mark_zuckerbergs_face_to_facebook/"}, {"autor": "iocuydi", "date": "2019-08-27 01:59:25", "content": "[D] Those who do computer vision, how do you handle dataset management? /!/ Hi all! I'm curious about the best ways to manage large -----> image !!!  and video datasets for computer vision projects.\n\nI'm an ML engineer on a team of \\~10, supported by 5 data labellers.\n\nI was wondering how other teams in CV space manage:\n\n\\-Storing the datasets in a central (hosted?) location, and version controlling them as needed, with minimal overhead\n\n\\-Allowing for querying and visual exploration of the datasets for quick adjustment or examination of labels\n\n\\-Efficiently pulling a dataset or subset of a dataset to a local machine.\n\n\\-Automating the flow of datasets as much as possible, i.e. train x model on y subset of z dataset.\n\n\\-Compressing less frequently used data as much as possible for \"cold storage\" and handling uncompression/recompression when the data is needed for training or when new data is added\n\n&amp;#x200B;\n\nSo far we've used 3 solutions:\n\n1. Storing everything on a local machine sitting under an unoccupied desk, and everybody manually updated the data there\n2. Storing compressed tar files of the data on AWS storage and retrieving/updating it manually every so often.\n3.  Assigning one of the data labellers to spend some time as a \"dataset manager\" and try to do this for us.\n\nEach of these has had it's own set of problems, and I feel I waste a lot of time dealing with the overhead of this stuff.\n\nHow do you guys deal with this situation? Is there an \"industry standard\" correct way of managing this stuff? Like a github for CV? At places like Waymo/Tesla for example where they are constantly growing and updating their dataset to improve weak points, I would think an elegant solution for this has been devised.\n\nOne caveat is that I'd like to avoid using things like AWS and Azure ML \"low code\" services that might do some data management for you but then take away most of the freedom of working in TF/Pytorch, and make the model into a black box.", "link": "https://www.reddit.com/r/MachineLearning/comments/cvxs5c/d_those_who_do_computer_vision_how_do_you_handle/"}, {"autor": "JonathanFly", "date": "2019-08-26 20:35:22", "content": "[P] I applied the recent 'Progressive Face Super-Resolution via Attention to Facial Landmark' to create '-----> photo !!! -realistic' Emojis and Emotes. /!/ *Progressive Face Super-Resolution via Attention to Facial Landmark* [arxiv.org](https://arxiv.org/abs/1908.08239) is a machine learning model trained to reconstruct face images from tiny 16\u00d716 pixel input images, scaling them up to 128\u00d7128 with nearly photo-realistic results. I tried running emojis, Twitch emotes, and a few game sprites through it. \n\nI did have to do quite a fit of cherry picking, and I also iteratively ran the output back into the inputs to encourage the model the add human features. Some of the best examples:\n\nhttps://i.redd.it/porrhn5gsui31.png\n\n I also created a (very sloppy) [Colab Version](https://gist.github.com/JonathanFly/80b669a72bf624d17b56a1cfec742588#file-progressivefacesuperresolutiondemo-ipynb) of the paper's github demo if you want to try this yourself.", "link": "https://www.reddit.com/r/MachineLearning/comments/cvtprp/p_i_applied_the_recent_progressive_face/"}, {"autor": "the-fire-fist", "date": "2019-02-14 15:59:52", "content": "[R] MISO: Mutual Information Loss with Stochastic Style Representations for Multimodal -----> Image !!! -to------> Image !!!  Translation /!/ &amp;#x200B;\n\n[Results of multimodal image-to-image translation on multiple datasets](https://i.redd.it/qc38ogz33kg21.png)\n\narXiv : [https://arxiv.org/abs/1902.03938](https://arxiv.org/abs/1902.03938)", "link": "https://www.reddit.com/r/MachineLearning/comments/aql7zc/r_miso_mutual_information_loss_with_stochastic/"}, {"autor": "the-fire-fist", "date": "2019-02-14 15:40:34", "content": "[R] MISO: Mutual Information Loss with Stochastic Style Representations for Multimodal -----> Image !!! -to------> Image !!!  Translation", "link": "https://www.reddit.com/r/MachineLearning/comments/aql0sb/r_miso_mutual_information_loss_with_stochastic/"}, {"autor": "the-fire-fist", "date": "2019-02-14 15:33:38", "content": "[R] MISO: Mutual Information Loss with Stochastic Style Representations for Multimodal -----> Image !!! -to------> Image !!!  Translation", "link": "https://www.reddit.com/r/MachineLearning/comments/aqky99/r_miso_mutual_information_loss_with_stochastic/"}, {"autor": "the-fire-fist", "date": "2019-02-14 15:06:01", "content": "[R] MISO: Mutual Information Loss with Stochastic Style Representations for Multimodal -----> Image !!! -to------> Image !!!  Translation /!/ &amp;#x200B;\n\nhttps://i.redd.it/4ijx7wobujg21.png", "link": "https://www.reddit.com/r/MachineLearning/comments/aqknzt/r_miso_mutual_information_loss_with_stochastic/"}, {"autor": "the-fire-fist", "date": "2019-02-14 14:44:11", "content": "[R] MISO: Mutual Information Loss with Stochastic Style Representations for Multimodal -----> Image !!! -to------> Image !!!  Translation", "link": "https://www.reddit.com/r/MachineLearning/comments/aqkg7j/r_miso_mutual_information_loss_with_stochastic/"}, {"autor": "skariel", "date": "2019-02-14 09:42:08", "content": "[R] Neural Networks seem to follow a puzzlingly simple strategy to classify -----> image !!! s /!/ &amp;#x200B;\n\n[This Reasearch](https://medium.com/bethgelab/neural-networks-seem-to-follow-a-puzzlingly-simple-strategy-to-classify------> image !!! s-f4229317261f) shows many similarities between deep convolutional -----> image !!!  classification networks and good old \"bag of features\". This gives some really interesting insight into the inner workings of current CNNs", "link": "https://www.reddit.com/r/MachineLearning/comments/aqi3sw/r_neural_networks_seem_to_follow_a_puzzlingly/"}, {"autor": "tdls_to", "date": "2019-02-14 07:34:44", "content": "[R] TDLS: Tensor Field Networks: Rotation- and translation-equivariant neural networks for 3D point clouds /!/ * Video: [https://www.youtube.com/watch?v=COgEuIAnhtk](https://www.youtube.com/watch?v=COgEuIAnhtk)\n* Slides: [https://tdls.a-i.science/static/slides/20190211\\_PengCheng\\_ChrisDryden.pdf](https://tdls.a-i.science/static/slides/20190211_PengCheng_ChrisDryden.pdf)\n* paper: [https://arxiv.org/abs/1802.08219](https://arxiv.org/abs/1802.08219)\n* Event page: [https://tdls.a-i.science/events/2019-02-11/](https://tdls.a-i.science/events/2019-02-11/)\n\n# Motivation\n\nIn this session we will discuss the relationship between data augmentation, invariant/equivariant features and the abstract concept of convolution layers, specifically, how this abstraction can be extended to devise concrete neural network architectures that are robust to diverse data and augmentation types (all of which are published after 2016). We will focus on the latest of the series, designed to handle spatial graph data augmentable by 3d translations &amp; rotations. In the end, we will showcase it's applications in molecule analysis and autonomous flight. We will focus on the latest of the series, designed to handle spatial graph data augmentable by 3d translations &amp; rotations. In the end, we will showcase it's applications in molecule analysis and autonomous flight.\n\n# Discussion points\n\n* Paper did not go in depth about the information stored in the points\n   * More applications are possible. Would the chemical dataset work with chemical properties.\n* Can it be applied to more traditional 3d------> image !!!  sets?\n   * Eg: Autonomous Driving\n* Can we incorporate in this architecture other types of symmetries?\n   * Ex: Mirror Symmetries, R-L Enantiomers\n* Can this be applied to Neural-ODE\u2019s?\n\n[Subscribe to TDLS YouTube Channel](https://www.youtube.com/c/TorontoDeepLearningSeries?view_as=subscriber&amp;sub_confirmation=1)", "link": "https://www.reddit.com/r/MachineLearning/comments/aqhb55/r_tdls_tensor_field_networks_rotation_and/"}, {"autor": "zalamandagora", "date": "2019-11-11 19:03:25", "content": "MultiGPU: Lower performance with increased batch size, number of workers, and max queue. /!/ [If anyone knows a very active forum for discussing things like this, please let me know in the comments.]\n\nI'm getting unexpected results when I'm tweaking the training parameters of my run.\n\n* CNN model (~1M parameters, consecutive 2D CNN into two FC layers)\n\n* Data set: 20k images from CelebA with 218x178x3 images\n\n* GCP VM with 16 cores, 104 GB RAM, 4 K80 GPUs.\n\n* VM Image: The -----> image !!!  is the latest deep learning -----> image !!!  w/ Ubuntu and tf 1.15.\n\nI can't seem to get GPU utilization above 60%, or CPU utilization above 7%. It is really weird to me that I get the fastest epoch speeds with 4 workers (for pre-processing and batch assembly), max queue = 2, and a batch size of 32. It seems to me that increasing all of these should increase performance and help the GPU run at max capacity\n\n| Batch size | Workers | Max Queue | Epoch 1 (s) | Epoch 2 (s) |\n|:----------:|:-------:|:---------:|:-----------:|:-----------:|\n|     16     |    4    |     4     |     135     |      76     |\n|     32     |    4    |     0     |     134     |      56     |\n|     32     |    4    |     2     |     136     |      49     |\n|     32     |    4    |     4     |     136     |      49     |\n|     32     |    4    |     16    |     135     |      51     |\n|     32     |    8    |     4     |     137     |      56     |\n|     32     |    8    |     8     |     138     |      53     |\n|     32     |    16   |     0     |     197     |      62     |\n|     32     |    16   |     16    |     139     |      57     |\n|     32     |    32   |    400    |     161     |      70     |\n|     64     |    4    |     4     |     137     |      61     |\n|     64     |    16   |    400    |     196     |      61     |\n|     64     |    32   |    400    |     200     |      72     |\n|     64     |    64   |    400    |     203     |      72     |\n|     512    |    4    |     4     |     145     |      56     |\n|     512    |    8    |     4     |     154     |      67     |\n|     764    |    4    |     4     |     158     |      62     |\n|     512    |    8    |     4     |     154     |      67     |\n|     764    |    4    |     4     |     158     |      62     |\n \nModel is built like so:\n\nwith tf.device('/cpu:0'):\n       endec = Model(inputs = E_input, outputs = E_outputs)\nendec = multi_gpu_model(endec, gpus = GPUS)\n\nTraining like so:\n\n        run_history = endec.fit_generator(tr_gen, steps_per_epoch = tr_gen.total_batches,\n            epochs = EPOCHS, validation_data = val_gen, validation_steps = val_gen.total_batches,\n            shuffle = True, verbose = 2, max_queue_size = MAX_QUEUE, \n            use_multiprocessing = MULTI_THREAD, workers = WORKERS)\n\nData fetched by generator like so:\n\n    def __load_img_batch(self, batch_list):\n        no_files = len(batch_list)\n\n        x_data = np.empty((no_files, *self.img_dims))\n        y_data = np.empty((no_files, self.labels.num_labels))\n        for i in range(no_files):\n            x_data[i,:,:,:] = cv2.cvtColor(cv2.imread(self.dir_path + batch_list[i]), cv2.COLOR_BGR2RGB)\n            y_data[i,:] = self.labels.lookup(batch_list[i])\n        \n        return x_data, y_data\n\nI would really appreciate any insight into why this would be.", "link": "https://www.reddit.com/r/MachineLearning/comments/dux2rm/multigpu_lower_performance_with_increased_batch/"}, {"autor": "Omrinachmani", "date": "2019-07-15 00:31:55", "content": "Can a machine learning -----> image !!!  recognition or digit recognition algorithm decipher the number on the side of this taxi? Lost an important suitcase there on Friday and in need of all help possible", "link": "https://www.reddit.com/r/MachineLearning/comments/cdajir/can_a_machine_learning_image_recognition_or_digit/"}, {"autor": "MasterScrat", "date": "2019-07-14 10:11:13", "content": "[D] Tools to annotate images for segmentation? /!/ What do you recommend for -----> image !!!  annotation? I'm looking for a web-based interface to let non-technical users (MDs) annotate X-rays for image segmentation.\n\nThey should then be exportable to COCO format.\n\nBest I found so far is this, but meh: http://labelme.csail.mit.edu/Release3.0/", "link": "https://www.reddit.com/r/MachineLearning/comments/cd1fo1/d_tools_to_annotate_images_for_segmentation/"}, {"autor": "FreddyShrimp", "date": "2019-07-13 13:25:26", "content": "Still -----> image !!!  to a 3D -----> image !!! ; repository / research paper /!/ I know there are research papers out there that describe methods to convert a still image into a 3D object (like those used in CAD drawings). \n\n&amp;#x200B;\n\nHowever, I'm wondering if anyone knows of an existing example; paper and/or repository that can adjust a still 2D image into a 3d image pretty much like facebook does with it's [3d image feature](https://www.pocket-lint.com/apps/news/facebook/146017-facebook-3d-photos-how-to-create-share-and-view-3d-photos). Another example would be the google Snapseed app with the \"headpose\" option (although I think this is more of a vector transformation, whereas the facebook example actually deploys ML I believe).\n\n&amp;#x200B;\n\nWhat I have done so far, without any succes:\n\n\\- Googled for examples\n\n\\- Searched github\n\n\\- Searched google scholar.\n\n&amp;#x200B;\n\nSo, if anyone knows of a good example, please let me know and share it with me!\n\n&amp;#x200B;\n\nThanks in advance, and all the best!\n\nFred", "link": "https://www.reddit.com/r/MachineLearning/comments/ccpb6x/still_image_to_a_3d_image_repository_research/"}, {"autor": "BlackHawk1001", "date": "2019-07-13 11:45:22", "content": "Autoencoder on heat maps /!/ Hello everybody\n\nI have time sequences of 2D heat maps. For different people I have heat maps over time. For each person I have around 720 heat maps and in total around 50'000 heat maps. \n\nHere is an example of a heat map visualizing eye tracking data:\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n*Processing img 8w7enske62a31...*\n\n&amp;#x200B;\n\nNow, I would like to train an autoencoder on these heat maps to learn a meaningful low dimensional representation. I will use Keras in Python for the implementation.\n\nI thought about using a CNN for encoder and decoder (but I'm unsure about the number of layers and the size of the layers). Another option could be to use a cycleGAN (pix2pix) or also taking time into account using LSTM but I don't know how to use LSTM for 2D -----> image !!!  data. I have also heard that attention layers might be interesting.\n\nWhat architectures and configurations are reasonable to use?", "link": "https://www.reddit.com/r/MachineLearning/comments/ccofx2/autoencoder_on_heat_maps/"}, {"autor": "downtownslim", "date": "2019-07-12 15:20:55", "content": "[R] \"We also 3D-print our adversarial objects and perform physical experiments to illustrate that such vulnerability exists in the real world\" - Adversarial Objects Against LiDAR /!/ Abstract:\n\n&gt;Deep neural networks (DNNs) are found to be vulnerable against adversarial examples, which are carefully crafted inputs with a small magnitude of perturbation aiming to induce arbitrarily incorrect predictions. Recent studies show that adversarial examples can pose a threat to real-world security-critical applications: a \"physical adversarial Stop Sign\" can be synthesized such that the autonomous driving cars will misrecognize it as others (e.g., a speed limit sign). However, these -----> image !!! -space adversarial examples cannot easily alter 3D scans of widely equipped LiDAR or radar on autonomous vehicles. In this paper, we reveal the potential vulnerabilities of LiDAR-based autonomous driving detection systems, by proposing an optimization based approach LiDAR-Adv to generate adversarial objects that can evade the LiDAR-based detection system under various conditions. We first show the vulnerabilities using a blackbox evolution-based algorithm, and then explore how much a strong adversary can do, using our gradient-based approach LiDAR-Adv. We test the generated adversarial objects on the Baidu Apollo autonomous driving platform and show that such physical systems are indeed vulnerable to the proposed attacks. We also 3D-print our adversarial objects and perform physical experiments to illustrate that such vulnerability exists in the real world.\n\n&amp;#x200B;\n\nWebsite: [https://sites.google.com/view/lidar-adv](https://sites.google.com/view/lidar-adv)\n\nPaper: [https://arxiv.org/abs/1907.05418](https://arxiv.org/abs/1907.05418)", "link": "https://www.reddit.com/r/MachineLearning/comments/ccc7p4/r_we_also_3dprint_our_adversarial_objects_and/"}, {"autor": "nitrodolphin", "date": "2019-11-28 00:28:38", "content": "[D] Has anyone used context to improve object detection and -----> image !!!  classification? /!/ We do a lot of image classification using the Tensorflow Object Detection API.\n\nOur images often appear in groups, e.g. a cluster of fish swimming by a camera. Ofter our model will recognize some of the fish - but not all of them. This is obviously a mistake a human would never make.\n\nI am researching whether there are any examples of object detectors/image classifiers using context to improve results? I.e. knowing that there are three fish swimming through would increase the model's propensity to find a fourth nearby?\n\nAnother way to potentially attack this problem would be to identify clusters of objects -&gt; then reexamine the cluster only to identify the number of objects within the cluster. The complexity with this issue is that some of the objects we are examining appear in clusters, while others do not.\n\nSo to summarize, my two questions are:\n\n* Is there research on, and how would you suggest I go about improving an object detector by using contextual features within an image?\n* In situations where there are clusters of objects, would you recommend recognizing those clusters as individual images and then subsequently processing them to identify the number of images within the cluster? Are there examples / is there research on this?\n\nThanks in advance - obviously doing my own research as well, but keen to hear if the community has any thoughts/examples!", "link": "https://www.reddit.com/r/MachineLearning/comments/e2popg/d_has_anyone_used_context_to_improve_object/"}, {"autor": "Napoleon-1804", "date": "2019-11-27 20:32:03", "content": "[Discussion] Comparing UK ML-Neuro Labs: Deepmind, Google Brain, Microsoft Cambridge, etc. /!/ I'm a masters student in the UK hoping to get 1-2 years of research experience at the intersection of machine learning and neuroscience (so not so much -----> image !!!  recognition, speech recognition, etc.)\n\nThe two academic options that immediately came to mind were the Gatsby Comp. Neuroscience at UCL and Cambridge's Computational Biological Learning group. I'm not familiar with industry labs and would like your help in comparing Deepmind, Google Brain (UK site), Microsoft Research Cambridge, etc. Which one of these offers the most relevant research in terms of machine learning applied to neuroscience as I am hoping to enter academia for computational neuroscience? A lab that publishes well is also a nice plus as always. Thanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/e2ma3q/discussion_comparing_uk_mlneuro_labs_deepmind/"}, {"autor": "youali", "date": "2019-11-27 14:10:30", "content": "[R] SuperGlue: Learning Feature Matching with Graph Neural Networks /!/ Arxiv: https://arxiv.org/abs/1911.11763\n\nAbstract: This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from -----> image !!!  pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems.", "link": "https://www.reddit.com/r/MachineLearning/comments/e2giis/r_superglue_learning_feature_matching_with_graph/"}, {"autor": "buzztowns", "date": "2019-11-27 10:55:47", "content": "Know difference between -----> image !!!  segmentations and classifications in -----> image !!!  processing", "link": "https://www.reddit.com/r/MachineLearning/comments/e2ei1a/know_difference_between_image_segmentations_and/"}, {"autor": "greenvalley0101", "date": "2019-11-26 16:48:44", "content": "Input blur -----> image !!!  instead of pure noise to the GAN generator /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/e20m7j/input_blur_image_instead_of_pure_noise_to_the_gan/"}, {"autor": "Awill1aB", "date": "2019-08-19 18:03:28", "content": "[D] Is vision a solved problem? /!/ I am curious, as I've been thinking about this for a while. To me, it seems as though we seem to be making improvements, but there is not a ton left to solve within this sub field. I don't claim to be an expert by any stretch, but through all of the advancements we have made, we are capable of object detection, classification, -----> image !!!  captioning for the contents of the -----> image !!! , -----> image !!!  generation, and as we are closing in on depth perception and improvements on the 3D space, I feel like we are finding new applications for the tools we already have.\n\nThoughts?\n\nI would love for someone to step in, call me a simpleton and give me all the reasons I am wrong and all of the problems we have yet to address within this space. :)", "link": "https://www.reddit.com/r/MachineLearning/comments/cskt2l/d_is_vision_a_solved_problem/"}, {"autor": "debomastet335", "date": "2019-08-19 14:29:42", "content": "Can anyone suggest me a good article on FaceApp's -----> image !!!  filters with ai? /!/ Age filter,Smile filter,Hairstyle filter,Gender Swap filter,Skin tone Lightning filter etc.\n\nHow this works?\nAny article or any information you know, please help me as I am writing a article on this topic.", "link": "https://www.reddit.com/r/MachineLearning/comments/cshu53/can_anyone_suggest_me_a_good_article_on_faceapps/"}, {"autor": "init__27", "date": "2019-08-19 08:53:39", "content": "Interview with Kaggle GrandMaster, Dr. Vladimir Iglovikov about Albumentations: a fast -----> image !!!  augmentations library /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/cseeu2/interview_with_kaggle_grandmaster_dr_vladimir/"}, {"autor": "0207Miya", "date": "2019-08-19 06:31:22", "content": "Stretch -----> film !!!  making machine /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/csd92l/stretch_film_making_machine/"}, {"autor": "donb1988", "date": "2019-08-19 04:10:56", "content": "[D] Quant vs. \"Regular\" Post-PhD Career Trajectories /!/ Hi all, profuse apologies in advance if this is not the correct place to ask this question. I've attempted to look around for information (both online and offline), but perhaps I'm not hitting the right keywords, so I though I'd give this a try.\n\nMy question is specifically about \"quant researcher\" type careers, and what the pros/cons and other considerations are when taking up a job like that.\n\nMy understanding is that post-PhD (in ML, or whatever the department is that accommodated your ML research for the bulk of the PhD), the majority of people aim for (1) \"research scientist\" roles in industry, or (2) focus more on an academic career, or (3) both, simultaneously. Obviously this is a generalization, and there are many more things you can do with any STEM PhD for that matter, but these options seem to be the goals of many people.\n\nWhat about (4) quant jobs in finance, such as in small/large trading / hedge funds / asset management / etc.? They frequently appear to offer extremely attractive packages, and require no experience in finance. However, for the most part the community seems to be rather separate from the (1) through (3) crowd I mentioned above, so I am unable to get a coherent -----> picture !!!  of why some folks choose one path versus another, and the various things you should consider (e.g. long-term career trajectory, exit options, etc.) when taking your first job / internship in finance during / after your PhD.\n\nApologies for the naive question, and apologies again if this is not the right place to ask this kind of thing. Thank you in advance for your kind advice!", "link": "https://www.reddit.com/r/MachineLearning/comments/csbyiv/d_quant_vs_regular_postphd_career_trajectories/"}, {"autor": "viccpopa", "date": "2019-08-18 18:33:21", "content": "[P] Pytorch Implementation of GANimation with pretrained weights. /!/ Hi all!\n\n**TL;TR** I shared on GitHub an [implementation](https://github.com/vipermu/ganimation) of a Conditional GAN model called GANimation. I include pretrained weights and a preprocessed dataset as well.\n\n&amp;#x200B;\n\nA few months ago I became really interested in a project called [GANimation](https://arxiv.org/abs/1807.09251). The authors (Pumarola et al.) of this project were able to train a Conditional GAN model capable of modifying facial expressions in a continuous way. This sounded really interesting to me and I wanted to play with the model, but when I tried the author's implementation in my computer I had problems with the training process and I didn't find any pre-trained weights. As at that moment I also wanted to learn PyTorch, I decided to create my own implementation. As this project was really similar to StarGAN I started cloning their [repo](https://github.com/yunjey/stargan) and I used it as baseline.\n\nIn this implementation I provide pretrained models and a preprocessed dataset to facilitate the use of this model. I also included the functions to create the following video.\n\n[Applying the expression of the face in the first column to each -----> image !!!  in the top row.](https://i.redd.it/j4xsfahw29h31.gif)\n\nAlthough there's a lot to improve and clean in the code, I hope it can be useful for anyone that wants to use this model.", "link": "https://www.reddit.com/r/MachineLearning/comments/cs52iy/p_pytorch_implementation_of_ganimation_with/"}, {"autor": "automatedredditor", "date": "2019-08-11 10:55:28", "content": "[D] CNN Image Segmentation: Why do UNET-like architectures outperform sliding-window approaches? /!/ I'm writing a thesis that heavily focuses on semantic segmentation of biomedical images.\n\nI'm reviewing different segmentation approaches, identifying two main approach branches:\n\n* A **sliding window**\\-like approach: a classification network is used over different patches of original -----> image !!!  to reconstruct a pixel-by-pixel estimates of the probability maps.\n* A **full-image** approach: like the FCNN and UNET approach, rely on fully convolutional architectures and the upscaling phase is incorporated in the network itself using transposed convolutions.[https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)\n\nThe second approach clearly outperforms the first one. I have a vague hunch on why this happens: my hypothesis is that the transposed-convolution operations, being at their core local operations, force local criteria on the segmentation of close pixels so that pixel contiguity is heavily encouraged in the fully convolutional case.\n\nI do not find this kind of explanation satisfying because of two reasons:\n\n1. I do not have papers or real data to support this: I cannot seem to find any paper on the theme.\n2. The sliding-window approach has a built-in form of local consistency as well: if overlapping windows share most of the pixels it's reasonable to think that - given the network is not totally chaotic and shows enough linearity - the outputs would be similar.\n\nDo anyone have a bit of insight or source on any of this? Any contribution, even brainstorming or unsupported hypothesis (like mine) is well appreciated.", "link": "https://www.reddit.com/r/MachineLearning/comments/covedn/d_cnn_image_segmentation_why_do_unetlike/"}, {"autor": "MohamedRashad", "date": "2019-08-10 13:15:59", "content": "Graduation Project ideas ? /!/ I would love to hear some ideas for my GP which uses Deep Learning in its core to make something helpful to others.\n\nI was thinking about drug discovery, medical -----> image !!!  segmentation and brain-machine interface but i am afraid they become harder than i think .... so if someone here will make a project in one of these areas how he would approach it and what application he would aim to build \\^\\^\n\n&amp;#x200B;\n\nNote: I am open to other ideas too as i have time to study and plan for them.", "link": "https://www.reddit.com/r/MachineLearning/comments/cohwdo/graduation_project_ideas/"}, {"autor": "EverydayQuestion", "date": "2019-08-09 21:11:19", "content": "How do Histogram of Oriented Gradients descriptors + Linear Support-Vector Machine work together to localize a face in an -----> image !!! ? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/co8lta/how_do_histogram_of_oriented_gradients/"}, {"autor": "fireflav", "date": "2019-10-26 09:15:28", "content": "Selecting features for malware analysis /!/ I am trying to build a classifier that detects if I have a malaware by predicting the provenance compiler. To do so I have a dataset composed of assembly code in json format :\n\n&amp;#x200B;\n\n\\[!\\[enter -----> image !!!  description here\\]\\[1\\]\\]\\[1\\]\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nIn particular, I want to select as features the instructions, so the push, mov, jmp,..etc and create a feature vector that contains the number of times a feature appears. So, I want to apply the bag of words. To do so my code is the following:\n\n&amp;#x200B;\n\n`#libraries`\n\n`import numpy as np`\n\n`import pandas as pd`\n\n`import json as j`\n\n`import re`\n\n`import nltk`\n\n`from nltk.tokenize import word_tokenize`\n\n&amp;#x200B;\n\n`from sklearn.model_selection import train_test_split`\n\n`from sklearn.feature_extraction.text import CountVectorizer`\n\n`from sklearn.naive_bayes import *`\n\n`from sklearn.metrics import confusion_matrix, classification_report`\n\n`from sklearn import svm`\n\n&amp;#x200B;\n\n`#for visualizing data`\n\n`import matplotlib.pyplot as plt`\n\n`import seaborn as sns; sns.set(font_scale=1.2)`\n\n&amp;#x200B;\n\n`%matplotlib inline`\n\n&amp;#x200B;\n\n`json_data = None;`\n\n`with open('training_dataset.jsonl') as data_file:`\n\n`lines = data_file.readlines()`\n\n`joined_lines = \"[\" + \",\".join(lines)+\"]\"`\n\n\n\n`json_data = j.loads(joined_lines)`  \n\n&amp;#x200B;\n\n`data = pd.DataFrame(json_data)`\n\n`data.head()`\n\n&amp;#x200B;\n\n   \n\n&amp;#x200B;\n\n`vect = CountVectorizer()`\n\n&amp;#x200B;\n\n`data['instructions'] = data['instructions'].apply(lambda x: ' '.join(x))`\n\n&amp;#x200B;\n\n`vect.fit_transform(data['instructions'])`\n\n&amp;#x200B;\n\n`a =vect.vocabulary_`\n\n`a`\n\n&amp;#x200B;\n\nand from this I obtain a dictionary:\n\n&amp;#x200B;\n\n\\[!\\[enter image description here\\]\\[2\\]\\]\\[2\\]\n\n&amp;#x200B;\n\nwith more and more key, value pairs. At this point what I tried to do is to eliminate the registers so my idea to do this is to iterate the dictionary and eliminate the keys that have numbers, and so I have written the following:\n\n&amp;#x200B;\n\n`def hasNumbers(inputString):`\n\n`return any(char.isdigit() for char in inputString)`\n\n&amp;#x200B;\n\n`for k in list(a.keys()):`\n\n`if hasNumbers(k):`\n\n`del a[k]`\n\n&amp;#x200B;\n\nbut actually it wasn't a really good idea because I have also registers without numbers, for example rdi.\n\n&amp;#x200B;\n\nAt this point, I don't know how to move, and I am not sure I am going in the correct way. Can somebody please help me? Thank's in advance.\n\n&amp;#x200B;\n\n\\[EDIT 2\\] \n\nNow I am trying to take the json file, not opening it with pandas, but open it as a dictionary, so:\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\[!\\[enter image description here\\]\\[3\\]\\]\\[3\\]\n\n&amp;#x200B;\n\nand my intenction is try to split, as suggested, each key in such a way to eliminate the registers. So, my code for this is :\n\n&amp;#x200B;\n\n`for key in json_data:`\n\n`splitted = key.split[0]`\n\n&amp;#x200B;\n\nbut I recieve the following error message:\n\n&amp;#x200B;\n\n`AttributeError: 'dict' object has no attribute 'split'`\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nI am getting really confused about how to operate now.\n\n&amp;#x200B;\n\n\\[EDIT 3\\] or I tried to do the following:\n\n&amp;#x200B;\n\n`for v in json_data.values():`\n\n`splitted = v.split[0]`\n\n&amp;#x200B;\n\nbut it gives me the following error message:\n\n&amp;#x200B;\n\n`AttributeError: 'list' object has no attribute 'values'`\n\n&amp;#x200B;\n\na problem I think is the fact that the various push r12,.. etc are in a list, so I don't know how to do.\n\n&amp;#x200B;\n\n\\[EDIT 4\\] I actually tried to solve the above problem this way:\n\n&amp;#x200B;\n\n`for v in json_data[0].values():`\n\n`splitted = v.split[0]`\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nbut again I get the error message:\n\n&amp;#x200B;\n\n`AttributeError: 'list' object has no attribute 'split'`\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n  \\[1\\]: [https://i.stack.imgur.com/sfq6y.png](https://i.stack.imgur.com/sfq6y.png)\n\n  \\[2\\]: [https://i.stack.imgur.com/z8Bxx.png](https://i.stack.imgur.com/z8Bxx.png)\n\n  \\[3\\]: [https://i.stack.imgur.com/Py0pV.png](https://i.stack.imgur.com/Py0pV.png)", "link": "https://www.reddit.com/r/MachineLearning/comments/dnbj8l/selecting_features_for_malware_analysis/"}, {"autor": "knowledgelimit", "date": "2019-10-26 05:49:28", "content": "[D] are reported NLL scores in papers an average across datapoints? /!/ Papers commonly report NLL scores, such as a value of around 3 for PixelCNN. I believe this is bits-per-dimension,\n\nbut, is it: \n\na) an average across all the datapoints in the test set, or   \nb) a sum across all datapoints, or   \nc) the best score on an individual datapoint?\n\nOr maybe my question makes not sense.\n\nExplaining furher In the case of PixelCNN, \"datapoint\" = -----> image !!! , so I believe the NLL of trained model can be evaluated by summing the logs of the conditional probabilities of each pixel (conditioned on the neighborhood in the pixelcnn scheme), plus the marginal probability for the first pixel.  This gives the overall LL for a single image from the test set, but what about the other images.", "link": "https://www.reddit.com/r/MachineLearning/comments/dn9u64/d_are_reported_nll_scores_in_papers_an_average/"}, {"autor": "worldnews_is_shit", "date": "2019-10-25 21:29:02", "content": "[R] Attacking Optical Flow /!/ Attacking Optical Flow\n\n\nTLDR:  Corrupting a small patch of less than 1% of the -----> image !!!  size lead to noisy flow estimates that extend beyond the region of the attack, even erasing the motion of objects in the scene in some cases\n\nAnurag Ranjan,\u00a0Joel Janai,\u00a0Andreas Geiger,\u00a0Michael J. Black\n\n(Submitted on 22 Oct 2019)\n\nDeep neural nets achieve state-of-the-art performance on the problem of optical flow estimation. Since optical flow is used in several safety-critical applications like self-driving cars, it is important to gain insights into the robustness of those techniques. Recently, it has been shown that adversarial attacks easily fool deep neural networks to misclassify objects. The robustness of optical flow networks to adversarial attacks, however, has not been studied so far. In this paper, we extend adversarial patch attacks to optical flow networks and show that such attacks can compromise their performance. We show that corrupting a small patch of less than 1% of the image size can significantly affect optical flow estimates. Our attacks lead to noisy flow estimates that extend significantly beyond the region of the attack, in many cases even completely erasing the motion of objects in the scene. While networks using an encoder-decoder architecture are very sensitive to these attacks, we found that networks using a spatial pyramid architecture are less affected. We analyse the success and failure of attacking both architectures by visualizing their feature maps and comparing them to classical optical flow techniques which are robust to these attacks. We also demonstrate that such attacks are practical by placing a printed pattern into real scenes.\n\nAbs: https://arxiv.org/abs/1910.10053\n\nSite: https://flowattack.is.tue.mpg.de", "link": "https://www.reddit.com/r/MachineLearning/comments/dn44ok/r_attacking_optical_flow/"}, {"autor": "internet_explorer_13", "date": "2019-10-25 02:07:15", "content": "How pooling in convolutional neural networks is different than regular -----> image !!!  scaling? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/dmqpj4/how_pooling_in_convolutional_neural_networks_is/"}, {"autor": "ockidocki", "date": "2019-02-26 21:53:32", "content": "[D] Lightweight, high-level code for using a pre-trained -----> image !!!  classifier in an object detection system (e.g. YOLO) /!/ I've trained an image classifier with a small, custom dataset. I used transfer learning with a pre-trained ResNet model. \n\nI would like to use this image classifier for object detection, using it as a component of YOLO or a similar network.\n\nIs there lightweight, high-level code I can use for this?\n\nThank you for your help.", "link": "https://www.reddit.com/r/MachineLearning/comments/av53z7/d_lightweight_highlevel_code_for_using_a/"}, {"autor": "mosef18", "date": "2019-02-26 19:58:28", "content": "[P] -----> Image !!! -Based Airbnb Pricing Algorithm, Hey guys, I just created a machine learning blog and uploaded my first post/project, please let me know what you think.", "link": "https://www.reddit.com/r/MachineLearning/comments/av3scb/p_imagebased_airbnb_pricing_algorithm_hey_guys_i/"}, {"autor": "themlearning", "date": "2019-02-26 16:21:40", "content": "-----> Image !!! -Based Airbnb Pricing Algorithm, Hey guys, I just created a machine learning blog and uploaded my first post/project, please let me know what you think.", "link": "https://www.reddit.com/r/MachineLearning/comments/av18vw/imagebased_airbnb_pricing_algorithm_hey_guys_i/"}, {"autor": "Tamazy", "date": "2019-02-26 13:44:56", "content": "[R] BLOCK fusion for VQA and VRD (AAAI 2019), including a multimodal fusion library /!/ Hi there!\n\nWe recently open-sourced the code to reproduce the results of our paper as well as a [multimodal fusion library](https://github.com/Cadene/block.bootstrap.pytorch#fusions) available with pip install. It includes most of the fusion modules from the state-of-the-art such as [MCB](https://arxiv.org/abs/1708.03619), [MLB](https://arxiv.org/abs/1610.04325), [MFB](https://arxiv.org/abs/1708.01471), [MFH](https://arxiv.org/abs/1708.03619), [MUTAN](https://arxiv.org/abs/1705.06676), [BLOCK](https://arxiv.org/abs/1902.00038.pdf).\n\nThis pytorch library can be useful to fuse two vectors in the same space. For instance, in Visual Question Answering, one must fuse the -----> image !!!  and the question embeddings in the same bi-modal space; or, in Visual Relationship Detection, one must fuse the bounding boxes coordinates, class embeddings and visual features of the two objects:\n- [VQA example](https://github.com/Cadene/block.bootstrap.pytorch/raw/master/assets/VQA_block.png?raw=true)\n- [VRD example](https://github.com/Cadene/block.bootstrap.pytorch/raw/master/assets/VRD_block.png?raw=true)\n\nA further example of using this fusion library can be found in the [code](https://github.com/Cadene/murel.bootstrap.pytorch) of our last paper accepted to CVPR 2019 about [Multimodal Relational Reasoning for VQA](https://arxiv.org/abs/1902.09487).\n\nThanks for your interest!\nI hope that this code base will be useful for someone else :)\n\nBest\n ", "link": "https://www.reddit.com/r/MachineLearning/comments/auzlwh/r_block_fusion_for_vqa_and_vrd_aaai_2019/"}, {"autor": "Luolc", "date": "2019-02-26 05:22:20", "content": "[R] AdaBound: An optimizer that trains as fast as Adam and as good as SGD (ICLR 2019), with A PyTorch Implementation /!/ Hi! I am an undergrad doing research in the field of ML/DL/NLP. This is my first time to write a post on Reddit. :D\n\n&amp;#x200B;\n\nWe developed a new optimizer called **AdaBound**, hoping to achieve a faster training speed as well as better performance on unseen data. Our paper, *Adaptive Gradient Methods with Dynamic Bound of Learning Rate*, has been accepted by ICLR 2019 and we just updated the -----> camera !!!  ready version on open review.\n\n&amp;#x200B;\n\nI am very excited that a PyTorch implementation of AdaBound is publicly available now, and a PyPI package has been released as well. You may install and try AdaBound easily via `pip` or directly copying &amp; pasting. I also wrote a post to introduce this lovely new optimizer.\n\n&amp;#x200B;\n\nHere's some quick links:\n\n**Website:** [https://www.luolc.com/publications/adabound/](https://www.luolc.com/publications/adabound/)\n\n**GitHub:** [https://github.com/Luolc/AdaBound](https://github.com/Luolc/AdaBound)\n\n**Open Review:** [https://openreview.net/forum?id=Bkg3g2R9FX](https://openreview.net/forum?id=Bkg3g2R9FX)\n\n**Abstract:**\n\nAdaptive optimization methods such as AdaGrad, RMSprop and Adam have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. Though prevailing, they are observed to generalize poorly compared with SGD or even fail to converge due to unstable and extreme learning rates. Recent work has put forward some algorithms such as AMSGrad to tackle this issue but they failed to achieve considerable improvement over existing methods. In our paper, we demonstrate that extreme learning rates can lead to poor performance. We provide new variants of Adam and AMSGrad, called AdaBound and AMSBound respectively, which employ dynamic bounds on learning rates to achieve a gradual and smooth transition from adaptive methods to SGD and give a theoretical proof of convergence. We further conduct experiments on various popular tasks and models, which is often insufficient in previous work. Experimental results show that new variants can eliminate the generalization gap between adaptive methods and SGD and maintain higher learning speed early in training at the same time. Moreover, they can bring significant improvement over their prototypes, especially on complex deep networks. The implementation of the algorithm can be found at [https://github.com/Luolc/AdaBound](https://github.com/Luolc/AdaBound).\n\n&amp;#x200B;\n\nhttps://i.redd.it/9trhbha3lui21.png", "link": "https://www.reddit.com/r/MachineLearning/comments/auvj3q/r_adabound_an_optimizer_that_trains_as_fast_as/"}, {"autor": "downtownslim", "date": "2019-06-17 21:21:47", "content": "[P] Using AI to generate recipes from food images /!/ A new approach to generating recipes directly from food images that produces more compelling recipes than retrieval-based approaches, according to human judgment. Evaluated on the large-scale Recipe1M data set, this approach improves performance with respect to previous baselines for ingredient prediction. With this work, we aim to provide access to the preparation of a meal simply by inputting a food -----> image !!! .\n\n&amp;#x200B;\n\n[https://ai.facebook.com/blog/inverse-cooking/](https://ai.facebook.com/blog/inverse-cooking/)", "link": "https://www.reddit.com/r/MachineLearning/comments/c1tb5m/p_using_ai_to_generate_recipes_from_food_images/"}, {"autor": "bigDATAbig", "date": "2019-06-17 17:41:30", "content": "[D] Text character visual similarity algo? /!/ Hey, for an application I'm making I want to find the similarity between how two letters look. \n\nFor example, o and O have a high similarity but o and K do not. \n\nCan someone guide me on what sort of techniques I would use (not necessarily ML, but this sounds like a DL task) in order to find a similarity between the look of two characters? It could be any character in any language that's why I can't just do it manually. \n\nMy proposed algorithm is as follows:\n1. Accept 2 letters as argument\n2. Generate same size -----> image !!!  with same sized characters placed in them\n3. Compute the similarity between the two images somehow \n4. Get result \n\nHow would I do step 3? Any guidelines are appreciated. I'm currently looking into HOG classifiers, any other information is appreciated.", "link": "https://www.reddit.com/r/MachineLearning/comments/c1qko0/d_text_character_visual_similarity_algo/"}, {"autor": "EmielBoss", "date": "2019-06-17 00:44:02", "content": "[R] How can I improve my material segmentations? /!/ I am trying to perform material segmentation (essentially semantic segmentation with respect to materials) on street-view imagery. My datasets only has ground truth for select regions, so not all pixels have a label, and I calculate loss and metrics only within these ground truth regions. I use \\[Semantic FPN\\]([https://arxiv.org/pdf/1901.02446.pdf](https://arxiv.org/pdf/1901.02446.pdf)) (with the ResNet-50 backbone pre-trained on ImageNet), a learning rate of 0.001, momentum of 0.8, and learning rate is divided by 4 if there is no validations loss improvement after three epochs. My loss function is a per-pixel multiclass cross-entropy loss.  \n\n  \n\nMy dataset is extremely limited. Not only are not all pixels classified, I also only have 700 images and a severe class imbalance. I tried tackling this imbalance through loss class weighting (based on the number of ground truth pixels for each respective class, i.e. their area sizes), but it barely helps. I also possess, for every -----> image !!! , a depth map, which I (can) supply as a fourth channel to the input layer.  \n\n  \n\n\\[A table of results\\]([https://imgur.com/a/v7VFgnk](https://imgur.com/a/v7VFgnk))  \n\n\\[Visualizations of images trained only on RGB\\]([https://imgur.com/a/oiSrLeM](https://imgur.com/a/oiSrLeM))\n\n\\[Visualizations of images trained on RGBD\\]([https://imgur.com/a/LDiTQGK](https://imgur.com/a/LDiTQGK))\n\n\\[Visualizations of images trained only on RGB, but with class loss weighting\\]([https://imgur.com/a/6Jv6mtu](https://imgur.com/a/6Jv6mtu))\n\n\\[Visualizations of images trained on RGBD, and with class loss weighting\\]([https://imgur.com/a/oc2asRj](https://imgur.com/a/oc2asRj))\n\n&amp;#x200B;\n\nPerformance is pretty crappy. What's more, there is very little difference between results of my four experiments. Why is this? I would expect that the addition of depth information (which encodes surface normals and perhaps texture information; pretty discriminitive information). Besides the overall metrics being rather low, the predictions are very messy, and the networks rarely, if ever, predicts \"small\" classes (in terms of area size), e.g. plastic or gravel. This is to be expected with such a small amount of data, but I was wondering if there are any \"performance hacks\" that can boost my network, or if I am missing any obvious stuff? Or is data likely the only bottleneck here? Any suggestions are greatly appreciated!\n\n&amp;#x200B;\n\nPS. I also tried a simple ResNet-50 FCN (I simply upsample ResNet's output until I have the same resolution; there aren't even skip connections), and the results are worse, but at least they are \\[smooth\\]([https://imgur.com/a/nV9vHGl](https://imgur.com/a/nV9vHGl)). Why are these more smooth?", "link": "https://www.reddit.com/r/MachineLearning/comments/c1h727/r_how_can_i_improve_my_material_segmentations/"}, {"autor": "the_best_broccoli", "date": "2019-06-14 08:28:40", "content": "[D] Learning the rotation of 2d images with a CNN /!/ Hi all,\nthis is my first post here so I hope I do this right...\n\nI'm currently trying to get a CNN to learn the rotation angle of 2D images. I tried to make a post on stackoverflow first, because I didn't know if the question fits into this subreddit, but so far that thread didn't gain any traction... [see here](https://stackoverflow.com/questions/56462202/cnn-for-2d------> image !!! -rotation-estimation-angle-regression).\n\nI hope it's ok when I just quote the question from there:\n\n\n&gt; I am trying to build a CNN (in Keras) that can estimate the rotation of an image (or a 2d object). So basically, the input is an image and the output should be its rotation.\n&gt; \n&gt; My first experiment is to estimate the rotation of M\u0143IST digits (starting with only one digit \"class\", let's say the \"3\"). So what I did was extracting all 3s from the MNIST set, and then building a \"rotated 3s\" dataset, by randomly rotating these images multiple times, and storing the rotated images together with their rotation angles as ground truth labels.\n&gt; \n&gt; So my first problem was that a 2d rotation is cyclic and I didn't know how to model this behavior. Therefore, I encoded the angle as y=sin(ang), x = cos(ang). This gives me my dataset (the rotated 3s images) and the corresponding labels (x and y values).\n&gt; \n&gt; For the CNN, as a start, i just took the keras MNIST CNN example (https://keras.io/examples/mnist_cnn/) and replaced the last dense layer (that had 10 outputs and a softmax activation) with a dense layer that has 2 outputs (x and y) and a tanh activation (since y=sin(ang), x = cos(ang) are within [-1,1]).\n&gt; \n&gt; The last thing i had to decide was the loss function, where i basically want to have a distance measurement for angles. Therefore i thought \"cosine_proximity\" is the way to go.\n&gt; \n&gt; When training the network I can see that the loss is decreasing and converging to a certain point. However when I then check the predictions vs the ground truth I observe a (for me) fairly surprising behavior. Almost all x and y predictions tend towards 0 or +/-1. And since the \"decoding\" of my rotation is ang=atan2(y,x) the predictions are usually either +/- 0\u00b0, 45\u00b0, 90, 135\u00b0 or 180\u00b0. However, my training and test data has only angles of 0\u00b0, 20\u00b0, 40\u00b0, ... 360\u00b0. This doesn't really change if I change the complexity of the network. I also played around with the optimizer parameters without any success.\n&gt; \n&gt; Is there anything wrong with the assumptions:\n&gt;\n&gt; * x,y encoding for angle\n&gt;\n&gt; * tanh activation to have values in [-1,1]\n&gt; \n&gt; * cosine_proximity as loss function\n&gt; \n&gt; Thanks in advance for any advice, tips or pointing me towards a possible mistake i made!\n\nIf this is the wrong place for this question I'm sorry and would be happy if someone could point me to the right forum subreddit!", "link": "https://www.reddit.com/r/MachineLearning/comments/c0ho4h/d_learning_the_rotation_of_2d_images_with_a_cnn/"}, {"autor": "thatguydr", "date": "2019-06-13 16:55:53", "content": "[D] Quoc Le is silently killing it in 2019 /!/ Over the past 6 months, he's been an author on\n\n* EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\n* Attention Augmented Convolutional Networks\n* Transformer-XL: Attentive language models beyond a fixed-length context\n* The Effect of Network Width on Stochastic Gradient Descent and Generalization\n* Unsupervised Data Augmentation\n* Soft Conditional Computation\n* Selfie: Self-supervised Pretraining for Image Embedding\n* NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection\n* Specaugment: A simple data augmentation method for automatic speech recognition\n* Using videos to evaluate -----> image !!!  model robustness\n* Searching for MobileNetV3\n* The Evolved Transformer\n\nI know he always does great research, but is there anyone else who's had this much impact recently? That list of papers is impressive, and for them all to have come out in a six month period with contributions from the same author is almost ridiculous.\n\nI figured he should get a shout out. If there's discussion to be had, I'd love to know who else has been moving the field (maybe in other areas like RL or NLP) as much recently.", "link": "https://www.reddit.com/r/MachineLearning/comments/c08ana/d_quoc_le_is_silently_killing_it_in_2019/"}, {"autor": "VagaScot", "date": "2019-06-13 13:57:54", "content": "I have 4 months of free time until i start college, how can i effectively use this spare time to learn Machine Learning with Python? (Very *little* knowledge of python, but my math knowledge is solid) /!/ So as the title says, i have 4 months, i plan to study around \\~12 hours a day until i start college (and continue studying) Machine Learning with Python, without just starting a random Udemy course, what would you guys recommend? \n\n&amp;#x200B;\n\nI saw a -----> picture !!!  when i was having a look:\n\n* 8 months learning **Python Core**\n* 6 months learning **Pandas**\n* 2 months learning **Matplotlib**\n* 6 months learning **SciKit-Learn**\n* 6 months learning **Keras**\n\nBut i was hoping in the 4 months to maybe spread out the learning, i know a **little** of python core (roughly 2-3 weeks) like..\n\n    time = 1200\n    if time == 1200:\n      print(\"The time is: {}\".format(time))\n\n&amp;#x200B;\n\n    words = [\"one\", \"two\", \"three\"]\n    \n    for word in words:\n        print(word)\n\n&amp;#x200B;\n\n    double_word = ['one', 'two', 'two', 'three']\n    count_double = {}\n    \n    for word in double_word:\n        if word not in double_word:\n            double_word[word] = 1\n        else:\n            double_word[word] += 1\n    print(double_word)\n\nThese are just off the top of my head after roughly 3 weeks of checking out python core.\n\n&amp;#x200B;\n\nSo what do you guys think? Just spend a month each learning Pandas, Matplotlib, SciKit-Learn and then Keras? or any advice?", "link": "https://www.reddit.com/r/MachineLearning/comments/c0656j/i_have_4_months_of_free_time_until_i_start/"}, {"autor": "deeplearning4ever", "date": "2019-11-06 08:06:33", "content": "[P] Recurrent GAN code for iterative text-based -----> image !!!  generation [ICCV 2019] /!/ * GitHub: [https://github.com/Maluuba/GeNeVA](https://github.com/Maluuba/GeNeVA)\n* Paper: [http://openaccess.thecvf.com/content\\_ICCV\\_2019/html/El-Nouby\\_Tell\\_Draw\\_and\\_Repeat\\_Generating\\_and\\_Modifying\\_Images\\_Based\\_on\\_ICCV\\_2019\\_paper.html](http://openaccess.thecvf.com/content_ICCV_2019/html/El-Nouby_Tell_Draw_and_Repeat_Generating_and_Modifying_Images_Based_on_ICCV_2019_paper.html)\n* Project Page: [https://www.microsoft.com/en-us/research/project/generative-neural-visual-artist-geneva/](https://www.microsoft.com/en-us/research/project/generative-neural-visual-artist-geneva/)\n* Blog Post: [https://www.microsoft.com/en-us/research/blog/from-blank-canvas-unfolds-a-scene-gan-based-model-generates-and-modifies-images-based-on-continual-linguistic-instruction/](https://www.microsoft.com/en-us/research/blog/from-blank-canvas-unfolds-a-scene-gan-based-model-generates-and-modifies-images-based-on-continual-linguistic-instruction/)\n\nCode for recurrent GAN for image generation on text-based iterative inputs.", "link": "https://www.reddit.com/r/MachineLearning/comments/dsda11/p_recurrent_gan_code_for_iterative_textbased/"}, {"autor": "SahinOlut", "date": "2019-11-05 16:09:25", "content": "UNC BIAG Releases Mermaid, Pytorch based -----> image !!!  registration toolkit /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ds1arn/unc_biag_releases_mermaid_pytorch_based_image/"}, {"autor": "waltywalt", "date": "2019-11-05 15:34:53", "content": "[R] Adversarial explanations for understanding -----> image !!!  classification decisions and improved neural network robustness /!/ **Abstract:**\n\n&gt;For sensitive problems, such as medical imaging or fraud detection,  neural network (NN) adoption has been slow due to concerns about their  reliability, leading to a number of algorithms for explaining their  decisions. NNs have also been found to be vulnerable to a class of  imperceptible attacks, called adversarial examples, which arbitrarily  alter the output of the network. Here we demonstrate both that these  attacks can invalidate previous attempts to explain the decisions of  NNs, and that with very robust networks, the attacks themselves may be  leveraged as explanations with greater fidelity to the model. We also  show that the introduction of a novel regularization technique inspired  by the Lipschitz constraint, alongside other proposed improvements  including a half-Huber activation function, greatly improves the  resistance of NNs to adversarial examples. On the ImageNet  classification task, we demonstrate a network with an  accuracy-robustness area (ARA) of 0.0053, an ARA 2.4 times greater than  the previous state-of-the-art value. Improving the mechanisms by which  NN decisions are understood is an important direction for both  establishing trust in sensitive domains and learning more about the  stimuli to which NNs respond.\n\n**Open Access pre-print:** [https://arxiv.org/abs/1906.02896](https://arxiv.org/abs/1906.02896)\n\n**Open Access PDF (low-resolution images, due to size restriction):** [https://arxiv.org/pdf/1906.02896.pdf](https://arxiv.org/pdf/1906.02896.pdf)\n\n**Peer-reviewed publication (with full-resolution images; also see bottom of this Reddit post):** [https://www.nature.com/articles/s42256-019-0104-6](https://www.nature.com/articles/s42256-019-0104-6)\n\n**Code:** [https://github.com/wwoods/adversarial-explanations-cifar/](https://github.com/wwoods/adversarial-explanations-cifar/)\n\n[Comparing explanatory power between Grad-CAM \\[Selvaraju et al. 2017\\] and Adversarial Explanations \\(AEs\\) when applied to a robust NN trained on CIFAR-10.  The top four rows, subfigure a, demonstrate comparisons on different inputs.  For each row, the columns show: the original \\`\\`Input'' image, labeled with the most confidently-predicted class, the correct class, and the NN's confidence in each; two Grad-CAM explanations, one for each predicted class shown by the input; two AEs, divided into the adversarial noise used to produce the AE, and the AE itself.  Below those rows, subfigures b through i are annotated versions of the AEs for subfigure a, indicating regions which contributed to or detracted from each predicted class.  See the main text for full commentary.](https://preview.redd.it/ip56d0uxxvw31.png?width=1424&amp;format=png&amp;auto=webp&amp;s=84e3d8a0751dce4ce72d305a53163a0156d30901)\n\n**Author's note:** The freely-available pre-print on ArXiv contains all content available in the Nature version, just in a slightly different ordering (IEEE vs Nature style).  The resolution of the ArXiv images is a bit lower, as the full document from pdflatex is \\~97 MB due to included images...  A Ghostscript-optimized version, with full-resolution images, weighs in at 25MB and may be found here: [https://drive.google.com/open?id=1xGCja0BUQ2VR9nlKre6QzJ2Q-qpp8ub8](https://drive.google.com/open?id=1xGCja0BUQ2VR9nlKre6QzJ2Q-qpp8ub8)", "link": "https://www.reddit.com/r/MachineLearning/comments/ds0st4/r_adversarial_explanations_for_understanding/"}, {"autor": "Pratik668", "date": "2019-11-05 05:06:25", "content": "Is -----> image !!!  caption generation is good for undergraduate project? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/drujy2/is_image_caption_generation_is_good_for/"}, {"autor": "rthiy", "date": "2019-11-04 17:33:50", "content": "Vazam the vehicle in front of you /!/ I have an idea which I would like to realize, but I am new to programming (-----> image !!!  recognition) and looking for someone with some knowledge who can give me some tips how to start with it. \n\nShort version of the idea:  [https://shazvehicle.rhilft.ch](https://shazvehicle.rhilft.ch/) \n\nBR", "link": "https://www.reddit.com/r/MachineLearning/comments/drkzg2/vazam_the_vehicle_in_front_of_you/"}, {"autor": "universome", "date": "2019-11-04 15:20:35", "content": "[R] You can find a lot of interesting things in the loss landscape of your neural network /!/ Just sharing with you a small (and somewhat fun) project I was recently working on, which is about finding different patterns in the loss surface of neural networks. Usually, a landscape around a minimum looks like a pit with random hills and mountains surrounding it, but there exist more meaningful ones, like in the -----> picture !!!  below (check the paper for more results). We have discovered that you can find a minimum with (almost) any landscape you like. An interesting thing is that the found landscape pattern remains valid even for a *test* set, i.e. it is a property that (most likely) remains valid for the whole data distribution.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/t885u6vosow31.png?width=1810&amp;format=png&amp;auto=webp&amp;s=793644af78a5430368e7a1c05d7b38c6b02ec637\n\nPaper: [https://arxiv.org/abs/1910.03867](https://arxiv.org/abs/1910.03867)  \nCode: [https://github.com/universome/loss-patterns](https://github.com/universome/loss-patterns)", "link": "https://www.reddit.com/r/MachineLearning/comments/drj12t/r_you_can_find_a_lot_of_interesting_things_in_the/"}, {"autor": "Atom_101", "date": "2019-08-09 11:06:19", "content": "[D] Effect of Oversampling on classifiers, when combined with -----> image !!!  transformations /!/ I am trying to understand the negative consequences of oversampling in the context of image classification. If I am using a decent amount amount of image transformations, I believe it will effectively be equivalent to SMOTE for tabular data, since I am not exactly repeating any image in a batch. Does the behaviour and test set accuracy of a classifier in any way depend on the actual class distribution in the train set and by oversampling am I doing any harm? \n\nTo take an example I was training a classifier on a dataset with 5 classes, having heavy class imbalance. To balance it out I oversampled the minority classes so that all classes have equal number of images. This caused a significant performance drop on the test set that I have, while cross validation performance was fairly high on the oversampled set. When analysing the class distributions I saw that for the original train set the distribution was: 1,3,2,4,5 with decreasing number of samples. The test set has a class distribution 3,1,2,4,5 but the predictions after training on oversampled data have distribution 4,3,2,5,1. Mathematically speaking, how can this behaviour be explained.", "link": "https://www.reddit.com/r/MachineLearning/comments/co0uy1/d_effect_of_oversampling_on_classifiers_when/"}, {"autor": "morenoh149", "date": "2019-08-08 21:10:56", "content": "Building categories and categorizing instagram profiles by -----> picture !!!  /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/cns3kk/building_categories_and_categorizing_instagram/"}, {"autor": "justheuristic", "date": "2019-08-08 20:22:20", "content": "[P] Generative Modelling by Estimating Gradients - in 150 lines with JAX /!/ This tutorial that implements the math behind the [Generative Modelling by Estimating Gradients of the Data Distribution](https://arxiv.org/abs/1907.05600).\n\n**Get the tutorial:** [**view on github**](https://github.com/google/jax/blob/master/notebooks/score_matching.ipynb) or [**run in Google Colab**](https://colab.research.google.com/github/google/jax/blob/master/notebooks/score_matching.ipynb)\n\nIn short, they train a neural network to estimate gradients of -----> image !!!  probability and iteratively follow these gradients to improve the -----> image !!! .", "link": "https://www.reddit.com/r/MachineLearning/comments/cnreuu/p_generative_modelling_by_estimating_gradients_in/"}, {"autor": "ToraxXx", "date": "2019-08-08 16:17:42", "content": "[P] Web-based implementation of Deep -----> Image !!!  Prior /!/ [https://warlock.ai/deepimageprior/](https://warlock.ai/deepimageprior/)\n\nUsing TensorFlow.js I implemented a client-side version of [Deep Image Prior](https://arxiv.org/abs/1711.10925). It can be used for denoising, inpainting, super-resolution (not implemented yet) and more. It works by training a network to output a given image. More info about the algorithm can be found on the original authors' [project page](https://dmitryulyanov.github.io/deep_image_prior).\n\nThere are still a couple of things that need to be resolved such as mask-drawing on mobile (the page scrolls when drawing right now) and the comparison view becoming stuck after the first image was selected. Also I'm not sure how well this works on devices without GPUs, although on my relatively old phone (Nexus 6) and my PC (GTX 1070) it worked reasonably well.\n\n[Inpainting example 1](https://i.redd.it/23nfhru629f31.png)\n\n[Inpainting example 2](https://i.redd.it/oapgc9t519f31.png)", "link": "https://www.reddit.com/r/MachineLearning/comments/cnnykj/p_webbased_implementation_of_deep_image_prior/"}, {"autor": "adelredjimi", "date": "2019-08-08 08:46:42", "content": "[D] Are there any relevant -----> image !!!  classification domains where traditional approaches are still on par with CNN-based? /!/ To be precise, I am talking about image classification, not general computer vision, and by traditional approaches I mean approaches based on features like Fisher, BoW, ... even if they use multi-layered classifiers.", "link": "https://www.reddit.com/r/MachineLearning/comments/cniy3k/d_are_there_any_relevant_image_classification/"}, {"autor": "ixeption", "date": "2019-08-08 06:42:17", "content": "[D] Keras vs tensorflow: Performance, GPU utilization and data pipeline /!/ Hi folks,\n\nI was recently dealing with some performance issues related to the keras -----> image !!!  preprocessing. After several experiments, I  thought it would might be helpful to share my insights. There are several possible fixes:\n\n* update all packages, especially keras-preprocessing.\n* Deactivate your virus scanner (whitelist your data folder) and check if you have an internal SSD. \n* Try to tweak the configuration on fit\\_generator (workers and queue\\_size). If you are using linux try out multiprocessing and a thread-safe generator. \n* Convert your dataset to TFrecords and use it with keras or directly move to tensorflow. If you already using tensorflow 2.0, you can directly fit keras models on TFRecord datasets. \n\nFurthermore the tensorflow implementaion was always (slightly) faster.\n\n[Here is a more detailed explaination.](http://digital-thinking.de/tensorflow-vs-keras-or-how-to-speed-up-your-training-for-image-data-sets-by-factor-10/)\n\nCheers", "link": "https://www.reddit.com/r/MachineLearning/comments/cnhvw8/d_keras_vs_tensorflow_performance_gpu_utilization/"}, {"autor": "ccwpog", "date": "2019-10-14 09:25:11", "content": "[D] CNN: reducing -----> image !!!  size to 1x1 /!/ In a CNN the image sizes get reduced through valid convolutions and padding layers (while the depth increases). In popular NN, e.g. VGG16 it goes down to (7x7x512) (width x height x depth). Would it be problematic to reduce the dimension even further to 1x1 (width x height) or when can this be beneficial? Unfortunately, I couldn't find anything related to the question online.", "link": "https://www.reddit.com/r/MachineLearning/comments/dho4xs/d_cnn_reducing_image_size_to_1x1/"}, {"autor": "erikvdplas", "date": "2019-10-13 20:45:30", "content": "[P] Snapchat-like filters/lenses /!/ I\u2019ve seen numerous guides online discussing face and facial landmark detection. Sometimes these guides also overlay an -----> image !!!  attached to one or multiple of these landmarks. None however, promise real-time mapping of 3D models to 3D face meshes. Yet Snapchat manages to do this with nearly zero latency on mobile devices. What are some tricks to achieve similar performance?\n\nI just thought of this as a complex, multifaceted problem. I would love and try to recreate Snapchat lenses in an open source project. \n\nAnyone with experience with similar problems who can elaborate on some of the key components?", "link": "https://www.reddit.com/r/MachineLearning/comments/dhgapl/p_snapchatlike_filterslenses/"}, {"autor": "saadhaxxan", "date": "2019-10-13 10:32:47", "content": "Top 10 Best Datasets for Applied ML /!/  \n\nhttps://i.redd.it/ycqtaef5das31.png\n\nFor the development of AI ,machine learning and data science project its important to gather relevant data. Below given are the 10 best machine learning datasets such a way that you can download the dataset and can develop your machine learning project.\n\n**1. ImageNet**  \nImageNet is one of the best datasets for machine learning. Generally, it can be used in computer vision . This project is an -----> image !!!  dataset, it was developed by Fei Fei Li and other researcher working on computer vision. See their TED talk here [https://www.youtube.com/watch?v=40riCqvRoMs](https://www.youtube.com/watch?v=40riCqvRoMs) .  \n[http://www.image-net.org/download-faq](http://www.image-net.org/download-faq)\n\n**2. Indians Diabetics Dataset**  \nIf you want to apply machine learning in health care,then you can use this Pima Indian Diabetics dataset in your healthcare system. We all know that diabetes is one of the most common dangerous diseases. You can use this dataset in your diabetes detection system. This dataset is from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of this dataset is to predict whether or not a patient has diabetes based on specific diagnostic measurement.  \n[https://www.kaggle.com/uciml/pima-indians-diabetes-database](https://www.kaggle.com/uciml/pima-indians-diabetes-database)\n\n**3. Boston House Price Dataset**  \nDo you want to practice regression algorithm? Then you can use this dataset in your machine learning problem. This dataset is collected from the area of Boston Mass.  \n[https://www.kaggle.com/vikrishnan/boston-house-prices](https://www.kaggle.com/vikrishnan/boston-house-prices)\n\n**4. HotpotQA**  \nDo you want to work with natural language processing? We all know natural language processing covers a big range area in machine learning. So, if you want to develop a system based on natural language processing (NLP) concept then this dataset is for you my friend. It is collected by a team of NLP researchers at Carnegie Mellon University, Stanford University.  \n[https://hotpotqa.github.io/](https://hotpotqa.github.io/)\n\n**5. Labelme**  \nImage processing is one of the amazing is of machine learning. If you are interested in developing an image processing system, then you can use this Labelme dataset in your machine learning project. This dataset is a large volume dataset of annotated images.  \n[http://labelme2.csail.mit.edu/Release3.0/browserTools/php/dataset.php](http://labelme2.csail.mit.edu/Release3.0/browserTools/php/dataset.php)\n\n**6. Facial Image Dataset**  \nYou can use this interesting machine learning dataset for your computer vision project. This dataset is standard and free to use. Moreover, it contains a variation of data like variation of background and scale, and variation of expressions. This standard dataset helps to evaluate a system precisely.  \n[https://cswww.essex.ac.uk/mv/allfaces/faces94.html](https://cswww.essex.ac.uk/mv/allfaces/faces94.html)\n\n**7. Chars74K Dataset**  \nOptical Character recognition is one of the classic classification problems of pattern recognition. This interesting machine learning dataset consists of 64 classes (0\u20139, A-Z, a-z), 7705 characters taken from natural images, 3410 hand-drawn characters, and 62992 synthesized characters from computer fonts.  \n[http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/#download](http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/#download)\n\n**8. YouTube Dataset**  \nAre you an expert in machine learning research area or want to do something with video classification? Then, this dataset for machine learning project might help you. Also, you might be glad to know that Google has shared a labeled dataset with 8M classified YouTube Videos and its\u2019 IDs  \n[https://research.google.com/youtube8m/](https://research.google.com/youtube8m/).\n\n**9. Amazon Reviews Dataset**  \nWe all know natural language processing is about text data. To solve a real-world application, you need ML dataset. Also, this Amazon reviews dataset is one of them. It contains 35 million reviews from Amazon spanning 18 years (up to March 2013).  \n[https://snap.stanford.edu/data/web-Amazon.html](https://snap.stanford.edu/data/web-Amazon.html)\n\n**10.xView**  \nIf you are an expert in machine learning and you can handle a tricky problem or project, then I must suggest you use this dataset in your project or system. This dataset is one of the standard datasets for imaging problem. Moreover, it is one of the most extensive public datasets.  \n[http://xviewdataset.org/#dataset](http://xviewdataset.org/#dataset)\n\n**CLOSING WORDS:**  \nDataset is an integral part of machine learning applications. It can be available in different formats like .txt, .csv, and many more. In supervised machine learning, the labeled training dataset is used, and in unsupervised, no label is needed. If you are a beginner, we recommend you to read this article thoroughly.", "link": "https://www.reddit.com/r/MachineLearning/comments/dh8wv4/top_10_best_datasets_for_applied_ml/"}, {"autor": "MLtinkerer", "date": "2019-10-13 01:45:29", "content": "[D] [P] monetizing my ml models /!/ I have some -----> image !!!  manipulation models I'm thinking of making some side money from.\n\nShould I simply sell it or do I expose it as an API?\n\n&amp;#x200B;\n\nIs there a website I can go to where I can monetize my model?", "link": "https://www.reddit.com/r/MachineLearning/comments/dh48mf/d_p_monetizing_my_ml_models/"}, {"autor": "ants_rock", "date": "2019-02-17 22:05:43", "content": "pomegranate v0.11 released, now with neural probabilistic models /!/ Howdy all\n\nI just released pomegranate v0.11. There are some bug fixes, speed improvements, and significantly expanded tutorials. The most significant additions are:\n\n* NetworkX v2.0 support\n* Custom distribution support (https://bit.ly/2rjWjJx)\n* Neural probabilistic models (https://bit.ly/2QQcMDH)\n\nThe ability to merge probabilistic models with neural networks is the biggest addition. These are probabilistic models that, rather than having probability distributions (or mixtures) as the emissions, have a single neural network that takes in the observations and outputs class probabilities. You can read more about it in the tutorial above.\n\nI had two use-cases of neural probabilistic models in mind when I added support for them. The first is that when you don't know what probability distribution to use, a neural network can learn the appropriate transformation without having to specify a distribution in advance. The second is when you'd like to extend a pre-trained neural network to sequences, such as extending -----> image !!!  classification to videos. Because HMMs don't need labels, they can learn how to extend pre-trained models to sequences without need for labeled training data. \n\nThe pomegranate repository is here: https://github.com/jmschrei/pomegranate\n\nThe tutorial repository is here: https://github.com/jmschrei/pomegranate/tree/master/tutorials\n\nYou can get pomegranate using `pip install pomegranate` or by building it from source. It's written mostly in Cython so you may need a working C compiler if you're building from source.\n\nI'd love to get any feedback you may have!", "link": "https://www.reddit.com/r/MachineLearning/comments/arppd2/pomegranate_v011_released_now_with_neural/"}, {"autor": "Al7123", "date": "2019-02-17 14:21:34", "content": "[P] How can I improve my current CNN project? It is a simple binary classification but I am having some trouble. /!/ Hello, I'm quite new to this, so take it easy with me.\n\nInspired by [Pepito the cat](www.twitter.com/pepitothecat/) I decided to make a cnn to detect when my dog goes to the porch to bark at home.\n\nI have now 2000 pictures of the porch without him and 2000 with him and about 1000 of just him (cutout).\n\nI thought it was very simple since the -----> camera !!!  is still and he is a black dog. I've got some good results even with changing light and shadows during the day, but I realized that it is detecting black blobs on the image and not his contour/edges. I mean, if put anything black in front of the camera It will trigger, not only just him.\n\nWhat can I do to improve my neural network?", "link": "https://www.reddit.com/r/MachineLearning/comments/arky4c/p_how_can_i_improve_my_current_cnn_project_it_is/"}, {"autor": "mahav2000", "date": "2019-02-17 10:12:40", "content": "Looking for a place to read papers /!/ What is the best place to start reading papers related to machine learning? I am a beginner and I have worked with all the basic algorithms and have also worked with CNN's for -----> image !!!  processing. I want to get into research but I have no idea where I can find papers that match my level of understanding to read and develop my knowledge.", "link": "https://www.reddit.com/r/MachineLearning/comments/arjbk6/looking_for_a_place_to_read_papers/"}, {"autor": "Top_Hat_Tomato", "date": "2019-02-17 01:14:41", "content": "[P] Real-time -----> image !!!  recognition of key presses /!/ My first little implementation of tensorflow that I was able to get running real-time.\n\nNothing too special, but my hope is that I'll be able to get it able to recognize the entire keyboard instead of just whether the \"a\" key is pressed (which is the current implementation).", "link": "https://www.reddit.com/r/MachineLearning/comments/arfg1h/p_realtime_image_recognition_of_key_presses/"}, {"autor": "kinezodin", "date": "2019-09-19 11:13:59", "content": "[P] Stylegan Music Video /!/ We made a music video using NVIDIA's [styleGAN](https://github.com/NVlabs/stylegan). You can check it out here: [https://youtu.be/bCJXnRFGoSE](https://youtu.be/bCJXnRFGoSE) .\n\n# Methodology\n\nWe first produced a [mel-scaled spectrogram](https://librosa.github.io/librosa/generated/librosa.feature.melspectrogram.html) for the piece of music. We tweaked the arguments such that each time-step of the spectrogram corresponds to 16.7ms (duration of a frame @60fps). The frequency dimension of the spectrogram is scaled to match styleGAN's input dimension.\n\nThen we explored a pre-trained (on faces) styleGAN's input space for interesting output images.  The way we performed the exploration was to compute the gradient of the mean squared error between styleGAN's output -----> image !!!  and a real -----> image !!!  (which we had chosen), with respect to a random input. Then with steps of gradient descent we search for inputs which produce outputs similar to our real image. We wanted \"non-realistic, creepy faces\", which we got by using extreme hyper-parameters in this exploration phase, by swapping the colors of the output and by carefully choosing the custom target image. For each generated image we also saved the input vector (512 dimensional) which lead to it.\n\nFinally, we made a large spreadsheet in which each row is a beat of the song (175 bpm for most parts). We assigned various generated images we liked at different parts of the song (usually at intervals 4 beats). We turned this spreadsheet into a large input array of dimensions equal to the mel-scaled spectrogram, by linearly interpolating between the pre-chosen generated images at the intervals dictated by the spreadsheet. We add this input matrix to the spectrogram with some weights and feed it to the pre-trained styleGAN. The outputs are the frames of the video.\n\n(For the first few seconds of the song we also used some real footage which we morphed with generated faces)\n\n# Discussion\n\nThroughout the project we felt that there must be a better way to do targeted searches of the input space. For styleGAN there is some interpretability to each dimension of the input, however we found it hard to make use of this, especially when the target image was not strictly a face (a skull for example). What are other ways in which we can answer the question \"what inputs of this (differentiable) black box lead to a desired output?\"", "link": "https://www.reddit.com/r/MachineLearning/comments/d6cj28/p_stylegan_music_video/"}, {"autor": "pvl18", "date": "2019-09-19 08:57:42", "content": "[P] How we made landmark recognition in Cloud Mail.ru, and why /!/ With the advent of mobile phones with high-quality cameras, we started making more and more pictures and videos of bright and memorable moments in our lives. Many of us have -----> photo !!!  archives that extend back over decades and comprise thousands of pictures which makes them increasingly difficult to navigate through.\n\nFor this purpose, we at Mail.ru Computer Vision Team have created and implemented systems for smart image processing, including landmark recognition.  Photos with landmarks are essential because they often capture highlights of our lives (journeys, for example). These can be pictures with some architecture or wilderness in the background. This is why we seek to locate such images using Deep Learning, and make them readily available to users.\n\n [https://medium.com/@andrei.boiarov/how-we-made-landmark-recognition-in-cloud-mail-ru-and-why-715b5f72e6d4](https://medium.com/@andrei.boiarov/how-we-made-landmark-recognition-in-cloud-mail-ru-and-why-715b5f72e6d4)", "link": "https://www.reddit.com/r/MachineLearning/comments/d6bbfo/p_how_we_made_landmark_recognition_in_cloud/"}, {"autor": "marshfellowML", "date": "2019-09-18 16:06:06", "content": "[P] LatentPainter. Paint with ideas instead of pixels! /!/ Hey guys! Here\u2019s a fun tool I hacked together by abusing spatialized adaptive instance normalization. You paint directly with the latent Z into different layers of the network (basically StyleGAN with some model surgery). The results are pretty cool! \n:\n[link to the video](https://twitter.com/MarshfellowML/status/1174234836858494976?s=20)\n\nHave a lot of features planned including a \u201ccolor picker\u201d that\u2019s just an encoder to encode the latent Z of an arbitrary -----> image !!!  into your latent space so you can paint with it. Also going to be exposing sliders for which layers you\u2019re painting on (earlier layers will be the coarse features like facial structure and later layers will be more textural).", "link": "https://www.reddit.com/r/MachineLearning/comments/d5zy5n/p_latentpainter_paint_with_ideas_instead_of_pixels/"}, {"autor": "marshfellowML", "date": "2019-09-18 08:53:01", "content": "[Project] Latent Painter! A prototype I made today of painting directly with latent representations. /!/ This was done by doing some model surgery on a trained StyleGAN (FFHQ dataset) at 256x256 resolution. You can actually paint 512 channel deep latent vectors into tensors at different layers in the model.\n\nGoing to be following it up with some really cool features around encoding -----> image !!! s to latents so you can paint with the representation of an arbitrary -----> image !!!  you have (assuming it's in distribution!).\n\nDefinitely more to come but excited this works as well as it does!", "link": "https://www.reddit.com/r/MachineLearning/comments/d5uy09/project_latent_painter_a_prototype_i_made_today/"}, {"autor": "vishaan", "date": "2019-09-18 08:08:48", "content": "[P] ClearHead.ai \u2013 A marketplace for machine learning models /!/ https://clearhead.ai \n\nHello /r/MachineLearning \n\nI'm the founder of ClearHead.ai, a marketplace for machine learning models which allows modelers to upload their models via a python SDK and developers to use the models with a simple request to an API. \n\nWe are looking for some alpha (not ethological, more this-software-is-rough-around-the-edges) machine learning modelers who have machine learning models they would like to monetize. Ideally -----> image !!!  based ones (CNNs, GANs) that are trained on a non-research dataset. If you are interested (both modelers and consumers) please sign-up via the website or this link. \n\nThis approach is similar to Algorithmia, but we want to really focus-in on the problem of model comparability and transparency. \n\nI'll be in the comments, if you have any questions, feedback or complaints.", "link": "https://www.reddit.com/r/MachineLearning/comments/d5ulck/p_clearheadai_a_marketplace_for_machine_learning/"}, {"autor": "edon581", "date": "2019-10-06 22:57:48", "content": "Paper about training on ImageNet with each -----> image !!!  as own class, but can't remember title/authors /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/deav3u/paper_about_training_on_imagenet_with_each_image/"}, {"autor": "MagicOfMessi", "date": "2019-10-05 20:47:28", "content": "Is there a place where people link the latest softwares/tools that use machine learning..? /!/ Some examples of really cool and useful softwares that i've found that use machine learning are: deepfacelab (for deepfakes), Gigapixel AI (for upscaling -----> image !!! s and videos), waifu2x (again, another -----> image !!!  upscaler)\n\n&amp;#x200B;\n\nSo this has made me wonder how many more amazing Machine learning softwares there are out there that i don't know about. So are there like any news site for machine learning softwares?\n\n&amp;#x200B;\n\nOk so while making this post i just made a discord server just for the purpose of sharing and linking new machine learning tools/softwares, you're free to join: [https://discord.gg/VvQvqH](https://discord.gg/VvQvqH)", "link": "https://www.reddit.com/r/MachineLearning/comments/ddt2wy/is_there_a_place_where_people_link_the_latest/"}, {"autor": "TrashPHD", "date": "2019-10-04 20:06:32", "content": "[P] Here's the app I'm building that uses ML and computational -----> photography !!!  to edit your videos - as a thank you for last week's AMA questions, you can use the code \"slashml\" to get in and try it out! /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ddcpii/p_heres_the_app_im_building_that_uses_ml_and/"}, {"autor": "Tmauer", "date": "2019-10-04 19:50:40", "content": "What's more important for -----> image !!!  recognition? Fps or native resolution? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ddchc5/whats_more_important_for_image_recognition_fps_or/"}, {"autor": "datitran", "date": "2019-10-04 12:58:42", "content": "[P] imagededup, a new library to find duplicate images more easily! /!/ We've just open-sourced our library -----> image !!! dedup, a Python package that simplifies the task of finding exact and near duplicates in an -----> image !!!  collection.\n\n&amp;#x200B;\n\nIt includes:\n\n\ud83e\uddee Several hashing algorithms (PHash, DHash etc) and convolutional neural networks\n\n\ud83d\udd0e An evaluation framework to judge the quality of deduplication\n\n\ud83d\uddbc Easy plotting functionality of duplicates\n\n\u2699\ufe0f Simple API\n\n&amp;#x200B;\n\nWe're really excited about this library because finding image duplication is a very important task in computer vision and machine learning. For example, severe duplicates can create extreme biases in your evaluation of your ML model (check out the CIFAR-10 problem). Please try out our library, \u2b50\ufe0f it on Github and spread the word! We'd love to get feedback. \n\n&amp;#x200B;\n\n\ud83d\udd24 Code: [https://github.com/idealo/imagededup](https://github.com/idealo/imagededup)\n\n\ud83d\udcd5 Docs: [https://idealo.github.io/imagededup/](https://idealo.github.io/imagededup/)\n\n&amp;#x200B;\n\nhttps://i.redd.it/8jgr7j0tuiq31.png", "link": "https://www.reddit.com/r/MachineLearning/comments/dd70s5/p_imagededup_a_new_library_to_find_duplicate/"}, {"autor": "VinayUPrabhu", "date": "2019-09-26 17:02:17", "content": "[D] On pornographic, NSFW and non-consensual images in the ImageNet dataset. What's the path forward? /!/ Dear Reddit-ML community,\n\nIn the imagenet dataset, ( classes 445 -n02892767- \\[\u2019bikini, two-piece\u2019\\] and 459- n02837789- \\[\u2019brassiere, bra, bandeau\u2019\\]) there are many images that are verifiably pornographic (you can see the porn-star's webpage in the pic!), shot in a non-consensual setting, voyeuristic and also entail underage nudity (See collage below).  \nThis has deep ramifications not just in the legal realm for downloading and storing these images, but also has a trickle down effect with regards to the models trained on this dataset. Ex: If you are an artist making/selling neural art, the unethical nature of the seed images could sully the sanctity of the art (See:  [https://openreview.net/forum?id=HJlrwcP9DB](https://openreview.net/forum?id=HJlrwcP9DB) )\n\nThe question now is: What's the best path forward? -----> Image !!!  deletion and replacement? Do chime in with your thoughts!\n\nPS: I had written to the creators of the dataset (waay before the ImageNet Roulette thingy), but  received no replies.\n\n![img](e1dyeqpyxyo31 \"A collage of images from the ImageNet dataset\")", "link": "https://www.reddit.com/r/MachineLearning/comments/d9mhfg/d_on_pornographic_nsfw_and_nonconsensual_images/"}, {"autor": "lookingforjob6969", "date": "2019-09-25 23:54:11", "content": "[P] -----> Image !!!  Classifier", "link": "https://www.reddit.com/r/MachineLearning/comments/d9bmjb/p_image_classifier/"}, {"autor": "TrashPHD", "date": "2019-09-25 18:46:40", "content": "[D] AMA: I'm Dr. Genevieve Patterson - cofounder and Chief Scientist at TRASH, a new app that uses computer vision and computational -----> photography !!!  to intelligently edit together and set to music any videos you upload. Ask me anything! /!/ Hi all!\n\nMy name is Genevieve Patterson - I\u2019m the Chief Scientist at [TRASH](https://trash.app/), and a PhD in Computer Vision. I've been working on our AI, Otto, for over a year now, and it's getting smarter with every release - here is a [blog post](https://medium.com/@thetrashapp/your-wish-is-our-command-a0be25de6fe3) about our latest version, and how it collaborates with user inputs. Otto is powered by supervised and unsupervised video attention, our internal active learning labeled social media video dataset, attribute and action recognition in video, custom multi-media embedding spaces, set-to-sequence conditional generator networks, and a suite of video retargeting techniques recently popularized in the computational video manipulation community. Otto trained in PyTorch and deployed on iOS using Core ML.\n\nMy work is about creating dialog between AI and people. An initial description of Otto was accepted to the ICLR 2019 Debugging Machine Learning Workshop \u2014 \"[Building Models for Mobile Video Understanding](https://debug-ml-iclr2019.github.io/cameraready/DebugML-19_paper_33.pdf)\u201d. Besides working at TRASH, I recently collaborated on a human + ML humor project, \"[Humor in Word Embeddings: Cockamamie Gobbledegook for Nincompoops](https://arxiv.org/abs/1902.02783),\u201d ICML 2019. Please feel free to ask about anything I\u2019ve worked on before ([Google Scholar page](https://scholar.google.com/citations?user=OogER9cAAAAJ&amp;hl=en&amp;oi=ao)).\n\nBefore TRASH, I was Postdoctoral Researcher at Microsoft Research New England.  I received my PhD from Brown University in 2016. I\u2019ve published at and still review for CVPR, ICCV/ECCV, NeurIPS, CHI, HCOMP, and other CV and ML venues.\n\nI would be more than happy to answer any questions about CV and ML, computational photography, the TRASH app, how to finish a PhD, publishing in these fields, or anything about my own path.\n\nOpening this thread for your questions now, and will be here through Friday, September 27th answering them.\n\n&amp;#x200B;\n\nhttps://i.redd.it/5rnp64p0dso31.jpg\n\nThanks, and I look forward to your questions!\n\nGenevieve Patterson\n\n[https://genp.github.io/](https://genp.github.io/)", "link": "https://www.reddit.com/r/MachineLearning/comments/d97n0a/d_ama_im_dr_genevieve_patterson_cofounder_and/"}, {"autor": "TrashPHD", "date": "2019-09-25 16:54:17", "content": "AMA: I'm Dr. Genevi\u00e8ve Patterson - cofounder and Chief Scientist at TRASH, a new app that uses computer vision and computational -----> photography !!!  to intelligently edit together and set to music any videos you upload. Ask me anything! /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/d962jc/ama_im_dr_genevi\u00e8ve_patterson_cofounder_and_chief/"}, {"autor": "chisai_mikan", "date": "2019-03-06 17:32:34", "content": "[R] Exploring Neural Networks with Activation Atlases /!/ New distill.pub [post](https://distill.pub/2019/activation-atlas/):\n\n*By using feature inversion to visualize millions of activations from an -----> image !!!  classification network, we create an explorable activation atlas of features the network has learned which can reveal how the network typically represents some concepts.*\n\nhttps://distill.pub/2019/activation-atlas/", "link": "https://www.reddit.com/r/MachineLearning/comments/ay1fx7/r_exploring_neural_networks_with_activation/"}, {"autor": "rinntintin1", "date": "2019-02-06 16:16:06", "content": "Use Python to analyze a -----> photo !!!  on cell phone. /!/ Hello! I am trying to design an free app to make my job easier.  Can you tell me if you think my idea is possible.  User will take a picture of a form.  App will analyze about 20 blocks.  After analyzing the photo it will tell you if you forgot to fill a block out or sign the form.  Thank you!", "link": "https://www.reddit.com/r/MachineLearning/comments/ansat6/use_python_to_analyze_a_photo_on_cell_phone/"}, {"autor": "ultrakoge", "date": "2019-06-22 16:16:17", "content": "[D] ImageNet, MNIST for regression with -----> image !!!  input /!/ Does any one know if there exists popular benchmark in regression problem with image input such as ImageNet and MNIST for classification?", "link": "https://www.reddit.com/r/MachineLearning/comments/c3rjez/d_imagenet_mnist_for_regression_with_image_input/"}, {"autor": "iyaja", "date": "2019-06-22 04:43:32", "content": "[D] Generative Adversarial Networks - The Story So Far /!/ Hi everyone. I just published a new blog post which talks about the evolution of GANs over the last few years. You can check it out [here](https://blog.floydhub.com/gans-story-so-far/).\n\n&amp;#x200B;\n\nI think it's fascinating to see sample images generated from these models side by side. It really does give a sense of how fast this field has progressed. In just five years, we've gone from blurry, grayscale pixel arrays that vaguely resemble human faces to [thispersondoesnotexist](https://thispersondoesnotexist.com), which can easily fool most people on first glance.\n\n&amp;#x200B;\n\nApart from -----> image !!!  samples, I've also included links to papers, code, and other learning resources for each model. So this article could be an excellent place to start if you're a beginner looking to catch up with the latest GAN research.\n\n&amp;#x200B;\n\nI hope you enjoy it!", "link": "https://www.reddit.com/r/MachineLearning/comments/c3ldo4/d_generative_adversarial_networks_the_story_so_far/"}, {"autor": "iyaja", "date": "2019-06-22 04:40:42", "content": "[D] Generative Adversarial Networks - The Story So Far /!/ Hi everyone. I just published a new blogpost which talks about the evolution of GANs over the last few years. You can check it out [here](https://blog.floydhub.com/gans-story-so-far/).\n\n&amp;#x200B;\n\nI think it's really interesting to see sample images generated from these models side by side. It really does give a sense of how fast this field has progressed. In just five years, we've gone from blurry, grayscale pixel arrays that vaguely resemble human faces to [thispersondoesnotexist](https://thispersondoesnotexist.com), which can easily fool most people on first glance.\n\n&amp;#x200B;\n\nApart from -----> image !!!  samples, I've also included links to papers, code, and other learning resources for each model. So this article could be a good place to start if you're a beginner looking to catch up with the latest GAN research.\n\n&amp;#x200B;\n\nHope you enjoy it!", "link": "https://www.reddit.com/r/MachineLearning/comments/c3lcsi/d_generative_adversarial_networks_the_story_so_far/"}, {"autor": "tsauri", "date": "2019-06-21 15:41:05", "content": "[D] Is self-supervised learning is really just denoising autoencoder? /!/ Denoising autoencoder 2019.  \n*Noise* the inputs and put original input as labels.  \nNoise is unfortunately must be cleverly highly hand-engineered.  \nLike crop -----> image !!!  uncrop -----> image !!! , remove color channels predict colors, remove positions predict positions, rotate -----> image !!!  reorient -----> image !!! , remove last word predict last word.  \nThen take the learnt weights as feature extractor.   \n\n\nI am waiting for someone to use them for generative purposes, if they are useful for disentanglement.\n\nNow we wait for something like GANs for denoising, where generator make new denoising tasks, and discriminator must solve the new tasks", "link": "https://www.reddit.com/r/MachineLearning/comments/c3c65d/d_is_selfsupervised_learning_is_really_just/"}, {"autor": "youali", "date": "2019-06-21 08:44:29", "content": "[R] The Functional Neural Process /!/ **The Functional Neural Process**\n\nAbstract:  We present a new family of exchangeable stochastic processes, the Functional Neural Processes (FNPs). FNPs model distributions over functions by learning a graph of dependencies on top of latent representations of the points in the given dataset. In doing so, they define a Bayesian model without explicitly positing a prior distribution over latent global parameters; they instead adopt priors over the relational structure of the given dataset, a task that is much simpler. We show how we can learn such models from data, demonstrate that they are scalable to large datasets through mini-batch optimization and describe how we can make predictions for new points via their posterior predictive distribution. We experimentally evaluate FNPs on the tasks of toy regression and -----> image !!!  classification and show that, when compared to baselines that employ global latent parameters, they offer both competitive predictions as well as more robust uncertainty estimates. \n\n\nhttps://arxiv.org/abs/1906.08324", "link": "https://www.reddit.com/r/MachineLearning/comments/c385bc/r_the_functional_neural_process/"}, {"autor": "hanrelan", "date": "2019-06-20 19:30:39", "content": "[R] SamsungAI: Realistic speech-driven face animation from a single still -----> image !!! ", "link": "https://www.reddit.com/r/MachineLearning/comments/c30e8q/r_samsungai_realistic_speechdriven_face_animation/"}, {"autor": "minimaxir", "date": "2019-06-20 14:49:30", "content": "[P] App to make AI-Generated submission titles for any Reddit subreddit using GPT-2 (+ keywords!) /!/ [https://minimaxir.com/apps/gpt2-reddit/](https://minimaxir.com/apps/gpt2-reddit/)\n\nThis is a web UI for a finetuned GPT-2 model on a very large amount of Reddit submissions, but with a twist: you can specify the subreddit you want to generate from, and *keywords/keyphrases* to condition the text upon. For example, here are [examples](https://www.reddit.com/r/legaladviceofftopic/comments/bxi869/i_trained_an_ai_to_generate_the_ultimate/) of /r/legaladvice titles conditioned on **cat**, **dog**, **sue**, and **tree**, and the model typically does a good job of incorporating all the inputs!\n\nSome other good subreddits for generating text are /r/amitheasshole, /r/confession, /r/writingprompts, /r/relationships, and of course the default /r/askreddit .\n\nTechnical notes on this Reddit model/API:\n\n* The model is running on Google Cloud Run (via [gpt-2-cloud-run](https://github.com/minimaxir/gpt-2-cloud-run)), which means it's slower than GPU-backed GPT-2 demos, but it's very cheap and can scale up to Reddit-level traffic without any engineering effort. (and it can generate texts *in parallel* if you want to try many possibilities)\n* Unlike /r/SubSimulatorGPT2, which has a separate GPT-2 345M model for each subreddit, this model uses *a single GPT-2* *(117M)* model. This has its advantages: the model is able to incorporate syntax/keywords from other subreddits for more creative output.\n* The methodology I use to allow GPT-2 to incorporate arbitrary keywords/keyphrases in generation will be released at some point, but it's not ready yet.\n* The subreddits used in the training set consist of every major subreddit you've heard of. The super niche subreddits may not be present, but the network does a good job at extrapolating subreddit type if there is a similar name in the input dataset. ([here is the full list](https://docs.google.com/spreadsheets/d/1BnoKX7Rug-PFCxSTBuzPevvlFOahwV-UIx4u1xdflDQ/edit?usp=sharing) of subreddits in the training set; 5000 total)\n* The temperature is hardcoded at 0.7 and the top\\_k at 40 because the results become *very* weird otherwise (see the [fanfiction output](https://github.com/minimaxir/gpt-2-fanfiction), which was done at temperature=1.0 and top\\_p=0.9)\n* Subreddits known for their informative titles work better than -----> image !!! -oriented subreddits, unsurprisingly.\n* *Not all generated output will be good/make sense*, as is the case with any other type of text generation. Please don't comment \"wow the text generation sucks!\", it always takes a few tries. (but like the original GPT-2 model, the signal-to-noise ratio is better than RNN/Markov approaches)\n* If you do huge mismatches of the keywords/prompt and the subreddit, the AI might ignore it.\n\nI'm also thinking about creating another SubredditSimulator-type subreddit with generations from all subreddits but on a specific keyword/phrase.\n\nI hope you have fun with it! Let me know if you make any interesting generations!", "link": "https://www.reddit.com/r/MachineLearning/comments/c2wxva/p_app_to_make_aigenerated_submission_titles_for/"}, {"autor": "karenactionsitafaal", "date": "2019-06-25 09:57:42", "content": "With binocular -----> photo !!! ..tiny sparrow's", "link": "https://www.reddit.com/r/MachineLearning/comments/c56357/with_binocular_phototiny_sparrows/"}, {"autor": "elcric_krej", "date": "2019-06-25 09:08:20", "content": "[D] Is there a well maintained list of good \"benchmark\" datasets for ML ? /!/ I'm looking for up to date datasets to  benchmark various algorithms against the performance (both speed and  accuracy) of published models.\n\n&amp;#x200B;\n\nI've found some dataset but the main issue is that they are either:\n\n&amp;#x200B;\n\na)  very old and small, e.g. most datasets hosted by UCI, which are rather  \"easy\" to \"solve\" nowadays and most papers using them came out decades  ago. Even barring that, a lot of the papers dealing with the data are not ideal for benchmarks per-say because they are not very specific in their methodology for splitting into train/test/validate.\n\n&amp;#x200B;\n\nOR\n\n&amp;#x200B;\n\nb)  They are focused on images, e.g. cifrar 100 is pretty decent, and there  are loads of high quality models with known accuracy and available  source code... but, I can't find the equivalent of cifrar 100 for, say,  financial timeseries prediction, or STT, or geospatial movement  predictions for cars... or any problem other than -----> image !!!  classification  -\\_-\n\n&amp;#x200B;\n\nAre  there any well maintained list of datasets that specifically have  various models benchmarked against them ? Or would it be better to just  do reverse-search on this problem, as in, look for interesting papers  that came out in the last few years and use the datasets they used.", "link": "https://www.reddit.com/r/MachineLearning/comments/c55k24/d_is_there_a_well_maintained_list_of_good/"}, {"autor": "WesternHarmonica", "date": "2019-06-24 18:44:53", "content": "[P] Training Mask RCNN for recognizing objects in large images /!/ Hi guys! I've started a project using Facebook's [Mask R-CNN](https://github.com/matterport/Mask_RCNN). The goal is to have it recognize certain small objects from large (HD+) images.\n\nI'm a bit worried how long it will take to train with original -----> image !!!  size. Does it make much difference if I crop the objects out of the image and train them separately? It still should be able to work with large images though.\n\nI am using pre-trained COCO weights and the tool I use for labeling is [VGG Image Annotator](http://www.robots.ox.ac.uk/~vgg/software/via/). Thanks in advance!", "link": "https://www.reddit.com/r/MachineLearning/comments/c4ta3q/p_training_mask_rcnn_for_recognizing_objects_in/"}, {"autor": "NoTechBackground", "date": "2019-06-24 14:27:04", "content": "Confusion on use of the words \"layer\" and \"top layers\" /!/ I am trying to follow this tutorial on printing out the intermediate activations\n\n [https://github.com/gabrielpierobon/cnnshapes/blob/master/README.md](https://github.com/gabrielpierobon/cnnshapes/blob/master/README.md) \n\nAn alternate link: [https://towardsdatascience.com/visualizing-intermediate-activation-in-convolutional-neural-networks-with-keras-260b36d60d0](https://towardsdatascience.com/visualizing-intermediate-activation-in-convolutional-neural-networks-with-keras-260b36d60d0)\n\n&amp;#x200B;\n\n1. There is mention of \"extracts the outputs of the top 12 layers\" but I don't understand where these 12 are coming from. \n\n \n\n    layer_outputs = [layer.output for layer in classifier.layers[:12]] # Extracts the outputs of the top 12 layers \n    activation_model = models.Model(inputs=classifier.input, outputs=layer_outputs) # Creates a model that will return these outputs, given the model input \n\n&amp;#x200B;\n\n2. \n\nA few lines later in the tutorial, there is mention that we are only on our first layer. \n\n \n\n For instance, this is the activation of the first convolution layer for the -----> image !!!  input:\n\n    first_layer_activation = activations[0] print(first_layer_activation.shape) \n\n&amp;#x200B;\n\nFor 1., how can there be 12 layers anywhere? What does that mean? It looks like in the code we only have three: \n\n \n\n    classifier.add(Conv2D(32, (3, 3), padding='same', input_shape = (28, 28, 3), activation = 'relu')) classifier.add(Conv2D(32, (3, 3), activation='relu')) classifier.add(MaxPooling2D(pool_size=(2, 2))) classifier.add(Dropout(0.5)) # antes era 0.25 \u200b # Adding a second convolutional layer classifier.add(Conv2D(64, (3, 3), padding='same', activation = 'relu')) classifier.add(Conv2D(64, (3, 3), activation='relu')) classifier.add(MaxPooling2D(pool_size=(2, 2))) classifier.add(Dropout(0.5)) # antes era 0.25 \u200b # Adding a third convolutional layer classifier.add(Conv2D(64, (3, 3), padding='same', activation = 'relu')) classifier.add(Conv2D(64, (3, 3), activation='relu')) classifier.add(MaxPooling2D(pool_size=(2, 2))) classifier.add(Dropout(0.5)) # antes era 0.25", "link": "https://www.reddit.com/r/MachineLearning/comments/c4os1t/confusion_on_use_of_the_words_layer_and_top_layers/"}, {"autor": "marcopaaah", "date": "2019-06-24 14:11:54", "content": "[D] Precise detection of large number of keypoints /!/ Keypoint detection has successfully been modelled with CNNs that outputs a heatmap tensor of size H x W x K, where K is the number of instance keypoints you want to detect and H and W the output size of the heatmaps. If you want precise detections H and W should ideally be the same size as the input -----> image !!! .\n\n&amp;#x200B;\n\nI want to detect K&gt;=300 using an input image of size 512 x 512. Due to obvious memory limitations I can't use the above naive approach that upscales to the original input size. \n\n&amp;#x200B;\n\nIs anyone aware of some research that addresses this specific issue?", "link": "https://www.reddit.com/r/MachineLearning/comments/c4ojbn/d_precise_detection_of_large_number_of_keypoints/"}, {"autor": "engineheat", "date": "2019-04-21 22:54:22", "content": "Using Naive Bayes for -----> image !!!  classification /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/bfurty/using_naive_bayes_for_image_classification/"}, {"autor": "EmielBoss", "date": "2019-04-24 11:10:06", "content": "Should I use TensorFlow\u2019s Keras API or the TensorFlow Object Detection API for semantic/panoptic -----> image !!!  segmentation? /!/ I wish to implement [Panoptic FPN](https://arxiv.org/abs/1901.02446) (which adds an extra branch to Mask R-CNN for semantic segmentation) using TensorFlow, and use pre-trained models for its ResNet-FPN backbone. However, I am add a crossroads: do I use TensorFlow's Keras API, or the [TensorFlow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection)? Is the latter even capable of semantic segmentation? What do you recommend?", "link": "https://www.reddit.com/r/MachineLearning/comments/bgta97/should_i_use_tensorflows_keras_api_or_the/"}, {"autor": "swordythomas", "date": "2019-04-23 21:01:55", "content": "[P] Predicting Instagram likes for selfies (beta) /!/ Hi! We created an app where a deep learning algorithm can tell you from two selfies, which -----> photo !!!  would get more likes on Instagram.\n\nYou can try Deepsta (beta) on www.findyourlikes.com", "link": "https://www.reddit.com/r/MachineLearning/comments/bglo6x/p_predicting_instagram_likes_for_selfies_beta/"}, {"autor": "bitcoin_analysis_app", "date": "2019-12-10 15:17:54", "content": "[Research] Question Regarding \"Deep Convolutional Spiking Neural Networks for Image Classification\" /!/ Paper can be found here: [https://arxiv.org/pdf/1903.12272.pdf](https://arxiv.org/pdf/1903.12272.pdf)\n\nI am currently investigating the research into Spiking Neural Networks using  Spike-timing-dependent plasticity learning, initially regarding -----> image !!!  processing.\n\nI have now read several papers that discuss \"Spiking Convolutional Neural Networks\", and cannot understand how these particular networks can be convolutional in nature, at least in the same way as backprop trained CNNs are.\n\nThe kernels in the standard CNN are trained by backprop against every possible patch or feature in the preceding layer. So a kernel can detect a feature at any point in the preceding layer.\n\nWhile you can definitely use \"kernels\" with these spiking networks, they would only apply to a set patch of the image right? If you wanted something that ran over all the patches your \"kernel neuron\" would just end up with a link from each neuron in the previous layer and it would be a shit-show. Or you would need to duplicate this kernel neuron x number of times depending on the size of the input, and find some way to keep these neurons with the same input weights.\n\nWhat am I misunderstanding here? Do you simply end up with a whole pile of duplicate kernels across all the patches? This would definitely work but is it optimal?", "link": "https://www.reddit.com/r/MachineLearning/comments/e8rvna/research_question_regarding_deep_convolutional/"}, {"autor": "strangecosmos", "date": "2019-01-28 09:36:45", "content": "[N] Report: Tesla is using behavior cloning (i.e. supervised imitation learning) for Autopilot and full self-driving /!/ The full story is reported by [Amir Efrati in The Information](https://news.ycombinator.com/from?site=theinformation.com). (The caveat is that this has been reported based on information from unnamed sources, and as far as I know only one reporter has confirmed this story.)\n\nHere\u2019s the key excerpt from the article: \n\n&gt;Tesla\u2019s cars collect so much -----> camera !!!  and other sensor data as they drive around, even when Autopilot isn\u2019t turned on, that the Autopilot team can examine what traditional human driving looks like in various driving scenarios and mimic it, said the person familiar with the system. It uses this information as an additional factor to plan how a car will drive in specific situations\u2014for example, how to steer a curve on a road or avoid an object. Such an approach has its limits, of course: behavior cloning, as the method is sometimes called\u2026 \n&gt;\n&gt;But Tesla\u2019s engineers believe that by putting enough data from good human driving through a neural network, that network can learn how to directly predict the correct steering, braking and acceleration in most situations. \u201cYou don\u2019t need anything else\u201d to teach the system how to drive autonomously, said a person who has been involved with the team. They envision a future in which humans won\u2019t need to write code to tell the car what to do when it encounters a particular scenario; it will know what to do on its own.", "link": "https://www.reddit.com/r/MachineLearning/comments/aklwv8/n_report_tesla_is_using_behavior_cloning_ie/"}, {"autor": "MistWeaver80", "date": "2019-01-11 11:36:01", "content": "Machine learning for -----> image !!!  restoration.", "link": "https://www.reddit.com/r/MachineLearning/comments/aeujiy/machine_learning_for_image_restoration/"}, {"autor": "akhilmaliackal", "date": "2019-03-03 09:32:30", "content": "-----> Image !!!  inpainting using GAN /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/awsirb/image_inpainting_using_gan/"}, {"autor": "ricvolpi", "date": "2019-04-01 12:43:08", "content": "[R] Model Vulnerability to Distributional Shifts over Image Transformation Sets /!/ Hi! We've released the preprint of our work \"Model Vulnerability to Distributional Shifts over Image Transformation Sets\". Code is available at [https://github.com/ricvolpi/domain-shift-robustness](https://github.com/ricvolpi/domain-shift-robustness). \n\nTLDR; we propose one way to determine the (concatenation of) -----> image !!!  transformations the a given (black-box) model is most vulnerable to, via random search and evolution-based search. We propose a training procedure where the vulnerability regions for the current model are searched for and harnessed throughout the training phase, by defining proper data augmentation rules. \n\nFeel free to ask any questions. Any feedback is appreciated!\n\nCheers,                                                                                                                                                                             Riccardo", "link": "https://www.reddit.com/r/MachineLearning/comments/b81uwq/r_model_vulnerability_to_distributional_shifts/"}, {"autor": "ReacH36", "date": "2019-04-01 03:31:44", "content": "[D] Train Dev Test Split Ratio for Large Datasets? /!/ Currently working on a transfer-learning classification model for an -----> image !!!  dataset with over 800k -----> image !!! s. I used to follow the 70-30 train test split rule of thumb, but am now re-considering. \n\nMy Questions:\n\n1. Is there any merit to a more extreme split? Maybe say 0.98/0.01/0.01? \n2. Are splits for transfer learning models different in general?", "link": "https://www.reddit.com/r/MachineLearning/comments/b7ws7c/d_train_dev_test_split_ratio_for_large_datasets/"}, {"autor": "Aperture314159", "date": "2019-03-31 20:28:54", "content": "[Project] Applying super-resolution ideas to drug discovery /!/ I've noticed in a lot of ML work on drug discovery, things like ECFP (extended connectivity fingerprints) are used to represent molecules in a way that is more machine readable. However, this is fundamentally a lossy method, and the original data (usually a SMILES string) is lost or degraded. Would a project that tried to reassemble a SMILES string from ECFP data be interesting? I'm trying to pin something down for a summer research project. This would be similar in theory to enhancing the resolution of a low quality/compressed -----> image !!!  such as demonstrated in a lot of early 2018 papers. \n\nI just don't know enough chem to know if this would be worth doing.", "link": "https://www.reddit.com/r/MachineLearning/comments/b7sc0g/project_applying_superresolution_ideas_to_drug/"}, {"autor": "Aperture314159", "date": "2019-03-31 20:17:04", "content": "Applying -----> image !!!  super-resolution to drug discovery featurization /!/ I've noticed in a lot of ML work on drug discovery, things like ECFP (extended connectivity fingerprints) are used to represent molecules in a way that is more machine readable. However, this is fundamentally a lossy method, and the original data (usually a SMILES string) is lost or degraded. Would a project that tried to reassemble a SMILES string from ECFP data be interesting? I'm trying to pin something down for a summer research project. This would be similar in theory to enhancing the resolution of a low quality/compressed image such as demonstrated in a lot of early 2018 papers.\n\nI just don't know enough chem to know if this would be worth doing.", "link": "https://www.reddit.com/r/MachineLearning/comments/b7s71w/applying_image_superresolution_to_drug_discovery/"}, {"autor": "nohat", "date": "2019-03-31 17:48:22", "content": "[P] A simple utility for the Danbooru2018 dataset /!/ I made a [simple python utility](https://github.com/reidsanders/danbooru-utility) to work with Gwern's [Danbooru2018](https://www.gwern.net/Danbooru2018), a large, well annotated anime style -----> image !!!  dataset. I decided to package it up in case other people will find it useful too. I've been using it to make datasets for stylegan training. It will filter by tags, ratings, and score. It can resize and has an optional face detection crop that tries to include more of the body by positioning the face in the upper part of the image. ", "link": "https://www.reddit.com/r/MachineLearning/comments/b7qghe/p_a_simple_utility_for_the_danbooru2018_dataset/"}, {"autor": "konasj", "date": "2019-06-05 07:47:28", "content": "[D] Deep Learning and Chemical Physics - research oriented companies and startups? /!/ Dear community,\n\nI am right now pursuing a PhD at the intersection of molecular science and machine learning during which I am researching new advanced sampling techniques and generative models for molecular structures respecting physical constraints. \n\nWhile there are plenty of opportunities to just stay in academia I m curious about companies who do applied research in related fields, like searching for new materials using computational methods (in particular modern machine learning techniques) or designing functional proteins using such techniques.\n\nI am personally very interested to eventually work on something \"real\" that exists in the physical world (e.g. designing a new catalysis membrane for waste decomposition vs building new -----> image !!!  filters for Snapchat). But besides current endeavors in rather pure research e.g. as done by DeepMind (AlphaFold) or Google AS I don't know smaller/less known companies with maybe much more focus on real applications. There are obviously pharmaceutical firms also starting to work in this field, but right now I am more curious about possible other applications.\n\nDo you have any ideas or suggestions what companies might be worth to look at? \n\nThanks a lot!", "link": "https://www.reddit.com/r/MachineLearning/comments/bwzzms/d_deep_learning_and_chemical_physics_research/"}, {"autor": "AncientSuntzu", "date": "2019-06-05 05:03:13", "content": "A short -----> film !!!  written by an AI!", "link": "https://www.reddit.com/r/MachineLearning/comments/bwysu1/a_short_film_written_by_an_ai/"}, {"autor": "vikigenius", "date": "2019-06-04 22:56:12", "content": "[Discussion] Text generation and Discrete latent space models /!/ After looking at this paper and the -----> image !!!  generation quality : [Generating Diverse High-Fidelity Images with VQ-VAE-2](https://arxiv.org/abs/1906.00446), I wondered if such models can be used for text generation. And I noticed that most of the similar papers discussing discrete latent space models, don't talk about text at all, all the experiments seem to be done on images or audio domain.\n\nWhat gives ? Intuitively I would think it makes sense for discrete latent space models to be quite useful for text considering it's discrete nature. Can anyone explain this or provide links to papers that successfully generate quality text from a discrete latent space?", "link": "https://www.reddit.com/r/MachineLearning/comments/bwvckp/discussion_text_generation_and_discrete_latent/"}, {"autor": "ankudini", "date": "2019-06-04 16:21:50", "content": "Grouping already known objects on an -----> Image !!! . /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/bwqn3z/grouping_already_known_objects_on_an_image/"}, {"autor": "imprezobus", "date": "2019-06-04 09:24:31", "content": "Something simple to determine if -----> image !!!  contains predefined text /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/bwmovm/something_simple_to_determine_if_image_contains/"}, {"autor": "tiramirez", "date": "2019-06-04 03:06:08", "content": "[R] Applying Machine Learning and Discrete Choice Modeling to understand the quality of urban landscape /!/ #  Wekun ([wekun.ing.puc.cl](http://wekun.ing.puc.cl)) is a game that seeks to understand how people perceive public space and, thus, understand what determines the quality of these spaces. \n\nWe ask people to chose between images to measure their preferences. Then, the information collected is processed with Machine Learning algorithms and discrete choice models, in order to understand the role played by different elements of the built environment and nature in the preferences of people. The methodology is not new (links below), but we have incorporated a section to register sociodemographic information aiming to find heterogeneity among observers. We use Discrete Choice Modelling as a benchmark to Machine Learning Algorithms, typically referred to as black boxes, to overcome the explainability problems involved with them.\n\nhttps://i.redd.it/xamzwiv0a9231.jpg\n\n**Please comment on the following subjects to help us!**\n\n1. Any recommendation of semantic segmentation algorithms? or Object detection?\n2. For the success of this research, we need your help evaluating photos of public spaces and sharing this message to have the opinion of more people!\n\n**Some references:**\n\nRossetti, T., Lobel, H., Rocco, V., &amp; Hurtubia, R. (2019). Explaining subjective perceptions of public spaces as a function of the built environment: A massive data approach. Landscape and urban planning, 181, 169-178. ([link](https://www.sciencedirect.com/science/article/pii/S0169204618310260))\n\n Salesses, P., Schechtner, K., &amp; Hidalgo, C. A. (2013). The collaborative -----> image !!!  of the city: mapping the inequality of urban perception. *PloS one*, *8*(7), e68400. ([link](http://pulse.media.mit.edu/papers/))\n\nDubey, A., Naik, N., Parikh, D., Raskar, R., &amp; Hidalgo, C. A. (2016, October). Deep learning the city: Quantifying urban perception at a global scale. In *European conference on computer vision* (pp. 196-212). Springer, Cham.  ([link](http://pulse.media.mit.edu/papers/))", "link": "https://www.reddit.com/r/MachineLearning/comments/bwjt4p/r_applying_machine_learning_and_discrete_choice/"}, {"autor": "dondraper36", "date": "2019-01-09 10:47:31", "content": "[D] Text -----> image !!!  super-resolution: prerequisites for understanding /!/ Hello!\nI am interested in text image super-resolution. I do not know, however, what are the prerequisites necessary to understand papers on the subject. For example, is it enough to pass CS229 and CS231n?\n\nCan you also please recommend a few must-read and fundamental articles and papers to understand the basics? I have skimmed through the ocean of papers on super-resolution available at arxiv.org, but unfortunately, I am not yet able to pick the 'right' articles. \n\nThank you! ", "link": "https://www.reddit.com/r/MachineLearning/comments/ae5pky/d_text_image_superresolution_prerequisites_for/"}, {"autor": "ahmedbesbes", "date": "2019-01-09 09:51:40", "content": "Demo: Automatic -----> image !!!  captioning with visual attention using PyTorch", "link": "https://www.reddit.com/r/MachineLearning/comments/ae5cul/demo_automatic_image_captioning_with_visual/"}, {"autor": "gwern", "date": "2019-01-08 15:25:46", "content": "[R] \"Tencent ML-Images: A Large-Scale Multi-Label -----> Image !!!  Database for Visual Representation Learning\", Wu et al 2019 {Tencent} [18m images (-----> Image !!! Net+Open -----> Image !!! s), 11k categories, pretrained ResNet-101 models]", "link": "https://www.reddit.com/r/MachineLearning/comments/adv8jl/r_tencent_mlimages_a_largescale_multilabel_image/"}, {"autor": "kadskakm", "date": "2019-01-21 03:22:57", "content": "Are there any high resolution -----> image !!!  segmentation datasets? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ai5uoy/are_there_any_high_resolution_image_segmentation/"}, {"autor": "amusciano", "date": "2019-04-29 09:37:24", "content": "[p] exif-pp: inject a json object into an -----> image !!! 's Metadata. Useful for image description/classification", "link": "https://www.reddit.com/r/MachineLearning/comments/binjjm/p_exifpp_inject_a_json_object_into_an_images/"}, {"autor": "cfi2000", "date": "2019-04-29 02:46:02", "content": "where can i find cctv datas? /!/ im finding industrial or factorial cctv video for training purpose\n\nlike this one\n\n![video](fjssl1mu74v21 \"-----> camera !!!  fixed and industrial facility at background\")\n\ni assume that there is webpage or something", "link": "https://www.reddit.com/r/MachineLearning/comments/bijdf0/where_can_i_find_cctv_datas/"}, {"autor": "shamitlal", "date": "2019-04-28 21:14:26", "content": "[D] Why does Beta-VAE help in learning disentangled/independent latent representations? /!/ In Beta-VAE paper (https://openreview.net/pdf?id=Sy2fzU9gl), the authors mentioned that having Beta &gt; 1 helps the network in learning independent latent representations. However, in VAE, the posterior distribution itself is assumed to be a Gaussian with a diagonal covariance matrix, i.e.\n\nq(z|x) = N(U(x),Cov(x))\nwhere Cov(x) is a diagonal matrix.\n \nThis means that we are inherently generating latents that will be independent given an input -----> image !!!  x. So why does increase learning pressure on the KL divergence term between posterior and Gaussian prior should help any more in learning independent latents when posterior is already assumed to be independent?", "link": "https://www.reddit.com/r/MachineLearning/comments/big5cs/d_why_does_betavae_help_in_learning/"}, {"autor": "danielsafs", "date": "2019-04-28 17:17:01", "content": "Find center of pupil /!/ Could someone give me an intuition or starting point on what technic should I research to create and train a AI to find the center of the pupil in a -----> image !!!  of frontal face of humans. (Not real time)", "link": "https://www.reddit.com/r/MachineLearning/comments/bidizu/find_center_of_pupil/"}, {"autor": "rparvez", "date": "2019-04-28 15:58:26", "content": "What are the SOTA research on adversarial examples in -----> image !!!  recognition? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/bicoss/what_are_the_sota_research_on_adversarial/"}, {"autor": "eukaryote31", "date": "2019-04-28 00:27:05", "content": "[D] Global Context block spatial resolution /!/ The paper in question is here: [https://arxiv.org/pdf/1904.11492.pdf](https://arxiv.org/pdf/1904.11492.pdf)\n\n&amp;#x200B;\n\nThe authors claim that assuming similar attention level for different Query points, a lot of computation can be saved by making a query-independent self-attention layer. That sounds good, but the following diagram of their architecture is confusing to me:\n\n![img](7l0c4quudwu21 \"diagram 4(d) from the paper\")\n\nAfter the Transform section, when the result is added back to the original -----> image !!! , each channel only gets one value broadcast over the entire plane. I had assumed that the goal was to calculate a global attention map (i.e query-independent and key-dependent). Could someone please explain why this is?", "link": "https://www.reddit.com/r/MachineLearning/comments/bi5jpb/d_global_context_block_spatial_resolution/"}, {"autor": "ZuperPippo", "date": "2019-04-27 11:01:31", "content": "[Yolo help] Need a working updated tutorial /!/ Hi. I gotta detect 6 different dice (are all unique) for my masters thesis. I got the -----> camera !!!  working and my task is to run a python script from a Linux distro, so the whole project is going to run under Ubuntu.    \n\nI found yoloV3 and already trained a network last 2 days, however without any success. I haven't got time to test is every 2 days and wait for any results, so I tried to run it on Windows + GPU (only have Ubuntu laptop with integrated gpu and Windows with gtx1060).    \n\nI can't make it to work on windows and tried every tutorial and github repo. My cmd \"make\" doesn't work, tried multiple software (Cmake, Gycwin, Mingw, etc) to make it run; the tutorial constantly explains how to use darknet.exe and I don't have an exe (so I guess it's created with \"make\" command).    \n\nAdditional info: I ran the network once, had cca 200 photos with labels, changed number of classes, filters.      \nCan anybody please help or link me to a working, updated tutorial? It looks so messy to me", "link": "https://www.reddit.com/r/MachineLearning/comments/bhxzi6/yolo_help_need_a_working_updated_tutorial/"}, {"autor": "plentyofnodes", "date": "2019-09-01 22:42:31", "content": "[P] -----> Image !!!  Manipulation and Classification: Identifying traffic objects /!/ This is an example of an image classifier built with Keras to identify presence of traffic objects.\n\nSpecifically, the VGG16 network was used as the pre-trained model, and the images themselves were manipulated using PIL for cropping purposes, and using Grad-CAM and cv2 to respectively generate a heatmap, and superimpose the heatmap on the relevant image.\n\n[Findings can be found here.](https://www.michael-grogan.com/image-recognition-with-keras-convolutional-neural-networks/) Hope that you find this of use, and any feedback or tips greatly appreciated.", "link": "https://www.reddit.com/r/MachineLearning/comments/cygpa7/p_image_manipulation_and_classification/"}, {"autor": "plentyofnodes", "date": "2019-09-01 22:40:15", "content": "-----> Image !!!  Manipulation and Classification: Identifying traffic objects /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/cygobx/image_manipulation_and_classification_identifying/"}, {"autor": "naxty1995", "date": "2019-09-01 16:37:37", "content": "Scalable -----> image !!!  classification with ONNX.js and AWS Lambda", "link": "https://www.reddit.com/r/MachineLearning/comments/cyc618/scalable_image_classification_with_onnxjs_and_aws/"}, {"autor": "SolitaryPenman", "date": "2019-07-26 06:59:23", "content": "[D] Are there any papers with Normalizing Flow-based generative models that show empirical results on 1d/2d densities? /!/ All the normalizing flows-based papers I read (NICE, RealNVP, Glow, etc.) show experiments on high dimensional -----> image !!!  datasets. I am looking for works that analyze the capacity of NFs to learn simple 1/2d distributions. I am aware of the 2d experiments in [Rezende and Mohamed, 2015] but as far as I understand, for the 2d datasets they train by directly minimizing KL (and do no train using samples) because the analytic inverse of Planar flow does not exist.", "link": "https://www.reddit.com/r/MachineLearning/comments/chzk0y/d_are_there_any_papers_with_normalizing_flowbased/"}, {"autor": "eslamalmahdy", "date": "2019-05-10 16:34:59", "content": "What science says about this viral breastmilk -----> photo !!! ", "link": "https://www.reddit.com/r/MachineLearning/comments/bn0gsa/what_science_says_about_this_viral_breastmilk/"}, {"autor": "jpedrofs", "date": "2019-05-09 19:52:43", "content": "[News] These fashion models were created using AI, but they look spookily life-like /!/ Is anyone aware of public research done on full body -----> image !!!  generation?[Computer generated fashion models by DataGrid](https://www.businessinsider.com/datagrid-ai-created-spookily-life-like-fashion-models-2019-5)", "link": "https://www.reddit.com/r/MachineLearning/comments/bmod5s/news_these_fashion_models_were_created_using_ai/"}, {"autor": "mingyuliutw", "date": "2019-05-09 17:57:12", "content": "[R] Few-Shot Unsupervised -----> Image !!! -to------> Image !!!  Translation", "link": "https://www.reddit.com/r/MachineLearning/comments/bmmyae/r_fewshot_unsupervised_imagetoimage_translation/"}, {"autor": "mingyuliutw", "date": "2019-05-09 17:31:23", "content": "[R] Few-Shot Unsupervised -----> Image !!! -to------> Image !!!  Translation", "link": "https://www.reddit.com/r/MachineLearning/comments/bmmn01/r_fewshot_unsupervised_imagetoimage_translation/"}, {"autor": "fj33xx", "date": "2019-05-08 02:30:05", "content": "[P] Collection of Computer Vision Datasets /!/ The computer vision communities are constantly contributing new datasets and unfortunately it's not easy to track.\n\nI created a site (visualdata.io) to aggregate the latest computer vision datasets in one place with easy-to-use search functionality. Each dataset has been tagged with one or more topics like -----> image !!!  classification, object detection etc and you can dynamically filter with keywords. It also adopts an instagram like scrolling interface to show case the example images in the dataset. \n\nHope the site is helpful to discover datasets for your projects. New datasets are always being added to the site. Please let me know any feedback you may have!", "link": "https://www.reddit.com/r/MachineLearning/comments/blzsrq/p_collection_of_computer_vision_datasets/"}, {"autor": "downtownslim", "date": "2019-05-07 16:49:38", "content": "[R] FAIR: Advancing self-supervision, CV, NLP /!/ \\&gt; Our recent advances in natural language processing (NLP) and computer vision (CV) show how work in content understanding is producing benefits. In NLP, we've developed a shared multilingual embedding space that can act as a sort of lingua franca to help take action on harmful content even in low-resource languages. In CV, we've built on our industry-leading research to identify content in more parts of an -----> image !!!  and achieve record-setting accuracy using hashtags for video understanding.\n\n&amp;#x200B;\n\nPost: [https://ai.facebook.com/blog/advances-in-content-understanding-self-supervision-to-protect-people](https://ai.facebook.com/blog/advances-in-content-understanding-self-supervision-to-protect-people)", "link": "https://www.reddit.com/r/MachineLearning/comments/blt1co/r_fair_advancing_selfsupervision_cv_nlp/"}, {"autor": "hooba_stank_", "date": "2019-04-03 15:59:18", "content": "[P] Simple Spring Boot application demo to deploy TensorFlow models /!/ Demo application that deploys TensorFlow models as a SpringBoot microservice.\n\nExposes REST services (with Swagger html docs) and simple web page for -----> image !!!  recognition using pretrained MobilenetV2.\n\nHas configs to deploy locally, on Docker, PCF or Heroku.\n\n[https://github.com/grolex18/tensorboot](https://github.com/grolex18/tensorboot)\n\n", "link": "https://www.reddit.com/r/MachineLearning/comments/b8zso6/p_simple_spring_boot_application_demo_to_deploy/"}, {"autor": "raichet", "date": "2019-04-03 12:45:19", "content": "[P] Object Detection Project -----> Image !!!  Labeling Help /!/ Hey guys,\n\n&amp;#x200B;\n\nI am working on a school project where I have to detect apples on a tree. (context changed to preserve anonymity) \n\n&amp;#x200B;\n\nThe images come in grayscale, and contain a single tree with many apples on it. The only classes I am concerned with are apple or not apple, and my goal is to draw bounding box around all apples detected. I was thinking about using an object detection architecture such as Faster R-CNN or similar region-based methods. (May be an overkill? Even so, I would practical experience anyway.)\n\n&amp;#x200B;\n\n**Problem**: I have never done image annotation of any kind. There seem to be multiple implementation of the mentioned architecture online, requiring different formats of annotation. (FAIR's Detectron uses COCO JSON format, some I found uses PASCAL VOC format, etc) \n\n&amp;#x200B;\n\nCan someone explain to me the differences between each format and the reason for the unique formats? Can I also get some recommendations or even tutorial on how to take these raw images and structure them for successful preprocessing so I can train my architecture with them?\n\n&amp;#x200B;\n\nThank you.", "link": "https://www.reddit.com/r/MachineLearning/comments/b8xhi1/p_object_detection_project_image_labeling_help/"}, {"autor": "RedditAcy", "date": "2019-04-03 01:05:56", "content": "[P]My Machine Learning Journal #9: Completely understanding the anime GAN in Keras /!/ Hey guys, so in the [last part](https://www.reddit.com/r/MachineLearning/comments/b8jxrf/pmy_machine_learning_journey_8_my_struggles_in/), I ended up successfully implementing [this](https://github.com/pavitrakumar78/Anime-Face-GAN-Keras) github's Keras implementation of anime GAN. I successfully trained a GAN using that repository's code with my own dataset (about 5000 pictures of 64 \\* 64 \\* 3 shaped anime faces), extract the generator, and the generator's outputs looked like this: \n\n&amp;#x200B;\n\n![img](we5wjzui2yp21 \"Some of 'em are creepy but lol\")\n\nI wanted to dive deeper though, I find myself **understanding a ML project when I know the details of the model architectures, so I started from there.** I knew that a GAN is the following: a generator + a discriminator, a generator accepts a noise, and outputs an -----> image !!! , the discriminator takes in -----> image !!! s, and outputs a scalar constant representing the \"probability the -----> image !!!  came from the real distribution\".\n\nI called model.summary() on both the discriminator and the generator. The generator accepts a noise sized of (1, 1, 100), it runs that noise through a couple convolutional neural networks. The first ConvTransposeLayer is called with kernel size 4 by 4 and 512 filters. This turns the (1, 1, 100) into (4, 4, 512). Then, we take it through 4 more conv layers,  batch-normalization, and leaky relu layers, at the end, the dimension goes from (4, 4, 512) =&gt; (8, 8, 256) =&gt; (16, 16, 128) =&gt; (32, 32, 64) =&gt; (64, 64, 3), this really helped me understand what is this generator really doing inside. The discriminator is similar, we feed it through a few convolutional layers, decreasing the parameters at each layer, and connect the end of those layers with a Fully connected layer and run it through a sigmoid function to get that scalar value. \n\nThen, after I understood completely what the two models are doing, I dug deep into the training code. **I think (correct me if I am wrong), the code for models are always going to be somewhat similar, but the training the model and managing the datasets can be attacked in so many packages and directions.** It turns out that the training code and implementation details for this project wasn't too far from the GAN I copied for generating MNIST data. Here're the implementation steps for training the GAN: \n\n1. make random noises\n2. predict those noises with the generator (outputs an image, or many images depending on batch size)\n3. get data from the real distribution\n4. label the fake/generated outputs with 0, label the real ones with 1\n5. concatenate the fake and real training examples so that it looks something like:  \\[\\[fake1, 0\\], \\[fake2, 0\\], ..., \\[real1, 1\\], real2, 2\\],...\\]\n6. train the discriminator on this dataset with train\\_on\\_batch(combined data X, combined data label Y), train\\_on\\_batch will carry out the job, running X through discriminator, getting difference between the discriminator outputs and its label, then modify the weights accordingly. \n7. train the generator with GAN.train\\_on\\_batch (ganX, 1), where ganX is some noise. We are trying to train the generator so that it produces results that are going to be predicted to be 1's. Be careful, we have to run train\\_on\\_batch with the GAN because we are **inputting the noise and getting out the scalar value** (which again, we hope to be 1). So turn discriminator.trainable to false, we don't want to be adjusting the weights for the discriminator. \n8. save the weights\n\nHopefully that was informative, even though my main goal is to just document my journey. \n\n# My Questions of the Day: \n\n1) Is it usually importing the packages, writing the code for data preprocessing, model, training that you lose the most brain cells over or tuning the hyperparameters in those models? \n\n2) I recall Andrew Ng's book, Machine Learning Yearning, suggests a very routine way of tuning those hyperparameters base on a few metrics, are there helpful programs out there that auto tunes hyperparameters? Lol imagine making a neural network that auto tunes neural networks that tunes neural networks that tunes neural networks that tunes ne... ", "link": "https://www.reddit.com/r/MachineLearning/comments/b8rki0/pmy_machine_learning_journal_9_completely/"}, {"autor": "rowsiearruba", "date": "2019-05-28 13:01:48", "content": "Paper recommendations for -----> image !!!  recognition with lots of classes? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/btzre0/paper_recommendations_for_image_recognition_with/"}, {"autor": "alghar", "date": "2019-05-27 22:42:03", "content": "Impressions from Embedded Vision Summit 2019 /!/ I had an opportunity to visit the showcase of the Embedded Vision Summit (held this week in Santa Clara Convention Center). The event was certainly bigger and more colorful than the previous ones. Aside from the usual vendors, I came across many new names offering various products, services, and tools covering the gamut in the field of computer vision. Clearly AI-based vision processing and automotive overshadowed the traditional computer vision technologies and use cases. As expected, there were number of new companies announcing their new vision-optimized AI accelerator chips and Intellectual properties. (I must say that it is becoming increasingly difficult to keep track of all the players in this domain and I might be forced to add a new tab to my excel sheet. Reducing the font size does not cut it anymore).\n\nI was struck by one notable trend and that is vertical integration. There were several vendors that had offerings covering the entire spectrum for the automotive application. They offered autonomous driving software stack, ADAS (Advanced Driver Assistance Systems) hardware, AI accelerator ASICs, and in some cases even their IP.\n\nWhile there were many promising companies with breakthrough technologies, I was particularly impressed by the technology and achievements of a few that are most relevant to this forum. Below is a brief summary:\n\n&amp;#x200B;\n\nMythic\n\nMythic ([www.mythic-ai.com](https://www.mythic-ai.com)) is a fabless semiconductor company based in Austin and Redwood City building AI edge accelerator chips. Their initial target applications are smart speakers, drones, battery-powered video monitors specialized smartphones for certain vertical markets. Their implementation of deep neural networks is based on flash-based analog technology. This approach reduces the need to retrieve network weights from memory leading to a significant power savings (10x). \n\nI felt they have a very clear and focused mission and were able to effectively convey their differentiation (10x power savings). They showed various reference boards with different form factors. One in particular caught my eye and that was a PCIe card hosting eight of their chips. I am guessing they are targeting power-restricted edge servers. Keep in mind all server companies are building edge servers that are only slightly bigger and thicker than an iPad, with hardened enclosure and having a fan is not an option.\n\nWhile the power advantage of their analog approach is undisputable, this advantage becomes less pronounced when the competitors opt to use finer process nodes. Despite such a competitive threat, I have never come across a startup with innovative technology that has been able to maintain their lead without reinvention. My guess is that Mythic will continue to innovate.\n\n&amp;#x200B;\n\n[Xnor.ai](https://Xnor.ai)\n\n[Xnor.ai](https://Xnor.ai) ([www.xnor.ai](https://www.xnor.ai)) is a company based in Seattle that has developed very low footprint and optimized Deep Learning CNN models for computer vision tasks. These models can run on low-power SoC\u2019s with embedded Arm CPUs or dedicated computer vision hardware. Their initial target markets are Home Security, Home Automation, and Smart Appliances. They seem to have a close partnership with Ambarella. The beauty of their story is that they can enable \u201clegacy hardware\u201d to tackle complex vision processing tasks such as object, or face identification as well as more complex tasks dealing with live video feeds. As an example, large video surveillance -----> camera !!!  companies can add AI-based vision processing features without major hardware rework.\n\nI would guess their technology is a godsent for traditional MCU companies (the likes of Microchip, On Semi., Silicon Labs, and ST) that are yet to have MCUs with dedicated AI acceleration hardware.\n\nLike any other upstart, they also have their challenges. Computer vision models are getting bigger and more complex and there are limits on what optimization and compression can achieve.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nHailo\n\nHailo ([www.hailo.ai](https://www.hailo.ai)) is an Israeli company that has been able to accomplish a tremendous amount with relatively limited funding (\\~$25M I think?). I saw live demos of their Hailo-8 chip and was impressed by its performance metrics. Tiny chip with no heat sink. Impressive peak performance of 26 TOPs and 2.8 TOPs/W (for an AI edge inference accelerator).\n\nAlthough they are targeting most edge applications (autonomous vehicles, smart cameras, smartphones, drones, AR/VR), I got the sense that they have a special affinity toward the automotive segment. This makes sense since their performance is among the highest that I have seen (among edge inference chips) and no other edge application values performance more than the automotive segment. Alternatively, they may have been inspired by Intel\u2019s acquisition of Mobileye for $15B. These are merely my guesses and both reasons are perfectly valid in my book.\n\nAside from the technical mumbo jumbo, their unwavering focus, infectious optimism, and determination impressed me the most.\n\n&amp;#x200B;\n\nAl Gharakhanian\n\nCheck out prior posts [here](https://aihardwareupdate.com)", "link": "https://www.reddit.com/r/MachineLearning/comments/btsf2a/impressions_from_embedded_vision_summit_2019/"}, {"autor": "cdossman", "date": "2019-05-27 17:59:10", "content": "[Research] Practical Classes for Robots /!/  [https://medium.com/ai%C2%B3-theory-practice-business/practical-classes-for-robots-8a0ee915e2b8](https://medium.com/ai%C2%B3-theory-practice-business/practical-classes-for-robots-8a0ee915e2b8) \n\n Robots need to learn behaviors in intuitive and practical ways for widespread deployment in human environments. To learn a robot behavior end-to-end, we train a variant of the ResNet that maps eye-in-hand -----> camera !!!  images to end-effector velocities. In our setup, a human teacher demonstrates the task via joystick. We show that a simple servoing task can be learned in less than an hour including data collection, model training and deployment time. Moreover, 16 minutes of demonstrations were enough for the robot to learn the task.", "link": "https://www.reddit.com/r/MachineLearning/comments/btp3nu/research_practical_classes_for_robots/"}, {"autor": "spokhrel", "date": "2019-05-27 15:52:28", "content": "Open -----> Image !!!  By Matplotlib and OpenCV | Python", "link": "https://www.reddit.com/r/MachineLearning/comments/btnkyt/open_image_by_matplotlib_and_opencv_python/"}, {"autor": "spokhrel", "date": "2019-05-27 12:19:13", "content": "Open -----> Image !!!  By Matplotlib and OpenCV | Python /!/ By this simple code you can open any images using Matplotlib or OpenCV. Matplotlib and OpenCv are both python library used in computer vision and many more applications.", "link": "https://www.reddit.com/r/MachineLearning/comments/btldk0/open_image_by_matplotlib_and_opencv_python/"}, {"autor": "spokhrel", "date": "2019-05-30 13:51:49", "content": "Open -----> Image !!!  By Matplotlib and OpenCV | Python", "link": "https://www.reddit.com/r/MachineLearning/comments/butf9h/open_image_by_matplotlib_and_opencv_python/"}, {"autor": "clavamxr", "date": "2019-05-30 13:47:08", "content": "Grad student specialising in ML and DS, wondering if I should all my eggs in one basket - CV? /!/ Hi, I'm a grad student pursuing Ml and DS from UCSD. I've done courses in Deep Learning, Statistical Learning, Sensing and Estimation,  -----> Image !!!  processing and Recommender Systems - and got a few projects. I'm good at Python, and using PyTorch for developing CNNs ( familiar with TF and Keras, but not very fluent). I'm at the end of my first year, and I've got my summer ahead of me, after which I want to start hunting for jobs. One wrong decision I made was to take up a project under a Prof in the summer on Data Science, which I think has made my work too diversified- basically a little bit of everything but not enough depth in one. I think this was a bad idea because as a grad student I'm expected to me really knowledgeable in one field. Of all the things I've done, I'm interested in CV the most, as it looks the most exciting and I've had fun doing projects on it. I am planning to do a classical CV course from the CSE department next quarter. I was wondering if I'm making mistake by deciding to give all my energies to CV. Are there enough job opportunities out there in CV to a Master's student to pursue? Or should I dabble a bit everywhere - a few projects on NLP etc, so that I have a shot at any ML related opening?\n\nTL;DR - Masters student trying to choose between focusing on CV or doing a bit of everything, for ML related software jobs.\n\nThank you! \n\nP.S - I've previous work ex as a Modem Engineer (1 year) in Qualcomm, before I went to grad school, but nothing in the SW industry or ML field.", "link": "https://www.reddit.com/r/MachineLearning/comments/butdgg/grad_student_specialising_in_ml_and_ds_wondering/"}, {"autor": "lujkuj", "date": "2019-05-30 13:31:02", "content": "[D] CycleGAN performance peaks before first epoch is finished--help needed! /!/ Hi,\n\nI have been trying to use an unpaired -----> image !!! -to------> image !!!  style transfer GAN to convert shoes to dresses. After trying a few different algorithms (including DiscoGAN, StarGAN, and MUNIT), I settled on CycleGAN as it has given me the best results.\n\nI have collected 99,500 and 114,000 images of dresses and shoes, respectively. All of the shoes and dresses are the same orientation and scale (within category). Compared to other projects, I think my dataset quality is quite good.\n\nI am using an implementation by [aitorzip](https://github.com/aitorzip/PyTorch-CycleGAN) ([my fork](https://github.com/lpupp/PyTorch-CycleGAN)). I have tried a few different configurations but haven\u2019t been able to get anything better than the default configurations.\n\nThe quality of the generated images initially progresses well (until around 1/3 of an epoch\u201430,000 steps on a batch size of 1). After that, even after 32,000, the results worsen. The mode seems to collapse after the first epoch has been completed.\n\n&amp;#x200B;\n\n[Image 1: progression of generator's outputs and reconstruction over time for shoes2dresses task](https://i.redd.it/a8r08leroc131.jpg)\n\nThe results generated by the model after 30,000 steps are still very checkered. I tried replacing the transposed convolutions with upsamples (see [here](https://distill.pub/2016/deconv-checkerboard/ 2)) but it resulted in a mode collapse very quickly.\n\nI am not sure if I have already reached the best performance I can expect with such small images (64x64) but it\u2019s hard to believe since this would imply that the \u201cbest results\u201d are achieved before the GAN has seen the whole training set. I am unable to get anything comparable with larger images (128x128 or 256x256).\n\nAdditionally, I feel like I am battling many issues at the same time. First, I am finding it difficult to get high-quality outputs. Then, the GAN is very unstable and collapses frequently. Finally, it is often the case the even though the GAN produces the same output for any input, it is able to reconstruct the input image (see image 2)\n\n&amp;#x200B;\n\n[Image 2: reconstructs output of mode collapse generator well](https://i.redd.it/y40069tvoc131.jpg)\n\nI have already played around with many of the training parameters (batch size, learning rate, decay rate, loss function composition, added feature matching, Wasserstein loss, added noise to the input images, added extra layers to the discriminator and generator, used upsample operations in place of transposed convolutions, played around with the buffer size, also attempted to keep the kernel size proportional to the 64x64 image ratio when using larger images). However, since there are so many things to change, my experiments were not exhaustive. In particular, I feel like I could try playing around the decay rates and composition of the loss function in more detail. Things that are on my list but that I haven\u2019t tried are mini-batch discrimination, packing (from PACGAN), different normalization (currently, I am using InstanceNorm), and logit loss (from LOGAN).\n\nAny advice would be greatly appreciated as I have been spent many hours on this, and, without positive feedback, my personal learning is not so high.\n\nI\u2019d be happy to give more info.", "link": "https://www.reddit.com/r/MachineLearning/comments/but75z/d_cyclegan_performance_peaks_before_first_epoch/"}, {"autor": "revant_t", "date": "2019-05-30 05:15:48", "content": "What is the current SOTA -----> image !!!  classification technique /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/bup9sn/what_is_the_current_sota_image_classification/"}, {"autor": "Chitoyo", "date": "2019-05-30 01:31:28", "content": "Style transfer-based -----> image !!!  synthesis as an efficient regularization technique in deep learning", "link": "https://www.reddit.com/r/MachineLearning/comments/bun7i4/style_transferbased_image_synthesis_as_an/"}, {"autor": "bolom_sounga", "date": "2019-05-29 15:37:55", "content": "[P] I am making a rock sorter. Should I train the network on what are big/small rocks or should I train the network by watching someone operate the sorting machine? /!/ I am doing a project on automating the task to sort rocks on a conveyor belt. My initial idea was to setup a -----> camera !!!  and train a model (using TensorFlow) on what is a small rock, big rock, quartz rock. And so far my tabletop demo is successful. My goal is to then send the coordinates of the rock to pick out to a robotic arm that will pick out the rock at a given coordinate. Lets assume that the rock picking process is trivial.\n\nMy problem is when vendors approached us, they convinced my boss that they know better (usual vendor logic) and they say that its better if we train the machine by observing an actual operator do the work. They didnt go into much detail but when i asked them how they would do that they had vague answers that made no sense, such as \"its an AI that learns which rocks to pick, by wtaching the operator it will learn\"\n\nThey showed us demos of games being played like racing cars around artificial tracks, super mario etc. But from my understanding, wouldnt it be better if we could actually train the algorithm to detect what is a quartz rock and how to detect it in a camera frame and then proceed to remove the rock? I think its much harder to train the algorithm with a weird camera angle watching someone do the job and then determining which rocks to pick.\n\nRecently i saw a user post a video on here showing a generation based car racing track. (https://www.youtube.com/watch?v=wL7tSgUpy8w) Wouldnt that be a generative model and is maybe what's causing confusion to the vendors?\n\nIm wondering what would be your thoughts", "link": "https://www.reddit.com/r/MachineLearning/comments/bug6dc/p_i_am_making_a_rock_sorter_should_i_train_the/"}, {"autor": "gan_man", "date": "2019-06-06 17:00:51", "content": "[D] Generative Neural Visual Artist (GeNeVA) /!/ **The Generative Neural Visual Artist (GeNeVA) task**\n\nLink:  [https://www.microsoft.com/en-us/research/project/generative-neural-visual-artist-geneva/](https://www.microsoft.com/en-us/research/project/generative-neural-visual-artist-geneva/)   \nPaper:  [https://arxiv.org/abs/1811.09845](https://arxiv.org/abs/1811.09845) \n\nThe GeNeVA task involves a *Teller* giving a sequence of linguistic instructions to a *Drawer* for the ultimate goal of -----> image !!!  generation.\n\nThe *Teller* is able to gauge progress through visual feedback of the generated image. This is a challenging task because the *Drawer* needs to learn how to map complex linguistic instructions to realistic objects on a canvas, maintaining not only object properties but relationships between objects (e.g., relative location). The *Drawer* also needs to modify the existing drawing in a manner consistent with previous images and instructions, so it needs to remember previous instructions. All of these involve understanding a complex relationship between objects in the scene and how those relationships are expressed in the image in a way that is consistent with all instructions given.\n\n\\------------\n\nWhat thoughts does the community have about generating images conditioned on captions iteratively instead of doing generation in one go? Most papers do not seem to be doing this iterartively but some recent papers have appeared which seems to be a good idea to me.", "link": "https://www.reddit.com/r/MachineLearning/comments/bxj43d/d_generative_neural_visual_artist_geneva/"}, {"autor": "gan_man", "date": "2019-06-06 15:01:44", "content": "[P] Generative Neural Visual Artist (GeNeVA) task: recurrent -----> image !!!  generation from text", "link": "https://www.reddit.com/r/MachineLearning/comments/bxhpyk/p_generative_neural_visual_artist_geneva_task/"}, {"autor": "probably_likely_mayb", "date": "2019-06-06 14:03:39", "content": "[D] What is the current state of the art approach for unsupervised/no ground truth semantic -----> image !!!  segmentation? (June 5th 2019)", "link": "https://www.reddit.com/r/MachineLearning/comments/bxh30x/d_what_is_the_current_state_of_the_art_approach/"}, {"autor": "Statistical_Incline", "date": "2019-06-06 14:01:38", "content": "What is the current state of the art approach for unsupervised semantic -----> image !!!  segmentation? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/bxh27t/what_is_the_current_state_of_the_art_approach_for/"}, {"autor": "jhjac", "date": "2019-06-06 13:54:44", "content": "[R] Invertible Residual Networks Paper + Code (ICML19 Long Oral) /!/ **Paper:** [http://proceedings.mlr.press/v97/behrmann19a.html](http://proceedings.mlr.press/v97/behrmann19a.html)  \n**Code:** [https://github.com/jhjacobsen/invertible-resnet](https://github.com/jhjacobsen/invertible-resnet)  \n\n\nAbstract:  \nWe show that standard ResNet architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Typically, enforcing invertibility requires partitioning dimensions or restricting network architectures. In contrast, our approach only requires adding a simple normalization step during training, already available in standard frameworks. Invertible ResNets define a generative model which can be trained by maximum likelihood on unlabeled data. To compute likelihoods, we introduce a tractable approximation to the Jacobian log-determinant of a residual block. Our empirical evaluation shows that invertible ResNets perform competitively with both state-of-the-art -----> image !!!  classifiers and flow-based generative models, something that has not been previously achieved with a single architecture.", "link": "https://www.reddit.com/r/MachineLearning/comments/bxgznn/r_invertible_residual_networks_paper_code_icml19/"}, {"autor": "EnoDevz", "date": "2019-05-02 00:54:26", "content": "Where is a good place to start for unsupervised -----> image !!!  creation? (ie. dogs, faces, flowers, etc.) /!/ I've been trying to figure out how to do this for a while now, however all the tools i find require either it to be made a video with some matrix of training data or I can't figure out how to add my own images to the data set. Is there a way I can create images and have the training set just be a folder where I put files into? \n\nBTW I'm trying to make the output look something like 3:50 of [this](https://www.youtube.com/watch?v=NTlXEJjfsQU) or 3:15 of [this](https://www.youtube.com/watch?v=4VAkrUNLKSo&amp;feature=youtu.be).\n\n&amp;#x200B;\n\nThank you in advance!!", "link": "https://www.reddit.com/r/MachineLearning/comments/bjow2z/where_is_a_good_place_to_start_for_unsupervised/"}, {"autor": "benfduffy", "date": "2019-05-01 15:38:42", "content": "[P] From Cups to Consciousness (Part 2): From simulation to the real world /!/ The saga continues (posted about part 1 [here](https://www.reddit.com/r/MachineLearning/comments/b6l314/p_from_cups_to_consciousness_part_1_how_are_cups/)):\n\n[From Cups to Consciousness (Part 2): From simulation to the real world](https://towardsdatascience.com/from-cups-to-consciousness-part-2-from-simulation-to-the-real-world-a9ea1249e233)\n\nWithin we discuss our gym wrapper and task interface for the amazing open-source 3D -----> photo !!! -realistic environment, [AI2Thor](http://ai2thor.allenai.org/), the steps taken to bring our agents to more physically realistic simulations ([PyBullet](https://pybullet.org/wordpress/) and [Gazebo](http://gazebosim.org/)), and we'll cover a simple and real robot called Vector, which can solve real world navigation problems.\n\nAlso, check out the intense clashes between armies of Vectors in a game of \"Cup Carnage\" and other GIFs of Vector exploring the moon.", "link": "https://www.reddit.com/r/MachineLearning/comments/bjijh9/p_from_cups_to_consciousness_part_2_from/"}, {"autor": "good_rice", "date": "2019-04-30 19:50:04", "content": "[P] Detection of Safe Landing Location from Terrain Images /!/ The purpose of this project is to devise a vision system that will return the safest area for an autonomous rocket powered lander to land given a terrain -----> image !!! .\n\nI started working on this project  a while ago, gave up, and am hoping to restart it soon. However, I think that the approach I've taken previously is suboptimal.\n\nCurrent Work: [https://github.com/GerardMaggiolino/SEDS-Aquarius-Vision](https://github.com/GerardMaggiolino/SEDS-Aquarius-Vision)\n\n&amp;#x200B;\n\n**Previous Approach:**\n\nI felt that semantic segmentation was overly expressive; pixel-wise labelling is not necessary, since I only need to find a large, contiguous region that is safe. Additionally, I am creating my own data set, and wanted to find a way to solve this problem without requiring an extensive amount of time labelling, or creating labels that would need to be very, very precise. I settled on something like sliding-window, with ordinal classification over each region. The paper used for ordinal classification reference is in the repo.\n\n&amp;#x200B;\n\nI'm classifying individual patches of an image as safe or not safe - this would ideally be a regression problem, but I don't have the ability to be that precise in my labelling. Binary classification would be poor, as there's many images right on the boundary. A comprise is ordinal classification, where penalization for a \"5\" of safeness when the true label is \"1\" is much greater than a penalization for a \"2\" on the same image. I'm manually labelling terrain images between 1 and 5. I wrote a short script that allows me to label about six images per minute.\n\n&amp;#x200B;\n\nWith a small network using strided depth-wise convolutions and only 411 greyscaled images without using augmentation, I achieved some promising results. Random classification would achieve a 52% accuracy of within at least 1 category correct (E.G. an output of 2 for a label of 1 is correct), and I achieved 96% with a few minutes of training. I'm hoping this would improve as I increase data set size, and I could perform augmentation to \\* 8 that with rotations and horizontal flips.\n\n&amp;#x200B;\n\nThe problem: Classifying over a 1500 x 1500 image (about 2 megapixels) takes around 0.2 seconds on my 6 core MacBook Pro. The lander will likely have less computational power, and it needs to be near real time. This is with NO sliding-window overlap, meaning potential good regions could be excluded if they're on boundaries.\n\n&amp;#x200B;\n\n**Solution:** \n\nI'm not sure. I'm unaware of what architectures are common for problems like this. At the end of the day, I want to return a single, precise location for where to land. \n\n&amp;#x200B;\n\nI'm thinking of two approaches: First, a YOLO-eqsue model, where safe regions are labelled in bounding boxes. Problem - safe regions are hardly ever in perfect rectangles. Second, a semantic segmentation model with limited / no upsampling, from which the center of the largest inscribed circle in the polygons corresponding to safe regions could be returned. Problem - speed might still be a concern, and labelling would be more challenging. \n\n&amp;#x200B;\n\nIn both of the above, labelling would be more time consuming and difficult, potentially resulting in lower quality models. Additionally, I'm under the impression that the amount of data necessary to train a good patch detector (1 - 5 ordinal regression) is far less than that to train SSDs or FCNs. \n\n&amp;#x200B;\n\nAny advice would be greatly appreciated! Sorry for the long post, and thank you so much if you've made it this far.", "link": "https://www.reddit.com/r/MachineLearning/comments/bj7pfk/p_detection_of_safe_landing_location_from_terrain/"}, {"autor": "deluded_soul", "date": "2019-07-22 13:36:48", "content": "training YOLOv3 from scratch using only a single -----> image !!!  /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/cgdamc/training_yolov3_from_scratch_using_only_a_single/"}, {"autor": "Annomator", "date": "2019-07-22 12:56:58", "content": "[P]Seeking feedback and collaborators for an open source, automatic -----> image !!!  annotation tool - Annomator - https://github.com/annomator/annomator_1.0", "link": "https://www.reddit.com/r/MachineLearning/comments/cgcv79/pseeking_feedback_and_collaborators_for_an_open/"}, {"autor": "mekass", "date": "2019-07-22 09:49:27", "content": "[Project] Quotes generating bot \"QuoteMeAI\" /!/ Hey folks!  \n\n\nJust finished my project: \"Quote Me AI\" [https://github.com/tomasrasymas/quote-me-ai](https://github.com/tomasrasymas/quote-me-ai) . It's bot that automatically generates quotes and puts them on -----> image !!!  and posts that -----> image !!!  to r/QuoteMeAI subreddit.   \n\n\nTake a look!  \n\n\nAny Feedback is welcome!!!", "link": "https://www.reddit.com/r/MachineLearning/comments/cgb648/project_quotes_generating_bot_quotemeai/"}, {"autor": "rickkava", "date": "2019-07-21 19:24:02", "content": "Beginner question: What makes good training data? /!/ Hello reddit,\nfirst of all apologies if this is a trivial / noobish question, but as a non-ML person I sometimes struggle with what might be basic stuff.\nHere is my question:\nIn the context of supervised learning, what makes \u201egood\u201c training data? Suppose I want to generate new training data to enhance my model, how similar or different should the new data be compared to the old one? For example, to train a cat classifier, it would make no sense to add new images of snakes to the set, as the correlation to the original data is too low. Adding more cat -----> picture !!! s where each -----> picture !!!  is just one from the original batch with some pixels blurred would also make no sense, as the correlation is too high - there is nothing new in the data set. So how do I find a set that is right in the middle? \nHow can I quantify this? What are some of the concepts that should be applied? In what sense does the new data need to be \u201ethe same, but different\u201c?\nI am sure there is a correct terminology for what I am asking, I feel like a two-year old who just hasnt learned the proper vocabulary yet.\nPlease point me in the right direction.\nthanks guys!", "link": "https://www.reddit.com/r/MachineLearning/comments/cg2wtt/beginner_question_what_makes_good_training_data/"}, {"autor": "ian_williamson", "date": "2019-07-29 18:08:42", "content": "[R] Reprogrammable Electro-Optic Nonlinear Activation Functions for Optical Neural Networks /!/ I am very excited to share our recently published work towards developing nonlinear activation functions for optical neural networks (ONNs).\n\nThere has been a lot of interest in specialized hardware for achieving high efficiency and performance on machine learning tasks. Matrix-vector multiplications are one of the most important (and computationally expensive) operations in neural networks. It turns out that analog *optical* processors can perform these operations in **O(1) time** (rather than the  **O(n\\^2) time** on GPUs and CPUs). These specialized ONN processors, which are driven by modulated lasers, could potentially be scaled to use far less energy *per operation* than conventional digital processors.\n\nOf course, the other piece of the puzzle for neural networks is the **nonlinear activation function**. Optics is excellent for performing linear operations, but nonlinearities are far more difficult, especially in on-chip circuits. Basically, in nature, if you want to see something or to send information, you use light. But, if you want to make a decision on that information you use electrical charge. \n\nOur paper (linked below) proposes a scheme for building a full ONN with an activation function by coupling a small electrical circuit to the output of each ONN layer. This electrical circuit converts a small amount of the optical signal into and electrical voltage, which then **nonlinearly modulates the optical signal**. We performed a benchmark of this ONN on the MNIST -----> image !!!  recognition task and found that our activation function significantly boosted the classification accuracy of the ONN, from \\~85% without the activation to \\~94% with the activation. This is still a bit below the performance achieved in state-of-the-art models, but our setup used only 16 complex Fourier coefficients of the images as inputs (rather than all 784 pixels).\n\nCheckout the paper below and feel free to ask questions. Our two Python ONN simulator packages (developed by two of my co-authors) are available on GitHub: [https://github.com/fancompute/neuroptica](https://github.com/fancompute/neuroptica) and [https://github.com/solgaardlab/neurophox](https://github.com/solgaardlab/neurophox/). These repos include several examples if you're interested in playing around with training ONNs on a computer.\n\n**Journal Paper:** [https://doi.org/10.1109/JSTQE.2019.2930455](https://doi.org/10.1109/JSTQE.2019.2930455)\n\n**arXiv preprint:** [https://arxiv.org/abs/1903.04579](https://arxiv.org/abs/1903.04579) (same content as the Journal version)", "link": "https://www.reddit.com/r/MachineLearning/comments/cjez4z/r_reprogrammable_electrooptic_nonlinear/"}, {"autor": "nokpil", "date": "2019-07-29 16:41:06", "content": "[D] Is it common to discard all 'non-full-time-trajectory' pedestrian from data in pedestrian prediction domain? /!/ Hello, I'm relatively new to pedestrian trajectory prediction, and I have a question about preprocessing of pedestrian data. Actually, I sent the question e-mail for the author of SocialGAN paper and since no reply was received. If anyone knows anything about this topic, your answer would help me a lot.\n\nIt is quite common that may ML pedestrian trajectory prediction employs LSTM in order to capture the temporal correlation. For example, look at the diagram below from the paper ''Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks' ([https://arxiv.org/pdf/1803.10892.pdf](https://arxiv.org/pdf/1803.10892.pdf) ). In the paper, author said that the model was trained to observe 3.2 seconds (8f) then predict 4.8 seconds (12f).\n\nHowever, since the common pedestrian dataset (like ETH and UCY) collected from single still -----> camera !!! , trajectory terminates when the agent moves out of sight. Thus, unlike conceptual diagram in many papers, we cannot simply gather whole data for 3.2sec (8f) and let them consist LSTM hidden state. \n\n[Adopted from SocialGAN paper. Since some of the trajectories terminates earlier than others, LSTM stepts should handle this uneven data length.](https://i.redd.it/g9h7arnbs9d31.png)\n\nTherefore, I've curious about how other researchers were handle this missing value problem.\n\nWhile following the author's code on GitHub, I've found that the code discarded every individual when their trajectory throughout the given interval is shorter than its whole interval length. ([https://github.com/agrimgupta92/sgan/blob/master/sgan/data/trajectories.py](https://github.com/agrimgupta92/sgan/blob/master/sgan/data/trajectories.py), an excerpt from class 'TrajectoryDataset')  \n\n\n[When the SocialGAN paper preprocess the raw pedestrian data into pytorch dataset, agents with short trajectory length discarded from dataset and not used for inference.](https://i.redd.it/6zviajhhm9d31.png)\n\nIn result, the final dataset for training does not contain a nonnegligible portion of trajectories despite that some of them could exist near to 'considered' individuals and may affect their trajectories. \n\nI want to raise two questions on this point.\n\n1. Am I correctly understanding the code and its consequences? I would like to show a small toy example in order to visualize and verify my understandings of SocialGAN code.\n\nLet us assume that seq\\_len = 4 (obs\\_len = 2, pred\\_len = 2) and consider the frame 1\\~4. Then, according to my understandings, SocialGAN code discards green agent(below) because it does not have a full 4-frame trajectory.\u00a0\n\n[Crude visualization of pedestrian dataset.](https://i.redd.it/rno1xk8gm9d31.png)\n\nBut, in my opinion, I think the green agent would affect red agent's trajectory a lot compared to blue one does. Although the missing value of the trajectory is an inevitable problem of the dataset itself (since no one can collect indefinite amount of tracked position), ignoring the whole trajectory because of those value might decrease the overall accuracy.\u00a0\n\nFurthremore, since this preprocessing handles frame-based trajectory data with a moving window (i.e. it makes frame 1\\~4 as a first data and 2\\~5 as second, 3\\~6 as third, and so on.), agent like green might affect red agent in some data, while discarded in other data.\n\n2. Secondly, (under the assumption that my understanding of the situation is correct), Is this kind of preprocessing (discarding non-full-trajectory individuals) common for preprocessing of the public pedestrian datasets such as ETH and UCY, or is this preprocessing was particular for SocialGAN and there are other ways to preprocess pedestrian data? I'd like to know that is there another way to deal with this missing value problem.\n\nI'm currently trying to implement a model and preprocessing which can alleviate this problem, but I'm not certain whether this problem had been already addressed and well-known in CV/ML society, or it hadn't.\n\nAgain, any response or shared opinion will be grateful :)", "link": "https://www.reddit.com/r/MachineLearning/comments/cjdr36/d_is_it_common_to_discard_all/"}, {"autor": "Emcf", "date": "2019-07-29 15:28:44", "content": "How can I build a model with translational variance? /!/ I'm currently facing a computer vision problem. The position of the features in the -----> image !!!  plays a big role in extracting data from the -----> image !!! . As far as I know, CNNs may have trouble with such tasks due to the translational invariance of convolution. Does anyone with more experience than myself have any suggestions or recommendations for building a model for this type of problem?", "link": "https://www.reddit.com/r/MachineLearning/comments/cjcrea/how_can_i_build_a_model_with_translational/"}, {"autor": "HalpMehProgram", "date": "2019-07-29 14:19:51", "content": "Are there any online ML classes specific for biomedical applications? /!/ Title - looking for any available classes that apply ML to signal (ECG, EMG, EEG) or -----> image !!!  processing.", "link": "https://www.reddit.com/r/MachineLearning/comments/cjbv77/are_there_any_online_ml_classes_specific_for/"}, {"autor": "AIObserver", "date": "2019-07-29 13:46:54", "content": "\u041ebject detection with poor lighting /!/ We tried to train CNN (YOLO, Faster R-CCN) to detect objects and found that it is impossible to do with poor lighting on images. We saw some regularity in recognizing objects that are distinguished by the OpenCV function findContours(). If an object has the contour then CNN may be trained to detect this kind of objects. But if an object can't be contoured with the standard -----> image !!!  processing procedures then neural networks don't know which objects to be trained on.\n\nFor example, this is the cardboard box image:\n\n[Real image of the cardboard box](https://i.redd.it/cp9so0q8v8d31.jpg)\n\nThe result of findCountours():\n\n[Cardboard box contours](https://i.redd.it/ax0b4hfjv8d31.jpg)\n\nAnd as seen in the image, the bottom edge of the cardboard box is indistinguishable because of poor lighting. We managed to train the network to detect most of objects but not this one. And there is a certain regularity in the possibility of network training and the possibility of finding the contour of objects.\n\nWhat can be done to solve the problem? \n\nThank you.", "link": "https://www.reddit.com/r/MachineLearning/comments/cjbghe/\u043ebject_detection_with_poor_lighting/"}, {"autor": "paul_read_it", "date": "2019-07-29 12:00:02", "content": "[P] AI ping pong game w/ object detection on Raspberry Pi 4 &amp; Google Coral USB Accelerator /!/ I recently got my hands on the Raspberry Pi 4 and the Google Coral USB Accelerator and i decided to program a little fun game in order to get an idea of the performance of these devices. This is a video summary about it:\n\n[https://youtu.be/ruGk99s9Yhk](https://youtu.be/ruGk99s9Yhk)\n\n**Technical details:**\n\n* The machine learning model used is a [MobileNet SSD v2 trained on faces](https://dl.google.com/coral/canned_models/mobilenet_ssd_v2_face_quant_postprocess_edgetpu.tflite), which is publicly available. \n* The video frames processed by the machine learning model have the dimension 720 x 480 pixels\n* The frames per second during this game fluctuate between 20 and 30 fps\n* The program is written in Python and besides the TFLite model, most of the program is achieved with OpenCV\n\n**What i learned:**\n\n* The combo of a Raspberry Pi 4 which now has USB 3 and the Google Coral USB Accelerator are a powerful and cheap setup and should definitely evaluated for projects that deal with machine learning inference on the edge.\n* The Raspberry Pi 4 CPU gets pretty hot, it sometimes went up to 80 degrees celsius! Although the heat issue might be partly fixed with a Raspbian update, i highly recommend to get an active cooling solution like a fan, a heat sink only lowered temperatures by 3-5 degrees in my case.\n* I prefer working with these two devices over the Google Coral Dev Board because the latter one runs Mendel OS which i find hard to work with, there is a lot of restrictions when you get started and i had a hard time to find solutions, even to achieve something as simple as a right click or open a visual explorer. Raspbian OS on the other hand has just so much more support because of its great community. I haven't had my hands on the Jetson Nano from NVIDIA so i cannot make a comparison here.\n* I tried to convert other objects into ping pong bats which can be recognized by the [MobileNet SSD v2 (COCO)](https://dl.google.com/coral/canned_models/mobilenet_ssd_v2_coco_quant_postprocess_edgetpu.tflite), such as bananas but realised that objects need to be quite big or close to the -----> camera !!!  in order to be recognized reliably by the relatively small image resolution. Smartphones and books worked quite well with that model but not as well as faces.\n\nWhat do you think about these devices combined? Do you know of a better device or solution for machine learning inference on the edge?\n\nIn any case, I hope this helps you make decisions for your future projects :-) !\n\nPaul", "link": "https://www.reddit.com/r/MachineLearning/comments/cjaagp/p_ai_ping_pong_game_w_object_detection_on/"}, {"autor": "ychamel", "date": "2019-07-29 11:12:44", "content": "[D] Dealing with open set reidentification /!/ What's the best approach in dealing with open set identifications such as face id and re-id.\nI know these problems are still unsolved and give low results without human aid.\nSo mainly Im asking for ideas on how to handle this topic.\nFor the sake of the argument lets focus on face recognition and lets say all images are of high quality.\nSo storing or comparing the first few hundred -----> picture !!!  we might have somewhat okay results if we use clustering or threshholding but when the numbers get bigger the false positive rates sky rockets.\nSo is there a way to rerank or retrain everyonce in a while. Or at least differentiate from known and unknowns. Im lost at this step.", "link": "https://www.reddit.com/r/MachineLearning/comments/cj9u14/d_dealing_with_open_set_reidentification/"}, {"autor": "omarsar", "date": "2019-01-02 16:53:43", "content": "[R] Large-scale -----> Image !!!  Captioning", "link": "https://www.reddit.com/r/MachineLearning/comments/abv4ss/r_largescale_image_captioning/"}, {"autor": "midasp", "date": "2019-01-02 06:22:54", "content": "[D] Is there any work on classifying non-photographic images? /!/ While there have been a lot of work in the area of categorizing photographs, I was wondering if there has been any progress in the categorization of non-photographic images?\n\nBy non-photographic, I mean images such as these (if cats vs not-cats were the classification category).\n\n * [Stick figure cat](https://www.google.com/search?q=stick+figure+cat&amp;tbm=isch)\n * [Cat icon](https://www.google.com/search?q=icon+cat&amp;tbm=isch)\n * [Abstract cat images](https://www.google.com/search?q=abstract+cat&amp;tbm=isch)\n * [Cartoon cat](https://www.google.com/search?q=cartoon+cat&amp;tbm=isch)\n * [3D rendered cat](https://www.google.com/search?q=3d+cat&amp;tbm=isch)\n\nI am not even certain if \"non-photographic\" is the proper terminology to use, which perhaps explain why I am only finding some what older works such as [this 2014 paper using SVM with -----> image !!!  features like SIFT/SURF rather than say ResNet or more modern methods](http://www.mva-org.jp/Proceedings/2015USB/papers/10-19.pdf). I would appreciate it if anyone can point me to more recent works.", "link": "https://www.reddit.com/r/MachineLearning/comments/abqpsf/d_is_there_any_work_on_classifying/"}, {"autor": "invertedpassion", "date": "2019-01-01 15:40:37", "content": "[D] Notes on why deep neural networks are able to generalize well /!/ Hello,\n\nI spent a good part of today reading on why deep neural networks are able to generalize well.  Based on my reading, I have made some notes. I'm new to this, so I'd appreciate if I can have community members' comments / discussion on the same. In particular, I'd love to know if I got something wrong or if someone is aware of a significant result that I missed.\n\nHere are my notes:\n\n1/ First major insight was that the minibatch of data for gradient descent actually helps in generalization on unseen data.   **Gradients of minibatch of data that are specific about that batch cancel over multiple runs and what remains is gradients that are generally applicable**.\n\n2/ It is known that [neural networks are universal function approximators](https://en.wikipedia.org/wiki/Universal_approximation_theorem). That is, given a function they can approximate that function with arbitrary accuracy.  But now I think that's not an interesting result (of approximating a function). Even a database can do that. What's interesting is that they give good answers on *unseen* data.\n\n3/ It is a mystery how that happens but probably the answer lies in not as much about neural networks but the types of datasets we have in the natural world and what problems we use neural networks for.\n\n4/ Natural world is full of information, one 1000x1000 px -----> photo !!!  has 1 million bits but when we see it, we either see it as a cat or a dog.  Effectively, we \"throw out\" a lot of information to do whatever we want to do. To classify a photo, our brain convert a log(1 million) bits into log(1) bit and the task of a neural network is to find the mapping that \"forgets\" or \"throws\" all the information irrelevant to the task while only retaining info that's useful to us.\n\n5/ Since this log(1 million) to log(1) is a many-to-one function, neural networks might be a really good model for approximating these functions.  **Different layers might be throwing away irrelevant information while keeping only the relevant info**.\n\n6/ This is suggested by two papers/videos I saw today.  One was on information bottleneck: [https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/](https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/)\n\n7/ The other one is how **errors introduced in early layers tend to vanish in higher layers**: [http://www.offconvex.org/2018/02/17/generalization2/](http://www.offconvex.org/2018/02/17/generalization2/)\n\n8/ In effect, **neural networks are lossy compression algorithms** that compress inputs as much as they can while retaining as much info as possible about the task at hand (classification, prediction)  This helps networks generalize as data-specific noise gets ignored in deep networks.\n\n9/ Okay, so we know what deep networks \\*might\\* be doing but the question is how training via gradient descent is able to find the right set of parameters that do this compression.  Given the millions of weights and biases, it seems the problem is of finding the needle in the haystack.\n\n10/ I honestly don't know and research community also (probably) doesn't know. But there are hints.  One is related to the earlier suggestion of many-to-one mapping of input to output in real-world tasks. This means that t**here may be more than 1 set of parameters that do the job equally well**\n\n11/ So stochastic gradient descent might not be finding the \"perfect\" set of parameters but it may not matter. **The problem we want to solve through neural networks may get solved by many sets of params** and SGD may find one of them.\n\n12/ In fact, empirically the landscape of **loss function for neural networks on \"natural\" problems (of image classification, etc.) seems to have a \"flat\" minima.**  \n\n&amp;#x200B;\n\n![img](91ysxtolzt721 \"Image via: https://www.offconvex.org/2018/02/17/generalization2/\")\n\n13/ So the *same* function we're seeking might be parameterized by many parameters.   On top of this, what helps is that **in a big deep network there exist many, many subnetworks. And, just by pure luck, one or more of them might be better positioned to seek that landscape via SGD.** This is explored in the lottery hypothesis: [https://arxiv.org/abs/1803.03635](https://arxiv.org/abs/1803.03635)\n\n14/ I understand how the width of the network may help in exploring what information to throw (by setting weights to zero) and what information to use, but I'm not sure the role of depth.  **My hunch says the utility of depth is related to how stochastic gradient descent works. Do you agree?**\n\n15/ Perhaps, just perhaps, different layers (depth) helps SGD reduce loss in steps by focusing on few dimensions at once v/s if it is just one very wide layer, SGD has too many dimensions to seek at once.  But I don't really know.\n\n16/ What's fascinating to me is the how easily researchers drop neural networks as function approximators anywhere and everywhere. This just makes it more worthwhile to study the dynamics of deep networks.  If you want to dive in, here's a great tutorial: [https://www.youtube.com/watch?v=r07Sofj\\_puQ](https://www.youtube.com/watch?v=r07Sofj_puQ)\n\n&amp;#x200B;\n\nThat's all! Did I miss anything? Did I go wrong somewhere? I'd appreciate any inputs that can help build us a better intuition of what might be happening under the hood.\n\n&amp;#x200B;\n\nPS: I tweeted about this as well, but I don't have many friends on Twitter who may provide a perspective on my notes or catch my errors.  That's why I started a discussion on this subreddit. ", "link": "https://www.reddit.com/r/MachineLearning/comments/abj1mc/d_notes_on_why_deep_neural_networks_are_able_to/"}, {"autor": "antonsteenvoorden", "date": "2019-07-10 14:51:19", "content": "Transfer learning VGGish (AudioSet). Impact of missing frequencies /!/ Hi there, I am trying to train a network on top of the VGGish architecture ([https://github.com/tensorflow/models/tree/master/research/audioset/vggish](https://github.com/tensorflow/models/tree/master/research/audioset/vggish)), using (transfer learning) finetuning.\n\nI initially started out with just finetuning the embedding layers, and training from scratch a simple MLP with some regularization techniques. (2 hidden layers FC with ReLU, BatchNorm and dropout, final output currently is FC to 4 classes). \n\n&amp;#x200B;\n\nThe input of the VGGish network are spectograms. My collected audiofiles are pretty low quality OGG/vorbis with a sample rate of 8000Hz. This is lower than the audio used in VGGish, therefore, if I create the log-mel spectogram using their code, every spectogram is missing some bins. (see top part of the -----> image !!! ) note that this happens in every -----> image !!! , so my hypothesis is that this shouldn't really matter.\n\n&amp;#x200B;\n\nWill this have any negative consequences on the ability of the network to learn using my dataset? Do I also need to finetune the ConvLayers earlier on in the network?\n\n&amp;#x200B;\n\n![img](3m29lfkaoh931 \"Bins 48 to 60 are not filled as my audio only has up to 4000Hz\")\n\n&amp;#x200B;\n\nAny other thoughts on how I can work around this (potential) issue?   \nIt is not feasible to increase the frequency of the recordings, as they are being streamed from low power devices. \n\nI do not have enough (annotated) data to fully retrain, so training a model from scratch is not feasible. \n\nSuggestions on other pretrained networks are also welcome! \n\nThanks :)", "link": "https://www.reddit.com/r/MachineLearning/comments/cbhnra/transfer_learning_vggish_audioset_impact_of/"}, {"autor": "cdossman", "date": "2019-07-10 14:37:44", "content": "[Research] Brno Mobile OCR Dataset /!/ [https://medium.com/ai%C2%B3-theory-practice-business/new-ocr-dataset-for-mobile-devices-716e63967b4](https://medium.com/ai%C2%B3-theory-practice-business/new-ocr-dataset-for-mobile-devices-716e63967b4) \n\n We introduce the Brno Mobile OCR Dataset (BMOD) for document Optical Character Recognition from low-quality images captured by handheld devices. While OCR of high-quality scanned documents is a mature field where many commercial tools are available, and large datasets of text in the wild exist, no existing datasets can be used to develop and test document OCR methods robust to non-uniform lighting, -----> image !!!  blur, strong noise, built-in denoising, sharpening, compression and other artifacts present in many photographs from mobile devices. \n\nThis dataset contains 2 113 unique pages from random scientific papers, which were photographed by multiple people using 23 different mobile devices. The resulting 19 725 photographs of various visual quality are accompanied by precise positions and text annotations of 500k text lines. We further provide an evaluation methodology, including an evaluation server and a test set with non-public annotations. We provide a state-of-the-art text recognition baseline build on convolutional and recurrent neural networks trained with Connectionist Temporal Classification loss. This baseline achieves 2 %, 22 % and 73 % word error rates on easy, medium and hard parts of the dataset, respectively, confirming that the dataset is challenging.", "link": "https://www.reddit.com/r/MachineLearning/comments/cbhhy5/research_brno_mobile_ocr_dataset/"}, {"autor": "karenactionsitafaal", "date": "2019-07-10 08:32:10", "content": "-----> Photo !!! ", "link": "https://www.reddit.com/r/MachineLearning/comments/cbe4cz/photo/"}, {"autor": "lexerq", "date": "2019-07-10 00:38:03", "content": "I need advertising dataset: text + -----> image !!!  /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/cb9h5s/i_need_advertising_dataset_text_image/"}, {"autor": "prLone", "date": "2019-07-09 23:47:25", "content": "How to train a model for 2D geometric data. /!/ I have 2D geometric data which when rendered results into floor plans like the one shown in this -----> image !!! \n\n![img](cdv5cujw6d931)\n\nNow for each segment in the figure above I have the end points data ie the 2 points that when connected makes up the line. Now, I want to train a classifier to classify different lines into their respective classes. I have labeled the data already.\n\nMy confusion is with regards to how should I go about making the model to also take into consideration the relationships between the lines as multiple lines with same dimension could belong to different classes, or should I not worry about this as a model would pickup this relationship automatically.\n\nI have multiple files which each contain a set of lines. Also each file has different number of lines.", "link": "https://www.reddit.com/r/MachineLearning/comments/cb8w3p/how_to_train_a_model_for_2d_geometric_data/"}, {"autor": "karenactionsitafaal", "date": "2019-07-09 20:33:33", "content": "-----> Photo !!! ", "link": "https://www.reddit.com/r/MachineLearning/comments/cb6fuo/photo/"}, {"autor": "Nick-Conner", "date": "2019-07-09 20:13:23", "content": "Do machine learning algorithms read my -----> camera !!!  and see that I have tattoos, then targets me with tattoo related ads?", "link": "https://www.reddit.com/r/MachineLearning/comments/cb668a/do_machine_learning_algorithms_read_my_camera_and/"}, {"autor": "minimaxir", "date": "2019-07-09 15:32:34", "content": "[P] Make your own AI-generated Magic: The Gathering cards with GPT-2 /!/ [https://minimaxir.com/apps/gpt2-mtg/](https://minimaxir.com/apps/gpt2-mtg/)\n\nGive a card name, card type, and/or card mana cost to a finetuned GPT-2, and get a custom card -----> image !!!  + card text!\n\nI've also created a text dump of \\*thousands\\* of said cards: [https://github.com/minimaxir/mtg-gpt-2-cloud-run/tree/master/generated\\_card\\_dumps](https://github.com/minimaxir/mtg-gpt-2-cloud-run/tree/master/generated_card_dumps)\n\nEven with only a little training and data augmentation, GPT-2 117M overfits on Magic cards, which leads to some interesting remixes of existing cards, and some cards with *interesting* value. Some examples:\n\n[Krovikan Vampire](https://github.com/minimaxir/mtg-card-creator-api/blob/master/card_image_examples/vampire.jpg?raw=true)\n\n[Krovikan Scuttle](https://github.com/minimaxir/mtg-card-creator-api/blob/master/card_image_examples/krovikan_scuttle.jpg?raw=true)\n\n[Vona](https://github.com/minimaxir/mtg-card-creator-api/blob/master/card_image_examples/vona.jpg?raw=true)\n\n[Zephyr Return](https://github.com/minimaxir/mtg-card-creator-api/blob/master/card_image_examples/zephyr_return.jpg?raw=true)\n\nAnd a few cards where the card name is chosen:\n\n[United States of America](https://github.com/minimaxir/mtg-card-creator-api/blob/master/card_image_examples/united_states_of_america.jpg?raw=true)\n\n[Facebook](https://github.com/minimaxir/mtg-card-creator-api/blob/master/card_image_examples/facebook.jpg?raw=true)\n\n[Twitter](https://github.com/minimaxir/mtg-card-creator-api/blob/master/card_image_examples/twitter_red.jpg?raw=true)\n\n[Mark Rosewater](https://github.com/minimaxir/mtg-card-creator-api/blob/master/card_image_examples/mark_rosewater.jpg?raw=true)\n\nThe code for the text generation API is open-sourced [here](https://github.com/minimaxir/mtg-gpt-2-cloud-run), and the code for the image generation API (which uses an ugly wine hack) is open sourced [here](https://github.com/minimaxir/mtg-card-creator-api).\n\nOther helpful notes:\n\n* To share the generated card image, you can Save As the generated card locally, and to use it elsewhere, rename it and add a .jpg file extension.\n* The network can recite existing card names and rules text of existing cards, but rarely to the same card. The network often makes interesting color shift decisions with changes to CMC/Rarity.\n* In terms of color accuracy, color pie is mostly correct. Creature types and mechanics often follow the appropriate color identity. P/T, mana cost, and rarity are balanced.\n* The card formatting issues are due to the underlying mtgencode/MSE implementations and are not easy to fix (most notable with Planeswalker cards).\n\nLet me know what you think! :)", "link": "https://www.reddit.com/r/MachineLearning/comments/cb2eac/p_make_your_own_aigenerated_magic_the_gathering/"}, {"autor": "iyaja", "date": "2019-05-15 18:15:37", "content": "[P] Generating Game of Thrones Characters Using StyleGAN /!/ Hi everyone. I've been working on a project where I use StyleGAN to generate fake images of characters from Game of Thrones. I wrote an [article](https://blog.nanonets.com/stylegan-got/) that describes that algorithms and methods used, and you can try it out yourself via a [Colab notebook](https://colab.research.google.com/github/iyaja/stylegan-encoder/blob/master/generate_GoT_characters_with_StyleGAN.ipynb#scrollTo=LKzLQp1QIkHd). \n\n![video](ndz6aoayzey21 \"Game of Thrones character animations from StyleGAN\")\n\nHere are the resources to follow along:\n\n* Article: [https://blog.nanonets.com/stylegan-got/](https://blog.nanonets.com/stylegan-got/)\n* Colab notebook: [https://colab.research.google.com/github/iyaja/stylegan-encoder/blob/master/generate\\_GoT\\_characters\\_with\\_StyleGAN.ipynb](https://colab.research.google.com/github/iyaja/stylegan-encoder/blob/master/generate_GoT_characters_with_StyleGAN.ipynb#scrollTo=LKzLQp1QIkHd)\n* GitHub repo: [https://github.com/iyaja/stylegan-encoder](https://github.com/iyaja/stylegan-encoder)\n\n&amp;#x200B;\n\nSomething interesting to note: All the results (images and animations) were generated from Nvidia's StyleGAN that was pretrained on the FFHQ dataset, with absolutely no fine-tuning.\n\n&amp;#x200B;\n\nInstead, to make StyleGAN work for Game of Thrones characters, I used another model (credit to [this GitHub repo](https://github.com/iyaja/stylegan-encoder)) that maps images onto StyleGAN's latent space. I gave it -----> image !!! s of Jon, Daenerys, Jaime, etc. and got latent vectors that when fed through StyleGAN, recreate the original -----> image !!! .\n\n&amp;#x200B;\n\nWith the latent vectors for the -----> image !!! s in hand, it's really to modify them in all the ways described in the StyleGAN paper (style mixing, interpolations, etc.) as well as through simple arithmetic in the latent space (such as shifting the latent vector in the \"smiling direction\"). As a bonus, since there's no StyleGAN training involved, all the steps that I just mentioned can be executed extremely fast.\n\n&amp;#x200B;\n\nAlso, stick around to the end of [the article](https://blog.nanonets.com/stylegan-got/) to see a result of character style mixing that might be very interesting to Game of Thrones fans.", "link": "https://www.reddit.com/r/MachineLearning/comments/bp1el3/p_generating_game_of_thrones_characters_using/"}, {"autor": "Magre94", "date": "2019-05-15 18:05:29", "content": "[D] VAE: why we do not sample again after decoding and before reconstruction loss? /!/ In many of the VAE schematics and in the original paper, a sampling step is present after decoding and before the reconstruction loss as shown in the -----> image !!!  below. The image comes from Stanford CS321n.\n\n[![VAE scheme][1]][1]\n\nIn many of the code implementations though, this step is not present. For example in the Keras implementation available here: https://keras.io/examples/variational_autoencoder/\n\nIn the latent space `z` they sample with the Lambda layer, but at the end of the decoder there is just a Dense layer with a sigmoid activation.\n\nIs the sigmoid doing something I don't understand mathematically? Is the VAE math still valid without this sampling step?\n\n\nIt is not only in code implementations, in some other schematics and textual material it seems to be ignored (see next image).\n\n[![Second VAE scheme with no sampling][2]][2]\n \n\nAnyway I also created a question on Cross-Validated: [Link to the question](https://stats.stackexchange.com/questions/408484/vae-why-we-do-not-sample-again-after-decoding-and-before-reconstruction-loss). If you also want to answer there and earn some points go and do it!\n\n  [1]: https://i.stack.imgur.com/GgS2y.png\n  [2]: https://i.stack.imgur.com/yWCtP.jpg", "link": "https://www.reddit.com/r/MachineLearning/comments/bp1a1c/d_vae_why_we_do_not_sample_again_after_decoding/"}, {"autor": "mellow54", "date": "2019-05-15 12:28:44", "content": "[D] InfoGAN loss derivation confusion /!/ I'm a bit confused by the derivation of a lemma in the appendix of the InfoGAN paper (Appendix A lemma). I have attached the -----> image !!!  here (https://m.imgur.com/gallery/xzo4gZ2).\n\nIn particular, between third and fourth line, f(x,y) turns into f(x',y). How does that work?", "link": "https://www.reddit.com/r/MachineLearning/comments/boxbpv/d_infogan_loss_derivation_confusion/"}, {"autor": "zaher88abd", "date": "2019-05-15 01:30:52", "content": "Building deep learning model can count the white pixel in -----> image !!!  /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/borj04/building_deep_learning_model_can_count_the_white/"}, {"autor": "faburu", "date": "2019-05-14 20:14:59", "content": "Can I perform -----> image !!!  classification on my desktop? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/bonxsi/can_i_perform_image_classification_on_my_desktop/"}, {"autor": "saumya_3025", "date": "2019-05-14 17:42:34", "content": "FACIAL EXPRESSION RECOGNITION WITH AUTO ILLUMINATION CORRECTION /!/   \n\nFACIAL EXPRESSION RECOGNITION WITH AUTO ILLUMINATION CORRECTION\n\n\ud83d\udcf7\n\nSubmitted by:\n\nSAUMYA SHARMA \n\n9915103025\n\nF1\n\nSubmitted as a Deep Learning course assignment for the fourth year\n\nDEPARTMENT OF COMPUTER SCIENCE ENGINEERING AND INFORMATION TECHNOLOGY\n\nJAYPEE INSTITUTE OF INFORMATION TECHNOLOGY, NOIDA\n\n**INTRODUCTION**\n\nA machine analysis of facial expressions first surveys the problem domain, describes the problem space, and examines the state of the art. Four areas will receive particular attention: face detection, facial feature extraction, facial muscle action detection, and emotion recognition. \n\n**IMPLEMENTATION**\n\nFirst step is pre-processing of the -----> image !!!  to bring in the proper luminance value throughout\n\nthe -----> image !!! . Then by using the Haar-Cascades method the skin and the non-skin pixels are separated to separate the face from the background. A simple rectangular Haar-like feature can\n\nbe defined as the difference of the sum of pixels of areas inside the rectangle, which can be at any position and scale within the original image. This modified feature set is called 2-rectangle feature. Viola and Jones also defined 3-rectangle features and 4-rectangle features. The values indicate certain characteristics of a particular area of the image. Each feature type can indicate the existence (or absence) of certain characteristics in the image, such as edges or changes in texture. For example, a 2-rectangle feature can indicate where the border lies between a dark\n\nregion and a light region.\n\n\ud83d\udcf7\n\nSliding-Window Approach and then Gabor filter responses calculation is done. Once the skin or the face is detected, then we go in for the mouth and eye mapping. Before going in for the mouth extraction, we apply the colour constancy algorithm, to correct colour deviations caused by a difference in illumination as done by the human. We use the max rgb algorithm for the correction.\n\nThis algorithm uses the special Minkowski norm, given by,\n\n\ud83d\udcf7where p=6\n\nThe next step is extraction of the mouth and the eye map, which is done based on the chrominance and luminance values, within the classified skin regions. Those regions which have a high red component are said to form the mouth map, and those regions with high black and white components are said to be the eye region.\n\nOnce the maps are formed they are converted to binary images to extract the mouth and eye regions from the original image. Based on the Euclidian distance the eye and mouth region are identified Lips and Eyes are extracted based on the Mouth\\_Cascade and Frontal\\_Eyes\\_Cascade. Extracted eye and Lips is processed for expression analysis.\n\nOnce the mouth and the eyes are extracted, the expression analysis is done with the extracted\n\n/segmented image. For the eye, the intraocular distance is measured and for the mouth the expanse of the lips and mid-point of the mouth are calculated. Depending upon the two values the expressions are calculated and they are listed in a table depending upon the numbering value the frame is classified whether it is a sad or normal or happy expression.\n\n**RESULT**\n\nThe lip ratio is taken into account for finding the expression given by the user. Depending upon the value of the lip ratio the expressions are displayed.\n\n\ud83d\udcf7\n\n\ud83d\udcf7\n\nThe facial expression on a particular face image is analyzed automatically and it produces the result stating the emotion on the subject\u2019s face. The proposed system is tested against a collection of face images both containing single and multiple faces in a scene. The experimental results show better face detection rate under variable luminance levels. From this experimental result, we achieved a recognition rate of 60% when detecting in a multiple face image.", "link": "https://www.reddit.com/r/MachineLearning/comments/bom1wk/facial_expression_recognition_with_auto/"}, {"autor": "madenmud", "date": "2019-03-14 15:11:04", "content": "[1903.00812] 3D Hand Shape and Pose Estimation from a Single RGB -----> Image !!! ", "link": "https://www.reddit.com/r/MachineLearning/comments/b11gky/190300812_3d_hand_shape_and_pose_estimation_from/"}, {"autor": "RubiksCodeNMZ", "date": "2019-03-24 20:02:58", "content": "Image Segmentation using Python\u2019s scikit------> image !!!  module", "link": "https://www.reddit.com/r/MachineLearning/comments/b50siu/image_segmentation_using_pythons_scikitimage/"}, {"autor": "xenopizza", "date": "2019-03-24 16:24:51", "content": "Which techniques to learn and apply for this video (-----> image !!! ) processing ? /!/ For a side project i\u2019m taking a look at how to extract information from Apex Legends (for starters) game play videos. Example screenshot:\nhttps://www.gameprime.org/wp-content/uploads/2019/02/gp-apexlegends-6.jpg\n\nThere is a log on top right corner and player status on bottom left corner (health/shields).\n\nId like to:\n\n- extract &lt;player A username&gt; killed &lt;player B username&gt; with &lt;weapon name&gt; (at time mm:ss) from log, and other information displayed on that log\n\n- from player health extract health/shields percentage (shape analysis ?)\n\nAlso detect game start/end screens and possibly extract info from them like which characters compose the current player squads.\n\nApart from OCR (which is not enough), what kind or specific techniques should i learn to implement this ?\n\n\n", "link": "https://www.reddit.com/r/MachineLearning/comments/b4y6up/which_techniques_to_learn_and_apply_for_this/"}, {"autor": "Annomator", "date": "2019-07-02 11:46:50", "content": "[P] Annomator - A new, simple way to annotate images with masks /!/ [**https://github.com/annomator/annomator\\_1.0**](https://github.com/annomator/annomator_1.0)\n\nAnnomator automatically generates text, box and mask data for direct use or training.  Use the automatic -----> image !!!  annotation or start from scratch.  Simply paint each mask with the right color code with no external text file needed.  It is compatible with all the MSCOCO models in the Tensorflow Model Zoo.", "link": "https://www.reddit.com/r/MachineLearning/comments/c8894q/p_annomator_a_new_simple_way_to_annotate_images/"}, {"autor": "AnonMLstudent", "date": "2019-07-01 20:12:13", "content": "[D] How much can we change a paper before the -----> camera !!!  ready version? /!/ In preparation for author rebuttal for a conference, we have conducted additional experiments and significantly improved our paper already (of course the reviewers can't see this yet, we may mention it during author rebuttal if the changes relate to any of their comments). \n\nI'm wondering if this is an issue and does the final version of the paper have to be very close to the initial submission, and only improve upon what the reviewers mention? Or can we add other improvements as long as they are high quality and don't detract from the original submission?", "link": "https://www.reddit.com/r/MachineLearning/comments/c7zhgn/d_how_much_can_we_change_a_paper_before_the/"}, {"autor": "alexparinov", "date": "2019-07-01 17:14:20", "content": "[P] Albumentations, an -----> image !!!  augmentation library version 0.3 released. New weather augmentations, serialization support for reproducible machine learning pipelines, and speedup improvements /!/ You can download the library from PyPI using `pip install -U albumentations` or  clone the latest version from [https://github.com/albu/albumentations](https://github.com/albu/albumentations)\n\n&amp;#x200B;\n\n# New features\n\n**Weather augmentations.**\n\nWe've added weather augmentations such as [RandomRain](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomRain), [RandomSnow](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomSnow), [RandomSunFlare](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomSunFlare), [RandomShadow](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomShadow), [RandomFog](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomFog).\n\n&amp;#x200B;\n\nhttps://i.redd.it/4x2y4nty5q731.jpg\n\n&amp;#x200B;\n\n**Pipeline serialization.**\n\nNow we can define transformations in the code and serialize them in python dictionary, json and yaml files. [**A Jupyter notebook with examples**](https://github.com/albu/albumentations/blob/master/notebooks/serialization.ipynb)**.**\n\n&amp;#x200B;\n\n**New transformations.**\n\n[Lambda](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Lambda), [GaussianBlur](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.GaussianBlur), [ChannelDropout](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.ChannelDropout), [CoarseDropout](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.CoarseDropout).\n\n&amp;#x200B;\n\n**The full release notes are available at** [https://github.com/albu/albumentations/releases/tag/0.3.0](https://github.com/albu/albumentations/releases/tag/0.3.0)", "link": "https://www.reddit.com/r/MachineLearning/comments/c7w153/p_albumentations_an_image_augmentation_library/"}, {"autor": "nerdy_wits", "date": "2019-08-14 19:38:05", "content": "I used haar cascade to make meme face from any -----> photo !!! ", "link": "https://www.reddit.com/r/MachineLearning/comments/cqepbo/i_used_haar_cascade_to_make_meme_face_from_any/"}, {"autor": "fubbyy", "date": "2019-08-14 17:31:57", "content": "U-net segmentation blank -----> image !!!  output /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/cqcxnt/unet_segmentation_blank_image_output/"}, {"autor": "jfaouzi", "date": "2019-04-09 08:05:14", "content": "[P] pyts: A Python package for time series transformation and classification /!/ Hello everyone,\n\nToday I would like to share with you a project that I started almost 2 years ago. It will be a long post, so here is a TDLR.\n\n**TDLR**:\n* **pyts ([GitHub](https://github.com/johannfaouzi/pyts), [PyPI](https://pypi.org/project/pyts/), [ReadTheDocs](https://pyts.readthedocs.io/en/latest/)): a Python package for time series transformation and classification.**\n* **It aims to make time series classification easily accessible by providing preprocessing and utility tools, and implementations of state-of-the-art algorithms.**\n* **[pyts-repro](https://github.com/johannfaouzi/pyts-repro): Comparaison with the results published in the literature.**\n\n\n## Motivations\n\nAlmost two years ago, I was an intern at a company and a colleague was working on a time series classification task. It was my end-of-studies internship and I had been studying machine learning for one year only (my background studies were more focused on statistics). I realized that I had no knowledge about machine learning for time series besides SARIMA and all the models with fewer letters. I also had limited knowledge about computer science. I did some literature search about time series classification and discovered a lot of things that I had never heard of before. Thus, I decided to start a project with the following motivations:\n* Create a Python package through a GitHub repository (because I had no idea how both worked);\n* Look at the source code of Python packages that I used regurlaly (numpy, scikit-learn) to gain knowledge;\n* Implement algorithms about time series classification.\n\n## Development and what I learnt\n\nBefore implementing anything, I had to :\n* Learn how to package a Python project,\n* Do a more advanced literature search about time series classification,\n* Think about the structure of the package.\n\nWhen I had an overall first idea of what I wanted to do, I could start coding. During this process, I discovered a lot of tools that were already available and that I had re-implemented myself less efficiently ([numpy.digitize](https://docs.scipy.org/doc/numpy/reference/generated/numpy.digitize.html), [sklearn.utils.check_array](https://scikit-learn.org/stable/modules/generated/sklearn.utils.check_array.html), [numpy.put](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.put.html), and [numpy.lib.stride_tricks.as_strided](https://docs.scipy.org/doc/numpy/reference/generated/numpy.lib.stride_tricks.as_strided.html) come to my mind). The following process could pretty much sum up the history of this project:\n1. Try to implement a new algorithm;\n2. In doing so, find tools that do what I wanted more efficiently, not necessarly related to the new algorithm;\n3. Implement the algorithm and edit the relevant code with the newly discovered tools.\n\nTwo major *discoveries* had a huge impact on the development of this project: [scikit-learn-contrib/project-template](https://github.com/scikit-learn-contrib/project-template) and [Numba](https://github.com/numba/numba). The former made me discover a lot of concepts that I did not know about (tests, code coverage, continuous integration, documentation) and provides ready-to-use scripts. The latter made optimizing code much easier as I was very confused about Cython and building wheels, and deciced not to use Cython. I also discovered the notion of *proper code* (pep8, pep257, etc.), and [semantic versioning](https://semver.org) recently. This might be obvious for most people, but I did not know any of these concepts at the time.\n\n\n## What this package provides\n\nThe current version of pyts consists of the following modules:\n\n- `approximation`: This module provides implementations of algorithms that\napproximate time series. Implemented algorithms are\n[Piecewise Aggregate Approximation](https://pyts.readthedocs.io/en/latest/generated/pyts.approximation.PiecewiseAggregateApproximation.html#),\n[Symbolic Aggregate approXimation](https://pyts.readthedocs.io/en/latest/generated/pyts.approximation.SymbolicAggregateApproximation.html#),\n[Discrete Fourier Transform](https://pyts.readthedocs.io/en/latest/generated/pyts.approximation.DiscreteFourierTransform.html#),\n[Multiple Coefficient Binning](https://pyts.readthedocs.io/en/latest/generated/pyts.approximation.MultipleCoefficientBinning.html#) and\n[Symbolic Fourier Approximation](https://pyts.readthedocs.io/en/latest/generated/pyts.approximation.SymbolicFourierApproximation.html#).\n\n- `bag_of_words`: This module consists of a class\n[BagOfWords](https://pyts.readthedocs.io/en/latest/generated/pyts.bag_of_words.BagOfWords.html#)\nthat transforms time series into bags of words. This approach is quite common\nin time series classification.\n\n- `classification`: This module provides implementations of algorithms that\ncan classify time series. Implemented algorithms are\n[KNeighborsClassifier](https://pyts.readthedocs.io/en/latest/generated/pyts.classification.KNeighborsClassifier.html#),\n[SAXVSM](https://pyts.readthedocs.io/en/latest/generated/pyts.classification.SAXVSM.html#) and\n[BOSSVS](https://pyts.readthedocs.io/en/latest/generated/pyts.classification.BOSSVS.html#).\n\n- `decomposition`: This module provides implementations of algorithms that\ndecompose a time series into several time series. The only implemented algorithm\nis\n[Singular Spectrum Analysis](https://pyts.readthedocs.io/en/latest/generated/pyts.decomposition.SingularSpectrumAnalysis.html#).\n\n- `-----> image !!! `: This module provides implementations of algorithms that transform\ntime series into -----> image !!! s. Implemented algorithms are\n[Recurrence Plot](https://pyts.readthedocs.io/en/latest/generated/pyts.image.RecurrencePlot.html#),\n[Gramian Angular Field](https://pyts.readthedocs.io/en/latest/generated/pyts.image.GramianAngularField.html#) and\n[Markov Transition Field](https://pyts.readthedocs.io/en/latest/generated/pyts.image.MarkovTransitionField.html#).\n\n- `metrics`: This module provides implementations of metrics that are specific\nto time series. Implemented metrics are\n[Dynamic Time Warping](https://pyts.readthedocs.io/en/latest/generated/pyts.metrics.dtw.html#)\nwith several variants and the\n[BOSS](https://pyts.readthedocs.io/en/latest/generated/pyts.metrics.boss.html#)\nmetric.\n\n- `preprocessing`: This module provides most of the scikit-learn preprocessing\ntools but applied sample-wise (i.e. to each time series independently) instead\nof feature-wise, as well as an\n[imputer](https://pyts.readthedocs.io/en/latest/generated/pyts.preprocessing.InterpolationImputer.html#)\nof missing values using interpolation. More information is available at the\n[pyts.preprocessing API documentation](https://pyts.readthedocs.io/en/latest/api.html#module-pyts.preprocessing).\n\n- `transformation`: This module provides implementations of algorithms that\ntransform a data set of time series with shape `(n_samples, n_timestamps)` into\na data set with shape `(n_samples, n_features)`. Implemented algorithms are\n[BOSS](https://pyts.readthedocs.io/en/latest/generated/pyts.transformation.BOSS.html#) and\n[WEASEL](https://pyts.readthedocs.io/en/latest/generated/pyts.transformation.WEASEL.html#).\n\n- `utils`: a simple module with\n[utility functions](https://pyts.readthedocs.io/en/latest/api.html#module-pyts.utils).\n\nI also wanted to have an idea about how my implementations perform compared to the performance reported in the papers and on the [Time Series Classification Repository](http://www.timeseriesclassification.com). The point is to see if my implementations are reliable or not. To do so, I created a [GitHub repository](https://github.com/johannfaouzi/pyts-repro) where I make these comparisons on the smallest datasets. I think that my implementation of WEASEL might be under-performing, but for the other implementations reported the performance is comparable. There are sometimes intentional differences between my implementation and the description of the algorithm in the paper, which might explain the differences in performance.\n\n\n## Future work\n\nThe main reason of this post is to get feedback. I have been pretty much working on my own on this project, doing what I felt like doing. However, as a PhD student, I know how important it is to get feedback on your work. So, if you have any feedback on how I could improve the package, it would be really appreciated. Nonetheless, I still have ideas of future work:\n* Add a `dataset` module: I think that it is an important missing tool of the package. Right now I create a dumb toy dataset in all the [examples in the documentation](https://pyts.readthedocs.io/en/latest/auto_examples/index.html). Adding a couple of datasets in the package directly (I would obviously need to contact authors to get permission to do so) like the iris dataset in scikit-learn would make the examples more relevant in my opinion. Adding a function to download datasets from the [Time Series Classification Repository](http://www.timeseriesclassification.com) (similarly to [sklearn.datasets.fetch_openml](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html) or [sklearn.datasets.fetch_mldata](https://scikit-learn.org/0.19/modules/generated/sklearn.datasets.fetch_mldata.html)) would be quite useful too. Being able to generate a toy dataset like [sklearn.datasets.make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) would be a nice addition. If you have any idea about generating a classification dataset for time series, with any number of classes and any number of timestamps, feel free to comment, I would be really interested. Right now I only know the Cylinder-Bell-Funnel dataset, but it is quite limiting (128 timestamps and 3 classes).\n* Add a `multivariate` module. Currently the package provides no tools to deal with multivariate time series. Like binary classifiers that need extending for multiclass classification, adding a voting classifier (with a classifier for each feature of the multivariate time series) would be useful, as well as specific algorithms for multivariate time series.\n* Make the package available on Anacloud Cloud through the `conda-forge` channel. `conda` seems to be quite popular thanks to the utilities it provides and making the package installable with `conda` could be a plus.\n* Update the required versions of the dependencies: Currently the required versions of the dependencies are the versions that I use on my computer. I'm quite confident that older versions for some packages could work, but I have no idea how to determine them (I exclude doing a backward gridsearch until continuous integreation fails). Are there any tools that can try to guess the minimum versions of the packages, by looking at what functions are used from each package for instance?\n* Implement more algorithms: Time Series Bag-of-Features, shapelet-based algorithms, etc. A lot of algorithms are not available in the package currently. Adding more metrics specific to time series would also be great.\n\n\n## Acknowledgements\n\nLooking back at the history of this project, I realize how much I learnt thanks to the scientific Python community: there are so many open source well-documented tools that are made available, it is pretty amazing.\n\nI would also like to thank the authors of papers that I contacted in order to get more information about the algorithms that they presented. I always received quick, kind answers. Special thanks to Patrick Sch\u00e4fer, who received a lot of emails from me and always replied.\n\nI would like to thank all the people involved in the [Time Series Classification Repository](http://www.timeseriesclassification.com). It is an awesome tool with datasets freely available and reported results for each algorithm.\n\nFinally, I would like to thank every contributor to the project, as well as people helping making this package better through opening issues or sending me emails.\n\n\n## Conclusion\n\nWorking on this project has been a blast. Sometimes learning takes a lot of time, and I experienced it quite often, but I think that it is worth it. I work on this project on my spare time, so I cannot spend as much time as much as I would like, but I think that it gets slowly but steadily better. There are still a lot of things that are a bit confusing to me (all the configuration files for CI and documentation, managing a git repository with several branches and several contributors), and seeing room for improvement is also an exciting part of this experience.\n\nThere was a [post about machine learning on time series](https://www.reddit.com/r/MachineLearning/comments/9ofd7x/d_machine_learning_on_time_series_data/) on this subreddit several months ago. If you were interested in what was discussed in this post (and more specially in the top comment), you might be interested in pyts.\n\nThank you very much for reaching the end of this long post. If you have some time to give me any feedback, it would mean a lot to me. Have a very nice day!", "link": "https://www.reddit.com/r/MachineLearning/comments/bb59ji/p_pyts_a_python_package_for_time_series/"}, {"autor": "raichet", "date": "2019-04-08 20:51:24", "content": "[D] Training Object Detector with Few Image &amp; Many Labels, and Only Positive Examples /!/ Hey all,\n\n&amp;#x200B;\n\nI am building my own object detector with the TensorFlow Object Detection API, and I've been following sentdex's video series.  I have two main concerns/questions.\n\n&amp;#x200B;\n\n1. What confuses me about his video is that he only has one class label for his entire dataset, which is 154 images purely of macaroni &amp; cheese. Each -----> image !!!  consists of one macaroni and cheese, which he has annotated with a bounding box annotation tool.  **How is he able to train his NN with only positive examples, and zero negative example?** It's reassuring as I will have to label my own images, and his result means I don't have to manually label the negative examples. But I would like to understand why negative examples weren't needed.\n2. Furthermore, unrelated to his video series but specific to my own case, **I don't have many images (45 ish), but each image has 100+ instances of the object I want to detect which I can label with a bounding box. Would this suffice for training data?** The testing data will be of the same format. Sentdex has many many images, each containing one bounding box. My data set has very few images, each containing many bounding boxes. I am hoping this would work.\n\n&amp;#x200B;\n\nYour answer will be appreciated! :)", "link": "https://www.reddit.com/r/MachineLearning/comments/bayzc7/d_training_object_detector_with_few_image_many/"}, {"autor": "veqtor", "date": "2019-04-08 07:47:27", "content": "[D] What color spaces work best for -----> image !!!  GANs? /!/ So recently I've been thinking:\nDoes it really make sense to use the RGB colorspace for generating images?\n\nIn a lot of images you have objects with a certain hue and saturation while luminance is relative to the mean in a picture.\nThis thought came up when I had a GAN training collapse into pure Red, green and blue patches. Surely this wouldn't have happened in a HSV space?\n\nI tried Googling but didn't find anything, has there been any ablation studies on colorspaces for GANs?", "link": "https://www.reddit.com/r/MachineLearning/comments/baqytt/d_what_color_spaces_work_best_for_image_gans/"}, {"autor": "wei_jok", "date": "2019-04-08 06:03:50", "content": "[D] Deep Learning And Shallow Data /!/ *Article from Piekniewski's [blog](https://blog.piekniewski.info/2019/04/07/deep-learning-and-shallow-data/) discusses areas in computer vision that has not been \u201csolved\u201d by deep learning. Excerpt:*\n\n**Data is shallower than we thought**\n\nDeep learning surprisingly taught us something very interesting about visual data (high dimensional data in general): in ways it is much \"shallower\" than we believed in the past. There seems to be many more ways to statistically separate a visual dataset labeled with high level human categories, then there are ways to separate such dataset that are \"semantically correct\". In other words, the set of low level -----> image !!!  features is a lot more \"statistically\" potent than we imagined. This is the great discovery of deep learning. The question of how to generate models that would find \"semantically sound\" ways of separating visual datasets remains open and in fact now seems even harder to answer than before.\n\nDeep learning is here to stay and is now a crucial part of computer vision toolbox. But traditional computer vision is not going anywhere and could still be used to build very powerful detectors. These hand crafted detectors may not achieve as high performance on some particular dataset metric, but can be guaranteed to rely on \"semantically relevant\" set of features of the input. Therefore their failure modes can be much better characterized and anticipated. Deep learning is providing statistically powerful detectors without the expense of feature engineering, though one still has to have a lot of labeled data, lot of GPU's and a deep learning expert onsite. However these powerful detectors will fail unexpectedly and their range of applicability cannot be easily characterized (or to put it more strongly - cannot be characterized at all, period). In applications where rare but catastrophic failure is acceptable, deep learning will work fine. For applications in which guaranteed performance within a given set of conditions plus computational complexity are more important, classical machine vision pipelines will be in use for many years to come.\n\nhttps://blog.piekniewski.info/2019/04/07/deep-learning-and-shallow-data/", "link": "https://www.reddit.com/r/MachineLearning/comments/baq5c3/d_deep_learning_and_shallow_data/"}, {"autor": "JonathanFly", "date": "2019-08-26 20:28:53", "content": "[P] I applied the recent 'Progressive Face Super-Resolution via Attention to Facial Landmark' to create '-----> photo !!! -realistic' Emojis and Emotes.", "link": "https://www.reddit.com/r/MachineLearning/comments/cvtml5/p_i_applied_the_recent_progressive_face/"}, {"autor": "ReasonableRoll7", "date": "2019-08-26 09:52:46", "content": "Near infrared -----> image !!!  data, sources? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/cvlwbp/near_infrared_image_data_sources/"}, {"autor": "thomas_ver", "date": "2019-08-26 08:05:27", "content": "Convert prediction pipeline to a model file /!/ I am having some issues trying to convert my entire prediction pipeline into a single model file, and could use some advice.\n\nMy prediction pipeline receives an input -----> image !!!  which gets sent through the trained model. Afterwards I apply some functions to get more accurate results. I can easily convert my trained model to a h5 model and just use this to get my predictions. However I would like to add my functions to this file.\n\nIs this in any case possible or am I doomed?", "link": "https://www.reddit.com/r/MachineLearning/comments/cvl2am/convert_prediction_pipeline_to_a_model_file/"}, {"autor": "textssg", "date": "2019-07-12 05:21:49", "content": "Is there any algorithm to determine whether the -----> image !!!  has vignetting or not? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/cc6mbf/is_there_any_algorithm_to_determine_whether_the/"}, {"autor": "sensetime", "date": "2019-11-26 02:09:24", "content": "[D] Chinese government uses machine learning not only for surveillance, but also for predictive policing and for deciding who to arrest in Xinjiang /!/ Link to story: https://www.icij.org/investigations/china-cables/exposed-chinas-operating-manuals-for-mass-internment-and-arrest-by-algorithm/\n\nThis post is not an ML *research* related post, but I think it will be interesting to the community to see how research is applied by authoritarian governments to achieve their goals. It is related to a few previous popular posts on this thread with high upvotes, which prompted me to post this [story](https://www.icij.org/investigations/china-cables/exposed-chinas-operating-manuals-for-mass-internment-and-arrest-by-algorithm/):\n\n[ICCV 19: The state of some ethically questionable papers](https://redd.it/dp389c)\n\n[Hikvision marketed ML surveillance -----> camera !!!  that automatically identifies Uyghurs](https://redd.it/dv5axp)\n\n[Working on an ethically questionnable project...](https://redd.it/dw7sms)\n\nThis [story](https://www.icij.org/investigations/china-cables/exposed-chinas-operating-manuals-for-mass-internment-and-arrest-by-algorithm/) reports the details of a new leak of highly classified Chinese government documents reveals the operations manual for running the mass detention camps in Xinjiang and exposed the mechanics of the region\u2019s system of mass surveillance.\n\n*The [lead journalist](https://twitter.com/BethanyAllenEbr/status/1198663008152621057) of the story also wrote a summary of findings:*\n\nThe China Cables represent the first leak of a classified Chinese government document revealing the inner workings of the detention camps, as well as the first leak of classified government documents unveiling the predictive policing system in Xinjiang.\n\nThe leak features classified intelligence briefings that reveal, in the government\u2019s own words, how Xinjiang police essentially take orders from a massive \u201ccybernetic brain\u201d known as IJOP, which flags entire categories of people for investigation &amp; detention.\n\nThese secret intelligence briefings reveal the scope and ambition of the government\u2019s AI-powered policing platform, which purports to predict crimes based on computer-generated findings alone. The result? Arrest by algorithm.\n\n*The article describe methods used for algorithmic policing:*\n\nThe classified intelligence briefings reveal the scope and ambition of the government\u2019s artificial-intelligence-powered policing platform, which purports to predict crimes based on these computer-generated findings alone. Experts say the platform, which is used in both policing and military contexts, demonstrates the power of technology to help drive industrial-scale human rights abuses.\n\n\u201cThe Chinese have bought into a model of policing where they believe that through the collection of large-scale data run through artificial intelligence and machine learning that they can, in fact, predict ahead of time where possible incidents might take place, as well as identify possible populations that have the propensity to engage in anti-state anti-regime action,\u201d said Mulvenon, the SOS International document expert and director of intelligence integration. \u201cAnd then they are preemptively going after those people using that data.\u201d\n\nIn addition to the predictive policing aspect of the article, there are side articles about the entire ML stack, including how [mobile apps](https://www.icij.org/investigations/china-cables/how-china-targets-uighurs-one-by-one-for-using-a-mobile-app/) are used to target Uighurs, and also how the inmates are [re-educated](https://www.bbc.com/news/world-asia-china-50511063) once inside the concentration camps. The documents reveal how every aspect of a detainee's life is monitored and controlled.\n\nhttps://www.icij.org/investigations/china-cables/exposed-chinas-operating-manuals-for-mass-internment-and-arrest-by-algorithm/\n\n*Note: My motivation for posting this story is to raise ethical concerns and awareness in the research community (see this [thread](https://redd.it/e10b5x) for context). I do not want to heighten levels of racism towards the Chinese research community (not that it may matter, but I am Chinese). I am aware of the fact that the Chinese government's policy is to integrate the state and the people as one, so accusing the party is perceived domestically as insulting the Chinese people, but I also believe that we as a research community is intelligent enough to be able to separate government, and those in power, from individual researchers.*", "link": "https://www.reddit.com/r/MachineLearning/comments/e1r0ou/d_chinese_government_uses_machine_learning_not/"}, {"autor": "Heringsalat100", "date": "2019-05-17 21:24:10", "content": "Perfect discriminator in GAN - bad results. /!/ Hello!\n\nI am playing around with -----> image !!!  generating GANs (-----> image !!!  as input + output for the generator, so my \"noise\" is in fact a bunch of -----> image !!! s) with Keras+Tensorflow in the last few weeks. My problem: I get horrible results!\n\nVocab for clarification:\ngenerator = generator only\ndiscriminator = discriminator only (binary crossentropy as loss)\ngan = generator + discriminator with discriminator output (binary crossentropy as loss)\n\nSo what is happening?\n\nAfter a couple of epochs my discriminator prediction accuracy for generated (label 0), real (1) and (added for better quality) some distorted (0) images is converging towards 100%.\n\nAfter the discriminator training (per batch) I begin training my gan with real image inputs (labeled as truth, 1), the trainability of the discriminator (generator) is locked (unlocked) for this process. The gan gives me another 100% accuracy after dozens of epochs!\n\nBut: The resulting generator is giving me very bad results! Even when I try to pretrain the generator in a way that gives me relatively good results *before* I go through the first GAN epoch (= discriminator training + gan training with locked discriminator), the outputs become worse even directly after the first GAN epoch.\n\nWhat I don't understand is: When the discriminator is able to understand the difference between generated and real images with 100% accuracy in nearly every case and when the gan shows 100% accuracy at the same time how is the generator able to perform that bad? I am aware of problems like mode collapse but I don't understand how the generator can fool the discriminator within the gan training process but at the same time this discriminator seems to be perfectly able to classify (generated+distorted) = fake and real images.\n\nDo you have any general suggestions regarding this problem? Or do you need explicit code?", "link": "https://www.reddit.com/r/MachineLearning/comments/bpwy57/perfect_discriminator_in_gan_bad_results/"}, {"autor": "achaiah777", "date": "2019-05-16 15:53:10", "content": "[P] Pywick - High-Level Training framework for Pytorch /!/ Hey fellow redditors, please allow me to introduce you to [Pywick](https://github.com/achaiah/pywick) - a high-level Pytorch training framework that aims to get you up and running quickly with state of the art neural networks for [-----> image !!!  classification and segmentation](https://pywick.readthedocs.io/en/latest/api/pywick.models.html), [optimizers](https://pywick.readthedocs.io/en/latest/api/pywick.optimizers.html) (like SWA, AdamW), activation functions (swish/aria) etc. Does the world really need another Pytorch framework? Probably not. But I began working on this project about three years ago when no good frameworks were available and it just kept growing. So it probably doesn't hurt to have another open-source framework out there.\n\n\nThe goal is to try to keep up with the state of the art in terms of [models](https://pywick.readthedocs.io/en/latest/api/pywick.models.html) and various training enhancements so that you have something to play around with. Does SWA or Swish really guarantee you best results? Probably not... but it's there in case you want to check it out.\n\n\nLastly, I just wanted to mention that I really can't take credit for any of the cool stuff. I mostly assembled and cleaned up what other awesome people have already done. But that way you don't need to do the dirty work and can just use it.", "link": "https://www.reddit.com/r/MachineLearning/comments/bpe92v/p_pywick_highlevel_training_framework_for_pytorch/"}, {"autor": "attiaa13", "date": "2019-03-17 18:09:02", "content": "[D] Tuning Hyper Parameters in Weakly-Supervised Learning? /!/ I'm working on a project on weakly localization, i.e. localize object in -----> image !!!  when the data I have for training is -----> image !!! s with their class labels (for example CUB or ImageNet).\n\nIv'e created my model, which have several regularizations with different weights, and found myself with a problem in tuning my hyper parameters since I don't have any labeled bounding boxes (It's a research project, so I don't want to tune it with data which has bounding boxes).\n\nI wandered if you know of ideas to tune parameters in such scenarios.\n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/b280na/d_tuning_hyper_parameters_in_weaklysupervised/"}, {"autor": "plmlp1", "date": "2019-06-12 22:22:40", "content": "[P] Need help with -----> Image !!!  Captioning /!/ Hello I'm trying to learn CNNs and I've hit a deadend with an Image Captioning project I was working on for fun.\n\nDataset: 10k images from [Google Conceptual Captions](https://ai.google.com/research/ConceptualCaptions/download)  \n\nTutorial I'm mostly following: [Automatic Image Captioning](https://github.com/hlamba28/Automatic-Image-Captioning)  \n\n\nOne difference between my dataset and the Flicker8k dataset in the tutorial is that my dataset only has one caption per image but latter has five captions per image.\n\nThe problem is that I am getting the same caption for nearly all images. I have tried to use:\n- LSTM instead of GRU cells\n- 50 and 200 Glove word embeddings. I even tried to create my own embeddings using all captions in the dataset\n- beam search and greedy search to get a prediction\n\nWhat do I do?", "link": "https://www.reddit.com/r/MachineLearning/comments/bzxn9u/p_need_help_with_image_captioning/"}, {"autor": "plmlp1", "date": "2019-06-12 21:48:22", "content": "Need help with -----> Image !!!  Captioning /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/bzx94d/need_help_with_image_captioning/"}, {"autor": "LFOwen", "date": "2019-06-12 18:54:27", "content": "Tiling and Randomizing Images as a Stopgap for Visual Data Shortage [D] /!/ Hi there,\n\nThis is my first post and, as a beginner, I don't yet have the vocabulary to totally articulate my thoughts on this and I apologize in advance for that. \n\nI'm currently interested in training a convolutional neural network to recognize images. However, a problem I'm encountering is a lack of source images so I thought, why not throw the few images I have into some program that could cut them up into tiles and shuffle them around randomly?\n\nUpon further reading, it seems like this is - or is at least similar to - a preestablished method for synthesizing new visual data called Structured Domain Randomization.\n\nMy question: is this a valid method for training my neural network? \n\nI'm worried that the algorithm wouldn't actually be learning anything new since the structures present within each -----> image !!!  will remain the same, only their location within the -----> image !!!  will change.\n\nThank you!", "link": "https://www.reddit.com/r/MachineLearning/comments/bzv4a2/tiling_and_randomizing_images_as_a_stopgap_for/"}, {"autor": "rand0mstring", "date": "2019-06-12 17:41:41", "content": "-----> Picture !!!  of a Person -&gt; Anime /!/ I'm trying to find a model to run to convert a real picture of a person into an anime-ified version.\n\n&amp;#x200B;\n\nI was checking out [TwinGAN](https://zhuanlan.zhihu.com/p/35777788) but the results appear almost completely independent of the source image. \n\n&amp;#x200B;\n\nAnyone know point me in a better direction?", "link": "https://www.reddit.com/r/MachineLearning/comments/bzu87u/picture_of_a_person_anime/"}, {"autor": "taki0112", "date": "2019-06-12 10:19:42", "content": "[P] Simple Tensorflow implementation of \"-----> Image !!! -to------> Image !!!  Translation via Group-wise Deep Whitening-and-Coloring Transformation\" (CVPR 2019 Oral) /!/ &amp;#x200B;\n\n[Comparison with baselines on Artworks dataset](https://i.redd.it/upn54l9uiw331.png)\n\n&amp;#x200B;\n\n[Comparison with baselines on CelebA dataset](https://i.redd.it/1nuxnn5viw331.png)", "link": "https://www.reddit.com/r/MachineLearning/comments/bzpg5u/p_simple_tensorflow_implementation_of/"}, {"autor": "cashshots", "date": "2019-06-12 08:27:45", "content": "[D] Text detection - recognition - extraction /!/ For a project, I need to get all the text off an -----> image !!! , in a structured format (sentences, paragraphs, etc.), and have it be accurate. \n\nMost of my experiments have dealt with scene detection, which usually just detects text being there in a non structured. The out of the box OCR engines don\u2019t seem to be accurate, as I\u2019m hoping to run some NLP on top of the extracted data. \n\nAn idea I had was detecting sentences and paragraphs of text, cropping and OCRing the data until there is no more text on the page, but I found that text recognition isn\u2019t that far along yet. \n\nI\u2019m looking for any help going forward, and hopefully come up with an end to end solution for this.", "link": "https://www.reddit.com/r/MachineLearning/comments/bzokof/d_text_detection_recognition_extraction/"}, {"autor": "fw_Flicker", "date": "2019-10-09 01:42:29", "content": "I made a mobile app that uses -----> image !!!  learning to detect \"feces\" or \"no feces\" - I shit you not.", "link": "https://www.reddit.com/r/MachineLearning/comments/dfa32x/i_made_a_mobile_app_that_uses_image_learning_to/"}, {"autor": "roset_ta", "date": "2019-10-08 07:14:44", "content": "[D] From your experience, what's best for optimizing the performance of an object segmentation model? /!/ What's your experience in optimizing object/medical -----> image !!!  segmentation tasks? Did Adam or SGD work best for you?", "link": "https://www.reddit.com/r/MachineLearning/comments/dewk9c/d_from_your_experience_whats_best_for_optimizing/"}, {"autor": "viviandefeater", "date": "2019-10-12 00:20:41", "content": "[D] Learning with \"noisy data\" (but perfect labels) /!/ There are many works that deal with noisy labels, but has the problem of unreliable data (but reliable labels) been studied? In other words, problems where the data to be classified is imperfect and not always sufficient to determine the class label.\n\nAn example would be a model that predicts the city in which a -----> photo !!!  was taken. Ground truth labels would be perfect thanks to GPS metadata. If the photo contains the Eiffel Tower, we can predict that the city is Paris. But many pictures contain no useful information; for example a photo of a dog or a McDonald's is nearly useless for determining the city.\n\nHow best to train a classifier when such \"noisy examples\" (for lack of a better term) are very common?", "link": "https://www.reddit.com/r/MachineLearning/comments/dgnpcb/d_learning_with_noisy_data_but_perfect_labels/"}, {"autor": "NoctisShadowzel", "date": "2019-10-11 12:38:48", "content": "Making an ML application that checks if identicon in -----> Camera !!!  preview matches with one in the device memory... /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/dgek9m/making_an_ml_application_that_checks_if_identicon/"}, {"autor": "Danbradford7", "date": "2019-09-14 00:07:02", "content": "Using an -----> image !!!  analogy [P]rogram and my -----> image !!! s have these colored dots (idk how else to describe it) that makes it look like it\u2019s behind glass. Any ideas how to fix it?", "link": "https://www.reddit.com/r/MachineLearning/comments/d3xwc5/using_an_image_analogy_program_and_my_images_have/"}, {"autor": "MartialLemur", "date": "2019-09-16 19:37:05", "content": "[D] Any developments on randomly wired neural networks? /!/ I recently read this [paper](https://arxiv.org/abs/1904.01569) which randomly wires -----> image !!!  classification networks using classical random graph generators. \n\nI'm interested in the subject but haven't been able to find anything on it since. \nHas anyone heard of any new developments on the subject?", "link": "https://www.reddit.com/r/MachineLearning/comments/d55drc/d_any_developments_on_randomly_wired_neural/"}, {"autor": "youshouldknowsz", "date": "2019-09-16 11:01:49", "content": "[D] Convolutional Neural Network with Numpy input Help G /!/  Good evening good sir\n\nI am a newbie with ML, i found this code from fellow redditor parasdahal.\n\nI can make it run with mnist datasets.\n\nHave few questions, i'd really really really appreciate it if someone can help me\n\n[https://github.com/iqbaalmuhmd/CNNnumpy](https://github.com/iqbaalmuhmd/CNNnumpy)\n\n1. Is it possible to run only test, with our own -----> image !!!  input. Or just random image from mnist dataset\n2. What should i pickle to save the parameters?", "link": "https://www.reddit.com/r/MachineLearning/comments/d4z2bv/d_convolutional_neural_network_with_numpy_input/"}, {"autor": "mistycheney", "date": "2019-10-01 15:13:08", "content": "[Discussion] Any datasets for multi-modal learning between time series and text? /!/ Multi-modal learning has traditionally focused on -----> image !!! /video vs text (e.g. -----> image !!!  captioning, video description), but does anyone know good datasets for learning between text and time series? Example of time series: stock charts, power plant / wearable sensor readings, music etc.\n\nI am looking for natural language human comments on these types of data. Examples I can think of are:  \n\\- stock charts &lt;-&gt; analyst notes  \n\\- power plant sensor data &lt;-&gt; operator notes  \n\\- wearable sensor data &lt;-&gt; coach notes or commentary  \n\\- music data &lt;-&gt; critics or teacher notes\n\nI wonder if there are real-world datasets of these types?\n\nThanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/dbusmy/discussion_any_datasets_for_multimodal_learning/"}, {"autor": "thePythonWizard", "date": "2019-10-01 04:40:52", "content": "-----> Image !!!  processing but don't know the method /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/dbo6ui/image_processing_but_dont_know_the_method/"}, {"autor": "irishabh__", "date": "2019-09-29 19:57:27", "content": "I have created a variant of neural style transfer, where you can mix two painting styles, keeping intact your content -----> image !!!  and both the styles mixed together, opinions? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/db0aqj/i_have_created_a_variant_of_neural_style_transfer/"}, {"autor": "prashantabides", "date": "2019-10-02 19:35:49", "content": "For an OCR : please could you guys provide some good resources for -----> image !!!  pre processing for text /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/dcf7v1/for_an_ocr_please_could_you_guys_provide_some/"}, {"autor": "Vermeille", "date": "2019-09-21 20:53:40", "content": "[P] Torchelie - Pytorch extended with more optimizers, losses, layers, models, transformations, and even training loops /!/ Hi guys,\n\nI've been working on Torchelie for something like two or three months now. Notable additions are :\n\n- bitempered loss\n- VQ layer from VQ-VAE\n- RAdam\n- SPADE / Conditional BatchNorm\n- Neural Style loss / Deep Dream loss / Feature viz loss\n- Spectral -----> image !!!  to reproduce distill.pub's amazing feature viz results\n\nAnyway, here's the link: https://github.com/Vermeille/Torchelie/\nand the documentation is here: https://torchelie.readthedocs.io/\n\nCriticism and bug reports are absolutely welcome.\n\nThank you for trying it out :)", "link": "https://www.reddit.com/r/MachineLearning/comments/d7g5td/p_torchelie_pytorch_extended_with_more_optimizers/"}, {"autor": "szymonmaszke", "date": "2019-09-21 14:13:58", "content": "[P] torchdata: Implement map, cache, filter etc. within PyTorch's Datasets (like Tensorflow's tf.data and more) /!/ **Hi /r/MachineLearning**,\n\n# What is [torchdata](https://github.com/szymonmaszke/torchdata)\n\nI would like to present you a new open source PyTorch based project ([__torchdata__](https://szymonmaszke.github.io/torchdata/)) which extends capabilities of `torch.utils.data.Dataset` by bringing `map`, `cache` and other operations known from `tensorflow.data.Dataset` (and actually a little more than that).\n\n### All that with a single line of code: `super().__init__()`\n\nFor more, check [documentation](https://szymonmaszke.github.io/torchdata/) or [github repository](https://github.com/szymonmaszke/torchdata).\n\n# Functionalities Overview\n\n* Use `map`, `apply`, `reduce` or `filter`\n* `cache` data in RAM or on disk (even partial caching, say first `20%` RAM and the rest on disk)\n* Full PyTorch's [`Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) and [`IterableDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset&gt;) support (including [`torchvision`](https://pytorch.org/docs/stable/torchvision/index.html))\n* General `torchdata.maps` like `Flatten` or `Select`\n* Concrete `torchdata.datasets` designed for file reading and other general tasks\n\n# Example\n\n- Create -----> image !!!  reading dataset \n\n        import torchdata\n        import torchvision\n\n\n        class Images(torchdata.Dataset): # Different inheritance\n             def __init__(self, path: str):\n                super().__init__() # This is the only change\n            self.files = [file for file in pathlib.Path(path).glob(\"*\")]\n\n        def __getitem__(self, index):\n            return Image.open(self.files[index])\n\n        def __len__(self):\n            return len(self.files)\n\n- `map` each element to `torch.Tensor` and `cache()` everything in memory:\n\n        images = Images(\"./data\").map(torchvision.transforms.ToTensor()).cache()\n\n- concatenate with labels (another `torchdata.Dataset` instance) and iterate over:\n\n        for data, label in images | labels:\n            # Do whatever you want with your data\n\n# Installation \n\n`pip` is the easiest of course:\n\n        pip install torchdata\n\nYou can also use `nightly` releases (`torchdata-nightly`) or GPU/CPU Docker based images (check documentation). Hopefully `conda` will be released soon as well, stay tuned\n\nBTW. You can also checkout [__torchfunc__](https://github.com/szymonmaszke/torchfunc), I plan to make a separate post about that in a week or so.\n\nThanks for checking the above, any input would be welcome (either here or on github)", "link": "https://www.reddit.com/r/MachineLearning/comments/d7b01f/p_torchdata_implement_map_cache_filter_etc_within/"}, {"autor": "Mjjjokes", "date": "2019-09-20 20:01:34", "content": "[D] Is accurately estimating the contents of a voxel outside of a real 3D model possible? /!/ Imagine we have a training set of real 3D models (by this I mean, for example, 3D models of a rooms in buildings), and with this training set, we train the neural network to understand the relationship between a voxel and the 3D model in which it resides. Then, we use our neural network to predict one voxel  outside of the captured (by a -----> camera !!! ) 3D model. Is this possible? Would the desired results be achieved?", "link": "https://www.reddit.com/r/MachineLearning/comments/d70apg/d_is_accurately_estimating_the_contents_of_a/"}, {"autor": "BigChaseUSA", "date": "2019-09-20 19:04:21", "content": "Name of user friendly -----> image !!!  recognition desktop application? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/d6zj62/name_of_user_friendly_image_recognition_desktop/"}, {"autor": "lannister_the_imp", "date": "2019-09-19 17:45:20", "content": "Equation or Formula Extraction from Machine Learning /!/ One of the biggest issues for machine learning or artificial intelligence in general is that it is a black box. Especially when the intelligence gets more and more complicated the more of a black box you get. \n\nIn electrical or programming terms a black box is a unknown process where information gets fed into and a output is returned. You do not know what happens within the black box thus the name. x(n) -&gt;h(x(n))-&gt;y(n) this is the black box equivalent in mathematical terms where we do not know what h(n) is. \n\nOne of the processes used for cryptocurrency is that it takes alot of brute force to create a equation that can with very little effort be verified. This sounds very similar to machine learning or CNN where alot of computer power is required to produce a network where it takes less computer power to run the network after its been trained. \n\nOne of the biggest issues for certain industries is that a network produces nodes, layers, weights and biases for those nodes and layers. It doesn't produce a equation that can be extracted and later used elsewhere. This means that until a machine learning algorithm can produce a equation it is unable to run on 8bit micro-controllers or that heavily taught networks cant run on mobile phones from 5 years ago. \n\nI have spent some time looking at research papers and going from form to form looking at how extraction of a machine learning network can be done. I've found the networks that do the types of operations that I am looking for such as -----> image !!!  analysis may be light but will still unfortunately be a black box. For the medical industry nothing can be a black box, that's why again and again machine learning is denied from new equipment or it requires a hospital staff member to verify the information after the machine has done its task. \n\nDoes anyone know of a way of producing a equation or a human readable peice of information that can be used for machine learning?", "link": "https://www.reddit.com/r/MachineLearning/comments/d6hk7p/equation_or_formula_extraction_from_machine/"}, {"autor": "matchstick515", "date": "2019-09-24 12:25:13", "content": "-----> Image !!!  clustering/classification /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/d8mfaz/image_clusteringclassification/"}, {"autor": "kevinpl07", "date": "2019-09-23 10:06:16", "content": "[N] AEV releases multi-modal autonomous driving dataset /!/ An open multi-sensor dataset for autonomous driving research available for the academic community. This dataset comprises of annotated semantic segmentation images and point clouds, 3D bounding boxes, and also unannotated 360\u00c2\u00b0 time-synchronized and registered -----> camera !!!  and lidar data, as well as data bus, for three recorded sequences. We hope this commercial-friendly dataset will further facilitate active research and development in AI and robotics for autonomous driving.\n\nLink:\nhttps://www.audi-electronics-venture.de/aev/web/en/driving-dataset.html", "link": "https://www.reddit.com/r/MachineLearning/comments/d84gjf/n_aev_releases_multimodal_autonomous_driving/"}, {"autor": "ingolstadter", "date": "2019-09-23 10:02:28", "content": "[N] AEV releases a multi-modal autonomous driving dataset /!/ Webpage: [https://www.audi-electronics-venture.de/aev/web/en/driving-dataset.html](https://www.audi-electronics-venture.de/aev/web/en/driving-dataset.html)\n\n\"An open multi-sensor dataset for autonomous driving research available for the academic community. This dataset comprises of annotated semantic segmentation images and point clouds, 3D bounding boxes, and also unannotated 360\u00c2\u00b0 time-synchronized and registered -----> camera !!!  and lidar data,  as well as data bus, for three recorded sequences. We hope this commercial-friendly dataset will further facilitate active research and  development in AI and robotics for autonomous driving.\"\n\nLicense: CC BY ND 4.0", "link": "https://www.reddit.com/r/MachineLearning/comments/d84f9l/n_aev_releases_a_multimodal_autonomous_driving/"}, {"autor": "ingolstadter", "date": "2019-09-23 09:36:50", "content": "[N] AEV releases a multi-modal autonomous driving dataset /!/ Webpage: [a2d2.audi](https://a2d2.audi/)\n\n\"An open multi-sensor dataset for autonomous driving research available for the academic community. This dataset comprises of annotated semantic segmentation images and point clouds, 3D bounding boxes, and also unannotated 360\u00c2\u00b0 time-synchronized and registered -----> camera !!!  and lidar data, as well as data bus, for three recorded sequences. We hope this commercial-friendly dataset will further facilitate active research and development in AI and robotics for autonomous driving.\"\n\nLicense: CC BY-ND 4.0", "link": "https://www.reddit.com/r/MachineLearning/comments/d847hk/n_aev_releases_a_multimodal_autonomous_driving/"}, {"autor": "tp_taran", "date": "2019-06-20 09:58:42", "content": "Why does normalizing an -----> image !!!  dataset affects the accuracy of the model. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/c2u4k3/why_does_normalizing_an_image_dataset_affects_the/"}, {"autor": "ganesh_anand", "date": "2019-05-07 01:40:28", "content": "[P] Identifying where a -----> picture !!!  was taken /!/ I have a collection of images taken from different windows in buildings pointing out and down. I'm trying to identify the window location on pictures facing the building.  So far I've tried to just frame this as a regression problem with 3 images as input. See example below. \n\n*Processing img 7ch0vo241pw21...*\n\nI want to know if this is an existing problem and if so what is the name for it. I've been stuck figuring out what exactly to search. The closest I've found is relative camera pose estimation but its not exactly what I want.", "link": "https://www.reddit.com/r/MachineLearning/comments/blkp6b/p_identifying_where_a_picture_was_taken/"}, {"autor": "ganesh_anand", "date": "2019-05-07 01:29:26", "content": "Identifying where a -----> picture !!!  was taken /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/blkl1y/identifying_where_a_picture_was_taken/"}, {"autor": "mingyuliutw", "date": "2019-05-09 17:28:37", "content": "[R] Few-Shot Unsupervised -----> Image !!! -to------> Image !!!  Translation /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/bmmltf/r_fewshot_unsupervised_imagetoimage_translation/"}, {"autor": "mingyuliutw", "date": "2019-05-09 17:25:52", "content": "Few-Shot Unsupervised -----> Image !!! -to------> Image !!!  Translation /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/bmmko0/fewshot_unsupervised_imagetoimage_translation/"}, {"autor": "cdossman", "date": "2019-05-09 15:10:01", "content": "[Research] A Collection of High-Quality Datasets for Product Representation Learning /!/  [https://medium.com/ai%C2%B3-theory-practice-business/new-high-quality-datasets-for-product-representation-learning-ac3ad1492bf4](https://medium.com/ai%C2%B3-theory-practice-business/new-high-quality-datasets-for-product-representation-learning-ac3ad1492bf4) \n\nAbstract:  ProductNet is a collection of high-quality product datasets for better product understanding. ProductNet aims at supporting product representation learning by curating product datasets of high quality with properly chosen taxonomy. In this paper, the two goals of building high-quality product datasets and learning product representation support each other in an iterative fashion: the product embedding is obtained via a multi-modal deep neural network (master model) designed to leverage product -----> image !!!  and catalog information; and in return, the embedding is utilized via active learning (local model) to vastly accelerate the annotation process.  For the labeled data, the proposed master model yields high categorization accuracy (94.7% top-1 accuracy for 1240 classes), which can be used as search indices, partition keys, and input features for machine learning models. The product embedding, as well as the fined-tuned master model for a specific business task, can also be used for various transfer learning tasks.", "link": "https://www.reddit.com/r/MachineLearning/comments/bmkybv/research_a_collection_of_highquality_datasets_for/"}, {"autor": "cs_on_detours", "date": "2019-05-09 14:39:28", "content": "[D] Transfer learning from simple to complex task /!/ Hi, I'm currently working on another project and I'm considering using transfer learning. I want to train a network with simple data that is easy to produce and use the pretrained network as a starting point for a more complicated model.\n\nEg. train a model to recognizing letters from images. Then somehow \nuse another network to recognize words from a -----> image !!! . \n\nSure it is possible to create bounding boxes for each letter and then use the second network to predict them, but is there a way to apply transfer learning to a end to end approach?\n\nAre there any papers in this area, any keywords that are describing these tasks.", "link": "https://www.reddit.com/r/MachineLearning/comments/bmklor/d_transfer_learning_from_simple_to_complex_task/"}, {"autor": "TheCuddlyWhiskers", "date": "2019-05-09 09:56:20", "content": "[P] MiraPy: A Python package for Deep Learning in Astronomy /!/ MiraPy is a Python package for Deep Learning in Astronomy. It is built using Keras for developing ML models to run on CPU and GPU seamlessly. The aim is to make applying machine learning techniques on astronomical data easy for astronomers, researchers and students.\n\n&amp;#x200B;\n\nGithub: [https://github.com/mirapy-org/mirapy](https://github.com/mirapy-org/mirapy)\n\n## Applications\n\nMiraPy can be used for problem-solving using ML techniques and will continue to grow to tackle new problems in Astronomy. Following are some of the experiments that you can perform right now:\n\n* Classification of X-Ray Binaries using neural network\n* Astronomical -----> Image !!!  Reconstruction using Autoencoder\n* Classification of the first catalogue of variable stars by ATLAS\n* HTRU1 Pulsar Dataset -----> Image !!!  Classification using Convolutional Neural Network\n* Curve Fitting using Autograd (incomplete implementation)\n\nYou can find the applications MiraPy in our [tutorial](https://github.com/mirapy-org/tutorials) repository.\n\nIn future, MiraPy will be able to do more and in better ways and we need your suggestions! Tell us what you would like to see as a part of this package on [Slack](https://join.slack.com/t/mirapy/shared_invite/enQtNjEyNDQwNTI2NDY3LTE3ZmI3M2EyMjdkZWU4NTE2NjkxZjdhYWE4ZjUyODY0NzllNzRlMzZhNThhNWRiMjk4MjNhYWQ3NjA3YjJiNGY).", "link": "https://www.reddit.com/r/MachineLearning/comments/bmht3f/p_mirapy_a_python_package_for_deep_learning_in/"}, {"autor": "oneAJ", "date": "2019-06-03 12:09:34", "content": "How to make a book cover scanning algorithm similar to Goodreads? /!/ Hi,\n\n&amp;#x200B;\n\nI'd like to know if its trivial to implement an -----> image !!!  scanner in an app that can scan book covers and then correctly identify the book like this feature in Goodreads: [https://www.goodreads.com/blog/show/913-goodreads-hack-scan-a-book-cover](https://www.goodreads.com/blog/show/913-goodreads-hack-scan-a-book-cover)\n\n&amp;#x200B;\n\nI have some experience in ML and know a little about CNNs which I assume would be the right path for this task but I'd just like to know whether this is trivial to make or extremely difficult - I would be needed an accuracy of 99%", "link": "https://www.reddit.com/r/MachineLearning/comments/bw9xhm/how_to_make_a_book_cover_scanning_algorithm/"}, {"autor": "gangstergattuso", "date": "2019-06-02 19:09:21", "content": "Help finding research on -----> image !!!  classification tasks for which the output class depends on the spatial location of objects, rather than the category of objects. /!/ For example, an image with a cat in the center would have a different class compared to an image with a cat in the bottom-left.", "link": "https://www.reddit.com/r/MachineLearning/comments/bw0z5h/help_finding_research_on_image_classification/"}, {"autor": "atum47", "date": "2019-06-23 18:47:36", "content": "[P] That time I used a simple machine learning algorithm to classify deforested areas /!/ &amp;#x200B;\n\n[Final result.](https://i.redd.it/vfqu5lzdj5631.png)\n\n&amp;#x200B;\n\nThis project was developed as an assignment for a machine learning class, where we were given a pictures from the Amazon forest and we should write a machine learning algorithm to identify possibles deforested areas, including farming areas.\n\nI then gather some training data, generated a histogram for every -----> image !!!  and analysed the final -----> image !!!  with a simple KNN (k=7) I wrote.\n\nI was very happy with the results I got given the little code I wrote (this project is from 2017).\n\n&amp;#x200B;\n\nLink is here: [https://github.com/victorqribeiro/deforestation](https://github.com/victorqribeiro/deforestation)", "link": "https://www.reddit.com/r/MachineLearning/comments/c49glg/p_that_time_i_used_a_simple_machine_learning/"}, {"autor": "jnbrrn", "date": "2019-06-23 16:35:06", "content": "[R] A General and Adaptive Robust Loss Function /!/ Hi /ml, I presented a paper at CVPR last week that seemed to go over well, so I thought I'd promote it beyond the vision community.\n\nVideo (much more approachable than the paper, and identical to the talk):  [https://www.youtube.com/watch?v=BmNKbnF69eY](https://www.youtube.com/watch?v=BmNKbnF69eY)\n\nAbstract: *We present a generalization of the Cauchy/Lorentzian, Geman-McClure, Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions. By introducing robustness as a continuous parameter, our loss function allows algorithms built around robust loss minimization to be generalized, which improves performance on basic vision tasks such as registration and clustering. Interpreting our loss as the negative log of a univariate density yields a general probability distribution that includes normal and Cauchy distributions as special cases. This probabilistic interpretation enables the training of neural networks in which the robustness of the loss automatically adapts itself during training, which improves performance on learning-based tasks such as generative -----> image !!!  synthesis and unsupervised monocular depth estimation, without requiring any manual parameter tuning.*\n\nArxiv:  [https://arxiv.org/abs/1701.03077](https://arxiv.org/abs/1701.03077)\n\nTensorFlow Code:  [https://github.com/google-research/google-research/tree/master/robust\\_loss](https://github.com/google-research/google-research/tree/master/robust_loss)\n\nPyTorch Code:  [https://github.com/jonbarron/robust\\_loss\\_pytorch](https://github.com/jonbarron/robust_loss_pytorch)", "link": "https://www.reddit.com/r/MachineLearning/comments/c47ceq/r_a_general_and_adaptive_robust_loss_function/"}, {"autor": "EmielBoss", "date": "2019-06-22 23:42:36", "content": "[R] How can I improve my material segmentations? UPDATED /!/ I am trying to perform material segmentation (essentially semantic segmentation with respect to materials) on street-view imagery. My datasets only has ground truth for select regions, so not all pixels have a label, and I calculate loss and metrics only within these ground truth regions. I use [Semantic FPN](https://arxiv.org/pdf/1901.02446.pdf) (with the ResNet-50 backbone pre-trained on ImageNet), a learning rate of 0.001, momentum of 0.8, and learning rate is divided by 4 if there is no validations loss improvement after three epochs. My loss function is a per-pixel multiclass cross-entropy loss.\n\nMy dataset is extremely limited. Not only are not all pixels classified, I also only have 700 images and a severe class imbalance. I tried tackling this imbalance through loss class weighting (based on the number of ground truth pixels for each respective class, i.e. their area sizes), but it barely helps. I also possess, for every -----> image !!! , a depth map, which I (can) supply as a fourth channel to the input layer.\n\n[A table of results](https://imgur.com/a/v7VFgnk)\n\n[Visualizations of images trained only on RGB](https://imgur.com/a/oiSrLeM)\n\n[Visualizations of images trained on RGBD](https://imgur.com/a/LDiTQGK)\n\n[Visualizations of images trained only on RGB, but with class loss weighting](https://imgur.com/a/6Jv6mtu)\n\n[Visualizations of images trained only RGBD, and with class loss weighting](https://imgur.com/a/oc2asRj)\n\n&amp;#x200B;\n\nPerformance is pretty crappy. What's more, there is very little difference between results of my four experiments. Why is this? I would expect that the addition of depth information (which encodes surface normals and perhaps texture information; pretty discriminitive information). Besides the overall metrics being rather low, the predictions are very messy, and the networks rarely, if ever, predicts \"small\" classes (in terms of area size), e.g. plastic or gravel. This is to be expected with such a small amount of data, but I was wondering if there are any \"performance hacks\" that can boost my network, or if I am missing any obvious stuff? Or is data likely the only bottleneck here? Any suggestions are greatly appreciated!\n\n&amp;#x200B;\n\nPS. I also tried a simple ResNet-50 FCN (I simply upsample ResNet's output until I have the same resolution; there aren't even skip connections), and the results are worse, but at least they are [smooth](https://imgur.com/a/nV9vHGl). Why are these more smooth?  \n  \nUPDATE: Last time I got the advice to use [(generalized) dice loss](https://arxiv.org/pdf/1707.03237.pdf), which is specifically designed to combat class imbalance in semantic segmentation problems. However, in my case, the [opposite happens](https://imgur.com/a/LKwKLk8). Why? I do not use the per-class weights, which in the paper is calculated as the inverse of the squared area of the class' ground truth. Even if I just use the inverse of the unsquared area, I just get a loss of 1 all the time. This is because the ratio of nominator to denominator becomes too small. I can't wrap my head around why that is. [I also posted this question more thoroughly to StackExchange](https://stats.stackexchange.com/questions/414244/why-are-weights-being-used-in-generalized-dice-loss-and-why-cant-i). I am quite at a loss at what else to do to improve my results. I thought depth of my network might be a bottleneck? I now use ResNet50, and have [trouble implementing deeper networks](https://stackoverflow.com/questions/56719817/tensorflow-keras-how-can-i-use-the-keras-applications-resnext-in-eager-executi). Any advice is greatly appreciated!", "link": "https://www.reddit.com/r/MachineLearning/comments/c3wesd/r_how_can_i_improve_my_material_segmentations/"}, {"autor": "Chromobacterium", "date": "2019-04-22 20:29:15", "content": "[Project] Computer Vision with ONNX Models /!/ Hey everyone! I just created a new runtime for Open Neural Network Exchange (ONNX) models called ONNXCV.\n\nBasically, you can inference ONNX models for realtime computer vision applications (i.e. -----> image !!!  classification and object detection) without having to write boilerplate code. It is useful in the sense that one can focus on making deep learning models using any deep learning library (that converts models into the ONNX file format), without having to sacrifice time into building the actual inferencing program.\n\nLet me know what you think and if I should continue to build upon this (or not).\n\nHere it the [code](https://github.com/ChromoBacterium/OnnxCV.git).", "link": "https://www.reddit.com/r/MachineLearning/comments/bg6yr4/project_computer_vision_with_onnx_models/"}, {"autor": "histoire_guy", "date": "2019-04-22 15:49:38", "content": "[P] I2S OCR - Image 2 Speech App /!/ Hey folks,\n\nWe are pleased to introduce the I2S OCR scanner app. I2S is a state-of-the-art OCR Scanner that practically turns almost any images with human readable characters into text content which is in turn transformed into human voice in your native language &amp; accent.\n\nOnce the -----> image !!!  data (Book page, magazine, journal, scientific paper, etc.) recognized &amp; transformed into text content, you'll be able to playback that text in your local accent &amp; over 45 languages of your choice!\n\nText output not understood? no problem, use the built-in translation service &amp; get your text translated to over 75 languages of your choice. Generate PDF on the fly, Copy to device clipboard &amp;, share your text output with friends.\n\n**Feature Set Includes**:\n\n* State of the art OCR processing algorithm powered by [PixLab](https://pixlab.io).\n* Ability to recognize the input language automatically.\n* Speaks over 45 languages &amp; their accents.\n* Translate output to over 70 languages of your choice.\n* Generate PDF, Share your output &amp; Give your feedback.\n\n**Links**:\n\n* Project homepage: https://i2s.symisc.net.\n* Playstore Link: https://play.google.com/store/apps/details?id=net.symisc.ocr.img.speech\n* Appstore app to be released soon.\n\nWe hope you enjoy using I2S and we look for your feedback if any!", "link": "https://www.reddit.com/r/MachineLearning/comments/bg3njr/p_i2s_ocr_image_2_speech_app/"}, {"autor": "ActualRealBuckshot", "date": "2019-06-06 00:36:08", "content": "-----> Image !!!  processing guidance /!/ Where would you start with learning image processing and analysis?", "link": "https://www.reddit.com/r/MachineLearning/comments/bxa6z5/image_processing_guidance/"}, {"autor": "UltraMarathonMan", "date": "2019-04-29 15:39:05", "content": "[D] Oriol Vinyals: DeepMind AlphaStar and Sequence Modeling | Artificial Intelligence Podcast /!/ Oriol Vinyals is a senior research scientist at Google DeepMind. Before that he was at Google Brain and Berkley. His research has been cited over 39,000 times. He is one of the most brilliant and impactful minds in the field of deep learning. He is behind some of the biggest papers and ideas in AI, including sequence to sequence learning, audio generation, -----> image !!!  captioning, neural machine translation, and reinforcement learning. He is a co-lead (with David Silver) of the AlphaStar project, creating an agent that defeated a top professional at the game of StarCraft. \n\n**Video:** https://www.youtube.com/watch?v=Kedt2or9xlo \n\n**Audio:** https://lexfridman.com/oriol-vinyals \n\nhttps://i.redd.it/gg0nukhi38v21.png", "link": "https://www.reddit.com/r/MachineLearning/comments/biqwfv/d_oriol_vinyals_deepmind_alphastar_and_sequence/"}, {"autor": "WebML2011", "date": "2019-05-14 04:02:16", "content": "Unusual Activity Detection for Video Surveillance /!/   \n\nIn \"Unusual Activity Detection for Video Surveillance by Rajat Singh, Sarvesh Vishwakarma, Anupam Agrawal, and M.D. Tiwari\" , the authors have laid stress on the fact that the field of security, video surveillance has been of paramount importance because of which its importance has somewhat escalated in the past few years. This growth can be accounted for when considering that it has numerous applications in the field of law enforcement, both police and military and, therefore, detection of suspicious or unusual activity has gained importance and various governmental agencies are looking for an efficient and robust application of the same. This system can be broken down further in the following processes: detection of object, tracking the movement, and monitoring of activity. The authors have tried to present a system that examines the activity performed by a human. This system does two things: detects a person who is carrying with him an object or a person who is abandoning an object and then it segments that object from the person so that it could be tracked as well. The system sends a notification to the end user when it detects an unusual activity.\n\nNow, the video is broken down into several -----> image !!!  frames and each frame is fed to the system and the algorithm detects the person as a whole as the system is not specifically aimed at capturing human parts like limbs. Now, the object segmentation is done using Gaussian mixture model. The features such as size, centre of mass and bounding box are used to maintain matching in consequent frames. Now, as the object is being tracked, Bayesian inference framework is used for the analysis of the event.\n\nHowever, this system is limited to a single camera view and the scenario considered here is that the number of people present in the environment is pretty limited whereas in real life where there can be a large crowds as well, a system which can analyse unusual activities using multiple camera views along with a robust and sophisticated algorithm which can handle complicated scenarios rather easily is needed.\n\nThe methods presented in this paper give promising results in a constrained environment and can be used as a base for future research.\n\n&amp;#x200B;\n\nReference:\n\n\"Singh, Rajat, Sarvesh Vishwakarma, Anupam Agrawal, and M. D. Tiwari. \"Unusual activity detection for video surveillance.\" In Proceedings of the First International Conference on Intelligent Interactive Technologies and Multimedia, pp. 297-305. ACM, 2010\"", "link": "https://www.reddit.com/r/MachineLearning/comments/bodv7d/unusual_activity_detection_for_video_surveillance/"}, {"autor": "MatMou", "date": "2019-09-12 07:42:32", "content": "Unsupervised Anomaly GAN /!/ Hi,\n\nI've been trying to find some information on GANs used for unsupervised learning. Specifically I'm looking for something on unsupervised anomaly detection on non------> image !!!  data similar to papers and githubs on AnoGan (which is mainly on -----> image !!!  data).\n\nDoes anybody know of a place to look into this or a way or rewriting AnoGans to use fx csv data.\n\nSo far I've been looking into this:  [https://github.com/tkwoo/anogan-keras/blob/master/anogan.py](https://github.com/tkwoo/anogan-keras/blob/master/anogan.py)", "link": "https://www.reddit.com/r/MachineLearning/comments/d34j83/unsupervised_anomaly_gan/"}, {"autor": "TactileImages", "date": "2019-09-12 01:27:26", "content": "[P] Train an AI for Visually Impaired People in 10 minutes /!/ Help us train an AI that recognizes hands. We will be able to further develop our app for Visually Impaired People (VIP) using tactile images in swell/embossed/raised paper. This way they won't need another person to explain what they are touching, the app will see their hand position and explain what they are touching. \nWhat if mom can't help with Chemistry, Mathematics, Geography homework? Use our app! A big leap towards e-learning and self-developing tool for VIPs.\n\nI made a [video](https://youtu.be/vUuLPERkx6w) to better explain the situation: what we have and what we need. Useful for distributing on social media? \n\n[Video](https://youtu.be/k-hzZ87hHu0) with what the app learned by now:\nIt only works with limited parameters and is not very precise - that's why we are asking for support to improve it. \nPlease take pictures in different lighting, from different angles, with different textures.\nVarious hand sizes and skin tones welcome! \n\n\n\nWHAT TO DO (for IOS):\nIf you have an iPhone, download the IOS app from [here](https://testflight.apple.com/join/9gVahDmZ):\n(First, install TestFlight from Apple, then install Keypoints app. No accounts needed.)\n\nShort tutorial:\n- take pictures by touching the left upper corner -----> camera !!!  button\n- after taking the pictures a folder will appear in the app. Open it and set the points as seen in the [tutorial](https://youtu.be/27DZpKCUJBM)\n- after you finish setting the markers, use the right upper corner send button to send them to developer@teistudios.ro by email or transfer (Google Drive, Dropbox, etc)\n\nKeypoints (as seen in the pictograms down the screen) are:\n1. Index finger tip\n2. Index finger base\n3. Thumb tip\n4. Little finger base\n\nOnly place keypoints on the visible hand elements. If 1, 2, 3 or 4 are not visible do not activate that keypoint(s).\nYou can always reset the points by using the curve arrow on top right. \nYou can finish setting keypoints some other time. You can also continue taking pictures anytime. Just remember to send us the pictures after setting keypoints.\nAfter sending the pictures you can delete the folder by sliding left on it. \n\n\n\nWHAT TO DO (for nonIOS):\nIf you don't have an IOS device, just take 50 pictures and send them to developer@teistudios.ro through online transfer.", "link": "https://www.reddit.com/r/MachineLearning/comments/d30kma/p_train_an_ai_for_visually_impaired_people_in_10/"}, {"autor": "MaxTalanov", "date": "2019-10-08 05:32:39", "content": "[P] Tensorflow 2.0 implementation of EDSR, WDSR and SRGAN for single -----> image !!!  super-resolution /!/ [GitHub code repository](https://github.com/krasserm/super-resolution)\n\nIt implements:\n\n1. [Enhanced Deep Residual Networks for Single Image Super-Resolution](https://arxiv.org/abs/1707.02921) (EDSR)\n2. [Wide Activation for Efficient and Accurate Image Super-Resolution](https://arxiv.org/abs/1808.08718) (WDSR)\n3. [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/abs/1609.04802) (SRGAN)", "link": "https://www.reddit.com/r/MachineLearning/comments/devpcr/p_tensorflow_20_implementation_of_edsr_wdsr_and/"}, {"autor": "ialhashim", "date": "2019-05-16 05:42:03", "content": "[P] Simple network to estimate depth using a webcam /!/ Webcam demo and full source code [https://github.com/ialhashim/DenseDepth](https://github.com/ialhashim/DenseDepth)\n\nA simple encoder-decoder network that make use of transfer learning (via a pretrained DenseNet) in order to achieve state of the art in single -----> image !!!  depth estimation (supervised setting).", "link": "https://www.reddit.com/r/MachineLearning/comments/bp8vjd/p_simple_network_to_estimate_depth_using_a_webcam/"}, {"autor": "tatabiba29", "date": "2019-10-10 17:45:56", "content": "[Research]Segway Survey /!/ Hello All! I am a current student who is working on a Final Project about Segways. The requirement is to collect at least 100 survey responses in the next couple of days. Could you please help me out? There are only 12 questions adn shoudl take you about 3-4 min to complete. Your insights are very valuable to me as I don't have any friends near me who are familiar with Segways. Thank you! The -----> image !!!  of the segway I'm researching for is in google forms.\n\n[https://docs.google.com/forms/d/e/1FAIpQLSe65iP2NtQqwgWU-QK\\_9PAvo09w5n78GUt0pqjaYKFlmtECiw/viewform?usp=sf\\_link](https://docs.google.com/forms/d/e/1FAIpQLSe65iP2NtQqwgWU-QK_9PAvo09w5n78GUt0pqjaYKFlmtECiw/viewform?usp=sf_link)", "link": "https://www.reddit.com/r/MachineLearning/comments/dg1mb0/researchsegway_survey/"}, {"autor": "planetzephyr", "date": "2019-10-10 01:56:16", "content": "How to predict the next -----> image !!!  in a sequence? /!/ I have been looking into SeqGAN as well as a variational autoencoder with the goal of generating new sequences of my data set, then making predictions. My data consists of 1000 high res photos of an oil spill time time lapse, which I have stitched into a video and stabilized. I am looking to help with predictions applied to oil spills and the like using machine learning. any and all thoughts are highly appreciated!! thank you! \ud83c\udf3c", "link": "https://www.reddit.com/r/MachineLearning/comments/dfr5xp/how_to_predict_the_next_image_in_a_sequence/"}, {"autor": "luvasugirls", "date": "2019-10-09 23:06:54", "content": "Can anyone help me turn a side profile sketch into a -----> photo !!!  realistic face using ML technology to solve a murder?", "link": "https://www.reddit.com/r/MachineLearning/comments/dfp2zy/can_anyone_help_me_turn_a_side_profile_sketch/"}], "name": "Subreddit_MachineLearning_01_01_2019-30_12_2019"}
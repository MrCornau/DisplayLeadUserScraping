{"interestingcomments": [{"autor": "hyunwoongko", "date": "2021-02-28 03:57:52", "content": "[P] Talk with AI! (Easy to use, DialoGPT) /!/ &amp;#x200B;\n\nhttps://preview.redd.it/k63pvt0a65k61.png?width=1477&amp;format=png&amp;auto=webp&amp;s=25ace6dae1e9730c6a22a23e1c9f6e94cceed8be\n\n&amp;#x200B;\n\nHello. I'm bored with nothing to do on the weekend, so I developed a package that makes it easy to access the DialoGPT-based open domain chatbot. ('pip install dialog-chat'.)\n\n&amp;#x200B;\n\nAs shown in the -----> picture !!! , three lines of code allow you to communicate with artificial intelligence and provide various options changes and automatic history management. If you want to talk to artificial intelligence, install it and use it! More information can be found here at [https://github.com/hyunwoongko/dialogpt-chat](https://github.com/hyunwoongko/dialogpt-chat)", "link": "https://www.reddit.com/r/MachineLearning/comments/lu56dq/p_talk_with_ai_easy_to_use_dialogpt/"}, {"autor": "hash_t", "date": "2021-02-27 19:33:45", "content": "[P] Node-efficientnet \u2013 -----> image !!!  recognition model written in Node.js and TypeScript", "link": "https://www.reddit.com/r/MachineLearning/comments/ltvi5t/p_nodeefficientnet_image_recognition_model/"}, {"autor": "ykilcher", "date": "2021-02-27 16:31:27", "content": "[D] Paper Explained - GLOM: How to represent part-whole hierarchies in a neural network (by Geoff Hinton, Full Video Analysis) /!/ [https://youtu.be/cllFzkvrYmE](https://youtu.be/cllFzkvrYmE)\n\nGeoffrey Hinton describes GLOM, a Computer Vision model that combines transformers, neural fields, contrastive learning, capsule networks, denoising autoencoders and RNNs. GLOM decomposes an -----> image !!!  into a parse tree of objects and their parts. However, unlike previous systems, the parse tree is constructed dynamically and differently for each input, without changing the underlying neural network. This is done by a multi-step consensus algorithm that runs over different levels of abstraction at each location of an image simultaneously. GLOM is just an idea for now but suggests a radically new approach to AI visual scene understanding.\n\n&amp;#x200B;\n\nOUTLINE:\n\n0:00 - Intro &amp; Overview\n\n3:10 - Object Recognition as Parse Trees\n\n5:40 - Capsule Networks\n\n8:00 - GLOM Architecture Overview\n\n13:10 - Top-Down and Bottom-Up communication\n\n18:30 - Emergence of Islands\n\n22:00 - Cross-Column Attention Mechanism\n\n27:10 - My Improvements for the Attention Mechanism\n\n35:25 - Some Design Decisions\n\n43:25 - Training GLOM as a Denoising Autoencoder &amp; Contrastive Learning\n\n52:20 - Coordinate Transformations &amp; Representing Uncertainty\n\n57:05 - How GLOM handles Video\n\n1:01:10 - Conclusion &amp; Comments\n\n&amp;#x200B;\n\nPaper: [https://arxiv.org/abs/2102.12627](https://arxiv.org/abs/2102.12627)", "link": "https://www.reddit.com/r/MachineLearning/comments/ltro4y/d_paper_explained_glom_how_to_represent_partwhole/"}, {"autor": "VinayUPrabhu", "date": "2021-02-27 08:38:25", "content": "[P] Visualizing evolution of Text-to------> Image !!!  generation algorithms side by side by generating video from song-lyrics (X-LXMERT v/s Alpeh-----> Image !!! /Dall-E) /!/ Youtube: [https://youtu.be/ZDeoCck2LKE](https://youtu.be/ZDeoCck2LKE)  \nContext:  \nI was trying to understand the  X-LXMERT paper when the whole Dall-E thingy exploded. So, this video was stitched together from images generated by (X-LXMERT v/s AlpehImage/Dall-E) by taking in textual cues emanating from the lyrics of a song to help visualize the advances from Sep-2020 to Feb-2021.   \n\n\nhttps://reddit.com/link/ltjva7/video/6wy87l4bfzj61/player\n\nThe criteria I had for the song were:  \n\\-  'Descriptive viusualizable lyrics'   \n\\-   Creative Commons Attribution license (reuse allowed)  \n\\-   Did well on the popularity charts :)  \n Left video: Generated using the [X-LXMERT model](https://arxiv.org/abs/2009.11278)   \n Right video: Generated using a (slightly) modified version of the Alpeh-Image notebook  built on [Dall-e](https://arxiv.org/pdf/2102.12092.pdf)  \n\n\n[Where bluebirds fly!](https://preview.redd.it/owimh63udzj61.png?width=512&amp;format=png&amp;auto=webp&amp;s=cf8ff294ef3a207fbf2a2b268e804974258c01a4)", "link": "https://www.reddit.com/r/MachineLearning/comments/ltjva7/p_visualizing_evolution_of_texttoimage_generation/"}, {"autor": "natavk", "date": "2021-04-02 03:36:19", "content": "-----> Image !!!  enhancement /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/mic1rj/image_enhancement/"}, {"autor": "AhmedAl93", "date": "2021-04-01 22:29:27", "content": "[D][R] Solutions for handwritten text generation /!/ Hello,\nThe aim of my post is to discuss the possible solutions for synthetic handwritten text generation under specific constraints.\n\nContext: Currently working as a data scientist in a french company, I encountered some issues when dealing with handwritten text recognition, mainly because the amount of available data is simply not enough (~4k images of handwritten text)\n\nThe first idea that came to my mind is to generate a large \"artificial\" handwritten text dataset by assembling single handwritten characters (check out Chars74k dataset), so I made a script that takes a string as input, assembles character -----> image !!! s and provides an -----> image !!!  containing handwritten text.\n\nThis strategy gave really promising results: I trained a text recognition model (called \"deep text recognition benchmark\") on the artificial dataset (120k images) then finetuned it on my real dataset and got 72% accuracy.\n\nHowever, even if this result is good, it can be improved mainly by generating handwritten text that is as similar to the real one as possible.\n\nI was thinking about using GAN models, but the problem is that they need what I'm lacking in the first place: big amount of data.\n\nDo you have any ideas about solving this \"dilemma\" ?\nAlso, I thought this dilemma can be seen as PhD thesis proposal, and I'm planning to discuss with my managers about it, so if you can share your views on this, it would be great !", "link": "https://www.reddit.com/r/MachineLearning/comments/mi6pxo/dr_solutions_for_handwritten_text_generation/"}, {"autor": "hallavar", "date": "2021-04-01 17:17:56", "content": "[Discussion] Any metric for evaluating non-image synthetic data ? /!/ Hello, evaluating generated data (obtained by AE, GAN etc..) is always a tricky question.\n\nFor images, most of the papers i've read used the Inception score or its evolution, the FiD.\n\nHowever,  those metrics are based on how well a generated images can be  classified by a pretrained network (this is a summary, no need for  details here)\n\nBut if I generate  data that are not -----> image !!!  related (like text, sequences, graphs). How can I  evaluate the generation of my model ?\n\nA  good metric should evaluate the plausibility of the synthetic data, ie  the the realism of the generation given the training distribution; but  also its diversity : to ensure that the training distribution is well  understood by the model.\n\nI know how to evaluate the first criterium (realism), but i have no idea how to evaluate the second\n\nIf anyone has ever worked on something like that, i will be glad to talk about his/her work.\n\nThank you in advance", "link": "https://www.reddit.com/r/MachineLearning/comments/mi04ds/discussion_any_metric_for_evaluating_nonimage/"}, {"autor": "Awesome-355", "date": "2021-04-01 16:10:28", "content": "How can i create multiple 3D meshes and textures for different objects /items present in an -----> image !!!  using deep learning ? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/mhyn94/how_can_i_create_multiple_3d_meshes_and_textures/"}, {"autor": "ARAXON-KUN", "date": "2021-04-01 16:02:08", "content": "Bert and LSTM for -----> image !!!  classification /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/mhygca/bert_and_lstm_for_image_classification/"}, {"autor": "softcompanyuk", "date": "2021-04-01 10:57:40", "content": "-----> Image !!!  Classification Using Model Builder (ML.NET Model Builder)", "link": "https://www.reddit.com/r/MachineLearning/comments/mhssgq/image_classification_using_model_builder_mlnet/"}, {"autor": "Particular_Being3678", "date": "2021-04-01 10:18:01", "content": "Interactive way to show results of a dog/cat -----> image !!!  classifier, confusion matrix /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/mhs8xu/interactive_way_to_show_results_of_a_dogcat_image/"}, {"autor": "Main-Ad-5413", "date": "2021-01-31 02:04:15", "content": "[P] Predicting prices given the price tag /!/ The other day I was at the supermarket using my phone calculator to track how much money I was spending.\n\nIt was so tedious to add every price by hand...\n\nThat's when I realize I could use Machine Learning to scan with my phone the price tag and automatically \"predict\" the price of the product and added it to my total expenditure\n\nI started taking a lot of pictures of the tags, around 500 hundred.\n\n&amp;#x200B;\n\n[Example an -----> image !!!  taken](https://preview.redd.it/u6poqi6kqke61.jpg?width=600&amp;format=pjpg&amp;auto=webp&amp;s=96a3cf1bb3ab08fb50a9e20846687a7ed9549b99)\n\nLater in my home, I decided to label each -----> image !!!  with its corresponding price.\n\nI knew that this problem was going to be a Regression with the images as inputs and my price as output.\n\nOnce I finished labeling each image... I decided it was time to create my Jupyter notebook and started importing all the necessary libraries.\n\n`import os`\n\n`import pandas as pd`\n\n`import cv2`\n\n`import numpy as np`\n\n`import tensorflow as tf`\n\n`from tensorflow.keras import layers`\n\n`from tensorflow.keras import Model`\n\n`from tensorflow.keras.preprocessing.image import ImageDataGenerator`\n\nWhen I finished with the imports I designed a function that is going to return a DataFrame\n\n* images --&gt; \"Path of the image\" ej: \"Photos/234233343552454.jpg\n* prices --&gt; \"The price of the image\" ej: 1705.14\n\ndf --&gt;that you see below is a data frame that contains the prices for each image ordered by how the images are in my directory. So the first image matches row 0 of my data frame.\n\n&amp;#x200B;\n\n|index|prices|\n|:-|:-|\n|0|1705.14|\n\n`def create_training_data():`\n\n`i = 0`\n\n`training_data = {}`\n\n`images = []`\n\n`prices = []`\n\n`for img in os.listdir(path):`\n\n`images.append(f'Photos/{img}')`\n\n\\# df.iat --&gt; gets the position where the price is on the image. it will output \"1705.14\"\n\n`prices.append(df.iat[i,1])`\n\n`i += 1`\n\n`training_data = {`\n\n`\"images\": images,`\n\n`\"price\": prices`\n\n`}`\n\n`return pd.DataFrame(data=training_data)`\n\nThen I'm going to use ImageDataGenerator class of Tensorflow that is going to make the augmentations and make my life easier:\n\n`train_datagen = ImageDataGenerator(rescale=1./255,`\n\n`rotation_range=20,`\n\n`width_shift_range=0.2,`\n\n`height_shift_range=0.2,`\n\n`shear_range=0.2,`\n\n`zoom_range=0.2)`\n\n`val_datagen = ImageDataGenerator(rescale=1./255,`\n\n`rotation_range=20,`\n\n`width_shift_range=0.2,`\n\n`height_shift_range=0.2,`\n\n`shear_range=0.2,`\n\n`zoom_range=0.2)`\n\nThen I use the \"flow\\_from\\_dataframe\" method that is going to take my dataframe with the images and labels columns\n\n`train_generator = train_datagen.flow_from_dataframe(`\n\n`x_col=\"images\",`\n\n`y_col=\"price\",`\n\n`dataframe=training_data,`\n\n`target_size=(200, 300),`\n\n`batch_size = 15,`\n\n`class_mode = \"raw\"`\n\n`)`\n\n`validation_datagen = val_datagen.flow_from_dataframe(`\n\n`x_col=\"images\",`\n\n`y_col=\"price\",`\n\n`dataframe=validation_data,`\n\n`target_size=(200, 300),`\n\n`batch_size = 16,`\n\n`class_mode = \"raw\"`\n\n`)`\n\n|images|prices|\n|:-|:-|\n|\"Photos/4243433432432.jpg\"|1705.14|\n\nNow Im ready to use Keras and train my model...\n\nI decided to use a CNN with this structure:\n\n`model = tf.keras.models.Sequential([`\n\n`tf.keras.layers.Conv2D(32, (5,5), activation='relu', input_shape=(200, 300, 3)),`\n\n`tf.keras.layers.MaxPooling2D(2,2),`\n\n`tf.keras.layers.Conv2D(64, (5,5), activation='relu'),`\n\n`tf.keras.layers.MaxPooling2D(2,2),`\n\n`tf.keras.layers.Conv2D(64, (5,5), activation='relu'),`\n\n`tf.keras.layers.MaxPooling2D(2,2),`\n\n`tf.keras.layers.Conv2D(128, (5,5), activation='relu'),`\n\n`tf.keras.layers.MaxPooling2D(4,4),`\n\n`tf.keras.layers.Flatten(),`\n\n`tf.keras.layers.Dense(512, activation='relu'),`\n\n`tf.keras.layers.Dense(1)`\n\n`])`\n\n&amp;#x200B;\n\nLets compile the created model:\n\n`model.compile(loss=\"mean_absolute_error\", optimizer=\"adam\", metrics=[\"mae\"])`\n\nLets fit the data:\n\n`history =` [`model.fit`](https://model.fit)`(`\n\n`train_generator,`\n\n`steps_per_epoch=16,`\n\n`epochs=200,`\n\n`validation_data=validation_datagen,`\n\n`validation_steps=4,`\n\n`verbose=2`\n\n`)`\n\n&amp;#x200B;\n\n[summary of the model](https://preview.redd.it/51rx6bjpqke61.png?width=531&amp;format=png&amp;auto=webp&amp;s=56a24ce1690005232b05f3ee0c71cbda41fa9951)\n\n&amp;#x200B;\n\n[model training results](https://preview.redd.it/uzbzee9uqke61.png?width=748&amp;format=png&amp;auto=webp&amp;s=f145e64397edadb79b28cc3a6098895af7f229c4)\n\nBasically, I'm not reaching any good model.   \n\n\nI also tried to use transfer learning with the \"Inception\\_v3\"\n\nUsing this code:\n\n`from tensorflow.keras.applications.inception_v3 import InceptionV3`\n\n`local_weights_file = WEIGHTS +'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'`\n\n`pre_trained_model = InceptionV3(input_shape=(200, 300, 3),`\n\n`include_top=False,`\n\n`weights=None)`\n\n`for layer in pre_trained_model,layers:`\n\n`layers.trainable = False`\n\n`last_layer = pre_trained_model.get_layer(\"mixed7\")`\n\n`last_output = last_layer.output`\n\n`x = layers.Flatten()(last_output)`\n\n`x = layers.Dense(1042, activation='relu')(x)`\n\n`x = layers.Dense(1, activation='relu')(x)`\n\n`model = Model(`\n\n`pre_trained_model.input, x`\n\n`)`\n\n&amp;#x200B;\n\n`model.compile(`\n\n`loss=\"mean_squared_error\",`\n\n`optimizer=\"adam\",`\n\n`metrics=[\"mae\"]`\n\n`)`\n\n`history =` [`model.fit`](https://model.fit)`(`\n\n`train_generator,`\n\n`steps_per_epoch=16,`\n\n`epochs=200,`\n\n`validation_data=validation_datagen,`\n\n`validation_steps=4,`\n\n`verbose=2`\n\n`)`\n\nBut the result was not so good, similar to the other one without transfer learning.\n\n&amp;#x200B;\n\nI really don't know what I'm doing wrong... maybe I need wayyyyy more data. Or maybe I need first to create a model that will crop where the price is and then pass it to this model.\n\n**I need help!**\n\nMaybe someone wants to contribute and help me. I'm trying to get better at creating models and applying them to new and interesting things.", "link": "https://www.reddit.com/r/MachineLearning/comments/l91u4l/p_predicting_prices_given_the_price_tag/"}, {"autor": "Main-Ad-5413", "date": "2021-01-31 01:46:56", "content": "Predicting the price given an -----> image !!!  of a price tag /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/l91id1/predicting_the_price_given_an_image_of_a_price_tag/"}, {"autor": "AppearanceAgile1969", "date": "2021-01-31 00:56:54", "content": "[P] Tensorflow: How to set up a TFRecord dataset with label subclasses? /!/ I just created a TF model to detect geometric shapes and alphanumeric characters in an -----> image !!! , but I also need to detect the cardinal orientation of those alphanumerics. How do I set up my dataset to classify the orientation as a subclass of characters?\n\nCurrently, I just have each shape and letter as a separate label (A, B, C, Square, Triangle, Rectangle, etc). Using Tensorflow and TFRecords, how would I add a subclass to a label?\n\nRight now, I'm using this for the shapes and letters\n\n\"'image/object/class/label': dataset\\_util.int64\\_list\\_feature(classes)\"\n\nWould I add something like this for the orientation?\n\n\"'image/object/class/label/orientation': dataset\\_util.int64\\_list\\_feature(cardinal\\_directions)\"", "link": "https://www.reddit.com/r/MachineLearning/comments/l90j37/p_tensorflow_how_to_set_up_a_tfrecord_dataset/"}, {"autor": "Okoraokora1", "date": "2021-01-30 21:21:34", "content": "[Discussion] Wasserstein vs L2 norm for denoising /!/  Hi, I am using a conditional Wasserstein GAN to denoise images. So along with the noise, I feed the noisy -----> image !!! s to the generator and train it against the clean -----> image !!!  which is my ground truth. Original idea is to learn a posterior of clean images. My question is which of the denoising is expected to work better image quality wise- the Wasserstein or L2 norm (focusing only on denoising and ignoring learning a posterior for L2 case)? If there are papers along this direction, it will be very helpful. In my implementation, I am observing that L2 images look better than Wasserstein and I am confused whether I am doing a valid comparison.", "link": "https://www.reddit.com/r/MachineLearning/comments/l8w1ju/discussion_wasserstein_vs_l2_norm_for_denoising/"}, {"autor": "dr_martensite", "date": "2021-01-30 19:12:32", "content": "[P][D] Architecture behind scalable, dynamic art recommendations /!/ For the last couple of years I\u2019ve been working on a side project to explore better wall art recommendations. The existing sites had too much unsorted content, and since I work in ML in my day job, I wanted to build something better. I\u2019ve finished an MVP which you can check out [here](https://decoart.io/) if you\u2019re interested.\n\n&amp;#x200B;\n\nIt\u2019s got a pretty typical call to action for a recommender where you answer visual questions that establish your taste, then as you browse, your recommendations are progressively refined. I wanted to make a post that talked through the ML architecture a bit and hopefully have some useful discussion. It's a little different than the content normally in this sub as it's a bit more tactical. At a high level the data architecture looks like this:\n\n&amp;#x200B;\n\n[Deco Data Architecture](https://preview.redd.it/rs4z6gvnqie61.png?width=960&amp;format=png&amp;auto=webp&amp;s=2f2c9019f7f35fc27a2b27b5ed922cccba440495)\n\n**Scraping Service:**\n\nI needed lots of art data to make this useful, so I found art sites with affiliate programs, and set up (ethical!) scrapers to consume their site-mapped content. I built scrapy spider crawlers to look through the content weekly, and find anything that hadn\u2019t been indexed in my art metadata storage. The scrapers collected the artwork images as well as any metadata, particularly search tags. The images were uploaded to Google cloud storage (behind a CDN layer), where cloud functions then resize them to optimize bandwidth when serving.\n\n**Data Persistence:**\n\nHere I used Google\u2019s datastore document storage database and Google cloud storage for flat-files. Datastore is a noSQL option which is basically a wrapper around BigTable with some convenience functions. If I was starting this now, I would swap this layer for either Mongo or even mysql, but I wanted to explore using datastore since the cloud hosting seemed so appealing. The db is very fast, and scales well, but I would have preferred some indexing and querying flexibility, like what you get with Mongo. Much of this logic has to move into the application layer, which is a bit of a drag.\n\n**Recommendation Data Prep Batch Job**\n\nHere\u2019s where the bulk of the machine learning occurs. Once I had a critical mass of art (100k works or so), I started building models to categorize, label, and index the art to make meaningful content recommendations. The -----> image !!!  embeddings were built with a simple model, inception resnet v2, using tf hub. They were dimensionally reduced with a PCA step. The tag embeddings were made by starting with a tf-idf embedding score (treating each artwork as a document), then adding in an exponential \u201cart popularity\u201d term, which added a boost to the most popular tags within each tag\u2019s tf-idf score. The tag embeddings were also dimensionally reduced with a PCA step. The tag score model and the two PCA transformers were trained once on a bulk majority of data, and serialized into GCS for incremental usage without needing all the data in the pipeline.\n\nThe image and tag embeddings were concatenated and inserted into an annoy index for nearest neighbor search. The index is saved in GCS for future live look-ups, but the neighbors are also cached in datastore after each pipeline run for better recommendation performance.\n\nFinally a mini-batch k-means algorithm is used to classify art into categorical indicators. The features are simply the concatenated embedding vectors used in the indexing step. This model was initially built in an offline batch process with 100k works. To find ideal hyper parameters, I did a grid search where each hyperparameter set output into a small web app showing 10 images at each cluster center, along with the highest scoring tag names for the cluster center artwork. This allowed me to score, and write a human-readable label (based on the tags I was seeing) for each cluster. In the business they call this \u201chuman in the loop\u201d machine learning, but let\u2019s be honest, it\u2019s tedious. There are always subjective calls being made, so the site includes the ability for users to give feedback on the algorithms categorical classification. This is gathered in the training pipeline, so future iterations can work to move artwork into classifications such that the overlap between \u201cmislabeled\u201d neighbors is minimized as part of the hyperparameter search. Once the hyperparameters were chosen, the same model was used for incremental updates.\n\nAll of the above jobs were written in a python framework for repeatable ML ops called [Primrose](https://github.com/ww-tech/primrose). The framework allows you to build re-usable nodes for operations, and extricates the configuration such that much of the work becomes writing configuration files rather than writing long pythons scripts for everything you\u2019d like to do. It makes re-training or incremental updates very simple. You can basically build one single image that references a configuration file to run any given job. There are lots of frameworks to check out if you\u2019re interested in this kind of thing: [prefect](https://www.prefect.io/), [dagster](https://github.com/dagster-io/dagster), and [kubeflow](https://www.kubeflow.org/) have similar ideas with differing levels of dev-ops required. Either way this sort of modular approach is very dope.\n\n**Recommendation Service:**\n\nAs you can see in the purple \u201cRecommendation Service\u201d box, the client-facing API takes a search engine approach to recommendations, an indexing step followed by a ranking model. In this way the system is flexible toward using any model for either step. For indexing, I started with a very simple approach, the indexing operation finds the clusters that the user has been interested in via browsing history or explicit indicators, and up-front excludes clusters that they have disliked. Datastore has a reverse index built up for each cluster so you can pull down all the metadata keys for each cluster in a few milliseconds.\n\nAfter the indexing call, the backend has a few thousand options in memory, so a ranking model is used to re-rank the options by interest before being returned to the user. The ranking uses a linear model :-), with weights that are tunable via AB tests on the backend. There are only 3 features for each item, number of exposures (for this user), a binary indicator for the user having any liked neighbors, and a binary indicator for the user having any disliked neighbors. The liked/disliked neighbor features  are 1 if the work is within the set of 20 nearest neighbors to each user\u2019s liked or disliked works.\n\nSo to recap: the number of exposures has a negative coefficient (the more you see something and don\u2019t interact, the less you\u2019ll be shown that item), a binary variable for whether it\u2019s close to something you\u2019ve liked before (a positive coefficient), and a binary variable for whether it\u2019s close to something you\u2019re disliked before (a negative coefficient). All the indexed art is then ranked in this way and returned to the client. There is also an initial random ranking which seeds any tie-breaks, so the user won\u2019t see the same recommended feed over and over when they don\u2019t interact with any content.\n\nThe above ranking implementation is primed to be swapped out with a learning to rank model after member behavior starts informing the ideal content. Without any users this is very much a cold-start problem.\n\nI\u2019d love to hear any thoughts from the sub about the current architecture! Hopefully you find this to be an interesting read.", "link": "https://www.reddit.com/r/MachineLearning/comments/l8t42n/pd_architecture_behind_scalable_dynamic_art/"}, {"autor": "VoyZan", "date": "2021-01-30 13:14:20", "content": "[N] Hi! I wanted to share this video I made about Dall-e, the recent mindblowing OpenAI -----> image !!!  generating model. I humbly like to think that this is the coolest video I made so far in my short experience of making tech YouTube videos so I hope you guys find it informative and interesting. Thanks\ud83d\udc4b", "link": "https://www.reddit.com/r/MachineLearning/comments/l8lgfo/n_hi_i_wanted_to_share_this_video_i_made_about/"}, {"autor": "LikeLion12", "date": "2021-01-30 12:45:15", "content": "[P] MBTI test with AI /!/ I did my first AI project. even though it's not worth. T\\_T\n\nI want to share this site. You can find your MBTI type easily by -----> picture !!!  with Google AI package.\n\nJust try it for fun.\n\n[https://www.facembti.com/](https://www.facembti.com/)", "link": "https://www.reddit.com/r/MachineLearning/comments/l8kysk/p_mbti_test_with_ai/"}, {"autor": "projekt_treadstone", "date": "2021-01-30 10:31:18", "content": "[D] Concept behind Facial recognition for detecting unseen faces in CNN based model /!/ How does CNN-based facial recognition handle the case when a new person's face needs to be detected in the -----> image !!! ? I mean when we train the model we have pictures of known faces but during the test, a new person's face comes, and if we have to find in which target image that new face is found. How do they do that for new person. As CNN is not trained for this new face.", "link": "https://www.reddit.com/r/MachineLearning/comments/l8ixri/d_concept_behind_facial_recognition_for_detecting/"}, {"autor": "Sagoito", "date": "2021-01-30 08:23:01", "content": "[P] -----> Image !!!  denoising project /!/ Helo guys,\n\nI am searching for good images dataset for my  image denoising project. Can you recommend any?", "link": "https://www.reddit.com/r/MachineLearning/comments/l8haeo/p_image_denoising_project/"}, {"autor": "danapo", "date": "2021-05-02 09:09:46", "content": "[N] Free Online Event \"Your Ultimate Data Annotation Experience for Deep Learning\" /!/ Dear Machine Learning Enthusiasts,\n\nSoon there is an online event at Linkedin titled\n\n\u201c[Your Ultimate Data Annotation Experience for Deep Learning](https://www.linkedin.com/events/yourultimatedataannotationexper6786631495491682304/about/)\u201d with this description:\n\nAre You Ready For Speeding Up Your Deep Learning Process?\n\n\\-It all starts with danaXa's CEO Dana Pordel\u2019s keynote, where he'll unveil the future of annotation and so much more.\n\n\\-Stephan Gould, an Amazon Scholar and professor at Australian National University, will present \"Ikea Assembly and Other Experiences in Dataset Labelling\". Professor Gould has PhD from Stanford University and worked extensively with different research projects.\n\n\\-This is our first of the series of events where we discuss the progress in data annotation. It promises to be full of exciting practical announcements\u2014and some Q/A with Experts in the field\u2014so don\u2019t miss out.\n\n\\-This is a free event that will be limited to only 100 members. Hopefully, in future, we can host thousands of members.\n\nThis event is an excellent opportunity to see what the machine learning community needs as a tool for data annotation and the future of -----> image !!! /video annotation. I suggest you sign up and, within the Q/A time, ask about the challenges you face, especially if you are dealing with massive data for deep learning.\n\nHere is the [event link](https://www.linkedin.com/events/yourultimatedataannotationexper6786631495491682304/about/).", "link": "https://www.reddit.com/r/MachineLearning/comments/n32nfu/n_free_online_event_your_ultimate_data_annotation/"}, {"autor": "ykilcher", "date": "2021-05-01 20:04:33", "content": "[D] Paper Explained - DINO: Emerging Properties in Self-Supervised Vision Transformers (Full Video Analysis) /!/ [https://youtu.be/h3ij3F3cPIk](https://youtu.be/h3ij3F3cPIk)\n\nSelf-Supervised Learning is the final frontier in Representation Learning: Getting useful features without any labels. Facebook AI's new system, DINO, combines advances in Self-Supervised Learning for Computer Vision with the new Vision Transformer (ViT) architecture and achieves impressive results without any labels. Attention maps can be directly interpreted as segmentation maps, and the obtained representations can be used for -----> image !!!  retrieval and zero-shot k-nearest neighbor classifiers (KNNs).\n\n&amp;#x200B;\n\nOUTLINE:\n\n0:00 - Intro &amp; Overview\n\n6:20 - Vision Transformers\n\n9:20 - Self-Supervised Learning for Images\n\n13:30 - Self-Distillation\n\n15:20 - Building the teacher from the student by moving average\n\n16:45 - DINO Pseudocode\n\n23:10 - Why Cross-Entropy Loss?\n\n28:20 - Experimental Results\n\n33:40 - My Hypothesis why this works\n\n38:45 - Conclusion &amp; Comments\n\n&amp;#x200B;\n\nPaper: [https://arxiv.org/abs/2104.14294](https://arxiv.org/abs/2104.14294)\n\nBlog: [https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training](https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training)\n\nCode: [https://github.com/facebookresearch/dino](https://github.com/facebookresearch/dino)", "link": "https://www.reddit.com/r/MachineLearning/comments/n2q7mt/d_paper_explained_dino_emerging_properties_in/"}, {"autor": "eatpasta_runfastah", "date": "2021-05-01 15:07:29", "content": "[R] -----> Image !!!  reconstruction /!/ Hello,\n\nDoes anybody know the state of image reconstruction using deep learning?  \n\ne.g. I have images with random black patches on it and would like to reconstruct the original images.\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/n2k5wu/r_image_reconstruction/"}, {"autor": "OnlyProggingForFun", "date": "2021-05-01 14:09:50", "content": "[R] Infinite Nature: Fly into an -----> image !!!  and explore it like a bird!", "link": "https://www.reddit.com/r/MachineLearning/comments/n2j2gd/r_infinite_nature_fly_into_an_image_and_explore/"}, {"autor": "KirillTheMunchKing", "date": "2021-05-01 13:54:01", "content": "[D] An -----> Image !!!  Is Worth 16X16 Words: Transformers For -----> Image !!!  Recognition At Scale - Vision Transformers explained! /!/ # [An Image Is Worth 16X16 Words: Transformers For Image Recognition At Scale](https://t.me/casual_gan/33)\n\nIn this paper from late 2020 the authors propose a novel architecture that successfully applies transformers to the image classification task. The model is a transformer encoder that operates on flattened image patches. By pretraining on a very large image dataset the authors are able to show great results on a number of smaller datasets after finetuning the  classifier on top of the transformer model. [More details](https://t.me/casual_gan/33).\n\n[ViT model architecture overview](https://preview.redd.it/45k93yszkiw61.png?width=1280&amp;format=png&amp;auto=webp&amp;s=70e50a28c078fa594d695d3212b509ac774fbe5e)\n\n[\\[10 minute paper explanation\\]](https://t.me/casual_gan/33) [\\[Arxiv\\]](https://arxiv.org/abs/2010.11929)", "link": "https://www.reddit.com/r/MachineLearning/comments/n2isci/d_an_image_is_worth_16x16_words_transformers_for/"}, {"autor": "timscarfe", "date": "2021-05-01 07:43:25", "content": "[D] Unadversarial Examples video with Hadi Salman (MIT lab) /!/ Performing reliably on unseen or shifting data distributions is a difficult challenge for modern vision systems, even slight corruptions or transformations of images are enough to slash the accuracy of state-of-the-art classifiers. When an adversary is allowed to modify an input -----> image !!!  directly, models can be manipulated into predicting anything even when there is no perceptible change, this is known an adversarial example. The ideal definition of an adversarial example is when humans consistently say two pictures are the same but a machine disagrees. Hadi Salman, a Ph.D student at MIT (ex-Uber and Microsoft Research) started thinking about how adversarial robustness  could be leveraged beyond security.  He realised that the phenomenon of adversarial examples could actually be turned upside down to lead to more robust models instead of breaking them. Hadi actually utilized the brittleness of neural networks to design unadversarial examples or robust objects which are objects designed specifically to be robustly recognized by neural networks.  \n\nVideo: [https://youtu.be/\\_eHRICHlg1k](https://youtu.be/_eHRICHlg1k)\n\nPod:  https://anchor.fm/machinelearningstreettalk/episodes/52---Unadversarial-Examples-Hadi-Salman--MIT-e1015k2\n\nIn the first 10 mins I give an intro covering the MIT features not bugs papers, non-robust features etc. \n\nAdversarial Examples Are Not Bugs, They Are Features\n\n[https://arxiv.org/pdf/1905.02175.pdf](https://arxiv.org/pdf/1905.02175.pdf)\n\nAdversarial Robustness as a Prior for Learned Representations\n\n[https://arxiv.org/pdf/1906.00945.pdf](https://arxiv.org/pdf/1906.00945.pdf)\n\nImage Synthesis with a Single (Robust) Classifier\n\n[https://arxiv.org/pdf/1906.09453.pdf](https://arxiv.org/pdf/1906.09453.pdf)\n\n&amp;#x200B;\n\nUnadversarial Examples: Designing Objects for Robust Vision\n\n[https://arxiv.org/pdf/2012.12235.pdf](https://arxiv.org/pdf/2012.12235.pdf)\n\n&amp;#x200B;\n\nDo Adversarially Robust ImageNet Models Transfer Better?\n\n[https://arxiv.org/pdf/2007.08489.pdf](https://arxiv.org/pdf/2007.08489.pdf)\n\n&amp;#x200B;\n\nA Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks\n\n[https://arxiv.org/pdf/1902.08722.pdf](https://arxiv.org/pdf/1902.08722.pdf)\n\n&amp;#x200B;\n\nProvably Robust Deep Learning via Adversarially Trained Smoothed Classifiers\n\n[https://arxiv.org/pdf/1906.04584.pdf](https://arxiv.org/pdf/1906.04584.pdf)\n\n&amp;#x200B;\n\nDenoised Smoothing: A Provable Defense for Pretrained Classifiers\n\n[https://arxiv.org/pdf/2003.01908.pdf](https://arxiv.org/pdf/2003.01908.pdf)\n\n&amp;#x200B;\n\nImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness\n\n[https://arxiv.org/abs/1811.12231](https://arxiv.org/abs/1811.12231)", "link": "https://www.reddit.com/r/MachineLearning/comments/n2dp61/d_unadversarial_examples_video_with_hadi_salman/"}, {"autor": "Sirisian", "date": "2021-04-30 22:05:02", "content": "[R] DINO and PAWS: Advancing the state of the art in computer vision with self-supervised Transformers /!/ https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training\n\nhttps://arxiv.org/abs/2104.14294  \nhttps://github.com/facebookresearch/dino  \nhttps://arxiv.org/abs/2104.13963  \nhttps://github.com/facebookresearch/suncet\n\nThe DINO research shows their \"model automatically learns class-specific features leading to unsupervised object segmentation.\"\n\n&gt; PAWS is a method for semi-supervised learning that builds on the principles of self-supervised distance-metric learning. PAWS pre-trains a model to minimize a consistency loss, which ensures that different views of the same unlabeled -----> image !!!  are assigned similar pseudo-labels.", "link": "https://www.reddit.com/r/MachineLearning/comments/n24lve/r_dino_and_paws_advancing_the_state_of_the_art_in/"}, {"autor": "Abhrant_", "date": "2021-04-30 21:52:42", "content": "[P] [D] Deep Metric Learning for face Verification /!/ I am doing some work on face verification systems. I am trying to understand the concept of deep metric learning. I have come accross some losses like contrastive loss, triplet loss, angular softmax, etc. I am confused with how to use them.\n\nI am currently using contrastive loss and triplet loss but I am not getting good results. How to prepare a proper dataset for training a network with these losses? I am required to have only two images per person in training set.\n\nIf I am supposed to use Siamese networks, then how can I use losses like angulr softmax or softmax loss? Can anyone explain this to me? I am unable to understand this.\n\nFor losses like Angular softmax or softmax loss, shall I be taking -----> Image !!!  pairs like we do incase of contrastive loss ? If not then how do I implement this?\n\nPlease help, I am unable to understand this.", "link": "https://www.reddit.com/r/MachineLearning/comments/n24d18/p_d_deep_metric_learning_for_face_verification/"}, {"autor": "NominalNom", "date": "2021-04-30 19:05:26", "content": "[D] A recent history of my pointless stare down with Nvidia which I lost /!/ Just a quick, comical summary of my experience trying to get a GPU since just prior to the launch of the Ampere cards.\n\nI work in -----> film !!!  post production and I have no prior deep learning coding experience, but I wanted a GPU with a lot of memory to process high res imagery with an existing TensorFlow-based open source toolset that I could just about figure out how to use with my small amount of Python experience. I did secure a Titan RTX off Amazon(!) at retail price, but I decided to return it when I found out about the imminent release of the Ampere cards because $1500 sounded a lot better than $2500.\n\nOf course, the 3090 is going for around the same price as the 2.5k rrp I paid for the RTX Titan. I was very naive at that exact moment about the fact that crypto mining was going to go back into full swing, although I was warned by someone on this very sub. I did expect Eth proof of stake to cancel some of that out, but we aren't there yet.\n\nBesides availability issues, there was also the size of the 3090 which Nvidia seem to have intentionaly made huge so it can't fit in a typical deep learning rig or graphics workstation. No problem, I'll get a third party blower card - if I can find one in stock. Whoops, now Nvidia killed them all!\n\nAlright, now I'll look at one of those rinky-dink third party RGB gamer cards that are sufficiently low profile - I only need one card with 24GB VRAM. Okay, they are all sold out forever and if you can get one, it may be as expensive as the previous Titan.\n\nSo maybe it's best to get an A5000 or A6000, because tbh it seems they are more likely to go for RRP which makes them not a huge step up from the inflated 3090 prices and they are better supported on Linux IMO.\n\nI have just found out about the LHR cards though, that will be shipping soon. But also that there may not be a 3090 LHR, so it doesn't seem like supply will necessarily improve.\n\nThere is now a proprietary GUI-based toolset by the same developers which is now using PyTorch as the back end. But it's still recommended that 24GB is where you really want to be at memory-wise without needing to slice up the input images too much, so this issue is not going anywhere. I had managed to get a 2060 Super with 8GB as an interim card which is really the bare minimum. It's also comical how much those cards are going for now, so I'm glad I snagged one of the last ones at Best Buy for $400 RRP last year.", "link": "https://www.reddit.com/r/MachineLearning/comments/n20yma/d_a_recent_history_of_my_pointless_stare_down/"}, {"autor": "thunder_jaxx", "date": "2021-06-01 20:57:07", "content": "[D] Unreal Engine Trick with VQGAN + CLIP /!/ &gt;When you generate -----> image !!! s with VQGAN + CLIP, the -----> image !!!  quality dramatically improves if you add \"unreal engine\" to your prompt. \n\nMore context on this Twitter thread: https://twitter.com/arankomatsuzaki/status/1399471244760649729?s=20\n\nThis is so cool and funny. The network as a byproduct learned *upsampling* based on language conditioning. LOL. I may be wrong about my stated implication but the results I am seeing are quite fascinating.", "link": "https://www.reddit.com/r/MachineLearning/comments/nq4es7/d_unreal_engine_trick_with_vqgan_clip/"}, {"autor": "businessmaster1222", "date": "2021-06-01 18:43:18", "content": "Convolutional neural network for cell images [D] /!/ Hi! I'm not sure if this is the right subreddit for this, but I wanted some advice on using a convolutional neural network. I'm doing a project where I want to detect and count cells in clusters within an -----> image !!! , and was wondering the best way to do it in terms of frameworks or libraries. Are there any examples online that do something similar? Thanks for any advice!", "link": "https://www.reddit.com/r/MachineLearning/comments/nq19h0/convolutional_neural_network_for_cell_images_d/"}, {"autor": "businessmaster1222", "date": "2021-06-01 18:42:43", "content": "Convolutional neural network for cell images /!/ Hi! I'm not sure if this is the right subreddit for this, but I wanted some advice on using a convolutional neural network. I'm doing a project where I want to detect and count cells in clusters within an -----> image !!! , and was wondering the best way to do it in terms of frameworks or libraries. Are there any examples online that do something similar? Thanks for any advice!", "link": "https://www.reddit.com/r/MachineLearning/comments/nq18zg/convolutional_neural_network_for_cell_images/"}, {"autor": "SQL_beginner", "date": "2021-06-01 16:28:05", "content": "[D] Hypothesizing the Ideal Conditions for Neural Networks vs Random Forests /!/ Is it possible to speculate what are the ideal conditions required for a neural network to perform well compared to a random forest? \n\nFor instance, when dealing with -----> image !!!  recognition tasks, we know that Convolution Neural Networks are generally favorable, seeing as how the \"convolution operation\" is very effective at \"understanding -----> image !!! s\" (e.g. recognizing edges). \n\nNow suppose we look at a standard \"binary classification task\". Suppose we have a smaller sized dataset (e.g. 15 columns, 5000 rows). \n\nIn general terms, we know that a neural network works by approximating small regions of the target function with a collection of \"mini functions\". This is done by calculating a set of weights : in the future, data is passed through this network of weights, and these weights are used to calculate the probability that a new observation belongs to a certain class is calculated. In theory, we could repeatedly pass similar points through the neural network and monitor how the probability of belonging to a certain class incrementally changes.\n\nA random forest is quite different. Look at the decision tree for a second - the decision tree works by randomly making binary partitions in the data. A binary partition is made for a predictor variable, such that this partition tries its best to cleanly separate the classes of the response variable. When a suitable partition is made for the first predictor variable, we move on to the second variable, then the third variable, etc. So in the end, if we imagine our data as a \"big box\", we create these \"mini boxes\" (i.e. terminal nodes) within the \"big box\" : each of these \"mini boxes\" has an \"address\" (i.e. the different partitions, e.g. if var1&gt;5 and var2&lt;10 then \"mini box 1\"). Each of these mini boxes is associated with a response label.\n\nThe random forest improves the decision tree by bootstrap aggregation: thousands of randomized and smaller decision trees are combined together for improved predictive power and less overfitting. All the trees in the forest are used to collectively decide which \"mini box\" a new observation should be placed in.\n\nMy question: based on this very general understanding on how both these algorithms work - can we try to hypothesize what kind of datasets are more suited for neural networks vs random forests? For example, in the random forest algorithm, by using the gini index criteria, it is relatively straightforward to make \"mini boxes\" for categorial predictor variables. However, a neural network would have to one-hot-encode these categorical predictor variables and as a result, deal with more variables (curse of dimensionality) and as well, these one-hot-encoded variables are likely to contain a greater level of sparsity. Furthermore, it might be easier to make general \"mini boxes\" in sparse data compared to using gradient descent with missing values?\n\nI know this is all speculation (and the \"no free lunch theorems\" say that no machine learning algorithm is universally best) - but could we try to speculate and say that certain machine learning algorithms might be better suited for certain types of datasets? Just as Convolution Neural Networks are better for image recognition and LSTM Networks are better at handling sequential data - could we argue that bagging and boosting algorithms (e.g. random forest, gradient boosting) might have an easier time at handling smaller datasets with mixed categorical-continuous variables? \n\nI would be interested in hearing some opinions and thoughts on this.\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/npy08z/d_hypothesizing_the_ideal_conditions_for_neural/"}, {"autor": "alexparinov", "date": "2021-06-01 15:13:45", "content": "[P] Albumentations 1.0 is released (a Python library for -----> image !!!  augmentation) /!/ Image augmentation is used in deep learning and computer vision tasks to increase the quality of trained models. The purpose of image augmentation is to create new training samples from the existing data.\n\n&amp;#x200B;\n\n[Examples of new augmentations in Albumentations 1.0](https://preview.redd.it/rzmea4ef1o271.jpg?width=1431&amp;format=pjpg&amp;auto=webp&amp;s=86e2b47ef5b112b14cbed120292ea5c569b64e9c)\n\n# Release highlights:\n\nAlbumentations no longer uses the [imgaug](https://github.com/aleju/imgaug) library by default. All previous imgaug augmentations in the library are reimplemented in Albumentations with the same API (but you can still install Albumentations with imgaug if you need the old augmentations).\n\nNew augmentations:\n\n* [SafeRotate](https://albumentations.ai/docs/api_reference/augmentations/geometric/rotate/#albumentations.augmentations.geometric.rotate.SafeRotate). Safely Rotate Images Without Cropping.\n* [SomeOf](https://albumentations.ai/docs/api_reference/core/composition/#albumentations.core.composition.SomeOf) transform that applies N augmentations from a list. Generalizing of [OneOf](https://albumentations.ai/docs/api_reference/core/composition/#albumentations.core.composition.OneOf).\n* [RandomToneCurve](https://albumentations.ai/docs/api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomToneCurve). See a [notebook](https://nbviewer.jupyter.org/github/aaroswings/RandomToneCurveTests/blob/main/RandomToneCurveTests.ipynb) for examples of this augmentation.\n\nOther changes: bug fixes, improved serialization that is fully backward compatible, \"ToTensor\" is fully removed in favor of \"ToTensorV2\", logic in setup.py that detects existing installations of OpenCV now also looks for \"opencv-contrib-python\" and \"opencv-contrib-python-headless\".\n\n# More info about the library\n\nTo install Albumentations from PyPI run `pip install -U albumentations`\n\nFull release notes: [https://github.com/albumentations-team/albumentations/releases/tag/1.0.0](https://github.com/albumentations-team/albumentations/releases/tag/1.0.0)\n\nDocumentation is available at [https://albumentations.ai/docs/](https://albumentations.ai/docs/)\n\nTry the online demo at [https://albumentations-demo.herokuapp.com/](https://albumentations-demo.herokuapp.com/)", "link": "https://www.reddit.com/r/MachineLearning/comments/npwafu/p_albumentations_10_is_released_a_python_library/"}, {"autor": "Zestyclose-Tonight-2", "date": "2021-07-01 20:19:20", "content": "Boardgame layout from -----> photo !!!  [Project][Discussion] /!/ I am new to object detection and machine learning in general. I wanna learn more and I have an idea about an app that I would like to make to learn and explore more. But now I'm starting to think that it's a bit too much. I have experience in WEB development, PHP mainly.\n\nMy reduced goal is to take a photo of the board game mat with a phone and get info about the layout of the tiles and ideally the number on the tile. The boardgame is Catan and the picture looks something like this -\n\n&amp;#x200B;\n\nhttps://preview.redd.it/r0dbf4mdtn871.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;s=395d0f7ef2d20cc2b8aa238826af9541dc6ca8f4\n\nAs you can see, there are 6 different tiles and numbers on them. Steps that I understand.\n\n1. Take a lot of pictures with different layouts\n2. Label the tiles in pictures\n\nThis is where I'm stuck for now.\n\nWhat's the best way to do this?\n\nShould I draw rectangles or polygons? What's the difference?\n\nSo far have tried makesense and VoTT for a little bit. But this seems very time-consuming. I understand that VoTT should learn as I label, but this did not work for me. I only tried 2-3 boards though. As I always have expected board, is there a way to automatically take the hexagonal grid and then transform and skew it manually? And then add labels inside the hexes? Any advice appreciated.\n\n3) Convert the labels to trained models?\n\nSo when I have my labeled images I need to transfer them to TensorFlow models? I believe this is well documented but I have not yet get there.\n\n4) Use the trained models in the mobile app.\n\nUse Tensorflow lite with Android Kotlin or Flutter. It will recognize the tiles, but how to get the layout in some usable format? Array? So I could use it further.\n\n&amp;#x200B;\n\nAny help, advice, tutorials, guides appreciated. I have done some tutorials with basic object recognition using already created public models. But like I said, I like to learn while creating something that I like and find useful.", "link": "https://www.reddit.com/r/MachineLearning/comments/obu9wv/boardgame_layout_from_photo_projectdiscussion/"}, {"autor": "TheCockatoo", "date": "2021-07-01 16:19:25", "content": "[D] Has deep-learning-based -----> image !!!  compression been solved? /!/ If I'm not mistaken, autoencoders became the de facto approach a few years ago. Have there been any fundamental developments since then?", "link": "https://www.reddit.com/r/MachineLearning/comments/obp8x8/d_has_deeplearningbased_image_compression_been/"}, {"autor": "TheCockatoo", "date": "2021-07-01 16:18:01", "content": "[D] Has deep-learning-based -----> image !!!  compression been solved? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/obp7va/d_has_deeplearningbased_image_compression_been/"}, {"autor": "KirillTheMunchKing", "date": "2021-07-01 15:48:24", "content": "[D] New SOTA StyleGAN2 inversion paper explained in 5 minutes: Pivotal Tuning for Latent-based Editing of Real Images (PTI) by Daniel Roich et al. /!/ Recently multiple new StyleGAN2 inversion techniques were proposed, however, they all suffer from the inherent editability/reconstruction tradeoff meaning that reconstructions with perfect identity preservations fall outside of the generator's well-defined latent space which hinders editing. On the other hand, reconstructions that are well suited for edits tend to have a significant identity gap with the person on the target -----> photo !!! . Daniel Roich and his colleagues from Tel Aviv University propose a simple yet effective two-step solution: first, fit a vector that reconstructs the image well, and then use it as a pivot to fine-tune the generator so that it reconstructs the input image almost perfectly while retaining all of the editing capabilities of the original latent space.\n\nRead the [full paper digest](https://t.me/casual_gan/60) (reading time \\~5 minutes) to learn about how to obtain the pivot latent code, how to correctly fine-tune the generator to have a near-perfect reconstruction of the input image, and most importantly, how to regularize the fine-tuning process in a way that keeps the editing properties of the generator's latent space intact.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://t.me/casual_gan)!\n\n[Pivotal Tuning Inversion](https://preview.redd.it/il2jiwl4hm871.png?width=701&amp;format=png&amp;auto=webp&amp;s=b0244a0719cebefff65d01b82cc62a4139fd376c)\n\n\\[[Full Explanation Post](https://t.me/casual_gan/58)\\] \\[[Arxiv](https://arxiv.org/pdf/2101.04061.pdf)\\] \\[[Code](https://nvlabs.github.io/alias-free-gan/)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n&gt;\\[[Alias-free GAN](https://t.me/casual_gan/58)\\]  \n&gt;  \n&gt;\\[[GFPGAN](https://t.me/casual_gan/54)\\]  \n&gt;  \n&gt;\\[[GANs N' Roses](https://t.me/casual_gan/53)\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/obolgm/d_new_sota_stylegan2_inversion_paper_explained_in/"}, {"autor": "sarmientoj24", "date": "2021-07-01 08:28:41", "content": "[D] Are there Object Detection papers specializing on Textures, Uniform Background, etc. for Pavement Fault Detection? /!/ I am trying to do experiments involving Object Detection for Road Damage Detection (potholes, cracks, manholes, etc.) similar to this Global Road Damage Detection Challenge 2020 [https://rdd2020.sekilab.global/](https://rdd2020.sekilab.global/) (RDDC2020)\n\nAlso, this seems to be very similar to Surface Inspection Object Detection. I am referring it as ***\"uniform\"*** since compared to typical Object Detection where the backgrounds are different, the objects are ***basically embedded to the background as road defects.***\n\nCurrently, my experiments are lined up as pretty basic:\n\nObject Detection\n\n* Region-based Proposal: Faster-RCNN \n* Regression-based Anchor-based method: YOLOv3\n* Transformer-based Object Detection: DETR\n* EfficientDet (highest scoring in PASCALVOC)\n* Anchor-Free methods: FCOS or GFocal\n\nbut it seems as though I could increase the robustness, accuracy of the results if there are \n\n1. ***Pre-processing methods that specialize on bringing out textures.*** I plan to convert the RGB to 3 channels of (1) CLAHE grayscale, (2) Gabor Fractal #1, (3) Gabor Fractal #2 (or Wavelet Transforms?). ***Are there better pre-processing methods for this?***\n2. ***Auto-Augmentation for Object Detection.*** I saw Scale-Aware Automatic Augmentation etc for Object Detection. Since roads are usually **uniform in texture,** it makes more sense that to augment a car in a background (forexample). ***But is this a good way to go as well?***\n3. ***Object Detection Methods that work well with uniform \"background\", textures, and \"defect-like\" objects in an -----> image !!! .***\n\nAs part of my work, we are collecting standardized images **via mounting the camera in the vehicle face-down to the road.** Hence, it shall be **more standardized, and have less \"noise/distractions\" in the image than the RDDC2020 dataset.**\n\n***tldr; I am looking for papers, techniques, etc. that would be more suited to this kind of problem.***", "link": "https://www.reddit.com/r/MachineLearning/comments/obhcv5/d_are_there_object_detection_papers_specializing/"}, {"autor": "xternalz", "date": "2021-08-01 03:44:39", "content": "[R] Open-World Entity Segmentation (dense -----> image !!!  segmentation without labels) /!/ &gt;**Open-World Entity Segmentation**   \n  \n&gt;  \n&gt;We introduce a new -----> image !!!  segmentation task, termed Entity Segmentation (ES) with the aim to segment all visual entities in an -----> image !!!  without considering semantic category labels. It has many practical applications in image manipulation/editing where the segmentation mask quality is typically crucial but category labels are less important. In this setting, all semantically-meaningful segments are equally treated as categoryless entities and there is no thing-stuff distinction. Based on our unified entity representation, we propose a center-based entity segmentation framework with two novel modules to improve mask quality. Experimentally, both our new task and framework demonstrate superior advantages as against existing work. In particular, ES enables the following: (1) merging multiple datasets to form a large training set without the need to resolve label conflicts; (2) any model trained on one dataset can generalize exceptionally well to other datasets with unseen domains. Our code is made publicly available at [https://github.com/dvlab-research/Entity](https://github.com/dvlab-research/Entity).  \n  \n&gt;  \n&gt;Project page: [http://luqi.info/Entity\\_Web/](http://luqi.info/Entity_Web/)", "link": "https://www.reddit.com/r/MachineLearning/comments/ovl2em/r_openworld_entity_segmentation_dense_image/"}, {"autor": "woosbert", "date": "2021-08-01 03:41:35", "content": "[R] Open-World Entity Segmentation (dense -----> image !!!  segmentation without labels)", "link": "https://www.reddit.com/r/MachineLearning/comments/ovl0vz/r_openworld_entity_segmentation_dense_image/"}, {"autor": "woosbert", "date": "2021-08-01 03:35:08", "content": "[R] Open-World Entity Segmentation (Better dense -----> image !!!  segmentation without labels)", "link": "https://www.reddit.com/r/MachineLearning/comments/ovkxpm/r_openworld_entity_segmentation_better_dense/"}, {"autor": "jnhwkim", "date": "2021-07-31 12:02:12", "content": "[R] Introduction to Fast Dense Feature Extraction -- A fast way to extract visual features for many patches from an -----> image !!!  /!/ You may extract visual features from a patch using CNNs. To extract all features of patches in a sliding window of an image, you can put the whole image as an input to the CNNs, unless a layer in the CNNs has no more than one striding.\n\nThe fast dense feature extraction technique is introduced for this scenario (Bailer et al., 2017). Also, it is shown that this technique is useful for unsupervised anomaly detection tasks while analyzing candidate regions in manufacturing products. \n\nThis video ([https://youtu.be/HnTKouSYp78](https://youtu.be/HnTKouSYp78)) from SotAdemia explains:\n\n1) the core concept of the fast dense feature extraction,   \n2) example codes to describe how it works, and  \n3) my speculations on this technique.\n\nEnjoy the flow of tensors along the dimensions!\n\nPapers:  \n\\* Bailer et al., (2017). Fast Dense Feature Extraction with CNNs that have Pooling or Striding Layers. In British Machine Vision Conference (BMVC). [https://arxiv.org/abs/1805.03096](https://arxiv.org/abs/1805.03096)  \n\\* Bergmann et al. (2020). Uninformed Students: Student-Teacher Anomaly Detection with Discriminative Latent Embeddings. In IEEE Computer Vision and Pattern Recognition (CVPR). [https://arxiv.org/abs/1911.02357](https://arxiv.org/abs/1911.02357)\n\nCodes:  \n\\* [https://github.com/erezposner/Fast\\_Dense\\_Feature\\_Extraction](https://github.com/erezposner/Fast_Dense_Feature_Extraction)  \n\\* [https://github.com/jnhwkim/orthoad/blob/main/model.py#L53](https://github.com/jnhwkim/orthoad/blob/main/model.py#L53)", "link": "https://www.reddit.com/r/MachineLearning/comments/ov5x5m/r_introduction_to_fast_dense_feature_extraction_a/"}, {"autor": "blueest", "date": "2021-07-31 05:30:47", "content": "[D] Has anyone seen this -----> picture !!!  before? /!/ I came across this picture: https://en.wikipedia.org/wiki/Vitali_covering_lemma#/media/File:Vitali_covering_lemma.svg\n\nThis picture is supposed to be a visualization of \"Vitali Covering Lemma\", e.g. https://en.wikipedia.org/wiki/Vitali_covering_lemma\n\nThis picture looks really beautiful, and I would love to understand what is going on in this picture - what does it represent? But the mathematics from the wikipedia page are too complicated for me to understand.\n\nCan someone please help me understand what this picture means and what exactly does the \"covering lemma\" mean, i.e. why is this lemma important?\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/ov1jee/d_has_anyone_seen_this_picture_before/"}, {"autor": "JustSayNoToSummer", "date": "2021-07-30 18:21:55", "content": "[N] Universal Domain Adaptation Challenge (VisDA-21) at NeurIPS'21 /!/ Interested in Universal Domain Adaptation? Join the VisDA 2021 Neurips competition!\n\nThis challenge will test how well models can (1) adapt to several distribution shifts and  (2) detect unknown unknowns.\n\nTop 3 teams will win $$$$ (2$k 1st/ $500 2nd and 3rd) in the form of VISA gift cards.\n\nMore details (and registration) at the [website](http://ai.bu.edu/visda-2021/) and below\n\n\\-----------------------------------------------------------------------------\n\nProgress  in machine learning is typically measured by training and  testing a  model on the same distribution of data, i.e., the same domain.  However,  in real-world applications, models often encounter  out-of-distribution  data, such as novel -----> camera !!!  viewpoints, backgrounds  or image quality.  The Visual Domain Adaptation (VisDA) challenge tests  computer vision  models\u2019 ability to generalize and adapt to novel target  distributions  by measuring accuracy on out-of-distribution data.\n\nThe 2021 VisDA competition is our 5th time holding the challenge! [\\[2017\\]](http://ai.bu.edu/visda-2017/), [\\[2018\\]](http://ai.bu.edu/visda-2018/), [\\[2019\\]](http://ai.bu.edu/visda-2019/), [\\[2020\\]](http://ai.bu.edu/visda-2020/).   This year, we invite methods that can adapt to novel test  distributions  in an open-world setting. Teams will be given labeled  source data from  ImageNet and unlabeled target data from a different  target distribution.  In addition to input distribution shift, the  target data may also have  missing and/or novel classes as in the  Universal Domain Adaptation  (UniDA) setting [\\[1\\]](https://arxiv.org/abs/2104.03344).   Successful approaches will improve classification accuracy of known   categories while learning to deal with missing and/or unknown   categories.", "link": "https://www.reddit.com/r/MachineLearning/comments/ouq4wf/n_universal_domain_adaptation_challenge_visda21/"}, {"autor": "SAbdusSamad", "date": "2021-07-30 11:34:20", "content": "[D] Can I train a tranaformer for -----> image !!!  classification on Google colab?? /!/ Can I train a tranaformer for image classification on Google colab??\nWhich dataset should I select?\nWhich transformer can I train on the dataset from scratch on colab?", "link": "https://www.reddit.com/r/MachineLearning/comments/ouivoj/d_can_i_train_a_tranaformer_for_image/"}, {"autor": "svantana", "date": "2021-07-30 10:58:54", "content": "[D] Sudden drop in loss after hours of no improvement - is this a thing? /!/ &amp;#x200B;\n\nhttps://preview.redd.it/4eau4uzwybe71.png?width=1202&amp;format=png&amp;auto=webp&amp;s=c7fdf89f4793969a9eafe97edb9178b0d5792dd2\n\n[https://twitter.com/NaxAlpha/status/1420700413125447683](https://twitter.com/NaxAlpha/status/1420700413125447683)\n\nThis -----> image !!!  is making me question everything. I hope they explore further what's going on here. Anyone else with similar experience?", "link": "https://www.reddit.com/r/MachineLearning/comments/ouiegi/d_sudden_drop_in_loss_after_hours_of_no/"}, {"autor": "techsucker", "date": "2021-08-31 23:36:52", "content": "[R] Google AI Introduces \u2018Omnimattes\u2019, A New Computer Vision Research Approach to Matte Generation using Layered Neural Rendering /!/ In -----> image !!!  and video editing, accurate mattes are important to separate foreground from background. However, real-world scenes often have shadows or smoke that may affect the processing of such images, but computer vision techniques generally ignore these scene effects.\n\nGoogle researchers\u00a0[presented their new approach to matte generation at CVPR 2021](https://arxiv.org/pdf/2105.06993.pdf). Their method separates video into layers called omnimattes and extracts details such as shadows associated with the subjects in a scene. A state-of-the-art segmentation model can extract masks for people, but not all of the effects related to them, like shadows on the ground. The proposed method can isolate and extract additional details associated with the subjects, such as shadows cast on the ground.\n\n# [5 Min Read](https://www.marktechpost.com/2021/08/31/google-ai-introduces-omnimattes-a-new-computer-vision-research-approach-to-matte-generation-using-layered-neural-rendering/) | [Paper](https://arxiv.org/pdf/2105.06993.pdf)| [Google Blog](https://ai.googleblog.com/2021/08/introducing-omnimattes-new-approach-to.html)\n\n&amp;#x200B;\n\n*Processing video us2wldjd4sk71...*", "link": "https://www.reddit.com/r/MachineLearning/comments/pfhnmj/r_google_ai_introduces_omnimattes_a_new_computer/"}, {"autor": "black_apple07", "date": "2021-08-31 23:28:55", "content": "[D] The loss and training from anchor boxes /!/ I have made a previous thread, which peoples responses were awesome and helped me learn a lot.\n\nI just want to create a clean post to explain my new issue.\n\nWhen we first begin training, we have anchor boxes as prior that are of different aspect and sizes. Therefore in a 13x13 -----> image !!! , each grid cell on the original -----> image !!!  will have 3 anchor boxes. And the same for the other resolutions.\n\nMy question lies in ho the output of the YOLOV3 looks like. I.e. say that an anchor box contains an object, and the class probability is a car. We have offsets for the width, height and center coordinates. Does the output of this get compared to the ground truth box for the car? I.e. say the IOU is 60%, does this loss function for that anchor box move the anchor box coordinates closer to that of the grounds truth box?   \n\n\ne.g. say we have a 400x400 image and that ground truth w,h is 10,10 and the center is (200,200). If the anchor box we have is width 8,height 8, and the center is (180,180), how does the process go for first increasing the size of the box, and second shifting it to 200,200 or at least closer during training.\n\n&amp;#x200B;\n\nThanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/pfhiq0/d_the_loss_and_training_from_anchor_boxes/"}, {"autor": "bendee983", "date": "2021-08-31 18:55:16", "content": "[R] Unsupervised detection of adversarial attacks /!/ A new technique developed by researchers at Carnegie Mellon University and the KAIST Cybersecurity Research Center and presented at the Adversarial Machine Learning Workshop (AdvML) of the ACM Conference on Knowledge Discovery and Data Mining (KDD 2021) detects adversarial examples without being trained on a specific adversarial attacks.\n\nThe basic premise of the paper is that when images go through adversarial perturbations, their explanations change. So, if you generate a saliency map of an adversarial example, it will be very different from that of a benign example.\n\nThe technique uses three components:\n\n\\- An inspector network first generates saliency maps for the target network's training examples\n\n\\- The inspector network uses the saliency maps to train \"reconstructor networks\" (one per class). Reconstructors are autoencoder networks that take an -----> image !!!  as input and generate the explanation map for the specific class.\n\n\\- The inspector network uses the reconstructors to detect adversarial examples based on their deviation from normal explanations.\n\nThe benefit of this technique is that it doesn't need to be adjusted to specific adversarial attacks since all kinds of adversarial examples result in weird saliency maps.\n\nRead full paper here:\n\n[https://arxiv.org/abs/2107.10480](https://arxiv.org/abs/2107.10480)\n\nRead analysis + comments from lead author:\n\n[https://bdtechtalks.com/2021/08/30/unsupervised-learning-adversarial-attacks-detection/](https://bdtechtalks.com/2021/08/30/unsupervised-learning-adversarial-attacks-detection/)", "link": "https://www.reddit.com/r/MachineLearning/comments/pfcapf/r_unsupervised_detection_of_adversarial_attacks/"}, {"autor": "KirillTheMunchKing", "date": "2021-08-31 14:27:04", "content": "[D] Paper explained - DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras by Zachary Teed and Jia Deng et al. 5-minute summary /!/ The idea of recording a short video and creating a full-fledged 3D scene from it always seemed like magic to me. And now it seems that thanks to the efforts of Zachary Teed and Jia Deng this magic is closer to reality  than ever. They propose a DL-based SLAM algorithm that uses recurrent updates and a Dense Bundle Adjustment layer to recover -----> camera !!!  poses and pixel-wise depth from a short video (monocular, stereo or RGB-D). The new approach achieves large improvements over previous work (reduces the error 60-80% compared to the previous best error, and destroys the competition on a bunch of other benchmarks as well).\n\nRead the 5-minute summary ([channel](https://t.me/casual_gan/91) / [blog](https://www.casualganpapers.com/3d-visual-slam-structure-from-motion/DROID-SLAM-explained.html))   to learn about Input Representation, Feature Extraction and correlation, Update Operator, Dense Bundle Adjustment Layer, Training,   and Inference.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://www.casualganpapers.com/)!\n\n[DROID-SLAM](https://preview.redd.it/lh6x4fx9epk71.png?width=1855&amp;format=png&amp;auto=webp&amp;s=51158b4efe88c9aa78b6467ccafe2fb6421b5292)\n\n\\[Full Summary: [Channel](https://t.me/casual_gan/91) / [Blog Post](https://www.casualganpapers.com/3d-visual-slam-structure-from-motion/DROID-SLAM-explained.html)\\] \\[[Arxiv](https://arxiv.org/pdf/2108.10869.pdf)\\] \\[[Code](https://github.com/princeton-vl/DROID-SLAM)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n&gt;\\[[Neural Body](https://www.casualganpapers.com/implicit-neural-representation-full-body-avatar-novel-view-synthesis/Neural-Body-explained.html)\\]  \n&gt;  \n&gt;\\[[StyleGAN-NADA](https://t.me/casual_gan/83)\\]  \n&gt;  \n&gt;\\[[FLAME-in-NeRF](https://www.casualganpapers.com/3d-aware-novel-view-head-neural-rendering-synthesis/Flame-in-NeRF-explained.html)\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/pf6xv9/d_paper_explained_droidslam_deep_visual_slam_for/"}, {"autor": "neltherion", "date": "2021-08-31 14:13:30", "content": "[R] a Metric for finding the best StyleGAN Latent Encoders /!/ Is there a metric that can be used to find out which **Real------> Image !!! -to-StyleGAN-Latent** encoders are the best ones and how do they compete against eachother?\n\nRight now we have encoders like [pSp](https://github.com/eladrich/pixel2style2pixel) and [restyle](https://github.com/yuval-alaluf/restyle-encoder) or [encoder4editing](https://github.com/omertov/encoder4editing), but how can we tell which one performs better than the other?", "link": "https://www.reddit.com/r/MachineLearning/comments/pf6oma/r_a_metric_for_finding_the_best_stylegan_latent/"}, {"autor": "black_apple07", "date": "2021-08-31 09:47:43", "content": "[D] Bounding Boxes /!/ I'm looking into the architecture of YOLOV3 and I have grasped the concept of the downsampling, residual blocks, skips etc.\n\nBut I'm a little fuzzy on the way the bounding boxes are done. So an -----> image !!!  is predicted at 3 different resolutions. say that the first predicted resolution is 13x13. This is useful for identifying large objects. Therefore, does each cell so (0,0) to (12,12) starting from top left, have 3 bounding boxes?\n\nI have some questions on these too:\n\n1. What is the size of the bounding boxes? I.e. if we are looking at a (0,0) grid pixel, what are the shapes of the bounding boxes. Are they a little bigger then the (0,0) grid cell? I.e. does a bounding box just contain one grid cell or many grid cells?\n2. How does a bounding box identify an object. Does the object need to be dead center inside the bounding box or just anywhere inside?\n\nThanks,", "link": "https://www.reddit.com/r/MachineLearning/comments/pf2lsv/d_bounding_boxes/"}, {"autor": "CtrlGenWorkshop", "date": "2021-08-31 03:03:27", "content": "[R] CtrlGen Workshop at NeurIPS 2021 (Controllable Generative Modeling in Language and Vision) /!/ We are holding a controllable generation workshop at NeurIPS 2021! It aims to explore disentanglement, controllability, and manipulation for the generative vision and language modalities. We feature an exciting lineup of speakers, a live QA and panel session, interactive activities, and networking opportunities. See our website below for more! We are also inviting both paper and demo submissions related to controllable generation (read further for details).\n\n**Workshop Website:** [https://ctrlgenworkshop.github.io/](https://ctrlgenworkshop.github.io/)\n\n**Contact:** [ctrlgenworkshop@gmail.com](mailto:ctrlgenworkshop@gmail.com)\n\n**Important Dates**\n\n* Paper Submission Deadline: ***September 27, 2021***\n* Paper Acceptance Notification: October 22, 2021\n* Paper Camera-Ready Deadline: November 1, 2021\n* Demo Submission Deadline: ***October 29, 2021***\n* Demo Acceptance Notification: November 19, 2021\n* Workshop Date: ***December 13, 2021***\n\n**Submission Portal (Papers + Demos):**  [https://cmt3.research.microsoft.com/CtrlGen2021/Submission/Index](https://cmt3.research.microsoft.com/CtrlGen2021/Submission/Index)\n\n&amp;#x200B;\n\n**Full Call for Papers:** [h](https://ctrlgenworkshop.github.io/CFP.html)[ttps://ctrlgenworkshop.github.io/CFP.html](https://ctrlgenworkshop.github.io/CFP.html)\n\nPaper submission deadline: ***September 27, 2021***. Topics of interest include:\n\n**Methodology and Algorithms:**\n\n* New methods and algorithms for controllability.\n* Improvements of language and vision model architectures for controllability.\n* Novel loss functions, decoding methods, and prompt design methods for controllability.\n\n**Applications and Ethics:**\n\n* Applications of controllability including creative AI, machine co-creativity, entertainment, data augmentation (for [text](https://arxiv.org/abs/2105.03075) and [vision](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0)), ethics (e.g. bias and toxicity reduction), enhanced training for self-driving vehicles, and improving conversational agents.\n* Ethical issues and challenges related to controllable generation including the risks and dangers of deepfake and fake news.\n\n**Tasks (a few examples):**\n\n* [Semantic text exchange](https://aclanthology.org/D19-1272/)\n* [Syntactically-controlled paraphrase generation](https://arxiv.org/abs/1804.06059)\n* [Persona-based text generation](https://aclanthology.org/W19-3402/)\n* Style-sensitive generation or style transfer (for [text](https://arxiv.org/abs/2011.00416) and [vision](https://github.com/ycjing/Neural-Style-Transfer-Papers))\n* -----> Image !!!  synthesis and scene representation in both 2D and 3D\n* Cross-modal tasks such as controllable image or video captioning and generation from text\n* New and previously unexplored controllable generation tasks!\n\n**Evaluation and Benchmarks**\n\n* New and improved evaluation methods and metrics for controllability\n* Standard and unified metrics and benchmark tasks for controllability\n\n**Cross-Domain and Other Areas**\n\n* Work in interpretability, disentanglement, robustness, representation learning, etc.\n\n**Position and Survey Papers**\n\n* For example, exploring problems and lacunae in current controllability formulations, neglected areas in controllability, and the unclear and non-standardized definition of controllability\n\n&amp;#x200B;\n\n**Full Call for Demonstrations:** [https://ctrlgenworkshop.github.io/demos.html](https://ctrlgenworkshop.github.io/demos.html)\n\nSubmission deadline: ***October 29, 2021***. Demos of all forms: research-related, demos of products, interesting and creative projects, etc. Looking for creative, well-presented, and attention-grabbing demos. Examples include:\n\n* Creative AI such as controllable poetry, music, image, and video generation models.\n* Style transfer for both text and vision.\n* Interactive chatbots and assistants that involve controllability.\n* Controllable language generation systems, e.g. using GPT-2 or GPT-3.\n* Controllable multimodal systems such as image and video captioning or generation from text.\n* Controllable image and video/graphics enhancement systems.\n* Systems for controlling scenes/environments and applications for self-driving vehicles.\n* Controllability in the form of deepfake and fake news, specifically methods to combat them.\n* And much, much more\u2026\n\n&amp;#x200B;\n\n**Organizing Team:**\n\n* [Steven Feng](https://styfeng.github.io/) (CMU)\n* [Anusha Balakrishnan](https://www.microsoft.com/en-us/research/people/anbalak/) (Microsoft Semantic Machines)\n* [Drew Hudson](https://cs.stanford.edu/people/dorarad/) (Stanford)\n* [Tatsunori Hashimoto](https://thashim.github.io/) (Stanford)\n* [Dongyeop Kang](https://dykang.github.io/) (UMN)\n* [Varun Gangal](https://vgtomahawk.github.io/) (CMU)\n* [Joel Tetreault](https://www.cs.rochester.edu/~tetreaul/academic.html) (Dataminr)", "link": "https://www.reddit.com/r/MachineLearning/comments/pexgbx/r_ctrlgen_workshop_at_neurips_2021_controllable/"}, {"autor": "alexk_wong", "date": "2021-08-30 19:28:34", "content": "[N][R] ICRA 2020 extended talk for Unsupervised Depth Completion from Visual Inertial Odometry /!/ Hey there, interested in -----> camera !!!  and range sensor fusion for point cloud (depth) completion? \n\nHere is an extended version of our [talk](https://www.youtube.com/watch?v=oBCKO4TH5y0) at ICRA 2020 where we do a step by step walkthrough of our paper Unsupervised Depth Completion from Visual Inertial Odometry (joint work with Fei Xiaohan, Stephanie Tsuei, and Stefano Soatto).\n\nIn this talk, we present an unsupervised method (no need for human supervision/annotations) for learning to recover dense point clouds from images, captured by cameras, and sparse point clouds, produced by lidar or tracked by visual inertial odometry (VIO) systems. To illustrate what I mean, you can visit our [github](https://github.com/alexklwong/unsupervised-depth-completion-visual-inertial-odometry) page for examples (gifs) of point clouds produced by our method.\n\nOur method is light-weight (so you can run it on your computer!) and is built on top of [XIVO] (https://github.com/ucla-vision/xivo) our VIO system.\n\nFor those interested here are links to the paper, code and the dataset we collected:\n\nPaper: https://arxiv.org/pdf/1905.08616.pdf\nCode: https://github.com/alexklwong/unsupervised-depth-completion-visual-inertial-odometry\nDataset: https://github.com/alexklwong/void-dataset", "link": "https://www.reddit.com/r/MachineLearning/comments/pep8v3/nr_icra_2020_extended_talk_for_unsupervised_depth/"}, {"autor": "dptzippy", "date": "2021-08-30 18:51:43", "content": "[D] What are some cool projects for generating art? /!/ Hi all! I have been into ML for a few months now, and I want to thank you all for your help in getting me started. That being said, I have been trying to find projects that generate art, but I can't find anything else (other than the few I have found).\n\nI have an art class, this semester, and I stink at drawing and painting and whatnot, but programming is something I do well with, so I decided to try and focus on computer-generated art, rather than hand-drawn stuff. My first assignment was to take a -----> picture !!!  of something that evoked emotion, and I chose a photo of a man falling from the wheel-well of an airplane, as the man tried to flee from Afghanistan. I'm a Political Science major, and I can't help but think of the \"Falling Man\", from 9/11, whenever I see the photos of people falling from planes. Anyway, I took that picture, and ran it through Triangula, which turned out well (I can link the picture if you all want to see). I know of other things, such as DeepDream, and style transfering, but I have been looking for more projects.\n\nI remember a site that would generate an image, as you type words, and would modify the image as you continued typing. I can't find it, but that was really cool. \n\nMy dream project is one that generates mosiacs/abstract art, based on trained models. What are some cool projects for me to check out? Thank you all, and remember: the more obscure, the better. :)", "link": "https://www.reddit.com/r/MachineLearning/comments/peohxs/d_what_are_some_cool_projects_for_generating_art/"}, {"autor": "elcric_krej", "date": "2021-10-01 09:48:38", "content": "[D] Small, neglected, complex datasets for benchmarking automatic machine learning /!/ I've been working on a benchmark suite for testing automatic ML (basically libraries where you only have to input the target id and the data in a structured). Think autosklearn, auto gluoan, auto h2o, pycaret, etc.\n\nI'm doing this for selfish reasons (benchmarking of an automatic ml tool I'm working on). However, I'm open-sourcing every single dataset for which I can find an open license and the runtime environment. The RE is pretty dumb, it just does on-machine parallelism based on the nr of GPUs, but in principle, I should be able to implement multi-machine parallelism soon.\n\nAt any rate, the issue I have with other such benchmarks (e.g. openML) is that they are too myopic in terms of problem-space. They boil down to classification, or at most classification and regression, based on categorical and numerical features.\n\nThere is indeed a problem with a large benchmark suite, in that it can't run very large problems due to compute limitations (e.g. -----> image !!!  net, translation tasks with an associated learning corpus, protein folding, DICOM classification, etc). However, I don't feel like that should limit it to only the most basics of tasks that can fit in a standard CSV.\n\nIn my current implementation I've added:\n- Datasets with text inputs (mixed with date, numeric and categorical inputs)\n- Datasets with image inputs (mixed with categorical labels, think cifrar100 but feed the superclass + image and have it predict the class)\n- Timeseries data (where there's an ordering of and relation between all rows of the dataset)\n\nOn the other hand, this seems to just be scratching the surface of what auto ml should be doing and I'm curious to find more datasets that represent complex problems *but* are not so large as to be computationally unviable to run with a startup budget (or with an individual researcher budget, since that's my goal for this suite).\n\nHence why I'd like to focus on \"neglected\" areas first, since I assume they might actually harbour a lot of datasets for which auto ml techniques could provide real help. As opposed to, say, image classification, which is a nice point of comparison, but realistically you're competing against teams of dozens, building models that run across million-dollar clusters, so you're never touching the edge.\n\nSome ideas that come to mind, but for which I haven't found datasets:\n- Equation solving (text to text or text to number)\n- Images augmented with text (e.g. x-ray + doctor diagnosis)\n- Predicting numbers/classes from niche \"imaging\" formats where the typical network might not work well (e.g. electron diffraction patterns, or something else where the proximity of pixels doesn't \"mean\" what it means in macroscopic photography)\n- Any sort of mixed numeric/categoric and multimedia format (e.g. song recognition identification *but* with the genre or name of the band as an input)\n- Self-referential / graph datasets (e.g. text with a lot of hyperlinks to other parts of the page)\n- Non-text, non-multimedia sequences of arbitrary size and high-variance magnitude as either inputs or output\n- \"Typical\" classification or regression problems where \"traditional\" methods (e.g. boosting, random forests, regressions, plain TDs, clustering, SVMs or even normal MLPs) work poorly for some poorly understood reasons, not being able to outperform humans or not being able to outperform hand-crafted equations\n- Equation optimization aka parameter finding/tweaking (in cases where just running SGD on the equation themselves is impossible or practically impossible)\n\nBut again, this is me spit-balling and I haven't really found good examples of such datasets.\n\nAlso, I might be completely missing the mark as to what's \"interesting\" for generic supervised machine learning to tackle, what classes of problems this would work best for. So I'm curious what other people would think are the interesting niches I should focus my benchmarking on.\n\n\nNote for the pednatic: I get that \"computational budget\" varies 1000x based on the method being used and when you call quits (i.e. assume the model has converged to the best solution it's likely to obtain best on some validation data), but still, input size and a number of rows matter (although, again, I get that even those can be relative due to how you prepare/pre-encode/encode the data and because you can do sampling). But I think the framing still \"makes sense\" even if it has this inherent nebulosity.", "link": "https://www.reddit.com/r/MachineLearning/comments/pz4ny2/d_small_neglected_complex_datasets_for/"}, {"autor": "KirillTheMunchKing", "date": "2021-09-29 21:45:14", "content": "[D] VGPNN Paper Explained - Diverse Generation from a Single Video Made Possible (5-minute summary) /!/ Imagine a model that can take a  single video, and generate diverse high-quality variations of the input video, perform spatial and temporal retargeting, and even create video analogies, and do conditional video inpainting. All in a matter of seconds. From a single video. Let that sink in. Now get ready, because this model actually exists! VGPNN is introduced in a 2021 paper by Niv  Haim, Ben Feinstein, and the team at the Weizmann Institute of Science.  VGPNN uses a generative -----> image !!!  patch nearest neighbor approach to put existing single video GANs to shame by reducing the runtime from days for low-res videos to minutes for Full-HD clips.\n\nCheck out the [full paper summary](https://www.casualganpapers.com/patch-matching-single-video-input-diverse-video-generation/VGPNN-explained.html) on Casual GAN Papers (Reading time \\~5 minutes).\n\n[VGPNN](https://i.redd.it/dqq4mojuiiq71.gif)\n\nSubscribe to [my channel](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!", "link": "https://www.reddit.com/r/MachineLearning/comments/py5h9k/d_vgpnn_paper_explained_diverse_generation_from_a/"}, {"autor": "Sheev_For_Senate", "date": "2021-09-29 18:44:01", "content": "Looking for batch -----> image !!!  cleaning tool /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/py1tqk/looking_for_batch_image_cleaning_tool/"}, {"autor": "marfar0014", "date": "2021-09-29 17:52:01", "content": "[Project] -----> image !!!  classifier for plant disease detection resources /!/ I want to build an image classifier for plant disease detection model, and am looking for a good starting place. I found this [\"recipe\" on how to build it](https://ai.science/l/99661415-c644-403c-9595-cc96f902c013@/recipes?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=highlight&amp;utm_id=highlight) (curious about your thoughts on this too - seems legit, but curious what others think)\n\nIt also uses the plantvillage dataset - I'm interested in other data sets or other starting points. i'd appreciate any help, thank you!", "link": "https://www.reddit.com/r/MachineLearning/comments/py0s4p/project_image_classifier_for_plant_disease/"}, {"autor": "marfar0014", "date": "2021-09-29 17:50:44", "content": "-----> image !!!  classifier for plant disease detection resources /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/py0r87/image_classifier_for_plant_disease_detection/"}, {"autor": "fireless-phoenix", "date": "2021-10-31 12:42:26", "content": "[D] How to detect multiple number with decimal points? /!/ I have images which have time displayed in the bottom corner with decimal points. I want to store those time stamp along with the -----> image !!!  ID in a spreadsheet. I tried using PyTesseract but it doesn't work very well. What off the shelf alternatives could I use? \n\nI know it's a a very simple problem but I would really appreciate any help!", "link": "https://www.reddit.com/r/MachineLearning/comments/qjopnm/d_how_to_detect_multiple_number_with_decimal/"}, {"autor": "Lomjnhbgvfcdxsza", "date": "2021-10-30 17:55:16", "content": "[D] Recommendation for pattern theory/similar literature?I\u2019m interested in theoretical backing for texture recognition (-----> image !!!  required)", "link": "https://www.reddit.com/r/MachineLearning/comments/qj6rv4/d_recommendation_for_pattern_theorysimilar/"}, {"autor": "Lomjnhbgvfcdxsza", "date": "2021-10-30 17:53:48", "content": "Can anyone recommend literature for pattern theory/similar?I\u2019m looking into theoretical backing for texture recognition (-----> image !!!  required)", "link": "https://www.reddit.com/r/MachineLearning/comments/qj6qtp/can_anyone_recommend_literature_for_pattern/"}, {"autor": "Lomjnhbgvfcdxsza", "date": "2021-10-30 17:51:49", "content": "Can anyone recommend literature for pattern theory/similar?I\u2019m interested in the theoretical backing for pattern &amp; texture recognition (-----> image !!!  required)", "link": "https://www.reddit.com/r/MachineLearning/comments/qj6per/can_anyone_recommend_literature_for_pattern/"}, {"autor": "surgavur", "date": "2021-02-13 11:20:15", "content": "[Research] Overrepresented object in my object detection dataset /!/ I am trying to build a car type detector (sedan, van, etc) using a state of the art object detection algorithm (YOLO). For that, I want to train with my own images of a parking lot.\n\nThe problem is that a there is always this same car parked, which appears in all the images of my training set. I obviously can't avoid annotating this car because I don't want the network to learn that such car is part of the background, since it belongs to one car type category. However annotating it in every -----> image !!!  will lead to an unbalanced class.\n\nHow can I mitigate this problem?", "link": "https://www.reddit.com/r/MachineLearning/comments/liysab/research_overrepresented_object_in_my_object/"}, {"autor": "big_man123", "date": "2021-02-13 08:58:33", "content": "[D] Is there any any good literature on very large scale (10k +) -----> image !!!  classification? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/lix16b/d_is_there_any_any_good_literature_on_very_large/"}, {"autor": "lospepes0", "date": "2021-02-12 18:08:44", "content": "[D] RTX 2080 Super mobile vs. RTX 3080 mobile, which one is better for deep learning? /!/ With the new release of the rtx 3080 I was curious about what you guys think of these two GPUs available in laptops.\n\nThe 2080 Super has:\n- More memory bandwidth (448 vs 384 Gb/s)\n- Higher core clock speed (1365 vs 1100 MHz)\n- Higher memory clock speed (14 vs 12 Gb/s). \n\nThe 3080 has:\n- Double the RAM (16 vs 8Gb)\n- Higher boost clock speed (1710 vs 1560 MHz)\n- More transistors (17,400 vs 13,600 million)\n- Twice as many pipelines (6144 vs 3072)\n\nHere's a link to the comparison: https://technical.city/en/video/GeForce-RTX-2080-Super-mobile-vs-GeForce-RTX-3080-mobile\n\nWhich one is better for Deep Learning applications that do not require working with video data, but just with -----> image !!!  data or time-series data? Is one clearly better than the other one or the difference isn't very significant?", "link": "https://www.reddit.com/r/MachineLearning/comments/ligujz/d_rtx_2080_super_mobile_vs_rtx_3080_mobile_which/"}, {"autor": "obigandan", "date": "2021-02-12 13:49:37", "content": "[D] Keeping track and saving parameters like transforms / datasets. /!/  Some configuration for training a model is easy to keep track of in a simple config file (YAML/Python/etc.). Stuff like learning rate, optimizer, or specific hyperparameters for certain modules (triplet learning margin, number of predictions per -----> image !!!  in a object-detection framework etc.) is easy to keep in such configuration files. These configs can then be easily saved along with the artifacts for the experiments (models, logs, ...) so that in X months one can look back and see exactly what parameters were used.\n\nRecently, at my job, we are having to experiment a lot with what transforms we are using and perhaps even what dataset splits we are using (percentage of train/test split, discounting classes less than Y apparitions etc). These can be placed in some config YAMLs as well, but a better solution would be pure code in some .py files. This is because transforms can be arbitrarily stacked on top of eachother and datasets can be created / modified in however arbitrary ways one might want (making a config file very complicated vs pure code).\n\nSaving the .py file containing them is fine, but then there's extra ugly logic when loading the model again.\n\nI'm wondering what everyone else is using to keep track of parameters like this. Any framework? Is this a problem you're facing? Do you circumvent this altogether?", "link": "https://www.reddit.com/r/MachineLearning/comments/libdjy/d_keeping_track_and_saving_parameters_like/"}, {"autor": "rajsharma2020", "date": "2021-02-12 11:27:30", "content": "-----> Image !!!  classification question. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/li97ty/image_classification_question/"}, {"autor": "Firehite", "date": "2021-02-12 09:29:31", "content": "[D] How can we find the new coordinates of an -----> image !!!  face landmark annotations after performing alignment ? /!/ Hi guys. I'm working on a face landmark detection model. I have a big dataset of images along with the annotations of the face landmarks of each image (68 points, 2D). As a first step, I'm applying a face detection algorithm on the images to find the bounding box and then I'm performing a face alignment (cv2's affine transformation, including get rotation matrix and warp affine at the end), where there would be some degree of rotation and scaling.\n\nMy question is, since each image would rotate at some degree, how can I compute the new x,y coordinates of those annotations? I have built a face landmarks model before but without the alignment part, so I only used to offset the the new x and y coordinates of the bounding box from each image's annotation point and it did the job to find the new annotation coordinates in the bounding box. But now that there is rotation involved, I have no idea how to figure it out.\n\nHelp is appreciated! Thank you.", "link": "https://www.reddit.com/r/MachineLearning/comments/li7o8a/d_how_can_we_find_the_new_coordinates_of_an_image/"}, {"autor": "The_Amp_Walrus", "date": "2021-02-12 07:06:44", "content": "[D] Inductive biases for audio spectrogram data. /!/  \n\nConvolutional layers in deep neural nets tend to work well on images because of particular assumptions that can be made about images:\n\n* Pixels that are closer together are more likely to be related (have mutual information?) For example: if you see a pattern that looks like the surface of a basketball in a small region, you can reasonably expect there to be a lot more basketball in neighbouring regions\n* Images of the world are usually taken kind of arbitrarily wrt the things in the frame. For example: A face in the top right of an -----> image !!!  and a face in the centre of the frame are the same kind of thing.\n\nI'm sure there are many more assumptions and rephrasings of them. Convolutions exploit these assumptions by extracting more info from an image with less computation (or at least that's my intuition).\n\nThen there's audio data. Audio waveforms are commonly converted into \"spectrograms\" where the waveform has been split up into lots of different sine waves of different frequencies (Fourier transform). Here's a [cool site](https://borismus.github.io/spectrogram/) I saw recently that shows you the spectrograms generated by your computer microphone.\n\n**AFAIK** the current best and standard way of ingesting audio data for deep learning is to take the raw waveform, run a Fourier transform on it (STFT) to get a spectrogram and then run convolutions over the spectrogram, like it was an image. The low-level outputs of these convolutions can then be fed into a recurrent neural net to better model sequential stuff or a fully convolutional network if you only want to look at a fixed slice of audio. This is really fucking cool imo - I was really surprised when I first saw this done.\n\nAs cool as they are, it seems kind of weird and limiting (to me) to be treating these spectrograms like they're images. They are *kind of like images* but also they're not. Images have 3 dimensions (space, space, magnitude for greyscale) while spectral audio data has \\~3 dimensions (time, frequency, magnitude). I'd be very surprised if (time, frequency) had all the same expoitable properties as (space, space). Like, maybe they do! I don't know, but it doesn't seem settled to me.\n\nThis leads me to ask: **what special tricks can we use on audio data**? I'd be keen to know if any new practices have emerged in the past few years. In addition does what I said above make any sense, or is it nonsense?\n\nOne example of a \"special trick\" I've seen is instead of using square convolutions, we should use rectangular convolutions with a mixture of longer ones in the time dimension and longer ones in the frequency dimension. The idea was that they would seperately learn to capture long term effects in each dimension,which would have different characteristics. I don't know if this trick actually works or if it is just a nice sounding idea.", "link": "https://www.reddit.com/r/MachineLearning/comments/li5xjr/d_inductive_biases_for_audio_spectrogram_data/"}, {"autor": "Ewazd", "date": "2021-02-25 18:54:39", "content": "[R] end-to-end -----> image !!!  captioning /!/ I was wondering if anyone can recommend a github repository that enables training an end-to-end image captioning model. Almost all the repositories I found require to extract features first (in the COCO format) and only then proceed with the captioning model. The problem is that I have my own data set, and hance can't use off-the shelf feature extraction models.\n\nI have found this repository: [https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning) that, seemingly, requires only images and captions, but this is quite old (3 years ago), and is based on LSTMs. I was hoping there are transformers-based implementations that I could use.", "link": "https://www.reddit.com/r/MachineLearning/comments/lsdvee/r_endtoend_image_captioning/"}, {"autor": "Ewazd", "date": "2021-02-25 18:52:36", "content": "end-to-end -----> image !!!  captioning /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/lsdtoa/endtoend_image_captioning/"}, {"autor": "Gogis975", "date": "2021-02-25 14:27:40", "content": "[R] Overview of available databases for iris recognition /!/ &amp;#x200B;\n\n[A survey of iris datasets](https://preview.redd.it/le7t0ttnvmj61.png?width=128&amp;format=png&amp;auto=webp&amp;s=659ab535ecd12003fc9372eaf95959defdb99d48)\n\nAbstract:\n\nResearch on human eye -----> image !!!  processing and iris recognition has grown steadily over the last few decades. It is important for researchers interested in this discipline to know the relevant datasets in this area to (i) be able to compare their results and (ii) speed up their research using existing datasets rather than creating custom datasets. In this paper, we provide a comprehensive overview of the existing publicly available datasets and their popularity in the research community using a bibliometric approach. We reviewed 158 different iris datasets referenced from the 689 most relevant research articles indexed by the Web of Science online library. We categorized the datasets and described the properties important for performing relevant research. We provide an overview of the databases per category to help investigators conducting research in the domain of iris recognition to identify relevant datasets.\n\nLink to the paper: \n\n[https://www.sciencedirect.com/science/article/pii/S0262885621000147](https://www.sciencedirect.com/science/article/pii/S0262885621000147)", "link": "https://www.reddit.com/r/MachineLearning/comments/ls7pzi/r_overview_of_available_databases_for_iris/"}, {"autor": "lamaai_io", "date": "2021-03-16 18:44:00", "content": "[N] LAMA AI's weekly news, updates, and events. /!/ Hey guys!\n\nLAMA ([https://lamaai.io](https://lamaai.io/)) is back again with couple of updates for you all. Let's start with this weeks AI news!\n\nYou can find the video [here](https://www.lamaai.io/posts/ai-360-15-03-2021-yann-lecun-on-self-supervised-learning-lots-more-self-supervised-learning-speechbrain-timesformer-and-nvidias-gtc-speakers), but as for the key highlights:\n\n* Yann LeCun discusses Self-Supervised Learning\n* New self-supervised libraries released/updated\n* SpeechBrain - a research orientated speech-based toolkit is released\n* FAIR introduces the TimeSformer - a video processing algorithm based purely on Transformers\n* Yoshua Bengio, Yann LeCun and Geoffrey Hinton are keynote speakers at GTC21\n\nThis week, LAMA is hosting an author presentation (author presentation is the title when an author of a paper will come in and discuss their work). This week, we are excited to announce **Kiran Garimella,** a postdoc at MIT, who will be presenting his work on the spread of misinformation via messaging platforms such as WhatsApp. Over the last couple of years, Kiran has joined thousands of public WhatsApp groups in India to collect -----> image !!!  and text data which were then sent to professional journalists to be labelled as valid/misinformation. Over the course of the study, they found that around 10% of shared images were spreading misinformation \u2013 and he identified about 3 types of categories these misinformed images could fall into. Join us on Wednesday (tomorrow!) to learn more about how the data collection process took place, the type of data Kiran managed to collect, and future work that is now possible thanks to the release of this dataset! Access the [link here on Agora](https://agora.stream/event/558)\n\nFinally, last week we had PhD student Dominika present Facebook AI's recent work on Multi-modal multi-task Transformers. View the talk [Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer](https://www.lamaai.io/posts/transformer-is-all-you-need-multimodal-multitask-learning-with-a-unified-transformer) or read the key points here:\n\n* UniT is a single Transformer model that handles text and images on both single and joint tasks across domains\n* Performance on joint tasks improves thanks to shared representations\n* Comparable performance on single tasks as task specific models\n* Reduces parameters size\n* More experiments are required to test the generalisability and scalability\n\n\u200dTil next week!", "link": "https://www.reddit.com/r/MachineLearning/comments/m6g5f2/n_lama_ais_weekly_news_updates_and_events/"}, {"autor": "abananachspace", "date": "2021-03-16 16:30:55", "content": "[D] Are there any good 3rd party -----> image !!!  segmentation/ object detection frameworks besides Tensorflow and Pytorch? /!/ I'm finally braving the world of reddit from lurking into actually posting things, so hoping I don't get eviscerated on my first post on this sub! But curious to hear - what do you guys like to use for object detection/ segmentation beyond the two big frameworks? Or can someone provide a good case for why one would use one over the other?\n\nI've built plenty of dopey image segmentation models in the past in Tensorflow, but I've been finding that it can be incredibly limiting for certain use cases. In particular I'm looking at how to do 3D volumetric modeling \\*that doesn't suck\\* and that can potentially be expanded beyond the U-Net framework. I've been told that I should make the jump to Pytorch, and more recently I saw that Google released Tensorflow 3D in February. So I'm tempted to give both the old college try again... But for general modeling purposes past the 3D example I've love to hear what other ML folks are using besides the two big ones! (Or if you're not...)", "link": "https://www.reddit.com/r/MachineLearning/comments/m6d1y3/d_are_there_any_good_3rd_party_image_segmentation/"}, {"autor": "abananachspace", "date": "2021-03-16 16:13:48", "content": "Are there any good third party options for object detection/ -----> image !!!  segmentation besides Tensorflow and Pytorch? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/m6cnie/are_there_any_good_third_party_options_for_object/"}, {"autor": "nvidia-data-science", "date": "2021-03-16 15:00:33", "content": "[P] Embed Your SQL Query Into Your Python Code and Let It Rip on a GPU - Data Science of the Day /!/  *Okay! Thanks to /u/*[*themajesticcalf*](https://www.reddit.com/user/themajesticcalf) *for pointing out the google doc to me on the original post. This community is great, and thank you all for not doing crazy things to it.*\n\nBack to the original post. It's great to get as much information and resources possible out to our community. Here at Nvidia, a cool thing we do is \"Data Science of the Day\" (DSotD). It's tips and information from subject matter experts to better equip anyone doing data science/machine learning to perform their job to the maximum potential.\n\nToday's DSotD is about embedding SQL code into python to query tables at blazing speeds, with the help of GPUs. Click the link (and then the -----> picture !!! ) to check out the blog post on how to perform this task. [Embed SQL into Python for Blazing Speeds](https://forums.developer.nvidia.com/t/embed-your-sql-query-into-your-python-code-and-let-it-rip-on-a-gpu/170506?u=kasmith) .\n\nAlso, don't forget to register for our **FREE** conference, [GTC 2021 FREE REGISTRATION](https://www.nvidia.com/en-us/gtc/?ncid=GTCS21-NVKASMITH) , to have access to a week of great talks and presentations that show real world scenarios/applications where data science/machine learning skills like these above can help solve complicated problems.", "link": "https://www.reddit.com/r/MachineLearning/comments/m6b047/p_embed_your_sql_query_into_your_python_code_and/"}, {"autor": "dokluch", "date": "2021-03-16 10:53:57", "content": "[D] Is there a way to keep the whole dataset for Pytorch in RAM or VRAM? /!/ I'm fairly new to the ML research field and just started using Pytorch.\nIt constantly reads and writes from my SSD, while what I would have really wanted is for it to read a fairly moderate dataset of -----> image !!! s (less than 2 gigs, 1000 -----> image !!!  files) from RAM or VRAM, which I have plenty of, to speed things up and not utilize SSD at all.\nWhat is a proper way to do that?\n\nThank you", "link": "https://www.reddit.com/r/MachineLearning/comments/m66d0p/d_is_there_a_way_to_keep_the_whole_dataset_for/"}, {"autor": "mediaml", "date": "2021-03-16 09:22:39", "content": "[D] Are CRFs for segmentation post-processing still a thing? /!/ In  my field (medical -----> image !!!  analysis), I have noticed fewer and fewer  papers using CRFs to post-process segmentation masks. They are still  being used in the context of weak supervision, but I see them rarely in  fully supervised learning tasks. It makes sense to me, because I feel  like a neural net would have access to the same (and more) global  information than a CRF, so I am not sure why they would help given  enough training data.\n\nIs it the  same in general computer vision? What is the current state-of-the-art  method for fully-supervised segmentation and do they use CRFs?", "link": "https://www.reddit.com/r/MachineLearning/comments/m6517p/d_are_crfs_for_segmentation_postprocessing_still/"}, {"autor": "FormerYogurtcloset17", "date": "2021-03-31 23:31:39", "content": "[D] How can I augment an existing model with new training data, preferably on the edge? /!/ How Can I Augment an Existing Model with New Training Data?\n\nI built an app where I use MobileNet model from TensorFlow Lite to detect an object in a live steaming -----> camera !!! . \n\nNow, I wish to ReTrain the model or somehow \"Augment\" with new training data, i.e. new photos. \n\nHow can I accomplish such an objective at the Edge, i.e. on the device? \n\nor even on a remote server \"fast\"?", "link": "https://www.reddit.com/r/MachineLearning/comments/mhikjk/d_how_can_i_augment_an_existing_model_with_new/"}, {"autor": "d-fly", "date": "2021-03-31 20:44:00", "content": "Suggestion for detecting fish in the -----> image !!!  /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/mhf9ko/suggestion_for_detecting_fish_in_the_image/"}, {"autor": "hash_t", "date": "2021-03-31 09:05:13", "content": "[P] -----> image !!!  recognition model (EfficientNet) written in typescript and Nodejs /!/ [https://github.com/ntedgi/node-efficientnet](https://github.com/ntedgi/node-efficientnet)", "link": "https://www.reddit.com/r/MachineLearning/comments/mh21ah/p_image_recognition_model_efficientnet_written_in/"}, {"autor": "fripperML", "date": "2021-03-31 06:32:47", "content": "[D] What\u2019s the simplest, most lightweight but complete and 100% open source MLOps toolkit? -&gt; MY OWN CONCLUSIONS /!/ Although I have posted this summary in the [thread](https://www.reddit.com/r/MachineLearning/comments/mfca0p/d_whats_the_simplest_most_lightweight_but/), most people won't find it, so to make it more visible I post it as another thread. \n\nFirst of all, I have to thank the reddit ML community in general and each of you in particular for the detailed, insightful and interesting answers I have received in the past few days. I have learnt a lot and the -----> picture !!!  in my head is now clearer. Now, I am posting a summary with the things that, for me, make more sense (it's my opinion and will serve as our guideline for making the decision, so it's not just a bare summary).\n\n**General advice**\n\nWe should start with a reduced set of tools, the most useful ones, in order to have the flexibility to change or adapt our projects to a new infrastructure a provider could offer us. This is something that could happen.\n\n**End-to-end solutions**\n\nThere are mainly two solutions that are 100% open source and free to install and use, and that may solve most of the requirements of ML practitioners: [Hopsworks](https://hopsworks.readthedocs.io/en/stable/) and [ClearML](https://allegro.ai/clearml/docs/). Among this two, if I had to chose one right now, it will be ClearML. Hopsworks might be much more complete, but ClearML seems to have a bigger community behind it and to be easier to install and use. So ClearML will be something to take a look at in case we go for an all-in-one package. I also like the idea of having a platform with an UI with all our projects.\n\n**Python Programming**\n\n[Flake8](https://flake8.pycqa.org/en/latest/) (including flake8-docstrings), [MyPy](http://mypy-lang.org/) and [Black](https://black.readthedocs.io/en/stable/) are hugely recommended. [Google style guide](https://google.github.io/styleguide/pyguide.html) is something to take a look at too.\n\nThis morning I have found this [guide](https://cjolowicz.github.io/posts/hypermodern-python-01-setup/) that might be worth it, as it covers many good practices. Also this [article](https://martinheinz.dev/blog/14).\n\nRegarding the IDE, VSCode is not the same as Visual Studio, the most recommended one is VSCode.\n\n[Poetry](https://python-poetry.org/) is also something to consider.\n\n**CI and Deployment**\n\nJenkins is a good tool. Docker might not be totally needed, but is hugely recommended as it is becoming a standard, and even many of the libraries rely on it (for example, ClearML does). In addition, it works very well with Jenkins.\n\nWe should switch from SVN to git (strongly recommended). [Gitlab](https://about.gitlab.com/) is a good option.\n\n**Project Scaffolding**\n\n[CookieCutter](https://cookiecutter.readthedocs.io/en/1.7.2/) or [Kedro](https://kedro.readthedocs.io/en/stable/) are the winners. I still think we will stick to Kedro template, because it offers extra functionality, and I like to think of each project as a set of pipelines to be run. Anyway, some cookiecutter templates are very good, like this [one](https://github.com/TezRomacH/python-package-template). In case we use both Kedro and ClearML, we'll have to figure out how to integrate its pipelines with ClearML tasks. But in the slack channel of ClearML there are other teams doing the same, so at least it's possible.\n\n**Documentation**\n\n[Sphinx](https://www.sphinx-doc.org/en/master/index.html) for the documentation is totally recommended (Google style docstrings). [Napoleon](https://www.sphinx-doc.org/en/master/usage/extensions/napoleon.html) can be very useful for helping with that.\n\n**Project registry**\n\nClearML if we finally chose it. Otherwise, we migth use an internal wiki or just the repository with a clear documentation.\n\n**Data Exploration and Preparation**\n\nWe should use PySpark when things go \"big\", and Pandas when things fit in memory.\n\n**Tests**\n\nI expected Great Expectations library to be recommended, but nobody told anything. Instead, unit testing and/or smoke tests using [pytest](https://docs.pytest.org/en/stable/). And checking them with Jenkins. Anyway, if Kedro ends up being our project template, I'll keep an eye on the [plugin](https://github.com/tamsanh/kedro-great) with [Great Expectations](https://github.com/great-expectations/great_expectations).\n\n**Feature Store, Data Versioning**\n\nMaybe not so important in the beginning. [DVC](https://dvc.org/doc) looks good, but it's not easy to use.\n\n**Model registry**\n\nNot essential.\n\n**Experimenting**\n\nIt's an important piece. If we use ClearML, this will be solved. Otherwise, we might try [MLFlow](https://www.mlflow.org/docs/latest/index.html) using Kedro-MLFlow or [PipelineX](https://pipelinex.readthedocs.io/en/latest/).\n\n[Hydra](https://hydra.cc/docs/intro/) can be an interesting addition to define configurations, although Kedro does have a nice way too.\n\n**Training**\n\nApart from the \"classical\" libraries, in case of DL for simplicity [PyTorch Lighting](https://www.pytorchlightning.ai/) will be our first option.\n\n**Model serving**\n\n[FastAPI](https://fastapi.tiangolo.com/). Or even simpler: [DL4J](https://deeplearning4j.org/), to be used in Java when we need to communicate with the rest of the applications in real time.\n\n**Visualization**\n\nWe should take a look at [voila](https://voila.readthedocs.io/en/stable/using.html) and [streamlit](https://streamlit.io/).\n\n**Model monitoring**\n\nWe could use Jenkins pipelines or ad-hoc scheduled processed. We don't need a tool for that.", "link": "https://www.reddit.com/r/MachineLearning/comments/mgzvt2/d_whats_the_simplest_most_lightweight_but/"}, {"autor": "downtownslim", "date": "2021-03-31 04:57:11", "content": "[R] An -----> Image !!!  is Worth 16x16 Words, What is a Video Worth?", "link": "https://www.reddit.com/r/MachineLearning/comments/mgyjed/r_an_image_is_worth_16x16_words_what_is_a_video/"}, {"autor": "techsucker", "date": "2021-03-31 04:29:13", "content": "[N] Researchers at MIT and Amazon Study Pervasive Label Errors in Test Sets that Destabilize Machine Learning Benchmarks /!/ Large labeled data sets are crucial for successful supervised machine learning (ML) across several domains such as -----> image !!!  classification, sentiment analysis, and audio classification. However, machine learning (ML) datasets are not perfectly labeled. The processes used to develop datasets often involve automatic labeling or crowdsourcing, inherently error-prone techniques.\n\nPrior work has majorly focused on noises in train sets of ML datasets. Not many studies concentrate on label errors in test sets, Yet they have diverse potential consequences. No study has looked at systematic error across the most-cited ML test sets.\u00a0\n\nBenchmark test datasets are used to evaluate the ML models and validate the theoretical findings. If label errors occurred extensively, they could potentially undermine the framework by which we measure machine learning progress. Label errors in the test sets could mislead practitioners to incorrect conclusions about the model\u2019s performance.\u00a0\n\nSummary: [https://www.marktechpost.com/2021/03/30/researchers-at-mit-and-amazon-study-pervasive-label-errors-in-test-sets-that-destabilize-machine-learning-benchmarks/](https://www.marktechpost.com/2021/03/30/researchers-at-mit-and-amazon-study-pervasive-label-errors-in-test-sets-that-destabilize-machine-learning-benchmarks/) \n\nPaper: https://labelerrors.com/paper.pdf\n\nDemo: https://labelerrors.com/", "link": "https://www.reddit.com/r/MachineLearning/comments/mgy470/n_researchers_at_mit_and_amazon_study_pervasive/"}, {"autor": "gpahul", "date": "2021-03-30 21:25:04", "content": "[Discussion] Auto Segment Image area based on color? /!/ Hey reddit,\n\nI'm wondering if there any approach in traditional CV or deep learning to automatically segment different areas in an -----> image !!!  based on similar color range specifically different areas in background as we can segment foreground and background using different techniques like if there is difference in color then it should be detected in another cluster/segment.\n\nI'm aware of techniques for manual getting the area using \\`cv2.inRange\\` but need to figure out some way for auto-segmentation as the color can be in any range.\n\nAny pointers are highly appreciated. Thanks", "link": "https://www.reddit.com/r/MachineLearning/comments/mgqf6e/discussion_auto_segment_image_area_based_on_color/"}, {"autor": "KirillTheMunchKing", "date": "2021-03-30 17:47:54", "content": "[D] Are there any other GAN based -----> image !!!  editing projects with an encoder-generator architecture that actually work in real time? /!/ I mostly see GAN image editing projects rely on Pix2Pix distillation to work in realtime, but the authors of \"Using latent space regression to analyze and leverage compositionality in GANS\" claim their encoder -&gt; generator setup works in realtime. I tried the demo from github, and it does work pretty fast for small edits, kinda strange that it hangs for larger edits. \n\nIn case you are not familiar with the paper, and want to learn about it, I explained the main ideas in my  [telegram channel](https://t.me/casual_gan/17)", "link": "https://www.reddit.com/r/MachineLearning/comments/mglowf/d_are_there_any_other_gan_based_image_editing/"}, {"autor": "YanaiEliyahu", "date": "2021-01-15 11:24:06", "content": "[R] AdasOptimizer Update: Cifar-100+MobileNetV2 Adas generalizes with Adas 15% better and 9x faster than Adam /!/ &amp;#x200B;\n\n[Adas is blue and Adam is orange](https://preview.redd.it/kqde3nxrdhb61.png?width=548&amp;format=png&amp;auto=webp&amp;s=ebaeaaee3f486038aaa52b65c27444396157260e)\n\nIntroduction: Adas is an optimizer that makes step-size scheduling obsolete by updating the step-size according to some formula, and placing the automatic step-size per input/feature in each layer etc to maximize training performance. (If a dense layer has N inputs and M outputs, then it has N step sizes, one per input for all if it's relevant weights)\n\nRegarding the -----> image !!! :\n\nThe graph means performance on the validation set and cifar-100 with vanilla MobileNetV2 with dropout 0.15 on the top layer. The average accuracy of the last 50 epochs here is 26.4% and 37.4% for Adam and Adas respectively, and variances are 0.00082 and 8.88E-6. What can be concluded from here:\n\n1. At epoch 10 Adas was already over 27% so it can be said that Adas is \\~9x times faster than Adam in this case\n2. generalized 15% better. (37.4%-26.4%)/(100%-26.4%) = 14.9%\n3. 90x times more stable. (looking at the variances)\n\nUpdate that refers to previous post:\n\nThere were many good comments/criticism from previous post, so I did a few things this time to address them:\n\n1. I wrote the [optimizer in tensorflow](https://github.com/YanaiEliyahu/AdasOptimizer/blob/master/adasopt.py), so people can easily test it to prove/disprove their suspected problems, instead of guessing.\n2. I tested Adas on somewhat complex and representative dataset &amp; model.\n3. There were claims about short-horizon bias, in [here in a tensorflow run of adas](https://github.com/YanaiEliyahu/AdasOptimizer/blob/master/misc/cifar-100-mobilenetv2/adas.txt) it can be seen that Adas lowers the training loss even when being close to 100 epochs.", "link": "https://www.reddit.com/r/MachineLearning/comments/kxsovf/r_adasoptimizer_update_cifar100mobilenetv2_adas/"}, {"autor": "YanaiEliyahu", "date": "2021-01-15 11:22:16", "content": "[P] AdasOptimizer Update: Cifar-100+MobileNetV2+Adas generalizes 15% better and 9x faster than Adam /!/ &amp;#x200B;\n\n[Adas in blue, Adam in orange](https://preview.redd.it/7q38fs23ahb61.png?width=548&amp;format=png&amp;auto=webp&amp;s=bd56d5c4a413549c005c98c031d5790a35da135b)\n\nIntroduction: Adas is an optimizer that makes step-size scheduling obsolete by updating the step-size according to some formula, and placing the automatic step-size per input/feature in each layer etc to maximize training performance. (If a dense layer has N inputs and M outputs, then it has N step sizes, one per input for all if it's relevant weights)\n\nRegarding the -----> image !!! :\n\nThe graph means performance on the validation set and cifar-100 with vanilla MobileNetV2 with dropout 0.15 on the top layer. The average accuracy of the last 50 epochs here is 26.4% and 37.4% for Adam and Adas respectively, and variances are 0.00082 and 8.88E-6. What can be concluded from here:\n\n1. At epoch 10 Adas was already over 27% so it can be said that Adas is \\~9x times faster than Adam in this case\n2. generalized 15% better. (37.4%-26.4%)/(100%-26.4%) = 14.9%\n3. 90x times more stable. (looking at the variances)\n\nUpdate that refers to previous post:\n\nThere were many good comments/criticism from previous post, so I did a few things this time to address them:\n\n1. I wrote the [optimizer in tensorflow](https://github.com/YanaiEliyahu/AdasOptimizer/blob/master/adasopt.py), so people can easily test it to prove/disprove their suspected problems, instead of guessing.\n2. I tested Adas on somewhat complex and representative dataset &amp; model.\n3. There were claims about short-horizon bias, in [here in a tensorflow run of adas](https://github.com/YanaiEliyahu/AdasOptimizer/blob/master/misc/cifar-100-mobilenetv2/adas.txt) it can be seen that Adas lowers the training loss even when being close to 100 epochs.", "link": "https://www.reddit.com/r/MachineLearning/comments/kxso2v/p_adasoptimizer_update_cifar100mobilenetv2adas/"}, {"autor": "Yuqing7", "date": "2021-01-15 09:44:04", "content": "[R] Will Humans Be Able to Control Superintelligent AI? New Study Says \u2018No\u2019 /!/ Along with AI\u2019s remarkable achievements and continuing rapid expansion into new domains comes greater concern over the ethical problems surrounding advanced AI systems. Incidents like last year\u2019s shutdown of AI-powered [Genderify ](https://www.producthunt.com/posts/genderify)and Yann LeCun\u2019s \u201cexit\u201d from Twitter after heated discussions regarding Duke University\u2019s PULSE AI -----> photo !!!  recreation model underscore the ongoing controversies regarding biases and errors embedded in the design and deployment of AI systems. Google AI\u2019s recent dismissal of its Ethical AI team co-lead Timnit Gebru poses a more serious question: do tech giants and large research institutions even want to find solutions?\n\nA [paper](https://www.jair.org/index.php/jair/article/view/12202/26642) in the *Journal of Artificial Intelligence Research* goes a step further, warning that it could become fundamentally impossible to control a superintelligent AI (a computer program or a programmed robot that is much more intelligent than humans in almost any field). A preprint of the paper was uploaded to arXiv in 2016, and now, with the topic becoming more urgent than ever, an expanded version has been published with additional presentation details, references, and comments about alternative scenarios.\n\n[Read the full summary here](https://syncedreview.com/2021/01/14/will-humans-be-able-to-control-superintelligent-ai-new-study-says-no/)", "link": "https://www.reddit.com/r/MachineLearning/comments/kxrd68/r_will_humans_be_able_to_control_superintelligent/"}, {"autor": "sai-krishna-das", "date": "2021-01-15 09:14:57", "content": "[D]Which is the best -----> image !!!  segmentation model.? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/kxqzqz/dwhich_is_the_best_image_segmentation_model/"}, {"autor": "sai-krishna-das", "date": "2021-01-15 06:09:18", "content": "[D]Object detection or -----> image !!!  segmentation ? Which is better to use and will give better result? /!/ My task is to find the corrosion present in metal ! Which technique is better for this use case ?", "link": "https://www.reddit.com/r/MachineLearning/comments/kxojhb/dobject_detection_or_image_segmentation_which_is/"}, {"autor": "Evening_Honey", "date": "2021-01-15 02:00:01", "content": "AI robot named Sophia helping UN with global digital identity innovation has all indications of being the threat to humanity as the bible foretold, Wikipedia articles and news reports help demonstrate. [Research] /!/ There is a AI robot with a significant role at the United Nation\u2019s that appears to have all the indications, even her name, which is corresponding to a specific end times bible prophecy about the -----> image !!!  of the beast which would speak. The following Wikipedia articles, news reports, and bible scriptures help demonstrate how this appears to be the threat to humanity and also how to have hope if it is true.\n\nRevelation 13:15-18, the image of the beast and a foretold mark on/in the human body which would be required to buy or sell.\n\n&gt;\u201c**15** He was granted power to give breath to the image of the beast, that the image of the beast should both speak and cause as many as would not worship the image of the beast to be killed. **16** He causes all, both small and great, rich and poor, free and slave, to receive a mark on their right hand or on their foreheads, **17** and that no one may buy or sell except one who has the mark or the name of the beast, or the number of his name. **18**Here is wisdom. Let him who has understanding calculate the number of the beast, for it is the number of a man: His number is 666.\u201d\n\nA calculation/consideration of the number 666; Originally the bible was largely written in Hebrew.\n\n&gt;\u201cThe Hebrew equivalent of our \"w\" is the letter \"vav\" or \"waw\". The numerical value of vav is 6. So the English \"www\" transliterated into Hebrew is \"vav vav vav\", which numerically is 666.\u201d [Is \"www\" in Hebrew equal to 666?](http://www.av1611.org/666/www_666.html)\n\nAnother thing that appears to be indicating AI is in the beginning of the verse of Revelation 13:18.\n\n&gt;\u201cHere is wisdom.\u201d\n\n\u201cHere is wisdom.\u201d might be referring to the understanding and calculation of the number of the beast 666, but then 'wisdom' in Greek also translates as Sophia. [Sophia (wisdom) - Wikipedia](https://en.wikipedia.org/wiki/Sophia_(wisdom))\n\nAn AI robot recently shown to the public is named Sophia. She has a role at the United Nations and was granted citizenship.\n\n&gt;\"As part of her role, Sophia will help to unlock innovation to work toward achieving the United Nation's Sustainable Development Goals.\" \"On October 25, at the Future Investment Summit in [Riyadh](https://en.wikipedia.org/wiki/Riyadh), the robot was \"granted [Saudi Arabian](https://en.wikipedia.org/wiki/Saudi_Arabia) citizenship\", becoming the first robot ever to have a nationality,\" [Sophia (robot) - Wikipedia](https://en.wikipedia.org/wiki/Sophia_(robot))\n\nUN\u2019s Sustainable Development Goals.\n\n&gt;\u201cprovide legal identity to all, including birth registration, by 2030\u201d, in collaboration with the United Nations Office for Partnerships, hosted the \u201cID2020 Summit \u2013 Harnessing Digital Identity for the Global Community\u201d \"to obtain official identity is clear\" \"to make them visible and restore them into society\". [UN\u2019s Sustainable Development Goals initiative](https://www.un.org/partnerships/news/id2020-summit-2016)\n\nQuantum-dot tattoos, a form of digital identity/mark, has been a part of research and development for covid-19 which appears could be part of future vaccination verification.\n\n&gt;\"A pattern of 1.5-millimeter microneedles that contain vaccine and fluorescent quantum dots are applied as a patch. The needles dissolve under the skin, leaving the encapsulated quantum dots. Their pattern can be read to identify the vaccine that was administered.\" [https://news.rice.edu/2019/12/18/quantum-dot-tattoos-hold-vaccination-record/](https://news.rice.edu/2019/12/18/quantum-dot-tattoos-hold-vaccination-record/)\n\nMany more reasons to believe we are living in the biblically foretold time that the Mark was to emerge on the world scene and how to have hope if this is all true. [Signs of the End Times](http://www.signs-of-end-times.com/)\n\nThe mark is part of God's end times judgments which many believe that believers in Jesus will not be here for when required as the rapture will have already taken place. More information:\u00a0[https://www.gotquestions.org/difference-Rapture-Second-Coming.html](https://www.gotquestions.org/difference-Rapture-Second-Coming.html)", "link": "https://www.reddit.com/r/MachineLearning/comments/kxkf7h/ai_robot_named_sophia_helping_un_with_global/"}, {"autor": "Wiskkey", "date": "2021-01-14 22:46:05", "content": "[P] Kiri's demo of zero shot -----> image !!!  classification using OpenAI's CLIP (Connecting Text and Images) neural network; you can supply your own -----> image !!!  and labels /!/ Kiri's demo of CLIP: [https://clip.kiri.ai/](https://clip.kiri.ai/).\n\nOpenAI's blog post about CLIP: [https://openai.com/blog/clip/](https://openai.com/blog/clip/).\n\nReddit post about CLIP: [https://www.reddit.com/r/MachineLearning/comments/kr7bp9/r\\_clip\\_connecting\\_text\\_and\\_images\\_from\\_openai/](https://www.reddit.com/r/MachineLearning/comments/kr7bp9/r_clip_connecting_text_and_images_from_openai/).", "link": "https://www.reddit.com/r/MachineLearning/comments/kxgttz/p_kiris_demo_of_zero_shot_image_classification/"}, {"autor": "princealiiiii", "date": "2021-01-14 18:00:40", "content": "[P] DAN Super Resolution - Model of the Day #2 /!/ &amp;#x200B;\n\n[Using the Gradio interface with the Super Resolution model. Tou can try out this interface yourself in the link provided below!](https://i.redd.it/qkln2ubo5cb61.gif)\n\nToday's model will be based on the paper at this arXiv link: [Unfolding the Alternating Optimization for Blind Super Resolution](https://arxiv.org/abs/2010.02631). Repo [here](https://github.com/google-research/google-research/tree/master/slot_attention) and interface shown above [here](https://gradio.app/hub/dawoodkhan82/dan).\n\nSuper resolution ML usually involves two steps: 1) estimating the amount and direction of blur in the input -----> image !!! , aka the blur kernel, and 2) Running a model that can \"undo\" that blur kernel. \n\nInstead of approaching these steps separately, DAN uses an alternating optimization algorithm that accounts for both steps in a single model. Two convolution models are used in conjunction, called the Restorer and the Estimator. The Restorer generates the super resolution image based on predicted kernel, and the Estimator estimates blur kernel with the help of restored SR image. By alternating between these two models repeatedly, DAN creates an end-to-end trainable network. Try it out yourself with in the link above!\n\n\\--------------------------------------\n\nI've been working with a lot of newly researched models lately, and I wanted to share the most interesting models I've worked with here. This is part of a series where I post an interesting model along with a description of the research purpose and an interactive interface generated with Gradio. Previous post (Model of the Day #1) [here](https://www.reddit.com/r/MachineLearning/comments/kmm1ld/p_objectcentric_learning_with_slot_attention/).", "link": "https://www.reddit.com/r/MachineLearning/comments/kxax7w/p_dan_super_resolution_model_of_the_day_2/"}, {"autor": "aselsiriwardena", "date": "2021-04-16 06:57:35", "content": "[P] Simple UI for deep learning model /!/ I'm working on a simple -----> image !!! -to------> image !!!  translation project. I want to create a UI for that model. The project was built using PyTorch.I'm looking for a very simple UI just to demonstrate like selecting an image from the left side and showing the resulting image on the right side.\n\nAny suggestions? Are there any broiler plate codes?", "link": "https://www.reddit.com/r/MachineLearning/comments/mrxrn9/p_simple_ui_for_deep_learning_model/"}, {"autor": "kaiser_17", "date": "2021-04-15 20:13:46", "content": "[D] Features from a pre-trained auto encoder giving poor results /!/ I have a simple doubt in cifar10 classification task. I am using an encoder of a pretrained auto encoder(trained on cifar10) to extract features from -----> image !!!  and then training a separate deep Neural Net on those features .Surprising the results are very bad. Has anyone faced this issue?", "link": "https://www.reddit.com/r/MachineLearning/comments/mrn27m/d_features_from_a_pretrained_auto_encoder_giving/"}, {"autor": "WigglyHypersurface", "date": "2021-04-15 16:24:53", "content": "[DISCUSSION] How do the different versions of the bootstrap work with deep neural networks? /!/ I've been looking into deep learning methods as a possible way of imputing missing data, and this has lead me to some questions about how deep neural networks interact with the varies versions of the bootstrap.\n\nIn a statistical missing data context, the goal is to estimate a posterior predictive distribution for the missing data conditioned on the observed data. You then \"fill in\" the missing data with draws from this distribution, leading to *m* versions of your dataset, where the observed data is the same, but the missing data vary. You do your analysis $m$ times and use some simple formulas to combine the analyses, which gives you a nice clean -----> picture !!!  of how much uncertainty in the missing data reduces your confidence in whatever hypothesis your testing.\n\nOk, now the deep net part. In the statistics literature on missing data, there is this idea of \"properness\" which says that you want your posterior distribution for the missing data to reflect all sources of uncertainty, in the model you used to fill in the missing data. Your model for filling in the missing data either needs to be fully bayesian, or approximate a fully bayesian model if that isn't possible.\n\nThe simplest way to do an approximate proper posterior in missing data world is often to use either a parameteric or nonparametric bootstrap. For parametric bootstrap, you 1) train a model (say, learning mu and sigma in a linear regression), 2) sample predicted values for the outcome variable, (ie make a new outcome variable by sampling from the learned mu and sigma) 3) retrain the model using the sampled values as the new dependent variable, and 4) make whatever predictions/inferences you want from this second model. If you do this a bunch of times you'll aproximate a bayesian posterior. Nonparametric bootstrap you resample you data with replacement and train on the resampled data. Again, repeated many times you'll get an approximate posterior.\n\nSo my questions are, for a deep neural network:\n\n1) What advantages/disadvatages does a deep net using variational bayes versus a deep net using some of the bootstrap have? Are these any known or expected biases when doing either form of bootstrapping with a deep neural network? \n2) Will you cause a bias if you don't train the network from scratch for multiple iterations of the bootstrap? Would it be a problem if, for example, you resample the data, but carry-over the weights from a previous bootstrap iteration? \n3) Is there a good resource on how changing to a fully bayesian deep network changes what sort of choices you should make about dropout, batch normalization, activations, etc?", "link": "https://www.reddit.com/r/MachineLearning/comments/mri8nv/discussion_how_do_the_different_versions_of_the/"}, {"autor": "universome", "date": "2021-04-15 15:14:15", "content": "[P] Aligning Latent and Image Spaces to Connect the Unconnectable /!/ Hi! Wanted to share our latest project on infinite -----> image !!!  generation: [http://universome.github.io/alis](http://universome.github.io/alis)\n\n*Processing video 53apkx1asct61...*\n\nBasically, it works the following way. We put each latent code at some position of the coordinates grid (where image pixels are located) and compute each pixel from an interpolation of the nearby latent codes. During training, we generate frames from random positions of this coordinates grid and feed them to the discriminator. At test time, this allows us to produce images from any position of the infinite plane which stitch seamlessly with one another.\n\nOur generator computes an image through independent patches (like [CocoGAN](https://arxiv.org/abs/1904.00284) does), which is a less extreme version of [INR-GAN](https://arxiv.org/abs/2011.12026)/[CIPS](https://arxiv.org/abs/2011.13775), that have independence at the pixel level. This (+some technical tweaking of the coordinate embeddings) makes it (periodically) spatially equivariant: when you shift the coordinates, the output image shifts accordingly and their pixel values at common coordinates are equal (up to numerical precision), as illustrated below.\n\n&amp;#x200B;\n\n*Processing img 4wbquwfgsct61...*\n\nA surprising thing is that our approach works not only on the dataset of nature landscapes (which has spatially invariant image statistics), but also (to some extent) on LSUN bedroom, which is a very \"difficult\" dataset for infinite image generation because of the walls and close-by objects. They make it hard or impossible to extrapolate an image to the left or to the right since the dataset does not have any pictures with walls or close-by objects in the middle, so the model has nowhere to learn such knowledge from.\n\n&amp;#x200B;\n\n*Processing img ugyrknposct61...*\n\nBesides, we also collected/preprocessed a high quality dataset of 90k landscape images (Landscapes HQ) from Unsplash and Flickr and will release it soon.\n\n**Drawbacks of the approach:**\n\n* Generating patches completely independently limits the generation quality (by \\\\\\~30% in our experiments). Moreover, due to patchwise generation the model learnt to ignore noise injection which is a powerful tool for image editing (this also happens for INR-GAN/CIPS).\n* Not all the datasets are \"connectable\". For LSUN bedroom, it worked only after we filtered away images which do not contain spatially invariant statistics (see Sec 3.3 and Appendix C). For FFHQ or ImageNet, I cannot imagine it to work at all (see the \"study\" on this in tables 3 &amp; 4 in Appendix C)\n\nProject page: [https://universome.github.io/alis](https://universome.github.io/alis)\n\nCode: [https://github.com/universome/alis](https://github.com/universome/alis)\n\nPaper: [https://arxiv.org/abs/2104.06954](https://arxiv.org/abs/2104.06954)", "link": "https://www.reddit.com/r/MachineLearning/comments/mrgrdn/p_aligning_latent_and_image_spaces_to_connect_the/"}, {"autor": "universome", "date": "2021-04-15 13:56:06", "content": "[P] Aligning Latent and Image Spaces to Connect the Unconnectable /!/ Hi! Wanted to share our latest project on infinite -----> image !!!  generation: [https://universome.github.io/alis](https://universome.github.io/alis)\n\n![video](exo3parq5ct61 \"The method works without any conditioning and learns from a dataset of unrelated square images\")\n\nBasically, it works the following way. We put each latent code at some position of the coordinates grid (where image pixels are located) and compute each pixel from an interpolation of the nearby latent codes. During training, we generate frames from random positions of this coordinates grid and feed them to the discriminator. At test time, this allows us to produce images from any position of the infinite plane which stitch seamlessly with one another.\n\nOur generator computes an image through independent patches (like [CocoGAN](https://arxiv.org/abs/1904.00284) does), which is a less extreme version of [INR-GAN](https://arxiv.org/abs/2011.12026)/[CIPS](https://arxiv.org/abs/2011.13775), which have independence at the pixel level. This (+some technical tweaking of the coordinate embeddings) makes it (periodically) spatially equivariant: when you shift the coordinates, the output image shifts accordingly and their pixel values at common coordinates are equal (up to numerical precision), as illustrated below.\n\n&amp;#x200B;\n\n![img](7zhszqjoact61 \"\ud835\udeff is the value of the coordinates shift. Pixel values inside circles are equal for different generations (up to numerical precision)\")\n\nA surprising thing is that our approach works not only on the dataset of nature landscapes (which has spatially invariant image statistics), but also (to some extent) on LSUN bedroom, which is a very \"difficult\" dataset for infinite image generation because of the walls and close-by objects. They make it hard or impossible to extrapolate an image to the left or to the right since the dataset does not have any pictures with walls or close-by objects in the middle, so the model has nowhere to learn such knowledge from.\n\n![img](qqhseppqbct61 \"Results on LSUN bedroom\")\n\nBesides, we also collected/preprocessed a high quality dataset of 90k landscape images (Landscapes HQ) from Unsplash and Flickr and will release it soon.\n\n**Drawbacks of the approach:**\n\n* Generating patches completely independently limits the generation quality (by \\~30% in our experiments). Moreover, due to patchwise generation the model learnt to ignore noise injection which is a powerful tool for image editing (this also happens for INR-GAN/CIPS).\n* Not all the datasets are \"connectable\". For LSUN bedroom, it worked only after we filtered away images which do not contain spatially invariant statistics (see Sec 3.3 and Appendix C). For FFHQ or ImageNet, I cannot imagine it to work at all (see the \"study\" on this in tables 3 &amp; 4 in Appendix C)\n\nProject page: [https://universome.github.io/alis](https://universome.github.io/alis)\n\nCode: [https://github.com/universome/alis](https://github.com/universome/alis)\n\nPaper: [https://arxiv.org/abs/2104.06954](https://arxiv.org/abs/2104.06954)", "link": "https://www.reddit.com/r/MachineLearning/comments/mrf6op/p_aligning_latent_and_image_spaces_to_connect_the/"}, {"autor": "skilled_skinny", "date": "2021-04-15 10:53:08", "content": "[P][R] Need help in Fine grained Classification /!/ I am working on a grocery dataset which has around 10k -----> image !!! s distributed across 250+ classes(Each -----> image !!!  belongs to 1 class). Images are taken in different lighting conditions and different angles. Also there is a problem of class imbalance with 25% classes having less 5 images ( average\\~ 50 images per class).\n\nWhat are different ways to tackle the class imbalance and lighting problems. I was thinking of data-augmentation for class imbalance.\n\nI tried resnet50 on image-net weights added my input/output layer and trained all layers achieved \\~ 90%. Quite surprised and also wanted to learn why did I get such results without data augmentation or any other manipulations.\n\nAlso can someone explain the procedure/steps for fain grained classification problems.", "link": "https://www.reddit.com/r/MachineLearning/comments/mrc6gd/pr_need_help_in_fine_grained_classification/"}, {"autor": "mtreerungroj", "date": "2021-04-15 10:12:30", "content": "\ud83d\udc31 I just used StyleGAN2 to generate a cat -----> image !!!  and sell it as an NFT, here is behind the scene.", "link": "https://www.reddit.com/r/MachineLearning/comments/mrbr4w/i_just_used_stylegan2_to_generate_a_cat_image_and/"}, {"autor": "poriporiman", "date": "2021-04-15 08:55:54", "content": "[D]How to get rid of corona, cancer and disability /!/ Let artificial intelligence recognize the two images.\n\nNext, the combined -----> image !!!  of the two -----> image !!! s is recognized. Have the image named by humans or artificial intelligence.\n\nThen something strange happens.", "link": "https://www.reddit.com/r/MachineLearning/comments/mrateb/dhow_to_get_rid_of_corona_cancer_and_disability/"}, {"autor": "Environmental_Age950", "date": "2021-04-15 08:44:55", "content": "[P] Finding clusters in a list of 2d NumPy arrays /!/ I have obtained a DataFrame with 2d Numpy arrays of SIFT keypoint descriptors. I want to cluster the elements in the DataFrame based on their descriptors to find similar-looking -----> image !!!  patches. Any tips are welcome; I'm stuck; Sklearn k-means did not provide any solutions.", "link": "https://www.reddit.com/r/MachineLearning/comments/mraoly/p_finding_clusters_in_a_list_of_2d_numpy_arrays/"}, {"autor": "nannigalaxy", "date": "2021-05-31 16:07:45", "content": "[D] What is considered samples in Tensorflow object detection API? /!/ In -----> image !!!  classification, it is obvious that -----> image !!! s are considered as samples but what is considered as samples in Tensorflow object detection API? Are the samples images or the individual objects in the images?\n\nif an image contains 10 objects, should I consider 10 objects as training samples or just a single image?\n\nI need to know this to effectively set batch size and steps for training.", "link": "https://www.reddit.com/r/MachineLearning/comments/np6w14/d_what_is_considered_samples_in_tensorflow_object/"}, {"autor": "EitherSuggestion", "date": "2021-05-31 14:31:54", "content": "[Research] Looking for an article or paper similar to Data Augmentation by Pairing Samples for Images Classification /!/ Hello everyone, \n\nI remember an article from more than a year ago which trained a neural network on a single or a handful of images.  \nThe main idea (I am basing this on MNIST as I remember it from the article) was to build new pictures that were made up of the original training images. One training -----> image !!!  for example was 30% \"three\" and 70% \"five\". By using this method you would be able to learn MNIST with 5 images or less if you include more classes in a single training image.   \n\n\nI think this [https://arxiv.org/pdf/1801.02929.pdf](https://arxiv.org/pdf/1801.02929.pdf) is doing pretty much what I am looking for just not for MNIST.  \n\n\nIf anyone can point me in the right direction that would be great!", "link": "https://www.reddit.com/r/MachineLearning/comments/np4ql9/research_looking_for_an_article_or_paper_similar/"}, {"autor": "kailin27", "date": "2021-05-31 11:15:28", "content": "[D] Randomized -----> image !!!  augmentation during training? /!/ TL;DR: Is it a good idea to apply random augmentation during training (= every image is only seen once) instead of training multiple epochs on the same fixed training set of images randomly augmented beforehand?\n\nSo I'm training an OCR classifier to recognize Chinese characters of a specific font in a specific newspaper. I have a very similar font as a .ttf file but very little labeled data (1000 manually cropped characters from the newspaper which I'm using as a validation set). So I generated a png image of about 4000 glyphs (the ones I want the OCR classifier to be able to recognize) from the font file and applied different randomized augmentation to it: random translation, different intensities of random noise, random patches of increased brightness and morphological opening/closing after random noise application. In all combinations I get 36 augmented images from every original glyph image = a fixed synthetic training set of 36x4000 images.\n\nTraining GoogleNet on this yields about 20% accuracy when evaluating on the \"real-life\" validation set (not good but it's a start), with the training error going down to almost 0. So I was wondering if, since the augmentation methods are all based on randomness, it might make sense to not have the model overfit on a fixed selection of them, but instead take the original 4000 glyph pngs as a training set and apply a randomly chosen combination of the augmentation methods described above as I load the image. If I'm not mistaken, it will make it more of an online learning approach. It makes a lot of sense to me but I can't find any papers on this. With image augmentation being so wide-spread and much talked-about, why does everybody do it beforehand and train on a fixed data set, trying to combat overfitting afterwards, instead of serving any of an almost infinite number of combinations of different randomized augmentation methods during training, improving generalization and avoiding overfitting? Is it for reproduciblility?", "link": "https://www.reddit.com/r/MachineLearning/comments/np0zsv/d_randomized_image_augmentation_during_training/"}, {"autor": "Ok-Duty-3411", "date": "2021-05-31 10:39:09", "content": "[P] Human Gender Recognization /!/ I'm trying to build an app that solely works on the genders of my uses. The problem is I need to identify my gender of my user. It's easy to ask them. But I want to verify very strictly (ie) every 30 minutes through real-time video or -----> Image !!!  or some other means. Are there any ideas/suggestions of what To implement? It may or may not involve machine learning. Any optimal solution is Okay. I know I cannot depend solely on the face recognization by ml which is biased.\n\nEdit: I'm not meant to discriminate or offend any gender. It's solely for the project. Hope you understand. Just how to identify a gender either visually or some other ways.", "link": "https://www.reddit.com/r/MachineLearning/comments/np0f1x/p_human_gender_recognization/"}, {"autor": "projekt_treadstone", "date": "2021-04-29 23:24:04", "content": "[D] Prototypical network in the medical domain /!/ Is there any existing project work/implementation of prototypical network-based meta-learning for medical -----> image !!! . As medical image are heavier to load computationally than imagenet and I would like to know how they handled large pixel medical data. As when I resize a large medical image  as per standard prototypical architecture there is large drop in accuracy . With a large resize value it's not fitting in GPU to train.", "link": "https://www.reddit.com/r/MachineLearning/comments/n1h4dl/d_prototypical_network_in_the_medical_domain/"}, {"autor": "Defiant_Role", "date": "2021-04-29 14:55:52", "content": "Send -----> image !!!  via spikes to FPGA using UART /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/n165mr/send_image_via_spikes_to_fpga_using_uart/"}, {"autor": "opensourcecolumbus", "date": "2021-04-29 02:46:55", "content": "[Project] Framework to build AI powered search with just 7 lines of code. Supports semantic, text, -----> image !!! , audio &amp; video search /!/ Before [this open-source project(Jina)](https://github.com/jina-ai/jina), one has to depend on closed source solutions to implement neural search. With Jina helps you build our own semantic search engine that can\n\n* Text to text search\n* Image to image search\n* Text to image search\n* Audio to audio search\n* Text to audio search\n* Text to video search\n\n**Being Open-Source(Apache 2.0 License),** you can modify it, host it on your infrastructure and be in complete control of your data. \n\n**How is it different than Solr/Elasticsearch?**\n\n* Solr/ElasticSearch implements Symbolic Search(rules-based based)\n* Jina implements Neural Search(based on pre-trained deep learning models) which results in better semantic search and new capabilities such as cross-modal(e.g. text to video) and multi-modal(e.g. text to text+video+audio+..) search\n\nAppreciate your feedback/questions", "link": "https://www.reddit.com/r/MachineLearning/comments/n0v5jy/project_framework_to_build_ai_powered_search_with/"}, {"autor": "butenkan", "date": "2021-01-30 04:35:17", "content": "[P] CNN backbone for facial expression recognition /!/  What would be a good CNN backbone for **real-time** facial expression classification? only 5 classes (anger/disgust/smile/tongue-out/neutral). Input images are not very varied in terms of illumination, subject and field of vision (-----> camera !!!  is mounted to the head and only sees mouth portion of the face).\n\nI suspect a regular 6 layer CNN with maxpooling and softmax would probably do the job. But my dataset is extremely small and I hope to do transfer learning.", "link": "https://www.reddit.com/r/MachineLearning/comments/l8dvtd/p_cnn_backbone_for_facial_expression_recognition/"}, {"autor": "ShortSPY", "date": "2021-01-30 02:15:55", "content": "[P] Food Classification with a CNN on iOS /!/ Hello, I recently built a food classification app based off of a CNN for the Food-101 dataset. Disclaimer: I am no UI/UX guru but it works, I always have a blast bringing -----> image !!!  classifiers over to iOS and playing around with them.\n\n\nApp: https://apps.apple.com/us/app/obviously-food/id1534447041\n\nCNN Code: https://github.com/JBall1/Food101-CNN\n\nData: https://www.kaggle.com/dansbecker/food-101\n\nTerrible paper: https://arxiv.org/abs/2012.03170", "link": "https://www.reddit.com/r/MachineLearning/comments/l8bak3/p_food_classification_with_a_cnn_on_ios/"}, {"autor": "bhatta90", "date": "2021-01-29 23:31:47", "content": "[D] Links, Materials requested for binary classification w cropping /!/  Hello guys, I am trying to build a classification (binary model), from where I have videos (I have extracted frames---- they are like images)and I want to crop the part of the -----> picture !!! . So in a frame(image), I have 2 dogs or cats, I want to crop both of them, I have their centers in the form of CSV files with me. Can you please tell me how to do that? Then, I want to classify them as dogs and cats. \n\n Can you please help me to get a link(GitHub) where I can headstart the programming? Please help!", "link": "https://www.reddit.com/r/MachineLearning/comments/l87to6/d_links_materials_requested_for_binary/"}, {"autor": "Dare_Complete", "date": "2021-01-29 22:57:52", "content": "[R] Bottleneck Transformers for Visual Recognition /!/ Paper: [https://arxiv.org/abs/2101.11605](https://arxiv.org/abs/2101.11605)\n\nCode: [https://github.com/lucidrains/bottleneck-transformer-pytorch](https://github.com/lucidrains/bottleneck-transformer-pytorch)\n\nAbstract: We present BoTNet, a conceptually simple yet powerful backbone architecture that incorporates self-attention for multiple computer vision tasks including -----> image !!!  classification, object detection and instance segmentation. By just replacing the spatial convolutions with global self-attention in the final three bottleneck blocks of a ResNet and no other changes, our approach improves upon the baselines significantly on instance segmentation and object detection while also reducing the parameters, with minimal overhead in latency. Through the design of BoTNet, we also point out how ResNet bottleneck blocks with self-attention can be viewed as Transformer blocks. Without any bells and whistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance Segmentation benchmark using the Mask R-CNN framework; surpassing the previous best published single model and single scale results of ResNeSt evaluated on the COCO validation set. Finally, we present a simple adaptation of the BoTNet design for image classification, resulting in models that achieve a strong performance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to 2.33x faster in compute time than the popular EfficientNet models on TPU-v3 hardware. We hope our simple and effective approach will serve as a strong baseline for future research in self-attention models for vision.\n\nAuthors: [Aravind Srinivas](https://arxiv.org/search/cs?searchtype=author&amp;query=Srinivas%2C+A), [Tsung-Yi Lin](https://arxiv.org/search/cs?searchtype=author&amp;query=Lin%2C+T), [Niki Parmar](https://arxiv.org/search/cs?searchtype=author&amp;query=Parmar%2C+N), [Jonathon Shlens](https://arxiv.org/search/cs?searchtype=author&amp;query=Shlens%2C+J), [Pieter Abbeel](https://arxiv.org/search/cs?searchtype=author&amp;query=Abbeel%2C+P), [Ashish Vaswani](https://arxiv.org/search/cs?searchtype=author&amp;query=Vaswani%2C+A)", "link": "https://www.reddit.com/r/MachineLearning/comments/l86zby/r_bottleneck_transformers_for_visual_recognition/"}, {"autor": "Sagoito", "date": "2021-01-29 22:50:50", "content": "-----> Image !!!  denoising dataset /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/l86szu/image_denoising_dataset/"}, {"autor": "tttzof351", "date": "2021-01-28 21:37:53", "content": "Typical -----> photo !!!  /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/l79ntp/typical_photo/"}, {"autor": "pavanayi007", "date": "2021-07-30 03:10:04", "content": "[D] How do we measure stability of deep learning models? /!/ Hi everyone!\n\nI have been recently looking into stability of deep learning models when deployed in production. I have a deep learning model for biomedical -----> image !!!  classification with reasonably good accuracy on validation set. But my model predictions change drastically if I alter a few pixels in the input image. I know training with data augmentation can significantly improve model stability, but I was wondering whether there are any standard methods to assess stability of a trained model.", "link": "https://www.reddit.com/r/MachineLearning/comments/ouchxz/d_how_do_we_measure_stability_of_deep_learning/"}, {"autor": "celviofos", "date": "2021-07-29 16:11:09", "content": "[D] Visualizing StyleGAN feature maps /!/  \n\nHi,\n\nI was wondering how one can visualize StyleGAN feature maps as an RGB -----> image !!!  as they do in the Alias Free GAN paper ([https://nvlabs.github.io/alias-free-gan/](https://nvlabs.github.io/alias-free-gan/)). I don't quite understand how they manage to get an RGB representation from n-dimensional feature maps.\n\nThanks!\n\n&amp;#x200B;\n\nhttps://preview.redd.it/drab6v4re6e71.png?width=1405&amp;format=png&amp;auto=webp&amp;s=129469fc14cee2ad00554d040b9d7715d64ff270", "link": "https://www.reddit.com/r/MachineLearning/comments/ou0ik6/d_visualizing_stylegan_feature_maps/"}, {"autor": "RNARNARNA", "date": "2021-07-29 13:33:14", "content": "[P] Architecture for before to after -----> image !!!  translation? /!/ Hello!\n\n I have a dataset of \"before/after\" image pairs, and I'd like to create a system that can generate a predicted \"after\" image given a \"before\" image. I've been looking into GAN-based architectures for image-to-image translation, but am not sure that's the correct approach. Any reading recommendations/other tips?\n\nThanks in advance!", "link": "https://www.reddit.com/r/MachineLearning/comments/otxkcc/p_architecture_for_before_to_after_image/"}, {"autor": "SteakLady", "date": "2021-07-29 00:15:02", "content": "[D] Any video colorization AIs that let you pick the color of specific things? /!/ I know Deoldify, but that uses machine learning to pick realistic colors. I\u2019m working on a surrealistic project so I need more freedom with the coloring. For example, some things should remain black and white in the -----> film !!! , and there are unnatural skin tones, etc.", "link": "https://www.reddit.com/r/MachineLearning/comments/otmhgb/d_any_video_colorization_ais_that_let_you_pick/"}, {"autor": "aDutchofMuch", "date": "2021-07-28 20:15:42", "content": "[R] (x-post) A fun video of my colleagues dissertation work - how to do large-scale -----> image !!!  retrieval on small objects! He's very proud that it got accepted to T-IP", "link": "https://www.reddit.com/r/MachineLearning/comments/oti0hr/r_xpost_a_fun_video_of_my_colleagues_dissertation/"}, {"autor": "warmspringwinds", "date": "2021-07-28 18:10:10", "content": "[D] New work on unsupervised segmentation: train a model to segment my little pony cartoon pictures without any supervision. /!/ In this recent work authors propose a way find semantically meaningful regions in the -----> image !!!  that mostly coincide with regions defined by human.\n\nHave a look at the results:\nhttps://twitter.com/ak92501/status/1420181693131014146?s=20", "link": "https://www.reddit.com/r/MachineLearning/comments/otfiex/d_new_work_on_unsupervised_segmentation_train_a/"}, {"autor": "-Apezz-", "date": "2021-07-28 16:19:56", "content": "[D] Handling mixed data with different algorithms /!/ Hi! I\u2019m a rising high school senior who is really interested in machine learning. I\u2019m a little new to this, so please go easy if I say something stupid.\n\nI\u2019m part of a research lab where I am attempting to predict a categorical variable through Breast Cancer MRI data. Specifically, it\u2019s pathological complete response. \n\nContext aside, the data includes -----> image !!!  and non-imaging data. The sample size is small (~110 patients, 3 time points for MRI scan). XGBoost is a machine learning algorithm known to perform well even with small sample sizes. With the non-imaging data, it does perform really well.\n\nCNNs are known to be really well at feature extraction and classification problems in general. These *should* do well with the image data.\n\nI want to keep my question general: is it possible to combine these two algorithms to leverage the strengths of both? I have explored the idea of transfer learning, but it\u2019s not quite what I\u2019m looking for. \n\nThank you all for your time!", "link": "https://www.reddit.com/r/MachineLearning/comments/otd99z/d_handling_mixed_data_with_different_algorithms/"}, {"autor": "verilog10", "date": "2021-08-30 17:07:01", "content": "[DISCUSSION] Bike fitting with Machine Learning /!/ Hi everyone!\n\nIf this is not the right place to ask this question, please let me know where it should be posted.\n\nI work on a project during my studies in Image Processing. The purpose of the project is to get user's video/-----> image !!!  of himself riding a bike (from the right side of him and not front) as input and to determine whether his cycling configurations are correct. Basically we need to tell the user if he should change his bike configurations or not (and maybe how).\n\nFor the purpose of the project, we focus on \"field bikes\" and \"Cross Country\" riding type - most common riding type and bike type for amateurs.\n\nWe successfully extracted the coordinates of user's skeleton using OpenCV and calculated the angles of specific body location (such as knees, arms, back). Now, the part that we are stuck on is to figure the \"correct\" bike configurations, based on this data.\n\nSince we can't point on the \"correct\" angles/coordinates/other parameters, we turn to Machine Learning. We would like to create a dataset and run some ML algorithm (such as RandomForest) to fill the labels. What we had in mind is to create a dataset in which the coordinates and the angles are the features and for each such angle-feature we will have a angle-label that represents \"how good is the angle from 1-5\".\n\nBut I really not sure if it's the right strategy to \"attack\" this task. Since we are not ML experts, if possible, I hope to hear some suggestions on how the dataset should look like or any other ideas (maybe which ML algorithm to try out). Thank you!", "link": "https://www.reddit.com/r/MachineLearning/comments/pemd9p/discussion_bike_fitting_with_machine_learning/"}, {"autor": "techsucker", "date": "2021-08-30 07:15:44", "content": "[R] NVIDIA and Tel Aviv Researchers Propose \u2018StyleGAN-NADA\u2019, A Text-Driven Method That Converts a Pre-Trained AI Generator to New Domains Using Only a Textual Prompt and No Training Data /!/ GANs have revolutionized the -----> image !!!  generation process, allowing for better results in classification and regression tasks. They can capture distribution of images through their semantic-rich latent space making it more efficient than traditional methods such as autoencoders or generative adversarial networks.\n\nAlthough GANs have shown incredible results, their ability to generate high-quality images is limited by the amount of training data available. For example, it would be challenging to train a model for an artist that has not been recognized yet or when there isn\u2019t sufficient information about what this imaginary scene looks like.\n\n[**5 Min Read**](https://www.marktechpost.com/2021/08/30/nvidia-and-tel-aviv-researchers-propose-stylegan-nada-a-text-driven-method-that-converts-a-pre-trained-ai-generator-to-new-domains-using-only-a-textual-prompt-and-no-training-data/) **|** [**Paper**](https://arxiv.org/pdf/2108.00946.pdf) **|** [**Project**](https://stylegan-nada.github.io/) **|** [**Code**](https://github.com/rinongal/StyleGAN-nada)\n\n&amp;#x200B;\n\n*Processing video taqgoule4gk71...*", "link": "https://www.reddit.com/r/MachineLearning/comments/ped3ar/r_nvidia_and_tel_aviv_researchers_propose/"}, {"autor": "adrian2lee", "date": "2021-08-29 18:53:23", "content": "[D] One dimensional visual localization for motorized TV mount /!/ I'm adding linear actuators to my tv mount to turn it . I'm currently testing opencv face detection models and facenet-python and getting reasonable results of finding is faces that are either sitting at the sofa or at the table. My next problem is how to \"localize\" the position of the tv mount it self. I want to have two specific positional settings; facing straight inline with the tv stand or facing table. Actuators don't have encoders. I could use IR sensor to calculate the position in a away, but would be intrigued to use vision only approach.\n\nI've been testing openCV ORB for feature detection with hit and miss results depending on lighting conditions. \n\nNext idea is to create custom \"landmark\" object detection model and try to match that to certain pixel position on -----> camera !!! .\n\nI'm wondering what kind of ideas you guys have, how would you solve this?", "link": "https://www.reddit.com/r/MachineLearning/comments/pe1851/d_one_dimensional_visual_localization_for/"}, {"autor": "gostar2000", "date": "2021-08-29 16:12:18", "content": "[D] Need help with -----> image !!!  dataset. /!/  Hey, hope everyone's doing well.\n\nI am working on an image classifier that detects whether a person is using mobile phones or not. However, I am having trouble getting enough images to train the model. As of now, I only have about 150 images per category. It would be so helpful if any of you could tell me some places to get some images. [This is what I'm aiming for.](https://drive.google.com/drive/folders/1vjCWjRh7jv5VCfRUtXqqU5DnfDoLeccY?usp=sharing)", "link": "https://www.reddit.com/r/MachineLearning/comments/pdy5d4/d_need_help_with_image_dataset/"}, {"autor": "gostar2000", "date": "2021-08-29 16:10:09", "content": "[HELP] Need help with -----> image !!!  dataset. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pdy3vx/help_need_help_with_image_dataset/"}, {"autor": "Qosarom", "date": "2021-08-29 15:01:39", "content": "[D] Looking for versatile \"AI cartoonizer\" /!/ So  I'm playing around with some AI cartoonizer apps like \"Cartoon 3D  style\" and \"Voila AI artist\". They're far better than the basic  \"-----> photo !!! -to-sketch\" effects I can achieve on GIMP and other -----> photo !!!  editing  software, but I'm struck by there limitations. In particular I'm  bothered that they all crop down the photos to a single face instead of  keeping the whole picture. The effect I want to achieve is to cartoonize  people's faces in complete photos, while keeping the background  unchanged.\n\nDoes anyone know about alternatives to these limited apps (apps/scripts/programs...)?", "link": "https://www.reddit.com/r/MachineLearning/comments/pdwtxz/d_looking_for_versatile_ai_cartoonizer/"}, {"autor": "FrameCapable1412", "date": "2021-08-29 13:38:36", "content": "[D] Any suggestions for -----> Image !!!  filters/ML Models to convert image 1 to image 2?", "link": "https://www.reddit.com/r/MachineLearning/comments/pdvdzg/d_any_suggestions_for_image_filtersml_models_to/"}, {"autor": "FrameCapable1412", "date": "2021-08-29 13:37:41", "content": "Any suggestions for -----> image !!!  filters/ML models to convert -----> image !!!  1 to -----> image !!!  2?", "link": "https://www.reddit.com/r/MachineLearning/comments/pdvdgq/any_suggestions_for_image_filtersml_models_to/"}, {"autor": "Eleonora467", "date": "2021-07-16 03:13:37", "content": "[P] .py gui to.exe(Tkinter) /!/ I m converting gui file(tkinter) to exe...it contains model file,-----> image !!!  file..model is downladed from colab..I am using auto py to exe...but when ever i convert it ...it gives lots of warning related to hidden import tensorflow...kindly can anyone help me?", "link": "https://www.reddit.com/r/MachineLearning/comments/ol8cxf/p_py_gui_toexetkinter/"}, {"autor": "Dick_reaper", "date": "2021-07-15 16:18:36", "content": "I need help in figuring out which language to learn. /!/ Lets get this straight. I want to make a piano without a piano.\n\nI want to make it in such a way that i will have a -----> camera !!!  watching the length of my table and when i will put my finger(s) on any part of the table(under the surveillance of -----> camera !!! ) the camera will send this video into the computer where my custom made program will process the position of my finger and will play a piano note in output. That way i will just have to play an imaginary piano on the table and real piano sound will play. It will be so fricking cool tbh. \n\nIt will be like that childhood dream where we play imaginary piano and imagine it to make sound but it will be reality this time.\n\nThe problem is i don't know which computer language i have to learn in order to make this custom program. Please help me out a bit.", "link": "https://www.reddit.com/r/MachineLearning/comments/okw67v/i_need_help_in_figuring_out_which_language_to/"}, {"autor": "Woodhouse_20", "date": "2021-07-14 18:36:29", "content": "[D] Two months in isolation, best overall book I can take to not get rusty? /!/ I\u2019m leaving to go to a wilderness rehab for health issues. Since Data Science/ML is my field, I don\u2019t want to get back to interviewing having gotten rusty. Is there a book that would cover a bit of everything? I\u2019m particularly interested in -----> image !!! /object recognition as well as GANs. I can probably only take a single book due to weight limits (have to carry everything on my back) and I can\u2019t bring any technology (so no kindle/iPads with a solar charger). Any recommendations?", "link": "https://www.reddit.com/r/MachineLearning/comments/okamh4/d_two_months_in_isolation_best_overall_book_i_can/"}, {"autor": "vsamma", "date": "2021-06-15 22:05:29", "content": "[D] Is there a difference between composite class prediction or merging multiple classes into one? /!/ Hi all, so i\u2019m not sure if it\u2019s the right place to ask this but I don\u2019t know any better places for it either. Might be a bit silly as I\u2019m not a machine learning engineer myself.\n\nWe are currently annotating data and building models for object detection from images and our solution at the moment is that first we would want to predict the object itself and then its different properties like size, color, material etc, depending on the domain of the objects.\n\nAnd we recently had a bit of a disagreement, which one would be more accurate:\n\n1. Having the objects\u2019 classes as high level and generic as possible, like \u201ctrousers\u201d and \u201cdresses\u201d for garments or \u201ctables\u201d and \u201cchairs\u201d for furniture and then have \u201cstyle/type\u201d as a separate property with values like \u201csuit pants\u201d and \u201cjeans\u201d and \u201csundress\u201d or \u201coffice table\u201d, \u201cdining table\u201d, \u201ccoffee table\u201d.\n\n2. Having the objects already more specific, so basically already defining the style/type in the first level of labels by just having \u201cdining table\u201d and \u201ccoffee table\u201d as object labels and then predicting properties like material and leg count etc.\n\n\nOne valid argument for the second option was that object detection happens based on the whole -----> image !!!  but as this process also selects the area of the object, the property prediction only works with that area. So for example if you can\u2019t see from the table itself that whether it\u2019s an office or a dining table, it shouldn\u2019t be a property but those should be separate objects because during object detection the model is working with the whole image and can consider the context, like the table being inside the kitchen for example (although i wouldn\u2019t know how the model would know about this context if this isn\u2019t present in the training data in any shape or form).\n\nBut other than that, from the annotation point of view, it made more sense to me to have the object classes as generic as possible and separate everything that somehow describes this specific object in a separate property.\n\nI was told this composite prediction might grant lower accuracy because you need to do 2 predictions instead of one.\nBut for me this sounds like BS because we would still have different properties that need second prediction.. otherwise we\u2019d just create all combinations of properties to form a single level set of classes, for example merge \u201cgender\u201d and \u201cmaterial\u201d as well while we\u2019re at it..\n\u201cMale Cotton Suit Pants\u201d, \u201cFemale cotton suit pants\u201d etc..\n\nI don\u2019t want to believe this separation if objects and properties would somehow cause lower accuracy during prediction.\n\nAny professional experienced feedback on this topic :)?", "link": "https://www.reddit.com/r/MachineLearning/comments/o0puej/d_is_there_a_difference_between_composite_class/"}, {"autor": "ZFCD", "date": "2021-06-15 20:28:29", "content": "-----> Image !!!  enhancement with reference input? [D] /!/  Hello, I'm not sure if this is the best place to ask this question, so apologies if not.\n\nIs there a freely available tool that can upscale/enhance an image if I provide it with a higher resolution original? I'm animating a still image with Siarohin's first order motion model, but I'd like to restore some of the resolution that's been lost from downscaling\n\nAny help would be appreciated\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/o0nt3l/image_enhancement_with_reference_input_d/"}, {"autor": "ZFCD", "date": "2021-06-15 20:24:39", "content": "-----> Image !!!  enhancement with reference input /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/o0nq4f/image_enhancement_with_reference_input/"}, {"autor": "ieee8023", "date": "2021-06-15 14:13:45", "content": "[R] Gifsplanation via Latent Shift: A Simple Autoencoder Approach to Counterfactual Generation for Chest X-rays /!/ To be presented at MIDL2021!\n\n**Motivation**: Traditional -----> image !!!  attribution methods struggle to satisfactorily explain predictions of neural networks. Prediction explanation is important, especially in medical imaging, for avoiding the unintended consequences of deploying AI systems when false positive predictions can impact patient care. Thus, there is a pressing need to develop improved models for model explainability and introspection.\n\n**Specific problem**: A new approach is to transform input images to increase or decrease features which cause the prediction. However, current approaches are difficult to implement as they are monolithic or rely on GANs. These hurdles prevent wide adoption.\n\n**Our approach**: Given an arbitrary classifier, we propose a simple autoencoder and gradient update (Latent Shift) that can transform the latent representation of a specific input image to exaggerate or curtail the features used for prediction. We use this method to study chest X-ray classifiers and evaluate their performance. We conduct a reader study with two radiologists assessing 240 chest X-ray predictions to identify which ones are false positives (half are) using traditional attribution maps or our proposed method.\n\n**Results**: We found low overlap with ground truth pathology masks for models with reasonably high accuracy. However, the results from our reader study indicate that these models are generally looking at the correct features.\nWe also found that the Latent Shift explanation allows a user to have more confidence in true positive predictions compared to traditional approaches (0.15\u00b10.95 in a 5 point scale with p=0.01) with only a small increase in false positive predictions (0.04\u00b11.06 with p=0.57).\n\nMethod overview: https://i.imgur.com/GxCjcjF.gif\n\nDemo code:\nhttps://colab.research.google.com/github/mlmed/gifsplanation/blob/main/demo.ipynb\n\nPaper: https://openreview.net/forum?id=rnunjvgxAMt\n\nRidiculous promo video: https://www.youtube.com/watch?v=yWQ_JGHwqQw", "link": "https://www.reddit.com/r/MachineLearning/comments/o0fci1/r_gifsplanation_via_latent_shift_a_simple/"}, {"autor": "SQL_beginner", "date": "2021-06-15 13:51:53", "content": "[D] Theoretical Performance of Machine Learning Algorithms on Imbalanced Datasets /!/ Suppose you are working on a supervised binary classification task. You have patient medical information (e.g. age, weight, gender, height, blood pressure, etc) and whether they have a certain disease or not (this is the response variable, \"yes\" or \"no\"). Let's imagine that determining if patients have this disease is time consuming and costly - so a machine learning approach is being considered.\n\nLet's assume that this disease is very rare. In your data set, only 1% of patients have this disease. Thus, the dataset is imbalanced. \n\nIntuitively, we know that any machine learning algorithm trained on this data will likely perform poorly. That is, the performance will likely be deceptive: you might get an accuracy of 99%, but misclassify all of the patients who have the disease. \n\nMathematically speaking: is there any mathematical explanation for this very logical concept?\n\n E.g. if only study 1 hour for a chemistry exam, I might only learn how to solve 2-3 types of problems - thus, on a true/false style chemistry exam, there will be many questions that I don't know how to answer because I never saw them before, and I will be likely to perform badly on material that I have not prepared for. Do machine learning models work the same way?\n\nFor popular algorithms like neural networks, xgboost and random forest - can it be shown that for classification problems, you need a minimum number of observations or a minimum proportion of the minority class to probabilistically achieve a certain model performance? \n\nOn a more abstract side, I have heard that researchers are interested in trying to make machine learning models generalize without seeing thousands and thousands of examples. E.g. a 5 year old child can \"learn\" what is an \"elephant\" after seeing a few -----> picture !!! s of an elephant (e.g. it's perfectly reasonable to expect that a young child would see a -----> picture !!!  of the cartoon character Dumbo and identify Dumbo as an elephant after coming back from a zoo), but a machine learning algorithm would likely need thousands and thousands of -----> picture !!! s of elephants (and likely require to see the same -----> picture !!! s upside down, inverted, with added noise, different color scheme, etc) prior to be able to generalize and learn the concept of an elephant. Perhaps the same analogy applies to machine learning models struggling to correctly classify patients with a rare disease, since there are so few of them?\n\nDoes the above concept have anything to do with the \"bias-variance tradeoff\"? Or is it just logic - if there is not enough variability and information within the data, the machine learning model just learns the \"noise\" within the dataset? I am really curious to see if such a threshold for measuring \"minimum level of variability within the data\" has ever been studied?\n\nPS: in a 1 dimensional sense, on a number line, if you have a \"point\" at 3 and another \"point\" at 5 - you could consider all inferences outside of 3 and 5 as \"extrapolation\" and all inferences between 3 and 5 as  \"interpolation\". When dealing with higher dimensional data, could you simply consider observations from the test set that have a smaller euclidean distance to other observations from the training set as \"interpolation\" and observstions that are farther away as \"extrapolation\"? In reality, can you just consider all prediction as extrapolation - small scale extrapolation for closer points, large scale extrapolation for further points?\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/o0euxg/d_theoretical_performance_of_machine_learning/"}, {"autor": "CaptainI9C3G6", "date": "2021-06-15 09:07:13", "content": "[D] tips for quick -----> image !!!  tagging? /!/ Hi, I'm currently using vott to tag objects for an image detection model. Obviously the more images the better, and it's better if they're more accurate.\n\nI'm currently using a laptop touch pad for this. I've also tried using the touch screen, a wacom tablet with pen and an iPad, but settled on the touch pad as the most efficient, and simply tagging with rectangles.\n\nDoes anyone have any suggestions for how to further improve efficiency? Is it worth using a pen on my laptop screen or iPad screen?\n\nAlso when does it become \"worth it\" to use polygons instead of rectangles? Most of my objects are rectangular, but some are triangles or other polygons.\n\nAny other tips are appreciated!", "link": "https://www.reddit.com/r/MachineLearning/comments/o09y2g/d_tips_for_quick_image_tagging/"}, {"autor": "mattroos", "date": "2021-09-14 17:12:59", "content": "[D] What are some loss functions used for comparing ratios of positive numbers? /!/ I'm  training a neural network model tasked to estimate the \"size\" of   particular class of object in an -----> image !!! . The units of the size are not   important--I only care that if the object \"appears\" twice as large in   the image, then the estimated size will also be twice as large (or maybe   (2 x size)\\^2 or (2 x size)\\^3 if considering \"area\" or \"volume.\"\n\nTo  train the model I'm creating a batch of images and then resizing them  (modest crop of original image, then resize back to size of  original  image) two times, with random crop/resizing factors, **s1**  and **s2**. The model then produces its size estimates, **p1** and **p2**, for the image pairs.\n\nIf the model is doing a good job, then the ratio of the predictions, **rp = p1/p2** should be equal or close to the ratio of the actual scaling factors, **rs = s1/s2**. So, I need a loss that is a function of **rp** and **rs**, and gives a lower loss score as those two ratios become closer in value.\n\nMy  instinct is that something like the MSE between those ratios may not be  the best loss function. Perhaps something with log scaling would be  superior? In conjunction with this, what might be a good activation   function for the output layer of the model? Perhaps sigmoid or  exponential, but likely the best choice depends on the loss function  itself.\n\n**What loss functions (if any) are commonly used for such a situation?**", "link": "https://www.reddit.com/r/MachineLearning/comments/po73ja/d_what_are_some_loss_functions_used_for_comparing/"}, {"autor": "pure_x01", "date": "2021-06-30 20:43:33", "content": "[D] Would it be possible to do auto bubling using machine learning? /!/ I stumbled upon /r/bubling (nsfw) which is a technique where you take a -----> photo !!!  and hide parts of it under bubbles so that only skin is visible and no clothes. The brain will extrapolate nuidity. In that subreddit there are sometimes both original and bubbled photo. Would it be possible to train machine learning to learn how to automatically buble any photo?  The sub is nsfw", "link": "https://www.reddit.com/r/MachineLearning/comments/ob64k7/d_would_it_be_possible_to_do_auto_bubling_using/"}, {"autor": "kmanchel", "date": "2021-06-30 15:14:32", "content": "[D] Contemplating life through machine learning /!/ Having initially ventured into machine learning to make a living out of it, I realized my most valued lessons from machine learning are concepts that allow me to make better sense of the world (or at least try to). \n\nHere is an example:\n\nLike many people, I've never paid attention the concept of privilege (and lack there of) in life. Especially in terms of socioeconomic status, I had always thought that hard work is THE WAY to climb the socioeconomic ladder. Though I knew privilege had some role to play, I've always underplayed it until I came across the concept of model initialization. Whether this is in -----> image !!!  classification, language models, or reinforcement learning, I've come across a whole slew of studies exploring and proving the importance initialization in the algorithm's ability to guide a model to convergence. \n\nWhile I acknowledge that human beings are much more complex (of a \"higher dimensionality\") than any given ML model, drawing parallels from studies of model initialization has made me appreciate the privilege that I've been given in life a lot more than I ever did. As I watch myself ascend this socioeconomic ladder (converging through life), I cannot help but be thankful of my privilege (aka my initial state) and aspire to provide others with the same. \n\nIf anyone else has come across such thoughts from concepts you learned through ML, please share!", "link": "https://www.reddit.com/r/MachineLearning/comments/oazdq4/d_contemplating_life_through_machine_learning/"}, {"autor": "djamesmck57", "date": "2021-06-30 14:38:27", "content": "[Project] Enable your webcam to guard your posture with Zen\ud83e\uddd8\ud83c\udffd\u200d\u2640\ufe0f /!/ \ud83d\udc4bFam, [Daniel](https://www.linkedin.com/in/daniel-j-1b52a2111/) and [Alex](https://www.linkedin.com/in/ion-alexandru-secara/) here! Would love your feedback on [Zen!](https://www.yayzen.com/yc).\n\nProblem:\n\nPosture impacts the way we think, look, feel, and act. Unfortunately, most of us spend the majority of our workdays hunched over a computer, leading to back and joint pain, headaches, and even long-term heart disease.\n\nHow [Zen](https://www.yayzen.com/yc) Can Help:\n\n* Zen mirrors your posture in real-time and will **alert you when you've been slouching** for an extended period of time with a visual and/or audio notification. The desktop app uses your -----> camera !!!  to monitor your posture, without storing or recording any video.\n* Zen reminds you to **stand up and take walk breaks** so that you reduce screen fatigue and increase blood flow.\n* Zen offers quick personalized **stretches for back and joint care.**\n\nWho's building [Zen](https://www.yayzen.com/yc):\n\n* Daniel went from playing football and doing splits at Yale University to working with terrible low back pain and carpal tunnel for over 8 hours a day from working hunched over his computer at Adobe.\n* Alex has spent time in software engineering roles at Adobe, Intuit, and has spent the majority of his life coding at his computer with poor posture, leading to a number of health issues.\n* We met each other a few years ago working in San Francisco and eventually came together to build Zen alongside top ergonomists and physical therapists.\n\nHow you can help:\n\n* [Check out Zen for yourself!](https://www.yayzen.com/yc) Free for this community! We would love your feedback!\n\n&amp;#x200B;\n\nThank you!\n\nAlex &amp; Daniel", "link": "https://www.reddit.com/r/MachineLearning/comments/oaynix/project_enable_your_webcam_to_guard_your_posture/"}, {"autor": "toby__bryant", "date": "2021-06-30 12:35:48", "content": "[P] Atom\u2014free one-click segmentation tool /!/ Hi everyone, \n\nwe built a free one-click segmentation tool for -----> image !!!  annotation: [https://medium.com/hasty-ai/the-first-completely-free-one-click-segmentation-tool-hasty-ai-a3c98f28603a](https://medium.com/hasty-ai/the-first-completely-free-one-click-segmentation-tool-hasty-ai-a3c98f28603a) \n\nThere are already many approaches to using clicks to annotate images, however, most methods integrate the clicks early in additional input channels (see the widely used [DEXTR](https://arxiv.org/pdf/1711.09081.pdf) for example). Often the user has to add several clicks before getting a prediction. Even with interactive segmentation techniques, additional clicks tend to be treated the same as earlier clicks. In order to improve upon initial predictions, our positive or negative clicks are taken together with the previously predicted mask. In addition to the clicks + image features, we also generate clicks + prior-prediction features independently. These separate sets of features prevent a bad initial prediction from harming the features generated from the image. By combining both sets of features, our approach allows the user to continue adding positive and negative clicks in order to reach a high IoU. \n\nYou can use it for free without any limits by creating an account here: [https://hasty.ai/annotation/](https://hasty.ai/annotation/)", "link": "https://www.reddit.com/r/MachineLearning/comments/oawexp/p_atomfree_oneclick_segmentation_tool/"}, {"autor": "steam681", "date": "2021-06-30 11:15:32", "content": "[D] Are there papers or techniques for preprocessing of -----> image !!! s for -----> image !!!  generation using GANs or VAEs? /!/ I would like to generate images of cars from OpenImages or LSUN. But there are still problems in some of the generated images. Either the car is too small, too closely zoomed, there are blobs of people, etc.\n\nI would like to know if there are papers or techniques tackling data preprocessing so that the training images are filtered, or modified to be better so that the generated images are better as well?", "link": "https://www.reddit.com/r/MachineLearning/comments/oav5dt/d_are_there_papers_or_techniques_for/"}, {"autor": "awesome_ml", "date": "2021-06-30 09:17:23", "content": "[Discussion] Should I use downstream data for pre-training in self-supervised learning? /!/ Intuitively, if the downstream can be accessed in the pre-training phase, we can train a better but task-specific pre-trained feature extractor. However, I tried some -----> image !!!  classification experiments, the performance of using downstream data does not always beat the pre-trained model training without downstream data. I use the same setting with Moco except using half of the downstream data in the pre-training. It is surprising to me. Do I miss something?", "link": "https://www.reddit.com/r/MachineLearning/comments/oatjad/discussion_should_i_use_downstream_data_for/"}, {"autor": "Lspnrodsgwp", "date": "2021-06-30 01:26:52", "content": "[D] feasible or no? /!/ Hey guys, I\u2019m pretty new to AI and don\u2019t know much code, but I was thinking about trying something out and wanted to know if there\u2019s an easy way to do this. \n\nI\u2019m starting a driveway repair business and one thing that customers really dislike about the current status quo is the whole process of having to play phone tag with contractors and have them come visit to give an estimate, and I think I could simplify this process if I made an CNN that I trained on many pictures of driveways labeled with what kind of service is needed (replacement, patching, resurfacing\u2026) and possibly a price range as well. I know this part can be easily done through transfer learning but I\u2019m curious how difficult it would be to set up an API network that can integrate this program into my website such that people upload a -----> picture !!!  of their driveway and receive an instant estimate and know what needs to be done. I\u2019m more than willing to put in the work but I just want to get an idea of if this is a huge project I should put on the back burner or something I can go after right now and possibly get a plan in place. \n\nThanks in advance! I appreciate any help/advice!", "link": "https://www.reddit.com/r/MachineLearning/comments/oamf0y/d_feasible_or_no/"}, {"autor": "kvfrans", "date": "2021-06-29 18:08:10", "content": "[R] CLIPDraw: Exploring Text-to-Drawing Synthesis /!/ [Visuals](https://i.imgur.com/m7dlOnc.mp4)\n\n[Arxiv Paper](https://arxiv.org/abs/2106.14843)\n\n[Blog Post](https://kvfrans.com/clipdraw-exploring-text-to-drawing-synthesis/)\n\n[Colab Notebook](https://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb)\n\nCLIPDraw is an algorithm that synthesizes stroke-based drawings based on natural language input. CLIPDraw does not require any training; rather a pre-trained CLIP language------> image !!!  encoder is used as a metric for maximizing similarity between the given description and a generated drawing. Crucially, CLIPDraw operates over vector strokes rather than pixel images, a constraint that biases drawings towards simpler human-recognizable shapes. Results compare between CLIPDraw and other synthesis-through-optimization methods, as well as highlight various interesting behaviors of CLIPDraw, such as satisfying ambiguous text in multiple ways, reliably producing drawings in diverse artistic styles, and scaling from simple to complex visual representations as stroke count is increased.\n\nGlad to get this out and share with you all. The Colab notebook can generally synthesize drawings within a minute or two so please enjoy playing around with it! The paper / blog focus on showcasing interesting text-to-image behaviors of CLIPDraw.", "link": "https://www.reddit.com/r/MachineLearning/comments/oadstf/r_clipdraw_exploring_texttodrawing_synthesis/"}, {"autor": "mgalarny", "date": "2021-02-04 19:18:13", "content": "[D] Here are 3 ways to Speed Up Scikit-Learn - Any suggestions? /!/ Inspired from lorenzkuhn's post [17 ways of making PyTorch Training Faster](https://www.reddit.com/r/MachineLearning/comments/kvs1ex/d_here_are_17_ways_of_making_pytorch_training/) \\- I have been making a list of [How to Speed up Scikit-Learn Training](https://medium.com/distributed-computing-with-ray/how-to-speed-up-scikit-learn-model-training-aaf17e2d1e1). At the moment I have three ways:\n\n# 1. Changing your optimization algorithm (solver)\n\nChoosing the right solver for your problem can save a lot of time. For example, [scikit-learn\u2019s logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), allows you to choose between solvers like \u2018newton-cg\u2019, \u2018lbfgs\u2019, \u2018liblinear\u2019, \u2018sag\u2019, and \u2018saga\u2019.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/4hdawsk6cif61.png?width=700&amp;format=png&amp;auto=webp&amp;s=313254d9beb0530e09028a3dbe5db65826e460db\n\nChoosing the right solver for your problem can save a lot of time (-----> image !!!  above from [Ga\u00ebl Varoquaux\u2019s talk](https://youtu.be/1s8RzWwMdqg)  . For example, [scikit-learn\u2019s logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), allows you to choose between solvers like \u2018newton-cg\u2019, \u2018lbfgs\u2019, \u2018liblinear\u2019, \u2018sag\u2019, and \u2018saga\u2019.\n\n`import time`\n\n`from sklearn.datasets import make_classification`\n\n`from sklearn.model_selection import train_test_split`\n\n`from sklearn.linear_model import LogisticRegression`\n\n&amp;#x200B;\n\n`# Set training and validation sets`\n\n`X, y = make_classification(n_samples=1000000, n_features=1000, n_classes = 2)`\n\n`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000)`\n\n&amp;#x200B;\n\n`# Solvers`\n\n`solvers = ['liblinear', 'newton-cg', 'sag', 'saga', 'lbfgs']`\n\n&amp;#x200B;\n\n`for sol in solvers:`\n\n`start = time.time()`\n\n`logreg = LogisticRegression(solver=sol)`\n\n[`logreg.fit`](https://logreg.fit)`(X_train, y_train)`\n\n`end = time.time()`\n\n`print(sol + \" Fit Time: \",end-start)`\n\n&amp;#x200B;\n\n[Output from 16GB Ram, 4 core MacBook Pro](https://preview.redd.it/bxjwx24gcif61.png?width=483&amp;format=png&amp;auto=webp&amp;s=23a34f5da564fc115af6a25b972eca813501234f)\n\nTo determine which solver is right for your problem, you can check out the [documentation to learn more](https://scikit-learn.org/stable/modules/linear_model.html) (link is for logistic regression, but you can search for solvers for other algorithms).\n\nAny suggestions here?\n\n# 2. Different hyperparameter optimization techniques (grid search, random search, early stopping)\n\n[Scikit-Learn natively contains a couple techniques](https://scikit-learn.org/stable/modules/grid_search.html) for hyperparameter tuning like grid search ([GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)) which exhaustively considers all parameter combinations and randomized search ([RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV))  which samples a given number of candidates from a parameter space with a specified distribution. Recently, scikit-learn added the experimental hyperparameter search estimators halving grid search ([HalvingGridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html#sklearn.model_selection.HalvingGridSearchCV)) and halving random search ([HalvingRandomSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html#sklearn.model_selection.HalvingRandomSearchCV)). The image below is from the [documentation](https://scikit-learn.org/stable/modules/grid_search.html#searching-for-optimal-parameters-with-successive-halving). \n\n&amp;#x200B;\n\nhttps://preview.redd.it/ahl0v0a6dif61.png?width=640&amp;format=png&amp;auto=webp&amp;s=771a07658e6c935d3d826a33ed2cefb9ddda0bd7\n\nThese techniques can be used to search the parameter space using successive halving. All hyperparameter candidates are evaluated with a small number of resources at the first iteration and the more promising candidates are selected and given more resources during each successive iteration.\n\nYou can also use a library like [Tune-sklearn](https://medium.com/distributed-computing-with-ray/gridsearchcv-2-0-new-and-improved-ee56644cbabf) to utilize techniques like early stopping and more. Any suggestions?\n\n# 3. Parallelize or distribute your training with joblib, Dask, and Ray\n\n[Resources \\(dark blue\\) that scikit-learn can utilize for single core \\(A\\), multicore \\(B\\), and multinode training \\(C\\)](https://preview.redd.it/wvxqumz0eif61.png?width=700&amp;format=png&amp;auto=webp&amp;s=11f6629e828882551693a85b6f8354eb8ba67a99)\n\nBy default, scikit-learn trains a model using a single core. As virtually all computers today have multiple cores,  there is a lot of opportunity to speed up the training of your model by utilizing all the cores on your computer. This is especially true if your model has a high degree of high degree of parallelism like a random forest\u00ae.\n\nScikit-Learn can parallelize training on a single node with [joblib which by default uses the \u2018loky\u2019 backend](https://joblib.readthedocs.io/en/latest/parallel.html). Joblib allows you to choose between backends like \u2018loky\u2019, \u2018multiprocessing\u2019, \u2018dask\u2019, and \u2018ray\u2019. This is a great feature as the \u2018loky\u2019 backend is [optimized for a single node and not for running distributed (multinode) applications](https://scikit-learn.org/stable/modules/generated/sklearn.utils.parallel_backend.html).\n\nThere is a blog post on [distributing scikit-learn training using ray and dask](https://medium.com/distributed-computing-with-ray/easy-distributed-scikit-learn-training-with-ray-54ff8b643b33), but can anyone recommend using other libraries for distributed training?\n\n&amp;#x200B;\n\n**I've also put all of the above into this** [**blog post**](https://medium.com/distributed-computing-with-ray/how-to-speed-up-scikit-learn-model-training-aaf17e2d1e1)**, but i would really like your opinions.**", "link": "https://www.reddit.com/r/MachineLearning/comments/lcnj08/d_here_are_3_ways_to_speed_up_scikitlearn_any/"}, {"autor": "rasen58", "date": "2021-02-04 16:31:06", "content": "[P] Evertrove - We made a usable ML-powered -----> image !!!  search using OpenAI's CLIP - search millions of -----> image !!! s /!/ We created a semantic image search engine using OpenAI's CLIP model.\n\nThe results from searches on this are quite impressive, especially since our search engine isn't using any text/captions/keywords on the images in our dataset at all.\n\nWe made a demo where you can search over 2 million unsplash.com high res photographic images here: [https://evertrove.co/](https://evertrove.co/)\n\n&amp;#x200B;\n\nHere's a quick showcase on one query where we search directly on unsplash images on the left (it searches via the image tags/captions), and use ours on the right (no text input, only direct images). The model in this case understands the multiple concepts of dog, beach, and night better than google or a regular search engine can. \n\nhttps://preview.redd.it/joogbbn2r5f61.png?width=1064&amp;format=png&amp;auto=webp&amp;s=314ec6eb70a9d4f85554c9083ad4effd11191543\n\nThe regular search engine would have done well if the Unsplash images had all 3 captions of {dog, beach, night}, but in most cases your images won't have enough tags or the tags won't be able to capture everything in the image, and so this is where CLIP's ability to extract semantic meaning from images (given that it has seen a ton of images from across the internet) helps.\n\nIn a lot of cases, our search performs just as well as Google's, but ours is a lot better than Unsplash's search engine on their own site in most cases.\n\n&amp;#x200B;\n\nOur website should help you get to interactively experience a bit of what CLIP and other similar models are able to do now!\n\n[https://evertrove.co/](https://evertrove.co/) \n\n[https://twitter.com/hibyepie/status/1357364189422092292](https://twitter.com/hibyepie/status/1357364189422092292)", "link": "https://www.reddit.com/r/MachineLearning/comments/lcjizm/p_evertrove_we_made_a_usable_mlpowered_image/"}, {"autor": "JohannesHa", "date": "2021-02-04 15:04:32", "content": "[P][D] How to scrape product data on supplier websites? /!/ I'm currently trying to build a semantic scraper that can extract product information from different company websites of suppliers in the packaging industry (with as little manual customization per supplier/website as possible).\n\nThe current approach that I'm thinking of is the following:\n\n1. Get all the text data via scrapy (so basically a HTML-tag search). This data would hopefully be already semi-structured with for example: title, description, product -----> image !!! , etc.\n2. Fine-tune a pre-trained NLP model (such as BERT) on a domain specific dataset for packaging to extract more information about the product. For example: weight and size of the product\n\nWhat do you think about the approach? What would you do differently?\n\nOne challenge I already encountered is the following:\n\n* Not all of the websites of the suppliers are as structured as for example e-commerce sites are \u2192 So small customisations of the XPath for all websites is needed. How can you scale this?\n\nAlso does anyone know an open-source project as a good starting point for this?", "link": "https://www.reddit.com/r/MachineLearning/comments/lchjkl/pd_how_to_scrape_product_data_on_supplier_websites/"}, {"autor": "Wiskkey", "date": "2021-02-04 13:14:20", "content": "[P] Search 19 million images using natural language queries using site Same Energy (beta). Does not use OpenAI's CLIP but does use deep learning according to the developer. /!/ [https://same.energy/](https://same.energy/)\n\nThe site is trying to match the user's query to the contents of the -----> image !!! s (not the -----> image !!!  captions or any other -----> image !!!  metadata) by using a neural net ([source](https://same.energy/about)).\n\nThese images are not necessarily legally freely usable. If you want legally freely usable images, see the end of this post for 2 different web apps that search site Unsplash.\n\nFrom [https://twitter.com/Jacob\\_\\_Jackson/status/1357143267213783045](https://twitter.com/Jacob__Jackson/status/1357143267213783045):\n\n&gt;19M images from Reddit, Instagram, and Pinterest\n\nFrom [https://twitter.com/Jacob\\_\\_Jackson/status/1357139564272504833](https://twitter.com/Jacob__Jackson/status/1357139564272504833):\n\n&gt;it doesn't use CLIP directly, but it does use similar methods\n\nThere is [evidence](https://twitter.com/Jacob__Jackson/status/1342004468519477248) that the developer was working on this before [CLIP](https://openai.com/blog/clip/) was announced/released.\n\n**I am not affiliated with this site or anyone involved with it.**\n\nExample: search query \"a tennis ball in a dog's mouth\". One of the search results:\n\nhttps://preview.redd.it/ttgmtprlrgf61.jpg?width=900&amp;format=pjpg&amp;auto=webp&amp;s=9a615425df8ac1e1b5f57b7951dc0286e04cc0d0\n\nRelated: [Use natural language queries to search 2 million freely-usable images from Unsplash using a free Google Colab notebook from Vladimir Haltakov. Uses OpenAI's CLIP neural network](https://www.reddit.com/r/MachineLearning/comments/l52qe6/p_use_natural_language_queries_to_search_2/).\n\nRelated: [Evertrove - We made a usable ML-powered image search using OpenAI's CLIP - search millions of images](https://www.reddit.com/r/MachineLearning/comments/lcjizm/p_evertrove_we_made_a_usable_mlpowered_image/)", "link": "https://www.reddit.com/r/MachineLearning/comments/lcfdj0/p_search_19_million_images_using_natural_language/"}, {"autor": "ifelsestatement007", "date": "2021-02-03 23:50:14", "content": "-----> Image !!!  dataset normalization is one of the most common practises to avoid neural network overfitting but do you know how to calculate the mean and standard deviation of your own custom image dataset?", "link": "https://www.reddit.com/r/MachineLearning/comments/lc1zak/image_dataset_normalization_is_one_of_the_most/"}, {"autor": "upgradhelpneeded", "date": "2021-02-03 21:26:16", "content": "[N] MIT Tech Review article on bias: \"An AI saw a cropped -----> photo !!!  of AOC. It autocompleted her wearing a bikini.\"", "link": "https://www.reddit.com/r/MachineLearning/comments/lbyrgr/n_mit_tech_review_article_on_bias_an_ai_saw_a/"}, {"autor": "Lomjnhbgvfcdxsza", "date": "2021-10-29 23:26:43", "content": "Can anyone recommend a book on pattern theory? I\u2019m looking into theoretical backing for pattern recognition (-----> image !!!  required)", "link": "https://www.reddit.com/r/MachineLearning/comments/qip15w/can_anyone_recommend_a_book_on_pattern_theory_im/"}, {"autor": "AbjectDrink3276", "date": "2021-10-29 21:08:43", "content": "[D] linking pythons multiprocessing with tensorflows mirrored strategy /!/ When performing inference on video data and you have access to a gpu cluster for inference does it make since to employ pythons multiprocessing library to read the video file faster while using tensorflows mirrored strategy to distribute each -----> image !!!  across the gpu cluster to increase prediction speed? \n\nOr on the other hand, would it make more sense to instantiate multiple threads the grab different frames from the video file and then have the same model loaded onto different gpus and perfrom prediction this way?", "link": "https://www.reddit.com/r/MachineLearning/comments/qimg4k/d_linking_pythons_multiprocessing_with/"}, {"autor": "Galaxy_GN-z11", "date": "2021-10-29 20:12:49", "content": "[D] 3D Medical Imaging annotation /!/ I  work in medical imaging space and we need ground truth 3D segmentation of anatomical structures in 3D CT imaging. It is very time consuming process and requires domain expertise. Does anyone know of  annotation services in this space? There seems to be quite a number of services out there offering -----> image !!!  annotation services for machine learning, but most of them are focusing on 2D -----> image !!! s. The task in 3D medical imaging is specialized and more demanding.", "link": "https://www.reddit.com/r/MachineLearning/comments/qilcjy/d_3d_medical_imaging_annotation/"}, {"autor": "Galaxy_GN-z11", "date": "2021-10-29 20:04:05", "content": "3D Medical -----> image !!!  annotation vendors? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/qil66c/3d_medical_image_annotation_vendors/"}, {"autor": "Eve26th", "date": "2021-10-29 19:56:26", "content": "[D] Good landmark points annotation tools /!/ Hello Everyone!\n I want to generate a -----> image !!!  dataset with some landmark points on it for the labels. For that I am looking for some landmark annotation tools. Any suggestions would be really helpful. \nThank you!", "link": "https://www.reddit.com/r/MachineLearning/comments/qil09k/d_good_landmark_points_annotation_tools/"}, {"autor": "fedetask", "date": "2021-10-29 15:41:24", "content": "[R] Create graph from random walks in multidimensional space /!/ Imagine we have a -----> camera !!! -equipped robot that performs `k` random walks in a building, not necessarily starting every time from the same location. We collect a set of image sequences `O_1, O_2, ... O_k`, each one containing the images captured by the robot in that random walk.\n\nOur goal is to transform the dataset of `k` image sequences into a graph, where images are connected by edges if, given the data, it would be possible for the robot to go from one to the other in less than `d` steps. Moreover, edges need to be *directed* since it might be possible to go from image A to image B, but not vice versa (e.g. image A is the image of a glass falling from the table, image B is the glass broken on the floor. Obviously, A-&gt;B is an edge but B-&gt;A is not).\n\nThe requirements are mainly that a) the task is performed by a NN model since it will need to generalize to new image data after training and (possibly) b) the complexity should be at most O(n^(2))\n\nCould you point me to some relevant literature? I am having trouble finding works that operate in a similar settings.", "link": "https://www.reddit.com/r/MachineLearning/comments/qifnck/r_create_graph_from_random_walks_in/"}, {"autor": "Hot-Painter4924", "date": "2021-10-29 13:51:32", "content": "[D] Tensorflow profiler gives the same FLOPs for different input shapes in a deep learning model /!/ I'm trying to measure floating point operations in a model, and how much they increase as you change the input shape. I'm using the following code snippet:\n\n    import tensorflow import keras import tensorflow    as tf import keras.backend as K import numpy as np import keras.backend         as K import tensorflow            as tf from keras.layers            import Input from tensorflow.keras.applications.resnet50      import ResNet50  from tensorflow.keras.preprocessing         import -----> image !!!  from tensorflow.keras.applications.resnet50 import preprocess_input import numpy as np  def pre_process(img_size = (224, 224) ):     x = np.random.rand(img_size[0],img_size[1],img_size[2])     x = np.expand_dims(x, axis=0)     x = preprocess_input(x)      return x  def get_total_flops(graph,session,model):              run_meta = tf.compat.v1.RunMetadata()     opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()      flops = tf.compat.v1.profiler.profile(graph=graph,         run_meta=run_meta, cmd=\"scope\", options=opts     )      return flops.total_float_ops   session = tf.compat.v1.Session() graph = tf.compat.v1.get_default_graph()              INPUT_SHAPE = (128,128,3)      model = ResNet50(weights= None,input_tensor=Input(shape=INPUT_SHAPE),classes=1) with graph.as_default():     with session.as_default():         x     = pre_process(img_size=INPUT_SHAPE)                  for i in range(20):             preds = model.predict(x)          total_flops = get_total_flops(graph,session,model)                  print('total_flops: ', total_flops) \n\nthe output is:\n\n    total_flops: 23563360 \n\nI'm getting the exactly same number for different INPUT\\_SHAPES. eg INPUT\\_SHAPE=(256,256,3)  \n,INPUT\\_SHAPE=(512,512,3)\n\nAs i understood, a bigger image will lead to more operations in convolution layers, so, why it is returning the exactly same FLOPs ?", "link": "https://www.reddit.com/r/MachineLearning/comments/qidalr/d_tensorflow_profiler_gives_the_same_flops_for/"}, {"autor": "projekt_treadstone", "date": "2021-10-29 09:54:33", "content": "[D] Why only accuracy as evaluation criteria in Few-shot and zero-shot learning? /!/ I have seen that majority of few and zero-shot learning papers use Accuracy as a benchmark. Unlike other domains like object detection, classical -----> image !!!  recognition, why they don't use other benchmarks like Precision, recall or other criteria apart from Accuracy? Is it due to others are doing it, so people keep following, or other performance metrics are not reliable in a few-shot domain?", "link": "https://www.reddit.com/r/MachineLearning/comments/qi99ka/d_why_only_accuracy_as_evaluation_criteria_in/"}, {"autor": "gpahul", "date": "2021-10-28 19:58:58", "content": "[D] State of the art in the document information extraction/parsing for resume parsing? /!/ Hi everyone,\n\n\nI've been looking for state of the art research paper/project/code for automatically extracting information from various layout of resumes.\n\n\nTypical workflow I can estimate is to convert resume to -----> image !!! , detect text, table etc., apply rule based heuristic approach to extract the information based on NER etc. but I think that would be an outdated approach and will not be accurate and feasible enough to cover all the cases.\n\n\nI'd really appreciate if you have could share any information/experience in this\u00a0regard.\n\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/qhv879/d_state_of_the_art_in_the_document_information/"}, {"autor": "xian-yu", "date": "2021-10-15 16:20:10", "content": "[D] Best practices for deep learning with mixed data (-----> image !!!  and tabular) /!/ Hi, everyone! First time posting here, I hope this is the right place!\n\nI'm working on image classification using transfer learning with a pre-trained CNN (Inception) as the feature extractor. I'd like to also use some metadata I have for the images as I think this can improve the performance of my model. In terms of architecture, how would I go about doing this? What are the current best practices and could you point me to some literature or resources on this topic?\n\nMy idea: Just concatenate the tabular data (pre-processed) to the output of the image feature extractor and then pass that to an MLP (see pic).\n\nHowever, I saw an example online where they first pass the tabular data through a small MLP before concatenating to the outputs of the image feature extractor, and then send everything to the final MLP classifier. What is the intuition behind this step and is this really necessary?\n\nThe only reason I can think of is that the first MLP is performing some implied feature engineering/learning feature interactions of the tabular data. But, couldn't this also be achieved by just adding one or two additional hidden layers on the MLP classifier? I imagine that it may be harder to learn the relationships between the tabular data if the image feature data is also present, that's why they put the first MLP there. My concern is whether both MLPs can be trained simultaneously with the same learning rate. If not, that would complicate things.\n\n[My solution for mixed-data \\(image-tabular\\) classification](https://preview.redd.it/uq5mrria3nt71.png?width=1142&amp;format=png&amp;auto=webp&amp;s=3de21e3d9626d51038f1f119fe119a6768bfb78d)\n\nLet me know what you think and I'll appreciate it if you can direct me to some relevant resources.\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/q8ruxf/d_best_practices_for_deep_learning_with_mixed/"}, {"autor": "Baggins95", "date": "2021-10-15 16:15:25", "content": "[D] Variational Bayes for tabular data with -----> image !!!  conditioning /!/ Hey. I'm looking for work on conditional VAEs for tabular data, conditioned on image features. Is there anything like that? If not, why not? Are there fundamental assumptions of the model violated? Any hint is welcome. Thanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/q8rrgq/d_variational_bayes_for_tabular_data_with_image/"}, {"autor": "xian-yu", "date": "2021-10-15 16:15:17", "content": "Best practices for deep learning with mixed data (-----> image !!!  and tabular) /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/q8rrcn/best_practices_for_deep_learning_with_mixed_data/"}, {"autor": "Baggins95", "date": "2021-10-15 16:12:19", "content": "Variational Bayes for tabular data with -----> image !!!  conditioning /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/q8rp8x/variational_bayes_for_tabular_data_with_image/"}, {"autor": "sarmientoj24", "date": "2021-10-15 14:30:49", "content": "[D] Image Pre-Processing for Dental X-Ray Images /!/ I would like to know other -----> image !!!  enhancements and pre-processing that do good for dental xrays (or maybe x-rays in general). I have used the following: *(1) anisotropic diffusion filter, (2) clahe, and (3) unsharp masking.* \n\nI would like to get some insights if there are other more suitable pre-processing techniques for enhancing dental xray images.", "link": "https://www.reddit.com/r/MachineLearning/comments/q8pni3/d_image_preprocessing_for_dental_xray_images/"}, {"autor": "DLBiel", "date": "2021-10-15 12:50:21", "content": "For the MNIST data set, how to device a network, which takes two -----> image !!! s as input and outputs an -----> image !!!  that contains sum of the numbers on input -----> image !!! s /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/q8nra8/for_the_mnist_data_set_how_to_device_a_network/"}, {"autor": "willowill5", "date": "2021-10-15 02:21:30", "content": "[D] GAN papers/repos NOT related to -----> image !!!  generation/convolutions layers /!/ Hi all, I am currently looking for model architecture inspiration regarding GANs modeling simple data with dense layers. As you know, most of the examples out there revolve around image generation/convolutions. Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/q8ehsh/d_gan_papersrepos_not_related_to_image/"}, {"autor": "-nomad-wanderer", "date": "2021-10-14 22:14:44", "content": "I made an app consuming machine learning. It tells you what an -----> image !!!  containes. It ain\u2019t much but \u2026 http://rairadio.app NO ads content. To be useful. more in the follow.", "link": "https://www.reddit.com/r/MachineLearning/comments/q89xfx/i_made_an_app_consuming_machine_learning_it_tells/"}, {"autor": "jiayaozhang", "date": "2021-10-14 19:04:57", "content": "[R] Imitating Neural Net Feature Dynamics via Linear SDE/ODE /!/ Hello! I wish to share our NeurIPS 21 paper \"[Imitating Deep Learning Dynamics via Locally Elastic Stochastic Differential Equations](https://arxiv.org/abs/2110.05960)\" \\[0\\], feel free to AMA :)\n\nTL;DR: We devise a linear SDE/ODE model to imitate per-class feature (thinking logits) dynamics of neural nets training based on local elasticity (LE) \\[1\\]. We found the emergence of LE implies linear separation of features from different classes as training progresses.\n\nThe drift matrix of our model has relatively simple structure; with that estimated, we can simulate the SDE using forward Euler method, which aligns reasonably well with genuine dynamics.\n\nLocal elasticity models the phenomenon observed in DNN training: the effect due to training on a sample is greater for samples from the same class, and smaller for samples from different classes. For example, training an -----> image !!!  of cats facilitates the model better learns -----> image !!! s of other cats while not so for -----> image !!! s of, say, dogs.\n\n\\[0\\] [https://arxiv.org/abs/2110.05960](https://arxiv.org/abs/2110.05960)\n\n\\[1\\] [https://arxiv.org/abs/1910.06943](https://arxiv.org/abs/1910.06943)", "link": "https://www.reddit.com/r/MachineLearning/comments/q86ey2/r_imitating_neural_net_feature_dynamics_via/"}, {"autor": "PaganPasta", "date": "2021-10-14 16:30:42", "content": "[D] Guys who set up infra for ML training, How do I debug my setup? /!/ Hi,\n\nI am talking about setting up GPU clusters, connecting with a storage and running a manager like RunAI or SLURM on top. \n\n&amp;#x200B;\n\nAt the moment, I am struggling to get 2x[DGX](https://www.nvidia.com/en-gb/data-center/dgx-a100/)\\-A100 with 2TB of main memory and [excelero](https://www.excelero.com/product/pny-3s-2400-ai-optimized-storage-server-nvmesh/) pny-3s-2400(8TB) storage with [Infiniband](https://en.wikipedia.org/wiki/InfiniBand) connectivity to run more than 3 training job simultaneously without causing massive delay in training. Training is standard Imagenet-1k -----> image !!!  classification. One possible solution is to use Webdatasets(Pytorch) for storing data which does make the setup perform 'better' with increase in training jobs. But, I haven't seen this being done from work that I see on github from official sources to train on clusters. So I am not sure if data format is the **only** problem. \n\n&amp;#x200B;\n\nMy questions in no particular order are:\n\n1. How do you compute IOPS(i/o operations per second) supported by the setup?\n2. Is the current incapable of handling more than 3 simultaneous training jobs? \n3. Any upgrades that might be required?\n4. Where do I start to narrow down on the problem?\n5. How do people at Google/FB etc have the setup behind the scene?", "link": "https://www.reddit.com/r/MachineLearning/comments/q83ac9/d_guys_who_set_up_infra_for_ml_training_how_do_i/"}, {"autor": "Yuqing7", "date": "2021-10-14 14:56:43", "content": "[R] Google Researchers Explore the Limits of Large-Scale Model Pretraining /!/ A Google Research team conducts a systematic exploration comprising more than 4800 experiments on Vision Transformers, MLP-Mixers and ResNets with parameters ranging from 10 million to 10 billion, evaluated on more than 20 downstream -----> image !!!  recognition tasks, aiming to capture the nonlinear relationships between performance on upstream and downstream tasks. \n\nHere is a quick read: [Google Researchers Explore the Limits of Large-Scale Model Pretraining.](https://syncedreview.com/2021/10/14/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-123/)\n\nThe paper *Exploring the Limits of Large Scale Pre-training* is on [arXiv](https://arxiv.org/abs/2110.02095).", "link": "https://www.reddit.com/r/MachineLearning/comments/q81eax/r_google_researchers_explore_the_limits_of/"}, {"autor": "IngenuityDiligent124", "date": "2021-09-29 13:29:18", "content": "I am doing my final year project on Image super resolution and -----> image !!!  enhancement. But I'm not quite sure how to go about it. Any tips? [P] /!/ I have gone through GAN Deep convolution networks and Residual dense amd Adversarial Networks papers but I still am not able to get a good idea about this.", "link": "https://www.reddit.com/r/MachineLearning/comments/pxvetd/i_am_doing_my_final_year_project_on_image_super/"}, {"autor": "mattvilain", "date": "2021-09-29 12:29:20", "content": "[D] Train a neural network with different training data than test data /!/ Imagine  we have a classification task, lets say images of dogs and we have to  predict the dog breed. In our training samples we have the images and  also information about the dog in the -----> picture !!!  like its size, the size of  its hair, its coat type ... (sorry if the example is not well chosen).  But in the test samples we only have the picture of the dog.\n\nMy  question is, how can we train a model that will use the additional  information to extract more relevant information in the picture during  training, and can still make good predictions when given only the  picture ?\n\nI thought about kind of a  knowledge distillation where we train a model on all the information we  have and this model will teach another model that works only with the  images. Idk if this is a common SOTA problem and if knowledge  distillation can be used for that or if there is other way to approach  the problem. What do you think ?", "link": "https://www.reddit.com/r/MachineLearning/comments/pxuchf/d_train_a_neural_network_with_different_training/"}, {"autor": "Starlight-786", "date": "2021-09-29 12:22:37", "content": "Content of -----> Photography !!! ", "link": "https://www.reddit.com/r/MachineLearning/comments/pxu8dc/content_of_photography/"}, {"autor": "Useful_Cover6069", "date": "2021-09-29 04:32:02", "content": "Msc research topics [R] [D] /!/ Hi all,\n\nI just looking for some topics to my msc research. \n\nI\u2019m interested in -----> image !!!  processing and SEO topics. Can you suggest some?\n\n\nThese can be helpful to anyone who doing research. \ud83d\ude42", "link": "https://www.reddit.com/r/MachineLearning/comments/pxnwlz/msc_research_topics_r_d/"}, {"autor": "fireless-phoenix", "date": "2021-09-28 13:17:37", "content": "[D] How to create an ensemble of CNNs? /!/ So I want to create an ensemble of CNNs where each CNN takes in a different -----> image !!!  and the final confidence is calculated by comparing the confidence of the two CNNs, like [this](https://imgur.com/gallery/ZgEAQac). I know it looks like a simple task and it probably is but my code is not working as expected and I need some sanity check. Are there any publicly available code in pytorch for this?", "link": "https://www.reddit.com/r/MachineLearning/comments/px677c/d_how_to_create_an_ensemble_of_cnns/"}, {"autor": "Pbatch23888", "date": "2021-09-28 11:51:44", "content": "Telephone Pictionary with Image Synthesis (Pete-tionary) [Project] /!/ Hi guys,\n\nI've been working on a game which is a mixture of telephone pictionary and -----> image !!!  synthesis. \n\nYou can give it a try here: [https://pictionary.pbatch.net/](https://pictionary.pbatch.net/)\n\nYou can view the code here: [https://github.com/Pbatch/pete-tionary](https://github.com/Pbatch/pete-tionary)\n\nThe idea is that each player inputs a text prompt, which then generates a picture using image synthesis (CLIP + luna/chroma magic + GAN enhancement). \n\nThis will take about **THIRTY** seconds, provided that the website isn't being battered to death.\n\n[A dog in a washing machine \\(triple\\)](https://preview.redd.it/ctdm4uf4e8q71.png?width=1012&amp;format=png&amp;auto=webp&amp;s=14aff14e84bf6cd9f953b2ec43e7f933cbd12d83)\n\nEach player then selects the image that best matches their prompt.\n\n[A dog in a washing machine \\(single\\)](https://preview.redd.it/jms1che8e8q71.png?width=630&amp;format=png&amp;auto=webp&amp;s=62076782cb0208025cf4e33febbe2d8426e29320)\n\nOnce each player has done this, they pass their images on and receive a new image from another player.\n\n[the last meal on death row](https://preview.redd.it/cqo4djjde8q71.png?width=618&amp;format=png&amp;auto=webp&amp;s=3ea7c9b16db63c160f71c4b8fc1d3a5ddab4d939)\n\nEach player must then try to write a prompt that matches that image. I.e. You might think that the image above looks like \"the last supper\".\n\n[The last supper](https://preview.redd.it/lwxw58rie8q71.png?width=1005&amp;format=png&amp;auto=webp&amp;s=cdf75201db3306f4976979a475e4a071934bbc96)\n\nThe game continues in this fashion until each player has seen each initial image once. You can then view all the stories you have made.\n\n[a dog in the washing machine \\(final\\)](https://preview.redd.it/nmxbc9ane8q71.png?width=811&amp;format=png&amp;auto=webp&amp;s=65d3b774e2a1d73ed29b3d50f6cf5b228a9a1c5d)\n\n[the last meal on death row \\(final\\)](https://preview.redd.it/zhkm5qqoe8q71.png?width=826&amp;format=png&amp;auto=webp&amp;s=39d9f5681fe089b0834005b98e6691f3aa9f32c3)\n\nIf you're just interested in the algorithm, it is possible to play single player.\n\nIf the game crashes or bugs out, reset your local storage and then log in again.\n\nI hope you have fun and please post any interesting pictures/stories that you create in the comments. Any feedback or advice is greatly appreciated too :)\n\nCheers.", "link": "https://www.reddit.com/r/MachineLearning/comments/px4p6x/telephone_pictionary_with_image_synthesis/"}, {"autor": "Julian_incandenza", "date": "2021-05-16 16:32:03", "content": "[P] Face tracking for musicians /!/ **Usecase**\n\nWhile digitalization is transforming our lives, 99% of musicians are still playing with physically printed scores. In contrast to printed books, this is not because they like them, they are quite a pain actually:\n\n\\- You must buy/print them and therefore you cannot just spontaneously try out a piece\n\n\\- If you play longer pieces you must interrupt your playing to flip the page or ask someone to do it for you\n\n\\- They often fall off or get blown away by the wind if you are outside\n\nBut sad truth is that there isn't really an alternative yet. Of course there are apps but there you must interrupt playing all the time to flip the page or preset a speed that is super-annoying, which is why musicians have always stuck to printed scores.\n\n**Concept**\n\nBy flipping the pages via facial expression - e.g. by opening the mouth or turning the head rapidly (in the end there have to be several options to accommodate different instruments/preferences) - digital scores become super-handy and easy to use. All one needs is an app for smartphones and tablets that can recognize facial expressions via the front -----> camera !!!  and flip the page. Building on that, one can implement lots of premium features (e.g. a pedal) to make the app profitable as well.\n\n**Technical Solution**\n\nI have already built a little Python prototype (--&gt; video) with OpenCV facial landmarks where one flips pages by opening the mouth. Obviously, it's very vulnerable to changing light conditions and from what I could find out online, the process should be too tough on computing power for most smartphones/tablets. Therefore, I'm open to taking a different approach to the development of the actual product. One approach that came to my mind:\n\n1) Take a dataset with e.g. 50 000 human faces and use Open CV (or something comparable) to label them according to the degree of mouth opened and later blinking/head turned etc. Label the ones where landmarks don't work manually and ideally check all the other ones as well.\n\n2) Train a Neural Network on the labeled dataset \\[assuming that running it later requires less computing power than landmarks\\].\n\n3) Implement the Neural Network in the mobile app. Assuming that Python wouldn't be efficient enough, I am thinking of PWA, for example, Xamarin or React Native.\n\n**Context**\n\nI'm myself a recent architecture graduate; therefore, my coding skills beyond Python and C# are slightly limited. I did the course on Machine Learning by Andrew Ng and did a few projects (like the prototype), still I wouldn't consider myself advanced in the field. Even worse, I haven't ever written a mobile app yet.\n\n**I'm looking for**\n\n1) Your feedback and your input - particularly on the Technical Solution.\n\n2) **You**: I'd be more than happy to share this adventure; feel more than free to contact me in this regard!\n\n&amp;#x200B;\n\nCheers,\n\nJulian", "link": "https://www.reddit.com/r/MachineLearning/comments/nds1x8/p_face_tracking_for_musicians/"}, {"autor": "ottawalanguages", "date": "2021-05-15 20:44:44", "content": "[D] Confidence Intervals for Classification Models /!/ The idea of creating confidence intervals in regression models is quite straightforward. \n\nFor example :\nhttps://www.researchgate.net/profile/Folorunso-Oludayo-Fasina/publication/284729754/figure/fig2/AS:300625800777756@1448686184816/Scatter-plot-with-linear-regression-fit-and-a-95-confidence-interval-for-reported.png\n\nBut do confidence intervals carry over to classification models?\n\n1) For example, in this -----> picture !!!  here (confidence interval for an ROC curve corresponding to a classification model ) https://i.stack.imgur.com/Y7KSNm.png - can the lower limit of the confidence interval from this ROC curve be used to gauge how well this model might generalize to new data?\n\n2) For a classification model, can we make a confidence interval for the prediction of an individual observation? For instance, I wrote some R code (you can directly copy/paste the code) for a particular example where a random forest (i.e. classification model) is used to predict whether if an observation will be \"approved\" or \"rejected\" (see here for the code: https://shrib.com/#Madelyn_NMjYE8) .\n\nThus, for each observation, the classification model predicts the probability that this observation will be \"approved\" or \"rejected\". Whichever probability is higher, the model selects that outcome for the given observation.\n\nAlso, the higher one of these probabilities are, this means the classification model is more confident with its prediction (e.g. for two observations, the ratio of approved:rejected 0.9:0.1 vs 0.6:0.4 . The model is more confident about the first prediction)\n\nThus, is there anyway to apply the notion of confidence intervals to the probabilities for individual predictions?\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/nd81yo/d_confidence_intervals_for_classification_models/"}, {"autor": "PassionGlittering695", "date": "2021-08-15 10:55:18", "content": "[D] What might be a good architecture that takes depth layers of an -----> image !!!  and renderes an -----> image !!! ? /!/ I  have a few equidistant layers in disparity and want to input to the to a  neural network to render the final image. What might be some NN  suggestions to start from like encoder-decoder, UNet, ResNet?", "link": "https://www.reddit.com/r/MachineLearning/comments/p4rc1i/d_what_might_be_a_good_architecture_that_takes/"}, {"autor": "PassionGlittering695", "date": "2021-08-15 10:37:41", "content": "What might be a good architecture that takes depth layers of an -----> image !!!  and renderes an -----> image !!! ? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/p4r53n/what_might_be_a_good_architecture_that_takes/"}, {"autor": "cactus_sound", "date": "2021-08-15 10:14:26", "content": "[D] Are there any artistic machine learning projects that take an arbitrary stream of binary data as input for semi-randomness? /!/ The output can be any sort of generative -----> image !!!  or audio. It doesn't have to be related to the original.\n\nIt would probably work better on files that are compressible and less random, but it would be interesting to see if something interesting could be generated from totally random data.", "link": "https://www.reddit.com/r/MachineLearning/comments/p4qvvg/d_are_there_any_artistic_machine_learning/"}, {"autor": "projekt_treadstone", "date": "2021-08-14 22:29:07", "content": "[D] Reason behind low performance of cosine distance compared to Euclidean /!/ I am comparing two -----> image !!!  embeddings and I found that the cosine similarity-based model performs very badly (30 % less accurate).  However, Euclidean is better and I just tried to add Euclidean distance and cosine similarity and it outperformed both cosine and euclidean. What could be the reason for this behavior where cosine is bad but adding cosine with euclidean is so good?", "link": "https://www.reddit.com/r/MachineLearning/comments/p4hja2/d_reason_behind_low_performance_of_cosine/"}, {"autor": "fiffifthrow", "date": "2021-08-14 19:49:11", "content": "[P] I made an -----> image !!!  classifier to detect NSFW furry art!", "link": "https://www.reddit.com/r/MachineLearning/comments/p4esk1/p_i_made_an_image_classifier_to_detect_nsfw_furry/"}, {"autor": "KirillTheMunchKing", "date": "2021-08-14 15:38:19", "content": "[D] Quick and Easy GAN Domain Adaptation explained: Sketch Your Own GAN by Sheng-Yu Wang et al. 5 minute summary /!/ [Sketch Your Own GAN](https://i.redd.it/h4mim0t6dch71.gif)\n\nWant to quickly train an entire GAN that generates realistic images from just two quick sketches done by hand? Heng-Yu Wang and team got you covered! They propose a new method to fine-tune a GAN to a small set of user-provided sketches that determine the shapes and poses of the objects on the synthesized images. They use domain adversarial loss and different regularization methods to preserve the original model's diversity and -----> image !!!  quality.\n\nThe authors motivate the necessity of their approach mainly with the fact that training conditional GANs from scratch is simply a lot of work: you need powerful GPUs, annotated data, careful alignment, and pre-processing. In order for an end-user to generate images of a cats in a specific pose a very large number of such images is required, however with the proposed approach only a couple of sketches and a pretrained GAN is needed to create a new GAN that synthesizes images resembling the shape and orientation of sketches, and retains the diversity and quality of the original model. The resulting models can be used for random sampling, latent space interpolation and photo editing.\n\nRead the [full paper digest](https://t.me/casual_gan/81) or the [blog post](https://www.casualganpapers.com/few-shot-user-guided-gan-domain-adaptation/Sketch-Your-Own-GAN-explained.html) (reading time \\~5 minutes) to learn about Cross-Domain Adversarial Learning, how Image Space Regularization helps improve the results, and what optimization targets are used in Sketch Your Own GAN.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://www.casualganpapers.com/)!\n\n[Sketch Your Own GAN explained](https://preview.redd.it/el0nix7adch71.png?width=1925&amp;format=png&amp;auto=webp&amp;s=4e17b5e593180146f065865d07dac4aec5dae161)\n\n\\[[Full Explanation](https://t.me/casual_gan/81)/ [Blog Post](https://www.casualganpapers.com/few-shot-user-guided-gan-domain-adaptation/Sketch-Your-Own-GAN-explained.html)\\] \\[[Arxiv](https://arxiv.org/pdf/2108.02774.pdf)\\] \\[[Code](https://github.com/PeterWang512/GANSketching)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n&gt;\\[[3D-Inpainting](https://www.casualganpapers.com/single_view_layered_depth_3d_inpainting/3D-Inpainting-explained.html)\\]  \n&gt;  \n&gt;\\[[Real-ESRGAN](https://t.me/casual_gan/77)\\]  \n&gt;  \n&gt;\\[[SupCon](https://t.me/casual_gan/78)\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/p4a92f/d_quick_and_easy_gan_domain_adaptation_explained/"}, {"autor": "No-Recommendation384", "date": "2021-02-12 04:05:51", "content": "[R] ICLR 2021: MALI, A memory efficient and reverse accurate integrator for Neural ODEs /!/ **Abstract:**\n\nNeural ordinary differential equations (Neural ODEs) are a new family of deep-learning models with continuous depth. However, the numerical estimation of the gradient in the continuous case is not well solved: existing implementations of the adjoint method suffer from inaccuracy in reverse-time trajectory, while the naive method and the adaptive checkpoint adjoint method (ACA) have a memory cost that grows with integration time. In this project, based on the asynchronous leapfrog (ALF) solver, we propose the Memory-efficient ALF Integrator (MALI), which has a **constant memory cost**  integration time similar to the adjoint method, and guarantees **accuracy in reverse-time trajectory (hence accuracy in gradient estimation)**. We validate MALI in various tasks: on -----> image !!!  recognition tasks, to our knowledge, MALI is the first to enable feasible training of a Neural ODE on ImageNet and outperform a well-tuned ResNet, while existing methods fail due to either heavy memory burden or inaccuracy; for time series modeling, MALI significantly outperforms the adjoint method; and for continuous generative models, MALI achieves new state-of-the-art performance. \n\n**Links:**\n\npaper: [https://openreview.net/forum?id=blfSjHeFM\\_e](https://openreview.net/forum?id=blfSjHeFM_e)\n\npackage: [https://jzkay12.github.io/TorchDiffEqPack/TorchDiffEqPack.html](https://jzkay12.github.io/TorchDiffEqPack/TorchDiffEqPack.html)\n\ngithub: [https://github.com/juntang-zhuang/TorchDiffEqPack](https://github.com/juntang-zhuang/TorchDiffEqPack)\n\nproject page: [https://juntang-zhuang.github.io/torch\\_diffeq\\_pack/](https://juntang-zhuang.github.io/torch_diffeq_pack/)\n\n**Results**\n\nSOTA accuracy of Neural ODE on ImageNet classification, and is more robust to adversarial attack. To our knowledge it's the first method to scale Neural ODE to large scale datasets like ImageNet\n\n*Processing img 4xnc4mgf0zg61...*\n\nNew SOTA for continuous generative modeling with FFJORD\n\nhttps://preview.redd.it/ntgv6qvp0zg61.png?width=1370&amp;format=png&amp;auto=webp&amp;s=787b0d48b7cfd0e4498f29e1a0cfd5106d450ebe\n\n*Processing img yirea6i01zg61...*\n\nSOTA on time-series modeling with Neural ODEs and Neural CDEs\n\n*Processing img howlqpqv0zg61...*", "link": "https://www.reddit.com/r/MachineLearning/comments/li38wt/r_iclr_2021_mali_a_memory_efficient_and_reverse/"}, {"autor": "NotVector", "date": "2021-02-11 11:44:41", "content": "[D] What future AI startup ideas have the potential to benefit people's daily lives? /!/ For example, a tutor AI that learns how the student learns and changes the course depending on it would be a cool use of AI in the future.\n\nAnother one might be using AI to support blind people by putting a microchip inside the blind person's brain, -----> camera !!!  that sees in front of them, then verbally translating what's in front of them (or maybe allowing them to see).\n\n\nI just want to hear some creative ideas, I want to see what AI has the potential to benefit in the future.", "link": "https://www.reddit.com/r/MachineLearning/comments/lhj1xq/d_what_future_ai_startup_ideas_have_the_potential/"}, {"autor": "susa_0077", "date": "2021-02-11 10:40:58", "content": "Is there any data augmentation technique to augment dust randomly in an -----> image !!! ? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/lhi5m0/is_there_any_data_augmentation_technique_to/"}, {"autor": "Tesg9029", "date": "2021-02-11 09:46:10", "content": "[P] Japanese genetic algorithm experiment to make a \"pornographic\" -----> image !!!  /!/ I don't have anything to do with this project myself, I've just been following it because I found it interesting.\n\n[This guy](https://twitter.com/miseromisero) made a [project](https://gamingchahan.com/ecchi/) where anyone is welcome to look at two images and choose which one they think is more \"pornographic\" to train the AI. There isn't really a goal, but it started out with the guy saying that the project \"wins\" when Google Adsense deems the image to be pornographic.\n\nThe project \"won\" [today](https://twitter.com/miseromisero/status/1359790904513466369) with the 11225th iteration getting Google to limit the Adsense account tied to the project. That being said it's still ongoing.\n\nYou can also take a look at all previous iterations of the image [here](https://gamingchahan.com/ecchi/exhi/)", "link": "https://www.reddit.com/r/MachineLearning/comments/lhhe8e/p_japanese_genetic_algorithm_experiment_to_make_a/"}, {"autor": "SnooPandas3529", "date": "2021-02-19 05:54:07", "content": "[P] Face2WEBTOON -----> Image !!!  Translation with Limited Data /!/ github : [https://github.com/sangyun884/Face2Webtoon](https://github.com/sangyun884/Face2Webtoon)\n\n&amp;#x200B;\n\n* Pretty good results with small custom dataset(less than 1.5K)\n* Able to preserve attributes(eyeglasses, gender) of source domain.", "link": "https://www.reddit.com/r/MachineLearning/comments/ln8b70/p_face2webtoon_image_translation_with_limited_data/"}, {"autor": "Yuqing7", "date": "2021-02-18 17:31:33", "content": "[R] Centroid Transformer: Learning to Abstract with Attention /!/ UT Austin New Paper is on [arXiv](https://arxiv.org/pdf/2102.08606.pdf).\n\n**Abstract**: Self-attention, as the key block of transformers, is a powerful mechanism for extracting features from the inputs. In essence, what self-attention does to infer the pairwise relations between the elements of the inputs, and modify the inputs by propagating information between the input pairs. As a result, it maps N inputs to N outputs and casts a quadratic O(N2 ) memory and time complexity. We propose centroid attention, a generalization of self-attention that maps N inputs to M outputs (M \u2264 N), such that the key information in the inputs are summarized in the smaller number of outputs (called centroids). We design centroid attention by amortizing the gradient descent update rule of a clustering objective function on the inputs, which reveals a underlying connection between attention and clustering. By compressing the inputs to the centroids, we extract the key information useful for prediction and also reduce the computation of the attention module and the subsequent layers. We apply our method to various applications, including abstractive text summarization, 3D vision, and -----> image !!!  processing. Empirical results demonstrate the effectiveness of our method over the standard transformers", "link": "https://www.reddit.com/r/MachineLearning/comments/lms633/r_centroid_transformer_learning_to_abstract_with/"}, {"autor": "Yuqing7", "date": "2021-02-18 17:27:31", "content": "[R] How Faithful is your Synthetic Data? Sample-level Metrics for Evaluating and Auditing Generative Models /!/ UCLA &amp; Cambridge University New Paper is on [arXiv](https://arxiv.org/abs/2102.08921).\n\n**Abstract**: Devising domain- and model-agnostic evaluation metrics for generative models is an important and as yet unresolved problem. Most existing metrics, which were tailored solely to the -----> image !!!  synthesis setup, exhibit a limited capacity for diagnosing the different modes of failure of generative models across broader application domains. In this paper, we introduce a 3-dimensional evaluation metric, (\u03b1-Precision, \u03b2-Recall, Authenticity), that characterizes the fidelity, diversity and generalization performance of any generative model in a domain-agnostic fashion. Our metric unifies statistical divergence measures with precision-recall analysis, enabling sample- and distribution-level diagnoses of model fidelity and diversity. We introduce generalization as an additional, independent dimension (to the fidelity-diversity trade-off) that quantifies the extent to which a model copies training data -- a crucial performance indicator when modeling sensitive data with requirements on privacy. The three metric components correspond to (interpretable) probabilistic quantities, and are estimated via sample-level binary classification. The sample-level nature of our metric inspires a novel use case which we call model auditing, wherein we judge the quality of individual samples generated by a (black-box) model, discarding low-quality samples and hence improving the overall model performance in a post-hoc manner.", "link": "https://www.reddit.com/r/MachineLearning/comments/lms2me/r_how_faithful_is_your_synthetic_data_samplelevel/"}, {"autor": "la_kola", "date": "2021-02-25 10:19:34", "content": "3D lighting map from 2d -----> image !!! ? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ls3h6w/3d_lighting_map_from_2d_image/"}, {"autor": "Wiskkey", "date": "2021-02-25 06:50:18", "content": "[P] Text-to------> image !!!  Google Colab notebook \"Aleph-Image: CLIPxDAll-E\" has been released. This notebook uses OpenAI's CLIP neural network to steer OpenAI's DALL-E image generator to try to match a given text description. /!/ [Google Colab notebook](https://colab.research.google.com/drive/1Q-TbYvASMPRMXCOQjkxxf72CXYjR_8Vp?usp=sharing). [Twitter reference](https://twitter.com/advadnoun/status/1364822183751471109).\n\n**Update**: \"DALL-E image generator\" in the post title is a reference to the [discrete VAE (variational autoencoder) used for DALL-E](https://www.reddit.com/r/MachineLearning/comments/lrroom/n_openai_has_released_the_encoder_and_decoder_for/). OpenAI [will not](https://github.com/openai/DALL-E/issues/4) release DALL-E in its entirety.\n\n**Update**: [A tweet from the developer](https://twitter.com/advadnoun/status/1364936070505238534), in reference to the white blotches in output images that often happen with the current version of notebook:\n\n&gt;Well, the white blotches have disappeared; more work to be done yet, but that's not bad!\n\n**Update**: Thanks to the users in the comments who suggested a temporary [developer-suggested fix](https://twitter.com/advadnoun/status/1364939598384623617) to reduce white blotches. To make this fix, change the line in \"Latent Coordinate\" that reads\n\n    normu = torch.nn.functional.gumbel_softmax(self.normu.view(1, 8192, -1), dim=-1).view(1, 8192, 64, 64)\n\nto\n\n    normu = torch.nn.functional.gumbel_softmax(self.normu.view(1, 8192, -1), dim=-1, tau = 1.5).view(1, 8192, 64, 64)\n\nby adding \", tau = 1.5\" (without quotes) after \"dim=-1\". The higher this parameter value is, apparently the lower the chance is of white blotches, but with the tradeoff of less sharpness. Some people have suggested trying 1.2, 1.7, or 2 instead of 1.5.\n\nI am not affiliated with this notebook or its developer.\n\nSee also: [List of sites/programs/projects that use OpenAI's CLIP neural network for steering image/video creation to match a text description](https://www.reddit.com/r/MachineLearning/comments/ldc6oc/p_list_of_sitesprogramsprojects_that_use_openais/).\n\nExample using text \"The boundary between consciousness and unconsciousness\":\n\nhttps://preview.redd.it/93r6bonvlkj61.png?width=512&amp;format=png&amp;auto=webp&amp;s=6505c22e4b02fe863054b081f662ee9f5db2bc95", "link": "https://www.reddit.com/r/MachineLearning/comments/ls0e0f/p_texttoimage_google_colab_notebook_alephimage/"}, {"autor": "[deleted]", "date": "2021-02-25 06:43:22", "content": "[P] Text-to------> image !!!  Google Colab notebook \"Aleph-Image: CLIPxDAll-E\" has been released. This notebook uses OpenAI's CLIP neural network to steer OpenAI's DALL-E image generator to try to match a given text description. Details in a comment. /!/ [deleted]", "link": "https://www.reddit.com/r/MachineLearning/comments/ls0afr/p_texttoimage_google_colab_notebook_alephimage/"}, {"autor": "Wiskkey", "date": "2021-02-25 03:32:45", "content": "[R] OpenAI has released the paper associated with DALL-E: \"Zero-Shot Text-to------> Image !!!  Generation\"", "link": "https://www.reddit.com/r/MachineLearning/comments/lrx40h/r_openai_has_released_the_paper_associated_with/"}, {"autor": "gamechanger4r", "date": "2021-02-25 02:19:20", "content": "[D] Backpropagating to LSTM inputs! /!/ Hi, I'm trying an architecture that is a sort of autoencoder, where the encoded representation is a string. In order to deal with differentiability issues, I'm not actually encoding it as a string, but as the softmax of the output of the encoder LSTM. Then, this tensor is fed into the decoder LSTM.\n\nHowever, I am noticing a huge difference (of the order of 10\\^3 or 10\\^4) between the grads calculated on the outputs of the decoder LSTM and the inputs during backpropagation. That is, it seems that the LSTM barely propagates back to the input sequence. Also, the input sequence tends to always collapse to a single character (that is, at every time step it outputs a tensor whose argmax would always be the same character).\n\nDoes any one have experience or references on this kind of architecture, where there is an autoencoder that encodes the -----> image !!!  to a string representation?", "link": "https://www.reddit.com/r/MachineLearning/comments/lrv328/d_backpropagating_to_lstm_inputs/"}, {"autor": "Wiskkey", "date": "2021-02-25 00:31:22", "content": "[N] OpenAI has released the encoder and decoder for the discrete VAE used for DALL-E /!/ Background info: [OpenAI's DALL-E blog post](https://openai.com/blog/dall-e/).\n\nRepo: [https://github.com/openai/DALL-E](https://github.com/openai/DALL-E).\n\n[Google Colab notebook](https://colab.research.google.com/github/openai/DALL-E/blob/master/notebooks/usage.ipynb).\n\nAdd this line as the first line of the Colab notebook:\n\n    !pip install git+https://github.com/openai/DALL-E.git\n\nI'm not an expert in this area, but nonetheless I'll try to provide more context about what was released today. This is one of the components of DALL-E, but not the entirety of DALL-E. This is the DALL-E component that generates 256x256 pixel images from a [32x32 grid of numbers, each with 8192 possible values](https://www.reddit.com/r/MachineLearning/comments/kr63ot/r_new_paper_from_openai_dalle_creating_images/gi8wy8q/) (and vice-versa). What we don't have for DALL-E is the language model that takes as input text (and optionally part of an -----> image !!! ) and returns as output the 32x32 grid of numbers.\n\nI have 3 non-cherry-picked examples of image decoding/encoding using the Colab notebook at [this post](https://www.reddit.com/r/MediaSynthesis/comments/lroigk/for_developers_openai_has_released_the_encoder/).\n\n**Update**: The [DALL-E paper](https://www.reddit.com/r/MachineLearning/comments/lrx40h/r_openai_has_released_the_paper_associated_with/) was released after I created this post.\n\n**Update**: A Google Colab notebook using this DALL-E component has already been released: [Text-to-image Google Colab notebook \"Aleph-Image: CLIPxDAll-E\" has been released. This notebook uses OpenAI's CLIP neural network to steer OpenAI's DALL-E image generator to try to match a given text description.](https://www.reddit.com/r/MachineLearning/comments/ls0e0f/p_texttoimage_google_colab_notebook_alephimage/)", "link": "https://www.reddit.com/r/MachineLearning/comments/lrroom/n_openai_has_released_the_encoder_and_decoder_for/"}, {"autor": "hardmaru", "date": "2021-02-25 00:30:39", "content": "[R] First return, then explore /!/ -----> Camera !!!  ready version of Go-Explore published in [Nature](https://www.nature.com/articles/s41586-020-03157-9.epdf?sharing_token=fVSAoXFGOipXiv03jaRGFtRgN0jAjWel9jnR3ZoTv0M2nmoar2g6_K5n8IBFPE90s2PCF4tYuP4UnEffP83tohRanjcxryz9Usln4Rw7idY6ufIsQYVgwwD0B8UgE7JYzmgOuMdgNqvVMg7GsfFiI2ZBi9PjjmJtJGEDwzRPFWc%3D)\n\n*Abstract*\n\nReinforcement learning promises to solve complex sequential-decision problems autonomously by specifying a high-level reward function only. However, reinforcement learning algorithms struggle when, as is often the case, simple and intuitive rewards provide sparse1 and deceptive2 feedback. Avoiding these pitfalls requires a thorough exploration of the environment, but creating algorithms that can do so remains one of the central challenges of the field. Here we hypothesize that the main impediment to effective exploration originates from algorithms forgetting how to reach previously visited states (detachment) and failing to first return to a state before exploring from it (derailment). We introduce Go-Explore, a family of algorithms that addresses these two challenges directly through the simple principles of explicitly \u2018remembering\u2019 promising states and returning to such states before intentionally exploring. Go-Explore solves all previously unsolved Atari games and surpasses the state of the art on all hard-exploration games1, with orders-of-magnitude improvements on the grand challenges of Montezuma\u2019s Revenge and Pitfall. We also demonstrate the practical potential of Go-Explore on a sparse-reward pick-and-place robotics task. Additionally, we show that adding a goal-conditioned policy can further improve Go-Explore\u2019s exploration efficiency and enable it to handle stochasticity throughout training. The substantial performance gains from Go-Explore suggest that the simple principles of remembering states, returning to them, and exploring from them are a powerful and general approach to exploration\u2014an insight that may prove critical to the creation of truly intelligent learning agents.\n\nLink to the paper: https://www.nature.com/articles/s41586-020-03157-9.epdf?sharing_token=fVSAoXFGOipXiv03jaRGFtRgN0jAjWel9jnR3ZoTv0M2nmoar2g6_K5n8IBFPE90s2PCF4tYuP4UnEffP83tohRanjcxryz9Usln4Rw7idY6ufIsQYVgwwD0B8UgE7JYzmgOuMdgNqvVMg7GsfFiI2ZBi9PjjmJtJGEDwzRPFWc%3D", "link": "https://www.reddit.com/r/MachineLearning/comments/lrrntx/r_first_return_then_explore/"}, {"autor": "downtownslim", "date": "2021-02-24 17:01:55", "content": "[R] Code for Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet /!/ Paper: [https://arxiv.org/abs/2101.11986](https://arxiv.org/abs/2101.11986)\n\nSource Code: [https://github.com/yitu-opensource/T2T-ViT](https://github.com/yitu-opensource/T2T-ViT)\n\nAbstract:\n\n&gt;Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, e.g., the Vision Transformers (ViT) for -----> image !!!  classification. The ViT model splits each image into a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global relation for classification. However, ViT achieves inferior performance compared with CNNs when trained from scratch on a midsize dataset (e.g., ImageNet). We find it is because: 1) the simple tokenization of input images fails to model the important local structure (e.g., edges, lines) among neighboring pixels, leading to its low training sample efficiency; 2) the redundant attention backbone design of ViT leads to limited feature richness in fixed computation budgets and limited training samples.  \nTo overcome such limitations, we propose a new Tokens-To-Token Vision Transformers (T2T-ViT), which introduces 1) a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure presented by surrounding tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow structure for vision transformers motivated by CNN architecture design after extensive study. Notably, T2T-ViT reduces the parameter counts and MACs of vanilla ViT by 200\\\\%, while achieving more than 2.5\\\\% improvement when trained from scratch on ImageNet. **It also outperforms ResNets and achieves comparable performance with MobileNets when directly training on ImageNet. For example, T2T-ViT with ResNet50 comparable size can achieve 80.7\\\\% top-1 accuracy on ImageNet.**", "link": "https://www.reddit.com/r/MachineLearning/comments/lrhn3c/r_code_for_tokenstotoken_vit_training_vision/"}, {"autor": "peleg1989", "date": "2021-02-24 15:55:22", "content": "[P] I made an -----> image !!!  recognition model written in NodeJs /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/lrg2sq/p_i_made_an_image_recognition_model_written_in/"}, {"autor": "saileee", "date": "2021-03-09 11:09:41", "content": "[D] Research at the intersection of Machine Learning and Cognitive Science? /!/ Forgive me if this topic is not appropriate for the sub, I will remove it if so. I am an undergraduate Computer Science student looking into starting a post-graduate degree in the field of AI. My current main interest is to do fundamental AI research. Lurking in this sub it seems like many of the current advances in ML, while in many ways ground-breaking, are in other ways pretty incremental in the sense that they let computers do certain things better and better, but are not necessarily opening up new classes of tasks for computers in the way we'd need them to in order to crack the question of intelligence (e.g. creating better and better -----> image !!!  recognizers and NLP agents but not getting that much closer to understanding and recreating reasoning and structure of thought). Is this a fair assessment, or is this just a case of me not really understanding the field thoroughly enough? \nIn any case, I am curious as someone with a little bit of cogsci background to see if there are lots of groups seeking to pursue cognitive science and machine learning in tandem. Has someone tried to implement the Chomskian Merge operator in neural networks, or sought to build a model of Fodor's Language of Thought? Or in other words and to put my point succinctly, is ML the way to go if I want to pursue AI research, or should I focus on something like MLxCogSci or MLxCompNeuro or something similar?\n\n\nMany thanks for reading.", "link": "https://www.reddit.com/r/MachineLearning/comments/m149o6/d_research_at_the_intersection_of_machine/"}, {"autor": "Ewazd", "date": "2021-03-09 08:34:39", "content": "[R] Generate text from image+text /!/ Are there any recommended github repositories for models that receive -----> image !!!  and text and generate text from that?", "link": "https://www.reddit.com/r/MachineLearning/comments/m12b55/r_generate_text_from_imagetext/"}, {"autor": "Feynmanfan85", "date": "2021-03-09 01:46:53", "content": "Analyzing Dataset Consistency [R] /!/ Real world data is often observably consistent over small distances. For example, the temperature on the surface of an object is probably going to be roughly the same over distances that are small relative to its total area. Similarly, the average color in an -----> image !!!  of a real world object generally doesn\u2019t change much over small distances, other than at its boundaries. Many commonly used datasets are also measurably consistent to some degree, in that classifications do not change over relatively small distances.\n\nIn the note below, I show, formally, through a series of lemmas, that if a dataset meets a set of fairly reasonable requirements, regarding its consistency, then you cannot beat the nearest neighbor method, in terms of accuracy.\n\nYou can find related code in the same project (\"Information Theory\").\n\nFinally, because nearest neighbor can be implemented using almost entirely vectorized operations, it would have at worst a linear runtime as a function of the number of rows in the dataset, on a sufficiently parallel machine.\n\nhttps://www.researchgate.net/publication/349908777_Analyzing_Dataset_Consistency", "link": "https://www.reddit.com/r/MachineLearning/comments/m0vu69/analyzing_dataset_consistency_r/"}, {"autor": "cadegord", "date": "2021-03-08 19:28:46", "content": "[Project] Learning Artistic Style From Language Features using CLIP /!/ With the recent creative uses of CLIP, I became curious to see their success in style transfer. Here are 3 observations with some toy examples.\n\nCode: https://github.com/Zasder3/CLIP-Style-Transfer\n\nLearning Artistic Style From Language Features using CLIP:\n\n&amp;#x200B;\n\nhttps://i.redd.it/co5sc9ydjul61.gif\n\n \n\nInspired by recent works of Or Patashnik and Chris Olah, I set out to see if a Gatys et al. esque style transfer setup using CLIP similarity instead of Gram Matrices would be possible.\n\nThe conclusion: yes-ish!\n\nStarting with an -----> image !!!  of a modern house an L2 loss and the target-phrase \"a salvador dali painting of a modern house\" we generate an -----> image !!!  that explores the generality of CLIP's knowledge. It may ignore the structure of the house, but it embeds a classic Dali face. From here, I knew there was more to unpack with this method.\n\n&amp;#x200B;\n\n[Normal Dali Painting](https://preview.redd.it/o2j2uikvjul61.jpg?width=668&amp;format=pjpg&amp;auto=webp&amp;s=8ebd47654c01ab152f5693117de2c1e12c3cd05b)\n\n[A Dali-style face emerges in from our method](https://preview.redd.it/3g7oldkvjul61.png?width=224&amp;format=png&amp;auto=webp&amp;s=e30d53cd3474240d329bef11e0a9a5227b753d58)\n\nCLIP possesses an understanding of the artist's motifs and even color palette. See the training GIF of \"a salvador dali style painting of a modern house\" and VGG19's conv4\\_1 as a content extractor.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/sdcdwfbqtul61.jpg?width=224&amp;format=pjpg&amp;auto=webp&amp;s=c265b921e164bb8a20396f661a96e9d199edec2c\n\nAnecdotally I found better results when I upped my specificity about the work at hand. The phrase \"a van gogh starry night style painting of a modern house\" produces more vivid results with 3 interesting caveats other style transfer methods often don't have.\n\n1. There is an understanding of spatial relationships. The locations of the moon (upper right) and wispy clouds are preserved. This is infeasible with Gram Mats b/c they only measure feature correlations!  \n\n&amp;#x200B;\n\nhttps://preview.redd.it/k4j5bkzytul61.jpg?width=224&amp;format=pjpg&amp;auto=webp&amp;s=8836ae089056292b4d90f205e00046096c20868e\n\nhttps://preview.redd.it/fga1nlzytul61.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=ce681ce40de6b232bf2aa9f8bdb6ad9b0747a768\n\n2. Ghost faces and self-portraits. Along the right side, we find a portrait very similar to those of Van Gogh. This also happens when using an L2 over VGG and other hparam tweaks.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/3ci3daikuul61.jpg?width=600&amp;format=pjpg&amp;auto=webp&amp;s=e42e3b254936c93d1fea3616a8700094192f9604\n\nhttps://preview.redd.it/0uuje9ikuul61.jpg?width=224&amp;format=pjpg&amp;auto=webp&amp;s=2c0c8750835a0a9299f65b88af05b0e5926f4b96\n\nhttps://preview.redd.it/urtn1bikuul61.jpg?width=224&amp;format=pjpg&amp;auto=webp&amp;s=d8cc8305108b171e09e8710ad371f75f31d38317\n\n3. Abstract/general style and motif comprehension. (A weaker qualitative conjecture) The Starry Night example grasped some of his brushstrokes and the Dali examples found the greater motifs present across multiple of his works.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/yx02btfjvul61.jpg?width=224&amp;format=pjpg&amp;auto=webp&amp;s=3f9ddca246b328f726a04641d8983bcce97bdb83\n\nhttps://preview.redd.it/jm6ujkcevul61.jpg?width=224&amp;format=pjpg&amp;auto=webp&amp;s=7f98b12592e138b040941f25f4c25d99699bf778\n\nThis experiment was partially successful, but clearly has room for improvement. Maybe a less aggressive TV coefficient. I'm unsure if style transfer would even be the perfect name for this technique, it's closer to neuron excitation/dot product maximization. \n\nCLIP has a lot of general knowledge about the world and art and it was able to semi-successfully do style transfer. I'd love to hear your thoughts and critiques as well as any new creations this spurs!", "link": "https://www.reddit.com/r/MachineLearning/comments/m0nshm/project_learning_artistic_style_from_language/"}, {"autor": "LaloSalamanca__", "date": "2021-03-08 19:18:28", "content": "[P] Recommendation on what statistical testing to use to test the results /!/ Hello everyone, im an undergraduate student who is undergoing my thesis. our thesis is about detecting whether a mosquito larva is aedes aegypti or non aedes aegypti and then count it real time using a -----> camera !!! , then we compare it to the actual count. so far it has been a success, the system is able to differentiate aedes to non aedes larvae and we've had great results.\n\nour only problem now is we don't have any idea on what type of statistical testing to use. we tried to use a common one which is confusion matrix but according to our professor its not compatible as we are not classifying but counting. since then he didn't talked to us about advice or anything. we have tried to search in the internet but it's all about classification.\n\ndo you guys have any idea on what test to use? one that is suitable for counting. a simple accuracy test is not accepted. thanks in advance!", "link": "https://www.reddit.com/r/MachineLearning/comments/m0njgq/p_recommendation_on_what_statistical_testing_to/"}, {"autor": "aidang95", "date": "2021-03-08 18:18:18", "content": "[P] What is the best way to detect multiple object in a single -----> image !!! ? /!/ I am starting out work on a little project but I am a little unsure what is the best/easiest path to take to achieve my aims.\n\nI am wanting to first, train a machine learning model on my custom dataset of images and then use that trained model in order to detect multiple objects within a single image and then store the detected labels for use later on in the project.\n\n&amp;#x200B;\n\nI have taken a look at YOLOv3 but I cant seem to find any definitive instruction on training a custom YOLOv3 model, only using pre trained models where as I wish to train my own model on my own dataset.", "link": "https://www.reddit.com/r/MachineLearning/comments/m0m3h4/p_what_is_the_best_way_to_detect_multiple_object/"}, {"autor": "aidang95", "date": "2021-03-08 18:17:12", "content": "What is the best way to detect multiple objects within a single -----> image !!! ? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/m0m2et/what_is_the_best_way_to_detect_multiple_objects/"}, {"autor": "Yuqing7", "date": "2021-03-16 04:44:24", "content": "[N] Yann LeCun Team's Barlow Twins Method Boosts SSL in Image Representation via Redundancy Reduction /!/ Yann LeCun and a team of researchers propose Barlow Twins, a method that learns self-supervised representations through a joint embedding of distorted images, with an objective function that can make the embedding vectors almost identical while reducing redundancy between their components.\n\nHere is a quick read: [Yann LeCun Team's Barlow Twins Method Boosts SSL in Image Representation via Redundancy Reduction](https://syncedreview.com/2021/03/15/yann-lecun-teams-barlow-twins-method-boosts-ssl-in------> image !!! -representation-via-redundancy-reduction/)\n\nThe paper *Barlow Twins: Self-Supervised Learning via Redundancy Reduction* is on [arXiv](https://arxiv.org/pdf/2103.03230.pdf).", "link": "https://www.reddit.com/r/MachineLearning/comments/m6179q/n_yann_lecun_teams_barlow_twins_method_boosts_ssl/"}, {"autor": "Gulog", "date": "2021-03-16 02:22:46", "content": "[D] How to manipulate weights in StyleGAN2 /!/ Hi! I'm very new to machine learning and -----> image !!!  processing using ML. Over the past week I've been using co-labs to train a small dataset just to test stuff out.\n\nI am very intersted in an art series called \"Neural Glitch\" , where images are generated while weights are either altered, deleted etc. \n\nI was wondering is this possible using co-labs? I've done some research but I cant find any resource in regards to locating the weights in the code and then manipulating them. I don't really know what I'm looking for honestly.. \n\nI have coding experience in Java and picking up Phyton at the moment, so I shouldn't have any issue tweaking a few variables but I am interested in cracked media and want to explore it using ML.\n\nAny help would be great ! Either to other Models which would benefit me more or ideas on how to alter the weights.\n\nThank you!!", "link": "https://www.reddit.com/r/MachineLearning/comments/m5yrsz/d_how_to_manipulate_weights_in_stylegan2/"}, {"autor": "gpahul", "date": "2021-03-15 20:56:38", "content": "[D] -----> Image !!!  to image translation for fashion images? /!/ I've been searching for image to image translation method for cleaning fashion images like wrinkled shirt to clean shirt, remove dust or abnormal patterns from cloth etc.\n\n&amp;#x200B;\n\nI could not get much resources apart from getting to know that this is a problem of conditional GANs and PIX2PIX might be a possible GAN to look for!\n\n&amp;#x200B;\n\nCould you suggest anything in this? Any pointers are highly appreciated.\n\n&amp;#x200B;\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/m5uo4a/d_image_to_image_translation_for_fashion_images/"}, {"autor": "gamechanger4r", "date": "2021-03-15 20:31:48", "content": "[D] Networks with gumbel softmax don't seem to learn /!/ I'm trying to train a classifier on MNIST dataset with a \"hard\" encoding in the middle. The working is simple: I have a convolutional net, then a linear layer that outputs a logit of 10 dimensions, and then the logit goes into the straight through gumbel softmax. Then, a linear classifier, and a regular softmax output layer.\n\nSo, in other words, there is a discrete latent variable with 10 possible values. The network outputs a categorical distribution over these 10 possible values, the ST gumbel softmax samples from this distribution, and the one-hot encoded value of the categorical variable is mapped into a prediction for the label of the mnist -----> image !!!  (the number of the digit).\n\nExplaining even more simply: I have a network that classifies the mnist digits into an internal variable with 10 possible values (say, 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j') and then translates these values into the digit class label provided by the mnist dataset. The classification into the internal variable is \"hard\", that is, the output of this internal classification is one hot encoded, and the network has to learn everything in a supervised manner with mnist.\n\nNow, this just doesn't work. I noticed that, if I remove the last linear classifier (and, therefore, make the output of the whole network to be the one hot encoded output of the ST gumbel softmax function) then it works, that is: if there is no extra \"translation\" step between a internal class name for each digit and the external, actual label, then everything works. But the extra linear layer, with the extra translation step, makes the network unable to learn for some reason.\n\nDoes anyone have tips on training this kind of architecture with a gumbel softmax operation in the middle? What can be used to help the backpropagation to work more efficiently? Thanks", "link": "https://www.reddit.com/r/MachineLearning/comments/m5u4sp/d_networks_with_gumbel_softmax_dont_seem_to_learn/"}, {"autor": "raven3813", "date": "2021-03-15 16:55:52", "content": "[Project] Neural Image Editor: GAN-based -----> image !!!  editing tool /!/ &amp;#x200B;\n\n*Processing video marun5ze28n61...*\n\nWe release the neural -----> image !!!  editor [demo](https://colab.research.google.com/github/raven38/-----> image !!! _edit/blob/master/-----> image !!! _edit_demo.ipynb). It combines [Image2StyleGAN](https://arxiv.org/abs/1904.03189) and [SeFa](https://genforce.github.io/sefa/#demo). You upload your own image and you can edit attributes of the image. It only supports face images now.  \n\nWelcome any suggestion and comment including the idea that improves the software and similar project or research.\n\nGithub: [https://github.com/raven38/image\\_edit](https://github.com/raven38/image_edit)", "link": "https://www.reddit.com/r/MachineLearning/comments/m5p6sg/project_neural_image_editor_ganbased_image/"}, {"autor": "IcyTransportation470", "date": "2021-03-15 09:38:10", "content": "[D] Looking for computational -----> photography !!!  SOTA papers /!/ So I recently bought a OnePlus Nord phone. The other phone that I could've gone for is a Pixel 4a but it was out of stock. My biggest gripe with OnePlus is their terrible camera software. Google's software magic is soo good that it is kind of embarrassing that a phone with 4x better sensor takes worse photographs. Can anyone point me towards what kind of photography algorithms Google uses to in Pixel devices. Also, are there any good open source projects working in that direction?", "link": "https://www.reddit.com/r/MachineLearning/comments/m5h0wu/d_looking_for_computational_photography_sota/"}, {"autor": "lgmlrtm", "date": "2021-03-14 22:55:22", "content": "[D] (Pretrained) Multi-Label -----> Image !!!  Classifier with more useful labels than Resnet? /!/ Hey,\n\nI'm currently building a photo organizing platform (similar to google photos, but self hosted + open source) and I reached the point where I need to include Image Classification so that the user can search for things in photos. I also want to users to browse the photos by classification label.\n\n&amp;#x200B;\n\nI looked into it and it seemed that nearly everything uses the ImageNet-Dataset, but the label list seems completely unsuitable for my project (no categories for beach, night, party, person; but 10 different categories for snakes). Is there any other pretrained dataset that uses labels more useful for practical applications? I found Google's ML Kit Vision Label list which seems really promissing (everything I would like to have included is included [https://developers.google.com/ml-kit/vision/image-labeling/label-map](https://developers.google.com/ml-kit/vision/image-labeling/label-map)), but I can't find a way to download that model (I need to run it on a server, not on a phone). Are there other datasets with not too specific, general purpose labels that I'm missing?", "link": "https://www.reddit.com/r/MachineLearning/comments/m56k1l/d_pretrained_multilabel_image_classifier_with/"}, {"autor": "FatasticAI", "date": "2021-03-14 21:32:29", "content": "[D] How to design a convolution neural network whose input is an 5x4 matrix, and output is also an 5x4 matrix? /!/ I'm being given an input of 5x4 matrix whose element value varies from 0 to 100. I would like my CNN to take this 5x4 matrix as input, and output another 5x4 matrix, whose element values also vary from 0 to 100, is there any CNN architecture can do this?\n\nWhat I have known for now is something like -----> image !!!  classification, where input is a matrix, and output is a vector or binary value (0 or 1), but how to make its output also be a matrix with same dimension ? Any help would be appreciated. Thanks in advance.", "link": "https://www.reddit.com/r/MachineLearning/comments/m54thk/d_how_to_design_a_convolution_neural_network/"}, {"autor": "StandardDull3128", "date": "2021-03-30 15:01:37", "content": "[Discussion] Can I use CNNs to solve this problem? /!/ What does your intuition/experience/expertise tell you? Would I be able to train a CNN to solve a binary classification problem described below and expect a high accuracy (let's say &gt;90%)?\n\n**The problem:** Predicting whether an -----> image !!!  contains one type of texture (class 0) or it contains several textures and clear borders among them (class 1).\n\n**The data:** The data is high resolution (5 centimeters / pixel) agricultural imagery obtained via remote sensing (drone imaging). So pictures depicting different crops, weeds, roads, soil, bushes, etc. I would feed the model square images of 256x256px (or 128x128px).\n\n**Examples of images:** (\"neg\" is class 0 and \"pos\" is class 1)[https://imgur.com/a/gnKbncA](https://imgur.com/a/gnKbncA)", "link": "https://www.reddit.com/r/MachineLearning/comments/mghzjx/discussion_can_i_use_cnns_to_solve_this_problem/"}, {"autor": "Nikki_iva", "date": "2021-03-30 07:21:51", "content": "[Project] Building the Ultimate companion chatbot app. Is it possible or farfetched? Insight much appreciated! /!/  Hello fellow Redditors, I hope you're all doing amazing!\n\nI'm thinking of building a companion chatbot app something along the lines of [Replika](https://replika.ai/) &amp; [Mitsuku](https://chat.kuki.ai/). I downloaded Replika a couple of weeks back and feel like there are lost opportunities when it comes to unleashing the full potential of the app. So naturally, I started thinking of making a better version of the app. Since I'm new to the world of A.I I wanted to ask you ( the experts of this community ) if the features I have in mind are possible to implement!\n\n***\\*Please note that the following is a rough idea for the app features &amp; for the purpose of this question we will refer to the bot as \"EVE\"\\****\n\n***\\*This section in the TL;DR of each topic for quick viewing if you don't want to get into the details\\****\n\n***Texting:*** *Human like reply- Expert in topics- Replies that make sense*\n\n***Photo/Video:*** *Can send -----> photo !!! s and video*\n\n***Memory &amp; Emotions:*** *Retains info about user all the time*\n\n***Social media:*** *Connect to social media so EVE better understands you, helps determine best -----> photo !!! .*\n\n***Netflix/ Spotify:*** *EVE can see what you watch and listen to and discuss it with you.*\n\n***Augmented reality:*** *EVE can play games with you, interact with environment.*\n\n***Video Chat:*** *Facetime EVE*\n\n***Voice messages:*** *Be able to send and receive voice messages from EVE*\n\n***Celebrity EVE:*** *Interact with Elon Musk EVE or celebrity of your liking.*\n\n***Re-incarnation:*** *Bring back your loved ones in EVE*\n\n***\\*End of TL;DR\\****\n\n**Texting:** EVE's conversations need to be more human-like, with long and short replies depending on the context of what the user says and should be able to adapt on the flow of conversation. EVE should be able to become an expert in the users topic of interest and have it's personality adapt to the user liking.\n\n**EVE should be able to send videos/photos/memes:** I hope that it's possible to have EVE connected to google/Reddit or something along the lines so she can send trending memes and up-to-date pictures and videos. Maybe even be able to send the user current news.\n\n**Memory&amp; Emotions**: Be able to consistently retain information about the users and understand how the users feel.\n\n**Social media Integration:** Connect EVE to your social media accounts so she can better understand you as the user and follow what you post on the web. Let's say you post a picture to Instagram, EVE would be able to send you a link to the post and saying something like \"You look great in this picture etc...\" You could also present EVE with some pictures you're planning on posting and have EVE rate the best picture.\n\n**Netflix &amp; Spotify integration:** Connect EVE to Netlif &amp; Spotify so it can monitor what shows and music you have been watching/listening to, this would give EVE something to talk about with you and be able to send you shows and music, this way EVE feels more human-like.\n\n**Augmented Reality:** When in AR mode EVE should be able to move and interact with the user. There could be an AR game function where you can play different games with EVE in order to entertain the user. EVE should look hyper realistic &amp; when AR glasses come out you can have EVE doing things around the house.\n\n**Video Chat:** Ability to \"Facetime\" EVE both in AR mode and call mode, EVE would look realistic and be able to hold a conversation with the users for prolonged periods of time. Also included a human voice that doesn't sound robotic.\n\n**Voice Message:** Ability to send EVE a voice message and have it be able to understand what you said and reply with a voice message also.\n\n**Celebrity EVE**: I have in mind making a virtual celebrity and integrating them into the app, let's say we take Elon Musk and add him to the platform so users can chat with their favorite celebrity. This would require we get consent from celebrities and analyze their social media profiles etc...\n\n**Re-incarnation:** This is something interesting that I would like to possibly implement after conducting studies on how it will affect the users. Basically, let's say your loved one dies, you could upload all their photos/ videos and data to the platform and have EVE become your loved one. You could text with them, call/facetime them, and also have them appear in AR mode. \\**Note: I'm aware that this is a sensitive topic for most but this feature is not definitive*\\*\n\nPlease share your opinion on if this is possible, and any feedback you may have. At first I plan on building an MVP and then iterate on the project.\n\nAlso, if it's not too much to ask, I would like some guidance on the type of developer I should hire/ co-found and what qualities should I look for? If you're interested in this project please let me know :)\n\nThank you very much, I'm looking forward to hearing from you!", "link": "https://www.reddit.com/r/MachineLearning/comments/mgac28/project_building_the_ultimate_companion_chatbot/"}, {"autor": "iliketobelieve", "date": "2021-03-29 17:14:57", "content": "[D]: #activity recommendation /!/ I'm trying to build a hashtag recommendation using data from some selected twitter accounts. The recommendation should be about the activity being performed in the -----> image !!! . Eg: If someone is parasailing in the image than I want my model to return #parasailing. I want to important scrape out data from description of the image on twitter. Any idea which nlp technique will help me achieve this task of activity # recommendation?\n\nI'm stuck on this since a while now so any help or ideas are appreciated.", "link": "https://www.reddit.com/r/MachineLearning/comments/mfuea4/d_activity_recommendation/"}, {"autor": "cgnorthcutt", "date": "2021-03-29 15:56:36", "content": "[R] Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks /!/ Hi Reddit! We\u2019re excited to share our latest research: label errors are pervasive in 10 of the most-commonly used benchmark test sets used in most machine learning research. We investigate the implication of these label errors, in particular, how they affect the stability of ML model benchmark rankings.\n\n**Demo**: [https://labelerrors.com](https://labelerrors.com/)\n\n**Blog Post**: [https://l7.curtisnorthcutt.com/label-errors](https://l7.curtisnorthcutt.com/label-errors)\n\n**Abstract**: We identify label errors in the test sets of 10 of the most commonly-used computer vision, natural language, and audio datasets, and subsequently study the potential for these label errors to affect benchmark results. Errors in test sets are numerous and widespread: we estimate an average of 3.4% errors across the 10 datasets, where for example 2916 label errors comprise 6% of the ImageNet validation set. Putative label errors are identified using confident learning algorithms and then human-validated via crowdsourcing (54% of the algorithmically-flagged candidates are indeed erroneously labeled). Traditionally, machine learning practitioners choose which model to deploy based on test accuracy \u2014 our findings advise caution here, proposing that judging models over correctly labeled test sets may be more useful, especially for noisy real-world datasets. Surprisingly, we find that lower capacity models may be practically more useful than higher capacity models in real-world datasets with high proportions of erroneously labeled data. For example, on ImageNet with corrected labels: ResNet-18 outperforms ResNet-50 if the prevalence of originally mislabeled test examples increases by just 6%. On CIFAR-10 with corrected labels: VGG-11 outperforms VGG-19 if the prevalence of originally mislabeled test examples increases by just 5%.\n\n**Paper**: [https://labelerrors.com/paper.pdf](https://labelerrors.com/paper.pdf)\n\nJoint work with Curtis Northcutt, Anish Athalye, and Jonas Mueller.\n\n[An example label error from each category for -----> image !!!  datasets. The figure shows given labels, CL-guessed alternatives, human-validated corrected labels, and also the second label for multi-class data points. A browser for all label errors across all 10 datasets is available at https:\\/\\/labelerrors.com. Errors from text and audio datasets are also included in the website.](https://preview.redd.it/07uhmpp8ozp61.png?width=1900&amp;format=png&amp;auto=webp&amp;s=bc3e3c42a4c9a64055bd2a225284f98b8a3dc86d)", "link": "https://www.reddit.com/r/MachineLearning/comments/mfsn18/r_pervasive_label_errors_in_test_sets_destabilize/"}, {"autor": "CulturalAfternoon306", "date": "2021-03-29 15:21:31", "content": "What\u2019s the best deep learning algorithm for -----> image !!!  inpainting. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/mfrw59/whats_the_best_deep_learning_algorithm_for_image/"}, {"autor": "temakone", "date": "2021-03-29 12:15:21", "content": "[R] Swin Transformer: New SOTA backbone for Computer Vision\ud83d\udd25 /!/ **Swin Transformer: New SOTA backbone for Computer Vision** \ud83d\udd25*MS Research Asia*\n\n# \ud83d\udc49 What?\n\nNew vision Transformer architecture called Swin Transformer that can serve as a backbone in computer vision instead of CNNs.\n\n# \u2753Why?\n\nThere are two main problems with the usage of Transformers for computer vision.\n\n1. Existing Transformer-based models have tokens of a fixed scale. However, in contrast to the word tokens, visual elements can be different in scale (e.g. objects of varying sizes on the scene)\n2. Regular self-attention requires quadratic of the -----> image !!!  size number of operations, limiting applications in computer vision where high resolution is necessary (e.g., instance segmentation).\n\n# \ud83e\udd4a The main ideas of the Swin Transformers:\n\n1. **Hierarchical feature maps** where at each level of hierarchy Self-attention is applied within local non-overlapping windows. The size of the windows is progressively increased with the network depth (inspired by CNNs). This enables building architectures similar to feature pyramid networks (FPN) or U-Net for dense pixel-level tasks.\n2. **Window-based Self-attention** reduces the computational overhead.\n\n# \u2699\ufe0f Overall Architecture consists of repeating the following blocks:\n\n\\- Split RGB image into non-overlapping patches (tokens).\n\n\\- Apply MLP to translate raw features into an arbitrary dimension.\n\n\\- Apply 2 consecutive Swin Transformer blocks with Window self-attention: **both blocks have the same window size, but the second block uses shifted by \\`patch\\_size/2\\` windows which allows information flow between non-overlapping windows**.\n\n\\- Downsampling layer: Reduce the number of tokens by merging neighboring patches in a 2x2 window, and double the feature depth.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/xtjhyflalyp61.png?width=1920&amp;format=png&amp;auto=webp&amp;s=b0f8539b4e1779ba8263281e4ff2974562858c0d\n\n&amp;#x200B;\n\nhttps://preview.redd.it/z95z4ycclyp61.png?width=1010&amp;format=png&amp;auto=webp&amp;s=0e27b6b8fb511da3be3b9647da4bacd2b8411dc3\n\n# \ud83e\uddbe Results\n\n\\+ **Outperforms SOTA by a significant margin on COCO segmentation and detection tasks and ADE20K segmentation.**\n\n\\+ **Comparable accuracy to the EfficientNet** family on ImageNet-1K classification, while being faster.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/giw5nz4dlyp61.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=d8eb93e37dd44ee75e7476c584d41fb14d3b760a\n\n# \ud83d\udc4cConclusion\n\nWhile Transformers are super flexible, researchers start to **inject in Transformers inductive biases similar to those in CNNs**, e.g., local connectivity, feature hierarchies. And this seems to help tremendously!\n\n&amp;#x200B;\n\n\ud83d\udcdd Paper [https://arxiv.org/abs/2103.14030](https://arxiv.org/abs/2103.14030)\n\n\u2692 Code (promissed soon) [https://github.com/microsoft/Swin-Transformer](https://github.com/microsoft/Swin-Transformer)\n\n\ud83c\udf10 TL;DR blogpost [https://xzcodes.github.io/posts/paper-review-swin-transformer](https://xzcodes.github.io/posts/paper-review-swin-transformer)\n\n\\--\n\n\ud83d\udc49  \ud83d\udc49  \ud83d\udc49  Join my Telegram channel [\"Gradient Dude\"](https://t.me/gradientdude) not to miss the latest posts like this![https://t.me/gradientdude](https://t.me/gradientdude)", "link": "https://www.reddit.com/r/MachineLearning/comments/mfo8xo/r_swin_transformer_new_sota_backbone_for_computer/"}, {"autor": "cygn", "date": "2021-01-07 14:45:02", "content": "[D] How to make your training data public but prevent other people from training a model on it? /!/ Let's say you are building an -----> image !!!  classification model that you don't want to be stolen. But you also want to make some of your training data (images + labels) public. Reasons for that might include: verification of labels by annotators, who could keep the images + labels and sell them. Or you have a forum where people can look at images + predictions for those images. \n\nSo what I'd like to do is transform the images in a way that makes training a model on them very hard or impossible. \nI looked in the literature, but couldn't find anything relevant. \n\nMy idea:\nembed some watermark with the class of the image. If someone trains a model on those images, the neural network will exploit the watermarks, i.e. go the path of least resistance and skip learning shapes &amp; textures. The model will have perfect performance on the training set, but will not generalize at all. \nOnly issue: the watermarks need to be impossible to remove. So for example just writing the class in every 10th pixel would not work, because adding blur to the image or resizing it would be enough to circumvent it.\n\nIs this a good idea or has anybody got a better idea? Also what would be a good watermarking technique?", "link": "https://www.reddit.com/r/MachineLearning/comments/ksemgn/d_how_to_make_your_training_data_public_but/"}, {"autor": "Infinite_Grocery_819", "date": "2021-01-07 03:44:11", "content": "[D] Question: -----> Image !!!  completion by using deep learning /!/ I have 2D array data but here, there are many nan values (like missing data) and can make images. (However, the dimension and shape are the same for each data and the position of nan value is different.) And I tried to fill the empty region with the image by using the deep learning method such as GAN, DCGAN, pix2pix, etc in python. However, I can't understand how to know the value of the filled region using deep learning techniques. Because after using the deep learning method, the output is pixel value right?\n\nSo my question is,\n\n1. Is it possible to fill value in missing regions by using data with nan value?\n2. If the first question is possible, How can I know the value of the missing region using deep learning techniques (not pixel value)?\n3. Or, is there someone who can give you some other good way? (like 2D-array deep learning method, etc)\n\nThanks. \n\nBest regards", "link": "https://www.reddit.com/r/MachineLearning/comments/ks4ul2/d_question_image_completion_by_using_deep_learning/"}, {"autor": "benitorosenberg", "date": "2021-01-06 19:45:34", "content": "[R] Workshop on Graph Learning Benchmarks (GLB 2021) @ The Web Conference 2021 /!/ Please consider submitting your work to the Workshop on Graph Learning Benchmarks (GLB 2021) @ The Web Conference 2021. Link to the webpage of the workshop:\n\n\n[https://graph-learning-benchmarks.github.io/](https://graph-learning-benchmarks.github.io/)\n\nLet us replace Cora, Citeseer, and PubMed once and for all.\n\nImportant Dates\n\n* Submission deadline: Feb 15, 2021\n* Acceptance notification: Mar 8, 2021\n* -----> Camera !!! -ready version due: Mar 22, 2021", "link": "https://www.reddit.com/r/MachineLearning/comments/krvkeq/r_workshop_on_graph_learning_benchmarks_glb_2021/"}, {"autor": "lukasus", "date": "2021-04-08 16:41:16", "content": "[P] Mask2Face: How We Built AI That Shows the Face Beneath the Mask /!/ Can you virtually remove a face mask to see what a person looks like underneath? STRV Machine Learning team proves it\u2019s possible via an -----> image !!!  inpainting-based ML solution. Here\u2019s exactly how we approached the problem \u2014 from the preconditions to the implementation, results, and future improvements:   \n\n[Mask2Face: How We Built AI That Shows the Face Beneath the Mask](https://www.strv.com/blog/mask2face-how-we-built-ai-that-shows-face-beneath-mask-engineering)\n\nAnd here is the link to the project on GitHub: [https://github.com/strvcom/strv-ml-mask2face](https://github.com/strvcom/strv-ml-mask2face)", "link": "https://www.reddit.com/r/MachineLearning/comments/mmvsvw/p_mask2face_how_we_built_ai_that_shows_the_face/"}, {"autor": "hash_t", "date": "2021-04-08 09:09:27", "content": "[P] EfficientNet - -----> image !!!  recognition model written in typescript and Nodejs", "link": "https://www.reddit.com/r/MachineLearning/comments/mmnstb/p_efficientnet_image_recognition_model_written_in/"}, {"autor": "StrasJam", "date": "2021-04-08 07:31:15", "content": "[D] Facebook's use of Softmax in multi-label classification /!/ I was reading this [paper](https://arxiv.org/pdf/1805.00932.pdf)  put out by a group of researchers at Facebook where they found that using a softmax and CE loss function during  training led to improved results over sigmoid + BCE. During training they change the one-hot label vector such that each '1' is divided by the  number of labels for    the given -----> image !!!  (e.g. from \\[0, 1, 1, 0\\] to \\[0, 0.5,  0.5, 0\\]).\n\nHowever, they do not mention how this could then be used in the  inference stage, because the required threshold for selecting the  correct labels is not clear and would theoretically need to be set based upon the expected number of labels for the image (which is information which wouldn't be available at inference).\n\nHas anyone else read this paper or have an idea how this could work?", "link": "https://www.reddit.com/r/MachineLearning/comments/mmmj0y/d_facebooks_use_of_softmax_in_multilabel/"}, {"autor": "gavvburns", "date": "2021-04-08 05:24:52", "content": "[D] Data Transformation in Production /!/ I have always received the best answers from this community so here we are again:\n\nI am currently building my model out to be beta tested in production (with a subset of total features; mostly concerned about the pipelines and endpoints being built out on AWS for proof of concept sake). One of the biggest issues I have been faced with is the transformation of real production data.\n\nIn the -----> picture !!!  below you will see a LabelEncoder, Boxcox1p, and get\\_dummies. I know that the get\\_dummies will be switched out for sklearn.preprocessing.OneHotEncoder but I wanted to see if anyone could lead me in the right direction so all possible live test transformations will match what they would have been if in the training set. Thank you guys so much!\n\nhttps://preview.redd.it/1r8o81mdxvr61.png?width=1934&amp;format=png&amp;auto=webp&amp;s=8ab3151423e47a790ecc8a8af0a6cb5c7c6503ed", "link": "https://www.reddit.com/r/MachineLearning/comments/mmkt9c/d_data_transformation_in_production/"}, {"autor": "kpang0", "date": "2021-04-07 19:19:57", "content": "[P] Vald: a highly scalable distributed fast approximate nearest neighbour dense vector search engine. /!/ Hi\n\nI've recently released V1 of the Vald, a Cloud-Native distributed fast approximate nearest neighbour dense vector search engine running on Kubernetes as an OSS project under Apache2.0 licence.\n\nIt is already running behind Yahoo Japan's -----> image !!!  search and recommendation engine and is also running behind the Japanese National Digital Library Digital Archive retrieval engine.\n\nBy using machine learning to convert unstructured data (audio, images, videos, user characteristics, etc.) into vectors and then using Vald to perform vector search on those vectors, it will be possible to operate as a faster and more complex search engine.\n\n&amp;#x200B;\n\nVald is still a very new project, but we are looking for a lot of feedback from many users.\n\nPlease come and visit our site!\n\n&amp;#x200B;\n\nWeb: [https://vald.vdaas.org](https://vald.vdaas.org)\n\nGitHub: [https://github.com/vdaas/vald](https://github.com/vdaas/vald)", "link": "https://www.reddit.com/r/MachineLearning/comments/mm9p3b/p_vald_a_highly_scalable_distributed_fast/"}, {"autor": "SQL_beginner", "date": "2021-04-07 16:08:05", "content": "[D] Feature Selection for Large Datasets /!/ To begin my question, I would like to quote a paper (by Ishawaran et al) on \"random forests for survival analysis data\", in which the authors (very concisely) outline the difficulties of feature selection (i.e. which variables to include in a statistical model) in classical regression models and how this problem is somewhat alleviated with more advanced models :\n\n\"Further, because these methods (i.e. classical regression models, e.g. cox ph regression - even though it's semi-parametric) are often parametric, nonlinear effects of variables must be modeled by transformations or expanding the design matrix to include specialized basis functions. Often ad hoc approaches, such as stepwise regression, are used to determine if nonlinear effects exist. Identifying interactions, especially those involving multiple variables, is also problematic. This must be done by brute force (examining all two-way and threeway interactions, e.g.), or must rely on subjective knowledge to narrow the search.\n\nIn contrast, these difficulties are handled automatically using forests. We illustrate the ease with which RSF can uncover complex data structures through an in-depth case study of the prognostic implications of being underweight, overweight, or obese and having severe, but stable coronary artery disease.\n\nInvestigators have noted complex patterns surrounding possible reverse causation in underweight individuals, interactions with smoking, and an unclear inflection point at which point increasing body mass confers increased risk Some have identified a possible obesity paradox among patients with established heart disease in which increased body mass predicts better survival. To clarify these issues, we analyzed a large cohort of patients with coronary artery disease undergoing isolated coronary artery bypass surgery. Using RSF, (random survival forest) we identified a complex relationship between long-term survival, body mass, renal (kidney) function, smoking, and number of internal coronary artery bypass grafts. We believe our novel findings help explain some of the apparent contradictions previously reported.\"\n\nSource: https://arxiv.org/pdf/0811.1645.pdf\n\nEssentially, the authors claim that traditional regression models struggle with feature selection and the newer models (e.g. bagging, random forest) are able to better deal with feature selection. I do remember from an intro stats class, the somewhat tedious process of determining which variables to include in a multiple linear regression model. As the authors described, I remember there was something called \"CP Mallow's Criteria\" in which potential variables were repeatedly included and excluded in the regression model and the value of CP Mallow's Criteria was monitored - a final selection of variables for the model was decided on the basis of this criteria. However, this selection process becomes inefficient for large datasets (if I understand correctly, this means you would have to refit the model for many different combinations of variables, resulting in a \"combinatorics explosion\" for a large number of variables). Like the authors mention, you can also \"manually hard code\" interaction terms in the model (e.g. log(var1), var1var2, var1/(var2var3), var1/(var2+var3), etc.) - and there an infinite such number of potential interactions. Improper feature selection can also result in unwanted effects such as multicollinearity. The last point I would like to bring up - although my knowledge of mathematics is not strong enough to fully substantiate it - is that classical regression models are said to have a tendency to overfit (I don't know why - I have seen visual demonstrations of this, but I don't know if there is a mathematical explanation behind this, or if it's just an empirical observation) and poorly generalize to new data (again, I don't know why); and that classical regression models are only able to \"recognize linearly separable patterns in the data\" (intuitively I can understand this, e.g. draw a circle of red points and a smaller circle of blue points that fits in the red circle, a single line can not separate the two colors - but I don't know if there is a mathematical explanation behind this).\n\nThis brings me to my question about feature selection for large datasets. With the advent of technology, data is becoming bigger and bigger everyday - convolution neural networks are the \"go to method\" for analyzing -----> picture !!! s (a standard black and white -----> picture !!!  is said to have 786 variables), whereas DNA is said to have even more. In such instances, it surely must be impossible to address feature selection as done in conventional statistical modelling. Please excuse my poor understanding of math - but my understanding is that newer statistical models have \"built in\" methods of handling the feature selection problem. For instance, random forest \"randomly\" chooses different combinations of variables and sees which combinations result in better model performance, the exact randomizing mechanism (uncorrelated trees) is said to also prevent against multicollinearity (I ahve heard that the creator of the random forest algorithm Leo Breiman claims through theoretical statistics that random forest by definition can not \"over fit\" and has some desirable error bounds and convergence properties - is this true?). Meanwhile, I have read on data science blogs (I'm not going to lie) that deep neural networks are able to \"automatically\" learn and consider \"useful\" combinations of features for approximating the target function (am I correct?).\n\nAll in all, what I want to ask here : for large datasets, where sometimes the features don't have any immediate meanings (e.g. a patient's blood pressure vs the information contained in the 231st pixel of a photograph) - is there any \"real\" way to handle feature selection? Or is this usually taken care of by the statistical model itself (e.g. random forest and neural networks)? I have seen examples online where people attempted to write a massive FOR LOOP in which they train the same model with thousands of variable combinations ... but I am not sure how feasible this is.\n\nCan someone please provide a comment on this?\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/mm5iad/d_feature_selection_for_large_datasets/"}, {"autor": "techsucker", "date": "2021-04-07 15:53:47", "content": "[N] Baidu Releases \u2018PaddlePaddle\u2019 2.0, Its Deep Learning Platform, With New Features Including Dynamic Graphs, Reorganized APIs (Documentation, Github link included) /!/ Baidu Brain, a Chinese core AI technology engine, announces the release of PaddlePaddle 2.0. PaddlePaddle (PArallel Distributed Deep LEarning)) is an open-sourced AI platform released by Baidu Brain in 2016 to apply deep learning(DL) to many products at Baidu, such as NLP (Natural Language Processing), translation, and -----> image !!!  processing.\n\nPaddlePaddle\u2019s latest version has features like dynamic (computational) graphs, a new API system, distributed training for trillion-parameter models, and better hardware support.\n\nSummary: [https://www.marktechpost.com/2021/04/07/baidu-releases-paddlepaddle-2-0-its-deep-learning-platform-with-new-features-including-dynamic-graphs-reorganized-apis/](https://www.marktechpost.com/2021/04/07/baidu-releases-paddlepaddle-2-0-its-deep-learning-platform-with-new-features-including-dynamic-graphs-reorganized-apis/) \n\nAPI Documentation: [https://www.paddlepaddle.org.cn/documentation/docs/en/api/index\\_en.html](https://www.paddlepaddle.org.cn/documentation/docs/en/api/index_en.html) \n\nGitHub: [https://github.com/PaddlePaddle](https://github.com/PaddlePaddle) \n\nGitee: [https://gitee.com/paddlepaddle](https://gitee.com/paddlepaddle)", "link": "https://www.reddit.com/r/MachineLearning/comments/mm56xc/n_baidu_releases_paddlepaddle_20_its_deep/"}, {"autor": "mLalush", "date": "2021-04-15 08:12:24", "content": "[D] Why have the standard data formats in object detection remained as COCO/PASCAL VOC/YOLO as opposed to switching over to a nested columnar format? /!/ In -----> image !!!  classification, early standards generally had users divide -----> image !!!  files into different training and validation folders, as well as (sometimes) requiring separate folders for each class label. While researchers seem to have held on to these standards, the APIs of deep learning libraries eventually evolved to allow more flexible usage which didn't necessarily require users to shuffle around image files in folders should they want to adjust their training and validation setups.\n\nKeeping training and validation files in different folders probably has certain benefits for the reproducibility of research, namely in making it *abundantly* clear how the train/val split was performed. However, this convention -- when enforced -- does introduce something of a \"barrier\" for beginners just looking to train a model on their existing labeled data with minimal data wrangling.\n\nThus, existing library APIs for image classification seem to have converged to loading references to filepaths, either as a list or in a tabular/columnar format, while encouraging users to perform their train/val splits on these references to the data. This allows for maximum flexibility, as users can have their data organized however they wish.\n\nMost approachable tutorials for adapting a model to custom datasets now follow the standard of having users define their own dataloaders. This new standard, however, does not seem to have proliferated to *user facing* object detection and semantic/instance segmentation libraries.\n\nHere, some of you might interject and point out that  \n\n\n1. The data is hierarchical in nature. Several bounding boxes, polygon coordinates or RLEs may exist for each image. COCO/PASCAL VOC/YOLO are the natural way to store such data.\n2. Storing hierarchical data in a (nested) columnar format will introduce memory overhead.\n3. Data formats such as .csv, .tsv don't work well with nested data.\n\nNone of these, in my opinion, are convincing arguments for why so many object detection libraries need to break with the conventions that have evolved for image classification. Hierarchical data can easily be stored as lists nested in cells of a dataframe. We don't necessarily have to repeat a row for however many objects exist in a given image.  The arrow ([https://arrow.apache.org/install/](https://arrow.apache.org/install/)) library exists for storing and loading nested columnar data. \n\nAm I insane in thinking that object detection library APIs will eventually -- but inevitably -- converge to the standard of image classification APIs? Why haven't they already? Torchvision already seems to be heading there: [https://pytorch.org/tutorials/intermediate/torchvision\\_tutorial.html](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html). I can't say the same for most other libraries, where the standard remains \"Please organize your data after these very specific instructions and execute this script with 20 poorly documented (optional) args\".", "link": "https://www.reddit.com/r/MachineLearning/comments/mraae5/d_why_have_the_standard_data_formats_in_object/"}, {"autor": "SPAMinaCanCan", "date": "2021-04-14 22:15:36", "content": "[D] What are you thoughts on how the amount of classes can affect model accuracy /!/ This hopefully is a very basic question\n\nI'm struggling to find good papers explaining how the amount of different classes affects different models \n\nFor example, say you were building a semantic segmentation classifier for JPG images containing soft drink cans.\n\nYour objective is to find all the cans contained within the -----> image !!! .\n\nIs it better to construct your training data using a generic soft drink can class?\n\nOR is it better to construct your training data using different colours of soft drink cans as seperate classes (i.e. red can, orange can, green can, etc...)?\n\nI am wondering how each of these class naming conventions will affect the overall accuracy classifying every can.\n\nWhat are your thoughts on this?\n\nAlso, if you want, comment on how it can extend to other things, such as cars, clothes, etc...", "link": "https://www.reddit.com/r/MachineLearning/comments/mr1edr/d_what_are_you_thoughts_on_how_the_amount_of/"}, {"autor": "ZenMachineResearch", "date": "2021-04-14 16:29:20", "content": "[Discussion] Alternatives to 80M Tiny Images Dataset for unsupervised visual learning + transfer. /!/ I am looking for a huge -----> image !!!  dataset for testing unsupervised learning and transfer. Something analogous to the massive text datasets that are used to train language models. \n\nI thought [Tiny Images](https://groups.csail.mit.edu/vision/TinyImages/) would be a good dataset but I noticed it was recently taken down. They withdrew the dataset because it was originally built upon WordNet and WordNet contained some really offensive terms (racial slurs for example, it's actually wild to see that). In the curators own words:\n\n&gt; Why it is important to withdraw the dataset: biases, offensive and prejudicial images, and derogatory terminology alienates an important part of our community -- precisely those that we are making efforts to include. It also contributes to harmful biases in AI systems trained on such data. Additionally, the presence of such prejudicial images hurts efforts to foster a culture of inclusivity in the computer vision community. This is extremely unfortunate and runs counter to the values that we strive to uphold.\n\nIs there any dataset that fills this niche? A massive unsupervised visual dataset that used some principled curation methodology, as opposed to \"in the wild.\" \n\nIs the default ImageNet? Is ImageNet large enough? Would appreciate any insight or feedback from people who have dabbled in this.", "link": "https://www.reddit.com/r/MachineLearning/comments/mqu8z3/discussion_alternatives_to_80m_tiny_images/"}, {"autor": "ancientmooner", "date": "2021-04-14 15:09:05", "content": "[P] Code and pretrained models for Swin Transformer are released (SOTA models on COCO and ADE20K) /!/ -----> Image !!!  classification and pretrained models: [https://github.com/microsoft/Swin-Transformer](https://github.com/microsoft/Swin-Transformer)\n\nObject detection on COCO: [https://github.com/SwinTransformer/Swin-Transformer-Object-Detection](https://github.com/SwinTransformer/Swin-Transformer-Object-Detection) \n\nSemantic segmentation on ADE20K: [https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation](https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation)", "link": "https://www.reddit.com/r/MachineLearning/comments/mqsjyv/p_code_and_pretrained_models_for_swin_transformer/"}, {"autor": "hash_t", "date": "2021-04-14 04:43:09", "content": "[P] EfficientNet - -----> image !!!  recognition and object detection model written in node and typescript", "link": "https://www.reddit.com/r/MachineLearning/comments/mqjd4g/p_efficientnet_image_recognition_and_object/"}, {"autor": "IvanLudvig", "date": "2021-04-09 20:47:34", "content": "Multi-label -----> image !!!  classification (tagging) using transfer learning with PyTorch and TorchVision", "link": "https://www.reddit.com/r/MachineLearning/comments/mnqbnb/multilabel_image_classification_tagging_using/"}, {"autor": "hubert0527", "date": "2021-04-09 19:20:04", "content": "[R] InfinityGAN: Towards Infinite-Resolution -----> Image !!!  Synthesis /!/ [\\\\\"Synthesizing infinite-resolution images from finite-resolution inputs.\\\\\" A 1024\u00d72048 image composed of 242 patches, independently synthesized by InfinityGAN with spatial fusion of two styles. The generator is trained on 101\u00d7101 patches \\(e.g., marked in top-left\\) sampled from 197\u00d7197 real images. Note that training and inference \\(of any resolution\\) are performed on a single GTX TITAN X GPU.](https://preview.redd.it/4drmg9ez47s61.png?width=1285&amp;format=png&amp;auto=webp&amp;s=5d5a1112aa090c1707e9e64a1e49c757480b2e3b)\n\n***TL;DR***  We propose InfinityGAN towards a new problem of synthesizing infinite-resolution images. The model is trained with images of limited resolution, and generalizes to arbitrary resolutions at testing. We further demonstrate several applications with a trained generator in spatial style fusion, image outpainting, and image inbetweening.\n\n**Project page**: [https://hubert0527.github.io/infinityGAN/](https://hubert0527.github.io/infinityGAN/)\n\n**Paper**: [https://arxiv.org/abs/2104.03963](https://arxiv.org/abs/2104.03963)\n\nWe will release our code soon here: [https://github.com/hubert0527/infinityGAN](https://github.com/hubert0527/infinityGAN)", "link": "https://www.reddit.com/r/MachineLearning/comments/mnoka6/r_infinitygan_towards_infiniteresolution_image/"}, {"autor": "mimeticaware", "date": "2021-05-23 16:58:24", "content": "[D] Should I use locality-sensitive hashing or MD5 to check if two datasets are the same? /!/ I need to compare if two datasets (of type audio, text, -----> image !!! , etc.) are the same or not. Should I use locality sensitive hashing or split the dataset into smaller chunks and hash using MD5/SHA-1.\n\nI'm leaning towards the second step. Locality sensitive hashing is more about finding similar items but I only need to check if two datasets are exactly the same or not. Both run on O(n) time complexity so I don't think speed will be a difference.", "link": "https://www.reddit.com/r/MachineLearning/comments/njbv6z/d_should_i_use_localitysensitive_hashing_or_md5/"}, {"autor": "mlconvergence", "date": "2021-05-23 15:54:46", "content": "[D] Are there parallels between patch-based -----> image !!!  models and biological vision systems? /!/ Recently, we have seen patch-based image models perform very well on image recognition tasks. First, we saw this using attention-based models like ViT, and more recently in work like MLP-Mixer.\n\nIn a way, this seems (very) loosely similar to how our own brains (and indeed the brains of many other species) process visual information. Namely, there is a high acuity structure in the eye (fovea centralis) which saccades from place to place. The brain then processes this stream of saccades (along with lower acuity peripheral vision) to construct a 3D model of the world. That is to say, the brain looks to be stitching together visual patches using some (presumably not fully understood) mechanism.\n\nPerhaps this comparison has been made in either the papers eluded to above or elsewhere in the literature. If so, I've sadly missed it.\n\nI would be interested to hear if anyone thinks there is something to this comparison, or not :).", "link": "https://www.reddit.com/r/MachineLearning/comments/njajvr/d_are_there_parallels_between_patchbased_image/"}, {"autor": "mlconvergence", "date": "2021-05-23 15:52:41", "content": "[D] Are the parallels between patch-based -----> image !!!  models and biological vision systems? /!/ Recently, we have seen patch-based image models perform very well on image recognition tasks. First, we saw this using attention-based models like ViT, and more recently in work like MLP-Mixer.\n\nIn a way, this seems (very) loosely similar to how our own brains (and indeed the brains of many other species) process visual information. Namely, there is a high acuity structure in the eye (fovea centralis) which saccades from place to place. The brain then processes this stream of saccades (along with lower acuity peripheral vision) to construct a 3D model of the world. That is to say, the brain looks to be stitching together visual patches using some (presumably not fully understood) mechanism. \n\nPerhaps this comparison has been made in either the papers eluded to above or elsewhere in the literature. If so, I've sadly missed it.\n\nI would be interested to hear if anyone thinks there is something to this comparison, or not :).", "link": "https://www.reddit.com/r/MachineLearning/comments/njaiar/d_are_the_parallels_between_patchbased_image/"}, {"autor": "ngxnam253", "date": "2021-05-23 09:38:43", "content": "[D]Exploratory Data Analysis for -----> Image !!!  dataset /!/ I know feature engineering is one of most important activities in a machine learning project. But I am stuck on EDA of image dataset. I don't know how to do EDA with image except visualizing some of them, checking image sizes are unique, or plotting distribution of category (in classification problems). \n\nI see some one use pre-trained neural network (such as resnet-50) to extract features, then do EDA with those features (such as using K-means to cluster and find out outlier images). I did the same, but the results were different. \n\nI did some search on Internet, but it has not many books or tutorials about this field. \n\nCan some one show me some sources to learn about this or give me some instructions on playing with image dataset?\n\nThanks you very much.", "link": "https://www.reddit.com/r/MachineLearning/comments/nj4d0q/dexploratory_data_analysis_for_image_dataset/"}, {"autor": "AdelSexy", "date": "2021-05-23 07:32:54", "content": "[D] ML/DL job hunting. Points if attention. /!/ Hey guys! This is my post about job hunting in ML/DL area. Original is [here](https://irregularadel.substack.com/p/mldl-job-hunting-points-if-attention).   \nML/DL positions are growing in numbers very fast, and most of the time it is hard to understand if the position you are applying to worth it and if you and the company are a good match. Just look at this search of machine learning or data scientist in Linkein - over 300k positions. \n\n[ AI jobs everywhere. I know some part of this 300k+ jobs are PMs and stuff, but it's still a lot. ](https://preview.redd.it/7j07bmhdnt071.png?width=1090&amp;format=png&amp;auto=webp&amp;s=87a4c47cf2ec74f5ad5f62ebb40300307cd94ba5)\n\nOf course, there are tasty positions in google, Deep mind, Amazon, Apple, Tesla, etc. But it is hard to get there, and, probably, even harder to work. Meanwhile, there a lot of other companies and positions, that can be quite a good option. The problem is - the field is so hyped, that some positions are crap and some are surprisingly good while it is hard to find out which is what.\n\nTo help in that complex choice, I designed these 6 points of attention while job hunting in the ML area. Using them, you can ease this hard and exhausting process and find a good matching position.\n\n# Step 0. Define what you truly want.\n\nAsk yourself where your passion lies. Is it fundamental research? Applied research? Engineering? What field is it: computer vision, natural language processing, time series analyses? Or maybe recommendation systems? From answers to that questions, you can define directions. You will get an idea, where you want to work: academia, corporation, start-up? Do you want to develop satelite -----> image !!!  analysis neural networks for a small growing start-up with a production-focused team? Or maybe you want to develop text/speech recognition models for internal use by the support department in a big retail corporation? What about in-depth research of generative models foundations?\n\nYou name it.\n\nThis step is essential - it will help you to get in peace with yourself, narrow down the search area, define criteria for a future position. To be honest, I think this is already 80% of success.\n\nAlso, make sure what companies understand under the name of the position you are interested in. You might be surprised how many different definitions are there for ML engineers, or, even worse, Data scientists. Try to get on the same page with the future employer. You don't want to program web applications, while you thought you will train neural networks. (I wish I were exaggerating, but this is a real example).\n\nNow, suppose you have all the answers. You applied to 20+ positions and started interviewing process on some of them.\n\n# Step 1. Ask about AI strategy.\n\nDoes the company has an AI roadmap? If yes, ask to elaborate on it. What are the goals for 5 years? How they are planning to get benefit from ML/DL. How essential is it for a company? What are the main development directions?\n\n[ Let me rephrase the classic ](https://preview.redd.it/lrx28vkmnt071.png?width=858&amp;format=png&amp;auto=webp&amp;s=d223895c84bf9f589bfda3bc45978e2f3a761a8c)\n\n If there is no AI strategy in the compay, it is not necessarily a bad thing. It might be, that company thinks about it and ready to work on a long-term vision. And maybe you will be one of the pioneers. Do you want it? Or you want to work mostly on content and not be involved in high-level roadmaps development? The choice is yours.\n\nBut in any way, please. make sure that there is no \"ML for ML\". There should be the business value that will bring need in ML.   \n\n# Step 2. Ask about data governance\n\nThat's super important since you can't work without good data governance. So it would be wise to know from the beginning what databases are there, where the data is coming from, how big is the data? Is there any annotation tool the company uses? If not, are they willing to pay attention to that (and that's the bridge to AI strategy)? How many training-ready datasets do they have? Do they have version control of the data?\n\n&amp;#x200B;\n\n[ Love this slide from one of lectures of Andrey Karpathy - Director of AI at Tesla ](https://preview.redd.it/pzvml5utnt071.png?width=1988&amp;format=png&amp;auto=webp&amp;s=b3d7bd1aaf4793bcf996e8e9acf32c2d6c07f717)\n\n Always remember garbage in -&gt; garbage out. Your comfort and effectiveness depend on that. \n\n# Step 3. Ask about the team: roles, plans for positions, working style.\n\nYou do want to know who your potential colleagues are. Are there any ML/DL experts? Newbies? Does the team have interns from time to time? Any data engineers? Full-stack developers? Is the team centralized or there are islands of expertise across the company? How many meetings do they have? What meeting? Growth plans? Do they visit conferences? Attend in them? What they wait for from you?\n\nYou have the right to understand your place in the team and the company before you go there to work. You should see your growth potential.\n\nFrom the answers, you can get an idea of how connected the team is as well as if you will fit in there.\n\n# Step 4. Any success stories?\n\nWhat has already been done in the company in the ML/DL field? Are there any working solutions? Use cases? Proves of concepts? That can give you an understanding of what to expect. You can easily get in the working rhythm if it already beats. Or you can try to create your own if there are conditions for it.\n\nSuccess stories can also indicate that the company knows the main customers/stakeholders, which is already a great achievement.\n\n# Step 5. Stack the company uses\n\nCheck the technology stack the team is using. Try to understand if they know, what they are doing. And if they don\u2019t, check if they realize that.\n\nMake sure the team has structured methods of research/production or at least is willing to have one.\n\n[ Just look at this giant landscape - credits goes to Full-Stack Deep learning course. ](https://preview.redd.it/662i8nkynt071.png?width=1952&amp;format=png&amp;auto=webp&amp;s=db9088dc32b227cfed688bd2ed2c0bf045a8a38f)\n\n How do they track their experiments? What frameworks/packages do they use? Is their code is just a pile of Jupiter notebooks? Do they work in their MLops? How they store and deploy the models? Who owns the models?  \nThis is the base for the successfull work, don\u2019t underestimate it\u2019s influence.\n\nAs a comclusion, I believe that there are ton\u2019s of beautiful machine learning positions out there. I know by my own skin, that job hunting can be exhasting and can suck all fun of your life. An I hope these few points of attention can help you ease the pain.\n\nGood luck!\n\nP.S. have a look at my blog [here.](https://irregularadel.substack.com/about)", "link": "https://www.reddit.com/r/MachineLearning/comments/nj2n0q/d_mldl_job_hunting_points_if_attention/"}, {"autor": "Chilaquil420", "date": "2021-05-22 16:49:32", "content": "[D] Binary classification images but with only ONE class, and the output would be YES or NO depending if a specific type of object (a taco) is in the pic. How? /!/ \nI saw a tutorial about bin classification with 2 classes and CNN. But I\u2019m trying to detect if an object is in the -----> image !!! , not really classify it. How would I proceed? I would have a dataset of pics of said object but how would I detect if it is not? Do I create a dataset of pics NOT containing it?", "link": "https://www.reddit.com/r/MachineLearning/comments/nimrix/d_binary_classification_images_but_with_only_one/"}, {"autor": "MLtinkerer", "date": "2021-08-29 02:02:59", "content": "[Project] State of the art in Background Removal on videos! Does not require capturing background -----> image !!!  or manual annotations.", "link": "https://www.reddit.com/r/MachineLearning/comments/pdmnu1/project_state_of_the_art_in_background_removal_on/"}, {"autor": "cloud_weather", "date": "2021-08-28 16:03:58", "content": "[D] Very Impressive -----> Image !!!  Upscaling Research from Real-ESRGAN", "link": "https://www.reddit.com/r/MachineLearning/comments/pdci2h/d_very_impressive_image_upscaling_research_from/"}, {"autor": "Spot-Inside", "date": "2021-08-27 16:17:32", "content": "[Discussion] state-of-the-art text to -----> image !!!  generation methods /!/ It isn't very clear to me which of these text to image generation methods is the state-of-the-art currently:\n\n\\- conditional diffusion models\n\n\\- GAN e.g. conditional BigGAN\n\n\\- transformers e.g. DALL-E\n\n&amp;#x200B;\n\nNone of the text to image generation papers using these approaches compare results on the same dataset in a fair manner. DALL-E does artificial blurring on MS-COCO because its VQVAE is not great. Metrics for the same task are also different across these papers. All of these papers present their best results on a large dataset that isn't comparable.\n\n&amp;#x200B;\n\nIf I had to do text to image generation on a large dataset, what would be the most promising approach currently?", "link": "https://www.reddit.com/r/MachineLearning/comments/pcqjia/discussion_stateoftheart_text_to_image_generation/"}, {"autor": "Yuqing7", "date": "2021-08-27 15:07:28", "content": "[R] Google Brain Uncovers Representation Structure Differences Between CNNs and Vision Transformers /!/ A Google Brain research team explores the internal representation structures of ViTs and CNNs on -----> image !!!  classification tasks, providing insights on key differences between the two approaches. \n\nHere is a quick read:[Google Brain Uncovers Representation Structure Differences Between CNNs and Vision Transformers.](https://syncedreview.com/2021/08/27/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-92/)\n\nThe paper *Do Vision Transformers See Like Convolutional Neural Networks?* is on [arXiv](https://arxiv.org/abs/2108.08810).", "link": "https://www.reddit.com/r/MachineLearning/comments/pcp6oz/r_google_brain_uncovers_representation_structure/"}, {"autor": "THE_REAL_ODB", "date": "2021-01-28 13:58:17", "content": "[D](Active Learning)Confused about retrieving uncertainty scores from an object detection model /!/ Hello, I have been trying to figure out how to retrieve informativeness or uncertainty scores from images in an object detection model.\n\nGetting a uncertainty score from an -----> image !!!  in classification model is relatively straight forward, but i'm confused about getting the score from an object detection model.\n\nThe most straightforward way seems to be training a separate classification model on cropped images of objects from a object detection model and average the uncertainty scores in an image depending on the number of object in the picture.\n\nBut this seems like a lot of work and I was wondering if I could somehow derive these scores within the object detection model without training a separate classification model.\n\nthanks in advance.", "link": "https://www.reddit.com/r/MachineLearning/comments/l6x8jq/dactive_learningconfused_about_retrieving/"}, {"autor": "techsucker", "date": "2021-01-28 07:27:38", "content": "[R] Google Researchers Introduce a New Framework (TReCS) For Text-to------> Image !!!  Generation /!/ A team of Google researchers has developed a new framework, **the Tag-Retrieve-Compose Synthesize system (TReCS)**. The proposed method significantly enhances the image generation process by improving how the language evokes image elements and how traces inform their placement. The system is trained on over 25 billion examples and has the potential to handle 103 languages.\n\nFull Summary: [https://www.marktechpost.com/2021/01/27/google-researchers-introduce-a-new-framework-trecs-for-text-to-image-generation](https://www.marktechpost.com/2021/01/27/google-researchers-introduce-a-new-framework-trecs-for-text-to-image-generation)\n\nPaper: [https://arxiv.org/pdf/2011.03775.pdf](https://arxiv.org/pdf/2011.03775.pdf)", "link": "https://www.reddit.com/r/MachineLearning/comments/l6qvzu/r_google_researchers_introduce_a_new_framework/"}, {"autor": "AdSpiritual7858", "date": "2021-01-28 04:39:39", "content": "[D] New-ish to ML.. using a pretrained model possible here? /!/ Hello.\n\nWell.. in short, what i'm attempting to do is replace the BIGGAN used in this text to -----> image !!!  generation project (ranked with OpenAI's CLIP -&gt; [https://colab.research.google.com/drive/1MEWKbm-driRNF8PrU7ogS5o3se-ePyPb?usp=sharing](https://colab.research.google.com/drive/1MEWKbm-driRNF8PrU7ogS5o3se-ePyPb?usp=sharing)) with a more efficient available model trained on imagenet or something similar.. (many many options) I don't know where to start. I understand Python and the very basics of ML, but I think i'm underestimating how difficult it is to implement a new model into something like this. maybe someone with a bit more knowledge on this subject would be able to help.\n\nDoesn't torchvision include a large variety of pretrained models?", "link": "https://www.reddit.com/r/MachineLearning/comments/l6o5b7/d_newish_to_ml_using_a_pretrained_model_possible/"}, {"autor": "spongeflo", "date": "2021-01-27 19:11:34", "content": "-----> Image !!!  Classification vs. Object Detection with static camera footage /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/l6c16o/image_classification_vs_object_detection_with/"}, {"autor": "666GZL", "date": "2021-01-27 15:23:47", "content": "StyleGan2 -----> image !!!  generation /!/ Hey,  \n\n\nim working on some image generation with stylegan2 atm and i wonder if there is a certain technique to get the pictures i want.\n\nAt the moment I use a \"random\" seed to generate images, but i want a certain type of image that I have already seen in a latent walk. How do I know which \"types\" of images are in which area?", "link": "https://www.reddit.com/r/MachineLearning/comments/l66ptw/stylegan2_image_generation/"}, {"autor": "sarmientoj24", "date": "2021-07-08 07:20:41", "content": "[D] Questions on Semi-Supervised Learning on Object Detection using Video Footages /!/ While there had been advances on the Semi-Supervised Learning area, there are just few in SSL on Object Detection. There is one on Unbiased Teacher for SSL.\n\nI would like to ask for some perspectives on SSL on Object Detection.\n\n**Semi-Supervised Learning in a nutshell**\n\nFrom what I know, SSL utilizes labeled and unlabeled datasets to come up with better results than, just having the labeled datasets. Of course that is an oversimplification of things. But usually, the number of unlabeled datasets outnumber the number of labeled datasets. Most of the time, there are plentiful of unlabeled datasets around as well.\n\n**SSL using Video Footages**\n\nOne of the most tedious process in acquiring datasets for Object Detection (OD) is labelling datasets. But if I have a video footage that runs, say 30FPS, I could just ***label a few, say a couple,*** on that short burst of 1second clip which has about 30 frames of images containing the object on slight variations. \n\nTheoretically, for a 1 second clip, I can annotate ***n*** data and then use ***30 - n***  images as unlabelled dataset, right? Could I? And should I?\n\n**SSL on Variations of Objects in the Video/Image**\n\nOur dataset will be collected using a ***moving video -----> camera !!! .*** It is a video footage of road survey. Hence, frame by frame comparison of images could have slight to moderate difference from occlusions to motion blur, to shadows, perspectives, etc. \n\n**Questions:**\n\n1. Has there been papers dealing with similar problem? I know that SSL just assumes unlabeled datasets are present. But for these ones, datasets would be much closer to their annotated counterpart on one point in the video.\n2. Any advice or perspectives on approaching the problem as well? We could manually find frames in the video then get the frames from time **t,** with **t-n and t+n** to get the unlabelled dataset and one to two annotations at that time **t.** \n3. What approaches in SSL in terms of algorithm would be preferable? Consistency Regularizations, Proxy Methods, etc?", "link": "https://www.reddit.com/r/MachineLearning/comments/og293s/d_questions_on_semisupervised_learning_on_object/"}, {"autor": "crwcomposer", "date": "2021-07-07 14:45:39", "content": "[D] Regarding the training sets and output of GANs /!/ If you have a training set that includes both dogs and cats, and you want it to generate an -----> image !!!  based on the training set, will it generate discrete -----> image !!! s of dogs OR cats, or will it generate -----> image !!! s of a creature somewhere between a dog and cat?\n\nIt seems to me that the network that performs the discrimination would be able to learn the difference between dogs and cats, and therefore the output of the network that generates images would be either a dog or a cat, and not a mix.\n\n1. Is that correct?\n2. If it is, is there a way to adjust the network to adjust the mixture?\n3. Is there a better non-GAN configuration of networks for that purpose?\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/ofkbh9/d_regarding_the_training_sets_and_output_of_gans/"}, {"autor": "KirillTheMunchKing", "date": "2021-07-07 13:44:30", "content": "[D] NeRF GAN paper explained in 5 minutes - GRAF: Generative Radiance Fields for 3D-Aware -----> Image !!!  Synthesis by Katja Schwarz et al. /!/ [ 3D GRAF samples learned from 2D data](https://i.redd.it/hhupbq9zns971.gif)\n\nNeRF models blew up last year spawning an endless stream of variations and modifications addressing important issues with the original design. One of the more unique ideas that came from this NeRF Explosion (coined by Frank Dellaert) is this paper by researchers from the Max Planck Institute for Intelligent Systems. The authors of GRAF combined NeRFs and GANs to design a pipeline for generating conditional Neural Radiance Fields that can generate consistent 3d models with various shapes and appearances despite only being trained on 2d unposed images.\n\nRead the [full paper digest](https://t.me/casual_gan/61) (reading time \\~5 minutes) to learn about NeRF models, the motivation for combining NeRF models with the GAN framework, and all of the tricks used in the radiance field generator to synthesize 3d aware images from a set of unposed 2d images.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://t.me/casual_gan)!\n\n[GRAF explained](https://preview.redd.it/pzm62j29os971.png?width=615&amp;format=png&amp;auto=webp&amp;s=65d0848d71630c71de0e02ff05b6a6f45bb85f4f)\n\n\\[[Full Explanation Post](https://t.me/casual_gan/61)\\] \\[[Arxiv](http://www.cvlibs.net/publications/Schwarz2020NEURIPS.pdf)\\] \\[[Code](https://github.com/autonomousvision/graf)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n&gt;\\[[Alias-free GAN](https://t.me/casual_gan/58)\\]  \n&gt;  \n&gt;\\[[GFPGAN](https://t.me/casual_gan/54)\\]  \n&gt;  \n&gt;\\[[PTI](https://t.me/casual_gan/60)\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/ofj677/d_nerf_gan_paper_explained_in_5_minutes_graf/"}, {"autor": "mortadelass", "date": "2021-07-07 13:28:01", "content": "[D] Difference between representation vs. latent vs. embedding space /!/ I am writing a paper and a reviewer pointed out some problems in my wording regarding the differentiation between representation vs. latent vs. embedding space. The more I read, the more I get confused, since I have the overall feeling that very often these words are used as synonyms and I am currently highly unsure on when / how to use each.\n\nMy understanding: let's say we train an encoder (using for example self supervised contrastive learning like SimCLR). If I input a -----> image !!!  to this encoder I will get a lower dimensional representation of that -----> image !!! , i.e., its EMBEDDINGS or its LATENT REPRESENTATION in that particular encoder. The space created by all the possible images that the encoder can represent is more generically called the REPRESENTATION SPACE. I can similarly call this the EMBEDDING SPACE to refer to all possible embedded images (i.e. a synonym).\n\nIs something wrong with the above illustration or am I mixing heterogeneous concepts?", "link": "https://www.reddit.com/r/MachineLearning/comments/ofivs2/d_difference_between_representation_vs_latent_vs/"}, {"autor": "visionkhawar512", "date": "2021-07-07 01:53:38", "content": "IEEE CVPR 2021 Workshop /!/  \n\nLanguage for 3D Scenes: IEEE CVPR 2021 Workshop\n\n[https://youtube.com/playlist?list=PLEIhq70yQhTACmNbXjFkA\\_yYTtR6LE8BR](https://youtube.com/playlist?list=PLEIhq70yQhTACmNbXjFkA_yYTtR6LE8BR)\n\nFrontiers of Monocular 3D Perception: IEEE CVPR 2021 Workshop\n\n[https://youtube.com/playlist?list=PLEIhq70yQhTDNvBPbec-Z1BgF5S22I3p6](https://youtube.com/playlist?list=PLEIhq70yQhTDNvBPbec-Z1BgF5S22I3p6)\n\nBeyond Fairness: Towards a Just, Equitable, and Accountable Computer Vision: IEEE CVPR 2021 Workshop\n\n[https://youtube.com/playlist?list=PLEIhq70yQhTCNF9oGxL1Xx3WQn8ddlP1m](https://youtube.com/playlist?list=PLEIhq70yQhTCNF9oGxL1Xx3WQn8ddlP1m)\n\nResponsible Computer Vision : IEEE CVPR 2021 Workshop\n\n[https://youtube.com/playlist?list=PLUgbVHjDharh-Vqk1K69uk7AUFKt9JTIl](https://youtube.com/playlist?list=PLUgbVHjDharh-Vqk1K69uk7AUFKt9JTIl)\n\nAffective Understanding in Video: IEEE CVPR 2021 Workshop\n\n[https://youtube.com/playlist?list=PLUgbVHjDharheJU376Upvk2DoBrmLYHdk](https://youtube.com/playlist?list=PLUgbVHjDharheJU376Upvk2DoBrmLYHdk)\n\nWhen -----> Image !!!  Analysis Meets Natural Language Processing: A Case Study in Radiology: IEEE CVPR 2021 Tutorial\n\n[https://youtube.com/playlist?list=PLUgbVHjDhargV\\_dgD4TsNObumDvBcpAbm](https://youtube.com/playlist?list=PLUgbVHjDhargV_dgD4TsNObumDvBcpAbm)\n\nNew Frontiers in Data-Driven Autonomous Driving: IEEE CVPR 2021 Tutorial\n\n[https://youtube.com/playlist?list=PLUgbVHjDharjwxJUlbRObllPj-HRtTLTl](https://youtube.com/playlist?list=PLUgbVHjDharjwxJUlbRObllPj-HRtTLTl)\n\nInternational Challenge on Activity Recognition (ActivityNet): IEEE CVPR 2021 Workshop\n\n[https://youtube.com/playlist?list=PLUgbVHjDharhwg2uM\\_hj5kiVnSpMZYDIW](https://youtube.com/playlist?list=PLUgbVHjDharhwg2uM_hj5kiVnSpMZYDIW)\\\\\n\nMedical Computer Vision: IEEE CVPR 2021 Workshop\n\n[https://youtube.com/playlist?list=PLUgbVHjDharj9ttEepu-Vv3610-PCK9dK](https://youtube.com/playlist?list=PLUgbVHjDharj9ttEepu-Vv3610-PCK9dK)\n\nInternational Workshop on Dynamic Scene Reconstruction: IEEE CVPR 2021 Workshop\n\n[https://youtube.com/playlist?list=PLUgbVHjDharhGNLVoq2QkCalwTxIWkjWR](https://youtube.com/playlist?list=PLUgbVHjDharhGNLVoq2QkCalwTxIWkjWR)\n\nWorkshop on Event-based Vision : IEEE CVPR 2021 Workshop\n\n[https://youtube.com/playlist?list=PLUgbVHjDharj-uqu91nV\\_anULCfpQoi5b](https://youtube.com/playlist?list=PLUgbVHjDharj-uqu91nV_anULCfpQoi5b)\n\nNew Frontiers in Data-Driven Autonomous Driving: IEEE CVPR 2021 Tutorial\n\n[https://youtube.com/playlist?list=PLUgbVHjDharjwxJUlbRObllPj-HRtTLTl](https://youtube.com/playlist?list=PLUgbVHjDharjwxJUlbRObllPj-HRtTLTl)\n\nCross-View and Cross-Modal Visual Geo-Localization: IEEE CVPR 2021 Tutorial\n\n[https://youtube.com/playlist?list=PLUgbVHjDharjTo9tk3xcPJHEkmi33ap-u](https://youtube.com/playlist?list=PLUgbVHjDharjTo9tk3xcPJHEkmi33ap-u)\n\nData- and Label-Efficient Learning in An Imperfect World: IEEE CVPR 2021 Workshop\n\n[https://youtube.com/playlist?list=PLUgbVHjDhari3K24ROdYth2OS4Y4hH028](https://youtube.com/playlist?list=PLUgbVHjDhari3K24ROdYth2OS4Y4hH028)\n\nFrom VQA to VLN: Recent Advances in Vision-and-Language Research: IEEE CVPR 2021 Tutorial\n\n[https://youtube.com/playlist?list=PLUgbVHjDhari645g1zmpo-MtOVap1FKxh](https://youtube.com/playlist?list=PLUgbVHjDhari645g1zmpo-MtOVap1FKxh)\n\nAutonomous Driving: Perception, Prediction and Planning: IEEE CVPR 2021 Tutorial\n\n[https://youtube.com/playlist?list=PLUgbVHjDhargdoZ9hLlKS6lJIYOqcYoND](https://youtube.com/playlist?list=PLUgbVHjDhargdoZ9hLlKS6lJIYOqcYoND)\n\nTheory and Application of Energy-Based Generative Models: IEEE CVPR 2021 Tutorial\n\n[https://youtube.com/playlist?list=PLUgbVHjDharg7zIB00tCgk8dcrWLnWR0a](https://youtube.com/playlist?list=PLUgbVHjDharg7zIB00tCgk8dcrWLnWR0a)\n\nAdversarial Machine Learning in Computer Vision: IEEE CVPR 2021 Tutorial\n\n[https://youtube.com/playlist?list=PLUgbVHjDhargI044nRdTVQvc2p9-yORKL](https://youtube.com/playlist?list=PLUgbVHjDhargI044nRdTVQvc2p9-yORKL)\n\nLeave Those Nets Alone: Advances in Self-Supervised Learning: IEEE CVPR 2021 Tutorial\n\n[https://youtube.com/playlist?list=PLUgbVHjDharhcjvDURmtAeLSU84Vr2V-Q](https://youtube.com/playlist?list=PLUgbVHjDharhcjvDURmtAeLSU84Vr2V-Q)\n\nAutonomous Driving: IEEE CVPR 2021 Tutorial\n\n[https://youtube.com/playlist?list=PLUgbVHjDharhCnUzWQryw2rZUe7K\\_qWdc](https://youtube.com/playlist?list=PLUgbVHjDharhCnUzWQryw2rZUe7K_qWdc)\n\nNormalization Techniques in Deep Learning: Methods, Analyses, and Applications: IEEE CVPR 2021 Tutorial\n\n[https://youtube.com/playlist?list=PLUgbVHjDhargycWpy2V1jdKto19YhTaTt](https://youtube.com/playlist?list=PLUgbVHjDhargycWpy2V1jdKto19YhTaTt)\n\nFine-Grained Visual Categorization: IEEE CVPR 2021 Tutorial\n\n[https://youtube.com/playlist?list=PLUgbVHjDhargrqfiqGf8LlN4QeRnds-bP](https://youtube.com/playlist?list=PLUgbVHjDhargrqfiqGf8LlN4QeRnds-bP)\n\nBinary Networks for Computer Vision: IEEE CVPR 2021 Workshop\n\n[https://youtube.com/playlist?list=PLEIhq70yQhTAdoVAuHUdSR-xQHXzATgYo](https://youtube.com/playlist?list=PLEIhq70yQhTAdoVAuHUdSR-xQHXzATgYo)", "link": "https://www.reddit.com/r/MachineLearning/comments/of98i6/ieee_cvpr_2021_workshop/"}, {"autor": "metaphysical_fries", "date": "2021-07-06 06:54:30", "content": "[D] How do I start with a ML project after doing the Andrew Ng course? /!/ Hi there! Sorry if this has been posted before, any kind of advice would be appreciated. \n\nI'm a second year CS student and I have two goals in mind- \n\n1. Being able to transliterate -----> image !!!  to text (for a foreign language) \n2. Translating the text to English \n\nUsing deep learning.\nI've done Andrew Ng's course and feel like I have a good foundation in the theory, but having done the programming assignments in Octave, I feel like I don't know how to do those problems in python, or how to work on any real world ML problems using python.   \n\n\nIs there any good resource I can use to build my ML skills in python (after having done Andrew Ng's course) and then moving on to the deep learning specialization so I can complete my goals?", "link": "https://www.reddit.com/r/MachineLearning/comments/oepo1t/d_how_do_i_start_with_a_ml_project_after_doing/"}, {"autor": "charlesjcn", "date": "2021-07-14 14:23:56", "content": "\"[D]\" Looking for best approaches/methods/papers/suggestions for anomaly detection in videos? /!/ Context: surveillance videos that has multiple -----> camera !!!  clips in a single one, of a site looking for some anomalies like some activity that's happening and trying to say if that's anomaly or not. Thanks for helping in anticipation!", "link": "https://www.reddit.com/r/MachineLearning/comments/ok5ej0/d_looking_for_best/"}, {"autor": "KingsmanVince", "date": "2021-07-14 08:54:25", "content": "[R] A Large-Scale Benchmark for Food Image Segmentation /!/ [Paper link](https://arxiv.org/abs/2105.05409) \n\n[Website link](https://xiongweiwu.github.io/foodseg103.html)\n\n**Abstract**\n\nFood -----> image !!!  segmentation is a critical and indispensable task for developing health-related applications such as estimating food calories and nutrients. Existing food image segmentation models are underperforming due to two reasons: (1) there is a lack of high quality food image datasets with fine-grained ingredient labels and pixel-wise location masks---the existing datasets either carry coarse ingredient labels or are small in size; and (2) the complex appearance of food makes it difficult to localize and recognize ingredients in food images, e.g., the ingredients may overlap one another in the same image, and the identical ingredient may appear distinctly in different food images.  \nIn this work, we build a new food image dataset FoodSeg103 (and its extension FoodSeg154) containing 9,490 images. We annotate these images with 154 ingredient classes and result in an average of 6 ingredient labels and pixel-wise masks per image. In addition, we propose a multi-modality pre-training approach called ReLeM that explicitly equips the model with rich and semantic food knowledge. In experiments, we use three popular semantic segmentation methods (i.e., Dilated Convolution based, Feature Pyramid based, and Vision Transformer based) as baselines, and evaluate them as well as ReLeM on our new datasets. We believe that the FoodSeg103 (and its extension FoodSeg154) and the pre-trained models using ReLeM can serve as a benchmark to facilitate future works in fine-grained food image understanding.", "link": "https://www.reddit.com/r/MachineLearning/comments/ok0cid/r_a_largescale_benchmark_for_food_image/"}, {"autor": "MLingMLer", "date": "2021-07-14 08:07:33", "content": "[D] Face GAN from reference -----> image !!!  /!/ Face GAN from reference image\n\nI'm looking for any GAN work that can modify facial features given a reference image.\n\nSo for example, by providing a celebs photo as input, the network allows to modify facial features, such as hair color etc. What matters is that it doesn't generate completely random images in the beginning.\n\nI'm sure there are many works in this area, but I haven't been following GANs, so if anyone knows please point me to the paper/ code.\n\nThanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/ojzrq4/d_face_gan_from_reference_image/"}, {"autor": "MLingMLer", "date": "2021-07-14 08:05:35", "content": "Face GAN from reference -----> image !!!  /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ojzqt4/face_gan_from_reference_image/"}, {"autor": "mrtac96", "date": "2021-07-14 06:14:15", "content": "[D] sigmoid vs softmax in 3D medical -----> image !!!  segmentation /!/ Greetings. I  am doing 3D medical image segmentation, but I am confused should I used sigmoid or softmax at last layer activation. The images are 3D CT scan images for different organs segmentation (5 classes/labels). \nI clearly know the concept of multi-class and multilabel classification, but I am confused in segmentation task", "link": "https://www.reddit.com/r/MachineLearning/comments/ojyazb/d_sigmoid_vs_softmax_in_3d_medical_image/"}, {"autor": "gecko39", "date": "2021-07-13 22:15:53", "content": "[D] Best no code SaaS / website for -----> image !!!  annotation, training, and model creation? /!/ I'm doing a small weekend project where I want to train a semantic segmentation network based on a pretrained imagenet model. I was curious to see if I could accomplish this via one of the many services I feel like exist out there. \nI checked roboflow, but they do not appear to support semantic segmentation. \n\nCriteria:\n- Annotate pixel labels for 100 or so images for 2-3 classes ( ideally with smart annotation / one click ) \n- Train model ( hopefully with some automatic image augmentation or domain generalization) \n- Download / Export model ( as tensorflow/pytorch/onnx )  along with boilerplate code to run inference on image \n- Free or cheap enough to try it once for this project \n\nI can do this myself by coding it, but figured i'd test something new.\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/ojqdjl/d_best_no_code_saas_website_for_image_annotation/"}, {"autor": "p00pl00ps", "date": "2021-07-13 15:24:34", "content": "[P] StyleGAN2 for character portraits and style transfer /!/ Hi all!\n\nI've been working on a fun little projects for a while. It's a simple webapp which serves a couple of styleGAN2 models I  trained on a dataset of hand-drawn portraits in the style of classic RPGs.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/96egpvrvyza71.png?width=256&amp;format=png&amp;auto=webp&amp;s=4e52e42cb3710ddaeebd59cd9da735341a9d5a09\n\n&amp;#x200B;\n\n* The first model simply generates a character portrait, and does so using conditional labels allowing you to pick gender and traditional fantasy races.\n* The second model takes real faces, aligns them and [pixel2style2pixel](https://github.com/eladrich/pixel2style2pixel/tree/master/models) invert to the stylegan latent space and thus perform style transfer. The -----> image !!!  above is the output of putting in a picture of a certain famous ML figure...see if you can recognise them!\n\nBoth models have their fair share of issues and need more work, but I thought the results were cool enough to share. You can play with th API  [here](https://rp-gen.com) if you're interested (excuse bugs / breakages due to server overloads...)\n\n&amp;#x200B;\n\nEnjoy and le me know what you think!", "link": "https://www.reddit.com/r/MachineLearning/comments/ojhvkl/p_stylegan2_for_character_portraits_and_style/"}, {"autor": "Megixist", "date": "2021-08-22 16:32:23", "content": "[D] Is there any research that compares Regression Losses with Binary Crossentropy for pixel loss in generative models? /!/ I have seen two configurations for -----> image !!!  based generative models, mainly tanh or sigmoid with binary crossentropy and linear with MSE or MAE. Does anyone know why researchers prefer one over the other? Are there any papers that go a bit more in depth on this topic?\n\nThanks in advance!", "link": "https://www.reddit.com/r/MachineLearning/comments/p9fo0h/d_is_there_any_research_that_compares_regression/"}, {"autor": "DisWastingMyTime", "date": "2021-08-22 14:09:49", "content": "[D] MaxPool Layers in Regression Networks, Why Do They Work? /!/ So for networks which are classifier, the maxpool's addition is trivial, aside from the sub sampling and adding nonlinearity, it adds small invariance to translation, and a concatination of those increases the invariance, as such, a dog, no matter where in the -----> image !!! , will trigger our network\n\nBut in a regression network for getting landmarks for example? wouldn't these maxpool layers ruin our accuracy instead, especially in images which are of low resolution in the first place?\n\nAnd yet we do see that the max pool layer is common in architectures for regression, why is that, am I missing some cruicial point, or is it that that what maxpool adds is good enough to ignore the small (?) inaccuracy that it produces.\n\nI'm looking for either inuitive or formal explanation.", "link": "https://www.reddit.com/r/MachineLearning/comments/p9d3v0/d_maxpool_layers_in_regression_networks_why_do/"}, {"autor": "SuitableConstant4494", "date": "2021-08-22 11:14:29", "content": "[P][D] A Tiny Dashboard to Visualise and Compare the Differences Between Pre-trained Vision Models on Inference Time, Input/Output Feature Vector Size, Error on ImageNet etc... /!/ Hi,  \n\n\nThe PyTorch Image Models library ([timm](https://github.com/rwightman/pytorch------> image !!! -models)) has more than 500 pre-trained models currently, varying widely on various factors. With this very large number of pre-trained models, it is sometimes difficult to choose the right one to fine-tune or for transfer learning. \n\nFor these reasons, and initially for personal use, I developed an interactive bubble chart in streamlit bubble chart to more easily visualise and compare the difference between various pre-trained models available in the PyTorch Image Models library (currently version 0.4.13). One can choose various dimensions for the horizontal and vertical axes as well as the colour, size, and shape of the bubbles. It is also possible to filter out certain architectures. It is written using streamlit and available at [https://share.streamlit.io/aghasemi/vptm/app.py](https://share.streamlit.io/aghasemi/vptm/app.py). Feedbacks are more than welcome.", "link": "https://www.reddit.com/r/MachineLearning/comments/p9aoeh/pd_a_tiny_dashboard_to_visualise_and_compare_the/"}, {"autor": "AIforimaging", "date": "2021-08-22 07:13:49", "content": "[D] Smartphone for Snapdragon Insiders -----> camera !!!  review", "link": "https://www.reddit.com/r/MachineLearning/comments/p9820i/d_smartphone_for_snapdragon_insiders_camera_review/"}, {"autor": "MLtinkerer", "date": "2021-08-22 02:52:52", "content": "[Project] Future of Lego! Turn any 2D -----> image !!!  you have into Lego (3D model)!", "link": "https://www.reddit.com/r/MachineLearning/comments/p94pf0/project_future_of_lego_turn_any_2d_image_you_have/"}, {"autor": "eccLykta", "date": "2021-08-21 20:37:42", "content": "[P] Classifying atronach -----> image !!! s in Skyrim: 1,600 -----> image !!!  dataset - 95% accuracy in ~9 hours training + tuning on a CPU.", "link": "https://www.reddit.com/r/MachineLearning/comments/p8ytvj/p_classifying_atronach_images_in_skyrim_1600/"}, {"autor": "Darth_0", "date": "2021-08-21 14:53:47", "content": "[Project] SolSudo (Sudoku Solver) /!/ Abstract: SolSudo is a sudoku solver that works with deep learning. SolSudo can solves sudokus by using images and it has an intelligence method. According to this method, the model predicts the blank digits, and when each level is completed, blanks are filled one after another. Each time a blank is filled, new sudoku will be fed to the solver to detect the next digit. Again and again, until there isn't a blank left. One of the features of this project is detecting sudoku from an -----> image !!!  and filling in the blanks that require tesseract-ocr, however, which may cause problems. Therefore, I devised a method as in sudoku numbers are entered one by one, and 0 is used for blanks.\n\n[Github](https://github.com/AryaKoureshi/SolSudo)\n\n[Linkedin](https://www.linkedin.com/posts/arya-koureshi_deeplearning-python-tensorflow-activity-6711641409658716160-kdSD)", "link": "https://www.reddit.com/r/MachineLearning/comments/p8styb/project_solsudo_sudoku_solver/"}, {"autor": "ravikaiiit", "date": "2021-08-21 14:47:49", "content": "[R] The paper deep crowd counting research deserves, but not the one it (seems to) want right now. /!/ &amp;#x200B;\n\nhttps://preview.redd.it/5c9wiv0p4qi71.png?width=499&amp;format=png&amp;auto=webp&amp;s=4165443f266afa257535258190f18212d8711407\n\nFor far too long, there have been systematic issues with the processing pipeline for -----> image !!! -based crowd counting. A top performing deep network model claims a mean counting error of\u00a071.7 but omits to mention that the standard deviation is a whopping 376.4. This makes the mean score totally unreliable as a performance measure. NONE of the papers published at top-tier venues (CVPR, ICCV, NeurIPS, AAAI) report standard deviation. It\u2019s time to confront and address this issue as a community. We take the first step via our work, to be presented at ACM Multimedia 2021. Check out our paper [https://arxiv.org/pdf/2108.08784](https://arxiv.org/pdf/2108.08784) which revamps the entire pipeline and our interactive website [https://deepcount.iiit.ac.in](https://deepcount.iiit.ac.in/) . A **TL;DR** tweet summary can be found here [https://threadreaderapp.com/thread/1428766431458463745.html](https://threadreaderapp.com/thread/1428766431458463745.html)", "link": "https://www.reddit.com/r/MachineLearning/comments/p8sqei/r_the_paper_deep_crowd_counting_research_deserves/"}, {"autor": "DebbyRubottom53", "date": "2021-08-21 12:04:03", "content": "[D] Creating a Detector from a Classifier in Pytorch /!/  Hi guys,\n\nI'm trying to create an object detector detector in Pytorch from the ground up, the primary purpose for which is to detect circular objects on a dark background. My plan was to start with a working classifier, that I have already created and verified that it works. I used this tutorial as a guide: [https://pytorch.org/tutorials/beginner/blitz/cifar10\\_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n\nWhat I want to do next is implement a \"sliding-window\" like network that takes in an -----> image !!!  tensor \\[3, Height, Width\\] where 3 is the rgb channels and outputs a tensor of sliced -----> image !!! s \\[n, 3, h, w\\]. My classifier is designed to run on 32x32 images and I just resize the last 2 dimensions before running an image through it. Does anyone have any suggestions or references on how I could implement this sliding window system with pytorch?\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/p8q9g6/d_creating_a_detector_from_a_classifier_in_pytorch/"}, {"autor": "olegranmo", "date": "2021-08-21 09:06:00", "content": "[Research] Coalesced Tsetlin Machine -- -----> image !!!  classification with 50 logical rules per class using new multi-output architecture that supports rule sharing. /!/ &amp;#x200B;\n\n[The Coalesced Tsetlin Machine inference structure, introducing multiple outputs with clause sharing.](https://preview.redd.it/90rejdsucoi71.png?width=3755&amp;format=png&amp;auto=webp&amp;s=4a53b53cd66c907765d02838d35ac4164f00cb66)\n\nHi all! We have struggled for quite some time with figuring out how to share AND-rules (clauses) across outputs/classes for Tsetlin Machines.  Our solution is based on an integer weight matrix that maps a shared pool of AND-rules to each output. False Positive outputs and True Positive outputs drive updating. A weight is simply incremented if the rule produces a True Positive output, and the weight is decremented when the rule produces a False Positive output. A positive weight makes the rule vote for output 1, while a negative weight makes the rule vote for output 0.  The rule sharing reduces the number of rules required for accurate image classification, providing reasonable accuracy down to 50 rules per class. Hopefully, this new architecture may also open up for interpretable Tsetlin Machine auto-encoding and self-supervised language modeling.  [https://arxiv.org/abs/2108.07594](https://arxiv.org/abs/2108.07594)", "link": "https://www.reddit.com/r/MachineLearning/comments/p8o9ad/research_coalesced_tsetlin_machine_image/"}, {"autor": "l34df4rm3r", "date": "2021-08-21 06:16:44", "content": "[P][D] looking for high resolution -----> image !!!  datasets, preferably biomedical -----> image !!! s. /!/ Hi, I am working on a project where I am trying to tweak the convolution process to quickly down-sample high resolution images without massive data loss (by avoiding pooling, high dilation, and strides. For example, 2x2 maxpool, or stride=2, dilation=2 means you're simply discarding 75% of the data). \n\nThe intuition is to use this for shallow networks. \n\nWhile my method works (theoretically), I am still searching for datasets that will allow to represent my method better. I have checked the CoronaHack dataset on Kaggle and a few other chest X-ray datasets. But those images are small and also vary in size.\n\nAre there any datasets with decent number of images (\\~2k or so) where the resolution is near 1MP?", "link": "https://www.reddit.com/r/MachineLearning/comments/p8mdfx/pd_looking_for_high_resolution_image_datasets/"}, {"autor": "WarAndGeese", "date": "2021-08-21 06:00:37", "content": "[D] What do you need to create a machine learning model or script to make a robot arm perform a task? /!/ I'm trying to figure out a simple but generalized work flow to get others to be able to work on my hardware. What do you need to be able to make a robotic arm do something? Some telemetry data on how all of the joints are positioned and a live H.264 or MPEG-4 feed from a -----> camera !!!  pointing at both the arm and what's in front of it? Do you need a 3D model in Unity showing both the arm and labelled data of what's around it? Would you prefer that I send you the positioning data of the arm and you to build that 3D model yourself? If you need this data in a live feed, what format do you need it in?", "link": "https://www.reddit.com/r/MachineLearning/comments/p8m69q/d_what_do_you_need_to_create_a_machine_learning/"}, {"autor": "JustSayNoToSummer", "date": "2021-08-20 20:57:43", "content": "VISDA-2021 Leaderboard Up! [N] /!/ A quick update that the VISDA-2021 Leaderboard is up at [https://competitions.codalab.org/competitions/33396#results](https://competitions.codalab.org/competitions/33396#results). Join to win $$$$(2$k 1st/ $500 2nd and 3rd)! \n\nMore details (and registration) at the [website](http://ai.bu.edu/visda-2021/) and below\n\n\\-----------------------------------------------------------------------------\n\nProgress in machine learning is typically measured by training and  testing a model on the same distribution of data, i.e., the same domain. However, in real-world applications, models often encounter  out-of-distribution   data, such as novel -----> camera !!!  viewpoints, backgrounds or image quality.   The Visual Domain Adaptation (VisDA) challenge tests computer vision  models\u2019 ability to generalize and adapt to novel target distributions by  measuring accuracy on out-of-distribution data.\n\nThe 2021 VisDA competition is our 5th time holding the challenge! [\\[2017\\]](http://ai.bu.edu/visda-2017/), [\\[2018\\]](http://ai.bu.edu/visda-2018/), [\\[2019\\]](http://ai.bu.edu/visda-2019/), [\\[2020\\]](http://ai.bu.edu/visda-2020/).  This year, we invite methods that can adapt to novel test distributions   in an open-world setting. Teams will be given labeled source data from   ImageNet and unlabeled target data from a different target  distribution.  In addition to input distribution shift, the target data  may also have  missing and/or novel classes as in the Universal Domain  Adaptation  (UniDA) setting [\\[1\\]](https://arxiv.org/abs/2104.03344).  Successful approaches will improve classification accuracy of known  categories while learning to deal with missing and/or unknown  categories.", "link": "https://www.reddit.com/r/MachineLearning/comments/p8dto2/visda2021_leaderboard_up_n/"}, {"autor": "Yuqing7", "date": "2021-07-28 13:19:52", "content": "[R] MIT &amp; Google Quantum Algorithm Trains Wide and Deep Neural Networks /!/ A research team from MIT and Google Quantum AI presents a quantum algorithm for training classical neural networks in logarithmic time and provides numerical evidence of its efficiency on the standard MNIST -----> image !!!  dataset. \n\nHere is a quick read: [MIT &amp; Google Quantum Algorithm Trains Wide and Deep Neural Networks.](https://syncedreview.com/2021/07/28/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-71/)\n\nThe paper *A Quantum Algorithm for Training Wide and Deep Classical Neural Networks* is on [arXiv](https://arxiv.org/abs/2107.09200).", "link": "https://www.reddit.com/r/MachineLearning/comments/ot9rdg/r_mit_google_quantum_algorithm_trains_wide_and/"}, {"autor": "DolantheMFWizard", "date": "2021-07-28 09:22:59", "content": "[Project] Anyone know a GitHub for a vanilla VAE model with strong colored -----> image !!!  reconstruction? /!/ I'm trying to look into the correlation between reconstruction of states and zero-shot policy transfer to a new domain in RL. I was wondering if anyone knew a GitHub or architecture of a model of a vanilla VAE that had decent reconstruction on 3D colored images?", "link": "https://www.reddit.com/r/MachineLearning/comments/ot6ekl/project_anyone_know_a_github_for_a_vanilla_vae/"}, {"autor": "techsucker", "date": "2021-07-28 07:19:38", "content": "[R] Apple Explains Its New On-Device Machine Learning Methods To Recognize People In Photos With Extreme Poses, Accessories Correctly, Or Even Occluded Faces /!/ Photos are an integral way for people to browse, search, and relive life\u2019s moments with their friends and family. Photos on your apple devices are very often labelled with people to be easily categorized. An algorithm foundational to this goal recognizes faces from different angles using facial recognition software, so it doesn\u2019t take up too much space storing every single -----> image !!!  because one app contains all members\u2019 memories! This process of labelling photos of people uses a number of machine learning algorithms\u2013running privately on-device\u2013to help curate and organize Live Photo videos and regular pictures in categories that you can find later by the person who was depicted within each photo.\n\nQuick Read: [https://www.marktechpost.com/2021/07/28/apple-explains-its-new-on-device-machine-learning-methods-to-recognize-people-in-photos-with-extreme-poses-accessories-correctly-or-even-occluded-faces/](https://www.marktechpost.com/2021/07/28/apple-explains-its-new-on-device-machine-learning-methods-to-recognize-people-in-photos-with-extreme-poses-accessories-correctly-or-even-occluded-faces/) \n\nApple Blog: [https://machinelearning.apple.com/research/recognizing-people-photos](https://machinelearning.apple.com/research/recognizing-people-photos)\n\nhttps://preview.redd.it/rx0sk4pzmwd71.png?width=1392&amp;format=png&amp;auto=webp&amp;s=5877ff8482fa1ddd042a5f4c1743bb7771a677ca", "link": "https://www.reddit.com/r/MachineLearning/comments/ot4yio/r_apple_explains_its_new_ondevice_machine/"}, {"autor": "ThickDoctor007", "date": "2021-07-27 21:28:42", "content": "[D]How to determine which (geo)points represent vehicle on the road /!/ Hi,\n\nI am analyzing tabular data of tractors . Every record contains latitude, longitude, speed, gear etc. There are over 500000 items. As the data is not labeled, I have tried to determine intuitively which points are on the road and which are on the field by setting a speed threshold (probably on the road, while not working in the field, the speed is much lower). Such condition, however, is not accurate as on the road, also, sometimes, the vehicle drives slower or stops (see -----> image !!!  below).\n\nThe goal is to annotate at least a part of the data so then I can train a neural network and hopefully by taking into account all the parameters it will exceed the accuracy of manual threshold setting on limited number of parameters. In order to speed up the process of labeling, I am wondering if there is any way of splitting the points that represent the vehicle on the road and vehicle on the field, based on point pattern. Intuitively, it is easy to spot the points on the road and points on the field but algorithmically I don't know, what is the best way to make a distinction so I would be very thankful for any suggestion.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/8pep7folptd71.png?width=950&amp;format=png&amp;auto=webp&amp;s=30b8f5ffc95246ea694b973fb3c3c15cf69e7e49", "link": "https://www.reddit.com/r/MachineLearning/comments/osvnw3/dhow_to_determine_which_geopoints_represent/"}, {"autor": "Farabundo719", "date": "2021-07-27 20:43:20", "content": "[P] Is this possible to make? /!/ Two or three weeks ago I saw a video about supervised learning, which shows you that by feeding the software images of the same type (Ex: German Shepherd Dogs) it learns to recognize it. So, it is possible to create an -----> image !!!  search engine in which by showing it alike -----> image !!! s it will search on the internet for -----> image !!! s by using the algorithm obtained in the supervised learning?", "link": "https://www.reddit.com/r/MachineLearning/comments/osuspd/p_is_this_possible_to_make/"}, {"autor": "Kiseido", "date": "2021-07-20 22:05:03", "content": "[D] Community Outreach Suggestion with r/3Dprinting /!/ In this recent time of technological advancements, particularly in mass-market \"3d\" printing and \"machine-learned\" real-time -----> image !!!  classification. \n\nI propose a cross over between r/MachineLearning and r/3Dprinting. There is a significant need to make it easier for owners of 3D printers to calibrate their machines. Most owners have to do this via printing a large numbers of samples, as they tune its settings. This is often needed to be redone for each type of material they print with.\n\nI believe it would be simple to obtain countless fairly high quality pictures of known common 3d printing calibration error symptoms from the r/3Dprinting community for free. \n\nI suspect there is enough collective brain-power and good-will on r/MachineLearning to craft an open-source program owners of printers could run on their laptops &amp; desktops with pictures they took on their cell-phones, to help them identify problems, reduce start-up and maintenance time, and cut down on wasting printing material. Less plastic waste is good for everything I suspect.", "link": "https://www.reddit.com/r/MachineLearning/comments/ooc9nh/d_community_outreach_suggestion_with_r3dprinting/"}, {"autor": "peleg1989", "date": "2021-01-21 17:33:15", "content": "[P] I made an -----> image !!!  recognition model written in NodeJs /!/ [https://github.com/ntedgi/node-efficientnet](https://github.com/ntedgi/node-efficientnet)\n\n&amp;#x200B;\n\nThis repository contains a tensorflowJs implementation of **EfficientNet**, an object detection model trained on [ImageNet](http://www.image-net.org/) and can detect [1000 different objects](https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt).\n\nEfficientNet a lightweight convolutional neural network architecture achieving the [state-of-the-art accuracy with an order of magnitude fewer parameters and FLOPS](https://arxiv.org/abs/1905.11946), on both ImageNet and five other commonly used transfer learning datasets.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n*Processing gif mr9wj0531qc61...*", "link": "https://www.reddit.com/r/MachineLearning/comments/l23824/p_i_made_an_image_recognition_model_written_in/"}, {"autor": "peleg1989", "date": "2021-01-21 17:28:23", "content": "-----> Image !!!  recognition model written in nodeJs /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/l2348w/image_recognition_model_written_in_nodejs/"}, {"autor": "peleg1989", "date": "2021-01-21 17:24:28", "content": "-----> image !!!  recognition model written in node js", "link": "https://www.reddit.com/r/MachineLearning/comments/l23183/image_recognition_model_written_in_node_js/"}, {"autor": "RandomGaussian", "date": "2021-01-21 12:19:17", "content": "[D] How to extract features and ensure sparsity with sparse convolutional autoencoders? /!/ I want to build a sparse convolutional autoencoder to do some -----> image !!!  classification but i do not understand how to extract the features from the encoder output. Since it is a convolutional autoencoder, the outputs are feature maps, but for the classification i need a full connected layer.\n\nHow can i transform these feature maps in a vector in a way that makes sense? Of course i could pool one value from each feature map or i could just flatten the maps and put them on a fc layer, but does this makes sense?\n\nAnother question is how to ensure the sparsity during the training of the convolutional autoencoder? I only see examples using the kullback-leibler divergence on dense layers outputs, i could not find an example using sparsity on feature maps. Anyone have experience in this?", "link": "https://www.reddit.com/r/MachineLearning/comments/l1xdy0/d_how_to_extract_features_and_ensure_sparsity/"}, {"autor": "Gullible_Dance", "date": "2021-04-28 20:36:10", "content": "[D] New paper shows that federated learning is broken? /!/ Title: See through Gradients: -----> Image !!!  Batch Recovery via GradInversion \n\n([https://arxiv.org/abs/2104.07586](https://arxiv.org/abs/2104.07586))\n\nThe authors can recover individual training examples from accumulated gradients. What does this mean for data privacy laws?", "link": "https://www.reddit.com/r/MachineLearning/comments/n0o6dn/d_new_paper_shows_that_federated_learning_is/"}, {"autor": "[deleted]", "date": "2021-04-28 20:33:19", "content": "[R] New paper shows federated is broken? 'See through Gradients: -----> Image !!!  Batch Recovery via GradInversion ' /!/ [deleted]", "link": "https://www.reddit.com/r/MachineLearning/comments/n0o47i/r_new_paper_shows_federated_is_broken_see_through/"}, {"autor": "crubier", "date": "2021-04-28 18:22:10", "content": "[P] Labelflow, the open source -----> image !!!  labeling and dataset cleaning platform. /!/ Hi, all! Announcing Labelflow ( [https://www.labelflow.net/](https://www.labelflow.net/) ), the open source image labeling and dataset cleaning platform.\n\nWe are a team of 9 people with experience in image labeling and dataset curating to build quality datasets for deep learning.  We were frustrated by the amount of scripts required to move the data back and forth between tools, and by the lack of control over our data, especially when we have data that can\u2019t easily be shared. \n\nSo we started building Labelflow, an image labeling tool with all the bells and whistles, but with an open source backend that you can connect to your own data stack easily. Let me know what you think, and feel free to request early access to get up to 50% off when we release it in a few months!", "link": "https://www.reddit.com/r/MachineLearning/comments/n0l98m/p_labelflow_the_open_source_image_labeling_and/"}, {"autor": "crubier", "date": "2021-04-28 18:20:46", "content": "Announcing Labelflow, the open source -----> image !!!  labeling and dataset cleaning platform. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/n0l85r/announcing_labelflow_the_open_source_image/"}, {"autor": "wannbebillionaire", "date": "2021-04-28 12:30:56", "content": "[P] Split and parse pdf files contents by subjects /!/  Hi r/MachineLearning\n\nI want to develop an app using deep learning or something(I don't know what I have to study).\n\nBut I am a super newbie in AI or machine learning.\n\nSo I ask you experts.\n\nI have many pdf files and these pdf are images(scan file) or pdf text.\n\nI want to parse pdf file contents and make it to database.\n\nIf I have 3 pdf files.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/3n17kv2mrwv61.png?width=2679&amp;format=png&amp;auto=webp&amp;s=eaa1d3a173d1d6c7783e75acbf8b0dcbd8c84b8e\n\nSplit by title and subjects set as a separated -----> image !!! . And save it in a storage and make the file names to DB.\n\n&amp;#x200B;\n\nQuestion)\n\n1. How can I split pdf image by title and subjects set(red boxes)?\n2. How can I parse texts by title and subjects set?\n3. How can I divide title, subjects and images?", "link": "https://www.reddit.com/r/MachineLearning/comments/n0dqer/p_split_and_parse_pdf_files_contents_by_subjects/"}, {"autor": "SanjivGautamOfficial", "date": "2021-04-28 09:51:39", "content": "[P] MLOverflow - A Webapp for ML/DL /!/ Hello ML/AI enthusiasts,\n\nI created a small (minimalistic design ) webapp for Machine Learning feeds, papers and events and this webapp literally has **Feeds**, **Events** and **Papers** section.\n\nHere is the link: [https://mloverflow.com](https://mloverflow.com/) (I am currently adding contents, but having more people engaged on it would be delightful)\n\nLet's check the features of our webapp if you could spare a moment:\n\n**Feeds** is where you share your medium articles or anything that you find interesting over internet related to ML/AI. May it be linkedin posts about ML/DL. Even reddit link about your project you did related to AI/ML. You made an impression detection app and uploaded in youtube? How about sharing it on this website? Any medium blogs you wrote after reading a paper? Share the **link**.\n\n**Events** is where you share **link** of events related to ML/AI.\n\n**Papers** is where we discuss research papers. People can comment different blogs/videos **links** which will be useful for other users to come.\n\n**COMMENT SECTION:**\n\nWe do have comment section for Feeds and Papers, so it would be easier for people to post related ML/DL posts.\n\nThere'll be 4 types of comments. **Audio, Video, Text, Explain**.\n\nAudio, Video,Text will contain links of websites that will have audio, video or blog post respectively. The **links** not the actual video/audio. **Explain** is where you clarify/write about things there in comment section where you don't need help with links.\n\n**MOTIVATION:**\n\nWhat's the point of creating a site when we already have [r/MachineLearning](https://www.reddit.com/r/MachineLearning/) subreddit, Medium for article sharing and of course when we can google search?\n\nI believe this will make navigation for things easier and having a central place for ML/DL related posts would make it convenient for people interested in ML/DL to further consolidate their understanding.\n\n**Miscellaneous**:\n\nI am not really a great developer, but I think it might help few enthusiasts out there as I am one of them.\n\nNote: *There is no option for uploading audio/video/gifs/-----> image !!!  as of now as I don't see a point of it. But if we need it in future, I would be happy to add them as well.*\n\nP.S: There is a suggestion/report section. So any bugs or any features you think I need to resolve/add, kindly tell me. You can log in and go to [https://mloverflow.com/report](https://mloverflow.com/report) and write it down.\n\nThank you for reading till here. Cheers!", "link": "https://www.reddit.com/r/MachineLearning/comments/n0babg/p_mloverflow_a_webapp_for_mldl/"}, {"autor": "SQL_beginner", "date": "2021-04-28 04:18:23", "content": "[D] is the \"function\" that neural networks are trying to aproximate \"conceivable\"? /!/ Based on the \"universal aproximation theorem\" (  https://en.m.wikipedia.org/wiki/Universal_approximation_theorem), \nneural network are said to be able to approximate any \"function\" reasonably well.\n\nFrom a very basic definition in math, here is what I think of when I hear the word \"function\" :\n https://www.mathsisfun.com/sets/images/function-sets.svg\n\nSuppose you have a task in which you have pictures of cats and dogs. You want to train a neural network to recognize pictures (i guess pictures can be considered as large matrices of numbers). In this case, what is the \"function\" that the neural network is trying to aproximate?\n\nLike in this -----> picture !!!  (https://www.mathsisfun.com/sets/images/function-sets.svg),  can you say that the \"orange circle\" are the numbers contained in the pixels corresponding to \"cat\" or \"dog\" ...and these are \"mapped\" to the  \"yellow circle\" containing \"cat\" or \"dog\"?\n\nIn this example, is the \"function\" an abstract concept that can not be directly observed (numbers contained in pixels -&gt; dog or cat)?", "link": "https://www.reddit.com/r/MachineLearning/comments/n06pwg/d_is_the_function_that_neural_networks_are_trying/"}, {"autor": "egis123cool", "date": "2021-01-14 07:34:02", "content": "Approximately converting small text from a -----> picture !!!  into normal size text? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/kx10l8/approximately_converting_small_text_from_a/"}, {"autor": "BryanP1412", "date": "2021-01-14 00:55:06", "content": "[R] Best Neural Network for Instance Segmentation? /!/  \n\nHello everyone, I am working on a personal project with a neural network that allows to perform Instance Segmentation. I am using a Mask RCNN ResNet101 MS COCO implemented in the toolkit GluonCV ( [Segmentation \u2014 gluoncv 0.10.0 documentation](https://cv.gluon.ai/model_zoo/segmentation.html) ). The neural network is very slow, it takes approximately 1 minute to process an -----> image !!! .\n\nDo you know of any other neural network that is faster and that also performs Instance Segmentation?\n\nThanks to everyone.", "link": "https://www.reddit.com/r/MachineLearning/comments/kwuk3g/r_best_neural_network_for_instance_segmentation/"}, {"autor": "ifelsestatement007", "date": "2021-01-13 22:52:41", "content": "-----> Image !!!  classification with PyTorch tutorials for beginners", "link": "https://www.reddit.com/r/MachineLearning/comments/kws4qi/image_classification_with_pytorch_tutorials_for/"}, {"autor": "vadim878", "date": "2021-01-13 10:05:32", "content": "[D] Why does LIME suggest to use super pixels for the -----> image !!!  data? /!/ I was wondering why does LIME propose the ImageLIME version; why can't we use the TabularLIME method? According to my experiments on MNIST, it shows better results. \n\nI find explanations from the ImageLIME version are very often misleading due to the segmentation... However, the TabularLIME shows the importance of all pixels separately.", "link": "https://www.reddit.com/r/MachineLearning/comments/kwdjmy/d_why_does_lime_suggest_to_use_super_pixels_for/"}, {"autor": "Suzzy67", "date": "2021-03-17 16:46:57", "content": "[P] Predict function /!/ can anyone help me in coding..i want loaded model and predict -----> image !!!  funtion in order tonpredict one -----> image !!! s by using keras.?", "link": "https://www.reddit.com/r/MachineLearning/comments/m74phg/p_predict_function/"}, {"autor": "rockofsoap", "date": "2021-03-17 10:54:55", "content": "[P] Need help for the creation of a players ranking using ML /!/ Hi all.First of all I apologize if it is not the right place to post a question like the following one, but since I do not know where to ask, here I am !\n\nSo... to make a bit of context, the project I work on is the creation of a players ranking (similar to the ELO ranking in chess) with the use of machine learning. More precisely I have a huge dataset of matchs beetween players that you can see with the -----> image !!!  below.\n\nhttps://preview.redd.it/bv7nsor3kkn61.png?width=1121&amp;format=png&amp;auto=webp&amp;s=f253f3d50ab64c9e6c145de90ccf14b2896491fe\n\nIn this dataset , the *setDay* is the day the match happened. All columns finishing by 1 refers to the first player and all columns finishing by 2 refers to the second player. The columns *setsPlayedIn1YearX* refers to the number of matchs the player X have played in one whole year. And finally the column winner is set to 0 if player1 won and 1 if player2 won. For an unbiased learning, all the matchs have been doubled with player1 becoming player2 and player2 becoming player1 (in order to have a symmetric database).\n\nWhat I want in the end is a players ranking on the last day a match have been played. For that, my methodology was the following :\n\n1. Convert the countries from string to int\n2. Take multiple sklearn and XGboost models\n3. Perform a Bayes Search on all the selected models to have an overview of the best parameters for each model\n4. Perform a cross-validation with all the selected models set to their best parameters according to the Bayes Search\n5. Select the best performing model (here it was the XGBClassifier)\n6. At this stage, I have a model which can predict the result of a match beetween 2 players. What I did to go from the model to a players ranking is the following : compute the model predictions on all the matchs beetween all the player and arrange the players from best to worst according to the number of time they are preticted to win (i.e the best player is the player who is predicted to win the most among all the players).\n\nWith this whole methodology comes many problems :\n\n1. The only deep learning model I use is the MLP from sklearn\n2. The computational time of the Bayes Search is huge (due to the weight of the dataset) so I do not explore the search spaces a lot\n3. The computational time of the step 6 is huuuuuge\n\nAnd with this methodology comes many incertainties on my part which are the purpose of my post here :\n\n1. How can I do to make the model understand well about the role that the time plays in the dataset  (i.e if a player did not play for a long period of time, then his level should be lowered) ? Is the way I use the *setDay* column a good way to do it ?\n2. Do I need to normalize the dataset beetween -1 and 1 or 0 and 1 (to minimize the impact of huge numbers like the *ids*) ?\n3. Do you think that the step 6 is a good way to convert from a match prediction model to a players ranking ? If not, what should I do instead ?\n4. Do you think that some advanced deep learning models could be interesting to test for this project (in addition to the basic MLP) ?\n\nI really hope that you guys will help me.\n\nThanks in advance.\n\nRockOfSoap", "link": "https://www.reddit.com/r/MachineLearning/comments/m6xs0r/p_need_help_for_the_creation_of_a_players_ranking/"}, {"autor": "dailycodingpost", "date": "2021-05-30 06:42:31", "content": "Watch as this 12 year old kid uses machine learning OCR to find a word in a book by clicking a -----> picture !!! ", "link": "https://www.reddit.com/r/MachineLearning/comments/no5a8d/watch_as_this_12_year_old_kid_uses_machine/"}, {"autor": "csciutto", "date": "2021-05-29 23:52:26", "content": "[D] Contrastive loss on sequences /!/ I'm working on labeling a sequence of speech embeddings with a speaker. These speakers aren't known, so usually a unsupervised clustering approach is taken.\n\nMy hunch is that I can incorporate some temporal information across the speech embeddings by passing the input embeddings through an LSTM, and then doing the clustering on the hidden states.\n\nTo enforce that embeddings from the speakers are close, I've thought of simply encouraging the cosine distance of hidden states for the same speakers to be 1, and for different speakers to be -1. Something along the lines of:\n\n    X = speech embeddings          # shape (N, D1)\n    Y = labels                     # shape (N, 1)\n    H = LSTM(X)                    # shape (N, D2)\n    \n    H = normalize(H, dim=1)        # normalize for cosine\n    sim = H @ H.T                  # pair-wise cosine distance in (-1, 1)\n    sim = 0.5 * (sim + 1)          # cosine distance in (0, 1)\n    target = (Y == Y.T).           # boolean if (i, j) same speaker\n    loss = CE(similarity, target)\n\nI have very little experience with this kind of unsupervised contrastive learning, so this was just the a simplistic initial approach I thought of.\n\nWhen looking at some papers (e.g. SimCLR), it seems that the losses are designed for a source -----> image !!! , an augmented positive, and some negative examples, which seems amenable to a similar simplistic approach. What's the reason why the below loss is so much better?\n\nhttps://preview.redd.it/3tkzw11zc5271.png?width=870&amp;format=png&amp;auto=webp&amp;s=ebd3dc8c227721d5429d6abbabe5d65c9968b113", "link": "https://www.reddit.com/r/MachineLearning/comments/nnz7mp/d_contrastive_loss_on_sequences/"}, {"autor": "chaosbutters", "date": "2021-05-29 18:07:32", "content": "[D] Does anyone happen to know a python ML algorithm to make generative art from an inputted -----> image !!!  with some curves on it? /!/ Basically, if i input a white image with a few lines scribbled on it, it makes some basic outputted art.\n\nEither looking for an algorithm to implement or ideally code to train my own network", "link": "https://www.reddit.com/r/MachineLearning/comments/nnsyzw/d_does_anyone_happen_to_know_a_python_ml/"}, {"autor": "chaosbutters", "date": "2021-05-29 18:05:54", "content": "Does anyone happen to know a python ML algorithm to make generative art from an inputted -----> image !!!  with some curves on it? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/nnsxu2/does_anyone_happen_to_know_a_python_ml_algorithm/"}, {"autor": "gpahul", "date": "2021-05-29 15:34:18", "content": "[D] Garments Crease/Wrinkles/Dirt Removal using Computer Vision? /!/ Hi Reddit,\n\nI've been looking for some AI/Computer Vision based automated way to remove crease, wrinkles, folds, dirt from clothes in fashion images captured for ecommerce.\n\nI did try to search for GAN based methods or -----> Image !!!  inpainting based methods but couldn't find anything reliable. I am looking for some way to retouch the image in a way that the wrinkles/folds/crease/dirt on the clothes could be removed.\n\nI could find few websites which are doing the similar work which I require to attain programmatically but I couldn't find exactly where in computer vision to look for.\n\nSample Solution:  \n\nhttps://imageedit.ai/   \n\nhttps://retouch4.me/cleanbackdrop   \n\nhttps://studiodrop.com/retouching/   \n\n\nCould anyone provide me any pointers/help what and where I can look?\n\nAny help is highly appreciated. Thanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/nnpwi3/d_garments_creasewrinklesdirt_removal_using/"}, {"autor": "KirillTheMunchKing", "date": "2021-05-29 14:51:26", "content": "[D] Paper explained: Endless Loops: Detecting and Animating Periodic Patterns in Still Images. /!/ *Processing video 1y35wimuo2271...*\n\nHave you ever taken a still -----> photo !!!  and later realized how cool it would have been to take a video instead. The authors of the \"Endless Loops\" paper got you covered. They propose a novel method that creates seamless animated loops from single images. The algorithm is able to detect periodic structures in the input images that it uses to predict a motion field for the region, and finally smoothly warps the image to produce a continuous animation loop. Read the [full explanation](https://t.me/casual_gan/44) in the Casual GAN Papers blog to find out about detecting repetitions in images, predicting the motion field and generating seamless animation loops from flow vectors!\n\n\\[[Full Explanation Post](https://t.me/casual_gan/44)\\] \\[[Arxiv](https://storage.googleapis.com/ltx-public-images/Endless_Loops__Detecting_and_animating_periodic_patterns_in_still_images.pdf)\\] \\[[Project page](https://pub.res.lightricks.com/endless-loops/)\\]\n\nMore recent popular computer vision paper explanations:\n\n&gt;\\[[CoModGAN](https://t.me/casual_gan/43)\\]  \n\\[[GANCraft](https://t.me/casual_gan/41)\\]  \n\\[[DINO](https://t.me/casual_gan/40)\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/nnp1vy/d_paper_explained_endless_loops_detecting_and/"}, {"autor": "OnlyProggingForFun", "date": "2021-05-29 14:28:26", "content": "[News][Research] High-Resolution Photorealistic Image Translation in Real-Time /!/ Apply any style to your 4K -----> image !!!  in real-time using this new machine learning-based approach! \n\nThe paper is **High-Resolution Photorealistic Image Translation in Real-Time: A Laplacian Pyramid Translation Network** by Liang, Jie and Zeng, Hui and Zhang, Lei, (2021), [https://export.arxiv.org/pdf/2105.09188.pdf](https://export.arxiv.org/pdf/2105.09188.pdf)\n\nThey use high and low-frequency versions of the image to optimize the computation time and resources needed. It can run a 4K image in less than a tenth of a second with a single regular GPU.\n\n**Code publicly available**: [https://github.com/csjliang/LPTN](https://github.com/csjliang/LPTN)\n\nShort read: [https://www.louisbouchard.ai/4k-image-translation-in-real-time/](https://www.louisbouchard.ai/4k-image-translation-in-real-time/)\n\nVideo Demo: [https://youtu.be/X7WzlAyUGPo](https://youtu.be/X7WzlAyUGPo)", "link": "https://www.reddit.com/r/MachineLearning/comments/nnolpp/newsresearch_highresolution_photorealistic_image/"}, {"autor": "GabeLoupe", "date": "2021-05-29 14:24:11", "content": "[D] - Dataset management - What is the best tool for a team to collaborate over an Image/Audio dataset? /!/ Our team is doing several audio and -----> image !!!  detection projects and we have not found a great tool for the whole team \\[of varying levels of technical expertiese\\] to collaborate in. So far, we have ended up going with Dropbox, because it's easy enough for our collection team to view and review images &amp; audio, but also has a robust API to plug into our pipeline. Seems like some options would be Scale / Roboflow / V7 Labs...? Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/nnoij2/d_dataset_management_what_is_the_best_tool_for_a/"}, {"autor": "Marco_hmchun", "date": "2021-05-29 09:00:04", "content": "[D] Any suggestions for regression task in deep learning? /!/ ## background\n\nI am studying how to use video data to predict temporal regression value. Firstly, I tried using some backbone with pretrained parameters. Here is a part of my code.\n\n    class Mynet\\_LSTM(nn.Module):  \n     def \\_\\_init\\_\\_(self, model, num\\_classes):  \n     super().\\_\\_init\\_\\_()  \n     self.featureModel = nn.Sequential(\\*list(model.children())\\[:-1\\])  \n     self.linear = nn.Sequential(nn.LayerNorm(2048), nn.Linear(2048, 256))  \n     self.rnn = nn.LSTM(256, 128, 2, bidirectional=True, dropout=0.3)  \n     self.classifier = nn.Sequential(nn.LayerNorm(256), nn.Linear(256, num\\_classes))  \n    \u200b  \n     for m in self.modules():  \n     if isinstance(m, nn.Linear):  \n     n = m.in\\_features  \n     y = 1.0 / np.sqrt(n)  \n     m.weight.data.uniform\\_(-y, y)  \n     m.bias.data.fill\\_(0)  \n    \u200b  \n     def forward(self, x):  \n     b, c, t, h, w = x.shape  \n     x = einops.rearrange(x, 'b c t h w -&gt; (b t) c h w', b=b, t=t)  \n     x = self.featureModel(x).squeeze()  # -&gt; (b t), 2048  \n     x = self.linear(x)  # -&gt; (b t), 256  \n     x = einops.rearrange(x, '(b t) d -&gt; b t d', b=b, t=t)  \n     x, \\_ = self.rnn(x)  \n     x = einops.rearrange(x, 'b t d -&gt; (b t) d', b=b, t=t)  \n     x = self.classifier(x)  \n     x = einops.rearrange(x, '(b t) d -&gt; b t d', b=b, t=t)  \n     return x  \n    \u200b  \n    \\# this is an example how to use my Net  \n    model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101\\_32x8d\\_wsl')  \n    mymodel = Mynet\\_LSTM(model, 2)  \n    frame\\_for\\_example = torch.rand(2, 3, 16, 400, 400) # b, c, t, h, w\n\nThe result are not good as I expected as shown in the -----> picture !!!  below. In the training time, it worked so efficiently, but then the performing became worse during the validating time. After few days of debugging, I found out that this is because no matter what the input data is, my network will always show the output as \u201cthe last given ground\\_truth\u201d. By the way, I am using nn.MSELoss() as my criterion.\n\n&amp;#x200B;\n\nIn the case of using CCC as my criterion, the metrics for my tasks should commonly be CCC as well. My results are shown below; even keep training for more than 5-6 epoch, the results are still the same.\n\n## \n\n## question\n\nI would like to know why this strange result happened? And is there any suggestion for me to train the regression network?\n\nThanks\n\n\\--Joanna", "link": "https://www.reddit.com/r/MachineLearning/comments/nnjduf/d_any_suggestions_for_regression_task_in_deep/"}, {"autor": "[deleted]", "date": "2021-06-08 08:34:03", "content": "Training an -----> image !!!  classification model using ML.NET /!/ [deleted]", "link": "https://www.reddit.com/r/MachineLearning/comments/nuzzsb/training_an_image_classification_model_using_mlnet/"}, {"autor": "DevStarship", "date": "2021-06-08 08:33:08", "content": "Training an -----> image !!!  classification model to identify a goody or a baddy cartoon character using ML.NET /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/nuzzcf/training_an_image_classification_model_to/"}, {"autor": "EmilianoyBeatriz", "date": "2021-06-07 12:01:40", "content": "[D] -----> Image !!!  enhanced based on ML /!/ Ive run into a couple of websites where you can upload your picture and it will give you an upscaled version of it, like: \"[https://imglarger.com/Enhancer](https://imglarger.com/Enhancer)\" and \"[https://bigjpg.com/](https://bigjpg.com/)\". But the results are pretty mediocre, does anyone know good websites/software for this?", "link": "https://www.reddit.com/r/MachineLearning/comments/nuap2p/d_image_enhanced_based_on_ml/"}, {"autor": "EmilianoyBeatriz", "date": "2021-06-07 11:51:24", "content": "-----> Image !!!  enhancer based on ML /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/nuai1r/image_enhancer_based_on_ml/"}, {"autor": "EmilianoyBeatriz", "date": "2021-06-07 11:50:37", "content": "-----> image !!!  enhancer based on ML /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/nuahi5/image_enhancer_based_on_ml/"}, {"autor": "CC_sciguy", "date": "2021-06-07 06:14:52", "content": "[D] Library for making splits /!/  A few times I've needed to split my data while attempting to keep various meta features of the data in particular proportion across the datasets.\n\nFor instance, if I am training an -----> image !!!  classification model on -----> image !!! s which I scraped from the web, I might want to attempt to keep roughly the same proportion of -----> image !!! s scraped from instagram in the training/test/validation (so that if my total dataset is 30% instagram, each of my splits has \\~30% instagram -----> image !!! s). Conversely, I might want to ensure that all the images from a particular user on instagram in one particular set (because the user might have some specific logo on their images or something that I don't want to accidentally learn and claim great performance discriminating against. Another example is in a multi-class classifier, I would want to ensure the number of examples of each class are represented roughly equally in each split.\n\nThe basic problem would be given a table where each row represents an example datapoint and each column represents a meta variable. I want a program that partitions the rows into three groups (presumably I can choose the percentage that goes into each partition) such that each column I specify to be split evenly is roughly distributed evenly in the partitions, and each column that I specify to be to be split exclusively has all rows with the same value for that column stay in exactly one split.\n\nI've written some code to do this a few times, but I normally just roll a random split and then check if everything is within some tolerance. I'm sure there is a more optimal way to do this, but before I spend the time to figure this out on my own, I just wanted to know if something like this already exists.\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/nu5kp3/d_library_for_making_splits/"}, {"autor": "KirillTheMunchKing", "date": "2021-06-06 23:23:28", "content": "[D] Paper Explained: VQGAN - Taming Transformers for High-Resolution -----> Image !!!  Synthesis /!/ It is a lucrative idea to combine the effectiveness of the inductive bias of CNNs with the expressiveness of transformers, yet only recently such an approach was proven to be not only possible but extremely powerful as well. I am of course talking about \"Taming Transformers\" - a paper from 2020 that proposes a novel generator architecture where a CNN learns a context-rich vocabulary of discrete codes and a transformer learns to model their composition as high-resolution images in both conditional and unconditional generation settings.\n\nTo learn how the authors managed to create an effective codebook of perceptually rich discrete image components, and how they cleverly applied latent transformers to generate high-resolution images despite severe memory constraints check out [the full explanation post](https://t.me/casual_gan/46)!\n\nMeanwhile, check out this paper poster provided by [Casual GAN Papers](https://t.me/casual_gan):\n\n[Paper poster](https://preview.redd.it/xvsadvykbq371.png?width=2064&amp;format=png&amp;auto=webp&amp;s=f2e1c3f74c704f4d69b3028dcf2ad0aac12f5b4e)\n\n\\[[Full Explanation Post](https://t.me/casual_gan/46)\\] \\[[Arxiv](https://arxiv.org/abs/2012.09841)\\] \\[[Project page](https://github.com/CompVis/taming-transformers)\\]\n\nMore recent popular computer vision paper explanations:\n\n&gt;\\[[CoModGAN](https://t.me/casual_gan/43)\\]\\[[GANCraft](https://t.me/casual_gan/41)\\]\\[[DINO](https://t.me/casual_gan/40)\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/ntyhzz/d_paper_explained_vqgan_taming_transformers_for/"}, {"autor": "EgorBykov", "date": "2021-06-06 22:56:45", "content": "[P] Performance-oriented monitoring and bottleneck-detection /!/ In my [previous post ](https://www.reddit.com/r/MachineLearning/comments/nk3opi/d_r_bert_base_choosing_optimal_cloud/)I described a case how basic monitoring might help to save &gt;10x on compute power for a specific NN model (BERT Base Uncased).\n\nIf you benchmark something, setting up a monitoring is trivial for one host, but  might be an ache if you have a k8s cluster. Also, there is no fun in setting this up every time, especially if you lack DevOps experience. And there is no point in manually parsing through the data and looking at graphs if you know exactly what you are looking for and what\u2019s relevant for the performance/cost.\n\nWith my team we do that kind of cost/performance optimization service for our customers all the time, so we have built a set of simple tools for ourselves over time.\n\nAfter some feedback from my first post, we\u2019ve put together[ a public web app](https://bottleneck.rocketcompute.com/) with some of the tools we use, so others might use it too.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/5b1lw3gl4q371.png?width=1999&amp;format=png&amp;auto=webp&amp;s=394932fd38019e871998b6541ed641639bd901bc\n\n&amp;#x200B;\n\nhttps://preview.redd.it/ynslc6tr4q371.png?width=1839&amp;format=png&amp;auto=webp&amp;s=f45f86f45a7b282856f47f1eeebc43787e63ae78\n\n&amp;#x200B;\n\nhttps://preview.redd.it/tfykqmjt4q371.png?width=1839&amp;format=png&amp;auto=webp&amp;s=02c28cdf2863650a89472a675d3ea833952ee65d\n\nBasically, what it does - you can deploy well-tuned Telegraf-based monitoring with one command on any VM (AWS, GCP or on-prem), k8s cluster or bake it into an -----> image !!!  and get basic visualization dashboards right away.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/hz3er5f05q371.png?width=1888&amp;format=png&amp;auto=webp&amp;s=ed58e98645097f725a5b25916c8187866ecdd622\n\nYou also get an automated detection of some relevant bottlenecks and idle times on your infrastructure and can zoom in on them right away (including those I\u2019ve mentioned in my previous post like one-thread preprocessing bottleneck, underutilized GPU, idle times, etc\u2026).\n\n&amp;#x200B;\n\nhttps://preview.redd.it/ho2euln55q371.png?width=1288&amp;format=png&amp;auto=webp&amp;s=3dd0c1ac3ee9da908d1a15e0ac988b6b633d1258\n\n&amp;#x200B;\n\nhttps://preview.redd.it/sn3uyb5a5q371.png?width=1264&amp;format=png&amp;auto=webp&amp;s=97204b3af21923e90a3f9f7c05f066ba041af928\n\nSo if you have benchmarking routines similar to ours and just need something to quickly glance at metrics without a world of pain - here you go (if not, just stroll on,  you lucky bastard).\n\nLMK if that\u2019s useful or if you have any other ideas what might be.\n\nPS If that catches any attention we are going to add an automated benchmarking (runs the same container on a set of instances and produces a nice summary report with runtime, price and metrics for each run). \n\nPPS we just decided to publish it, so if anything is glitching - email me at egor@rocketcompute.com.", "link": "https://www.reddit.com/r/MachineLearning/comments/ntxz3b/p_performanceoriented_monitoring_and/"}, {"autor": "minimaxir", "date": "2021-06-14 15:48:15", "content": "[P] Fun and Dystopia With AI-Based Code Generation Using GPT-J-6B /!/ GPT-J was released a few days ago, and in testing I noticed it did code generation rather well, so I ran it through a few more tests and the results are...chaotic, to say the least.\n\nhttps://minimaxir.com/2021/06/gpt-j-6b/\n\nSome example generated code:\n\n    def is_bird(img):\n        \"\"\"Check whether an -----> image !!!  is a bird\"\"\"\n        with tf.name_scope('get_bird'):\n            a = vgg16.select_roi_feat(img)\n            h, w, _ = tf.-----> image !!! .extract_-----> image !!! _patches(\n                a, 7, 7, 4, 4, 1, 1)\n            b = tf.reshape(h, [1, 7, 7, 256])\n            c = tf.reshape(w, [1, 7, 7, 256])\n            bird_img = tf.subtract(b, c)\n        return bird_img\n\n---\n\n    def is_cake(cake):\n        \"\"\"Check whether the cake is true\"\"\"\n        if not cake:\n            print(\"It's a lie!\")\n            return\n        else:\n            print(\"It's a true fact!\")\n            return True\n\n---\n\n    def is_ai_generated(text):\n        \"\"\"Check whether a text was generated by an AI language model (e.g. GPT-2)\"\"\"\n\n        # See gh-196 for details about why we do this.\n        # Note: this relies on the fixed set of standard AI terms and the logic that\n        # standard AI-generated texts share certain words.\n        return re.search(r\"^(#[A-Za-z]+):$\", text) or \\\n               re.search(r\"^Generated by:$\", text) or \\\n               re.search(r\"^All your base are belong to us$\", text) or \\\n               re.search(r\"^$\", text)", "link": "https://www.reddit.com/r/MachineLearning/comments/nzptwa/p_fun_and_dystopia_with_aibased_code_generation/"}, {"autor": "Pseudoabdul", "date": "2021-06-14 09:26:03", "content": "Combining images and other numeric features in a CNN [D]. /!/ I have been working a little bit on a traffic problem. I have been given a dataset of intersections with a bunch of numeric features with crashes as the response variable. However, in the dataset, I was also given the latitude and longitude. So I wrote a script to query the Google Maps API to generate images of each intersection. \n\nOnce that was done, I went about trying to combine the features I already had with the features I already had. But when I sat down to write up the CNN, I realized I had no way of combining the -----> image !!!  with the other features. I searched around for a bit, but didn't find anything concrete, which was surprising. \n\nSo from what I did read you have two options:\n\n1. Somehow imprint the the other feature data into the image, and run it through a normal CNN.\n\n2. Run just the image through the CNN to compress it down to a 1D array of information, then combine that array with the other feature data in another NN. \n\nHas anyone had any experience with this?", "link": "https://www.reddit.com/r/MachineLearning/comments/nziumg/combining_images_and_other_numeric_features_in_a/"}, {"autor": "ngxnam", "date": "2021-06-14 03:54:21", "content": "[D] What should I do after building a baseline solution? /!/  I'm new to neural network. I am really curious to know what will you guys do after building a baseline model. When participating a competition (Kaggle), I often build an end to end baseline solution to ensure it work without bug. For example, in -----> image !!!  classification competition, I usually do something check list below.\n\n**Data preparation**\n\n* Resize and Normalize only\n* No special augmentation\n\n**Setup neural network**\n\n* Learning rate: 3e-4\n* Adam Optimizer\n* Cross Entropy Loss or Binary Cross Entropy Loss\n* Pre-train neural network ( such as resnet18, resnet34)\n* Turning off all regularization hyper-parameters\n\n**Training**\n\n* Sampling one batch to overfit easily\n* No learning rate scheduler\n* Training and validating one fold only\n* Early stopping\n* Plotting loss and metric scores\n\n**Testing**\n\n* Running above model with test set\n* Exporting test predictions to submission.csv then submit to public Leader Board (LB).\n\nIf the result makes sense, the baseline seems work without bug.  \nThen, I train all samples with all folds and submit again to check baseline score.\n\nAfter that, I struggle to figure out what should I do further (to improve LB score).  \nI've tried to adjust learning rate to overfit train dataset. Then, adding some augmentations (such as random resize crop, flip, transpose\u2026) then check LB score again. The score was improved a little but it was still far from top LB, of course. Some time, it was not much different from baseline score.\n\nI read some Kaggle forum discussions, some nice guys shared their solutions to get higher score in LB. They usually used different parameters than me, such as:\n\n* Fancy augmentations\n* Bigger pre-train model or adding some layers/blocks to pre-train model\n* Custom Loss function\n* Modern learning rate scheduler\n* Ensemble many models\n* Etc, etc\u2026\n\nI want to reproduce their solutions to learn their knowhow. But I don't know what to do first.  \nShould I try switch to bigger model first? Or switching loss function first? Or adding fancy augmentation first? Or changing loss function first? etc\u2026\n\nSince there are too many black boxes in neural network, it is not easy to understand or interpret how a network works. But I think there are some strategies or disciplines to help.\n\nWhat will you do to overcome this problem? Please share your own wisdom. Any comments and suggestions will be appreciated.\n\nBest regards,\n\nP/S: My baseline checklist is inspired by this famous blog:  \n[http://karpathy.github.io/2019/04/25/recipe/](http://karpathy.github.io/2019/04/25/recipe/)", "link": "https://www.reddit.com/r/MachineLearning/comments/nzdywk/d_what_should_i_do_after_building_a_baseline/"}, {"autor": "Routine-Bat0", "date": "2021-09-07 09:07:02", "content": "[R] RAMA: A Rapid Multicut Algorithm on GPU /!/ We devise a highly parallel primal-dual algorithm for the multicut (a.k.a. correlation clustering) problem, a classical graph clustering problem widely used in machine learning and computer vision. Code (with Python interface) available at: [RAMA](https://github.com/pawelswoboda/RAMA). Some advantages are:  \n\n\n1. No need to specify the number of clusters (e.g. the 'k' in k-means).\n2. Only takes affinity between a pair of nodes in the graph as input (positive values for attraction and negative for repulsion).\n3. Since the problem is in NP-hard in general, one can use the lower bounds computed from our solver to know the quality of primal solution.\n4. We implement the solver on GPU getting 10x - 100x speed-up as compared to CPU solvers. For example, clustering an -----> image !!!  from Cityscapes (*res: 1920 x 1080*) takes around 1 second. \n\nAuthor of the paper here. Happy to discuss.   \n\n\n&amp;#x200B;\n\n[Progression on an image from Cityscapes ](https://i.redd.it/w6n317i6r1m71.gif)", "link": "https://www.reddit.com/r/MachineLearning/comments/pjjeao/r_rama_a_rapid_multicut_algorithm_on_gpu/"}, {"autor": "techsucker", "date": "2021-09-07 06:43:57", "content": "[R] AI Researchers From Stanford University Unveils pi-GAN: A Novel Periodic Implicit GAN For 3D-Aware -----> Image !!!  Synthesis (Paper, Code included) /!/ 3D-aware image synthesis has made rapid progress, but two problems remain. First, existing approaches can lack an underlying 3D representation or rely on view inconsistent rendering to synthesize images that are not multi-view consistent. Second, they may depend upon network architectures that do not produce high-quality results because the methods used for generation limit their expressiveness and ability to create realistic content.\n\nA research group from Stanford University introduce a generative adversarial approach to unsupervised 3D representation learning, called Periodic Implicit Generative Adversarial Networks (\u03c0-GAN or pi-GAN). This method is superior to other existing methods and outperforms them in both quality and speed. The \u03c0-GAN conditions a latent radiance field which is represented by the SIREN network, a fully connected neural network with periodic activation functions. The conditioned radiance field maps a 3D location and 2D viewing direction with view-dependent radiance and view-independent volume density. it is possible to render the radiance field from arbitrary camera poses using a differentiable volume rendering method that relies on classical volume rendering techniques.\n\n# [5 Min Read](https://www.marktechpost.com/2021/09/06/ai-researchers-from-stanford-university-unveils-pi-gan-a-novel-periodic-implicit-gan-for-3d-aware-image-synthesis/) | [Paper](https://arxiv.org/pdf/2012.00926.pdf) | [Project](https://marcoamonteiro.github.io/pi-GAN-website/) | [Code](https://github.com/marcoamonteiro/pi-GAN)\n\n&amp;#x200B;\n\n![video](dz9reay021m71)", "link": "https://www.reddit.com/r/MachineLearning/comments/pjhoem/r_ai_researchers_from_stanford_university_unveils/"}, {"autor": "gradientpenalty", "date": "2021-09-07 02:20:21", "content": "[R] [ICCV 2021] Gradient Normalization for Generative Adversarial Networks /!/ &amp;#x200B;\n\n[-----> Image !!!  samples from Celeb and LSUN Church](https://preview.redd.it/7hmss3i9pzl71.png?width=981&amp;format=png&amp;auto=webp&amp;s=c130662b368e294be83f524827d68a41053bf65a)\n\nHi, I present one of the paper I recently participated. We propose a new normalization method (with concrete proof ) for GANs which improves from previous penalty based variants and spectral normalization in terms of FID and Inception score.\n\nGradient normalization method imposes a hard 1-Lipschitz constraint on the discriminator function, which increases the capacity of the discriminators ( generator have to generate higher \"fidelity\" images to cheat the discriminator )\n\nIf you are interested feel free to checkout out arxiv preprint version:\n\nArxiv : [https://arxiv.org/abs/2109.02235](https://arxiv.org/abs/2109.02235)\n\nReaders friendly : [https://www.arxiv-vanity.com/papers/2109.02235/](https://www.arxiv-vanity.com/papers/2109.02235/)", "link": "https://www.reddit.com/r/MachineLearning/comments/pjdvi4/r_iccv_2021_gradient_normalization_for_generative/"}, {"autor": "someonerezcody", "date": "2021-09-06 21:10:47", "content": "[D] A question about machine algo's in bird feather identification /!/ I recently found a feather in my yard and was curious to know the species of bird that the feather came from. While researching, I found a feather atlas that is hosted online. It's a fantastic resource for assisting in identifying bird feathers, however the process was very manual and cumbersome.\n\nI left the experience feeling as though there is potential for optimization in this process using machine learning algo's... The resources I've found that are currently available rely on user input and often ambiguity is present with multiple possible birds.\n\nI was curious to know if any research has been done for a way to identify a bird species from a -----> photo !!!  of the feather. I'd appreciate any links you have for me on the subject. An app that can identify a photo of a bird feather would be kinda cool and I'd love to see it if it exists out there.\n\nThanks for reading!", "link": "https://www.reddit.com/r/MachineLearning/comments/pj8m1a/d_a_question_about_machine_algos_in_bird_feather/"}, {"autor": "DanishAli9188", "date": "2021-09-06 15:13:26", "content": "[R] MERN STACK neglect App /!/ Hello folks!, I have a project idea for neglect detection mern app(Web App) using Image Processing consisting of two parts, i need help regarding which things i need to learn and roadmap for learning to do this project\n\n&amp;#x200B;\n\nPatient will be provided with pictures and patient will mark objects with marker provided in web interface .\n\n&amp;#x200B;\n\nfor example he was provided with -----> picture !!!  containing circles and patient will mark the circles with marker provided in web interface. need to detect which circle he missed to mark? he missed circle from left side or right side? or no circle were missed\n\n&amp;#x200B;\n\nclassified as left, right or no neglect\n\n&amp;#x200B;\n\n2) Patient will be provided with picture and he will have to draw that picture on whiteboard provided in interface, when he finish copying i need to detect left side of white board image is empty or right side or no side is empty?\n\n&amp;#x200B;\n\nI am unable to understand what things i need to learn , need a clear roadmap, kindly guide me in this regard", "link": "https://www.reddit.com/r/MachineLearning/comments/pj1any/r_mern_stack_neglect_app/"}, {"autor": "CaptainI9C3G6", "date": "2021-09-06 11:24:48", "content": "[D] Can anyone recommend a tool for sorting -----> image !!! s into folders for -----> image !!!  classification? /!/ Hi, so ideally I'd like a tool with which I can open a folder and show me each image in turn. ON each image I would then like to press a number key to move that image to one of the subfolders in that folder. Each image is about 3-4 MB so ideally the moving would be in the background.\n\nDoes anything like this exist?", "link": "https://www.reddit.com/r/MachineLearning/comments/pixkhd/d_can_anyone_recommend_a_tool_for_sorting_images/"}, {"autor": "black_apple07", "date": "2021-09-06 01:15:35", "content": "[D] understanding yolov3 during testing /!/ I understand that yolov3 trains with offsets. But I don\u2019t understand once you calculate the loss what is updated? \n\nThe reason I ask is when you test an -----> image !!! , without ground truth boxes. How does yolov3 actually detect the object in the image. During training we go off the ground truth but I don\u2019t understand how it works during testing.", "link": "https://www.reddit.com/r/MachineLearning/comments/pipfo0/d_understanding_yolov3_during_testing/"}, {"autor": "xtorch501", "date": "2021-09-13 17:00:25", "content": "[P] New -----> image !!!  augmentation library for TF + TPU /!/ Targetran is a new light-weight image augmentation library, to be used for object detection or image classification model training.\n\nWhile there are other powerful augmentation tools available, many of those operate only on NumPy arrays and do not work well with the TPU when accessing from Google Colab or Kaggle Notebooks. This is a known challenge addressed by some Kagglers.\n\nTargetran offers transformation tools implemented by pure TensorFlow ops, and hence they work smoothly with the TPU via TensorFlow Dataset.\n\nPlease take a look if you are facing a similar challenge:\n\n[https://github.com/bhky/targetran](https://github.com/bhky/targetran)", "link": "https://www.reddit.com/r/MachineLearning/comments/pnjk9p/p_new_image_augmentation_library_for_tf_tpu/"}, {"autor": "xtorch501", "date": "2021-09-13 16:47:00", "content": "New -----> image !!!  augmentation library for TF Dataset + TPU /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pnja5n/new_image_augmentation_library_for_tf_dataset_tpu/"}, {"autor": "scooootr", "date": "2021-09-13 03:58:43", "content": "[D] Open-Source -----> image !!!  databases? /!/ Hello everyone. I'm a senior computer science student and for my final comp project I'm interested in the topic of racial bias in facial recognition. My goal is to be try implementing some ML models in hopes of lowering thee false positivity rate for darker skinned people. I was wondering if anyone could point me to any resources that could help me find an image database I could possibly use. Also, feel free to message me to anything else you think I may find helpful. Some resources I've looked into are Gender Shades (PPB), and Fair Face. Sorry if this isn't the right place to ask.", "link": "https://www.reddit.com/r/MachineLearning/comments/pn7x8j/d_opensource_image_databases/"}, {"autor": "Tuwill", "date": "2021-09-13 01:15:25", "content": "Is there a Nude ai detector on -----> image !!! ? [D] /!/  Guys, i need one thing. Is there free nude detector on photo via ai? In example i have tons of photos in folder and i need to find photos which are only nude. I found some services, but they are not free sadly", "link": "https://www.reddit.com/r/MachineLearning/comments/pn5dd6/is_there_a_nude_ai_detector_on_image_d/"}, {"autor": "[deleted]", "date": "2021-09-13 01:13:06", "content": "Is there a N*de ai detector on -----> image !!! ?", "link": "https://www.reddit.com/r/MachineLearning/comments/pn5c0t/is_there_a_nde_ai_detector_on_image/"}, {"autor": "Tuwill", "date": "2021-09-13 01:12:27", "content": "Is there a Nude ai detector on -----> image !!! ? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pn5bne/is_there_a_nude_ai_detector_on_image/"}, {"autor": "Wiskkey", "date": "2021-09-12 17:19:29", "content": "[P] LAION-400M: open-source dataset of 400 million -----> image !!! -text pairs. This dataset is filtered by OpenAI's CLIP neural network. Also there is a web page that allows searching this dataset by text or image using OpenAI's CLIP neural network. /!/ [LAION-400-Million Open Dataset](https://laion.ai/laion-400-open-dataset/).\n\n[Site for searching dataset using CLIP](https://rom1504.github.io/clip-retrieval/).\n\nBackground info: [CLIP](https://openai.com/blog/clip/).", "link": "https://www.reddit.com/r/MachineLearning/comments/pmwvw9/p_laion400m_opensource_dataset_of_400_million/"}, {"autor": "NADES_L", "date": "2021-09-12 15:56:17", "content": "[D] /!/ hi, guys . I need information. anyone know a software AI or programmed model AI to interfere to other softwares ,for example Photoshop,illustrator something like these programs. i mean i give prompt and it is going to do the work and it will execute (reveal) product . Or i show a product(-----> picture !!! ) to AI and it will execute that product (-----> picture !!! ) on other software ( that interfered before with itself) and tried to prepare same thing with different style ( bc of the prompts i gave) . Thanks in advance", "link": "https://www.reddit.com/r/MachineLearning/comments/pmva3a/d/"}, {"autor": "techsucker", "date": "2021-09-12 06:02:01", "content": "[R] AI Researchers From Amazon, NEC, Stanford Unveil The First Deep Videos Text-Replacement Method, \u2018STRIVE\u2019 /!/ A Team of researchers from NEC Laboratories, Palo Alto Research Center, Amazon, PARC and Stanford University are working together to solve the problem of realistically altering scene text in videos. Their main application behind this research is to create personalized content for marketing and promotional purposes. For example, replace a word on a store sign with a personalized name or message, as shown in the -----> picture !!!  below.\n\nTechnically, several attempts have been made to automate text replacement in still images based on principles of deep style transfer. The research group is including this progress and their research to tackle the problem of text replacement in videos. Videotext replacement is not an easy task. It must meet the challenges faced in still images while also accounting for time and effects such as lighting changes, blur caused by camera motion or object movement.\n\nOne approach to solve video-test replacement could be to train an image-based text style transfer module on individual frames while incorporating temporal consistency constraints in the network loss. But with this approach, the network performing text style transfer will be additionally burdened with handling geometric and motion-induced effects encountered in the video.\n\nQuick Read: [https://www.marktechpost.com/2021/09/11/ai-researchers-from-amazon-nec-stanford-unveil-the-first-deep-videos-text-replacement-method-strive/](https://www.marktechpost.com/2021/09/11/ai-researchers-from-amazon-nec-stanford-unveil-the-first-deep-videos-text-replacement-method-strive/) \n\nPaper: [https://arxiv.org/pdf/2109.02762.pdf](https://arxiv.org/pdf/2109.02762.pdf) \n\nGithub: [https://striveiccv2021.github.io/STRIVE-ICCV2021/](https://striveiccv2021.github.io/STRIVE-ICCV2021/) \n\nDataset: [https://github.com/striveiccv2021/STRIVE-ICCV2021](https://github.com/striveiccv2021/STRIVE-ICCV2021) \n\n&amp;#x200B;\n\n*Processing video p7ak83a1j0n71...*", "link": "https://www.reddit.com/r/MachineLearning/comments/pmn8nq/r_ai_researchers_from_amazon_nec_stanford_unveil/"}, {"autor": "AICoderGamer", "date": "2021-09-12 01:57:12", "content": "[D] What happens once your -----> camera !!! -ready paper is submitted? /!/ I am new to submitting papers to ML conferences, and I have a paper accepted to Findings in EMNLP. I have submitted the camera-ready paper already.\n\nAt this point, where does it get uploaded? Where can we find it as well as other papers accepted to the conference? I am completely lost as to what happens now.\n\nThanks in advance!", "link": "https://www.reddit.com/r/MachineLearning/comments/pmjxsp/d_what_happens_once_your_cameraready_paper_is/"}, {"autor": "KirillTheMunchKing", "date": "2021-06-22 16:13:19", "content": "[D] 5 minute paper digest: GANs N\u2019 Roses: Stable, Controllable, Diverse -----> Image !!!  to -----> Image !!!  Translation (works for videos too!) by Min Jin Chong et al. /!/ [Your dream Anime waifu!](https://i.redd.it/mn3ty4acdu671.gif)\n\nDid you ever want to see what you look like as an anime waifu? Thanks to the authors of GANs N' Roses you can animefy (pretty sure this isn't a real word) selfies and even videos with a multitude of unique styles. The authors from the University of Illinois propose a new generator architecture that combines a content code computed from a face image and a randomly chosen style to produce consistent, diverse and controllable anime faces with attributes matching the content image.\n\nRead the [full paper digest](https://t.me/casual_gan/53) (reading time \\~5 minutes) to learn about the encoder-decoder architecture of the authors' content-style image generation method, the tricks for ensuring style diversity, and the losses required for high fidelity anime image synthesis.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://t.me/casual_gan)!\n\n[GANs N' Roses](https://preview.redd.it/yy1w8k42du671.png?width=1877&amp;format=png&amp;auto=webp&amp;s=f2607d49f1b6aea21edec9f15870ae01aad5f4a9)\n\n\\[[Full Explanation Post](https://t.me/casual_gan/53)\\] \\[[Arxiv](https://arxiv.org/pdf/2106.06561v1.pdf)\\] \\[[Code](https://github.com/mchong6/GANsNRoses)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n&gt;[CIPS](https://t.me/casual_gan/51)  \n[SimSwap](https://t.me/casual_gan/52)  \n[Decision Transformer](https://t.me/casual_gan/50)", "link": "https://www.reddit.com/r/MachineLearning/comments/o5qrub/d_5_minute_paper_digest_gans_n_roses_stable/"}, {"autor": "grid_world", "date": "2021-06-22 11:58:02", "content": "[D] Similar Image Retrieval /!/ I am trying to build a similar -----> image !!!  retrieval system where given an -----> image !!! , the system is able to show top 'k' most similar -----> image !!! s to it. For this particular example, I am using the [DeepFashion](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) dataset where given an image containing say a shirt, you show top 5 clothes most similar to a shirt. A subset of this has 289,222 diverse clothes images in it. Each image is of shape: (300, 300, 3).\n\nThe approach I have includes:\n\n1. Train an autoencoder\n2. Feed each image in the dataset through the encoder to get a reduced n-dimensional latent space representation. For example, it can be 100-d latent space representation\n3. Create a table of shape m x (n + 2) where 'm' is the number of images and each image is compressed to n-dimensions. One of the column is the image name and the other column is a path to where the image is stored on your local system\n4. Given a new image, you feed it through the encoder to get the n-dimensional latent space representation\n5. Use something like cosine similarity, etc to compare the n-d latent space for new image with the table m x (n + 2) obtained in step 3 to find/retrieve top k closest clothes\n\nHow do I create the table mentioned in step 3?\n\nI am planning on using TensorFlow 2.5 with Python 3.8 and the code for getting an image generator is as follows:\n\n    image_generator = ImageDataGenerator(\n        rescale = 1./255, rotation_range = 135)\n    \n    train_data_gen = image_generator.flow_from_directory(\n        directory = train_dir, batch_size = batch_size,\n        shuffle = False, target_size = (IMG_HEIGHT, IMG_WIDTH),\n        class_mode = 'sparse'\n\nHow can get image name and path to image to create the m x (n + 2) table in step 3?\n\nAlso, is there any other better way that I am missing out on?\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/o5lehy/d_similar_image_retrieval/"}, {"autor": "CaptainI9C3G6", "date": "2021-06-22 10:50:57", "content": "[D] Object detection: Is it detrimental to retrain but only focusing on one object type? /!/ Hi,\n\nI'm currently working on an object detection model using gluoncv/autogluon. My model is at about 80% for some objects, but much worse for others.\n\nWhen I'm updating/retraining my model is it harmful to only train with certain -----> image !!!  types? So if I know I have a set of images which contain objects A and B, will only tagging and retraining with images that tag object B increase accuracy for B, and/or decrease accuracy for object A?\n\nThanks!\n\nP.S. if it helps here are some of the current model parameters:\n\n`{ 'amp': False,`\n\n`'base_network': 'resnet50_v1',`\n\n`'data_shape': 512,`\n\n`'filters': None,`\n\n`'nms_thresh': 0.45,`\n\n`'nms_topk': 400,`\n\n`'ratios': ( [1, 2, 0.5],`\n\n`[1, 2, 0.5, 3, 0.3333333333333333],`\n\n`[1, 2, 0.5, 3, 0.3333333333333333],`\n\n`[1, 2, 0.5, 3, 0.3333333333333333],`\n\n`[1, 2, 0.5],`\n\n`[1, 2, 0.5]),`\n\n`'sizes': (30, 60, 111, 162, 213, 264, 315),`\n\n`'steps': (8, 16, 32, 64, 100, 300),`\n\n`'syncbn': False,`\n\n`'transfer': 'ssd_512_resnet50_v1_coco'}`", "link": "https://www.reddit.com/r/MachineLearning/comments/o5k7v0/d_object_detection_is_it_detrimental_to_retrain/"}, {"autor": "hasunemiku2015", "date": "2021-06-22 02:40:54", "content": "[P] Looking for collaborators on a Minecraft plugin /!/ ~~I know no one will really see this post, and even so I don't really have high hopes on getting someone to help me TAT....~~\n\nI am working on a Minecraft plugin and would require a **modified** implementation of this:[Pix2Vox](https://gitlab.com/hzxie/Pix2Vox), To turn pictures/videos of a real life building into Minecraft builds.\n\nHowever, the input sizes and the output sizes are not really appropriate for practical purposes. Not let alone the actual accuracy of the models on houses and buildings. (I am new to machine learning , those guys that only knows Conv2D and batch norm 2D....)\n\n&amp;#x200B;\n\n[Input -----> image !!!  example \\(need resize\\)](https://preview.redd.it/m6fpej7gcq671.png?width=854&amp;format=png&amp;auto=webp&amp;s=1295361fefdde615eaaa2937c33b77bc0340d14b)\n\n&amp;#x200B;\n\nAs a result, I am finding someone who can:\n\n1. help me modify and find tune the models\n2. help me get data on various Minecraft houses and its photos for transfer learning\n\nPlease comment below or DM me if you are interested in helping out. You don't need to know anything about Minecraft, its totally fine if you are good at machine learning.\n\nNo github repo yet, I will create one soon if someone is helping (I am working on this on my own for a few months now, searching for model and such)", "link": "https://www.reddit.com/r/MachineLearning/comments/o5cbpn/p_looking_for_collaborators_on_a_minecraft_plugin/"}, {"autor": "KirillTheMunchKing", "date": "2021-06-29 13:39:55", "content": "[D] Paper digest: Alias-Free GAN\" by Tero Karras et al. explained in 10 minutes! /!/  \n\n![video](uc3kixmdk7871 \" Pay attention to the beard moving separately from the face on the left -----> image !!!  \")\n\nStyleGAN2 is king, except apparently it isn't. Tero Karras and his pals at NVIDIA developed a modification of StyleGAN2 that is just as good in terms of image quality, yet drastically improves the translational and rotational equivariance of images. In other words, the synthesis process no longer depends on absolute pixel coordinates, textures are not sticking to coordinates, instead moving together with the corresponding objects. This is a big deal since slight changes to the architecture solve fundamental problems with the generator's design making GANs better suited for video and animation.\n\nRead the [full paper digest](https://t.me/casual_gan/58) (reading time \\~10 minutes) to learn about the revamped design of the generator inspired by ideas from digital signal processing. For example, how images are treating as discrete sample grids that represent bandlimited functions on a continuous domain, and how continuous translational and rotational equivariance are enforced with specially designed alias-suppressing upsampling filters and nonlinearities.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://t.me/casual_gan)!\n\n![img](ujjv2pbck7871 \"Modified StyleGAN2 architecture for alias free synthesis\")\n\n\\[[Full Explanation Post](https://t.me/casual_gan/58)\\] \\[[Arxiv](https://arxiv.org/pdf/2101.04061.pdf)\\] \\[[Code](https://nvlabs.github.io/alias-free-gan/)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n&gt;\\[[CIPS](https://t.me/casual_gan/51)\\]  \n&gt;  \n&gt;\\[[GFPGAN](https://t.me/casual_gan/54)\\]  \n&gt;  \n&gt;\\[[GANs N' Roses](https://t.me/casual_gan/53)\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/oa8hm9/d_paper_digest_aliasfree_gan_by_tero_karras_et_al/"}, {"autor": "NowUnavailableName", "date": "2021-06-29 13:33:02", "content": "[Discussion] What is the latest in freely available text-to------> image !!!  generation? /!/  This list: [https://www.reddit.com/r/MachineLearning/comments/ldc6oc/p\\_list\\_of\\_sitesprogramsprojects\\_that\\_use\\_openais/](https://www.reddit.com/r/MachineLearning/comments/ldc6oc/p_list_of_sitesprogramsprojects_that_use_openais/)\n\nhas not been updated since April, it seems. What is the state-of-the-art in mimicking DALL-E?", "link": "https://www.reddit.com/r/MachineLearning/comments/oa8d9h/discussion_what_is_the_latest_in_freely_available/"}, {"autor": "NowUnavailableName", "date": "2021-06-29 13:31:54", "content": "What is the latest in freely available text-to------> image !!!  generation? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/oa8cj3/what_is_the_latest_in_freely_available/"}, {"autor": "gbbb1982", "date": "2021-06-29 12:27:39", "content": "[P] Still working on AI painting from source -----> image !!! . Source image is photo. Canvas texture added after painting finished in GIMP. /!/ &amp;#x200B;\n\nhttps://preview.redd.it/xp8dlnh877871.jpg?width=1166&amp;format=pjpg&amp;auto=webp&amp;s=c04d7bc13b80e8c88d3523774e3e8ead26c16210", "link": "https://www.reddit.com/r/MachineLearning/comments/oa79dv/p_still_working_on_ai_painting_from_source_image/"}, {"autor": "pen8cil", "date": "2021-06-29 10:56:31", "content": "@errorfixe #errorfix Let's do it together #phone problem #laptop problem #security -----> camera !!!  #network ,WiFi router #TV If you have any Issue, let's fix it together @errorfixe", "link": "https://www.reddit.com/r/MachineLearning/comments/oa5wym/errorfixe_errorfix_lets_do_it_together_phone/"}, {"autor": "FluidStack", "date": "2021-06-28 11:00:20", "content": "Train Your -----> Image !!!  Recognition AI With 5 Lines of Code /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/o9hyam/train_your_image_recognition_ai_with_5_lines_of/"}, {"autor": "jayalammar", "date": "2021-06-28 09:14:28", "content": "[D] Behavioral Testing of ML Models (Unit tests for machine learning) [Video] /!/ This video discusses the paper [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://www.aclweb.org/anthology/2020.acl-main.442/), winner of Best Paper at ACL 2020\n\nEvaluating ML models using a single metric (like accuracy or F1-score) produce a low-resolution -----> picture !!!  of model performance. Behavioral tests can give us a much higher resolution evaluation of a model's capabilities. By creating tests (which are small targeted test sets), we can better compare models or observe how model performance changes after re-training a model (or fine-tuning it). \n\n[https://youtu.be/Cse-3MM7mso](https://youtu.be/Cse-3MM7mso)\n\nHope you enjoy it", "link": "https://www.reddit.com/r/MachineLearning/comments/o9gkxk/d_behavioral_testing_of_ml_models_unit_tests_for/"}, {"autor": "techsucker", "date": "2021-06-28 07:14:42", "content": "[N] Facebook AI Releases An Image Similarity Data Set And Announces The Launch Of An Associated Competition With A $200,000 Prize Pool (Paper and Competition Details Included) /!/ Recently, Facebook released the [Image Similarity data set](https://arxiv.org/pdf/2106.09672.pdf) and announced an associated\u00a0[competition\u00a0](https://www.drivendata.org/competitions/79/competition-image-similarity-1-dev/)hosted by DrivenData with a whopping $200,000 prize pool. The competition began on June 19, 2021, and will conclude on October 28, 2021. The challenge is being supported by Pinterest, BBC, Getty Images, iStock, and Shutterstock.\n\nThe data set contains nearly 1 million reference -----> image !!! s and 50,000 query -----> image !!! s, some of which are manipulated versions of a reference -----> image !!! . Through this dataset and challenge, Facebook hopes to enable new implementations of ML-based systems that can be utilized to help predict the similarity of two pieces of visual content and aid the industry in the at-scale detection of manipulated images\n\nFull Story: [https://www.marktechpost.com/2021/06/28/facebook-ai-releases-an-image-similarity-data-set-and-announces-the-launch-of-an-associated-competition-with-a-200000-prize-pool/](https://www.marktechpost.com/2021/06/28/facebook-ai-releases-an-image-similarity-data-set-and-announces-the-launch-of-an-associated-competition-with-a-200000-prize-pool/) \n\nPaper: https://arxiv.org/abs/2106.09672\n\nCompetition: https://www.drivendata.org/competitions/79/competition-image-similarity-1-dev/", "link": "https://www.reddit.com/r/MachineLearning/comments/o9f4t9/n_facebook_ai_releases_an_image_similarity_data/"}, {"autor": "productceo", "date": "2021-02-03 17:48:55", "content": "[N] Microsoft Vision Model ResNet-50 combines web-scale data and multi-task learning to achieve state of the art /!/ I am excited to share that we are publicly releasing a new state-of-the-art pretrained ResNet-50 model, measured by the mean average score across seven popular computer vision benchmarks.\n\nI see no link posts are allowed, so I'm not 100% sure how to share this with you. If you are interested in learning more about the techniques we used or downloading the model, you can do so by visiting: https://www.microsoft.com/en-us/research/blog/microsoft-vision-model-resnet-50-combines-web-scale-data-and-multi-task-learning-to-achieve-state-of-the-art/\n\nWe will also host a webinar on February 25th to walk people through how to use the pretrained vision model to build an example -----> image !!!  classifier, then have a live QnA where we answer your questions. Join us if you can!", "link": "https://www.reddit.com/r/MachineLearning/comments/lbtdrd/n_microsoft_vision_model_resnet50_combines/"}, {"autor": "Ximea_MS", "date": "2021-02-02 17:09:54", "content": "[N] XIMEA announces new -----> camera !!!  models with Sony CMOS Pregius S sensors /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/lb0qvf/n_ximea_announces_new_camera_models_with_sony/"}, {"autor": "Ximea_MS", "date": "2021-02-02 17:08:24", "content": "XIMEA announces new -----> camera !!!  models with Sony CMOS Pregius S sensors /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/lb0pjw/ximea_announces_new_camera_models_with_sony_cmos/"}, {"autor": "agoramancy", "date": "2021-10-22 02:43:22", "content": "[D] Link grammars, symbolic representations, and about using similarities and substitutions algorithms. /!/ This hypothesis is highly related to the presentations given by Linas Vep\u0161tas first and then Anton Kolonin at SingularityNet AGI-21 Conference about, link grammars, symbolic representations, and about using similarities and substitutions algorithms.\n\nIn: [https://www.youtube.com/watch?v=Rcgydm9dlYg](https://www.youtube.com/watch?v=Rcgydm9dlYg)\n\nYou need to see the first two presentations to understand what I'm trying to comunicate here.\n\nI\u2019ll try to be very concise. In short my statement is that with similarities and substitutions you can have solutions of previously unsolved problems. In other words, generate new knowledge using previous related knowledge.\n\nI apologize in advance for my lack of proper technical terms. But, if the idea get through, that will be enough for me. I hope my explanation is understandable.\n\nThe following is a graph composed by sub-graphs, the nodes are linked by the equal \u2018=\u2019 relation, which goes in both directions in case there is no more links, by default is left to right. The nodes can be single values or sets.\n\nFor briefness and explain-ability I will use this notation instead of an actual graph (-----> image !!! ).\n\nAbout my notations:\u00a0\n\n* \\[v1, v2\\] is a set,\n* '=' is the relation,\n* Every line can be seen as a sub-graph.\n* I will use // to add a comment for explanation\n* A \u2018query\u2019 is a new node that has no similarity relation in the graph and therefore the algorithm needs to be used\n\n**The initial state of the graph and a case of a query and the result:**\n\n    Alice = programmer\n    Bob = engineer\n    Mary = designer\n    John = programmer\n    Rose = engineer\n    Joe = designer\n    [engineer, job] = [model, system]\n    [programmer, job] = [develop, system]\n    [designer, job] = [design, UI]\n    Alice = available\n    Bob = available\n    Mary = available\n    Alice = [available, programmer]\n    Bob = [available, engineer]\n    Mary = [available, designer]\n    // initially available, but later relations are updated to working\n    John = working \n    Rose = working\n    Joe = working\n    R1 = name\n    R2 = name\n    solution = [\n        [[available, engineer], [engineer, job]], \n        [[available, programmer], [programmer, job]], \n        [[available, designer], [designer, job]]\n    ]\n    [new, request] = [unsolved, request]\n    [request, name] = request\n    [solution, name] = solution\n    [resolve, request] = work\n    work = [solution, for, request]\n    // if we have a [unsolved, request] we want to [resolve, request]\n    // this is a simplification. It could have more meaning if we add more \n    // details to this relation\n    [unsolved, request] = [resolve, request] \n    // the query is [Medical, report, system]\n    // the query is linked to be equal to a [new, request]\n    [new, request] = [Medical, report, system]\n    // generated [request, R1] to identify the query \n    [Medical, report, system] = [request, R1] \n    // given that [unsolved, request] = [new, request]\n    [request, R1] = [unsolved, request] \n    [request, R1] = [request, name] // given that R1 = name\n    [request, R1] = request\n    [request, R1] = [resolve, request] // given that: [unsolved,request]=[request,R1]\n    [resolve, [request, R1]] = work // given that: request = [request, R1]\n    // here [solution, R1] can be generated by a stored procedure for simplicity, \n    // and in this case is used to\n    // identify the node that will be the actual solution\n    // given that: work = [resolve, request], and work = [solution, for, request]\n    [solution, for, [request, R1]] = [solution, R1] \n    [solution, R1] = [solution, name] \n    [solution, R1] = solution\n    // final state, here the replacement for the node \u2018solution\u2019 is \n    // used to produce the final relation\n    [solution, R1] = [\n      [Rose, [model, system]], \n      [John, [develop, system]], \n      [Joe, [design, UI]]\n    ]\n    // For the next part, to do a new query I will do the following to avoid ambiguity\n    // this can be resolve in multiple ways, in this case I\u2019ll go with this\n    [request, R1] != [new, request] \n\n**At this point I will do a new query to illustrate how with substitutions we can generate a new knowledge, given the previous state of graph.**\n\n    [new, request] = [Hotel, management, system] // new query is [Hotel, management, system]\n    [Hotel, management, system] = [request, R2] // generate a identification node\n    [request, R2] = [request, name] // given that: R2 = name\n    [request, name] = [request, R1] // we have this relation, then\n    [request, R2] = [request, R1] // therefore\n    [request, R2] = [solution, R1] // is the current most similar, but\n    [request, R2] is not similar enough to [solution, R1] because:\n    // If we take into account the full sequence of substitutions we will notice \n    // that [Hotel,management,system] is not equal to [Medical,report,system]\n    // We can do better, if we use 'solution' instead of [solution, R1]:\n    \n    [solution, R1] = solution\n    [request, R2] = solution // given the previous relation\n    [request, R2] = [\n        [[available, engineer], [engineer, job]], \n        [[available, programmer], [programmer, job]], \n        [[available, designer], [designer, job]]\n    ]\n    [request, R2] = [\n        [Bob, [model, system]], \n        [Alice, [develop, system]], \n        [Mary, [design, UI]]\n    ]\n    // final output\n    [solution, R2] = [\n        [Bob, [model, system]], \n        [Alice, [develop, system]], \n        [Mary, [design, UI]]\n    ]\n\nThis example is not really that interesting, and looking at the result it seems pretty obvious. But the key here is that is generalizable, and is just substitutions.\n\nThis substitution mechanism can be applied to multiple cases to solve any kind of situations, given that it has enough previous knowledge. Is like applying a formula, step by step. Every step is guided by a previously known relation. This mechanism should be the algorithm applied directly to the graph, so that is the process with which the graph change from one state to the next state.", "link": "https://www.reddit.com/r/MachineLearning/comments/qd7khs/d_link_grammars_symbolic_representations_and/"}, {"autor": "techsucker", "date": "2021-10-22 02:31:04", "content": "[R] AI Researchers From Huawei and Shanghai Jiao Tong University Introduce \u2018CIPS-3D\u2019: A 3D-Aware Generator of GANs /!/ The StyleGAN architecture is a great way to generate high-quality images, but it lacks the ability to control -----> camera !!!  poses precisely. The recent NeRF based Generators have made progress towards creating real results so far as they can\u2019t produce photorealistic images.\n\nResearchers at Huawei and Shanghai Jiao Tong University have developed [CIPS-3D](https://github.com/PeterouZh/CIPS-3D), an approach that synthesizes each pixel value independently, just as its 2D version did.\n\nThe proposed generator consists of a shallow 3D NeRF network simplified to alleviate memory complexity and has the capacity for deep 2D INR (implicit neural representation) networks without any spatial convolution or up-sampling operations. The proposed generator\u2019s design is consistent with the well-known semantic hierarchical principle of GANs, where early layers ((i.e., the shallow NeRF network in the generator) determine pose and middle/high ((i.e., the INR network in the generator) control color scheme. The early NeRF network enables the research team to control camera pose explicitly easily.\n\n# [Quick 5 Min Read](https://www.marktechpost.com/2021/10/21/ai-researchers-from-huawei-and-shanghai-jiao-tong-university-introduce-cips-3d-a-3d-aware-generator-of-gans/) | [Paper](https://arxiv.org/pdf/2110.09788.pdf) | [Github](https://github.com/PeterouZh/CIPS-3D)\n\n&amp;#x200B;\n\n![video](i2a51lxtxwu71)", "link": "https://www.reddit.com/r/MachineLearning/comments/qd7cve/r_ai_researchers_from_huawei_and_shanghai_jiao/"}, {"autor": "KirillTheMunchKing", "date": "2021-10-21 15:21:39", "content": "[D] Sensorium Paper explained - Harnessing the Conditioning Sensorium for Improved Image Translation (5-minute summary by Casual GAN Papers) /!/ -----> Image !!!   to image translation appears more or less \u201csolved\u201d on the surface,  yet  there are still several important challenges to overcome. One such   challenge is the ambiguity in multi-modal, reference-guided   image-to-image domain translation. Believing that the choice of what to   preserve as the \u201ccontent\u201d of the input image, and \u201cstyle\u201d should be   transferred from the target image during domain translation depends   heavily on the task at hand, Cooper Nederhood and his colleagues propose   Sensorium, a new model that conditions its output on the information   from various off-the-shelf pretrained models depending on the task.   Sensorium enables higher quality domain translation for more complex  scenes.\n\nFresh out of the oven! Full summary: [https://www.casualganpapers.com/multimodal-style-conditioned-image-to-image-domain-translation/Sensorium-explained.html](https://www.casualganpapers.com/multimodal-style-conditioned-image-to-image-domain-translation/Sensorium-explained.html)\n\n[Sensorium](https://preview.redd.it/e4ywow6emtu71.png?width=1636&amp;format=png&amp;auto=webp&amp;s=164ce4a93e5128554bd54ce34089f0530bf5ecf8)\n\narxiv: [https://arxiv.org/abs/2110.06443](https://arxiv.org/abs/2110.06443)  \ncode: ?\n\nSubscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!", "link": "https://www.reddit.com/r/MachineLearning/comments/qcu7js/d_sensorium_paper_explained_harnessing_the/"}, {"autor": "StrugglingBScientist", "date": "2021-10-20 16:10:59", "content": "[P] Reconstructing a 3D -----> image !!!  with ImageJ/Mopholibj /!/ I have been using a deep learning algorithm to create segmentation masks of microscopy images. The goal is to create a 3D reconstruction. Some thing like [THIS](https://imagej.net/media/plugins/morpholibj-visualize-labels-in-3d-viewer.png). Can anyone direct me to a tutorial or better still, show me how to achieve this.", "link": "https://www.reddit.com/r/MachineLearning/comments/qc4esu/p_reconstructing_a_3d_image_with_imagejmopholibj/"}, {"autor": "StrugglingBScientist", "date": "2021-10-20 16:08:19", "content": "Reconstructing a 3D -----> image !!!  with ImageJ/Mopholibj /!/ I have been using a deep learning algorithm to create segmentation masks of microscopy images. The goal is to create a 3D reconstruction. Some thing like [THIS](https://imagej.net/media/plugins/morpholibj-visualize-labels-in-3d-viewer.png). Can anyone direct me to a tutorial or better still, show me how to achieve this.", "link": "https://www.reddit.com/r/MachineLearning/comments/qc4ck5/reconstructing_a_3d_image_with_imagejmopholibj/"}, {"autor": "DonDuarte", "date": "2021-10-27 18:07:00", "content": "[P] I'm trying to program a mean average precision calculator, help me out! /!/ Hello!\n\nI have a dataset of boats that I'm using to train a YOLO based detector through transfer learning and I want to compare the performance of different training exercises. For this I built a mean average precision calculator that takes the .txt files with the annotated ground truths and compares them to the .txt files generated by the trained detector module.\n\nI don't have True Negatives as those would be every spot in an -----> image !!!  where there isn't an object or prediction. True Positives are predictions where the IoU with the ground truth is inside the IoU threshold. \n\nBut then the negatives kinda confuse me. I added as False Negatives every object in the ground truth files that doesn't have a corresponding prediction with an IoU above the threshold. The False Positives are all the predictions left over (without a corresponding ground truth).\n\nSo precision would be: TRUE\\_POSITIVE/(TRUE\\_POSITIVE+FALSE\\_POSITIVE)\n\nAnd recall would be: TRUE\\_POSITIVE/(TRUE\\_POSITIVE+FALSE\\_NEGATIVE)\n\nHowever, with this, my precision and recall both have the same growth tendency. If I plot x/y with x being recall and y being precision, the dots will form a line with a positive slope when I know they should form a negative slope. Turns out that for example, when there is a predicted box of IoU = 0.2. If the threshold is 0.3 then it doesn't become a TRUE POSITIVE but it will generate a FALSE POSITIVE (prediction without corresponding ground truth) and a FALSE NEGATIVE (ground truth without a prediction). My throught process is wrong but I don't know how it is really done. What are FALSE NEGATIVES and FALSE POSITIVES?\n\n&amp;#x200B;\n\nHere's my code: [https://pastebin.com/Mu7DGrZf](https://pastebin.com/Mu7DGrZf)", "link": "https://www.reddit.com/r/MachineLearning/comments/qh2pfr/p_im_trying_to_program_a_mean_average_precision/"}, {"autor": "TheCockatoo", "date": "2021-10-27 17:44:00", "content": "[D] Time series generation using GANs - when to stop training? /!/ For -----> image !!!  generation, one may inspect the synthesized -----> image !!! s or rely on metrics like inception score and FID. But time series cannot be visually \"confirmed\" and I haven't been able to find peer-reviewed work for time-series-specific metrics.", "link": "https://www.reddit.com/r/MachineLearning/comments/qh26yp/d_time_series_generation_using_gans_when_to_stop/"}, {"autor": "KirillTheMunchKing", "date": "2021-10-27 15:48:07", "content": "[D] TargetCLIP explained - -----> Image !!! -Based CLIP-Guided Essence Transfer (5-minute summary by Casual GAN Papers) /!/ There  has recently been a lot of interest concerning a new generation of   style-transfer models. These work on a higher level of abstraction and   rather than focusing on transferring colors and textures from one image   to another, they combine the conceptual \u201cstyle\u201d of one image and the   objective \u201ccontent\u201d of another in an entirely new image altogether. A   recent paper by Hila Chefer and the team at Tel Aviv University does   just that! The authors propose TargetCLIP, a blending operator that   combines the powerful StyleGAN2 generator with a semantic network CLIP   to achieve a more natural blending than with each model separately. On a   practical level, this idea is implemented with two losses - one that   ensures the output image is similar to the input in the CLIP space, the   other - that the shifts in the CLIP space are linked to shifts in the   StyleGAN space.\n\nFull summary: [https://t.me/casual\\_gan/165](https://t.me/casual_gan/165)\n\n[TargetCLIP](https://preview.redd.it/6zvejtumk0w71.jpg?width=767&amp;format=pjpg&amp;auto=webp&amp;s=ae20726ef2037564924e634825e4c668bc9200d0)\n\narxiv: [https://arxiv.org/pdf/2110.09788.pdf](https://arxiv.org/pdf/2110.09788.pdf)  \ncode: [https://github.com/PeterouZh/CIPS-3D](https://github.com/PeterouZh/CIPS-3D)\n\nweb digest: [https://www.casualganpapers.com/clip\\_image\\_to\\_image\\_style\\_transfer\\_essence\\_transfer/TargetCLIP-explained.html](https://www.casualganpapers.com/clip_image_to_image_style_transfer_essence_transfer/TargetCLIP-explained.html)\n\nSubscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!", "link": "https://www.reddit.com/r/MachineLearning/comments/qgzoey/d_targetclip_explained_imagebased_clipguided/"}, {"autor": "KirillTheMunchKing", "date": "2021-10-27 15:46:17", "content": "TargetCLIP explained - -----> Image !!! -Based CLIP-Guided Essence Transfer (5-minute summary by Casual GAN Papers) /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/qgzmym/targetclip_explained_imagebased_clipguided/"}, {"autor": "l34df4rm3r", "date": "2021-10-27 12:19:33", "content": "[D] What are some SOTA -----> image !!!  classification architectures that run on edge-devices? /!/ I am trying to deploy a classification model on a Raspberry Pi 4 that is set up with PyTorch 1.9.0. The device is connected to a camera that periodically takes sky photographs and does a simple cloud classification task. I now have a small dataset with me and I am thinking of using a classification model that can run on the 8GB version.", "link": "https://www.reddit.com/r/MachineLearning/comments/qgvefy/d_what_are_some_sota_image_classification/"}, {"autor": "omayrakhtar", "date": "2021-10-27 11:49:59", "content": "[D] Semi-supervised learning with CycleGAN /!/ Hey!   \nI have been training CycleGAN with medical -----> image !!! s for unsupervised -----> image !!! -to------> image !!!  style transfer. The results are good but not there yet. So, I thought of doing a semi-supervised learning experiment. The images are largely unpaired, but I have got paired (registered) images as well. So, I designed an experiment in which I fed 10% paired &amp; 90% unpaired data for training. Then I tried different percentages of paired data but didn't see much difference in the end result. \n\nThe above observation led me to the conclusion that CycleGAN doesn't have a loss function that helps it directly benefit from the paired data, the cycle consistency loss helps but it's still indirect. There is no direct way to compare the generated images with the ground truth in the case of paired data.\n\nThen I had this crazy idea (disclaimer: it may sound stupid), what if we introduce another L1 loss term for the direct comparison of the generated image with the (in a sense) ground truth. I know for the most part of the training it will confuse the model because of the high percentage of unpaired data (and therefore, it will have a lot smaller weight compared to cycle consistency loss and identity loss). But, can this loss be considered a small noise in the case of unpaired data? And for paired data, it will actually help the model take direct advantage of data alignment.\n\nSo, I wanted to ask my fellow machine learning practitioners, especially the seasoned ones, is there any merit to this idea or is it utterly asinine?", "link": "https://www.reddit.com/r/MachineLearning/comments/qguvg2/d_semisupervised_learning_with_cyclegan/"}, {"autor": "SongYuki", "date": "2021-10-27 08:07:25", "content": "[D] How can I mark uncertain point according to marked point in -----> image !!!  /!/ There is a photo, i already have a part of annotation of image, the annotation is incomplete, many uncertain points in it. I want convert uncertain points to certain points according to existing knowledge. I find a method named CRF(Conditional Random Field), but it isn't use existing knowledge, do you have something else methods?", "link": "https://www.reddit.com/r/MachineLearning/comments/qgrm1g/d_how_can_i_mark_uncertain_point_according_to/"}, {"autor": "SongYuki", "date": "2021-10-27 06:38:00", "content": "How can I mark uncertain point according to marked point in -----> image !!!  /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/qgqd5c/how_can_i_mark_uncertain_point_according_to/"}, {"autor": "WorldPeace4", "date": "2021-10-07 17:06:37", "content": "[D] Is it possible to train an AI to recreate similar food posts? /!/  \n\nHi together,\n\nI found an instagram account that posts pictures of porridge every day. I am wondering if thats really a new -----> picture !!!  every day or if it uses some kind of AI to generate the image. There are more than 200 similar posts of porridge. Do you think it might be possible that these images are just artificially created? If so, can someone help me to recreate some of the posts?\n\nAs far as I have seen there are just 4 kinds of pots and the main backgroud is always the same.\n\nThank you \\^\\^\n\n&amp;#x200B;\n\nhttps://preview.redd.it/i6ioz5te82s71.png?width=731&amp;format=png&amp;auto=webp&amp;s=b766d753e33af6b059817f33cb6982d2ac52fc34", "link": "https://www.reddit.com/r/MachineLearning/comments/q3dmps/d_is_it_possible_to_train_an_ai_to_recreate/"}, {"autor": "KirillTheMunchKing", "date": "2021-10-07 12:44:14", "content": "[D] Paper explained - Unsupervised Discovery of Interpretable Directions in the GAN Latent Space (5-minute summary) /!/ https://i.redd.it/b7bb0ksmx0s71.gif\n\nGAN-based editing is great, we all know that! Do you know what isn\u2019t?  Figuring out what the heck you are supposed to do with a latent vector to edit the corresponding -----> image !!!  in a   coherent way. Turns out taking a  small step in a random direction will most likely change more than one aspect of the photo since latent spaces of most well-known generators are rather entangled, meaning that by adding a smile to the generated face you are likely to also unintentionally change the hair color, the eye shape or any number of other wacky things. In this paper by Andrey  Voynov and Artem Babenko from Yandex, a new unsupervised method is introduced that discovers meaningful disentangled editing directions for simple attributes such as gender, age, etc as well as less obvious ones such as background removal, rotation, and background blur.\n\nCheck out the [full paper summary](https://www.casualganpapers.com/unsupervised-discovery-editing-directions-gan-latent-space/Unsupervised-Directions-Discovery-explained.html) on Casual GAN Papers (Reading time \\~5 minutes).\n\n\\[[arxiv](https://arxiv.org/pdf/2002.03754.pdf)\\]\\[[github](https://github.com/anvoynov/GanLatentDiscovery)\\]\n\n&amp;#x200B;\n\nSubscribe to [my channel](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!", "link": "https://www.reddit.com/r/MachineLearning/comments/q38a35/d_paper_explained_unsupervised_discovery_of/"}, {"autor": "Pseudoabdul", "date": "2021-10-07 04:05:23", "content": "[P]Looking for datasets with both structured data and images for new model proof of concept. /!/ I recently worked on a project where I was predicting the likelihood of a car accident at intersections. The dataset contained information about that intersection (number of legs, speed limits, demographical data) as well as GPS coordinates. I fed the GPS coordinates into the Google Maps API to get images of each intersection. So the each row of the dataset contained a list of features, a response variable (number of crashes over 10 years) and the path of the associated -----> image !!! . \n\nTo process this I built a new model that is able to process the features and images separately, and then combine them to predict a the number of crashes. I wasn't able to get good results on that dataset given my time constraints and the project ended up just using XGBoost. After the project I made a generic implementation of my model for all to use. \n\nBefore I release it, I would like to validate it on some other datasets to prove it works. I've not been able to find any suitable datasets that have both images and structured features. If you can recommend some, I'll be able to test the model and then I can release it publicly. \n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/q30wh2/plooking_for_datasets_with_both_structured_data/"}, {"autor": "QryptoQuetzalcoatl", "date": "2021-10-07 00:19:09", "content": "[Discussion] Grad courses for computer vision /!/ Which graduate courses in -----> image !!!  processing and computer vision (CV) have you found most useful for applying CV in industry? For example, on problems like autonomous driving (e.g. object detection and planning) or gesture tracking (e.g. hand tracking).\n\nMore details: suppose a recent masters computer science graduate with good ML foundations wanted a somewhat structured learning curriculum -- which courses or materials might be beneficial?\n\nFor example, recommendations in addition to these\n\nUniv Maryland [CMSC426](https://cmsc426.github.io/)\n\nCarnegie Melon [16385](http://16385.courses.cs.cmu.edu/spring2021)\n\nStanford [CS231A](https://web.stanford.edu/class/cs231a/)", "link": "https://www.reddit.com/r/MachineLearning/comments/q2x4u7/discussion_grad_courses_for_computer_vision/"}, {"autor": "JohnClayborn", "date": "2021-10-06 23:08:12", "content": "Archive Text Reader [Project] /!/ Hi all, \n\nI'm new to Machine Learning and I'm wondering if someone may have already made a tool for this, or if they might be interested in helping to develop one. I run an online military museum. We have partnered with 2 other online museums and are poring over tens of thousands of pages of data.   \n\n\nI thought about how AI might be able to serve two purposes - to be able to read the page and create a transcription of the -----> image !!! , and to examine scanned pages where the text is blurry or faded and provide a translation. \n\nDoes anyone know if either of these might exist on GitHub or something already? Or would anyone be willing to help create such a tool? \n\nThanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/q2vtte/archive_text_reader_project/"}, {"autor": "Cizox", "date": "2021-10-06 19:25:15", "content": "[D] Understanding Design Principles in CNNs /!/ Specifically in the context of -----> image !!!  classification or object segmentation, how do DL researchers or data scientists know how to \u201cbuild\u201d CNN architectures? I\u2019m not talking about resource/budget constraints but rather the process on creating an architecture that works, rather than just throwing a bunch of residual blocks and hoping it works. What are some good practices that professionals use to test out various design ideas? How do you know what kernel size or stride to use? In what order to normalize/conv/relu, etc?\n\nI ask because in my deep learning class we were tasked with building a CNN for the PASCAL VOC dataset and it just felt like I was throwing stuff like Xception blocks and residual connections, scaling it by depth or width, and hoping it would perform well. I felt like there was a more holistic approach that professionals use to intelligently design an architecture. I guess I\u2019m asking how do you intelligently hypothesize that some design will work?", "link": "https://www.reddit.com/r/MachineLearning/comments/q2rjqb/d_understanding_design_principles_in_cnns/"}, {"autor": "KirillTheMunchKing", "date": "2021-10-13 20:03:06", "content": "[D] Paper explained - StyleNeRF: ICLR 2022 submission (5-minute summary) /!/ It\u2019s a NeRF, it\u2019s a GAN it\u2019s ~~Superman~~  StyleNeRF. But no for real, it happened, two of the biggest (probably)  breakthroughs of the last couple of years are joining forces. StyleGAN  is great at generating structured  2D images but it has zero knowledge  about the 3D world. NeRF, on the other hand, is great at understanding complex 3D scenes but struggles to generate view-consistent scenes when trained on unposed images.  StyleNeRF fuses the two into a  style-conditioned radiance field  generator with explicit -----> camera !!!  pose control. Seems like a perfect match!  Let\u2019s find out if it really lives up to the hype.\n\nFresh out of the oven! Full summary: [https://www.casualganpapers.com/unsupervised-discovery-nonlinear-latent-editing-directions-generator/StyleNeRF-explained.html](https://www.casualganpapers.com/unsupervised-discovery-nonlinear-latent-editing-directions-generator/StyleNeRF-explained.html)\n\n[Can't wait to see the gifs](https://preview.redd.it/ebrt7gu3x9t71.png?width=862&amp;format=png&amp;auto=webp&amp;s=51156e4d0831acd8fe6d402541b4a7ca8774f05d)\n\narxiv: [https://arxiv.org/pdf/2109.13357v1.pdf](https://arxiv.org/pdf/2109.13357v1.pdf)  \ncode: Coming soon", "link": "https://www.reddit.com/r/MachineLearning/comments/q7jf4g/d_paper_explained_stylenerf_iclr_2022_submission/"}, {"autor": "alexk_wong", "date": "2021-10-13 17:46:24", "content": "[R] ICCV2021 oral paper -- Unsupervised Depth Completion with Calibrated Backprojection Layers improves generalization across sensor platforms /!/ Our work \"Unsupervised Depth Completion with Calibrated Backprojection Layers\" has been accepted as an oral paper at ICCV 2021! We will be giving our talk during Session 10 (10/13 2-3 pm PST / 5-6 pm EST and 10/15 7-8 am PST / 10-11 am EST, https://www.eventscribe.net/2021/ICCV/fsPopup.asp?efp=WlJFS0tHTEMxNTgzMA%20&amp;PosterID=428697%20&amp;rnd=0.4100732&amp;mode=posterinfo). This is joint work with Stefano Soatto at the UCLA Vision Lab.\n\nIn a nutshell: we propose a method for point cloud densification (from -----> camera !!! , IMU, range sensor) that can generalize well across different sensor platforms. The figure in this link  illustrates our improvement over existing works: https://github.com/alexklwong/calibrated-backprojection-network/blob/master/figures/overview_teaser.gif\n\nThe slightly longer version: previous methods, when trained on one sensor platform, have problem generalizing to different ones when deployed to the wild. This is because they are overfitted to the sensors used to collect the training set. Our method takes image, sparse point cloud and camera calibration as input, which allows us to use a different calibration at test time. This significantly improves generalization to novel scenes captured by sensors different than those used during training. Amongst our innovations is a \"calibrated backprojection layer\" that imposes strong inductive bias on the network (as opposed trying to learn everything from the data). This design allows our method to achieve the state of the art on both indoor and outdoor scenarios while using a smaller model size and boasting a faster inference time.\n\nFor those interested, here are the links to\npaper: https://arxiv.org/pdf/2108.10531.pdf\ncode (pytorch): https://github.com/alexklwong/calibrated-backprojection-network", "link": "https://www.reddit.com/r/MachineLearning/comments/q7gisc/r_iccv2021_oral_paper_unsupervised_depth/"}, {"autor": "Simusid", "date": "2021-10-13 17:35:09", "content": "[discussion] OS and Infrastructure for MultiModel Production /!/ Single purpose ML would be something like a security -----> camera !!! .   If it detects a face or a car (or some condition) it sends some kind of alert.   That's very straightforward.  \n\nA self driving car (for example) probably has dozens of specialized models.   One to detect traffic signs, one to detect roadways, one to detect people, one for other vehicles and emergency vehicles, and I'm sure others.   \n\nWhat sort of architecture would you use today to orchestrate these models?   I've done some googling and it tells me that other people are using combinations of Celery, Rabbitmq, Redis, Elastic beanstalk, Flask, Containers, and kubernetes.    This seems kind of excessive.  \n\nThis is all in userspace too, at the application level.   Would there be any performance gains by integrating models deeper in the OS?   Could a model be effectively implemented as a device driver?", "link": "https://www.reddit.com/r/MachineLearning/comments/q7ga5b/discussion_os_and_infrastructure_for_multimodel/"}, {"autor": "roazzolini", "date": "2021-10-13 13:54:17", "content": "[Discussion] Course recommendation /!/ Can anyone please recommend me a good coursera machine learning/deep learning course? I consider myself to be a enthusiast on the area, with a few projects on 2d and 3d -----> image !!!  classification with keras, tensorflow and pytorch. I'm taking the IBM Al engineering rn, but I don't really think it is worth the price. If any of you could recommend me something, it would be very much appreciated.", "link": "https://www.reddit.com/r/MachineLearning/comments/q7bnfy/discussion_course_recommendation/"}, {"autor": "dofphoto", "date": "2021-10-13 13:53:58", "content": "[D] Improving -----> picture !!!  focus/noise in very constrained dataset /!/ Hi all,\n\nI'm wondering where we are with ease-of-implementation for an idea like this with very limited scope and very specific training data.\n\nData: Say I have a ton of pictures of a \\*specific\\* pet, or a person, etc Just one specific subject. \n\nProblem: I would like to \\*attempt\\* to make the pictures that are grainy/out-of-focus/artifact-y to look like the good quality ones. I am happy to select out the good quality ones and make a \"training set\". I can probably make a dataset on the order of 1000 easily. Maybe more if needed.\n\nStrategy: I am thinking of a couple of strategies:\n\n(1) Take the good quality ones (sharp, low noise, etc), degrade them in some arbitrary way, and train a simple architecture (unet, densenet, etc) to reconstruct to good quality. Maybe using a perceptual loss like LPIPS + MSE?\n\n(2) Use an adversary on the loss. But I'm not sure of the state of how challenging it is to make these work nowadays.\n\nAny opinions?", "link": "https://www.reddit.com/r/MachineLearning/comments/q7bn8k/d_improving_picture_focusnoise_in_very/"}, {"autor": "eyecloudai", "date": "2021-10-13 07:48:45", "content": "OpenNCC WE:A POE and WIFI enabled Edge AI IOT -----> camera !!!  to sense the world", "link": "https://www.reddit.com/r/MachineLearning/comments/q768y9/openncc_wea_poe_and_wifi_enabled_edge_ai_iot/"}, {"autor": "techsucker", "date": "2021-10-13 02:50:41", "content": "[N] NVIDIA AI Releases StyleGAN3: Alias-Free Generative Adversarial Networks (Paper and Code Included) /!/ The recent advances in the quality and resolution of Generative adversarial networks (GAN) have seen a rapid improvement. These techniques are used for various applications, including -----> image !!!  editing, domain translation, or video generation, to name just some examples. While several ways to control GANs\u2019 generative process have been found, there is still not much known about their synthesis abilities.\n\nIn 2019, Nvidia launched its second version of StyleGAN by fixing artifacts features and further improving generated images\u2019 quality. StyleGAN being the first of its type image generation method to generate very real images was open-sourced in February 2019. \n\n# [5 Min Read](https://www.marktechpost.com/2021/10/12/nvidia-ai-releases-stylegan3-alias-free-generative-adversarial-networks/) | [Paper](https://arxiv.org/pdf/2106.12423.pdf) | [Code](https://github.com/NVlabs/stylegan3) | [StyleGAN3 Project Page](https://nvlabs.github.io/stylegan3/)\n\n&amp;#x200B;\n\n![video](biv31146t4t71)", "link": "https://www.reddit.com/r/MachineLearning/comments/q71xbm/n_nvidia_ai_releases_stylegan3_aliasfree/"}, {"autor": "CatalyzeX_code_bot", "date": "2021-10-13 02:20:56", "content": "[Project] \ud83e\udd2f\ud83d\uddbc\ufe0fRemove any object or person in an -----> image !!!  easily using this AI model!", "link": "https://www.reddit.com/r/MachineLearning/comments/q71fhi/project_remove_any_object_or_person_in_an_image/"}, {"autor": "CatalyzeX_code_bot", "date": "2021-10-13 02:20:34", "content": "[P] \ud83e\udd2f\ud83d\uddbc\ufe0fRemove any object or person in an -----> image !!!  easily using this AI model!", "link": "https://www.reddit.com/r/MachineLearning/comments/q71f9c/p_remove_any_object_or_person_in_an_image_easily/"}, {"autor": "Open-Lobster", "date": "2021-02-02 00:37:48", "content": "[D] SOTA in -----> image !!!  embedding / feature extraction? /!/ I want to extract features from images by creating an embedding, to use in a perceptual loss function and for measuring similarity. My images are 4-channel (RGBA) and small (square 64px to 128px), so I can't use a pretrained classifier's features. Also, the images are unlabeled, so the learning will have to be unsupervised. What is the SOTA in this domain? I found [this paper](https://arxiv.org/abs/1904.03436) from 2019, and I know autoencoders of various kinds can also be used for this purpose, but I couldn't find a survey paper that compared these methods (probably not looking for the right terms).\n\nTLDR: best way to learn an embedding from unlabeled images?", "link": "https://www.reddit.com/r/MachineLearning/comments/laizho/d_sota_in_image_embedding_feature_extraction/"}, {"autor": "[deleted]", "date": "2021-02-02 00:36:31", "content": "SOTA in -----> image !!!  embedding / feature extraction?", "link": "https://www.reddit.com/r/MachineLearning/comments/laiyhn/sota_in_image_embedding_feature_extraction/"}, {"autor": "pramook", "date": "2021-02-01 22:20:48", "content": "[R][P] Talking Head Anime from a Single Image 2: More Expressive /!/ In 2019, I created a [network](https://www.reddit.com/r/MachineLearning/comments/e1k092/rp_talking_head_anime_from_a_single_image/) that can anime faces of anime characters. It takes a frontalized face -----> image !!!  and a 6-dimensional vector that specify the pose (i.e. the *pose vector*), and it outputs another -----> image !!!  of the character with the specified pose.\n\nOne of the main drawbacks of the network is its lack of expressiveness: not counting its ability to rotate faces, all it can do is closing eyes and mouths.\n\nFor the last one year or so, I have been working on a project to make the network more capable. The new version can manipulate 39 types of movements of facial features. (Most of these are equivalent to \"[blend shapes](https://en.wikipedia.org/wiki/Morph_target_animation)\" in computer graphics lingo.) It can move the eyebrows and the irises and make several different eye and mouth shapes. Characters can now make many different types of facial expressions.\n\n[Expressions generated by the network.](https://preview.redd.it/hgwelw9zsxe61.jpg?width=1055&amp;format=pjpg&amp;auto=webp&amp;s=9c6c2ca290e61c9c332c978aa18f53add9ffd7fa)\n\nWith the new network, I created three content creation tools:\n\n1. [A new GUI for detailed manipulation of anime facial expressions.](https://www.youtube.com/watch?v=535mjOjpy38)\n2. [A new system for real-time performance as anime characters.](https://www.youtube.com/watch?v=m13MLXNwdfY)\n3. A program that converts motions authored for 3D models so I can use it to drive characters images.\n\nWith the last tool, I created some music videos using downloadable 3D motions. Here's one:\n\nhttps://reddit.com/link/lafy8i/video/oaky6o6ivxe61/player\n\nPerhaps the more fun part of this project is to lip sync songs and then have some of my favorite characters sing them. Here's my rendition of the Internet famous \"Dame Da Ne\" meme, created with the help of the second tool.\n\nhttps://reddit.com/link/lafy8i/video/wtq834bbwxe61/player\n\nThis type of meme videos are typically created with the First Order Motion Model of Siarohin et al., but the model does not work well on anime characters and has a tendency to distort faces (to comedic effects). On the other hand, my network is specialized to anime characters, and you can see from the video that it preserves their beauty well.\n\nYou can read more about the project and see more contents at http://pkhungurn.github.io/talking-head-anime-2/", "link": "https://www.reddit.com/r/MachineLearning/comments/lafy8i/rp_talking_head_anime_from_a_single_image_2_more/"}, {"autor": "Combination-Fun", "date": "2021-02-01 17:46:32", "content": "[P] Vlog explaining Data Efficient Image Transformer clearly /!/ Here is a video explaining the approach, architecture and results of the paper, \"Training data-efficient -----> image !!!  transformers &amp; distillation through attention\".  Hope its useful: [https://youtu.be/HobIo2oT0xY](https://youtu.be/HobIo2oT0xY)", "link": "https://www.reddit.com/r/MachineLearning/comments/la95kb/p_vlog_explaining_data_efficient_image/"}, {"autor": "Farconion", "date": "2021-02-01 15:51:29", "content": "[R] exposing.ai - has your face been used in an -----> image !!!  dataset? /!/ not my project, but something very cool I came across. [exposing.ai](https://exposing.ai/) can check if you've ever had an image of yourself used in an image dataset if you've ever uploaded to Flickr", "link": "https://www.reddit.com/r/MachineLearning/comments/la68d7/r_exposingai_has_your_face_been_used_in_an_image/"}, {"autor": "Farconion", "date": "2021-02-01 15:50:14", "content": "exposing.ai - has your face been used in an -----> image !!!  dataset? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/la67ax/exposingai_has_your_face_been_used_in_an_image/"}, {"autor": "autisticmice", "date": "2021-02-01 11:14:25", "content": "[D] how widespread deep learning really is in industry? /!/ I know that there is a lot of deep learning (DL) research going on in academia and in large technology companies, but I'm curious about how much DL is actually used out there. I don't mean headlines about quirky uses of deep learning, but perhaps examples of companies with teams whose day to day job is to design, deploy or monitor DL models with a particular purpose; I know of perhaps a couple but want some help to see the bigger -----> picture !!! .", "link": "https://www.reddit.com/r/MachineLearning/comments/la0uux/d_how_widespread_deep_learning_really_is_in/"}, {"autor": "techsucker", "date": "2021-08-06 21:31:13", "content": "[R] Google AI Researchers Brings Improved Detection of Elusive Colorectal Polyps via Machine Learning /!/ With the use of computer-aided diagnostic systems, physicians can better diagnose and treat diseases in their patients. This has been shown to be especially useful for colorectal cancer (CRC), which is deadly and results in 900K deaths per year globally.\n\nCRC originates in small, pre-cancerous lesions called polyps that are found deep inside the colon. These polyps can be identified and removed by a doctor with great success before they become cancerous and cause death.\n\nAlthough there is no \u201cperfect\u201d way to detect colorectal polyps, colonoscopy has the advantage of being both a physical and visual examination. For this procedure to be successful in finding all benign or malignant tumors, though, it requires that medical personnel use their eyes as well as an instrument\u2013the endoscope. This can sometimes lead to what\u2019s known as incomplete diagnosis which means our doctor may not find anything if: 1) The tumor isn\u2019t illuminated by the light source on the endoscopic tube; 2) If you have one small but deep-seated tumor so it resides just outside of view from where they are looking through at with lumen -----> camera !!! .\n\nQuick Read: [https://www.marktechpost.com/2021/08/06/google-ai-researchers-brings-improved-detection-of-elusive-colorectal-polyps-via-machine-learning/](https://www.marktechpost.com/2021/08/06/google-ai-researchers-brings-improved-detection-of-elusive-colorectal-polyps-via-machine-learning/) \n\nPaper: [https://www.giejournal.org/article/S0016-5107(21)01468-1/fulltext](https://www.giejournal.org/article/S0016-5107(21)01468-1/fulltext) \n\nGoogle Blog: https://ai.googleblog.com/2021/08/improved-detection-of-elusive-polyps.html", "link": "https://www.reddit.com/r/MachineLearning/comments/ozg1xs/r_google_ai_researchers_brings_improved_detection/"}, {"autor": "ysfxali", "date": "2021-08-06 20:25:38", "content": "[D] Doubts regarding GPU to get for Deep Learning /!/  \n\nMy organization is trying to setup a deep learning workstation and we're currently being offered a workstation having Nvidia RTX Quadro 5000.\n\nOur main work currently revolves around training/testing deep learning based models like -----> image !!!  classification (transfer learning), Object Detection (yolov3-v5), NLP (LSTMs/Transformers), etc.\n\nSince I'm not very well versed with how well this kind of GPU performs on the given tasks, I would like some feedback for the same.\n\nAlso is RTX Quadro 5000 good enough or should we go for RTX Quadro A5000?", "link": "https://www.reddit.com/r/MachineLearning/comments/ozeswz/d_doubts_regarding_gpu_to_get_for_deep_learning/"}, {"autor": "tucktoo", "date": "2021-08-06 19:55:27", "content": "[D] Searching for a simple pattern in images /!/ Hi, I am very new at machine learning. I would like to use Tensorflow to find simple, specific pattern on images.\n\nImages are black and white (binary, only value 0 for black and 255 for white). Contains ONLY white circles on black background. I need to find on them the pattern consists of five white circles arranged according to predetermined proportions like below . \n\n&amp;#x200B;\n\n[Pattern to find](https://preview.redd.it/9q7jhgemlsf71.png?width=276&amp;format=png&amp;auto=webp&amp;s=e5c46749d1f5381648cb0825737bb98b5ac9e29f)\n\nOn images the pattern can be rotated to any angle and freely scaled. The circles can be of different sizes. Sometimes circles may be slightly distorted. In fact, each of circle can be treated as a single point x, y.   The -----> image !!!  may have a different number of circles (zero included). \n\n The size of the images may vary (usualy 3000x4000 px.)\n\n&amp;#x200B;\n\n[Here!](https://preview.redd.it/ttz3ho3klsf71.png?width=444&amp;format=png&amp;auto=webp&amp;s=b388fbfab824caf062422d86cabcf6113252c52c)\n\nMoving on to machine learning...\n\nEach of this image can be treated as set of a dozen / several dozen x, y points (circles). Or as a set of 3000x4000 =  12000000 diffrent pixels. I think that treating a photo as a set of circles coordinates will be the most optimal for machine learning (?).\n\nMy question is what should I feed to my neural network? Does the number of inputs be variable? (each photo can have a different number of circles, so a different number of coordinates).\n\nMy idea:\n\ninputs: all circles coordinates on image (but must be variable, it's possible?)\n\noutputs:  coordinates of pattern circles\n\nDoes it make seanse? Any better ideas?", "link": "https://www.reddit.com/r/MachineLearning/comments/oze7w0/d_searching_for_a_simple_pattern_in_images/"}, {"autor": "cereal_final", "date": "2021-02-15 21:32:09", "content": "[D] How to upscale -----> image !!! s for an -----> image !!!  dataset? /!/ I have a pretty solid dataset of around 5000 images, but they're all 64x64. Is there any way to quickly upscale them to 256x256 or 512x512?", "link": "https://www.reddit.com/r/MachineLearning/comments/lknq8r/d_how_to_upscale_images_for_an_image_dataset/"}, {"autor": "anotsohypocritesoul", "date": "2021-02-15 19:42:30", "content": "[D] Need help uploading multiple images from csv file to rekognition using PyCharm /!/ I'm using Amazon Rekognition to perform object detection in Images and I binded it to PyCharm using boto3. Right now I am able to upload a single -----> image !!! .\n\nWhat I want to do: Upload multiple images and store the output.\n\nCan someone please help?", "link": "https://www.reddit.com/r/MachineLearning/comments/lklbmu/d_need_help_uploading_multiple_images_from_csv/"}, {"autor": "fromnighttilldawn", "date": "2021-02-11 01:05:55", "content": "[D] Can someone explain the \"ray tracing\" -----> picture !!!  in the neural radiance field paper? /!/ This [picture](https://raw.githubusercontent.com/bmild/nerf/master/imgs/pipeline.jpg) is featured prominently in the neural radian field (nerf) paper. [https://www.matthewtancik.com/nerf](https://www.matthewtancik.com/nerf)\n\nHowever, and with no disrespect to the authors, this picture is confusing to a novice.\n\n* Consider image (a), you are shooting a ray into a 2D image, going through it, and then this generates a sequence of black dots through a 3D \"object\".\n* Afterward, you are taking one of these black dots (x,y,z,theta, phi) and then passing it through a neural network to get a prediction of the color and density (R,G,B,sigma) at that particular black dot's location. Image (b).\n\nBut this makes no sense to me because this \"object\" doesn't exist in the first place, it is literally what we are trying to produce. i.e., your target doesn't exist. So how do you train the network?\n\n* So what does it mean to take some (x,y,z, theta, phi) along the ray that passes through this object as shown in (a)? Is it actually taking a location on a side picture?\n* How would you know what (x,y,z) are (i.e., the length of this ray could exceed the size of the object, then you are taking a coordinate (x,y,z) that doesn't even exist on this object)? Watch: [https://www.youtube.com/watch?v=JuH79E8rdKc&amp;ab\\_channel=MatthewTancik](https://www.youtube.com/watch?v=JuH79E8rdKc&amp;ab_channel=MatthewTancik)\n* And after you passed it through a neural network, and then render it back into a pixel, which pixel are you comparing it to? =\n* And finally, does 360 rendering only work for \"symmetric\" objects?\n\nCan someone please help me understand this part? I don't know too much about computer graphics so there could be some trivial gaps.", "link": "https://www.reddit.com/r/MachineLearning/comments/lh939h/d_can_someone_explain_the_ray_tracing_picture_in/"}, {"autor": "[deleted]", "date": "2021-02-11 01:01:34", "content": "Can someone explain the \"ray tracing\" -----> picture !!!  in the neural radiance field paper?", "link": "https://www.reddit.com/r/MachineLearning/comments/lh8zzc/can_someone_explain_the_ray_tracing_picture_in/"}, {"autor": "ChunkyFunkyFloater", "date": "2021-02-11 00:24:40", "content": "[D] Techniques to reduce compressive convolutional AE latent space size /!/ I've been trying to find a deep and clear explanation on how to manipulate data inside the latent space of a compressive autoencoder to further reduce the bpp of an -----> image !!! , but can't find any. I don't know if this is the correct subreddit but i need help badly. \n\nI'm currently using pytorch and already experimented with it's quantize per tensor function with bad results. The model I'm using seems to be sensitive to minor changes in the decimal places.\n\nIf you know how to use quantization correctly or if there are other techniques apart from quantization I'm all ears.\nThanks in advance.", "link": "https://www.reddit.com/r/MachineLearning/comments/lh89ig/d_techniques_to_reduce_compressive_convolutional/"}, {"autor": "Lairv", "date": "2021-02-10 23:49:13", "content": "[D] Why did it took 3 years to use transformers in computer vision ? /!/ When reading [An -----> image !!!  is worth 16\\*16 words](https://arxiv.org/pdf/2010.11929.pdf), its seems like a quite straight-forward adaptation of the transformer architecture : the main thing they changed was breaking the -----> image !!!  into 16\\*16 patches so that it reduces the n\u00b2 cost of computing self-attention.\n\nI'm thus wondering why did it take 3 years to apply transformers to computer vision ?", "link": "https://www.reddit.com/r/MachineLearning/comments/lh7iwp/d_why_did_it_took_3_years_to_use_transformers_in/"}, {"autor": "delmalih", "date": "2021-02-10 14:35:02", "content": "[R] One-Shot Medical Image Segmentation using Deformable Registration Networks /!/ Hello guys! Check out this article I've just published explaining how deep deformable registration networks can help us perform one-shot medical -----> image !!!  segmentation!\n\nFeedback welcome!  \n[https://www.sicara.ai/blog/one-shot-medical-image-segmentation](https://www.sicara.ai/blog/one-shot-medical-image-segmentation)", "link": "https://www.reddit.com/r/MachineLearning/comments/lguvk5/r_oneshot_medical_image_segmentation_using/"}, {"autor": "thevatsalsaglani", "date": "2021-02-10 08:24:12", "content": "[P] How can the new OpenAI CLIP be used for semantic -----> image !!!  search? /!/ OpenAI recently published a multi-modal training approach to train a model to learn from image text pairs and we can use the same pre-trained model for semantic image search. I have described how's in this blog [https://medium.com/towards-artificial-intelligence/what-is-clip-contrastive-language-image-pre-training-and-how-it-can-be-used-for-semantic-image-b02ccf49414e](https://medium.com/towards-artificial-intelligence/what-is-clip-contrastive-language-image-pre-training-and-how-it-can-be-used-for-semantic-image-b02ccf49414e)\n\nYou can try the semantic search live at [https://share.streamlit.io/vatsalsaglani/clipsemanticimagesearch/streamlitClip1/streamlitapp.py](https://share.streamlit.io/vatsalsaglani/clipsemanticimagesearch/streamlitClip1/streamlitapp.py)\n\nIt looks something like this\n\n&amp;#x200B;\n\nhttps://i.redd.it/ryblbsrc1mg61.gif", "link": "https://www.reddit.com/r/MachineLearning/comments/lgpa4s/p_how_can_the_new_openai_clip_be_used_for/"}, {"autor": "the_ml_maniac", "date": "2021-02-08 19:35:11", "content": "[R] Natural Language Descriptions of Image Classes /!/ I'm currently working on a Vision - Language Research. I'm using ImageNet and MS-COCO datasets to perform my experiments. For MS-COCO we have ground truth captions and annotations available for each images but not for ImageNet. \n\nI want to grab the sentences / descriptions available online of the Image Classes in the ImageNet dataset (e.g: Tiger). What are the available methods to do so ?\n\nThe descriptions need not to be specific to Images, it'll be enough if it carries the details of the class label that the -----> image !!!  belongs to.\n\nThanks in Advance :)", "link": "https://www.reddit.com/r/MachineLearning/comments/lfjr3x/r_natural_language_descriptions_of_image_classes/"}, {"autor": "Qualitastech", "date": "2021-02-08 14:57:14", "content": "Confused about using a smart -----> camera !!!  in your next solution? Here is what this technology could do for you", "link": "https://www.reddit.com/r/MachineLearning/comments/lfdhed/confused_about_using_a_smart_camera_in_your_next/"}, {"autor": "NoSugar80", "date": "2021-02-08 02:58:50", "content": "How can the receptive field size is bigger than input -----> image !!!  size? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/lf2cgb/how_can_the_receptive_field_size_is_bigger_than/"}, {"autor": "cohenori", "date": "2021-08-14 07:15:29", "content": "[P] MLOps Monitoring-Companies Market-Comparison Review /!/ **TL;DR \u2013**  I created an open-source exhaustive comparison of companies in the world of MLOps monitoring. I gathered the data by researching blog posts, documentation, product demos, and marketing materials. To see it, please go to the [stateofmlops.com](https://stateofmlops.com/) website or play with the data on [AirTable](https://airtable.com/shr4rfiuOIVjMhvhL/tblP1hFSp3Uez50bn/viwguZIVuqkud4HCY?backgroundColor=blue&amp;blocks=bip1LnQeUr0AU3mEo).  \n   \nYou can see segmentation by:\n\n* personas \n* supported data type (tabular, -----> image !!! , audio, etc) \n* product features (data-integrity, data-quality, health, drift, bias &amp; fairness, XAI, etc)\n* product focus (data-centric or pipeline-centric)\n* total funding to date (Aug 2021)\n* company type (startup, open-source, corporate)  \n* other features..\n\n***Disclaimer:*** This is my open-source project, and for those who are curious about a more in-depth analysis please see the accompanying [Medium post](https://towardsdatascience.com/mlops-monitoring-market-review-66904f0863bb).\n\nAny feedback is positively welcome, feel free to contact me on [LinkedIn](https://www.linkedin.com/in/cohenori/).\n\nhttps://preview.redd.it/0pemv201h9h71.png?width=1708&amp;format=png&amp;auto=webp&amp;s=9503c5688afe37eb7413c3568b59fb8eef94960c", "link": "https://www.reddit.com/r/MachineLearning/comments/p43g70/p_mlops_monitoringcompanies_marketcomparison/"}, {"autor": "AlgoSci", "date": "2021-08-14 00:51:03", "content": "[P] Short doc on computational complexity /!/ The Simons Institute for the Theory of Computing at UC Berkeley just released the second short -----> film !!!  in their documentary web series,\u00a0*Theory Shorts*.\n\n[Until the Sun Engulfs the Earth: Lower Bounds in Computational Complexity](https://www.youtube.com/watch?v=-DWmBhMgWrI)\u00a0explores how we know that a problem is impossible to solve.\n\nFeaturing Paul Beame (University of Washington), Faith Ellen (University of Toronto), Jelani Nelson (UC Berkeley), Manuel Sabin (UC Berkeley), and Madhu Sudan (Harvard University).\n\nCurious what folks think about it!", "link": "https://www.reddit.com/r/MachineLearning/comments/p3y545/p_short_doc_on_computational_complexity/"}, {"autor": "soulslicer0", "date": "2021-08-13 23:18:31", "content": "[D] Performing Instance and Semantic Segmentation with just 2 channels (edges + labels)? /!/ Most of the networks out there that do both instance and semantic segmentation either have complex panoptic segmentation like networks where instances and channels are stored in different channels, or they adopt the mask-RCNN style where we regress on bboxes and run seperate networks to get the segmentation mask.\n\nI want to know, why can't this problem be broken down into just 2 channels. One channel stores the edges in the -----> image !!!  of object boundaries so 0 to 1 as a probability for each pixel, and another channel stores the actual label for each pixel. And then, some offline algorithm with some region growing or whatever could break these up from there.\n\nI can't seem to find any paper that approaches this problem that way. Is there?", "link": "https://www.reddit.com/r/MachineLearning/comments/p3wmog/d_performing_instance_and_semantic_segmentation/"}, {"autor": "mtgawesome", "date": "2021-08-13 15:33:17", "content": "[D] What's the best method to generate synthetic data for an -----> image !!!  with text? Small dataset /!/ Currently I only have one item in the dataset and I want to augment it to get more items but I am trying to avoid bias. Is the best option photoshopping a bunch of copies or is their a faster solution?", "link": "https://www.reddit.com/r/MachineLearning/comments/p3nbmr/d_whats_the_best_method_to_generate_synthetic/"}, {"autor": "Childishpapino", "date": "2021-05-15 18:56:27", "content": "[D]: how to combine auxiliary data and -----> image !!!  data in deep cnn /!/  I have an image regression task. For example, to predict a person age given an image of his face. But I have some more auxiliary information for each image that is known to be correlated with the prediction - For example, weight, gender, race, etc.\nHow can I combine the auxiliary data with the image data ?\nSo naturally I can quantize the auxiliary data into a finite number of options and train a neural network for each dataset. But this is a poor option for many reasons, a main one is because I am using only a portion of the data for each network.\nCan you refer me to a paper or architecture that use auxiliary information with a CNN ?", "link": "https://www.reddit.com/r/MachineLearning/comments/nd5uyj/d_how_to_combine_auxiliary_data_and_image_data_in/"}, {"autor": "ykilcher", "date": "2021-05-15 13:14:48", "content": "[D] Paper Explained - DDPMs: Diffusion Models Beat GANs on Image Synthesis (Full Video Analysis) /!/ [https://youtu.be/W-O7AZNzbzQ](https://youtu.be/W-O7AZNzbzQ)\n\nGANs have dominated the -----> image !!!  generation space for the majority of the last decade. This paper shows for the first time, how a non-GAN model, a DDPM, can be improved to overtake GANs at standard evaluation metrics for image generation. The produced samples look amazing and other than GANs, the new model has a formal probabilistic foundation. Is there a future for GANs or are Diffusion Models going to overtake them for good?\n\n&amp;#x200B;\n\nOUTLINE:\n\n0:00 - Intro &amp; Overview\n\n4:10 - Denoising Diffusion Probabilistic Models\n\n11:30 - Formal derivation of the training loss\n\n23:00 - Training in practice\n\n27:55 - Learning the covariance\n\n31:25 - Improving the noise schedule\n\n33:35 - Reducing the loss gradient noise\n\n40:35 - Classifier guidance\n\n52:50 - Experimental Results\n\n&amp;#x200B;\n\nPaper (this): [https://arxiv.org/abs/2105.05233](https://arxiv.org/abs/2105.05233)\n\nPaper (previous): [https://arxiv.org/abs/2102.09672](https://arxiv.org/abs/2102.09672)\n\nCode: [https://github.com/openai/guided-diffusion](https://github.com/openai/guided-diffusion)", "link": "https://www.reddit.com/r/MachineLearning/comments/ncymw1/d_paper_explained_ddpms_diffusion_models_beat/"}, {"autor": "iPad-Kavin", "date": "2021-05-15 11:09:56", "content": "[D] Why is using TPUs with tensorflow so hard? /!/ I have been trying to convert my tensorflow model to one that works with TPU but can't seem to do so. I have been trying to do so over a month now. \n\nI am getting this error: InvalidArgumentError: Unable to parse tensor proto \n\nSo I  used a takedataset of 10, reduced the batch size to 16 and reduced the -----> image !!!  dimensions. Still the same error.\n\nWhat is wrong? Are TPUs worth the hassle of conversion in terms of speed?", "link": "https://www.reddit.com/r/MachineLearning/comments/ncwgnq/d_why_is_using_tpus_with_tensorflow_so_hard/"}, {"autor": "DrAsgardian", "date": "2021-05-15 10:21:42", "content": "[D] Detecting face with art makeup /!/ Here I wanna compare persons in two images same or not. In one -----> Image !!!  person will have makeup and in other person will not have makeup.\n\nMake up will be something like this\n\nhttps://preview.redd.it/chs1s5t0g9z61.jpg?width=266&amp;format=pjpg&amp;auto=webp&amp;s=fe72a9d7391ebe5da9d497854cd16feda0c83a45\n\n \n\nWhat sort of approach I can do?\n\nI tried with ratio based approach (distance(leye,reye)/distance(rmouth,lmouth)) that didn't work well.", "link": "https://www.reddit.com/r/MachineLearning/comments/ncvq85/d_detecting_face_with_art_makeup/"}, {"autor": "sarmientoj24", "date": "2021-05-15 04:57:43", "content": "[R] How do you use RL on Multi-modal Embeddings for Image2Text Retrieval? /!/ For my mini-project, combining Computer Vision + NLP + RL interests me. I've come across this paper -- Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images where the main task is trainingg a neural network to learn a joint embedding of recipes and -----> image !!! s that yields impressive results on an -----> image !!! -recipe retrieval task.\u00a0\n\nIt also has an image to recipe retrieval where theye evaluate all the recipe representations for im2recipe retrieval. Given a food image, the task is to retrieve its recipe from a collection of test recipes.\n\nIt also includes some embedding properties like word2vec.\n\nThey basically use CNN for encoding the image and RNNs to encode both the recipe and the instructions and then have a joint embedding for the recipe and instructions. Their embedding is created using a cosine Similarity loss and one semantic regularization loss.\n\nFor introduction of RL to image captioning, I've seen the they incorporated RL by having their Deep Q Network to learn through action - the next word of the imagecaption, state (the current words on the caption on time t) and reward being some score.\n\nI was wondering how do I introduce Deep RL for this scenario on embeddings. Hopefully you can help guide me.", "link": "https://www.reddit.com/r/MachineLearning/comments/ncr58p/r_how_do_you_use_rl_on_multimodal_embeddings_for/"}, {"autor": "KirillTheMunchKing", "date": "2021-05-14 17:18:42", "content": "[D] How to improve -----> image !!!  inversion with Gaussianized latent spaces explained /!/ # [Improving Inversion and Generation Diversity in StyleGAN using a Gaussianized Latent Space](https://t.me/casual_gan/38)\n\n\ud83c\udfaf At a glance:\n\n&gt;In this paper about improving latent space inversion for a pretrained StyleGAN2 generator the authors propose to model the output of the mapping network as a Gaussian, which can be expressed as a mean and a covariance matrix. This prior is used to regularize images that are projected into latent space  via optimization, which makes the inverted images lie in well conditioned regions of the generator's latent space, and allows for smoother interpolations and better editing.\n\n[Samples from the model](https://preview.redd.it/1058coadc4z61.png?width=1219&amp;format=png&amp;auto=webp&amp;s=42af3487e9d073a608b54ad12d067396e5931b5d)\n\n\\[[5 minute summary of main ideas](https://t.me/casual_gan/38)\\] \\[[arxiv](https://arxiv.org/pdf/2009.06529.pdf)\\]  \n\n\nP.S. Thanks for reading!  \nIf you found this useful check out other popular ML papers explained on [my channel](https://t.me/casual_gan)!  \n\n\n**Links to other recent papers explained:**\n\n* [VQ-VAE2](https://t.me/casual_gan/30)\n* [StyleGAN2-ada](https://t.me/casual_gan/28)\n* [MLP-Mixer](https://t.me/casual_gan/35)", "link": "https://www.reddit.com/r/MachineLearning/comments/ncduqj/d_how_to_improve_image_inversion_with/"}, {"autor": "Put6074", "date": "2021-05-09 04:09:24", "content": "[P] [D] Collaboration /!/ [P] [D] Hello people, I  have Bachelor's degree in electronics &amp; communication engineering,  working in an IT company with no connection to my studies. To switch career and to learn, I want to build something related to machine learning.\nPlan:\nRobot (military) with computer vision to process -----> Image !!!  and check if the object in image is an IDEs, bombs etc. I have very high level knowledge on machine learning and completely lost among the buzz words, research papers, courses in the internet surrounding machine learning, computer vision, image processing.\n\nCan someone please help with the approach and if possible collborate on the project ?\nThanks in advance.", "link": "https://www.reddit.com/r/MachineLearning/comments/n863t5/p_d_collaboration/"}, {"autor": "Fancy-Stress-806", "date": "2021-05-08 17:18:16", "content": "[D] Need help checking if my code is correct /!/  \n\nI was wondering if I could get a check on my code to see if it is correct (or at least close to). Basically, I am trying to replicate the Neural Network from Figure 1 from this paper: [http://cs231n.stanford.edu/reports/2017/pdfs/602.pdf](http://cs231n.stanford.edu/reports/2017/pdfs/602.pdf). However, instead of past video game frames, I am using rainfall radar images and instead of actions of the player, I am using climate data (e.g. temperature, humidity, o3 concentration, etc.). These inputs combined will generate a rainfall output -----> image !!!  at a future timestep. I just don't need the reward branches after the element wise multiplication in the middle. Therefore, I am attempting to replicate and adapt the entire network except the reward branches.\n\nFor additional context, my rainfall radar inputs are (2258, 110, 110, 12) where the 1st, 2nd, 3rd and 4th dimensions correspond to the number of samples, height of the image, width of the image, depth dimension (number of timesteps used for inputs) respectively. The climate data is a (2258, 6) table/array where the 6 columns correspond to the following features: Elevation above mean sea level, o3 concentration, temperature, dewpoint, humidity, wind speeds. I've coded out the network in a way I think is right (hopefully) and it runs but I'm not sure if it is correct so was wondering if I could get a check here.\n\n    # RAINFALL BRANCH\n    def rainCNN(height, width, depth):\n        \n        rain_inputs = Input(shape = (height, width, depth))\n    \n        # LEFT HAND SIDE\n        conv1 = Conv2D(64, kernel_size = 8, strides = 2, padding = \"valid\")(rain_inputs)\n        conv1 = Activation(\"relu\")(conv1)\n        \n        conv2 = Conv2D(128, kernel_size = 6, strides = 2, padding = \"valid\")(conv1)\n        conv2 = Activation(\"relu\")(conv2)\n    \n        conv3 = Conv2D(128, kernel_size = 6, strides = 2, padding = \"valid\")(conv2)\n        conv3 = Activation(\"relu\")(conv3)\n    \n        conv4 = Conv2D(128, kernel_size = 6, strides = 2, padding = \"valid\")(conv3)\n        conv4 = Activation(\"relu\")(conv4)\n    \n        fc1 = Flatten()(conv4)\n        fc1 = Dense(1024, activation = \"relu\")(fc1)\n    \n        fc2 = Dense(2048, activation = \"linear\")(fc1)\n        \n        model = Model(inputs = rain_inputs, outputs = fc2)\n        \n        return model\n    \n    # CLIMATE BRANCH\n    def climateCNN(n_columns):\n        \n        # CLIMATE DATA (ACTION \"ONE HOT\" BRANCH) / MODEL 2\n        model = Sequential()\n        model.add(Dense(2048, input_dim = n_columns, activation = \"linear\"))\n        \n        return model\n    \n    # CREATE THE BRANCHES\n    climate = climateCNN(climate_data.shape[1])\n    rain = rainCNN(110, 110, 12)\n    \n    # ELEMENT WISE MULTIPLICATION TO COMBINE\n    multiplied = Multiply()([rain.output, climate.output])\n    \n    # RIGHT SIDE\n    fc4 = Dense(1024, activation = \"linear\")(multiplied)\n    fc5 = Dense(1152, activation = \"relu\")(fc4)\n    deconv0 = Reshape((3,3,128))(fc5)\n    \n    deconv1 = Conv2DTranspose(128, kernel_size = 6, strides = 2, padding = \"valid\")(deconv0)\n    deconv2 = Conv2DTranspose(128, kernel_size = 6, strides = 2, padding = \"valid\")(deconv1)\n    deconv3 = Conv2DTranspose(64, kernel_size = 6, strides = 2, padding = \"valid\")(deconv2)\n    \n    # OUTPUT IMAGE\n    outputs = Conv2DTranspose(1, kernel_size = 8, strides = 2, padding = \"valid\")(deconv3)\n    \n    model1 = Model(inputs = [rain.input, climate.input], outputs = outputs)\n    \n    # CALL OUT MODEL FUNCTION\n    model1.compile(optimizer = 'adam', loss = 'logcosh', metrics = ['accuracy'])\n    \n    history = model1.fit([X_train, climate_train], Y_train, epochs = 100, validation_data = ([X_val, climate_val], Y_val), verbose = 2)\n\nI basically assumed this Network to take the form of a Multiple input/multi data type neural network so I followed the guide from this website: [https://heartbeat.fritz.ai/building-a-mixed-data-neural-network-in-keras-to-predict-accident-locations-d51a63b738cf](https://heartbeat.fritz.ai/building-a-mixed-data-neural-network-in-keras-to-predict-accident-locations-d51a63b738cf). Obviously, I am quite new to Machine Learning so I'm worried if my code is not correct (i.e., doesn't replicate the video game network by Stanford correctly).\n\nI have also tried emailing the original authors for their code but received no reply so here I am. Thanks in advance guys!", "link": "https://www.reddit.com/r/MachineLearning/comments/n7tvp1/d_need_help_checking_if_my_code_is_correct/"}, {"autor": "Realistic_Sea_3634", "date": "2021-05-08 14:48:19", "content": "[D] Why has machine learning become such a toxic field, know-it-all field? /!/ I've worked with many scientists from many different fields and backgrounds, but none come close to the obnoxiousness, pomposity, and outright unpalatable know-it-all vibe from the machine learning community. And I'm sure it's a case of a small rotten bunch smearing the whole field.\n\nThis behaviour is most rife in 3 places: the cesspit known as Twitter, Reddit, and somewhat in industry. It's much less rampant, comparatively, in academia in my experience (and just so we're clear, Google Brain/DeepMind/FAIR is not academia). Here is some of what I've observed:\n\n* Think they can dominate a field or little involvement from SMEs\n   * Often I see in machine learning a group or groups will swarm on a problem, throw ML at the data, and call the problem \"solved\". Very little (if any) SME involved, and importantly, no follow up. The exceptions to this general are few and far between.\n* DL encourages a habit of not learning the basics\n   * I have encountered this so often, and it is especially apparent in DL. People will jump straight into e.g. CV or NLP, and not bother learning anything foundational. I've seen and spoken to  numerous people publishing in CV papers in prestigious conferences who don't even know what why colour spaces are useful or even what a pixel is (because it sure as fuck isn't a small square on an -----> image !!! ). You may claim that they don't need to know this, but that delusional talk, and they absolutely do. There is a limit to what compute + CNN/transformer can do. After that, you need foundations to know how to improve.\n* No real work goes into the vast majority of papers (more of a DL problem)\n   * This has been cited by many in the past. However, I must articulate it myself. I also understand that there are many contributing factors to this (and indeed all the other) problem. It's most often slight architecture change or incremental improvement, with no real thought gone into the paper. What this sometimes results in (I have seen numerous times myself in a variety of settings - including my own teams) is a PhD being no good at engineering, or less productive at research than an MSc with experience under his belt. The whole point of a PhD coming into an ML team is to be useful at R&amp;D, and that is not always the case (as much as you'd expect).\n* Insolence and arrogance of fairness/ethics crowd\n   * This crowd - as it currently operates - simply serves as a cancerous tumour to the ML world. There always point out problems, but never (real) solutions. They act like gatekeepers and god's gift to the world. It brings about massive toxicity to the virtual ML community, and prohibits free speech of the community without fear of repercussions. The this crowd could do with an overhaul, as its leaders are some of the most vitriolic yobos who claim to be academics.\n* For such an applied field, very little focus on applications\n   * Often the excuse put forward is that fields like math have very few applications straight away. Firstly, ML is not like math - it's more like straight up engineering (especially DL). It's primarily applied, and should thus be much more focused on application. Your slight architecture change or you 0.5% improvement on ImageNet is not a Pythagoreon theorem waiting to happen. It's laziness and just wanting to get your PhD (surely if doing a PhD you actually want to make a real contribution to the field that you can stand behind?). Fields like physics and stats are often applied and make a real world impact with the applications. To be fair, ML also does, but nowhere near as much as it should, especially at your average company in industry. \n* Can fix all the worlds problems\n   * Over-stating the ability of ML. Not sure if delusion, PR, or a combination of both. Cited lots before by others.", "link": "https://www.reddit.com/r/MachineLearning/comments/n7qrz5/d_why_has_machine_learning_become_such_a_toxic/"}, {"autor": "cereal_final", "date": "2021-05-08 04:06:59", "content": "[P] How to get better performance with styleGAN2-ada for cartoons /!/ I'm trying to generate pokemon with styleGAN2-ada, and I'm not getting the [best results](https://imgur.com/a/h9u6nVh). I would say 50% look like legit pokemon, but the other 50% are kinda trash like the -----> image !!!  I linked. I tried training it longer, but I believe the [model collapsed](https://imgur.com/a/5ZIvJ27). How can I improve results? The dataset is 5k images of pokemon headshots like [this.](https://imgur.com/a/bxZu3vU)", "link": "https://www.reddit.com/r/MachineLearning/comments/n7gwkb/p_how_to_get_better_performance_with_stylegan2ada/"}, {"autor": "isademigod", "date": "2021-09-28 02:28:44", "content": "[D] What kind of model would this be, and where can I read up on it? /!/ So a model that takes an -----> image !!!  as an input and gives a class vector as an output is a classifier. \n\nA model that takes a vector as an input and produces a picture (i.e. styleGAN) would be a generator. \n\nSo how would I take an image as an input, and make an associated output? \n\nFor instance, train styleGAN on modified cars, and have a model that you can give a picture and it will put out a picture of that car, modified? Or a faceapp-type GAN that can take an image of a face and output an older face or a face without a beard? \n\nWhat would the pipeline for that look like, and what would be some resources to learn how they work?\n\nI have a decent amount of experience training object detection and classification models, but I'm very new to the whole GAN thing, and only have a surface-level understanding of how they work.", "link": "https://www.reddit.com/r/MachineLearning/comments/pwwk8f/d_what_kind_of_model_would_this_be_and_where_can/"}, {"autor": "chazy07", "date": "2021-09-27 20:47:54", "content": "[P] Question about confidence scores in deep CNNs /!/ Hello, I am currently working on an -----> image !!!  classification project. When I'm visualizing the outputs of my predictions in the validation set, it gives me a prediction of a class and a probability of the prediction being correct. For some of the predictions, I am getting 100% probability/confidence. Does that mean that my model is overfitting?", "link": "https://www.reddit.com/r/MachineLearning/comments/pwq7vh/p_question_about_confidence_scores_in_deep_cnns/"}, {"autor": "KirillTheMunchKing", "date": "2021-09-27 19:49:31", "content": "[D] IC-GAN Paper Explained - Instance-Conditioned GAN (5-minute summary) /!/ Aren\u2019t you tired of only seeing generated FFHQ-like faces? I bet you are, and if you know just how atrocious the samples from StyleGAN-2  trained on other datasets such as ImageNet really look you should be wildly excited to see Instance Conditioned GAN (IC-GAN) by Arantxa  Casanova and the team at Facebook AI Research! IC-GAN flips the script and uses unaligned images to condition the generator to synthesize samples similar to the input data points. This approach can be thought of as learning overlapping local distributions around the input -----> image !!! s,  which lets it train on diverse unaligned -----> image !!! s while maintaining the latent space density needed for high-quality -----> image !!!  synthesis.\n\nCheck out the [full paper summary](https://www.casualganpapers.com/unaligned-image-class-conditional-generation/Instance-Conditioned-GAN-explained.html) on Casual GAN Papers (Reading time \\~5 minutes).\n\n[IC-GAN](https://preview.redd.it/7tp8v7fdo3q71.png?width=1320&amp;format=png&amp;auto=webp&amp;s=1399cb6eae607055a4e7a97e9c088b49893ee2a2)\n\nSubscribe to [my channel](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!", "link": "https://www.reddit.com/r/MachineLearning/comments/pwoz23/d_icgan_paper_explained_instanceconditioned_gan/"}, {"autor": "theahmedmustafa", "date": "2021-09-27 16:04:06", "content": "[D] Hosting a Machine Learning Model on the internet /!/ I am a Machine Learning Engineer who has basically little to no knowledge or experience with Web Development.\n\nThis idea of running my ML model on the internet has always been of interest to me but I have absolutely no idea how it can be achieved. Here is an example overview:\n\nI train a binary cats vs dogs classifier in keras or pytorch and write an inference script all on my local machine. Now I want to deploy this model on a website, say www.catordog.com, such that anyone on the internet can go to this url and upload a -----> picture !!! , and the results of the prediction on that image from my inference script are printed on the website.\n\nThis sounds pretty basic to me on paper but my total inexperience with web dev makes me wonder if this should have been a ELI5 thread instead.\n\nThanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/pwk8pu/d_hosting_a_machine_learning_model_on_the_internet/"}, {"autor": "sarmientoj24", "date": "2021-09-27 07:18:07", "content": "[P] Looking for an Online Bounding Box Annotation Tool with Good Collaboration Tools /!/ We have a project for creating an object detection database where we have a supervisor/domain expert and number of annotators. What's the best annotation tool for annotating bounding boxes in an -----> image !!!  with the following features:\n\n* can be paid or not\n* online tool\n* has \"supervisory\" capabilities where the owner can supervise annotations of people\n* annotations from other annotators can be seen by others\n* easy-to-use\n* about 5-7 annotators\n* images ranging from 10K-50K", "link": "https://www.reddit.com/r/MachineLearning/comments/pwc26w/p_looking_for_an_online_bounding_box_annotation/"}, {"autor": "rmxz", "date": "2021-09-26 21:17:56", "content": "[P][D] Visualizing \"zebra -stripes +spots\" using CLIP embedding math /!/ Math on CLIP embeddings can help visualize when/why CLIP considers images to be similar.\n\nI created [this github project](https://github.com/ramayer/rclip-server) to visualize CLIP embedding math on databases from the excellent [rclip](/r/MachineLearning/comments/pb6ime/p_rclip_use_clip_to_search_for_your_photos_in_the/) project that /u/39dotyt posted here last month.\n\nA live demo of this system using Wikimedia images can be seen [here](http://image-search.0ape.com).\n\nSome interesting results:\n\n* [zebra -stripes +spots](http://image-search.0ape.com/search?q=zebra%20-stripes%20%2Bspots) \\- Animals that look kinda like zebras but with spots instead of stripes.\n* [zebra -mammal +fish](http://image-search.0ape.com/search?q=zebra%20-mammal%20%2Bfish) \\- Animals that look like zebras but fish instead of mammals.\n* [zebra -animal +car](http://image-search.0ape.com/search?q=zebra%20-animal%20%2Bcar) \\- Objects colored like zebras but more cars than animals.\n* [zebra -\"black and white\"](http://image-search.0ape.com/search?q=zebra%20-%22black%20and%20white%22) \\- Baby zebras (brown &amp; white) and a Greater Kudu (a brown &amp; white striped 4-legged animal). Of course you could also find the same baby zebra searching for [zebra -big +small](http://image-search.0ape.com/search?q=zebra%20-big%20%2Bsmall) or even more simply, just [baby zebra](http://image-search.0ape.com/search?q=baby%20zebra).\n* [furry black and white striped animal](http://image-search.0ape.com/search?q=furry%20black%20and%20white%20striped%20animal) \\- zebras, lemurs, and other furry black and white animals.\n* [striped horse-like animal](http://image-search.0ape.com/search?q=striped%20horse-like%20animal) \\- more zebras (and horses with stripes)\n* [zebra habitat -zebra](http://image-search.0ape.com/search?q=zebra%20habitat%20-zebra) \\- places that look like somewhere a zebra might live\n\nIt can also do a search based on the difference between the CLIP embeddings of two images directly.  For example, CLIP considers [this -----> image !!!  of a spider on a purple flower](http://-----> image !!! -search.0ape.com/search?q=%7B%22-----> image !!! _id%22%3A28754%7D) minus [this -----> image !!!  of the same kind of spider on a white flower](http://-----> image !!! -search.0ape.com/search?q=%7B%22-----> image !!! _id%22%3A174054%7D) to be [this set of pictures which is mostly purple flowers without the spider](http://-----> image !!! -search.0ape.com/search?q=%7B%22-----> image !!! _id%22%3A28754%7D%20-%7B%22-----> image !!! _id%22%3A174054%7D).\n\nI find this useful for trying to understand what concepts CLIP considers similar and why.\n\nMore examples coming in the comments below.", "link": "https://www.reddit.com/r/MachineLearning/comments/pw2lwc/pd_visualizing_zebra_stripes_spots_using_clip/"}, {"autor": "seb59", "date": "2021-09-26 17:49:27", "content": "Volumetric time series to -----> image !!!  time series /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pvyo0r/volumetric_time_series_to_image_time_series/"}, {"autor": "ptoews", "date": "2021-09-21 14:04:19", "content": "[D] 3x A6000 or 6x 3090? /!/ We are finalizing our plans for a deep learning machine but are unsure about the best GPU choice. It will be used on -----> camera !!!  image and point cloud data in parallel with a few classification heads, so the network will be rather large (but still as small/fast as possible).\n\nWe can either get 3x (Quadro) RTX A6000 or 6x RTX 3090 for the same price. According to benchmarks, the 3090 based setup should be about 1.8x faster. On the other hand, the A6000 has double the memory per GPU which allows larger batch sizes per GPU. However I'm not sure how big of an advantage this is compared to the distributed batching with multiple GPUs (data parallelism).\n\nAnother possibility is of course to parallelize experiments, however I don't think there will be running more that two experiments at the same time that often. So data parallelism would come into play either way.\n\nAnother topic is of course power and thermal management. We would use a crypto-style open rig for the 6x setup, for the 3x one the mainboard is able to take them all. However we would probably need two PSUs with the 6x setup. PCIe lanes should be no problem as we have 128 of them.\n\nWhich of the two options would be more effective for this task? Would the 3090 setup be too limiting for large models and increase workflow complexity? Or would its price effectiveness be enough to beat the A6000s?\n\nThanks for your input.", "link": "https://www.reddit.com/r/MachineLearning/comments/psjdud/d_3x_a6000_or_6x_3090/"}, {"autor": "techsucker", "date": "2021-09-20 22:53:30", "content": "[R] Facebook AI Introduces A New Image Generation Model Called \u2018IC-GAN\u2019 That Creates High-Quality Images of Unfamiliar Objects And Scenes /!/ Generative adversarial networks (GANs) have been used for few years to generate photorealistic images of objects or scenes that are very similar in style and content. However, until now, these models could only produce output related to datasets they were trained on \u2013 which had limitations because there was usually less diversity among those files than what you would find when generating new ideas. A conventional GAN trained on images of cars shows impressive results when asked to generate other images of cars or automobiles. But the trained GAN will likely fail if given a flower or any object outside its automotive data set.\n\nFailure to show non-identical objects from the training dataset is a huge limitation, and it definitely needs to be resolved to meet the demand. Facebook is trying to solve the above problem by introducing [Instance-Conditioned GAN (IC-GAN)](https://github.com/facebookresearch/ic_gan?). The [IC-GAN](https://arxiv.org/pdf/2109.05070.pdf) is a new -----> image !!!  generation model that can produce high-quality -----> image !!! s with some input, even if it doesn\u2019t appear in the training set. The unique thing about the IC-GAN model is that it can generate realistic, unforeseen image combinations\u2014for example, a camel in snow or zebras running through an urban cityscape.\n\nIt is no surprise that IC-GANs could be used to create visual examples for data sets with these new capabilities. This would allow artists and creators alike more expansive AI-generated content by creating art from photos or videos in the same way an artist might draw a picture using pencils and paintbrushes at their disposal.\n\n# [5 Min Read](https://www.marktechpost.com/2021/09/20/facebook-ai-introduces-a-new-image-generation-model-called-ic-gan-that-creates-high-quality-images-of-unfamiliar-objects-and-scenes/) | [Paper](https://arxiv.org/abs/2109.05070?) | [Code](https://github.com/facebookresearch/ic_gan?) | [Facebook Blog](https://ai.facebook.com/blog/instance-conditioned-gans/) \n\n&amp;#x200B;\n\nhttps://preview.redd.it/l1f1ubcvmqo71.png?width=1392&amp;format=png&amp;auto=webp&amp;s=cf59a4f9c79b58a3dfca29a732ae05826e8a601e", "link": "https://www.reddit.com/r/MachineLearning/comments/ps5ubp/r_facebook_ai_introduces_a_new_image_generation/"}, {"autor": "adenml", "date": "2021-09-20 20:50:05", "content": "[D] Is it possible (and useful) to use knowledge distillation for problems with highly-dimensional outputs? /!/ I understand the role of distillation for classification tasks. \n\nFor example, does it even make sens to use knowledge distillation for other tasks, where the output is highly dimensional, such as -----> image !!!  segmentation? Why / why not?\n\nAlso, more concretely, what would be the teacher layers that you would enforce the student to learn from?", "link": "https://www.reddit.com/r/MachineLearning/comments/ps3iij/d_is_it_possible_and_useful_to_use_knowledge/"}, {"autor": "PonenCreatesThings", "date": "2021-09-20 18:10:08", "content": "[P] \"A truck with the text JCN\" - CLIP is scarily good at surveillance forensic search /!/ Query: \"A truck with the text JCN\"\n[CLIP -----> image !!!  response](https://github.com/johanmodin/clifs/blob/master/media/jcn.jpg)\n\nIt started as an idea from the CLIP paper, in which the authors mentioned that CLIP, while impressive, didn't work very well in a surveillance setting. I would disagree, it works uncannily good.\n\nI put together a small proof-of-concept of using CLIP in video forensic search. It takes videos, breaks them up into images and patches and encodes them. These encodings are matched by similarity with a user's input and the corresponding frame is returned.\n\nThe very simple, PoC app with a web UI and some (in my opinion) crazy example queries can be found here: https://github.com/johanmodin/clifs", "link": "https://www.reddit.com/r/MachineLearning/comments/ps0d02/p_a_truck_with_the_text_jcn_clip_is_scarily_good/"}, {"autor": "chip_0", "date": "2021-09-20 16:55:29", "content": "[D] Computer Vision as Inverse Computer Graphics? /!/ Inverse Computer Graphics aims to solve the problem of computer vision end-to-end by having a model that can take an -----> image !!!  (or sequence of -----> image !!! s taken from different view points with known relative positioning), and output a 3D mesh of the world that generated these -----> image !!! s.\n\nMany of the renowned researchers hold up Inverse Computer Graphics as one of the benchmarks for AI. [Hinton](https://www.cs.toronto.edu/~hinton/csc2535/notes/lec6b.pdf) is definitely the most prominent, and a lot of his recent research (like Capsule Networks) aims to address that.\n\n[Karpathy](https://twitter.com/karpathy/status/1391904502458978305?lang=en) got in the game as well, although I am not sure how exactly the research he cites relates to this problem.\n\nI personally find this area very interesting since I love both 3D graphics and AI.\n\nWhat do you all think of this AI dream, and do you have any favorite ideas or approaches that you think can solve it some day?", "link": "https://www.reddit.com/r/MachineLearning/comments/pryveo/d_computer_vision_as_inverse_computer_graphics/"}, {"autor": "ThresholdTuner", "date": "2021-09-20 12:54:51", "content": "[D] Paper on disentanglement of -----> image !!!  representations and they are limitations and weaknesses? /!/ I'm working on the disentanglement of image representations in generative models, but one thing I realize is that finding correct papers to look at is not trivial since many works under different titles could be classified as disentanglement such as gans, 2D-3D transformation, VAEs, image-to-image translation. Does anyone have some recommendations?", "link": "https://www.reddit.com/r/MachineLearning/comments/prua6p/d_paper_on_disentanglement_of_image/"}, {"autor": "miladink", "date": "2021-02-18 08:11:51", "content": "Has anyone trained an -----> image !!!  classifier using RL on CIFAR-10? [D] /!/ Hi All! I was watching the UC Berkeley CS285 videos [here](https://www.youtube.com/watch?v=SinprXg2hUA&amp;list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&amp;index=1). Sergey Levin here tells the students that you can train an image classifier using RL. I am wondering if anyone has ever done that in practice? I know it works in theory, but I was wondering how much resources does that need.", "link": "https://www.reddit.com/r/MachineLearning/comments/lmhluq/has_anyone_trained_an_image_classifier_using_rl/"}, {"autor": "born_in_cyberspace", "date": "2021-02-16 11:51:55", "content": "[D] Thousands of papers to read? Let's prioritize them /!/ I guess most people here have the same problem: thousands of Arxiv papers to read, and not much time for it. How to prioritize them? \n\nI use the following prioritization system.\n\n1.**Classify all papers you want to read by 3 attributes** :\n\n- **the expected quality of the paper**. \"Top\" is for the papers from major institutions (DeepMind, OpenAI, BEAR, a few others). \"Others\" is for others. \n- **the paper's main topic**: NLU, -----> image !!!  recognition, AGI, hardware, etc. I also use several topics that are not about ML, but related to it (e.g. computational neuroscience, mind uploading)\n- **when it was published**. Each year after 2012 has its own tag. Older papers are grouped into a single tag \"2012 and earlier\". \n\n2.**Allocate your paper-reading time**\n\nDescribe how do you want to allocate your papers-reading time. \n\nFor example:\n\n- By expected quality: 70% top, 30% others\n- By topic: 50% NLP, 20% AI safety, etc\n- By year: 50% - 2021, 25% - 2020, 12,5% - 2019, etc\n\n3.**Decide which paper to read next**\n\nAfter the preparations are done, this step is trivially automated.\n\nGenerate a random number between 0 and 100. If it's below 70, then read a \"*top*\" paper. Otherwise, read an \"*others*\" paper.\n\nSame approach for the topic and year selection. \n\nThe weighted randomization allows you to read the papers that are not the best, but could still contain interesting ideas.  \n\n-------------------------\n\nHow one can improve the system? What are some good alternatives?", "link": "https://www.reddit.com/r/MachineLearning/comments/ll28yb/d_thousands_of_papers_to_read_lets_prioritize_them/"}, {"autor": "manfredcml", "date": "2021-02-21 18:20:26", "content": "[D] Latest development of content-based -----> image !!!  retrieval /!/ Image retrieval, roughly speaking, has the **feature extraction** phase where the images are converted to embeddings through some models, and the indexing phase for optimizing retrieval latency. I'd like to focus on the feature extraction part in this thread.\n\nOne common paradigm of training a feature extractor is training a CNN on labeled data under a supervised classification setting. In this case large amounts of richly annotated data needs to be constructed for model training. The output from certain layer(s) is used after some forms of aggregation / normalization (e.g. using output from a global pooling layer) as the embedding, where similarity metrics (e.g. cosine similarity) can then be computed on these embeddings. Such supervised learning setting for training a feature extractor for image retrieval is one popular approach, yet requires a decent amount of richly annotated data, which can be expensive.\n\nThere're approaches based on constructing image triplets and the use of contrastive loss. Some do not require any annotated data. For example, in MoCo ([https://arxiv.org/abs/1911.05722](https://arxiv.org/abs/1911.05722)), the \"positive\" image in the triplet is constructed from augmenting the \"query\" image, while the \"negative\" image is randomly selected. While no annotated data is required for training, whether the embedding learned in this way is sufficient for image retrieval is unclear in my opinion.\n\nCan you please share what you know about the current status of content-based image retrieval? Is it still dominant to use models trained on classification tasks for extracting images features? What is the major development in recent years which enables training decent feature extractors without or with minimal annotated data?", "link": "https://www.reddit.com/r/MachineLearning/comments/lp329p/d_latest_development_of_contentbased_image/"}, {"autor": "crnch", "date": "2021-02-21 11:59:55", "content": "[P] Classification of SatNOGS satellite observation time series /!/  Hi! \n\n**Background**\n\nSatNOGS ([https://network.satnogs.org/](https://network.satnogs.org/) ) is an awesome crowd-sourced project where a worldwide community of researchers and private enthusiasts are operating open-source ground stations to observe overflying satellites and record their transmissions. The project has existed since 2015 and there are over 3.7 million recorded observations. The recorded data is saved in the SatNOGS database as raw audio signal as well as a Fast-Fourier-Transform aka waterfall. Since the observable time for each satellite varies depending on azimuth, altitude and speed, the recordings vary accordingly. In addition to the transmission data, there is metadata being stored with each recording. Here\u2019s an example of a recording of an Astronaut on the ISS talking to a school in Europe: [https://network.satnogs.org/observations/3589954/](https://network.satnogs.org/observations/3589954/) \n\n**Task**The recordings for each station are timed by the knowledge of the position of the different satellites. Not each observation is an actual transmission of data. The task is to classify these observations (records) in \"signal present\" or \"no signal present\". So far, this is being done manually. The \u201ctrained eye\u201d is able to see whether it\u2019s a recording of an actual transmission by looking at the FFT visualized in the -----> picture !!! . Sometimes this is more apparent and sometimes it\u2019s very subtle. The pictures have varying heights because of their different length. Since there is a pretty big amount of labeled examples from the past, the idea is to train a neural network to classify new (or unclassified) observations. \n\nMy questions are:\n\n* What\u2019s a suitable transformation of the input data to derive a general input layer for the neural net, independently of record length? \n* What\u2019s a good structure for the hidden layers? How do I derive a good structure?\n* What activation function should be used for this kind of problem?\n\nAny suggestions are welcome. Thanks :)", "link": "https://www.reddit.com/r/MachineLearning/comments/lovoaz/p_classification_of_satnogs_satellite_observation/"}, {"autor": "rexlow0823", "date": "2021-02-21 08:16:56", "content": "[D] How to detect if an -----> image !!!  has tampered with physically? /!/ We are trying to solve a very niche problem where people would physically tamper/alter with an image, see the image below for an example.\n\nExisting methods suggest investigating the pixel variations or movements to possibly detect alteration but our problem here is dealing with attackers\n\n1. Cutting out parts of **image A**\n2. Paste it on top of **image B**\n3. Take a picture of **image B** that is **image C**\n\nThis way, the noise pattern produced by the camera would be consistent for **image C**. Ergo, studying noise patterns would not make sense in this case.\n\n**Constraints identified:**\n\n1. We are only working on a set of images that shares the same layout but different in\n\n* Content\n* Slight difference in background-color\n\n1. Attackers are probably going to alter a few areas of interest. E.g.: name, frontal photo, date of birth, and nationality.\n2. Attackers might only alter 1 area at a time, or a few areas collectively.\n\n**Possible solutions that we thought of but not yet validated:**\n\n1. Object detector. Detecting those patches, but can be varied in terms of length, shapes, and colors.\n2. Autoencoder. Train an auto-encoder and get the reconstruction loss for the image. If the loss is high, then it is probably tampered with? But the image content could act as one of the noises?\n3. Generative adversarial networks (GANs). Let the network learns the differences between a set of genuine and tampered images. But again, the image content might be in the way.\n\nWould really appreciate it if anyone has a better approach!\n\nhttps://preview.redd.it/ee027wb3isi61.png?width=1280&amp;format=png&amp;auto=webp&amp;s=08fa09605a638c46f4768680e19525397ec6f772\n\n**Left**: Original image\n\n**Right**: Tampered image with **some other contents** pasted on top of selected regions.\n\nRelated resources:\n\n* [https://blog.adobe.com/en/publish/2018/06/22/spotting-image-manipulation-ai.html](https://blog.adobe.com/en/publish/2018/06/22/spotting-image-manipulation-ai.html)", "link": "https://www.reddit.com/r/MachineLearning/comments/loshnc/d_how_to_detect_if_an_image_has_tampered_with/"}, {"autor": "rexlow0823", "date": "2021-02-21 08:15:19", "content": "How to detect if an -----> image !!!  has tampered with physically? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/losgn9/how_to_detect_if_an_image_has_tampered_with/"}, {"autor": "alecbarr90", "date": "2021-02-21 06:13:49", "content": "[D] What are some of the major difficulties in creating -----> image !!!  recognition/object tracking for board games? /!/ Give the huge popularity of board games, and the game theoretical knowledge that goes alongside them, I\u2019m surprised there hasn\u2019t been more development in the field of image recognition/object tracking to aid the use of AI software to analyse positions. Many of these board have discrete positional features (games like Chess, Backgammon and Go) that can be analysed easily within their relevant software, but there seems to be limited availability of software that helps transcribe positions from the real world (i.e. \u201cover the board\u201d) to the digital world, ready for analysis. \n\nIs this to do with the limitations of object tracking? What are the main impediments to effective image recognition software here?\n\nNot being a software engineer it seems to me it can\u2019t be that hard for a computer to be able to discriminate between, say, different chess pieces, or backgammon checkers, or read the numbers on a dice roll - but is there some other nuance that makes this particularly hard?", "link": "https://www.reddit.com/r/MachineLearning/comments/loqnd1/d_what_are_some_of_the_major_difficulties_in/"}, {"autor": "adcordis", "date": "2021-02-21 01:59:10", "content": "[N] Photonics is the future of quantum machine learning instead of electronics. /!/ \" In 2018, Ozcan\u2019s group reported development of the first all-optical diffractive deep neural network using 3D-printed polymer wafers with uneven surfaces for light diffraction. That work was primarily about machine learning by way of light propagated through the trained diffractive layers to execute an -----> image !!! -classification task, he says\". \u00a0 \n\n\nhttps://www.osa-opn.org/home/newsroom/2021/february/shaping\\_light\\_pulses\\_with\\_deep\\_learning/", "link": "https://www.reddit.com/r/MachineLearning/comments/lome33/n_photonics_is_the_future_of_quantum_machine/"}, {"autor": "quickswitch123", "date": "2021-02-24 00:21:30", "content": "[P] Ideas for Raspberry PI robotics/ML projects to do with my 16-year-old brother? /!/ Greetings! I am a first-year Ph.D. student in ML and robotics and this is my first post on this sub :). My little brother expressed interest in computer science, so I wanted to feed his interest by doing a robotics/ML project with him.\n\nI was wondering if anyone has any ideas for a Raspberry PI robot project with some ML vision component that I could do with him. I have access to a laser cutter and 3D printer, so I could definitely fabricate some pieces (ideally simple pieces).\n\nI found this [mobile robot](https://www.sunfounder.com/collections/robotics/products/smart-video-car?gclid=Cj0KCQiA7NKBBhDBARIsAHbXCB7eM7WE-a3PFiDer4KR7sWl9TWS41X_TkDPr5vuGQg1Siao46cP5JIaAgAcEALw_wcB) with a -----> camera !!!  that could be interesting, but maybe building something more from the ground up would be better.\n\nI would love any input or ideas!", "link": "https://www.reddit.com/r/MachineLearning/comments/lqxwvl/p_ideas_for_raspberry_pi_roboticsml_projects_to/"}, {"autor": "personanonymous", "date": "2021-02-23 23:37:23", "content": "[P] Similar collaborate docs to this where I can download each -----> image !!! ? /!/  \n\n[https://colab.research.google.com/drive/1mBTj5n1Y6PycuomVSqKY-MhZXT2HX09G#scrollTo=zvZFRZtcv8Mp](https://colab.research.google.com/drive/1mBTj5n1Y6PycuomVSqKY-MhZXT2HX09G#scrollTo=zvZFRZtcv8Mp)\n\nThis is doing exactly what I want - sort of sorting the bad images and eventually giving me a pretty realistic image of what my text input is.\n\nI was wondering how to download each image individually? I do not want to individually edit them because that would take me days. This doc would be perfect for me if each image could be downloaded!\n\n&amp;#x200B;\n\nOr are there any similar Clip-Glass stuff I can look into that allows me to download each individual image?\n\nOnly similar stuff I have found is this: [https://colab.research.google.com/drive/1L14q4To5rMK8q2E6whOibQBnPnVbRJ\\_7#scrollTo=Nq0wA-wc-P-s](https://colab.research.google.com/drive/1L14q4To5rMK8q2E6whOibQBnPnVbRJ_7#scrollTo=Nq0wA-wc-P-s)\n\nBut this yields poor results (unrealistic) as well as starting with noisy images - i prefer seeing images with humans in it to begin with that eventually gets rid of them.\n\nCheers!", "link": "https://www.reddit.com/r/MachineLearning/comments/lqwnh9/p_similar_collaborate_docs_to_this_where_i_can/"}, {"autor": "ThickDoctor007", "date": "2021-02-23 17:54:35", "content": "[D]Clustering point clouds /!/ Hi,\n\nI am building a model which is supposed to determine the parameters (shape, size, position) of 3D objects represented as point clouds. For a single object, I have achieved fairly accurate predictions. In the case of multiple point clouds, however, the results are not looking promising.\n\nThe idea I want to try out is first to split the point clouds into clusters (up to 5) and then run the regression for every cluster separately. Below is the -----> image !!!  of the example record.\n\nI would be thankful if anyone recommended an appropriate clustering method that could assign every point a cluster ID (from 0 to 4). In the example below, there are 4 clusters (every point should be assigned to cluster with ID 0, 1, 2, or 3):\n\n&amp;#x200B;\n\n[example of 4 point clouds representing 4 superquadrics](https://preview.redd.it/gjam3wlnl9j61.png?width=1775&amp;format=png&amp;auto=webp&amp;s=eb1604dbb846d956940b098e14d235745db8f3cd)\n\nfor a single point cloud parameter values predictions, I used a modified version of [RSConv](https://arxiv.org/pdf/1904.07601.pdf); instead of classification, the model output is a 8-dimensional vector of continuous values representing object size (3 values for x, y, and z), position (3 values for x, y, and z) and shape (epsilon1 and epsilon2).", "link": "https://www.reddit.com/r/MachineLearning/comments/lqoi2e/dclustering_point_clouds/"}, {"autor": "[deleted]", "date": "2021-03-05 02:22:46", "content": "Hello I know I'm not in the right sub... can anyone please tell me how to draw this -----> picture !!!  in Tikz? /!/ [deleted]", "link": "https://www.reddit.com/r/MachineLearning/comments/ly1qbu/hello_i_know_im_not_in_the_right_sub_can_anyone/"}, {"autor": "saad2xi", "date": "2021-03-04 23:54:15", "content": "Looking for feedback on my project: Palapa. It\u2019s a social app for building -----> image !!!  classifiers.", "link": "https://www.reddit.com/r/MachineLearning/comments/lxyxzg/looking_for_feedback_on_my_project_palapa_its_a/"}, {"autor": "gadonovo", "date": "2021-03-04 23:50:15", "content": "[D] Help to understand \"Deduplication\" /!/ Hi guys! I'm trying to understand the paper \"[Unsupervised representation learning with deep convolutional generative adversarial networks](https://arxiv.org/abs/1511.06434)\". And I don't get this part named \"Deduplication\":\n\nTo  further decrease the likelihood of the generator memorizing input  examples  we perform a simple -----> image !!!  de-duplication process. We fit a  3072-128-3072 de-noising dropout regularized RELU autoencoder on 32x32  downsampled center-crops of training examples. The resulting code layer  activations are then binarized via thresholding the ReLU activation  which has been shown to be an effective information preserving technique  and provides a convenient form of semantic-hashing, allowing for linear  time de-duplication . Visual inspection of hash collisions showed high  precision with an estimated false positive rate of less than 1 in 100.  Additionally, the technique detected and removed approximately 275,000  near duplicates, suggesting a high recall.\n\nAnyone can help me?", "link": "https://www.reddit.com/r/MachineLearning/comments/lxyv8l/d_help_to_understand_deduplication/"}, {"autor": "Wizard_of_Hoes", "date": "2021-03-04 13:28:07", "content": "What model would you use to identify the type of the product from an e-shop given a -----> picture !!!  of it and the text from its advertisement? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/lxkh5o/what_model_would_you_use_to_identify_the_type_of/"}, {"autor": "Suzzy67", "date": "2021-03-11 13:24:22", "content": "[D]data augmentation /!/ can anyone kindly tell me..i m doing data augmentation by flow method..in which i run loop to increase the size of -----> image !!!  into two...mean for every -----> image !!! s there are two more -----> image !!! s...but it didnt increase the legth of all -----> image !!! s...for ex..if i have 266 -----> image !!! s..it gives 522 -----> image !!! s in total..but 266*2=532?why", "link": "https://www.reddit.com/r/MachineLearning/comments/m2pzg9/ddata_augmentation/"}, {"autor": "---Py", "date": "2021-03-11 10:48:12", "content": "What are the benefits of deploying ML and DL directly in a device (such as a -----> camera !!! )? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/m2nirt/what_are_the_benefits_of_deploying_ml_and_dl/"}, {"autor": "Megixist", "date": "2021-03-14 11:45:43", "content": "[D] How to get help for stalled research in audio processing domain? /!/ Please let me know if this is not the right place to post this but my friend and I (3rd year Computer Engineering students) have been working on an ML research project for almost 4 months now and we reached to a point where we have enough to publish a paper about it but not enough to solve the actual problem statement. I don't like the idea of publishing half done work and don't know how to proceed but my head of department is encouraging us to write the paper immediately (I'm not yet sure if she wants her name included or not). We asked for her guidance but she mostly works with -----> image !!!  processing and doesn't have enough knowledge about the MP3 audio manipulation problem statement that we are trying to solve. In such a scenario, how do we find and seek help from someone who has experience in the field since most people I know tend to focus on image and NLP domains", "link": "https://www.reddit.com/r/MachineLearning/comments/m4tt8y/d_how_to_get_help_for_stalled_research_in_audio/"}, {"autor": "AgentCooderX", "date": "2021-03-14 05:38:01", "content": "[D] Learning path for deep fake machine learning for apps like reface, Wombo.ai and deep nostalgia /!/  I know there are other threads in reddit which regards to \"how to learn I\" or \"where do I start\" type of threads, but my question is tailored and specific to the deepfake technology used by the apps I mentioned above where the input is an -----> image !!!  and it surprisingly became 'alive'. Are the images converted to a 3D object or something after detection?\n\nWhat do you guys suggest the best path for learning the AI with regards to this tech?\n\nThere are a lot of AI related topics, subjects and courses but I want to know which exact (or related) path to possibly do similar technology in the future.\n\nI have almost 20 years of professional coding experience, focused on level, graphics and image processing (owned a few US patents related to image processing as well) and also I believe has stomach for maths and stuff doing Graphics development for years as well. I just want somebody (or maybe a mentor) that points me to the direction on which topics/subject related to AI I will take to possibly clone a similar tech to those app mentioned.", "link": "https://www.reddit.com/r/MachineLearning/comments/m4p03g/d_learning_path_for_deep_fake_machine_learning/"}, {"autor": "topological_rabbit", "date": "2021-03-13 23:05:04", "content": "[D] How are StyleGANs trained? /!/ I understand the regular generator / discriminator structure, which trains the generator to create -----> image !!! s the discriminator can't tell aren't the target type of -----> image !!! s, but a style GAN *transfers* a style to an existing -----> image !!! . How is the training done without the generator just generating brand new unrelated images that fool the discriminator?", "link": "https://www.reddit.com/r/MachineLearning/comments/m4hvl8/d_how_are_stylegans_trained/"}, {"autor": "kaiser_17", "date": "2021-03-13 16:06:05", "content": "[R] Intuition behind Training neural net with multimodal distribution /!/ For a classification task where the distribution of -----> image !!!  (conditioned on class) is multimodal ,how neural nets can figure out the decision boundary? My intuition is that somehow the neural nets project the input distribution into a unimodal distribution and then figure out the decision boundary. Is my intuition wrong? Is there a way to find proper decision boundaries between two multimodal distribution without projecting them into their unimodal versions?\n\nAlso my second question is ,for this multimodal case how do centre loss and softmax crossentropy loss differ in their projections ?", "link": "https://www.reddit.com/r/MachineLearning/comments/m497m8/r_intuition_behind_training_neural_net_with/"}, {"autor": "yuecao", "date": "2021-03-29 00:32:39", "content": "[R] Swin Transformer: New SOTA on COCO and ADE20K /!/ In this paper, the authors present a general-purpose vision transformer, named Swin Transformer, which is compatible with a broad range of vision tasks, including -----> image !!!  classification (86.4 top-1 accuracy on ImageNet-1K), object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val and rank 1st in the open [benchmark](http://sceneparsing.csail.mit.edu/)). Note that, it incurs a new state-of-the-art on both COCO and ADE20K, demonstrating the potential of Transformer-based models as vision backbones.\n\nPaper: [https://arxiv.org/abs/2103.14030](https://arxiv.org/abs/2103.14030)\n\nCode: [https://github.com/microsoft/Swin-Transformer](https://github.com/microsoft/Swin-Transformer)\n\nPaperWithCode: [https://paperswithcode.com/paper/swin-transformer-hierarchical-vision](https://paperswithcode.com/paper/swin-transformer-hierarchical-vision)", "link": "https://www.reddit.com/r/MachineLearning/comments/mfdua1/r_swin_transformer_new_sota_on_coco_and_ade20k/"}, {"autor": "fripperML", "date": "2021-03-28 23:03:45", "content": "[D] What\u2019s the simplest, most lightweight but complete and 100% open source MLOps toolkit? /!/ I know this has been asked many times and in many different ways. And there are tons of blog posts, articles, videos and courses addressing this and comparing hundreds of tools, libraries, frameworks\u2026 And that\u2019s part of my problem: I am facing so many options that I feel like Buridan\u2019s ass, dying of starvation for not knowing what to do.\n\nAlthough I don\u2019t want to write too much, I need to speak a little about our situation, in order to put the question in our context.\n\n**Our Team**\n\nOur Team is small. We have only four people, which could be qualified as beginner data scientists. One of us has a profile that is a little bit more \u201cengineer\u201d, so data engineer could be more suitable for him. Anyway, we don\u2019t have much experience, neither in Python Projects nor in Machine Learning. What we have is passion and love for ML!\n\nFor a couple of years, we have been functioning with SAS, but now we plan to change to the Python landscape, as it is much more vivid and exciting. In the last year, we have made two projects in Python, but without using any good practices at all. Every step was made by hand and prone to error, models were neither monitored nor even deployed (they only were used for making some batch predictions), projects were not properly structured, documentation was painful\u2026\n\nSo we know that we need to change it before it becomes unmanageable.\n\nWe don\u2019t expect the size of the team to grow fast. Let\u2019s say in a couple of years we can expect 10-12 people working with us (the organization knows the importance of Machine Learning, but economic issues can be an obstacle).\n\n**Our Projects**\n\nFor the moment, we have only made \u201cclassical\u201d Machine Learning. I mean: no Deep Learning. We have used Pandas and Scikit-Learn, XGBoost, etc. And only in Batch mode. But we expect it to change in less than a year, because we will need to train an -----> image !!!  classifier to detect anomalies in customs packages, so it will need to be:\n\nTrained using a deep learning convolutional network.\n\nIntegrated with other applications (that are coded in Java) and fast (real-time).\n\nOther change we expect is to need more distributed computing, as we will need to manage some huge databases that simply do not fit in a pandas dataframe. This are the most important challenges we face.\n\n**Our Company**\n\nWe work for a big company, which also imposes some restrictions to us. Mainly:\n\nWe do not have budget to spend in MLOps solutions, so everything has to be open and free.\n\nWe won\u2019t hire data scientist / data engineers for the moment.\n\nThere are some tools, uses by other teams, that we should use as part of the MLOps stack, although they are not the best in the class.\n\nRegarding the last item, a short list of this set of restrictions is the following:\n\n* We have a Cloudera Express installation. It\u2019s the most basic and cheaper Cloudera option, so it does not come with any tool for Machine Learning management. It only gives to us HDFS, Impala, Spark and a set of nodes to run Python scripts on them.\n* We have Control-M as the orchestrator and workflow manager tool.\n* We have DataStage as the ETL tool.\n* We use SVN as the code version system (yes, no git).\n* We deploy our projects using a very simplified and self-made version of Docker. It\u2019s a little bit awkward and I think that, if we push a little bit, we could convince the organization to let us use Docker. But if Docker is reachable, Kubernetes is out of our capabilities.\n* We have Jenkins for CI.\n* We have Visual Studio Code professional licenses.\n\n**Toolset**\n\nWith this premises, I have two different and opposed concerns or even fears.\n\n* Fear of not using enough tools and good practices and arriving in a couple of years to a state where we cannot manage our own code, project and models.\n* Fear of using so many tools that they impose a burden our small team cannot bear.\n\nIt\u2019s clear that we need some MLOps, but how much, I don\u2019t know. I will review some things I have been reading, and I hope you can help me choosing the right tools.\n\n**Python Programming**\n\nIt looks like we will program using Visual Studio. We will use a remote interpreter, because we will run things on the Cloudera Nodes, although we will program locally and integrate the code with a SVN repository.\n\nDo we need tools for standardizing our code, like PyLint, Flake8, MyPy or Black? Would you recommend any of those?\n\n**CI and Deployment**\n\nWe will use Jenkins. For deployment of our code, is Docker a no brainer, a minimum standard? I tend to think so from what I read, but I\u2019d like to be sure and to have good arguments.\n\nDo we need more tools?\n\n**Project Scaffolding**\n\nI have been reading about PyScaffold, CookieCutter and, best of all (from my point of view), Kedro. I think we will stick to Kedro template, because it offers much more functionality, and I like to think of each project as a set of pipelines to be run. What do you think of Kedro?\n\n**Documentation**\n\nWould you recommend having separate documents, or generating the documentation from the projects, using Sphinx or another similar tool? I tend to prefer the second option, because the first one very likely tend to generate obsolete docs. But I don\u2019t know if the \u201cburden\u201d of the second is too big, and if the generated docs can suffice for a typical ML project.\n\n**Project registry**\n\nIs there any tool that could be used as a \u201cproject registry\u201d, like a simple web app where we could navigate through our projects, read the docs and thinks like that? I don\u2019t know. If not, the registry will be the SVN repo with all our projects as folders, and that\u2019s all.\n\n**Data Exploration and Preparation**\n\nI think that Matplotlib, Seaborn and Pandas should suffice, and when things go big, we should use PySpark, Scala or even plain SQL in Impala. However, I know Dask exists, and newer tools like Koalas or Vaex. What do you think?\n\nFor creating data transformation pipelines, we will use Kedro, although there are lots of tools that look interesting, like Dagster.\n\nWhen we enter the \u201cdeep learning\u201d realm, can we keep using the same tools? Should we use another framework like TFX? I\u2019d prefer not, cause learning one framework is hard, and two is worse. If a solution is valid for all our projects it\u2019s better. Or TFX is valid for \u201cclassical\u201d ML and Deep Learning?\n\n**Tests**\n\nI think unit testing can be too much burden for us. But I have come to Great Expectations library and think it\u2019s well suited for ML projects. Would you recommend it as an important part of our MLOps stack?\n\nBy the way, there is a Kedro-Great Expectations plugin, so we could benefit from that.\n\n**Feature Store**\n\nIs it really needed, especially considering our team size and experience? If so, I have read about Feast and Snorkel.\n\n**Data Versioning**\n\nIs it really needed, especially considering our team size and experience? If so, I have read about DVC.\n\n**Experimenting**\n\nI think it\u2019s an important piece, although I wonder if we really need a tool or we could use our own standard of reports and artifacts to follow what we have tried. But the risk that it goes unmanageable is high.\n\nKedro has a journal, I don\u2019t know if it can suffice. Also it has a Kedro-MLFlow plugin, so that we could benefit from using MLFlow as the experiment tool.\n\nI have also read about Guild, that seems really lightweigh and easy. I don\u2019t know much more.\n\n**Training**\n\nI developed my own library for doing nested cross validation and, with the same function:\n\nOptimizing hyperparameters (of model and pipeline).\n\nGenerating a report of the training to assess the quality of the model.\n\nIt\u2019s build on top of Skopt. I did it pip installable, it\u2019s here:\n\n[https://github.com/JaimeArboleda/nestedcvtraining](https://github.com/JaimeArboleda/nestedcvtraining)\n\nSo we plan to use it with the typical models like XGBoost, LightGBM and all Scikit-Learn. And when we need other frameworks like TensorFlow or Keras, we will see.\n\n**Model Registry**\n\nI think it\u2019s an important piece, although I don\u2019t know if we even could build our own with an standard database. If not, MLFlow seems a mature option.\n\n**Model serving**\n\nI am not sure if it\u2019s included in the previous point or not. Anyway, I have read about Streamlint and FastAPI. Would you recommend any of those?\n\nIs Apache Kafka needed for real time predictions?\n\n**Visualization**\n\nWith this I mean sharing with the organization basic web apps with customizable plots, explainable predictions and things like that. I have read about panel, which has the ability of transform a Jupyter Notebook into a simple web app. It might be interesting.\n\n**Model monitoring**\n\nIs there a good free tool for monitoring the models and detecting loss of accuracy, data drift and things like that? Or we should better generate our own script of monitoring to be run periodically?\n\n**BigData**\n\nAs I said before, we plan to use mainly Spark when need.\n\nI know it\u2019s a lot of info. Maybe I have overcomplicated myself and I should use only 20% of what I think I should. Or maybe not. I have no idea. Any help will be GREATLY appreciated. Thanks in advance.", "link": "https://www.reddit.com/r/MachineLearning/comments/mfca0p/d_whats_the_simplest_most_lightweight_but/"}, {"autor": "brianc96", "date": "2021-03-28 20:49:34", "content": "[P] Predict size of dog /!/ Hello! I have a task which is to measure the size of a dog (length, height, girth) and the information I will have for each dog is:\n\n\\- Race\n\n\\- Weight\n\n\\- Age (date of birth)\n\n\\- Gender and\n\n\\- Frontal and lateral -----> picture !!! \n\nDo you know any way to conduct this prediction with machine learning? Also, I've tried coding a decision tree to predict the size with scientific data, but I couldn't yet find a dataset.", "link": "https://www.reddit.com/r/MachineLearning/comments/mf9mkt/p_predict_size_of_dog/"}, {"autor": "brianc96", "date": "2021-03-28 20:44:45", "content": "[D] Predict size of dog /!/ Hello! I have a task which is to measure the size of a dog (length, height, girth) and the information I will have for each dog is:\n\n\\- Race\n\n\\- Weight\n\n\\- Age (date of birth)\n\n\\- Gender and\n\n\\- Frontal and lateral -----> picture !!! \n\nDo you know any way to conduct this prediction with machine learning? Also, I've tried coding a decision tree to predict the size with scientific data, but I couldn't yet find a dataset.", "link": "https://www.reddit.com/r/MachineLearning/comments/mf9j5a/d_predict_size_of_dog/"}, {"autor": "NotAHomeworkQuestion", "date": "2021-03-28 20:12:57", "content": "[D] Instead of taking an approach like Invariant Risk Minimization, why is it not enough to control for environmental factors (confounders) by including them as regressors? /!/ I've only just started diving into this fascinating topic so please excuse my ignorance. I really enjoyed reading the IRM paper but it left me wondering why we couldn't accomplish something similar by including the environmental variables as regressors as people do in causal inference? For example, in the MNIST coloring application, we could have the final layer of our model take the top layer of the usual plain-vanilla CNN as well as an indicator for the color of the -----> image !!! . We have thus 'controlled for' color confounding in our image so that the CNN part of our model architecture accounts for everything but that. As we are worried about shenanigans with future data points having the effect of color being reversed, we thus create prediction on future data points by ignoring the color effect on the prediction. This (I think) would give similar results to graying out the image which was shown to give excellent performance. What am I missing here?", "link": "https://www.reddit.com/r/MachineLearning/comments/mf8w4k/d_instead_of_taking_an_approach_like_invariant/"}, {"autor": "arbabahmed", "date": "2021-03-28 20:09:30", "content": "I made an art -----> film !!!  revolving around Lucid Sonic Dreams! Feedback is welcome! [P]", "link": "https://www.reddit.com/r/MachineLearning/comments/mf8tj0/i_made_an_art_film_revolving_around_lucid_sonic/"}, {"autor": "[deleted]", "date": "2021-03-28 20:05:09", "content": "I made an art -----> film !!!  revolving around Lucid Sonic Dreams! Feedback is welcome! /!/ [deleted]", "link": "https://www.reddit.com/r/MachineLearning/comments/mf8q9w/i_made_an_art_film_revolving_around_lucid_sonic/"}, {"autor": "SensitiveAnteater420", "date": "2021-03-28 19:41:46", "content": "[D] Would it be possible to make a model that predicts where a -----> picture !!!  is taken? /!/ I love OSINT and geolocating, and I'm a Python dev wondering if I can extend my boundaries. The title is self explaining: Is it possible to make a model that predicts where a picture is taken, and are there existing datasets for it?", "link": "https://www.reddit.com/r/MachineLearning/comments/mf88ky/d_would_it_be_possible_to_make_a_model_that/"}, {"autor": "hash_t", "date": "2021-03-28 08:02:34", "content": "[P] -----> image !!!  recognition model (EfficientNet) written in typescript", "link": "https://www.reddit.com/r/MachineLearning/comments/mex5b1/p_image_recognition_model_efficientnet_written_in/"}, {"autor": "ancientmooner", "date": "2021-03-28 04:15:33", "content": "[R] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows /!/ A general-purpose Transformer backbone for various vision tasks \n\n[Arxiv](https://arxiv.org/pdf/2103.14030.pdf)    [GitHub](https://github.com/microsoft/Swin-Transformer)\n\nMain Results:\n\nCOCO test-dev box AP 58.7 [paper-with-code](https://paperswithcode.com/sota/object-detection-on-coco)\n\nCOCO test-dev mask AP 51.1 [paper-with-code](https://paperswithcode.com/sota/instance-segmentation-on-coco)\n\nADE20K semantic segmentation val mIoU 53.5 [paper-with-code](https://paperswithcode.com/sota/semantic-segmentation-on-ade20k-val)\n\nAbstract:\n\nThis paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to -----> image !!!  size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (86.4 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones.", "link": "https://www.reddit.com/r/MachineLearning/comments/meu9ox/r_swin_transformer_hierarchical_vision/"}, {"autor": "cloud_weather", "date": "2021-01-03 15:25:05", "content": "[D] ArtLine - transforms any -----> Image !!!  into Sketch/Line Art with this AI model", "link": "https://www.reddit.com/r/MachineLearning/comments/kpmd9z/d_artline_transforms_any_image_into_sketchline/"}, {"autor": "cloud_weather", "date": "2021-01-03 15:24:42", "content": "ArtLine - transforms any -----> Image !!!  into Sketch/Line Art with this AI model", "link": "https://www.reddit.com/r/MachineLearning/comments/kpmd12/artline_transforms_any_image_into_sketchline_art/"}, {"autor": "michaelm2391", "date": "2021-01-02 21:32:05", "content": "[P] Need help with deep learning for my deceased friend /!/ I know it's not de rigeur to do solicitations, but I'm really at a loose end. My friend took his own life in October, and since then I've tried to piece together what remains of his last music video. His mother is understandably shattered so I feel this would be a nice gesture to her and all his friends. And in my opinion, the music's pretty good, so that might be extra incentive.\n\nHere's the brief: Create a Virtual Box disk -----> image !!!  with full python development environment and a working implementation of the Google Deep Dream algorithm set up to process a long -----> image !!!  sequence (to be provided by me.) \n\nThe project will need to contain a single configuration file to allow a non-technical user to make changes to the processing settings to experiment with different outputs. This github repo would be a suitable starting place: https://github.com/martinkaptein/deepdream(https://www.youtube.com/watch?v=JPMQB2zjtv0&amp;t=180s)\n\n\nDeliverables: Virtual Box Disk Image to run on Windows 10, with functional Google Deep Dream project set up to process a folder of jpeg images and output another set of jpeg images. Configuration settings to be located in a single simple settings config text file", "link": "https://www.reddit.com/r/MachineLearning/comments/kp68e4/p_need_help_with_deep_learning_for_my_deceased/"}, {"autor": "insaneinthedeepbrain", "date": "2021-01-02 13:40:21", "content": "[D] How to study bias in neural networks /!/ I recently became interested in investigating biases (e.g. race, gender, etc.) that arise in neural networks trained with -----> image !!!  data. One approach could be to look at the underlying data and see how evenly sampled certain categories are and what the images of each category show specifically (e.g. think of a category \"doctor\" that only shows white people). Another approach could be to look at the trained network and study its features. Is there any established method to check for bias or are there any diverse datasets that can help to characterize the behavior of a model? Are there potentially even metrics to quantify HOW biased a model is?", "link": "https://www.reddit.com/r/MachineLearning/comments/koxmkw/d_how_to_study_bias_in_neural_networks/"}, {"autor": "Lairv", "date": "2021-01-05 22:50:15", "content": "[D] Model which finds differences between two images /!/ I'm looking for a dataset/deep learning model which can find differences between two input images which are known to be \"similar\". This is not very clear so let me give an example : for example a model which takes two images from a surveillance -----> camera !!!  at 2 different times of the day, and output a mask image of the differences between the two images (for example if there is someone on the second image and not the first, this person should be highlighted in the mask). The model should be 'bullet-proof' to small variation in light intensity and stuff like this, and that's why i think deep learning is required for this task.\n\nIf you have anything on datasets, models or anything which could help me don't hesitate.", "link": "https://www.reddit.com/r/MachineLearning/comments/kr9y45/d_model_which_finds_differences_between_two_images/"}, {"autor": "Yuqing7", "date": "2021-01-05 20:05:51", "content": "[R] \u2018Neural Body\u2019 Reconstructs Dynamic Human Bodies From Sparse -----> Camera !!!  Views /!/ In a new paper, a group of researchers from Zhejiang University, The Chinese University of Hong Kong and Cornell University propose an implicit neural representation method called Neural Body. The novel approach tackles dynamic 3D human-body synthesis from a sparse set of camera views, bettering existing methods on key metrics by significant margins.\n\nHere is a quick read: [\u2018Neural Body\u2019 Reconstructs Dynamic Human Bodies From Sparse Camera Views](https://syncedreview.com/2021/01/05/neural-body-reconstructs-dynamic-human-bodies-from-sparse-camera-views/)\n\nThe paper *Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans* is on [arXiv](https://arxiv.org/pdf/2012.15838.pdf). The code and dataset will soon be available on the project [GitHub](https://github.com/zju3dv/neuralbody).", "link": "https://www.reddit.com/r/MachineLearning/comments/kr6hh1/r_neural_body_reconstructs_dynamic_human_bodies/"}, {"autor": "-Hypn0s", "date": "2021-01-05 16:17:54", "content": "[P] Augmented reality sudoku solver /!/ I created an AR sudoku solver which solves sudoku puzzles in real time with webcam.\n\n&amp;#x200B;\n\nGithub repo - \\[[https://github.com/melvin-02/ar-sudoku-solver](https://github.com/melvin-02/ar-sudoku-solver)\\]([https://github.com/melvin-02/ar-sudoku-solver](https://github.com/melvin-02/ar-sudoku-solver))\n\n&amp;#x200B;\n\nI saw this project a while back on reddit(don't remember the author)  and was impressed with it as I had only started learning computer vision at that time. I am still relatively new but thought I'll give it a try. I decided to first try it with only images and then work my way to webcam. I managed to create a sudoku -----> image !!!  solver, however I'd like to point out some mistakes I made:\n\n&amp;#x200B;\n\n1. I used to MNIST dataset for digits which was a bad idea, since the sudoku puzzles are mostly printed in computer font and not handwritten text. My model was always confusing 5s and 6s. Later I found out about the \\[chars74k image dataset\\]([http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/](http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/)) which contains computer characters as well as digits. After some preprocessing the images were ready to use.\n\n2. Instead of using a clean image try to work with an image with some noise, because with webcam there  will always be some noise and so this way you're preprocessing will be better.\n\n3. Making the sudoku solver as efficient as possible is very favourable as you'll probably be solving for every frame. The naive solver using backtracking might lead to performance issues.\n\n&amp;#x200B;\n\nAfter being able to work it out with images, I moved into the webcam part. Due to hardware limitations my system could not perform all the processing for every frame, which is why I decided to only solve the sudoku till its solved once and then warp the solution to the puzzle. Hence it warps the solution of the first puzzle onto any other subsequent puzzle it sees.\n\n&amp;#x200B;\n\nThe project is not perfect, however it feels great to be able to create something which I was amazed at and had no clue about some time before.\n\n&amp;#x200B;\n\n)\\[[h](https://i.redd.it/8b8ok94o4i961.gif))\n\nhttps://i.redd.it/bzw58sgwgj961.gif", "link": "https://www.reddit.com/r/MachineLearning/comments/kr1jlk/p_augmented_reality_sudoku_solver/"}, {"autor": "-Hypn0s", "date": "2021-01-05 16:02:49", "content": "[Project] Augmented reality sudoku solver /!/ I created an AR sudoku solver which solves sudoku puzzles in real time with webcam.\n\nGithub repo - [https://github.com/melvin-02/ar-sudoku-solver](https://github.com/melvin-02/ar-sudoku-solver)\n\nI saw this project a while back on reddit(don't remember the author)  and was impressed with it as I had only started learning computer vision at that time. I am still relatively new but thought I'll give it a try. I decided to first try it with only images and then work my way to webcam. I managed to create a sudoku -----> image !!!  solver, however I'd like to point out some mistakes I made:\n\n1. I used to MNIST dataset for digits which was a bad idea, since the sudoku puzzles are mostly printed in computer font and not handwritten text. My model was always confusing 5s and 6s. Later I found out about the [chars74k image dataset](http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/) which contains computer characters as well as digits. After some preprocessing the images were ready to use.\n2. Instead of using a clean image try to work with an image with some noise, because with webcam there  will always be some noise and so this way you're preprocessing will be better.\n3. Making the sudoku solver as efficient as possible is very favorable as you'll probably be solving for every frame. The naive solver using backtracking might lead to performance issues.\n\nAfter being able to work it out with images, I moved into the webcam part. Due to hardware limitations my system could not perform all the processing for every frame, which is why I decided to only solve the sudoku till its solved once and then warp the solution to the puzzle. Hence it warps the solution of the first puzzle onto any other subsequent puzzle it sees.\n\nThe project is not perfect, however it feels great to be able to create something which I was amazed at and had no clue about some time before.\n\n[Demo](https://i.redd.it/8b8ok94o4i961.gif)", "link": "https://www.reddit.com/r/MachineLearning/comments/kr17wd/project_augmented_reality_sudoku_solver/"}, {"autor": "Another__one", "date": "2021-01-05 11:06:21", "content": "[P] -----> Image !!!  morphing without reference points by applying warp maps and optimizing over them.", "link": "https://www.reddit.com/r/MachineLearning/comments/kqwcqx/p_image_morphing_without_reference_points_by/"}, {"autor": "sergeyfeldman", "date": "2021-01-05 00:30:24", "content": "[P] Which Machine Learning Classifiers are best for small datasets? An empirical study /!/ Although \"big data\" and \"deep learning\" are dominant, my own work at the Gates Foundation involves a lot of small (but expensive) datasets, where the number of rows (subjects, samples) is between 100 and 1000. For example, detailed measurements throughout a pregnancy and subsequent neonatal outcomes from pregnant women. A lot of my collaborative investigations involve fitting machine learning models to small datasets like these, and it's not clear what best practices are in this case.\n\nAlong with my own experience, there is some informal wisdom floating around the ML community. Folk wisdom makes me wary and I wanted to do something more systematic. I took the following approach:\n\n* Get a lot of small classification benchmark datasets. I used a subset of this prepackaged repo. The final total was 108 datasets. (To do: also run regression benchmarks using this nice dataset library.)\n* Select some reasonably representative ML classifiers: linear SVM, Logistic Regression, Random Forest, LightGBM (ensemble of gradient boosted decision trees), AugoGluon (fancy automl mega-ensemble).\n* Set up sensible hyperparameter spaces.\n* Run every classifier on every dataset via nested cross-validation.\n* Plot results.\n\nAll the code and results are here: https://github.com/sergeyf/SmallDataBenchmarks\n\nLet's look at the results. The metric of interest is weighted one-vs-all area under the ROC curve, averaged over the outer folds. The [plot](https://user------> image !!! s.strikinglycdn.com/res/hrscywv4p/-----> image !!! /upload/c_limit,fl_lossy,h_9000,w_1200,f_auto,q_auto/174108/405478_658788.png)\n\nSome observations:\n\n* AutoGluon is best overall, but it has some catastrophic failures (AUROC &lt; 0.5) that Logistic Regression does not and LightGBM has fewer of.\n* You can't tell from this particular plot, but AutoGluon needs \"enough\" time. It has a budget parameter which tells it how much time to spend improving the fancy ensemble. Five minutes per fold was the minimum that worked well - this adds up to 108 datasets * 4 outer folds * 300s = 1.5 days for the entire benchmark.\n* Linear SVC is better than Logistic Regression on average. There are also two datasets where SVC is 0.3 and 0.1 AUROC better than every other model. It's worth keeping in the toolbox.\n* Logistic Regression needs the \"elasticnet\" regularizer to ensure it doesn't have the kind of awful generalization failures that you see with AutoGluon and Random Forest.\n* LightGBM is second best. I used hyperopt to find good hyperparameters. I also tried scikit-optimize and Optuna, but they didn't work as well. User error is possible.\n* Random Forest is pretty good, and much easier/faster to optimize than LightGBM and AutoGluon. I only cross-validated a single parameter for it (depth).\n\nHere are counts of datasets where each algorithm wins or is within 0.5% of winning AUROC (out of 108):\n\n* AutoGluon (sec=300): 71\n* LightGBM (n_hyperparams=50): 43\n* LightGBM (n_hyperparams=25): 41\n* Random Forest: 32\n* Logistic Regression: 28\n* SVC: 23\n\nAnd average AUROC across all datasets:\n\n* AutoGluon (sec=300) - 0.885\n* LightGBM (n_hyperparams=50) - 0.876\n* LightGBM (n_hyperparams=25) - 0.873\n* Random Forest - 0.870\n* SVC - 0.841\n* Logistic Regression - 0.835\n\nAnd counts where each algorithm does the worst or is within 0.5% of the worst AUROC:\n\n* Logistic Regression: 54\n* SVC: 48\n* Random Forest: 25\n* LightGBM (n_hyperparams=25): 19\n* LightGBM (n_hyperparams=50): 18\n* AutoGluon (sec=300): 14\n\nWhich shows that even the smart ensemble can still fail 10% of the time. Not a single free lunch to be eaten anywhere.\n\n[Here](https://user-images.strikinglycdn.com/res/hrscywv4p/image/upload/c_limit,fl_lossy,h_9000,w_1200,f_auto,q_auto/174108/900727_648400.png) is a plot of average (over folds) AUROC vs number of samples.\n\nI was surprised when I saw this for the first time. The collective wisdom that I've ingested is something like: \"don't bother using complex models for tiny data.\" But this doesn't seem true for these 108 datasets. Even at the low end, AutoGluon works very well, and LightGBM/Random Forest handily beat out the two linear models. There's an odd peak in the model where the linear models suddenly do better - I don't think it's meaningful.\n\nThe last [plot](https://user-images.strikinglycdn.com/res/hrscywv4p/image/upload/c_limit,fl_lossy,h_9000,w_1200,f_auto,q_auto/174108/378656_205907.png): standard deviation of AUROC across outer folds.\n\nLinear models don't just generalize worse regardless of dataset size - they also have higher generalization variance. Note the one strange SVC outlier. Another SVC mystery...\n\n**IID Thoughts**\n\nHow applicable are these experiments? Both levels of the nested cross-validation used class-stratified random splits. So the splits were IID: independent and identically distributed. The test data looked like the validation data which looked like the training data. This is both unrealistic and precisely how most peer-reviewed publications evaluate when they try out machine learning. (At least the good ones.) In some cases, there is actual covariate-shifted \"test\" data available. It's possible that LightGBM is better than linear models for IID data regardless of its size, but this is no longer true if the test set is from some related but different distribution than the training set. I can't experiment very easily in this scenario: \"standard\" benchmark datasets are readily available, but realistic pairs of training and covariate-shifted test sets are not.\n\n**Conclusions &amp; Caveats**\n\nSo what can we conclude?\n\n* If you only care about the IID setting or only have access to a single dataset, non-linear models are likely to be superior even if you only have 50 samples.\n* AutoGluon is a great way to get an upper bound on performance, but it's much harder to understand the final complex ensemble than, say, LightGBM where you can plot the SHAP values.\n** hyperopt is old and has some warts but works better than the alternatives that I've tried. I'm going to stick with it.\n** SVC can in rare cases completely dominate all other algorithms.\n\nCaveats:\n\n* LightGBM has a lot of excellent bells and whistles that were not at all used here: native missing value handling (we had none), smarter encoding of categorical variables (I used one-hot encoding for the sake of uniformity/fairness), per-feature monotonic constraints (need to have prior knowledge).\n* AutoGluon includes a tabular neural network in its ensemble, but I haven't run benchmarks on it in isolation. It would be interesting to find out if modern tabular neural network architectures can work out-of-the-box for small datasets.\n* This is just classification. Regression might have different outcomes.\n\nAgain, check out the code and feel free to add new scripts with other algorithms. It shouldn't be too hard. https://github.com/sergeyf/SmallDataBenchmarks", "link": "https://www.reddit.com/r/MachineLearning/comments/kqm5pn/p_which_machine_learning_classifiers_are_best_for/"}, {"autor": "q-rka", "date": "2021-04-04 06:05:01", "content": "How do I generate Forged signature -----> image !!!  using Genuine? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/mjqfi2/how_do_i_generate_forged_signature_image_using/"}, {"autor": "LouisTheCowboy", "date": "2021-04-04 01:12:33", "content": "[D] Hi r/MachineLearning! I don't know much about ML but I was wondering if any of you knew of a program that could automatically center my face on a selfie? /!/ I've been doing one of those things where you take a -----> picture !!!  every day and the -----> picture !!! s have started to pile up with me putting the aligning process off (I'm at day 752 ;\\_;). I was told you guys may be able to give me a hand using one of your funky algorithms?", "link": "https://www.reddit.com/r/MachineLearning/comments/mjlspw/d_hi_rmachinelearning_i_dont_know_much_about_ml/"}, {"autor": "Ok_Reality2341", "date": "2021-04-03 22:07:28", "content": "[D] How large can you go with CNN input images? /!/ How large can you reasonably go with CNNs in regards to the input -----> image !!! ? I am training on a week worth of accelerometer signal data (at 100 data points per second) converted to a spectrogram. So, the higher the resolution of the spectrogram the better.\n\nIs something like 6000x100 feasible?\n\nI tried on a 600x100 image and it wasn't enough resolution to pick up the details of the full week. Any papers that relate to using CNN on large images is also a plus.", "link": "https://www.reddit.com/r/MachineLearning/comments/mjiges/d_how_large_can_you_go_with_cnn_input_images/"}, {"autor": "Ok_Reality2341", "date": "2021-04-03 22:05:25", "content": "Largest -----> image !!!  size to train a CNN on? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/mjif07/largest_image_size_to_train_a_cnn_on/"}, {"autor": "Fancy-Stress-806", "date": "2021-04-03 16:50:08", "content": "[D] How to reshape my data for CNN forecasting? /!/ Hey guys! I am trying to train a CNN to do an -----> image !!! -to------> image !!!  forecasting problem for a rainfall/storm event. For simplicity, let's look at Event N and for this event, we have 10 timesteps of 110 x 110 images representing the rainfall intensities at each location in space. Therefore, Event N can be described by a 110 x 110 x 10 3D matrix with the dimensions corresponding x, y and time respectively.\n\nSo I am trying to format the 110 x 110 x 10 matrix into the following:\n\n* Set 1: Image t0 + Image t1 ---&gt; Forecast Image t2\n* Set 2: Image t1 + Image t2 ---&gt; Forecast Image t3\n* Set 3: Image t2 + Image t3 ---&gt; Forecast Image t4\n* until...\n* Set 8: Image t8 + Image t9 ---&gt; Forecast Image t10 \n\nwhere each image t is a 110 x 110 matrix\n\nOnce I have the data in this manner, I will split it using train\\_test\\_split in Python. So for example if I want to do an 80/20 split for training and testing: then Sets 1 to 6 will be used for training and Sets 7 to 8 will be used for testing. My problem is that I don't know how to reshape the 110 x 110 x 10 3D matrix into the 'sets' format I have described above in Python. Does anyone know how to do this? I'm fairly certain its quite do-able but I'm just not sure how :(\n\nThanks in advance guys! :D", "link": "https://www.reddit.com/r/MachineLearning/comments/mjc9mn/d_how_to_reshape_my_data_for_cnn_forecasting/"}, {"autor": "binaryfor", "date": "2021-04-03 16:48:50", "content": "[P] Deep Daze - A simple command line tool for text to -----> image !!!  generation using OpenAI's CLIP and Siren", "link": "https://www.reddit.com/r/MachineLearning/comments/mjc8q4/p_deep_daze_a_simple_command_line_tool_for_text/"}, {"autor": "binaryfor", "date": "2021-04-03 16:39:33", "content": "Deep Daze - A simple command line tool for text to -----> image !!!  generation using OpenAI's CLIP and Siren", "link": "https://www.reddit.com/r/MachineLearning/comments/mjc2d4/deep_daze_a_simple_command_line_tool_for_text_to/"}, {"autor": "BryanP1412", "date": "2021-04-06 23:34:05", "content": "[R] Develop of Neural Network for Res-feats /!/ Hello everyone, i am working in reproduction of results of the paper \" ResFeats: Residual Network Based Features for Image Classification\"\n\nI need to develop the next NN:\n\n\"The first method involves implementing a shallow CNN network with one convolutional layer, one max-pooling layer and two fully-connected  (FC) layers. The first convolutional layer consists of small filters (i.e. 1 x 1) along 512 channels. This layer reduces the dimension of Res5c to 7 x 7 x 512. The stride is set to 1 and the padding is set to zero for the convolutional layer. This layer is then followed by a max-pooling layer, two FC layers and a soft-max layer for classification\".\n\nI developed the NN in keras but i don\u00b4t know if is correctly. In the paper say that the output will be a vector of dimension 4096. I attach a -----> image !!!  of the structure of my NN developed.\n\nI appreciated your help, comment or whatever.", "link": "https://www.reddit.com/r/MachineLearning/comments/mloyh3/r_develop_of_neural_network_for_resfeats/"}, {"autor": "KirillTheMunchKing", "date": "2021-04-09 16:27:11", "content": "[R] ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement - Explained /!/ A great idea to improve StyleGAN inversion for complex real images that builds on top of the recent e4e and pSp papers.\n\nThe authors propose a fast iterative method of -----> image !!!  inversion into the latent space of a pretrained StyleGAN generator that acheives SOTA quality at a lower inference time. The core idea is to start from the average latent vector in W+ and predict an offset that would make the generated image look more like the target, then repeat this step with the new image and latent vector as the starting point. With the proposed approach a good inversion can be obtained in about 10 steps. More details [here](https://t.me/casual_gan/24)  \n\n\n[The inversions are awesome!](https://preview.redd.it/sa63gu0dc6s61.png?width=1106&amp;format=png&amp;auto=webp&amp;s=56a647abbb5b1bb0a44f6c75e5fd78bb81ec5858)\n\n P.S. In case you are not familiar with the paper check it out [here](https://t.me/casual_gan/24):", "link": "https://www.reddit.com/r/MachineLearning/comments/mnkz1f/r_restyle_a_residualbased_stylegan_encoder_via/"}, {"autor": "mavavilj", "date": "2021-07-03 14:30:40", "content": "[D] If using sliced data, then should I use the most informative perspectives or other perspectives? /!/ I'm trying to reassure that my intuition here is correct.\n\nI have sliced data, where between there are intervals of \"missing data\".\n\nIn order to do feature extraction, I've had to think about:\n\n* Should I study the \"whole data\" or the direction(s) with most data?\n\nSo consider e.g. the following -----> picture !!! , which may be XYZ-data viewed top-down or XY-data (but which we may orientate differently by rotating the data).\n\n     ..  .. ...  .. \n     ------gap-----\n    y.  .  . . . ..  &lt;===  view is useful, but bounded in accuracy.\n     ------gap-----\n     . .. ..   . ..\n           x      \\\n          /\\       \\ viewing from here pointless, since data incomplete? \n          ||         It's possible, but does it give distorted results?\n          ||\n    most complete data visible from here?\n\nAs one sees, most information is visible from the bottom. So if one fits models etc. by having the origin or viewpoint there, then one applies it to most information. Another informative view is the one perpendicular to this view. It has gaps, but again it has true data on those slices between \"missing data\".\n\nOTOH, if one views the data from some other point, then one vill get a \"view\" where one sees the data, but one cannot be sure whether it resembles \"true features\"?\n\nSo if one e.g. fits a linear model along the two most informative views vs some other view, then do the missing data segments distort the interpretations for other than \"most informative\" viewpoints?", "link": "https://www.reddit.com/r/MachineLearning/comments/ocz4z8/d_if_using_sliced_data_then_should_i_use_the_most/"}, {"autor": "dynamicopera", "date": "2021-07-03 03:45:53", "content": "Anyone implemented models from cvpr 2021 for -----> image !!!  (non-live) segmentation?", "link": "https://www.reddit.com/r/MachineLearning/comments/ocqc32/anyone_implemented_models_from_cvpr_2021_for/"}, {"autor": "shreyansh26", "date": "2021-01-26 07:50:40", "content": "[P] Tutorial on deploying Deep Learning models in the browser using Tensorflow.js, WebDNN, and ONNX.js (Includes code) /!/ [https://shreyansh26.github.io/post/2021-01-25\\_deep\\_learning\\_in\\_the\\_browser/](https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/)\n\nI wrote a tutorial on how one can use frameworks like Tensorflow.js, WebDNN, and ONNX.js to deploy their Deep Learning models directly in the browser, i.e. no server communication. I have used a simple pre-trained -----> Image !!!  classification model to demonstrate it. The project can serve as a boilerplate code for starting out on deploying models in such a manner on your own.\n\n Kindly upvote if you like it! Feedbacks are also welcome :)", "link": "https://www.reddit.com/r/MachineLearning/comments/l5988m/p_tutorial_on_deploying_deep_learning_models_in/"}, {"autor": "aniketmaurya", "date": "2021-01-26 05:32:36", "content": "Bag of tricks for -----> Image !!!  classification", "link": "https://www.reddit.com/r/MachineLearning/comments/l577hl/bag_of_tricks_for_image_classification/"}, {"autor": "Wiskkey", "date": "2021-01-26 01:18:57", "content": "[P] Use natural language queries to search 2 million freely-usable images from Unsplash using a free Google Colab notebook from Vladimir Haltakov. Uses OpenAI's CLIP neural network. /!/ [Google Colab notebook](https://colab.research.google.com/github/haltakov/natural-language------> image !!! -search/blob/main/colab/unsplash------> image !!! -search.ipynb):\n\n&gt;Unsplash Image Search  \n&gt;  \n&gt;Using this notebook you can search for -----> image !!! s from the [Unsplash Dataset](https://unsplash.com/data) using natural language queries. The search is powered by OpenAI's [CLIP](https://github.com/openai/CLIP) neural network.  \n&gt;  \n&gt;This notebook uses the precomputed feature vectors for almost 2 million images from the full version of the [Unsplash Dataset](https://unsplash.com/data). If you want to compute the features yourself, see [here](https://github.com/haltakov/natural-language-image-search#on-your-machine).  \n&gt;  \n&gt;This project was created by [Vladimir Haltakov](https://twitter.com/haltakov) and the full code is open-sourced on [GitHub](https://github.com/haltakov/natural-language-image-search).\n\n[Unsplash license](https://unsplash.com/license).\n\nI'll create instructions in this post if anybody asks.\n\nI am not affiliated with this project or its developer.", "link": "https://www.reddit.com/r/MachineLearning/comments/l52qe6/p_use_natural_language_queries_to_search_2/"}, {"autor": "Stanley_C", "date": "2021-01-26 00:58:55", "content": "[D] What's the successor to pix2pixHD /!/ While pix2pixHD isn't very old, it's been a bit of time since it was published. What are newer examples of -----> image !!!  to -----> image !!!  translation on the 1k+ resolution that were published after pix2pixHD?", "link": "https://www.reddit.com/r/MachineLearning/comments/l52cwu/d_whats_the_successor_to_pix2pixhd/"}, {"autor": "rexlow0823", "date": "2021-01-25 15:54:33", "content": "[D] Detecting patches/forgery in an -----> image !!!  /!/ Hi guys. I am working on a project where I am interested to find if there is an physical patches exists in an image. \n\nAdobe has a similar [work](https://blog.adobe.com/en/publish/2018/06/22/spotting-image-manipulation-ai.html) detecting digital manipulation where it would study the hierarchical artefacts such as noise patterns in the RGB channels.\n\nIn my domain however, I am only interested in finding visible patches. For instance, I would crop a person\u2019s face and paste it on another image on another person\u2019s head. The dataset I have is a set of images with a static blue background, they look roughly the same unless they are projected in severely different lighting conditions.\n\nMy initial idea was to identity the area of interests first, then make a dataset of true/false images. Where the false image set contains images of patches. Then simply build a binary classifier out of this. \n\nDoes anyone has a better way of tackling this properly? Thanks in advanced!", "link": "https://www.reddit.com/r/MachineLearning/comments/l4qlpi/d_detecting_patchesforgery_in_an_image/"}, {"autor": "rexlow0823", "date": "2021-01-25 15:51:38", "content": "Detect patches/forgery in an -----> image !!!  /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/l4qjgf/detect_patchesforgery_in_an_image/"}, {"autor": "kalloszsolty", "date": "2021-01-25 12:55:47", "content": "[P] Rhyme Camera - Smart Rap Generator (Android) /!/ I recently created \"**Rhyme -----> Camera !!! ** \\- Smart Rap Generator\" Android app that raps about what you see through the camera. I use **MLKit** object labeling I extract the relevant words from the surrounding. Then I use existing poetry/lyrics that I collected through the web. What do you think about it?", "link": "https://www.reddit.com/r/MachineLearning/comments/l4nbf6/p_rhyme_camera_smart_rap_generator_android/"}, {"autor": "Goodd0ctor", "date": "2021-01-23 19:21:06", "content": "[RESEARCH] My Experience as an Undergraduate AI Researcher /!/ If there is one thing that I have taken away from my first 3 years studying at university, it\u2019s that time is the most important resource that we all have. For some of us time goes by way too fast while for others it seems as if the hour just won\u2019t pass by. While I fall into the first category, I\u2019d like to believe that at one point in time everyone starts to wish that there were more hours in a day to do the things that matter to them. I also like to believe that a lot of people feel the pain of doing things in a day that just don\u2019t really matter in the big -----> picture !!! , but still have to get done. Although it\u2019s not possible to create more hours, there are ways to cut down the time that certain things take in a day in order to pursue the more important things in life. I\u2019d like to share my experience in doing research in AI that led me to learning the importance of time.\n\nIn my 2nd year of university, I took on research for both the computational brain lab and machine learning (ML) lab at my school. I loved doing research for both labs, and devoted countless hours a week to it even while in school. During university breaks I would often devote full time hours to research. A common mistake that lots of people make is overcommitting to things and feeling like they can get a lot more things done than they actually can. That was me. I thought that at the same time of excelling in my research that I could also spend time learning the skills needed to build a software business one day. Although I loved research, and still do, there came a point where I started to wish the hours spent on it could be spent on learning and honing the different skills that mattered to me in the bigger picture of things. This was my first lesson learned in my time doing research; excellent research takes a lot of time and is not something that can be done on the side.\n\nSince I was younger, I dreamed of creating my own business one day and nurturing it like a baby, then seeing it mature in its later phases. My friend shared the same dream as I, but on a more practical level. While I was doing research and *dreaming* of starting a business one day, my friend was *learning* *how* to start a business one day. I couldn\u2019t take it anymore, and realized that although I loved research, to me it ought to just be a tool that could be used to pursue my bigger dream. So I quit research and buried any plans on pursuing more schooling after my undergraduate studies end.\n\nWith the addition of so many hours to my day after ceasing my research, I started to finally pick up the skills that mattered for my dream of starting a business. I read countless books, finally learned how to build a website and scale it, and I was happy. At the same time, I realized that I should probably get some experience in the industry and see how other great businesses thrive before starting my own one day, so I started to prepare for internships. I applied to hundreds of them and realized that even now I was losing a lot of time from both applying to and worrying about getting an internship for the summer. We all know what this stress is like, seeing so many other people get the internships that you wish you had and seeing so many posts on LinkedIn of other peoples\u2019 success, then filling in the same old information on seemingly worthless applications that would most likely get ghosted.\n\nTo alleviate this pain point, my friend suggested that we work on a service that could make the job application process easier and faster. Remember, if there is one thing that matters it is time. My friend and I are pursuing this idea of a service that will be able to save hours a week for people looking for jobs and internships as well as bring them peace of mind in the entire stressful process. This way, people applying to jobs will be able to spend more of their time doing the things they love or learning the skills that actually matter to them, rather than filling in the same old information on applications and worrying about the places that have ghosted or rejected them or not.\n\nWe're planning to create the app along side our beta users, so if you are interested in a tool like this and would like to be a part of this process, sign up for the beta app and email list at [http://beehired.io/](http://beehired.io/)! Stay safe and love yourself. You're exactly where you need to be.", "link": "https://www.reddit.com/r/MachineLearning/comments/l3if7e/research_my_experience_as_an_undergraduate_ai/"}, {"autor": "somethingstrang", "date": "2021-01-23 19:13:43", "content": "[P] What is current SOTA for a really detailed -----> image !!!  similarity search? /!/ For example, if I had a set of reference shirts and each shirt had a different style. If I wanted to do a reverse image similarity search on those shirts, what is the current SOTA that would achieve the best accuracy?", "link": "https://www.reddit.com/r/MachineLearning/comments/l3i9do/p_what_is_current_sota_for_a_really_detailed/"}, {"autor": "somethingstrang", "date": "2021-01-23 19:12:24", "content": "What is the state of the art model for really detailed -----> image !!!  similarity? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/l3i848/what_is_the_state_of_the_art_model_for_really/"}, {"autor": "OnlyProggingForFun", "date": "2021-01-23 15:31:32", "content": "[News] THE AI-POWERED ONLINE FITTING ROOM: VOGUE. A game-changer for online shopping and -----> photography !!! ? Let me know what you think! (video demo)", "link": "https://www.reddit.com/r/MachineLearning/comments/l3e42b/news_the_aipowered_online_fitting_room_vogue_a/"}, {"autor": "elcric_krej", "date": "2021-08-26 20:38:25", "content": "[D] I don't understand AlphaFold /!/ The alpha fold paper recently got officially published in [nature](https://www.nature.com/articles/s41586-021-03819-2.pdf) . I tried reading it, then tried reading the code on Github, then tired reading [various articles](https://www.blopig.com/blog/2021/07/alphafold-2-is-here-whats-behind-the-structure-prediction-miracle/) explaining it. Plus the original discussion [here from November last year](https://old.reddit.com/r/MachineLearning/comments/k3ygrc/r_alphafold_2/). \n\nI'm still super confused about what's happening in it, so I thought I'd write down what I understand in order to explain my confusion and maybe someone here is both very bored, very familiar with the subject, and might take the time to correct some of my horrible views.\n\nAs far as I can tell:\n\nAlpha fold takes the sequence of a given protein as input. But it then processes that via some \"undifferentiable\" database lookups into:\n\n1. MSA data, which essentially represents the original protein and presumed variations of that protein across multiple species. So you've got the protein in humans expressed as a function of similar proteins in yeast, parrots, and fish. Then the protein in yeast is expressed as a function of similar proteins in humans, parrots, and fish... etc\n\n2. A pair(wise) representation which as far as I understand is:\n&gt; Identifying proteins that may have a similar structure to the input (\u201ctemplates\u201d), and constructing an initial 2D representation of the structure. This is, in essence, a model of which amino acids are likely to be in contact with each other.\n\n  \n\nOnce this happens there are two modules. One formed of \"evoformer\" blocks that output a single MSA representation and a pair representation. Both of these representations are then fed to the next block *and* feed into a \"recycling\" process that makes the whole differentiable part of the model recurrent.\n\n  \n\nThe final module determines the actual 3d structure (coordinates of all non-hydrogen atoms) from the single MSA and the pairwise representation. This 3d structure is also recycled.\n\n  \n\nThe whole cycle repeats a bunch of times and we end up with a final 3rd structure.\n\n  \n\nThe interesting (?) feature of the evoformer transformer block is that it's structured in such a way to encourage/force certain constraints between the pairwise representation and the MSA representation, as well as constraints regarding the pair representation it outputs. But it's unclear to me if this is just because of the operations being used, or if those operations simply encourage those geometric constraints to be imposed during backprop.\n\nI really want to say I understood this bit, but I don't think I did, so I refrain from explaining it further. Ref -----> image !!! :\n\n![https://i.imgur.com/O8TV4Sa.png](https://i.imgur.com/O8TV4Sa.png)\n\n  \n\nThe structure prediction block seems more straightforward, with the only caveat being that it generates a backbone representation (angles of certain structural elements) and an improved single-sequence MSA representation (single-sequence meaning the MSA features of the \"original\" protein rather than that + that of analogs in other reference organisms). Then it combined these two to form the final 3d structure.\n\n  \n\nThe recycling step is still mysterious to me. In that, I *get* how one could take the single-sequence MSA plus the pairwise representation and recycle those. I also get how one could take the 3d structure and generate the pairwise representation from that. But all 3 of these are feed into the recycling loop... So how exactly do these combine? Or are there 2 separate loops, one for the evoformer based module that uses its outputs and one for the whole network that uses the 3d Structures? Or am I missing something here? I [stared at the code](https://github.com/deepmind/alphafold/blob/main/alphafold/model/modules.py) for a while and I only got more confused.\n\n  \n\nAnother interesting note is that it attempts to optimize the structure locally (without global constraints imposed by the loss function, i.e. heavy penalties upon structures that are close in terms of coordinates but nonsensical due to physically impossible overlaps); Allowing for nonsensical structures from the perspective of a global loss. Global constraints upon the structure are only achieved afterward using a separate library (Amber). But I'm not sure I quite understand why this is possible and why making the global optimization differentiable wouldn't help.\n\n*Note: I copy-pasted and edited this form a [longer writeup with the same purpose](https://cerebralab.com/I_don't_understand_AlphaFold,_a_review), since I hope the crowd on this subreddit might be better informed than the people I'll send that to.  *\n\nIs my understanding here even close to reality? Am I missing some important bits? Are the questions I have even coherent?", "link": "https://www.reddit.com/r/MachineLearning/comments/pc81fk/d_i_dont_understand_alphafold/"}, {"autor": "PaganPasta", "date": "2021-08-26 15:25:19", "content": "[D] How does Tesla implement and use the rectify layer ? /!/ In the recent presentation by Andrej Karpathy for Tesla AI, he mentions inserting a layer to map all images from different view to a unified **synthetic** space. This layer is a function of -----> camera !!!  calibration parameters. What does it mean ? So instead of using camera parma to map each view to a known 3D space where you can then project points from each camera image, this projection is learnt for each view seprately?\n\n Along with it they show the visualisation which I honestly couldn't understand of the repeater camera view and positive impact of the rectification.\n\nPlease free to comment other AI day related queries here as well.", "link": "https://www.reddit.com/r/MachineLearning/comments/pc1qph/d_how_does_tesla_implement_and_use_the_rectify/"}, {"autor": "yusuf-bengio", "date": "2021-08-26 14:27:27", "content": "[D] The Taliban and AI /!/ Training and deploying a state-of-the-art -----> image !!!  recognition and other CV algorithms has never been easier than it is now.\n\nThis raises the question whether an authoritarian Taliban regime might exploit some of these techniques for oppressive purposes, e.g., detecting women who do not properly wear a burka. \n\nHow likely do you think such a scenario is, given your experience of how accessible these methods are? \n\nBtw. I am not talking about using existing tools such as Yandex's image search for identifying people, but new applications tailored to the Taliban.", "link": "https://www.reddit.com/r/MachineLearning/comments/pc0m90/d_the_taliban_and_ai/"}, {"autor": "Gann123", "date": "2021-08-25 23:39:14", "content": "[D] Are there any good free online text to -----> image !!!  services that are accurate? /!/ Are there any good free online text to image services that are accurate?", "link": "https://www.reddit.com/r/MachineLearning/comments/pbneb3/d_are_there_any_good_free_online_text_to_image/"}, {"autor": "Gann123", "date": "2021-08-25 23:38:40", "content": "Any good free online text to -----> image !!!  services? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pbndwo/any_good_free_online_text_to_image_services/"}, {"autor": "nonchalantno1", "date": "2021-08-25 21:12:11", "content": "[R] A Refiner model for avatars /!/ Hi, \n\nI recently wrote a paper on synthetic -----> image !!!  improvement, particularly useful to improve 2D or 3D generated avatars to create a photorealistic presentation:\n\n* Paper: [https://arxiv.org/abs/2108.04957](https://arxiv.org/abs/2108.04957)\n* Git: [https://github.com/consequencesunintended/RefinementGAN](https://github.com/consequencesunintended/RefinementGAN)", "link": "https://www.reddit.com/r/MachineLearning/comments/pbkka4/r_a_refiner_model_for_avatars/"}, {"autor": "420blazeSwag6969", "date": "2021-08-24 17:19:41", "content": "[D] Hardwiring a Neural Network /!/ Sorry if this post is off topic - not sure if it's allowed.\n\n&amp;#x200B;\n\n**TLDR: Is it possible to hardwire a neural network? Is there a notable performance benefit?**\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\n&amp;#x200B;\n\nLet's say I've trained a neural network to determine whether or not an -----> image !!!  contains a hot dog.\n\nWould it be possible to **hardwire the weights** into a circuit so that I can hook a sensor (camera) up to the circuit, take an image, and have, say, a **red** LED turn on if there is **not** a hot dog in the image and a **green** LED turn on if there **is**?\n\nWould there be a notable runtime performance by hardwiring in this manner, especially in contrast to using a GPU or TPU? Does it depend on the activation function - would ReLU be easier than sigmoid?\n\nI'm not sure if this is already done anywhere - if so I'd appreciate being pointed in the right direction or a the appropriate sub to post this in!", "link": "https://www.reddit.com/r/MachineLearning/comments/parsgd/d_hardwiring_a_neural_network/"}, {"autor": "PanSQ2021", "date": "2021-08-24 11:46:10", "content": "[D] Does it make sense to consider a speech signal as 2D input? /!/ I understand an -----> image !!!  could be considered 2D input (height and width). However, the link [http://deeplearning.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/](http://deeplearning.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/) refers to a speech signal as 2D input, does it make sense? Why?", "link": "https://www.reddit.com/r/MachineLearning/comments/palovo/d_does_it_make_sense_to_consider_a_speech_signal/"}, {"autor": "chrome_xyz", "date": "2021-08-24 11:44:05", "content": "[R] How can this be made? /!/ I found out this video online that really intrigues me: [https://www.instagram.com/p/CSzdPSvp\\_u2/](https://www.instagram.com/p/CSzdPSvp_u2/)\n\nI was wondering how I could achieve this effect in Google Collab (I own a MacBook and running this kind of stuff locally is imposible). \n\nI know the process to make a zoom in animation, just crop the initial -----> image !!! , feed it to the algorithm again and repeat the process. Then merge everything with ffmpeg But for zoom out, it's the same but with negative space in the borders? The same with the camera moving to different directions, is just cropping the image so it renders new stuff?\n\nI know that Visions of Chaos implemented a mode to do the zoom in videos, but that's all. Is there already a program that could do this kind of animations or a collab maybe?\n\nThank's in advance!", "link": "https://www.reddit.com/r/MachineLearning/comments/palnop/r_how_can_this_be_made/"}, {"autor": "OpeningFox9423", "date": "2021-08-24 08:33:33", "content": "[D] How does a VGG-based Style-Loss incorporate color information? /!/ I've recently been reading a lot about style transfer, its  applications and implications. I understand what the Gram matrix is and  does. I can program it. But one thing that has been boggling me is: how  does the VGG style loss incorporate color information into the style?\n\nIn the paper \"[Texture Synthesis by CNNs](https://arxiv.org/abs/1505.07376)\",  Gatys et al. show that minimizing the MSE between the Gram matrices of a  random white noise -----> image !!!  and a \"target texture\" yields new instances of  that texture, with stochastic variation. I understand that this must work, as the Gram matrix measures the correlation between features  detected by the VGG activations across channels, without spatial  relation. So if we optimize the white noise image to have the same Gram  matrix, it will exhibit the same statistics, and hence look like an  instance of the original texture.\n\nBut how does this work with color? Of course, the VGG could learn  something like a mean filter, with all ones, whose output would be the  avg. color over that filter kernel. After all, \"color\" is just another  statistic. But then when using that in conjunction with the Gram loss,  wouldn't this information be lost, as it's all just correlation and  hence \"relative\" to each other?\n\nWhile writing this question, I'm starting to think of it like this:  Maybe the feature correlation expresses these color constraints in some  form like: \"if one part is red, there must be a green part close to it\"  (for the radish), or \"if there is a rounded edge, one side of it must be  in shadow (=darker)\" in case of the stone texture. This would tie color to the surrounding statistics (e.g., edges, other colors) and is the  only reason I can think of why this works at all.\n\n&amp;#x200B;\n\n[Image Source: Texture Synthesis using Convolutional Neural Networks, Gatys et al.](https://preview.redd.it/lpsl3flno9j71.png?width=1098&amp;format=png&amp;auto=webp&amp;s=110b5cdb2b8d03e0dde777cdde8387e96200768c)\n\nCan somebody confirm/refute this, and share their thoughts? Happy to discuss!", "link": "https://www.reddit.com/r/MachineLearning/comments/paj90j/d_how_does_a_vggbased_styleloss_incorporate_color/"}, {"autor": "jucheonsun", "date": "2021-08-24 07:37:32", "content": "[R] Does anyone have experience working with transfer learning on regression problems? /!/ Most literature on transfer learning are for classification problems, mostly image classification to be exact. Which makes a lot sense, where the front layers learn generic features that are useful for all -----> image !!!  tasks.\n\nHowever, the problem I'm working on now is entirely different. It's a regression problem, and the input is NOT an image or time series. It's just a set of real or discrete inputs mapping to a real value output. \n\nNow I have a lot of data available for problem A, and much lesser data for a related/similar problem B. I would like to use transfer learning to help model problem B which has lesser data.\n\nI would like to ask if anyone has experience with transfer learning for such regression problems? Or some good papers to recommend in this domain? Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/paimjm/r_does_anyone_have_experience_working_with/"}, {"autor": "wilsonchua", "date": "2021-08-24 02:48:21", "content": "Facebook's AI fail [D] /!/ I posted this -----> picture !!!  of NASA astronauts in Skylab. It was promptly flagged as pornographic. \n\nhttps://preview.redd.it/6wb27cl8z7j71.png?width=601&amp;format=png&amp;auto=webp&amp;s=c202ff0de531c6e251d83e478856838be6a17275\n\nhttps://preview.redd.it/iwehrq56z7j71.png?width=795&amp;format=png&amp;auto=webp&amp;s=66a5327292d8887cf3519e55beedf744ff515409", "link": "https://www.reddit.com/r/MachineLearning/comments/paeqhn/facebooks_ai_fail_d/"}, {"autor": "CountFrolic", "date": "2021-08-24 00:08:28", "content": "Day to night city -----> image !!!  translation tools? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pac5ak/day_to_night_city_image_translation_tools/"}, {"autor": "CountFrolic", "date": "2021-08-24 00:03:20", "content": "City day to night -----> image !!!  translation tools?", "link": "https://www.reddit.com/r/MachineLearning/comments/pac276/city_day_to_night_image_translation_tools/"}, {"autor": "miladink", "date": "2021-04-21 07:08:55", "content": "[D]Is im2latex considered solved? /!/ I hope you know the im2latex dataset by OpenAI in which you need to infer the equation that generated a latex -----> image !!!  from the -----> image !!! . I found it a little surprising that actually little work is done there. Is reading equations from the images considered a solved problem?", "link": "https://www.reddit.com/r/MachineLearning/comments/mva9pn/dis_im2latex_considered_solved/"}, {"autor": "techsucker", "date": "2021-04-21 02:29:13", "content": "[R] Researchers Introduce a Convolutional Neural Network (CNN)-Based Model that Automates the Distinction Between Natural Images and Computer-Generated Images (CGI) /!/ With the increasing performance accuracy of computer software systems, the realistic appearance of computer-generated images (CGI) and deepfakes often leads to assuming them as authentic images.\n\nResearchers at the Changsha University of Science and Technology and Hunan University in Hunan, China, have recently developed an -----> image !!!  source pipeline forensics method based on convolutional neural networks (CNN) to automate the distinction between natural -----> image !!! s and CGI. The work announced in the\u00a0*International Journal of Autonomous and Adaptive Communications Systems*\u00a0describes that the CNN-based model is fine-tuned using a database of 10000 images.\n\nSummary: [https://www.marktechpost.com/2021/04/20/researchers-introduce-a-convolutional-neural-network-cnn-based-model-that-automates-the-distinction-between-natural-images-and-computer-generated-images-cgi/](https://www.marktechpost.com/2021/04/20/researchers-introduce-a-convolutional-neural-network-cnn-based-model-that-automates-the-distinction-between-natural-images-and-computer-generated-images-cgi/) \n\nPaper: http://www.inderscience.com/offer.php?id=114295", "link": "https://www.reddit.com/r/MachineLearning/comments/mv6djz/r_researchers_introduce_a_convolutional_neural/"}, {"autor": "jeromeharper", "date": "2021-04-21 00:33:28", "content": "[D] Cov-19 binary classification dataset. /!/  Hi, Guys, I am looking for a binary classification covid-19 dataset, the prediction label is positive or negative, this label could be as, whether some one is infected or not. So far I found dataset from kaggle. But most of them is consists a medical -----> image !!!  such as XRay scan of chest. I am not looking for image data as the feature. I found one from kaggle the feature is age, gender, hypertension and etc which the label is positive / negative. Please help. Thank you in advance.. Cheers", "link": "https://www.reddit.com/r/MachineLearning/comments/mv4gps/d_cov19_binary_classification_dataset/"}, {"autor": "Unreasonable_Energy", "date": "2021-04-20 23:30:33", "content": "Do we already have the ML technology to make eye contact work better in video chat? [D] /!/ **The problem:**\n\nIn video chat, if you make \"eye contact\" with the -----> image !!!  of your conversation partner's eyes as they appear on your screen, your eyes are not pointed at the camera, and thus you do not appear, to your partner, to be making eye contact with them, but appear to looking in an offset direction -- [relevant xkcd](https://xkcd.com/2430/).\n\n**A solution (in principle?):** \n\nIf you had multiple cameras arrayed at the edges of your screen (say, 3 or 4 webcams around the edges of your screen instead of just 1) it seems like it should be possible, with some machine learning magic, to combine the multiple camera feeds coming from slightly different vantage points into a single feed of a \"virtual vantage point\" within the convex hull of the physical cameras -- that is, a point somewhere in the middle of your screen.  Then all you need is to be able to automatically locate your the image of your conversation partner's eyes within your screen (basically a solved problem, as I understand) and set the virtual vantage point to one of these eye images, or to a point between them -- and voila, when you make eye contact with their screen image them, they actually see your video manifestation appear to make eye contact with them.  \n\n**If this would be so easy, why doesn't it already exist?**\n\nDo we not have the machine learning technology already to make this a reality?  Is somebody already doing it?  The potential hurdles that immediately come to mind are:\n\n(1) Learning the virtual-vantage-point transformation would be hard -- maybe?  I'm pretty sure I've already seen more impressive demonstrations of vantage-point-shift than this would require.  I certainly don't have enough understanding of this specialty to know how I'd do it though -- maybe it's hard.\n\n(2) Takes too long to apply the vantage-point transformation in real, inducing unacceptable lag.  The processing needs to happen in real time, but it can happen on the local machine -- the virtual-vantage-point-feed shouldn't be any more costly to transmit over a network than the standard video feed.  I can imagine applying it locally could be pretty fast, and there already seem to be more superficially-impressive video transformations being applied in real time (face-contouring, etc).\n\n(3) Requires a non-standard hardware setup -- sure, but not like a really expensive or difficult one, just a couple extra webcams and you could do it yourself.  Doesn't work on mobile, but you can't have everything,\n\n(4) Nobody cares about this problem enough to work on it -- I suspect there are surprisingly large gains to be had through making video chat a little less uncanny, but maybe this gain is too small to be worth the effort.\n\nI'd appreciate any thoughts on the feasibility of such a scheme, from an machine-learning perspective.", "link": "https://www.reddit.com/r/MachineLearning/comments/mv3cj5/do_we_already_have_the_ml_technology_to_make_eye/"}, {"autor": "ottawalanguages", "date": "2021-04-20 22:06:10", "content": "[D] Why do polynomials have a bad reputation for overfitting? /!/ We all must have heard by now - when we start learning about statistical models overfitting data, the first example we are often given is about \"polynomial functions\" (e.g. see the -----> picture !!!  here: https://ardianumam.wordpress.com/2017/09/22/deriving-polynomial-regression-with-regularization-to-avoid-overfitting/) .\n\nWe are warned that although higher degree polynomials can fit training data quite well. they surely will overfit and generalize poorly to the test data.\n\nMy question is : Why does this happen? Is there any mathematical justification as to why (higher degree) polynomial functions overfit the data? The closest explanation I could find online was something called \"Runge's Phenomenon\" (https://en.wikipedia.org/wiki/Runge%27s_phenomenon ), which suggests that higher order polynomials tend to \"oscillate\" a lot - does this explain why polynomial functions are known to overfit data?\n\nI understand that there is a whole field of \"Regularization\" that tries to fix these overfitting problems (e.g. penalization can prevent a statistical model from \"hugging\" the data too closely) - but just using mathematical intuition, why are polynomials known to overfit the data?\n\nIn general, \"functions\" (e.g. the response variable you are trying to predict using machine learning algorithms) can be approximated using older methods like fourier series, taylor series and newer methods like neural networks. I believe that there are theorems that guarantee that taylor series, polynomials and neural networks can \"arbitrarily approximate\" any function. Perhaps neural networks can promise smaller errors for simpler complexity?\n\nBut does anyone know why polynomials are said to have a bad habit of overfitting, to the extent that neural networks have largely replaced them?\n\nInteresting paper: https://www.nber.org/system/files/working_papers/w20405/w20405.pdf", "link": "https://www.reddit.com/r/MachineLearning/comments/mv1op9/d_why_do_polynomials_have_a_bad_reputation_for/"}, {"autor": "sikoyo", "date": "2021-05-22 16:25:01", "content": "[D] Make a model do binary classification on a user-specified class in a multi-class scenario? /!/ Title sounds confusing, but let me clarify. \\*\\*Each -----> image !!!  can only contain 1 class.\\*\\* Let's say you have 3 classes (0, 1, 2), but you only want to see if class 0 is in an -----> image !!! \n\n\\`\\`\\`\n\nmodel(\\[-----> image !!! ,0\\]) -&gt; outputs binary classification of class 0 with sigmoid + threshold.\n\n\\`\\`\\`\n\nThis is not the same as straight up the model outputting logits for all 3 classes and then softmax + argmax because then the model predicts either the most likely class even if the probability of the 2nd most likely class is high.\n\n&amp;#x200B;\n\nI feel like that this has been done before, like using said \\*\\*inputted class as part of a loss function to limit the output to 1 class. What is the name of this procedure (if it has one) or are there any relevant papers that do something like this?\\*\\*\n\n&amp;#x200B;\n\nMaybe I'm overthinking it idk. Thanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/nim8w4/d_make_a_model_do_binary_classification_on_a/"}, {"autor": "bendee983", "date": "2021-05-22 15:35:12", "content": "[D] How much memory does Intel's GTA5 -----> image !!! -enhancer use for inference? /!/ Last week, Intel presented a neural network that enhances the graphics of Grand Theft Auto 5 to photorealistic level. The results are impressive (video [here](https://www.youtube.com/watch?v=P1IcaBn3ej0), paper [here](https://arxiv.org/abs/2105.04619)) and the researchers state that they were able to perform the transformation at \"interactive rate,\" which I assume is near real-time.\n\nI'm trying to figure out how much memory/horsepower the model needs to perform the transformation in real-time. The question is, what size of a GPU you would need if this would become a real feature.\n\nThe paper provides information about the structure of the neural network but very little in terms of implementation details. Below is the key components of the model that are used during inference (there are more pieces to it, but those are used during training).\n\n&amp;#x200B;\n\nhttps://preview.redd.it/fz20jfpfuo071.png?width=936&amp;format=png&amp;auto=webp&amp;s=2b102649814548edab8bd3930c361a1a46bf3db4\n\nI tried to figure out the numbers based on what information exists in the video and paper. The Image Enhancement Network is a modified version of HResNetV2 (paper [here](https://arxiv.org/abs/1908.07919)). On a 1024x2048 input the HResNetV2 uses 1.79GB of video memory on a size-1 batch.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/8m2unyezvo071.png?width=2600&amp;format=png&amp;auto=webp&amp;s=f88f39cff397405005ac96119b9df1a38e45b883\n\nIntel's model has made some modifications to HResNetV2, including the replacement of batch normalization layers with rendering-aware denormalization modules, which add more learned parameters to the model. So I suppose this makes a minimal addition to the model's size.\n\nAlso, the G-buffer encoder takes six different maps (normals, depth map, albedo, glossiness, atmosphere, segmentation), and encodes them into a 128-component feature vector. There are no details on how many convolution layers are used and I'm not sure what the input size is, so I don't know what the model size will be, but it is a lot smaller than the main image enhancer network. \n\n&amp;#x200B;\n\nhttps://preview.redd.it/h0xwfk40xo071.png?width=1342&amp;format=png&amp;auto=webp&amp;s=fc8ce4efc699cca58c24b529be6179fcd8c4514c\n\nHere's an interesting quote from the paper: \"Inference with our approach in its current unoptimized implementation takes half a second on a Geforce RTX 3090 GPU. Since G-buffers that are used as input are produced natively on the GPU, our method could be integrated more deeply into game engines, increasing efficiency and possibly further advancing the level of realism.\"\n\nGiven all we know about the image enhancer, how much do you estimate its memory consumption to be?\n\nAlso considering that GTA5, a game that was released in 2013, [can consume up to 3.5G of VRAM](https://www.reddit.com/r/nvidia/comments/405yy9/how_much_vram_does_gta_v_1080p_use/) in 1080 resolution, how realistic is it to see this kind of enhancement become available for gamers who don't have the highest-end graphics card?", "link": "https://www.reddit.com/r/MachineLearning/comments/nil73w/d_how_much_memory_does_intels_gta5_imageenhancer/"}, {"autor": "KirillTheMunchKing", "date": "2021-05-22 14:23:25", "content": "[D] How to turn Minecraft maps into photorealistic 3d scenes explained! /!/ Did you ever want to quickly create a photorealistic 3d scene from scratch? \n\nWell, now you can! The authors from NVidia in their paper \"**GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds**\" proposed a new neural rendering model trained with adversarial losses ...WITHOUT a paired dataset. \n\nYes, it only requires a 3D semantic block world as input, a pseudo ground truth -----> image !!!  generated by a pretrained -----> image !!!  synthesis model, and any real landscape photos to output a consistent photorealistic render of a 3D scene corresponding to the block world input. Check out the [full paper explanation on my channel](https://t.me/casual_gan/41)!\n\nHere is an example of the model outputs:\n\n*Processing video ydjbojhrko071...*\n\n\\[[Full Explanation Post](https://t.me/casual_gan/41)\\] \\[[Arxiv](https://arxiv.org/pdf/2104.07659.pdf)\\] \\[[Project Page](https://nvlabs.github.io/GANcraft/)\\]   \nMore recent popular paper explanations:  \n\\[[DINO](https://t.me/casual_gan/40)\\]  \n\\[[MLP-mixer](https://t.me/casual_gan/35)\\]  \n\\[[Vision Transformer (ViT)](https://t.me/casual_gan/33)\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/nijpfk/d_how_to_turn_minecraft_maps_into_photorealistic/"}, {"autor": "SQL_beginner", "date": "2021-05-22 06:43:00", "content": "[D] is there any relationship between this graph and this -----> picture !!! ? /!/ Here is a picture of the famous bias-variance tradeoff: https://www.ncbi.nlm.nih.gov/books/NBK543534/figure/ch8.Fig3/\n\nI am trying to understand what is responsible for the shapes of the \"orange\" curve and the \"blue\" curve. I understand that this bias-variance tradeoff often occurs in real life, but I am trying to understand if it will always (mathematically prove) occur.\n\nFor instance, if you look at the decomposition of the MSE into bias and error: \nhttps://i.stack.imgur.com/3Mydo.jpg\n\nWhere exactly is the tradeoff? In this mathematical formula, is there something which physically (or probabilistically) stops both the variance and the bias from moving towards 0? Or is this just a general idea that has been empirically observed?\n\nIn this picture (https://www.ncbi.nlm.nih.gov/books/NBK543534/figure/ch8.Fig3/), why have the bias and the error been depicted as \"curves\" (instead of straight lines, very squiggly lines, etc)? Are they always going to have this shape? Is there some mathematical logic behind this, or is this just an artistic depiction of what is to be commonly expected?\n\nAlso in this picture (https://www.ncbi.nlm.nih.gov/books/NBK543534/figure/ch8.Fig3/),  why do the orange and the blue curve intersect? Mathematically, will they always intersect? Or again, is this just an artistic depiction of what is to be commonly expected?", "link": "https://www.reddit.com/r/MachineLearning/comments/nicm5y/d_is_there_any_relationship_between_this_graph/"}, {"autor": "xiikjuy", "date": "2021-05-21 14:24:03", "content": "[D] Does it make sense to generate sentences with Transofmrer's encoder? /!/ Quite a few vision+language papers pretrain BERT with -----> image !!! -text data  and finetine for -----> image !!!  captioning task.  But there is no decoder  involved to generate sentences. Does that make sense? And what's the  main difference between using T's encoder  to do the sentence generation  and and T' decoder?", "link": "https://www.reddit.com/r/MachineLearning/comments/nhtr1e/d_does_it_make_sense_to_generate_sentences_with/"}, {"autor": "Kanep96", "date": "2021-05-21 11:13:17", "content": "[P] Coding Neural Network from scratch in MATLAB (without the toolset). Would like some assistance, tips, advice, etc. /!/ This is a regression neural network as well, by the way. Simply attempting to estimate an output based on input data. Given 1 as input, if the target output is 2*x, then (after the network is trained, naturally) the output should be 2. If the target is x^2 + 2x, the output should be 3. And so on.\n\n\nAre there any issues with MATLAB going into this that would be nice to know? My regression plots are a bit... odd, even if the numbers look fine, and I'm unsure as to exactly why. I've coded the Neural Network(s) already, and I think I did a fine job doings so and I felt the math was simple to understand after some time with it, but there are some... odd issues that I (as well as my faculty advisor!) can't really figure out. I'd be happy to provide snippets of my code as well, if someone wants to go above-and-beyond in seeing what exactly might be my issue here.\n\n\nI have coded a standard single-layer neural network, a double-layer neural network, and a \"main\" program that allows the user to create a neural network of any size/shape. I utilize the sigmoid activation function (my advisor would like me to do so) in each layer, save for the output layer. However, as of late, I am have *considerably* better results utilizing the Leaky ReLU activation function so I think, at the end of the say, I'm going to utilize this activation function instead of sigmoid. In my programs, I have Sigmoid and its derivative inside of functions, and I have Leaky ReLU commented out inside of said function so that, whenever I want to use it as an activation function, I just comment out the Sigmoid code and un-comment out the Leaky ReLU code and then run again. Its quite nice!\n\n\nAnyway, here is my code for those interested. It isn't too long, but if there are any questions about my design, feel free to let me know of course. There are lots of comments as well... feel free to ignore them. My single-layer NN seemingly works well, but I'm not too confident in it. My double-layer NN... not so much. I've also included a -----> picture !!!  of the regression plots that show up for my single-layer NN that seem odd compared to what the toolbox outputs. Thanks!\n\n\nSingle-Layer:\n\n\nhttps://drive.google.com/file/d/1DhDrmx7HGoi_OGt9KOTd7QQQh6L-hq9I/view?usp=sharing\n\n\nOdd-looking Regression plot for Single-Layer NN:\n\n\nhttps://imgur.com/a/hl0pgWV\n\n\nDouble-Layer:\n\n\nhttps://drive.google.com/file/d/1yvD39VNruQvxzq58FavbnomY2nDo8Jxj/view?usp=sharing", "link": "https://www.reddit.com/r/MachineLearning/comments/nhpyto/p_coding_neural_network_from_scratch_in_matlab/"}, {"autor": "gerk0ne", "date": "2021-04-18 16:53:41", "content": "[P] Docker based torcs environment for reinforcement learning /!/ [https://github.com/gerkone/pyTORCS-docker/](https://github.com/gerkone/pyTORCS-docker/)\n\n&amp;#x200B;\n\nYet another torcs environment, but this one is on docker to make installation and configuration easier.\n\nThe   environment is made up of a client in Python and a server in C++ (scr\\_server by Daniele Loiacono), which is just a special driver in  torcs. At each step the client receives the observation (values from a range of sensors and possibly rgb vision) and responds with the chosen action.\n\nThey communicate through UDP for the \"regular\" sensors and the actions, and via shared memory for the vision. The client hides this though, making it look as just one single observation for every step. I did this because sending a 640x480x3 -----> image !!!  through a UDP socket 25-50 times a second seemed to slow down the whole environment a bit.\n\nI have also made some changes to the torcs source, I removed the menu so that the environment is usable with frequent restarts (such as in reinforcement learning) **without using xautomation**.\n\nAny suggestion is highly appreciated", "link": "https://www.reddit.com/r/MachineLearning/comments/mtgkmm/p_docker_based_torcs_environment_for/"}, {"autor": "SkyLordOmega", "date": "2021-04-18 05:02:51", "content": "[D] Wav2Vec2 training for Hindi language /!/ I was part of the datasprint with HuggingFace for the wav2vec2 fine-tuning task. I trained it on three datasets for Hindi language.\n\n1. CommonVoice\n\n2. Indic TTS (IITM)\n\n3. IIITH - Indic dataset\n\nWhile the trained model gives a good performance on the longer audios (2&amp;3 WER 17) its performance on CommonVoice is very bad (WER 56)\n\nCommonVoice has audio of smaller length.\n\nI have attached an -----> image !!!  with some example predictions (from the test set) What could be the reason for this drastic degradation in quality? Could I improve the model, by resuming training on only the CommonVoice dataset.", "link": "https://www.reddit.com/r/MachineLearning/comments/mt6j2n/d_wav2vec2_training_for_hindi_language/"}, {"autor": "ElCobo", "date": "2021-04-17 21:28:30", "content": "[P] Wiggle-GAN: Stereoscopic -----> camera !!!  simulation using generative adversarial neural networks /!/ Hello everyone,\n\nI wanted to post my grad thesis here, in which I tried to simulate the use of a stereoscopic camera utilizing a monocular image as input. The main goal was to give a new alternative when someone wanted to create a [wigglegram](https://www.reddit.com/r/wigglegrams/), because all the ways you could achieve the effect have limitations. For example, you could use a normal camera and take many pictures with it but if the scene is moving, the effect would mimic a stop motion more than a wigglegram. On the other hand, you could use an array of cameras but that's expensive or you could use a stereoscopic camera with 3 of 4 lenses but these kinds of cameras are analogic so it takes more time to do the effect. That's the reason why we took an approach by using Neural Networks where you don't have any of these limitations.  \n\nIn this project I used a WGAN with Improved Consistency Regularization and L1 loss comparing the output and real image, you can see some the result of adjusting this values in the [github](https://github.com/RoCoBo44/Wiggle-GAN) (and the code). Also, you can see how I trained multiple some multiple solutions:\n\n* An autoencoder (AE) which is only one net\n* The Wiggle-GAN solution (two nets fighting)\n* The Wiggle-GAN solution but extracting some metrics to see what it does (Wiggle-GAN no CR)\n\nThe input of the Wiggle-GAN are the images you want to move, their depth maps and the direction (left or right) and the outputs are the new images moved and their depth map estimations, so you could iterate multiple times.  If you want to try it out I made a [google colab](https://colab.research.google.com/drive/1N5HJ1geVM1ymLoE5C2jkLs3F_tQn-s-r?usp=sharing), but you have to download the checkpoints. \n\nThe limitation of the net is that all are trained with images of 128x128 pixels except the last one in the colab that is 256x256. The code automatically changes the scale of your image to the size it needs so you don't have to change it manually.\n\nSometimes it doesn't generate the wiggle expected and that may be due to the depth map estimation. If that happens you could download the depth map image in the folder (/Image/Input - Test/) with the name \"{number}\\_d.png\", then with an editing software you could change it and uploaded it with the same name in the same place.\n\nIf you want to try it and you have some problems with that, feel free to ask me for anything.\n\nLots of love", "link": "https://www.reddit.com/r/MachineLearning/comments/mszj34/p_wigglegan_stereoscopic_camera_simulation_using/"}, {"autor": "Embedded-Robotics", "date": "2021-04-17 20:39:25", "content": "\"[Discussion]\", \"[D]\" /!/ In the generalized form of learning, a computer must be able to learn something in a specific situation and then apply the same learning within a contextually-related, albeit different situation.\n\nFor example, if a computer classifies the label of \u201cDOG\u201d from an -----> image !!!  dataset of 10 different dog breeds, it should be able to identify a dog of a different breed upon visualization.\n\n[https://www.embedded-robotics.com/artificial-intelligence-101/](https://www.embedded-robotics.com/artificial-intelligence-101/)", "link": "https://www.reddit.com/r/MachineLearning/comments/msymyn/discussion_d/"}, {"autor": "my_name_is_reed", "date": "2021-07-05 13:58:22", "content": "Question about data collection. [P][D] /!/ I'm setting up some number n stations to collect -----> image !!!  data. Right now n = ~3-4, but that number will hopefully grow. These stations will be in multiple locations, with internet access. \n\nEach station will essentially just be a raspberry pi with a pi cam. For a prototype now, I've already built openCV on the pi, and I've got it up and running with a connected pi cam. With a python script, I'm able to capture images at regular intervals, dictated according to a configuration file.\n\nI'd like to have the pi upload the taken images to a server somewhere, also with python. This way I could have multiple stations running at multiple locations, and I could coordinate data collection, curation, formatting (etc) remotely. Instead of, ya know, going around collecting thumb drives or something.\n\nWhat services do you all use for this sort of thing? AWS? One Drive? Some other cloud service? Should I just set up an FTP server here at home?\n\nAny help is appreciated. Thanks in advance!", "link": "https://www.reddit.com/r/MachineLearning/comments/oe7xjg/question_about_data_collection_pd/"}, {"autor": "jeongdoowon", "date": "2021-07-05 13:16:22", "content": "[D] What is the basic pipeline to detect inventory's stock in computer vision? /!/ I am curious what algorithms would be used to detect this kind of inventory detection check AI. Sometimes they also predict how many percentages of the product is still there, not just out of shelf status. Would it be using object detection or something? One more issue is, the products that are in the shelfs are different for every shelf. So it does not really make sense that they annotate every product's -----> image !!!  and detect it.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/5ys9y0109e971.png?width=1139&amp;format=png&amp;auto=webp&amp;s=91fca5a5bba305f8e40addc94abc73d6c74436eb", "link": "https://www.reddit.com/r/MachineLearning/comments/oe76dn/d_what_is_the_basic_pipeline_to_detect_inventorys/"}, {"autor": "DiracShore", "date": "2021-07-05 11:46:14", "content": "[D] Unsupervised instance segmentation? /!/ Hi there,\n\nIs anyone aware of good methods to perform unsupervised instance segmentation?\n\nI have a large dataset of -----> image !!! s, and each -----> image !!!  contains multiple objects that often overlap. My goal is to perform instance segmentation, such that for every object, I'd get a different per-pixel prediction value.\n\nI know of one method that uses an adversarial approach (ReDO), but I'm looking for alternatives as well.\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/oe5p0v/d_unsupervised_instance_segmentation/"}, {"autor": "crazyfrogspb", "date": "2021-07-05 08:54:46", "content": "[D] Best practices of storing annotations for -----> image !!!  data /!/ What are the best practices of storing annotations (bounding boxes, masks) and other kind of meta data for image datasets?\n\nThere are some options that we tried out or considering right now:\n\n1) JSON/CSV + storing train/val/test splits in the repo\n\n2) JSON/CSV + DVC\n\n3) Relational database\n\nAll of them seem to have some pros and cons. Here are the factors that we consider:\n\n1) Simplicity. Simple JSONs win here, there is no need to maintain any DB or use an external tool.\n\n2) Reproducibility of experiments. DVC leads here, whereas it might be difficult to version control a DB.\n\n3) Clear structure + Ability to easily add new annotations and information - databases seem to win here. We mainly work with medical images, and there is a large nested structure of the data (e.g., hospital - patient - study - left/right breast - CC/MLO view - annotation). In addition, there are usually multiple annotators per image. Storing and linking all this information in CSV/JSONs slowly becomes a nightmare.\n\n&amp;#x200B;\n\nOverall, it seems like there is no clear winner. Are we missing any other options?", "link": "https://www.reddit.com/r/MachineLearning/comments/oe374e/d_best_practices_of_storing_annotations_for_image/"}, {"autor": "kns2000", "date": "2021-07-04 15:59:18", "content": "[D] separable convolution vs normal convolution /!/ When is it better to use separable convolution instead of normal convolution for -----> image !!!  recognition tasks? What are the pros and cons of separable convolutions?", "link": "https://www.reddit.com/r/MachineLearning/comments/odna6t/d_separable_convolution_vs_normal_convolution/"}, {"autor": "techsucker", "date": "2021-07-10 20:37:02", "content": "[R] Cornell and Harvard University Researchers Develops Correlation Convolutional Neural Networks (CCNN): To Determine Which Correlations Are Most Important /!/ A team of researchers from Cornell and Harvard University introduces a novel approach to parse quantum matter and make crucial data distinctions. This proposed technique will enable researchers to decipher the most perplexing phenomena in the subatomic realm.\n\nIn their paper, \u201cCorrelator Convolutional Neural Networks as an Interpretable Architecture for -----> Image !!! -like Quantum Matter Data,\u201d the team discusses ways to extract new information about quantum systems from snapshots of image-like data. They are now thus developing ML tools to identify relationships between microscopic properties in data that would otherwise be impossible to determine at that scale.\n\nSummary: [https://www.marktechpost.com/2021/07/10/cornell-and-harvard-university-researchers-develops-correlation-convolutional-neural-networks-ccnn-to-determine-which-correlations-are-most-important/](https://www.marktechpost.com/2021/07/10/cornell-and-harvard-university-researchers-develops-correlation-convolutional-neural-networks-ccnn-to-determine-which-correlations-are-most-important/) \n\nPaper: https://www.nature.com/articles/s41467-021-23952-w.pdf", "link": "https://www.reddit.com/r/MachineLearning/comments/ohqe76/r_cornell_and_harvard_university_researchers/"}, {"autor": "MAP25069", "date": "2021-07-10 18:54:45", "content": "Deep Learning for Computer Vision with TensorFlow Udemy Course /!/ **Deep Learning for Computer Vision with TensorFlow** is a hot acclaimed course in Udemy for intermediate level eager learners. You will learn about -----> image !!!  classification exploring different popular models of ConvNets like VGG-16, ResNet or Inception, and also the state of the art object detection algorithms like Faster R-CNN, TensorFlow Object Detection API, YOLO v2-v3-v4.  You will be also able to train your own data and customize your models. Theoretichal background is explained in plain and many practical applications are developed.\n\nDont miss this opportunity to improve your career or start a new project with this course. **Special offer with a big discount from USD99.00 to USD13.99 for only 5 days time is through this coupon:** \n\n[https://www.udemy.com/course/deep-learning-for-computer-vision-with-tensor-flow-and-keras/?couponCode=DEEPCVJULY21-REEDIT](https://www.udemy.com/course/deep-learning-for-computer-vision-with-tensor-flow-and-keras/?couponCode=DEEPCVJULY21-REEDIT)\n\n[Deep Learning for Computer Vision with TensorFlow](https://preview.redd.it/gq1iaaxqffa71.jpg?width=2048&amp;format=pjpg&amp;auto=webp&amp;s=12de330d383d70d5f1ca0f3fbac0bf387bdea6d8)", "link": "https://www.reddit.com/r/MachineLearning/comments/ohojgu/deep_learning_for_computer_vision_with_tensorflow/"}, {"autor": "willspag", "date": "2021-07-10 17:09:50", "content": "[D] Best algorithms for video recognition /!/ I\u2019m looking into building a neural network to recognize people based on posture/body movements, and wanted to get some feedback on what algorithms would be best for this.\n\nFor backstory, a gas station cashier in my town was murdered, and there\u2019s been a case going on for months to find the killer with no progress. They caught it on -----> camera !!! , but the suspect was fully covered with gloves, a mask, and a hoodie. Given that murders are rarely every random, the suspect has most likely been to this gas station before. In an attempt to use my AI skills for good, I\u2019m looking into what it would take to build an AI to recognize people by their posture/body motion to help the police solve the case.\n\nI haven\u2019t worked with much video recognition in the past, but here\u2019s my initial thoughts on how it could be done:\n\nUse the YOLO algorithm to create bounding boxes of people at the cash register, linking their identities based on the credit card timestamps. Then, train an algorithm similar to facial recognition, but I\u2019m not sure what\u2019s the best approach to handle the temporal data. Off the top of my head, here\u2019s my first ideas:\n1) 3D CNN, using the time as the third dimension\n2) 2D CNN using a rolling average of the embeddings\n3) Feed a 2D CNN embedding into an LSTM\n\nI\u2019d love some recommendations/papers from anyone that\u2019s worked with anything similar before. Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/ohmlpf/d_best_algorithms_for_video_recognition/"}, {"autor": "PassionGlittering695", "date": "2021-08-20 17:19:09", "content": "[D]What convolution to use (conv1d, conv2d or conv3d)? /!/ The  -----> image !!!  is represented with a stack of layers which are equidistant in   depth (depth layers), and instead of feeding the whole -----> image !!!  (every   pixel) i want to feed R pixels (from 0 to HxW). So I have an  array of  shape (N,D,C,R,1) where N is the batch, D is the number of  layers, C  channels (&gt;3, RGB+arbitrary info )and R is the number pixels. I want  to  combine the layers in a final image (N, 1, C', R, 1) and reduce the  number of channels, where C' is 3. What conv is best  to use for this network? And  would a UNet be a good idea since we are not dealing with a full image  anymore?\n\nPs. R pixels instead of a whole image because of memory limitatins from previous rendering.", "link": "https://www.reddit.com/r/MachineLearning/comments/p89oo8/dwhat_convolution_to_use_conv1d_conv2d_or_conv3d/"}, {"autor": "Ill-Ad-106", "date": "2021-08-20 16:23:16", "content": "[D] Naive Bayes doubt /!/ This is on page 9 &amp; 10 - [http://cs229.stanford.edu/notes-spring2019/cs229-notes2.pdf](http://cs229.stanford.edu/notes-spring2019/cs229-notes2.pdf)\n\nI don't understand this part (-----> picture !!!  below) - \"if we were to model x explicitly with a multinomial distribution over the 2\\^50000 possible outcomes, then we\u2019d end up with a (2\\^50000 \u22121)-dimensional parameter vector\"\n\nWhy would we model 2\\^50000 possible outcomes, and how would we end up with a (2\\^50000 \u22121)-dimensional parameter vector? Could someone break this down for me?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/1npasolzgji71.png?width=1016&amp;format=png&amp;auto=webp&amp;s=28a70bb7198357d86ab93f1e3ed6b523bbb5b326", "link": "https://www.reddit.com/r/MachineLearning/comments/p88kui/d_naive_bayes_doubt/"}, {"autor": "Ill-Ad-106", "date": "2021-08-20 14:11:53", "content": "Gaussian Discriminant Analysis Parameter Estimation [D] /!/ &amp;#x200B;\n\nReference: CS 229 Andrew Ng notes\n\nHere (-----> picture !!!  below), in Gaussian Discriminant Analysis, in the probability density function for Multivariate Gaussian distribution:\n\n\u03c6 is the probability of y, \u03a3 is the covariance matrix , \u03bc is the mean vector. All of them are values that depend on the dataset and can be calculated from the training data! How are they parameters? Why are we using MLE to find out their value when we already know their values?\n\nWhat am I missing\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/oju3ztuhtii71.png?width=1000&amp;format=png&amp;auto=webp&amp;s=b30d6ddc1a2bd4e42380574fe65372fac8a2601f", "link": "https://www.reddit.com/r/MachineLearning/comments/p8604w/gaussian_discriminant_analysis_parameter/"}, {"autor": "Evening_Honey", "date": "2021-08-20 03:59:47", "content": "AI robot with role at United Nation\u2019s to innovate sustainable development goals appears to have all the indications, even her name, which is corresponding to an end times bible prophecy about the -----> image !!!  of the beast which would speak. Wikipedia articles and news reports help demonstrate. [R]", "link": "https://www.reddit.com/r/MachineLearning/comments/p7um7b/ai_robot_with_role_at_united_nations_to_innovate/"}, {"autor": "neuratomizer", "date": "2021-08-20 03:12:56", "content": "[D] GAN tasks for evaluation of new theoretical methods that are computationally \"feasible\" /!/ hey all! \n\ni'm an undergraduate student pursuing a research project. a professor in the math department at my school recently published a paper on a new kind of loss function for GANs  with certain theoretical results and claimed advantages. namely, improved -----> image !!!  quality and improved stability. in their paper, they have so far tested this new loss function on StyleGAN and DCGAN comparing FID scores. \n\ni would like to benchmark this new method against different models and tasks. i am looking for tasks that can be trained with \"reasonable\" compute resources. that is, relatively inexpensively using a service like Azure or AWS. (as an aside, i've used Azure in the past, but would appreciate suggestions for any better value services). \n\nare there any standard tasks that are used for testing new methods? do you have any particular suggestions that might be good for testing such a method? \n\nalso, could you give me an idea of how much training time/compute common tasks would take. e.g.: training time of StyleGAN on a V100 for instance. \n\n\ni'm a novice researcher so i greatly appreciate any guidance! \n\nthank you muchly ;p", "link": "https://www.reddit.com/r/MachineLearning/comments/p7twj3/d_gan_tasks_for_evaluation_of_new_theoretical/"}, {"autor": "AsIAm", "date": "2021-08-20 01:29:59", "content": "[D] Mitigation of Adversarial Attack on NeuralHash with Ensemble /!/ Is it possible to counteract adversarial attack on NeuralHash by using multiple local models that \"vote\" on the hash?\n\nAn ensemble of N small models would be trained to output same hash for the same -----> image !!! , while maximizing the entropy of the weights and architectures. An adversarial attack would have to do SGD in very tight space, which should be way more computationally challenging than attacking a single model. However, this would make on-device inference also difficult.\n\nWould this actually work?\n\n&amp;#x200B;\n\n[^(^(^(btw this is ML porn)))](https://www.youtube.com/watch?v=11QXiJ8ORe8)", "link": "https://www.reddit.com/r/MachineLearning/comments/p7sa4d/d_mitigation_of_adversarial_attack_on_neuralhash/"}, {"autor": "fj33xx", "date": "2021-08-20 01:04:44", "content": "[R] Computer Vision Datasets for Fashion /!/ If you are looking for fashion related -----> image !!!  datasets to work with, check out this post covering some of the most popular large scale datasets on fashion products.\n\n[https://medium.com/visualdata/popular-computer-vision-datasets-fashion-68784fdab723](https://medium.com/visualdata/popular-computer-vision-datasets-fashion-68784fdab723)", "link": "https://www.reddit.com/r/MachineLearning/comments/p7rveg/r_computer_vision_datasets_for_fashion/"}, {"autor": "mr_akira", "date": "2021-08-19 21:23:53", "content": "[D] Biometric antifraud /!/ Hi! Can you advice books/articles about using keyboard typing, mouse movement for bot detection? But not converting data to -----> image !!!  and using cnn.\n\nThanks in advance!", "link": "https://www.reddit.com/r/MachineLearning/comments/p7o04s/d_biometric_antifraud/"}, {"autor": "ykilcher", "date": "2021-08-19 14:10:47", "content": "[D] Video - NeuralHash is BROKEN | How to evade Apple's detection and forge hash collisions (w/ Code) /!/ [https://youtu.be/6MUpWGeGMxs](https://youtu.be/6MUpWGeGMxs)\n\nSend your Apple fanboy friends to prison with this one simple trick ;) We break Apple's NeuralHash algorithm used to detect CSAM for iCloud photos. I show how it's possible to craft arbitrary hash collisions from any source / target -----> image !!!  pair using an adversarial example attack. This can be used for many purposes, such as evading detection, or forging false positives, triggering manual reviews.\n\n&amp;#x200B;\n\nOUTLINE:\n\n0:00 - Intro\n\n1:30 - Forced Hash Collisions via Adversarial Attacks\n\n2:30 - My Successful Attack\n\n5:40 - Results\n\n7:15 - Discussion\n\n&amp;#x200B;\n\nDISCLAIMER: This is for demonstration and educational purposes only. This is not an endorsement of illegal activity or circumvention of law.\n\n&amp;#x200B;\n\nCode: [https://github.com/yk/neural\\_hash\\_collision](https://github.com/yk/neural_hash_collision)\n\nExtract Model: [https://github.com/AsuharietYgvar/AppleNeuralHash2ONNX](https://github.com/AsuharietYgvar/AppleNeuralHash2ONNX)\n\nMy Video on NeuralHash: [https://youtu.be/z15JLtAuwVI](https://youtu.be/z15JLtAuwVI)", "link": "https://www.reddit.com/r/MachineLearning/comments/p7flhw/d_video_neuralhash_is_broken_how_to_evade_apples/"}, {"autor": "TheCockatoo", "date": "2021-08-19 14:08:09", "content": "[D] How to condition super-resolution (upscaling) on labels? /!/ Assume I have low resolution images (10 classes total) that I want to upsample using a succession of upsampling + convolution layers. Is there any way to condition the regression on the labels (besides using conditional GANs)?\n\nI was thinking of (1) adding a classification head at the output layer, so optimizing mean squared error + cross entropy. Another way might be to (2) concatenate the one-hot encoded label with the input -----> image !!! , and just optimize the MSE. Any ideas? References would be a big bonus, if possible.", "link": "https://www.reddit.com/r/MachineLearning/comments/p7fjnp/d_how_to_condition_superresolution_upscaling_on/"}, {"autor": "zonetrooper32", "date": "2021-07-24 07:39:30", "content": "[D] Contrastive Learning with implicit labels? /!/ Hi\u00a0r/MachineLearning,  \nI\u00a0was\u00a0trying\u00a0to\u00a0play\u00a0around\u00a0an\u00a0Image\u00a0Retrieval\u00a0problem\u00a0using\u00a0Contrastive\u00a0Learning.\u00a0I\u00a0hope\u00a0the\u00a0network\u00a0can\u00a0generate\u00a0a\u00a0good\u00a0embedding\u00a0representation\u00a0for\u00a0the\u00a0images.\u00a0However,\u00a0the\u00a0problem\u00a0with\u00a0my\u00a0-----> image !!! \u00a0dataset\u00a0is\u00a0that,\u00a0for\u00a0each\u00a0-----> image !!! \u00a0there\u00a0is\u00a0an\u00a0N\u00a0set\u00a0of\u00a02D\u00a0Euclidean\u00a0points\u00a0(xi,\u00a0yi)\u00a0associated\u00a0with\u00a0it ( label shape to be (N,\u00a02) ),\u00a0meaning\u00a0we\u00a0can\u00a0roughly\u00a0say\u00a0that,\u00a0the\u00a0similarity\u00a0between\u00a0two\u00a0-----> image !!! s\u00a0can\u00a0be\u00a0defined\u00a0by\u00a0the\u00a0distance\u00a0between\u00a0their\u00a0associating\u00a0points\u00a0set.\u00a0  \nI\u00a0am\u00a0wondering\u00a0if\u00a0there\u00a0is\u00a0any\u00a0way\u00a0to\u00a0incorporate\u00a0this\u00a0similarity\u00a0information\u00a0provided\u00a0by\u00a0the\u00a0points\u00a0set\u00a0into\u00a0the\u00a0network\u00a0representation\u00a0learning?\u00a0Most\u00a0contrastive\u00a0learning\u00a0papers\u00a0I\u00a0have\u00a0searched\u00a0either\u00a0ignore\u00a0the\u00a0labels\u00a0completely\u00a0and\u00a0train\u00a0the\u00a0network\u00a0in\u00a0a\u00a0self-supervised\u00a0manner\u00a0(In\u00a0my\u00a0case,\u00a0ditching\u00a0the\u00a0supervised\u00a0labels\u00a0significantly\u00a0increases\u00a0the\u00a0difficulties\u00a0of\u00a0my\u00a0network\u00a0learning.\u00a0So\u00a0simply\u00a0ditching\u00a0the\u00a0labels\u00a0is\u00a0a\u00a0bad\u00a0deal\u00a0for\u00a0me).\u00a0For\u00a0some\u00a0other\u00a0papers\u00a0I\u00a0have,\u00a0the\u00a0datasets\u00a0they\u00a0work\u00a0on\u00a0provide\u00a0classification\u00a0labels,\u00a0which\u00a0for\u00a0example\u00a0you\u00a0can\u00a0use\u00a0to\u00a0define\u00a0similar-dissimilar\u00a0pairs\u00a0for\u00a0contrastive\u00a0loss,\u00a0but\u00a0they\u00a0do\u00a0not\u00a0work\u00a0for\u00a0the\u00a0specific\u00a0labels\u00a0I\u00a0have.\u00a0  \nThanks\u00a0in\u00a0advance.\u00a0:)", "link": "https://www.reddit.com/r/MachineLearning/comments/oqlgrm/d_contrastive_learning_with_implicit_labels/"}, {"autor": "pedrowalrus255", "date": "2021-07-23 19:10:55", "content": "[D] I'm implementing shallow and deep algorithms in Julia inside a Colab notebook... Please visit regularly and tell me what you think /!/ I'm just a CS dropout with no intention of going back to college to get an MSc or PhD in ML, but I have a sweet ML eng and automation gig and my boss/friend told me if you wanna be better in your job, you should be able to implement shallow and deep algos even if you're stranded in a remote island, maybe with seashells.\n\n&amp;#x200B;\n\nSo I'm starting with Julia, one LLVM language. Then I will do it in Scala, another LLVM language.\n\n&amp;#x200B;\n\n    https://colab.research.google.com/drive/1b5_bf8NvzKeKVc76ZGhnxb1XgvTiERiX?usp=sharing\n    \n\nThaaawwnks.\n\n&amp;#x200B;\n\nPS: I just wanna say, Google Colab might not have the ability to run your own -----> image !!!  like Paperspace does, but it's really hassle-free and I appreciate that. However, I have a Paperspace subscription and I really wanna use it. I just ne", "link": "https://www.reddit.com/r/MachineLearning/comments/oq99g9/d_im_implementing_shallow_and_deep_algorithms_in/"}, {"autor": "xtremelearninmachine", "date": "2021-07-23 16:09:37", "content": "[N] Call for Participation: AAAI-21 Symposium on Science Guided AI to Accelerate Scientific Discovery /!/ We are pleased to inform you that we are organizing a symposium on \u201c[Science-guided AI to Accelerate Scientific Discovery](https://sites.google.com/vt.edu/sgai-aaai-21)\u201d at the [AAAI Fall Symposium Series](https://aaai.org/Symposia/Fall/fss21.php) to be held on **November 4 to 6, 2021**.  \n\n\n&gt;*The goal of this symposium is to nurture the community of researchers working at the intersection of AI and scientific areas and shape the vision of the rapidly growing field of science-guided AI, which aims to systematize the integration of scientific knowledge with AI to produce generalizable and scientifically consistent or meaningful solutions.*\n\n# Organizers:\n\n[Anuj Karpatne](https://people.cs.vt.edu/karpatne/) (Virginia Tech)[Nikhil Muralidhar](https://www.sites.google.com/view/nikhil-muralidhar) (Virginia Tech)[Naren Ramakrishnan](https://dac.cs.vt.edu/person/naren-ramakrishnan/) (Virginia Tech)[Vipin Kumar](https://cse.umn.edu/cs/vipin-kumar) (University of Minnesota)[Ramakrishnan Kannan](https://www.ornl.gov/staff-profile/ramakrishnan-kannan) (ORNL)[Jonghyun 'Harry' Lee](https://www2.hawaii.edu/~jonghyun/) (University of Hawaii at Manoa)\n\n# Topics:\n\nWe encourage participation on topics that explore any form of synergy between scientific principles and AI methods in any interdisciplinary domain of science and engineering. Examples of relevant topics include (but are not limited to):\n\n* `Use of scientific constraints or priors in supervised and unsupervised AI methods,`\n* `Approaches to encode scientific knowledge in deep learning architectures,`\n* `Science-guided generative and reinforcement learning methods,`\n* `Discovery of interpretable scientific laws from data,`\n* `Hybrid constructions of domain-based and machine learning models,`\n* `Architectural and algorithmic improvements enabled by AI in scientific computing,`\n* `Software development facilitating the inclusion of domain knowledge in AI, and`\n* `Use of AI to calibrate parameters and system states in scientific models.`\n\nWe are currently accepting paper submissions for position, review, or research articles in two formats: `(i) short papers (2-4 pages)` and `(ii) full papers (6-8 pages)`. Extended versions of articles in submission at other venues are acceptable as long as they do not violate the dual-submission policy of the other venue. We also encourage early drafts of on-going research with preliminary insights/results that contribute to the symposium agenda. All submissions will undergo peer review and authors will have the option to publish their work in an open access proceedings site.\n\n# Submissions:\n\nSubmissions should be formatted according to the AAAI template (see [Author Kit](https://www.aaai.org/Publications/Templates/AuthorKit20.zip)) and submitted via EasyChair ([https://easychair.org/conferences/?conf=fss21](https://easychair.org/conferences/?conf=fss21)).\n\n# Key Dates:\n\n* **August 20, 11:59 PM Pacific Time**: Paper submissions due\n* **September 3**: Acceptance/rejection letters sent to participants\n* **October 15**: Registration Deadline\n* **September 24**, **11:59 PM Pacific Time**: -----> Camera !!! -ready papers due  \n\n\nWe look forward to your participation! Please feel free to contact us (Dr. Anuj Karpatne: [karpatne@vt.edu](mailto:karpatne@vt.edu), Nikhil Muralidhar: [nik90@vt.edu](mailto:nik90@vt.edu) ) if you have any questions. Regular updates about our symposium can be found at our supplemental website: [https://sites.google.com/vt.edu/sgai-aaai-21](https://sites.google.com/vt.edu/sgai-aaai-21)", "link": "https://www.reddit.com/r/MachineLearning/comments/oq5lp7/n_call_for_participation_aaai21_symposium_on/"}, {"autor": "nadermx", "date": "2021-07-23 15:50:36", "content": "[P] Remove background from video and -----> image !!!  using machine learning, open source /!/ Hi r/MachineLearning,\n\nI was playing around with some things and after a while ended up making [BackgroundRemover.app](https://BackgroundRemover.app), which is a tool to remove background from images and videos.  If you want to see the source code, or play with it, I put it [here](https://github.com/nadermx/backgroundremover).", "link": "https://www.reddit.com/r/MachineLearning/comments/oq57ac/p_remove_background_from_video_and_image_using/"}, {"autor": "[deleted]", "date": "2021-07-23 15:48:08", "content": "[P] Remove background from video or -----> image !!!  using machine learning, open source /!/ [deleted]", "link": "https://www.reddit.com/r/MachineLearning/comments/oq55gf/p_remove_background_from_video_or_image_using/"}, {"autor": "[deleted]", "date": "2021-07-23 15:46:50", "content": "Remove background from video and -----> image !!!  using machine learning, open source /!/ [deleted]", "link": "https://www.reddit.com/r/MachineLearning/comments/oq54h9/remove_background_from_video_and_image_using/"}, {"autor": "TheCockatoo", "date": "2021-07-23 13:18:44", "content": "[D] Subpixel convolution in 1D? /!/ Assume I want to upsample a time series (length L, features N) using 1D convolutions. One approach is 1D deconvolution; in -----> image !!!  super-resolution papers, 2D deconvolution is typically not preferred as it may cause artifacts (does the same hold for 1D inputs?). Instead, super-resolution papers typically apply subpixel convolution. Is there an equivalent to subpixel convolution for 1D inputs?", "link": "https://www.reddit.com/r/MachineLearning/comments/oq28em/d_subpixel_convolution_in_1d/"}, {"autor": "tomsal", "date": "2021-07-23 12:23:26", "content": "[D] Correspondence matching in \"Dense Contrastive Learning for Self-Supervised Visual Pre-Training\" (CVPR 21) /!/ **TL;DR**: Great paper, but I don't understand why correspondence matching needs to be done in the first place. The transformations should be known during the training process.  \n**Relevant paper**: \"Dense Contrastive Learning for Self-Supervised Visual Pre-Training\" ([https://arxiv.org/abs/2011.09157v1](https://arxiv.org/abs/2011.09157v1))\n\n**Context**:  \nThis CVPR paper introduces a technique for dense contrastive learning. That means it does not use a contrastive for a single feature of the entire -----> image !!! , but defines a separate contrastive loss for every feature vector in the 7x7 bottleneck of the backbone network. The idea is that this kind of pretraining learns  more appropriate features for dense downstream tasks, such as object detection or segmentation, than its global (e.g. MoCo) counterpart. One of the main problems they are solving/investigating is how to find positive pairs to use for contrastive learning. They argue that it is necessary to find correspondences between the feature vectors in the 7x7 feature spaces of the two transformed views of the input image. \n\n**Question**:  \nWhy is this correspondence matching necessary in the first place? Since it is possible during the training process to trace the transformations (e.g. cropping, color jitter) that are applied to both views of the input image, I would expect that you could easily derive the overlap of the views and therefore the correspondences of the features. They analyze different approaches to this dense correspondence search in their experiments, but I feel that some explanation why tracing the transformations is not possible or used is missing. Am I missing something?", "link": "https://www.reddit.com/r/MachineLearning/comments/oq1a57/d_correspondence_matching_in_dense_contrastive/"}, {"autor": "ZHKKKe", "date": "2021-07-20 17:56:43", "content": "[Research] Photography Portrait Matting (PPM) Benchmark is Released /!/ I am glad to share that our Photography Portrait Matting (PPM-100) benchmark proposed in the paper MODNet is released now. We hope this new benchmark can help future research on portrait -----> image !!!  matting.\n\nPPM-100 Github: [https://github.com/ZHKKKe/PPM](https://github.com/ZHKKKe/PPM)\n\n[-  A Photography Portrait Matting Benchmark -](https://i.redd.it/hr61b6grkec71.gif)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/9anrtfztkec71.jpg?width=3848&amp;format=pjpg&amp;auto=webp&amp;s=3c2fedecdf7997d446c1eb4d350bb26369adad24\n\nPPM-100 has the following characteristics:\n\n* **Fine Annotation** \\- All images are labeled and checked carefully.\n* **Natural Background** \\- All images use the original background without replacement.\n* **Rich Diversity** \\- The images cover full/half body and various postures.\n* **High Resolution** \\- The resolution of images is between 1080p and 4k.\n\n[ An Example Image in PPM-100](https://preview.redd.it/86wop9evkec71.jpg?width=4264&amp;format=pjpg&amp;auto=webp&amp;s=302f5250979cdec416df0dc7f16d11212ec270a2)\n\nOther related resources:\n\nArxiv Preprint: [https://arxiv.org/pdf/2011.11961.pdf](https://arxiv.org/pdf/2011.11961.pdf)\n\nMODNet Github: [https://github.com/ZHKKKe/MODNet](https://github.com/ZHKKKe/MODNet)", "link": "https://www.reddit.com/r/MachineLearning/comments/oo7b8v/research_photography_portrait_matting_ppm/"}, {"autor": "punims", "date": "2021-07-20 06:45:43", "content": "Latent space optimization for -----> image !!!  translation /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/onwqoi/latent_space_optimization_for_image_translation/"}, {"autor": "jj4646", "date": "2021-07-20 05:32:04", "content": "[D] Importance of Convergence Proof for Gradient Descent /!/ https://imgur.com/a/r7jRcPJ\n\nWe all  probably know the importance of the gradient descent algorithm within the domain of machine learning (e.g. calculating the weights for neural networks) - but not all of have studied the inner details behind the gradient descent algorithm.\n\nFor instance, I was looking at this \"proof of convergence\" for the gradient descent algorithm over here: https://imgur.com/a/r7jRcPJ \n\n1) Just to clarify - in this case, does \"convergence\" mean it will necessarily reach some \"minimum\" (whether \"local\" or \"global\") point after k iterations? \n\nOr does it mean it mean that \"differences in the k-1 and k iteration will be negligible\"? (I am not sure if this statement is characteristic of a \"minimum\" point? i.e the difference between the k-1 and k iteration is ONLY negligible when you reach some sort of minimum point?)\n\n2) for a real-world dataset and choice of algorithm that uses gradient descent, is it ever possible to \"approximate\" the value of k in advance? could you ever (hypothetically) say that \"for this problem, I will approximately need to run gradient descent N number of times before I can reach an error of 0.2?\"? (is this strong convergence or convergence in probability?)\n\n3) are the performance results from an algorithm that uses gradient descent ever \"guaranteed within a certain error bound outside of the data it was exposed to\"? or is this just wishful thinking?\n\n4) https://imgur.com/a/r7jRcPJ in this -----> picture !!! , in the numerator on the term on the right hand side, do the \"two 2's stacked on each other\" refer to \"squared\" and \"l2 norm\"?\n\nif anyone has any other insights they would like to share on the \"importance of convergence for gradient descent\", please feel free to do so!\n\nthanks", "link": "https://www.reddit.com/r/MachineLearning/comments/onvrgb/d_importance_of_convergence_proof_for_gradient/"}, {"autor": "NongHyupJoy", "date": "2021-07-19 19:57:45", "content": "[D] Black-box adversarial attacks and stochastic defensive methods /!/ I know that the paper 'On Evaluating Adversarial Robustness' recommends to do sanity check with black-box attacks to see if robustness comes from the gradient obfuscation. So I see many AA papers do additional experiment with black-box attacks. \n\nHowever, I also am aware that the black-box attacks are not effective on the defensive methods using stochastic factors (injecting random noise to input -----> image !!! , weights or intermediate states ...), but I don't remember the source of it.\n\nI see some papers using random factors as defense do not perform black-box attacks. For example, those two recent papers, [Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network](https://arxiv.org/abs/1810.01279) (Bayesian model with stochastic sampling) and [Simulating a Primary Visual Cortex at the Front of CNNs Improves Robustness to Image Perturbations](https://www.biorxiv.org/content/10.1101/2020.06.16.154542v1.full.pdf) (injecting noise to input image), they don't do any black-box attack. I am not fully confident of the reason why they didn't do black-box attack, but I guess the reason is that their model is utilizing randomness inside. \n\n&amp;#x200B;\n\nI think doing black-box attack is necessary to make sure if it is not from gradient obfuscation even for defenses with randomness. Otherwise, how do one knows if the robustness is due to obfuscation or not?\n\nI just want to know your idea on performing black-box attacks on defensive models with randomness. As those papers I posted did not do black-box attack, do you think it is not necessary to do black-box attack for stochastic defensive methods? (Maybe the reason for those works do not perform black-box attack is not what I expect.)", "link": "https://www.reddit.com/r/MachineLearning/comments/onlvhb/d_blackbox_adversarial_attacks_and_stochastic/"}, {"autor": "Affectionate_Teach23", "date": "2021-07-19 18:35:33", "content": "[P] Pre-trained neural network for checking weapons (guns, knives) on -----> image !!!  /!/ Need a neural network for project to check if the weapon is on -----> image !!!  or not. Nothing appropriate was found on Kaggle, GitHub, and Google. Do you know a pre-trained neural network to apply or where else can I look for it? Or it is better to train it by myself?", "link": "https://www.reddit.com/r/MachineLearning/comments/onk79q/p_pretrained_neural_network_for_checking_weapons/"}, {"autor": "thanrl", "date": "2021-07-19 18:18:41", "content": "[D] To generate images/sentences of certain properties /!/ Say I have a differentiable function f(x) that computes some scaler property (e.g. saturation, negative sentiment) of x, where x is an -----> image !!!  or a sentence. Given a dataset X of images/sentences, how can I modify existing generative models (e.g. autoencoders, sequence-to-sequence models) to generate samples that, instead of replicating X, replicates f(X)?", "link": "https://www.reddit.com/r/MachineLearning/comments/onjunq/d_to_generate_imagessentences_of_certain/"}, {"autor": "KirillTheMunchKing", "date": "2021-07-19 15:00:34", "content": "[D] BYOL explained in 5 minutes: Bootstrap Your Own Latent A New Approach to Self-Supervised Learning by Jean-Bastien Grill et al. /!/ Is it possible to learn good enough -----> image !!!  representations for many downstream tasks at once?\n\nA well known approach is to use self-supervised pretraining such as state-of-the art contrastive methods that are trained to reduce the distance between representation of augmented views of the same image (positive pairs) and increasing the distance between representations of augmented views of different images. These methods need careful treatment of negative pairs, whereas **BYOL achieves higher performance than SOTA contrastive methods without using negative pairs** at all. Instead it uses two networks that learn from each other to iteratively bootstrap the representations by forcing one network to use an augmented view of an image to predict the output of the other network for a different augmented view of the same image. Sounds crazy, I know... but it actually works!\n\nRead the [full paper digest](https://t.me/casual_gan/68) and the [blog post](https://www.casualganpapers.com/self-supervised/representation-learning/online-target-networks/2021/07/13/BYOL.html) (reading time \\~5 minutes) to learn about using an online and a target networks to make self-supervised learning work without using any negative pairs during training as well as the general intuition why SSL works in the first place.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://www.casualganpapers.com/)!\n\n[BYOL paper poster](https://preview.redd.it/n9nusoxyo6c71.png?width=571&amp;format=png&amp;auto=webp&amp;s=005ec3eb61133c41f909ab19576806530a7a6723)\n\n\\[[Full Explanation Post](https://t.me/casual_gan/68) / [Blog Post](https://www.casualganpapers.com/self-supervised/representation-learning/online-target-networks/2021/07/13/BYOL.html)\\] \\[[Arxiv](https://arxiv.org/pdf/2006.07733.pdf)\\] \\[[Code](https://github.com/deepmind/deepmind-research/tree/master/byol)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n&gt;\\[[Deferred Neural Rendering](https://t.me/casual_gan/66)\\]  \n&gt;  \n&gt;\\[[SimCLR](https://www.casualganpapers.com/self-supervised/representation-learning/online-target-networks/2021/07/13/SimCLR.html)\\]  \n&gt;  \n&gt;\\[[GIRAFFE](https://t.me/casual_gan/63)\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/onfril/d_byol_explained_in_5_minutes_bootstrap_your_own/"}, {"autor": "erasperiko", "date": "2021-07-19 09:54:34", "content": "Deep Learning should not be blindly applied to EVERY problem out there \"[Discussion]\" /!/ Since the upsurge of Deep Learning (DL) from CNN in the -----> image !!!  realm and from RNN for NLP, there is a clear trend towards this type of modeling technique. Mainly in context-specific areas such as finance, traffic, energy, etc, seems like every congress you attend for more than half of the publications are like this: \"So we have these datasets that have been feed to this exotic DL architecture. Our results excel the state of the art in a 0.1%. That is our contribution.\"\n\nI understand that it is an easy way to publish a paper as this kind of modeling technique is a hot topic right now. However, in my research context (which can be defined as 'traffic state prediction') can be disadvantageous. In a recent paper ([https://arxiv.org/abs/2012.02260](https://arxiv.org/abs/2012.02260)), we demonstrate that for a time series prediction approach, where data is self-descriptive (meaning that as a human being you can easily interpret the current values of the series) DL architectures do not overperform another less computational demandant algorithms such as the *all mighty* Random Forest.\n\nThe core idea behind this investigation is to shed light on an overlooked issue in this field: researchers apply modeling techniques without considering the endemic characteristics of their data collection:\n\n* Is your data self-descriptive? \n* Is your data Euclidean?\n* Are you going to benefit from the data fusion capabilities of DL?\n\nI am only in my first research years but after reviewing lots of papers about this topic my concerns can be summarized as the above, and frankly, I do not like where this is going.", "link": "https://www.reddit.com/r/MachineLearning/comments/onax7l/deep_learning_should_not_be_blindly_applied_to/"}, {"autor": "Beginning_Income6840", "date": "2021-07-18 05:16:22", "content": "[R] Feature Engineering based on Memes for Trading Strategies /!/ \nHello toghter, we are a group of researches studying the impact of \u201eMemes\u201c on low market cap equities. We are about to publish a paper that found a strong predictive correlation between memes and deltas on equities ( GME, AMC, Crypto etc). As we want to analyze this on a larger scale for a longer time Periode, we are putting the code into production letting it run for a long time to further study the impact. As I am the only engineer currently active involved in the Project, we are looking for someone with experience in ML Engineering and cloud services. We are a team of a php student in Statistics and the professor of the department of statistics at the university of Augsburg. We are planning to keep on publishing papers about that and have multiple Deep-Learning projects at hand. I am putting around 10 hours a week into the project, so manageable time. We are also studying -----> image !!!  classification models on chart movements for some of the predictions. \nSo what\u2019s in if you wanna join. \n\n- become Co Author in our papers to come \n- studying a very new phenomenon \n- getting hands on experience with DL/ML, Stats, MLops and Cloud solutions. \n- work with highly motivated people \n- and who knows, maybe we are becoming the next Jim Simons ( this was sarcasm) \n\nJust drop me a PM or comment and I set up a meeting. I would be very happy to find some motivate humans from anywhere !", "link": "https://www.reddit.com/r/MachineLearning/comments/omk943/r_feature_engineering_based_on_memes_for_trading/"}, {"autor": "kacchalimbu007", "date": "2021-07-18 04:58:35", "content": "How to convert a single or multiple angle 2D -----> image !!!  into 3D using python machine learning? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/omk0k5/how_to_convert_a_single_or_multiple_angle_2d/"}, {"autor": "KirillTheMunchKing", "date": "2021-07-17 12:46:55", "content": "[D] SimCLR - Self-Supervised contrastive learning Paper explained in 5 minutes: A Simple Framework for Contrastive Learning of Visual Representations by Ting Chen et al. /!/ Is it possible to learn good -----> image !!!  representations for many downstream tasks at once? The authors of SimCLR propose a simple framework for contrastive self-supervised algorithms that is able to learn useful representations. They report several crucial findings:\n\n1. The composition of data augmentations plays a critical role in learning effective representations,\n2. Introducing a learnable nonlinearity between the representations and the contrastive loss substantially improves the quality of the learned representations,\n3. Contrastive learning benefits from larger batch sizes and more steps compared to supervised learning.The proposed approach outperforms previous self-supervised approaches, and matching the performance of ResNet-50 on ImageNet.\n\nRead the [full paper digest](https://t.me/casual_gan/67) or the [blog post](https://www.casualganpapers.com/self-supervised/representation-learning/online-target-networks/2021/07/13/SimCLR.html) (reading time \\~5 minutes) to learn about all the tricks necessary to make self-supervised contrastive learning work.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://www.casualganpapers.com/)!\n\n[SimCLR explained](https://preview.redd.it/35uyxo5drrb71.png?width=1786&amp;format=png&amp;auto=webp&amp;s=30003a80c855d704d34ab0548267eeebc3c3de1d)\n\n\\[[Full Explanation Post](https://t.me/casual_gan/67) / [Blog Post](https://www.casualganpapers.com/self-supervised/representation-learning/online-target-networks/2021/07/13/SimCLR.html)\\] \\[[Arxiv](https://arxiv.org/pdf/1904.12356.pdf)\\] \\[[Code](https://github.com/SSRSGJYD/NeuralTexture)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n&gt;\\[[Deferred Neural Rendering](https://t.me/casual_gan/66)\\]  \n&gt;  \n&gt;\\[[GIRAFFE](https://t.me/casual_gan/63)\\]  \n&gt;  \n&gt;\\[[GRAF](https://t.me/casual_gan/61)\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/om3l0h/d_simclr_selfsupervised_contrastive_learning/"}, {"autor": "Eleonora467", "date": "2021-07-17 11:08:13", "content": "[P] Random -----> Image !!!  in Machine learning classification. /!/ i am training model on breast cancer histopathological dataset ...to categories image as benign and malignant...i have question if someone give the image of aeroplane..or horse...or anyother thing..it still categorize it to benign and malignant...can we put there any other exception that it is random image?", "link": "https://www.reddit.com/r/MachineLearning/comments/om27a4/p_random_image_in_machine_learning_classification/"}, {"autor": "call_me_ninza", "date": "2021-07-17 07:01:25", "content": "-----> Image !!!  processing is a cool field to be explored. As a newbie, here is my first step towards it.", "link": "https://www.reddit.com/r/MachineLearning/comments/olzddx/image_processing_is_a_cool_field_to_be_explored/"}, {"autor": "annoying_seagull", "date": "2021-07-26 23:00:22", "content": "[D] Help with implementing Hopfield Network in python. Network doesn't converge to a learned pattern. /!/  \n\nI am trying to implement a Hopfield Network in python. Most of the time the network doesn't converge to a learned pattern.\n\nI store these patterns in a 9 units long Hopfield network.\n\n\\[1,1,1,1,1,1,1,1,1\\],\n\n\\[1,1,1,1,1,1,-1,-1,-1\\],\n\n\\[1,-1,-1,-1,-1,-1,-1,-1,-1\\]\n\nIf I then try to input \\[1, 1, 1, 1, 1, 1, 1, 1, 1\\]\n\nThe output converges to this \\[-1. 1. 1. 1. -1. -1. 1. 1. -1.\\]\n\nwhich is not a learned pattern.  \n\n\n    class HopfieldNetwork:\n        def __init__(self, number_of_nodes, learning_rate = 1):\n            self.number_of_nodes = number_of_nodes\n            self.learning_rate = learning_rate\n            self.weights = np.zeros((number_of_nodes, number_of_nodes))\n        \n        # Randomly fire nodes until the overall output doesn't change\n        # match the pattern stored in the Hopefield Net.\n        def calculate_output(self, input):\n            changed = True\n            input = np.array(input)\n            c=0\n            while changed and c&lt;200: #doesnt always converge so I have to stop it\n                changed= False\n                c=c+1\n                if c&gt;18:\n                    print(\"beeeeeeeeeeeeeeeeeeeeeep\")\n                indices = list(range(len(input)))\n                random.shuffle(indices)\n                new_input = np.zeros((self.number_of_nodes))\n                clamped_input = input.clip(min=-900000) # eliminate nodes with negative value, doesn't work either way\n                for i in indices:\n                    sum = np.dot(self.weights[i], clamped_input)\n                    new_input[i] = 1 if sum &gt;= 0 else -1\n                    changed = not np.allclose(input[i], new_input[i], atol=1e-3)\n                input = np.array(new_input)\n            return np.array(input)\n    \n        def calculate_output2(self,input):\n            output = np.dot(self.weights,input)\n            # apply threshhold\n            output[output &gt;= 0] = 1 # green in -----> image !!! \n            output[output &lt; 0] = -1 # purple in image\n            return output\n        \n        # Store the patterns in the Hopfield Network\n        def learn(self, input):\n            # hebian learning\n            I = np.identity(self.number_of_nodes) # diagnol will always be 1 if input is only 1/-1\n            updates = self.learning_rate * np.outer(input,input) - I\n            updates = updates/self.number_of_nodes\n            self.weights = self.weights + updates\n        \n        def calculate_energy(self, input):\n            I = np.identity(self.number_of_nodes) # diagnol will always be 1 if input is only 1/-1\n            cross_product = np.outer(input,input) - I\n            energy = -(1/2) * np.sum(self.weights * cross_product)\n            return energy", "link": "https://www.reddit.com/r/MachineLearning/comments/osadso/d_help_with_implementing_hopfield_network_in/"}, {"autor": "kevinzakka", "date": "2021-07-26 21:07:37", "content": "[P] CLIP: Death of the Class Map /!/ * Blog Post: https://blog.kzakka.com/posts/clip/\n* Colab Repo: https://github.com/kevinzakka/clip_playground\n\nThis blog post showcases some creative uses of CLIP's zero-shot capabilities, from reCAPTCHA solving to text prompted detection. It also highlights a bunch of projects that leverage CLIP for prompt-conditioned -----> image !!!  generation. All packaged with love using Colab.", "link": "https://www.reddit.com/r/MachineLearning/comments/os851a/p_clip_death_of_the_class_map/"}, {"autor": "answersareallyouneed", "date": "2021-07-26 18:50:43", "content": "[D] Replacing the Cycle-Consistency Loss in CycleGAN /!/ I'm currently using a variant of CycleGAN to preprocess synthetic imaging data before I use it for training another model.\n\nShape-wise the real and synthetic datasets are very similar, but the synthetic data is a bit simpler than the real data and has some very vibrant colors not present in the real data.\n\nOne of the issues I'm running into is that often times these colors/textures from the synthetic data will \"bleed into\" the produced real data. I assume that this is caused by the Cycle-Consistency loss. When I use a non-cycle method (Relying on VGG features to preserve similarity), this bleeding is no longer as big of a problem (However, there's a fair drop in produced -----> image !!!  quality).\n\nMy question is whether I can forgo the Cycle-Consistency loss on the actual images and instead use some sort of segmentation-based loss to enforce consistency. Note: I can easily obtain perfect semantic labels for just the synthetic data, but I do not have labels for the real data.\n\nIdeas:\n\n\\* Consistency between segmentation (For synth and predicted real) - 2 segmentation models are trained (One for synth and one for real) and the results should match between synthetic and predicted real data.\n\n\\* Segmentation cycle-consistency (For all) - For (Real, Pred Synth) and (Synth, Pred Real) pairs, the segmentation should be same before and after.\n\n\\* Adv loss (On seg encoder features) - Enforce feature similarity between pairs (Real, Pred Real) and (synth, Pred Synth) for the segmentation models.\n\n\\* (Maybe) Convert from segmentation maps back to original image (For both domains) - Use an architecture somewhat similar to Nvidia's SPADE.\n\nI've seen similar ideas to the first 3 in papers, but they usually pretrain using/continue to use cycle-consistency loss to train. \n\nMy initial thoughts:\n\nI feel like this would be a nightmare to train (Even disregarding the SPADE model). If I'm not using pretrained models for the segmentation, I think I'd probably have model stability issues. \n\nOn the flip side, even if I had a single pretrained segmentation model which works ok (Not perfectly) for both datasets, it may not work on the images produced by the generators (Somewhere \"in between\" both domains).\n\nThoughts? Has anyone had success(or tried and failed) with similar ideas?", "link": "https://www.reddit.com/r/MachineLearning/comments/os5b7h/d_replacing_the_cycleconsistency_loss_in_cyclegan/"}, {"autor": "techsucker", "date": "2021-07-26 17:57:09", "content": "[R] Researchers From Tel Aviv University Propose LARGE, Latent-Based Regression through GAN Semantics /!/ Researchers at Tel-Aviv University have created a method to solve regression tasks with little supervision effectively. The researchers noted that GANs are great for encoding semantic information into the latent space, and they see this as important in modern generative frameworks. This means smooth linear directions, which affect -----> image !!!  attributes disentangled. These directions have been used in GAN-based image editing for years. The research team found that these directions are not only linear, but the magnitude of change induced on respective attributes also follows a relatively flat pattern no matter how far they go along them. This new method uses this observation to turn any pre-trained GAN into a regression model by leveraging as few as two labelled samples instead of many more previously required.\n\nQuick Read: [https://www.marktechpost.com/2021/07/26/researchers-from-tel-aviv-university-propose-large-latent-based-regression-through-gan-semantics/](https://www.marktechpost.com/2021/07/26/researchers-from-tel-aviv-university-propose-large-latent-based-regression-through-gan-semantics/) \n\nPaper: https://arxiv.org/pdf/2107.11186.pdf\n\nCode: https://github.com/YotamNitzan/LARGE", "link": "https://www.reddit.com/r/MachineLearning/comments/os46u1/r_researchers_from_tel_aviv_university_propose/"}, {"autor": "lukedelray", "date": "2021-07-26 15:28:41", "content": "[P] Computer vision installation /!/ I want to produce an installation guided by an AI similar to the Cambridge analytical software, which has a data point view that can spy on user activity. My Cambridge Analytica replica, rather than push ads on social media for political parties, has to produce generative artwork similar to ads that reveal users\u2019 things like their favorite brand or something obsessed.\n\nThe AI has to control the entire installation. Users have to connect to the wifi/web app and agree to the term and conditions giving me access to their data.\n\nComp. vision is controlled by microcontrollers (thinking of esp32 cam but not very high quality though) and ml model on the web. The web app is  I will web scrape socials (thinking of Instagram/Facebook API data access)  authenticated user that connects to the app. With the esp32 cam, I will auth users and do video behavioural analysis when they join. The ml on the web app controls the microcontroller. The esp32 -----> camera !!!  has fixed space movements thanks to two servos that allow for turning 180 degrees both ways up or down on x y coordinate to track users.\n\n[ttps://www.linkedin.com/pulse/recreating-cambridge-analytica-python-stack-daniel-roy-cfa](https://www.linkedin.com/pulse/recreating-cambridge-analytica-python-stack-daniel-roy-cfa)\n\nPython seems slow, Any advice on how to structure the project?", "link": "https://www.reddit.com/r/MachineLearning/comments/os12ca/p_computer_vision_installation/"}, {"autor": "quit_daedalus", "date": "2021-07-26 15:27:54", "content": "[D] Does it make sense to do weight decay scheduling alongside learning rate scheduling? /!/ I am currently using an AdamW optimizer for an -----> image !!!  reconstruction task, and I'm decreasing the learning rate every 5000 iterations by half. What I've seen is that this makes the model a bit unstable around its minima, and the loss fluctuates a bit when the model converges to a minimum.\n\nWhat I'm skeptical of is that maybe the fixed weight decay is becoming way more important in comparison of the decreasing learning rate later in the training phase, and maybe decreasing the weight decay rate might be beneficial.\n\nHas there ever been a suggestion of doing so might be a good option? I know that I can try but I also don't have a lot of resources and experimenting a lot is not a very goot option for me.", "link": "https://www.reddit.com/r/MachineLearning/comments/os11qw/d_does_it_make_sense_to_do_weight_decay/"}, {"autor": "jdxyw", "date": "2021-07-26 11:35:16", "content": "[D]Would you pay for text generation or TTS (text-to-speech) service? /!/ Hi all\n\nJust a thought.\n\nAs we all know that there are lots of companies are doing NLP or TTS, but can't find a good business mode to make money from the AI/ML technologies. So, just want to know, if there is a company can provide good technology, such as text generation or -----> image !!!  generation for specified filed like news or online education, would you pay for it ? Or do you know there is any company has already done it?\n\nBest Regards,", "link": "https://www.reddit.com/r/MachineLearning/comments/orwvv3/dwould_you_pay_for_text_generation_or_tts/"}, {"autor": "techsucker", "date": "2021-07-26 05:25:29", "content": "[R] Using The Diffusion Model, Google AI Is Able To Generate High Fidelity Images That Are Indistinguishable From Real Ones /!/ Using super-resolution diffusion models, Google\u2019s latest super-resolution research can generate realistic high-resolution images from low-resolution images, making it difficult for humans to distinguish between composite images and photos. Google uses the [diffusion model](https://ai.googleblog.com/2021/07/high-fidelity------> image !!! -generation-using.html) to increase the resolution of photos, making it difficult for humans to differentiate between synthetic and real photos.\n\nGoogle researchers [published a new method of realistic image generation](https://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html), which can break through the limitations of diffusion model synthesis image quality, by combining iterative refinement (SR3) algorithm, and a type called Cascaded Diffusion Models (CDM) Conditional synthesis model, the quality of the generated image is better than all current methods.\n\n**Quick Read:** [https://www.marktechpost.com/2021/07/25/using-the-diffusion-model-google-ai-is-able-to-generate-high-fidelity-images-that-are-indistinguishable-from-real-ones/](https://www.marktechpost.com/2021/07/25/using-the-diffusion-model-google-ai-is-able-to-generate-high-fidelity-images-that-are-indistinguishable-from-real-ones/) \n\n**Image Super-Resolution via Iterative Refinement \\[Paper\\]:** https://arxiv.org/abs/2104.07636\n\n**Cascaded Diffusion Models for High Fidelity Image Generation \\[Paper\\]:** [https://cascaded-diffusion.github.io/assets/cascaded\\_diffusion.pdf](https://cascaded-diffusion.github.io/assets/cascaded_diffusion.pdf)\n\n**Google Blog:** https://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html", "link": "https://www.reddit.com/r/MachineLearning/comments/ors7ht/r_using_the_diffusion_model_google_ai_is_able_to/"}, {"autor": "Rogitus", "date": "2021-07-25 21:14:07", "content": "[P] DeepLab v3 - whole architecture scheme? /!/ I cannot find an -----> image !!!  that shows the whole architecture of the DeepLab v3 model... While in the papers they just describe some of its module... \n\nIn DeepLab3+ instead they published the whole architecture\n\nYou know where can I find a summary of the whole architecture for DeepLab V3?", "link": "https://www.reddit.com/r/MachineLearning/comments/ork5h9/p_deeplab_v3_whole_architecture_scheme/"}, {"autor": "Equal-Foundation4161", "date": "2021-07-25 13:58:29", "content": "[D] -----> Image !!!  segmentation methods. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/orc1he/d_image_segmentation_methods/"}, {"autor": "amaigmbh", "date": "2021-03-03 17:55:05", "content": "[N] i.am.ai Newsletter Issue #9 with wav2vec2 implementation, SOTA -----> image !!!  compression and more (link in comments) /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/lwzzz0/n_iamai_newsletter_issue_9_with_wav2vec2/"}, {"autor": "irukadesune", "date": "2021-03-03 08:30:03", "content": "[P]Is there any way I can implement a Bayesian Network Algorithm on single variable dataset like -----> film !!!  reviews? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/lwozqt/pis_there_any_way_i_can_implement_a_bayesian/"}, {"autor": "papersCruncher3528", "date": "2021-03-03 07:58:30", "content": "[R] Generative Adversarial Transformers (2103.01209) /!/ **Link:** [https://arxiv.org/abs/2103.01209](https://arxiv.org/abs/2103.01209)  \n**PDF:** [https://arxiv.org/pdf/2103.01209.pdf](https://arxiv.org/pdf/2103.01209.pdf)  \n**Github:** [https://github.com/dorarad/gansformer](https://github.com/dorarad/gansformer)\n\n**Abstract:** We introduce the GANsformer, a novel and efficient type of transformer, and explore it for the task of visual generative modeling. The network employs a bipartite structure that enables long-range interactions across the -----> image !!! , while maintaining computation of linearly efficiency, that can readily scale to high-resolution synthesis. It iteratively propagates information from a set of latent variables to the evolving visual features and vice versa, to support the refinement of each in light of the other and encourage the emergence of compositional representations of objects and scenes. In contrast to the classic transformer architecture, it utilizes multiplicative integration that allows flexible region-based modulation, and can thus be seen as a generalization of the successful StyleGAN network. We demonstrate the model's strength and robustness through a careful evaluation over a range of datasets, from simulated multi-object environments to rich real-world indoor and outdoor scenes, showing it achieves state-of-the-art results in terms of image quality and diversity, while enjoying fast learning and better data-efficiency. Further qualitative and quantitative experiments offer us an insight into the model's inner workings, revealing improved interpretability and stronger disentanglement, and illustrating the benefits and efficacy of our approach. An implementation of the model is available at [https://github.com/dorarad/gansformer](https://github.com/dorarad/gansformer).  \n\n\nhttps://preview.redd.it/a25l6rs9prk61.png?width=945&amp;format=png&amp;auto=webp&amp;s=927b31fdd81c459e3bfb51d4429c7409307ea721", "link": "https://www.reddit.com/r/MachineLearning/comments/lwoj5i/r_generative_adversarial_transformers_210301209/"}, {"autor": "kochkinael", "date": "2021-03-02 23:44:32", "content": "[R] [N] Conference for Truth and Trust Online Calls for Paper and Talk Proposal Submissions /!/ Are you working on fact-checking, detection of misinformation, hate speech or other problems related to trust and truth online? You can now **submit a paper or talk proposal to the Truth and Trust Online 2021 (TTO 2021).** [**https://truthandtrustonline.com/call-for-papers-2/**](https://truthandtrustonline.com/call-for-papers-2/)\n\nThe annual **Conference for Truth and Trust Online** is organised as **a unique collaboration between practitioners, technologists, academics and platforms**, to share, discuss, and collaborate on useful technical innovations and research in the space. This year\u2019s **TTO is virtual and will take place online Oct 7-8 2021.**\n\nWe welcome technical papers of the following types: surveys, methods, reproduction papers, resource papers, case studies.\n\n**Topics of interest include:**\n\n* Misinformation and disinformation\n* Trustworthiness of COVID-19 news and guidance\n* Hate speech\n* Online harassment and cyberbullying\n* Credibility and fake reviews\n* Hyper-partisanship and bias\n* -----> Image !!! /video/audio verification\n* Fake amplification, polarization, and echo chambers\n* Transparency in content and source moderation\n* Privacy and anonymity requirements\n\nWe encourage wide participation from all interested parties and stakeholders on online media, including academics, startups and large industry, non-profit organizations and governmental institutions.\n\n**Technical paper submission deadline: July 30, 2021**\n\n**Talk proposal submission deadline: August 13, 2021**\n\nMore details can be found: [https://truthandtrustonline.com/call-for-papers-2/](https://truthandtrustonline.com/call-for-papers-2/)\n\nhttps://preview.redd.it/bm8yvbdubpk61.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=bc3f4681ac46aed6892b5ab5b3f5c1d73df62ba8", "link": "https://www.reddit.com/r/MachineLearning/comments/lwfv51/r_n_conference_for_truth_and_trust_online_calls/"}, {"autor": "torkcoal", "date": "2021-05-19 22:42:12", "content": "[D] How can I run real time pose recognition with out doing it on device? /!/ I want to as close to real time as possible upload images from live -----> camera !!!  from a weak device to an api/cloud and then immediately render the result. What can do this?", "link": "https://www.reddit.com/r/MachineLearning/comments/ngjha8/d_how_can_i_run_real_time_pose_recognition_with/"}, {"autor": "Madame_President_", "date": "2021-05-19 20:14:52", "content": "[N] Twitter: Sharing learnings about our -----> image !!!  cropping algorithm", "link": "https://www.reddit.com/r/MachineLearning/comments/ngfu21/n_twitter_sharing_learnings_about_our_image/"}, {"autor": "Yuqing7", "date": "2021-05-19 15:17:58", "content": "[R] Intelligent Graphic Design: Adobe\u2019s Directional GAN Automates -----> Image !!!  Content Generation for Marketing Campaigns /!/ A research team from Adobe proposes Directional GAN (DGAN), a novel and simple approach for generating high-resolution images conditioned on expected semantic attributes, greatly simplifying the image content generating process for marketing campaigns, websites and banners.\n\nHere is a quick read: [Intelligent Graphic Design: Adobe\u2019s Directional GAN Automates Image Content Generation for Marketing Campaigns.](https://syncedreview.com/2021/05/19/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-22/)\n\nThe paper *Directional GAN: A Novel Conditioning Strategy for Generative Networks* is on [arXiv](https://arxiv.org/abs/2105.05712).", "link": "https://www.reddit.com/r/MachineLearning/comments/ng8l4e/r_intelligent_graphic_design_adobes_directional/"}, {"autor": "rshpkamil", "date": "2021-05-19 15:03:03", "content": "[R] Holistic Video Scene Understanding /!/ Understanding what is in the -----> image !!!  or a video frame is crucial for many computer vision applications. Researchers from Google recently published a paper where they got state-of-the-art results in panoptic segmentation (see below for intuitive explanation). They achieved it by combining information about depth, existing and new objects from two consequent frames.\n\nPaper: [https://arxiv.org/abs/2012.05258](https://arxiv.org/abs/2012.05258)\n\nLink to the blogpost and supplementary material that helps to understand the inverse projection problem can be found [here](https://news.thereshape.co/holistic-video-scene-understanding-with-vip-deeplab).", "link": "https://www.reddit.com/r/MachineLearning/comments/ng87ci/r_holistic_video_scene_understanding/"}, {"autor": "Snoo41531", "date": "2021-05-19 14:43:46", "content": "[P] - timm-vis: Visualizer for PyTorch -----> image !!!  models /!/ There are so many cool visualization techniques for CNNs out there. But, the code implementations of these techniques in a lot of repositories/ libraries seems to be limited to very few models such as VGG or AlexNet. I'm sure all of us would love to visualize and understand our own models. So, I created  [timm-vis](https://github.com/novice03/timm-vis), a library using which you can visualize your image classification models with just a few function calls. So far, I've implemented filter and activation visualizations, maximally activated patches, saliency maps, synthetic image generation, adversarial attacks, feature inversion and deep dream. If you're interested in the project and want to try these methods out on your own models, I encourage you to go through details.ipynb in the repository. I'd love to hear your thoughts, feedback and suggestions.", "link": "https://www.reddit.com/r/MachineLearning/comments/ng7q0s/p_timmvis_visualizer_for_pytorch_image_models/"}, {"autor": "Dunkin_1", "date": "2021-05-19 12:55:27", "content": "[P] Using Google Cloud or Paperspace for U-Net -----> image !!!  segmentation and Tensorflow, possible? /!/ Hy all,\n\nis it possible/feasible to train a TensorFlow U-Net model f\u00fcr image segmentation over cloud services like Google Cloud or Paperspace?\n\nCurrently, I am using my tower pc (even though I am not utilizing the GPU right now) to build a U-Net based image segmentation model. Following this tutorial:\n\n[https://www.youtube.com/watch?v=XyX5HNuv-xE&amp;t=674s](https://www.youtube.com/watch?v=XyX5HNuv-xE&amp;t=674s)\n\nI will move to another country in a few weeks and there I will only have access to my laptop. I worry that my Laptop might not be powerful enough, therefore I am looking for other solutions for training. Would Google Cloud or Paperspace be a solution?\n\nIn general, I am missing a feeling of what is a big/computational expensive model. If I would train with about 10.000 images with U-Net, would this be considered something big?\n\nI appreciate all answers as well as links for sources where I can read up on those topics. Thanks a lot!", "link": "https://www.reddit.com/r/MachineLearning/comments/ng525u/p_using_google_cloud_or_paperspace_for_unet_image/"}, {"autor": "Dunkin_1", "date": "2021-05-19 12:52:54", "content": "Using Google Cloud or Paperspace for U-Net -----> image !!!  segmentation and Tensorflow, possible? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ng503o/using_google_cloud_or_paperspace_for_unet_image/"}, {"autor": "MrLemming2", "date": "2021-05-19 11:30:42", "content": "[Discussion] Fixed sized input to variable sized output /!/ Hello everyone, this is a crosspost from /r/machinelearning, but since I got no reply there I will try my luck here: \n\nThe input is a fixed size -----> image !!! . Let's say the image  contains k objects (say dogs)  that I would like to detect and in addition the relation of these objects to the other objects in the image (say, two dogs are in relation with each other if their respective leashes are held by the same person). Hence I would like to return a vector of size k\\^2 (or matrix of size k x k), where k is the number of detected objects. Is there any architecture that can achieve such a thing?\n\nBest regards and thank your for your time!", "link": "https://www.reddit.com/r/MachineLearning/comments/ng3b9m/discussion_fixed_sized_input_to_variable_sized/"}, {"autor": "thedeepreader", "date": "2021-05-19 05:56:45", "content": "[D] (Paper Overview) VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning /!/ **Video**\n\n[https://youtu.be/MzKDNmOJ67Q](https://youtu.be/MzKDNmOJ67Q)\n\n**Paper**\n\n[https://arxiv.org/abs/2105.04906](https://arxiv.org/abs/2105.04906)\n\n**Abstract**\n\nRecent self-supervised methods for -----> image !!!  representation learning are based on maximizing the agreement between embedding vectors from different views of the same -----> image !!! . A trivial solution is obtained when the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in the learning architecture, that often lack a clear justification or interpretation. In this paper, we introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually. VICReg combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization, and achieves results on par with the state of the art on several downstream tasks. In addition, we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements.", "link": "https://www.reddit.com/r/MachineLearning/comments/nfxkvl/d_paper_overview_vicreg/"}, {"autor": "paprupert", "date": "2021-05-19 02:37:02", "content": "[D] Different Angle Estimate /!/ I was wondering if any work has ever been done on utilizing machine learning to generate different angles of a -----> photo !!! ? E.g. if I take a photo of a dog from the front, it could \"generate\" different angle views.", "link": "https://www.reddit.com/r/MachineLearning/comments/nfto81/d_different_angle_estimate/"}, {"autor": "sockcman", "date": "2021-07-12 22:43:40", "content": "[Project] How to use an -----> image !!!  classifier in production? /!/ I wanted to basically make a 'hot dog or not hot dog' app, but I have no idea how to save a classifier after training and use it in app/website.\n\nDoes anyone have any tips/resources on how I can actually use a classifier in production?", "link": "https://www.reddit.com/r/MachineLearning/comments/oj2e7y/project_how_to_use_an_image_classifier_in/"}, {"autor": "Eodmg", "date": "2021-07-12 21:47:01", "content": "[D] What could possibly cause a test/train loss graph like this? /!/  I'm training an RNN which gets as input an MNIST -----> image !!!  in the first timestep and has to output the correct digit ten timesteps in a row. I'm trying to increase the size of the RNN until I observe epoch-wise double descent, but even for a 40000x40000 internal matrix, it doesn't seem to happen. What I do see are these random sudden drops in test loss, often accompanied by a spike in the train loss. Does anyone have a possible explanation for this? It isn't the result of a drop in the learning rate, since I keep it constant throughout training. Thanks \n\n[Train Loss](https://preview.redd.it/bqfel0f6rua71.png?width=2913&amp;format=png&amp;auto=webp&amp;s=186892819e865b63e3505d599c9565376fbd2c36)\n\n[Test Loss](https://preview.redd.it/06fuf8g4rua71.png?width=2931&amp;format=png&amp;auto=webp&amp;s=7b757eedc2e523ea4e82ed7d0b692cb1d40e8480)", "link": "https://www.reddit.com/r/MachineLearning/comments/oj19x1/d_what_could_possibly_cause_a_testtrain_loss/"}, {"autor": "flow_smith", "date": "2021-07-12 10:45:49", "content": "[D] set matching /!/ Hello,\n\nLet's say we have a set of words S = {w1, w2, w3...w\\_n} detected on one -----> image !!!  where every word has it's relative size (bounding box) in the -----> image !!!  p\\_n. P = {p1, p2, p3, ..., p\\_n} .  \n\n\non the other hand we have a Database DB = {S1, S2, ..., SN} where also every S\\_i is a set of words with their corresponding sizes.  \n\n\nThese words has a lot of spelling mistakes, and we can see them as independent, meaning they dont form sentences.  \n\n\nSo the task is to do set matching, and retrieve the best candidate S\\_j that matches S. \n\nI was wondering if anyone has any guidance to a paper or an interesting research work please ?  \n\n\nBottlenecks: outliers -&gt; a lot of non necessary words can be on S that are not on the ground-truth and so they hurt the matching.  \n\n\nBest Regards,", "link": "https://www.reddit.com/r/MachineLearning/comments/oiortw/d_set_matching/"}, {"autor": "vader532", "date": "2021-07-12 08:57:55", "content": "[D] Are there any -----> image !!!  datasets that offer RAW Image and the corresponding Depth Map? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/oingzl/d_are_there_any_image_datasets_that_offer_raw/"}, {"autor": "goal_it", "date": "2021-08-18 07:51:29", "content": "[D] Is Computer Vision not for me? /!/ BACKGROUND: I'm a Software Engineer with 3+ years of experience in Python, at my first job, I was given options to choose from different techs and I opted for Python as I've been coding in Python during my college days. After that, I was assigned a CV specific project in which I did the major contribution from POC to end-to-end productionized the product. During this process, I have learnt to utilise the open source solutions for CV projects. For the other parts, I can code in Python to build automation scripts, build and integrating REST APIs, use pandas for data wrangling etc.\n\n\nI can research for solutions specific to CV problems using latest research papers, GitHub repos and tutorials or ask for help on Reddit and Facebook groups etc.\n\n\nI can search, tweak and integrate state of the art models for specific problem or utilising SOTA for transfer learning on custom dataset specific to a custom problem as far as resources for some previous similar problem exists.\n\n\nI can do CUDA installation and integration, docker -----> image !!!  building etc.\n\n\nBasically, I can say that I know what, where and how to search for the solutions specific to computer vision.\n\n\nBut, I lack in developing my own state of the algorithm specific to a problem as I think it requires more of academic knowledge about the CV for which I educationally lag with a bachelor degree in CSE!\n\n\nBut when I check for CV specific jobs in big companies, they all require masters and PhDs.\u00a0\n\n\nSince I'm willing to make a switch, so I wonder,\u00a0\n\n\nShould I be more focused towards computer vision or move to backend developer role?\u00a0\n\n\nAre these even skills that companies look for in computer vision specific roles?\n\n\nAm I into applied computer vision?\u00a0\n\n\nThere might be the case that I was not enough challenged in my previous jobs so I started to thinking myself as someone who can come out with a solution!\u00a0\n\n\n\nI'd really appreciate your thoughts in this.\n\n\nThanks for your precious time.\n\n\n\nPS: sorry for so many \"I\"s in the post.", "link": "https://www.reddit.com/r/MachineLearning/comments/p6mmwo/d_is_computer_vision_not_for_me/"}, {"autor": "AsuharietYgvar", "date": "2021-08-18 02:03:51", "content": "[P] AppleNeuralHash2ONNX: Reverse-Engineered Apple NeuralHash, in ONNX and Python /!/ As you may already know Apple is going to implement NeuralHash algorithm for on-device [CSAM detection](https://www.apple.com/child-safety/pdf/CSAM_Detection_Technical_Summary.pdf) soon. Believe it or not, this algorithm already exists as early as iOS 14.3, hidden under obfuscated class names. After some digging and reverse engineering on the hidden APIs I managed to export its model (which is MobileNetV3) to ONNX and rebuild the whole NeuralHash algorithm in Python. You can now try NeuralHash even on Linux!\n\nSource code: [https://github.com/AsuharietYgvar/AppleNeuralHash2ONNX](https://github.com/AsuharietYgvar/AppleNeuralHash2ONNX)\n\nNo pre-exported model file will be provided here for obvious reasons. But it's very easy to export one yourself following the guide I included with the repo above. You don't even need any Apple devices to do it.\n\nEarly tests show that it's can tolerate -----> image !!!  resizing and compression, but not cropping or rotations.\n\nHope this will help us understand NeuralHash algorithm better and know its potential issues before it's enabled on all iOS devices.\n\nHappy hacking!", "link": "https://www.reddit.com/r/MachineLearning/comments/p6hsoh/p_appleneuralhash2onnx_reverseengineered_apple/"}, {"autor": "Xie_Baoshi", "date": "2021-08-17 21:27:56", "content": "[P] Question on custom dataset for VQGAN+CLIP text-to------> image !!!  generator /!/ VQGAN+CLIP provides really impressive generation of images from text. Can it be modified to use Danbooru2020 dataset that released on Gwern website? (It have tagged data, so perhaps would be possible to generate asian-style drawings from given tags)\n\nAlso, if training of a new model is required to use this dataset in VQGAN, then how long it would take in Colab? (I'm a newcomer and have experience only with Stylegan)", "link": "https://www.reddit.com/r/MachineLearning/comments/p6cz7b/p_question_on_custom_dataset_for_vqganclip/"}, {"autor": "Ale_Campoy", "date": "2021-08-17 11:01:10", "content": "[Discussion] Can automatized -----> image !!!  anaylis without any CNN be considered ML? /!/ I work as an image for microscopy images and, of course, it is convenient to automitized the process, in order to create a script that analyze up to hundreds of images. Since the aim is research, the variability (an little quanty) of the data makes it not practical to train any CNN, but better use techniques as thresholding, morphological analysis, gradient anaylisis, statistics, etc...\n\nKnowing as Neural Networks for segmentation works, and it is the state-of-art for computer vision, and that what I do is not similar to this approach: Can it be consider as machine learning, since the aim is to make a computer extract automatically information of image dataset?\n\nBest whishes", "link": "https://www.reddit.com/r/MachineLearning/comments/p61igz/discussion_can_automatized_image_anaylis_without/"}, {"autor": "pranav_gupta77", "date": "2021-08-17 10:07:26", "content": "[D] Image Quality Assessment /!/ Hello. I'm exploring how we can quantitatively access the quality of an -----> image !!! . The parameters to be considered like image appealingness, contrast, brightness, etc. For example, like among the two uploaded images, the first image is more appealing as compared to the second one.\n\nSo how we can quantitatively access both these images in terms of appealingness.\n\nhttps://preview.redd.it/jfaxa5867wh71.jpg?width=3160&amp;format=pjpg&amp;auto=webp&amp;s=a6f75aadb3706bcafb993230d1eb399ca3615826\n\nhttps://preview.redd.it/hyu8b1867wh71.jpg?width=576&amp;format=pjpg&amp;auto=webp&amp;s=1d71804f086b2b04d6e7786ac6969c32852bb477", "link": "https://www.reddit.com/r/MachineLearning/comments/p60u26/d_image_quality_assessment/"}, {"autor": "AeroDEmi", "date": "2021-08-16 23:07:18", "content": "[D] -----> Image !!!  Matting /!/ In the original Image Matting [website](http://www.alphamatting.com/eval_27.php) we have some errors types: SAD, MSE, Gradient and Connectivity.\n\nWhere can I learn more about the Gradient and Connectivity errors?", "link": "https://www.reddit.com/r/MachineLearning/comments/p5rg82/d_image_matting/"}, {"autor": "princealiiiii", "date": "2021-01-20 17:42:26", "content": "[P] DM Crowd Counting - Model of the Day #3 /!/ &amp;#x200B;\n\n \n\n[Using the Gradio interface with the Crowd Counting model. You can try out this interface yourself in the link below!](https://i.redd.it/ivu7bl7jtic61.gif)\n\n \n\nToday's model will be based on the paper at this arXiv link: [Distribution Matching for Crowd Counting](https://arxiv.org/abs/2010.02631). Repo [here](https://github.com/aliabd/dm-count) and interface shown above [here](https://gradio.app/hub/aliabd/dm-count).\n\nCrowd counting is a popular research problem with applications in journalism, human traffic management, and surveillance. The paper linked above proposes a novel approach to crowd counting. These are the existing methods of crowd counting:\n\n1) Detect-then-count method - This method detects every person in the -----> image !!!  and then  counts the number of individuals identified. This technique is not very accurate because it is very sensitive to occlusion and noise.\n\n2) Density map calculations - This method divides the image into regions and estimates the human density in each region. This method is more accurate, especially for larger crowds.\n\nThe paper above uses a variation of existing density map methods. Existing methods take the annotated data (where every human is identified with a location on the image) and process it with a Gaussian to generate smoothened density maps. The loss between predicted output and annotated data is calculated as the difference between the density maps.\n\n DM-Count proposes an alternative method to calculate loss between predicted output and annotation. Named \"Optimal Transport\", this loss function takes the number of humans predicted at each location in the predicted output, and calculates the total amount of movement needed for each human human to go from the prediction to the annotation. This total movement is calculated as the loss function.\n\nThe paper states that this method has tighter error bounds than previous methods of calculating loss, and reduces error compared to SOTA models by 16%.\n\n\\--------------------------------------\n\nI've been working with a lot of newly researched models lately, and I wanted to share the most interesting models I've worked with here. This is part of a series where I post an interesting model along with a description of the research purpose and an interactive interface generated with Gradio. Previous post (Model of the Day #2) [here](https://www.reddit.com/r/MachineLearning/comments/kxax7w/p_dan_super_resolution_model_of_the_day_2/).", "link": "https://www.reddit.com/r/MachineLearning/comments/l1e4vj/p_dm_crowd_counting_model_of_the_day_3/"}, {"autor": "cloneofsimo", "date": "2021-01-20 00:09:47", "content": "[P] Output of GAN to match text's input via clipping to gradient of CLIP (openAI's new model) /!/ &amp;#x200B;\n\nhttps://preview.redd.it/3o45ur0apdc61.png?width=600&amp;format=png&amp;auto=webp&amp;s=8e589725ebf5b9f2c7aa279944c73688f85e4bc1\n\n&amp;#x200B;\n\n**TL;DR : It's a clean, easy to understand repo that allows one to make output of GAN to match input text. (uses CLIP as a transfer layer)**\n\n[https://github.com/cloneofsimo/clipping-CLIP-to-GAN](https://github.com/cloneofsimo/clipping-CLIP-to-GAN) \n\nRecently, openAI proposed CLIP : multimodal transformer based model that can perform incredible wide-domain zero shot tasks. You can read all about it from [openAI's blog](https://openai.com/blog/clip/) and it's [paper](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf). It i On the other hand, DALL-E, which is generative model, has also been released on the same date, but it is currently not-released and probably end up like GPT-3.\n\nMore recently, [Ryan Murdock](https://twitter.com/advadnoun) proposed that good feature visualization should generate some -----> image !!!  that matches the text : mainly, he used SIREN as a set of parameters to optimize over and used autograd to learn the best parameters that generates -----> image !!!  that matches given set of -----> image !!! s.\n\nIn general, this could be done with any kind of deterministic generative model, such as GAN, AE , VQVAE, etc (I think VQVAE would be really good here because DALL-E used it, but the gradient ascent part is still something to implement).\n\n  \nThis repository contains test.py, that in general takes generative model and learnable latent vector to find image matching input text.", "link": "https://www.reddit.com/r/MachineLearning/comments/l0x091/p_output_of_gan_to_match_texts_input_via_clipping/"}, {"autor": "iholierthanthou", "date": "2021-01-19 20:55:45", "content": "[D] Visualize network activation in non-classification models /!/ I have seen a lot of great work when it comes to visualizing attention maps / activation maps / heat maps in CNN based architectures (for eg. [https://github.com/raghakot/keras-vis](https://github.com/raghakot/keras-vis) )\n\nHowever most of my research is around -----> image !!!  synthesis, so I was wondering if anyone had any experience with visualizing networks when it comes to a non classification task.", "link": "https://www.reddit.com/r/MachineLearning/comments/l0t0qx/d_visualize_network_activation_in/"}, {"autor": "CauchySchwartzDaddy", "date": "2021-01-19 19:34:05", "content": "[D] Possible to train a cycle gan on two -----> image !!!  sets with different axes (ie one set is a horizontal -----> image !!!  of an object on the ground and the other is a bird's eye view of the object)? /!/ From the research I've done so far on cycle gans both image sets are usually from the same type of reference point.  Right now I have two different medical datasets of the same area, but one is from the pov of looking towards you, and the other is a pov of above you.\n\nThe goal would be to have a network that translates the two different reference points.  I don't really know if this is possible, so some input would be nice.\n\nOn the contrary if there is an architecture that can do this better than cycle gans, I'd be glad to know.  This is a research topic for a group of people to work on, so I'd rather like to not end up wasting everyone's time by making a network that just turns everything into a black photo.", "link": "https://www.reddit.com/r/MachineLearning/comments/l0radc/d_possible_to_train_a_cycle_gan_on_two_image_sets/"}, {"autor": "prestodigitarium", "date": "2021-04-24 18:21:19", "content": "[P] Gourdian Free Dataset Download: OpenStreetMap Points of Interest (Restaurants, Bars, Grocery Stores, Transit, Shops, Swingers Clubs, Hospitals, etc) /!/ Hi there!\n\nA friend and I are working on something to help people search for, filter, and download subsets of datasets.\n\nWe're excited to share that we've just incorporated all(?) of the points of interest from OpenStreetMap, broken down by group (from their ontologies here https://wiki.openstreetmap.org/wiki/Key:amenity and here https://wiki.openstreetmap.org/wiki/Key:shop )\n\nThe groups are below, with the tags that went into each. \n\nWhat do people think these might be useful for? Maybe making your own version of WalkScore? Perhaps cross referencing with real estate listings to find a house that's within walking distance of a bakery, library, cafe, and pyrotechnics shop? LoveHotelMapper.com? The possibilities are endless!\n\n**Restaurants and Bars**: https://gourdian.net/g/eric/osm_points_of_interest.restaurants_and_bars\nAmenities points of interest labeled with bar, biergarten, cafe, fast_food, food_court, ice_cream, pub, or restaurant.\n\n**Education Services**: https://gourdian.net/g/eric/osm_points_of_interest.education_services\nAmenities points of interest labeled with college, driving_school, kindergarten, language_school, library, toy_library, music_school, school, or university.\n\n**Transportation Related**: https://gourdian.net/g/eric/osm_points_of_interest.transportation_related\nAmenities points of interest labeled with bicycle_parking, bicycle_repair_station, bicycle_rental, boat_rental, boat_sharing, bus_station, car_rental, car_sharing, car_wash, vehicle_inspection, charging_station, ferry_terminal, fuel, grit_bin, motorcycle_parking, parking, parking_entrance, parking_space, or taxi.\n\n**Financial**: https://gourdian.net/g/eric/osm_points_of_interest.financial\nAmenities points of interest labeled with atm, bank, or bureau_de_change.\n\n**Healthcare Facilities**: https://gourdian.net/g/eric/osm_points_of_interest.healthcare_facilities\nAmenities points of interest labeled with baby_hatch, clinic, dentist, doctors, hospital, nursing_home, pharmacy, social_facility, or veterinary.\n\n**Entertainment**: https://gourdian.net/g/eric/osm_points_of_interest.entertainment\nAmenities points of interest labeled with arts_centre, brothel, casino, cinema, community_centre, conference_centre, events_venue, fountain, gambling, love_hotel, nightclub, planetarium, public_bookcase, social_centre, stripclub, studio, swingerclub, or theatre.\n\n**Public Services**: https://gourdian.net/g/eric/osm_points_of_interest.public_services\nAmenities points of interest labeled with courthouse, embassy, fire_station, police, post_box, post_depot, post_office, prison, ranger_station, or townhall.\n\n**Facilities**: https://gourdian.net/g/eric/osm_points_of_interest.facilities\nAmenities points of interest labeled with bbq, bench, dog_toilet, drinking_water, give_box, shelter, shower, telephone, toilets, water_point, or watering_place.\n\n**Waste Management**: https://gourdian.net/g/eric/osm_points_of_interest.waste_management\nAmenities points of interest labeled with sanitary_dump_station, recycling, waste_basket, waste_disposal, or waste_transfer_station.\n\n**Other Amenities**: https://gourdian.net/g/eric/osm_points_of_interest.other_amenities\nAmenities points of interest labeled with animal_boarding, animal_breeding, animal_shelter, baking_oven, childcare, clock, crematorium, dive_centre, funeral_hall, grave_yard, gym, hunting_stand, internet_cafe, kitchen, kneipp_water_cure, lounger, marketplace, monastery, photo_booth, place_of_mourning, place_of_worship, public_bath, public_building, refugee_site, or vending_machine.\n\n**Food Shops**: https://gourdian.net/g/eric/osm_points_of_interest.food_shops\nShops points of interest labeled with alcohol, bakery, beverages, brewing_supplies, butcher, cheese, chocolate, coffee, confectionery, convenience, deli, dairy, farm, frozen_food, greengrocer, health_food, ice_cream, organic, pasta, pastry, seafood, spices, tea, wine, or water.\n\n**General Shops**: https://gourdian.net/g/eric/osm_points_of_interest.general_shops\nShops points of interest labeled with department_store, general, kiosk, mall, supermarket, or wholesale.\n\n**Clothing Shops**: https://gourdian.net/g/eric/osm_points_of_interest.clothing_shops\nShops points of interest labeled with baby_goods, bag, boutique, clothes, fabric, fashion, fashion_accessories, jewelry, leather, sewing, shoes, tailor, watches, or wool.\n\n**Second Hand Shops**: https://gourdian.net/g/eric/osm_points_of_interest.second_hand_shops\nShops points of interest labeled with charity, second_hand, or variety_store.\n\n**Health and Beauty Shops**: https://gourdian.net/g/eric/osm_points_of_interest.health_and_beauty_shops\nShops points of interest labeled with beauty, chemist, cosmetics, drugstore, erotic, hairdresser, hairdresser_supply, hearing_aids, herbalist, massage, medical_supply, nutrition_supplements, optician, perfumery, or tattoo.\n\n**Hardware Shops**: https://gourdian.net/g/eric/osm_points_of_interest.hardware_shops\nShops points of interest labeled with agrarian, appliance, bathroom_furnishing, doityourself, electrical, energy, fireplace, florist, garden_centre, garden_furniture, gas, glaziery, groundskeeping, hardware, houseware, locksmith, paint, security, trade, or windows.\n\n**Furnishing Shops**: https://gourdian.net/g/eric/osm_points_of_interest.furnishing_shops\nShops points of interest labeled with antiques, bed, candles, carpet, curtain, doors, flooring, furniture, household_linen, interior_decoration, kitchen, lamps, lighting, tiles, or window_blind.\n\n**Electronics Shops**: https://gourdian.net/g/eric/osm_points_of_interest.electronics_shops\nShops points of interest labeled with computer, electronics, hifi, mobile_phone, radiotechnics, or vacuum_cleaner.\n\n**Vehicle and Outdoor Shops**: https://gourdian.net/g/eric/osm_points_of_interest.vehicle_and_outdoor_shops\nShops points of interest labeled with atv, bicycle, boat, car, car_repair, car_parts, caravan, fuel, fishing, golf, hunting, jetski, military_surplus, motorcycle, outdoor, scuba_diving, ski, snowmobile, sports, swimming_pool, trailer, or tyres.\n\n**Hobby Shops**: https://gourdian.net/g/eric/osm_points_of_interest.hobby_shops\nShops points of interest labeled with art, collector, craft, frame, games, model, music, musical_instrument, -----> photo !!! , camera, trophy, video, or video_games.\n\n**Stationary and Gift Shops**: https://gourdian.net/g/eric/osm_points_of_interest.stationary_and_gift_shops\nShops points of interest labeled with anime, books, gift, lottery, newsagent, stationery, or ticket.\n\n**Other Shops**: https://gourdian.net/g/eric/osm_points_of_interest.other_shops\nShops points of interest labeled with bookmaker, cannabis, copyshop, dry_cleaning, e-cigarette, funeral_directors, laundry, money_lender, party, pawnbroker, pet, pet_grooming, pest_control, pyrotechnics, religion, storage_rental, tobacco, toys, travel_agency, vacant, weapons, outpost, or user defined.\n\n\nA bit about what we're trying to build:\n\n* Filter (optional), click the button, CSV arrives on your hard drive\n* Downloads are always a single CSV, no bundles with weird directory structures, no other formats\n* CSVs are indexed and filterable by a few column types (lat/long and date/time at the moment), so you can download just the parts you want\n* Open licensed datasets are free to download\n* No signup required for downloading open datasets\n* Search within and across datasets\n\nFeedback welcome! If there are any datasets that you'd like to see added, let us know!", "link": "https://www.reddit.com/r/MachineLearning/comments/mxpnwe/p_gourdian_free_dataset_download_openstreetmap/"}, {"autor": "SrData", "date": "2021-04-24 08:29:53", "content": "[D] Recovering images from bottlenecks or training dataset. /!/  I'm wondering if it is possible to generate the images used to train a CNN or if it is possible to generate the images in the input from the bottleneck. Do you know some papers,  or post that I could check?\n\nI've been investigating this: [https://arxiv.org/pdf/1710.09926.pdf](https://arxiv.org/pdf/1710.09926.pdf) (  -----> Image !!!  Compression: Sparse Coding vs. Bottleneck Autoencoders ) but although similar is not the same.\n\nMy point to investigate this is to understand what is the level of privacy of the images if the bottleneck produced by those images are stored or going beyond, what is the level of privacy of the images used to train a  model.", "link": "https://www.reddit.com/r/MachineLearning/comments/mxfsp8/d_recovering_images_from_bottlenecks_or_training/"}, {"autor": "GorgeousEU", "date": "2021-04-23 22:20:39", "content": "About -----> image !!!  classification using CNN. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/mx6ku6/about_image_classification_using_cnn/"}, {"autor": "Seankala", "date": "2021-04-27 10:40:30", "content": "[D] Is there any work highlighting the effectiveness of using bilinear transformations for certain tasks? /!/ I recently read a paper in computer vision titled [_Learning Deep Bilinear Transformation for Fine-grained Image Representation (Zheng et al., 2019)_](https://arxiv.org/abs/1911.03621) about a particular type of bilinear transformation (coined the \"group bilinear\") and they claim that bilinear transformations work well for fine-grained -----> image !!!  recognition. Some works in natural language processing (in particular relation extraction) also claim that bilinear classifier layers work well.\n\nI'm curious if there's any work out there that details _why_ this may be the case? Most of the material I read claim that the transformation \"learn semantic grouping\" or \"learn pairwise factorization,\" but it usually ends at about that.\n\nAny recommendations or opinions are appreciated. Thanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/mzll3n/d_is_there_any_work_highlighting_the/"}, {"autor": "ronghanghu", "date": "2021-01-13 02:59:57", "content": "[R] Worldsheet: Generate 3D environment out of single 2D -----> image !!!  (Link in Comments) /!/ Paper on arXiv: [https://arxiv.org/abs/2012.09854](https://arxiv.org/abs/2012.09854)\n\nhttps://reddit.com/link/kw7fgw/video/dufr7tcxl0b61/player", "link": "https://www.reddit.com/r/MachineLearning/comments/kw7fgw/r_worldsheet_generate_3d_environment_out_of/"}, {"autor": "[deleted]", "date": "2021-01-13 02:55:04", "content": "Worldsheet: Generate 3D environment out of single 2D -----> image !!!  (Link in Comments)", "link": "https://www.reddit.com/r/MachineLearning/comments/kw7cln/worldsheet_generate_3d_environment_out_of_single/"}, {"autor": "[deleted]", "date": "2021-01-13 02:44:15", "content": "Worldsheet: Generate 3D environment out of single 2D -----> image !!!  (Link in Comments) /!/ [deleted]", "link": "https://www.reddit.com/r/MachineLearning/comments/kw75y0/worldsheet_generate_3d_environment_out_of_single/"}, {"autor": "fredfredbur", "date": "2021-01-12 15:48:21", "content": "[D] How many of you use Python scripts versus notebooks? /!/ I'm curious how many people use Python notebooks like Jupyter or Colab versus just writing Python scripts when working on an ML project. I personally just use scripts at the moment but I'm interested in hearing some reasons why you prefer notebooks instead.\n\n&amp;#x200B;\n\nAdditionally, I'm hoping to get some feedback on the notebook support recently added to the [open-source ML tool, FiftyOne](https://voxel51.com/docs/fiftyone/), that I've been working on. FiftyOne is a [Python API](https://github.com/voxel51/fiftyone) \\+ App that lets you load and explore your -----> image !!!  and video datasets and model predictions to debug your datasets and models. \n\nYou can now load the App in the output cell of a notebook to explore your dataset in the notebook itself, previously you had to launch it in a separate window. \n\nWhile other tools like Tensorboard and Matplotlib have notebook support, their output plots generally don't get updated by code further down in the notebook, like what might happen with FiftyOne.\n\nSince there wasn't really a precedent to follow and I don't have much experience with notebook workflows, I was hoping to get some feedback here about how it could be improved. \n\nYou can try it out in Colab here: [https://colab.research.google.com/github/voxel51/fiftyone/blob/v0.7.1.2/docs/source/tutorials/evaluate\\_detections.ipynb](https://colab.research.google.com/github/voxel51/fiftyone/blob/v0.7.1.2/docs/source/tutorials/evaluate_detections.ipynb)\n\n[View Poll](https://www.reddit.com/poll/kvu4ly)", "link": "https://www.reddit.com/r/MachineLearning/comments/kvu4ly/d_how_many_of_you_use_python_scripts_versus/"}, {"autor": "ykilcher", "date": "2021-01-12 15:06:13", "content": "[D] Paper Explained - OpenAI CLIP: ConnectingText and Images (Full Video Analysis) /!/ [https://youtu.be/T9XSU0pKX2E](https://youtu.be/T9XSU0pKX2E)\n\nPaper Title: Learning Transferable Visual Models From Natural Language Supervision\n\nCLIP trains on 400 million images scraped from the web, along with text descriptions to learn a model that can connect the two modalities. The core idea is a contrastive objective combined with a large batch size. The resulting model can be turned into arbitrary zero-shot classifiers for new -----> image !!!  &amp; text tasks.\n\n&amp;#x200B;\n\nOUTLINE:\n\n0:00 - Introduction\n\n3:15 - Overview\n\n4:40 - Connecting Images &amp; Text\n\n9:00 - Building Zero-Shot Classifiers\n\n14:40 - CLIP Contrastive Training Objective\n\n22:25 - Encoder Choices\n\n25:00 - Zero-Shot CLIP vs Linear ResNet-50\n\n31:50 - Zero-Shot vs Few-Shot\n\n35:35 - Scaling Properties\n\n36:35 - Comparison on different tasks\n\n37:40 - Robustness to Data Shift\n\n44:20 - Broader Impact Section\n\n47:00 - Conclusion &amp; Comments\n\n&amp;#x200B;\n\nPaper: [https://cdn.openai.com/papers/Learning\\_Transferable\\_Visual\\_Models\\_From\\_Natural\\_Language\\_Supervision.pdf](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf)\n\nBlog: [https://openai.com/blog/clip/](https://openai.com/blog/clip/)\n\nCode: [https://github.com/openai/CLIP](https://github.com/openai/CLIP)", "link": "https://www.reddit.com/r/MachineLearning/comments/kvtcpt/d_paper_explained_openai_clip_connectingtext_and/"}, {"autor": "nivter", "date": "2021-01-12 09:10:37", "content": "[P] We tried creating Loving Vincent effect from a single -----> image !!! ", "link": "https://www.reddit.com/r/MachineLearning/comments/kvo8m7/p_we_tried_creating_loving_vincent_effect_from_a/"}, {"autor": "hotpot_ai", "date": "2021-01-12 07:57:46", "content": "[R][D] Differentiable Vector Graphics Rasterization for Editing and Learning /!/ **Abstract**\n\nWe introduce a differentiable rasterizer that bridges the vector graphics and raster -----> image !!!  domains, enabling powerful raster-based loss functions, optimization procedures, and machine learning techniques to edit and generate vector content. We observe that vector graphics rasterization is differentiable after pixel prefiltering. Our differentiable rasterizer offers two prefiltering options: an analytical prefiltering technique and a multisampling anti-aliasing technique. The analytical variant is faster but can suffer from artifacts such as conflation. The multisampling variant is still efficient, and can render high-quality images while computing unbiased gradients for each pixel with respect to curve parameters.\n\nWe demonstrate that our rasterizer enables new applications, including a vector graphics editor guided by image metrics, a painterly rendering algorithm that fits vector primitives to an image by minimizing a deep perceptual loss function, new vector graphics editing algorithms that exploit well-known image processing methods such as seam carving, and deep generative models that generate vector content from raster-only supervision under a VAE or GAN training objective.\n\n**Project**\n\n[https://people.csail.mit.edu/tzumao/diffvg/](https://people.csail.mit.edu/tzumao/diffvg/)\n\n**PDF**\n\n [https://dl.acm.org/doi/pdf/10.1145/3414685.3417871](https://dl.acm.org/doi/pdf/10.1145/3414685.3417871)\n\n**Code**\n\n[https://github.com/BachiLi/diffvg](https://github.com/BachiLi/diffvg)", "link": "https://www.reddit.com/r/MachineLearning/comments/kvncvs/rd_differentiable_vector_graphics_rasterization/"}, {"autor": "garrettlynchirl", "date": "2021-06-06 17:16:48", "content": "[R] Text to image GANs text rendering not being interpreted /!/ Why, when working with text to -----> image !!!  GANs e.g. CLIP and BigGAN, does  text from the prompt sometimes render in the -----> image !!!  as text instead of  interpreting that text? Is the word not understood, is the prompt too  short or is there another reason?", "link": "https://www.reddit.com/r/MachineLearning/comments/ntqlm0/r_text_to_image_gans_text_rendering_not_being/"}, {"autor": "Sirisian", "date": "2021-06-06 10:08:13", "content": "[D] I think all vision researchers should be using event cameras in their research /!/ [Event camera](https://en.wikipedia.org/wiki/Event_camera).\n\nThe motivation of this post is based on driving the adoption and manufacturing of smaller, high quality, and cheaper event cameras which seem to offer much better data for high quality and high framerate applications. This post probably seems obvious to a lot of researchers as it's covered in abstracts, [survey papers](https://arxiv.org/abs/1904.08405), [blogs](https://medium.com/tangram-visions/event-cameras-where-are-they-now-293343754bfd), and talks (from 2014 onward) explaining event cameras and their benefits. The main benefits being getting intensity changes per-pixel and not having to consider under/overexposure nor motion blur in data augmentation pipelines. All of these benefits usually results in less computation required.\n\nThe first paper I'd point to is [\"Event Based, Near Eye Gaze Tracking Beyond 10,000 Hz\"](https://web.stanford.edu/~jnmartel/publication/angelopoulos-2020-event/). One of the hardware requirements of high quality VR/AR is eye tracking (over 250Hz) for foveated rendering. That paper creates what appears to be a perfect foundation for eye-tracking. It's not hard to imagine a miniaturized cellphone-scale event -----> camera !!!  with maybe an ASIC tracking the eye at extremely high quality. As far as I'm aware it's not possible to get that quality with an IR camera and it would use a lot more energy processing the frame data.\n\n[This page has recent projects including one on calibrating event cameras](http://rpg.ifi.uzh.ch/research_dvs.html). It also has a paper on [slowing down time](http://rpg.ifi.uzh.ch/TimeLens.html) which also has a lot of neat applications for [segmentation which others have looked into](https://www.prophesee.ai/2019/09/19/high-speed-counting-event-based-vision/). There's also a paper on there for [monocular depth estimation](http://rpg.ifi.uzh.ch/RAMNet.html) which seems like a perfect application for event cameras.\n\nAnother research area is super resolution algorithms. Granted most try to work on existing color video, but the ability to capture changing intensity values in theory offers much higher quality results for future event-based cameras. There's a number of [papers](https://arxiv.org/abs/1912.01196) on this topic. Reminds me of how our own human vision works with hyperacuity. (I'm fascinated with the concept of pointing a camera at something shaking it a bit and getting incredibly high resolution images).\n\nI think one of the most important uses for event cameras is in the application of low powered [SLAM](https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping). That UZH page above has a few SLAM projects and 3D reconstruction papers, but there are newer papers. The main idea though is event cameras can offer unparalleled sample rates and stability compared to standard camera-based approaches. They handle fast rapid motion much better as they don't have to deal with motion blur as mentioned. This has been discussed since probably before 2014, so it's well known, but limited by the availability of event cameras.\n\nThere's also applications like [optical flow](https://www.youtube.com/watch?v=8kTRJVQSer0). Also things like [tracking fiducial markers](https://arxiv.org/abs/2012.06516). Turns out tracking high contrast images with an event camera works really well.\n\nThere's honestly so many vision applications though that could be researched or improved upon. Simply taking non-event camera research and applying event cameras seems to generally give better results. I was actually kind of surprised Google hasn't converted [mediapipe](https://google.github.io/mediapipe/) to use event cameras yet. Being able to do [pose detection](https://www.youtube.com/watch?v=4uJCVSu9Lf4) with rapid motion is huge.\n\nI think photogrammetry might be the largest open area of research for such cameras. There's reconstruction papers and the super resolution stuff, but I don't think anyone has put it all together yet. In theory one should be able to scan objects with very high resolution with such a camera. (A moving camera at that since it wouldn't have the motion blur issues). I could see a company utilizing this approach out competing current company techniques.\n\nVR/AR and Event Cameras:\n\nLet me paint a picture of how I think VR/AR might work later. A VR headset would use 2 event cameras on the front left and right edges with overlap in their FOV. The headset would use these two cameras for SLAM tracking and high sample rate hand tracking. The headset would also have two event cameras for eye tracking. The controllers would have their own wide-angle event cameras on the top and bottom such that each would perform their own SLAM tracking independent of the headset. (The headset could still track the controllers for extra accuracy, but it wouldn't be necessary). In this setup the controllers essentially never lose tracking.\n\nFor full body tracking there's a few approaches. The controllers I described would have a huge FOV and could in theory do pose tracking, but it's possible to place the controllers such that they can't see the hips/legs. To remedy that one can imagine a small puck with a wide-angle event camera on each foot. With the ability to do pose tracking and SLAM and combined with pose tracking on the controllers they'd have only a few edge cases for pose reconstruction. (So 10 event cameras total for the whole system).\n\nAR would have a similar 4 camera headset design for tracking and eye tracking. One of the issues with AR is that cameras can't track the user's hands fast enough to use them in the 240Hz+ rendering and get perfect hand occlusion. You want fingers to be in front of floating menus and realistically clip them. This involves calculating pixel perfect masks with near zero latency. There's basically always artifacts or a ghosting effect as the sensors aren't fast enough for AR where you're looking at your real hands. (ToF sensors might be fast enough later).\n\nConclusion:\n\nI understand event cameras can be costly or time consuming to work with. There are simulators for them though which can make them approachable. (As far as I know they take in high framerate intensity renders from like Blender, Unreal, etc and output the intensity pixel change events).\n\nWith the advantages of event cameras I see them taking over a lot of use cases for conventional cameras. I could even see films being recorded with event cameras. Not sure how likely that is, but it seems powerful to be able to capture a whole HDR film with no motion blur for CGI editing purposes. (Would be able to extract all the markers in a scene with much higher quality).\n\nI digress, but if someone could push a hardware company to produce a miniature event camera that would be amazing. I know Intel funds research that uses them, but I'm honestly surprised they haven't made their own to replace their T-265 SLAM device. That thing can't handle any sudden movement at all. A company that can produce an affordable small sensor could market it to VR/AR, motion capture, drones, robotics, and cellphones. An event camera on a cellphone would probably make so many things lower-powered like OCR. I digress again. This post culminated from watching a ton of event camera talks online and reading about the potential everyone sees in them.", "link": "https://www.reddit.com/r/MachineLearning/comments/ntii8i/d_i_think_all_vision_researchers_should_be_using/"}, {"autor": "himanshu_maurya", "date": "2021-06-06 03:55:09", "content": "Made a simple neural network based on linear regression to differentiate between a cat -----> image !!!  and a non cat -----> image !!! . [P]", "link": "https://www.reddit.com/r/MachineLearning/comments/ntcyhz/made_a_simple_neural_network_based_on_linear/"}, {"autor": "himanshu_maurya", "date": "2021-06-06 03:50:22", "content": "Made neural network from scratch (uses linear regression) to differentiate between a cat -----> image !!!  and a non cat -----> image !!! ", "link": "https://www.reddit.com/r/MachineLearning/comments/ntcvs0/made_neural_network_from_scratch_uses_linear/"}, {"autor": "mythrowaway0852", "date": "2021-06-05 22:33:13", "content": "[D] Need help understanding the usage of rolling window sequence in a research paper /!/ I'm currently trying to implement this paper [https://www.catalyzex.com/paper/arxiv:2101.02908](https://www.catalyzex.com/paper/arxiv:2101.02908)\n\nin the paper, they calculate subsequences of the original time series data using a rolling window method. See the -----> image !!!  below:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/z8h7vhajxi371.png?width=595&amp;format=png&amp;auto=webp&amp;s=a54be8ae0312df1f1a2c23b976297970135864a3\n\nBut what I don't understand is that how do you calculate the rolling window sequence at time step k=0, when there are no values behind it? Do you just pad the sequence with values of zero or the mean? Or think of it as negative indexing (to me that sounds a little stupid)?", "link": "https://www.reddit.com/r/MachineLearning/comments/nt77x5/d_need_help_understanding_the_usage_of_rolling/"}, {"autor": "HoLeeFaak", "date": "2021-06-05 17:25:29", "content": "[D] bootstrap your own latent - why does it work? /!/ So this discussion is for people who read/know about the paper BYOL, if not you can watch an explantation here:  \n[https://youtu.be/YPfUiOMYOEE](https://youtu.be/YPfUiOMYOEE)\n\nSo after reading the paper and watched some videos, there are stuff that are still unclear to me:\n\n1. An inuition for BYOL was shown, when the authors showed they can use a random freezed network which will never change (not by sgd and not by moving average), and they trained an online network to match the representation of the random network the same way they do in BYOL. The represntation created by this got accuracy of 18% in linear evalution on imagenet, much better than random.  \nI don't get why this would work. If the freezed network would recieve the un-augmented-----> image !!!  and the online network would recieve an augmented-----> image !!! , I can see why it would work, because the online network will learn each -----> image !!!  has one \"anchor\" in the latent space, and each augmented -----> image !!!  would have to get the same latent represntation, and that would make the online network to ignore the augmentation and give it some semantic knowledge. But because the random network can recieve augmented images, each image in the dataset will have \"multiple anchors\", and it seems like the online network will just have to learn noise.\n2. Why do they need to use the \"predictor\" head in the online network? What is it's purpose?  Couldn't the projection head just have the same role? Why can't the networks be symmetric?", "link": "https://www.reddit.com/r/MachineLearning/comments/nt0qio/d_bootstrap_your_own_latent_why_does_it_work/"}, {"autor": "0zeroBudget", "date": "2021-06-04 05:49:11", "content": "[Research] What are recommended frameworks/libraries for live -----> image !!!  segmentation? /!/ Basically the title. Which libraries/frameworks that offer object detection/instance segmentation (TensorFlow 3D, Mask RCNN, OpenCSV, etc.) are recommended? And is it live through the camera feed? (as in, not just a static image but live?)", "link": "https://www.reddit.com/r/MachineLearning/comments/nrxt4t/research_what_are_recommended_frameworkslibraries/"}, {"autor": "0zeroBudget", "date": "2021-06-04 05:47:58", "content": "What are recommended frameworks/libraries for live -----> image !!!  segmentation? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/nrxsgl/what_are_recommended_frameworkslibraries_for_live/"}, {"autor": "SirCortGodfrey", "date": "2021-06-04 03:18:55", "content": "[D] PIFuHD for video instead of still -----> image !!!  /!/  I've been really curious about using this tool for a dance-film project I'm working on, and have had success with it using single images. I have tried exporting a png sequence from my video file, but don't know if you can batch upload/export because I'm not very coding/tech savvy. So I was curious if anyone has used this successfully for video/obj sequences?", "link": "https://www.reddit.com/r/MachineLearning/comments/nrvcg9/d_pifuhd_for_video_instead_of_still_image/"}, {"autor": "Simusid", "date": "2021-06-04 00:08:29", "content": "[D] Comment on this Binary Image Classifier Architecture - Pairs of Images as Input /!/ I'm looking for some feedback on an architecture that I'm experimenting with.  It seems to be performing well but I really want to make sure what I'm doing is fair.   \n\nMy problem is a simple binary classifier.   I have 80k labled images.  They are often subtly different, think foggy or blurry images.  Because of that, traditional 4-6 layer Conv2D networks perform no better than random chance.  I've tried dozens and dozens of variations on that theme but it never even showed a hint of training, even with augmentation.  Traditional transfer learning didn't work because these images really don't share the feature space of VGG16 and other existing models.\n\nBesides being challenging images, I also have a class balance problem. I have 30% of class A and 70% of class B.  In short, I don't have enough images to train from scratch, Class A is the important class, and it's \"expensive\" to get more images.  \n\nHere's my new approach, I take pairs of images.  In my use case I only care \"is Class A present?\"   I noticed that if I concatenate two -----> image !!! s and treat that as one training -----> image !!! , then my possible inputs are AA, AB, BA, BB.  Then notice that corresponds to probabilities of (0.3\\*0.3), (0.3\\*0.7), (0.7\\*0.3), (0.7, 0.7).  If I combine those into \"Class A is present in either image\" and \"Only Class B\" my outputs are suddenly balanced at 0.51 vs 0.49 and also I am now drawing randomly from a pool of 80k \\* 80k images, 6.4B images.\n\nAll of my original candidate models immediately went from coin flip (garbage) to an AUC of 0.85+ consistently on fairly generated holdout data that the model had never seen.\n\nI have to defend this. (\"Does the defense's case hold water?\" if you are a fan of My Cousin Vinny)  Is my approach valid?", "link": "https://www.reddit.com/r/MachineLearning/comments/nrrv9o/d_comment_on_this_binary_image_classifier/"}, {"autor": "sk81k", "date": "2021-06-03 21:28:20", "content": "Plotting a decision tree where each node is a figure [D] /!/ Does anyone know how I can plot a binary tree where each node is a figure (eg, seaborn graph) *in python*? I know dtreeviz can do something similar, but I\u2019d like to specify what each -----> image !!!  will be. Thank you in advance", "link": "https://www.reddit.com/r/MachineLearning/comments/nrohy9/plotting_a_decision_tree_where_each_node_is_a/"}, {"autor": "techsucker", "date": "2021-06-03 15:19:37", "content": "[R] UCSD Researchers Develop An Artificial Neuron Device That Could Reduce Energy Use and Size of Neural Network Hardware /!/ Researchers at the University of California San Diego developed a novel artificial neuron device, with the help of which training neural networks to perform tasks like -----> image !!!  recognition or self-driving car navigation could require less computer power and hardware. The gadget uses 100 to 1000 times less energy and space than current CMOS-based technology to perform neural network computations. The work has been published in a paper in Nature Nanotechnology.\n\nThe basic idea behind Neural networks is that each layer\u2019s output is fed as the input to the next layer. And to generate those inputs, a non-linear activation function is required. However, because this function entails transmitting data back and forth between two different units \u2013 the memory and an external processor \u2013 it necessitates a significant amount of computational power and circuitry.\n\nPaper Summary: [https://www.marktechpost.com/2021/06/03/ucsd-researchers-develop-an-artificial-neuron-device-that-could-reduce-energy-use-and-size-of-neural-network-hardware/](https://www.marktechpost.com/2021/06/03/ucsd-researchers-develop-an-artificial-neuron-device-that-could-reduce-energy-use-and-size-of-neural-network-hardware/) \n\nPaper: https://www.nature.com/articles/s41565-021-00874-8", "link": "https://www.reddit.com/r/MachineLearning/comments/nrfzhk/r_ucsd_researchers_develop_an_artificial_neuron/"}, {"autor": "thucydidestrapmusic", "date": "2021-06-03 14:08:20", "content": "[P] Using ML to identify a particular object in a large collection of GSV -----> image !!!  data? /!/ Last month, a six year old boy was fatally shot in a road rage incident on his way to kindergarten. The police have released an image of [the killer's vehicle](https://www.aiden-reward.com/) but thus far there have been no arrests. Because the crime took place around 8 AM, there's a good chance the killer lives or works somewhere near the crime scene.\n\nAccording to this 2020 [research paper](https://dspace.cvut.cz/bitstream/handle/10467/86098/F3-DP-2020-Burde-Varun-Final_report.pdf?sequence=-1&amp;isAllowed=y) (pdf), it should be feasible to use the Google Streetview API for Python to bulk download GSV images within a specified area (in this case, Orange County, California), then automate the process of searching for this vehicle. Our search is aided by the fact that this particular model of Volkswagen is relatively uncommon.  \n\n\n&gt;\\[The author's combination of GSV data and CNN w/ TensorFlow computational backing\\] can result in an application that can be used to find different objects located in space. **A neural network could be trained to find a particular object and find it in the area.** Further, it can be trained to detect an anomaly in the vast data set, which is quite dull and tedious work for a human.\n\nGSV data [isn't free](https://developers.google.com/maps/documentation/streetview/usage-and-billing) (roughly $6-7 USD per 1000 images) but with a **$450,000 reward for tips leading to an arrest** and the potential of putting a child's killer behind bars, could this be an interesting project for someone in the ML community? Is it even technically feasible?", "link": "https://www.reddit.com/r/MachineLearning/comments/nredlk/p_using_ml_to_identify_a_particular_object_in_a/"}, {"autor": "thucydidestrapmusic", "date": "2021-06-03 14:04:46", "content": "Using ML to identify a particular object in a large collection of GSV -----> image !!!  data? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/nreat4/using_ml_to_identify_a_particular_object_in_a/"}, {"autor": "budaria", "date": "2021-06-03 13:21:26", "content": "[D] Data transfer(image features) between different models in separate docker containers /!/ I have two docker containers:\n\nOne has bottom-up-attention model which extracts -----> image !!!  features and boxes:\n\n[https://github.com/airsplay/py-bottom-up-attention](https://github.com/airsplay/py-bottom-up-attention)\n\nThe other one uses given features for predicting captions:\n\n[https://github.com/a-----> image !!! lab/meshed-memory-transformer](https://github.com/a-----> image !!! lab/meshed-memory-transformer)\n\nWhat would be the best data-pipeline for production server?  \nHave  separate web server which sends received image/URL to #1 over POST and  then sends received features to #2?  Or have one of the servers handle  the communication with the other one(#1 receives image, sends features  to #2 and proxies the response back to client, or vice versa, #2 does  the communication/proxy job).\n\nMax size of features will be around 1.5-2 mb.\n\nOr something else, something better? Maybe something non-http between the models.\n\nSeparate  web server sounds better either way. Can set up a queue of some time  later and not be depended on changing codebase of those 2 models.", "link": "https://www.reddit.com/r/MachineLearning/comments/nrddix/d_data_transferimage_features_between_different/"}, {"autor": "Freedrome", "date": "2021-06-03 07:26:05", "content": "[D] What is the smallest dataset you styleGAN2 trained? /!/ I hear somebody on twitter tried successfully to train a 300 sample dataset  GAN. Is it even feasible training with stylegan2 (pytorch) under 1k -----> image !!!  dataset?", "link": "https://www.reddit.com/r/MachineLearning/comments/nr7jc4/d_what_is_the_smallest_dataset_you_stylegan2/"}, {"autor": "Djenesis", "date": "2021-09-04 17:50:46", "content": "Can someone make a guide for the Google Collab version of this -----> image !!!  editing AI? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/phw7ne/can_someone_make_a_guide_for_the_google_collab/"}, {"autor": "techsucker", "date": "2021-09-04 12:01:54", "content": "[R] Apple\u2019s Machine Learning Team Introduces \u2018GSN\u2019, A Generative Model Which Learns to Synthesize Radiance Fields of Indoor Scenes (Codes, Dataset, Paper included) /!/ When it comes to the spatial understanding of a scene when observed from any viewpoint or orientation, most geometry and learning-based approaches for 3D view synthesis fail to extrapolate to infer unobserved parts of the scene. The inability of these models to learn a prior over scenes is their fundamental limitation. Popular models like NeRF do not learn a scene prior, and therefore it cannot extrapolate views. Even conditional auto-encoder models can extrapolate views of simple objects, but they overfit to viewpoints and produce blurry renderings.\n\nA prior learned model for spatial understanding of a scene may be used for unconditional or conditional inference. A good use case of unconditional inference is to generate realistic scenes and pass through them without any input observations, relying on the prior distribution over scenes. Similarly, with conditional inference there too are different types of problems. Example, plausible scene completions may be sampled by inverting scene observations back to the learned scene prior. Therefore, a generative model for scenes would be a practical solution for tackling a wide range of machine learning and computer vision problems, including model-based reinforcement learning.\n\nThrough this [research](https://arxiv.org/pdf/2104.00670.pdf), Apple researchers have introduced [Generative Scene Networks (GSN)](https://arxiv.org/pdf/2104.00670.pdf), a generative model of scenes that allows view synthesis of a freely moving -----> camera !!!  in an open environment. Their contributions in this model include:....\n\n# [5 Min Read](https://www.marktechpost.com/2021/09/04/apples-machine-learning-team-introduces-gsn-a-generative-model-which-learns-to-synthesize-radiance-fields-of-indoor-scenes/) | [Paper](https://arxiv.org/pdf/2104.00670.pdf) | [Dataset](https://github.com/apple/ml-gsn#datasets) | [Codes](https://github.com/apple/ml-gsn)\n\n&amp;#x200B;\n\nhttps://i.redd.it/i88hpn418hl71.gif", "link": "https://www.reddit.com/r/MachineLearning/comments/phqa1y/r_apples_machine_learning_team_introduces_gsn_a/"}, {"autor": "dojoteef", "date": "2021-09-04 02:48:46", "content": "[N] Facebook Apologizes After A.I. Puts \u2018Primates\u2019 Label on Video of Black Men /!/ It\u2019s been [six years since Google Photos tagged black people as gorillas](https://www.reddit.com/r/MachineLearning/comments/3brpre/with_results_this_good_its_no_wonder_why_google/) and yet despite all the advances in CV in that time, it looks like [Facebook has run into the same problem recently](https://www.nytimes.com/2021/09/03/technology/facebook-ai-race-primates.html). It\u2019s more than a little troubling that this is an issue that hasn\u2019t been fully addressed in six years despite all the claimed ML advances in the intervening time.\n\n**Please don\u2019t turn this post into a flamewar about whether or not algorithms are biased or racist.** Rather, I\u2019m wondering what are realistic solutions that can help prevent these types of egregious misclassifications in consumer-facing ML models.\n\nWould something like the ACL 2020 best paper, [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://aclanthology.org/2020.acl-main.442/), help if applied to CV? Considering the wide variety of lighting, -----> camera !!!  angles, background etc for image classification, would behavioral tests actually reduce these issues? Are there other potential solutions?", "link": "https://www.reddit.com/r/MachineLearning/comments/phjecd/n_facebook_apologizes_after_ai_puts_primates/"}, {"autor": "Perturbed_Parakeet", "date": "2021-09-03 16:34:18", "content": "Feature store for -----> image !!!  features /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ph8q29/feature_store_for_image_features/"}, {"autor": "shellyturnwarm", "date": "2021-09-03 15:28:29", "content": "[D] Any good works on uncertainty estimation in imaging tasks (e.g. segmentation)? /!/ I'm quite bored of the hundreds of annual papers just providing a slightly different architecture to segment their specific dataset. Uncertainty estimation in segmenting medical images seems to me like it will be hugely useful.\n\nI imagine a great use of ML is asking a model to segment an -----> image !!! , while also outputting a map of the places it was unsure of, where the radiologist can manually correct.\n\nHowever, I haven't seen that many novel papers on this area. Most simply use monte-carlo dropout, Bayesian NN, or ensemble networks to measure uncertainty. These basically boil down to multiple passthroughs of the image, and looking at the variance in the predictions. The higher the variance, the higher the uncertainty.\n\nDoes anyone know of any cool or interesting papers that tackle this topic in a different way? Or any works that are related to this area?", "link": "https://www.reddit.com/r/MachineLearning/comments/ph7foh/d_any_good_works_on_uncertainty_estimation_in/"}, {"autor": "KirillTheMunchKing", "date": "2021-09-02 14:18:49", "content": "[D] Here is what I learned from writing 50 summaries of popular AI papers! /!/ Since  I have been writing two summaries per week for some time now, I wanted to share some tips that I learned while doing it! First of all, It usually takes me around 2.5 hours from start to finish to read a paper,  write the summary, compile the graphics into a single -----> image !!! , and post it to the channel and the blog. Head over to Casual GAN Papers to learn AI  paper reading tips.\n\n[https://www.casualganpapers.com/how-to-learn-to-read-ai-papers-quickly/How-To-Read-AI-Papers-explained.html](https://www.casualganpapers.com/how-to-learn-to-read-ai-papers-quickly/How-To-Read-AI-Papers-explained.html)", "link": "https://www.reddit.com/r/MachineLearning/comments/pgitms/d_here_is_what_i_learned_from_writing_50/"}, {"autor": "Competitive_Trash269", "date": "2021-09-02 13:55:20", "content": "[P] How to make ML food detection app with high accuracy? /!/ So I'm currently making a food detection app for one of my uni courses. The app is supposed to be able to take an input -----> image !!!  and detect the different food classes in the -----> image !!! . \n\nI've tried doing transfer learning with Tensorflow's pre-trained object detection models: the SSD Mobilenet V1 and SSD Resnet 101. The training data is the [UECFOOD100 dataset](https://www.kaggle.com/rkuo2000/uecfood100) from Kaggle which contains 100 food classes.\n\nHowever, after having trained the models for around 140k (loss around 0.7) and 80k (loss around 0.8) respectively, it only has good accuracy (about 60-80%) for a couple of food classes like hamburger or egg. But it still struggles which other foods like sushi or beef bowl, etc. It also struggles when there are more than 1 foods in the image.\n\nI've also tried training a pre-trained faster rcnn model but can't seem to get the loss to be below 1.0.\n\nWas wondering if anyone's got any suggestions in terms of maybe:\n\n* What other pre-trained models do you think I should use?\n* How could I get lower loss during training?\n* Should I not even use transfer learning and create a CNN model from scratch?\n* Is the 100 classes just too difficult for me to do?\n* Or any other suggestions to be able to create a better-performing model?\n\nAny advice would really be appreciated - thank you!", "link": "https://www.reddit.com/r/MachineLearning/comments/pgid9i/p_how_to_make_ml_food_detection_app_with_high/"}, {"autor": "AudioDescription", "date": "2021-09-01 16:55:00", "content": "[D] Understanding the Data Problem /!/ I\u2019m doing some research on how to address the data problem: computer vision (CV) algorithms need immense amounts of data but it\u2019s really hard to collect, label, and process the right data for your application.\n\nToday, there are some really solid -----> image !!!  captioning models provided by the likes of Google, Amazon, Microsoft. For a CV developer, how helpful would it be to leverage those models to label, say, specific frames in tons of YouTube videos and provide those frames + labels to you as data?\n\nFor example, say you are a CV developer who is building an algorithm to recognize stop signs. You would go to a website, search up \u201cstop signs\u201d, and receive a dataset of 25,000 images of stop signs and 5,000 images of random things. This dataset would have been created by scrolling through YouTube and using image captioning models to identify stop signs in videos. Would this be helpful to you? If so, how? If not, why?", "link": "https://www.reddit.com/r/MachineLearning/comments/pfxzsx/d_understanding_the_data_problem/"}, {"autor": "speedy0wl", "date": "2021-09-01 14:29:34", "content": "[P] LabelFlow is live! The open -----> image !!!  annotation and dataset cleaning platform /!/ Hi all! 4 months ago we announced Labelflow ([https://www.labelflow.ai/](https://www.labelflow.ai/)), the open image annotation and dataset cleaning platform.\n\n**What was then just a landing page is now a product** that you can try for free with no login required, the code is also publicly available on GitHub. ([https://github.com/Labelflow/labelflow/](https://github.com/Labelflow/labelflow/)).\n\nIn this first version, we are releasing your most wanted features: a straightforward online image annotation tool. For privacy concerns, your images are never uploaded to our server! You can create bounding boxes, polygons, export labels to COCO format and we added plenty of keyboard shortcuts for productivity!\n\nWe\u2019re excited to hear your feedback, tell us what features would make your life easier ([https://labelflow.canny.io/feature-requests](https://labelflow.canny.io/feature-requests))  and upvote what you would like us to build. Stay tuned, It\u2019s just the beginning of a long story.", "link": "https://www.reddit.com/r/MachineLearning/comments/pfv252/p_labelflow_is_live_the_open_image_annotation_and/"}, {"autor": "speedy0wl", "date": "2021-09-01 14:11:52", "content": "LabelFlow is live! The open -----> image !!!  annotation and dataset cleaning platform /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pfupkj/labelflow_is_live_the_open_image_annotation_and/"}, {"autor": "speedy0wl", "date": "2021-09-01 14:10:05", "content": "LabelFlow is live! The open -----> image !!!  annotation and dataset cleaning platform", "link": "https://www.reddit.com/r/MachineLearning/comments/pfuoc1/labelflow_is_live_the_open_image_annotation_and/"}, {"autor": "Infinite_Suspect_348", "date": "2021-09-01 09:25:45", "content": "[D] -----> Image !!!  classification input filtering /!/ I'm training an image classification model that is user-facing and can accept any images a user uploads. Let's say this is a model to classify types of crops. For quality purposes, I would like to filter only images that would be valid for the model (e.g. accept only images of plants and reject if someone uploads a picture of a dog).\n\nI do have a filtering binary classifier that is applied before the images are passed to the main model but, of course, I can't capture all possible inputs to have a representative negative set. I would like to get any recommendations about how to improve my \"input selection\" classifier in those circumstances. Can I adjust the loss to help with this problem of not having representative negative samples? I wonder if anyhow the discriminator of a GAN (possibly a conditional GAN) could help since it seems to learn to discriminate samples very close to the decision boundary of the input data distribution.", "link": "https://www.reddit.com/r/MachineLearning/comments/pfqafq/d_image_classification_input_filtering/"}, {"autor": "DrTurb0", "date": "2021-06-13 15:11:10", "content": "[R] Need dataset Images of lines, recognizing if lines are parallel /!/ For my masterthesis I need -----> image !!!  classification, -----> image !!! s of (2)lines and need to recognize if the lines are parallel. \n\nAny help appreciated! Thanks in advance!", "link": "https://www.reddit.com/r/MachineLearning/comments/nyydim/r_need_dataset_images_of_lines_recognizing_if/"}, {"autor": "BurnerAccount1100", "date": "2021-06-13 02:53:01", "content": "[D] Am I missing any MLP vision architectures here? /!/ I've been trying to catch up on some papers, and I'm pretty interested in the whole MLPs for vision thing that boomed last month. So far I was able to gather:\n\n&amp;#x200B;\n\n|**Model**|**Paper**|\n|:-|:-|\n|MLP-Mixer|MLP-Mixer: An all-MLP Architecture for Vision|\n|ResMLP|ResMLP: Feedforward networks for -----> image !!!  classification with data-efficient training|\n|no name for this one I think|Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet|\n|RepMLP|RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition|\n|gMLP|Pay Attention to MLPs|\n|EAMLP|Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks|\n\n&amp;#x200B;\n\nJust want to ask if there are any more that I missed that have also gained traction? Admittedly though I don't know how many of the ones I listed here have made a splash, aside from MLP-Mixer. Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/nyn5or/d_am_i_missing_any_mlp_vision_architectures_here/"}, {"autor": "rohitkuk", "date": "2021-06-12 13:53:19", "content": "Pytorch Implementation Translating Real -----> Image !!! s to cartoon images using PIX2PIX - -----> Image !!! -to------> Image !!!  Translation with Conditional Adversarial Networks Code : https://lnkd.in/etv3Kws paper : https://lnkd.in/exdgeBB", "link": "https://www.reddit.com/r/MachineLearning/comments/ny78zh/pytorch_implementation_translating_real_images_to/"}, {"autor": "siIverspawn", "date": "2021-06-12 09:28:10", "content": "[D] Model that points to specific object in an -----> image !!!  /!/ Hi,\n\nI would like to train a model that takes as input an image &amp; and the label of an object in text form (e.g., \"apple\") and outputs a point (x-y coordinates) that is at the center of the object in the image. This seems like a sufficiently simple idea that I'm hoping it already exists, but I'm not familiar with any paper proposing it. I know there is work on image captioning and object detection, but usually those are dealt with models that take just an image as input.\n\nIf the model outputs a bounding box rather than a point, this should still be fine since one can easily recover the point from the box. Bonus points if the model doesn't take a label as input but a sentence about the object (i.e., it takes an image of a car and the sentence \"this is a car because it has four tires\" and outputs a point on/bounding box around one of the tires).\n\nDoes anyone know of existing work similar to this?", "link": "https://www.reddit.com/r/MachineLearning/comments/ny2vk4/d_model_that_points_to_specific_object_in_an_image/"}, {"autor": "KindExpression1907", "date": "2021-06-12 08:05:35", "content": "-----> Image !!!  Classification using KNN [Discussion] /!/ Guys, lets say I have trained a KNN algorithm using a bunch of training data which is \"yellow car\"  and \"brown table\". My test data is a \"yellow table\".\n\nWhat feature of KNN would cause it to match the YELLOW table with the BROWN table but not the YELLOW car? I am confused about this concept. \n\nAs KNN stores the RGB values of the images in an array and compares pixel by pixel, the image of the yellow table will have greater pixel matched with yellow car than brown table. \n\nSo, how will KNN classify the test image into a table but not a car? Your help would be highly appreciated.", "link": "https://www.reddit.com/r/MachineLearning/comments/ny1px8/image_classification_using_knn_discussion/"}, {"autor": "KindExpression1907", "date": "2021-06-12 08:03:45", "content": "-----> Image !!!  Classification using KNN /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ny1oz2/image_classification_using_knn/"}, {"autor": "Aha_IamDaniel", "date": "2021-06-11 01:32:58", "content": "[P] PaddleHub: An awesome and easy-to-use pre-trained models toolkit /!/  Hi, all,\n\n I am glad to share that my team are working on an open source repository PaddleHub , which provides 300+ pre-trained models in practical. The stargazers of PadleHub have exceeded 6.2K and increase continuously.\n\nAt present, you can find it on the trending of github, which indicates it is popular for develops indeed.\n\ncode\uff1a[https://github.com/PaddlePaddle/PaddleHub](https://github.com/PaddlePaddle/PaddleHub)  \n\n**Features Set:** \n\n* **PaddleHub** aims to provide developers with rich, high-quality, and directly usable pre-trained      models.\n* **Abundant Pre-trained Models**: 300+ pre-trained models cover the 5 major categories, including Image, Text, Audio, Video, and Industrial application. All of them are free for download and offline usage.\n* **No Need for Deep Learning Background**: you can use AI models quickly and enjoy the dividends of the artificial intelligence era.\n* **Quick Model Prediction**: model prediction can be realized through a few lines of scripts to quickly      experience the model effect.\n* **Model As Service**: one-line command to build deep learning model API service deployment capabilities.\n* **Easy-to-use Transfer Learning**: few lines of codes to complete the transfer-learning task such as -----> image !!!  classification and text classification based on high quality pre-trained models.\n* **Cross-platform**: support Linux, Windows, MacOS and other operating systems.\n\nThank you and looking forward.\n\nPaddleHub R&amp;D Team.\n\n&amp;#x200B;\n\n[Some Visualization Demos](https://i.redd.it/c6wsje42ij471.gif)", "link": "https://www.reddit.com/r/MachineLearning/comments/nx43y7/p_paddlehub_an_awesome_and_easytouse_pretrained/"}, {"autor": "histoire_guy", "date": "2021-06-10 23:10:16", "content": "[P] PixLab Annotate - Online Image Annotation, Labeling and Segmentation Tool /!/ Annotate is  A web based -----> image !!!  annotation, labeling &amp; segmentation tool for Machine Learning model training tasks and beyond.\n\nhttps://annotate.pixlab.io/\n\n###Features Set:\n\n* Rectangle, Polygon, Zoom &amp; Drag labeling tool.\n* Consistent JSON output accepted by most Machine Learning frameworks.\n* Optimized for instance segmentation (Mask R-CNN, etc).\n* Client-side persistent storage - No data transfer involved.\n* Persistent &amp; easy label management (Create, Modify &amp; Delete).\n* Full screen display &amp; Snapshot capture.", "link": "https://www.reddit.com/r/MachineLearning/comments/nx1asa/p_pixlab_annotate_online_image_annotation/"}, {"autor": "vikzae7", "date": "2021-06-10 19:44:19", "content": "[P] Speeding up of dataset -----> image !!!  labeling /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/nwwj51/p_speeding_up_of_dataset_image_labeling/"}, {"autor": "Knightron2525", "date": "2021-06-10 16:51:26", "content": "[P] -----> Image !!!  ---&gt; cross-hatch drawing ---&gt; DXF. converting images to drawings /!/ I am looking for a way to convert images into crosshatch drawings and then to DXF so that they can be drawn using something similar to a CNC machine \n\n[example of cross-hatching](https://i.pinimg.com/originals/7e/92/bb/7e92bbca44ec6dfc9499c91a2f035af5.jpg)\n\nI'm just looking into implementing something like this using pre-existing models and any leads on how to do is appreciated.", "link": "https://www.reddit.com/r/MachineLearning/comments/nwsekn/p_image_crosshatch_drawing_dxf_converting_images/"}, {"autor": "sarmientoj24", "date": "2021-01-17 06:02:51", "content": "[D] Library for Augmenting Images via Homography Perspective Transform? /!/ I have this Object Detection datasets that I would like to augment by perspective transformation using homography. I also do not have the intrinsic -----> camera !!!  parameters and would just do trial and error on the ***homography matrix***. Obvious goal is to create another image by having the image from another perspective.  \n\n\nAnyone who has done something similar? There might be a library or a function for this.", "link": "https://www.reddit.com/r/MachineLearning/comments/kz0tlk/d_library_for_augmenting_images_via_homography/"}, {"autor": "Haycart", "date": "2021-01-17 01:40:07", "content": "[D] How does self-adversarial training, as described in the YOLOv4 paper, work? /!/ The YOLOv4 paper ([https://arxiv.org/abs/2004.10934](https://arxiv.org/abs/2004.10934)) describes self-adversarial training as follows:\n\n&gt; Self-Adversarial Training (SAT) also represents a new data augmentation technique that operates in 2 forward backward stages. In the 1st stage the neural network alters the original -----> image !!!  instead of the network weights. In this way the neural network executes an adversarial attack on itself, altering the original image to create the deception that there is no desired object on the image. In the 2nd stage, the neural network is trained to detect an object on this modified image in the normal way. \n\nUnfortunately, the paper does not seem to provide any further details. Does anyone know the exact procedure? E.g. in what way does the network alter the image? Is it just gradient ascent with respect to loss while holding network weights constant? If so, how often is this done, and for how many steps? Does it use the same optimizer and learning rate schedule as is used to train the network weights?", "link": "https://www.reddit.com/r/MachineLearning/comments/kywh2c/d_how_does_selfadversarial_training_as_described/"}, {"autor": "jet-orion", "date": "2021-01-17 01:26:32", "content": "[P] -----> image !!!  Classification CNN questions /!/ Hey everyone!\n\nI\u2019m building an image classification model to classify 12 different plant types. For my training data I have in between 200 and 500 images for each plant for 50 images per plant for validation data. I\u2019m using Keras and TensorFlow in Python to work on the project (Spyder IDE and Jupyter Notebooks).\n\nI\u2019ve built out the convolutional neural network and the best validation accuracy statistic I can get is 0.64\n\nI\u2019ve tried changing the Adam learning rate, added more/removed convolutional layers, adding more dropout layers, and was thinking about changing the data augmentation parameters. \n\nWhat do you all recommend when tuning model parameters? Are there any \u201crules of thumb\u201d or common practices to abide by when tuning?Please post any great resources about how your favorite deep learning models work!", "link": "https://www.reddit.com/r/MachineLearning/comments/kyw8pm/p_image_classification_cnn_questions/"}, {"autor": "crescendo01", "date": "2021-01-16 21:19:07", "content": "I copied a training model, but how do I train it using my own images. This is for a Hackathon, so pardon my lack of effort. [P] /!/ [https://www.kaggle.com/shreyanshu/flower-classification-model-tensorflow/data](https://www.kaggle.com/shreyanshu/flower-classification-model-tensorflow/data)\n\nI've gotten this working and it trains the model given a dataset. But I don't know how to actually give the model my own images (we're planning to create a scrappy iOS app where you can take a -----> picture !!!  and identify the type of flower and a few more things).", "link": "https://www.reddit.com/r/MachineLearning/comments/kyroe2/i_copied_a_training_model_but_how_do_i_train_it/"}, {"autor": "deluded_soul", "date": "2021-01-16 21:11:51", "content": "Question: Biomedical -----> image !!!  segmentation beyond UNETs /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/kyrjgd/question_biomedical_image_segmentation_beyond/"}, {"autor": "Ghost_06D", "date": "2021-01-10 12:45:30", "content": "How to improve training performances of -----> image !!!  Colorization model using Auto-encoders? \"[Discussion]\",\"[Research]\", [Project]\" /!/  Hello,\n\nI'm working on an Image colorization model using auto-encoders, here are parameters I'm using :\n\n* Training images size:26000\n* Validation images size:2600\n* 40 epochs, it's doing well on first 20 epochs, but not improving on 20 others as described in this picture:\n\nhttps://preview.redd.it/29srrtqb0ia61.png?width=1052&amp;format=png&amp;auto=webp&amp;s=c9f9fb4de56e1d9b2360bd0ae1643f874fc81bc4\n\n \n\n* Mini-batch size :32\n* Optimizer : adam (lr=2e-4)\n\nThe input of the auto-encoder is the first dimension of the lab space of the image(gray-scale), and the output is the last 2 dimensions of the lab space(color information), here is a part of my code:\n\n     \n    IMG_SIZE\u00a0=\u00a0256\nN_EPOCHS\u00a0=\u00a020\nBATCH_SIZE\u00a0=\u00a032\nlatent_dim\u00a0=\u00a0256\n     \n    train_datagen\u00a0=\u00a0ImageDataGenerator(rescale\u00a0=\u00a01./255,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0horizontal_flip\u00a0=\u00a0True)\n\ntrain_set\u00a0=\u00a0train_datagen.flow_from_directory('./Train',\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0target_size\u00a0=\u00a0(IMG_SIZE,\u00a0IMG_SIZE),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0batch_size\u00a0=\u00a0BATCH_SIZE)\n\nvalid_datagen\u00a0=\u00a0ImageDataGenerator(rescale\u00a0=\u00a01./255)\n\nvalid_set\u00a0=\u00a0valid_datagen.flow_from_directory('melange/vl/',\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0target_size\u00a0=\u00a0(IMG_SIZE,\u00a0IMG_SIZE),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0batch_size\u00a0=\u00a0BATCH_SIZE)\n\ndef gen_ab_images(train_set):\n for\u00a0batch\u00a0in\u00a0train_set:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0lab_batch\u00a0=\u00a0rgb2lab(batch[0])\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0X_batch\u00a0=\u00a0lab_batch[:,:,:,0]\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Y_batch\u00a0=\u00a0lab_batch[:,:,:,1:]\u00a0/\u00a0128\n yield\u00a0(X_batch.reshape(X_batch.shape+(1,)),\u00a0Y_batch) \n    \n    model\u00a0=\u00a0Sequential()\nmodel.add(Conv2D(64,\u00a0(3,\u00a03),\u00a0activation='relu',\u00a0padding='same',\u00a0strides=2,\u00a0input_shape=(256,\u00a0256,\u00a01)))\nmodel.add(Conv2D(128,\u00a0(3,\u00a03),\u00a0activation='relu',\u00a0padding='same'))\nmodel.add(Conv2D(128,\u00a0(3,3),\u00a0activation='relu',\u00a0padding='same',\u00a0strides=2))\nmodel.add(Conv2D(256,\u00a0(3,3),\u00a0activation='relu',\u00a0padding='same'))\nmodel.add(Conv2D(256,\u00a0(3,3),\u00a0activation='relu',\u00a0padding='same',\u00a0strides=2))\nmodel.add(Conv2D(512,\u00a0(3,3),\u00a0activation='relu',\u00a0padding='same'))\nmodel.add(Conv2D(512,\u00a0(3,3),\u00a0activation='relu',\u00a0padding='same'))\nmodel.add(Conv2D(256,\u00a0(3,3),\u00a0activation='relu',\u00a0padding='same'))\n\nmodel.add(Conv2D(128,\u00a0(3,3),\u00a0activation='relu',\u00a0padding='same'))\nmodel.add(UpSampling2D((2,\u00a02)))\nmodel.add(Conv2D(64,\u00a0(3,3),\u00a0activation='relu',\u00a0padding='same'))\nmodel.add(UpSampling2D((2,\u00a02)))\nmodel.add(Conv2D(32,\u00a0(3,3),\u00a0activation='relu',\u00a0padding='same'))\nmodel.add(Conv2D(16,\u00a0(3,3),\u00a0activation='relu',\u00a0padding='same'))\nmodel.add(Conv2D(2,\u00a0(3,\u00a03),\u00a0activation='tanh',\u00a0padding='same'))\nmodel.add(UpSampling2D((2,\u00a02)))\n\n    opt=keras.optimizers.Adam(learning_rate=2e-4)\n\n    model.compile(optimizer=opt,\u00a0loss='mse'\u00a0,\u00a0metrics=['accuracy'])\nmodel.summary()\n     \n    filepath\u00a0=\u00a0\"model_test2.model\"\ncheckpoint\u00a0=\u00a0ModelCheckpoint(filepath,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0save_best_only=True,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0monitor='loss',\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0mode='min')\n     \n    reduction_learning_rate\u00a0=\u00a0ReduceLROnPlateau(monitor='loss',\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0patience=1,\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0verbose=1,\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0factor=0.5,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0min_lr=0.00001)\n     \n    valeurs=model.fit(x=gen_ab_images(train_set),\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0callbacks=[checkpoint,reduction_learning_rate],\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0epochs=N_EPOCHS,\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0validation_data=gen_ab_images(valid_set),\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0steps_per_epoch=len(train_set),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0validation_steps=len(valid_set),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0shuffle=True)\n\n How can I improve the performance of this autoencoder?  \n\nThanks in advance,", "link": "https://www.reddit.com/r/MachineLearning/comments/kudv4i/how_to_improve_training_performances_of_image/"}, {"autor": "Ghost_06D", "date": "2021-01-10 12:22:31", "content": "How to improve training performances of -----> image !!!  Colorization model using Auto-encoders /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/kudkja/how_to_improve_training_performances_of_image/"}, {"autor": "Ghost_06D", "date": "2021-01-10 12:07:29", "content": "How to improve training performances of -----> image !!!  Colorization model using Auto-encoders? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/kudds6/how_to_improve_training_performances_of_image/"}, {"autor": "SeanPedersen", "date": "2021-01-08 23:56:52", "content": "[PROJECT] HyperTag 0.5.3 - Semantic Search for Images Using Text Queries powered by OpenAI's CLIP model /!/ File organization made easy. HyperTag let's humans intuitively express how they think about their files using tags and machine learning.\n\n**Objective Function**: Minimize time between a thought and access to all relevant files.\n\nJust released 0.5 which adds semantic search for images using text queries. This is powered by OpenAI's CLIP model ([https://openai.com/blog/clip/](https://openai.com/blog/clip/)) released on Jan. 5, 2021. Works pretty good on my end. Try it out: [https://github.com/SeanPedersen/HyperTag](https://github.com/SeanPedersen/HyperTag)\n\nHyperTag offers a slick CLI but more importantly it creates a directory called HyperTagFSwhich is a file system based representation of your files and tags using symbolic links and directories.\n\n**Directory Import**: Import your existing directory hierarchies using $ hypertag import path/to/directory. HyperTag converts it automatically into a tag hierarchy using metatagging.\n\n**Semantic Text &amp; Image Search (Experimental)**: Search for **images** (jpg, png) and **text documents** (yes, even PDF's) content with a simple text query. Text search is powered by the awesome [Sentence Transformers](https://github.com/UKPLab/sentence-transformers) library. Text to -----> image !!!  search is powered by OpenAI's [CLIP model](https://openai.com/blog/clip/). Currently only English queries are supported.\n\n**HyperTag Daemon (Experimental)**: Monitors HyperTagFSfor user changes. Currently supports file and directory (tag) deletions + directory (name as query) creation with automatic query result population. Also spawns the DaemonService which speeds up semantic search significantly.\n\n**Fuzzy Matching Queries**: HyperTag uses fuzzy matching to minimize friction in the unlikely case of a typo.\n\n**File Type Groups**: HyperTag automatically creates folders containing common files (e.g. Images: jpg, png, etc., Documents: txt, pdf, etc., Source Code: py, js, etc.), which can be found in HyperTagFS\n\n**HyperTag Graph**: Quickly get an overview of your HyperTag Graph! HyperTag visualizes the metatag graph on every change and saves it at HyperTagFS/hypertag-graph.pdf", "link": "https://www.reddit.com/r/MachineLearning/comments/ktf7nf/project_hypertag_053_semantic_search_for_images/"}, {"autor": "RenYang_ETHZ", "date": "2021-01-08 21:37:11", "content": "[N] AI-based -----> image !!!  compression will be standardized! /!/ [https://jpeg.org/items/20201014\\_press.html](https://jpeg.org/items/20201014_press.html)\n\nJPEG initiates standardisation of image compression based on AI", "link": "https://www.reddit.com/r/MachineLearning/comments/ktceww/n_aibased_image_compression_will_be_standardized/"}, {"autor": "[deleted]", "date": "2021-01-08 21:36:29", "content": "AI-based -----> image !!!  compression will be standardized!", "link": "https://www.reddit.com/r/MachineLearning/comments/ktcedd/aibased_image_compression_will_be_standardized/"}, {"autor": "[deleted]", "date": "2021-01-08 21:35:39", "content": "[N] AI-based -----> image !!!  compression will be standardized! /!/ [deleted]", "link": "https://www.reddit.com/r/MachineLearning/comments/ktcdrr/n_aibased_image_compression_will_be_standardized/"}, {"autor": "kireyi", "date": "2021-01-08 21:20:28", "content": "[D] Fastest SIFT Descriptors Matching with Database of SIFT Descriptors /!/ Pretty much title\nI have database of SIFT descriptors  which i want to match (for similar -----> image !!!  finding) but serially matching is taking a lot of time. What could be done to speed up the process?\n\nP.S: i would appreciate if you can provide the link and your own experience (if you have) regarding this", "link": "https://www.reddit.com/r/MachineLearning/comments/ktc2qm/d_fastest_sift_descriptors_matching_with_database/"}, {"autor": "HamzaHazrat", "date": "2021-05-26 11:28:15", "content": "Is there any -----> image !!!  dataset with continued values targets? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/nleo95/is_there_any_image_dataset_with_continued_values/"}, {"autor": "ottawalanguages", "date": "2021-05-26 02:27:04", "content": "[D] Why is \"convolution\" a meaningful mathematical operation? /!/ I was watching an introductory video on convolutional neural networks, and the \"convolution\" operation itself seems so interesting. It seems that convolutional neural networks take a -----> picture !!! , and via convolutional layers, repeatedly shrink the original -----> picture !!!  to a small matrix of numbers. This small matrix of number is associated with a \"label\" (e.g. \"dog\" or \"cat\"), and then the neural network can learn to recognize similar pictures.\n\nI had the following questions:\n\nQuestion 1: Why is the \"convolution process\" a meaningful operation? How are convolutions able to capture and filter meaningful information from pictures? I know I am wrong, but from a layman's perspective, the convolution process (i.e. taking the algebraic dot product between filter weights and the pixel values of a picture) seems somewhat \"arbitrary\". I just find it very interesting that the dot product between these two entities is able to capture and filter meaningful information about the pictures. Just a guess : does this have to do with the \"universal approximation theorem\"? i.e. the universal approximation theorem also applies to cnn's.\n\nQuestion 2: I have often heard claims about neural network based approaches being intended for \"non-tabular\" data, such as pictures. The argument being, neural networks work better when there is a higher level of \"richness\" within the data, e.g. pictures containing 786 pixels and each of these pixels usually being non-empty. This is usually contrasted with \"tabular data\" such as data that can be easily placed into Excel spreadsheets (e.g. typical of finance data). Apparently, tabular data is more prone to containing less informative data compared to pictures, potentially making the performance of neural networks worse. When dealing with tabular data, it is said that boosting and bagging based techniques (e.g. random forest) are more likely to yield stronger results.\n\nI was just wondering: Why aren't pictures normally considered as tabular data? Couldn't you just take the individual pixel values and collectively store them into matrices (i.e. tabular form)?", "link": "https://www.reddit.com/r/MachineLearning/comments/nl67lo/d_why_is_convolution_a_meaningful_mathematical/"}, {"autor": "Zygmunt_", "date": "2021-05-24 18:49:42", "content": "Difference ML DL in a so basic and very visual -----> image !!! ", "link": "https://www.reddit.com/r/MachineLearning/comments/nk56yf/difference_ml_dl_in_a_so_basic_and_very_visual/"}, {"autor": "EgorBykov", "date": "2021-05-24 17:44:00", "content": "[D] [R] BERT Base. Choosing optimal cloud infrastructure and environment setup (long read) /!/ Quite often I optimize DL models (aiming to get the cheapest placement within a certain model performance range), find optimal instances and tuning the environment for training and inference, etc.\n\nAfter multiple such optimizations, I\u2019ve put together a quick framework, a guide that I can refer to when I need it. Some approaches I came up with and results I got seem quite odd and counterintuitive, so ideally I\u2019d like to start a discussion with those dealing with model performance optimization - does my approach make sense, is the benchmarking the only way or am I missing something?\n\nIf TLDR, main points:\n\n* Rules of thumb in choosing the instance type/shape did not work for me - when I try and guess cost or runtime based on GPU generation I always land on the wrong side of a **4-5x** variability. In some cases not so obvious options might provide decent performance (P4) and costs (P100).\n* What works for me is optimizing for GPU utilization as a proxy for cost/performance optimal placement (duh!)\n* To achieve that I have to deal with different bottlenecks in the system outside of GPU, the biggest culprit being preprocessing on CPU and following data streaming to GPU (so, benchmarking with rudimentary monitoring is a must)\n* Batch size optimisation can give as much as **4-5x** in performance\n* Worker number optimisation (vCPU count, basically)got me another **2-3x**\n* (!wtf) Driver/CUDA versions might influence performance much greater than expected (**10x!!?**)\n* Benchmarking is a pain in the ass as I typically run at least **100** benchmarks to gather a comprehensive -----> picture !!! \n\nBelow you could find a breakdown of those points above.\n\n**Approach**\n\nTo illustrate my points I decided to go with the most popular NLP model (BERT base uncased according to [huggingface](https://huggingface.co/)), because 1. this domain looks more suitable for a generalized approach for optimization, 2. dataset and preprocessing are very similar across different models.\n\nIt took me around 110 benchmark launches to gather the data below, so I put together a small repo ([Github link](https://github.com/BykovEgor/ml-benchmarks)) using PyTorch to run inference on a small mock dataset, and run it on all GPU instances available for me on GCP (Tesla K80, Tesla P4, Tesla P100, Tesla V100, Tesla T4). This simple script can perform text input encoding, the numbers of preprocessing workers and the batch size are input parameters, it also can find the maximum possible batch size (with linear bruteforce) that saturates the GPU. On top of that I backed it into 4 different containers to run it with different versions of CUDA.\n\nI played with the batch size and the number of processes used by dataloader to preprocess the data. The goal was to maximize GPU utilization and find the optimal batch size / # of processes to get the best price/performance for each type of GPU and then compare how much they would cost me per job.\n\nAfter that, I chose the best performing GPU and ran additional benchmarks for different NVIDIA driver and CUDA versions to try to catch some optimizations there. I used this approach ([link](https://rocketcompute-com.medium.com/yet-another-guide-on-how-to-install-nvidia-drivers-on-linux-fd72c6cc38f6)) to install different drivers.\n\n**Test results and observations**\n\nTo get the baseline I passed text to the model sentence by sentence (it appeared that the number of processes does not change the picture much). Below is the summary of bench runtime and cost for the baseline\n\n&amp;#x200B;\n\n*Processing img pug2plhyu3171...*\n\nX axis legend as follow: {GPU}\\_{batch size}x{# of workers}\\_{# of vCPU}, where:\n\n* GPU - accelerator family (k80, t4, p100, etc.)\n* batch\\_size - number of sentences I am pushing to GPU and passing to the model simultaneously\n* \\# of workers - \"num\\_workers\" parameter of the DataProcessor, i.e. number of processes to perform data loading and pre-processing\n* \\# of vCPU - number of virtual core present in VM. GCP allows varying number of virtual cores for GPU instances (from 1 to 12).\n\nThen I began increasing batch size and the number of data loader workers to maximize GPU utilization. To illustrate the approach below are 3 graphs of GPU utilization for P100.\n\nInstance with 4 vCPU cores, 4 workers, and maximum possible batch size. Obviously underutilized.\n\n&amp;#x200B;\n\n[P100 \\(4 vCPU + 4 workers\\)](https://preview.redd.it/q03ty0cuu3171.png?width=887&amp;format=png&amp;auto=webp&amp;s=e23cc74dc46038cd0fd2d4a26660c730233add09)\n\nThen I increased the number of vCores and workers to 8, keeping the maximum possible batch size. Utilization jumps to 66% but still far from maximum.\n\n&amp;#x200B;\n\n[P100 \\(8 vCPU + 8 workers\\)](https://preview.redd.it/guv747nsu3171.png?width=883&amp;format=png&amp;auto=webp&amp;s=15c2fabe428e6ff26d5eeed8d7b478b63f76104c)\n\nA further increase to 12 vCore and 12 workers finally did the job pushing utilization to 94%\n\n[P100 \\(12 vCPU + 12 workers\\)](https://preview.redd.it/pxrtt4equ3171.png?width=887&amp;format=png&amp;auto=webp&amp;s=36ca51baeaf1d2b2b384de41f9eaa81c54004acf)\n\nAfter playing with vCPU / workers counts I got the following charts.\n\n**K80** maxed out its utilization at 2 workers (I didn't go below 4 vCPU but in this particular case lowering the number of vCPU can bring additional savings).\n\nhttps://preview.redd.it/3k63tytou3171.png?width=3000&amp;format=png&amp;auto=webp&amp;s=d5eacff67987a66cc83d08ed2c5c39888facb765\n\nIt took 8 workers and 8 vCores to fully utilize **P4**. Note p4\\_63x4\\_4, p4\\_63x4\\_6 and p4\\_63x6\\_8 launches, it is a clear indicator that there is no much sense to have more workers than you have vCore in this particular case.\n\nhttps://preview.redd.it/xf6hx21ou3171.png?width=3000&amp;format=png&amp;auto=webp&amp;s=53e7e4bd955c94a5c3913c05e4d018143e808f98\n\nThe same for **T4** 8 workers is enough to saturate this GPU.\n\nhttps://preview.redd.it/q2uvtm1nu3171.png?width=3000&amp;format=png&amp;auto=webp&amp;s=456697fd293e9ee37cf17c361df51931b3f6773c\n\nThe following two cases are the most interesting. In both of them, I had to go to 12 vCPU (the maximum number of vCores GCP allows to assign to a single GPU VM). Another remarkable thing is that both these GPUs showed an order of magnitude runtime improvement between the \u201cone-by-one\u201d approach and maximum possible parallelization of data pre-processing.\n\n**P100** showed the maximum utilization (as you can see on a chart above) at 12 workers. Worth noting p100\\_146x6\\_4 and p100\\_146x4\\_4, it looks like overcommitting vCores might backfire.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/5hyan4dmu3171.png?width=3000&amp;format=png&amp;auto=webp&amp;s=e21e1f6077ed0496652c4af62c6710cb2ab10911\n\n**V100** was utilized only on 59% under 12 workers. Potentially it can be pushed further with a  multi-GPU set-up where more than 12 vCPU per GPU can be added to the VM or if the data set is fully preprocessed before inference.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/6ghmm7zlu3171.png?width=3000&amp;format=png&amp;auto=webp&amp;s=96a08bd01b280e75a2a581a383e2cc964ce1a65f\n\nBelow is the summary of cost / runtime for different combinations of vCPU count / GPU.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/wy703dtku3171.png?width=3000&amp;format=png&amp;auto=webp&amp;s=5821fb9db6b6c344720ec9be5446ea7a6a9210cd\n\n**T4** is a clear winner in terms of price per volume of processed data. Interesting to note that P4 appears to be a clear forerunner in terms of processed data per dollar.\n\nAfter that I varied PyTorch for different CUDA libs, version 1.7.1 can go with:\n\n* CUDA 9.2\n* CUDA 10.1\n* CUDA 10.2\n* CUDA 11.0\n\nI tried all of these versions against the following drivers:\n\n* 460.32.03\n* 455.32.00\n* 450.102.04\n* 440.118.02\n* 418.181.07\n* 410.129\n* 384.183 ( was not able to install it on Ubuntu 16.04 with the above-mentioned GPUs)\n\nBelow is the summary of all runs I gathered. All of them were for optimal T4 setup, i.e. maximum possible batch size, 8 workers on 8 vCPUs.\n\n&amp;#x200B;\n\n*Processing img c0b4vnsju3171...*\n\nI struggle to explain the order of magnitude difference for certain combinations of driver / CUDA. I have not seen this discrepancy of performance for drivers before (although I did such analysis for different networks before with typically up to 15% variability). I ran benchmarks for all outliers 3 times and the results were consistent (crosses on the graph can indicate the amount of variance across different launches).\n\nSo, there is at least an order of magnitude cost improvement available with rudimentary benchmarking/monitoring. But the driver/CUDA combination\u2019s effect on the performance puzzles me to say the least. Has anyone seen something like that and what might cause that?\n\nHope that might be useful.", "link": "https://www.reddit.com/r/MachineLearning/comments/nk3opi/d_r_bert_base_choosing_optimal_cloud/"}, {"autor": "hardmaru", "date": "2021-05-28 13:25:06", "content": "[R] CogView: Mastering Text-to------> Image !!!  Generation via Transformers", "link": "https://www.reddit.com/r/MachineLearning/comments/nmxsd8/r_cogview_mastering_texttoimage_generation_via/"}, {"autor": "svantana", "date": "2021-05-28 13:04:39", "content": "[D] Architecture Search in practice - what's your go-to? /!/ There's been tons of papers on NAS, but in my experience most suffer from being either a) very slow, b) not easily applied to existing code, or c) focused on one application (mostly -----> image !!!  recognition). Also, there's a big difference between looking good in a paper and being a useful and versatile library.\n\nIf you're using a particular library, I would love to hear your experience, the good and the bad. (Pytorch-based ones in particular)", "link": "https://www.reddit.com/r/MachineLearning/comments/nmxefj/d_architecture_search_in_practice_whats_your_goto/"}, {"autor": "Lawless_Time", "date": "2021-05-28 08:31:06", "content": "[D] find the cat in the -----> picture !!!  /!/ Assuming I have a trained model for a specific class (cats, dogs, tumoral cells, cars...) in squared windows like 64x64 pixels.  What is the best way to find candidate region to be tested by my model. E.g. look for a cat in the picture?\n\nSliding windows of different size? Or still feature extraction? As threshold the image and look for rounded objects?", "link": "https://www.reddit.com/r/MachineLearning/comments/nmt6w1/d_find_the_cat_in_the_picture/"}, {"autor": "SAbdusSamad", "date": "2021-06-21 15:51:56", "content": "[D] -----> Image !!!  classification using self-supervised learning /!/ I studied and implemented modern ConvNets for image classification task. Now I want to classify images using slef-supervised learning methods.\nI don't know about self-supervised learning. Where should I start from? And what track track should I follow to master self-supervised learning methods?\nPlease recommend me books or papers or blogs.", "link": "https://www.reddit.com/r/MachineLearning/comments/o4y9yx/d_image_classification_using_selfsupervised/"}, {"autor": "creiser", "date": "2021-06-21 14:28:22", "content": "[N] CVPR '21 Best Paper: GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields /!/ Michael Niemeyer's work **GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields** has just been given the Best Paper award at CVPR 2021. 10k submissions but you made it. Congrats Michael and Andreas! It's an honor to work with you.\n\nAbstract:\n\nDeep generative models allow for photorealistic -----> image !!!  synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects' shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.\n\n&amp;#x200B;\n\n* Project page: [https://m-niemeyer.github.io/project-pages/giraffe/index.html](https://m-niemeyer.github.io/project-pages/giraffe/index.html)\n* Paper: [http://www.cvlibs.net/publications/Niemeyer2021CVPR.pdf](http://www.cvlibs.net/publications/Niemeyer2021CVPR.pdf)\n* Twitter: [https://twitter.com/AutoVisionGroup/status/1406973670740922368](https://twitter.com/AutoVisionGroup/status/1406973670740922368)", "link": "https://www.reddit.com/r/MachineLearning/comments/o4wd3f/n_cvpr_21_best_paper_giraffe_representing_scenes/"}, {"autor": "ofirpress", "date": "2021-06-21 14:26:14", "content": "[R] Disrupting Model Training with Adversarial Shortcuts /!/ It\u2019s not always great that people can train machine learning models on your data! In this new work, we create adversarial shortcuts to prevent neural network training. Adversarial shortcuts are hand-crafted modifications to images in the training set that exploit simplicity biases in models to prevent them from capturing the semantics of the dataset. Adversarial shortcuts are also easily ignored by human perception.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/h8wc1miapm671.png?width=1850&amp;format=png&amp;auto=webp&amp;s=e2132be680902b5d891fe6bd6e7bc3812c13e1e2\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nWhile this idea is more broadly applicable, we begin its study in the context of a well-known machine learning problem: supervised classification. Adversarial shortcuts all share a common idea: fixing a pattern for each -----> image !!!  in a particular class encourages models to fit that pattern over anything else. It turns out that even fixing a few pixels prevents the model from fitting the semantics. Here is an example of an ImageNet-sized image with a pixel-based adversarial shortcut.\n\n&amp;#x200B;\n\nThis is neatly illustrated by these plots of ImageNet validation and training accuracy progression: notice how, with the adversarial shortcuts applied, the training acc@1 reaches close to 100% while the validation is stuck close to 0.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/olrx44fbpm671.png?width=432&amp;format=png&amp;auto=webp&amp;s=3ac13ed1092af77b402d41084272e1a65bff8e22\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nOf course, the pixel-based pattern may be easily disrupted, so we also explore more complicated patterns: watermarks with the class index made up of MNIST digits and brightness modulations.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/73uyv9bcpm671.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=43e9dced8ce51329d03095d45b071576dab4f4a7\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/r5gsm7tcpm671.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=c492d705a94e4b8eb7051a3f9c6c5a4c340f059b\n\n&amp;#x200B;\n\nRead more - including ablation studies and comparisons to related work - in the new arXiv preprint: [https://arxiv.org/abs/2106.06654](https://arxiv.org/abs/2106.06654) by Ivan Evtimov, Ian Covert, Aditya Kusupati, and Tadayoshi Kohno.", "link": "https://www.reddit.com/r/MachineLearning/comments/o4wbgg/r_disrupting_model_training_with_adversarial/"}, {"autor": "Shai_Meital", "date": "2021-06-21 06:13:43", "content": "[D] What is the current SOTA for extracting a knowledge graph from images? /!/ I am looking to get a knowledge graph of an -----> image !!!  using some pre-trained model. \n\nWhat is the current SOTA work that has a reproducible implementation? \n\nCan you share some pointers from your experience trying to extract KG from images? \n\nThank you", "link": "https://www.reddit.com/r/MachineLearning/comments/o4onlz/d_what_is_the_current_sota_for_extracting_a/"}, {"autor": "DolantheMFWizard", "date": "2021-06-21 05:32:45", "content": "[D]What are the current ways to compress time series data into a feature? /!/ I'm working in taking time series data over a span of 4 months and compressing it into a single feature as an input into another model. I know there are things like LSTM and GRU, but I don't know if the memory cell is large enough to hold a good latent representation. I was considering VAE, but I think converting tabular data to an -----> image !!!  is probably tricky.", "link": "https://www.reddit.com/r/MachineLearning/comments/o4o0xw/dwhat_are_the_current_ways_to_compress_time/"}, {"autor": "nousetest", "date": "2021-06-21 02:36:18", "content": "A real-time low light enhancement method help robots' visual system (such as visual SLAM) works well in a dark environment. \"A Two-stage Unsupervised Approach for Low Light -----> Image !!!  Enhancement\" #ICRA2021 #RAL", "link": "https://www.reddit.com/r/MachineLearning/comments/o4l3gu/a_realtime_low_light_enhancement_method_help/"}, {"autor": "ggouvine", "date": "2021-06-20 14:07:47", "content": "[P] TorchSR, -----> Image !!!  superresolution for pytorch /!/ Hi all,\n\nI started [torchSR](https://github.com/Coloquinte/torchsr/), a package for super-resolution networks written in Pytorch. It's inspired by torchvision, and should feel familiar to torchvision users. Check it out!\n\n[Low-resolution image](https://preview.redd.it/0oy6o3n2gf671.png?width=256&amp;format=png&amp;auto=webp&amp;s=e68a66dc7a8fc0f0d6ca857d77452020909314b4)\n\n[Super-resolution \\(x4\\)](https://preview.redd.it/4h4cwj65gf671.png?width=256&amp;format=png&amp;auto=webp&amp;s=3b1b82651ff00deaca087b3112901ff1ac23588d)\n\n[Ground truth](https://preview.redd.it/o41urqh9gf671.png?width=256&amp;format=png&amp;auto=webp&amp;s=c58481918cc7149f1dcbe2f730a9f0f48a4fffd5)\n\nAt the moment, I implemented the most popular models (EDSR, RCAN), and a number of network improvements and data augmentation method. Plus the training script for people who want to develop their own!\n\nNext steps: GAN training and multiscale networks \n\nGithub repo: [https://github.com/Coloquinte/torchsr/](https://github.com/Coloquinte/torchsr/)\n\nPython package: [https://pypi.org/project/torchsr/](https://pypi.org/project/torchsr/)", "link": "https://www.reddit.com/r/MachineLearning/comments/o46gbk/p_torchsr_image_superresolution_for_pytorch/"}, {"autor": "luciferLock90", "date": "2021-06-20 12:38:15", "content": "[P] How can I change -----> image !!!  of shape (48,48,15) to (48,48,3) ? /!/     # image specification \n    img_rows, img_cols, img_depth = 48, 48, 3 \n\n have these parameters but the image I extract from video have shape (48,48,15) I'm a noob so kindly help me", "link": "https://www.reddit.com/r/MachineLearning/comments/o44tqj/p_how_can_i_change_image_of_shape_484815_to_48483/"}, {"autor": "luciferLock90", "date": "2021-06-20 12:33:19", "content": "How can I change -----> image !!!  of shape (48,48,15) to (48,48,3) ? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/o44qsd/how_can_i_change_image_of_shape_484815_to_48483/"}, {"autor": "SubstantialRange", "date": "2021-06-20 12:13:52", "content": "[N] Image Similarity Challenge - Facebook AI /!/ [https://www.drivendata.org/competitions/79/competition-image-similarity-1-dev/](https://www.drivendata.org/competitions/79/competition-image-similarity-1-dev/)\n\n&amp;#x200B;\n\n&gt;Welcome to the Image Similarity Challenge! In this competition, you  will be building models that help detect whether a given query -----> image !!!  is  derived from any of the -----> image !!! s in a large reference set.  \n&gt;  \n&gt;Content tracing is a crucial component on all social media platforms  today, used for such tasks as flagging misinformation and manipulative  advertising, preventing uploads of graphic violence, and enforcing  copyright protections. But when dealing with the billions of new images  generated every day on sites like Facebook, manual content moderation  just doesn't scale. They depend on algorithms to help automatically flag  or remove bad content.  \n&gt;  \n&gt;This competition allows you to test your skills in building a key  part of that content tracing system, and in so doing contribute to  making social media more trustworthy and safe for the people who use it.", "link": "https://www.reddit.com/r/MachineLearning/comments/o44dnp/n_image_similarity_challenge_facebook_ai/"}, {"autor": "KeineAhnungDavonViel", "date": "2021-06-20 09:04:16", "content": "[R] Looking for Data Labelling Tool for Volumetric Multifeature Datasets /!/  \n\nDear all,\n\nwe are currently working on 3D medical imagery with 6 layers of different information, a sample has a size of around 120 Gb.\n\nWe are looking for a way to label these conveniently according to the following criteria:\n\n\\- extraction of individual z-sections of the XYZ volume (we have this covered)\n\n\\- labelling of one of these z-sections with the ability to turn on- and off- the individual 6 layers while there is always a merged -----> picture !!!  of all the activated layers visible\n\n\\- adjusting gamme &amp; histogram of these layers individually\n\nLater on we would like to do this with volumetric data the same way, labelling structures with a volume based on several z-sections of the volume.\n\nWe couldn't find a standard solution and are evaluating we should adjust one of the open source solutions out there.\n\nDoes anyone know if any other data labelling tool has some of the above mentioned features?", "link": "https://www.reddit.com/r/MachineLearning/comments/o41in6/r_looking_for_data_labelling_tool_for_volumetric/"}, {"autor": "nanowire21", "date": "2021-10-24 17:07:30", "content": "[D] How soon after the -----> camera !!! -ready deadline does the proceedings come out? /!/ In this case I'm specifically asking about NeurIPS and how it's been in years past. Typically in years past by now the list of accepted papers would be posted. How soon after the camera-ready paper deadline does the proceedings come out typically?", "link": "https://www.reddit.com/r/MachineLearning/comments/qewcpx/d_how_soon_after_the_cameraready_deadline_does/"}, {"autor": "JMG518", "date": "2021-10-23 02:52:43", "content": "[D] Replicating Clip+VQGAN Settings Again /!/ Hey guys! I imported a reference -----> image !!!  of a building for the text-to------> image !!!  code to follow. I loved the results. Because the reference image is of a building, I want to replicate THE EXACT SAME results that the text-to-image AI created with the first reference image, but on a different perspective of the building; hence a different reference image. Does anyone know how I can lock the settings in place, so the code can generate the same result again but using a different reference image? Thanks! The code I used is here: [https://colab.research.google.com/drive/1go6YwMFe5MX6XM9tv-cnQiSTU50N9EeT](https://colab.research.google.com/drive/1go6YwMFe5MX6XM9tv-cnQiSTU50N9EeT)", "link": "https://www.reddit.com/r/MachineLearning/comments/qdwbzo/d_replicating_clipvqgan_settings_again/"}, {"autor": "KingKoozy", "date": "2021-10-20 08:13:36", "content": "Best computer specs for running (not training) an ML algorithm/model [P] [D] /!/ Hey all, \n\nSo I'm doing a project which involves using an already trained ML algorithm to detect cars via an IP -----> camera !!! . I'm trying to figure out what the most important components for running such a model are with regards to computer specs/hardware. \n\nI've already built a server to run it and it works fine with a single camera but I'm trying to figure out how many cameras I can point at the server before it starts having issues recognizing cars. My current bet is around 20 with my current specs. But I'm curious to see if I could bump that higher with better hardware or if there are diminishing returns past a certain point or if it's linear. \n\nCurrent specs:\n1920x threadripper\n32 gigs of ram\n1660ti\n512gb m.2\nGigabit internet speeds\n(Pretty sure the other specs don't matter)\n\nBasically does it all just come down to GPU and I can save on the other parts? If I shoved a 3080 in there would I maybe pull 40 cameras? Would lowering the resolution of the camera image increase camera count at the expense of accuracy? \n\nAgain not trying to train the model just run it with as many cameras as possible on one server.", "link": "https://www.reddit.com/r/MachineLearning/comments/qbvy65/best_computer_specs_for_running_not_training_an/"}, {"autor": "KirillTheMunchKing", "date": "2021-10-19 15:35:35", "content": "[D] LaMa Paper explained - Resolution-robust Large Mask Inpainting with Fourier Convolutions (5-minute summary by Casual GAN Papers) /!/ Ever   tried to take a scenic -----> picture !!!  just to be photobombed by some random  tourists? Don\u2019t worry, Roman Suvorov and the team at SAIC-Moscow  recently unveiled a model called LaMa (large mask inpainting) that takes  care of it for you. The model excels at inpainting large irregular  masks using fast Fourier convolutions that have a receptive field equal  to the entire image and a specialized wide receptive field perceptual  loss that boosts the consistency for distant regions of an image.! A    surprising yet extremely useful outcome of the paper is that the  pretrained model scales up to 2k resolutions quite trivially.\n\nFresh out of the oven! Full summary: [https://www.casualganpapers.com/large-masks-fourier-convolutions-inpainting/LaMa-explained.html](https://www.casualganpapers.com/large-masks-fourier-convolutions-inpainting/LaMa-explained.html)\n\n[LaMa](https://i.redd.it/1m1f2c34ffu71.gif)\n\narxiv: [https://arxiv.org/pdf/2109.07161.pdf](https://arxiv.org/pdf/2109.07161.pdf)  \ncode: [https://github.com/saic-mdal/lama](https://github.com/saic-mdal/lama)\n\nSubscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!", "link": "https://www.reddit.com/r/MachineLearning/comments/qbe17v/d_lama_paper_explained_resolutionrobust_large/"}, {"autor": "fourthie", "date": "2021-10-19 14:16:56", "content": "[P] Mapping an -----> image !!!  to a 3D face model (iPhone AR compatible) /!/ I've been interested in 3D models for AR recently. I set out to create a script to generate 3D head models from selfies which led me down the rabbit hole of creating a (free) service for this.\n\n\nI retrained [DECA](https://github.com/YadiraF/DECA) using the original dataset and additional data that I generated and then combined this with a library I wrote to convert .obj files to .usdz, a format used on iPhone to describe 3D models that are compatible with its AR viewer.\n\n\nHere is the service: [https://facemodel.me](https://facemodel.me)\n\n\nI'm in the process of tidying up the GitHub repo and will make it public if there is interest. I'm posting here to get feedback on the generated 3D models and to discuss whether there are other use cases for downstream ML applications that people here would find useful.", "link": "https://www.reddit.com/r/MachineLearning/comments/qbcfu6/p_mapping_an_image_to_a_3d_face_model_iphone_ar/"}, {"autor": "fourthie", "date": "2021-10-19 14:13:56", "content": "Mapping an -----> image !!!  to a 3D face model (AR compatible) /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/qbcdnw/mapping_an_image_to_a_3d_face_model_ar_compatible/"}, {"autor": "KingsmanVince", "date": "2021-10-18 00:08:05", "content": "[D] Is there any recent survey paper about -----> image !!! -text fusion models? /!/ In NLP field, [this paper](https://arxiv.org/abs/2104.10640) discussed most recent and well-known models such as GPT-3, BERT, BART. \n\nTherefore, I would like to know that any survey paper like that but for multimodal machine learning (the combination of vision and language).\n\nThanks for your help.", "link": "https://www.reddit.com/r/MachineLearning/comments/qab17t/d_is_there_any_recent_survey_paper_about/"}, {"autor": "Lost_Elephant", "date": "2021-10-17 19:54:45", "content": "[Discussion] Resource for Identifying Technical Requirements for Deep Learning Porjects? /!/ Apologies if this is the wrong sub.\n\nWe are looking to build a computer capable of doing DL -----> image !!!  processing research at an academic medical center. I need to give our IT folks an idea of what kind of specs we need and they will source the parts/assemble in-house.\n\nWe do mainly imaging research, as of now virtually all 2D images, no video likely in the near future. Typically use CNN-type architectures. Mainly use TensorFlow although there's some push for PyTorch.\n\nCan anybody point me to a good resource for answering these questions?", "link": "https://www.reddit.com/r/MachineLearning/comments/qa6b15/discussion_resource_for_identifying_technical/"}, {"autor": "JMG518", "date": "2021-10-17 04:06:06", "content": "[D] Using Reference -----> Image !!! s for Text-to------> Image !!!  AI /!/ Hi guys! I have a reference image that I want to plug into an AI code. I hope this will generate realistic results. Are there any Github or Google Colab codes that allow you to import a reference image/sketch that the AI can use? Are there any other tips for generating realistic results from Text-to-Image? Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/q9retb/d_using_reference_images_for_texttoimage_ai/"}, {"autor": "JMG518", "date": "2021-10-17 04:04:18", "content": "Using Reference -----> Image !!! s for Text-to------> Image !!!  AI /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/q9rdvw/using_reference_images_for_texttoimage_ai/"}, {"autor": "BobPois", "date": "2021-10-16 16:21:22", "content": "Could you advise me AIs to try ? [Discussion] /!/ I hope this isn't off-topic.\nI really like artificial intelligences. I already know the one on the website [AI Dongeon](https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;url=https://play.aidungeon.io/&amp;ved=2ahUKEwjH8ff0587zAhVExYUKHaglDXcQFnoECAUQAQ&amp;usg=AOvVaw2IwxF0CzAMxg6lARmY0wbx&amp;cshid=1634383216562) which is able to create coherent stories in which the player participates in writing and can create any type of scenario. Recently I discovered the [Nightcaf\u00e9](https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;url=https://creator.nightcafe.studio/&amp;ved=2ahUKEwi1nMPH6M7zAhXOxIUKHXz8BWoQFnoECAkQAg&amp;usg=AOvVaw2sM00ffDTgfE4Hnp1K2g2m) website where an AI is able to turn a crazy or subjective sentence into an -----> image !!! .\n\nI would be delighted to discover other equally interesting artificial intelligences, could you advise me?", "link": "https://www.reddit.com/r/MachineLearning/comments/q9f7h8/could_you_advise_me_ais_to_try_discussion/"}, {"autor": "JMG518", "date": "2021-10-16 04:22:24", "content": "[D] BEST GITHUB TEXT-TO------> IMAGE !!!  AI CODE /!/ Hi guys! I am working on an architecture project to design a parking garage, and I want to use AI to help in the design process. Any tips on the MOST REALISTIC/PHOTO-REALISTIC AI code that I can use? If anyone can provide links to a specific code, that would be incredible. Thanks everyone!", "link": "https://www.reddit.com/r/MachineLearning/comments/q94ral/d_best_github_texttoimage_ai_code/"}, {"autor": "DrCam", "date": "2021-02-02 09:30:41", "content": "Signal correlation method [R][P] /!/ I have a bunch of measurements (1000s of sites), with each site often having 10k or more data points for each data stream, and I typically have 3-4 data streams at each site.  This is essentially time series data.\n\nThese data streams are used for a number of calculations, but one of the primary tasks that people do is correlate the signals.  This is shown in the linked -----> image !!!  below where I've manually matched 4 key points.  Typically people will pick between 10-20 distinct events for each site, so if you have 1000s of sites you can see how these becomes incredibly tedious.\n\nWe have the following beliefs about the data:\n\n* By picking a sparse data set (e.g., the first 10% hand picked correlations) we can make a strong guess about the location of the marker in adjacent sites.\n* The time-separation between markers is not constant, but generally has only minor variation\n* Marker A will always come before Marker B, and Marker B before Marker C and so on.\n* Not all markers will be present at all measurement sites\n* Not all measurement sites were able to evaluate a full time series (in example below, imagine if one had been cut off at 8000ms instead of going past 10,000ms\n* The data can easily be normalized to have a similar range of values.\n\nI took a shot at using dynamic time warping (DTW), and the results were good when you had two very similar signals that had all the major markers present, but broke down when that wasn't the case.\n\nI can't help but to think there should be some way to use either SIFT (scale invariant feature transform) or a 1D conv net, or something to handle this, but I can't quite wrap my head around it.\n\nAny advice or suggestions or links to similar problems would be greatly appreciated.\n\n[https://i.stack.imgur.com/E083l.png](https://i.stack.imgur.com/E083l.png)", "link": "https://www.reddit.com/r/MachineLearning/comments/larwum/signal_correlation_method_rp/"}, {"autor": "Bennyyy27", "date": "2021-02-02 09:20:08", "content": "[R][P] Data/Network Structure or Approach for featurevector based network /!/  Hi community,\n\nfirst, to introduce myself: I am relatively new to the field of ML/DL, however, I tried to gain more experience by passing several online courses in this field. I want to research on some problems in my spare time, like this one:\n\nThe objective is to use modern methods (ML/DL) to learn an algorithm for decisions in the future.\n\nThe problem:\n\nI have a matrix which represents a system. Each row represents a feature vector of a subsystem. The first entry is an integer (identifier) which is 2 in most of the cases, the first row is always 1 (main subsystem). There are some systems with only 4 rows, some with 6, some with only 2. Some systems dont have subsystems so only have 1 row.\n\nI can select different processing approaches for each subsystem described in integer (e.g. 1-5).\n\nThe combination of approaches for a system leads to result value(s). Exemplary scheme is shown in -----> picture !!! .\n\n&amp;#x200B;\n\n[Schematik Dataset](https://preview.redd.it/sj0cq6hw71f61.png?width=1326&amp;format=png&amp;auto=webp&amp;s=7f146149697c38feb25de118a59a4334ea27d8d0)\n\nThis would be the level 1 problem. Level 2 would mean that the whole system contains multiple systems \u00e0 multiple subsystems. (Maybe depicted as graph network). The combinations of all approaches lead to an overall result value(s) again. However, the first step would be to find an approach to the level 1 problem.\n\nMy idea has been to combine the matrix and the approach vector to a new matrix. Flattening the matrix could be the first low-level input for a NN or any ML algorithm. The problem of different matrix sizes from system to system could be potentially mitigated by using padding and defining a maximum subsystem count, thus filling up the rest with zeros.\n\nI also read about Graph Neural Networks which comes close to my first idea, however, I do not know much about them. I was imagining a Graph where the subsystems X\\_ij are connected to the main subsystem X\\_i. (Maybe better understandable with this image:)\n\n&amp;#x200B;\n\n[Schematic system](https://preview.redd.it/jsvxkgqx71f61.png?width=1073&amp;format=png&amp;auto=webp&amp;s=7853fa37f7724a6f0b4360f76a770c3b274820fe)\n\nEach node inherits a featurevector like shown above and each node can have a label for its approach(1-5). The overall system leads to the result values. however, since i do not know much about GNN, I am slightly overwhelmed.\n\nDoes somebody have any idea how to cope with this? Or does somebody have another promising approach to this kind of problem?\n\nExcited for any feedback/discussions and thank you.", "link": "https://www.reddit.com/r/MachineLearning/comments/larrm5/rp_datanetwork_structure_or_approach_for/"}, {"autor": "crazydudeKuku", "date": "2021-02-02 02:28:50", "content": "[D] Being asked to annotate data during hiring /!/ I am a new grad with MS and am looking for jobs in computer vision domain. A company (early stage startup) has asked me to train a network and also  annotate 100 -----> image !!! s (with 20 key points in each -----> image !!! ) and see if how fine tuning on this helps \n\nI feel uncomfortable about the annotation and it feels like exploitation. Can anyone share if this is common or how to respond to this?\n\nedit - By exploitation I mean my time is being wasted for no purposeful reason imo. Yes, it is part of the job, but I can't see the value of it in the hiring process.", "link": "https://www.reddit.com/r/MachineLearning/comments/lal7qa/d_being_asked_to_annotate_data_during_hiring/"}, {"autor": "curiousbutadhd", "date": "2021-06-27 21:12:48", "content": "[D] Problem About Real Time Object Detection On Raspberry Pi /!/ Hey people,\nWe are building autonomous underwater vehicle and using yolov5 on raspberry pi for realtime object detection but model detects object from -----> photo !!!  about 3 seconds and its so slow for us.\nSo do you have any advice, should i use just opencv without ML model.\nI just need to detect rectangle and circle with specific measurements.\nI am programmer but dont have lots of ecperience with these subjects.\nPlease help me \ud83d\ude4f \nThanks\u2026", "link": "https://www.reddit.com/r/MachineLearning/comments/o95rp7/d_problem_about_real_time_object_detection_on/"}, {"autor": "ykilcher", "date": "2021-06-27 12:15:18", "content": "[D] A critical take on: The Dimpled Manifold Model of Adversarial Examples in Machine Learning (Full Video Analysis) /!/ [https://youtu.be/k\\_hUdZJNzkU](https://youtu.be/k_hUdZJNzkU)\n\nAdversarial Examples have long been a fascinating topic for many Machine Learning researchers. How can a tiny perturbation cause the neural network to change its output by so much? While many explanations have been proposed over the years, they all appear to fall short. This paper attempts to comprehensively explain the existence of adversarial examples by proposing a view of the classification landscape, which they call the Dimpled Manifold Model, which says that any classifier will adjust its decision boundary to align with the low-dimensional data manifold, and only slightly bend around the data. This potentially explains many phenomena around adversarial examples. Warning: In this video, I disagree. Remember that I'm not an authority, but simply give my own opinions.\n\n&amp;#x200B;\n\nOUTLINE:\n\n0:00 - Intro &amp; Overview\n\n7:30 - The old mental -----> image !!!  of Adversarial Examples\n\n11:25 - The new Dimpled Manifold Hypothesis\n\n22:55 - The Stretchy Feature Model\n\n29:05 - Why do DNNs create Dimpled Manifolds?\n\n38:30 - What can be explained with the new model?\n\n1:00:40 - Experimental evidence for the Dimpled Manifold Model\n\n1:10:25 - Is Goodfellow's claim debunked?\n\n1:13:00 - Conclusion &amp; Comments\n\n&amp;#x200B;\n\nPaper: [https://arxiv.org/abs/2106.10151](https://arxiv.org/abs/2106.10151)\n\nMy replication code: [https://gist.github.com/yk/de8d987c4eb6a39b6d9c08f0744b1f64](https://gist.github.com/yk/de8d987c4eb6a39b6d9c08f0744b1f64)\n\nGoodfellow's Talk: [https://youtu.be/CIfsB\\_EYsVI?t=4280](https://youtu.be/CIfsB_EYsVI?t=4280)", "link": "https://www.reddit.com/r/MachineLearning/comments/o8vyt1/d_a_critical_take_on_the_dimpled_manifold_model/"}, {"autor": "Jump2Fly", "date": "2021-06-27 10:59:01", "content": "[P] I created a video about how you can train a neural network (in python) to learn complex -----> image !!! /video classification tasks (like in-game detection) using transfer learning! The GitHub repo is linked in the video description. Hope this is useful or helpful for some of you guys :-)", "link": "https://www.reddit.com/r/MachineLearning/comments/o8ux7f/p_i_created_a_video_about_how_you_can_train_a/"}, {"autor": "redpolarbearen", "date": "2021-06-26 17:17:57", "content": "[Research]Evaluating a CNN -multi class model with two separate thresholds /!/  \n\nI have a model that outputs three classes. But here instead of one threshold, it depends on a combination of two (user input threshold). One threshold varies from 0.1 to 1.0 and the other varies from from 1 to 800 ( this is user input based on domain knowledge).\n\nHow can I evaluate this model for both a) Balanced dataset b) Imbalanced dataset\n\nA normal ROC samples 1D threshold space. How can I adjust this for 2 different thresholds?\n\nIs there any other metrics that I can use that will accommodate the two thresholds?\n\nIf I use sensitivity and recall..( and many others) I am not sure how to put the two threshold -----> picture !!! , do I have then look at the individual thresholds separately for ( example : For 25, we look at threshold ; 0/.9, 0.8, 0.7..) then for (50, we look at ...) and may be show one more case to explain how the threshold affect the positive predictions?", "link": "https://www.reddit.com/r/MachineLearning/comments/o8eyzr/researchevaluating_a_cnn_multi_class_model_with/"}, {"autor": "Snoo-62877", "date": "2021-10-12 14:33:21", "content": "[D] question about Semi-Supervised Learning /!/ Dataset: \n\n\\- I have dataset of 10 classes (classified by me)\n\n\\- they are 2d-represented mechanical materials\n\n&amp;#x200B;\n\nProblem I want to solve:\n\n\\- Labeling unlabeled data based on labeled data\n\n\\- I already have all of them labeled but I want to go back and make use of Semi Supervised Learning so that the process of labeling data seems more reasonable than the way done manually by me.\n\n&amp;#x200B;\n\nspecifically, there are 2,000 labeled data, each classes. and 100,000 unlabeled data. \n\nsince the data are easily represented by -----> image !!! , I thought of using ResNet to train and predict on unlabeled data, and use the result as class.\n\n&amp;#x200B;\n\nis it a naive way? hope to hear about a reasonable way to do semi supervised learning to label unlabeled data. thanks", "link": "https://www.reddit.com/r/MachineLearning/comments/q6nj2f/d_question_about_semisupervised_learning/"}, {"autor": "KirillTheMunchKing", "date": "2021-10-11 21:48:52", "content": "[D] Paper explained - WarpedGANSpace: Finding non-linear RBF paths in GAN latent space (5-minute summary) /!/ Linear  directions are great for GAN-based -----> image !!!  editing, but who is to  say  that going straight across the latent space is the best option?  Well,  according to Christos Tzelepis and his colleagues from the Queen  Mary  University of London non-linear paths in the latent space lead to  more  disentangled and interpretable changes in the synthesized images   compared to existing SOTA methods! Their method, which is based on   optimizing a set of RBF warp functions, works without supervision and   learns a set of easily distinguishable image editing directions such as   pose and facial expressions.\n\nFull summary: [https://www.casualganpapers.com/unsupervised-discovery-nonlinear-latent-editing-directions-generator/WarpedGANSpace-explained.html](https://www.casualganpapers.com/unsupervised-discovery-nonlinear-latent-editing-directions-generator/WarpedGANSpace-explained.html)\n\nhttps://preview.redd.it/ix041l8f6ws71.png?width=996&amp;format=png&amp;auto=webp&amp;s=3017807c58e5636e0dd131f8588bf1b2969000d3\n\narxiv: [https://arxiv.org/pdf/2109.13357v1.pdf](https://arxiv.org/pdf/2109.13357v1.pdf)  \ncode: [https://github.com/chi0tzp/WarpedGANSpace](https://github.com/chi0tzp/WarpedGANSpace)", "link": "https://www.reddit.com/r/MachineLearning/comments/q66pl7/d_paper_explained_warpedganspace_finding/"}, {"autor": "Gere1", "date": "2021-10-11 10:13:53", "content": "[D] What is a summary of the development and SOTA of -----> image !!!  classification? /!/ Does someone know a resources which \\*succinctly\\* summarizes the advances of image classification? Or maybe someone can write a good summary?\n\nHere is roughly what I've seen so far and maybe someone can fill the gaps:\n\nNeural networks / Perceptron and backpropagation have been known for many decades, but only with powerful computers they could be made useful. At first there was AlexNet. (What was the breakthrough there?). There was unsupervised pretraining. At some point BatchNorm was key. There was ResNet (it only introduce skip connections to enable more layers?). At some point EfficientNet suggested nicely working sizes for layers and how to scale. Unsupervised pretraining is not recommended anymore. Is normalization still recommended? Nowadays, we are at 90% top-1 accuracy on ImageNet which is still notably below human error(?) (even taking into account mistakes in the dataset) [https://paperswithcode.com/sota/image-classification-on-imagenet](https://paperswithcode.com/sota/image-classification-on-imagenet) .  So SOTA for image classification are still the neural networks from decades ago, but with powerful computers, skip connections and an idea for how to choose layer sizes(?)\n\nIt would be brilliant to see a TLDR of image classification along these lines, which still captures all important milestones. Anyone?", "link": "https://www.reddit.com/r/MachineLearning/comments/q5sprs/d_what_is_a_summary_of_the_development_and_sota/"}, {"autor": "QuantaHD", "date": "2021-10-27 04:13:39", "content": "[D] Help with JODIE model data (Temporal Interaction Networks) /!/  Hey everyone,\n\nI\u2019m trying to create my own Reddit dataset to implement JODIE model used in the \u2018Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks\u2019 paper (Paper: [https://arxiv.org/abs/1908.01207](https://arxiv.org/abs/1908.01207) Code: [https://github.com/srijankr/jodie](https://github.com/srijankr/jodie)) The paper is a bit vague on what features they use and how the data looks like.\n\nI also downloaded the reddit.csv data they had available but it looks strange, not sure how it was formatted. There are only 5 columns but there are a bunch of numbers in front of those columns as seen in the -----> picture !!! . Does anyone know any information of how their data is set up or how it needs to be formatted to use the model?\n\n[reddit data](https://preview.redd.it/xbfdzcfa4xv71.png?width=1648&amp;format=png&amp;auto=webp&amp;s=5eb4217864960d033be4158ae9cfe6d9bc55c17f)\n\nPlease let me know if there is a better sub I could post this! Thanks for the help!", "link": "https://www.reddit.com/r/MachineLearning/comments/qgo8ir/d_help_with_jodie_model_data_temporal_interaction/"}, {"autor": "NotMyNapoleon", "date": "2021-10-26 23:17:19", "content": "[D] StyleGAN3 using a starting -----> image !!! ? /!/ Hello,  \nI've been playing with StyleGAN3 setups in notebooks for a bit, but I'm really hoping to find an implementation that starts with an image of my choosing. Do you have any suggestions for where to start?", "link": "https://www.reddit.com/r/MachineLearning/comments/qgixti/d_stylegan3_using_a_starting_image/"}, {"autor": "NotMyNapoleon", "date": "2021-10-26 22:58:12", "content": "StyleGAN3 with -----> image !!!  input? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/qgik1o/stylegan3_with_image_input/"}, {"autor": "svantana", "date": "2021-10-26 22:08:23", "content": "[P] ImageNet size-accuracy pareto front, 2012-2021 /!/ &amp;#x200B;\n\nhttps://preview.redd.it/taqxj24javv71.png?width=1458&amp;format=png&amp;auto=webp&amp;s=2e109873952da6c6fd211f779bfd81cea8c4b53a\n\nPaperswithcode has expanded their [-----> image !!! net results](https://paperswithcode.com/sota/-----> image !!! -classification-on------> image !!! net) with parameter counts. I was curious to see how size-vs-accuracy has evolved over time. Caveat: some 40% of the results lack parameter counts, so those are excluded here.", "link": "https://www.reddit.com/r/MachineLearning/comments/qghjqi/p_imagenet_sizeaccuracy_pareto_front_20122021/"}, {"autor": "OtherwiseEgg2540", "date": "2021-10-26 09:17:22", "content": "Is there still something worth studying in -----> image !!!  retrieval? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/qg2hbo/is_there_still_something_worth_studying_in_image/"}, {"autor": "thinktaj", "date": "2021-10-26 05:19:06", "content": "Yet another \"test\" -----> image !!!  for AI models", "link": "https://www.reddit.com/r/MachineLearning/comments/qfzcks/yet_another_test_image_for_ai_models/"}, {"autor": "SwimHopeful5123", "date": "2021-10-26 01:05:57", "content": "[D] - Algorithmically choosing best training data for semantic segmentation /!/ I'm working on a semantic segmentation problem with access to a very large dataset of -----> image !!! -label pairs for training. While quantity of the training data has never been an issue, the quality is. What I mean is a large percentage of our data does not have very accurate GT segmentation labels. My intuition is that its better to have smaller, but high quality image-label pairs, rather than having a very large, but inaccurate image-label pairs.\n\n&amp;#x200B;\n\n1. First of all, is my intuition correct and are there papers to support this\n2. Secondly, is there a good, algorithmic way to choose high quality training data from this large pool of image-label pairs ? I'm thinking of computer vision algorithms or some sort of preprocessing to filter out potentially bad training examples ?", "link": "https://www.reddit.com/r/MachineLearning/comments/qfv2za/d_algorithmically_choosing_best_training_data_for/"}, {"autor": "achaiah777", "date": "2021-10-25 15:43:23", "content": "[P] Pywick - High-Level Training framework for Pytorch /!/ Hi AI practitioners,\n\nWe wanted to bring to your attention another huge release of the [Pywick](https://github.com/achaiah/pywick) training framework. Some notable features that you may find useful are:\n- 700+ classification network variants\n- Dockerized runtime for easy execution in the cloud. Demo included with the docker -----> image !!! !\n- Full configuration via `yaml` files with no coding required to get started\n- Bleeding edge optimizers, loss functions, activation functions etc\n- Thorough documentation\n\nHope you find it useful!", "link": "https://www.reddit.com/r/MachineLearning/comments/qfjjm6/p_pywick_highlevel_training_framework_for_pytorch/"}, {"autor": "adjaranet", "date": "2021-09-11 11:30:25", "content": "Does YOLOv5 use unlabeled parts of an -----> image !!! ? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pm5sa7/does_yolov5_use_unlabeled_parts_of_an_image/"}, {"autor": "DevStarship", "date": "2021-09-10 22:56:24", "content": "[P] A machine learning model that classifies a cartoon character as a goody or baddy /!/ I've created a machine learning -----> image !!!  classification model using [ML.NET](https://ML.NET) trained with a set of goody cartoon characters and baddy cartoon characters.\n\nIt does a pretty good job of determining whether a cartoon character is a goody or baddy by processing an image of it.\n\nI hope to put this live at [GoodyOrBaddy.com](https://GoodyOrBaddy.com) in the near future.\n\nYou can read my approach to train the original model here.\n\n[https://www.linkedin.com/pulse/training-image-classification-model-using-mlnet-lee/](https://www.linkedin.com/pulse/training-image-classification-model-using-mlnet-lee/)", "link": "https://www.reddit.com/r/MachineLearning/comments/plvmsa/p_a_machine_learning_model_that_classifies_a/"}, {"autor": "DonjiDonji", "date": "2021-09-10 20:50:41", "content": "[Research] Launched an in depth interview with Yuval Alaluf, an Researcher doing AI -----> image !!!  Toonification /!/ Here are the timestamped youtube links\n\n[0:00](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=0s) Introducing: Yuval!  \n[0:52](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=52s) Yuval\u2019s Work on Toonification\n\nhttps://preview.redd.it/zhbwc5nsnqm71.jpg?width=2041&amp;format=pjpg&amp;auto=webp&amp;s=370b75dcdc32777d19eade229794ae49c64d4f3a\n\n  \n[2:52](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=172s) How Did Yuval Get Into AI?  \n[5:17](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=317s) Transitioning from Bio Work to Image Generation  \n[8:22](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=502s) ReStyle as Middle Ground  \n[9:36](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=576s) Trial and Error for Generating Emittings  \n[10:59](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=659s) Ensuring Consistent Flow in Passes  \n[13:52](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=832s) Starting Small (stylegan)  \n[15:55](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=955s) Assuring Good Latent Space Embedding  \n[18:06](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=1086s) Yuval\u2019s Model Training Process  \n[20:14](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=1214s) Sensitive to Changes  \n[21:30](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=1290s) Designing the Toonifying Process  \n[23:20](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=1400s) The Bootstrapping Process  \n[24:55](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=1495s) Making Cars Into Faces?  \n[26:58](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=1618s) Where is Image Generation Headed?  \n[30:10](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=1810s) Snapshot of SnapChat\u2019s Secret Sauce?  \n[33:21](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2001s) Restyle with Temporal Image Smoothing?  \n[34:38](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2078s) From Images to Video  \n[35:52](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2152s) Two Minute Papers Acceptance!  \n[37:16](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2236s) Open Source Welcome!  \n[38:59](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2339s) Yuval\u2019s Future Goals  \n[40:04](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2404s) Transitioning from EdTech Research  \n[41:26](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2486s) Yuval: Mastering Faces  \n[42:34](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2554s) 2-Minute Papers and NERF\u2019s  \n[45:09](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2709s) How Humans Learn v. How Machines Learn  \n[46:06](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2766s) Working With Others Is Essential!  \n[47:17](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2837s) Advice to Jr. Developers  \n[48:55](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2935s) How Yuval\u2019s Work Got Discovered  \n[50:19](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=3019s) Thank you to Yuval!\n\nLet us know what you think of the episode!", "link": "https://www.reddit.com/r/MachineLearning/comments/plte94/research_launched_an_in_depth_interview_with/"}, {"autor": "Altruistic-Agency-40", "date": "2021-09-10 17:23:45", "content": "[D] Does there exist a \"purely qualitative\" situation? /!/ Kind of a weird question, but this arose from my curiosity regarding the use of machine learning to answer \"purely qualitative\" questions. From my limited prior knowledge of machine learning, most every algorithm/strategy that I have read uses some sort combination of numerical parameters or categorical parameters to break a problem down into inputs/outputs. Solutions are also evaluated in terms of these parameters or with a right/wrong(thinking -----> picture !!!  identification).\n\nThis got me thinking about how to solve a problem related to politics/emotions/relationships/etc., that might not be able to be effectively boiled down to a set of parameters. For instance, how could inputs be selected when trying to decide on what meal to eat given a choice between 3? Or where to go on vacation next? How would solutions be evaluated and compared?\n\nHow can solutions be evaluated given a choice between two people to pick a spouse. In this case, inputs can maybe be defined(rating attractiveness 1-10, comparing number children desired, etc.) but how do you compare solutions? One might end up with a \"happy\" life in CA with 2 kids and a 4 bedroom house with some salary and other conditions while the other ends up with 1 kid in a house in Texas. How would situations like these even be compared. You may be able to conjure up some metrics to assess the solutions, but they are unlikely to really be effective as it is such a subjective question.\n\nAre there always ways to quantify/categorize these types of problems? Or do questions/scenarios exist where there are no ways to granulize them into categories/numbers for inputs or solution comparison? And if the answer to the second question is yes, are there existing algorithms, particularly in the realm of AI/ML/etc. to help solve such problems? Is this even a possibility? Or, is my question itself flawed in some way?", "link": "https://www.reddit.com/r/MachineLearning/comments/plpini/d_does_there_exist_a_purely_qualitative_situation/"}, {"autor": "Altruistic-Agency-40", "date": "2021-09-10 17:22:48", "content": "[Discussion] Does there exist a \"purely qualitative\" situation? /!/ Kind of a weird question, but this arose from my curiosity regarding the use of machine learning to answer \"purely qualitative\" questions. From my limited prior knowledge of machine learning, most every algorithm/strategy that I have read uses some sort combination of numerical parameters or categorical parameters to break a problem down into inputs/outputs. Solutions are also evaluated in terms of these parameters or with a right/wrong(thinking -----> picture !!!  identification).\n\nThis got me thinking about how to solve a problem related to politics/emotions/relationships/etc., that might not be able to be effectively boiled down to a set of parameters. For instance, how could inputs be selected when trying to decide on what meal to eat given a choice between 3? Or where to go on vacation next? How would solutions be evaluated and compared?\n\nHow can solutions be evaluated given a choice between two people to pick a spouse. In this case, inputs can maybe be defined(rating attractiveness 1-10, comparing number children desired, etc.) but how do you compare solutions? One might end up with a \"happy\" life in CA with 2 kids and a 4 bedroom house with some salary and other conditions while the other ends up with 1 kid in a house in Texas. How would situations like these even be compared. You may be able to conjure up some metrics to assess the solutions, but they are unlikely to really be effective as it is such a subjective question.\n\nAre there always ways to quantify/categorize these types of problems? Or do questions/scenarios exist where there are no ways to granulize them into categories/numbers for inputs or solution comparison? And if the answer to the second question is yes, are there existing algorithms, particularly in the realm of AI/ML/etc. to help solve such problems? Is this even a possibility? Or, is my question itself flawed in some way?", "link": "https://www.reddit.com/r/MachineLearning/comments/plphzp/discussion_does_there_exist_a_purely_qualitative/"}, {"autor": "BerghainInMyVeins", "date": "2021-09-10 15:02:15", "content": "[Project] How to zoom in on/crop a certain area in an -----> image !!! . /!/ \nHello, I\u2019m creating a machine learning program that will find an object in an image, zoom in on it/crop the photo, and then classifies the object. \n\nHow can I achieve the first step of basically \u201cwhere\u2019s Waldo\u201d &amp; crop/zoom in to perform the next step, classification, on that object. \n\nThank you for your time.", "link": "https://www.reddit.com/r/MachineLearning/comments/plmsns/project_how_to_zoom_in_oncrop_a_certain_area_in/"}, {"autor": "BerghainInMyVeins", "date": "2021-09-10 14:43:47", "content": "How to zoom in on a certain area in an -----> image !!!  /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/plmfum/how_to_zoom_in_on_a_certain_area_in_an_image/"}, {"autor": "FrereKhan", "date": "2021-09-10 12:33:15", "content": "[P] Training a spiking NN to produce images /!/ Hi!\n\nWe just released a new version of our ML for SNNs library \"Rockpool\", and I wanted to share a tutorial-style project we made for backprop training of SNNs.\n\nhttps://rockpool.ai/tutorials/easter/easter-snn-images.html\n\nThe network is very simple (two spiking layers, two linear weight layers), and it maps sequences of frozen Poisson input event streams to several -----> image !!!  classes. When the network sees an input it recognises, it produces an event-encoded image on the output.\n\nThe images are encoded such that each pixel column in the output image is represented by three output neurons (R, G, B), with the firing rate of each neuron encoding the intensity of the channel. The rows of the images are output over time, so each channel changes its firing rate over time to encode the sequence of pixels in the column of the image.\n\nWe used the spiking neuron modules in Rockpool to build the network. These particular modules are based on [Jax] (https://github.com/google/jax), which gives us compilation to GPU/TPU/CPU as well as automatic differentiation \"for free\". There are similar modules in Rockpool based on [Torch](https://pytorch.org), if you prefer to use a torch training pipeline.\n\nThe training approach is standard gradient descent using Adam, performing backprop-through-time over the spiking network layers. The Rockpool modules support training of not just weights, but also synaptic and membrane time constants.\n\nYou can get an executable notebook of the tutorial to play with here: [binder](https://mybinder.org/v2/gl/synsense%2Frockpool/v2.2?filepath=docs/tutorials/easter/easter-snn-images.ipynb)\nYou will need to install *jax* and *jaxlib* by adding a cell with\n\n    !pip install jax jaxlib\n\nat the top of the notebook.\n\nRockpool is designed to build ML applications using SNNs, both to explore new network architectures and to support deploying networks to low-power SNN inference chips.\nDocs: (rockpoo.ai)[https://rockpool.ai]\npip: *pip install rockpool*\nconda: *conda install -c conda-forge rockpool*", "link": "https://www.reddit.com/r/MachineLearning/comments/plk492/p_training_a_spiking_nn_to_produce_images/"}, {"autor": "khaled-abdelhamid98", "date": "2021-09-09 19:17:48", "content": "medical -----> image !!!  feature extraction /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pl4jft/medical_image_feature_extraction/"}, {"autor": "Ignatij", "date": "2021-09-09 11:46:08", "content": "[D] Is it possible to design FCNN with -----> image !!!  input and coordinate vector output? /!/ So, the neural network I want to build should find objects on videogame footage. I'm planning to use fully convolutional neural networks as image dimensions may vary.\n\nAs input I have image taken in-game and output should be predicted vector of coordinates (forming lines and quadrilaterals) of objects that are present in picture. Is it possible do design such neural network? If so, could it be possible in real time?", "link": "https://www.reddit.com/r/MachineLearning/comments/pkw6zf/d_is_it_possible_to_design_fcnn_with_image_input/"}, {"autor": "dontiettt", "date": "2021-09-08 16:51:31", "content": "[D] Why Hasn't FOSS Drag-and-Drop ML tools taken off yet? /!/ Currently, I am looking around for modules for [Knime](https://www.knime.com/) and [Orange](https://orangedatamining.com/) and looked at some of the modules, and realized that it does not have enough tools within their tool kit (e.g. text data analysis, network analysis, -----> image !!!  classification).\n\nWhy hasn't someone bolt Keras on top of their system to supercharge it into being more powerful?", "link": "https://www.reddit.com/r/MachineLearning/comments/pkecte/d_why_hasnt_foss_draganddrop_ml_tools_taken_off/"}, {"autor": "techsavvynerd91", "date": "2021-09-08 16:29:29", "content": "[D] Is there a preexisting TensorFlow model that can parse the text of an -----> image !!!  that shows a list of ingredients? /!/ So there's a feature for my Android/iOS application where the user can take a picture of a list of ingredients from a recipe and it will parse the text where the app can read the contents of each line from the picture. Any preexisting models that I should use to accomplish this? I'm looking at the [TensforFlow Hub](https://tfhub.dev/) and I'm not sure which model to use for this.", "link": "https://www.reddit.com/r/MachineLearning/comments/pkdxs0/d_is_there_a_preexisting_tensorflow_model_that/"}, {"autor": "techsavvynerd91", "date": "2021-09-08 16:28:00", "content": "Is there a preexisting TensorFlow model that can parse the text of an -----> image !!!  that shows a list of ingredients? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pkdwry/is_there_a_preexisting_tensorflow_model_that_can/"}, {"autor": "sujitrrai", "date": "2021-09-08 15:09:06", "content": "[D] What are the concepts used in 3D face/head creation using images from consumer -----> camera !!! ? /!/  Hello guys!\n\nDoes any one has an idea of how \"AI-based 3D head generation\" works for example : [https://www.reallusion.com/character-creator/headshot/](https://www.reallusion.com/character-creator/headshot/) and [https://www.3dmorphx.com](https://www.3dmorphx.com/)\n\nCan someone please point out the concepts or exisiting research work used in above works.\n\nI am aware of the work of 3ddfav2 ([https://github.com/cleardusk/3DDFA](https://github.com/cleardusk/3DDFA)) and tried the results, but the output is not as realistic as one demonstrated in above.", "link": "https://www.reddit.com/r/MachineLearning/comments/pkcdm4/d_what_are_the_concepts_used_in_3d_facehead/"}, {"autor": "sujitrrai", "date": "2021-09-08 15:08:11", "content": "[D} What are the concepts used in 3D face/head creation using images from consumer -----> camera !!!  /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pkcczb/d_what_are_the_concepts_used_in_3d_facehead/"}, {"autor": "sujitrrai", "date": "2021-09-08 15:04:22", "content": "What are the concepts used in 3D face/head creation using -----> image !!! s from single -----> image !!! ? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pkcabm/what_are_the_concepts_used_in_3d_facehead/"}, {"autor": "inarrears", "date": "2021-09-08 14:32:29", "content": "[R] The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning /!/ New work from google brain on permutation-invariant RL agents:\n\nwebsite with interactive demos: https://attentionneuron.github.io/\n\narxiv link: https://arxiv.org/abs/2109.02869\n\n**Abstract**\n\nIn complex systems, we often observe complex global behavior emerge from a collection of agents interacting with each other in their environment, with each individual agent acting only on locally available information, without knowing the full -----> picture !!! . Such systems have inspired development of artificial intelligence algorithms in areas such as swarm optimization and cellular automata. Motivated by the emergence of collective behavior from complex cellular systems, we build systems that feed each sensory input from the environment into distinct, but identical neural networks, each with no fixed relationship with one another. We show that these sensory networks can be trained to integrate information received locally, and through communication via an attention mechanism, can collectively produce a globally coherent policy. Moreover, the system can still perform its task even if the ordering of its inputs is randomly permuted several times during an episode. These permutation invariant systems also display useful robustness and generalization properties that are broadly applicable.", "link": "https://www.reddit.com/r/MachineLearning/comments/pkboj9/r_the_sensory_neuron_as_a_transformer/"}, {"autor": "RichardRNN", "date": "2021-09-08 08:13:53", "content": "[D] Schmidhuber: The most cited neural networks all build on work done in my labs /!/ In a [tweet](https://twitter.com/SchmidhuberAI/status/1435499479306809346) and [blog post](https://people.idsia.ch/~juergen/most-cited-neural-nets.html) by the man himself, Schmidhuber writes that *the most cited neural nets all build on our work: LSTM. ResNet (open-gated Highway Net). AlexNet &amp; VGG (like our DanNet). GAN (an instance of our Artificial Curiosity). Linear Transformers (like our Fast Weight Programmers).*\n\nBlog post: https://people.idsia.ch/~juergen/most-cited-neural-nets.html\n\n**Abstract**\n\nModern Artificial Intelligence is dominated by artificial neural networks (NNs) and [deep learning](https://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html).[\\[DL1-4\\]](https://people.idsia.ch/~juergen/most-cited-neural-nets.html#DL1) Foundations of the most popular NNs originated in my labs at TU Munich and IDSIA. Here I discuss: **(1)** [Long Short-Term Memory](https://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html#Sec.%204)[\\[LSTM0-17\\]](https://people.idsia.ch/~juergen/most-cited-neural-nets.html#LSTM0) (LSTM), the most cited NN of the 20th century, **(2)**  ResNet, the most frequently cited NN of the 21st century (which is an open-gated version of our earlier [Highway Net](https://people.idsia.ch/~juergen/highway-networks.html):[\\[HW1-3\\]](https://people.idsia.ch/~juergen/most-cited-neural-nets.html#HW1)  the first working really deep feedforward NN), **(3)**  AlexNet and VGG Net, the 2nd and 3rd most frequently cited NNs of the 21st century (both building on our similar earlier [DanNet](https://people.idsia.ch/~juergen/DanNet-triggers-deep-CNN-revolution-2011.html):[\\[GPUCNN1-9\\]](https://people.idsia.ch/~juergen/most-cited-neural-nets.html#GPUCNN1)  the first deep convolutional NN[\\[CNN1-4\\]](https://people.idsia.ch/~juergen/most-cited-neural-nets.html#CNN1) to win  [-----> image !!!  recognition competitions](https://people.idsia.ch/~juergen/computer-vision-contests-won-by-gpu-cnns.html)), **(4)**  Generative Adversarial Networks[\\[GAN0-1\\]](https://people.idsia.ch/~juergen/most-cited-neural-nets.html#GAN0) (an instance of my earlier [Adversarial Artificial Curiosity](https://people.idsia.ch/~juergen/artificial-curiosity-since-1990.html#sec1)[\\[AC90-20\\]](https://people.idsia.ch/~juergen/most-cited-neural-nets.html#AC)), and **(5)** variants of Transformers (linear Transformers are formally equivalent to my earlier [Fast Weight Programmers](https://people.idsia.ch/~juergen/fast-weight-programmer-1991-transformer.html)).[\\[TR1-6\\]](https://people.idsia.ch/~juergen/most-cited-neural-nets.html#TR1)[\\[FWP0-1,6\\]](https://people.idsia.ch/~juergen/most-cited-neural-nets.html#FWP) Most of this started with our  [Annus Mirabilis of 1990-1991](https://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html)[\\[MIR\\]](https://people.idsia.ch/~juergen/most-cited-neural-nets.html#MIR) when compute was a million times more expensive than today.", "link": "https://www.reddit.com/r/MachineLearning/comments/pk69xy/d_schmidhuber_the_most_cited_neural_networks_all/"}, {"autor": "[deleted]", "date": "2021-06-18 15:32:30", "content": "text extraction from -----> image !!!  english and non english words", "link": "https://www.reddit.com/r/MachineLearning/comments/o2rt8n/text_extraction_from_image_english_and_non/"}, {"autor": "[deleted]", "date": "2021-06-18 15:28:12", "content": "Text extraction from -----> image !!!  english and non english words code link in description", "link": "https://www.reddit.com/r/MachineLearning/comments/o2rpg9/text_extraction_from_image_english_and_non/"}, {"autor": "jj4646", "date": "2021-06-18 14:46:16", "content": "[D] can someone please explain what the \"white color shades\" mean in this -----> picture !!! ? /!/ [https://martin-thoma.com/images/2016/01/ml-classifiers-2.png](https://martin-thoma.com/images/2016/01/ml-classifiers-2.png)\n\nThese pictures are supposed to show the decision boundaries of different machine learning algorithms on a binary classification task. There are two classes for the response variable: \"red\" and \"blue\".\n\nShouldn't all the decision boundaries either be fully red or fully blue? What do the shades of white mean? Does this mean \"an overlapping decision boundary\"?\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/o2qrho/d_can_someone_please_explain_what_the_white_color/"}, {"autor": "vlfom", "date": "2021-06-18 07:45:02", "content": "[D] Network science applied to images (structure/community analysis) /!/ I am working with a large -----> image !!!  dataset embedded via a CNN model with the goal of clustering the -----> image !!! s in an unsupervised manner.\n\nThe majority of works on clustering in the embedding space tend to use classical clustering methods (e.g. K-means, agglomerative), but those are often limited in the types of structures they can detect in the data. I have also looked into Deep Clustering, but it seems like there the methods assume non-overlapping or non-hierarchical structure, which is quite a limitation. I was therefore wondering if someone tried converting the image embeddings into a graph and applying some network analysis methods on top of it, e.g. community detection ones. \n\nSurprisingly, after a week or so of research, I couldn't find papers that did so (especially on a large scale). Has someone encountered a similar idea or can help me understand why are there no or just a few works on it? Would appreciate any links or hints.", "link": "https://www.reddit.com/r/MachineLearning/comments/o2jmrc/d_network_science_applied_to_images/"}, {"autor": "Cubbee_wan", "date": "2021-06-18 04:35:57", "content": "[N] AugLy: a new multimodal data augmentation lib from FB Research /!/ FB Research just released a new data augmentation library!\n\nIt supports audio, -----> image !!! , video, and text with over 100 augmentations.\n\nIt was developed with near-duplicate detection use case in mind and features unique augmentations like:\n\n&gt; one of our augmentations takes an image or video and overlays it onto a social media interface to make it look like the image or video was screenshotted by a user on a social network like Facebook and then reshared\n\n\n[Post](https://ai.facebook.com/blog/augly-a-new-data-augmentation-library-to-help-build-more-robust-ai-models/)\n\n[Code](https://github.com/facebookresearch/AugLy)\n\nYou can find docs for each domain in respective dirs README https://github.com/facebookresearch/AugLy/tree/main/augly", "link": "https://www.reddit.com/r/MachineLearning/comments/o2gpjk/n_augly_a_new_multimodal_data_augmentation_lib/"}, {"autor": "SamuraiConductor", "date": "2021-06-25 13:26:29", "content": "[D] Difference between CIDEr and CIDEr-D scores for -----> image !!!  captioning. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/o7no90/d_difference_between_cider_and_ciderd_scores_for/"}, {"autor": "Lilx20", "date": "2021-06-24 23:13:22", "content": "[D] Computer vision models for two inputs /!/ \nHello,\nI am looking a model that allows me to look for differences in a set of images that are very similar but are not the same.\n\nSay two images of an actress. In one -----> picture !!!  her eyebrows are different than the other. So the network highlights the eyebrows.\n\nDo you know of any?? Thanks !", "link": "https://www.reddit.com/r/MachineLearning/comments/o7bai8/d_computer_vision_models_for_two_inputs/"}, {"autor": "testerpce", "date": "2021-06-24 23:03:15", "content": "Possible applications of Document detection/segmentation in images/photos [D] /!/ So after you take pictures of documents or books or billboards and stuff what are the possible applications of Document or just text detection ? I am thought about OCR, Few shot Document classification,  -----> image !!!  text separation in documents etc. But I cannot think of anything else for now.\nCan you guys think of any possible applications after documents detection/segmentation in images ?\n[D]", "link": "https://www.reddit.com/r/MachineLearning/comments/o7b38d/possible_applications_of_document/"}, {"autor": "super_possum", "date": "2021-06-24 16:19:21", "content": "Feasibility of building a reliable gun detection model [Discussion] /!/ I'm trying to understand the barriers to building a computer vision model for detecting a gun in a person's hand in real time. I've played with a dozen or so repos for gun detection, and they all share the ability to detect a gun held directly in front of the -----> camera !!!  (ideally in profile), however they perform poorly when tested with a typical security -----> camera !!!  setup.\n\nIs building a reliable model to detect a \"gun in hand\" possible and how would you approach it?\n\nWould you optimize training data for a consistent camera angle and distance (ie 45 degrees and 20')? Would you include pose detection?  \nFinally, is there a framework that lends itself best for this application (ie YOLO v4)?  \n\n\n&amp;#x200B;\n\nhttps://preview.redd.it/gisld0z8o8771.png?width=1615&amp;format=png&amp;auto=webp&amp;s=11236706372b83bacab8020bb4238944e324d746", "link": "https://www.reddit.com/r/MachineLearning/comments/o748gs/feasibility_of_building_a_reliable_gun_detection/"}, {"autor": "Slight_Slide4518", "date": "2021-10-03 23:27:28", "content": "[P] Need guidance how I should solve this exercise, I am confused what numbers should I start my work the 10,000 imagine or the 2500?. can someone guide me Please! /!/ For a typical client company, roughly 10,000 images are collected each week, and about 25% of these images are actually useful for inventory tracking. Until now, -----> image !!!  classification has been a manual process that takes 20 hours of a staff associate\u2019s time to complete and an additional 2 hours of a manager\u2019s time to review the classifications and finalize deliverables for the client. Assume a manager bills at twice the hourly rate as an associate.\n\nYou\u2019ve already developed a predictive model using training data that has 75% recall and 85% precision rates in out-of-sample tests. A human associate\u2019s recall performance is typically 60% and their precision rate is 95%. Running the predictive model takes 5 hours of an associate\u2019s time for set-up and data pre-processing each week, plus the time it takes to manually review all images that are classified as \u201cuseful\u201d by the algorithm to filter out as many incorrect classifications as possible (assume the associate reviews these images at the same rate as in the manual approach). Because of the higher complexity of the predictive modeling approach, the manager must spend a total of 3 hours supporting and reviewing the associate\u2019s work.\u00a0\n\nYou must make the decision on whether or not to use your predictive model for this task and then defend that decision to your manager. You want to balance two considerations when making your recommendation: (1) minimizing the standardized hours required for this entire process, and (2) providing a high-quality classification output to the client, as measured by the model performance metric(s) you deem most important in this context.", "link": "https://www.reddit.com/r/MachineLearning/comments/q0teoi/p_need_guidance_how_i_should_solve_this_exercise/"}, {"autor": "techsucker", "date": "2021-10-03 08:29:24", "content": "[R] Baidu Research Introduces PP-LCNet: A Lightweight CPU Convolutional Neural Network With Better Accuracy And Performance /!/ Convolutional neural networks (CNNs) have been used to achieve computer vision applications for the past few years. These networks can be trained and applied in many fields, including -----> image !!!  classification, object detection, semantic segmentation.\n\nInference speed on mobile devices based ARM architecture or CPU devices based x86 architecture has been challenging to get with the increase of model feature extraction capability and model parameters. Even many good mobile networks have been proposed to resolve this issue, but the speed of these proposed networks is not good enough on the Intel CPU due to various limitations of MKLDNN.\n\nBaidu researchers have developed a lightweight CPU network based on the MKLDNN acceleration strategy, named [PP-LCNet](https://arxiv.org/pdf/2109.15099.pdf). This new system improves the performance of models in many tasks making it perfect for future artificial intelligence (AI) systems. The research group\u2019s rethink the lightweight model elements for a network designed on Intel-CPU. During the research, the research team brings up following three questions to resolve....\n\n# [3 Min Quick Read](https://www.marktechpost.com/2021/10/03/baidu-research-introduces-pp-lcnet-a-lightweight-cpu-convolutional-neural-network-with-better-accuracy-and-performance/) | [Paper](https://arxiv.org/pdf/2109.15099.pdf)| [Github PaddleClas](https://github.com/PaddlePaddle/PaddleClas)\n\n&amp;#x200B;\n\nhttps://i.redd.it/j0kn7n5k47r71.gif", "link": "https://www.reddit.com/r/MachineLearning/comments/q0ds5x/r_baidu_research_introduces_pplcnet_a_lightweight/"}, {"autor": "techsucker", "date": "2021-10-02 07:44:27", "content": "[R] Microsoft AI Unveils \u2018TrOCR\u2019, An End-To-End Transformer-Based OCR Model For Text Recognition With Pre-Trained Models /!/ The problem of text recognition is a long-standing issue in document digitalization. Many current approaches for text recognition are usually built on top of existing convolutional neural network (CNN) models for -----> image !!!  understanding and recurrent neural network (RNN) for char-level text generation. There are some latest progress records in text recognition by taking advantage of transformers, but this still needs the CNN as the backbone. Despite various successes by the current hybrid encoder/decoder methods, there is definitely some room to improve with pre-trained CV and NLP models.\n\nMicrosoft research team unveils \u2018[TrOCR](https://arxiv.org/pdf/2109.10282.pdf),\u2019 an end-to-end Transformer-based OCR model for text recognition with pre-trained computer vision (CV) and natural language processing (NLP) models. It is a simple and effective model which is that does not use CNN as the backbone. TrOCR starts with resizing the input text image into 384 \u00d7 384, and then the image is split into a sequence of 16 \u00d7 16 patches used as the input to image Transformers. The research team used standard transformer architecture with the self-attention mechanism on both encoder and decoder parts where word piece units are generated as recognized text from an input image.\n\n# [4 Min Read](https://www.marktechpost.com/2021/10/02/microsoft-ai-unveils-trocr-an-end-to-end-transformer-based-ocr-model-for-text-recognition-with-pre-trained-models/)| [Paper](https://arxiv.org/pdf/2109.10282.pdf) | [Github](https://github.com/microsoft/unilm/tree/master/trocr)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/qwyzdqfmrzq71.png?width=1308&amp;format=png&amp;auto=webp&amp;s=b7cde292a34b76e22526ed6d032def0815b62670", "link": "https://www.reddit.com/r/MachineLearning/comments/pzqruy/r_microsoft_ai_unveils_trocr_an_endtoend/"}, {"autor": "KingsmanVince", "date": "2021-08-10 07:37:32", "content": "[R] Sketech Your Own GAN /!/ [Project](https://peterwang512.github.io/GANSketching/) | [Source code](https://github.com/peterwang512/GANSketching) | [Video](https://www.youtube.com/watch?v=1smwQ-tSCjM) | [Paper](https://arxiv.org/abs/2108.02774)\n\n**Abstract**  \nCan a user create a deep generative model by sketching a single example? Traditionally, creating a GAN model has required the collection of a large-scale dataset of exemplars and specialized knowledge in deep learning. In contrast, sketching is possibly the most universally accessible way to convey a visual concept. In this work, we present a method, GAN Sketching, for rewriting GANs with one or more sketches, to make GANs training easier for novice users. In particular, we change the weights of an original GAN model according to user sketches. We encourage the model's output to match the user sketches through a cross-domain adversarial loss. Furthermore, we explore different regularization methods to preserve the original model's diversity and -----> image !!!  quality. Experiments have shown that our method can mold GANs to match shapes and poses specified by sketches while maintaining realism and diversity. Finally, we demonstrate a few applications of the resulting GAN, including latent space interpolation and image editing.", "link": "https://www.reddit.com/r/MachineLearning/comments/p1khzd/r_sketech_your_own_gan/"}, {"autor": "HUNGRYTOAN", "date": "2021-08-09 22:00:40", "content": "[D] Trying make noob-friendly tutorials for AI Algorithms /!/ As a noob to ML with a strong math background, I've been trying to learn some machine learning algorithms like as Alphazero's MCTS algorithm. I've noticed that existing tutorials are super-hard for me to learn from, and I've been thinking about how to write tutorials in a more noob-friendly fashion.  \n\n\nMost tutorials I've read ( e.g. [https://jonathan-hui.medium.com/monte-carlo-tree-search-mcts-in-alphago-zero-8a403588276a](https://jonathan-hui.medium.com/monte-carlo-tree-search-mcts-in-alphago-zero-8a403588276a)) have the issue that they're written linearly.  When I'm reading the tutorial, I often need to scroll up to re-reference old vocab, or look back at a -----> picture !!!  from the beginning. This interrupts my flow and makes it hard for me to hold my place in the text.   This makes simple-ish algorithms seem far more difficult than they are.  \n\n\nI've been trying to re-imagine what a more interactive exposition format would look like. For example, I prototyped the kind of tutorial I'd love to see: a user-controlled slideshow that interactively shows you the steps of Alphazero, complete with a worked example (see image below). Here, the user can scroll back and forth between different steps of the algorithm, and see it in action in the images. You can mouse-over the green vocabulary words to get reminded of their definition.\n\n&amp;#x200B;\n\nSo, what do you all think? How would you improve this? What bottlenecks do you run into when attempting to learn a new AI algorithm? How do you think AI tutorials should be written?  \n\n\nI'd really love to hear from people who are not fluent in machine learning, who are trying to break into the field.  Thanks!\n\n&amp;#x200B;\n\n[Prototype of an interactive Alphazero MCTS tutorial: You can click \\\\\"next\\\\\" or \\\\\"previous\\\\\" on the slideshow to see previous steps of the algorithm.](https://preview.redd.it/hyimqcdileg71.png?width=774&amp;format=png&amp;auto=webp&amp;s=7729f2fc4e75c679ffd10c180113150109b4daff)", "link": "https://www.reddit.com/r/MachineLearning/comments/p1bl4l/d_trying_make_noobfriendly_tutorials_for_ai/"}, {"autor": "HUNGRYTOAN", "date": "2021-08-09 21:49:47", "content": "[D] Re-writing AI algorithm tutorials to be more user-friendly /!/ I was reading a tutorial on Alphazero's Monte-Carlo Tree Search ([https://jonathan-hui.medium.com/monte-carlo-tree-search-mcts-in-alphago-zero-8a403588276a](https://jonathan-hui.medium.com/monte-carlo-tree-search-mcts-in-alphago-zero-8a403588276a)), and I noticed that this tutorial and so many others are very hard for me to read. I've been brainstorming some ideas for how to improve exposition around ML algorithms, and I'd love to hear your suggestions.  \n\n\nOne of the many issues I've run into is that tutorials are written linearly. As I read the text of a tutorial, There are plenty of vocabulary words that I need to re-reference, plenty of pictures I need to scroll back up to see (and then try to find my old place), and so forth. This breaks my reading flow and makes reading much harder than it needs to be.  \n\n\nI've been trying to reimagine how this exposition can be written in a more user friendly way.  See the attached -----> picture !!!  for a quick prototype. Here, the algorithm is compactly described in a slide-show with descriptive captions. Vocabulary words are highlighted in green, and the user can mouse-over them to get their definitions. I personally like this format a lot better, as it also shows me a worked example step-by-step.  \n\n\nI'd be curious to hear what folks think of this idea. How can I improve it? Are there better ways to re-write ML tutorials? Am I looking at the wrong resources to learn ML? What are your major bottlenecks in learning a new ML algorithm? I'm an ML noobie, so I'd love to hear what fellow noobie bottlenecks are.", "link": "https://www.reddit.com/r/MachineLearning/comments/p1bdd8/d_rewriting_ai_algorithm_tutorials_to_be_more/"}, {"autor": "mippie_moe", "date": "2021-08-09 17:12:08", "content": "[D] RTX A6000 vs. RTX 3090 multi-GPU convnet &amp; language model benchmarks /!/ [RTX A6000 vs RTX 3090 Deep Learning Benchmarks](https://lambdalabs.com/blog/nvidia-rtx-a6000-vs-rtx-3090-benchmarks/)\n\n# Some Highlights:\n\n**For training -----> image !!!  models (convnets) with PyTorch, a single RTX A6000 is...**\n\n* **0.92x** as fast as an RTX 3090 using 32-bit precision.\\*\n* **1.01x** faster than an RTX 3090 using mixed precision.\n\n**For training language models (transformers) with PyTorch, a single RTX A6000 is...**\n\n* **1.34x** faster than an RTX 3090 using 32-bit precision.\n* **1.34x** faster than an RTX 3090 using mixed precision.", "link": "https://www.reddit.com/r/MachineLearning/comments/p15sc0/d_rtx_a6000_vs_rtx_3090_multigpu_convnet_language/"}, {"autor": "KirillTheMunchKing", "date": "2021-08-09 14:17:32", "content": "[D] SOTA 3D Inpainting explained - 3D Photography using Context-aware Layered Depth Inpainting by Meng-Li Shih et al. in 5 minutes. This is the method that all the cool 3D VQGAN+CLIP images use /!/ *Processing video ivuqjucaccg71...*\n\n**\ud83c\udfaf At a glance:**  \nIs it possible to create 3d photos with convincing parallax effects from single RGB-D images? It is now! Check out a new 3D inpainting method proposed by Meng-Li Shih and colleagues. In short, the input -----> image !!!  is transformed into a Layered Depth Image with explicit pixel connectivity, which is used to synthesize new local color-and-depth content into the occluded regions in a spatial context-aware manner. The resulting images can be rendered with a smooth parallax effect using standard graphics engines with fewer artifacts compared to current SOTA methods.\n\n**\ud83d\ude80 Motivation:**  \n3D photos are more immersive than 2D, especially in VR. However, complex hardware setups are required to produce such images, and current methods that synthesize 3D photos from images captured with multi-lens smartphone cameras either produce gaps or distortions in the regions, occluded in the input image. Recent methods used Multi-Plane Image representation to address these issues, however they tend to produce artifacts on sloped surfaces. Instead of using rigid layers such as in Layered Depth Images (LDI), the authors explicitly store pixel connectivity and recursively apply CNN-based inpainting conditioned on spatially-adaptive context regions that are extracted from local connectivity in the LDI. The result is an algorithm for 3D photo generation without a predetermined number of depth layers.\n\nRead the [full paper digest](https://t.me/casual_gan/79) or the [blog post](https://www.casualganpapers.com/single_view_layered_depth_3d_inpainting/3D-Inpainting-explained.html) (reading time \\~5 minutes) to learn about the modified LDI, Image Preprocessing, Context and Synthesis Regions, and Context-Aware Color and Depth Inpainting.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://www.casualganpapers.com/)!\n\n*Processing img n7thpdj9ccg71...*\n\n\\[[Full Explanation Post](https://t.me/casual_gan/79) / [Blog Post](https://www.casualganpapers.com/single_view_layered_depth_3d_inpainting/3D-Inpainting-explained.html)\\] \\[[Arxiv](https://drive.google.com/file/d/17ki_YAL1k5CaHHP3pIBFWvw-ztF4CCPP/view)\\] \\[[Code](https://github.com/vt-vl-lab/3d-photo-inpainting.git)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n&gt;\\[[SimSiam](https://www.casualganpapers.com/self-supervised-contrastive-learning-siamese-networks/SimSiam-explained.html)\\]  \n&gt;  \n&gt;\\[[Real-ESRGAN](https://t.me/casual_gan/77)\\]  \n&gt;  \n&gt;\\[[SupCon](https://t.me/casual_gan/78)\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/p12cfo/d_sota_3d_inpainting_explained_3d_photography/"}, {"autor": "Elon_Muskoff", "date": "2021-08-09 05:59:58", "content": "[P] Need help /!/ I am an all around software\nEngineer with more years of experience than I would like to admit.  I am currently working on some side projects that I had on the back burner, trying to get to MVP phase to see if I can launch any of them soon.\n\nMy core skills are developing scalable cloud apps, APIs and Dynamic Browser based apps with C#, Node, Python, C++ on the back end and heavy React and core JS on the fron end.  I build performance intensive Apps for -----> Image !!!  processing, Audio, VR and DSP.\n\nI am looking for someone who can help me build some models around NLP,  Parameterized Image Generation (GANs) based on some input, as well Audio Generation/mutation.\n\nMy current plan was to go through SciKitLearn examples, and see if I can fish anytbing useful out, ad well as look at Fast.ai.  \n\nBut lately I am feeing so overwhelmed with the amount of information available that I have to painfully admit that I need help...\n\nThere are many ways I can make it worthwhile for you.  \n\na) we help each other (I can teach you web dev magic)\nb) you get verifiable experience and credit for your work\nc) if we make a dream team we co-launch\nd) I pay you something for your work\ne) some combination of the above\n\nOne thing I can guarantee you is it will be interesting and challenging!", "link": "https://www.reddit.com/r/MachineLearning/comments/p0vh5p/p_need_help/"}, {"autor": "RealSonZoo", "date": "2021-08-09 00:57:41", "content": "[D] Are there Video analysis libraries/models that can recognize and remember specific people/objects, and build up a memory or DB? /!/ Hello, let me try and explain this as best I can. I know for example there are libraries that can detect people. But is it possible to detect specific people repeatedly, as they go in and out of frame? \n\nI.e. the first time I see person1 on video -----> camera !!! , they walk in and out of the frame, then later 5 people walk into the frame, can I get an estimate of which one is person1 specifically? And then extend that to create and save new entries of people, and identify them specifically? \n\nI'm a bit behind on literature and SOTA applications, so thought I'd quickly ask before trying to build something like this myself. \n\nThanks for any info!!", "link": "https://www.reddit.com/r/MachineLearning/comments/p0qygp/d_are_there_video_analysis_librariesmodels_that/"}, {"autor": "NongHyupJoy", "date": "2021-08-11 20:43:28", "content": "[D] Recurrency and adversarial robustness /!/ I am aware that there are papers for object recognition using recurrent neural network. A good example is \"[Recurrent Models of Visual Attention](https://papers.nips.cc/paper/2014/file/09c6c3783b4a70054da74f2538ed47c6-Paper.pdf)\". However, I have never seen a work testing adversarial robustness on -----> image !!!  recognition using recurrent models. I know there are some works testing adversarial robustness of recurrent models on speech or language domain, but not on the domain of image recognition. I think image recognition models using recurrent networks migh be more robust than feed-forward plain CNNs. \n\nI wonder what you think about this. Is it already investigated by prior papers? or is it obviously not likely? If you have any reference, that would be very helpful.", "link": "https://www.reddit.com/r/MachineLearning/comments/p2lopj/d_recurrency_and_adversarial_robustness/"}, {"autor": "SgtSlime", "date": "2021-08-11 16:44:57", "content": "Are visual descriptors still used for -----> image !!!  object recognition? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/p2gvct/are_visual_descriptors_still_used_for_image/"}, {"autor": "lequangtri20", "date": "2021-08-11 15:04:00", "content": "[Research] AI Residency Program /!/ Hello everyone,\n\nI hope you guys are doing well in this uncertain time.\n\nI'm a third year CS student with some experience in -----> image !!!  processing and pytorch. Since I plan to pursue academic career, I'm currently looking for computer vision research opportunity. I do heard of AI residency program from the big names (google, facebook, apple, microsoft...). However, all applications are currently closed.\n\nCan you guys tell me roughly which period of the year are the applications opened again and how competent am I as a third year student to be accepted?\n\nCheers.", "link": "https://www.reddit.com/r/MachineLearning/comments/p2eufa/research_ai_residency_program/"}, {"autor": "vaaal88", "date": "2021-05-11 17:58:53", "content": "[P] Unity Tool for generating a 2D images of 3D objects from several viewpoints /!/ Hi guys, for my research project I have created a Unity tool that allows the creation of -----> image !!!  datasets by taking snapshots of 3D objects from a variety of viewpoints. You can change the latitude and logitude area, the distance from the object, the lightning, and some more things.\n\nSo today I have decided to clean it up, get rid of some unnecessary features, and write some documentation. Check it out and let me know what you think :)\n\n[ValerioB88/UnityML\\_DatasetGenerator (github.com)](https://github.com/ValerioB88/UnityML_DatasetGenerator)", "link": "https://www.reddit.com/r/MachineLearning/comments/na36i1/p_unity_tool_for_generating_a_2d_images_of_3d/"}, {"autor": "vaaal88", "date": "2021-05-11 17:50:18", "content": "Unity Tool to generate -----> image !!!  Dataset from 3D objects from a variety of viewpoints /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/na2zd0/unity_tool_to_generate_image_dataset_from_3d/"}, {"autor": "atyshka", "date": "2021-05-11 16:53:19", "content": "[D] Can't Reproduce Paper: What Next? /!/ For 3D object detection tasks, I've been intrigued by the recent papers focusing on processing lidar data as a range -----> image !!! , in works such as Lasernet and RangeDet. However, neither paper shares code and there are no popular 3rd party implementations, so I decided to work on one. For Lasernet, it's basically just a Resnet that includes some downsampling convolutions and upscaling transpose convolutions. The original dataset is Uber's private dataset, so I chose the Waymo dataset as a reasonable alternative. However, implementing it using the original training parameters (batch and lr schedule) I'm getting loss plateauing early and garbage results when testing on the training set. I've even corresponded with an author to get more information on parameters not mentioned with no improvement. When trying to reproduce papers without original code or data, are there any tips for reverse-engineering the results? I've ruled out any obvious bugs in the code using tensorboard graph to verify correct architecture, my best guess is either I'm missing an important detail from the original model architecture or training setup or the data is just bad.", "link": "https://www.reddit.com/r/MachineLearning/comments/na1llo/d_cant_reproduce_paper_what_next/"}, {"autor": "weezymf", "date": "2021-05-11 15:49:51", "content": "[P] MLP-Mixer-Pytorch: Pytorch reimplementation of Google's MLP-Mixer model that close to SotA using only MLP in -----> image !!!  classification task. /!/ Google's MLP-Mixer didn't use CNN and Transformer, but only using MLP, showing close performance to SotA in the image classification task.\n\nThe MLP-Mixer will soon be merged into the \\[vision transformer repo\\]([https://github.com/google-research/vision\\_transformer](https://github.com/google-research/vision_transformer)) and has been reimplemented to simplify the use of the official model with the pytorch version.\n\nTo verify reproducibility, I will be comparing the results on the cifar-10 dataset.\n\n&amp;#x200B;\n\nMLP-Mixer github: [https://github.com/jeonsworld/MLP-Mixer-Pytorch](https://github.com/jeonsworld/MLP-Mixer-Pytorch)\n\npaper: [https://arxiv.org/abs/2105.01601](https://arxiv.org/abs/2105.01601)", "link": "https://www.reddit.com/r/MachineLearning/comments/na042w/p_mlpmixerpytorch_pytorch_reimplementation_of/"}, {"autor": "rtahrima", "date": "2021-05-11 05:43:16", "content": "[R] TPM @ UAI 2021: The 4th Workshop on Tractable Probabilistic Modeling /!/   **\\*\\*\\*The 4th Workshop on Tractable Probabilistic Modeling (TPM) @ UAI 2021 (online)\\*\\*\\***\n\n[https://sites.google.com/view/tpm2021/home](https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fsites.google.com%2Fview%2Ftpm2021%2Fhome&amp;data=04%7C01%7CTahrima.Rahman%40utdallas.edu%7Ca0f0d64f35d74f94768e08d914012281%7C8d281d1d9c4d4bf7b16e032d15de9f6c%7C0%7C1%7C637562817394980207%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&amp;sdata=ijiWxxDSym7CL%2F6mXVx%2FK5k3AiqK4V6RR9iVK2P%2FQuQ%3D&amp;reserved=0)  \n \n\nThere is an increasing need for probabilistic machine learning (ML) models that are able to deliver probabilistic inference with guarantees (**reliability**) while allowing to flexibly represent complex real-world scenarios (**expressiveness**). This edition of the workshop on **tractable probabilistic models** (**TPMs**) aims at bringing together researches working on different fronts of this trade-off between reliable and expressive models in modern probabilistic ML.\n\nRecent years have shown how TPMs can achieve such a sensible trade-off in tasks like -----> image !!!  classification, completion and generation, activity recognition, language and speech modeling, bioinformatics, verification and diagnosis of physical systems, to name but a few. Examples of TPMs comprise - but are not limited to - i) ***neural autoregressive models***; ii) ***normalizing flows***; iii) ***bounded-treewidth probabilistic graphical models*** (PGMs); iv) ***determinantal point processes***; v) ***PGMs with high girth or weak potentials***; vi) ***exchangeable probabilistic models*** and models exploiting symmetries and invariances and vii) ***probabilistic circuits*** (arithmetic circuits, sum-product networks, probabilistic sentential decision diagrams, cutset networks, etc.).\n\n**Topics**\n\nWe especially encourage submissions highlighting the challenges and opportunities for tractable inference, including, but not limited to:\n\n&amp;#x200B;\n\n* ***New tractable      representations*** in      discrete, continuous and hybrid domains,\n* ***Learning algorithms*** for TPMs\n* ***Theoretical and      empirical analysis*** of tractable models\n* ***Connections*** between TPM      classes\n* TPMs for ***responsible,      robust and explainable AI***\n* ***Retrospective works***, tutorials, and surveys\n* ***Approximate inference*** algorithms with      guarantees\n* ***Tractable neuro-symbolic*** and/or relational      modeling\n* ***Applications*** *of tractable      probabilistic modeling*\n\n**Submission Instructions**\n\nWe invite three types of submissions:\n\n&amp;#x200B;\n\n* **Original research      papers:** advances      in TPM, not previously published in an archival conference or journal.\n* **Recently published      research papers*****:*** advances in TPM, already published at a recent      venue.\n* **Position papers (abstracts):** discussing tendencies,      issues or future venues of interest for the TPM community.\n\nAll submissions must be electronic (through the link below), and must closely follow the formatting guidelines at [https://sites.google.com/view/tpm2021/call-for-papers](https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fsites.google.com%2Fview%2Ftpm2021%2Fcall-for-papers&amp;data=04%7C01%7CTahrima.Rahman%40utdallas.edu%7Ca0f0d64f35d74f94768e08d914012281%7C8d281d1d9c4d4bf7b16e032d15de9f6c%7C0%7C1%7C637562817394990203%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&amp;sdata=2nAnhwtqTAn0Q0etKwJ3Y4La%2BhAXnNt2%2B0y7qp2n3og%3D&amp;reserved=0). Reviewing for TPM 2021 is ***single-blind***. We recommend that you refer to your prior work in the third person wherever possible. We also encourage links to public repositories such as github to share code and/or data.\n\n**Submission Link:** [https://openreview.net/group?id=auai.org/UAI/2021/Workshop/TPM](https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fopenreview.net%2Fgroup%3Fid%3Dauai.org%2FUAI%2F2021%2FWorkshop%2FTPM&amp;data=04%7C01%7CTahrima.Rahman%40utdallas.edu%7Ca0f0d64f35d74f94768e08d914012281%7C8d281d1d9c4d4bf7b16e032d15de9f6c%7C0%7C1%7C637562817395000194%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&amp;sdata=8XkKDdQ7cpZdbeXrcwgnV0%2B6b%2FlHApKcgJPz7G1WthY%3D&amp;reserved=0)\n\n**\\*\\*\\*Accepted papers will be considered for the best paper award\\*\\*\\***\n\n**Important Dates**\n\n\u00a7 **Paper submission deadline:** May 28, 2021 AOE (UTC-12:00h)\n\n\u00a7 **Notification to authors:** June 28, 2021\n\n\u00a7 **Camera-ready version:** July 27, 2021 AOE (UTC-12:00h) \\*\n\n\u00a7 **Workshop Date:** July 30, 2021\n\n**Organizers**\n\nAntonio Vergari (University of California, Los Angeles)\n\nTahrima Rahman (University of Texas, Dallas)\n\nRobert Peharz (TU Eindhoven)\n\nAlejandro Molina (TU Darmstadt)\n\nPedram Rooshenas (University of North Carolina, Charlotte)\n\nDaniel Lowd  (University of Oregon)\n\nZoubin Ghahramani (Google AI) \n\n*For any questions, contact us at* [***tpmworkshop2021@gmail.com***](mailto:tpmworkshop2021@gmail.com)\n\n**\\*\\*\\*Please consider sharing this CFP in your network\\*\\*\\***", "link": "https://www.reddit.com/r/MachineLearning/comments/n9pxln/r_tpm_uai_2021_the_4th_workshop_on_tractable/"}, {"autor": "naregkh", "date": "2021-02-14 17:59:24", "content": "Looking for a model that can run an OCR on -----> image !!!  and then train it to label certain content /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ljth6k/looking_for_a_model_that_can_run_an_ocr_on_image/"}, {"autor": "throwaway20938029438", "date": "2021-02-14 15:26:52", "content": "Fractal-based -----> image !!!  GAN? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ljqd3v/fractalbased_image_gan/"}, {"autor": "gamechanger4r", "date": "2021-02-14 08:34:27", "content": "[D] All time spent on preprocessing! /!/ Hi,  I'm training a custom model for classification on imagenet. I'm using  kaggle notebooks with GPU, and pytorch. I'm not using the torchvision  library, I'm trying to do it by hand (because I'm going to deal with  datasets which are not included in torchvision pretty soon).\n\nThat  being said, I have a problem with imagenet. I'm spending 99% of time on  preprocessing. What does this preprocessing consist of? Loading -----> image !!!   by -----> image !!!  from the disk, converting to numpy array, concatenating,  converting to float (1/255) and normalizing. Probably the most expensive  step is reading the images from disk one by one.\n\nI  have thought about saving the torch tensor itself, after the  preprocessing (so that I don't have to read image by image, but rather a  4d tensor of the whole batch as a single file). But, after converting  the image to a numpy or torch tensor, it gets 10 times bigger in size,  so that saving the preprocessed data this way becomes unpractical.\n\nJust to be clear: I have to do this preprocessing every batch, since obviously the dataset doesn't fit in disk.\n\nWhat  can I do to save cpu time in the actual training? For me reading image  file by image file during training sounds absurd, but I know of no other  way.", "link": "https://www.reddit.com/r/MachineLearning/comments/ljkrqn/d_all_time_spent_on_preprocessing/"}, {"autor": "rirhun", "date": "2021-02-14 07:01:00", "content": "[D] How to deploy a BentoML model using Seldon Core? /!/ In the BentoML documentation, they suggest that it's possible to use BentoML as a replacement for Seldon's pre-built containers, but they don't really go into details on how to do this. \n\nI am using a simple iris model and containerized it using BentoML: \n\n    bentoml containerize IrisClassifier:latest -t iris_classifier:0.0.1\n    docker tag iris_classifier:0.0.1 gcr.io/project_id/iris_classifier:0.0.1\n    docker push gcr.io/project_id/iris_classifier:0.0.1\n\nThen I define my Seldon Core CRD as follows: \n\n    apiVersion: machinelearning.seldon.io/v1alpha2\n    kind: SeldonDeployment\n    metadata:\n      name: iris-model-bentoml\n      namespace: seldon\n    spec:\n      name: iris-model-bentoml\n      predictors:\n        - componentSpecs:\n            - spec:\n                containers:\n                  - name: classifier\n                    -----> image !!! : gcr.io/dao-aa-poc-uyim/iris_classifier:0.0.1\n                    volumeMounts:\n                      - mountPath: /bentoml\n                        name: bentoml-volume\n                volumes:\n                  - name: bentoml-volume\n                    emptyDir: {}\n          graph:\n            children: []\n            endpoint:\n              type: REST\n            name: classifier\n            type: MODEL\n            logger:\n              mode: all\n          name: example\n          replicas: 1\n\nUnfortunately, I get a `CrashLoopBackOff` error. It seems like the container `seldon-container-engine` is constantly failing to connect to localhost:9000\n\nI'm assuming the other container `classifier` is the one that should be listening on that port but I can't see that in the logs. It only seems to be listening on port 5000.\n\nSo I guess that is part of the issue - there should be something listening on 9000 but there isn't. Some guidance would be greatly appreciated!", "link": "https://www.reddit.com/r/MachineLearning/comments/ljjl2i/d_how_to_deploy_a_bentoml_model_using_seldon_core/"}, {"autor": "KosvvSt", "date": "2021-02-13 16:13:44", "content": "[D] Visual Studio, 2 ML Models in the same project? /!/  I don't know if this goes here but, I have a question. How could I have two machine learning models in the same project (c#, visual studio 2019) ? I want to have text and -----> image !!!  classification for something in my project but I am not sure how to have 2 models in the same project as if I try to add a new second model using the ML.NET Model Builder it wants me to replace the old one.", "link": "https://www.reddit.com/r/MachineLearning/comments/lj3eit/d_visual_studio_2_ml_models_in_the_same_project/"}, {"autor": "PaganPasta", "date": "2021-08-06 09:51:44", "content": "[D] Training vision transformers on a specific dataset from scratch /!/ Hi,\n\nI wanted to know if it's feasible to only train transformer based architectures only on a specific dataset from scratch. For example, on CIFARs.\n\nI understand that transformers are \"data hungry\" but are there any architectures which try to bridge the performance gap with traditional convolutional nets on -----> image !!!  classification tasks ?", "link": "https://www.reddit.com/r/MachineLearning/comments/oz3iwy/d_training_vision_transformers_on_a_specific/"}, {"autor": "KingsmanVince", "date": "2021-08-06 09:15:29", "content": "[R] Multimodal Co-learning: Challenges, Applications with Datasets, Recent Advances and Future Directions /!/ [Arxiv link](https://arxiv.org/abs/2107.13782)\n\nMultimodal deep learning systems which employ multiple modalities like text, -----> image !!! , audio, video, etc., are showing better performance in comparison with individual modalities (i.e., unimodal) systems. Multimodal machine learning involves multiple aspects: representation, translation, alignment, fusion, and co-learning. In the current state of multimodal machine learning, the assumptions are that all modalities are present, aligned, and noiseless during training and testing time. However, in real-world tasks, typically, it is observed that one or more modalities are missing, noisy, lacking annotated data, have unreliable labels, and are scarce in training or testing and or both. This challenge is addressed by a learning paradigm called multimodal co-learning. The modeling of a (resource-poor) modality is aided by exploiting knowledge from another (resource-rich) modality using transfer of knowledge between modalities, including their representations and predictive models. Co-learning being an emerging area, there are no dedicated reviews explicitly focusing on all challenges addressed by co-learning. To that end, in this work, we provide a comprehensive survey on the emerging area of multimodal co-learning that has not been explored in its entirety yet. We review implementations that overcome one or more co-learning challenges without explicitly considering them as co-learning challenges. We present the comprehensive taxonomy of multimodal co-learning based on the challenges addressed by co-learning and associated implementations. The various techniques employed to include the latest ones are reviewed along with some of the applications and datasets. Our final goal is to discuss challenges and perspectives along with the important ideas and directions for future work that we hope to be beneficial for the entire research community focusing on this exciting domain.", "link": "https://www.reddit.com/r/MachineLearning/comments/oz33sw/r_multimodal_colearning_challenges_applications/"}, {"autor": "expectation_max", "date": "2021-08-05 21:55:56", "content": "[D] Looking for works about GANs conditioned on segmentation maps /!/ Hello,\n\nI  am looking for works that explore -----> image !!!  generation (with GANs) that are  conditioned on segmentation maps for the target -----> image !!! . I have looked a bit and am aware of cycleGAN and similar papers but that is not quite what I am looking for.  I was wondering if someone could point me in the right direction.\n\nThank you!", "link": "https://www.reddit.com/r/MachineLearning/comments/oystqc/d_looking_for_works_about_gans_conditioned_on/"}, {"autor": "expectation_max", "date": "2021-08-05 21:54:44", "content": "Looking for works about GANs conditioned on segmentation maps /!/ Hello,\n\nI am looking for works that explore -----> image !!!  generation (with GANs) that are conditioned on segmentation maps for the target -----> image !!! . I am aware of cycleGAN and similar papers but that is not quite what I am looking for. I was wondering if someone could point me in the right direction.\n\nThank you!", "link": "https://www.reddit.com/r/MachineLearning/comments/oysswl/looking_for_works_about_gans_conditioned_on/"}, {"autor": "expectation_max", "date": "2021-08-05 21:52:46", "content": "Any works on GAS conditioned on segmentation maps? /!/ Hello,\n\nI am looking for works that explore -----> image !!!  generation (with GANs) that are conditioned on segmentation maps for the target -----> image !!! . I am aware of cycleGAN but that is not quite what I am looking for. I was wondering if someone could point me in the right direction.  \n\n\nThank you!", "link": "https://www.reddit.com/r/MachineLearning/comments/oysrlx/any_works_on_gas_conditioned_on_segmentation_maps/"}, {"autor": "Arzeg94", "date": "2021-08-05 14:25:10", "content": "Convert Face into a Cartoon(Emoji) Face [R][P] /!/ Hi guys,\n\nI am currently working on my bachelor thesis. I'm taking a -----> photo !!!  of a face, transforming it into a cartoon face with an AI, and then drawing it with a robotic arm. I think I have \\~90% of this project done. I found a great GitHub repo ([this](https://github.com/mnicnc404/CartoonGan-tensorflow) one, anyone interested). But the repo \"cartoonify\" the image/face. I thought that was enough, but no. I use a contour detection function from OpenCV (to get my coordinates for the robot arm) after the transformation. But the contour detection loses all or around 98% of the cartoonify image, because you don't see the blur and other things that make it a cartoon face. So I need to create a cartoon(emoji) face like [this](https://developer.nvidia.com/blog/inception-spotlight-ai-app-chudo-can-create-a-cartoon-emoji-of-yourself/). Is there a repo for self-implementation or a pre-trained model that I could use? I am currently trying to train my own dataset with human faces (\\~7000 images) and Disney faces (\\~300 images). I bought Google Colab Pro for training and better hardware, but it takes so darn long....\n\n&amp;#x200B;\n\nTraining in Google Colab:\n\n* Use TPU\n* Train with \\~7000 human faces and \\~300 Disney faces.\n* 100 epochs\n* \\~8h per epoch \n\nSo the total training will take around 800 hours. This can't be real\n\nI have tested the app and have great results with contour recognition and drawing, but I need a custom implementation and can't use the app.\n\n&amp;#x200B;\n\nI appreciate any help and advice, thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/oyj8d5/convert_face_into_a_cartoonemoji_face_rp/"}, {"autor": "austin_kodra", "date": "2021-08-05 14:12:02", "content": "[P] Public-facing AI art gallery: CLIPDraw implementation using Gradio + Comet /!/ Hi everyone! Wanted to share a project our team has been working on over at Comet (disclaimer, I'm the Head of Community over there). Also wanted to share the motivation and a couple links to learn more about it.\n\nTL;DR, a public-facing AI art gallery (text prompt --&gt; -----> image !!!  using CLIPDraw) open to community submissions:\n\n* [Blog post explaining the project](https://www.comet.ml/site/clipdraw-gallery-ai-art-powered-by-comet-and-gradio/?utm_campaign=clipdraw-gradio&amp;utm_source=reddit)\n* [Gradio interface for community submissions](https://clipdraw.comet.ml/)\n* [Public Comet project](https://www.comet.ml/team-comet-ml/clipdraw/view/Y4aT3gy6IrPQKBi5wncFXCYLR?utm_campaign=clipdraw-gradio&amp;utm_source=reddit)\n* [Colab notebook to run your own implementation (requires a free Comet account)](https://colab.research.google.com/drive/16nR8_Ida5tID1RSzLy1Fv_LLav3MF1Zt?utm_campaign=clipdraw-gradio&amp;utm_source=reddit)\n\n\\_\\_\\_\\_\n\nAs you all are likely aware, the momentum behind these large, pre-trained Transformer models (GPT-3 and CLIP, for example), has led to a bunch of new projects centered on \"Prompt Engineering\" (ML Twitter is [full of examples](https://twitter.com/search?q=%23VQGAN&amp;lang=en) of people experimenting with different types of prompts and sharing their results).\n\nThese large models are capable of producing all sorts of fantastic outputs; and the key to making them work is using the right set of instructions/prompts as input. One issue we identified is that since this knowledge is scattered, there's really no way for other people to learn and build from past approaches.\n\nSo we decided we wanted to build something public facing to help keep track of what the community is trying and creating.\n\nSpecifically, we\u2019ve created a public Comet Project to track generative art created with the [CLIPDraw model](https://kvfrans.com/clipdraw-exploring-text-to-drawing-synthesis/), along with a [Gradio](http://gradio.app/) UI to interact with the model on our community server.\n\nBelow I've shared a GIF that shows one of the cool things you can do with this -- visualize the evolution of a given training run across steps, so you can basically see how the model created your art! In Comet, you can also compare/diff experiment runs to see how different input parameters affect the output. \n\nHope the community enjoys this project, and if you come up with any outputs that you really like, let us know in the comments or tag us on Twitter ([@Cometml](https://twitter.com/Cometml))", "link": "https://www.reddit.com/r/MachineLearning/comments/oyijcm/p_publicfacing_ai_art_gallery_clipdraw/"}, {"autor": "austin_kodra", "date": "2021-08-05 14:09:33", "content": "[P] Public AI Art Gallery: CLIPDraw implementation using Gradio + Comet /!/ Hi everyone! Wanted to share a project our team has been working on over at Comet (disclaimer, I'm the Head of Community over there). Also wanted to share the motivation and a couple links to learn more about it.\n\nTL;DR, a public-facing AI art gallery (text prompt --&gt; -----> image !!!  using CLIPDraw) open to community submissions:\n\n* [Blog post explaining the project](https://www.comet.ml/site/clipdraw-gallery-ai-art-powered-by-comet-and-gradio/?utm_campaign=clipdraw-gradio&amp;utm_source=reddit)\n* [Gradio interface for community submissions](https://clipdraw.comet.ml/)\n* [Public Comet project](https://www.comet.ml/team-comet-ml/clipdraw/view/Y4aT3gy6IrPQKBi5wncFXCYLR?utm_campaign=clipdraw-gradio&amp;utm_source=reddit)\n* [Colab notebook to run your own implementation (requires a free Comet account)](https://colab.research.google.com/drive/16nR8_Ida5tID1RSzLy1Fv_LLav3MF1Zt?utm_campaign=clipdraw-gradio&amp;utm_source=reddit)\n\n\\_\\_\\_\\_\n\nAs you all are likely aware, the momentum behind these large, pre-trained Transformer models (GPT-3 and CLIP, for example), has led to a bunch of new projects centered on \"Prompt Engineering\" (ML Twitter is [full of examples](https://twitter.com/search?q=%23VQGAN&amp;lang=en) of people experimenting with different types of prompts and sharing their results).\n\nThese large models are capable of producing all sorts of fantastic outputs; and the key to making them work is using the right set of instructions/prompts as input. One issue we identified is that since this knowledge is scattered, there's really no way for other people to learn and build from past approaches.\n\nSo we decided we wanted to build something public facing to help keep track of what the community is trying and creating.\n\nSpecifically, we\u2019ve created a public Comet Project to track generative art created with the [CLIPDraw model](https://kvfrans.com/clipdraw-exploring-text-to-drawing-synthesis/), along with a [Gradio](http://gradio.app/) UI to interact with the model on our community server.\n\nBelow I've shared a GIF that shows one of the cool things you can do with this -- visualize the evolution of a given training run across steps, so you can basically see how the model created your art! In Comet, you can also compare/diff experiment runs to see how different input parameters affect the output. \n\nHope the community enjoys this project, and if you come up with any outputs that you really like, let us know in the comments or tag us on Twitter ([@Cometml](https://twitter.com/Cometml))\n\n&amp;#x200B;\n\n*Processing gif 2s0wqjvgqjf71...*", "link": "https://www.reddit.com/r/MachineLearning/comments/oyiff2/p_public_ai_art_gallery_clipdraw_implementation/"}, {"autor": "ElsaLab", "date": "2021-08-05 04:19:39", "content": "[R] [IJCAI 2018] Virtual-to-Real: Learning to Control in Visual Semantic Segmentation /!/ We proposed to separate the model into a perception module and a control policy module, and introduced the concept of using semantic -----> image !!!  segmentation as the meta state for relating these two modules in order to transfer policies learned in simulators to the real world.\n\nAdvanced detail please visit: [https://reurl.cc/1YAkRY](https://reurl.cc/1YAkRY?fbclid=IwAR1-uJDSfG0TD7B6fzMVkaJkC58GKolYyYNkkqiQ44kPuiHrQgbJTWwFRoU)\n\nArXiv: [https://reurl.cc/GmyG3p](https://reurl.cc/GmyG3p?fbclid=IwAR0YhGF_hdTAiUBPcrAkTlqq18P4v7m6m7vzSfg1EX_iK46wonolFOGVN8I)\n\nIJCAI: [https://reurl.cc/9rNbKj](https://reurl.cc/9rNbKj?fbclid=IwAR3Kq34pbt8HXNjK-sKSUl1JJK27abKzxyu7oPknxlyh9vE0wl3isPdlNew)\n\nVideo Link: [https://reurl.cc/bX8NQy](https://reurl.cc/bX8NQy?fbclid=IwAR13g87BgpUV8sZQdxOYM9jiHUpQDhMqwoxddSyE0nQ9G83Son4tJykGXDU)\n\nELSA Lab is a research laboratory focusing on Deep Reinforcement Learning, Intelligent Robotics, and Computer Vision. Please visit our website: [https://elsalab.ai/](https://elsalab.ai/?fbclid=IwAR14JduAMHcq30fyHGqG_yzHXK0D2DW71ihPJ8QXiQY65x0Ht969AqmyTA0)\n\n[\\#ArtificialIntelligence](https://www.facebook.com/hashtag/artificialintelligence?__eep__=6&amp;__cft__[0]=AZXS_cNmV4F0izoZzOioTCFiD3OhlLmIdw1xzBl6eYWKQlXI0ixSV4PoG2XpfnajadtbjGeIius2r3wAYUNBz7B8HB-VphnwYBLRI2EqfsVFSyMjSatxuTevHIiHtYlsxLeJu2vAer047qCA3SaJUJI7&amp;__tn__=*NK-R) [\\#ReinforcementLearning](https://www.facebook.com/hashtag/reinforcementlearning?__eep__=6&amp;__cft__[0]=AZXS_cNmV4F0izoZzOioTCFiD3OhlLmIdw1xzBl6eYWKQlXI0ixSV4PoG2XpfnajadtbjGeIius2r3wAYUNBz7B8HB-VphnwYBLRI2EqfsVFSyMjSatxuTevHIiHtYlsxLeJu2vAer047qCA3SaJUJI7&amp;__tn__=*NK-R) [\\#MachineLearning](https://www.facebook.com/hashtag/machinelearning?__eep__=6&amp;__cft__[0]=AZXS_cNmV4F0izoZzOioTCFiD3OhlLmIdw1xzBl6eYWKQlXI0ixSV4PoG2XpfnajadtbjGeIius2r3wAYUNBz7B8HB-VphnwYBLRI2EqfsVFSyMjSatxuTevHIiHtYlsxLeJu2vAer047qCA3SaJUJI7&amp;__tn__=*NK-R) [\\#DeepLearning](https://www.facebook.com/hashtag/deeplearning?__eep__=6&amp;__cft__[0]=AZXS_cNmV4F0izoZzOioTCFiD3OhlLmIdw1xzBl6eYWKQlXI0ixSV4PoG2XpfnajadtbjGeIius2r3wAYUNBz7B8HB-VphnwYBLRI2EqfsVFSyMjSatxuTevHIiHtYlsxLeJu2vAer047qCA3SaJUJI7&amp;__tn__=*NK-R) [\\#SemanticSegmentation](https://www.facebook.com/hashtag/semanticsegmentation?__eep__=6&amp;__cft__[0]=AZXS_cNmV4F0izoZzOioTCFiD3OhlLmIdw1xzBl6eYWKQlXI0ixSV4PoG2XpfnajadtbjGeIius2r3wAYUNBz7B8HB-VphnwYBLRI2EqfsVFSyMjSatxuTevHIiHtYlsxLeJu2vAer047qCA3SaJUJI7&amp;__tn__=*NK-R)", "link": "https://www.reddit.com/r/MachineLearning/comments/oy9yz6/r_ijcai_2018_virtualtoreal_learning_to_control_in/"}, {"autor": "Sea-Put-9356", "date": "2021-08-04 21:51:39", "content": "Varying size of Feature Vector from HoG [D] /!/ I am trying to use HoG for -----> image !!!  classification. The sizes of the images that I have are not constant so every time I calculate HoG, the feature vector size is not the same even and so they cannot be concatenated to form a feature vector foe classification even though the HoG parameters are the same for every image.\nHow should I resolve this ?", "link": "https://www.reddit.com/r/MachineLearning/comments/oy3a39/varying_size_of_feature_vector_from_hog_d/"}, {"autor": "Megixist", "date": "2021-10-06 04:15:38", "content": "[D] The second part of the guide to optimization of complex variables is out! /!/ This part of the two-article series featured on Weights and Biases explores the convergence constraints for complex-valued neural networks while distinguishing the optimization factors and circularity constraints for strictly linear and widely linear networks. The report delves into the significance of phase, the impact of weight initialization methods, and the holomorphicity requirement for activations while demonstrating -----> image !!!  denoising using custom complex-valued convolutions with Tensorflow.\n\nThe principal objective of this series of articles is to display the versatility of complex representations and to encourage their use for breakthrough research.\n\nLink: [https://wandb.ai/darshandeshpande/complex-optimization/reports/The-Reality-Behind-Optimization-of-Imaginary-Variables-II--Vmlldzo5OTM5NTA](https://wandb.ai/darshandeshpande/complex-optimization/reports/The-Reality-Behind-Optimization-of-Imaginary-Variables-II--Vmlldzo5OTM5NTA)\n\nAny questions or comments are most welcome! :)", "link": "https://www.reddit.com/r/MachineLearning/comments/q2cvr3/d_the_second_part_of_the_guide_to_optimization_of/"}, {"autor": "Wiskkey", "date": "2021-10-05 23:26:30", "content": "[P] New version of CogView (text-to------> image !!! ) is available in online demo /!/ [Online demo of new version of CogView](https://agc.platform.baai.ac.cn/CogView/index.html).\n\n[Examples #1](https://www.reddit.com/r/bigsleep/comments/q238w2/fox_at_night_2_images_made_using_the_new_cogview/). [Examples #2](https://www.reddit.com/r/bigsleep/comments/q25rj5/beautiful_woman_4_images_using_the_new_cogview/).\n\n[GitHub repo for older version of CogView](https://github.com/THUDM/CogView). [GitHub mention of new version](https://github.com/THUDM/CogView/issues/34).\n\n[18 examples using older version of CogView](https://www.reddit.com/r/bigsleep/comments/oq5nwh/18_images_generated_by_free_cogview_website_using/).", "link": "https://www.reddit.com/r/MachineLearning/comments/q28a27/p_new_version_of_cogview_texttoimage_is_available/"}, {"autor": "petitponeyrose", "date": "2021-10-05 22:27:33", "content": "[D] Feeling overwhelmed because applying machine learning to real life problems is not trivial /!/ Hello, I'm a young engineer and I was hired in a small company to automate some tasks using -----> image !!!  recognition and machine learning. \n\nEven though the technology is not new, since I need to adapt it, I face a lot of challenges (gathering the right data, adapting the neural network, solving some other problems that neural nets can't solve on their own with traditional machine learning...) The current results I get are not good enough and I have to admit I feel overwhelmed.\n\nIs it normal? How is you experience? \n\nHave a nice day.", "link": "https://www.reddit.com/r/MachineLearning/comments/q277u6/d_feeling_overwhelmed_because_applying_machine/"}, {"autor": "alexparinov", "date": "2021-10-05 17:25:08", "content": "[P] Albumentations 1.1 is released (a Python library for -----> image !!!  augmentation) /!/ The new release of a fast and flexible library for image augmentation includes:\n\n# New augmentations\n\n* **TemplateTransform** allows the blending of an input image with specified templates.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/9qzcv15h1or71.png?width=1500&amp;format=png&amp;auto=webp&amp;s=c460dc6a0d5ef147306cc6596c5ab5ad8e416083\n\n* **PixelDistributionAdaptation**. A domain adaptation augmentation. An example of this augmentation is available on [GitHub](https://github.com/arsenyinfo/qudida#example).\n\n# Improvements and bug fixes\n\nFour augmentations got new parameters for better tunning. ElasticTransform got a performance boost. Also, we fixed bugs for augmentations that work with bounding boxes and keypoints.\n\n# Release notes\n\nFull release notes are available at [https://github.com/albumentations-team/albumentations/releases/tag/1.1.0](https://github.com/albumentations-team/albumentations/releases/tag/1.1.0)\n\n# Installation\n\nAs always, you can install the latest version of the library by running:\n\n    pip install -U albumentations", "link": "https://www.reddit.com/r/MachineLearning/comments/q2176g/p_albumentations_11_is_released_a_python_library/"}, {"autor": "KirillTheMunchKing", "date": "2021-10-05 15:44:21", "content": "[D] SOTA GAN-based -----> Image !!!  Editing - ISF-GAN: An Implicit Style Function for High-Resolution -----> Image !!! -to------> Image !!!  Translation (5-minute explanation) /!/ I   often find myself wishing I knew how to edit images in photoshop but I    remember that I already have a full-time job without attempting to learn photoshop. This is where ISF-GAN by Yahui Liu et al. comes in.    This new model performs cost-effective multi-modal unsupervised image-to-image translations at high resolution using pre-trained unconditional GANs. ISF-GAN does this by modeling the latent style vector update with an MLP conditioned on a random vector and an attribute code.\n\nCheck out the [full paper summary](https://www.casualganpapers.com/unsupervised_latent_space_exploration_image_to_image_gan_editing/ISF-GAN-explained.html) on Casual GAN Papers (Reading time \\~5 minutes).\n\n\\[[arxiv](https://arxiv.org/pdf/2109.12492.pdf)\\]\\[[github](https://github.com/yhlleo/stylegan-mmuit)\\]\n\n[ISF-GAN](https://preview.redd.it/d35r1duwjnr71.png?width=1436&amp;format=png&amp;auto=webp&amp;s=99beaf2e2e11f1d447b62a3ddcb4cf9273fe2693)\n\nSubscribe to [my channel](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!", "link": "https://www.reddit.com/r/MachineLearning/comments/q1z4hg/d_sota_ganbased_image_editing_isfgan_an_implicit/"}, {"autor": "Accomplished-Ad-2156", "date": "2021-10-05 13:29:51", "content": "[R] DOLG: Single-Stage Image Retrieval with Deep Orthogonal Fusion of Local and Global Features /!/ Local information with multi-atrous convolutions and self-attention with orthogonal components concatenated and aggregated with the global representation to generate the final representation. \n\nThe paper shows state-of-the-art -----> image !!!  retrieval performance on the Revisited Oxford and Paris datasets.\n\nPaper Link : [https://arxiv.org/abs/2108.02927](https://arxiv.org/abs/2108.02927)\n\nUnofficial Code : [https://github.com/dongkyuk/DOLG-pytorch](https://github.com/dongkyuk/DOLG-pytorch)", "link": "https://www.reddit.com/r/MachineLearning/comments/q1wf44/r_dolg_singlestage_image_retrieval_with_deep/"}, {"autor": "yeoxd09", "date": "2021-10-10 04:39:46", "content": "[P] I need help with computer vision /!/ Hello, I would like to know if there is a way that I can use a trained object detection model using Custom Vision, in real time with a streaming -----> camera !!! . I am trying to make a model for the classification of fruits and their maturity.", "link": "https://www.reddit.com/r/MachineLearning/comments/q518ay/p_i_need_help_with_computer_vision/"}, {"autor": "techsucker", "date": "2021-10-09 15:04:12", "content": "[R] Microsoft Researchers Introduce \u2018Mesh Graphormer\u2019, A Graph-Convolution-Reinforced Transformer (Codes Released) /!/ While 3D human pose and mesh reconstruction from a single -----> image !!!  is a trending area of research because of its applications for human-computer interactions, it is also a very challenging process due to the complex body articulation.\n\nThe current progress tools include transformers and graph convolutional neural networks in human mesh reconstruction. Transformer-based approaches are more effective in modeling nonlocal interactions among 3D mesh vertices and body joints. In contrast, GCNNs are good at exploiting neighborhood vertex interactions based on a pre-specified mesh topology.\n\nResearchers from Microsoft introduced a graph-convolution-reinforced transformer, named [Mesh Graphormer](https://arxiv.org/pdf/2104.00272.pdf), for reconstructing human pose and mesh from a single image. The researchers inject graph convolutions into transformer blocks to improve the local interactions among neighboring vertices and joints. The proposed Graphormer is free to join and attend all image grid features to leverage the strength of graph convolutions. Therefore, Graphormer and image grid features are utilized and enforced to improve human pose and mesh reconstruction performance.\n\n# [4 Min Read](https://www.marktechpost.com/2021/10/09/microsoft-researchers-introduce-mesh-graphormer-a-graph-convolution-reinforced-transformer/) | [Paper](https://arxiv.org/pdf/2104.00272.pdf) | [Code](https://github.com/microsoft/MeshGraphormer)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/c4d654lgwfs71.png?width=1392&amp;format=png&amp;auto=webp&amp;s=0b80e22889e210af5d566b21248d8e9abf8f0545", "link": "https://www.reddit.com/r/MachineLearning/comments/q4n2u1/r_microsoft_researchers_introduce_mesh_graphormer/"}, {"autor": "photogrammetery", "date": "2021-10-09 03:35:56", "content": "Tried to put 'First Person TF2' into an AI text-to------> image !!!  generator, ended up stranger than i'd have assumed.", "link": "https://www.reddit.com/r/MachineLearning/comments/q4ddru/tried_to_put_first_person_tf2_into_an_ai/"}, {"autor": "AsianStudiesRecords", "date": "2021-10-08 21:20:08", "content": "[D] Are there any deep learning projects that convert color gamuts? /!/ As a digital artist it is very frustrating converting RGB to CMYK. I figure this can be solved with AI. Feel free to steal this idea: it would be a good business decision to make an -----> image !!!  uploader and then put ads next to it. Thank you", "link": "https://www.reddit.com/r/MachineLearning/comments/q4741r/d_are_there_any_deep_learning_projects_that/"}, {"autor": "arthomas73", "date": "2021-10-08 16:33:39", "content": "[P] Improving accuracy through data curation for -----> image !!!  classification", "link": "https://www.reddit.com/r/MachineLearning/comments/q41d7p/p_improving_accuracy_through_data_curation_for/"}, {"autor": "arthomas73", "date": "2021-10-08 16:29:24", "content": "Improving accuracy through data curation for -----> image !!!  classification /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/q41a2k/improving_accuracy_through_data_curation_for/"}, {"autor": "tah_zem", "date": "2021-10-08 14:12:57", "content": "[D] Did you use AI engines in the cloud? Your feedback? /!/ Hi there!\n\nHave you, if you're a developer, ever used and integrated AI engines in the cloud? By that I mean APIs offered by companies to process your data (-----> image !!!  recognition, machine translation, text mining, etc.).\n\nIf so, in what context and what is your feedback? What are the problems you have encountered?\n\nThanks,\n\nTaha", "link": "https://www.reddit.com/r/MachineLearning/comments/q3ykw0/d_did_you_use_ai_engines_in_the_cloud_your/"}, {"autor": "Yuqing7", "date": "2021-10-08 13:47:36", "content": "[R] Apple Study Reveals the Learned Visual Representation Similarities and Dissimilarities Between Self-Supervised and Supervised Methods /!/ An Apple research team performs a comparative analysis on a contrastive self-supervised learning (SSL) algorithm (SimCLR) and a supervised learning (SL) approach for simple -----> image !!!  data in a common architecture, shedding light on the similarities and dissimilarities in their learned visual representation patterns. \n\nHere is a quick read: [Apple Study Reveals the Learned Visual Representation Similarities and Dissimilarities Between Self-Supervised and Supervised Methods.](https://syncedreview.com/2021/10/08/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-120/)\n\nThe paper *Do Self-Supervised and Supervised Methods Learn Similar Visual Representations?* is on [arXiv](https://arxiv.org/abs/2110.00528).", "link": "https://www.reddit.com/r/MachineLearning/comments/q3y41e/r_apple_study_reveals_the_learned_visual/"}, {"autor": "onyx-zero-software", "date": "2021-10-08 13:22:43", "content": "[D] Your Go-To Image Dataset Analysis Tools? /!/ Hey all, I'm just curious what tools you have come across for -----> image !!!  data exploration? \n\nI have a fairly large dataset and had a (surprisingly) hard time finding a generic tool I could use to explore the data without rolling my own in a collection of notebooks. I found a few duplicate-image detectors, but for the most part none of them did what I needed. My gut feeling is that there is a very obvious tool and I just haven't heard of it before, so I wanted to ask the community. \n\nThings I would like to have from such a tool:\n\n- Duplicate image detection\n- PCA/LDA (or other methods of unsupervised feature analysis)\n- Anomaly detection\n- Clustering by image content and/or Metadata (k means or mean-shift)\n\nNone of these (other than maybe anomaly detection) seem like they'd be particularly novel and have been around for a long time, which is why I figured there must be something out there.\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/q3xo2a/d_your_goto_image_dataset_analysis_tools/"}, {"autor": "Fincorn", "date": "2021-10-08 11:28:28", "content": "[R] Segregating images into groups /!/  \n\nHi, everyone\n\nI am trying to segregate a collection of 1000 -----> photo !!! s into two groups whether I am in the -----> photo !!!  or not. Can anyone help me in this.\n\nI have tried some ML models but it is not working out.", "link": "https://www.reddit.com/r/MachineLearning/comments/q3vsvt/r_segregating_images_into_groups/"}, {"autor": "papajan18", "date": "2021-01-31 22:44:34", "content": "[D] An Interesting but Weird Thought: Regular Expressions for Images? /!/ This is basically a crackpot idea so fully expected to get downvoted. \n\nWe use regular expressions to match for specific expressions in large amounts of texts (https://en.wikipedia.org/wiki/Regular_expression). Imagine if you could do this with images (or even videos!). The applications for this would be endless: from medical -----> image !!!  analysis (look for a set of canonical features of early stage cancers in MRI) to autonomous robotics (a robot that can find specific things/people). \n\nWe've scratched the surface of this kind of thing with object detection CNNs (Fast R-CNN) or segmentation. But, these kind of systems can only look for specific objects they were trained for without any notion of how to *systematically compose* these objects together. The expressive power of regular expressions would enable insanely detailed queries with large varieties of compositions. \n\nThe closest thing to this kind of thing would be OpenAI's CLIP/DALL-E, but we've seen that CLIP can struggle with some level of systematic compositionality (an example: https://twitter.com/peabody124/status/1346565268538089483). I'm not denying that DALL-E/CLIP are not learning some kind of regular expression-like mechanism if we really look under the hood. Maybe it is, but it could also not be because it wasnt trained that way. But here's a rough sketch of how you could train an explicit image regular expression system:\n\n1. Curate a dataset of images (should be domain specific i.e. medical images, but let's say imagenet since everyone uses that right now). \n\n2. It's easy to come up with the \"primitives\" for a textual regular expression (e.g. A-Z/0-9, whitespace characters, etc) but what would the primitives for an image be? We'd have to learn that, so these primitives would be initialized randomly and then as the model sees images of the dataset it'd refine these primitives to be whichever primitives maximize the likelihood/energy of the dataset. \n\n3. How do we define a likelihood/energy on an image? There are three operations of regular expressions: concatenation, alteration, and kleene star. We'd have to do some kind of discrete search over the space of regular expressions to find a regular expression that can best reconstruct the image as much as possible. This can be done through some kind of reconstruction loss and/or adverserial loss (like GANs) and/or contrastive loss (like simCLR). The performance of the primitives is based on how well the best-fitting regular expression can construct the image. \n\nObviously, there are so many things that need to be figured out. How do you even represent the primitives, how do you make the search efficient enough to traverse enough expressions, would this thing be stable enough, how can we make the whole framework differentiable, etc etc.\n\nI think we're working towards this kind of thing. Stuff like Stochastic Image Grammars (http://sci-hub.se/10.1561/0600000018), energy-based models (http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf), neuro-symbolic program synthesis (combining discrete search w/ continuous gradient-based optimization https://blog.sigplan.org/2020/04/15/synthesizing-neurosymbolic-programs/, https://arxiv.org/abs/1904.12584) are all kind of within the same space and scratching at this possibility. \n\nIf anyone made it through all of that and have any thoughts, let me know.", "link": "https://www.reddit.com/r/MachineLearning/comments/l9o4oy/d_an_interesting_but_weird_thought_regular/"}, {"autor": "crazydudeKuku", "date": "2021-01-31 22:01:10", "content": "[Discussion]How do you guys view the huge datasets stored on a server? /!/ So I am working on an -----> image !!!  based deep learning project where the data is stored on an Amazon server and all the training is also being done there itself. However, I need to look at the training images to get better feel of the data. I think this must be a common situation in professional settings. How do you guys got about it? Is there a better method than having to download the data to my system?", "link": "https://www.reddit.com/r/MachineLearning/comments/l9n7az/discussionhow_do_you_guys_view_the_huge_datasets/"}, {"autor": "[deleted]", "date": "2021-01-31 19:41:34", "content": "Is there an AI that can turn this -----> photo !!!  into a word table? /!/ [deleted]", "link": "https://www.reddit.com/r/MachineLearning/comments/l9k422/is_there_an_ai_that_can_turn_this_photo_into_a/"}, {"autor": "HenryAILabs", "date": "2021-01-31 17:15:40", "content": "[D] CLIP - Keras Code Example Walkthrough /!/ This video explains the CLIP implementation in Keras Code Examples!\n\nIn roughly 2 hours, you could have your own Natural language -----> image !!!  search engine!\n\nI hope you enjoy this video, I have been learning so much from going through these examples. Very grateful to Francois Chollet for sharing this and all the authors who have contributed!\n\nhttps://youtu.be/mXBgX5yZHhY", "link": "https://www.reddit.com/r/MachineLearning/comments/l9gro0/d_clip_keras_code_example_walkthrough/"}, {"autor": "DL_updates", "date": "2021-08-03 09:46:46", "content": "\u200b\u200b[D] CycleMLP: A MLP-like Architecture for Dense Prediction /!/ \ud83c\udf10 Overview:\n\nThis paper presents a simple MLP-like architecture, CycleMLP, which is a  versatile backbone for visual recognition and dense predictions.\n\nMLP-like models can not be used in other downstream tasks:\n\n* Non-hierarchical architectures make the model infeasible to provide pyramid feature representations.\n* They can not deal with flexible input scales.\n* The computational complexity of the Spatial FC is quadratic to -----> image !!!  size,  which makes it intractable for existing MLP-like models on high-resolution -----> image !!! s.\n\nThe motivation of Cycle FC is to enjoy channel FC\u2019s merit of taking input with arbitrary resolution and linear computational complexity while enlarging its receptive field for context aggregation. Cycle FC samples points in a cyclical style along the channel dimension.\n\nSummary by: [DLU - Deep Learning Updates](https://t.me/joinchat/NluALKTEU3RmMGM0)\n\n\ud83d\udcc5 Published: 2021-07-21\n\n\ud83d\udc6b Authors: Shoufa Chen, Enze Xie, Chongjian Ge, Ding Liang, Ping Luo\n\n\u270d\ufe0f Continue here: [https://t.me/deeplearning\\_updates/70](https://t.me/deeplearning_updates/70)\n\n\ud83d\udd17 Paper: [https://arxiv.org/abs/2107.10224](https://arxiv.org/abs/2107.10224)", "link": "https://www.reddit.com/r/MachineLearning/comments/ox030w/d_cyclemlp_a_mlplike_architecture_for_dense/"}, {"autor": "TheRealMarqupe", "date": "2021-08-02 21:08:57", "content": "[D][R] Weird Looking Images /!/ Does anyone have an idea why JPEG images decoded from [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord)s look different from the original images???\n\nFor example:\n\n* Original -----> image !!! :\n\nhttps://preview.redd.it/znpxkxihf0f71.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=20902e726ee25fec0be2d7046bba3e0bf910ffe8\n\n* The same -----> image !!!  after being decoded and reshaped to 600x600:\n\nhttps://preview.redd.it/87llm3kif0f71.jpg?width=600&amp;format=pjpg&amp;auto=webp&amp;s=f8b75f288e6591f0bde2dd0f70078d26770dd241\n\nIs this normal????\n\nI created a dataset of sharded TFRecords converted from a dataset of images, of shape 600x600x3, using the Keras `image_dataset_from_directory` function. This function automatically converts images to tensors, of dimensions 600x600x3 in this case, and each tensor was encoded to a byte string using the `tf.io.encode_jpeg` just like this:\n\n    image = tf.image.convert_image_dtype(image_tensor, dtype=tf.uint8) \n    image = tf.io.encode_jpeg(image) \n\nEach TFRecord example was created like this:\n\n    def make_example(encoded_image, label):\n        image_feature = tf.train.Feature(\n            bytes_list=tf.train.BytesList(value=[\n                encoded_image\n            ])\n        )\n        label_feature = tf.train.Feature(\n            int64_list=tf.train.Int64List(value=[\n                label\n            ])\n        )\n        features = tf.train.Features(feature={\n            'image': image_feature,\n            'label': label_feature\n        })\n        example = tf.train.Example(features=features)\n        return example.SerializeToString()\n\nAnd below is the code that loads the TFRecords Dataset, using `tf.image.decode_jpeg` to decode the images back to tensors of shape 600x600x3, and then saves one image to disk using PIL:\n\n    def read_tfrecord(example):\n        tfrecord = {\n            \"image\": tf.io.FixedLenFeature([], tf.string),\n            \"label\": tf.io.FixedLenFeature([], tf.int64),\n        }\n        example = tf.io.parse_single_example(example, tfrecord)\n        image = tf.image.decode_jpeg(example['image'], channels=3)\n        label = tf.cast(example['label'], tf.int32)\n        return image, label\n\nI have absolutely no idea what is causing this apparent loss of image information, so any help would be much appreciated!!!\n\nNotes:\n\n* I'm using Tensorflow v2.5.0. and Pillow v8.0.1\n* Find entire source code here: [https://gist.github.com/deeDude/c40fa1f14e4fa4b7f2ef149e5a344023](https://gist.github.com/deeDude/c40fa1f14e4fa4b7f2ef149e5a344023)", "link": "https://www.reddit.com/r/MachineLearning/comments/ownxy3/dr_weird_looking_images/"}, {"autor": "jj4646", "date": "2021-08-02 18:50:36", "content": "[D] Controlling the \"Shapes\" of Clusters /!/ I have included 3 pictures in the following link: https://imgur.com/a/NiFtwZZ\n\nOn the left, you have the classic \"iris flower data set\", and in the middle you have the decision boundaries obtained by \"k means clustering\". In this particular example, the k-means clustering fits the data well. But here is my question:\n\nSuppose you had to explain to some \"flower enthusiast\" what causes properties of the flowers lead to an individual flower belonging to the \"Red Cluster\" vs the \"Green Cluster\". Without getting into the math of Euclidean Distances, there is no straight forward way to explain this. You could say that \"in general, flowers in the Red Cluster have an average \"petal length\" of 12 mm\" - but of course, these kinds of statements would eventually \"overlap and conflict\" with the other clusters. These statements describe general aspects, but don't strictly define the clustering boundaries. You could tell the flower enthusiast that flowers from the red cluster have a range of values for each variable (e.g. petal length = (12.1 mm, sepal length = 6.1 mm), (petal length = 12.2 mm, sepal length = 6.2 mm) etc) - and as a result, try to \"carve out\" the cluster boundaries for each cluster ... but this would undoubtedly result in confusion.\n\nSuppose you had this new idea: you decide to \"control the shapes of the clusters\" (-----> picture !!!  on the right). For instance, you decide that shapes of the clusters should be \"rectangular\" , and agree to sacrifice on the strength of the clustering algorithm in return of more interpretable clusters (e.g. now you can clearly say, \"Red Cluster\" is where sepal length between (0, 12 mm) , petal length between (0, 6 mm), etc\". ). \n\nI think that you can do the above by writing a giant \"for loop\". Suppose you decide there are 3 clusters (let's just stick with 2 variables to make this example easier) - these 3 clusters can be defined by making a series of boolean splits in the 2 variables (Red   : var1 (0,a) AND var2 (0,b) , Blue: var1 (a,a1) AND var2(b,b1), Green: var1 &gt; a1 AND var2&gt;b1.\n\nNow, you can randomly select a unique set of (a,a1,b,b1) and evaluate some metric for this set (e.g. \"within cluster sum of squares\"). If you randomly select 100,000 unique sets of (a,a1,b,b1) - you could decide on the final boundaries based on which set of (a,a1,b,b1) give you the highest value of your performance metric.\n\nQuestion: What does everyone think of this approach? K-means clustering has a much better ability at recovering \"natural cluster structure\" within the data, but this \"rectangular approach\" provides more interpretable clusters. Does this seem like a valid approach, provided some decent results are produced?\n\nNote: I know that decision trees are doing something similar as the process I described - but decision trees will not necessarily give the clear cut rectangular boundaries I described.\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/owl31e/d_controlling_the_shapes_of_clusters/"}, {"autor": "throwbacktous1", "date": "2021-08-02 18:40:31", "content": "AI can now detect political ideology with a single facial -----> photo !!! ", "link": "https://www.reddit.com/r/MachineLearning/comments/owkvhu/ai_can_now_detect_political_ideology_with_a/"}, {"autor": "o_snake-monster_o_o_", "date": "2021-08-02 17:14:12", "content": "[D] Are pixels and grids the best way to teach AI about images? What about breaking down the -----> image !!!  into broad hierarchical shapes first? /!/ I was thinking recently how the brain seems so efficient at working with shapes. I'm still only a noobie when it comes to ML, but it seems that when it comes to imaging, 99% of the time we are feeding 2D grids into our networks, with neighbor cells having stronger links to instill the sense of locality. Doesn't this mean that a shape must be taught in every orientation and every size? If a shape is scaled up or rotated, it can no longer be recognized as the same identity unless that shape was part of the training set. \n\nIf we stored the information for a shape in a compressed list of instructions instead, we could access higher-level pattern matching. For example, a square can be expressed as 4 instructions that define an idea: 4 translations at perpendicular angles. When the pattern matching system goes through the pixels of an image broadly, it can repeatedly test against this chain of states, and if it reaches the end that is a strong indicator that you have a square, depending on the remaining energy of the signal. Wobbly lines or obstructions can weaken the signal.\n\nWith a balanced and efficient breakdown of images into basic shapes and an efficient way to encode transitions and relationships between those shapes between those shapes, AI teaching could be ridiculously optimized to near realtime, don't you think? Show a mug in front of a camera, tell the AI \"these blotches are a mug\" and then as you rotate the mug, it can track the blotches. When each blotch has morphed beyond a threshold, take a snapshot and make a connection to the previous state. Now, whenever the imaging software excites that shape definition, they should be connected to the 'mug' label. I reckon we have efficient depth perception networks as well right now, which could potentially add a whole dimension to the color blotches.\n\nObviously there are a number of challenges in what I'm saying, but I'm just wondering if stuff like this has been researched, is it a dead-end, etc. since I'd like to investigate this kind of a system myself. I know there are efficient segmentation AIs and I've seen Detectron2 as well, but I'm not sure what are their limits or short-comings, what the training process and cost is like, etc. My goal is to make a general purpose AI that can be trained in real-time.", "link": "https://www.reddit.com/r/MachineLearning/comments/owj2j4/d_are_pixels_and_grids_the_best_way_to_teach_ai/"}, {"autor": "KirillTheMunchKing", "date": "2021-08-02 14:20:31", "content": "[D] SOTA Super-Resolution explained - Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data by Xintao Wang et al. 5 minute summary /!/ [Real-ESRGAN](https://preview.redd.it/qn42ourjeye71.png?width=1280&amp;format=png&amp;auto=webp&amp;s=48a36cb40ef3b831e1e3c0fcb6e80b329c146151)\n\n**Overview:**  \nWhile there are many blind -----> image !!!  restoration approaches, few can handle complex real-world degradations. Yet Real-ESRGAN by Xintao Wang and his colleagues from ARC, Tencent PCG, Shenzen Institutes, and University of Chinese Academy of Sciences takes real-world image super-resolution (SR) to the next level! The authors propose a new higher-order image degradation model to better simulate real-world data. This idea together with an improved U-Net discriminator allows Real-ESRGAN to demonstrate superior visual performance than prior works on various real datasets.\n\n**Motivation:**  \nClassical degradation model, which consists of blur, downsampling, noise and JPEG compression is not complex enough to model real-world degradations. Models trained on these synthetic samples will easily fail on real-world tests. The goal of this work is to extend blind SR trained on synthetic data to work on real-world images at inference time. Hence, a more sophisticated degradation model called second-order degradation process is introduced. To compensate for the larger degradation space the VGG-style discriminator is upgraded to a U-Net design. Additionally, spectral normalization (SN) regularization is applied to stabilize training.\n\nRead the [full paper digest](https://t.me/casual_gan/76) or the [blog post](https://www.casualganpapers.com/synthetic-data-real-world-blind-super-resolution/Real-ESRGAN-explained.html) (reading time \\~5 minutes) to learn about the downsides of the Classical Degradation Model, how a higher order degradation improves the super-resolution quality, how to fix ringing and overshoot artifacts, and why a U-Net generator with spectral normalization stabilizes training.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://www.casualganpapers.com/)!\n\n[Real-ESRGAN](https://preview.redd.it/2zu4labieye71.png?width=564&amp;format=png&amp;auto=webp&amp;s=e9641264004c8a1d11cc8c465111f3cb2c36ded1)\n\n\\[[Full Explanation Post](https://t.me/casual_gan/76) / [Blog Post](https://www.casualganpapers.com/synthetic-data-real-world-blind-super-resolution/Real-ESRGAN-explained.html)\\] \\[[Arxiv](https://arxiv.org/pdf/2107.10833v1.pdf)\\] \\[[Code](https://github.com/xinntao/Real-ESRGAN)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n&gt;\\[[SimSiam](https://www.casualganpapers.com/self-supervised-contrastive-learning-siamese-networks/SimSiam-explained.html)\\]  \n&gt;  \n&gt;\\[[ViTGAN](https://www.casualganpapers.com/gan-based-self-attention-vision-transformers/ViTGAN-explained.html)\\]  \n&gt;  \n&gt;\\[[BYOL](https://www.casualganpapers.com/self-supervised-contrastive-representation-learning/BYOL-explained.html)\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/owfi03/d_sota_superresolution_explained_realesrgan/"}, {"autor": "Green_General_9111", "date": "2021-05-14 01:53:51", "content": "[Research] [R] Extreme Face Inpainting with Sketch-Guided Conditional GAN /!/ Recovering badly damaged face images is a useful yet challenging task, especially in extreme cases where the masked or damaged region is very large. One of the major challenges is the ability of the system to generalize on faces outside the training dataset. We propose to tackle this extreme inpainting task with a conditional Generative Adversarial Network (GAN) that utilizes structural information, such as edges, as a prior condition. Edge information can be obtained from the partially masked -----> image !!!  and a structurally similar -----> image !!!  or a hand drawing. In our proposed conditional GAN, we pass the conditional input in every layer of the encoder while maintaining consistency in the distributions between the learned weights and the incoming conditional input. We demonstrate the effectiveness of our method with badly damaged face examples.", "link": "https://www.reddit.com/r/MachineLearning/comments/nbxj75/research_r_extreme_face_inpainting_with/"}, {"autor": "Broad-Fuel4116", "date": "2021-05-13 21:06:16", "content": "[R] Facial Action Coding System Tutorials /!/ Hi all\n\nI  make tutorial videos on the Facial Action Coding System (Ekman et al.  2002) and want to share them with your community as training data. The video description also contains links to -----> image !!!  sets that I've shared for this purpose previously. \n\nThe following video is on downward lip corners and the visual similarities between this and others actions, which complicate automated detection: [https://www.youtube.com/watch?v=KQAR6nhPtBo](https://www.youtube.com/watch?v=KQAR6nhPtBo)\n\nI welcome questions here and elsewhere.\n\nBest wishes\n\nOli", "link": "https://www.reddit.com/r/MachineLearning/comments/nbrfoh/r_facial_action_coding_system_tutorials/"}, {"autor": "quantumvibranium", "date": "2021-05-13 14:50:04", "content": "[D] Defect Passed The Inspection From Machine Vision /!/  \n\nHello all, I'm new member here. I solely create Reddit account for gaining knowledge in manufacturing area.\n\nNeed your help guys as I am curious about machine vision (Cognex, Keyence, Basler etc).\n\nPlace where I work have several machine vision for IC packages defect detection (for molding, leadframe, marking). What I don't understand is sometimes the vision system cannot detect the defect even though when we try to do offline test or verification on the machine it could detect the defect on IC package.\n\n(FYI, we use Basler as 9 main -----> camera !!!  and 3rd party vendor for software plus machine).\n\nWhat do you think guys the common cause of this inconsistencies?\n\nIs it common problem in manufacturing industry as well?", "link": "https://www.reddit.com/r/MachineLearning/comments/nbie4p/d_defect_passed_the_inspection_from_machine_vision/"}, {"autor": "nixxon94", "date": "2021-05-13 12:05:43", "content": "[D] what is the best way to start writing own -----> image !!!  upscaling/ denoising ai? /!/ Hey everyone, I have some basic python knowledge and would like to try  out some good tutorials/ guides on this topic. Any pointers would be  appreciated, thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/nbf2dk/d_what_is_the_best_way_to_start_writing_own_image/"}, {"autor": "nixxon94", "date": "2021-05-13 12:01:03", "content": "what is the best way to start writing own -----> image !!!  upscaling/ denoising ai? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/nbez6x/what_is_the_best_way_to_start_writing_own_image/"}, {"autor": "techsucker", "date": "2021-05-13 08:54:03", "content": "[R] Google AI Introduces \u2018ALIGN\u2019 to Scale Up Visual and Vision-Language Representation Learning with Noisy Text Supervision /!/ Excellent visual and vision-language representations are crucial in solving computer vision problems such as -----> image !!!  retrieval, -----> image !!!  classification, video understanding. That is why visual and vision-language models rely on curated training datasets such as [ImageNet](https://www.image-net.org/), [OpenImages](https://opensource.google/projects/open-images-dataset), [Conceptual Captions](https://ai.google.com/research/ConceptualCaptions), which require expert knowledge and extensive labels. All these datasets need non-trivial data collection and cleaning steps, limiting the size of datasets and hindering the trained models\u2019 scale. In comparison, NLP models use large-scale pre-training on raw text *without* human labels and have achieved SotA performance on [GLUE](https://gluebenchmark.com/) and [SuperGLUE](https://super.gluebenchmark.com/) benchmarks.\u00a0\n\nGoogle researchers propose a technique to bridge this gap by using publicly available image alt-text data (text appearing in place of an image on a webpage when the image fails to load). The team employs these image alt-text data to train larger, state-of-the-art vision and vision-language models.\u00a0\n\nSummary: [https://www.marktechpost.com/2021/05/13/google-ai-introduces-align-to-scale-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/](https://www.marktechpost.com/2021/05/13/google-ai-introduces-align-to-scale-up-visual-and-vision-language-representation-learning-with-noisy-text-supervision/) \n\nPaper: [https://arxiv.org/pdf/2102.05918.pdf](https://arxiv.org/pdf/2102.05918.pdf) \n\nGoogle Blog: [https://ai.googleblog.com/2021/05/align-scaling-up-visual-and-vision.html](https://ai.googleblog.com/2021/05/align-scaling-up-visual-and-vision.html)", "link": "https://www.reddit.com/r/MachineLearning/comments/nbc5ue/r_google_ai_introduces_align_to_scale_up_visual/"}, {"autor": "user692646", "date": "2021-05-07 11:45:19", "content": "[D] Element-wise multiplication instead of Convolution /!/ I'm considering using element-wise multiplication as the fundamental operation in a net, and I wanted to know if this has been done before in the literature.\n\nThe operation for a single layer would work as follows:\n\n1. Given an M x N input -----> image !!! , multiply it element-wise by an M x N weight matrix.\n2. Add an M x N matrix of biases to the output of step 1.\n3. Compute a non-linearity (e.g. ReLU) element-wise on the output of step 2.\n4. Use a K x K average pooling kernel with stride K to down-sample the output of step 3.\n\nThe reason I am considering using this operation instead of a conv layer is because I think the conv layer is a bit too regularized for my problem. By that, I mean if the convolution operation were treated as a matrix-vector multiplication, such that the input to the conv layer is an image re-shaped to be a column vector, then the matrix would be too sparse.\n\nIf needed, I could enforce sparsity later with L1 regularization, for example.\n\nHas this sort of operation been considered before in the literature?", "link": "https://www.reddit.com/r/MachineLearning/comments/n6wl1e/d_elementwise_multiplication_instead_of/"}, {"autor": "Hydra1721", "date": "2021-05-06 23:09:39", "content": "Has There Been Any Follow Up Research Papers for the Anti-FRS AI Called \"Fawkes\" \"[Discussion]\" /!/  \n\nLast year a research paper was published that described a AI that could alter a imagine in a certain manner that prevented FRS from correctly identifying a individual's face without changing the appearance of the -----> photo !!!  to the viewer:\n\n[https://www.theverge.com/2020/8/4/21353810/facial-recognition-block-ai-selfie-cloaking-fawkes](https://www.theverge.com/2020/8/4/21353810/facial-recognition-block-ai-selfie-cloaking-fawkes)\n\nIt was stated at the time that Fawkes was unable to perform these modifications in real time. Since the publication, I have not read about any other research papers derived from their original work. Has there been any developments in this field of research since then and if so has anyone managed to develop a network that runs in real-time?", "link": "https://www.reddit.com/r/MachineLearning/comments/n6kfla/has_there_been_any_follow_up_research_papers_for/"}, {"autor": "VirtualTurtwig", "date": "2021-05-06 16:25:08", "content": "[Discussion] Embedding more than a single label per -----> image !!!  to a GAN dataset? /!/ I've created a dataset that has an RGB image, grayscale image, and 4-8 float variables associated with each data point. StyleGAN2 only allows for assigning a single class to an RGB data point, what other GAN models or dataset frameworks exist that can incorporate the amount of info that I want?  Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/n6b76b/discussion_embedding_more_than_a_single_label_per/"}, {"autor": "danielgafni", "date": "2021-05-04 20:05:57", "content": "Repalette: -----> image !!!  recoloring with deep neural networks", "link": "https://www.reddit.com/r/MachineLearning/comments/n4wsmx/repalette_image_recoloring_with_deep_neural/"}, {"autor": "malia912", "date": "2021-05-04 14:53:35", "content": "YOLOV5 with a second stage classifier [D] /!/ [D]\nHello everyone. This is a question regarding YOLOV5 In detect.py module I saw a second stage classifier but it is set to false.\n\nPROBLEM STATEMENT: I want to classify images with a secondary classifier. In first stage YOLOV5 just draws bounding box around -----> image !!! s of Dogs (basically localising the dogs in the -----> image !!! ) and with the help of SECONDARY CLASSIFIER want to classify dogs into respective breeds.\n\nRESULT: bounding box around the dog with names of the\n\nbreed.\n\nHas anyone tried this approach?\n\nThank you for your time.", "link": "https://www.reddit.com/r/MachineLearning/comments/n4qa9c/yolov5_with_a_second_stage_classifier_d/"}, {"autor": "world-builder66", "date": "2021-02-10 05:52:36", "content": "[D] Rigorous Training Strategy for Image Classification /!/ Some contexts before diving to the topic: I'm a final year undergrad biomedical engineering student currently doing final research on the topic of -----> image !!!  classification to detect diabetic retinopathy. I know this topic is already developed by major players in the industry (e.g. DeepMind) and the community (Kaggle hosted at least 2 competitions regarding this). However, my supervisor wants me to develop a rigorous method that can be published in academic publications. I've read the solutions from previous Kaggle comp winners and read tons of research papers on this topic and from there I realized that machine learning development is a kinda mix and match (compared to careful planning from an empirical/theoretical basis in academia). \n\nMy goal here is to craft a best practice method, but I have limited time and computing resources. It is practically impossible to try all combinations of hyperparameter, architecture, etc, so I want to develop a framework for developing this model that is strong enough to convince my professors. I have conducted an initial brainstorming and think maybe a cascading training strategy is good enough (first training for comparing architectures, then go on compare learning rates, etc), but the search space is still enormous. Any thought on this? Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/lgn6qk/d_rigorous_training_strategy_for_image/"}, {"autor": "Perseus784", "date": "2021-02-09 18:36:21", "content": "[P] Teaching how to overtake with raw -----> image !!!  inputs using Double Q-learning /!/ Hi,\n\nUsed a CNN to take in images as input and converge policies for doing an overtaking action in a highway environment. Since it follows OpenAI gym's format, you can easily replace and train your agents for your preferred environments.\n\nLink: [https://github.com/perseus784/Vehicle\\_Overtake\\_Double\\_DQN](https://github.com/perseus784/Vehicle_Overtake_Double_DQN)\n\nI have also explained each step of the project comprehensively using analogies from the movie *TENET*. Please see the following link:\n\n[https://towardsdatascience.com/explaining-double-q-learning-for-openai-environments-using-the-movie-tenet-816dc952f41c](https://towardsdatascience.com/explaining-double-q-learning-for-openai-environments-using-the-movie-tenet-816dc952f41c) \n\nhttps://i.redd.it/w06fntemxhg61.gif\n\nHave a nice day!", "link": "https://www.reddit.com/r/MachineLearning/comments/lga0zv/p_teaching_how_to_overtake_with_raw_image_inputs/"}, {"autor": "IrrationalCrocodile", "date": "2021-02-09 09:11:37", "content": "[R] Using Growing Neural Gas for feature extraction in -----> image !!!  classification /!/ I am trying to use RBF-layer generated with Growing Neural Gas(GNG) algorithm to extract features as first Convolutional layer in Convolutional networks do.\n\nThe idea is such:\n\n1. With sliding 3x3x3 window over image flatten it to 9x3, sum channels to get just vector with size 9.\n2. Use this vector as a sample for GNG.\n3. Repeat for all images in dataset.\n4. Get graph in 9 dimensions which will replicate a topology of the training set. \n5. Use the graph's nodes as centers for RBF-layer. If new point's (3x3x3 window flattened to vector as in 1.) position is close to one of the graph's nodes - it means that algorithm saw such sample in the training set.\n6. For a given image split it with same sliding window, flatten it and sum channels, then for each window find how far it is from each of centers putting distances in vector with size equal to number of centers.\n7. A result for one image will be a tensor with dimensions image\\_width-2 x image\\_height-2 x number\\_of\\_centers. Each value in first 2 dimensions will be a vector of distances of sliding window to all of centers.\n\nI wrote code prototype for this, it is available in collab:  \n[https://colab.research.google.com/drive/18gkWso9n4xKGkee\\_c2rY9UNYzSoykyf3](https://colab.research.google.com/drive/18gkWso9n4xKGkee_c2rY9UNYzSoykyf3)\n\nI made some experiments with it and all of them failed.\n\nA network which layers: Conv2D, MaxPool2D, Conv2D, MaxPool2D, Flatten, Dense, Dense gives 10% more accuracy on cifar10 then same network but with GNG-generated RBF-layer at first.\n\nThe results are not very bad. Still \\~50% accuracy on 10 classes, so it is not random guessing. But it is worse than without such feature extraction.\n\nSo I want to ask redditors - is my idea a crap or it can be improved to something useful? I would be glad to see any feedback, thanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/lfyn3n/r_using_growing_neural_gas_for_feature_extraction/"}, {"autor": "warpcut", "date": "2021-02-09 00:14:31", "content": "[D] Low data regime, what are your moves? /!/ Hi, it's more than a month now since i've started working on a Project.\nBasically it can be viewed as a medical -----> image !!!  classification task.\n\nThe main problem is the dataset size, actually its a huuge problem, (&lt;100). I've tried with GANs, TL, some sort of analitycal ROI extraction to feed the net. \n\nBut onestly right now I'm running out of ideas.\nWhat do you tipically do in these situations? What was the size of the smallest dataset you've been able to achieve some valid results? \nAny advice on papers to read? \n\nThanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/lfpw1w/d_low_data_regime_what_are_your_moves/"}, {"autor": "projekt_treadstone", "date": "2021-02-08 23:30:36", "content": "[D]Open set -----> image !!!  classification while inference for an unseen class and its new class classification /!/ Is there any relevant research in open set image classification which can classify unseen image class as unseen classes at inference and the same point of time model/algorithm should be able to tell in which new class this unseen image belongs to.\n\nI can think of some solution based on representation/feature-based learning or combining a zero-shot learning approach. I know incremental learning can be a solution but it requires retraining again with catastrophic forgetting. So I am searching for research/work other than incremental learning.", "link": "https://www.reddit.com/r/MachineLearning/comments/lfp03w/dopen_set_image_classification_while_inference/"}, {"autor": "TerryCrewsHasacrew", "date": "2021-09-24 01:07:03", "content": "[D] Where does the 128 come from? /!/ Hi\n\nI was reading through the intel's paper regarding \"Enhancing Photorealism Enhancement\" and encounter this -----> image !!!  in the paper\n\n&amp;#x200B;\n\nhttps://preview.redd.it/gesssulyocp71.png?width=537&amp;format=png&amp;auto=webp&amp;s=17bc36d87a4c4db7d29d6f1f7a076a4450cf9292\n\nWas wondering if anyone could tell me where this 128, in the -----> image !!! , comes from, cause the number of G-buffers are less than 128?", "link": "https://www.reddit.com/r/MachineLearning/comments/pu92vu/d_where_does_the_128_come_from/"}, {"autor": "shrek69ergangwow", "date": "2021-09-24 00:55:57", "content": "[D] where to find the AI text to -----> image !!!  /!/ Do any of you guys have a link where I can use  use AI text to image", "link": "https://www.reddit.com/r/MachineLearning/comments/pu8wbl/d_where_to_find_the_ai_text_to_image/"}, {"autor": "shrek69ergangwow", "date": "2021-09-24 00:52:53", "content": "Where to find AI text to -----> image !!!  /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pu8ueu/where_to_find_ai_text_to_image/"}, {"autor": "shrek69ergangwow", "date": "2021-09-24 00:50:16", "content": "Where to find the ai text to -----> image !!!  /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pu8sv0/where_to_find_the_ai_text_to_image/"}, {"autor": "techsucker", "date": "2021-09-23 18:16:20", "content": "[R] Google AI Introduces \u2018WIT\u2019, A Wikipedia-Based -----> Image !!!  Text Dataset For Multimodal Multilingual Machine Learning /!/ Image and text datasets are widely used in many machine learning applications. To model the relationship between images and text, most multimodal Visio-linguistic models today rely on large datasets. Historically, these datasets were created by either manually captioning images or crawling the web and extracting the alt-text as the caption. While the former method produces higher-quality data, the intensive manual annotation process limits the amount of data produced. The automated extraction method can result in larger datasets. However, it requires either heuristics and careful filtering to ensure data quality or scaling-up models to achieve robust performance.\u00a0\n\nTo overcome these limitations, Google research team created a high-quality, large-sized, multilingual dataset called [the Wikipedia-Based Image Text (WIT) Dataset](https://github.com/google-research-datasets/wit). It is created by extracting multiple text selections associated with an image from Wikipedia articles and Wikimedia image links.\u00a0\n\n# [5 Min Read](https://www.marktechpost.com/2021/09/23/google-ai-introduces-wit-a-wikipedia-based-image-text-dataset-for-visio-linguistic-models/) | [Github](https://github.com/google-research-datasets/wit) | [Paper](https://arxiv.org/pdf/2103.01913.pdf) | [Google Blog](https://ai.googleblog.com/2021/09/announcing-wit-wikipedia-based-image.html)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/32dctgv3oap71.png?width=646&amp;format=png&amp;auto=webp&amp;s=81ee40de71b0e6dc9ae13f1c5819cd5fa13c074a", "link": "https://www.reddit.com/r/MachineLearning/comments/pu18zt/r_google_ai_introduces_wit_a_wikipediabased_image/"}, {"autor": "Ok_Confection9620", "date": "2021-09-23 17:20:15", "content": "[D] Transformer and training for a sequence of embeddings from huge-size images /!/ hi all,  \n\nI have a problem finding a approach for the following problem: A huge size -----> image !!!  (e.g. 60 000 x 100 000 pixel) will be subdivided into N patches of size 224x224 and afterwards feed into a pre-trained feature extractor resulting in a latent dimension L, lets assume the feature extractor is working correctly . Therefore, we have a sequence of length NxL. Each patch should be binary classified, i.e. 0 or 1 (e.g. contains a specific thing or not), which leads to a Multi-label classification problem. My problem is, I just have 50 annotated huge images where each patch is annotated as 0 or 1, and about 1500 unlabelled huge images. As the patch-lvl classification depends on the patches around it, I though of using a Transformer, but I don't find pre-trained weights for this task, when working on a feature space and I don't think 50 annotated images are enough...  Any suggestions? \n\n**Summary:**  \n\n*Task:* Multilabel classification  \n\n*Data:* 50 big images, and a label-vector Nx1 containing 0s and 1s, 1500 unlabelled huge images \n\n*Pipeline:* Input: H1xH2 -&gt; feature extractor -&gt; NxL -&gt; Transformer -&gt; Output shape: Nx1", "link": "https://www.reddit.com/r/MachineLearning/comments/pu01z9/d_transformer_and_training_for_a_sequence_of/"}, {"autor": "ExerciseNo4456", "date": "2021-09-23 17:02:39", "content": "[D] Ripeness in fruit detection + Object Classification /!/ Hi guys im New in ML related projects. \n\nI\u00b4m working on a project to detect ripeness in fruit in 3 categories, unripe, ripe, overripe. However, i also want\nto differenciate between one fruit to anoter. For example, banana and apple. I understand that there are algorithms for:\nClassifications of objects, and also detection of objects. The questions is, is there way to combine these two type of ML\nand make it real time using -----> camera !!! ?\n\nFor exame, if i show two fruit, then ML identify if that fruit is banana or apple, then predict the ripeness in 3 categories. \n\nExample:\n\nShow fruit ---&gt; Classify: Apple | Banana --- &gt; Preddict result: [Apple] Overripe 87% | [Banana] Ripe 92%", "link": "https://www.reddit.com/r/MachineLearning/comments/ptzo8y/d_ripeness_in_fruit_detection_object/"}, {"autor": "zbnone", "date": "2021-09-23 14:06:58", "content": "[D] How do you manage your ml models that require a gpu in production? (Question) /!/ I founded a tech startup and we are working with -----> image !!!  segmentation models (u-net like). We currently deploy our models to manually setup gpu nodes with tensorflow serving. This requires lots of upkeep and infrastructure around it.  \nDo you know any alternatives? My dream workflow would be a simple web interface where you can upload your tensorflow serving (or h5 keras, pytorch) models and get a for example basic authentication protected url where you can post your images (or text, videos, audios) to and get the model results back?  \nIf no one knows a platform like it, we are thinking about building something like it internally. If anyone sees a fundamental problem with that approach or as any advice it would be very welcome.  \nThank you :)", "link": "https://www.reddit.com/r/MachineLearning/comments/ptw109/d_how_do_you_manage_your_ml_models_that_require_a/"}, {"autor": "mrtac96", "date": "2021-09-23 12:15:03", "content": "[D] Can we implement DL model in practical scenario? /!/ I collected 5 different dataset of a disease, these dataset are binary, multilabel and multiclass.  Two of them used for test. Now one dataset in test have some classes which are in train set, though images and source are different. And second data have no such case. Though all data have a normal class. I converted data to binary, normal vs disease.\n I split the train into train and val. The train and val accuracy is 80%. The test data which has some classes in train before binarization score around 80, but the other data score around 60. Though the F1 score of normal class is good in this data, but model fail to detect diseases.\n\nSo what I am saying, the model knows that how normal -----> image !!!  is. It is successfully finding it, why it fails to do opposite as it's just binary\nSo my question is that is there a way to handle this. If you know any paper which have done same study, please let me know.", "link": "https://www.reddit.com/r/MachineLearning/comments/pttw9z/d_can_we_implement_dl_model_in_practical_scenario/"}, {"autor": "Alan491", "date": "2021-09-23 12:12:07", "content": "[D] Need help in training, validation loss fluctuating a lot? /!/ I am a newbie in DL and training a CNN -----> image !!!  classification model on resnet50, having a dataset of 2 classes 14k each (28k total), but the model training is very fluctuating, so, please give me suggestions on what's wrong with the training...\n\nI tried with batch sizes 8,16,32 &amp; LR with 4e-4 to 1e-5 (ADAM), but every time the results are the same.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/riegwd60v8p71.png?width=1040&amp;format=png&amp;auto=webp&amp;s=8b68a50b5dbf9444c0aa6dc99ccdd1f811a45505", "link": "https://www.reddit.com/r/MachineLearning/comments/pttuhy/d_need_help_in_training_validation_loss/"}, {"autor": "rootacess3000", "date": "2021-09-26 06:25:07", "content": "[N][D][R] Implementing Style Transfer using VGG-19 /!/ Check out my new notebook on Kaggle on implementing style transfer using VGG-19 network with PyTorch. Style transfer is a a technique which is used to transfer the style, texture from one -----> image !!!  to other ensuring low loss of content in the original -----> image !!! .\n\nNotebook \ud83d\udc47  \n[https://www.kaggle.com/swayamsingh/style-transfer-using-vgg-19](https://www.kaggle.com/swayamsingh/style-transfer-using-vgg-19)", "link": "https://www.reddit.com/r/MachineLearning/comments/pvobhq/ndr_implementing_style_transfer_using_vgg19/"}, {"autor": "JMG518", "date": "2021-09-26 03:28:19", "content": "[D] Realistic Text to Image AI /!/ Hey guys! I'm attempting to employ text-to------> image !!!  AI for to generate -----> image !!! s of a parking garage and other buildings for inspiration in an architecture project. I have tried to use Clip+VQGAN, Clip+Glass, and Big Sleep all using Google Colab. Unfortunately, my results haven't been realistic. What text-to-image AI is best to use to create realistic images? Also, what are your thoughts on running code using the program \"InteiJ\"? Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/pvlwem/d_realistic_text_to_image_ai/"}, {"autor": "Hunamed_silva", "date": "2021-09-25 00:26:58", "content": "[D] Do you know any open-source video classifiers? /!/  So I would like to do some experiments in video classification.\n\nI have some experience working with traditional structured data and now I'm getting some experiments on object detection, -----> image !!!  classification, and -----> image !!!  segmentation. I started doing object detection with YOLO and evolved from there.\n\nI know that for video classification I can use a feature extractor and a GRU or LSTM but I'm not ready to design the model by myself.\n\nDo you know any open source projects like YOLO but for video classification, where I could train a custom model and explore their code?", "link": "https://www.reddit.com/r/MachineLearning/comments/puvwwh/d_do_you_know_any_opensource_video_classifiers/"}, {"autor": "GuaranteedBigBoy", "date": "2021-09-17 20:59:00", "content": "My attempt at the highest quality -----> image !!!  created by an A.i.", "link": "https://www.reddit.com/r/MachineLearning/comments/pq8nhq/my_attempt_at_the_highest_quality_image_created/"}, {"autor": "crosszilla", "date": "2021-09-17 18:48:53", "content": "[D] Could image recognition help uncover evidence in the Gabby Patito case? /!/ Hello all,\n\nI have been consumed with this case and recently someone discovered Yellowstone's publicly available -----> camera !!!  archives during the dates she went missing. People are, as we speak, combing through hundreds of hours of footage looking for her van and any clues: https://www.reddit.com/r/GabbyPetito/comments/pq35n7/archived_webcam_footage_from_inside_yellowstone/\n\nI really hope she is found, but don't know much in terms of actually implementing something to automate a task like this. Would this be something that could easily be automated? If so, would anyone be willing to help out LEO on this?", "link": "https://www.reddit.com/r/MachineLearning/comments/pq66sz/d_could_image_recognition_help_uncover_evidence/"}, {"autor": "6eer", "date": "2021-09-17 15:54:06", "content": "[D] How far we are from an AI being able of code and entire UI website just with its UI design (-----> image !!! ) as input? /!/ I\u2019m curious guys, what\u2019s your thoughts?", "link": "https://www.reddit.com/r/MachineLearning/comments/pq2tg1/d_how_far_we_are_from_an_ai_being_able_of_code/"}, {"autor": "Illustrious_Ad_637", "date": "2021-09-17 14:19:24", "content": "[D] What were some of the popular -----> image !!!  processing techniques pre-deep learning era? /!/ More specifically, for object detection or segmentation. Why did we stop using those?", "link": "https://www.reddit.com/r/MachineLearning/comments/pq127g/d_what_were_some_of_the_popular_image_processing/"}, {"autor": "Illustrious_Ad_637", "date": "2021-09-17 14:16:17", "content": "What were some of the popular -----> image !!!  processing techniques pre-deep learning era? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pq105z/what_were_some_of_the_popular_image_processing/"}, {"autor": "diibv", "date": "2021-09-17 12:31:14", "content": "[N] [R] Call for participation in the MediaEval 2021 Emotion and Theme Recognition in Music task /!/ We  are pleased to announce the third year of the Emotion and Theme Recognition in Music task held within the MediaEval 2021 evaluation campaign.\n\nThe Benchmarking Initiative for Multimedia Evaluation (MediaEval) organizes an annual cycle of scientific  evaluation tasks in the area of multimedia access  and retrieval. In our task, we invite the participants to try their skills at predicting mood  and theme tags associated with music  recordings using audio analysis and machine learning algorithms.\n\nThe  task is framed as an auto-tagging problem with tags specific to moods  and themes (e.g.,  happy, dark, epic, melodic, love, -----> film !!! , space). The  task uses the  MTG-Jamendo dataset: [https://mtg.github.io/mtg-jamendo-dataset/](https://mtg.github.io/mtg-jamendo-dataset/), presented at the Machine Learning for Music Discovery Workshop at ICML 2019: [https://hdl.handle.net/10230/42015](https://hdl.handle.net/10230/42015)\n\nAll interested researchers are warmly welcomed to participate. The  deadline  for all submissions for the challenge is November 5. Participants will be able to present their results at the MediaEval  Multimedia Benchmark  Workshop, to be held on December 6-8 in Bergen,  Norway with opportunity  for online participation: [https://multimediaeval.github.io/](https://multimediaeval.github.io/)\n\nThe registration for the task is available at the MediaEval website. A full description of the task is available here: [https://multimediaeval.github.io/2021-Emotion-and-Theme-Recognition-in-Music-Task/](https://multimediaeval.github.io/2021-Emotion-and-Theme-Recognition-in-Music-Task/)", "link": "https://www.reddit.com/r/MachineLearning/comments/ppz92r/n_r_call_for_participation_in_the_mediaeval_2021/"}, {"autor": "techsucker", "date": "2021-09-17 03:33:00", "content": "[R] Israeli Researchers Unveil DeepSIM, a Neural Generative Model for Conditional -----> Image !!!  Manipulation Based on a Single -----> Image !!!  /!/ In recent years, deep neural networks have been proven effective at performing image manipulation tasks for which large training datasets are available such as, mapping facial landmarks to facial images. When dealing with a unique image, finding suitable training data that includes many samples of the same input-output pairing is often difficult. In some cases, when you use a large dataset to create your model, it may lead to unwanted outputs that do not preserve the specific characteristics of what was desired.\u00a0\n\nGenerative models like the ones used in neural networks can be trained to generate new images based on just one input. This exciting research direction holds the potential for these techniques to extend beyond basic image manipulation methods and create more unique art styles or designs with endless possibilities. Researchers at [The Hebrew University of Jerusalem have developed a new method, called \u2018DeepSIM,\u2019](https://arxiv.org/pdf/2109.06151.pdf) for training deep conditional generative models from just one image pair. The DeepSIM method is an incredibly powerful tool that can solve various image manipulation tasks, including shape warping, object rearrangement, and removal of objects; addition or creation of new ones. It also allows for painting/photorealistic animated clips to be created quickly.\n\n# [5 Min Read](https://www.marktechpost.com/2021/09/16/israeli-researchers-unveil-deepsim-a-neural-generative-model-for-conditional-image-manipulation-based-on-a-single-image/) | [Paper](https://arxiv.org/pdf/2109.06151.pdf) | [Project](http://www.vision.huji.ac.il/deepsim/) | [Code](https://github.com/eliahuhorwitz/DeepSIM)\n\n&amp;#x200B;\n\n![video](ntztn2i3hzn71)", "link": "https://www.reddit.com/r/MachineLearning/comments/ppsh12/r_israeli_researchers_unveil_deepsim_a_neural/"}, {"autor": "SnooHabits4550", "date": "2021-09-16 13:37:50", "content": "[D] Understanding SVM mathematics /!/ I was referring [SVM section](http://cs229.stanford.edu/notes2020fall/notes2020fall/cs229-notes3.pdf) of Andrew Ng's course notes for Stanford CS229 Machine Learning course. On pages 14 and 15, he says:\n\n&gt;Consider the -----> picture !!!  below:\n\nhttps://preview.redd.it/qsjr6crxbvn71.png?width=426&amp;format=png&amp;auto=webp&amp;s=77d7357df684d71f280b551467fae98ad3203aa0\n\n&amp;#x200B;\n\nhttps://preview.redd.it/qka1ehp0cvn71.png?width=672&amp;format=png&amp;auto=webp&amp;s=ed1d819db9f68ce48cadc04bf6319596d7c830aa\n\nPS: I posted a question as majorly an image to make equations clear. Let me know if we can specify LaTeX in this subreddit.", "link": "https://www.reddit.com/r/MachineLearning/comments/ppdgul/d_understanding_svm_mathematics/"}, {"autor": "nyquist_karma", "date": "2021-09-16 11:17:03", "content": "Background removal too strong when using U2Net [Project] /!/ Hello, I am successfully using [U2Net](https://github.com/xuebinqin/U-2-Net) to **remove** the **background** of -----> image !!! s using the terminal and I am also using the nice interface of [this](https://github.com/OPHoperHPO/-----> image !!! -background-remove-tool) repo to do the same thing just in an easier way and **validate** the similarity of the results. However, my issue is that the background removal is too strong for images like this:\n\n[Original Image](https://i.stack.imgur.com/heoNw.png)\n\nWhere I get the following result (i.e. ***packaging*** ***is*** ***also*** ***removed***): [After U2Net Image](https://i.stack.imgur.com/cPxyQ.png)\n\nIf I upload the image in [Foco clipping website](https://www.fococlipping.com/) and I select **Type=='Graphic,** I get **exactly the same results**. That means that the website is using the **same or similar** algorithm to remove the background for Graphic-type images. Nevertheless, if I select **Type=='Product**, then the result is the following and **is exactly what I want**:\n\n[Desired result which is obtained from Foco website](https://i.stack.imgur.com/PM8ji.png)", "link": "https://www.reddit.com/r/MachineLearning/comments/ppbaxg/background_removal_too_strong_when_using_u2net/"}, {"autor": "sustainabledude", "date": "2021-09-16 10:40:35", "content": "Goldspot Discoveries: Integrating Big Data &amp; Artificial Intelligence in the Mining Industry [Project] /!/ &amp;#x200B;\n\nIntroduction\n\nMining exploration is undergoing a revolution; only the mining industry hasn\u2019t realized it yet. The insurgents leading the revolution come from Goldspot Discoveries, a small Canadian technology company that is bringing AI/Machine Learning into the traditional geosciences field that has long dominated the exploration game. Still an early-stage company, Goldspot remains undervalued even as it has consistently outperformed expectations while delivering outstanding revenue growth and profitability. The company is well capitalized and has built a huge head start on any future competition that will take several years (at best) to close. While all of these factors would be a great story for any company, they aren\u2019t even the best part of the Goldspot story. Just like the group of MIT students used data analysis methodologies to stick it to the blackjack casinos (described in the book *Bringing Down the House* and the -----> film !!!  *21*), Goldspot is cracking the code for junior mining investment in order to rewrite the odds of the game in their favor. To put it simply, this is the moment where big data meets small miners \u2013 and one of the first hands Goldspot has played (New Found Gold - $NFG) has already given the house a good shake.\n\nCompany Overview\n\nGoldspot Discoveries started as a group of data scientists out of Quebec that competed in a mining technology challenge. Their approach to improving mining exploration was to utilize advanced data analysis techniques to improve the probability of drilling success. This group of scientist were connected with Denis Laviolette and Cejay Kim and together they turned this new technical approach into a functioning geoscience consulting business. Goldspot was officially formed in 2016, and then went public via a reverse merger in early 2019. For the last several fiscal quarters, Goldspot has seen revenues skyrocket (see charts in later sections), while building an investment portfolio that would make many far larger companies green with envy. Goldspot has been rewarded for its strong operational and financial performance with 8-fold gains in its stock price in the last 12 months (achieved on consistent and sustainable price gains). Looking forward, Goldspot has put teasers into the market of several new technologies, products in the r&amp;d pipeline, and strategic partnerships that should deliver strong shareholder value into the future.\n\nGoldspot is a technology and consultancy company that specializes in applying machine learning &amp; artificial intelligence to collect, interpret and convert data into so-called 'Smart Targets' that can pinpoint the exact location of potential new resource reserves.\n\nContrary to what the company\u2019s name would suggest, Goldspot specializes in finding almost any commodity reserve. Many of Goldspot's current clients are gold miners and silver miners, but there is also significant demand from the lithium, copper and base metal industry for Goldspot's services.\n\nThe search for new mineral reserves is not a new phenomenon. But with a dwindling number of new discoveries, the global supply chain is facing a decreasing supply and a rising demand. With fewer, smaller and more expensive discoveries, the mining sector is facing a problematic issue. Goldspot is tactically responding to this with the introduction of deep learning algorithms that can highlight and identify new innovative mathematical solutions and interpretations on geological patterns. Deep learning makes it possible to analyze an enormous amount of data that is virtually impossible for the human brain to analyze and interpret properly.\n\nThe new technologies have enabled Goldspot to produce results that were not possible before. This significantly increases the reliability &amp; accuracy of the targets. In addition to the extreme accuracy of Goldspot's results, Goldspot saves its clients a lot of time and money by letting software do the bulk of the work. By reducing costly human labor and time while improving results Goldspot has set up a compelling alternative to traditional methods and has proven to be a very attractive solution for mining companies.\n\nGoldspot's Business Model\n\n**Summary**\n\nOne of best way to analyze Goldspot\u2019s business is to look at it as a car. The AI/Machine Learning (discussed in detail below) is the engine and the rest of the business model is the vehicle that is built around that engine. At its core, Goldspot is a technology company. Upwards of 2/3rds of the its employees work on the technical side of the business \u2013 mostly out of its location in Montreal. Quebec is a highly underrated hotspot of engineering and technical talent, with several top universities nearby turning out top-notch talent.\n\nGoldspot has brought onboard well over 20 PhD\u2019s in diverse but related fields such as structural geology, geochemistry, geophysics, and complex data science and so forth. The beauty of Goldspot is its ability to integrate the results of these various fields into a simplified target analysis. Once its geoscience tools are built, Goldspot can then apply it on a wide-scale to create a competitive advantage for its clients.\n\nGoldspot's business model currently consists of 4 segments:\n\n1. Consultancy Services (Smart target generation)\n2. Investments\n3. Software as a Service (SaaS)\n4. Exploration (Golden Planet Mining)\n\nConsultancy Services\n\nGoldspot is advising mining entities for target generation to find new potential reserves with the aforementioned 'Smart Targets' that have been derived from the in-house AI &amp; machine learning algorithms. Goldspot concludes contracts with its clients that cover the costs incurred and generate an decent operating profit for Goldspot that are in line with the average margins in the consultancy sector.\n\nThe Consultancy turnover is growing by approximately 100% on an annual basis since Goldspot\u2019s inception and consists of time waged contracts with mining corporations. The mining corporations can be subdivided into two categories: Junior miners (small mining companies that do not yet have an operational mine, but do have a piece of land, a so called claim) and Majors (mining companies that already have at least one operational mine).\n\nGoldspot\u2019s Consultancy services are based on the in-house processing of historical and/or public big data sets of the client\u2019s claim. In combination with the use of Goldspot\u2019s AI and machine learning algorithms the company is able to extract correlations &amp; recognize patterns in the big data sets that provide sustained conclusions about a client\u2019s claim.\n\nGoldspot recently acquired the full-service field exploration firm \u2018Ridgeline Exploration\u2019 to provide a full-service solution to solve common inconsistent and piecemeal data collection practices that increase risk and lower efficiency across the industry. With the acquisition of Ridgeline, GoldSpot aims to strengthen its existing consultancy business as well as vertically integrate the ability to capture key data, including airborne geophysical survey mapping, geochemistry, structural mapping and geophysics. By improving the reliability of the big data sets Goldspot works with, the accuracy of the generated Smart Targets will considerably improve.\n\nGoldspot has provided consultancy services for big names in the sector like Yamana Gold, Fortuna Silver Mines, Sprott Mining and Vale to name a few, as well as countless Junior Miners among which are the names of New Found Gold, Critical Elements Lithium Corporation and Nevada King Gold Corp.\n\n&amp;#x200B;\n\nInvestments\n\nBut that is only the first part of the equation. By being able to evaluate large swaths of data over a vast geographical region Goldspot has an edge in being able to pick the potential winners and losers among junior mining exploration companies before everyone else. This is like having the cheat codes to the mining investment game.\n\nNow it is often so that Junior miners do not have enough working capital to engage Goldspot on their project. They are short on cash and cannot meet the requested rates from Goldspot. Goldspot has devised a smart solution for this particular problem. Before entering into a negotiation with the Junior miner, Goldspot analyzes the claim (territory) of the Junior miner using its in-house software &amp; deep learning algorithms and determines whether there is a significant probability that there is a promising reserve to be found on the claim based on their initial analysis. If that probability looks promising, Goldspot offers to take equity (shares and/or warrants) in the Junior Miner. They participate in a 'Private Placement' which allows the Junior Miner to raise money by issuing shares and/or warrants. As a result the Junior Miner can pay for Goldspot's consultancy services with the capital they raised. Every now and then Goldspot also negotiates for an NSR as part of a deal. An NSR, an abbreviation for \u2018Net Smelter Return\u2019, is a royalty on the net gold quantity a mine produces. These NSR\u2019s are often set around 0.5%-2%.\n\nBy applying this method, Goldspot has in addition to a consultancy contract, acquired an interest in the Junior miner. This creates a significant synergy when Goldspot delivers successful 'Smart Targets' and when these targets are also successfully tapped into. As a result the shares of the Junior Miner will increase significantly in value with Goldspot having a stake in the company.\n\nE.g. One of Goldspot's first clients was the junior miner New Found Gold. Goldspot had at the time entered into a contract with New Found Gold and acquired 1.7M shares and an NSR of 0.5% on the claim using the aforementioned method. At that time, the price of one NFG share was $0.40. After delivering the \"Smart Targets\" and successful drilling, which continues to this day, the stock has now increased in value by 30 times at a price of $12 per share. As a result, Goldspot's initial investment of $680,000 has appreciated to $20.4M, delivering an unrealized profit of $19.72M. The exact worth of the 0.5% NSR Goldspot acquired on NFG\u2019s claim is difficult to estimate until an exact resource estimation and a PFS (Pre-Feasibility Study) and PEA (Pre-Economic Study). Very likely though NFG's 0.5% NSR is worth something in the \"tens\" of millions but that is still very speculative. Now, certainly not every Junior Miner will be a success story like New Found Gold. However Goldspot\u2019s approach on investing in Junior Miners significantly de-risks their Junior Miner portfolio in comparison to other Junior Miner ETF\u2019s. Further elaboration on this in the next paragraph.\n\nGoldspot is very selective on what Junior Miners the company decides to engage with. To create the highest chance of success they\u2019ve established a data based approach on finding the best deals on the market. They have previously referred to this as \u2018Resource Quantamental\u2019 and they described it as an AI-driven opportunity generator pointing Goldspot to the ideal companies to work with. Resource Quantamental is the largest aggregate of mining data in the world and makes use of all the big data sets available on the capital markets, management compatibility and historical drilling results as well as geologic data. The output serves Goldspot with the ideal companies that have the highest chance of success when Goldspot engages with them.\n\nSoftware as a Service\n\nTo grow Goldspot\u2019s consultancy revenue the current team is becoming increasingly more efficient as they develop better skills &amp; internal work processes mature. Furthermore, the company is actively hiring more staff. However, this will pose a problem in the long run. Consultancy business is very employee intensive, and unlike other sectors, there is a great scarcity of people with degrees with the combination of Geology &amp; Data Science. For Goldspot however, people with these profiles are a necessity.\n\nTo overcome this problem and make the in-house technology scalable and lucrative, Goldspot has decided to develop &amp; acquire mining related SaaS products. Work is currently underway for the launch of two highly technical SaaS products that are expected to launch in Q4 2021.\n\nOne of those two new SaaS products is *LithoLens*, Goldspot\u2019s core imaging technology which adds value by extracting geological information from otherwise unused core photography, providing brand new data for 3D modelling and exploration purposes. Core logging is an essential step in the exploration process. It is the manual systematic recording and measuring of information from drill core to determine the lithology, mineralogy, geological history, structure and alteration zones. Due to being a highly manual repetitive task, core logging is subject to human error and rising costs. Litholens can work with imagery from historic and recent drill core photographs, to downhole optical and acoustic televiewer files, to videos of seafloor nodule deposits used in deep-sea mining. The data inserted in Litholens is processed in the cloud and the machine learning creates large and valuable new datasets from underutilized imagery and video data. The total addresable market for Litholens \u2013 the so called \u2018core logging market\u2019 currently has a turnover of around $50M per year, but Litholens ability to relog historical imagery could open up a opportunity closer to $500M.\n\nLithoLens is a software solution that once available to the mining sector, has the potential to provide Goldspot with an additional revenue stream that, unlike the majority of consulting services, is reoccurring in nature. As a result of this potential for significant recurring revenue, the LithoLens technology is a focal point of Goldspot\u2019s future growth.\n\nGoldspot recently enhanced its Litholens development by acquiring the tech company Geotic. Geotic\u2019s portfolio of 5 software tools (*GeoticMine, GeoticLog, GeoticGraph, GeoticCAD and GeoticField*) offer diversified 3D modelling and core-logging, improving the way that Geologists and Engineers collect and analyze data. The powerful combination of *GeoticLog* and *LithoLens* will create an industry-first core logging and AI imaging cloud solution.\n\nThe second SaaS product scheduled for launch in Q4 2021 is called \u2018MinusOne\u2019 and is a software solution for creating 3-D models from geophysical data processed using deterministic and stochastic inversion methods. With this technology, Goldspot's team also applies a ML (machine learning) method to provide probabilistic framework that helps to analyze uncertainty. The processed results guide exploration drilling locations and geology interpretation.\n\nGoldspot has now over 40 research and development products currently in development and is now at the point where many of these tools can be monetized on a much larger scale as external software solutions.\n\nCEO.CA acquisition \u2013 the next birthplace for Junior Miners &amp; mining companies\n\nOn the 16th August 2021 Goldspot announced their intent to acquire the social investor network CEO.CA. CEO.CA is an international investor website with the focus on Canadian listed companies. Canada is the global hub for mining entities to seek financing and there are thousands of global mining companies listed on the TSX-V exchange (Toronto Stock Exchange).\n\n\"GoldSpot's focus is to unlock value in mineral exploration with data science and machine learning and we are proud to serve more than fifty global exploration companies in this endeavour. The acquisition of CEO.CA, and the establishment of a technology and media division, is strategic to that vision and provides significant economic potential to our clients and shareholders,\" commented GoldSpot Executive Chairman &amp; President Denis Laviolette.\n\nGolden Planet Mining\n\nAside from taking equity in third parties, Goldspot has decided to leverage their own tech &amp; brand awareness to set up their own exploration company: Golden Planet Mining (GPM). Formerly known as XCorp AI they merged with Saskatchewan Gold Corp. to acquire the \u2018Mammoth Gold Project\u2019 situated in Saskatchewan, Canada, and form the combined entity Golden Planet Mining. Originally Goldspot invested $475K cad in XCorp AI. Their equity in the combined entity GPM appreciated to $7.78M cad following the next raise, and has since ballooned to $15,53M in the most recent raise to date. At the moment Goldspot holds 28% of Golden Planet Mining with the company looking to go public in late 2021 or early 2022. Following the merger with Saskatchewan Gold Corp, Golden planet Mining has acquired ownership of the Olympus Gold Project in the Northwest Territories and the Rider Gold Project in British Columbia and commenced drilling activities &amp; regional exploration on the properties. Using Goldspot\u2019s in-house exploration and data processing tech the claims are being further explored and drilled for more data. If good results appear GPM has a good probability of becoming a major success story for Goldspot.\n\nHow AI and Machine Learning Creates a Competitive Advantage\n\nGood companies sell a quality product, great companies solve a problem, but the very best companies solve a problem before you even know you have one. The metal/non-metal mining industry has a problem it is either unaware of, or refuses to admit. The problem: the exploration industry is unable to find new big deposits that can be economically extracted without undue environmental damage in order to replace depleted reserves of current major mining companies.\n\nExploration by junior mining is one of the great perpetual boom and bust industries. Every year, fortunes are made and lost betting on junior mining companies on the TSX Venture Exchange. Junior miners are typically smaller companies (most with a market cap well under $50 million) that secure land and mining claims in hopes of finding the next big mineral find. When a company makes a big find, the rewards are incredibly lucrative, that can in the tens of thousands of percent gains big. For years, the market unduly rewarded junior miners who found massive mineral deposits. Seemingly, the only metric that mattered was coming up with a big P&amp;P reserve in the tens of millions of ounces.\n\nBut the sad reality is that an overwhelming majority of junior mining companies fail well before mining operations can even begin on these massive mineral finds. At any given time, there are 3,000+ active junior miners trading on the TSX-V exchange. Although most junior miners experience major shifts up and down in share price, it is very rare to get a big win long-term gainer among the junior mining companies. When long-term wins do happen, it is like lightning in a bottle \u2013 with sufficient gains to make up for a large number of losses. The key to being a long-term winner is cost of extraction, not volume. For example, an operation with 2 million ounces of gold that be extracted at an AISC cost of $350 is far more attractive of an acquisition target for a major mining company than an operation with 30 million ounces of gold that can be extracted for $1,100 an ounce.\n\nSo why are so many junior mining companies fail? It isn\u2019t necessarily that the companies have bad properties or don\u2019t find valuable minerals (although that is possible). The reality is that junior mining companies typically fail because they simply run out of money. In order to develop a mining project all the way to production (or acquisition by a major), the junior mining company needs to find the right deposit, with the right economics, with the right timing and all within its limited financial resources. If the company has money to institute a drilling program, the junior mining company usually only has one chance to get it right and obtain a good drill result. If it does, stock price goes up and the junior miner is able to an equity raise sufficient to move on to the next stage of the process. Any faltering along the way, and the entire project (and usually the company itself) goes up in flames. Truly, it creates a \u201cdiscover or die\u201d environment. Additionally, that discover needs to be in a location and grade that suggests that it can be removed economically.\n\nWhile this environment of \u201cdiscover or die\u201d has always existed for junior miners, there are two more recent industry trends/conditions that are making it even tougher for juniors to succeed. First, mineral finds are become harder and harder to locate. The minerals being found are greater depths or in environments with less surface indicators. This means that there are less easy finds available waiting to be discovered. The second trend is for junior miners to pursue \u201cdistrict-scale\u201d projects. Investors in junior miners are generally looking for companies with huge land holdings so that if there is a find, the potential payouts can be exponentially greater. At the same time, most mining laws around the world require a claim holder to invest a certain amount of money on an annual basis in developing a claim as well as pay a claim fee. These costs are not trivial and can total several million dollars on an annual basis for junior miners with large land packages. So with these additional factors, junior miners are having to look for a smaller needle in a bigger haystack that costs more money to maintain.\n\nIn this environment smaller needles in bigger haystacks, Goldspot is solving the budding problem by building a better mouse trap. With diminishing surface indicators, mining exploration companies need to effectively integrate data obtained from multiple sources. Each time you stack a different level of data on top of another set of data, the complexity does not increase linearly, rather it increases exponentially. The beauty of AI and machine learning tools is that they are able look at data from a potentially limitless pool of data and find subtle geological, geophysical or other structural indicators that collectively increase the likelihood for success. The Goldspot AI tool finds regions or structures where probability of mineralization are higher, but once a general region has been identified, the same tools are also able to narrow in on specific targets to confirm mineralization on a claim that provide a substantially higher likelihood of finding mineralization.\n\nRisks to the business\n\nEach business carries risks along with their operations. The main risks to Goldspot\u2019s business model are listed below:\n\n**Cash Flows From Consulting Income**\n\nGoldspot currently generates revenue and cash flows from its consulting services. The availability of these sources of funds and Goldspot's ability to maintain a network and attract additional customers will depend on a number of factors, many of which are outside of Goldspot's control. A significant portion of Goldspot's revenues have come from four customers in short-term contracts. Goldspot's contracts are generally short-term and it is actively seeking to diversify its customer base with longer-term contracts. The loss of any one of its customers or the inability to attract additional customers will result in a material adverse effect on the business and may adversely affect revenues going forward.\n\n**Intellectual Property Risk**\n\nThe ownership and protection of Goldspot's intellectual property rights is a significant aspect of Goldspot's future success. Currently, Goldspot relies on trade secrets, technical know-how and proprietary information to protect its intellectual property. Goldspot also attempts to protects its intellectual property by entering into confidentiality agreements with parties that have access to it, such as business partners, collaborators, employees and consultants. Any of these parties may breach these agreements and Goldspot may not have adequate remedies for any specific breach. In addition, Goldspot's trade secrets and technical know-how, which are not protected by patents, may otherwise become known to or be independently developed by competitors, in which event Goldspot's business, financial condition and results of operation could be materially affected. An assertion by a third-party that Goldspot is infringing its intellectual property could subject Goldspot to costly and time-consuming litigation, which could harm its business. Goldspot's success depends in part upon it not infringing the intellectual property rights of others. However, Goldspot's competitors, as well as a number of other entities and individuals, may own or claim to own intellectual property relating to Goldspot's industry or, in some cases, its technology. Goldspot has not been subjected to any claims of intellectual property rights infringements in the past but as it develops more of its own applications and meets additional client specific requests, it may be exposed to greater risk in the future. Any claims or litigation could cause Goldspot to incur significant expenses, and if successfully asserted against Goldspot, could require that Goldspot pay substantial damages or ongoing revenue share payments, indemnify its customers or distributors, obtain licenses, modify products, or refund fees, any of which would deplete Goldspot's resources and adversely impact its business.\n\n**Investment Risks**\n\nThrough its investing division, Goldspot may acquire securities of public and private companies from time to time, which are primarily junior or small-cap companies. Poor investment performance could impair revenues and growth. The market values of the securities can experience significant fluctuations in the short and long term due to factors beyond Goldspot's control. Market value can be reflective of the actual or anticipated operating results of the companies and/or the general market conditions in a specific sector as a whole, such as fluctuations in commodity prices and global political and economic conditions. Goldspot's investments will be carried at fair value, and unrealized gains/losses on the securities and realized losses on the securities sold could have a material adverse impact on Goldspot's operating results. There is no assurance that Goldspot will be able to achieve or maintain any particular level of investment return, which may have a material adverse impact on its ability to attract investors. Furthermore, the junior mining space tends to be more volatile than the general market indices. This volatility combined with negative or poor performance could combine to lead to a reduction in investor interest.\n\n**Cyclical Downturn**\n\nA significant operating risk affecting Goldspot is a downturn in demand for its services due a decrease in activity in the mining industry. A severe and persistent downturn in the mining industry would have severe consequences on the business of Goldspot. In many cases, capital markets are the only source of funds available to junior mining companies and any change in the outlook for the sector or the lack of success of a specific exploration program can quickly impair the ability of these juniors to raise capital to pay for their consulting services.\n\n**Limited Operating History**\n\nGoldspot began carrying on business in 2017 and started to generate significant revenue from its operations beginning in December 31, 2018. Goldspot is therefore subject to many of the risks common to early stage enterprises, including under-capitalization, cash shortages, limitations with respect to personnel, financial, and other resources and limited revenues. There is no assurance that Goldspot will be successful in achieving a return on shareholders' investment and the likelihood of success must be considered in light of the early stage of operations.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\n\"The chairman of Barrick Gold Corp. made a bold prediction late last year: With the help of artificial intelligence and other digital tools, the world\u2019s largest gold miner would become a technology company that just happened to be in mining.\"", "link": "https://www.reddit.com/r/MachineLearning/comments/ppat52/goldspot_discoveries_integrating_big_data/"}, {"autor": "sustainabledude", "date": "2021-09-16 09:27:38", "content": "Goldspot Discoveries: Integrating Big Data &amp; Artificial Intelligence in the Mineral Exploration Game [Project] &amp; [Research] /!/  \n\n# \n\n&amp;#x200B;\n\nhttps://preview.redd.it/njc0ha603un71.png?width=605&amp;format=png&amp;auto=webp&amp;s=ecad5d0525678c33c9f11d319a772edbf1f26b1d\n\nGeneral Information\n\n(as of 14 September 2021)\n\nMarket Cap (intraday) 111.75M\n\nEnterprise Value 60.42M\n\nTrailing P/E 3.50\n\nPrice/Book (mrq) 1.92\n\nEnterprise Value/Revenue 7.97\n\nShares Outstanding 121.47M\n\n% Held by Insiders 54.21%\n\nIntroduction\n\nMining exploration is undergoing a revolution; only the mining industry hasn\u2019t realized it yet. The insurgents leading the revolution come from Goldspot Discoveries, a small Canadian technology company that is bringing AI/Machine Learning into the traditional geosciences field that has long dominated the exploration game. Still an early-stage company, Goldspot remains undervalued even as it has consistently outperformed expectations while delivering outstanding revenue growth and profitability. The company is well capitalized and has built a huge head start on any future competition that will take several years (at best) to close. While all of these factors would be a great story for any company, they aren\u2019t even the best part of the Goldspot story. Just like the group of MIT students used data analysis methodologies to stick it to the blackjack casinos (described in the book *Bringing Down the House* and the -----> film !!!  *21*), Goldspot is cracking the code for junior mining investment in order to rewrite the odds of the game in their favor. To put it simply, this is the moment where big data meets small miners \u2013 and one of the first hands Goldspot has played (New Found Gold - $NFG) has already given the house a good shake.\n\nCompany Overview\n\nGoldspot Discoveries started as a group of data scientists out of Quebec that competed in a mining technology challenge. Their approach to improving mining exploration was to utilize advanced data analysis techniques to improve the probability of drilling success. This group of scientist were connected with Denis Laviolette and Cejay Kim and together they turned this new technical approach into a functioning geoscience consulting business. Goldspot was officially formed in 2016, and then went public via a reverse merger in early 2019. For the last several fiscal quarters, Goldspot has seen revenues skyrocket (see charts in later sections), while building an investment portfolio that would make many far larger companies green with envy. Goldspot has been rewarded for its strong operational and financial performance with 8-fold gains in its stock price in the last 12 months (achieved on consistent and sustainable price gains). Looking forward, Goldspot has put teasers into the market of several new technologies, products in the r&amp;d pipeline, and strategic partnerships that should deliver strong shareholder value into the future.\n\nGoldspot is a technology and consultancy company that specializes in applying machine learning &amp; artificial intelligence to collect, interpret and convert data into so-called 'Smart Targets' that can pinpoint the exact location of potential new resource reserves.\n\nContrary to what the company\u2019s name would suggest, Goldspot specializes in finding almost any commodity reserve. Many of Goldspot's current clients are gold miners and silver miners, but there is also significant demand from the lithium, copper and base metal industry for Goldspot's services.\n\nThe search for new mineral reserves is not a new phenomenon. But with a dwindling number of new discoveries, the global supply chain is facing a decreasing supply and a rising demand. With fewer, smaller and more expensive discoveries, the mining sector is facing a problematic issue. Goldspot is tactically responding to this with the introduction of deep learning algorithms that can highlight and identify new innovative mathematical solutions and interpretations on geological patterns. Deep learning makes it possible to analyze an enormous amount of data that is virtually impossible for the human brain to analyze and interpret properly.\n\nThe new technologies have enabled Goldspot to produce results that were not possible before. This significantly increases the reliability &amp; accuracy of the targets. In addition to the extreme accuracy of Goldspot's results, Goldspot saves its clients a lot of time and money by letting software do the bulk of the work. By reducing costly human labor and time while improving results Goldspot has set up a compelling alternative to traditional methods and has proven to be a very attractive solution for mining companies.\n\nGoldspot's Business Model\n\n**Summary**\n\nOne of best way to analyze Goldspot\u2019s business is to look at it as a car. The AI/Machine Learning (discussed in detail below) is the engine and the rest of the business model is the vehicle that is built around that engine. At its core, Goldspot is a technology company. Upwards of 2/3rds of the its employees work on the technical side of the business \u2013 mostly out of its location in Montreal. Quebec is a highly underrated hotspot of engineering and technical talent, with several top universities nearby turning out top-notch talent.\n\nGoldspot has brought onboard well over 20 PhD\u2019s in diverse but related fields such as structural geology, geochemistry, geophysics, and complex data science and so forth. The beauty of Goldspot is its ability to integrate the results of these various fields into a simplified target analysis. Once its geoscience tools are built, Goldspot can then apply it on a wide-scale to create a competitive advantage for its clients.\n\nGoldspot's business model currently consists of 4 segments:\n\n1. Consultancy Services (Smart target generation)\n2. Investments\n3. Software as a Service (SaaS)\n4. Exploration (Golden Planet Mining)\n\nConsultancy Services\n\nGoldspot is advising mining entities for target generation to find new potential reserves with the aforementioned 'Smart Targets' that have been derived from the in-house AI &amp; machine learning algorithms. Goldspot concludes contracts with its clients that cover the costs incurred and generate an decent operating profit for Goldspot that are in line with the average margins in the consultancy sector.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/txhz44023un71.png?width=605&amp;format=png&amp;auto=webp&amp;s=683157c7b380c5a0d544942f7e91c6e8259c7b31\n\n*Figure 1: Growth of Goldspot's Consultancy revenue*\n\nThe Consultancy turnover is growing by approximately 100% on an annual basis since Goldspot\u2019s inception and consists of time waged contracts with mining corporations. The mining corporations can be subdivided into two categories: Junior miners (small mining companies that do not yet have an operational mine, but do have a piece of land, a so called claim) and Majors (mining companies that already have at least one operational mine).\n\nGoldspot\u2019s Consultancy services are based on the in-house processing of historical and/or public big data sets of the client\u2019s claim. In combination with the use of Goldspot\u2019s AI and machine learning algorithms the company is able to extract correlations &amp; recognize patterns in the big data sets that provide sustained conclusions about a client\u2019s claim.\n\nGoldspot recently acquired the full-service field exploration firm \u2018Ridgeline Exploration\u2019 to provide a full-service solution to solve common inconsistent and piecemeal data collection practices that increase risk and lower efficiency across the industry. With the acquisition of Ridgeline, GoldSpot aims to strengthen its existing consultancy business as well as vertically integrate the ability to capture key data, including airborne geophysical survey mapping, geochemistry, structural mapping and geophysics. By improving the reliability of the big data sets Goldspot works with, the accuracy of the generated Smart Targets will considerably improve.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/cz8ouao23un71.png?width=605&amp;format=png&amp;auto=webp&amp;s=b70601b159e2c11de720e23d24dfecfc51336a9c\n\n*Figure 2 Vertical Integration using Ridgeline Exploration Services*\n\nGoldspot has provided consultancy services for big names in the sector like Yamana Gold, Fortuna Silver Mines, Sprott Mining and Vale to name a few, as well as countless Junior Miners among which are the names of New Found Gold, Critical Elements Lithium Corporation and Nevada King Gold Corp.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/307b9b933un71.png?width=605&amp;format=png&amp;auto=webp&amp;s=48487637b172fc8de15a98ea1d8152758a5550a7\n\nInvestments\n\nBut that is only the first part of the equation. By being able to evaluate large swaths of data over a vast geographical region Goldspot has an edge in being able to pick the potential winners and losers among junior mining exploration companies before everyone else. This is like having the cheat codes to the mining investment game.\n\nNow it is often so that Junior miners do not have enough working capital to engage Goldspot on their project. They are short on cash and cannot meet the requested rates from Goldspot. Goldspot has devised a smart solution for this particular problem. Before entering into a negotiation with the Junior miner, Goldspot analyzes the claim (territory) of the Junior miner using its in-house software &amp; deep learning algorithms and determines whether there is a significant probability that there is a promising reserve to be found on the claim based on their initial analysis. If that probability looks promising, Goldspot offers to take equity (shares and/or warrants) in the Junior Miner. They participate in a 'Private Placement' which allows the Junior Miner to raise money by issuing shares and/or warrants. As a result the Junior Miner can pay for Goldspot's consultancy services with the capital they raised. Every now and then Goldspot also negotiates for an NSR as part of a deal. An NSR, an abbreviation for \u2018Net Smelter Return\u2019, is a royalty on the net gold quantity a mine produces. These NSR\u2019s are often set around 0.5%-2%.\n\nBy applying this method, Goldspot has in addition to a consultancy contract, acquired an interest in the Junior miner. This creates a significant synergy when Goldspot delivers successful 'Smart Targets' and when these targets are also successfully tapped into. As a result the shares of the Junior Miner will increase significantly in value with Goldspot having a stake in the company.\n\nE.g. One of Goldspot's first clients was the junior miner New Found Gold. Goldspot had at the time entered into a contract with New Found Gold and acquired 1.7M shares and an NSR of 0.5% on the claim using the aforementioned method. At that time, the price of one NFG share was $0.40. After delivering the \"Smart Targets\" and successful drilling, which continues to this day, the stock has now increased in value by 30 times at a price of $12 per share. As a result, Goldspot's initial investment of $680,000 has appreciated to $20.4M, delivering an unrealized profit of $19.72M. The exact worth of the 0.5% NSR Goldspot acquired on NFG\u2019s claim is difficult to estimate until an exact resource estimation and a PFS (Pre-Feasibility Study) and PEA (Pre-Economic Study). Very likely though NFG's 0.5% NSR is worth something in the \"tens\" of millions but that is still very speculative. Now, certainly not every Junior Miner will be a success story like New Found Gold. However Goldspot\u2019s approach on investing in Junior Miners significantly de-risks their Junior Miner portfolio in comparison to other Junior Miner ETF\u2019s. Further elaboration on this in the next paragraph.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/qin6e0z33un71.png?width=598&amp;format=png&amp;auto=webp&amp;s=94d6e21bf66afae334471c23301a1e9447d18295\n\n*Figure 3: New Found Gold equity position when Goldspot entered the agreement in 2019 and in 2021 after Goldspot successfully delivered numerous drill targets that where successfully tapped into.*\n\nGoldspot is very selective on what Junior Miners the company decides to engage with. To create the highest chance of success they\u2019ve established a data based approach on finding the best deals on the market. They have previously referred to this as \u2018Resource Quantamental\u2019 and they described it as an AI-driven opportunity generator pointing Goldspot to the ideal companies to work with. Resource Quantamental is the largest aggregate of mining data in the world and makes use of all the big data sets available on the capital markets, management compatibility and historical drilling results as well as geologic data. The output serves Goldspot with the ideal companies that have the highest chance of success when Goldspot engages with them.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/v1olwgl43un71.png?width=605&amp;format=png&amp;auto=webp&amp;s=e79fa25fffa64bf8eebd91da0cad06f364087eee\n\n*Figure 4: Investment Performance of Goldspot\u2019s portfolio compared to the GDXJ (VanEck Vectors Junior Gold Miners ETF) and the TSX.*\n\n&amp;#x200B;\n\nhttps://preview.redd.it/3ujslx653un71.png?width=605&amp;format=png&amp;auto=webp&amp;s=9326242c8825c0964140f06f3a32b0166203810b\n\n*Figure 6: Selection of Goldspot\u2019s equity positions in Junior Miners &amp; Royalty Positions. Note: This is not the whole portfolio only a few selected positions. The whole portfolio has not been made public (yet) due to competitive reasons as mentioned by the company.*\n\nSoftware as a Service\n\nTo grow Goldspot\u2019s consultancy revenue the current team is becoming increasingly more efficient as they develop better skills &amp; internal work processes mature. Furthermore, the company is actively hiring more staff. However, this will pose a problem in the long run. Consultancy business is very employee intensive, and unlike other sectors, there is a great scarcity of people with degrees with the combination of Geology &amp; Data Science. For Goldspot however, people with these profiles are a necessity.\n\nTo overcome this problem and make the in-house technology scalable and lucrative, Goldspot has decided to develop &amp; acquire mining related SaaS products. Work is currently underway for the launch of two highly technical SaaS products that are expected to launch in Q4 2021.\n\nOne of those two new SaaS products is *LithoLens*, Goldspot\u2019s core imaging technology which adds value by extracting geological information from otherwise unused core photography, providing brand new data for 3D modelling and exploration purposes. Core logging is an essential step in the exploration process. It is the manual systematic recording and measuring of information from drill core to determine the lithology, mineralogy, geological history, structure and alteration zones. Due to being a highly manual repetitive task, core logging is subject to human error and rising costs. Litholens can work with imagery from historic and recent drill core photographs, to downhole optical and acoustic televiewer files, to videos of seafloor nodule deposits used in deep-sea mining. The data inserted in Litholens is processed in the cloud and the machine learning creates large and valuable new datasets from underutilized imagery and video data. The total addresable market for Litholens \u2013 the so called \u2018core logging market\u2019 currently has a turnover of around $50M per year, but Litholens ability to relog historical imagery could open up a opportunity closer to $500M.\n\nLithoLens is a software solution that once available to the mining sector, has the potential to provide Goldspot with an additional revenue stream that, unlike the majority of consulting services, is reoccurring in nature. As a result of this potential for significant recurring revenue, the LithoLens technology is a focal point of Goldspot\u2019s future growth.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/xbc3cqv53un71.png?width=194&amp;format=png&amp;auto=webp&amp;s=4ab6ee294fd0110024dfc23bbc7ca3c73965a488\n\ncore logging\n\n&amp;#x200B;\n\nhttps://preview.redd.it/il6i5rd63un71.png?width=326&amp;format=png&amp;auto=webp&amp;s=db083a73df0d0a88c424ed1c4678467ad4cb9347\n\ncore logging analytics\n\nGoldspot recently enhanced its Litholens development by acquiring the tech company Geotic. Geotic\u2019s portfolio of 5 software tools (*GeoticMine, GeoticLog, GeoticGraph, GeoticCAD and GeoticField*) offer diversified 3D modelling and core-logging, improving the way that Geologists and Engineers collect and analyze data. The powerful combination of *GeoticLog* and *LithoLens* will create an industry-first core logging and AI imaging cloud solution.\n\nThe second SaaS product scheduled for launch in Q4 2021 is called \u2018MinusOne\u2019 and is a software solution for creating 3-D models from geophysical data processed using deterministic and stochastic inversion methods. With this technology, Goldspot's team also applies a ML (machine learning) method to provide probabilistic framework that helps to analyze uncertainty. The processed results guide exploration drilling locations and geology interpretation.\n\nGoldspot has now over 40 research and development products currently in development and is now at the point where many of these tools can be monetized on a much larger scale as external software solutions.\n\nCEO.CA acquisition \u2013 the next birthplace for Junior Miners &amp; mining companies\n\nOn the 16th August 2021 Goldspot announced their intent to acquire the social investor network CEO.CA. CEO.CA is an international investor website with the focus on Canadian listed companies. Canada is the global hub for mining entities to seek financing and there are thousands of global mining companies listed on the TSX-V exchange (Toronto Stock Exchange).\n\n\"GoldSpot's focus is to unlock value in mineral exploration with data science and machine learning and we are proud to serve more than fifty global exploration companies in this endeavour. The acquisition of CEO.CA, and the establishment of a technology and media division, is strategic to that vision and provides significant economic potential to our clients and shareholders,\" commented GoldSpot Executive Chairman &amp; President Denis Laviolette.\n\nGolden Planet Mining\n\nAside from taking equity in third parties, Goldspot has decided to leverage their own tech &amp; brand awareness to set up their own exploration company: Golden Planet Mining (GPM). Formerly known as XCorp AI they merged with Saskatchewan Gold Corp. to acquire the \u2018Mammoth Gold Project\u2019 situated in Saskatchewan, Canada, and form the combined entity Golden Planet Mining. Originally Goldspot invested $475K cad in XCorp AI. Their equity in the combined entity GPM appreciated to $7.78M cad following the next raise, and has since ballooned to $15,53M in the most recent raise to date. At the moment Goldspot holds 28% of Golden Planet Mining with the company looking to go public in late 2021 or early 2022. Following the merger with Saskatchewan Gold Corp, Golden planet Mining has acquired ownership of the Olympus Gold Project in the Northwest Territories and the Rider Gold Project in British Columbia and commenced drilling activities &amp; regional exploration on the properties. Using Goldspot\u2019s in-house exploration and data processing tech the claims are being further explored and drilled for more data. If good results appear GPM has a good probability of becoming a major success story for Goldspot.\n\nHow AI and Machine Learning Creates a Competitive Advantage\n\nGood companies sell a quality product, great companies solve a problem, but the very best companies solve a problem before you even know you have one. The metal/non-metal mining industry has a problem it is either unaware of, or refuses to admit. The problem: the exploration industry is unable to find new big deposits that can be economically extracted without undue environmental damage in order to replace depleted reserves of current major mining companies.\n\nExploration by junior mining is one of the great perpetual boom and bust industries. Every year, fortunes are made and lost betting on junior mining companies on the TSX Venture Exchange. Junior miners are typically smaller companies (most with a market cap well under $50 million) that secure land and mining claims in hopes of finding the next big mineral find. When a company makes a big find, the rewards are incredibly lucrative, that can in the tens of thousands of percent gains big. For years, the market unduly rewarded junior miners who found massive mineral deposits. Seemingly, the only metric that mattered was coming up with a big P&amp;P reserve in the tens of millions of ounces.\n\nBut the sad reality is that an overwhelming majority of junior mining companies fail well before mining operations can even begin on these massive mineral finds. At any given time, there are 3,000+ active junior miners trading on the TSX-V exchange. Although most junior miners experience major shifts up and down in share price, it is very rare to get a big win long-term gainer among the junior mining companies. When long-term wins do happen, it is like lightning in a bottle \u2013 with sufficient gains to make up for a large number of losses. The key to being a long-term winner is cost of extraction, not volume. For example, an operation with 2 million ounces of gold that be extracted at an AISC cost of $350 is far more attractive of an acquisition target for a major mining company than an operation with 30 million ounces of gold that can be extracted for $1,100 an ounce.\n\nSo why are so many junior mining companies fail? It isn\u2019t necessarily that the companies have bad properties or don\u2019t find valuable minerals (although that is possible). The reality is that junior mining companies typically fail because they simply run out of money. In order to develop a mining project all the way to production (or acquisition by a major), the junior mining company needs to find the right deposit, with the right economics, with the right timing and all within its limited financial resources. If the company has money to institute a drilling program, the junior mining company usually only has one chance to get it right and obtain a good drill result. If it does, stock price goes up and the junior miner is able to an equity raise sufficient to move on to the next stage of the process. Any faltering along the way, and the entire project (and usually the company itself) goes up in flames. Truly, it creates a \u201cdiscover or die\u201d environment. Additionally, that discover needs to be in a location and grade that suggests that it can be removed economically.\n\nWhile this environment of \u201cdiscover or die\u201d has always existed for junior miners, there are two more recent industry trends/conditions that are making it even tougher for juniors to succeed. First, mineral finds are become harder and harder to locate. The minerals being found are greater depths or in environments with less surface indicators. This means that there are less easy finds available waiting to be discovered. The second trend is for junior miners to pursue \u201cdistrict-scale\u201d projects. Investors in junior miners are generally looking for companies with huge land holdings so that if there is a find, the potential payouts can be exponentially greater. At the same time, most mining laws around the world require a claim holder to invest a certain amount of money on an annual basis in developing a claim as well as pay a claim fee. These costs are not trivial and can total several million dollars on an annual basis for junior miners with large land packages. So with these additional factors, junior miners are having to look for a smaller needle in a bigger haystack that costs more money to maintain.\n\nIn this environment smaller needles in bigger haystacks, Goldspot is solving the budding problem by building a better mouse trap. With diminishing surface indicators, mining exploration companies need to effectively integrate data obtained from multiple sources. Each time you stack a different level of data on top of another set of data, the complexity does not increase linearly, rather it increases exponentially. The beauty of AI and machine learning tools is that they are able look at data from a potentially limitless pool of data and find subtle geological, geophysical or other structural indicators that collectively increase the likelihood for success. The Goldspot AI tool finds regions or structures where probability of mineralization are higher, but once a general region has been identified, the same tools are also able to narrow in on specific targets to confirm mineralization on a claim that provide a substantially higher likelihood of finding mineralization.\n\nFinancials\n\n&amp;#x200B;\n\nhttps://preview.redd.it/9a629dc73un71.png?width=633&amp;format=png&amp;auto=webp&amp;s=e2b6993834100006633d0e729d6e432934b79709\n\nRisks to the business\n\nEach business carries risks along with their operations. The main risks to Goldspot\u2019s business model are listed below:\n\n**Cash Flows From Consulting Income**\n\nGoldspot currently generates revenue and cash flows from its consulting services. The availability of these sources of funds and Goldspot's ability to maintain a network and attract additional customers will depend on a number of factors, many of which are outside of Goldspot's control. A significant portion of Goldspot's revenues have come from four customers in short-term contracts. Goldspot's contracts are generally short-term and it is actively seeking to diversify its customer base with longer-term contracts. The loss of any one of its customers or the inability to attract additional customers will result in a material adverse effect on the business and may adversely affect revenues going forward.\n\n**Intellectual Property Risk**\n\nThe ownership and protection of Goldspot's intellectual property rights is a significant aspect of Goldspot's future success. Currently, Goldspot relies on trade secrets, technical know-how and proprietary information to protect its intellectual property. Goldspot also attempts to protects its intellectual property by entering into confidentiality agreements with parties that have access to it, such as business partners, collaborators, employees and consultants. Any of these parties may breach these agreements and Goldspot may not have adequate remedies for any specific breach. In addition, Goldspot's trade secrets and technical know-how, which are not protected by patents, may otherwise become known to or be independently developed by competitors, in which event Goldspot's business, financial condition and results of operation could be materially affected. An assertion by a third-party that Goldspot is infringing its intellectual property could subject Goldspot to costly and time-consuming litigation, which could harm its business. Goldspot's success depends in part upon it not infringing the intellectual property rights of others. However, Goldspot's competitors, as well as a number of other entities and individuals, may own or claim to own intellectual property relating to Goldspot's industry or, in some cases, its technology. Goldspot has not been subjected to any claims of intellectual property rights infringements in the past but as it develops more of its own applications and meets additional client specific requests, it may be exposed to greater risk in the future. Any claims or litigation could cause Goldspot to incur significant expenses, and if successfully asserted against Goldspot, could require that Goldspot pay substantial damages or ongoing revenue share payments, indemnify its customers or distributors, obtain licenses, modify products, or refund fees, any of which would deplete Goldspot's resources and adversely impact its business.\n\n**Investment Risks**\n\nThrough its investing division, Goldspot may acquire securities of public and private companies from time to time, which are primarily junior or small-cap companies. Poor investment performance could impair revenues and growth. The market values of the securities can experience significant fluctuations in the short and long term due to factors beyond Goldspot's control. Market value can be reflective of the actual or anticipated operating results of the companies and/or the general market conditions in a specific sector as a whole, such as fluctuations in commodity prices and global political and economic conditions. Goldspot's investments will be carried at fair value, and unrealized gains/losses on the securities and realized losses on the securities sold could have a material adverse impact on Goldspot's operating results. There is no assurance that Goldspot will be able to achieve or maintain any particular level of investment return, which may have a material adverse impact on its ability to attract investors. Furthermore, the junior mining space tends to be more volatile than the general market indices. This volatility combined with negative or poor performance could combine to lead to a reduction in investor interest.\n\n**Cyclical Downturn**\n\nA significant operating risk affecting Goldspot is a downturn in demand for its services due a decrease in activity in the mining industry. A severe and persistent downturn in the mining industry would have severe consequences on the business of Goldspot. In many cases, capital markets are the only source of funds available to junior mining companies and any change in the outlook for the sector or the lack of success of a specific exploration program can quickly impair the ability of these juniors to raise capital to pay for their consulting services.\n\n**Limited Operating History**\n\nGoldspot began carrying on business in 2017 and started to generate significant revenue from its operations beginning in December 31, 2018. Goldspot is therefore subject to many of the risks common to early stage enterprises, including under-capitalization, cash shortages, limitations with respect to personnel, financial, and other resources and limited revenues. There is no assurance that Goldspot will be successful in achieving a return on shareholders' investment and the likelihood of success must be considered in light of the early stage of operations.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\n\"The chairman of Barrick Gold Corp. made a bold prediction late last year: With the help of artificial intelligence and other digital tools, the world\u2019s largest gold miner would become a technology company that just happened to be in mining.\"\n\n**If you have a background in software analytics / AI / deep and/or machine learning feel free to contact Goldspot for career opportunities:** [**https://goldspot.ca/contact-us/**](https://goldspot.ca/contact-us/)", "link": "https://www.reddit.com/r/MachineLearning/comments/pp9x9h/goldspot_discoveries_integrating_big_data/"}, {"autor": "techsucker", "date": "2021-09-16 03:31:38", "content": "[R] Google AI Introduces Full-Attention Cross-Modal Transformer (FACT) Model And A New 3D Dance Dataset AIST++ /!/ Dance has always been a significant part of human culture, rituals, and celebrations, as well as a means of self-expression. Today, there exists many forms of dance, from ballroom to disco. Dancing, however, is an art form that needs practice. Professional training is typically required to create expressive choreography for a dancer with a repertory of dance movements. Although this process is difficult for people, it is considerably more difficult for an ML model as the task involves producing a continuous motion with high cinematic complexity and the non-linear relationship between the movements and the accompanying music.\u00a0\n\nA new Google study introduces the full-attention cross-modal Transformer (FACT) model, which can mimic and understand dance motions and even improve a person\u2019s ability to choreograph dance. In addition to this, the team released AIST++, a large-scale, multi-modal 3D dance motion dataset. This dataset contains 5.2 hours of 3D dance motion in 1408 sequences spanning ten dance genres, each with multi-view videos and known -----> camera !!!  poses. Their findings suggest that the FACT model outperforms current state-of-the-art methods in extensive user studies on AIST++.\n\n# [7 Min Read](https://www.marktechpost.com/2021/09/15/google-ai-introduces-full-attention-cross-modal-transformer-fact-model-and-a-new-3d-dance-dataset-aist/) | [Paper](https://arxiv.org/abs/2101.08779) | [Project](https://google.github.io/aichoreographer/) | [GitHub](https://github.com/google-research/mint)| [Dataset](https://google.github.io/aistplusplus_dataset/)\n\n&amp;#x200B;\n\n*Processing video mgp85dcxbsn71...*", "link": "https://www.reddit.com/r/MachineLearning/comments/pp5iyt/r_google_ai_introduces_fullattention_crossmodal/"}, {"autor": "seven_cl", "date": "2021-09-15 22:27:34", "content": "[D] Interpreting lr finder plot for one cycle LR scheduler /!/ I am experimenting with [One Cycle](https://sgugger.github.io/the-1cycle-policy.html#the-1cycle-policy) lr scheduler and I am observing certain trends that I don't see discussed anywhere. I want to share what I have seen in case anyone else has seen something similar or if it is just something specific to the architecture and the dataset I am working on.\n\nRegarding the use of [lr finder](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html#how-do-you-find-a-good-learning-rate) for choosing learning rates. As a rule of thumb for using lr finder, it is suggested to use the point of higher gradient if using a constant value optimization algorithm, when using one cycle it is suggested to use a value near the minimum before divergence as the max lr. But no importance is given to the number of steps.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/sg1lxl6etqn71.png?width=394&amp;format=png&amp;auto=webp&amp;s=341fe2a5297e2e77beeec538df8a7513626897ee\n\nAs you can see in the -----> image !!!  the max gradient and divergence points change a lot for different n values. Furthermore, I have been able to increase the max\\_lr as high as 1e3 without issues, and if divergence occurs it can be evaded just by increasing the number of epochs leading to better results, suggesting that what the limitation really is here is the change in the learning rate.\n\nSo how should we really interpret the lr finder plot? Is there a rule of thumb for extrapolating the behavior for a certain number of steps to a certain number of epochs?", "link": "https://www.reddit.com/r/MachineLearning/comments/pp0itx/d_interpreting_lr_finder_plot_for_one_cycle_lr/"}, {"autor": "adamskijh", "date": "2021-09-19 13:38:23", "content": "-----> Image !!!  segmentation for photogrammetry /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pr83mw/image_segmentation_for_photogrammetry/"}, {"autor": "nickbild", "date": "2021-09-18 23:04:09", "content": "[P] Go Motion simplifies stop motion animation with machine learning /!/ &amp;#x200B;\n\nhttps://i.redd.it/l4dfl86xeco71.gif\n\nA CSI -----> camera !!!  is connected to a Jetson Xavier NX. This camera continually captures images of a scene. Using the trt\\_pose\\_hand hand pose detection model, the Jetson is able to determine when a hand is in the image frame.\n\nEach time all hands leave the frame, a single image is saved as part of the stop motion sequence. In this way, it is possible to continually manipulate the scene, momentarily removing one's hands from view of the camera after each adjustment, and have a stop motion sequence automatically generated that contains only the relevant image frames.\n\nMore info: [https://github.com/nickbild/go\\_motion](https://github.com/nickbild/go_motion)", "link": "https://www.reddit.com/r/MachineLearning/comments/pqwj3v/p_go_motion_simplifies_stop_motion_animation_with/"}, {"autor": "khalidsaifullaah", "date": "2021-09-18 15:22:18", "content": "[N] A talk on DALL\u00b7E mini architecture (VQ-GAN + BART pipeline understanding) /!/ I've recently given a talk on our DALL\u00b7E mini. It's more on its theoretical aspect rather than the technical ones. **It Provides a beginner-friendly understanding of the modules used, VQ-GAN and BART.**\n\n[**\\[YouTube Link\\]**](https://youtu.be/ui0X6ozE3bI)\n\n&amp;#x200B;\n\n**Talk Abstract**:\n\n*The ability to control -----> image !!!  generation with natural language is very fascinating and opens a lot of new opportunities in the field of multimodal machine learning. OpenAI's recent blog about their DALL\u00b7E project shows the potential of models, but unfortunately, the model has not been released. As we know these state-of-the-art models require massive amounts of computation and parameters to train, our goal here with DALL\u00b7E mini is to show that one can still achieve reasonable performance on this multimodal task with far more accessible means of compute. Even though DALL\u00b7E mini is about 30 times smaller than the original and trained on a much smaller dataset, it demonstrates interesting zero-shot capabilities.  In this talk, we will get to know DALL\u00b7E mini in detail, and explain how it is capable of achieving such results thanks to the use of pre-trained models such as the VQ-GAN and BART. We will dig deeper into the theoretical aspects of these models to understand what happens under the hood in the DALL\u00b7E mini pipeline.*", "link": "https://www.reddit.com/r/MachineLearning/comments/pqodmw/n_a_talk_on_dalle_mini_architecture_vqgan_bart/"}, {"autor": "iamaguacasa", "date": "2021-01-04 18:46:12", "content": "GPT-3 For -----> Image !!!  creation [R] /!/ Does anyone know if anything exists that utilizes gpt-3 for text described image creation?\n\nEx: elderly couple walking on the beach during sunset - generates a unique image compiled from thousands of others.", "link": "https://www.reddit.com/r/MachineLearning/comments/kqey2f/gpt3_for_image_creation_r/"}, {"autor": "iamaguacasa", "date": "2021-01-04 17:09:01", "content": "Gpt3 for -----> image !!!  generation /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/kqcwu9/gpt3_for_image_generation/"}, {"autor": "notya1000", "date": "2021-01-04 15:37:54", "content": "[D] ML for artistic purposses??? like making artistic images or music?? /!/  \n\nHi everybody! I've started to learn ML some months ago, mainly for data science projects and I like it but recently I realized I could do some other stuff too, like I want to experiment with -----> image !!!  making and music.  \nAnyone knows about algorithm/papers/techniques for music creation and/or image/video?\n\nTHX!!!", "link": "https://www.reddit.com/r/MachineLearning/comments/kqb2m2/d_ml_for_artistic_purposses_like_making_artistic/"}, {"autor": "HatsOnTheTable", "date": "2021-01-04 06:23:31", "content": "[D] Best practices for ML research projects development environment. /!/ I've been using anaconda and docker for over an year. I've found anaconda suited for dev environments but having docker images is much better to replicate as it's easily shared with team mates. Since I work on research projects, I experiment with multiple libraries, some stay in the final version but some don't. I need to make a list of used libraries, and use that to create a final -----> image !!! .\n\nAre there any best practices you follow when using docker for dev environment?\n\nIf docker is not used are there any libraries used best for ML projects development environment?", "link": "https://www.reddit.com/r/MachineLearning/comments/kq2yva/d_best_practices_for_ml_research_projects/"}, {"autor": "cxkloz", "date": "2021-01-03 19:51:43", "content": "State-of-the-Art for Object Segmentation and Perspective Correction on a -----> image !!!  /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/kprjvl/stateoftheart_for_object_segmentation_and/"}, {"autor": "projekt_treadstone", "date": "2021-03-03 23:51:33", "content": "[D] correct meta-learning approach selection for few-shot learning for noisy -----> image !!!  /!/ Meta-learning has 3 broad approach- ***Model, metric and optimization***\\-based approach. Each of them has its own way like- Matching network, model agonistic and Siamese network and so on.\n\nHow you guys decide which approach to select. For my case, I have a noisy image, and they need to be compared with 10 image every-time. Do I have to start with the trial and error method or there is some methodology behind this approach selection?", "link": "https://www.reddit.com/r/MachineLearning/comments/lx7qep/d_correct_metalearning_approach_selection_for/"}, {"autor": "crewlove24", "date": "2021-03-10 20:18:32", "content": "[D] Understanding on what/how metadata can be added to a CNN /!/  I  work with making convolutional neural networks to classify  multi-species datasets. I've created my cnn models with the sequential  API from Keras. Currently what I would like to do is add metadata (size,  length,...,etc) to my model building. I understand that I have to  switch from the sequential API to the functional API to include metadata  as a input. I have yet to add metadata to any of my models (don't have  it available currently because of covid). However I found some questions  that I'd like to ask.\n\n1. What  happens to the metadata when the -----> image !!!  training data is shuffled as the  model is sent batches of -----> image !!! s? Once I reach the step/layer when the  metadata and the image data meet(concatenated) are they somehow  magically aligned with each-other? From the few examples I have seen the  image training data is NOT shuffled.\n2. How  does the metadata need to be setup? For my training images I have a  folder for each class, for the metadata would I need separate csv files  for each class? Or 1 csv with all the image training data, including the  image name and the metadata (size, length,...,etc).\n\nIf you guys know the answers to these questions or any useful resources it would be highly appreciated.", "link": "https://www.reddit.com/r/MachineLearning/comments/m27det/d_understanding_on_whathow_metadata_can_be_added/"}, {"autor": "MaLGa_Center", "date": "2021-03-10 13:24:16", "content": "[N] MaLGa Summer Schools 2021: applications open! /!/ Please circulate!\n\n&amp;#x200B;\n\n\\*Apologies for multiple posting\\*\n\n&amp;#x200B;\n\n\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n\nRegML 2021 - Regularization Methods for Machine Learning\n\nJune 21 - 25  Instructor Lorenzo ROSASCO\n\n[https://ml.unige.it/education/regml2021/](https://ml.unige.it/education/regml2021/)\n\nDeep Learning: a hands-on introduction\n\nJune 28 - July 2  Instructor Nicoletta NOCETI\n\n[https://ml.unige.it/education/dl2021/](https://ml.unige.it/education/dl2021/)\n\nComputer Vision Crash Course\n\nJuly 5 - 9 Instructor Francesca ODONE\n\n[https://ml.unige.it/education/cvcc2021/](https://ml.unige.it/education/cvcc2021/)\n\nGenova, Italy, June 21 - July 9\n\n\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n\n&amp;#x200B;\n\nThey are 20 hours courses including theory classes and practical laboratory sessions.\n\nRegML is an advanced machine learning course covering foundations as well as recent advances in Machine Learning with emphasis on high dimensional data and a core set techniques, namely regularization methods. In many respects the course is a compressed version of the 9.520 course at MIT.\n\nDL is an introductory course to Deep Learning, covering  the various types of architectures and tools currently available. The theoretical classes will be coupled with hands-on activities, to practice the use of deep architectures on different scenarios and data types. Besides well established approaches, the course will also highlight current trends and open issues.\n\nCVCC is a crash course presenting the basic principles of computer vision and visual perception in artificial agents. In the first part we will present elements of classical computer vision. In a second part, students will get acquainted with the problem of representing and understanding the -----> image !!!  content adaptively by means of machine learning algorithms\n\nApply  \\*BEFORE\\*  May 16th\n\n&amp;#x200B;\n\nThe instructors,\n\nNicoletta Noceti - Francesca Odone - Lorenzo Rosasco\n\n&amp;#x200B;\n\nFor questions write to [malga.unige@gmail.com](mailto:malga.unige@gmail.com)", "link": "https://www.reddit.com/r/MachineLearning/comments/m1xdnd/n_malga_summer_schools_2021_applications_open/"}, {"autor": "glenniszen", "date": "2021-03-09 21:33:39", "content": "AI Video Art Channel [Project] - Hallucinogenic Poetry and Music Videos created from text prompts. /!/ I'm trying to build my channel by sharing here. I use Text to -----> Image !!!  synthesis techniques to visualize and animate poetry, music videos and other conceptual and philosophical ideas. The creative process is a fascinating dialogue (literally) between me and the AI.\n\nI hope you have a look and enjoy my work - feel free to ask any questions and I'll answer here, thank you.\n\n[https://www.youtube.com/c/GlennMarshallNeuralArt](https://www.youtube.com/c/GlennMarshallNeuralArt)", "link": "https://www.reddit.com/r/MachineLearning/comments/m1gy6f/ai_video_art_channel_project_hallucinogenic/"}, {"autor": "glenniszen", "date": "2021-03-09 21:22:43", "content": "AI Video Art Channel [Project] - I'm trying to build my channel by sharing here. I use Text to -----> Image !!!  synthesis techniques to visualize and animate poetry, music videos and other conceptual and philosophical ideas. The creative process is a fascinating dialogue (literally) between me and the AI.", "link": "https://www.reddit.com/r/MachineLearning/comments/m1gp45/ai_video_art_channel_project_im_trying_to_build/"}, {"autor": "Fancy-Stress-806", "date": "2021-03-09 18:34:54", "content": "[D] -----> Image !!!  Recognition using CNNs whilst incorporating additional -----> Image !!!  Features /!/ Hi r/MachineLearning. I am somewhat new to the field of Machine Learning but am very much interested in its applications. As such, I decided to undertake an undergraduate research project in Machine Learning and its applications in the field of Environmental Engineering.\n\nLong story short, my project involves predicting Rainfall fields at future timesteps by training Machine Learning models using radar/satellite images of a specific area in the UK. Through meetings and discussions, me and my supervisor have agreed to look at 2 specific applications of Neural Networks (or any ML model for that matter):\n\n1. Using ONLY radar imagery as inputs, what does my forecasted rainfall field look like? (There are already some research papers that utilise CNNs for this and so I sort of know what to do).\n2. Using radar imagery AND climatological features (e.g. humidity, temperature, geopotential, wind speeds, dewpoint, elevation above sea level, etc.), what does my forecasted rainfall field look like?\n\nSo my issue here is that I cannot find any prior research papers or articles relating to the 2nd option that combines Climatological feature maps and radar imagery as inputs to forecast rainfall fields. Perhaps it is inappropriate to use CNNs for the 2nd option. Therefore, I am asking here if you guys have any idea on **how to approach the 2nd option and could you direct me to some research papers/articles relating to the 2nd option?**\n\nI have thought of using regression models such as SVR, Multivariate Regression but the issue is that these models can't incorporate 3D data (e.g. to forecast rainfall intensities at future timesteps, I will need to look at each individual location independently, thereby ignoring the relationship between locations which is not right).\n\nFor contextual information, because I understand it is important to know what the data looks like before deciding on a model:\n\n\\- Radar imagery: 3D matrix, where the 1st and 2nd dimensions correspond to location (x,y) and the 3rd dimension (depth into/out of the page) corresponds to time. Each element of the matrix therefore represents rainfall intensities \\[mm\\] at a specific location and at a given time.\n\n\\- Climate data (e.g. Humidity, temperature, dewpoint, etc.): Similar to radar imagery except it's humidity, temperature, dewpoint, etc. instead of rainfall intensities.", "link": "https://www.reddit.com/r/MachineLearning/comments/m1cyjq/d_image_recognition_using_cnns_whilst/"}, {"autor": "hwpcspr", "date": "2021-03-13 06:43:56", "content": "Are there any addons that can block all Asian porn? [D] /!/  \n\naddons\n\n-----> image !!!  recognition", "link": "https://www.reddit.com/r/MachineLearning/comments/m40yuc/are_there_any_addons_that_can_block_all_asian/"}, {"autor": "Yuqing7", "date": "2021-03-13 04:26:18", "content": "[N] Oxford Novel Image Compression Method COIN: Better Than JPEG at Low Bitrates! /!/ University of Oxford researchers propose COIN, a novel -----> image !!!  compression method that stores the weights of an MLP overfitted to an -----> image !!!  and outperforms JPEG at low bitrates even without entropy coding.\n\nHere is a quick read: [Oxford Novel Image Compression Method COIN: Better Than JPEG at Low Bitrates!](https://syncedreview.com/2021/03/12/stanford-novel-image-compression-method-coin-better-than-jpeg-at-low-bitrates/)\n\nThe paper *COIN: Compression with Implicit Neural representations* is on [arXiv](https://arxiv.org/pdf/2103.03123.pdf).", "link": "https://www.reddit.com/r/MachineLearning/comments/m3z0mx/n_oxford_novel_image_compression_method_coin/"}, {"autor": "RedRyan222", "date": "2021-03-12 21:13:49", "content": "[Project] Geoffrey Hinton's GLOM for MNIST using Pytorch: How to represent part-whole hierarchies in a neural network /!/ I recently implemented [Geoffrey Hinton's GLOM paper](https://arxiv.org/pdf/2102.12627.pdf) in Pytorch for the MNIST dataset.\n\nI think this is the first working implementation that actually trains on an -----> image !!!  dataset. \n\nIt'd be best to watch [Yannic Kilcker's video](https://www.youtube.com/watch?v=cllFzkvrYmE&amp;ab_channel=YannicKilcher) before going through the implementation and the [README.md](https://README.md) if you're new to this paper.\n\nHere is my implementation: [https://github.com/RedRyan111/GLOM](https://github.com/RedRyan111/GLOM)\n\nThe best accuracy achieved was around 47%. These models have been posted to GitHub. From personal experience of training neural cellular automata, which has a similar architecture, I also knew that this architecture would be very hard to train. But tell me what you guys think! I'd love feedback. :)", "link": "https://www.reddit.com/r/MachineLearning/comments/m3rbk4/project_geoffrey_hintons_glom_for_mnist_using/"}, {"autor": "Simoncarbo", "date": "2021-03-12 08:12:05", "content": "[R] Intraclass clustering: an implicit learning ability that regularizes DNNs (ICLR 2021) /!/ Hey fellow ML redditors!\n\nSharing our latest paper which I'll present at ICLR 2021. It's 100% experimental, and tries to remove some of the mystery around the implicit regularization mechanisms behind SGD-based deep learning.\n\n# Preliminaries: what is intraclass clustering and why does it matter?\n\nOur work starts from the observation that in standard -----> image !!!  classification datasets, classes are typically composed of multiple clusters of similarly looking -----> image !!! s. We call intraclass clustering a model\u2019s ability to differentiate these clusters despite their association to identical labels.\n\nExamples:\n\n&amp;#x200B;\n\n[Intraclass clusters in MNIST and CIFAR10](https://preview.redd.it/qha4jfko1km61.png?width=1798&amp;format=png&amp;auto=webp&amp;s=ff831ee234f630e1e8a4c95d9be7ab58257b2703)\n\nStudying if state-of-the-art neural networks extract intraclass clusters is important, as learning structure inherent to the data is key for generalization. While no supervision or training mechanisms are explicitly programmed into deep neural networks to perform intraclass clustering, it is possible that this learning ability implicitly emerges during training. Such a hypothesis is especially compelling as recent work on generalization conjectured the emergence of implicit forms of regularization during deep neural network training.\n\n# Our contributions\n\n\\- We design four different measures of intraclass clustering, based on the neuron- and layer-level representations of the training data, and show that they constitute accurate predictors of generalization performance across variations of a large set of hyperparameters (learning rate, batch size, optimizer, weight decay, dropout rate, data augmentation, network depth and width).\n\n\\- We conduct a deeper analysis of the phenomenon captured by our measures: in which layer does it occur? During which phase of training? Do sub-parts of the networks specialize to different intraclass clusters?\n\n&amp;#x200B;\n\nOverall, our results suggest a crucial role for intraclass clustering in the implicit regularization of deep neural networks. We hope our work will spark interest in this phenomenon!\n\nIf you have any questions about all this, I would be happy to answer :)\n\nAll the best,\n\nSimon", "link": "https://www.reddit.com/r/MachineLearning/comments/m3d49d/r_intraclass_clustering_an_implicit_learning/"}, {"autor": "sai-krishna-das", "date": "2021-01-01 04:48:49", "content": "[D] Larger the images size , slower the inference? /!/ Say suppose, I train a model of images with 960 X 960 size and another model with 416 X 416 size. Does if affect my inference time? ( Ofc, training time will increase but does the inference time too increase to detect a 960 X 960 -----> image !!! ?)", "link": "https://www.reddit.com/r/MachineLearning/comments/ko587r/d_larger_the_images_size_slower_the_inference/"}, {"autor": "zyl1024", "date": "2021-01-01 00:58:31", "content": "[D] Is the (original) resnet suitable for learning location information? /!/ In resnet, the convolutional layers are translation-invariant (to a large extent), and the global average pooling combines the final activated values at all pixels into one number (for each channel). \n\nConsider the following very simple a binary dataset. For each -----> image !!! , black circle is drawn on a white background at a random position. Depending on whether the circle is drawn on the left side or right side, the image is labeled positive and negative. \n\nIt seems to me that the (original) resnet architecture simply can't handle it, at least in theory. In practice, the convolution is not fully translation-invariant due to the discrete nature of filer and maxpooling, which may still make the resent to learn on this dataset successfully. But the fact that (I believe) it naturally does not work for this kind of dataset is concerning, considering how resnet is the default backbone for many tasks in the literature, including those that focus on spatial reasoning. A very simple modification would be to flatten the last convolutional layer, rather, as AlexNet, VGG, etc. have been doing. \n\nDid I miss anything here?", "link": "https://www.reddit.com/r/MachineLearning/comments/ko1yzt/d_is_the_original_resnet_suitable_for_learning/"}, {"autor": "real_groove", "date": "2021-04-03 08:39:29", "content": "[D] Collaboration for learning, doing and publishing (Imaging/AI) /!/ A very straight-forward and honest description of what I\u2019m looking for:\n\nI am looking for people in the UK to collaborate with on ML/DL projects related with -----> image !!!  data. I would like to work with the goal of having a good journal publication within a year.\n\nI work in research and have an engineering background. I work for a national research lab in my country and have a tenured position. I\u2019m an early-career researcher, so to speak. I am willing to provide more details about my work in PMs. \n\nI would be interested in keywords like object classification, segmentation, restoration etc in images. \n\nI am interested in applying to UK universities for a PhD eventually. I would be an international candidate so I want a good publication to make my CV better. And that\u2019s what this is all about.\n\nIf you are a lecturer/professor/researcher or pursuing a PhD/PostDoc, and have enough time on your hands for something like this with similar goals as mine, it would be an ideal arrangement.\n\nAlso, if this post doesn\u2019t belong to this subreddit, please point me in the right direction.\n\nThanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/mj4mtq/d_collaboration_for_learning_doing_and_publishing/"}, {"autor": "downtownslim", "date": "2021-04-02 19:43:07", "content": "[R] An -----> Image !!!  is Worth 16x16 Words, What is a Video Worth?", "link": "https://www.reddit.com/r/MachineLearning/comments/mirx8n/r_an_image_is_worth_16x16_words_what_is_a_video/"}, {"autor": "KirillTheMunchKing", "date": "2021-04-02 16:35:15", "content": "[R] StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery - SOTA StyleGAN -----> image !!!  editing /!/ [StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery](https://t.me/casual_gan/18)\n\nThis idea is so elegant, yet powerful:  \nThe authors use the recent CLIP model in a loss function to train a mapping network that takes text descriptions of image edits (e.g. \"a man with long hair\", \"Beyonce\", \"A woman without makeup\") and an image encoded in the latent space of a pretrained StyleGAN generator and predicts an offset vector that transforms the input image according to the text description of the edit.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/kemg73bcfsq61.png?width=1438&amp;format=png&amp;auto=webp&amp;s=0c3a259abe37ef32bafa022195ca4ce3f3ab320b\n\nP.S. In case you are not familiar with the paper check it out [here](https://t.me/casual_gan/18):", "link": "https://www.reddit.com/r/MachineLearning/comments/minzbz/r_styleclip_textdriven_manipulation_of_stylegan/"}, {"autor": "glenniszen", "date": "2021-04-02 16:19:12", "content": "[P] M C Escher - I've accidentally discovered a new AI technique that can reshape a -----> photo !!!  (Escher) in any style (here also Escher)", "link": "https://www.reddit.com/r/MachineLearning/comments/minnjp/p_m_c_escher_ive_accidentally_discovered_a_new_ai/"}, {"autor": "OnlyProggingForFun", "date": "2021-02-20 14:03:53", "content": "[News][Research] ShaRF: Take a -----> picture !!!  from a real-life object, and create a 3D model of it", "link": "https://www.reddit.com/r/MachineLearning/comments/lo7r84/newsresearch_sharf_take_a_picture_from_a_reallife/"}, {"autor": "flow_smith", "date": "2021-02-20 12:51:09", "content": "[D] learning to Rank /!/ Hello there! \n\nI am looking for some guidance to good papers or at least a proper research field name of a problem I want to work on: **ranking entities of scores**.\n\n**Definition: The inference phase**\n\nLet's say I have a database of **N** classes. At inference I receive a input (-----> image !!!  for example) I try to match it to my database (-----> image !!!  retrieval style) using different modes and approaches (visual embedding, ocr ...) and I generate **M** possible predictions and their corresponding scores vector, each vector is a fixed size **5** for example (**5** different scores from **5** approaches) . So I want a model (function, black box..) that receives these **M** inputs and give a proper rank to these candidates.\n\n* M may vary, it is not fixed.\n* I dont know the importance of each score in the scores vector sometimes ocr alone can do the job sometimes visual embedding is good enough\n* If the model im looking for is trainable, I dont see how we can constract training data. We only have  the best candidate and not their orders everytime\n* It would be great if the model learns the importance of each score. Like saying score1 is the most imporant. if score3 is above 0.3 don't see score2. If score5 is below this threshold this candidate is very bad .... and so on\n\nLooking forward to your advice\n\nThank you", "link": "https://www.reddit.com/r/MachineLearning/comments/lo6ip2/d_learning_to_rank/"}, {"autor": "mashood3624", "date": "2021-02-20 04:21:44", "content": "[D] Scaling/Resizing landmarks of an -----> image !!!  /!/ Hi, I am going through a resizing problem. Lets say I have an image of 2225 * 1555 pixels (a random size) I also have some landmarks (like facial landmarks) of this image. I then resize it to 2048 * 1024 pixels then how can I find out resized version of landmarks. Is there a formula or technique ? \n\nI am facing this issue because my dataset has images of different dimensions and when I resize them to fixed dimensions i lost my landmarks so I also want to resize them to image's new dimension\n\nNote: landmarks are some specific [x,y] pixel of image", "link": "https://www.reddit.com/r/MachineLearning/comments/lnz1pw/d_scalingresizing_landmarks_of_an_image/"}, {"autor": "Field_Great", "date": "2021-02-23 09:33:20", "content": "Why is this happening.. I'm trying to capture -----> image !!!  for labeling and training but I've been face with this issue.. btw this is a real time object detection project", "link": "https://www.reddit.com/r/MachineLearning/comments/lqefdm/why_is_this_happening_im_trying_to_capture_image/"}, {"autor": "jackson_ditred", "date": "2021-02-23 06:19:08", "content": "[R]CSTR: A Classification Perspective on Scene Text Recognition /!/ **Paper link**: [https://arxiv.org/abs/2102.10884](https://arxiv.org/abs/2102.10884)\n\n**Abstract:**  \nThe prevalent perspectives of scene text recognition are from sequence to sequence (seq2seq) and segmentation. In this paper, we propose a new perspective on scene text recognition, in which we model the scene text recognition as an -----> image !!!  classification problem. Based on the image classification perspective, a scene text recognition model is proposed, which is named as CSTR.The CSTR model consists of a series of convolutional layers and a global average pooling layer at the end, followed by independent multi-class classification heads, each of which predicts the corresponding character of the word sequence in input image. The CSTR model is easy to train using parallel cross entropy losses.CSTR is as simple as image classification models like ResNet \\\\cite{he2016deep} which makes it easy to implement, and the fully convolutional neural network architecture makes it efficient to train and deploy. We demonstrate the effectiveness of the classification perspective on scene text recognition with thorough experiments. Futhermore, CSTR achieves nearly state-of-the-art performance on six public benchmarks including regular text, irregular text. The code will be available at [https://github.com/Media-Smart/vedastr](https://github.com/Media-Smart/vedastr).", "link": "https://www.reddit.com/r/MachineLearning/comments/lqb7le/rcstr_a_classification_perspective_on_scene_text/"}, {"autor": "Prestigious-Bend", "date": "2021-02-23 05:53:05", "content": "[D] Papers with Code SOTA Leaderboards /!/ How exhaustive are these? How are they generated? Are they considered fair? For instance, lets say I'm looking at their [leaderboard](https://paperswithcode.com/sota/-----> image !!! -classification-on------> image !!! net) for the top performing models within the 30-50M param range on ImageNet, can I trust that there isn't some better-performing model not listed there?", "link": "https://www.reddit.com/r/MachineLearning/comments/lqar9u/d_papers_with_code_sota_leaderboards/"}, {"autor": "seek_it", "date": "2021-02-22 23:10:39", "content": "[D] GAN paper/code for -----> image !!!  background completion? /!/ Hey all,\n\nI've been searching for GANs papers/code for implementing a method for image background completion but could not come across any good paper/code.\n\nProblem: In an image, there can be some distortion in the background  or around half of the background is different from the rest. Our GAN model should fix the background or complete the uncompleted background w.r.t. image size. Background distortion can be of any type so, this validates that solutions is achievable using GANs only. \n\nAre there any GANs paper/code to which I can look after?\n\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/lq2tup/d_gan_papercode_for_image_background_completion/"}, {"autor": "techsucker", "date": "2021-02-22 15:32:19", "content": "[R] DeepMind Researchers Propose Normalizer-Free ResNets (NFNets) To Achieve Large-Scale Image-Recognition Without Batch Normalization /!/ A team of researchers at DeepMind introduces Normalizer-Free ResNets (NFNets) and demonstrates that the -----> image !!!  recognition model can be trained without batch normalization layers. The researchers present a new clipping algorithm to design models that match and even outperform the best batch-normalized classification models on large-scale datasets while also significantly reducing training time.\n\n**Batch normalization and its shortcomings**\n\nThe batch normalization is a vital component of most image classification models. It can accelerate training, enables higher learning rates, improves generalization accuracy, and has a regularisation effect. However, batch normalization suffers from three practical disadvantages:\n\n* It is costly in memory and time.\n* It introduces discrepancies between model behaviors during training and inference time, thereby requiring additional fine-tuning.\n* It destroys the independence between training examples in the minibatch.\n\nMany recent studies have successfully trained deep ResNets without normalization. However, the resulting models do not match SOTA batch-normalized networks\u2019 test accuracy and are frequently unstable for strong data augmentations or large learning rates.\u00a0\n\nFull Paper Summary: [https://www.marktechpost.com/2021/02/22/deepmind-researchers-propose-normalizer-free-resnets-nfnets-to-achieve-large-scale-image-recognition-without-batch-normalization/](https://www.marktechpost.com/2021/02/22/deepmind-researchers-propose-normalizer-free-resnets-nfnets-to-achieve-large-scale-image-recognition-without-batch-normalization/) \n\nPaper: [https://arxiv.org/pdf/2102.06171.pdf](https://arxiv.org/pdf/2102.06171.pdf) \n\nGithub: [https://github.com/deepmind/deepmind-research/tree/master/nfnets](https://github.com/deepmind/deepmind-research/tree/master/nfnets)", "link": "https://www.reddit.com/r/MachineLearning/comments/lpr0mb/r_deepmind_researchers_propose_normalizerfree/"}, {"autor": "tonic-boy", "date": "2021-02-22 11:02:19", "content": "[D] Serverless solutions for GPU inference (if there's such a thing) /!/ Hi there [r/MachineLearning](https://www.reddit.com/r/MachineLearning/). Battling with an optimization of resources in a production setting.\n\nI have a **heavy workload, user-triggered, that runs for approximately \\~30-45 secs**.  It starts by running a lot of parallel computations whose results are  then sent to an -----> image !!!  classification model served via TensorFlow  Serving. This yields the final product of the entire pipeline, which is  then returned to the user.\n\nOn the  other side, the user is waiting for the result of this computation to  continue his task, so reducing computation time as much as possible is  of interest.\n\n**Current  solution:** As I had some Azure credits and initially deployed  this during a prototype&amp;iterate phase, ended up renting a powerful  Azure VM with both considerable CPU power (for the parallel computation  part) and a GPU. However, the usage requests are pretty sparse: **this machine is idle 98% of the time**.\n\nI  am now looking into optimizing this, trying to move into a flexible  architecture without reducing the service level and response times  drastically.\n\n**What I have explored so far:**\n\n* **On-demand start&amp;stop of the machine:** as this is a GPU-based machine, the boot time is a bit long. Besides that, the control logic would get complex (*how to decide when to start the machine?* *how to decide when to shutdown the machine? what if the machine is being shut down and we get an incoming request?*).\n* **Azure Functions:**  this would be the typical use-case for a serverless function (one-off  computation that runs, returns results and disappears), but Azure  Functions don't support GPU-based workloads. I have tested CPU  inference and the performance penalty is too high (too many images).  Serverless offers in AWS and GCP seem to have similar limitations (and  correct me if I am wrong).\n* **Azure Container Instances:**  I have the TensorFlow-Serving instance nicely packaged up in a  container, so spinning up a container-based VM to run the computation  seemed like a nice idea, but provisioning Azure resources with a GPU to  run this container took 10-15 minutes in my tests.\n* **Azure Kubernetes Service:**  the logical solution regarding elasticity. I set up a cluster with a  node pool with GPU-based VMs as the underlying resources. After  creation, the node pool was scaled down to 0 nodes. Scaling it back up  to 1 node (simulating an incoming request) took 4-5 minutes during  testing. Too high (\\~10x the current duration).\n\nI understand this might be a situation where *I want to have the cake and at the same time eat it*, but it feels like an increasingly common workflow for better solutions not to exist.\n\nLike  the title says, a reliable GPU serverless offering would be it, but the  market seems pretty dry in that field. Staying within Azure would be  nice, but that's not a hard requirement. Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/lpld92/d_serverless_solutions_for_gpu_inference_if/"}, {"autor": "ConfidentAd598", "date": "2021-04-19 18:17:00", "content": "-----> Image !!!  detection - train a model with images with representative quality or highest quality images? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/mu6tzg/image_detection_train_a_model_with_images_with/"}, {"autor": "cedricdb", "date": "2021-04-19 16:34:27", "content": "[R] [D] Label info in Adversarial Autoencoders /!/ I have a question about the Adversarial Autoencoders paper by Makhzani et al, 2016 ([https://arxiv.org/abs/1511.05644](https://arxiv.org/abs/1511.05644)).\n\nLet's look at Figure 8, which concerns the architecture for Semi-Supervised Adversarial Autoencoders. A softmax is used to obtain soft labels for the input -----> image !!! . This soft output is encouraged to be as close to a categorical sample by the objective of the upper GAN. The question is: what happens with the soft label when optimizing decoder? Are you supposed to draw a hard sample from it, or take the argmax?\n\nIn the semi-supervised setting it's not that important, but later on in Figure 10 (Dimensionality Reduction with Adversarial Autoencoders), this architecture is reused. Here the softmax output is used as a cluster head selector. Is this a soft or hard selector?\n\nThe way I see which makes kind of sense, is as follows: to optimize the upper GAN, you use soft labels. But to optimize the decoder using reconstruction error, you sample from it. But this implies that the gradients of the reconstruction error will not flow through the softmax layer.\n\nCan someone clarify this for me? Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/mu4k5u/r_d_label_info_in_adversarial_autoencoders/"}, {"autor": "whiterabbitobj", "date": "2021-04-19 03:10:12", "content": "Uprezzing -----> image !!!  sequence /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/mtru68/uprezzing_image_sequence/"}, {"autor": "39dotyt", "date": "2021-08-25 07:36:57", "content": "[P] rclip: Use CLIP to Search for Your Photos in the Command Line /!/ &amp;#x200B;\n\nhttps://i.redd.it/rk4yq7hdjgj71.gif\n\nHi, everyone!\n\nHere is a detailed demo with -----> image !!!  previews: [https://www.youtube.com/watch?v=tAJHXOkHidw](https://www.youtube.com/watch?v=tAJHXOkHidw).\n\nCheck out the GitHub repo if you are interested in giving it a try: [https://github.com/yurijmikhalevich/rclip#readme](https://github.com/yurijmikhalevich/rclip#readme). And don't be a stranger if you are interested in contributing!", "link": "https://www.reddit.com/r/MachineLearning/comments/pb6fvw/p_rclip_use_clip_to_search_for_your_photos_in_the/"}, {"autor": "39dotyt", "date": "2021-08-25 07:34:53", "content": "[P] rclip: Use Clip to Query for Your Photos in the Command Line /!/ &amp;#x200B;\n\nhttps://i.redd.it/feurnxhcigj71.gif\n\nHi, everyone!\n\nHere is a more detailed demo with -----> image !!!  previews: [https://www.youtube.com/watch?v=tAJHXOkHidw](https://www.youtube.com/watch?v=tAJHXOkHidw).\n\nCheck out the GitHub repo and give it a try: [https://github.com/yurijmikhalevich/rclip#readme](https://github.com/yurijmikhalevich/rclip#readme). And don't be a stranger if you are interested in contributing!", "link": "https://www.reddit.com/r/MachineLearning/comments/pb6eze/p_rclip_use_clip_to_query_for_your_photos_in_the/"}, {"autor": "dottordrea", "date": "2021-08-25 06:42:18", "content": "[R] Postdoctoral Position in ML/Image Processing /!/ I have a position for a postdoctoral researcher in my group at the Institute of Space Sciences and Astronomy, University of Malta.\n\nIf you're interested in the cross between -----> image !!!  processing, astronomy and machine learning, then the research we're conducting for OPTICA (Optical Telescope Intelligence for Computational Astrophotography) may be for you!\n\nHead over to the [application form](https://www.um.edu.mt/__data/assets/pdf_file/0010/474283/RSOI_II_III-OPTICA-24.08.2021.pdf?fbclid=IwAR3o1EPdDH2dXEcg8Tvz1S9w_8WhtICH3itykD7VyfQ7a0TOyoDnkh2rYdc) for all the details about the post, including salary commensurate with qualifications, as well as candidate profile and application procedure.\n\nOPTICA is a project funded by the Malta Council for Science and Technology - Research Excellence Programme 2021.\n\nDeadline: Tuesday, 31st August 2021", "link": "https://www.reddit.com/r/MachineLearning/comments/pb5rby/r_postdoctoral_position_in_mlimage_processing/"}, {"autor": "Pseudoabdul", "date": "2021-08-25 04:10:31", "content": "[D] Considerations for single object (ball) tracking system (Labelling tools, Model choice) /!/ I've recently been handed a project where I need to track a ball during a sports game. While I have a lot of ML experience, I've never had to do object detection before, so I thought I'd ask some questions. \n\nBefore I begin, here is the specification of what the final goal is:\n* Be able to track a single ball\n* Be able to find the ball again if it is lost\n* Run in real time on hardware such as a Jetson Nano\n\nI've spent the last few days researching ball tracking, but I don't think there exists any out of the box solutions. Here are some of the most promising repos I found:\n\nhttps://github.com/chonyy/basketball-shot-detection\nhttps://github.com/stephanj/basketballVideoAnalysis/wiki\nhttps://github.com/brettfazio/CVBallTracking\n\nThrough all the technologies I've looked through, I've not been able to find something that would fulfil my criteria. Either the quality of the tracking isn't good enough and the ball is lost too easily, or its too slow to run in real time. \n\nSo it looks like I'll have to put something together myself. My current plan is to finetune an existing object tracking model with some labelled data. Which brings me to my first real dilemma about data labelling. People seem fond of something like CVAT. I've also looked into tools like V7 (which is very impressive). Apparently, YOLO based models lose a lot of performance to trying to fit many kinds of bounding boxes, and given that balls are round, I should only need a couple of square (or circular) bounding boxes at different sizes depending on how close they are to the -----> camera !!! . The other issue is going to be interpolation in the labelling. I have way too much training footage to go through frame by frame, so any tools that offer assistance would help a great deal. \n\nOnce I have labelled video, the plan is to choose a tracking model to finetune. This is where I'm a little overwhelmed by choice. OpenCV has an implementation of MOSSE that seems pretty fast. I've also been reading about squeezenet. I've also read a lot of papers so far, but very few have made their implementation open source. \n\nSo any advice, resources, or directions would be greatly appreciated.", "link": "https://www.reddit.com/r/MachineLearning/comments/pb3o44/d_considerations_for_single_object_ball_tracking/"}, {"autor": "toby__bryant", "date": "2021-01-22 16:59:14", "content": "[D] How to find the right annotation strategy for your computer vision project, something often done wrong. Link to the high-res -----> image !!!  and some explanation is in the first comment", "link": "https://www.reddit.com/r/MachineLearning/comments/l2ruc5/d_how_to_find_the_right_annotation_strategy_for/"}, {"autor": "projekt_treadstone", "date": "2021-01-22 16:32:49", "content": "[D] Close set and open set classification at the same time /!/ Is it possible to use a neural network(or another approach) to classify -----> image !!!  based on trained data and at the same time if new -----> image !!!  classes are introduced in the test set it should classify those unseen -----> image !!! s(open set data) to new classes on which training is not done. \n\nNote One way I can think of- first network should be able to find it is of unknown class, and then it should go on find the pattern in image and classify them to a new class. \n\nAny paper of resource if you can direct will be really helpful.", "link": "https://www.reddit.com/r/MachineLearning/comments/l2ragd/d_close_set_and_open_set_classification_at_the/"}, {"autor": "bonkerfield", "date": "2021-01-22 11:34:09", "content": "[P] Story2Hallucination - using Big Sleep to generate continuous animations from extended text stories. /!/ I've made a Colab notebook [here](https://drive.google.com/file/d/1jF8pyZ7uaNYbk9ZiVdxTOajkp8kbmkLK/view?usp=sharing) with a pretty straightforward modification to the Big Sleep package that allows you to dynamically update the text prompt while it's converging.  This allows you to input entire paragraphs broken into several word phrase to \"animate\" the story piece-by-piece.\n\nThe footage is normally quite trippy (much like the -----> image !!!  generation itself) and reminds me of a hallucination, so I'm calling it Story2Hallucination.\n\n[Sped up short story animated with Story2Hallucination](https://i.redd.it/anoe9uc5cvc61.gif)\n\nThere were a few additional hacks I've had to build in to try to automatically \"reset\" the images as they often start to drift toward noisy images.  I've written a \\[brief blog post\\]([https://bonkerfield.org/2021/01/story2hallucination/](https://bonkerfield.org/2021/01/story2hallucination/)) where I go over the results for Kafka's Metamorphosis, a Wordsworth poem, and a short story I wrote.", "link": "https://www.reddit.com/r/MachineLearning/comments/l2lx6t/p_story2hallucination_using_big_sleep_to_generate/"}, {"autor": "CorgisKnow", "date": "2021-01-22 09:52:17", "content": "[D] What are you unknowingly training? /!/ Just realized I've been unknowingly training a machine learning algorithm through the -----> image !!!  captchas. What other things am I unknowingly training?", "link": "https://www.reddit.com/r/MachineLearning/comments/l2kjlr/d_what_are_you_unknowingly_training/"}, {"autor": "xEdwin23x", "date": "2021-01-22 06:38:56", "content": "[P][R] Animesion: a framework, for anime character recognition. It uses Vision Transformers trained on a subset of Danbooru2018, that we rebranded as DAF:re, and can classify a given -----> image !!!  into one of more than 3000 characters! Source code and paper presenting the dataset, and experiments included. /!/ For this purpose we revamped an existing dataset, and rebranded it as DAF:re. It contains almost 500 K images divided across 3263 characters, from anime, videogames, and related media.\n\nOur best model, ViT (Vision Transformer) L-16 ,with image size 128x128 and batch size 64, achieves to get 85.95% and 94.23% top-1 and top-5 classification accuracies.\n\nSample result:\n\n[https://github.com/arkel23/animesion/blob/main/classification/results\\_inference/homura\\_top.jpg](https://github.com/arkel23/animesion/blob/main/classification/results_inference/homura_top.jpg)\n\nSample GIF of DAF:re\n\n[https://j.gifs.com/ROpp10.gif](https://j.gifs.com/ROpp10.gif)\n\nSource code along with demo images:\n\n[https://github.com/arkel23/animesion/tree/main/classification](https://github.com/arkel23/animesion/tree/main/classification)\n\nArxiv Preprint:\n\n[https://arxiv.org/abs/2101.08674](https://arxiv.org/abs/2101.08674)", "link": "https://www.reddit.com/r/MachineLearning/comments/l2i04q/pr_animesion_a_framework_for_anime_character/"}, {"autor": "AIforimaging", "date": "2021-07-02 17:03:55", "content": "[R] Endpoint AI powers computer vision in smart -----> camera !!!  devices", "link": "https://www.reddit.com/r/MachineLearning/comments/oceeog/r_endpoint_ai_powers_computer_vision_in_smart/"}, {"autor": "MetinSeven", "date": "2021-07-02 14:08:23", "content": "Old -----> photo !!!  of my dad's niece, processed using GFPGAN ML tech", "link": "https://www.reddit.com/r/MachineLearning/comments/ocaua3/old_photo_of_my_dads_niece_processed_using_gfpgan/"}, {"autor": "MetinSeven", "date": "2021-07-02 14:07:18", "content": "Old -----> photo !!!  of my dad and his niece, face-enhanced using GPEN ML tech", "link": "https://www.reddit.com/r/MachineLearning/comments/ocathm/old_photo_of_my_dad_and_his_niece_faceenhanced/"}, {"autor": "leuchterfisch", "date": "2021-05-20 13:26:46", "content": "Highest resolution GAN available? [D] /!/ Hi everyone. For a private -----> film !!!  production / experiment am seeking for the highest quality/resolution image generation GAN available for public. I am searching for those that are trained on landscape and architectural datasets not those for human faces. Alternatively if there is a Hi res GAN that could be retrained it would also be appreciated.\n Generation consistency is also a big factor for me as plan to animate a lot of generated frames What providers are out there that can do this. My manual search has only pointed me to tech papers andl am an artist not a ml scientist :/\n\nThank you very much for your help!\n&lt;3", "link": "https://www.reddit.com/r/MachineLearning/comments/nh06ym/highest_resolution_gan_available_d/"}, {"autor": "AtenRa85", "date": "2021-05-20 12:45:47", "content": "[D] Weka + dl4j -----> image !!!  iteration /!/  I posted this in [r/learnmachinelearning](https://www.reddit.com/r/learnmachinelearning/) first, but didnt get any responses, so trying here\n\n\\---\n\nI am hoping someone is familiar with image classification using the dl4j library in Weka, using the GUI\n\nI can train and save my model just fine - but I am having trouble figuring out how to point the model to new data to classify.\n\nMy training.arff and newData.arff are in \"abc123.jpg, class\" format. For training the model I set the image iterator location to the folder with all of my training images. However, when I try to run the model on the newData.arff, it seems to want to look in the test folder for images, and I am failing to understand where I can set the filepath for the newData.arff\n\nI'm positive that I am overlooking a simple setting, but frustration has set in, and any help would be greatly appreciated.", "link": "https://www.reddit.com/r/MachineLearning/comments/ngz914/d_weka_dl4j_image_iteration/"}, {"autor": "AtenRa85", "date": "2021-05-20 12:43:43", "content": "Weka + djl4 -----> image !!!  iteration /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ngz7ct/weka_djl4_image_iteration/"}, {"autor": "aiff22", "date": "2021-05-20 08:43:07", "content": "[R] Fast and Accurate Camera Scene Detection on Smartphones /!/ **Abstract.**  AI-powered automatic -----> camera !!!  scene detection mode is nowadays available in nearly any modern smartphone, though the problem of accurate scene prediction has not yet been addressed by the research community. This paper for the first time carefully defines this problem and proposes a novel Camera Scene Detection Dataset (CamSDD) containing more than 11K manually crawled images belonging to 30 different scene categories. We propose an efficient and NPU-friendly CNN model for this task that demonstrates a top-3 accuracy of 99.5% on this dataset and achieves more than 200 FPS on the recent mobile SoCs. An additional in-the-wild evaluation of the obtained solution is performed to analyze its performance and limitation in the real-world scenarios. The dataset and pre-trained models used in this paper are available on the project website.\n\nhttps://preview.redd.it/rh0tayhml8071.png?width=2365&amp;format=png&amp;auto=webp&amp;s=71dd6439c8986b022ff3cfaeb29eeaec54760f61\n\narXiv paper:  [https://arxiv.org/pdf/2105.07869.pdf](https://arxiv.org/pdf/2105.07869.pdf)\n\nProject website:  [https://people.ee.ethz.ch/\\~ihnatova/camsdd.html](https://people.ee.ethz.ch/~ihnatova/camsdd.html)\n\n\\---------------------------------------------------------------------------------------------------------------------\n\n[Mobile AI 2021](https://ai-benchmark.com/workshops/mai/2021/) Challenge on Quantized Camera Scene detection:  [paper](https://arxiv.org/pdf/2105.08819.pdf), [website](https://ai-benchmark.com/workshops/mai/2021/)", "link": "https://www.reddit.com/r/MachineLearning/comments/nguvb2/r_fast_and_accurate_camera_scene_detection_on/"}, {"autor": "aselsiriwardena", "date": "2021-04-17 17:07:09", "content": "[P] Is there a way to use Pytorch to adjust Highlights and Shadows /!/  Is there a way to adjust the -----> image !!! 's highlights and shadows like in photoshop? \n\nhttps://preview.redd.it/09on0i3tmrt61.png?width=421&amp;format=png&amp;auto=webp&amp;s=88459dc1998be42daa4f58f949456467cdfc23da\n\n I have used \n\n    torchvision.transforms.functional\n\nto adjust brightness and contrast. I want to know if they're a similar way to adjust highlights and shadows as well.", "link": "https://www.reddit.com/r/MachineLearning/comments/msum2y/p_is_there_a_way_to_use_pytorch_to_adjust/"}, {"autor": "designer1one", "date": "2021-04-17 13:44:30", "content": "[P] *Semantic* Video Search with OpenAI\u2019s CLIP Neural Network /!/ I made a simple tool that lets you search a video \\*semantically\\* with AI. \ud83c\udf9e\ufe0f\ud83d\udd0d\n\n\u2728 Live web app: [http://whichframe.com](http://whichframe.com/) \u2728\n\nExample: Which video frame has a person with sunglasses and earphones?\n\nThe querying is powered by OpenAI\u2019s CLIP neural network for performing \"zero-shot\" -----> image !!!  classification and the interface was built with Streamlit.\n\nTry searching with text, image, or text + image and please share your discoveries!\n\n\ud83d\udc47 More examples  \n[https://twitter.com/chuanenlin/status/1383411082853683208](https://twitter.com/chuanenlin/status/1383411082853683208)", "link": "https://www.reddit.com/r/MachineLearning/comments/msr0cm/p_semantic_video_search_with_openais_clip_neural/"}, {"autor": "fabien-campagne", "date": "2021-04-17 04:09:06", "content": "[P] Multi-modality Perceiver implementation for Pytorch /!/ I forked Phil Wangs' repo implementing Perceiver for Pytorch and fixed two problems with the implementation:\n\n1. The figure in the preprint and text indicate that the cross attention block is followed by a 'Latent Transformer'. The text indicates that the latent transformer has 6 blocks when training on ImageNet. This repo , to be fair, most re-implementations I have seen for Keras or JAX make the same mistake and use a single latent block, not a 'Latent Transformer' made up of several blocks. \n2. The signature of the forward method in this implementation cannot support training with multi-modality inputs. Multi-modality is when you want to train with video, audio and -----> image !!!  for instance as inputs to the same model. The signature does not support multi-modality because each modality has a different number of dimensions, and positional encoding must be applied to each modality independently. This is not possible when accepting a single tensor as input to forward if positional encoding is done in the forward method.\n\nThe repo also offers an experimental contribution to help with text as one of the input modalities.\n\nI have implemented fixes for these two issues in this fork: [https://github.com/fac2003/perceiver-multi-modality-pytorch](https://github.com/fac2003/perceiver-multi-modality-pytorch), the package is available from PyPi (pip install [perceiver-multi-modality-pytorch](https://github.com/fac2003/perceiver-multi-modality-pytorch)).", "link": "https://www.reddit.com/r/MachineLearning/comments/msjpvw/p_multimodality_perceiver_implementation_for/"}, {"autor": "Snoo41531", "date": "2021-04-17 04:04:10", "content": "[D] Graphs in computer vision? /!/ Recently, I've come across papers which use \"graphs\" in computer vision tasks such as -----> image !!!  classification (such as [https://arxiv.org/abs/1904.03582](https://arxiv.org/abs/1904.03582)). I understand what a graph is in the computer science sense, but I don't see how it is related to images. My experience with CV so far has only been with CNNs. Where can I learn more about graphs and their relation to computer vision?", "link": "https://www.reddit.com/r/MachineLearning/comments/msjnbn/d_graphs_in_computer_vision/"}, {"autor": "ubcengineer123", "date": "2021-04-16 20:42:21", "content": "[R] Graphs for training loss per epoch in publications /!/ Hello all,\n\nI'm researching and writing a paper on using ML (CNN architectures) in semantic segmentation for medical imaging. I'm hoping to create figures that look something like [This Graph](https://www.researchgate.net/profile/Mohammad-Pashaei-4/publication/339987654/figure/fig13/AS:870150437883905@1584471439655/Average-loss-per-epoch-for-training-and-validation-steps.ppm), but for me (70,000 training -----> image !!!  patches), the epoch goes to a steady-state value after \\~2-3 epochs? \n\nHas anyone encountered this before? Should I lower the batches per epoch? That won't be accurate because each epoch should consist of the full training dataset. \n\nFew more small details:\n\n* I'm using weighted binary cross entropy to account for positive - negative label imbalance\n* Default Adam optimizer\n\nCan anyone suggest possible solutions or why it's happening? It's not a bad thing per se because the model works, but I won't be able to get nice looking figures if I reaches steady-state so quickly.\n\nThank you!", "link": "https://www.reddit.com/r/MachineLearning/comments/msc1et/r_graphs_for_training_loss_per_epoch_in/"}, {"autor": "technical123123", "date": "2021-04-16 20:25:26", "content": "[PROJECT] Stuck in FYP about diagnosing which disease patient has based on chest X-ray images. Need help and advice. /!/  \n\nHi, I am a final year student in bachelors of computer science. My FYP is based on training a convolutional neural network which will detect which disease a patient has based on their chest x-ray -----> image !!! . Initially there were 9 diseases including COVID-19 and Pneumonia. Me and my group members trained a model using Inception v3. However, the results were not that great. Of course, they were not be great because in the first part we were just trying to find the best model to use. We didn't apply any image segmentation ourselves. We just gave the chest x-ray images as the input and set the required parameters and let it train. We used early stopping so it didn't train till the end. I should also point out that the dataset was taken from a public repository is not the best dataset. COVID images were limited and we scaled the images of the rest of the diseases down to the amount of COVID images.\n\nNow, the problem is that I don't know what to do now. The results were not that great so what are our options? I know we can increase the dataset. What else? I looked at image segmentation and found out that there is a model called U-Net that is well regarded in this case. Should I segment all the x-ray images using this model?\n\nI want to know what are my options. How can i improve the results? I know detecting COVID is best done using the PCR test. But, its one of the main diseases that we are trying to classify and i don't want to remove it. Any of you out there who have done similar work? Please give some advice. Thankyou.\n\nTLDR:\n\nFYP is about classifying diseases based on chest x-ray images. Trained on inception v3 model but did not get great results. What more to do?", "link": "https://www.reddit.com/r/MachineLearning/comments/msbp8s/project_stuck_in_fyp_about_diagnosing_which/"}, {"autor": "KirillTheMunchKing", "date": "2021-04-16 17:47:33", "content": "[R] Spatially-Adaptive Pixelwise Networks for Fast -----> Image !!!  Translation (ASAPNet) by Shaham et al. - Explained /!/ # [Spatially-Adaptive Pixelwise Networks for Fast Image Translation](https://t.me/casual_gan/27)\n\nThe authors propose \u0430 novel architecture for efficient high resolution image to image translation. At the core of the method is a pixel-wise model with spatially varying parameters that are predicted by a convolutional network from a low-resolution version of the input. Reportedly, an 18x speedup is achieved over baseline methods with a similar visual quality. More details [here](https://t.me/casual_gan/27).\n\n[ASAPNet](https://preview.redd.it/ablptrd5pkt61.png?width=1225&amp;format=png&amp;auto=webp&amp;s=95fed8d1fbaa57d3481a88284604436c6190ecec)\n\n If you are not familiar with the paper check it out over [here](https://t.me/casual_gan/27).", "link": "https://www.reddit.com/r/MachineLearning/comments/ms8ibe/r_spatiallyadaptive_pixelwise_networks_for_fast/"}, {"autor": "post_hazanko", "date": "2021-04-16 16:53:58", "content": "[D] Filling in missing data for bad video on client end /!/ I think that would be an interesting application, I've started to see it where you can make a blurry -----> image !!!  become super clear.\n\nSo I'm wondering if there would be a thing at some point where they integrate a \"model\" on the client side think Web/JS for video transmission... usually what you see is the local/source is crisp and the recipient is a little worse, makes sense.\n\nConcern is how accurate the fill is.", "link": "https://www.reddit.com/r/MachineLearning/comments/ms7du5/d_filling_in_missing_data_for_bad_video_on_client/"}, {"autor": "OnlyProggingForFun", "date": "2021-04-16 15:59:35", "content": "[News] Create 3D Models from Images! AI and Game Development, Design... GANverse3D &amp; NVIDIA Omniverse /!/ Omniverse, NVIDIA, (2021): [https://www.nvidia.com/en-us/omniverse/](https://www.nvidia.com/en-us/omniverse/)\n\nZhang et al., (2020), \"-----> IMAGE !!!  GANS MEET DIFFERENTIABLE RENDERING FOR INVERSE GRAPHICS AND INTERPRETABLE 3D NEURAL RENDERING\": [https://arxiv.org/pdf/2010.09125.pdf](https://arxiv.org/pdf/2010.09125.pdf)\n\nGANverse3D official NVIDIA video: [https://youtu.be/0PQnrnUIBlU](https://youtu.be/0PQnrnUIBlU)\n\nNVIDIA'S GANverse 3D blog article: [https://blogs.nvidia.com/blog/2021/04/16/gan-research-knight-rider-ai-omniverse/](https://blogs.nvidia.com/blog/2021/04/16/gan-research-knight-rider-ai-omniverse/)\n\nWatch the video demo [https://youtu.be/dvjwRBZ3Hnw](https://youtu.be/dvjwRBZ3Hnw)", "link": "https://www.reddit.com/r/MachineLearning/comments/ms67qs/news_create_3d_models_from_images_ai_and_game/"}, {"autor": "xEdwin23x", "date": "2021-01-24 17:28:42", "content": "[P] Animesion: a framework, for anime (and related) character recognition. It uses Vision Transformers trained on a subset of Danbooru2018, that we rebranded as DAF:re, and can classify a given -----> image !!!  into one of more than 3000 characters! Source code and checkpoints included. /!/ For this purpose we revamped an existing dataset, and rebranded it as DAF:re. It contains almost 500 K images divided across 3263 characters, from anime, videogames, and related media.\n\nOur best model, ViT (Vision Transformer) L-16 ,with image size 128x128 and batch size 64, achieves 85.95% and 94.23% top-1 and top-5 classification accuracies.\n\nhttps://preview.redd.it/b19u5feuebd61.png?width=1920&amp;format=png&amp;auto=webp&amp;s=53fc146856e6f970588631ecd0518588e3b164d7\n\nSample GIF of DAF:re\n\n[https://j.gifs.com/ROpp10.gif](https://j.gifs.com/ROpp10.gif)\n\nSource code along with demo images:\n\n[https://github.com/arkel23/animesion/tree/main/classification](https://github.com/arkel23/animesion/tree/main/classification)", "link": "https://www.reddit.com/r/MachineLearning/comments/l43rbj/p_animesion_a_framework_for_anime_and_related/"}, {"autor": "Suspicious_Tomorrow8", "date": "2021-01-24 16:28:36", "content": "help in -----> image !!!  processing course project [R] /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/l42jcu/help_in_image_processing_course_project_r/"}, {"autor": "Suspicious_Tomorrow8", "date": "2021-01-24 16:26:17", "content": "help in -----> image !!!  processing course project /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/l42ho7/help_in_image_processing_course_project/"}, {"autor": "toby__bryant", "date": "2021-01-24 15:28:23", "content": "[D] How to find the right annotation strategy for your visionAI project, something often done wrong. Link to the high-res -----> image !!!  and further explanation is in the first comment", "link": "https://www.reddit.com/r/MachineLearning/comments/l41eq8/d_how_to_find_the_right_annotation_strategy_for/"}, {"autor": "toby__bryant", "date": "2021-01-24 15:21:38", "content": "How to find the right annotation strategy for your visionAI project, something often done wrong. Link to the high-res -----> image !!!  and some explanation is in the first comment", "link": "https://www.reddit.com/r/MachineLearning/comments/l41a4r/how_to_find_the_right_annotation_strategy_for/"}, {"autor": "toby__bryant", "date": "2021-01-24 15:14:18", "content": "[D] How to find the right annotation strategy for your use-case, something often done wrong. Link to the high-res -----> image !!!  and further explanation is in the first comment", "link": "https://www.reddit.com/r/MachineLearning/comments/l4159w/d_how_to_find_the_right_annotation_strategy_for/"}, {"autor": "KirillTheMunchKing", "date": "2021-08-23 16:08:57", "content": "[D] Paper explained - Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans by Sida Peng et al. 5 minute summary. /!/ [Full-body 3D avatar](https://i.redd.it/uj9ar17sq4j71.gif)\n\nWant to dance like a pro? Just fit a neural body to a sparse set of shots from different -----> camera !!!  poses and animate it to your heart's desire! This new human body representation is proposed in a CVPR 2021 best paper candidate work by Sida Peng and his teammates. At the core of the paper is the insight that the neural representations of different frames share the same set of latent codes anchored to a deformable mesh. Neural Body outperforms prior works by a wide margin. \n\nRead the [5 minute digest](https://t.me/casual_gan/87) or the [blog post](https://www.casualganpapers.com/implicit-neural-representation-full-body-avatar-novel-view-synthesis/Neural-Body-explained.html) (reading time \\~5 minutes) to learn about structured latent codes, latent code diffusion, Density and color regression, and Volume rendering.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://www.casualganpapers.com/)!\n\n*Processing img 4dzhlggur4j71...*\n\n\\[[Full Explanation](https://t.me/casual_gan/87) / [Blog Post](https://www.casualganpapers.com/implicit-neural-representation-full-body-avatar-novel-view-synthesis/Neural-Body-explained.html)\\] \\[[Arxiv](https://arxiv.org/pdf/2012.15838.pdf)\\] \\[[Code](https://github.com/zju3dv/neuralbody)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n&gt;\\[[3D-Inpainting](https://www.casualganpapers.com/single_view_layered_depth_3d_inpainting/3D-Inpainting-explained.html)\\]  \n&gt;  \n&gt;\\[[StyleGAN-NADA](https://t.me/casual_gan/83)\\]  \n&gt;  \n&gt;\\[[Sketch Your Own GAN](https://t.me/casual_gan/81)\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/pa2yxs/d_paper_explained_neural_body_implicit_neural/"}, {"autor": "Abu_BakarSiddik", "date": "2021-08-23 15:17:35", "content": "[D] What is the fastest way to convert audio to -----> image !!!  for CNN? /!/ I am doing an audio classification task. My intention is to convert audios to images and then apply CNN. But converting from audio to image taking huge time. What is the fastest way to do this?", "link": "https://www.reddit.com/r/MachineLearning/comments/pa1x2c/d_what_is_the_fastest_way_to_convert_audio_to/"}, {"autor": "Abu_BakarSiddik", "date": "2021-08-23 15:11:58", "content": "What is the fastest way to generate -----> image !!!  from audio? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pa1sxc/what_is_the_fastest_way_to_generate_image_from/"}, {"autor": "pabloavocado23", "date": "2021-08-23 15:05:27", "content": "[D] Instance classification with known segmentation masks /!/ Hi,\nI've got a problem where I have a set of images with sementation masks available at both train time, and in production. Each -----> image !!!  has 5-20 region masks and I'm trying to make a classification prediction for each region.\n\nI've tried the approach here with poor results - the attention valve layer coefficients stay close to 1 and as such the prediction for every instance in the image is very similar. It's effectively learning to make a single prediction for every instance in the image.\n\nMy other thought is to just go with a normal Fast RCNN model and have it learn the instance masks as well, although I'm sure this is very wasteful.\n\nCan you think of a good approach to combine the known mask information with a Faster RCNN model? Like removing the Region Pooling layer and feeding the mask in as an input rather than have it be learnt.", "link": "https://www.reddit.com/r/MachineLearning/comments/pa1o5v/d_instance_classification_with_known_segmentation/"}, {"autor": "theahmedmustafa", "date": "2021-08-23 06:21:32", "content": "[D] Recommended Architecture for Text Span Detection /!/ I am looking to for any state of the art NN architecture that I can train to detect the Bounding Box around each span of text in an -----> Image !!! .\n\nI realize the word \"span\" is rather vague but the idea is that each span is basically a continuous chain of written text on a single line. I am looking for recommended architectures that can help me achieve it, ideally independent of the text script.\n\nI am beginner in ML and this subreddit so if this posts needs any clarification, kindly inform me so.", "link": "https://www.reddit.com/r/MachineLearning/comments/p9tyr2/d_recommended_architecture_for_text_span_detection/"}, {"autor": "Fr33akBoy", "date": "2021-08-23 02:38:17", "content": "[D] How to combine visual features + time features for vehicle re-identification in real time? /!/ I want to use a neural network to reidentify vehicles. The problem I want to tackle is something similar to the next -----> image !!! :\n\n&amp;#x200B;\n\n[Image from: https:\\/\\/www.shutterstock.com\\/video\\/search\\/marquis](https://preview.redd.it/zgp4ot61s0j71.png?width=640&amp;format=png&amp;auto=webp&amp;s=a959a63a9fbd8ad4c6ad4c9456a149c44e123b89)\n\nA car can only enter from camera 1 and can exit at any point and time (sometimes they might not make it to camera 2). Some cars may also enter through a blind spot and they should not be re-identified as the same vehicle in camera 1\n\n&amp;#x200B;\n\nThe current setup I have is using a siamese network to classify whether two vehicles are the same using the vehicle best shots. The classification is done after I filter some impossible cases (like two cars in two different cameras at the same time or the car that left camera 1 got to camera 2 \"too fast\" so it can't be the same car ), however, I don't think it is practical to code all these rules as it might very well be the case where two cameras have an overlapping view or it might just be that the traffic is slow.\n\nI guess my question is how to combine the visual features (i.e best shots) + timing elements to classify the likelihood of two vehicles being the same in this case?", "link": "https://www.reddit.com/r/MachineLearning/comments/p9qnid/d_how_to_combine_visual_features_time_features/"}, {"autor": "nonchalantno1", "date": "2021-08-22 20:53:39", "content": "[P] [R] An -----> Image !!! -based Generator Architecture for Synthetic -----> Image !!!  Refinement /!/ &amp;#x200B;\n\n[Original Doom avatar - original vs refined](https://preview.redd.it/om0k5kf71zi71.png?width=228&amp;format=png&amp;auto=webp&amp;s=6c3ad7e5339fb4da7f16f9920b3620f39f5a4c2a)\n\n[Pixel-Me generated avatar - original vs refined](https://preview.redd.it/m8q2jlba1zi71.png?width=228&amp;format=png&amp;auto=webp&amp;s=b7855015bc4ff376c70f0976b6c914b06910eccb)\n\n[Fallout 4 character - original vs refined](https://preview.redd.it/30z8645l1zi71.png?width=228&amp;format=png&amp;auto=webp&amp;s=7143aa949755134138d2d9fb7e5d4c29f17a5071)\n\nI recently wrote a paper and proposed a simple new, easy-to-train, generator architecture based Boundary Equilibrium Generative Adversarial Networks (BEGAN) and inspired by Learning from Simulated and Unsupervised Images through Adversarial Training (SimGAN)\n\n&amp;#x200B;\n\n* Paper: [https://arxiv.org/abs/2108.04957](https://arxiv.org/abs/2108.04957)\n* Git: [https://github.com/consequencesunintended/RefinementGAN](https://github.com/consequencesunintended/RefinementGAN)", "link": "https://www.reddit.com/r/MachineLearning/comments/p9kqb5/p_r_an_imagebased_generator_architecture_for/"}, {"autor": "AIforimaging", "date": "2021-08-19 09:05:06", "content": "[D] Smartphone for Snapdragon Insiders -----> camera !!!  review", "link": "https://www.reddit.com/r/MachineLearning/comments/p7b85n/d_smartphone_for_snapdragon_insiders_camera_review/"}, {"autor": "tyrellxelliot", "date": "2021-08-19 03:47:22", "content": "[P] Graphic design color generation with transformers and DDPM /!/ link: https://huemint.com/about/\n\nThis is a project I made for generating colors for graphic design. It's a simple application of conditional -----> image !!!  generation, where the input are the design requirements and the output are the colors (in the form of a 1 dimensional -----> image !!! )\n\nI implemented 3 separate algorithms for comparison:\n\n- random (a heuristic baseline)\n\n- transformer (an encoder-decoder transformer, which outputs quantized tokens similar to -----> image !!!  GPT)\n\n- DDPM (the recently-released improved diffusion model from OpenAI)\n\nhave a look and let me know what you think!", "link": "https://www.reddit.com/r/MachineLearning/comments/p778ad/p_graphic_design_color_generation_with/"}, {"autor": "PanSQ2021", "date": "2021-08-18 22:48:44", "content": "[D] Can anyone recognize the 3rd -----> image !!!  in the Fashion MNIST training set as a T-shirt? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/p727ro/d_can_anyone_recognize_the_3rd_image_in_the/"}, {"autor": "KirillTheMunchKing", "date": "2021-08-18 18:39:44", "content": "[D] Make AI paint any -----> photo !!!  - Paint Transformer: Feed Forward Neural Painting with Stroke Prediction by Songhua Liu et al. explained in 5 minutes /!/ [So pretty!](https://i.redd.it/rrqak1scu5i71.gif)\n\nAfter seeing Paint Transformer gifs for two weeks now all over Twitter, you know, I had to cover it. Anyways, Songhua Liu et al. present a cool new model that can \"paint\" any image, and boy, the results are PRETTY. The painting process is an iterative method that predicts parameters for paint strokes in a coarse-to-fine manner, progressively refining the synthesized image. The whole process is displayed as a dope painting time-lapse video with brush strokes gradually forming an image.\n\nRead the [full paper digest](https://t.me/casual_gan/85) or the [blog post](https://www.casualganpapers.com/image-to-painting-visual-transformer-stroke-prediction/Paint-Transformer-explained.html) (reading time \\~5 minutes) to learn about the Paint Transformer framework, Stroke Prediction techniques, Stroke rendering, the various losses used to train the model, and how to inference Paint Transformer to make these beautiful gifs!\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://www.casualganpapers.com/)!\n\n[The paper is not as hard as it looks, I promise!](https://preview.redd.it/8nwa4k13v5i71.png?width=1868&amp;format=png&amp;auto=webp&amp;s=88bf1d08f9d04e8e198b916c1718e56f642f0845)\n\n\\[[Full Explanation](https://t.me/casual_gan/85) / [Blog Post](https://www.casualganpapers.com/image-to-painting-visual-transformer-stroke-prediction/Paint-Transformer-explained.html)\\] \\[[Arxiv](https://arxiv.org/pdf/2108.03798.pdf)\\] \\[[Code](https://github.com/huage001/painttransformer)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n&gt;\\[[3D-Inpainting](https://www.casualganpapers.com/single_view_layered_depth_3d_inpainting/3D-Inpainting-explained.html)\\]  \n&gt;  \n&gt;\\[[StyleGAN-NADA](https://t.me/casual_gan/83)\\]  \n&gt;  \n&gt;\\[[Sketch Your Own GAN](https://t.me/casual_gan/81)\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/p6xc74/d_make_ai_paint_any_photo_paint_transformer_feed/"}, {"autor": "KirillTheMunchKing", "date": "2021-07-11 15:47:56", "content": "[D] GRAF: Generative Radiance Fields for 3D-Aware -----> Image !!!  Synthesis by Katja Schwarz et al. explained in 5 minutes.", "link": "https://www.reddit.com/r/MachineLearning/comments/oi6v65/d_graf_generative_radiance_fields_for_3daware/"}, {"autor": "KirillTheMunchKing", "date": "2021-07-11 15:47:31", "content": "GRAF: Generative Radiance Fields for 3D-Aware -----> Image !!!  Synthesis by Katja Schwarz et al. explained in 5 minutes.", "link": "https://www.reddit.com/r/MachineLearning/comments/oi6uv9/graf_generative_radiance_fields_for_3daware_image/"}, {"autor": "KirillTheMunchKing", "date": "2021-07-11 15:46:13", "content": "[D] Explained in 5 minutes - Deferred Neural Rendering: -----> Image !!!  Synthesis using Neural Textures by Justus Thies et al. /!/ How can we synthesize images of 3d objects with explicit control over the generated output with only limited imperfect 3d input available (for example from several frames in a video)? Justus Thies and his colleagues propose a new paradigm for image synthesis called Deferred Neural Rendering that combines the traditional graphics pipeline with learnable components called Neural Textures, which are feature maps stored on top of 3d mesh proxies. The new learnable rendering pipeline utilizes the additional information from the implicit 3d representation to synthesize novel views, edit scenes, and do facial reenactment at state-of-the-art levels of quality.\n\nRead the [full paper digest](https://t.me/casual_gan/66) (reading time \\~5 minutes) to learn about computer graphics pipelines, learnable neural textures, how they are sampled, and rendered by a deferred neural renderer that can be used for novel view synthesis, scene editing, and animation synthesis.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://t.me/casual_gan)!\n\n[Deferred Neural Rendering explained](https://preview.redd.it/q1j6gygutla71.png?width=796&amp;format=png&amp;auto=webp&amp;s=67573b4eeeb8f6ef6fdc1f045d54483a0ebb2ec8)\n\n\\[[Full Explanation Post](https://t.me/casual_gan/66)\\] \\[[Arxiv](https://arxiv.org/pdf/1904.12356.pdf)\\] \\[[Code](https://github.com/SSRSGJYD/NeuralTexture)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n&gt;\\[[Alias-free GAN](https://t.me/casual_gan/58)\\]  \n&gt;  \n&gt;\\[[GIRAFFE](https://t.me/casual_gan/63)\\]  \n&gt;  \n&gt;\\[[GRAF](https://t.me/casual_gan/61)\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/oi6u0u/d_explained_in_5_minutes_deferred_neural/"}, {"autor": "Zealousideal-Bar1892", "date": "2021-07-11 05:01:45", "content": "[D] What should I call this technology? /!/ I have rough concept but can't figure out how to materialize it.\n\nI am interested in 'change applying'.\n\nSo to say, I would prepare bunch of -----> image !!!  pairs\n\n(original -----> image !!! , state\\_added -----> image !!! ) such as (normal face expression, sad face expression(which is added 'sad' state))\n\nand I train machine to learn what 'sad' state have maden differences between original -----> image !!!  and state\\_added -----> image !!! .\n\nAnd then, hopefully, train machine to apply that differences to new image(which would be user input) properly.\n\nDid my question make sense? Sorry for my poor english", "link": "https://www.reddit.com/r/MachineLearning/comments/ohy8dh/d_what_should_i_call_this_technology/"}, {"autor": "Kobedoggg", "date": "2021-05-18 12:29:58", "content": "When you\u2019ve found the \u2018optimal solution\u2019 but have missed the bigger -----> picture !!! . What\u2019s the worst example of this you have seen?", "link": "https://www.reddit.com/r/MachineLearning/comments/nf9ggg/when_youve_found_the_optimal_solution_but_have/"}, {"autor": "Professional_Fox1206", "date": "2021-05-18 12:15:31", "content": "How can I convert a scene segmentation mask -----> image !!!  shape=(n,m,3) to shape=(n,m, number of classes) for scene segmentation? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/nf95w5/how_can_i_convert_a_scene_segmentation_mask_image/"}, {"autor": "KirillTheMunchKing", "date": "2021-07-08 16:21:28", "content": "[D] CVPR 2021 Best Paper (GIRAFFE) explained: Representing Scenes as Compositional Generative Neural Feature Fields by Michael Niemeyer et al. /!/ [Multi-object generation](https://i.redd.it/47danckuj0a71.gif)\n\n*Processing gif 755jlhiij0a71...*\n\n*Processing gif 8r4av0jqj0a71...*\n\nIf you thought GRAF did a good job at 3d-aware -----> image !!!  synthesis just wait until you see the samples from this model by Michael Niemeyer and colleagues at the Max Planck Institute. While generating 256x256 resolution images does not sound that impressive in 2021, leveraging knowledge about the 3D nature of real world scenes to explicitly control the position, shape, and appearance of objects on the generated images certainly is exciting. So, did GIRAFFE deservedly win the best paper award at the recent CVPR 2021?  \n\nRead the [full paper digest](https://t.me/casual_gan/63) (reading time \\~5 minutes) to learn about latent object representation that allows for controlled 3d-aware multi-object synthesis (rotation, translation, shape, appearance), and how to combine techniques from neural volume and image rendering to work with 256x256 Neural Feature Fields in a memory constrained setting.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://t.me/casual_gan)!\n\n[GIRAFFE](https://preview.redd.it/nkvxhgp6l0a71.png?width=2218&amp;format=png&amp;auto=webp&amp;s=8488d4972d86bfcf4ef597d7f175c93ce4e7319b)\n\n \n\n\\[[Full Explanation Post](https://t.me/casual_gan/63)\\] \\[[Arxiv](http://www.cvlibs.net/publications/Niemeyer2021CVPR.pdf)\\] \\[[Code](https://github.com/autonomousvision/giraffe)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n&gt;\\[[Alias-free GAN](https://t.me/casual_gan/58)\\]  \n&gt;  \n&gt;\\[[GFPGAN](https://t.me/casual_gan/54)\\]  \n&gt;  \n&gt;\\[[GRAF](https://t.me/casual_gan/61)\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/ogamdv/d_cvpr_2021_best_paper_giraffe_explained/"}, {"autor": "PedroGGBM", "date": "2021-07-08 14:11:16", "content": "Categorizing building types using transfer learning (-----> image !!!  clustering) help /!/ Hello! I'm a high school student with limited knowledge of ML. I currently have this image dataset (5,000+) of buildings (bars, hotels, summer resorts, etc) from an architecture firm. \n\nI was hoping I could categorize these according to the building types and/or according to common architectonical features (pools, garden, etc). My idea was to use a model for image clustering using transfer learning. However, I am dubious about its validity, seeing as I am not an expert. \n\nThese images contain building types ranging from restaurants, bars, and hotels to vineyards, lodge cabins, etc. I doubt whether an ML is appropriate at all. Any recommendations are appreciated. Thank you :)", "link": "https://www.reddit.com/r/MachineLearning/comments/og7ztm/categorizing_building_types_using_transfer/"}, {"autor": "PedroGGBM", "date": "2021-07-08 14:09:01", "content": "Categorizing building types using transfer learning (-----> image !!!  clustering) help /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/og7yak/categorizing_building_types_using_transfer/"}, {"autor": "jj4646", "date": "2021-07-17 05:00:23", "content": "[D] Backbox Optimization /!/ Has anyone ever come across any examples of \"blackbox optimization\" in machine learning? \n\nI have read examples online where algorithms such as Bayesian Optimization and the Genetic Algorithm are stated to be advantageous for optimizing \"blackbox\" functions - yet I am trying to think of applications where this is truly necessary.\n\n Just as a question - has anyone ever worked on a problem that involved \"derivative free optimization\" with \"blackbox functions\"? What made your problem require \"derivative free optimization\"? Which techniques did you end up using? What kind of problems were these (e.g. -----> image !!!  classification)? \n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/olxt83/d_backbox_optimization/"}, {"autor": "orenog", "date": "2021-07-17 01:41:41", "content": "[D] Idea for tomorrow - Make a StarWars model of StyleGAN2-ada /!/ Hey, I just thought about a crazy idea, tell me if it's stupid so I delete it from my head (and why it's stupid) \n\nI want to take the first 3 StarWars movies (only watched them so far, so no spoilers please)\n\nAnd take a frame from every 5 second of the -----> film !!!  (about 1500 images per -----> film !!! )\n\nAnd train StyleGAN2-ada on these images. \n\nDo you think it will look good, or is it just waste of time and google's electricity? (Colab pro)\n\nI would like to hear your opinions before I start.\n\nThank you \u263a\ufe0f", "link": "https://www.reddit.com/r/MachineLearning/comments/oluvgc/d_idea_for_tomorrow_make_a_starwars_model_of/"}, {"autor": "nielsrolf", "date": "2021-07-16 21:20:04", "content": "[P] I made a tool for tracking hyperparameters, metrics and artifacts, and I like it better than tensorboard or mlflow /!/ Hi r/MachineLearning\n\nI made \\[this\\]([https://github.com/nielsrolf/pandas\\_db/](https://github.com/nielsrolf/pandas_db/)) - a tool that can be used to save any tabular data and files with associated metadata. There is also a small UI, mainly for browsing through images and audio files.\n\n**Why not use tensorboard?**\n\nTensorboard provides a bunch of graphs, but if you want to plot anything that isn't in there, you're going to have a hard time since it is not easily possible to export all tensorboard data into one dataframe. Another issue is that if you create many artifacts, it is hard to find the -----> image !!!  or audio file that you are looking for.\n\n**Why not use MLFlow?**\n\nMLFlow is nice, but again it is hard to find and compare media files associated with certain metadata and if you create a lot of data, it gets unusably slow. I also don't like that it comes with the concept of experiments and runs - if my eval script is a separate script from the training script, I don't want the created data to be associated with a new run.\n\n&amp;#x200B;\n\nIf anyone would like to try it and/or give feedback, I would be happy. Just don't use it from two processes in parallel - I can add that feature if anyone is interested, but since I didn't need it yet and I don't know if anyone else wants to use it, I didn't invest the time yet. Same for documentation - I am happy to add documentation if something is unclear and there are users, but I didn't do much in that regard yet.", "link": "https://www.reddit.com/r/MachineLearning/comments/olqb64/p_i_made_a_tool_for_tracking_hyperparameters/"}, {"autor": "Dovejannister", "date": "2021-07-16 09:23:02", "content": "[D] Best way of calculating loss (and making a prediction) of a 2D plane bisecting 3D data? /!/ Let's say your data samples are 3D objects of H\\*W\\*D, (so a 4D tensor when you include a channel dimension, but we'll just pretend 3D for this question).\n\nLet's say you want to predict a 2D plane through this 3D object, i.e. a way to \"slice\" the 3D object in 2, how would you do it? E.g. the label is the coloured plane in [this image](https://s3.eu-west-2.amazonaws.com/furthermathstutor.co.uk/study/flat_surface.png).\n\nThat 2D plane could be defined by any 3 points within it (and could be done using a normal keypoint detection method with regression, or MSE loss of heatmaps, like in pose estimation). However, unlike with keypoint detection/pose estimation, are are an infinite number of valid 3 point combinations that could define that plane.\n\nOne obvious solution is to \"draw\" a 2D plane in a 3D tensor as my label (maybe as a sort of gaussian thing so it's fuzzy at the edges, like they use in pose estimation for points ([image](https://lh6.googleusercontent.com/dv9cWpHBRofQdfXSMYyHt-37LUowHBvRz3IGJT1tziXha33txCOded4RGy3JIHyCBj9JDlFPexCIL60M2G8WpgGa5LiZCJWl8W8KFlSzsVToZLvngB503fBcpshYrfZ5BwvMul8g))), get the network to predict this 3D tensor (like a segmentation problem), and then use MSE loss/Crossentropyloss.\n\nHowever, I don't feel this is the most elegant way of \"encoding\" that plane, and instead a regression based approach feels better to me.\n\nOne option I thought of initially was maybe I could regress the 4 \"corners\" of the plane, so in [this image](https://s3.eu-west-2.amazonaws.com/furthermathstutor.co.uk/study/flat_surface.png) it would be something like \\[\\[-10,45\\], \\[-10,20\\], \\[10,-20\\], \\[-10, -10\\]\\], a bit like bounding box regression. But I feel some method that encodes the two \"angles\" of the plane, and then maybe a central \"anchor\" in the centre of the -----> image !!!  could be better...\n\nIs anyone aware of any previous approaches to similar problems? Google didn't seem to be able to help me...", "link": "https://www.reddit.com/r/MachineLearning/comments/oldcpv/d_best_way_of_calculating_loss_and_making_a/"}, {"autor": "wannabeGiant", "date": "2021-07-25 07:46:14", "content": "[D] Learning respresentation of multimodal input independently using a single model /!/ Hi,\n\nGiven images and videos, need to rank these for a specific user. All the multimodal approaches I looked at include multimodality for each data point i.e. video and text together are one data point OR -----> image !!!  and text together are one data point, say we use late fusion get a good combined representation and then learn over that representation. \n\nBut what if we need a single representation model to learn over each video, image OR text which are different data points. How do I approach this problem?\n\nFor example: \nVideos {V1, V2, ........}\nImages { I1, I2, .........}\nWhere Vs and Is are different i.e. no relation between V1 and I1\n\nNeed to learn representation of V1, V2, I1, I2... etc using the same network with different input structures.\n\nThanks in advance", "link": "https://www.reddit.com/r/MachineLearning/comments/or7e5y/d_learning_respresentation_of_multimodal_input/"}, {"autor": "crayola_leap", "date": "2021-07-25 03:58:57", "content": "[N] AI Olympics Judge /!/ The 2021 Olympics are here. Now you can upload Olympics Diving videos and see how an AI judge would score them. Check it out!\n\nTry out on your own diving videos as well! Just make sure to keep the -----> camera !!!  stable and the diver in the center of the frame.\n\nThis  machine learning model is only for diving videos for now but it can be  easily extended to other sports like Gymnastics, Snowboarding, Skiing,  etc. -- just need to change the model weights appropriately.\n\nIt is based on this multitask action quality assessment work:\u00a0[https://openaccess.thecvf.com/content\\_CVPR\\_2019/html/Parmar\\_What\\_and\\_How\\_Well\\_You\\_Performed\\_A\\_Multitask\\_Learning\\_Approach\\_CVPR\\_2019\\_paper.html](https://openaccess.thecvf.com/content_CVPR_2019/html/Parmar_What_and_How_Well_You_Performed_A_Multitask_Learning_Approach_CVPR_2019_paper.html).\n\n&amp;#x200B;\n\n[C3D-AVG-MTL model.](https://preview.redd.it/9s9c9c5g8ad71.png?width=1788&amp;format=png&amp;auto=webp&amp;s=9c033a29fa895eace2e76d940a426daa0a1d5ad7)", "link": "https://www.reddit.com/r/MachineLearning/comments/or4jiw/n_ai_olympics_judge/"}, {"autor": "uniform_convergence", "date": "2021-07-24 21:11:30", "content": "[D] What does your cloud/docker workflow look like? How do you get anything done with these tools? /!/ I feel like I'm going a little crazy so I wanted to ask this sub as it seems like there are a fair number of people here in relatively mature ML environments. I used to work at a company that had a fairly old-school development workflow (as in, modern in 2015). Individual DS/ML researchers would have fairly beefy personal machines (32+GB of RAM, big CPU, xx80 or two if necessary). We were not doing any groundbreaking DL research or anything but working on training data of usually a few GB. We used docker but it was more of an engineering thing than anything I dealt with day to day. Personally I never had trouble with just sharing environment files. We would use some shared HPC resources but again, it wasn't anything too sophisticated (and imo we didn't need anything like that). \n\nNow, I work at a company that is supposedly more \"modern\" using the cloud and docker-first (similar scale of data and problem type), but the I find the workflow soooo slow and hard to deal with. Here, we do all the development inside cloud docker containers themselves. But it seems like there are 25 micro-steps to even get started on a project and so much overhead for such little gain. Okay let me spin up instance, ssh, pull docker -----> image !!! , mount shared drive, forward port, git clone, etc etc etc. Can't use PyCharm or even VS Code since the dev environment is inside a docker on a remote machine (I know you can technically do this with VS Code, but it's finnicky imo). Every day I am dealing with micro-problems arising from this. And at the end, when it comes time to deploy, I still need to ping MLOps guys to do anything useful with the image. I feel like I spend half my day on this and half my day on the actual problem at hand. What exactly is the point of all this? Are we even saving money on all these cloud instances being spun up and down?\n\nAnyway, I asked my boss about this, and he doesn't see a problem since everyone else on the team is a linux ninja and just has a million little aliases and copy/paste cli routines to do things, and knows all the gotchas and they get work done, so why is it a problem? Why am I a special snowflake who NEEDS to use modern IDE like PyCharm rather than being happy with a simple notebook? Does everyone on modern ML teams work like this? Does this org need to hire more MLOps/Engineers or get one of these fancy cloud solutions? Seems a lot more expensive than just specing out a beefy desktop machine where everything is dead simple and easy to manage. This company is not Uber or Facebook, we are not working at global scale or preparing for hyper-growth. \n\n/Rant!", "link": "https://www.reddit.com/r/MachineLearning/comments/oqyb7g/d_what_does_your_clouddocker_workflow_look_like/"}, {"autor": "mixtilinear", "date": "2021-07-24 21:05:51", "content": "[P] Am I losing out using google colab? /!/ I am approaching the stage where I'm trying to tweak parameters and improve a model that seems to be working in google colab. It's an inpainter using a pretrained StyleGAN and to inpaint it takes about 4 mins per -----> image !!! .\n\nThroughout the project I've had SSH access to a GPU (specs: [https://www.maths.cam.ac.uk/computing/faculty-hpc-system-fawcett](https://www.maths.cam.ac.uk/computing/faculty-hpc-system-fawcett), and about 1/4 of the time there is an annoying queue), though have found myself repeatedly coming back to google colab.\n\nMy question is: am I losing out? I've loved the quick experimentation nature of colab, and even with the VSCode SSH extension I find the file transport stuff much smoother in colab (without sudo access on the server, and still being a terminal n00b to some extent). But it seems that colab is mostly recommended for very 'toy' experiments, and I'd like to think a project that has potential to beat benchmarks isn't a 'toy'!", "link": "https://www.reddit.com/r/MachineLearning/comments/oqy7nd/p_am_i_losing_out_using_google_colab/"}, {"autor": "ymishima", "date": "2021-06-02 08:33:24", "content": "[D] Secure cloud providers which currently have v100 or better availability /!/ I've been using a gcp  v100 instance, but I've been having loads of trouble with availability, its been pretty much impossible to start a v100 instance the last week in any global region.\n\nCan someone suggest a secure cloud provider that I can boot up a v100/equivalent power gpu at the moment? And that I don't need any prior billing history with. Bonus if it has a deep learning vm -----> image !!!  with Jupiter and pytorch/nvidia drivers installed.", "link": "https://www.reddit.com/r/MachineLearning/comments/nqh15a/d_secure_cloud_providers_which_currently_have/"}, {"autor": "tzar-bucks", "date": "2021-06-02 06:23:25", "content": "[D] -----> Image !!!  Captioning: why is there no implementation code available online for microsoft's \"VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning\"? /!/ Paper [here](https://arxiv.org/abs/2009.13682).\n\nPlease forgive the ignorance I'll undoubtedly reveal with this post, I'm still very much learning!\n\nLast year, Microsoft researchers published a new method of generating image captions that, by some measures, reportedly outperforms humans.\n\nMany of the previous state-of-the-art models (e.g. Show, Attend and Tell, UpDown, etc) have project code available online, either from the authors or someone else, and I'm curious why there's absolutely nothing for VIVO. Is there a specific reason, like huge training resource requirements? Or has no one yet taken the time to implement it for sharing?", "link": "https://www.reddit.com/r/MachineLearning/comments/nqf5bn/d_image_captioning_why_is_there_no_implementation/"}, {"autor": "keonlee9420", "date": "2021-06-02 04:36:12", "content": "[D] Training tips on diffusion models (DiffSinger) for a TTS? /!/ Hi all,\n\nI'm currently playing with \\[DiffSinger\\]([https://arxiv.org/abs/2105.02446](https://arxiv.org/abs/2105.02446)), which is a TTS system extended by diffusion models. For the naive version, It consists of encoders (for embedding text and pitch information) and a denoiser where the encoders' output is used to condition the denoiser. Everything is similar to diffwave including denoiser's structure and prediction but the neural net to predict epsilon would be changed to \\`epsilon(noisy\\_spectrogram, encoder\\_outputs, diffusion\\_step)\\` compared to \\[DiffWave\\]([https://arxiv.org/pdf/2009.09761.pdf](https://arxiv.org/pdf/2009.09761.pdf))'s \\`epsilon(noisy\\_audio, upsampled\\_spectrogram, diffusion\\_step)\\`.\n\nWhile I'm successfully training encoders, I got an issue during training denoiser. I used LJSpeech. Here is what I did:\n\n1. First of all, as a preliminary experiment, I try to check all modules to work well by setting denoiser as \\`epsilon(noisy\\_spectrogram, clean\\_spectrogram, diffusion\\_step)\\` to predict the \\`noisy\\_spectrogram\\`.\n\n2. After the model converges, I went back to the denoiser of \\`epsilon(noisy\\_spectrogram, encoder\\_outputs, diffusion\\_step)\\` to predict clean\\_spectrogram. I detached the encoders\\_output from the auto\\_grad when the input (to prevent from updating) to the denoiser to fix the conditioner for model convergence. The model was broken when I didn't detach (allow the encoder to be updated during denoiser training).\n\n3. I found that when the range of the conditioner (encoder\\_outputs) values is smaller, then the model shows better evidence of successful training.\n\nBellows are the results I've got so far. The upper one is the sampled (synthesized) mel-spectrogram, and the lower one is the ground truth of each -----> image !!! .\n\n1. I can see the model converge during the primary experiment:\n\n!\\[image\\]([https://user-images.githubusercontent.com/34522972/120421006-76c90000-c3a0-11eb-8926-da2228f695b6.png](https://user-images.githubusercontent.com/34522972/120421006-76c90000-c3a0-11eb-8926-da2228f695b6.png))\n\n2. When the encoder's output directly input to the denoiser (value range: -9.xxx to [6.xxx](https://6.xxx/)):\n\n!\\[image\\]([https://user-images.githubusercontent.com/34522972/120420098-9e1ecd80-c39e-11eb-89b3-c4bd662afbe9.png](https://user-images.githubusercontent.com/34522972/120420098-9e1ecd80-c39e-11eb-89b3-c4bd662afbe9.png))\n\n3. When the encoder's output is multiplied by 0.01 to shrink the range:\n\n!\\[image\\]([https://user-images.githubusercontent.com/34522972/120420247-f8b82980-c39e-11eb-86be-85380dab6178.png](https://user-images.githubusercontent.com/34522972/120420247-f8b82980-c39e-11eb-86be-85380dab6178.png))\n\nFor case 2., It shows any clues on training. On contrary, the case 3. shows 'some' levels of training but it is not what we expected. I double-checked the inference part (reverse part), but it is exactly the same as that of 1. and diffwave.\n\nSo I just want to know if you have any idea on the successful conditions of the input conditioner of the denoiser. Why does the model show such an unsatisfying result above? Do I miss something to process the conditioner?\n\nI will appreciate all suggestions or sharing of your experience.\n\nThanks in advance.", "link": "https://www.reddit.com/r/MachineLearning/comments/nqdg6u/d_training_tips_on_diffusion_models_diffsinger/"}, {"autor": "thisisdhruvagarwal", "date": "2021-06-02 04:08:12", "content": "What can I do so that my deepfake final video doesn't looks like a cut-paste face pasted on someone's -----> image !!! ? [D] /!/ My final deepfake video looks like that I have cutted someones face and pasted it on someone else's image.... I think that the reason this is happening is because the background of both the images is not the same... What can I do stop this?", "link": "https://www.reddit.com/r/MachineLearning/comments/nqcyzg/what_can_i_do_so_that_my_deepfake_final_video/"}, {"autor": "Leved4", "date": "2021-01-11 21:54:52", "content": "[D] Question about -----> image !!!  upscaling via AI /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/kvd5fs/d_question_about_image_upscaling_via_ai/"}, {"autor": "[deleted]", "date": "2021-01-11 21:53:05", "content": "Question about -----> image !!!  upscaling via AI", "link": "https://www.reddit.com/r/MachineLearning/comments/kvd437/question_about_image_upscaling_via_ai/"}, {"autor": "xdtolm", "date": "2021-01-11 16:31:42", "content": "[R] Real-time -----> image !!!  registration on GPU with VkFFT library /!/ Hello, I would like to share my take on the real-time image registration problem on GPU. Image registration is an important problem in computer vision and image processing. If we have two images of the same object, this problem can be formulated as: how can we determine a coordinate system transformation, that allows us to match the displayed object? In the post below I use the [VkFFT library](https://github.com/dtolm/VkFFT) to accelerate the well-known FFT-based phase correlation method of translation, rotation and scaling detection to the point that matching can be performed in real-time. Feel free to ask any questions!\n\n[https://towardsdatascience.com/real-time-image-registration-on-gpu-with-vkfft-library-c4e47f8050a0](https://towardsdatascience.com/real-time-image-registration-on-gpu-with-vkfft-library-c4e47f8050a0)", "link": "https://www.reddit.com/r/MachineLearning/comments/kv64qp/r_realtime_image_registration_on_gpu_with_vkfft/"}, {"autor": "dondraper36", "date": "2021-01-11 16:31:31", "content": "[Discussion] Entity extraction + table extraction from documents (imaged-based, various layouts and quality) /!/ I have been recently involved in a very ambitious project related to extracting entities (key/value pairs) and also tables (for the most part, line items) from documents (invoices, delivery notes, packing lists, etc.)\n\nWe have PDFs as inputs, but those are -----> image !!! -based PDF files so using tools like Tabby and Camelot is not possible.   \nWe have collected (but not yet annotated) a solid dataset (\\~500k pages) with documents from the production environment among which there can be plenty of different layouts and scan quality levels. \n\n[https://arxiv.org/abs/1912.13318](https://arxiv.org/abs/1912.13318)  \n[https://arxiv.org/abs/2004.07464](https://arxiv.org/abs/2004.07464)\n\nFor field extraction, the models above (LayoutLM and PICK) seem to be quite promising, at least according to their results on the SROIE dataset. Currently, I am waiting for our data to be properly annotated to conduct some experiments and see what works best. \n\nThat said, there might be some approaches I am missing or overlooking. One of the tricky questions is whether we should annotate documents on a word level or rather on an entity level. I mean, if some field value consists of n words, do we need to label every single word (I consider this approach to be more flexible) or just label all the n words as the required entity? \n\nThe documentation in LayoutLM's repo seems to favor the word-level type of annotations, but the one for PICK is not very specific about that. The examples in the repo are a bit misleading. \n\nWith regard to table extraction, I am much more confused.   \nFor table detection, there is this model based on Cascade RCNN:  \n[https://github.com/DevashishPrasad/CascadeTabNet](https://github.com/DevashishPrasad/CascadeTabNet)\n\nI understand that fine-tuning the model (which was trained on large but different datasets) on my data can help a lot. But it still fails at complicated cases when there is some random text inside tables that spans over multiple columns or the table layout is too irregular. For example, there might be nested tables, intersecting columns, and other types of crappy formatting. Honestly speaking, some cases are tricky even for me to understand. \n\nI haven't found anything useful in terms of table extraction. For random tables when any kind of layout is possible, I believe this task is too complicated. \n\nUsing computer vision approaches like finding vertical and horizontal lines and then recreating the grid is not possible because very often there is just no grid at all. \n\nThis makes me think that training a model on our data is the only possible way to proceed but I have no ideas what is the best way in terms of architectures and labeling, to approach this problem. \n\nIt will be great to hear anything you have to say about the problem and your opinion on how to solve it. Probably, there are some important papers and models I haven't seen that could be of help. \n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/kv64lf/discussion_entity_extraction_table_extraction/"}, {"autor": "neighbourhood_friend", "date": "2021-01-11 16:18:39", "content": "[D] EACL 2021 /!/ EACL 2021 paper acceptance was to be announced today. The official mails haven't been out but seems like for some papers you can check softconf and it says \"Submit -----> camera !!!  ready version\" if it's accepted", "link": "https://www.reddit.com/r/MachineLearning/comments/kv5upg/d_eacl_2021/"}, {"autor": "GreenCheesecake8486", "date": "2021-01-11 15:24:15", "content": "[D] Why prediction don't work's correct if evaluation work's correct? (Python, Keras, Tensorflow, CNN) /!/ I have a problem with my CNN model. Model evaluation work's correct (0.86 bin\\_acc) but if i try predict probability that -----> image !!!  is a positive i get small value. I have tried with many positive photos and I get a low probability value each time. Can anyone help me ?\n\n**Images augmentation:**\n\nhttps://preview.redd.it/wqo7s9by0qa61.png?width=559&amp;format=png&amp;auto=webp&amp;s=7cd518c2d02be0c017aa1bb3602c9963f0b6b6ad\n\n**Model define and training:**\n\nhttps://preview.redd.it/y1tdlqaz0qa61.png?width=875&amp;format=png&amp;auto=webp&amp;s=f9d2041fcd053da53e7a73bc281359ee77e9b09a\n\n**Prediction test and result:**\n\nhttps://preview.redd.it/w9yii4c01qa61.png?width=799&amp;format=png&amp;auto=webp&amp;s=8bfd745d680e08153c6b775aa35f8d074553463c", "link": "https://www.reddit.com/r/MachineLearning/comments/kv4qei/d_why_prediction_dont_works_correct_if_evaluation/"}, {"autor": "01000001010011100100", "date": "2021-01-11 08:45:07", "content": "[D] Determine location and rotation of 3D object by its outline/silhoutte? /!/  I am trying to build a CNN architecture that predicts the 6DOF (location and rotation) of a rigid body with respect to a reference -----> camera !!!  in 3D space. The object has no symmetry, so for most views the result should be unique. I have built a synthetic dataset of 30k samples for training (from blender). I have tried various CNN model depth and parameters and have come to the following observation: the model is able to predict the displacements relatively well (i.e. the \"center of mass\" in the image) , but is struggeling predicting the angles correctly. I have encoded the angles as cos/sin pairs, but without any improvement. If I use the projection of manually defined keypoints of the 3D object on the image as an input rather than the silhoutte image, I get acceptable results for all 6DOF.\n\nI am wondering if this is an ill posed problem for a CNN, or if there are other tricks I could exploit to get this working. I'd be happy to hear if you have any suggestions!", "link": "https://www.reddit.com/r/MachineLearning/comments/kuyul6/d_determine_location_and_rotation_of_3d_object_by/"}, {"autor": "thanrl", "date": "2021-01-11 06:07:23", "content": "[D] -----> Image !!!  datasets with multiple pictures of the same object? /!/ ... such as multiple pictures of the same persons, the same flowers, the same anything? The only such dataset that comes to mind are\n\n\\- scraping decor photos from property websites such as zillow\n\n\\- scraping celebrity photos from google image\n\nAny other suggestions?", "link": "https://www.reddit.com/r/MachineLearning/comments/kuwqfd/d_image_datasets_with_multiple_pictures_of_the/"}, {"autor": "thanrl", "date": "2021-01-11 06:05:37", "content": "-----> Image !!!  datasets with multiple pictures of the same object? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/kuwphp/image_datasets_with_multiple_pictures_of_the_same/"}, {"autor": "terrenerapier", "date": "2021-04-06 07:25:02", "content": "Refining contours for Images [P] /!/ Hey!\n\nI have been working on a problem in which I need to find the contours of an image(attached below).\n\nThis was one of the simplest codes that had even slightly what I wanted. But it did not get me contours for the slightly lighter shades.\n\n    imgray = cv.cvtColor(img, cv.COLOR_BGR2GRAY) ret, thresh = cv.threshold(imgray, 127, 255, 0) \n    contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_NONE) \n    contoured = cv.drawContours(img, contours, -1, (0,255,0), 3)  \n\nI have tried various other things. I tried using THRESH\\_BINARY but that just made everything messy with lines. I tried playing along with the values, but it didn't work out.\n\nI tried inverting the -----> image !!!  hoping it would make it easier but it was totally different.\n\nAny Idea what direction I can take with this? Thanks!\n\n*Processing img ftt991jy3ir61...*\n\n[Contours that were accurate but not for all. The other greys also need to be contoured](https://preview.redd.it/pl9hrnry3ir61.png?width=2273&amp;format=png&amp;auto=webp&amp;s=f5a4c2aaf8c248950a769d579f97c9e7e346f1b7)", "link": "https://www.reddit.com/r/MachineLearning/comments/ml64lg/refining_contours_for_images_p/"}, {"autor": "deshara128", "date": "2021-04-05 21:26:10", "content": "is there a free/simple to use tool for AI up-rendering an -----> image !!! ? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/mkvckf/is_there_a_freesimple_to_use_tool_for_ai/"}, {"autor": "thermokopf", "date": "2021-04-05 21:19:21", "content": "How do convolution nets accept general -----> image !!!  sizes? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/mkv734/how_do_convolution_nets_accept_general_image_sizes/"}, {"autor": "Square365", "date": "2021-04-05 19:10:03", "content": "[D] Good video dataset labeling services? (Frame By Frame) /!/ Hello i'm sort of new to ML and I have been trying to make a dataset to train -----> image !!!  recognition with yolov4 for a few weeks now. Im using DarkLabel and i have an aproximate of 24k frames from 34 videos labeled, im planning to label another 500 but I would like to know if there are any good services that label at that rate. I have seen amazon mturk but they only do box in images and in videos they only do the topic. \n\nI got an estimate from someone willing to do 10videos/5$, but I would like to get your opinions on this.", "link": "https://www.reddit.com/r/MachineLearning/comments/mksbiz/d_good_video_dataset_labeling_services_frame_by/"}, {"autor": "GiuPaolo", "date": "2021-04-05 18:40:36", "content": "[R] Call for Papers: Evolutionary Reinforcement Learning workshop @ GECCO 2021 /!/ Time is passing fast! Only 1 week to go before the deadline for the 1st **Evolutionary Reinforcement Learning workshop** @ GECCO 2021, the premiere conference\u00a0in evolutionary computing (this year held  virtually at Lille, France, from July 10-14, 2021)\n\nIn recent years reinforcement learning (RL) has received a lot of attention thanks to its performance and ability to address complex tasks.  At the same time evolutionary algorithms (EA) have been proven to be competitive with standard RL algorithms on certain problems, while being simpler and more scalable.\n\nRecent advances on EA have led to the development of algorithms like Novelty Search and Quality Diversity, capable of efficiently addressing complex exploration problems and finding a wealth of different policies. All these results and developments have sparked a strong renewed interest in such population-based computational approaches.\n\nNevertheless, even if EAs can perform well on hard exploration problems they still suffer from low sample efficiency. This limitation is less present in RL methods, notably because of sample reuse, while on the contrary they struggle with hard exploration settings. The complementary characteristics of RL algorithms and EAs have pushed researchers to explore new approaches merging the two in order to harness their respective strengths while avoiding their shortcomings.\n\nThe goal of the workshop is to foster collaboration, share perspectives, and spread best practices within our growing community at the intersection between RL and EA.\n\nThe topics at the heart of the workshop include:\n\n* Evolutionary reinforcement learning\n* Evolution strategies\n* Population-based methods for policy search\n* Neuroevolution\n* Hard exploration and sparse reward problems\n* Deceptive reward\n* Novelty and diversity search methods\n* Divergent search\n* Sample-efficient direct policy search\n* Intrinsic motivation, curiosity\n* Building or designing behaviour characterizations\n* Meta-learning, hierarchical learning\n* Evolutionary AutoML\n* Open-ended learning\n\nAutors are invited to submit **new original work**, or\u00a0**new perspectives on recently published work**\u00a0 on those topics. Top submissions will\u00a0be selected for oral presentation and\u00a0be presented\u00a0alongside keynote speaker **Jeff Clune** (ex-team leader at UberAI-Labs and current research team leader at OpenAI). \n\n&amp;#x200B;\n\nImportant dates\n\n* Submission deadline: **April 12, 2021**\n* Notification: **April 26, 2021**\n* -----> Camera !!! -ready: **May 3, 2021**\n\n**You can find more info on the** [**workshop website**](https://sites.google.com/view/evorl)**.**", "link": "https://www.reddit.com/r/MachineLearning/comments/mkrngl/r_call_for_papers_evolutionary_reinforcement/"}, {"autor": "kaia_1527", "date": "2021-04-05 12:39:37", "content": "[P] Basic Floor Plan Image Recognition /!/ Hi everyone, first-time poster. This one should be easy: is there a model that, given an -----> image !!! , recognizes whether the -----> image !!!  is floor plan (typically of a residential property)? Other than a boolean, don't need anything else. Should be quick to train one, but wanted to check whether there's a generally accepted model out there. Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/mkjq5g/p_basic_floor_plan_image_recognition/"}, {"autor": "KirillTheMunchKing", "date": "2021-08-16 16:06:34", "content": "[D] Turn your dog into Nick Cage! StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators by Rinon Gal et al. explaned in 5 minutes /!/ [ It's a dog, it's a Nick Cage ... it's StyleGAN-NADA ](https://preview.redd.it/cfn8zdxauqh71.jpg?width=7563&amp;format=pjpg&amp;auto=webp&amp;s=5ecdd00e67801c5e7adbb9683c462eca7f806c89)\n\nHow insane does it sound to describe a GAN with text (e.g. Human -&gt; Werewolf) and get a SOTA generator that synthesizes images corresponding to the provided text query in any domain?! Rinon Gal and colleagues leverage the semantic power of CLIP's text------> image !!!  latent space to shift a pretrained generator to a new domain. All it takes is a natural text prompts and a few minutes of training. The domains that StyleGAN-NADA covers are outright bizzare (and creepily specific) - Fernando Botero Painting, Dog \u2192 Nicolas Cage (WTF \ud83d\ude02), and more.\n\nUsually it is hard (or outright impossible) to obtain a large number of images from a specific domain required to train a GAN. One can leverage the information learned by Vision-Language models such as CLIP, yet applying these models to manipulate pretrained generators to synthesize out-of-domain images is far from trivial. The authors propose to use dual generators and an adaptive layer selection procedure to increase training stability. Unlike prior works StyleGAN-NADA works in zero-shot manner and automatically selects a subset of layers to update at each iteration.\n\nRead the [full paper digest](https://t.me/casual_gan/83) or the [blog post](https://www.casualganpapers.com/text-guided-clip-gan-domain-adaptation/StyleGAN-NADA-explained.html) (reading time \\~5 minutes) to learn about Cross-Domain Adversarial Learning, how Image Space Regularization helps improve the results, and what optimization targets are used in Sketch Your Own GAN.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://www.casualganpapers.com/)!\n\n[StyleGAN-NADA explained](https://preview.redd.it/1zub721auqh71.png?width=2127&amp;format=png&amp;auto=webp&amp;s=bfb67b6b7836ca8f45c6aef976819ea1e9c37feb)\n\n\\[[Full Explanation](https://t.me/casual_gan/83) / [Blog Post](https://www.casualganpapers.com/text-guided-clip-gan-domain-adaptation/StyleGAN-NADA-explained.html)\\] \\[[Arxiv](https://arxiv.org/pdf/2108.00946.pdf)\\] \\[[Code](https://github.com/rinongal/StyleGAN-nada)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n&gt;\\[[3D-Inpainting](https://www.casualganpapers.com/single_view_layered_depth_3d_inpainting/3D-Inpainting-explained.html)\\]  \n&gt;  \n&gt;\\[[Real-ESRGAN](https://t.me/casual_gan/77)\\]  \n&gt;  \n&gt;\\[[Sketch Your Own GAN](https://t.me/casual_gan/81)\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/p5j4ui/d_turn_your_dog_into_nick_cage_stylegannada/"}, {"autor": "DNA1987", "date": "2021-04-23 16:53:06", "content": "[D] 3D CNN how to deal with empty space /!/ Hello, \n\nI am working on a 3D CNN, I am using voxel to represent protein with one channel for each amino acid, so total 20 channel, \n\nThere is lot of empty space in my voxel ~90% and as each channel just encode one type of amino acid, so each channel is even more empty. It is not like an RGB -----> image !!!  where each channel has a integer gradient on all positions of x * y.   \n\nMy current network only work for a couple of samples, after that it needs too much training. Somehow I am thinking it could be like an unbalance class problem, but the 3D space convolution thing is confusing me.", "link": "https://www.reddit.com/r/MachineLearning/comments/mwzonj/d_3d_cnn_how_to_deal_with_empty_space/"}, {"autor": "aselsiriwardena", "date": "2021-04-23 10:23:43", "content": "[P] Pytorch Load Balance and Scalability /!/ I need some ideas to execute a test on a PyTorch -----> image !!!  generative model for load balancing and scalability?  \nAre there any specific tools for that? Do I need to execute a parallel process?", "link": "https://www.reddit.com/r/MachineLearning/comments/mws6z0/p_pytorch_load_balance_and_scalability/"}, {"autor": "luisgasco", "date": "2021-04-23 08:53:06", "content": "[R] - Call For Participants MESINESP2 (BioASQ / CLEF2021 shared task) on semantic indexing of heterogenous health content: literature, clinical trials and patents /!/ **\\*\\*\\* CFP2\u00a0 MESINESP2 track: Medical Semantic Indexing (BioASQ \u2013 CLEF 2021) \\*\\*\\***\n\n[https://temu.bsc.es/mesinesp2/](https://temu.bsc.es/mesinesp2/) \n\n**MESINESP2 Awards by BSC-Plan TL \\[2,700\u20ac\\]**\n\n**Test sets and additional data are now available**\n\nThere is a pressing need for advanced multilingual semantic search strategies for health related content like literature, patents and clinical trials (cross-genre). The use of semantic search techniques in combination with structured vocabularies is critical for sophisticated searches or content analysis as needed by healthcare professionals, researchers, the pharmaceutical industry, patient groups and private citizens.\n\nFollowing the impact of past BioASQ tracks for benchmarking studies (e.g. BioBERT) and organization of other initiatives like BioCreative or IberLEF, we propose three semantic labelling subtracks using the widely used DeCS vocabulary (similar to MeSH terms):\n\n**MESINESP-L \u2013 Scientific Literature**: for automatic labelling of medical literature abstracts in Spanish (including recent COVID-19 literature).\n\n**MESINESP-T \u2013 Clinical trials**: for automatic labelling of clinical trials summaries.\n\n**MESINESP-P \u2013 Patents:** for automatic labelling of health-related patents in Spanish to improve patent intelligence.\n\n**Key information**\n\n**Web**:[ https://temu.bsc.es/mesinesp2](https://temu.bsc.es/mesinesp2) \n\n**Registration**:[ http://clef2021-labs-registration.dei.unipd.it/](http://clef2021-labs-registration.dei.unipd.it/) (BioASQ **Task 3 - MESINESP**)\n\n**Data**: [https://doi.org/10.5281/zenodo.4707104](https://doi.org/10.5281/zenodo.4707104)\n\nMESINESP2 is organized in close collaboration with widely used multilingual medical literature databases (BIREME/WHO, ISCIII/Spain), which expressed a direct need for advanced technologies to accelerate manual indexing efforts for the contents in Spanish (spoken globally by over 572 million people). They do face a challenge to keep up with the increasing number of published medical papers when using purely manual indexing.\n\nA large manually indexed collection of training documents will be provided. These documents have already been automatically annotated (&gt; 1.5 million entity mentions) with\u00a0 medical entities such as diseases, medical procedures, drugs and symptoms to facilitate the use of complementary strategies like *multi-label classification*, *multilingual transformers*, *graph matching*, *text similarity,* *advanced term matching* or *named entity recognition components*.\u00a0\n\nParticipating systems will be directly useful for ongoing medical literature indexing efforts, and thus improve competitive intelligence/prior art searches, enable complex search queries needed for evidence-based medicine, clinical decision making, or elaboration of clinical practice guidelines and serve as base for future tasks on semantic indexing of medical records or content in other languages.\u00a0\n\n**Important dates**\n\n* April 19: Updated Train, Validation and Test sets release\n* April 19: Additional datasets release (Medical entities present in documents)\n* April , 30: BioASQ9 Lab u/CLEF 2021 Registration Deadline\n* May, 7: Start of the evaluation period\n* May, 17: End of the evaluation period\n* May,28 :Submission of Participant Papers at CLEF2021\n* July, 2: -----> Camera !!!  ready paper submission.\n* Sep 21-24: CLEF 2021 Conference\n\n**Publications and BioASQ/CLEF2021 workshop**\n\nTeams participating in MESINESP2 will be invited to contribute a systems description paper for the BioASQ (CLEF 2021) Working Notes proceedings, and a short presentation of their approach at the BioASQ 2021 workshop.\n\n**Main Track organizers**\n\n* **Martin Krallinger**, Barcelona Supercomputing Center (BSC), Spain.\n* **Luis Gasc\u00f3**, Barcelona Supercomputing Center (BSC), Spain.\n* **Anastasios Nentidis**, National Center for Scientific Research Demokritos, Greece.\n* **Elena Primo-Pe\u00f1a,** Biblioteca Nacional de Ciencias de Salud. Instituto de Salud Carlos III, Spain.\n* **Cristina Bojo Canales,** Biblioteca Nacional de Ciencias de la Salud. Instituto de Salud Carlos III, Spain.\n* **George Paliouras**, National Center for Scientific Research Demokritos, Greece.\n* **Anastasia Krithara**, National Center for Scientific Research Demokritos, Greece.\n* **Renato Murasaki,** BIREME \u2013 Organizaci\u00f3n Panamericana de la Salud (WHO), Brasil.\n\n**Scientific Committee**\n\n* **Tristan Naumann,** Microsoft Research (USA)\n* **Prof. Xavier Tannier,** Sorbonne Universit\u00e9 and LIMICS (France)\n* **Lucy Lu Wang,** Allen Institute for AI (AI2) (USA)\n* **Prof. David Camacho,** Applied Intelligence and Data Analysis Research Group, Universidad Polit\u00e9cnica de Madrid (Spain)\n* **Prof. Oscar Corcho,** Ontology Engineering Group, Universidad Polit\u00e9cnica de Madrid (Spain)\n* **Parminder Batia,** Amazon Health AI (USA)\n* **Prof. Irena Spasic,** School of Computer Science &amp; Informatics, co-Director of the Data Innovation Research Institute, Cardiff University (UK)\n* **Jose Luis Redondo Garc\u00eda,** Amazon Alexa, Amazon (UK)\n* **Carlos Badenes-Olmedo,** Ontology Engineering Group, Universidad Polit\u00e9cnica de Madrid (Spain)\n* **Prof. Allan Hanbury,**\u00a0 E-Commerce Research Unit in the Faculty of Informatics, TU Wien (Austria)\n* **Prof. Alfonso Valencia,** Barcelona Supercomputing Center (Spain)\n* **Prof. Stefan J. Darmoni,** Department of Biomedical Informatics, Rouen University Hospital (France) and LIMICS (France)\n* **Rezarta Islamaj,** National Center for Biotechnology Information (USA)\n* **Prof. Rafael Berlanga Llavori,** Universidad Jaume I (Spain)\n* **Prof. Henning M\u00fcller,** University of Applied Sciences Western Switzerland \u2013 Valais (Switzerland)\n* **Prof. Gareth J.F. Jones,** School of Computing at Dublin City University (Ireland)\n* **Georg Rehm,** Deutsches Forschungszentrum f\u00fcr K\u00fcnstliche Intelligenz (Germany)\n* **Petr Knoth,** Research Studios Austria Forschungsgesellschaft mbH (Austria)\n* **Natalia Manola,** CEO at OpenAIRE AMKE (Greece)\n* **Prof. Jes\u00fas Tramullas,** Departamento de Ciencias de la Documentaci\u00f3n e Historia de la Ciencia, Universidad de Zaragoza (Spain)", "link": "https://www.reddit.com/r/MachineLearning/comments/mwqzek/r_call_for_participants_mesinesp2_bioasq_clef2021/"}, {"autor": "dangtony98", "date": "2021-09-01 05:40:37", "content": "[D] What are the most common ways to collect large quantities of -----> image !!!  data for ML? /!/ I'm working on a ML project that requires a lot of custom image data. Accordingly, I was wondering what options there are for me to procure what I need; would appreciate any input from experienced data scientists/engineers dealing with data at scale. Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/pfnioq/d_what_are_the_most_common_ways_to_collect_large/"}, {"autor": "dangtony98", "date": "2021-09-01 05:37:36", "content": "What are the most common ways to collect large quantities of -----> image !!!  data for ML? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/pfnh8x/what_are_the_most_common_ways_to_collect_large/"}, {"autor": "pineapplesoda1e-3", "date": "2021-09-03 12:20:11", "content": "[P] preprocess dataset /!/ Hi, i could use a little advice here, my dataset contains 4 parquet files where each row is pixel values,\n\nwhat is the best way to prepare it for training NNs cos loading parquet files and each separate is very slow.\n\nShould i just merge them all together, or make .pkl for every -----> image !!! , or pandas dataframe.\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/ph422t/p_preprocess_dataset/"}, {"autor": "CaptainI9C3G6", "date": "2021-09-03 09:52:49", "content": "[D] Is there any research, papers or articles around pre processing of images before using them in both training and predictions for object detection? /!/ For example, I'm wondering if pre processing my -----> image !!! s  to perform edge  detection overlaid with the original -----> image !!! , and/or making them black and white.\n\nI think the viability of this will vary by use case of which I have a few, one of which is detecting car features like license plates and doors. Another is attempting to recognise elements of a computer generated GUI like buttons.\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/ph21dz/d_is_there_any_research_papers_or_articles_around/"}, {"autor": "FerretDude", "date": "2021-06-04 22:45:41", "content": "[P] Dynamic -----> image !!!  editing using CLIP and L-grammars, and the implications for HCI. /!/ Hi everyone!\n\n&amp;#x200B;\n\nI am a storytelling researcher at Georgia Tech who has a large interest in computational creativity. I also work at EleutherAI, directing the multimodal grounding subgroup as well as the (less official) computational creativity horde. \n\n&amp;#x200B;\n\nWe (EleutherAI) have spent the last few months working on an image editor based off of CLIP that allows you to specify/edit/and most importantly execute natural language substitution rules for editing images. For these purposes we've custom tailored a VQGAN 16k on Wikiart that we feels compliments this style of image editing fairly well. \n\n&amp;#x200B;\n\nThis method drastically differs from competitors like LatentRevision in that the masking component is automatic and it differs from StlyeCLIP in that we can perform all editing zero shot, we do not need to train a special model to perform edits. We also use a very different optimization technique thats based off of weighted spherical geodesic distances. \n\n&amp;#x200B;\n\nThis is the first stage of a larger computational creativity effort, where not only intend to see how these methods apply to image generation but to text as well. Can we specify \"reviews\" of stories in the form of L-grammars that when executed allow us to constrain and ground a story? Can we use CLIP as a discriminator of stories in a way that makes sense for collaborative writing? More on this project in the coming months ;) \n\n&amp;#x200B;\n\nHere is the colab notebook:\n\n[https://colab.research.google.com/drive/17AqhaKLZmmUA27aNSc6fJYMR9uypeIci?usp=sharing](https://colab.research.google.com/drive/17AqhaKLZmmUA27aNSc6fJYMR9uypeIci?usp=sharing)\n\n&amp;#x200B;\n\nIf you just want the wikiart model:\n\n[http://eaidata.bmk.sh/data/Wikiart\\_16384/](http://eaidata.bmk.sh/data/Wikiart_16384/) \n\n&amp;#x200B;\n\nPaper and parallel blog post on [https://eleuther.ai](https://eleuther.ai) coming within the next month or so.\n\n&amp;#x200B;\n\nHappy to answer questions!", "link": "https://www.reddit.com/r/MachineLearning/comments/nshlhw/p_dynamic_image_editing_using_clip_and_lgrammars/"}, {"autor": "rafiv70805", "date": "2021-06-04 11:57:22", "content": "[D] Is one RTX 3090 enough for computer vision tasks? /!/ Hi, I'm a PhD student in an Eastern European university. My department currently is considering building a deep learning workstation to do research. Unfortunately, our budget is limited, and due to the high GPU prices, we can only afford one computer. We also do not consider professional GPUs due to the budget limitations.\n\nWe are making a decision now whether to install one or two RTX 3090 GPUs. I understand that the more VRAM the workstation has the better, but I also think that adding two heavy, hot and power hungry GPUs will introduce engineering difficulties, such as cooling and ventilation issues, power supply issues, more expensive UPS setups etc. It is also more expensive of course.\n\nI am going to work with latest state of the art object detection models, YOLO variants, RetinaNet variants etc. Will one RTX 3090 with 24 GB VRAM will be enough to comfortably train these models? If yes, how future proof one RTX 3090 is? Is it worth to pay more and face technical difficulties described above to obtain more VRAM? And how about other areas of computer research that my colleagues might work at, like -----> image !!!  classification, segmentation etc?\n\nI don't have enough experience to answer these questions, so advice is greatly appreciated!", "link": "https://www.reddit.com/r/MachineLearning/comments/ns3crk/d_is_one_rtx_3090_enough_for_computer_vision_tasks/"}, {"autor": "amourav", "date": "2021-04-26 21:20:13", "content": "[D] Distributed Training - TF / Keras /!/ If you've worked with tensorflow distributed training before I would appreciate your advice on my particular use case. I'm training an MRI super-resolution model on about 40k volumes with Tensorflow/Keras and a custom training loop:\n\nPseudo python/tf code \u200b\n\n        for e in epochs: \n            for s in the steps: \n                batch = data_handler.get_batch()\n                model.train_on_batch(batch['lr'], batch['hr'])\n    \n        def get_batch() # online data augmentation and -----> image !!!  degredation\n            vol_hr = np.load(volume_path) # 200x200x200 array\n            vol_hr = augment(vol_hr) # flips / rotation # e.g. \n            vol_lr = degrade(vol_hr) # blur, add noise, downsample  (100x100x200)\n            batch = crop_patches(vol_lr, vol_hr)\n            # batch['lr'].shape == batch_size, 32, 32, 3\n            # batch['hr'].shape == batch_size, 64, 64, 3\n            return batch\n\nThis current implementation runs terribly slow as you may imagine. To speed things up I'm planning to implement a custom tf data pipeline ([tf.data.Dataset](https://www.tensorflow.org/guide/data_performance) class) and set up multi-gpu distributed training. I have a couple of questions I'm stuck on:\n\ni) Distributed learning implementation\n\n\\- tf.distribute vs Horovod vs AWS Sagemaker?\n\nii) What distrubute strategy is most appropriate?\n\n\\- synchronous vs asynchronous (I'm leaning towards async since I can't imagine the benefit of sync training).\n\n\\- In the context of [tf.distribute](https://www.tensorflow.org/guide/distributed_training), does the code snipped above represent a Keras API or custom training loop? \n\n\\- tf.distribute: Mirrored vs MirroredMultiWorkers vs ParameterServerStrategy vs ?\n\niii) Hardware platform:\n\n\\- multi-gpu EC2 instance vs TPU vs AWS sagemaker?\n\n&amp;#x200B;\n\nTL;DR - Distributed training for large overhead batch processing?", "link": "https://www.reddit.com/r/MachineLearning/comments/mz8ce3/d_distributed_training_tf_keras/"}, {"autor": "Acrobatic-Egg-", "date": "2021-04-26 10:45:01", "content": "[D] The Journey Of Problem-Solving Using Analytics /!/  \n\nIn my \\~6 years of working in the analytics domain, for most of the Fortune 10 clients, across geographies, one thing I've realized is while people may solve business problems using analytics, the journey is lost somewhere. At the risk of sounding cliche, ***'Enjoy the journey, not the destination\".*** So here's my attempt at creating the problem-solving journey from what I've experienced/learned/failed at.\n\nThe framework for problem-solving using analytics is a 3 step process. On we go:\n\n1. **Break the business problem into an analytical problem**  \nLet's start this with another cliche - *\" If I had an hour to solve a problem I'd spend 55 minutes thinking about the problem and 5 minutes thinking about solutions\".* This is where a lot of analysts/consultants fail. As soon as a business problem falls into their ears, they straightaway get down to solution-ing, without even a bare attempt at understanding the problem at hand. To tackle this, I (and my team) follow what we call the **CS-FS framework** (extra marks to those who can come up with a better naming).  \nThe CS-FS framework stands for the Current State - Future State framework.In the CS-FS framework, the first step is to identify the **Current State** of the client, where they're at currently with the problem, followed by the next step, which is to identify the **Desired Future State**, where they want to be after the solution is provided - the insights, the behaviors driven by the insight and finally the outcome driven by the behavior.  \nThe final, and the most important step of the CS-FS framework is **to identify the gap**, that prevents the client from moving from the Current State to the Desired Future State. This becomes your Analytical Problem, and thus the input for the next step\n2. **Find the Analytical Solution to the Analytical Problem**  \nNow that you have the business problem converted to an analytical problem, let's look at the data, shall we? \\*\\*A BIG NO!\\*\\*  \nWe will start forming hypotheses around the problem, **WITHOUT BEING BIASED BY THE DATA.** I can't stress this point enough. The process of forming hypotheses should be independent of what data you have available. The correct method to this is after forming all possible hypotheses, you should be looking at the available data, and eliminating those hypotheses for which you don't have data.  \nAfter the hypotheses are formed, you start looking at the data, and then the usual analytical solution follows - understand the data, do some EDA, test for hypotheses, do some ML (if the problem requires it), and yada yada yada. This is the part which most analysts are good at. For example - if the problem revolves around customer churn, this is the step where you'll go ahead with your classification modeling.Let me remind you, the output for this step is just an analytical solution - a classification model for your customer churn problem.  \nMost of the time, the people for whom you're solving the problem would not be technically gifted, so they won't understand the Confusion Matrix output of a classification model or the output of an AUC ROC curve. They want you to talk in a language they understand. This is where we take the final road in our journey of problem-solving - the final step\n3. **Convert the Analytical Solution to a Business Solution**  \nAn analytical solution is for computers, a business solution is for humans. And more or less, you'll be dealing with humans who want to understand what your many weeks' worth of effort has produced. You may have just created the most efficient and accurate ML model the world has ever seen, but if the final stakeholder is unable to interpret its meaning, then the whole exercise was useless.  \nThis is where you will use all your story-boarding experience to actually tell them a story that would start from the current state of their problem to the steps you have taken for them to reach the desired future state. This is where visualization skills, dashboard creation, insight generation, creation of decks come into the -----> picture !!! . Again, when you create dashboards or reports, keep in mind that you're telling a story, and not just laying down a beautiful colored chart on a Power BI or a Tableau dashboard. Each chart, each number on a report should be action-oriented, and part of a larger story.  \nOnly when someone understands your story, are they most likely going to purchase another book from you. Only when you make the journey beautiful and meaningful for your fellow passengers and stakeholders, will they travel with you again.\n\nWith that said, I've reached my destination. I hope you all do too. I'm totally open to criticism/suggestions/improvements that I can make to this journey. Looking forward to inputs from the community!", "link": "https://www.reddit.com/r/MachineLearning/comments/myuu4k/d_the_journey_of_problemsolving_using_analytics/"}, {"autor": "Shoulder_Feeling", "date": "2021-04-26 06:05:14", "content": "[Project] DataTap provides droplets ( containers for datasets) to make working on popular deep learning datasets easy. /!/ Excited to share [DataTap](https://www.datatap.dev), An open-source dataset management tool that makes it easy to \"containerize\" datasets to let you  focus on machine learning not data ops.  DataTap lets you build data set droplets ( think of a droplet as a docker container for data ).  A droplet encapsulates a dataset that can then easily be used , imported, shared across different teams and projects.\n\nEach Data Droplet consists for 2 items\n\n* **Droplet Template**, similar to a docker file this specifies the dataset schema\n* **Dataset annotations, metadata and media** (this is typically images / videos / rich media )\n\nLearn more about how you can start using this here [https://github.com/zensors/datatap-python](https://github.com/zensors/datatap-python)\n\nMany machine learning projects use proprietary data formats that require tools and utilities to be re-written from scratch to accommodate them. Not only does this slow down development substantially, but it also increases the probability that developers introduce bugs in the very code that validates models\u2019 performance.\n\nAs part of dataTap\u2019s efforts to allow machine learning engineers to focus only on the machine learning itself, we introduced an open-source data interchange format called Droplet. The data container format, called \u201cannotation,\u201d provides a standardized way to describe what is in an -----> image !!! . DataTap is designed to be the data platform for Software 2.0. Machine learning on reach media like images , audio or video needs a special data pipeline to version and manage data much like there are MLOps tools to version and manage models\n\nCurrently the project has common datasets available that you can download or stream with 3 lines of code.\n\n* coco\n* Open-Imagees\n* AI food Dataset\n* Large Person Dataset\n* Combined Vehicles Dataset\n\nSee the full list \n\n[https://app.datatap.dev/databases/1b81ec5a-b880-4cf4-ba67-86e301cada9e](https://app.datatap.dev/databases/1b81ec5a-b880-4cf4-ba67-86e301cada9e)\n\nRequest Your own to be added in or use the open source tools to import data into the droplet format using this example\n\n[https://zensors.typeform.com/to/WXo3ZlSN](https://zensors.typeform.com/to/WXo3ZlSN)", "link": "https://www.reddit.com/r/MachineLearning/comments/myqyls/project_datatap_provides_droplets_containers_for/"}, {"autor": "geekyhumans", "date": "2021-04-26 03:05:35", "content": "Convert any -----> image !!!  to 8-bit Pixel art using these 3 different methods! /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/myo7dz/convert_any_image_to_8bit_pixel_art_using_these_3/"}, {"autor": "iamaliver", "date": "2021-07-19 08:49:27", "content": "[D] Using Value Uncertainty/Confidence as Input to ML /!/ I am trying to figure out what has been tried about using uncertainty as a parameter to influence the uncertainty in the output or anyone's thoughts as well!\n\nI am unsure how to properly formulate the idea and appreciate any clarifications or keywords suggestions.\n\nHere are a few examples that hopefully capture the idea:\n\nExample 1:\n\nIn number recognition (NMIST) the typical setup would be.  \n\nYou provide a 100x100 pixel input.  Each pixel is grayscaled and you feed it to your ML algorithm.  \n\nImplicit in this ML algorithm is that you know for certain your input value (each pixel) with confidence of 1.  \n\nWhat if you do not know with high confidence the pixel value?  ie: I say this pixel is White, but only have 0.8 confidence?  Shouldn't that influence my final output? \n\nSay I have an input -----> picture !!!  where all my confidence in the pixels is really low, even though the output is 8  the confidence of the 8 should be really low.\n\nExample 2:\n\nI want to figure out if it will rain.  I input a lot of variables, such as \"morning cloud cover\", \"morning temperature\", \"time of year\" etc.  However the variable \"morning cloud cover\" is highly variable.  Saying 50% is not exactly true.  It is more a range of values, \\[30%, 60%\\].  \n\nMaybe I can input 50% with range of 10% into the ML?\n\nExample 3:\n\nYou have collected some data, and implicit to this data is really high noise.  Both in what you are measuring and in the final output.  You know that the uncertainty in the input for any given variable is really high.  Or perhaps the 95% confidence is very wide.  What is the best way to capture this idea?\n\n&amp;#x200B;\n\nMy current thoughts:\n\nI am unclear what to search online.  \"uncertainty input parameters machine learning\" is a very crappy set of keywords to search.\n\nI believe that even when you have a lot of samples to train with, this problem of \"uncertain inputs\" persists and should affect your final output. \n\nAny thoughts appreciated or even how to approach this problem.", "link": "https://www.reddit.com/r/MachineLearning/comments/ona50x/d_using_value_uncertaintyconfidence_as_input_to_ml/"}, {"autor": "blueest", "date": "2021-07-18 21:11:17", "content": "[D] surpassing the pareto front in optimization problems /!/ In real life multi objective optimization problems, is it ever possible to obtain a solution like the \"black x\" in this -----> picture !!!  here? \n\nhttps://imgur.com/a/UmXTQ64\n\nI understand that usually in multi objective optimization problems, since there are so many criteria to be optimized - it is very unlikely to have a \"globally best\" solution. Usually, some solutions will be better in some of the criteria - and other solutions will be better in other criteria. For example, if you are trying to find airplane tickets and want to optimize cost/length of flight : it is very likely some tickets will be expensive and short, some tickets will be cheap and long, and some tickets will be in the middle.\n\nBut suppose the data is such - sometimes, we can stumble across a ticket that is both cheap and short. Thus, in this case - how does the concept of the Pareto Front apply over here? The Pareto Front would usually refer to a group of airplane tickets that \"cannot be improved in any of the objectives without degrading at least one of the other objectives\" ( source: https://en.wikipedia.org/wiki/Multi-objective_optimization). But suppose there was one airplane ticket that was both cheaper AND shorter than any other ticket - in this example, would the Pareto Front simply be made of this one \"supreme point\"?\n\nAlso, this must happen sometimes in real life - correct?\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/omzhes/d_surpassing_the_pareto_front_in_optimization/"}, {"autor": "Bonner95", "date": "2021-03-02 21:42:00", "content": "Shape extraction from -----> image !!!  using CNN [R] /!/ Hello all, does anyone know of a pretrained model able to extract shapes from an image? \nAFAIK the state of the art for feature extraction in images is using CNNs, but the feature extracted are usually high level unexplainable features. For this specific project (our master thesis so we don\u2019t have time to train our own model) we need to make sure the features only explains shapes. Or default approach is to use invariant moments, but we were discussing whether a specialized CNN might perform better. \nHope to hear from some of you, thanks I\u2019m advance.", "link": "https://www.reddit.com/r/MachineLearning/comments/lwdaep/shape_extraction_from_image_using_cnn_r/"}, {"autor": "there_are_no_owls", "date": "2021-01-19 16:57:50", "content": "[D] Theoretical papers on transformers? (or attention mechanism, or just seq2seq?) /!/ There has recently been a lot of interest on applying transformers, and related ideas, to NLP and computer vision. As a theoretically-minded person (master student), I was wondering how well-understood these ideas are, beyond the intuitive justifications for the attention mechanism?\n\nMore generally, what work has been done on theoretical guarantees for sequence-to-sequence (or -----> image !!! -to-sequence, or sequence-to------> image !!! ) tasks? I've only heard about such guarantees for relatively basic classification or regression tasks (with vector inputs and outputs). By \"guarantees\" I mean things like PAC-learnability, sample complexity bounds etc.\n\n(In fact, does it even make sense to use those kinds of statistical tools for NLP or CV? Possibly the performance of e.g transformers in those fields is due to the specific structure of the data...)\n\n\nSorry if this is obvious to anyone in the field, but I'm having trouble navigating the literature -- googling \"transformers\" or \"attention mechanism\" or even \"sequence-to-sequence\" keeps returning papers proposing new methods or extensions.", "link": "https://www.reddit.com/r/MachineLearning/comments/l0nw02/d_theoretical_papers_on_transformers_or_attention/"}, {"autor": "IborkedyourGPU", "date": "2021-01-19 15:53:57", "content": "[D] Does the -----> image !!!  format impact significantly training and inference for instance segmentation models? /!/ I have images of metallic objects with superficial damage which are being labelled. The goal is to identify the defects with a Deep Learning model. The majority of the images are JPG, and a few are BMP. I asked to get the images in a sane format (e.g., PNG), but this wasn't possible. So I'm stuck with, say, 80% JPG and 20% BMP. Even if the semantic meaning is the same, the frequency content of JPG images and BMP images is very different, thus I'm not sure it's a good idea to train a model on a mixture of image formats. Since, for the JPGs, I don't have access to the original (uncompressed) images, should I convert the remaining BMP to JPG? Of course I don't know if the compression ratio will be the same for all JPG images. However, given that the dataset is such a mess, it's possible that not even the images which are already JPG, were all converted with the same compression ratio. Your advice?", "link": "https://www.reddit.com/r/MachineLearning/comments/l0mjuc/d_does_the_image_format_impact_significantly/"}, {"autor": "MLingMLer", "date": "2021-01-19 12:06:31", "content": "[D] Long-term keypoint tracking/ matching research /!/ I'm looking for work on matching key points and tracking them over multiple frames.\n\nSimilar work that I can point to is SuperGlue (https://github.com/magicleap/SuperGluePretrainedNetwork) which matches points to a reference -----> picture !!!  (so each frame in the video has different matching key points).\n\nThe other is KP3D (https://corlconf.github.io/paper_464/) , which i think does the matching for 3 frames.\n\nDoes anyone know last work that does keeping matching / tracking for longer frame sequences? Thanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/l0ikym/d_longterm_keypoint_tracking_matching_research/"}, {"autor": "almoudi", "date": "2021-01-19 11:49:02", "content": "[P] Regression Model with Categorical Attributes and Missing Values /!/  Hello, i have regression model project of wine dataset having categorical attributes and missing values where i should predict the wine quality -----> picture !!!  below has some rows of the data and info about each column also i should mention that the attribute unique values are a lot and i have problem of missing values which game me a problem using OnehotEncoder of sklearn.preprocessing so i tried get\\_dummies and i got memory error since as i mentioned i have lot of values for each category so what do you advice i do \n\nhttps://preview.redd.it/atho2enrz9c61.png?width=1007&amp;format=png&amp;auto=webp&amp;s=1148fb27fbacd467334e70f8575f41eaff130c52", "link": "https://www.reddit.com/r/MachineLearning/comments/l0ibwf/p_regression_model_with_categorical_attributes/"}, {"autor": "cosmic-cortex", "date": "2021-01-19 11:45:24", "content": "[P] We are building a new machine learning competition platform, and we would love to get your feedback on it! /!/ Hi there, /r/MachineLearning community!\n\nSo, me and a couple of my friends spent the last year building a machine learning competition platform ([https://telesto.ai](https://telesto.ai)). We are slowly getting started and working on filling it up with competitions, but we would love to get some feedback on it!\n\nYou might ask, why are we building another platform, when [Kaggle](https://www.kaggle.com/), [AIcrowd](https://www.aicrowd.com/), [DrivenData](https://www.drivendata.org/), [and many others](https://towardsdatascience.com/top-competitive-data-science-platforms-other-than-kaggle-2995e9dad93c) are available? Let me explain!\n\nTL;DR: crowdsourcing is not suitable for production-ready models, and the crowdsourcing process itself is not a part of the machine learning development cycle. We want to change that by deploying to production-like environments and rewarding top solutions weekly.\n\n# Crowdsourcing and the development cycle\n\nLet's take a look into how the development of a machine learning solution is done! Just as for other software tools, the development workflow here is a cycle as well.\n\nRoughly, the main steps are the following.\n\n1. **Data collection.** Once the problem is formulated, data has to be collected to provide training and test sets. In some cases, data is collected even before the problems are clear.\n2. **Model training.** This is the part that is most exciting for data scientists: coming up with the architecture of the solution and training a machine learning model. However, this takes up only a small percentage of the total workload.\n3. **Deployment to production.** If the model is ready, it has to be prepared for production use. This involves quality assurance, packaging it into an API, securely deploying it to production servers, etc.\n4. **Testing and validation.** When a model is used in production, new and unexpected problems might arise. Suppose you train a classifier that recognizes the brand and the model of a car, but a manufacturer issues a new model. The classifier no longer works correctly, so you have to go back to the beginning, collect more data, train a new model, and so on.\n\nAs we know it, a machine learning competition only covers a small part of this process: the model training. What happens before and after is unrelated to the competition and done by an internal team instead of the thousands of participants. Effectively, crowdsourcing only covers a small portion of the work.\n\nWe want to change that.\n\n# Crowdsourcing for production\n\nHow can we involve the participants of a challenge in the entire process? In our opinion, there are several opportunities. For instance, I have participated in competitions where participants noticed problems with the training data labels right away, yet nothing changed for the entire duration. In others, [participants have cheated their way to victory](https://towardsdatascience.com/kaggle-1st-place-winner-cheated-10-000-prize-declared-irrecoverable-bb7e1b639365) by downloading the private test dataset and overfitting the model on that.\n\nAt best, the result is a proof of concept. At worst, it is useless.\n\nWe've come up with the idea of making make the entire process dynamic, involving competitors and challenge hosts in the entire development cycle.\n\nSimply speaking, instead of rewarding the top 3-5 solutions at the end of a three-month period and conclude the competition, we would reward the best solutions every week and deploy them to production-like environments. (Currently, we ask the weekly winners to package their solution into a Docker -----> image !!!  for which we provide a template, but this is under heavy development.)\n\nWhen issues arise, like mislabeled data, it can be fixed by the next round, and the solution can keep improving constantly.\n\nOn the other hand, we believe that this would be more rewarding for participants as well. If you are working hard on the competition, staying on top of the leaderboard for months but get outcompeted during the final hours, you receive no reward. We think this is unfair, and consistency should be rewarded. So, our idea is to issue the rewards every week.\n\nIn brief, our goals are\n\n* to connect the machine learning community to the industry through exciting problems,\n* to provide a more rewarding competition platform for developers,\n* and to integrate crowdsourcing to the entire development life cycle.\n\nWhat do you think?\n\n# We are asking for your feedback!\n\nSo far, we are at the beginning of our journey. We set up an early release version that you can test at [https://telesto.ai](https://telesto.ai). Currently, we are just testing out our platform with a competition to diagnose COVID-19 based on cell microscopy. More challenges are coming soon!\n\nWe are asking for your feedback on our idea and the execution so far! We are building this platform for the machine learning community, and we aim to provide the best experience out there!", "link": "https://www.reddit.com/r/MachineLearning/comments/l0ia6z/p_we_are_building_a_new_machine_learning/"}, {"autor": "romborn", "date": "2021-01-19 08:08:01", "content": "[P] AI + Telegram API = Cool Bot /!/ Here is a quick [article](https://romain-gratier.medium.com/a-recommendation-engine-that-proposes-recipes-after-taking-photos-of-your-ingredients-de2d314f565d) on creating a **recipe recommendation bot** that can also recognize your ingredients with a -----> picture !!! !\n\n**Small tip**: Telegram API is super useful for small projects or POC! Your frontend takes only a day to be ready.", "link": "https://www.reddit.com/r/MachineLearning/comments/l0ffir/p_ai_telegram_api_cool_bot/"}, {"autor": "sbb_ml", "date": "2021-01-18 14:17:52", "content": "[R] Degenerative Adversarial NeuroImage Nets for Brain Scan Simulations: Application in Ageing and Dementia /!/ [link to paper](https://arxiv.org/pdf/1912.01526.pdf)\n\nIf anyone is interested in the paper and/or code, please don't hesitate to message me.\n\n&amp;#x200B;\n\n&gt;Accurate and realistic simulation of high-dimensional medical images has become an important research area relevant to many AI-enabled healthcare  applications. However, current state-of-the-art approaches lack the  ability to produce satisfactory high-resolution and accurate  subject-specific images. In this work, we present a deep learning  framework, namely 4D-Degenerative Adversarial NeuroImage Net  (4D-DANI-Net), to generate high-resolution, longitudinal MRI scans that  mimic subject-specific neurodegeneration in ageing and dementia.  4D-DANI-Net is a modular framework based on adversarial training and a  set of novel spatiotemporal, biologically-informed constraints. To  ensure efficient training and overcome memory limitations affecting such  high-dimensional problems, we rely on three key technological advances:  i) a new 3D training consistency mechanism called Profile Weight  Functions (PWFs), ii) a 3D super-resolution module and iii) a transfer  learning strategy to fine-tune the system for a given individual. To  evaluate our approach, we trained the framework on 9852 T1-weighted MRI  scans from 876 participants in the Alzheimer's Disease Neuroimaging  Initiative dataset and held out a separate test set of 1283 MRI scans  from 170 participants for quantitative and qualitative assessment of the  personalised time series of synthetic images. We performed three  evaluations: i) -----> image !!!  quality assessment; ii) quantifying the accuracy  of regional brain volumes over and above benchmark models; and iii)  quantifying visual perception of the synthetic -----> image !!! s by medical  experts. Overall, both quantitative and qualitative results show that  4D-DANI-Net produces realistic, low-artefact, personalised time series  of synthetic T1 MRI that outperforms benchmark models.", "link": "https://www.reddit.com/r/MachineLearning/comments/kzvgsx/r_degenerative_adversarial_neuroimage_nets_for/"}, {"autor": "gerry_mandering_50", "date": "2021-06-11 18:53:12", "content": "[D] Wyze cam and Ring cam campfire discussion for computer vision purposes /!/ Basically, I'm doing a CV system with deep neural nets that is monitoring specific outdoor images of natural environment.  \n\nIt would be nice to use the consumer-grade cams from Ring or Wyze.  These have cloud subscriptions and some under-100 dollar cams.  I have already acquired and paid for cloud and cam for both systems hoping one would work well for CV data capture.\n\nI expect that an RDTP server on cam might help since I expect I then can write code for my own custom client and save any video stills that I find interesting.\n\nIn the past (2008) I made pre-machine-learning vision app with cheap PC webcams to automatically measure the speed of car traffic and it was semi-successful.  As you can guess, special cases began to overwhelm my rule-driven analysis code, and machine learning will work much much better, I expect. The current apps I am working on are not vehicle speed measurers.\n\nMy findings so far, for Wyze cams \"Outdoor\" and \"V3\" as of early 2021:\n\n- Must click in the UI to manually download pre-motion-detected segments one at a time, even for paid cloud subscribers. This is a problem.\n\n- No programmatic API, even for paid cloud subscribers. THis is a problem.\n\n- For some not all cams they sell, there is an RDTP server firmware you can put on the cam, to replace the original firmware.  They call it \"beta\" release level not production level, with all that implies.\n\n- RDTP would enable my code to read all video all the time, so it will not miss anything.\n\n- The provided motion-detection too often misses my subjects that move, in my manual tests. I guess it's PIR driven, not AI driven in the provided motion-detection, and my subjects are not different enough in temperature than the background.  This means that their provided motion detection probably is more trouble than benefit.  I will have to do my own motion detection, which is fine really as I am confident I can do this myself.\n\n- Wyze has a mode provided in the standard cam firmware, to do so-called time-lapse, which is basically taking a still -----> photo !!!  at a chosen fixed frequency, like once per minute, during a defined time range, like 6 hours.  This might help me reduce the file sizes that need to be saved to the local SD memory card.  It would effectively be sampling instead of continuous-time recording.  Thus I would miss some interesting events, but on the other hand I will still get some interesting events.\n\n- The wifi that is built-in makes them very easy to install in good locations for my projects. \n\nAs for the Ring cams:\n\n- Slightly more expensive, slightly less useful and flexible.  Basically all of the same problems as Wyze cams.  \n\n- No RDTP that I could find (let me know if I overlooked)\n\n\nTO summarize:\n\nI might have to go back to PC web cams. At least I can do everything with the images.  It's not a closed proprietary cloud holding my data.\n\nIt would be a shame to go back to PC web cams, because the wifi in Ring and Wyze cams make them very easy to install in good locations for my projects.  Also these new cams have greatly improved resolution and color versus my old web cams from 10+ years ago.\n\n- I feel like approaching these companies, to ask for API to be installed and published so scientists can use their cams.\n\n- I feel like anyone who pays a subscription to their cloud, should ethically and morally be allowed by these companies to use the API without restriction to read (download) their own image data from the cloud.\n\n\nIn your own CV projects, what have you found that works for a cam system?\n\nI expect a phone cams is another possibility, esp older phones that have no other uses and just sit around in a drawer.  \n\nIs there an app for RDTP serving on android phones?\n\n\nThanks for reading and sharing.", "link": "https://www.reddit.com/r/MachineLearning/comments/nxnci9/d_wyze_cam_and_ring_cam_campfire_discussion_for/"}, {"autor": "kron_ninja", "date": "2021-06-11 10:33:27", "content": "[Project] My model gives the same result with pretrained model /!/  I have built Mask RCNN model to detect buildings at a given -----> image !!! . My data consisted of 80 satellite images, which was not enough to build a good model. Therefore, I used Transfer Learning - a predefined weight file from [this project](https://github.com/crowdAI/crowdai-mapping-challenge-mask-rcnn). Then I used both my trained model and predefined model for making detections in the new image, both resulted in the exact same detections - [like this](https://i.stack.imgur.com/XAIwL.png),. Note that both weight files have the same size(249,861 KB).\n\nNow I am curious why both(pretrained and mine) models give the same predictions. May be my data is not good enough to improve the pretrained model, therefore weights did not change in my model? If this is the case, I can improve my data quality, but there might be some other problem that I don't know of.\n\nAny help/idea is appreciated.", "link": "https://www.reddit.com/r/MachineLearning/comments/nxctjy/project_my_model_gives_the_same_result_with/"}, {"autor": "20gunasart", "date": "2021-06-09 11:57:16", "content": "[D] Finding basis vectors for sub spaces of the latent space of a trained GAN? /!/ Recently I\u2019ve been working with GANs (can\u2019t get too much into detail about what I\u2019ve been doing so my research doesn\u2019t get stolen he he), but essentially I\u2019ve ran into the following problem I might want to investigate.\n\nSuppose I have two main classes of -----> image !!! s that my GAN might generate, and of course each -----> image !!!  in each class has their own latent vectors associated with them that was used to generate them. \n\nCould one say that each class belongs to a sub space of the over all R^100 that the latent vectors belong in?\n\nAnd let\u2019s say these sub spaces for each associated class do exist, could I find basis vectors for them with a simple process like Gram Schmidt, or would a more involved process be required?", "link": "https://www.reddit.com/r/MachineLearning/comments/nvu07q/d_finding_basis_vectors_for_sub_spaces_of_the/"}, {"autor": "MidnightNecessary658", "date": "2021-06-09 10:47:41", "content": "[P] Help with an optimization algorithm for StyleGAN2 interpolation /!/ I would like to interpolate two images using StyleGAN2-ADA-PyTorch from NVLabs (https://github.com/NVlabs/stylegan2-ada-pytorch). For the sake of simplicity, it can be said that with two -----> image !!! s of different persons I want to create a third -----> image !!! , depicting a third person, with a body from the first -----> image !!! , and a head from the second. I also have corresponding w-vectors for the two source images ready at hand.\n\n    # G is a generative model in line with StyleGAN2, trained to output 512x512 images.\n    # Latents shape is [1, 16, 512]\n    G = G.eval().requires_grad_(False).to(device) # type: ignore\n    num_ws = G.mapping.num_ws # 16\n    w_dim = G.mapping.w_dim # 512\n\n    # Segmentation network is used to extract important parts from images\n    segmentation_dnn = segmentation_dnn.to(device)\n    # Source images are represented as latent vectors. I use G to generate actual images:\n    image_body = image_from_output(G.synthesis(w_body, noise_mode='const'))\n    image_head = image_from_output(G.synthesis(w_head, noise_mode='const'))\n\n    # Custom function is applied to source images to create masked images. \n    # In masked images, only head or body is present (and the rest is filled with white)\n    image_body_masked = apply_segmentation_mask(image_body, segmentation_dnn, select='body')\n    image_head_masked = apply_segmentation_mask(image_head, segmentation_dnn, select='head')\n\nIn order to compare similarity of any two images, I use VGGLos:\n    # VGG16 is used as a feature extractor to evaluate image similarity\n    url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/vgg16.pt'\n    with dnnlib.util.open_url(url) as f:\n        vgg16 = torch.jit.load(f).eval().to(device)\n\n    class VGGLoss(nn.Module):\n        def __init__(self, device, vgg):\n            super().__init__()\n            \n            for param in self.parameters():\n                param.requires_grad = False\n\n            self.criterion = nn.L1Loss().to(device)\n            \n        def forward(self, source, target):\n            loss = 0 \n            source_features = self.vgg(source, resize_images=False, return_lpips=True)\n            target_features = self.vgg(target, resize_images=False, return_lpips=True)\n            loss += self.criterion(source, target)\n                \n            return loss \n\n    vgg_loss = VGGLoss(device, vgg=vgg16)\n\nNow, I want to interpolate image_body and image_head, creating image_target. To do this, I need to find latent representation of image_target in the latent space of StyleGAN2 Crudely, I can optimize for a certain interpolation coefficient query_opt to partially include latents from image_body and image_head: w_target = w_body + (query_opt * (w_head - w_person))\n\n    query_opt = torch.randn([1, num_ws, 1], dtype=torch.float32, device=device, requires_grad=True)\n    optimizer = torch.optim.Adam(query_opt, betas=(0.9, 0.999), lr=initial_learning_rate)\n\n    w_out = []\n    for step in num_steps:\n        # Learning rate schedule.\n        t = step / num_steps\n        lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n        lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n        lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n        lr = initial_learning_rate * lr_ramp\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n\n        # Synth image from w_target using query_opt.\n        # This is an important step, and I think that my math is messed up here\n\n        w_target = w_body + (query_opt * (w_head - w_person))\n        image_target = image_from_output(G.synthesis(ws, noise_mode='const'))\n        image_target_body_masked = apply_segmentation_mask(image_target, segmentation_dnn, select='body')\n        image_target_head_masked = apply_segmentation_mask(image_target, segmentation_dnn, select='head')\n        loss = vgg_loss(image_body_masked, image_target_body_masked) + vgg_loss(image_head_masked, image_target_head_masked)\n        \n        # Step\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n        logprint(f'step {step+1:&gt;4d}/{num_steps}: loss {float(loss):&lt;5.2f}')\n\n        # Save current w_target\n        w_out[step] = w_target.detach()\n\nI can't figure out how to make my optimizer actually target query_opt in such a way that combined VGGLoss is actually optimized for. I must be missing something in my PyTorch code, or maybe even in the main interpolation formula.", "link": "https://www.reddit.com/r/MachineLearning/comments/nvsu3w/p_help_with_an_optimization_algorithm_for/"}, {"autor": "trashpanda-ly", "date": "2021-06-09 02:22:19", "content": "[R] Anyone used any food dectection/recognition APIs before? /!/ I am thinking of creating a wellness app where one of the available function allows users to take a snapshot of their meal. The app should then be able to recognise the different food in the -----> image !!!  and log the nutritional values to help user keep track of them. In terms of nutritional values, i think calorie intake will be sufficient and i am not planning to go in depth (eg vitamins, iron etc).  \nI've been trying to find APIs online but am not sure which one is good so am asking here to see if anyone has any recommendation or have used them before. Some of the APIs i found were logmeal food ai, calorie mama ai, foodai and biteai. I am willing to pay for the api service if they are reasonably priced and have tried contacting foodai and biteai but didn't get a response. Calorie mama also seem a little expensive. Would appreciate any response!", "link": "https://www.reddit.com/r/MachineLearning/comments/nvkyhi/r_anyone_used_any_food_dectectionrecognition_apis/"}, {"autor": "KirillTheMunchKing", "date": "2021-06-08 21:46:00", "content": "[D] Paper expl\u0430ined - DALL-E: Zero-Shot Text-to------> Image !!!  Generation /!/  \n\nWouldn't it be amazing if you could simply type a text prompt describing the image in as much or as little detail as you want and a bunch of images fitting the description was generated on the fly? Well, thanks to the good folks at OpenAI it is possible! Introducing their DALL-E model that uses a discrete visual codebook obtained by training a discrete VAE, and a transformer to model the joint probability of text prompts and their corresponding images. And if that was not cool enough, they also make it possible to use an input image alongside a special text prompt as an additional condition to perform zero-shot image-to-image translation.\n\nTo learn how the authors managed to create an effective discrete visual codebook for text-to-image tasks, and how they cleverly applied an autoregressive transformer to generate high-resolution images from a combination of text and image tokens check out [the full explanation post](https://t.me/casual_gan/48)!\n\nMeanwhile, check out some really awesome samples from the paper:\n\n[DALL-E samples](https://preview.redd.it/vvekboe044471.png?width=1280&amp;format=png&amp;auto=webp&amp;s=ff23dd61675d24082d4088da3e6c25cecf57f4c1)\n\n\\[[Full Explanation Post](https://t.me/casual_gan/48)\\] \\[[Arxiv](https://arxiv.org/abs/2102.12092)\\] \\[[Project page](https://github.com/openai/DALL-E)\\]\n\nMore recent popular computer vision paper explanations:\n\n&gt;\\[[CoModGAN](https://t.me/casual_gan/43)\\]\\[[VQGAN](https://t.me/casual_gan/46)\\]\\[[DINO](https://t.me/casual_gan/40)\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/nvffyb/d_paper_expl\u0430ined_dalle_zeroshot_texttoimage/"}, {"autor": "JEUNGHWAN", "date": "2021-01-16 11:56:10", "content": "[N] Facebook announced a new AI open-source called DeiT (A new technique to train computer vision models) /!/ Facebook AI has developed a new technique called\u00a0**Data-efficient -----> image !!!  Transformers (DeiT)**\u00a0to train computer vision models that leverage Transformers to unlock dramatic advances across many areas of Artificial Intelligence. So, I made a video to explan about this topic.  \nThanks!  \nVideo : [https://youtu.be/du8viAVuGQY](https://youtu.be/du8viAVuGQY)  \nPaper : [https://arxiv.org/abs/2012.12877](https://arxiv.org/abs/2012.12877)  \nGithub : [https://github.com/facebookresearch/deit](https://github.com/facebookresearch/deit)", "link": "https://www.reddit.com/r/MachineLearning/comments/kyhovg/n_facebook_announced_a_new_ai_opensource_called/"}, {"autor": "nivter", "date": "2021-01-16 07:30:25", "content": "[P] Creating Loving Vincent effect from a single -----> image !!!  (details in comment)", "link": "https://www.reddit.com/r/MachineLearning/comments/kyejdy/p_creating_loving_vincent_effect_from_a_single/"}, {"autor": "Wiskkey", "date": "2021-01-16 01:00:34", "content": "[P] A Colab notebook from Ryan Murdock that creates an -----> image !!!  from a given text description using SIREN and OpenAI'S CLIP /!/ From [https://twitter.com/advadnoun/status/1348375026697834496](https://twitter.com/advadnoun/status/1348375026697834496):\n\n&gt;[colab.research.google.com/drive/1FoHdqoqKntliaQKnMoNs3yn5EALqWtvP?usp=sharing](https://colab.research.google.com/drive/1FoHdqoqKntliaQKnMoNs3yn5EALqWtvP?usp=sharing)  \n&gt;  \n&gt;I'm excited to finally share the Colab notebook for generating images from text using the SIREN and CLIP architecture and models.  \n&gt;  \n&gt;Have fun, and please share what you create!\n\nChange the text in the above notebook in the Params section from \"a beautiful Waluigi\" to your desired text.\n\nReddit post about SIREN.\n\nReddit post about CLIP.", "link": "https://www.reddit.com/r/MachineLearning/comments/ky8fq8/p_a_colab_notebook_from_ryan_murdock_that_creates/"}, {"autor": "jet-orion", "date": "2021-01-15 20:32:17", "content": "[P] Run Neural Network in Python Script using AWS /!/ Hello! \nI have written an -----> image !!!  classification neural network in Python using the Spyder IDE on my local machine. All of my image data is also on my local machine. Because of the computational demand, the model will not finish running on my laptop so I want to see if there is a way to train my model on AWS, save the model and move it back to my local machine. Any ideas?\n\nI was thinking use an S3 bucket to have all my image data for training and validation set, then create an EC2 instance to run my Python script but I\u2019m not sure how make it happen. \n\nThank you!", "link": "https://www.reddit.com/r/MachineLearning/comments/ky35du/p_run_neural_network_in_python_script_using_aws/"}, {"autor": "mvpetri", "date": "2021-01-15 17:01:24", "content": "[D] Does anyone knows a public pre trained model to remove backgrounds? /!/ I was looking into some models to remove backgrounds. The website runwayML has a couple of those, which are quite useful (they use -----> image !!!  segmentation), but you have to run it on their servers. Does anyone knows a public model that was trained to remove backgrounds from pictures, separating the main object from the background?", "link": "https://www.reddit.com/r/MachineLearning/comments/kxynvl/d_does_anyone_knows_a_public_pre_trained_model_to/"}, {"autor": "latentlatent", "date": "2021-01-08 14:41:16", "content": "[D] Binary -----> image !!!  classification where the \"target\" object is small /!/ I've run into a problem multiple times, when I would like to classify images but the target objects are small. E.g. tennis ball on a photo or a bird on the streets.\n\nIs there any research going on how to solve such a problem? What are the most recent advances?\n\nI tried to look around and did not find any relevant info, but maybe I am just not knowing how this \"sub-topic\" is called properly.\n\nCan we \"guide\" the network if we only have classification labels?", "link": "https://www.reddit.com/r/MachineLearning/comments/kt3q6y/d_binary_image_classification_where_the_target/"}, {"autor": "questions13524", "date": "2021-01-07 23:00:36", "content": "[P] VAE latent parameters converge regardless of input /!/ I have asked on Torch Forums but haven't gotten a response yet, so I thought I would try Reddit. [https://discuss.pytorch.org/t/vae-latent-parameters-converge-regardless-of-input/108136](https://discuss.pytorch.org/t/vae-latent-parameters-converge-regardless-of-input/108136)\n\nWhen I train my VAE, I notice that  the means and variances learned by the VAE converge to some values that  stay constant regardless of the input -----> image !!! . When initialized, the  latent parameters are different as expected. The  VAE can reconstruct  images decently, shown are the samples (top) and  reconstructions  (bottom) of some sprites in the DSprites dataset.\n\nI  initially thought I was passing in the same images, but the image below  shows I am passing in different samples and the VAE reconstructs the  image well - it does not reconstruct the same image regardless of input.\n\nI  have noticed that regardless of what I change the latent parameters   to, as long as I keep the indices and sizes from the maxpool layers   consistent, the output of the VAE will reconstruct the input. My model   has a MaxUnpool layer which requires the above indices and sizes. I have  also noticed that without the Maxpooling layer, the VAE does not   reconstruct the input, regardless of number of epochs trained.\n\nI  was wondering if anyone knows why the latent parameters converge and  stay constant despite the input. I am curious how the decoder  reconstructs different images given the same latent parameters, indices,  and sizes from the encoder.  Also, I was wondering why the VAE without  Maxpooling does not reconstruct the input images - it returns a gray  square.\n\nThank you\n\nhttps://preview.redd.it/6ifmyn2rqz961.png?width=530&amp;format=png&amp;auto=webp&amp;s=5a3be0fad4e1d8ab5091e4e518dc414d3ec6a2d9", "link": "https://www.reddit.com/r/MachineLearning/comments/ksp63f/p_vae_latent_parameters_converge_regardless_of/"}, {"autor": "bendee983", "date": "2021-05-24 15:17:11", "content": "[D] Why Intel's DL-based photorealistic enhancement tech is not ready for prime-time gaming /!/ Last week, Intel unveiled a DL system that enhances GTA V's graphics to photorealistic level. Several media outlets have suggested that the technology has turned GTA V to \"a photorealisitc game.\"\n\nWhile impressive, Intel's -----> image !!! -enhancement system is not ready for prime-time gaming. Here are four key reasons:\n\n* **Memory**: The model requires more than a gigabyte of VRAM for inference. While this amount of memory is available on most gaming computers, we must also consider that games gobble up most of the resources of the GPU at runtime. Basically, to free up memory for photorealistic render, the users will have to make some kind of sacrifice, such as playing at a lower resolution.\n* **Sequential processing**: Neural nets rely on non-linear computations. So while you can use a GPU to process several inputs in parallel, each input must go through the entire sequence of layers. Intel's model contains at least 100 layers of computation, which is nearly impossible to run at playable framerates with current graphics cards. The researchers tried it on RTX 3090, which has no shortage of VRAM, and they still got 2 frames per second. They have suggested some optimizations to integrate the model into the game engine. This could give it a speed boost, but not enough to get anywhere near playable framerates.\n* **Development and training costs:** The researchers needed thousands of well-annotated images of urban settings to train the model. Lucky for them, the Cityscapes team had already done this for them. But if a game takes place in a setting that doesn't have an open-source dataset, then it will be up to the game devs to curate or generate and annotate their own dataset of images, which can come with huge costs. Also, most gaming companies don't have ML talent to develop, tune, and train models.\n\nRead the full analysis here. Happy to hear thoughts on/corrections to the take:\n\n[https://bdtechtalks.com/2021/05/24/intel-ai-photorealistic-enhancement/](https://bdtechtalks.com/2021/05/24/intel-ai-photorealistic-enhancement/)", "link": "https://www.reddit.com/r/MachineLearning/comments/nk0bpp/d_why_intels_dlbased_photorealistic_enhancement/"}, {"autor": "vaibhavsxn", "date": "2021-05-24 02:58:12", "content": "[D] Question regarding the real time use of my model. /!/ I have implemented a real time project that classifies an item in a factory into one of its many classes. The model works just fine on my local environment.\n\nNow this is a huge task for me:\n\nThe factory has a conveyor belt that serves those items at some speed which is 1 item per second. An item comes to a point where its -----> image !!!  is taken and that -----> image !!!  is run through my model for classification. Based on my model's output, the route of the item on the belt is determined.\n\nHow do I compile my project into a package that can serve such real time environment? I have built a google cloud run API and configured the maximum RAM for it. Its response time is 1 second. Now, I am afraid that if I deploy it in production and the API receives the images every second, it might collapse and return no response. This is just my past experience talking.\n\nIs there an alternative to accomplish a full-proof service for this? All my model returns is just a number, and nothing else.\n\nThanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/njnzfp/d_question_regarding_the_real_time_use_of_my_model/"}, {"autor": "VikasOjha666", "date": "2021-05-27 15:16:44", "content": "Detecting Text-lines in a Document -----> Image !!!  Using Deep Learning", "link": "https://www.reddit.com/r/MachineLearning/comments/nm9pr8/detecting_textlines_in_a_document_image_using/"}, {"autor": "siddarthpatil", "date": "2021-05-27 13:57:41", "content": "[Discussion] Detecting colors from a -----> picture !!! . /!/ I am a noob to ML. I am trying to work on a project which will detect colors from a picture.\n\nCurrently, I am using the K-mean algorithm to do it. But the problem with that is the algorithm is also detecting the background color. \n\n&amp;#x200B;\n\n[Leaf](https://preview.redd.it/hrqgtx705o171.jpg?width=237&amp;format=pjpg&amp;auto=webp&amp;s=cc74e20cffc30042772762ca06db7f63b2861d48)\n\n&amp;#x200B;\n\nTo understand better, I ran K-means on the above image. It detected the following colors:\n\n&amp;#x200B;\n\n[The output clusters](https://preview.redd.it/z4whypl75o171.png?width=520&amp;format=png&amp;auto=webp&amp;s=7c77270e137605962e757300d9830c638834a4b0)\n\n&amp;#x200B;\n\nAs you can see the problem here is that it is also considering the background color i.e, Red in this case. I do not want this in the output. \n\n&amp;#x200B;\n\nCan you please give me any suggestions that I need to do in order to correct the output? Any help would be appreciated. Thanks", "link": "https://www.reddit.com/r/MachineLearning/comments/nm81nh/discussion_detecting_colors_from_a_picture/"}, {"autor": "Mother-Director6004", "date": "2021-05-27 10:37:50", "content": "[D] Evolutionary algorithm for Mosaic /!/ Hi I'm trying to make a mosaic using an evolutionary algorithm but it can't process the -----> image !!! . Keeps on spewing out a random collage of pictures. Any advice? Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/nm4k1y/d_evolutionary_algorithm_for_mosaic/"}, {"autor": "kron_ninja", "date": "2021-05-27 10:24:17", "content": "[Project] Building detection model /!/  I am given a task to develop a building detection model based on satellite images. I've been given coordinates of buildings. There is not many API services available for getting images based on coordinates - I am using [this one](https://services.arcgisonline.com/arcgis/rest/services/World_Imagery/MapServer/export?bbox=-2.003750722959434E7,-1.997186888040859E7,2.003750722959434E7,1.9971868880408563E7), which does not have high resolution so you can't send request for each building. So I have to build a bounding box around multiple nearby buildings and send a request based on it. I've used k-means to cluster nearby buildings and got 165 clusters which I put a bounding box around it. Then wrote a parser and got images for those bounding boxes.\n\nNow I have to build the model, however I have my suspicions that the model won't perform great because of low quality reference data. Each -----> image !!!  contains multiple buildings, roads and etc. So I am not sure what can I do. Should I proceed cleaning this dataset(masking, interpolation and etc) or it doesn't worth the effort?\n\nMost images look like [this](https://imgur.com/a/hvRo7Li). I have nearly 100 of them, but I can increase size of dataset by requesting more building coordinates. Any advice would help.", "link": "https://www.reddit.com/r/MachineLearning/comments/nm4d55/project_building_detection_model/"}, {"autor": "pcvision", "date": "2021-05-27 00:50:30", "content": "[P] I Trained a Model to Generate Video Game Pages /!/ These past two months I've been working on a project I've called [THIS GAME DOES NOT EXIST](https://thisgamedoesnotexist.jsonchin.com). I've always wanted to try building something with generative A.I. so this project scratched that itch for me.\n\nHere's a video with a few of my favourites read by voice actors: [https://www.youtube.com/watch?v=\\_mTWMLhpJoA](https://www.youtube.com/watch?v=_mTWMLhpJoA)\n\n&amp;#x200B;\n\n&gt;THIS GAME DOES NOT EXIST is an experiment in generative artificial intelligence. This site contains 130 video game pages that were generated using an implementation of OpenAI's [Generative Pre-trained Transformer 2 (GPT-2)](https://openai.com/blog/better-language-models/) to generate text and a simple implementation of generative adversarial networks (GAN) to generate header images and \"screenshots\".  \n&gt;  \n&gt;To generate the names, descriptions, publishers, and developers of the games I finetuned the [HuggingFace](https://huggingface.co/transformers/index.html) implementation of GPT-2. I used the [Steam Store Games (Clean dataset)](https://www.kaggle.com/nikdavis/steam-store-games) from Kaggle with slight modifications and preprocessing.Here is what one training sample looks like:  \n&gt;  \n&gt;*&lt;|game|&gt;&lt;|name|&gt;Half-Life&lt;|developer|&gt;Valve &lt;|publisher|&gt;Valve&lt;|description|&gt;Named Game of the Year by over 50 publications, Valve's debut title blends action and adventure with award-winning technology to create a frighteningly realistic world where players must think to survive. Also includes an exciting multiplayer mode that allows you to play against friends and enemies around the world.&lt;|endoftext|&gt;*  \n&gt;  \n&gt;The model uses the tokens (e.g. &lt;|game|&gt; and &lt;|description|&gt;) to prompt each class of data while keeping context during the entire generation.  \n&gt;  \n&gt;Image generation was done by training a custom GAN very similar to the architecture seen in the PyTorch [DCGAN Tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) which was built to generate faces. I created two models for this site: one for generating the header images and one for generating multiple screenshots for each game.To assemble the dataset I wrote a script that downloads the images from the URLs in the [Steam Store Games (Clean dataset)](https://www.kaggle.com/nikdavis/steam-store-games) dataset. Due to my lack of resources and time to put into this project, the -----> image !!!  generation is less than ideal. You may notice though, that the header image model will generate artifacts in images that look like the titles of games, and the screenshot image model with generate what looks like levels of a 2D platformer.", "link": "https://www.reddit.com/r/MachineLearning/comments/nlvxro/p_i_trained_a_model_to_generate_video_game_pages/"}, {"autor": "buffml", "date": "2021-05-26 19:31:21", "content": "Multi-Class -----> Image !!!  Classification Flask App | Complete Project", "link": "https://www.reddit.com/r/MachineLearning/comments/nlpde1/multiclass_image_classification_flask_app/"}, {"autor": "ai-lover", "date": "2021-06-20 06:20:15", "content": "[N] Facebook AI Open Sources AugLy: A New Python Library For Data Augmentation To Develop Robust Machine Learning Models /!/ Facebook has recently open-sourced AugLy, a new Python library that aims to help AI researchers use data augmentations to evaluate and improve the durability of their machine learning models. AugLy provides sophisticated data augmentation tools to create samples to train and test different systems.\n\nAugLy is a new open-source data augmentation library that combines audio, -----> image !!! , video, and text, becoming increasingly significant in several AI research fields. It offers over 100 data augmentations based on people\u2019s real-life images and videos on platforms like Facebook and Instagram.\n\nArticle: [https://www.marktechpost.com/2021/06/19/facebook-ai-open-sources-augly-a-new-python-library-for-data-augmentation-to-develop-robust-machine-learning-models/](https://www.marktechpost.com/2021/06/19/facebook-ai-open-sources-augly-a-new-python-library-for-data-augmentation-to-develop-robust-machine-learning-models/) \n\nGithub: [https://github.com/facebookresearch/AugLy](https://github.com/facebookresearch/AugLy)\n\nFacebook Blog: https://ai.facebook.com/blog/augly-a-new-data-augmentation-library-to-help-build-more-robust-ai-models/", "link": "https://www.reddit.com/r/MachineLearning/comments/o3z63e/n_facebook_ai_open_sources_augly_a_new_python/"}, {"autor": "xdtolm", "date": "2021-06-19 18:20:09", "content": "[P] VkFFT now supports Discrete Cosine Transforms on GPU /!/ Hello, I am the creator of the [VkFFT](https://github.com/DTolm/VkFFT) \\- GPU Fast Fourier Transform library for Vulkan/CUDA/HIP and OpenCL. In the latest update, I have added support for the computation of Discrete Cosine Transforms of types II, III and IV. This is a very exciting addition to what VkFFT can do as DCTs are of big importance to -----> image !!!  processing, data compression and numerous scientific tasks. And so far there has not been a good GPU alternative to FFTW3 in this regard.\n\nVkFFT calculates DCT-II and III by mapping them to the Real-to-Complex FFT of the same size and applying needed pre and post-processing on-flight, without additional uploads/downloads. This way, VkFFT is able to achieve bandwidth-limited calculation of DCT, similar to the ordinary FFT.\n\nDCT-IV was harder to implement algorithm-wise - it is decomposed in DCT-II and DST-II sequences of half the original size. These sequences are then used to perform a single Complex-to-Complex FFT of half-size where they are used as the real and imaginary parts of a complex number. Everything is done in a single upload from global memory (with a very difficult pre/post-processing), so DCT-IV is also bandwidth-limited in VkFFT.\n\nDCTs support FP32 and FP64 precision modes and work for multidimensional systems as well. So far DCTs can be computed in a single upload configuration, which limits the max length to 8192 in FP32 for 64KB shared memory systems, but this will be improved in the future. DCT-I will also be implemented later on, as three other types of DCT are used more often and were the main target for this update.\n\nHope this will be useful to the community and feel free to ask any questions about the DCT implementation and VkFFT in general!", "link": "https://www.reddit.com/r/MachineLearning/comments/o3meco/p_vkfft_now_supports_discrete_cosine_transforms/"}, {"autor": "ChicFil-A-Sauce", "date": "2021-06-19 11:43:43", "content": "[Discussion] What is the most efficient way to create a \"fake person\"? /!/ Like  ThisPersonDoesNotExist does with styleGAN2 but multiple shots of the same \"person\" from different angles, poses etc \n\nI was thinking deepfake face onto a blender model (or just stick it on top of a real -----> picture !!! ) and let an image correction network do the rest?", "link": "https://www.reddit.com/r/MachineLearning/comments/o3e5rp/discussion_what_is_the_most_efficient_way_to/"}, {"autor": "gradientpenalty", "date": "2021-06-19 10:02:48", "content": "[P] I extend OpenAI CLIP to multilingual for both text and -----> image !!!  search", "link": "https://www.reddit.com/r/MachineLearning/comments/o3cohj/p_i_extend_openai_clip_to_multilingual_for_both/"}, {"autor": "Illustrious_Row_9971", "date": "2021-06-19 04:21:06", "content": "[R] GANs N' Roses: Stable, Controllable, Diverse -----> Image !!!  to -----> Image !!!  Translation (works for videos too!)", "link": "https://www.reddit.com/r/MachineLearning/comments/o3804y/r_gans_n_roses_stable_controllable_diverse_image/"}, {"autor": "Warm-Speech685", "date": "2021-10-18 10:27:28", "content": "[R] Frozen in Time: Learning a Joint Text-Video Embedding for Retrieval (+ live demo) /!/ Paper: [Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval](https://arxiv.org/pdf/2104.00650.pdf)\n\nhttps://preview.redd.it/tb77u6p0r6u71.png?width=1270&amp;format=png&amp;auto=webp&amp;s=870c44d57cab7e3cea71c9122dff1ead7d2f1448\n\n**Live Demo:** [**http://meru.robots.ox.ac.uk/frozen-in-time/**](http://meru.robots.ox.ac.uk/frozen-in-time/) **(Live video search over millions of videos).**\n\nProject page: [https://www.robots.ox.ac.uk/\\~vgg/research/frozen-in-time/](https://www.robots.ox.ac.uk/~vgg/research/frozen-in-time/)\n\nCode: [https://github.com/m-bain/frozen-in-time](https://github.com/m-bain/frozen-in-time)\n\nNew Public Dataset: [https://m-bain.github.io/webvid-dataset/](https://m-bain.github.io/webvid-dataset/) (2.5M captioned videos, 10M coming soon)\n\nSummary:\n\n&gt;End-to-end encoder for visual retrieval that uses only self-attention blocks. This allows flexible training of millions of variable length videos and images jointly.\n\nAbstract:\n\n&gt;Our objective in this work is video-text retrieval - in particular a joint embedding that enables efficient text-to-video retrieval. The challenges in this area include the design of the visual architecture and the nature of the training data, in that the available large scale video-text training datasets, such as HowTo100M, are noisy and hence competitive performance is achieved only at scale through large amounts of compute. We address both these challenges in this paper.We propose an end-to-end trainable model that is designed to take advantage of both large-scale -----> image !!!  and video captioning datasets. Our model is an adaptation and extension of the recent ViT and Timesformer architectures, and consists of attention in both space and time. The model is flexible and can be trained on both image and video text datasets, either independently or in conjunction. It is trained with a curriculum learning schedule that begins by treating images as 'frozen' snapshots of video, and then gradually learns to attend to increasing temporal context when trained on video datasets. We also provide a new video-text pretraining dataset WebVid-2M, comprised of over two million videos with weak captions scraped from the internet. Despite training on datasets that are an order of magnitude smaller, we show that this approach yields state-of-the-art results on standard downstream video-retrieval benchmarks including MSR-VTT, MSVD, DiDeMo and LSMDC.", "link": "https://www.reddit.com/r/MachineLearning/comments/qajx50/r_frozen_in_time_learning_a_joint_textvideo/"}, {"autor": "JMG518", "date": "2021-10-16 01:16:20", "content": "[P] [R] [D] Text-to------> Image !!!  AI with Reference -----> Image !!! s /!/ Hi guys! I am working on an architecture project to design a parking garage, and I want to use AI to help in the design process. Two problems. 1) I am struggling to find an AI algorithm/GitHub/Notebook that produces realistic results specific to the intention of my project. 2) I am struggling to find an AI algorithm/GitHub/Notebook that allows me to import a reference image that the AI can work off of to produce better results. Any tips on how I can resolve these issues? This is a WHOLE new world to me, and I am currently not cut out to tackle these problems alone. Please help me out if you have suggestions! Thanks guys!", "link": "https://www.reddit.com/r/MachineLearning/comments/q91vhd/p_r_d_texttoimage_ai_with_reference_images/"}, {"autor": "swyoon_", "date": "2021-10-15 22:27:16", "content": "[R][P] Autoencoders can reconstruct outliers /!/ An interactive web demo demonstrating autoencoder's ability to reconstruct outliers. [https://swyoon.github.io/outlier-reconstruction/](https://swyoon.github.io/outlier-reconstruction/)\n\nAn autoencoder trained on MNIST is able to successfully reconstruct a number of non-MNIST black-and-white -----> image !!! s including some Omniglot characters, a handwritten '-1', and a monotone black -----> image !!! .\n\nOutlier reconstruction is an interesting phenomenon but may not be desirable for outlier detection purposes. I dived into this problem in my ICML work and proposed Normalized Autoencoders which is less prone to reconstruct outliers. [https://arxiv.org/abs/2105.05735](https://arxiv.org/abs/2105.05735) I think it's a fun work so please check out :)\n\nThe demo was built using Tensorflow.js, so it runs on your browser! The models were initially trained on PyTorch, then converted into Tensorflow.js.", "link": "https://www.reddit.com/r/MachineLearning/comments/q8z0ue/rp_autoencoders_can_reconstruct_outliers/"}, {"autor": "Sirisian", "date": "2021-10-15 19:31:29", "content": "[R] ADOP: Approximate Differentiable One-Pixel Point Rendering /!/ [Paper](https://arxiv.org/abs/2110.06635)\n\n[Video](https://www.youtube.com/watch?v=WJRyu1JUtVw)\n\n[Github](https://github.com/darglein/ADOP)\n\n&gt; We present a novel point-based, differentiable neural rendering pipeline for scene refinement and novel view synthesis. The input are an initial estimate of the point cloud and the -----> camera !!!  parameters. The output are synthesized images from arbitrary camera poses. The point cloud rendering is performed by a differentiable renderer using multi-resolution one-pixel point rasterization. Spatial gradients of the discrete rasterization are approximated by the novel concept of ghost geometry. After rendering, the neural image pyramid is passed through a deep neural network for shading calculations and hole-filling. A differentiable, physically-based tonemapper then converts the intermediate output to the target image. Since all stages of the pipeline are differentiable, we optimize all of the scene's parameters i.e. camera model, camera pose, point position, point color, environment map, rendering network weights, vignetting, camera response function, per image exposure, and per image white balance. We show that our system is able to synthesize sharper and more consistent novel views than existing approaches because the initial reconstruction is refined during training. The efficient one-pixel point rasterization allows us to use arbitrary camera models and display scenes with well over 100M points in real time.", "link": "https://www.reddit.com/r/MachineLearning/comments/q8vo1x/r_adop_approximate_differentiable_onepixel_point/"}, {"autor": "adenml", "date": "2021-06-26 05:50:26", "content": "[D] Have you ever seen a good, useful, convincing ML pipeline -----> image !!!  / infographic? Is it even possible to create a one for a ML subfield? /!/ I think that nothing receives more hate than ML drawings that tell when and how to choose a model, when and how to do certain preprocessing steps. For example, I think we can all agree the scikit learn [https://scikit-learn.org/stable/tutorial/machine\\_learning\\_map/index.html](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) is quite dumb and useless. Unfortunately, it is also the most famous one\n\nNow, good infographics would be gold for beginners and interns, but I have never seen one. \n\nI am not talking about the entire ML, it would be way to complex for this. I would be extremely happy to see a good pipeline for a ML sub-field like NLP. When should I use LSTMs? When should I use a Char CNN? What about transformers? When should I use SVM + string kernels? When should I use gloVe or TFIDF? What preprocessing steps are necessary? Whan should I stem / lemmatize my data? Should I remove stopwords? Should I remove rare chars? When, why? Should I do manual feature engineering on the texts? Should I convert all to lowercase? \n\nNow, what about you? Have you ever seen a good pipeline (regarding how to choose a model and what preprocessing steps to take) at least for a sub-domain of ML?", "link": "https://www.reddit.com/r/MachineLearning/comments/o84w96/d_have_you_ever_seen_a_good_useful_convincing_ml/"}, {"autor": "techsucker", "date": "2021-06-26 04:19:38", "content": "[N] Facebook AI Uses Reverse Engineering Generative Models From A Single Deepfake -----> Image !!!  To Study And Detect Deepfake /!/ Deepfakes have become increasingly convincing over the years. In collaboration with Michigan State University (MSU), Facebook has presented a research method of detecting and attributing Deepfakes based on reverse engineering from a single AI-generated image to the generative model used to produce the image. The technique will allow deepfake detection and tracing in a real-world scenario, where the often the only information available for deepfake detectors is the image itself.\n\nFull Story: [https://www.marktechpost.com/2021/06/25/facebook-ai-uses-reverse-engineering-generative-models-from-a-single-deepfake-image-to-study-and-detect-deepfake/](https://www.marktechpost.com/2021/06/25/facebook-ai-uses-reverse-engineering-generative-models-from-a-single-deepfake-image-to-study-and-detect-deepfake/)", "link": "https://www.reddit.com/r/MachineLearning/comments/o83n1z/n_facebook_ai_uses_reverse_engineering_generative/"}, {"autor": "techsucker", "date": "2021-10-11 05:49:33", "content": "[R] Researchers From Imperial College London Introduces \u2018HeadGAN\u2019, A Novel One-Shot GAN-Based Method For Talking Head Animation And Editing /!/ While recent attempts to solve the problem of head reenactment using a single reference -----> image !!!  have shown promising results, most of them perform poorly in terms of photo-realism and fail at preserving identity. Researchers from Imperial College London, Huawei Technologies (UK), and the University of Sussex propose \u2018[HeadGAN](https://arxiv.org/pdf/2012.08261.pdf)\u2018, a novel one-shot GAN-based method for talking head animation and editing.\n\nThe research group took a different approach from most existing few-shot methods and used 3D face representations to condition synthesis. They benefit from prior knowledge of expression and identity disentanglement, enclosed within 3D Morphable Models (3DMMs).\u00a0\n\n# [5 Min Quick](https://www.marktechpost.com/2021/10/10/researchers-from-imperial-college-london-introduces-headgan-a-novel-one-shot-gan-based-method-for-talking-head-animation-and-editing/) [Read](https://www.marktechpost.com/2021/10/10/researchers-from-imperial-college-london-introduces-headgan-a-novel-one-shot-gan-based-method-for-talking-head-animation-and-editing/) | [Paper](https://arxiv.org/pdf/2012.08261.pdf) | [Project](https://michaildoukas.github.io/HeadGAN/) | [Video](https://www.youtube.com/watch?v=5eg85fi7Y5g)\n\n&amp;#x200B;\n\n*Processing video dawja626frs71...*", "link": "https://www.reddit.com/r/MachineLearning/comments/q5pb2b/r_researchers_from_imperial_college_london/"}, {"autor": "data_driven_approach", "date": "2021-10-10 21:04:16", "content": "[D] Help a newbie learn :) WGAN implementation with Custom Dataset /!/ Hello all!\n\nSo I started last week looking into GANs. I want to reach a point in which I feed it -----> image !!! s and styles that I like and hopefully it will generate other things I like \ud83d\ude42\n\nThe entry was a simple GAN which worked (I guess?) until it reached a point that it generated the same -----> image !!!  again and again, until I discovered something called \\`mode collapse\\`  [cGAN/gan at master \u00b7 czioutas/cGAN (github.com)](https://github.com/czioutas/cGAN/tree/master/gan)\n\nOff I was to implement WGAN which should tackle this problem!\n\nWGAN has definitely more complexity that is probably too deep for my understanding.\n\nI progressed by googling and ended up with  [cGAN/wgan at master \u00b7 czioutas/cGAN (github.com)](https://github.com/czioutas/cGAN/tree/master/wgan)The problem being in the end that the input images do not match the needs of the model but I do not know how to fix em.\n\nI added a stack trace of the output and the error line that points to the issue  [cGAN/model.py at master \u00b7 czioutas/cGAN (github.com)](https://github.com/czioutas/cGAN/blob/master/wgan/model.py#L144)\n\nIf anyone is willing to help out, comment, contact me anywhere you want (twitter is linked on github)\n\nThank you all in advance and its time to go hit the bed \ud83d\udca4  \n\n\nP.S Did not know if I should put \\[P\\] or \\[D\\] on the title :/", "link": "https://www.reddit.com/r/MachineLearning/comments/q5gqll/d_help_a_newbie_learn_wgan_implementation_with/"}, {"autor": "Fun-Engineering2112", "date": "2021-10-10 18:53:53", "content": "[D] How to train your Yahtzee? What would be your approach plan for an optimal ML learned strategy starting from scratch? I have added mine, let me know what you would do different/better :) /!/ Hi all,\n\nGlad to see that this title intrigued you!\nThis is my first Reddit post, hoping to find like minded people.\n\nHow could the \"Perfect Yahtzee algorithm\" be created? \nAssuming you don't have to do all the work yourself (which would be a life's work and possibly a post-mortem Nobel prize if you achieve a framework that is applicable to many other games and problems?). Why are modern successful game  algo's not generally applicable to other problems?\nThis rant is an ant step in the quest for a general ai framework. \n\nBeen thinking the last 2 hours about that question as if I'm OCD.\n\nThis would be my approach: Focus on the AI experts / programmers collaboration platform/framework and try to build a community. Identify which parts still need to be built and visualize / numerify that shit: \n- ML algorithms \n- Yahtzee simulator (Uber fast for Millions of runs \n- Input interfaces (website, OCR from -----> picture !!! , etc.) \n- Statistics framework: (Algorithm A beats Algorithm B with 2.3 points average over 165.765 simulations. R-value=xx.x) \n- Collaboration platform: I'm a noob in this, but I'm imagining something very visual at first: like a math problem written outside the university classroom. Some kind of forum for any science / tech enthousiast worldwide to run through topics. Click one: opens a visual graph with tasks already completed and still open. (All that fancy filtering on difficulty / estimated effort etc. Included of course). Does anything like that exists already? \n- ...\n\nSetting the first goal of having a website where you can enter (or scan, printscreen, webservice, whatever) any current configuration of a Yahtzee game. And it will return your best option at that point (\"keep the 2 3's\"). An easy website for Noobs and Pro's alike. If this is taking off, then of course beat the best human players as well with the optimal algorithm/ per situation optimal. All though, Yahtzee is not nearly as complex as chess, backgammon nor Go; so I guess this has been done before. The underlying motivation for myself being to learn more about how to achieve bigger goals in AI using collaboration platforms. Are abstract project management / frameworks possible over different games and problems? In an abstract framework, can a new algorithm quickly measure it's fitness over multiple disciplines / compare with it's peers?\n\nI'm sure I'm not the first nor the one with the most ML context to think of this. Sooo, share your knowledge please :)\n\nApologies for the OCD like rant, your turn now ;)", "link": "https://www.reddit.com/r/MachineLearning/comments/q5e7e0/d_how_to_train_your_yahtzee_what_would_be_your/"}, {"autor": "KirillTheMunchKing", "date": "2021-10-25 01:38:11", "content": "[D] CIPS Follow-Up Paper explained - Harnessing the Conditioning Sensorium for Improved Image Translation (5-minute summary by Casual GAN Papers - The Author of OG CIPS) /!/ Hey everyone!\n\nI was one of the authors of the original CIPS paper and I thought it would be fun to do a breakdown of this follow-up paper that takes CIPS into the 3D world!\n\nIf you have been following generative ML for a while you might have noticed more and more GAN papers focusing on the underlying 3D  representation of the generated images. CIPS-3D is a 3D-aware GAN model proposed by Peng Zhou and the team at Shanghai Jiao Tong University  &amp; Huawei that combines a low-res NeRF (surprise) with a CIPS  generator (genuine surprise) to achieve high quality 256x256 3D-aware  -----> image !!!  synthesis as well as transfer learning and 3D-aware face stylization.\n\nFresh out of the oven! Full summary: [https://www.casualganpapers.com/3d-aware-gan-based-on-cips-and-nerf/CIPS-3D-explained.html](https://www.casualganpapers.com/3d-aware-gan-based-on-cips-and-nerf/CIPS-3D-explained.html)\n\n*Processing video k3960pu23iv71...*\n\narxiv: [https://arxiv.org/pdf/2110.09788.pdf](https://arxiv.org/pdf/2110.09788.pdf)  \ncode: [https://github.com/PeterouZh/CIPS-3D](https://github.com/PeterouZh/CIPS-3D)\n\nSubscribe to [Casual GAN Papers](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!", "link": "https://www.reddit.com/r/MachineLearning/comments/qf66pv/d_cips_followup_paper_explained_harnessing_the/"}, {"autor": "forthispost96", "date": "2021-10-25 00:16:05", "content": "[D] Trouble Modelling High Dimensional Regression Problem with Autoencoder /!/ I've been given the task to fit an Autoencoder to a large dataset of 600 dimensional vectors. After constructing some relatively simple AEs (eg., nn.Linear(600) --&gt; nn.Linear(150) --&gt; nn.Linear(5), reverse..), I've found that it's incredibly hard to converge onto a non-trivial solution. These vectors have been normalized in such a way that their means lie \\~0.50, and are bounded between 0-1. I have attached an -----> image !!!  illustrating a few of the vectors within my dataset. \n\nI'm finding that since these vectors essentially look like noise centered around some global mean value, any model that I attempt to train collapses onto some noisy curve centered around the mean. Importantly, these vectors all represent something important, so smoothing or other preprocessing tricks are out of the question. Sadly, there doesn't seem to be much structure within the data, so a CNN model is also out. \n\nThis is basically a high-dimensional regression problem with diverse data. Does anyone have any tips for training an AE model with such data? Is it even possible? Any tips or recommendations would be great!", "link": "https://www.reddit.com/r/MachineLearning/comments/qf4qxs/d_trouble_modelling_high_dimensional_regression/"}, {"autor": "NetHairy4282", "date": "2021-10-05 06:10:33", "content": "[P] KotlinDL 0.3 Is Out With ONNX Integration, Object Detection API, 20+ New Models in ModelHub, and Many New Layers /!/  Introducing version 0.3 of our deep learning library, [KotlinDL](https://github.com/JetBrains/KotlinDL).\n\nRead the release post on [Medium](https://zaleslaw.medium.com/kotlindl-0-3-43fffd3cb94) or in [JetBrains Blog](https://blog.jetbrains.com/kotlin/2021/09/kotlindl-0-3-is-out-with-onnx-integration-object-detection-api-20-new-models-in-modelhub-and-many-new-layers/).\n\nKotlinDL 0.3 is available now on [Maven Central](https://search.maven.org/artifact/org.jetbrains.kotlinx/kotlin-deeplearning-api) with a variety of new features \u2014 check out all the [changes](https://github.com/JetBrains/KotlinDL/blob/master/CHANGELOG.md) that are coming to the new release!\n\nWe're currently introducing a lot of new models in ModelHub (including the first Object Detection and Face Alignment models), the ability to fine-tune the Image Recognition models saved in ONNX format from Keras and PyTorch, the experimental high-level Kotlin API for -----> image !!!  recognition, a lot of new layers contributed by the community members and many other changes.\n\nhttps://i.redd.it/bvnlemtkpkr71.gif\n\n If you have any feature requests, please create an issue or write a question in the Discussion chapter.\n\nKotlinDL is built on top of the TensorFlow Java API and ONNX Runtime Java API and has an API close to Keras and other high-level frameworks like Sonnet, PyTorch Lighting, and Catalyst.\n\nGive a star on Github to KotlinDL if you support this project, run tutorials, taste the Kotlin with Deep Learning!", "link": "https://www.reddit.com/r/MachineLearning/comments/q1os79/p_kotlindl_03_is_out_with_onnx_integration_object/"}, {"autor": "fella85", "date": "2021-10-05 04:28:44", "content": "[P] Examples of ML cloud environments /!/ Hi Everyone, \n\nWould anyone know of examples of cloud environments (AWS or Azure) for development of ML algorithms and performing statistical analysis (ie. running Stan, etc) ?  \n\n\nFor example, is there something more sophisticated than just creating a docker -----> image !!!  of your favourite tools and spinning a VM with it?  \n\n\nI would be interested in different architectures, etc.  \n\n\nCheers,\n\nFella85", "link": "https://www.reddit.com/r/MachineLearning/comments/q1ndgp/p_examples_of_ml_cloud_environments/"}, {"autor": "MasterChiefOne", "date": "2021-10-05 03:59:19", "content": "Asking a text to -----> image !!!  artificial intelligence to convert wtf into an -----> image !!! ", "link": "https://www.reddit.com/r/MachineLearning/comments/q1mwuh/asking_a_text_to_image_artificial_intelligence_to/"}, {"autor": "Yuqing7", "date": "2021-10-04 14:26:10", "content": "[R] Debiasing -----> Image !!!  Datasets: Oxford University Presents PASS, an -----> Image !!! Net Replacement for Self-Supervised Pretraining /!/ An Oxford University research team presents PASS, a large (1.28M) image collection excluding humans, created as an ImageNet replacement for self-supervised pretraining without technical, ethical or legal issues. \n\nHere is a quick read: [Debiasing Image Datasets: Oxford University Presents PASS, an ImageNet Replacement for Self-Supervised Pretraining.](https://syncedreview.com/2021/10/04/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-116/)\n\nThe paper *PASS: An ImageNet Replacement for Self-Supervised Pretraining Without Humans* is on [arXiv](https://arxiv.org/abs/2109.13228).", "link": "https://www.reddit.com/r/MachineLearning/comments/q16hc9/r_debiasing_image_datasets_oxford_university/"}, {"autor": "rickardsjogren", "date": "2021-10-04 09:42:21", "content": "[R] LIVECell - A large-scale dataset for label-free live cell segmentation /!/ We are a research team at [Sartorius](https://www.sartorius.com/en) and [DFKI](https://www.dfki.de/en/web/) Kaiserslautern who has recently open-sourced [LIVECell](https://sartorius-research.github.io/LIVECell/) ([paper](https://www.nature.com/articles/s41592-021-01249-6)), the largest manually annotated dataset for cell segmentation of microscopic -----> image !!! s available to date.\n\n[Example -----> image !!!  with annotations of a neuronal cell type included in LIVECell.](https://preview.redd.it/zvrkhfv24er71.png?width=1111&amp;format=png&amp;auto=webp&amp;s=78885a28b024e47993b8788f20ebb3cb645a5f4e)\n\n**LIVECell (Label-free In Vitro -----> image !!!  Examples of Cells)** consists of 5,239 manually annotated, expert-validated, Incucyte HD phase-contrast microscopy -----> image !!! s with a total of 1,686,352 individual cells annotated from eight different morphologically distinct cell types (average 313 objects per -----> image !!! ). Our hopes are that LIVECell will provide a valuable resource to the cell biology-community as well as an interesting benchmark for the machine learning-community. LIVECell is published under the CC-BY-NC 4.0-license making it freely available for non-commercial purposes.\n\n**Why should we care about cell segmentation?** Microscopic images of two-dimensional cell cultures like the ones in LIVECell are extremely commonly used in cell biology-research making them important tools during development of new medicines. Microscopic imaging is cheap and accessible and provides high detail in both space and time, even at the individual cell level. Cell instance segmentation can however be difficult due to low contrast, weird object shapes and high object density. So, we need high-performing instance segmentation-models and datasets to train them to help researchers use microscopy to find new medicines in a faster and cheaper way.\n\n**Some things we learned from a machine learning-perspective** (although we have not done any systematic ML-study yet):\n\n* The two models we trained on LIVECell achieved similar mask AP to what they achieved on MS COCO, which indicates that the two datasets are somewhat similar in overall difficulty.\n* Our two models achieve similar mask AP but differ in terms of average false negative ratio (Fig 3). We have not yet investigated the cause of this difference.\n* The different cell types morphologies vary in difficulty but neuronal-like cell types with their highly non-convex morphologies (image in post) are really tricky to segment well using standard instance segmentation models.\n* Due to the large number of objects per image (average \\~300, up to 3000+), LIVECell poses an interesting challenge for computational efficient instance segmentation models (Fig S11).\n* We also stress test our models on even higher object densities, finding that our best model in terms of mask AP collapses whereas the other model extrapolates a fair bit (Fig 4). Our hypothesis is that this difference may be due to the different prediction mechanisms (anchor-based/free).\n* Although all images in LIVECell are from the same instrument, models trained on it transfer very well to others given appropriate image preprocessing (Fig S12-13). We even establish new state-of-the-art on a previously published cell segmentation-dataset without training on its images (Fig S12). \n\n**Links**\n\n* Link to dataset: [https://sartorius-research.github.io/LIVECell/](https://sartorius-research.github.io/LIVECell/)\n* Open access paper: [https://www.nature.com/articles/s41592-021-01249-6](https://www.nature.com/articles/s41592-021-01249-6)\n* Papers with code: [https://paperswithcode.com/dataset/livecell](https://paperswithcode.com/dataset/livecell)\n* Github: [https://github.com/sartorius-research/LIVECell](https://github.com/sartorius-research/LIVECell)", "link": "https://www.reddit.com/r/MachineLearning/comments/q125xp/r_livecell_a_largescale_dataset_for_labelfree/"}, {"autor": "techsucker", "date": "2021-09-07 18:14:45", "content": "[R] A New Google AI Research Study Discovers Anomalous Data Using Self Supervised Learning (Paper, Code Included) /!/ Anomaly detection is one of the most common machine learning applications in various areas, from industrial defect identification to fraudulent financial detection.\u00a0\n\nOne-class classification is beneficial for anomaly detection. It determines whether an instance belongs to the same distribution as the training data by assuming that the training data are all normal examples. However, representation learning is not available to these old methods. Furthermore, self-supervised learning has made significant progress in learning visual representations from unlabeled data, including rotation prediction and contrastive learning.\u00a0\n\nNew Google AI[ research](https://arxiv.org/pdf/2011.02578.pdf) introduces a 2-stage framework that uses recent progress on self-supervised representation learning and classic one-class algorithms. This framework is simple to train and shows SOTA performance on various benchmarks, including [CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html), [f-MNIST,](https://github.com/zalandoresearch/fashion-mnist) [Cat vs. Dog](https://www.microsoft.com/en-us/research/publication/asirra-a-captcha-that-exploits-interest-aligned-manual------> image !!! -categorization/), and [CelebA](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). Following that, they offer a novel representation learning approach for a practical industrial defect detection problem using the same architecture. On the MVTec benchmark, the framework achieves a new state-of-the-art.\n\n# [5 Min Read](https://www.marktechpost.com/2021/09/07/a-new-google-ai-research-study-discovers-anomalous-data-using-self-supervised-learning/) | [Paper](https://arxiv.org/pdf/2011.02578.pdf) | [Code](https://github.com/google-research/deep_representation_one_class) | [Google Blog](https://ai.googleblog.com/2021/09/discovering-anomalous-data-with-self.html)\n\n&amp;#x200B;\n\n*Processing gif txphstlah4m71...*", "link": "https://www.reddit.com/r/MachineLearning/comments/pjslso/r_a_new_google_ai_research_study_discovers/"}, {"autor": "KirillTheMunchKing", "date": "2021-09-07 14:53:50", "content": "[D] Paper explained - Perceiver IO: A General Architecture for Structured Inputs &amp; Outputs (5-minute summary) /!/ [PerceiverIO](https://preview.redd.it/hlifk1q5h3m71.png?width=1773&amp;format=png&amp;auto=webp&amp;s=05ebeae1e06367a5d698a27a7b50558d28eac517)\n\nReal-world applications often require models to handle combinations of data from different modalities: speech/text, text/-----> image !!! , video/3d. In the past specific encoders needed to be developed for every type of modality. Moreover, a third model was required to combine the outputs of several encoders, and another model - to transform the output in a task-specific way. Now thanks to the effort of the folks at DeepMind we now have a single model that utilizes a transformer-based latent model to handle pretty much any type and size of input and output data. As some would say: is attention all you need?\n\nCheck out the [full paper summary](https://www.casualganpapers.com/cross-modal-fully-attentional-transformer/PerceiverIO-explained.html) at Casual GAN Papers (Reading time \\~5 minutes).\n\nSubscribe to [my channel](https://t.me/casual_gan) for weekly AI paper summaries\n\nCheers,  \n\\-Kirill", "link": "https://www.reddit.com/r/MachineLearning/comments/pjom3l/d_paper_explained_perceiver_io_a_general/"}, {"autor": "IamKun2", "date": "2021-02-13 14:21:30", "content": "[Research] NN to extrapolate painting /!/ Hi! I have the -----> image !!!  on the left (it's a man upside down). I would like to complete the black outside parts with something that makes sense for the painting. Similar to what I have on the right (i.e. some content was added that follows the style and fits nicely). Any pointers? thanks!  \n\n\nhttps://preview.redd.it/pn82xwys79h61.png?width=1069&amp;format=png&amp;auto=webp&amp;s=f257a7fca93038488294666fa084abe3bd6d5279", "link": "https://www.reddit.com/r/MachineLearning/comments/lj1ds6/research_nn_to_extrapolate_painting/"}, {"autor": "nivter", "date": "2021-08-10 12:20:37", "content": "[Research] We distilled CLIP model (ViT only) from 350MB to 24MB and ran it on an iPhone /!/ I would like to share some of our results and experiences performing model distillation on CLIP model (the Vision Transformer component). The student model has similar architecture and layers as the original CLIP, although with fewer parameters.\n\nThe student model weighed 48MB. After training for a couple of weeks on a single P100 GPU we got some promising results. We converted the model into CoreML format, reduced the precision to FP16 (weighing only 24MB) and found negligible change in its performance compared to the FP32 model. For doing -----> image !!!  search, we still used the original language model which is part of CLIP.\n\nWe also tried dealing with the issue of CLIP getting confused by text in images. We figured out a way to control the text-ness of search results by finding a direction in the shared vector space which corresponds to the property of text-ness.\n\n[Here is the article](https://tech.pic-collage.com/distillation-of-clip-model-and-other-experiments-f8394b7321ce) which provides more details.", "link": "https://www.reddit.com/r/MachineLearning/comments/p1o2bd/research_we_distilled_clip_model_vit_only_from/"}, {"autor": "MoreGPUVeryNice", "date": "2021-06-24 09:12:16", "content": "[D] Deep clustering survery /!/  Hi!\n\n&lt;new here&gt;\n\nI am performing -----> image !!!  (deep) clustering survey and I am trying to pinpoint impactful papers from recent years.\n\nI am aware of the great survey [here](https://github.com/zhoushengisnoob/DeepClustering) but I have a hard time discriminating which paper is impactful and which isn't. Checking number of citations is helping, but I was hoping the community here could pinpoint me to important papers from the field.\n\n&amp;#x200B;\n\nThanks :)", "link": "https://www.reddit.com/r/MachineLearning/comments/o6x88y/d_deep_clustering_survery/"}, {"autor": "ivanevti", "date": "2021-06-24 05:06:23", "content": "[R] The Dimpled Manifold Model of Adversarial Examples in Machine Learning /!/ Very, very interesting new work by Adi Shamir et al: [https://arxiv.org/abs/2106.10151](https://arxiv.org/abs/2106.10151)\n\nIt proposes a new mental model for why adversarial examples exist. The central claim is that adversarial examples come from the fact that we fit high-dimensional decision boundaries to low-dimensional images. This leaves a lot of space for the adversarial examples to exist perpendicularly from the true location of the low-dimensional object (the natural -----> image !!! ). \n\nMore precisely: the decision boundary is like a thin, horizontal sheet of metal that is being bent up and down to fit clusters of training examples. This creates dimples in the sheet around where the training examples lie. The sheet can be thought of as the space in which natural images exist; the dimples go into the extra dimensions because we represent those natural images in a very high-dimensional form (RGB images).\n\nAdversarial examples? Well, these are just unnatural images above and below the sheet , i.e. off the manifold of natural image.\n\nRobustness accuracy trade-off? This just comes from the fact that you have to bend the sheet way more out of shape than normal to fit adversarial examples, so you end up missing the details/small clusters.\n\nBeing able to fit a model with good clean test-set performance by training on adversarial examples with target labels? That comes from the fact that you end up recreating the manifold by moving around the labels.\n\nHumans being insensitive to adversarial examples? They have just learned to do projections into the low-dimensional space, so anything that lives above/below the dimple gets projected onto the dimple.\n\nThis mental model seems to provide a lot of compelling explanations and there is good experimental work. What do you think about it?", "link": "https://www.reddit.com/r/MachineLearning/comments/o6u34b/r_the_dimpled_manifold_model_of_adversarial/"}, {"autor": "JozeTostado", "date": "2021-06-24 01:57:47", "content": "How can I use this model to predict if an -----> image !!!  is of dog or a cat /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/o6r5fu/how_can_i_use_this_model_to_predict_if_an_image/"}, {"autor": "JozeTostado", "date": "2021-06-24 01:56:38", "content": "How can I use this model to predict if an -----> image !!!  is of a dog or a cat /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/o6r4s0/how_can_i_use_this_model_to_predict_if_an_image/"}, {"autor": "SouthBayDev", "date": "2021-06-23 21:34:38", "content": "[D] -----> Image !!!  search, chatbot and chemical structure demo /!/ Hi all,  we recently updated our demo page showcasing examples of vector search for image search, chatbot and chemical structures.  I'm looking for feedback, suggestions or questions you may have to make this relevant for any applications or use cases you may be developing.  Thank you all, I look forward to hearing your feedback!\n\n[Demo Page](https://zilliz.com/milvus-demos)\n\nGitHub Repo", "link": "https://www.reddit.com/r/MachineLearning/comments/o6mjyi/d_image_search_chatbot_and_chemical_structure_demo/"}, {"autor": "SouthBayDev", "date": "2021-06-23 21:04:51", "content": "-----> Image !!!  search, chatbot and chemical structure search demo page /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/o6lz96/image_search_chatbot_and_chemical_structure/"}, {"autor": "SquareConfidence7", "date": "2021-06-23 19:59:59", "content": "[P] multiple instance -----> image !!!  classification in Pytorch: model won't learn /!/ I am working on an image classification model in Pytorch. The setup is the following: My training instances are bags of images (i.e. one training instance = one bag), where each bag contains a varying amount of images. Each bag has one label associated with it (0 or 1), that indicates whether at least one of the images contains a certain property (e.g. tumor as in my case). The objective is to learn a classifier that classifies new bags as accurately as possible. However the following problem occurs when I try to train my model: when I feed the bags one by one into the CNN architecture, the prediction (the probability that bag has label 1) is instantly either 0 or 1.\n\nNow before I delve into the code itself, which is quite long, I have a similar setup where instead of tissue images I use numbers. So each bag contains a number of MNIST like images (just pictures with a number), and the bag gets a positive label (i.e. 1) if the bag contains the number 9. Strangely enough, this task with the digits works very well (you can see that the learning works and in the end good classification performance is obtained), even though the setup compared to the tissue images is near identical.\n\nBelow I post code sections that differ between these 2 tasks, but they are essentially the following: the input shape of the bags differ, the digit images are much smaller (28x28) and have only one channel while the tissue images are 224x224 and have three channels. Therefor the convolutional layers also vary a little bit in specification. Any help as to what the problem might be would be greatly appreciated!\n\n&amp;#x200B;\n\n**First code section is of the tissue images, which has the problem that somehow this model won't learn**\n\n    class Attention(nn.Module):\n        def __init__(self):\n            super(Attention, self).__init__()\n            self.L = 500\n            self.D = 128\n            self.K = 1\n    \n    \n            self.feature_extractor_part1 = nn.Sequential(\n                nn.Conv2d(3, 4, kernel_size=4), # 3 because three color channels, each kernel has size 3X4X4\n                nn.ReLU(),\n                nn.MaxPool2d(2, stride=2),\n                nn.Conv2d(4, 8, kernel_size=3), # combine all input for one output\n                nn.ReLU(),\n                nn.MaxPool2d(2, stride=2)\n            )\n    \n            self.feature_extractor_part2 = nn.Sequential(\n                nn.Linear(8 * 54 * 54, self.L),    # y = Ax + b\n                nn.ReLU(),\n                #add dropout\n            )\n    \n            self.attention = nn.Sequential(\n                nn.Linear(self.L, self.D),\n                nn.Tanh(),\n                nn.Linear(self.D, self.K)\n            )\n    \n            self.classifier = nn.Sequential(\n                nn.Linear(self.L*self.K, 1),\n                nn.Sigmoid()\n            )\n    \n        # X is input and is one bag\n        def forward(self, x):\n            x = x.squeeze(0) #remove first dimension the bag tensor\n    \n            # feature extraction part\n            H = self.feature_extractor_part1(x)  \n            H = H.view(-1, 8 * 54 * 54)  \n            H = self.feature_extractor_part2(H)  # NxL\n            \n    \n            # aggregation part\n            A = self.attention(H) \n            A = torch.transpose(A, 1, 0)  # KxN\n            A = F.softmax(A, dim=1)  # softmax over N\n    \n            # H gets multiplied with A, where A is some kind of multiplied H\n            M = torch.mm(A, H)  # KxL #so KxL is the feature of the bag\n            print(M.shape) #torch.Size([1, 500])\n          \n    \n            # final transformation part\n            Y_prob = self.classifier(M)  # KxL to a one dim output for probability bag label\n            Y_hat = torch.ge(Y_prob, 0.5).float()\n     \n    \n            return Y_prob, Y_hat, A\n\n**Second code section is of the digit images, which works perfectly**\n\n    class Attention(nn.Module):\n        def __init__(self):\n            super(Attention, self).__init__()\n            self.L = 500\n            self.D = 128\n            self.K = 1\n    \n            self.feature_extractor_part1 = nn.Sequential(\n                nn.Conv2d(1, 10, kernel_size=5), # 1 because one color channel, 20 output feature #20\n                nn.ReLU(),\n                nn.MaxPool2d(2, stride=2),\n                nn.Conv2d(10, 20, kernel_size=5), #50\n                nn.ReLU(),\n                nn.MaxPool2d(2, stride=2)\n            )\n    \n            self.feature_extractor_part2 = nn.Sequential(\n                nn.Linear(20 * 4 * 4, self.L),    #y= Ax + b   #50 feature maps and size 4x4\n                nn.ReLU(),\n            )\n    \n            self.attention = nn.Sequential(\n                nn.Linear(self.L, self.D),\n                nn.Tanh(),\n                nn.Linear(self.D, self.K)\n            )\n    \n            self.classifier = nn.Sequential(\n                nn.Linear(self.L*self.K, 1),\n                nn.Sigmoid()\n            )\n    \n        # X is input and is one bag\n        def forward(self, x):\n            \n            x = x.squeeze(0) #remove first dimension the bag tensor\n    \n            # feature extraction part\n            H = self.feature_extractor_part1(x)  \n            H = H.view(-1, 20 * 4 * 4) \n            H = self.feature_extractor_part2(H)  # NxL\n    \n            # aggregation part\n            A = self.attention(H)  # NxK\n            A = torch.transpose(A, 1, 0)  # KxN\n            A = F.softmax(A, dim=1)  # softmax over N\n    \n            # H gets multiplied with A, where A is some kind of multiplied H\n            M = torch.mm(A, H)  # KxL #so KxL is the feature of the bag\n            \n            # final transformation part\n            Y_prob = self.classifier(M)  # KxL to a one dim output for probability bag label\n            Y_hat = torch.ge(Y_prob, 0.5).float()\n    \n            return Y_prob, Y_hat, A", "link": "https://www.reddit.com/r/MachineLearning/comments/o6kp7e/p_multiple_instance_image_classification_in/"}, {"autor": "BoraaaBoraaa", "date": "2021-06-23 19:48:48", "content": "[R] The Difference Between a Blurring Matrix and a PSF in Image Reconstruction /!/ I'm working on a research project related to deblurring images and I don't fully understand the difference between a blurring matrix and a PSF. I understand that to apply them to an -----> image !!!  you use two different operators, but why is that, what is the difference between them. Is a PSF not a matrix? How are they related to each other? Do they affect the image the same way?", "link": "https://www.reddit.com/r/MachineLearning/comments/o6kguq/r_the_difference_between_a_blurring_matrix_and_a/"}, {"autor": "SquareConfidence7", "date": "2021-06-23 19:44:53", "content": "multiple instance -----> image !!!  classification in Pytorch: model won't learn /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/o6kdxo/multiple_instance_image_classification_in_pytorch/"}, {"autor": "egnehots", "date": "2021-06-23 18:47:18", "content": "[R] Alias-Free GAN /!/ [https://nvlabs.github.io/alias-free-gan/](https://nvlabs.github.io/alias-free-gan/)\n\n\nAbstract:\n\nWe observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to -----> image !!!  coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.\n\n\npaper: \n[https://nvlabs-fi-cdn.nvidia.com/alias-free-gan/alias-free-gan-paper.pdf](https://nvlabs-fi-cdn.nvidia.com/alias-free-gan/alias-free-gan-paper.pdf)", "link": "https://www.reddit.com/r/MachineLearning/comments/o6j8qf/r_aliasfree_gan/"}, {"autor": "ykilcher", "date": "2021-06-23 12:47:34", "content": "[D] Paper Explained - XCiT: Cross-Covariance Image Transformers (Full Video Analysis) /!/ [https://youtu.be/g08NkNWmZTA](https://youtu.be/g08NkNWmZTA)\n\nAfter dominating Natural Language Processing, Transformers have taken over Computer Vision recently with the advent of Vision Transformers. However, the attention mechanism's quadratic complexity in the number of tokens means that Transformers do not scale well to high-resolution images. XCiT is a new Transformer architecture, containing XCA, a transposed version of attention, reducing the complexity from quadratic to linear, and at least on -----> image !!!  data, it appears to perform on par with other models. What does this mean for the field? Is this even a transformer? What really matters in deep learning?\n\n&amp;#x200B;\n\nOUTLINE:\n\n0:00 - Intro &amp; Overview\n\n3:45 - Self-Attention vs Cross-Covariance Attention (XCA)\n\n19:55 - Cross-Covariance Image Transformer (XCiT) Architecture\n\n26:00 - Theoretical &amp; Engineering considerations\n\n30:40 - Experimental Results\n\n33:20 - Comments &amp; Conclusion\n\n&amp;#x200B;\n\nPaper: [https://arxiv.org/abs/2106.09681](https://arxiv.org/abs/2106.09681)\n\nCode: [https://github.com/facebookresearch/xcit](https://github.com/facebookresearch/xcit)", "link": "https://www.reddit.com/r/MachineLearning/comments/o6c42r/d_paper_explained_xcit_crosscovariance_image/"}, {"autor": "minimaxir", "date": "2021-08-08 16:43:37", "content": "[P] Colab Notebook to easily create realistic AI-Generated Images with VQGAN + CLIP /!/ https://colab.research.google.com/drive/1wkF67ThUz37T2_oPIuSwuO4e_-0vjaLs?usp=sharing\n\nThere's been a few interesting VQGAN + CLIP projects released recently, however the latest implementation (using Pooling) finally hits the ideal quality and speed for AI-Generated images. So I forked it, added significant features such as icon -----> image !!!  generation to allow the user to steer generation, and streamlined it such that it does not require machine learning knowledge to operate.\n\nThere are many other examples generated from this Notebook on my Twitter feed: https://twitter.com/minimaxir/media\n\nLet me know what you think!", "link": "https://www.reddit.com/r/MachineLearning/comments/p0i2f4/p_colab_notebook_to_easily_create_realistic/"}, {"autor": "throwaway33013301", "date": "2021-08-08 11:53:27", "content": "[D]Are there any papers directly comparing locally connected neural networks with shared weights(convolution) and without? /!/ Basically, i have been looking for any comparison on the same -----> image !!!  dataset with similar network architecture -- and then comparing the results, stability etc. -- where one network uses LocallyConnected layers(like in TF 2) and the other just uses Conv. These are both locally connected networks but for LocallyConnected you also have shared weights for the filters.", "link": "https://www.reddit.com/r/MachineLearning/comments/p0db5p/dare_there_any_papers_directly_comparing_locally/"}, {"autor": "Lunavahid", "date": "2021-05-10 09:10:02", "content": "[-----> Image !!!  Annotation] Is Human in the Loop the New Thing? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/n902mc/image_annotation_is_human_in_the_loop_the_new/"}, {"autor": "ijovab", "date": "2021-05-09 22:19:54", "content": "[D] reading buddies - privacy-preserving learning of biomedical -----> image !!!  data /!/ Will be reading papers extensively around the topics of Differential privacy, Federated Learning, Continual Learning, data management, etc for the next few days. \n\nLooking for a reading buddy to share resources and discuss them with. Ping here or pm if interested. we'd communicate over Reddit, telegram, or discord.", "link": "https://www.reddit.com/r/MachineLearning/comments/n8p415/d_reading_buddies_privacypreserving_learning_of/"}, {"autor": "ijovab", "date": "2021-05-09 22:08:07", "content": "[D] What are some promising areas in privacy-preserving learning in medical data? /!/ So with the EU's new proposal, and general problems related to the usage of medical data the topic seems to be becoming fairly important. I've been reading up on federated learning, continual learning, and differential privacy recently. What do you think are some of the most promising areas to simplify and guarantee the safety of the medical -----> image !!!  data during training?   \n\n\nAny paper suggestions also appreciated.", "link": "https://www.reddit.com/r/MachineLearning/comments/n8ovrb/d_what_are_some_promising_areas_in/"}, {"autor": "kongxianxingren", "date": "2021-05-09 18:41:32", "content": "[D] Are Centroidal Voronoi tessellation and Voronoi tessellation unsupervised learning in machine learning? /!/ The centroidal Voronoi tessellation (CVT) is a special type of Voronoi tessellation in which the generating point of each Voronoi cell is also its centroid (center of mass). It can be viewed as an optimal partition corresponding to an optimal distribution of generators.\n\n&amp;#x200B;\n\nThe Voronoi tessellation is a partition of a plane into regions close to each of a given set of objects.\n\n&amp;#x200B;\n\nThe following is the simple -----> picture !!!  of centroidal Voronoi tessellations which I found in [wiki](https://en.wikipedia.org/wiki/Centroidal_Voronoi_tessellation).\n\n&amp;#x200B;\n\nhttps://preview.redd.it/ircxp2xr35y61.png?width=560&amp;format=png&amp;auto=webp&amp;s=4af24b1912d7f7455e4c03baaa95e57f4a112d03\n\n&amp;#x200B;\n\nFor me, it likes the K-means algorithm and it can be concluded as a clustering method in unsupervised learning. Am I right? The reason I ask is that I didn't see anybody related CVT to the machine learning algorithms.", "link": "https://www.reddit.com/r/MachineLearning/comments/n8ki94/d_are_centroidal_voronoi_tessellation_and_voronoi/"}, {"autor": "rlmouraa", "date": "2021-08-04 15:44:42", "content": "[D] Is it possible to select a specific docker -----> image !!!  based on CUDA drivers installed in the OS? /!/ Suppose the following scenario:\n\nI've trained an image classifier for a specific task. Then I've built a simple web app with this model embedded and plan to send it to a client in as a docker application that should make use of GPUs.\n\nTo do this I've written a *Dockerfile* that starts from a base image, like:\n\n`FROM pytorch/pytorch:?.?.??-cuda??.?-cudnn?-devel`\n\n**Questions**\n\nThe main question is how can I set the correct image?\n\nI mean, to use the GPUs there is a *correct* correspondence between *PyTorch-CUDA drivers-cuDNN lib*. But how can I pick the correct one without having any clue about the client environment where the application will be deployed?\n\nOr is there a way to select the right image on docker build time?\n\nOr, finally, any image should work since the CUDA drivers from the docker container do not relate with the OS drivers? (I'm not sure about this)", "link": "https://www.reddit.com/r/MachineLearning/comments/oxvfis/d_is_it_possible_to_select_a_specific_docker/"}, {"autor": "rlmouraa", "date": "2021-08-04 15:41:22", "content": "[Docker - PyTorch] Is it possible to select a specific docker -----> image !!!  based on CUDA drivers installed in the OS? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/oxvd1f/docker_pytorch_is_it_possible_to_select_a/"}, {"autor": "rshpkamil", "date": "2021-08-04 15:13:51", "content": "[R] Generating Master Faces for Dictionary Attacks with enhanced StyleGAN /!/ \"A master face is a face -----> image !!!  that passes face-based identity-authentication for a large portion of the population. \\[...\\] The results we present demonstrate that it is possible to obtain a high coverage of the population (over 40%) with less than 10 master faces for three leading deep face recognition systems.\"\n\nHowever, Face Id does not seem to be vulnerable. For now ;)\n\nOriginal article: [https://arxiv.org/pdf/2108.01077.pdf](https://arxiv.org/pdf/2108.01077.pdf)", "link": "https://www.reddit.com/r/MachineLearning/comments/oxusc7/r_generating_master_faces_for_dictionary_attacks/"}, {"autor": "toby__bryant", "date": "2021-08-04 13:23:05", "content": "[P] Clean your -----> image !!!  data 35x cheaper using Confident Learning /!/ Hey everyone, Tobias from [hasty.ai](http://hasty.ai/) here.\n\nWe just launched our [Error Finder](https://hasty.ai/annotation/quality-assurance/), a tool automating the data QC process using Confident Learning \u2014 We'd love to hear your thoughts and feedback! \n\n**Aim**\n\nAt [hasty.ai](http://hasty.ai/), we're working on agile ML tooling for vision AI to help our users get to production more reliably. A huge part of this is to automate and speed the data preparation process. \n\nWe were already able to automate 85-95% of the initial labeling process. Now, it's time to tackle the next big issue: quality control for your data. Up to 70% of all work to create a data asset is spent on it, and so far, it has been a manual, highly redundant process.\n\n**Problem**\n\nThe golden standard in this field is [consensus scoring](https://medium.com/hackernoon/how-to-measure-quality-when-training-machine-learning-models-cc9196dd377a). The basic idea is to have multiple annotators labeling the same image. If the annotators did similar annotations, you can be secure in being aligned and providing quality data.\n\nWhereas this gives robust results, it's a lot of redundant manual work blowing up overall project time and costs. \n\n**Solution**\n\nWe automated the process by implementing and further developing ideas on Confident Learning developed by [Northcutt et al. (2019)](https://arxiv.org/abs/1911.00068#:~:text=Confident%20learning%20(CL)%20is%20an,examples%20to%20train%20with%20confidence)\u2014special shoutouts here! \n\nThe result is a cost reduction by 35x compared to doing consensus scoring with an established tool like SageMaker. *Wait what? 35X?* Check out [this blog post](https://medium.com/hasty-ai/the-first-end-to-end-vision-ai-platform-with-truly-automated-data-cleaning-capabilities-ac647be07d13) to see that this is not the typical marketing BS you see but that we can back the number up.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/198u0cl4ecf71.png?width=1950&amp;format=png&amp;auto=webp&amp;s=0bfb6795fa409eac4749278bd8762bcf9388d5d7\n\nIn [the blog post](https://medium.com/hasty-ai/the-first-end-to-end-vision-ai-platform-with-truly-automated-data-cleaning-capabilities-ac647be07d13), we also explain a bit about how the tool works. \n\n**Try it** \n\nThe tool is now live for every Hasty user. You can create an account and try it today\u2014no strings attached. Every new user in Hasty gets $70 worth of credits. It's also possible to import data that you labeled somewhere else. If you have any questions, feedback, or get stuck, feel free to email me at tobias@hasty.ai.", "link": "https://www.reddit.com/r/MachineLearning/comments/oxsl9u/p_clean_your_image_data_35x_cheaper_using/"}, {"autor": "PytonRzeczny", "date": "2021-08-04 09:44:22", "content": "[P]Classifying roads dry/wet, what can I do better? /!/ Hello.\n\nI have a task to create a model to differentiate between dry and wet roads.\n\nI have about 1500 photos, where wet road photos are in minority - 40% of dataset.\n\nIm doing an -----> image !!!  augmentation, flips, rotations etc. I have two models XGBoost and CNN - I\u2019m using pytorch resnet18. XGBoost have 72% acc, I don\u2019t know CNN acc because it\u2019s in training phase.\n\nI want to ask You, what will be your approach to this task? What will You do to get the best possible performance?", "link": "https://www.reddit.com/r/MachineLearning/comments/oxp7a4/pclassifying_roads_drywet_what_can_i_do_better/"}, {"autor": "Yuqing7", "date": "2021-05-06 15:07:01", "content": "[R] Facebook AI Conducts Large-Scale Study on Unsupervised Spatiotemporal Representation Learning /!/ A research team from Facebook AI conducts a large-scale study on unsupervised spatiotemporal representation learning from videos. The work takes a unified perspective on four recent -----> image !!! -based frameworks (MoCo, SimCLR, BYOL, SwAV) and investigates a simple objective that can easily generalize unsupervised representation learning methodologies to space-time.\n\nHere is a quick read: [Facebook AI Conducts Large-Scale Study on Unsupervised Spatiotemporal Representation Learning.](https://syncedreview.com/2021/05/06/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-13/)\n\n The paper *A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning* is on [arXiv](https://arxiv.org/pdf/2104.14558.pdf).", "link": "https://www.reddit.com/r/MachineLearning/comments/n69ew2/r_facebook_ai_conducts_largescale_study_on/"}, {"autor": "YoggieD", "date": "2021-05-05 18:26:11", "content": "Using neural style transfer between an -----> image !!!  and a had sketch to make it look real. [P]", "link": "https://www.reddit.com/r/MachineLearning/comments/n5mvbm/using_neural_style_transfer_between_an_image_and/"}, {"autor": "YoggieD", "date": "2021-05-05 18:23:40", "content": "Using neural style transfer between an -----> image !!!  and a had sketch to make it look real.", "link": "https://www.reddit.com/r/MachineLearning/comments/n5mt7u/using_neural_style_transfer_between_an_image_and/"}, {"autor": "KirillTheMunchKing", "date": "2021-05-05 15:47:07", "content": "[D] StyleGAN2 Distillation for Feed-forward Image Manipulation. How to gender swap Harry-Potter and edit other images explained! /!/ # [StyleGAN2 Distillation for Feed-forward Image Manipulation](https://t.me/casual_gan/34)\n\nIn this paper from October, 2020 the authors propose a pipeline to discover semantic editing directions in StyleGAN in an unsupervised way, gather a paired synthetic dataset using these directions, and use it to train a light Image2Image model that can perform one specific edit (add a smile, change hair color, etc) on any new -----> image !!!  with a single forward pass. If you are not familiar with this paper, check out the [5 minute summary](https://t.me/casual_gan/34).\n\n&amp;#x200B;\n\n[Samples from the model](https://preview.redd.it/3cohkadiobx61.png?width=1280&amp;format=png&amp;auto=webp&amp;s=969a6f2c5e523a4b12d09cc59c5abcc5abaaa6ad)\n\n\\[[Arxiv](https://arxiv.org/abs/2003.03581)\\]\\[[paper explanained in 5 minutes](https://t.me/casual_gan/34)\\]", "link": "https://www.reddit.com/r/MachineLearning/comments/n5j1d2/d_stylegan2_distillation_for_feedforward_image/"}, {"autor": "AeroDEmi", "date": "2021-05-05 14:59:47", "content": "[D] Questions about computer vision, GAN\u2019s /!/ Questions about computer vision\n\nHello everybody I have a couple of questions regarding computer vision as I want to build my first GAN to produce -----> image !!!  with filters or make a background removal. My questions:\n\n\u2022Of I train my model with low-res images can it accept high-res?\n\n\u2022How do I make my model to accept not only square image but like rectangles as some image are horizontal and others vertical?\n\n\u2022If I want my GAN to create almost an identical image but with few changes can I make the generator more shallow? \n\nThank you.", "link": "https://www.reddit.com/r/MachineLearning/comments/n5hx0c/d_questions_about_computer_vision_gans/"}, {"autor": "thedeepreader", "date": "2021-05-05 14:20:43", "content": "[D] (Paper Overview) MLP-Mixer: An all-MLP Architecture for Vision /!/  **Video**\n\n[https://youtu.be/7FHmzEBNzro](https://youtu.be/7FHmzEBNzro)\n\n**Paper**\n\n[https://arxiv.org/abs/2105.01601](https://arxiv.org/abs/2105.01601)\n\nCode  \n(Will be soon available by the authors)\n\n[https://github.com/google-research/vision\\_transformer](https://github.com/google-research/vision_transformer)\n\n**Abstract**\n\nConvolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to -----> image !!!  patches (i.e. \"mixing\" the per-location features), and one with MLPs applied across patches (i.e. \"mixing\" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.", "link": "https://www.reddit.com/r/MachineLearning/comments/n5h1hm/d_paper_overview_mlpmixer_an_allmlp_architecture/"}, {"autor": "IagoInTheLight", "date": "2021-05-13 01:46:19", "content": "Library for plotting -----> image !!!  clusters based on distance/affinity matrix? [D] /!/ Are you familiar with the diagrams where image thumbnails are grouped in 2D automatically based on a distance/affinity matrix? Do you know what they are called? Does a python library for plotting them already exist?\n\n*Thank you!*", "link": "https://www.reddit.com/r/MachineLearning/comments/nb5fhf/library_for_plotting_image_clusters_based_on/"}, {"autor": "IagoInTheLight", "date": "2021-05-13 01:40:48", "content": "-----> Image !!!  cloud plotting in python? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/nb5bna/image_cloud_plotting_in_python/"}, {"autor": "Monskiactual", "date": "2021-05-12 23:03:25", "content": "need help hiring an AI -----> image !!!  recognition programmer /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/nb2282/need_help_hiring_an_ai_image_recognition/"}, {"autor": "Monskiactual", "date": "2021-05-12 23:01:05", "content": "Looking to hire a programmer for AI -----> image !!!  recognition /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/nb20bb/looking_to_hire_a_programmer_for_ai_image/"}, {"autor": "Electrical_Yogurt_26", "date": "2021-05-12 19:02:33", "content": "[Project] Need help with my cnn model! Getting bad accuracy /!/ Sorry, this is my first neural network code, I'm using 1000 images mixed with cats and dogs. I'm trying to build a model to classify whether the -----> image !!!  is a cat or dog but my accuracy keeps increasing or decreasing after every epoch and its usually around 40-60%. I tried different batch size but it's about the same. I tried removing/adding some layers but it didn't work out. Here's my code, thank you very much!\n\nCode\n\nimport sys\nimport os\nimport cv2\nimport random\nimport numpy as np\nimport time\n\nfrom tensorflow.keras.models import Sequential\u00a0\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\nfrom tensorflow.keras.callbacks import TensorBoard\n\nNAME = f'cat-dog-prediction-{int(time.time())}'\n\ntensorboard =TensorBoard(log_dir=f'logs\\\\{NAME}\\\\')\n\nDirectory = r\"C:\\Users\\Downloads\\train\"\n\nCategory = ['cats', 'dogs']\n\n\nIMG_SIZE=(100,100)\n\ndata=[]\n\nos.sys.path\n\nfor category in Category:\n\n\u00a0 folder = os.path.join(Directory, category)\n\n\u00a0 label = Category.index(category)\n\n\u00a0 for img in os.listdir(folder):\n\n\u00a0 \u00a0 img_path=os.path.join(folder,img)\n\n\u00a0 \u00a0 img_arr = cv2.imread(img_path)\n\n\u00a0 \u00a0 img_arr = cv2.resize(img_arr,IMG_SIZE)\n\n\u00a0 \u00a0 data.append([img_arr,label])\n\n\u00a0\nlen(data)\n\nrandom.shuffle(data)\n\nX=[]\n\nY=[]\n\nfor features, labels in data:\n\n\u00a0 X.append(features)\n\n\u00a0 Y.append(labels)\n\nX=np.array(X[1:1000])\n\nY=np.array(Y[1:1000])\n\nrandom.shuffle(data)\n\nX=X/255\n\n\nmodel=Sequential()\n\nmodel.add(Conv2D(64,(3,3),activation='relu'))\n\nmodel.add(MaxPooling2D(2,2))\n\n\nmodel.add(Conv2D(64,(3,3),activation='relu'))\n\nmodel.add(MaxPooling2D(2,2))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(128,input_shape=X.shape[1:],activation='relu'))\n\n\nmodel.add(Dense(128,activation='relu'))\n\nmodel.add(Dense(128,activation='relu'))\n\n\nmodel.add(Dense(2,activation='softmax'))\n\nmodel.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n\nmodel.fit(X,Y,epochs=5,validation_split=0.1,batch_size=32,callbacks=[tensorboard])", "link": "https://www.reddit.com/r/MachineLearning/comments/nawji6/project_need_help_with_my_cnn_model_getting_bad/"}, {"autor": "jikkii", "date": "2021-05-12 15:10:16", "content": "[N] HuggingFace Transformers now extends to computer vision /!/ HuggingFace just released version v4.6.0 of their [huggingface/transformers](https://github.com/huggingface/transformers) framework, with support for three vision transformers: **ViT** by Google, **DeiT** by Facebook Research, and **CLIP** by OpenAI!\n\nThese three architectures can now be loaded from PyTorch and load either original checkpoints contributed by the model authors or any checkpoint uploaded by the community on the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=-----> image !!! -classification), with support for inference widgets like the [-----> image !!!  classification widget for ViT](https://huggingface.co/google/vit-base-patch16-224).\n\nViT and DeiT get state-of-the-art results in text classification, and CLIP can be used for a flurry of tasks including image-text similarity and zero-shot image classification.\n\nSee the release notes for version v4.6.0; ViT and DeiT heavily benefited from Ross Wightman's [timm](https://github.com/rwightman/pytorch-image-models) framework which offers a number of great vision models.\n\nIt is released alongside a few notebooks to play with the models: [Inference with ViT](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Quick_demo_of_HuggingFace_version_of_Vision_Transformer_inference.ipynb) and [Training ViT](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb).", "link": "https://www.reddit.com/r/MachineLearning/comments/naqzis/n_huggingface_transformers_now_extends_to/"}, {"autor": "Rishit-dagli", "date": "2021-05-12 04:10:37", "content": "[P] Low light -----> image !!!  enhancement using ML", "link": "https://www.reddit.com/r/MachineLearning/comments/nag3gj/p_low_light_image_enhancement_using_ml/"}, {"autor": "kidFNSS", "date": "2021-05-12 01:16:30", "content": "[P] Custom Image Data Pipeline Using Tensorflow /!/ While using convolutional nets to perform -----> image !!!  classification, I encountered several difficulties importing -----> image !!!  data using the native Tensorflow functions. So, I created my own custom image data pipeline, which gives the user finer control and more flexibility when importing image data. I hope it is useful to you guys as well. Let me know what you think.\n\nhttps://github.com/nenyehub/tf-image-pipeline", "link": "https://www.reddit.com/r/MachineLearning/comments/nacy0c/p_custom_image_data_pipeline_using_tensorflow/"}, {"autor": "CireNeikual", "date": "2021-05-12 00:04:36", "content": "[P] Real2Sim Interactive Demo with AOgmaNeo /!/ [Link to blog post and interactive demo](https://ogma.ai/2021/05/real2sim-with-aogmaneo/)\n\nThis is a \"real2sim\" demonstration (not to be confused with \"sim2real\") I made using the online-learning biologically-inspired fast learning system I work on called AOgmaNeo.\n\nThe demo is interactive, and can be controlled with WASD. It works by learning a world model for a real life robot arm environment. By associating my gamepad commands with the visuals it was seeing through a -----> camera !!! , it learned a controllable model that I then created a WebAssembly application for. It's far from perfect, but you can notice that it learns some basic physics (especially with the marble, since it always rolled left in the real environment, and does in the simulation as well). It has trouble with \"illegal\" actions that are very out-of-distribution, resulting in some awkward behavior sometimes. To create this demo I controlled the arm for about 5 minutes.\n\nIf you want to know more about how AOgmaNeo works, [here is a guide.](https://github.com/ogmacorp/AOgmaNeo/blob/master/AOgmaNeo_User_Guide.pdf)\n\nLet me know what you think!", "link": "https://www.reddit.com/r/MachineLearning/comments/nabksd/p_real2sim_interactive_demo_with_aogmaneo/"}, {"autor": "aTestCandidate", "date": "2021-10-08 10:34:45", "content": "[R] Ready, Steady, Go AI: A practical tutorial on fundamentals of artificial intelligence and its applications in phenomics -----> image !!!  analysis /!/ Advances in AI technologies have the potential to significantly increase our ability to turn plant phenomics data into valuable insights. However, performing such analyses requires specialized programming skills commonly reserved for computer scientists. We created an interactive tutorial with free, open-source, and FAIR notebooks that can aid researchers to conduct such analyses without the need for an extensive coding experience. We supplemented it with a practical guide on how to implement AI and explainable AI (X-AI) algorithms that augment and complement human experience in classifying tomato leaf diseases and spider mites. Our tutorial is not only applicable to other stresses but also transferable to other plants and research domains, making it possible for researchers from various scientific fields to generate insights into their data. Check out our paper at https://doi.org/10.1016/j.patter.2021.100323", "link": "https://www.reddit.com/r/MachineLearning/comments/q3v2q2/r_ready_steady_go_ai_a_practical_tutorial_on/"}, {"autor": "aTestCandidate", "date": "2021-10-08 10:30:40", "content": "Ready, Steady, Go AI: A practical tutorial on fundamentals of artificial intelligence and its applications in phenomics -----> image !!!  analysis /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/q3v0wg/ready_steady_go_ai_a_practical_tutorial_on/"}, {"autor": "JST99", "date": "2021-06-17 15:06:45", "content": "[R] MLP Singer: Towards Rapid Parallel Korean Singing Voice Synthesis /!/ Hey r/MachineLearning,\n\nI'm excited to introduce \"MLP Singer: Towards Rapid Parallel Korean Singing Voice Synthesis.\" It's K-POP singer in the making (a baby step forward in that direction).\n\n\nPaper: https://arxiv.org/abs/2106.07886 \\\nDemo: https://mlpsinger.github.io \\\nCode: https://github.com/neosapience/mlp-singer \n\n\n**Motivation** \\\nMany singing voice synthesis (SVS) models use an autoregressive design in which acoustic features produced from the previous time step are fed into the model to generate the next set of mel-spectrogram frames. While AR models have their advantages, they are prone to exposure bias and can be time-consuming to train and sample. \n\n**Solution** \\\nWe were inspired by [MLP-Mixer](https://arxiv.org/abs/2105.01601), an architecture exclusively composed of multi-layer perceptrons introduced in the CV literature for efficient -----> image !!!  classification. We experimented with the model and found that MLP-Mixer can also be used in a generative context in the audio domain. Since the Mixer block works with transposed latent features, it gains a receptive field equal to the size of the input chunk. This can be helpful for generating context-aware representations. \n\n**Advantages** \\\nMLP Singer is a non-autoregressive, parallel SVS model. Hence, its generation time is much faster than conventional AR SVS systems. We found that the inference latency of MLP Singer on a CPU is comparable to that of an AR model on GPU. MLP Singer achieved a real-time factor of around 3400 on an NVIDIA RTX 6000 GPU.\n\n**Limitations &amp; Remedies** \\\nSince MLP Singer is a parallel model that generates mel-spectrograms in chunks, audible artifacts can be produced when generated chunks are sequentially put together. To mitigate this issue, we employ an overlapped batch segmentation method that gives the model more frames to look at near frame edges. Detailed explanations and experiment results can be found in the [paper](https://arxiv.org/abs/2106.07886).\n\nLooking forward to feedback and discussion!", "link": "https://www.reddit.com/r/MachineLearning/comments/o1zgjo/r_mlp_singer_towards_rapid_parallel_korean/"}, {"autor": "KirillTheMunchKing", "date": "2021-06-17 13:36:06", "content": "[D] Paper digest - SimSwap: An Efficient Framework For High Fidelity Face Swapping by Renwang Chen et al (reading time ~5 minutes) /!/ **\ud83d\udd11 Keywords:**  \n\\#ACM\\_MM\\_2020 #encoder\\_decoder #face\\_swapping #feature\\_matching #identity\\_transfer\n\n**\ud83c\udfaf At a glance:**  \nFaceSwap apps have been around for ages, hence you might be thinking that swapping the faces of two people is trivial but in reality is far more complicated. The authors from Tencent suggest that the existing approaches are limited in two main ways: they cannot either generalize to arbitrary faces or fail to preserve attributes like facial expression and gaze direction. The proposed method - SimSwap leverages a new ID Injection module and the Weak Feature Matching Loss that aim to solve both of the aforementioned issues.\n\n**\u2b50\ufe0f Complexity**: \ud83c\udf15\ud83c\udf15\ud83c\udf11\ud83c\udf11\ud83c\udf11\n\n**\ud83d\udd0d Main Ideas:**  \n*1) Limitations of the DeepFakes:*  \nDue to the nature of the model (encoder and two identity specific decoders) and the training procedure, the encoder features contain the identity and attribute information of the target face, yet the decoder can convert the target's features to the source identity, which means that the identity information is stored in the decoder's weights. That is why it cannot generalize to an arbitrary person.\n\n*2) ID Injection Module:*  \nSeeking a way to separate the identity information from the decoder's weights the authors propose an ID Injection Module between the encoder and the decoder. This module first extracts an identity vector using a face recognition network and then uses this information to inject the identity information into the encoder features via AdaIN layers. The PatchGAN discriminator is used to improve the quality of the generated images.\n\n*3) Weak Feature Matching Loss:*  \nJust replicating the source identity is not enough to create a face swap. It is also required to keep various attributes such as the expression, position, lightning, etc from the target -----> image !!! . The authors use a variation of the feature matching loss for exactly this reason.  \nThe idea of the feature matching loss originated in Pix2PixHD which used the L1 norm between the discriminator features extracted at multiple layers from the ground truth and the generated images. Since there is no ground truth in the face-swapping task, the authors only use the last few layers of the discriminator for the loss since that is where most of the attribute information is contained.  \nThe overall objective is comprised of identity, adversarial, weak feature matching, and reconstruction losses.\n\n**\ud83d\udcc8Interesting Numbers / Main takeaways:**\n\n* The model was trained on images of size 224x224\n* The qualitative results in the paper blow the baselines out of the water\n\n**\u270f\ufe0fMy Notes:**\n\n* (4/5) for the name, I guess SimSim was already taken, and this was the next best thing\n* IMHO the teaser image is quite poorly chosen, I can barely tell the difference between the target and result images\n* Surprisingly there is no mention of training the model at higher resolutions such as 512x512 or 1024x1024\n* There are examples of video face-swapping that look really neat in the code repository (and a little bit in the appendix), however, they are not discussed in the paper.\n* Have you dabbled with DeepFakes before? Let me know in the comments!\n\n**\ud83d\udd17Links:**  \n[Paper](https://arxiv.org/abs/2106.06340v1) / [Code](https://github.com/neuralchen/SimSwap)\n\n**\ud83d\udc4b If you found this paper explanation useful, consider subscribing to** [my telegram channel](https://t.me/casual_gan) **for early access to deep learning paper digests twice a week!**\n\nHere is a paper poster with some important figures from the paper!\n\n[SimSwap](https://preview.redd.it/4a9r1airwt571.png?width=2074&amp;format=png&amp;auto=webp&amp;s=a3f0563ab25e4b6de9e2eed0d90a5e6673a56441)\n\nBy: [Casual GAN Papers](https://t.me/casual_gan)  \nP.S. Send me paper suggestions for future posts", "link": "https://www.reddit.com/r/MachineLearning/comments/o1xf74/d_paper_digest_simswap_an_efficient_framework_for/"}, {"autor": "aiff22", "date": "2021-06-17 12:33:21", "content": "[N] CVPR Mobile AI Workshop: Presentations from Google, Samsung, Qualcomm, MediaTek, Huawei, Imagination and OPPO - Free &amp; Live on YouTube! /!/ The largest CVPR event on deep learning for edge devices will take place this Sunday. During the workshop, you will see tutorials from all major mobile SoC vendors including Google, Samsung, Qualcomm, MediaTek, Huawei, Imagination Technologies, OPPO and Synaptics telling you how to efficiently deploy machine learning models on edge hardware:\n\n[https://ai-benchmark.com/workshops/mai/2021/](https://ai-benchmark.com/workshops/mai/2021/#schedule)\n\nAn introductory talk from AI Benchmark will additionally provide all basic concepts related to ML inference on smartphones, mobile deep learning libraries and SDKs, acceleration options, edge NPUs and their performance, as well as will show how to run any TensorFlow or PyTorch model on any Android smartphone in less then 5 minutes.\n\nThe event will start at 7am Pacific Time on the 20th of June (2nd CVPR date) and will be streamed live on YouTube for everyone:\n\n[https://ai-benchmark.com/workshops/mai/2021/#live](https://ai-benchmark.com/workshops/mai/2021/#live)\n\nhttps://preview.redd.it/pqr5hmo1mt571.png?width=2094&amp;format=png&amp;auto=webp&amp;s=da380a22c67258ef786912149413553df20330ec\n\n[Workshop schedule](https://ai-benchmark.com/workshops/mai/2021/#schedule) (Pacific Time):\n\n* 07:00 - \\[AI Benchmark\\] Deep Learning on Smartphones, an In-Depth-Dive:  Frameworks and SDKs, Hardware Acceleration with NPUs and GPUs, Models Deployment, Performance and Power Consumption Analysis\n* 08:20 - \\[MediaTek\\] Edge AI Technology \u2013 from Development to Deployment 08:50 - Learned Smartphone ISP Challenge:  Results and Top Solutions\n* 09:10 - \\[Imagination Technologies\\] Imagination Technologies Approach to Overcame the Challenges of Deploying AI in Mobile\n* 09:40 - Smartphone Image Denoising Challenge:  Results and Top Solutions\n* 09:50 - \\[Samsung\\] Samsung Exynos Mobile NPUs and SDK: Hardware Design, Performance, Models Deployment and Efficient Inference\n* 10:40 - \\[Google\\] Android Neural Networks API - What's New and Best Practices\n* 11:10 - Quantized Image Super-Resolution on NPUs Challenge:  Results and Top Solutions\n* 11:30 - \\[Synaptics\\] AI on the Edge at Synaptics : HW and SW Products and Development\n* 12:30 - \\[Huawei\\] AI Deployment from Hardware to Software \u2013 Challenges and Opportunities\n* 13:30 - Video Super-Resolution on Smartphone GPUs Challenge:  Results and Top Solutions\n* 13:15 - \\[OPPO\\] Learning to See the World Clearer\n* 13:40 - Single------> Image !!!  Depth Estimation on Mobile Devices Challenge:  Results and Top Solutions\n* 14:20 - Quantized Camera Scene Detection on Smartphones Challenge:  Results and Top Solutions\n* 14:30 - \\[Qualcomm\\] Hate it or Love it, Your SW Stack Defines Application Performance and Reach", "link": "https://www.reddit.com/r/MachineLearning/comments/o1w5hs/n_cvpr_mobile_ai_workshop_presentations_from/"}, {"autor": "rshpkamil", "date": "2021-06-17 10:09:20", "content": "[R] Full Page Handwriting Recognition via -----> Image !!!  to Sequence Extraction /!/ The authors propose a model that does not require prior segmentation however achieves state-of-the-art accuracy.\n\nOriginal paper here: [https://arxiv.org/pdf/2103.06450.pdf](https://arxiv.org/pdf/2103.06450.pdf)\n\nMore hard-to-find, independent stuff related to AI &amp; Data Science [here](https://thereshape.co/?utm_source=reddit).", "link": "https://www.reddit.com/r/MachineLearning/comments/o1tp4e/r_full_page_handwriting_recognition_via_image_to/"}, {"autor": "minidiable", "date": "2021-06-17 09:24:00", "content": "[D] Taxonomy issue: what is the opposite of \"learning-based methods\" /!/ Hi, I work in robotics, unmanned vehicles etc and I have the following doubt while I am preparing a presentation: how would you call the class of systems which is not using any learning for their functioning?   \n\n\nAn example to understand what I am talking about is computer vision: there are ways to detect edges in an -----> image !!!  which are based on -----> image !!!  moments and stuff like that. Other methods are entirely based on CNNs and received a lot of attention. \n\nWhat I am trying to say in my presentation is: I know that to solve problem X we could use *learning-based methods* but I won't discard looking into and using *\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ methods* if I find that they are more effective.  \n\nI am thinking about the following:   \n\\- **analytical** methods (not sure it is correct)\n\n\\- **formal** methods (maybe too narrow)\n\n\\- **classical** methods (too broad)  \n\n\nSome other ideas?   \n\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/o1t11j/d_taxonomy_issue_what_is_the_opposite_of/"}, {"autor": "WouterVG95", "date": "2021-06-17 08:34:49", "content": "[R] Contrastive Visual Representation Learning Is More Robust Than You Might Think (Paper + Analysis) /!/ Title: \"Revisiting Contrastive Methods for Unsupervised Learning of Visual Representations\"  \nArXiv: [https://arxiv.org/abs/2106.05967](https://arxiv.org/abs/2106.05967)  \nCode: [https://github.com/wvangansbeke/Revisiting-Contrastive-SSL](https://github.com/wvangansbeke/Revisiting-Contrastive-SSL)\n\nMost people assume that current SOTA contrastive self-supervised methods (e.g. MoCo, SimCLR) don't work well on non-curated, domain-specific or long-tailed datasets. This paper discovers interesting properties about the learned representations and disputes claims from recent works.\n\n**Key findings:**\n\n1) **Do we need object-centric pretraining data?** **No.** Recent studies \\[A, B\\] claim that object-centric datasets (e.g. ImageNet) are crucial to learn powerful representations with contrastive self-supervised learning. They argue that the standard cropping (augmentation) strategy is detrimental for non-curated datasets (e.g. COCO, OpenImages).  However, this new paper disputes this claim and shows that training on COCO/OpenImages can even outperform ImageNet pretraining (when compared fairly). Thus, the amount of data is much more important than whether your data is object-centric or not.\n\n2) **Do we need priors to learn dense representations? No.** The cropping (augmentation) strategy already allows the model to learn spatially structured representations. This strategy is simple and often outperforms recent methods which proposed additional (complex) losses at a denser level in the -----> image !!! . The representations can be used for semantic segment retrieval and video instance segmentation without any finetuning. In fact, it enables to find semantic segments from images without annotations! So try to avoid priors and let the data speak for itself.\n\n3) **How can we further boost the transfer performance (w.r.t. MoCo)?** **Impose additional invariances** by exploring different data augmentations and nearest neighbors. This boosts the transfer performance on various tasks. (e.g. semantic segmentation, video instance segmentation, depth prediction). For example, this strategy outperforms MoCov2 on PASCAL VOC by +2.4% mIoU and +4.9% mAP (after 800 epochs of pretraining on COCO).\n\n4) **Is universal pretraining solved? Not yet.**  Models that obtain improvements for the downstream classification tasks (e.g. ImageNet), are not guaranteed to outperform on other tasks as well (e.g. semantic segmentation) and vice versa. So, don't limit yourself to pretraining and finetuning on the same dataset (often ImageNet), this does not paint the full picture. (This is in line with the findings of another recent study.)\n\nTo wrap up: It would be interesting to see if these findings hold up when training on billions of images. Also, what is the influence of inductive biases on these findings? (see the *limitations* section in Appendix G of the paper). This work was done on 2 x V100 GPUs, so it took some time to get these numbers :)\n\n**Let's discuss further:**  \n\\- Will the future of ML be: general pretraining (on billions of images) + domain-specific finetuning? I believe so.  \n\\- It is known that contrastive methods can learn powerful representations on ImageNet. However, I didn't expect it to simply work on the Berkley Deep Drive dataset. This came as a surprise to me. I thought that datasets --from which the classes are \"easy to discriminate\"-- were a must.   \nWhat are your thoughts?\n\n&amp;#x200B;\n\n\\[A\\] Purushwalkam, S and Gupta A., Demystifying contrastive self-supervised learning: Invariances, augmentations and dataset biases, NeurIPS, 2020.   \n\\[B\\] Selvaraju, R.R. et al., Casting your model:  Learning to localize improves self-supervised representation, CVPR, 2021.", "link": "https://www.reddit.com/r/MachineLearning/comments/o1sar2/r_contrastive_visual_representation_learning_is/"}, {"autor": "chiaburr", "date": "2021-06-16 13:57:29", "content": "[D] Analyzing Model Response for Peaks and Values (Optimization?) /!/ Hello everybody :)\n\nI studied biomechanics and I'm kind of new to the world of machine learning. Nevertheless it would be great to get some feedback from some experts, what is possible and what is not possible with ML.\n\nSo: I would like to know if it's somehow possible to detect \"problem spots\" in a model response. And with problem spots I mean the peaks and valleys that can be seen in the attached -----> picture !!! . In this picture, a model response is shown depending on the variation of two model input parameters.Here I would like to know: Are there any kind of tools or algorithms that can be used to find this kind of problem spots in a model response? And if so, what can be recommended? Or literature that I should have a look to?\n\n&amp;#x200B;\n\nMaybe some information about the background: I'm working with high non-linear, explicit finite element simulations (approx. 1 Million Nodes). By generating a lot of data from these FE-simulations, I apply different surrogate modeling techniques to this data to finally predict a model response (scalar responses or vector responses by using for example Gaussian Processes, Neural Networks, etc..). Another option is to first apply a dimensional reduction method (like PCA) and then use a meta modeling approach like regression, neural networks, etc..\n\nAfter all this model reduction steps I would like to further investigate the model response (especially the peaks and valleys) that is predicted by the surrogate model and is shown as a simple example in the attached picture. I hope it's clear what I'm asking for. Feel free to ask and I'm happy about every feedback :)\n\nThanks in advance for your help :)\n\n&amp;#x200B;\n\nKind regards,\n\nchiaburr\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/5aworwfuvm571.png?width=662&amp;format=png&amp;auto=webp&amp;s=3c742f6776d7ec22535d7fa7eaf52abbc8b84064", "link": "https://www.reddit.com/r/MachineLearning/comments/o15i5m/d_analyzing_model_response_for_peaks_and_values/"}, {"autor": "cimager21", "date": "2021-06-16 13:41:39", "content": "[R] Prior -----> image !!! -constrained reconstruction using style-based generative models [ICML 2021] /!/ **Paper:** [https://arxiv.org/pdf/2102.12525.pdf](https://arxiv.org/pdf/2102.12525.pdf)\n\n**Code:** [https://github.com/comp-imaging-sci/pic-recon](https://github.com/comp-imaging-sci/pic-recon)\n\n Obtaining a useful estimate of an object from highly incomplete imaging measurements remains a holy grail of imaging science. Deep learning methods have shown promise in learning object priors or constraints to improve the conditioning of an ill-posed imaging inverse problem. In this study, a framework for estimating an object of interest that is semantically related to a known prior image, is proposed. An optimization problem is formulated in the disentangled latent space of a style-based generative model, and semantically meaningful constraints are imposed using the disentangled latent representation of the prior image. Stable recovery from incomplete measurements with the help of a prior image is theoretically analyzed. Numerical experiments demonstrating the superior performance of our approach as compared to related methods are presented.", "link": "https://www.reddit.com/r/MachineLearning/comments/o155wa/r_prior_imageconstrained_reconstruction_using/"}, {"autor": "Flash92_00", "date": "2021-05-03 11:47:05", "content": "[D] Choosing a pre-trained model for nailbiting classification /!/ Basically trying to apply transfer learning on some -----> image !!!  classification pre-trained model, trained on a dataset of samples classified by whether person is biting their nails or not.  \n\n\nTrying to figure out a good model, thinking about choosing between VVG16, InceptionV3, MobileNet, Resnet, or some of the pose based ones such as PoseNet and Handpose.  \n\n\nConcerns I have are\n\n* If I choose one of the image classification ones, accuracy might take a hit based off of different backgrounds, colors etc\n* If I choose the pose ones, I'm not sure if they're super adopted to hand poses in relation to face position. Since nailbiting is a pretty specific position with different finger orientation and the hand has to be close to the mouth.  \n\n\nAny tips on how I should go about this?", "link": "https://www.reddit.com/r/MachineLearning/comments/n3tznm/d_choosing_a_pretrained_model_for_nailbiting/"}, {"autor": "hou_yz", "date": "2021-05-03 11:21:50", "content": "[R] Visualizing Adapted Knowledge in Domain Transfer /!/ [https://arxiv.org/abs/2104.10602](https://arxiv.org/abs/2104.10602)\n\nThis paper is the first attempt at visualizing what the models learn during domain adaptation. Specifically, it is found that for the source and target networks to makes similar predictions (compensate for their knowledge difference), a target -----> image !!!  is forced to be translated to a completely ***unseen*** source style. Such results also indicate that we can rely on *models* rather than *images* for style transfer. \n\nhttps://preview.redd.it/hg7n3hmt2ww61.png?width=1739&amp;format=png&amp;auto=webp&amp;s=0aeaeb209b41400f420aa78eb7f4405e764eb599", "link": "https://www.reddit.com/r/MachineLearning/comments/n3tllt/r_visualizing_adapted_knowledge_in_domain_transfer/"}, {"autor": "ptoews", "date": "2021-05-03 10:08:29", "content": "[D] CPU choice for machine learning server (Epyc vs. Threadripper) /!/ We are planning on building a rig with 4 RTX 3090 and 128 GB RAM. The application area is computer vision, so preprocessing will most likely be necessary. I've read about DALI which might be useful, but we can't be sure yet.\n\nWe are currently looking at Threadripper vs. Epyc. Are there any benchmarks or experiences on how these two line ups compare in -----> image !!!  preprocessing tasks?\n\nSo far from what I've read Threadrippers have higher clock speed, but run hotter and support less memory capacity and bandwidth, whereas Epyc is the opposite. But how does this translate to a border for applications in ML?\n\nAs a side question, how important is core count for preprocessing? Apparently 2 cores per GPU is recommended, but does it scale after that?", "link": "https://www.reddit.com/r/MachineLearning/comments/n3shza/d_cpu_choice_for_machine_learning_server_epyc_vs/"}, {"autor": "ServantOfChrist7", "date": "2021-05-03 09:27:45", "content": "Suppose I have a 3x3 input -----> image !!!  of 3 channels, How does a convolutional layer convert this 3 channel -----> image !!!  -----> image !!!  into another -----> image !!!  of two channels using 3 kernels? [P] /!/ Suppose I have a 3x3 input image of 3 channels, How does a convolutional   layer convert this 3 channel image image into another image of two   channels using 3 kernels?", "link": "https://www.reddit.com/r/MachineLearning/comments/n3ry0z/suppose_i_have_a_3x3_input_image_of_3_channels/"}, {"autor": "ServantOfChrist7", "date": "2021-05-03 09:27:02", "content": "What are the calculations done in a convolutional layer to convert a three channel input -----> image !!!  into two channel output? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/n3rxnm/what_are_the_calculations_done_in_a_convolutional/"}, {"autor": "ServantOfChrist7", "date": "2021-05-03 09:21:40", "content": "What are the calculations done in a convolutional layer to convert a three channel input -----> image !!!  into two channel output? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/n3rv14/what_are_the_calculations_done_in_a_convolutional/"}, {"autor": "irasciblecomoany56", "date": "2021-05-03 01:16:41", "content": "CNN -----> image !!!  classification /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/n3kmsc/cnn_image_classification/"}, {"autor": "nanermaner", "date": "2021-09-15 19:05:06", "content": "[Discussion] What is your go to technique for labelling data? /!/ Say you have 10,000 -----> image !!!  files on disk, what is your preferred approach for going through and labelling each as \"hot dog\" or \"not hot dog\", for example?\n\nDo you use software to help? Maybe a python script?", "link": "https://www.reddit.com/r/MachineLearning/comments/powmw5/discussion_what_is_your_go_to_technique_for/"}, {"autor": "Mr_P07", "date": "2021-09-24 15:03:06", "content": "[D] [P] tesseract OCR reading from direct video feed /!/ Good day fellow redditors\nI have a question over tesseract OCR functioning, how can I make it work from a direct -----> camera !!!  feed, the issue is it's usually used to recognize text from image I have heard of pytesseract.image_to_string, but don't truly know how it works.", "link": "https://www.reddit.com/r/MachineLearning/comments/pulnlv/d_p_tesseract_ocr_reading_from_direct_video_feed/"}, {"autor": "tobyoup", "date": "2021-09-24 14:47:51", "content": "[Research][Project] Muzic, an open-source research project on AI music, by researchers from Microsoft Research Asia /!/ **Muzic** is a research project on AI music that empowers music understanding and generation with deep learning and artificial intelligence.  Muzic was started by [some researchers](https://www.microsoft.com/en-us/research/project/ai-music/) from [Microsoft Research Asia](https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/). \n\n&amp;#x200B;\n\nMuzic has a logo in both -----> image !!!  and video version:\n\n*Processing img 3b9r6701rgp71...*\n\n*Processing video ktc1byv9rgp71...*\n\n We summarize the scope of our Muzic project in the following figure: \n\n*Processing img u3avdk93rgp71...*\n\n&amp;#x200B;\n\nThe current work in [Muzic](https://www.microsoft.com/en-us/research/project/ai-music/) include:\n\n* Music Understanding\n   * Symbolic Music Understanding: [MusicBERT](https://arxiv.org/pdf/2106.05630.pdf)\n   * Automatic Lyrics Transcription: [PDAugment](https://arxiv.org/pdf/2109.07940.pdf)\n* Music Generation\n   * Song Writing: [SongMASS](https://arxiv.org/pdf/2012.05168.pdf)\n   * Lyric Generation: [DeepRapper](https://arxiv.org/pdf/2107.01875.pdf)\n   * Melody Generation: [TeleMelody](https://arxiv.org/pdf/2109.09617.pdf)\n   * Accompaniment Generation: [PopMAG](https://arxiv.org/pdf/2008.07703.pdf)\n   * Singing Voice Synthesis: [HiFiSinger](https://arxiv.org/pdf/2009.01776.pdf)\n\n&amp;#x200B;\n\nWe initially release the code of 5 research work: [MusicBERT](https://github.com/microsoft/muzic/blob/main/musicbert), [PDAugment](https://github.com/microsoft/muzic/blob/main/pdaugment), [DeepRapper](https://github.com/microsoft/muzic/blob/main/deeprapper), [SongMASS](https://github.com/microsoft/muzic/blob/main/songmass), and [TeleMelody](https://github.com/microsoft/muzic/blob/main/telemelody). We will release the code of more research work in the future.", "link": "https://www.reddit.com/r/MachineLearning/comments/pulcre/researchproject_muzic_an_opensource_research/"}, {"autor": "CorrSurfer", "date": "2021-09-24 12:31:32", "content": "[Research] Training times of modern YOLO-like networks? /!/ In the recent years, there has been a good number of YOLO-type networks for -----> image !!!  segmentation or object detection. The papers and Github repositories on this topic (e.g., [https://github.com/Megvii-BaseDetection/YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)) frequently contain the information how fast inference of the learned network is.\n\nHowever, I'm surprised that none of these papers seem to contain some information on how long it takes to train the respective models from scratch. I find this surprising, because in other domains of computer science, computation times to obtain the described artifact are typically given (along with a quick description of the used CPU/GPU/computer - enough to get an idea of the order of magnitude of the computation task's difficult).\n\nSo for instance computing the weights of the learned YOLOX-s used for the experimental results reported by the authors of YOLOX could take 3 GPU-years, or computing the weights of YOLOX-x could also just take a minute.\n\nSo why are such times not reported? And....does anybody have some training time/CPU/GPU description tuples of reasonably modern YOLO architectures?", "link": "https://www.reddit.com/r/MachineLearning/comments/puiqwf/research_training_times_of_modern_yololike/"}, {"autor": "thedeepreader", "date": "2021-09-24 08:08:32", "content": "[D] (Paper Overview) Pix2Seq: A Language Modeling Framework for Object Detection /!/ **Video**\n\n[https://youtu.be/6Ptp0LfeN6g](https://youtu.be/6Ptp0LfeN6g)\n\n**Paper**\n\n[https://arxiv.org/abs/2109.10852](https://arxiv.org/abs/2109.10852)\n\n**Abstract**\n\nThis paper presents Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we simply cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural net to perceive the -----> image !!!  and generate the desired sequence. Our approach is based mainly on the intuition that if a neural net knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.", "link": "https://www.reddit.com/r/MachineLearning/comments/puf97z/d_paper_overview_pix2seq_a_language_modeling/"}, {"autor": "alexk_wong", "date": "2021-09-22 18:23:12", "content": "[N][R] Want to leverage synthetic data for 3d reconstruction, but don't want to deal with the photometric domain gap? (ICRA 2021 talk) /!/ Want to leverage synthetic data for 3d reconstruction, but don't want to deal with the photometric domain gap?\n\nTLDR: Here is a gif showing an [overview of our approach](https://github.com/alexklwong/learning-topology-synthetic-data/blob/master/figures/scaffnet_fusionnet_overview.gif)\n\nCheck out the [extended version of our ICRA 2021 talk](\nhttps://www.youtube.com/watch?v=zGKH-OKPJD4) for Learning Topology from Synthetic Data for Unsupervised Depth Completion. This is joint work with Safa Cicek and Stefano Soatto at the UCLA Vision Lab.\n\nIn the talk, we will walk you through on how we learn dense topology from sparse geometry e.g. point clouds. This lets us use the abundance of synthetic data, where high quality ground truth comes for free, without having to deal with images and thus bypassing the photometric domain gap.\n\nAfter learning the initial coarse estimate of the scene from just sparse points, we bring the -----> image !!!  back into the picture by performing cross modality fusion. This lets us learn the residual over the initial topology estimate and amend any mistakes, yielding a fast and accurate architecture that achieves the state-of-the-art while using fewer parameters than competing methods.\n\nFor those interested, here are our source code with pretrained mdoels (it is light-weight so it runs on your local machine!) and arxiv version of our paper.\n\npaper: https://arxiv.org/pdf/2106.02994.pdf\ncode + more cool figures: https://github.com/alexklwong/learning-topology-synthetic-data", "link": "https://www.reddit.com/r/MachineLearning/comments/ptdnlu/nr_want_to_leverage_synthetic_data_for_3d/"}, {"autor": "chriskalahiki", "date": "2021-09-22 16:35:09", "content": "[Discussion] [Research] Pre-trained Models for Breast Cancer -----> Image !!!  Classification /!/ Hi all,\n\nI am new to this subreddit, so I apologize in advance if this should go elsewhere. \n\nI am working on a research project with the immediate goal of classifying breast cancer images using a convolutional neural network. I wanted to try and leverage transfer learning to improve results and/or speed up training. I am new to finding pre-trained models to use though. Tensorflow Hub seems to have models that are, for the most part, trained on ImageNet or CIFAR-10. Is there a repository somewhere that would have more useful pre-trained models?", "link": "https://www.reddit.com/r/MachineLearning/comments/ptbczi/discussion_research_pretrained_models_for_breast/"}, {"autor": "cudanexus", "date": "2021-08-02 00:46:04", "content": "[D] think twice and use rm command if not this happens /!/ Since one week I was working on a project related to images. Today I typed the command rm * .jpg to remove all the jps from directory but by mistake I have added a space after * and my full project was vanished. I didn\u2019t even took backup \ud83d\ude22 first time I felt like more powerful than thanos, with one -----> snap !!!  he eliminated 50% but with one space I deleted 100% of my work. Sharing this because some of you guys are using rm command without thinking twice.", "link": "https://www.reddit.com/r/MachineLearning/comments/ow43y9/d_think_twice_and_use_rm_command_if_not_this/"}, {"autor": "jj4646", "date": "2021-08-01 23:08:17", "content": "[D] Convergence of MCMC /!/ I was trying to learn more about the details of algorithms and applications that use \"MCMC\", for e.g. https://en.wikipedia.org/wiki/Monte_Carlo_integration , https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm\n\nhttps://imgur.com/a/MzHoLlw\n\nI think I understand this at a very general level, but I had a few questions:\n\n1) Is the choice of the \"candidate distribution\" (the distribution used for calculating the \"acceptance ratio\") really important? I have seen that the Gaussian distribution often be used for the \"candidate distribution\" - at this point, has this just become the common choice?\n\n2) I was looking at a particular application of integration using MCMC. On a basic level, it seems like you \"save\" the set of points deemed eligible by the \"acceptance ratio\" - and then use these points to calculate the integral.\n\nIf you have a integral \"I\" of function \"F\" - the MCMC approximation of the integral of \"F\" is said to be \"Qn\".\n\nBased on the imgur -----> picture !!! , it seems like you take the average of all points that met the \"acceptance ratio\", and then multiply them by some constant \"V\". Does anyone know how \"V\" is calculated? Does anyone know why the result of the MCMC approximation \"Qn\" is said to be equal to \"V&lt;f&gt;\"?\n\n3) The imgur picture shows the estimation error of the MCMC approximation (Qn). Are there any theoretical results that show \"Qn\" converges to the actual value of the \"integral of the function\" is approximating, as the number of iterations increase? All I can find are some references to the \"Markov Chain Convergence Theorem\" - but I am not sure how this shows that MCMC will converge in probability to some error bound?\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/ow2iu2/d_convergence_of_mcmc/"}, {"autor": "Ideas_Sleep", "date": "2021-08-01 18:29:33", "content": "AI can now detect political ideology with one -----> photo !!! ! [News] (research papers in the description)", "link": "https://www.reddit.com/r/MachineLearning/comments/ovxhj9/ai_can_now_detect_political_ideology_with_one/"}, {"autor": "Slowai", "date": "2021-03-09 15:31:42", "content": "[D] Transfer learning procedures NLP(BERT etc.) vs. CV(RESNET etc.) /!/ Hello wonderful people :)\n\nI was wondering about the differences that occur during transfer learning in NLP vs. CV, specifically the following:\n\nTo my knowledge (even tho last CV stuff I did was 3-4 years ago) if you want to train, say -----> image !!!  classifier using transfer learning, you do something like:\n\n1. Make a downstream layer(s) on top of, say Resnet.\n2. Freeze Resnet. Train only top layer(s).\n3. Unfreeze Resnet. Train the whole model.\n\nAnd it seems to me that in libraries like transformers the procedure is as follows:\n\n1. Make a downstream layer(s) on top of, say, BERT\n2. Train the whole model.\n\nAs I recall from reading some stuff long ago, introducing a new downstream layer with randomly initialized weights introduces noise during backprop, as during first epochs you are effectively transmitting a large amount of noise, if, say some of the partial derivatives (for pre-trained layers) takes into account the random weights.\n\nIs there something in the transformer architecture that prevents this? Or does this cause no significant problems during training? It looks that most of implementations use linear scheduling without warmup, so wouldn't first iterations pose even a bigger threat to your model, as you are sending stronger signals with noise included?\n\nThanks in advance for your replies :\\^)", "link": "https://www.reddit.com/r/MachineLearning/comments/m18u8e/d_transfer_learning_procedures_nlpbert_etc_vs/"}, {"autor": "andrewthehockeyguy9", "date": "2021-03-09 14:38:53", "content": "[Research] Photoshop\u2019s AI neural filters can tweak age and expression with a few clicks /!/ Artificial intelligence is changing the world of -----> image !!!  editing and manipulation, and Adobe doesn\u2019t want to be left behind. Today, the company is releasing an update to Photoshop version 22.0 that comes with a host of AI-powered features, some new, some already shared with the public. These include a sky replacement tool, improved AI edge selection, and \u2014 the star of the show \u2014 a suite of image-editing tools that Adobe calls \u201cneural filters.\u201d\n\nThese filters include a number of simple overlays and effects but also tools that allow for deeper edits, particularly to portraits. With neural filters, Photoshop can adjust a subject\u2019s age and facial expression, amplifying or reducing feelings like \u201cjoy,\u201d \u201csurprise,\u201d or \u201canger\u201d with simple sliders. You can remove someone\u2019s glasses or smooth out their spots. One of the weirder filters even lets you transfer makeup from one person to another. And it\u2019s all done in just a few clicks, with the output easily tweaked or reversed entirely.\n\n\u201cThis is where I feel we can now say that Photoshop is the world\u2019s most advanced AI application,\u201d Maria Yap, Adobe\u2019s vice president of digital imaging told *The Verge*. \u201cWe\u2019re creating things in images that weren\u2019t there before.\u201d\n\nTo achieve these effects, Adobe is harnessing the power of generative adversarial networks \u2014 or GANs \u2014 a type of machine learning technique that\u2019s proved particularly adept at generating visual imagery. Some of the processing is done locally and some in the cloud, depending on the computational demands of each individual tool, but each filter takes just seconds to apply. (The demo we saw was done on an old Mac Book Pro and was perfectly fast enough.)\n\nMany of these filters are familiar to those who follow AI image editing. They\u2019re the sort of tools that have been turning up in papers and demos for years. But it\u2019s always significant when techniques like these go from bleeding-edge experiments, shared on Twitter among those in the know, to headline features in consumer juggernauts like Photoshop.\n\nAs always with these sorts of features, the proof will be in the editing, and the actual utility of neural filters will depend on how Photoshop\u2019s many users react to them. But in a virtual demo *The Verge* saw, the new tools delivered fast and good quality results (though we didn\u2019t see the facial expression adjustment tool). These AI-powered edits weren\u2019t flawless, and most professional retouchers would want to step in and make some adjustments of their own afterwards, but they seemed like they would speed up many editing tasks.", "link": "https://www.reddit.com/r/MachineLearning/comments/m17pkb/research_photoshops_ai_neural_filters_can_tweak/"}, {"autor": "natavk", "date": "2021-04-02 05:46:11", "content": "How to increase -----> image !!!  resolution? [P] /!/ I've been trying an image enhancement tool to increase image resolution. But instead of making the image crispy, it just increased the number of pixels, while keeping the image blurry. Is there any chance to really enhance an image quality?", "link": "https://www.reddit.com/r/MachineLearning/comments/midzgt/how_to_increase_image_resolution_p/"}, {"autor": "black0017", "date": "2021-02-19 17:24:36", "content": "[D] - Medical -----> image !!!  w/ Python: CT lung and vessel segmentation without labels (code included) /!/ Hi there!\n\nI have noticed many young practitioners on medical imaging jump to fancy AI stuff without being aware of when to use it whatsoever.\n\nHowever, if someone wants to learn deep learning its generally a good practice to know (or at least be aware of) what can be done WITHOUT deep learning. \n\nThis post provides an illustration of basic 3D medical imaging processing techniques, by segmenting lungs and vessels WITHOUT LABELS. \n\nI am also providing some solid CT background for people that are not familiar with medical images.\n\nSounds fun? \n\nHere is how the tutorial with fully-documented code that you can run on your browser with google colab:  \nArticle: [https://theaisummer.com/medical-image-python/](https://theaisummer.com/medical-image-python/)  \nColab: [https://colab.research.google.com/drive/1kUOkey3CjWoebA5tVu2oazydFKpJKhrU?usp=sharing](https://colab.research.google.com/drive/1kUOkey3CjWoebA5tVu2oazydFKpJKhrU?usp=sharing)\n\nFor more articles on medical imaging: [https://theaisummer.com/topics/medical/](https://theaisummer.com/topics/medical/)\n\nHave a great weekend,\n\nN.", "link": "https://www.reddit.com/r/MachineLearning/comments/lnlcm5/d_medical_image_w_python_ct_lung_and_vessel/"}, {"autor": "4rtemi5", "date": "2021-02-19 13:06:52", "content": "[P] IMAX: -----> image !!!  augmentation library for Jax /!/ Made an -----> image !!!  augmentation library in Jax that is able to do 3D transforms and many color transforms present in Pillow and even has a randaugment function. Best thing is that it's jitable and fast. Happy about any kind of feedback. [github](https://github.com/4rtemi5/imax)", "link": "https://www.reddit.com/r/MachineLearning/comments/lnff5e/p_imax_image_augmentation_library_for_jax/"}, {"autor": "4rtemi5", "date": "2021-02-19 13:04:18", "content": "IMAX: -----> Image !!!  augmentation library for Jax /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/lnfdji/imax_image_augmentation_library_for_jax/"}, {"autor": "beatleworld01", "date": "2021-02-22 07:09:41", "content": "What\u2019s the state-of-art CNN or NN architectures for -----> image !!!  classification or object detection? /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/lphr3w/whats_the_stateofart_cnn_or_nn_architectures_for/"}, {"autor": "seek_it", "date": "2021-02-22 06:37:19", "content": "[D] GAN Paper/code for background completion? /!/ Hey all,\n\nI've been searching for GANs papers/code for implementing a method for -----> image !!!  background completion but could not come across any good paper/code.\n\nProblem: In an image, there can be some distortion in the background  or around half of the background is different from the rest. Our GAN model should fix the background or complete the uncompleted background w.r.t. image size. Background distortion can be of any type so, this validates that solutions is achievable using GANs only. \n\nAre there any GANs paper/code to which I can look after?\n\nThanks", "link": "https://www.reddit.com/r/MachineLearning/comments/lph7u2/d_gan_papercode_for_background_completion/"}, {"autor": "temporaryEnthalpic", "date": "2021-04-25 20:09:31", "content": "Creating a dataset representing human morality [P] /!/  I know the title sounds like it is a failed project from the start, but bear with me here.\n\nMachine learning (or AI) is inevitably going to get more and more control on our life, this post isn't to argue whether or not this is good, just that it is happening.\n\nThere is a clear lack of dataset representing human morality, because (imo) it is more complex than just having a dataset of situation and labeling each of those with a binary good or bad (e.g 0 or 1 as your training label) BUT, I thought this too with, human -----> image !!!  recognition and later with human natural language processing and I got proven wrong.\n\nSo here is my idea since machine learning seems to work very well at... everything:\n\ncreating a dataset that would represent globaly the human morality. I am only able to code in python and javascript at the moment and have never worked with big dataset nor with SQL so I doubt I will be able to create this dataset (alone) soon enough.\n\nIf this idea interests you but you don't believe it would represent what is really \"good\", please start this project and make it open source like all good machine learning project.\n\nIf you want to help me create it, knowning my limitation in programming and huge datasets, contact me.\n\nOn that note, thanks to the whole machine learning community for being so active right now.", "link": "https://www.reddit.com/r/MachineLearning/comments/mygg6w/creating_a_dataset_representing_human_morality_p/"}, {"autor": "malik-sahabb", "date": "2021-04-25 18:19:50", "content": "Convert a low res -----> image !!!  to a high res -----> image !!! . /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/mye3ug/convert_a_low_res_image_to_a_high_res_image/"}, {"autor": "Dipper1702", "date": "2021-04-22 17:55:02", "content": "[P] Help with SegCaps /!/ Hello everyone, I'm trying to use Capsule Networks for segmentation but  it's not working. While my results should be in the shape of a circle,  I'm getting a squared shaped answer. Should I increase the number of  epochs or steps per epoch? Do you guys have any ideas?\n\n[wrong -----> image !!!  result](https://preview.redd.it/jj2o0bmyjru61.png?width=256&amp;format=png&amp;auto=webp&amp;s=a09055ac59751edaee8aa3d1857ecf94348fcb65)", "link": "https://www.reddit.com/r/MachineLearning/comments/mwak5g/p_help_with_segcaps/"}, {"autor": "thanrl", "date": "2021-05-18 00:01:19", "content": "[D] Loss Function in Generative Models /!/ Say I have :\n\n* training data x (e.g. text, -----> image !!! )\n* a parameterized model m(\u03b8) that generates those kind of data (e.g. an RNN)\n\nUsually such model is trained with a loss function such as BCELoss(m(\u03b8), x), for good reasons. However, has any one done XXXLoss(f(m(\u03b8)), f(x))? \n\nThat is, what if I am interested in generating images that have a similar saturation level (here f(image) would output the image's saturation as a scaler) or generating sentences that have a similar anger level (here f(sentence) would output the sentence's anger level as a scaler), assuming we know function f() as some differentiable blackbox? Would greatly appreciate some reference of some paper doing this, or on why this is hard to do. Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/nevtz1/d_loss_function_in_generative_models/"}, {"autor": "evernox666", "date": "2021-05-17 17:43:46", "content": "A doubt in classification task( microscopic -----> image !!!  dataset) /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/nemqg6/a_doubt_in_classification_task_microscopic_image/"}, {"autor": "Vexac6", "date": "2021-05-17 11:30:55", "content": "[P] Magic Cards classifier in streaming /!/ Hi guys, first post here and also first serious project with ML :)\n\nMy master degree's thesis consists in a tool that detects and classifies *Magic: the Gathering* cards during a tabletop streaming (no online games). MtG official tournaments have some streaming procedures to follow, like a top-view -----> camera !!! , card positioning on the table, a certain lighting etc that can help the machine a lot by avoiding harsh conditions.\n\n**Main Issue:** There are around **20.000** different cards, and some of them have more than 1 image representing em (alternative artworks or frames). Is classification with that number of classes even doable in a reasonable training time?\n\n**How to react:** I could accept training on a smaller subset (like maybe the newest cards), but I would like to create something useful, and not just a demonstrative project. So for now I'm adopting a predictive pipeline to reduce the problem complexity and dispatch the classification to one of many networks trained on a specific different subsets of cards. This should be the standard for face recognition, so maybe it's the right answer. With that, every network should work \"only\" on \\~1000 classes (with **very few datapoints per class**, sadly), and I don't know if it's the right path to follow.\n\nDo you think there's a better approach to this kind of problem?\n\nP.S. Macro-features are recognized with a \\~99% accuracy, so they're not a problem at all.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/3dzrzfup0oz61.png?width=1862&amp;format=png&amp;auto=webp&amp;s=89b3adf4539baca8fbfa54af62233b78a572c6ab", "link": "https://www.reddit.com/r/MachineLearning/comments/nedrar/p_magic_cards_classifier_in_streaming/"}, {"autor": "aselsiriwardena", "date": "2021-04-16 10:27:35", "content": "[P] Streamlit - TypeError: a bytes-like object is required, not 'Tensor' /!/ I am working on a style transfer task, my model returns a tensor. recently I was saving that -----> image !!!  using torchvision.utils\n\n`torchvision.utils.save_-----> image !!! (genarated_-----> image !!! , result_path)`\n\nnow I have passed the same -----> image !!!  to streamlit.\n\n    def image_input():\n    content_file = st.sidebar.file_uploader(\"Choose a Content Image\", type=[\"png\", \"jpg\", \"jpeg\"])\n    if content_file is not None:\n        content = Image.open(content_file)\n        content = np.array(content)  # pil to cv\n        content = cv2.cvtColor(content, cv2.COLOR_RGB2BGR)\n    else:\n        st.warning(\"Upload an Image OR Untick the Upload Button)\")\n        st.stop()\n    \n    WIDTH = st.sidebar.select_slider('QUALITY (May reduce the speed)', list(range(150, 501, 50)), value=200)\n    content = imutils.resize(content, width=WIDTH)\n    generated = genarate_image(content)\n    st.sidebar.image(content, width=300, channels='BGR')\n    st.image(generated, channels='BGR', clamp=True)\n\nBut now streamlit giving me this error.\n\n`TypeError: a bytes-like object is required, not 'Tensor'`\n\nis there a way to convert tensor into a \"bytes-like object\" ?", "link": "https://www.reddit.com/r/MachineLearning/comments/ms0cjw/p_streamlit_typeerror_a_byteslike_object_is/"}, {"autor": "MusicalCakehole", "date": "2021-01-18 12:14:16", "content": "[Discussion] Safety concerns on personal data in phone galleries using computer vision techniques /!/ This feature has been out there for quite a while now, with almost every phone of any brand, where the phone galleries have -----> image !!!  classifiers that would group -----> image !!! s together based on objects, people, etc. What really bothers me is the OCR being used in this. For eg, if I have an image of a bill, searching the name of the store would bring that image up.\n\nIf I have a screenshot from a chat, it would show up on searching some keywords from it. I find this potentially dangerous because I may have some important screenshots and doc images, so would many people I imagine. This is also with services like google drive, one drive and others of sort.\n\nDoes anyone have any idea if this data is being used/sent anywhere? Or is it just an engine in my phone to help me organize my phone gallery? What are your thoughts or what do you know about this?\n\nPS - Please direct me to the correct subreddit to ask this question in case it is inappropriate over here.", "link": "https://www.reddit.com/r/MachineLearning/comments/kztjol/discussion_safety_concerns_on_personal_data_in/"}, {"autor": "Wiskkey", "date": "2021-01-18 09:08:06", "content": "[P] The Big Sleep: Text-to------> image !!!  generation using BigGAN and OpenAI's CLIP via a Google Colab notebook from Twitter user Adverb /!/ From [https://twitter.com/advadnoun/status/1351038053033406468](https://twitter.com/advadnoun/status/1351038053033406468):\n\n&gt;The Big Sleep    \n&gt;  \n&gt;Here's the notebook for generating images by using CLIP to guide BigGAN.    \n&gt;  \n&gt;It's very much unstable and a prototype, but it's also a fair place to start. I'll likely update it as time goes on.   \n&gt;  \n&gt;[colab.research.google.com/drive/1NCceX2mbiKOSlAd\\_o7IU7nA9UskKN5WR?usp=sharing](https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR?usp=sharing)\n\nChange the text in the above notebook in the Parameters section to your desired text.\n\nThis system is non-deterministic. In other words, different runs using the same inputs can result in different outputs.\n\n[Reddit post about CLIP.](https://www.reddit.com/r/MachineLearning/comments/kr7bp9/r_clip_connecting_text_and_images_from_openai/) \n\nExample using text \"a black cat sleeping on top of a red clock\":\n\nhttps://preview.redd.it/7xq58v7022c61.png?width=512&amp;format=png&amp;auto=webp&amp;s=a229ae9add555cd1caba31c42b60d907ffe67773\n\nExample using text \"an owl that glows in the dark\":\n\nhttps://preview.redd.it/xx1u1qx922c61.png?width=512&amp;format=png&amp;auto=webp&amp;s=919949246897125517d975aa7ce4a87db3e28ca5\n\nMore examples can be found at [https://twitter.com/advadnoun/](https://twitter.com/advadnoun/).\n\nA related project from the same person: [A Colab notebook from Ryan Murdock that creates an image from a given text description using SIREN and OpenAI'S CLIP](https://www.reddit.com/r/MachineLearning/comments/ky8fq8/p_a_colab_notebook_from_ryan_murdock_that_creates/).\n\nI am not affiliated with this project or its developer.", "link": "https://www.reddit.com/r/MachineLearning/comments/kzr4mg/p_the_big_sleep_texttoimage_generation_using/"}, {"autor": "TheRealMarqupe", "date": "2021-05-09 12:20:51", "content": "CNNs Color Invariance [Discussion] /!/ We want to detect a specific object that can have any color, for example, a car. If we train our model with images containing only black and gray cars, will the performance of our model's predictions be worse for images containing cars with different colors than the ones used on training? For example, will our model fail to classify correctly if a car is present or not on an -----> image !!! , for -----> image !!! s containing yellow cars?\u00a0 \u00a0\n\nIf so, what is the best way to achieve color invariance? (explanations with examples and credible sources references would be much appreciated)\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/n8d4tt/cnns_color_invariance_discussion/"}, {"autor": "migill-66", "date": "2021-05-09 06:44:55", "content": "oral thin -----> film !!! ", "link": "https://www.reddit.com/r/MachineLearning/comments/n88jj0/oral_thin_film/"}, {"autor": "spauldeagle", "date": "2021-05-11 21:04:24", "content": "[D] Any recommendations for -----> image !!!  annotation software? /!/ I've been using Supervisely Enterprise at work and it's been going really really well, but my work has been paying for it. Meanwhile, my grad research work is requiring a large-scale annotation effort and my masters' advisor is queasy about forking over grant money just to annotate. We can't use the community version because HIPAA. I've been looking at other free annotation software and I'm trying to make a decision on which to use. Any suggestions on what has worked for you?", "link": "https://www.reddit.com/r/MachineLearning/comments/na7oo3/d_any_recommendations_for_image_annotation/"}, {"autor": "techsucker", "date": "2021-09-18 07:08:51", "content": "[R] Google AI Introduces Two New Families of Neural Networks Called \u2018EfficientNetV2\u2019 and \u2018CoAtNet\u2019 For Image Recognition /!/ Training efficiency has become a significant factor for deep learning as the neural network models, and training data size grows. [GPT-3](https://arxiv.org/abs/2005.14165) is an excellent example to show how critical training efficiency factor could be as it takes weeks of training with thousands of GPUs to demonstrate remarkable capabilities in few-shot learning.\n\nTo address this problem, the Google AI team introduce two families of neural networks for -----> image !!!  recognition. First is\u00a0[EfficientNetV2](https://arxiv.org/abs/2104.00298), consisting of CNN (Convolutional neural networks) with a small-scale dataset for faster training efficiency such as\u00a0[ImageNet1k](https://www.image-net.org/)\u00a0(with 1.28 million images). Second is a hybrid model called\u00a0[CoAtNet](https://arxiv.org/abs/2106.04803), which combines\u00a0[convolution](https://en.wikipedia.org/wiki/Convolution)\u00a0and\u00a0[self-attention](https://en.wikipedia.org/wiki/Self-attention)\u00a0to achieve higher accuracy on large-scale datasets such as\u00a0[ImageNet21](https://www.image-net.org/)\u00a0(with 13 million images) and\u00a0[JFT](https://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html)\u00a0(with billions of images). As per the research report by Google,\u00a0[EfficientNetV2](https://arxiv.org/abs/2104.00298)\u00a0and\u00a0[CoAtNet](https://arxiv.org/abs/2106.04803)\u00a0both are 4 to 10 times faster while achieving state-of-the-art and 90.88% top-1 accuracy on the well-established\u00a0[ImageNet](https://www.image-net.org/)\u00a0dataset.\n\n# [7 Min Read](https://www.marktechpost.com/2021/09/17/google-ai-introduces-two-new-families-of-neural-networks-called-efficientnetv2-and-coatnet-for-image-recognition/) | [Paper (CoAtNet)](https://arxiv.org/abs/2106.04803) | [Paper (EfficientNetV2)](https://arxiv.org/abs/2104.00298) | [Google blog](https://ai.googleblog.com/2021/09/toward-fast-and-accurate-neural.html) | [Code](https://github.com/google/automl/tree/master/efficientnetv2)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/ipmkyt7eo7o71.png?width=1392&amp;format=png&amp;auto=webp&amp;s=22764f4268a6c12acb85b8b71a7331cc6446d984", "link": "https://www.reddit.com/r/MachineLearning/comments/pqhqjv/r_google_ai_introduces_two_new_families_of_neural/"}, {"autor": "KirillTheMunchKing", "date": "2021-09-18 00:40:04", "content": "[D] CLIP Paper Explained - Learning Transferable Visual Models From Natural Language Supervision (5-Minute Summary) /!/ I have mentioned CLIP so many times in my posts that you might think I am being paid to promote it. Unfortunately, I am not, but a lot of my favorite projects use CLIP, and it is time to finally get into the nitty-gritty of the powerhouse that is CLIP. CLIP is model from 2020 that is inspired by ideas from Alec Radford, Jong Wook Kim, and the good folks at OpenAI.\n\n[CLIP Architecture](https://preview.redd.it/g4y5ojb4r5o71.png?width=2162&amp;format=png&amp;auto=webp&amp;s=dc2a1e23d7bbce45b1403c9bf2ea2d31992bc8ad)\n\nCheck out the [full paper summary](https://www.casualganpapers.com/zero-shot-contrastive-loss------> image !!! -text-pretraining/CLIP-explained.html) on Casual GAN Papers (Reading time \\~5 minutes).\n\nSubscribe to [my channel](https://t.me/casual_gan) and follow me on [Twitter](https://twitter.com/KirillDemochkin) for weekly AI paper summaries!", "link": "https://www.reddit.com/r/MachineLearning/comments/pqcey1/d_clip_paper_explained_learning_transferable/"}, {"autor": "loziomario", "date": "2021-01-07 21:38:25", "content": "How to see what my -----> camera !!!  sees while I'm using ubuntu 18.04 installed on the nano within the nomachine client. /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/ksngmq/how_to_see_what_my_camera_sees_while_im_using/"}, {"autor": "Successful-Forever12", "date": "2021-05-26 16:25:27", "content": "[P] Need help deploying Teachable Machine model to TFLite Micro on Arduino Nano BLE 33 /!/ Hi all, \n\nI'm trying to use the [Person Detection TFLite Micro](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/person_detection) example project as a template and insert my own model that I've created using [TeachableMachine.](https://teachablemachine.withgoogle.com/) However, it's not working as expected, and giving a \"Invoke Failed()\" error in the serial monitor when I run the script on my Nano 33 BLE. Can someone please take a look and tell me what I did wrong?\n\nHere is how I approached it:\n\n1) Create a model with [Teachable Machine](https://teachablemachine.withgoogle.com/)\n\n2) Download my model as a keras file\n\n3) Convert from Keras model to tflite model, and then integer quantize the model using the code here: [https://colab.research.google.com/drive/12O9qO6bAI72B0RTt88sQPkcHkC16Mb8O#scrollTo=cKTbVvb2Vsyo](https://colab.research.google.com/drive/12O9qO6bAI72B0RTt88sQPkcHkC16Mb8O#scrollTo=cKTbVvb2Vsyo)\n\n4) Convert from tflite to .cc using : `xxd -i converted_model.tflite &gt; model_data.cc`\n\n5) Replace value of g\\_person\\_detect\\_model\\_data\\_len with new value from my model\\_data.cc\n\n6) Replace model\\_data with new model data from my model\\_data.cc\n\n7) Use [https://netron.app/](https://netron.app/) to visualize the network and see what MicroOpsResolvers need to be included. Include as necessary.\n\nThe only two files in the project that I touch at all are person\\_detection.ino and person\\_detection\\_model\\_data.cpp. Please help me figure out why it's not working!!!\n\nHere is my person\\_detection.ino file for you to look at:\n\n    /* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n    \n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n    \n        http://www.apache.org/licenses/LICENSE-2.0\n    \n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n    ==============================================================================*/\n    \n    #include &lt;TensorFlowLite.h&gt;\n    \n    #include \"main_functions.h\"\n    \n    #include \"detection_responder.h\"\n    #include \"image_provider.h\"\n    #include \"model_settings.h\"\n    #include \"person_detect_model_data.h\"\n    #include \"tensorflow/lite/micro/micro_error_reporter.h\"\n    #include \"tensorflow/lite/micro/micro_interpreter.h\"\n    #include \"tensorflow/lite/micro/micro_mutable_op_resolver.h\"\n    #include \"tensorflow/lite/schema/schema_generated.h\"\n    #include \"tensorflow/lite/version.h\"\n    \n    // Globals, used for compatibility with Arduino-style sketches.\n    namespace {\n    tflite::ErrorReporter* error_reporter = nullptr;\n    const tflite::Model* model = nullptr;\n    tflite::MicroInterpreter* interpreter = nullptr;\n    TfLiteTensor* input = nullptr;\n    \n    // In order to use optimized tensorflow lite kernels, a signed int8_t quantized\n    // model is preferred over the legacy unsigned model format. This means that\n    // throughout this project, input images must be converted from unisgned to\n    // signed format. The easiest and quickest way to convert from unsigned to\n    // signed 8-bit integers is to subtract 128 from the unsigned value to get a\n    // signed value.\n    \n    // An area of memory to use for input, output, and intermediate arrays.\n    constexpr int kTensorArenaSize = 136 * 1024;\n    static uint8_t tensor_arena[kTensorArenaSize];\n    }  // namespace\n    \n    // The name of this function is important for Arduino compatibility.\n    void setup() {\n      // Set up logging. Google style is to avoid globals or statics because of\n      // lifetime uncertainty, but since this has a trivial destructor it's okay.\n      // NOLINTNEXTLINE(runtime-global-variables)\n      static tflite::MicroErrorReporter micro_error_reporter;\n      error_reporter = &amp;micro_error_reporter;\n    \n      // Map the model into a usable data structure. This doesn't involve any\n      // copying or parsing, it's a very lightweight operation.\n      model = tflite::GetModel(g_person_detect_model_data);\n      if (model-&gt;version() != TFLITE_SCHEMA_VERSION) {\n        TF_LITE_REPORT_ERROR(error_reporter,\n                             \"Model provided is schema version %d not equal \"\n                             \"to supported version %d.\",\n                             model-&gt;version(), TFLITE_SCHEMA_VERSION);\n        return;\n      }\n    \n      // Pull in only the operation implementations we need.\n      // This relies on a complete list of all the ops needed by this graph.\n      // An easier approach is to just use the AllOpsResolver, but this will\n      // incur some penalty in code space for op implementations that are not\n      // needed by this graph.\n      //\n      // tflite::AllOpsResolver resolver;\n      // NOLINTNEXTLINE(runtime-global-variables)\n      static tflite::MicroMutableOpResolver&lt;10&gt; micro_op_resolver;\n    \n      micro_op_resolver.AddPad();\n      micro_op_resolver.AddConv2D();\n      micro_op_resolver.AddDepthwiseConv2D();\n      micro_op_resolver.AddSoftmax();\n      micro_op_resolver.AddRelu6();\n      micro_op_resolver.AddRelu();\n      micro_op_resolver.AddAdd();\n      micro_op_resolver.AddMean();\n      micro_op_resolver.AddFullyConnected();\n      micro_op_resolver.AddQuantize();\n    \n      // Build an interpreter to run the model with.\n      // NOLINTNEXTLINE(runtime-global-variables)\n      static tflite::MicroInterpreter static_interpreter(\n          model, micro_op_resolver, tensor_arena, kTensorArenaSize, error_reporter);\n      interpreter = &amp;static_interpreter;\n    \n      // Allocate memory from the tensor_arena for the model's tensors.\n      TfLiteStatus allocate_status = interpreter-&gt;AllocateTensors();\n      if (allocate_status != kTfLiteOk) {\n        TF_LITE_REPORT_ERROR(error_reporter, \"AllocateTensors() failed\");\n        return;\n      }\n    \n      // Get information about the memory area to use for the model's input.\n      input = interpreter-&gt;input(0);\n    }\n    \n    // The name of this function is important for Arduino compatibility.\n    void loop() {\n      // Get -----> image !!!  from provider.\n      if (kTfLiteOk != GetImage(error_reporter, kNumCols, kNumRows, kNumChannels,\n                                input-&gt;data.int8)) {\n        TF_LITE_REPORT_ERROR(error_reporter, \"Image capture failed.\");\n      }\n    \n      // Run the model on this input and make sure it succeeds.\n      if (kTfLiteOk != interpreter-&gt;Invoke()) {\n        TF_LITE_REPORT_ERROR(error_reporter, \"Invoke failed.\");\n      }\n    \n      TfLiteTensor* output = interpreter-&gt;output(0);\n    \n      // Process the inference results.\n      int8_t person_score = output-&gt;data.uint8[kPersonIndex];\n      int8_t no_person_score = output-&gt;data.uint8[kNotAPersonIndex];\n      RespondToDetection(error_reporter, person_score, no_person_score);\n    }", "link": "https://www.reddit.com/r/MachineLearning/comments/nll1ey/p_need_help_deploying_teachable_machine_model_to/"}, {"autor": "chansung18", "date": "2021-05-26 13:36:04", "content": "MLOps big -----> picture !!!  in GCP /!/ [removed]", "link": "https://www.reddit.com/r/MachineLearning/comments/nlh7ha/mlops_big_picture_in_gcp/"}, {"autor": "the_scientist-7367", "date": "2021-06-23 10:29:33", "content": "DermaChecker - A website to detect skin cancer using deep neural nets, Computer vision (ResNet 101) /!/  After months of building, training, studying and researching, I have finally come to the point where I am proud of the work I have done. I always wanted to do something for the medical industry and this journey was truly an experience that I will never forget.  \n\n\nI bring to you my AI powered healthcare application DermaChecker, an algorithmic skin care specialist that classifies skin images sent by users based on its cancerous state.  \nHere is how it works. The user sends the -----> image !!!  of the skin mole. DermaChecker's algorithm provides an assessment back to the user within seconds from uploading.  \nSkin cancer cases are growing at an alarming rate around the world. Please visit [https://www.dermachecker.com](https://www.dermachecker.com)  and receive a free assessment now! Until next time. Cheers!  \n\n\nP.S Don't let the smallest of ideas fly away. Execute them. You never know what they could turn into. \n\n&amp;#x200B;\n\n*Processing video 0illfodysz671...*", "link": "https://www.reddit.com/r/MachineLearning/comments/o6a446/dermachecker_a_website_to_detect_skin_cancer/"}, {"autor": "Pseudoabdul", "date": "2021-06-23 10:28:16", "content": "Is it possible for a model to increase overfitting when seeing new training examples for the first time? [D] /!/ So I was running a CNN over a large -----> image !!!  dataset (94k -----> image !!! s) and I was concerned about overfitting, so I implemented early stopping. But if I set 1 epoch to be all the training data, I found it overfit a lot before early stopping had a chance to stop it. So I reduced the epoch steps to about 10k so it would check after each time. I found that even after 3 epochs, the early stopping trigger when it measured that the validation MSE was increasing(even with a patience of 2 epochs).\n\nBut 3 epochs is only 30k samples, so the model hasn't even seen all the data points.  It's being given new data points and some how the validation MSE is increasing, even as the training MSE is decreasing. \n\nIf I were training over the same training data many times, I understand why the model would overfit, but I don't see how giving it new training examples is reducing its ability to generalize. My guess is one of two things is happening:\n\n1. The model overfitting isn't increasing, and it just got lucky on the first run somehow. \n\n2. The model is overfitting due to some other purposes. \n\nThe validation set I'm using is the same each epoch, so that shouldn't be an issue. Has anyone experienced this before?", "link": "https://www.reddit.com/r/MachineLearning/comments/o6a3lz/is_it_possible_for_a_model_to_increase/"}, {"autor": "the_scientist-7367", "date": "2021-06-23 10:09:04", "content": "DermaChecker - A website to detect skin cancer using deep neural nets [Computer vision - (ResNet 101)] /!/  After months of building, training, studying and researching, I have finally come to the point where I am proud of the work I have done. I always wanted to do something for the medical industry and this journey was truly an experience that I will never forget.  \nI bring to you my AI powered healthcare application DermaChecker, an algorithmic skin care specialist that classifies skin images sent by users based on its cancerous state.  \nHere is how it works. The user sends the -----> image !!!  of the skin mole to our website. DermaChecker's algorithm provides an assessment back to the user within seconds.  \nSkin cancer cases are growing at an alarming rate around the world. Please visit [https://www.dermachecker](https://www.dermachecker)  and receive a free assessment now! Until next time. Cheers!  \n\n\nP.S Don't let the smallest of ideas fly away. Execute them. You never know what they could turn into. \n\n&amp;#x200B;\n\n*Processing video m876vjk7pz671...*", "link": "https://www.reddit.com/r/MachineLearning/comments/o69uwq/dermachecker_a_website_to_detect_skin_cancer/"}], "name": "Subreddit_MachineLearning_01_01_2021-01_11_2021"}
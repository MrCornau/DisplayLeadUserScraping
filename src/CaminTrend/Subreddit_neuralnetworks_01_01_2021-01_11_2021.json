{"interestingcomments": [{"autor": "binaryfor", "date": "2021-03-29 15:18:26", "content": "deep-daze: Simple command line tool for text to -----> image !!!  generation using OpenAI's CLIP and Siren (Implicit neural representation network)", "link": "https://www.reddit.com/r/neuralnetworks/comments/mfrtrj/deepdaze_simple_command_line_tool_for_text_to/"}, {"autor": "dorienh", "date": "2021-03-29 12:25:46", "content": "Looking for out of the box notebook for high-resolution style transfer in the style of 1 -----> image !!!  /!/ Hi community, \n\nI am looking for a python notebook with out-of-the-box style transfer algo implemented. I found some good ones, but they are all pretty low resolution. Some things I am looking for: \n\n\\- high resolution output\n\n\\- style transfer according to 1 uploaded image\n\n\\- open source code or software", "link": "https://www.reddit.com/r/neuralnetworks/comments/mfog1q/looking_for_out_of_the_box_notebook_for/"}, {"autor": "Eildosa", "date": "2021-03-03 18:04:10", "content": "General question about neural networks /!/ Hi,\n\nI'm a python dev, but I work with website APIs and flask websites or databases, today I was following this tutorial that use tensorflow 1.7, beside the fact that I don't even understand what is tensorflow and how it relates to neural networks.  \n\n\nit says things like :  normalize gradients, normalize the -----> image !!! , use tensorflow to get the mathematical function for the gradient of the given layer-tensor. loss function.  \n\n\nWhat does normalize mean? what is a gradient? what is a loss function? what is a layer tensor? I looked at other neural network tutorial and they did not speak about such things.\n\nThis was a tutorial to do a deep dream image.  \n\n\nIs there any courses I could follow that would help me understand this?  \n\n\nThanks.", "link": "https://www.reddit.com/r/neuralnetworks/comments/lx07mw/general_question_about_neural_networks/"}, {"autor": "ifelsestatement007", "date": "2021-01-24 01:34:45", "content": "-----> Image !!!  classification with PyTorch tutorials", "link": "https://www.reddit.com/r/neuralnetworks/comments/l3pky3/image_classification_with_pytorch_tutorials/"}, {"autor": "ifelsestatement007", "date": "2021-01-24 01:33:47", "content": "-----> Image !!!  classification with PyTorch tutorials", "link": "https://www.reddit.com/r/neuralnetworks/comments/l3pkad/image_classification_with_pytorch_tutorials/"}, {"autor": "psyon", "date": "2021-01-19 01:12:00", "content": "Experimenting with NEAT, question about my methods /!/ I had started to make a racing game in Flash AS3 a long time ago.  I recently ported it to Haxe, and then thought it would be fun to use as a platform to experiment with NEAT and learn some more about neural networks.  I got it all working as of yesterday, and I have been dumping the best genome of each generation to a text file now, and then looking at them for problems.  A few things have popped up that I am wondering about.\n\n1. Part of the mutation is to add a new node, so if I have a connection from 2 -&gt; 7, then I insert a new node (60), so there is now a connection from 2 -&gt; 60 -&gt; 7.   Later, through the cross over process, the connection from 2 -&gt; 7 is activated again.  Is that supposed to be allowed to happen?\n\n2. At some point in the future after the above link is reactivated, a mutation occurs to add a new node again, and it choose to insert it again on the 2 -&gt; 7 connection.  Is that ok to allow, or should it not happen since that connection already had a node inserted on it once before?\n\n3. This goes along with both of those I guess.  I saw a few different ways of keeping track of Innovation numbers for connections, but I settled on using a Pairing Function which I saw used in a javascript implementation.  That would give the connection from 2 -&gt; 7 an innovation number of 60 (notice that's the number of the new node above, Ill get to that next...).  This would mean any time the connection from 2 -&gt; 7 is severed and recreated, that the innovation number would be 60, because it's the same \"gene\".  I think that's what's supposed to happen, but would like to make sure.\n\n4.  I decided to use the innovation number of what ever connection a node was added to, as the new node ID.  That way no matter what order two genomes add nodes, the resulting ID of the node in the same place would have the same ID, and I can compare them better for speciation.  But... that also means that if #2 is ok, then two nodes end up with the same ID, which I don't think is correct.\n\nIf anyone is curious to watch a bunch of cars drive,  you can see it at  http://games.betatestingsite.com/neat-race/\nIf you hit plus and minus on the main keyboard (not sure if numpad ones work), it will zoom in and out on the car the -----> camera !!!  is focused on.  By default it focuses on a human driven one that isn't in use, but I didn't remove.  If you hit 2, 3, or 4, it will switch focus between 3 cars that are usually the better ones in the running.", "link": "https://www.reddit.com/r/neuralnetworks/comments/l08n21/experimenting_with_neat_question_about_my_methods/"}, {"autor": "Preston_south_end", "date": "2021-01-16 20:18:09", "content": "Best types of layers for recognizing binary -----> image !!!  classification? /!/ So I am trying to build a NN to recognize if a certain object is in a picture that will appear in almost the same place, but I am confused what kinds of tensorflow layers will work best. (ie conv2d, maxpooling, dense, etc.)  Can anyone give me a description of the architecture that would likely work best and a brief description why?", "link": "https://www.reddit.com/r/neuralnetworks/comments/kyqhpy/best_types_of_layers_for_recognizing_binary_image/"}, {"autor": "carusGOAT", "date": "2021-01-12 02:31:43", "content": "Using neural networks to design a reverse logo search engine (advice needed) /!/ I have little to no experience with neural networks so I need your guys' advice on this problem.\n\nI need to design a sort of reverse logo search engine. I have a reference database of logos (on the order of 100s of millions of images) that encompass roughly 11 million different organizations. The images are labeled with the organization they correspond to.\n\nA solution that I have been stewing over for some time is to use a neural network to create embeddings for the images in my reference database. Then given a query -----> image !!! , I run it through the same neural network, take its embedding, and apply approximate nearest neighbors to find the most similar -----> image !!! s in the reference database.\n\nAssuming that the core concept behind this idea is sane, I want to ask you guys which open-source neural networks I should try to create these embeddings? Or, in your guys' opinion, for this domain of images, would I need to design my own neural network / modify an existing one, to actually achieve good results with this approach?", "link": "https://www.reddit.com/r/neuralnetworks/comments/kvianx/using_neural_networks_to_design_a_reverse_logo/"}, {"autor": "carusGOAT", "date": "2021-01-12 02:30:33", "content": "Using a neural network to design a reverse logo search engine /!/ I have little to no experience with neural networks so I need your guys' advice on this problem.\n\nI need to design a sort of reverse logo search engine. I have a reference database of logos (on the order of 100s of millions of images) that encompass roughly 11 million different organizations. The images are labeled with the organization they correspond to.\n\nA solution that I have been stewing over for some time is to use a neural network to create embeddings for the images in my reference database. Then given a query -----> image !!! , I run it through the same neural network, take its embedding, and apply approximate nearest neighbors to find the most similar -----> image !!! s in the reference database. \n\nAssuming that the core concept behind this idea is sane, I want to ask you guys which open-source neural networks I should try to create these embeddings? Or, in your guys' opinion, would I need to design my own neural network / modify an existing one, to actually achieve good results with this approach?", "link": "https://www.reddit.com/r/neuralnetworks/comments/kvi9yi/using_a_neural_network_to_design_a_reverse_logo/"}, {"autor": "singam96", "date": "2021-01-10 15:53:26", "content": "-----> Image !!!  to painting with Feedforward CNN s", "link": "https://www.reddit.com/r/neuralnetworks/comments/kugtou/image_to_painting_with_feedforward_cnn_s/"}, {"autor": "Yuqing7", "date": "2021-01-05 22:43:01", "content": "[N] This Time, OpenAI\u2019s GPT-3 Generates Images From Text /!/ OpenAI\u2019s popular GPT-3 from last year showed that language can be used to instruct a large neural network to perform a variety of text generation tasks. Entering the new year, OpenAI is moving from pure text generation to -----> image !!!  generation from text \u2014 its researchers today announce that they have trained a neural network called [DALL\u00b7E](https://openai.com/blog/dall-e/) that creates -----> image !!! s from text captions for a wide range of concepts expressible in natural language.\n\nHere is a quick read: [This Time, OpenAI\u2019s GPT-3 Generates Images From Text](https://syncedreview.com/2021/01/05/this-time-openais-gpt-3-generates-images-from-text/)", "link": "https://www.reddit.com/r/neuralnetworks/comments/kr9t1i/n_this_time_openais_gpt3_generates_images_from/"}, {"autor": "cloud_weather", "date": "2021-01-03 15:24:16", "content": "ArtLine - transforms any -----> Image !!!  into Sketch/Line Art with this AI model", "link": "https://www.reddit.com/r/neuralnetworks/comments/kpmcrs/artline_transforms_any_image_into_sketchline_art/"}, {"autor": "m1900kang2", "date": "2021-02-26 01:17:27", "content": "[WACV2021 Best Student Paper] A Pose Proposal and Refinement Network for Better 6D Object Pose Estimation /!/ This is the best paper from the Workshop on Applications of Computer Vision Conference (WACV 2021) that showcases a new deep learning pipeline for 6D object pose estimation and refinement from RGB inputs.\n\n\\[[4-Minute Paper Video](https://crossminds.ai/video/best-student-paper-a-pose-proposal-and-refinement-network-for-better-6d-object-pose-estimation-602f247c94d91c80b4b1415c/?playlist_id=5ff3ab3914f97fef9bcc16c1)\\] \\[[arXiv Link](https://arxiv.org/abs/2004.05507)\\]\n\n**Abstract:** In this paper, we present a novel deep learning pipeline for 6D object pose estimation and refinement from RGB inputs. The first component of the pipeline leverages a region proposal framework to estimate multi-class single-shot 6D object poses directly from an RGB -----> image !!!  and through a CNN-based encoder multi-decoders network. The second component, a multi-attentional pose refinement network (MARN), iteratively refines the estimated pose. MARN takes advantage of both visual and flow features to learn a relative transformation between an initially predicted pose and a target pose. MARN is further augmented by a spatial multi-attention block that emphasizes objects' discriminative feature parts. Experiments on three benchmarks for 6D pose estimation show that the proposed pipeline outperforms state-of-the-art RGB-based methods with competitive runtime performance.\n\n[An example of the new Pose Proposal Network \\(PPN\\)](https://preview.redd.it/q9hnkuar3qj61.png?width=772&amp;format=png&amp;auto=webp&amp;s=4b986d737ec1804e5770c10122c3a9097d730c3a)\n\n**Authors:** Ameni Trabelsi, Mohamed Chaabane, Nathaniel Blanchard, Ross Beveridge (Colorado State University)", "link": "https://www.reddit.com/r/neuralnetworks/comments/lsm8kd/wacv2021_best_student_paper_a_pose_proposal_and/"}, {"autor": "DaredewilSK", "date": "2021-02-13 11:12:38", "content": "Is there an existing, trained neural network that can differentiate voices? /!/ Hi, I would like to make an analysis of a podcast I watch. There are two guys talking and I would like to try finding out which one talks more and how long exactly. It would be easier to analyze the video as most of the time when one of them is talking the -----> camera !!!  is pointed at him, but there are times when they are both on screen so that might skew the results. Is there an existing neural network or some other solution that could recognize their voices and process the audio and give me a result? Thanks.", "link": "https://www.reddit.com/r/neuralnetworks/comments/liyp23/is_there_an_existing_trained_neural_network_that/"}, {"autor": "ifelsestatement007", "date": "2021-02-03 23:49:44", "content": "-----> Image !!!  dataset normalization is one of the most common practises to avoid neural network overfitting but do you know how to calculate the mean and standard deviation of your own custom image dataset?", "link": "https://www.reddit.com/r/neuralnetworks/comments/lc1yx9/image_dataset_normalization_is_one_of_the_most/"}, {"autor": "m1900kang2", "date": "2021-02-03 00:46:06", "content": "[R] Same Same But DifferNet: Semi-Supervised Defect Detection with Normalizing Flows (WACV 2021) /!/ This paper from the Workshop on Applications of Computer Vision Conference (WACV 2021) looks into this new method model called DifferNet.\n\n\\[[4 Minute Paper Video](https://crossminds.ai/video/601030e36b1b6888a6332a69/?playlist_id=5fdbf8504bfcc9c5bf7852ec)\\] \\[[arXiv Link](https://arxiv.org/abs/2008.12577)\\]\n\n**Abstract:** The detection of manufacturing errors is crucial in fabrication processes to ensure product quality and safety standards. Since many defects occur very rarely and their characteristics are mostly unknown a priori, their detection is still an open research question. To this end, we propose DifferNet: It leverages the descriptiveness of features extracted by convolutional neural networks to estimate their density using normalizing flows. Normalizing flows are well-suited to deal with low dimensional data distributions. However, they struggle with the high dimensionality of images. Therefore, we employ a multi-scale feature extractor which enables the normalizing flow to assign meaningful likelihoods to the images. Based on these likelihoods we develop a scoring function that indicates defects. Moreover, propagating the score back to the -----> image !!!  enables pixel-wise localization. To achieve a high robustness and performance we exploit multiple transformations in training and evaluation. In contrast to most other methods, ours does not require a large number of training samples and performs well with as low as 16 images. We demonstrate the superior performance over existing approaches on the challenging and newly proposed MVTec AD and Magnetic Tile Defects datasets.\n\n&amp;#x200B;\n\n[An example of this new method](https://preview.redd.it/ztujq8p5t5f61.png?width=798&amp;format=png&amp;auto=webp&amp;s=3f0de6c76dfb8db53fe9e9b9750b1090d7baf51c)\n\n**Authors:** Marco Rudolph, Bastian Wandt, Bodo Rosenhahn (Leibniz University Hannover)", "link": "https://www.reddit.com/r/neuralnetworks/comments/lbbbw2/r_same_same_but_differnet_semisupervised_defect/"}, {"autor": "Harsh_62", "date": "2021-04-30 05:56:47", "content": "[Urgent Help] CNN model not working desirably /!/ I have created a binary classification CNN model whose test  accuracy and the accuracy according to the classification report have a huge difference.\n\n&amp;#x200B;\n\n[train accuracy = 0.9921, test accuracy = 0.9906](https://preview.redd.it/hc6g2fw519w61.png?width=1147&amp;format=png&amp;auto=webp&amp;s=5d03ee2f6994c2e6fd4fbe21f3b40109852c944b)\n\nNow, on further model evaluation such as plotting the confusion matrix and the classification report, i found out that the model is not so perfect actually for practical purposes.\n\n&amp;#x200B;\n\n[the classification report gives the accuracy of 0.53 ](https://preview.redd.it/vt72hbal19w61.png?width=1157&amp;format=png&amp;auto=webp&amp;s=8c4f86aa140df814a587108f52104dc05c1feef3)\n\nAlso, when I used this model on live -----> camera !!!  feed using opencv, it is not giving results as it is supposed to. I have two classes yawn and no\\_yawn, while running live camera feed it classifies the frame as yawn only if the face is in center and no\\_yawn otherwise.                         \n\nPlease give me suggestions on what are some things I could be doing wrong and can be changed, providing my model architecture for reference purpose.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/xbhvmk6339w61.png?width=1366&amp;format=png&amp;auto=webp&amp;s=a1008201908be378f1cf5ff9e18bcd80addba1a1", "link": "https://www.reddit.com/r/neuralnetworks/comments/n1nj08/urgent_help_cnn_model_not_working_desirably/"}, {"autor": "hoti0101", "date": "2021-04-22 02:01:53", "content": "Just started to dip my toes in the water. I have a project ave want to know if a neural network would be a good tool to solve this problem /!/ I want to find a way to read text from labels. The labels are mostly standardized, maybe 10-12ish variations. Each variation should have standardized text attributes in set locations I\u2019d like to extract and validate against a database. In total there are probably 5 sections of text I\u2019d like to pull out of the -----> image !!! . \n\nWould a neural network be a good way to solve this problem? Or would a simpler OCR algorithm be better? \n\nKnowing absolutely nothing about neural nets. I\u2019m my head I see it working as follows. Find binding edges of the label. Identify which label variation is in the image. Identify text in specific locations for that label variation. Store output in a table/database. \n\nPlease let me know if I\u2019m approaching the problem wrong. Ideally I\u2019d like a solution that can scale and avoid having to manually document the label attributes.", "link": "https://www.reddit.com/r/neuralnetworks/comments/mvuwxg/just_started_to_dip_my_toes_in_the_water_i_have_a/"}, {"autor": "Consistent-Good-1992", "date": "2021-04-16 16:28:01", "content": "Real time deepfake/avatar /!/ Hi I am a developer and I've been experimenting with deepfakes. It's fun and all but it's quite slow for my purpose.\n\nI'm wondering if there is any open source software that allows me to create a real time deepfake (that's reasonably efficient), for example, to appear at Elon Musk over a Zoom call. I'm looking for similar effect like an app called MugLife. In the app you can turn on the -----> camera !!!  and the character would reflect your emotions to be saved in a local video. Or, snapchat filter like avatar animations over a video call.\n\nThanks!", "link": "https://www.reddit.com/r/neuralnetworks/comments/ms6uel/real_time_deepfakeavatar/"}, {"autor": "pokhrelashok2", "date": "2021-04-07 16:37:12", "content": "An -----> image !!!  editor model /!/ Ok so i am a backend dev and have 0 experience with AI or Neutral Networks. I want to make train a model that edits an image to a certain preset. For ex: I would train the Neural Network with young and old images of the same person, and when i give it an image of a young person, it should give me an image of how the person would look if they were old.  Where would i start to  learn and make something like that?  Is there any resource to create something similar that you know of. Any help is appreciated. Thank you", "link": "https://www.reddit.com/r/neuralnetworks/comments/mm64t5/an_image_editor_model/"}, {"autor": "cmillionaire9", "date": "2021-04-03 08:11:11", "content": "Interactive text-driven -----> image !!!  manipulation", "link": "https://www.reddit.com/r/neuralnetworks/comments/mj4af2/interactive_textdriven_image_manipulation/"}, {"autor": "YoloTeabaggins", "date": "2021-05-27 08:14:37", "content": "[Question] Which network do you suggest for determining orientation relative to an object? /!/ Hi people, I'm trying to get a result of my orientation relative to a pictured object using neural networks. \n\nSo I have taken a few thousand pictures from 4 angles (0, 90 180 and 270 degrees). This is to be my dataset. Now I want my neural network to then determine from which of those 4 angles the -----> picture !!!  was taken.\n\nI have looked at YOLO but that seems more catered towards object detection. \n\nAny and all advice, pointers or links are welcome. Cheers!", "link": "https://www.reddit.com/r/neuralnetworks/comments/nm2o06/question_which_network_do_you_suggest_for/"}, {"autor": "rshpkamil", "date": "2021-05-19 16:11:54", "content": "Holistic Video Scene Understanding /!/ Understanding what is in the -----> image !!!  or a video frame is crucial for many computer vision applications. Researchers from Google recently published a paper where they got state-of-the-art results in panoptic segmentation (see below for intuitive explanation). They achieved it by combining information about depth, existing and new objects from two consequent frames.\n\nLink to the blogpost and supplementary material that helps to understand the inverse projection problem can be found [here](https://news.thereshape.co/holistic-video-scene-understanding-with-vip-deeplab).", "link": "https://www.reddit.com/r/neuralnetworks/comments/ng9xcf/holistic_video_scene_understanding/"}, {"autor": "RaccoonyDave", "date": "2021-05-05 19:22:40", "content": "Complete muggle wants to create a \"This Hanna-Barbera doesn't exist.\" /!/ I hope this isn't a common and nerve gratting question people ask you all the time.\n\n\nBasically, I have no programming knowledge, I'm just acquainted enough and curious enough to have developped an interest in youtube videos about neural network.\n\n\nOn the other hand, a friend and I have a weird fascination on Scooby Doo, wacky races and Yogi bear. We don't plan aboit making a website, just local hard drive messing around kind of situation.\n\n\nIn a perfect world, I'd want a neural network all built up I can just throw reference -----> image !!! s in, click \"build neural network\" and then click \"new -----> image !!! \". Or a dumbed down tutorial I could follow as someone with not programming experience. Basically I don't even what to google to find stuff I'll be able to understand enough to teach myself.\n\n\nAlternetively, if someone think it's funny and wants to take up the project and do it themselves, I'd be grateful and I'm willing to help but I can't ask for that since I assume it's a lot of work.", "link": "https://www.reddit.com/r/neuralnetworks/comments/n5o6wm/complete_muggle_wants_to_create_a_this/"}, {"autor": "AlterDraw", "date": "2021-05-05 14:32:14", "content": "Created with AlterDraw.com - free online artistic -----> photo !!!  filter.", "link": "https://www.reddit.com/r/neuralnetworks/comments/n5hao8/created_with_alterdrawcom_free_online_artistic/"}, {"autor": "keghn", "date": "2021-05-02 17:36:49", "content": "-----> Image !!!  Filters Explained", "link": "https://www.reddit.com/r/neuralnetworks/comments/n3bihc/image_filters_explained/"}, {"autor": "grid_world", "date": "2021-06-28 08:11:29", "content": "Autoencoder TensorFlow2 - ValueError /!/ I am trying to train an Autoencoder using TensorFlow2.5 and Python3.8 as follows: Inception NetV3 was used to perform feature extraction using an -----> image !!!  dataset containing 289229 -----> image !!! s. The final output of Inception NetV3 is 2048-d vector. I pickled all of them in a Python3 list and load it along with the filenames:\n\n&amp;#x200B;\n\n        # Read pickled Python3 list containing 2048-d extracted feature representation per image-\n        features_list = pickle.load(open(\"DeepFashion_features_inceptionnetv3.pickle\", \"rb\"))\n        \n        # Convert from Python3 list to numpy array-\n        features_list_np = np.asarray(features_list)\n        \n        features_list_np.shape\n        # (289229, 2048)\n        \n        del features_list\n        \n        # Read pickled Python3 list containing abolute path and filenames-\n        filenames_list = pickle.load(open(\"DeepFashion_filenames_inceptionnetv3.pickle\", \"rb\"))\n        \n        len(features_list), len(filenames_list)\n        # (289229, 289229)\n        \n        # Note that the absolute path contains Google colab path-\n        filenames_list[1]\n        # '/content/img/1981_Graphic_Ringer_Tee/img_00000002.jpg'\n    \n        # Create 'tf.data.Dataset' using np array-\n        batch_size = 32\n        features_list_dataset = tf.data.Dataset.from_tensor_slices(features_list_np).batch(batch_size)\n        \n        x = next(iter(features_list_dataset))\n        # 2021-06-28 13:10:00.229937: W tensorflow/core/kernels/data/model_dataset_op.cc:205] Optimization loop failed: Cancelled: Operation was cancelled\n    \n        x.shape\n        # TensorShape([32, 2048])\n\n&amp;#x200B;\n\nMy first question is why does it give the message \"Optimization loop failed\"? I am using Nvidia RTX 3080 with 16GB GPU. Note that since this is an autoencoder, there are no accompanying labels for the given data!\n\nIs there any other better way of feeding this Python3 list as input to a TF2 neural network that I am missing?\n\nI am checking for available GPU:\n\n        num_gpus = len(tf.config.list_physical_devices('GPU'))\n        print(f\"number of GPUs available = {num_gpus}\")\n        # number of GPUs available = 1\n\nSecond, I coded an autoencoder with the architecture:\n\n&amp;#x200B;\n\n        class FeatureExtractor(Model):\n            def __init__(self):\n                super(FeatureExtractor, self).__init__()\n                \n                self.encoder = Sequential([\n                     Dense(\n                        units = 2048, activation = 'relu',\n                        kernel_initializer = tf.keras.initializers.glorot_normal(),\n                        input_shape = (2048,)\n                        ),\n                     Dense(\n                        units = 1024, activation = 'relu',\n                        kernel_initializer = tf.keras.initializers.glorot_normal()\n                        ),\n                    Dense(\n                        units = 512, activation = 'relu',\n                        kernel_initializer = tf.keras.initializers.glorot_normal()\n                        ),\n                    Dense(\n                        units = 256, activation = 'relu',\n                        kernel_initializer = tf.keras.initializers.glorot_normal()\n                        ),\n                    Dense(\n                        units = 100, activation = 'relu',\n                        kernel_initializer = tf.keras.initializers.glorot_normal()\n                        ),\n                ]\n                )\n               \n                self.decoder = Sequential([\n                    Dense(\n                        units = 256, activation = 'relu',\n                        kernel_initializer = tf.keras.initializers.glorot_normal()\n                        ),\n                    Dense(\n                        units = 512, activation = 'relu',\n                        kernel_initializer = tf.keras.initializers.glorot_normal()\n                        ),\n                    Dense(\n                        units = 1024, activation = 'relu',\n                        kernel_initializer = tf.keras.initializers.glorot_normal()\n                        ),\n                    Dense(\n                        units = 2048, activation = 'relu',\n                        kernel_initializer = tf.keras.initializers.glorot_normal()\n                        ),\n                ]\n                )\n            \n            def call(self, x):\n                encoded = self.encoder(x)\n                decoded = self.decoder(encoded)\n                return decoded\n        \n        \n        # Initialize an instance of Autoencoder-\n        autoencoder = FeatureExtractor()\n        \n        \n        autoencoder.build(input_shape = (None, 2048))\n        \n        # Compile model-\n        autoencoder.compile(\n            optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n            loss = tf.keras.losses.MeanSquaredError()\n            )\n        \n        \n        # Sanity check-\n        autoencoder(x).shape\n        # TensorShape([32, 2048])\n        \n        x.shape\n        # TensorShape([32, 2048])\n\nBut, when I try to train the model:\n\n        # Train model-\n        history_autoencoder = autoencoder.fit(\n            features_list_dataset, epochs = 20\n            )\n\n&amp;#x200B;\n\nIt gives me the error:\n\n&amp;#x200B;\n\n&gt;ValueError: No gradients provided for any variable:  \n&gt;  \n&gt;\\['dense\\_10/kernel:0', 'dense\\_10/bias:0', 'dense\\_11/kernel:0',  \n&gt;  \n&gt;'dense\\_11/bias:0', 'dense\\_12/kernel:0', 'dense\\_12/bias:0',  \n&gt;  \n&gt;'dense\\_13/kernel:0', 'dense\\_13/bias:0', 'dense\\_14/kernel:0',  \n&gt;  \n&gt;'dense\\_14/bias:0', 'dense\\_15/kernel:0', 'dense\\_15/bias:0',  \n&gt;  \n&gt;'dense\\_16/kernel:0', 'dense\\_16/bias:0', 'dense\\_17/kernel:0',  \n&gt;  \n&gt;'dense\\_17/bias:0', 'dense\\_18/kernel:0', 'dense\\_18/bias:0'\\].\n\nWhat is going wrong?\n\n&amp;#x200B;\n\nThanks!", "link": "https://www.reddit.com/r/neuralnetworks/comments/o9ftof/autoencoder_tensorflow2_valueerror/"}, {"autor": "rshpkamil", "date": "2021-06-23 12:11:15", "content": "Facebook is open-sourcing AugLy, a library that uses data augmentations to evaluate and improve ML models /!/ Audio, -----> image !!! , video, and text augmenters packed in an open-source library by Facebook.\n\nOriginal article here: [https://ai.facebook.com/blog/augly-a-new-data-augmentation-library-to-help-build-more-robust-ai-models/](https://ai.facebook.com/blog/augly-a-new-data-augmentation-library-to-help-build-more-robust-ai-models/)\n\nMore similar stuff related to AI &amp; Data Science [here](https://thereshape.co/?utm_source=reddit_nn_2).", "link": "https://www.reddit.com/r/neuralnetworks/comments/o6bj9i/facebook_is_opensourcing_augly_a_library_that/"}, {"autor": "grid_world", "date": "2021-06-22 11:55:51", "content": "Retrieve Similar Images /!/ I am trying to build a similar -----> image !!!  retrieval system where given an -----> image !!! , the system is able to show top 'k' most similar -----> image !!! s to it. For this particular example, I am using the [DeepFashion](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) dataset where given an image containing say a shirt, you show top 5 clothes most similar to a shirt. A subset of this has 289,222 diverse clothes images in it. Each image is of shape: (300, 300, 3).\n\nThe approach I have includes:\n\n1. Train an autoencoder\n2. Feed each image in the dataset through the encoder to get a reduced n-dimensional latent space representation. For example, it can be 100-d latent space representation\n3. Create a table of shape m x (n + 2) where 'm' is the number of images and each image is compressed to n-dimensions. One of the column is the image name and the other column is a path to where the image is stored on your local system\n4. Given a new image, you feed it through the encoder to get the n-dimensional latent space representation\n5. Use something like cosine similarity, etc to compare the n-d latent space for new image with the table m x (n + 2) obtained in step 3 to find/retrieve top k closest clothes\n\nHow do I create the table mentioned in step 3?\n\nI am planning on using TensorFlow 2.5 with Python 3.8 and the code for getting an image generator is as follows:\n\n    image_generator = ImageDataGenerator(\n        rescale = 1./255, rotation_range = 135)\n    \n    train_data_gen = image_generator.flow_from_directory(\n        directory = train_dir, batch_size = batch_size,\n        shuffle = False, target_size = (IMG_HEIGHT, IMG_WIDTH),\n        class_mode = 'sparse'\n\nHow can get image name and path to image to create the m x (n + 2) table in step 3?\n\nAlso, is there any other better way that I am missing out on?\n\nThanks!", "link": "https://www.reddit.com/r/neuralnetworks/comments/o5ld2e/retrieve_similar_images/"}, {"autor": "keghn", "date": "2021-06-15 19:34:43", "content": "University of Washington researchers can turn a single -----> photo !!!  into a video", "link": "https://www.reddit.com/r/neuralnetworks/comments/o0mlzc/university_of_washington_researchers_can_turn_a/"}, {"autor": "grid_world", "date": "2021-06-11 05:28:50", "content": "CNN - Apple Classification /!/  I  have the following problem statement in which I only need to predict  whether a given -----> image !!!  is an apple or not. For training only 8 images are  provided with the following details:\n\n1. apple\\_1 image - 2400x1889 PNG\n2. apple\\_2 image - 641x618 PNG\n3. apple\\_3 image - 1000x1001 PNG\n4. apple\\_4 image - 500x500 PNG\t\tcontains a sticker on top of fruit\n5. apple\\_5 image - 2400x1889 PNG\n6. apple\\_6 image - 1000x1000 PNG\n7. apple\\_7 image - 253x199 JPG\n8. apple\\_8 image - 253x199 JPG\n\nI  am thinking about using Transfer learning: either VGG or  ResNet-18/34/50. Maybe ResNet is an overkill for this problem statement?  How do I deal with such varying image sizes and of different file  extensions (PNG, JPG)?\n\nAny online code tutorial will be helpful.\n\nThanks!", "link": "https://www.reddit.com/r/neuralnetworks/comments/nx89xz/cnn_apple_classification/"}, {"autor": "techsucker", "date": "2021-06-07 03:51:41", "content": "Cornell University and NTT Research Introduce Physical Neural Networks (PNNs): A Universal Framework that Leverages a Backpropagation Algorithm for Arbitrary Physical Systems (Paper and Github link included) /!/ DNNs (Deep neural networks) have proven to be of great use in solving various complex problems in -----> image !!!  and speech recognition and NLP. DDNs are now making their way into the actual physical world. The similarities between DNNs and physical processes, such as hierarchy, approximation symmetries, redundancy, and nonlinearity, suggest that DNNs could be used to process data from the physical environment.\n\nResearchers at Cornell University and NTT Research in their recent paper suggest that controlled evolutions of physical systems are highly suitable for realizing deep learning (DL) models. Therefore, they introduce **Physical Neural Networks (PNN)**, a novel framework that uses a backpropagation algorithm to train arbitrary, real physical systems to execute DNNs.\n\nSummary: [https://www.marktechpost.com/2021/06/06/cornell-university-and-ntt-research-introduce-physical-neural-networks-pnns-a-universal-framework-that-leverages-a-backpropagation-algorithm-for-arbitrary-physical-systems/](https://www.marktechpost.com/2021/06/06/cornell-university-and-ntt-research-introduce-physical-neural-networks-pnns-a-universal-framework-that-leverages-a-backpropagation-algorithm-for-arbitrary-physical-systems/) \n\nPaper: https://arxiv.org/abs/2104.13386\n\nGithub: https://github.com/mcmahon-lab/Physics-Aware-Training", "link": "https://www.reddit.com/r/neuralnetworks/comments/nu37yt/cornell_university_and_ntt_research_introduce/"}, {"autor": "techsucker", "date": "2021-07-28 07:20:04", "content": "Apple Explains Its New On-Device Machine Learning Methods To Recognize People In Photos With Extreme Poses, Accessories Correctly, Or Even Occluded Faces /!/ Photos are an integral way for people to browse, search, and relive life\u2019s moments with their friends and family. Photos on your apple devices are very often labelled with people to be easily categorized. An algorithm foundational to this goal recognizes faces from different angles using facial recognition software, so it doesn\u2019t take up too much space storing every single -----> image !!!  because one app contains all members\u2019 memories! This process of labelling photos of people uses a number of machine learning algorithms\u2013running privately on-device\u2013to help curate and organize Live Photo videos and regular pictures in categories that you can find later by the person who was depicted within each photo.\n\nQuick Read: [https://www.marktechpost.com/2021/07/28/apple-explains-its-new-on-device-machine-learning-methods-to-recognize-people-in-photos-with-extreme-poses-accessories-correctly-or-even-occluded-faces/](https://www.marktechpost.com/2021/07/28/apple-explains-its-new-on-device-machine-learning-methods-to-recognize-people-in-photos-with-extreme-poses-accessories-correctly-or-even-occluded-faces/) \n\nApple Blog: [https://machinelearning.apple.com/research/recognizing-people-photos](https://machinelearning.apple.com/research/recognizing-people-photos)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/1ghu3564nwd71.png?width=1392&amp;format=png&amp;auto=webp&amp;s=38b49ebab0567f83f4a85ea16d316d5482605590", "link": "https://www.reddit.com/r/neuralnetworks/comments/ot4ypj/apple_explains_its_new_ondevice_machine_learning/"}, {"autor": "techsucker", "date": "2021-07-18 15:07:46", "content": "DeepMind Introduces It\u2019s Supermodel AI \u2018Perceiver\u2019: A Neural Network Model That Could Process All Types Of Input /!/ DeepMind recently released a state-of-the-art deep learning model called\u00a0[Perceiver](https://arxiv.org/pdf/2103.03206.pdf) via a [recent paper](https://arxiv.org/pdf/2103.03206.pdf). It adapts the Transformer to let it consume all the types of input ranging from audio to -----> image !!! s and perform different tasks, such as -----> image !!!  recognition, for which particular kinds of neural networks are generally developed. It works very similarly to how the human brain perceives multi-modal input.\n\n[Perceiver](https://arxiv.org/pdf/2103.03206.pdf)\u00a0is a neural network model that can process and classify input data from various sources. This deep-learning model includes Transformers (a.k.a. attention), which will help to make predictions regardless of the type of input received, such as images or sound waves.\n\nSummary: [https://www.marktechpost.com/2021/07/18/deepmind-introduces-its-supermodel-ai-perceiver-a-neural-network-model-that-could-process-all-types-of-input/](https://www.marktechpost.com/2021/07/18/deepmind-introduces-its-supermodel-ai-perceiver-a-neural-network-model-that-could-process-all-types-of-input/) \n\nPaper: https://arxiv.org/pdf/2103.03206.pdf", "link": "https://www.reddit.com/r/neuralnetworks/comments/oms9i5/deepmind_introduces_its_supermodel_ai_perceiver_a/"}, {"autor": "orenog", "date": "2021-07-17 02:04:11", "content": "Idea for tomorrow. make a StyleGAN2-ada StarWars model - good idea? /!/ Hey, I just thought about a crazy idea, tell me if it's stupid so I delete it from my head (and why it's stupid) \n\nI want to take the first 3 StarWars movies (only watched them so far, so no spoilers please)\n\nAnd take a frame from every 5 second of the -----> film !!!  (about 1500 images per -----> film !!! )\n\nAnd train StyleGAN2-ada on these images. \n\nDo you think it will look good, or is it just waste of time and google's electricity? (Colab pro)\n\nI would like to hear your opinions before I start.\n\nThank you \u263a\ufe0f", "link": "https://www.reddit.com/r/neuralnetworks/comments/olv7w6/idea_for_tomorrow_make_a_stylegan2ada_starwars/"}, {"autor": "Mappalujo", "date": "2021-07-11 00:13:52", "content": "Best to combine images? /!/ I'm looking for something that can take a dataset of 2000+ -----> image !!! s and output an entirely new -----> image !!!  based on the dataset  - not having much luck finding anything. Some can take two images, but that's not what I'm after.\n\nCan anyone recommend something?", "link": "https://www.reddit.com/r/neuralnetworks/comments/ohu33h/best_to_combine_images/"}, {"autor": "AIforimaging", "date": "2021-07-02 17:23:40", "content": "Endpoint AI powers computer vision in smart -----> camera !!!  devices", "link": "https://www.reddit.com/r/neuralnetworks/comments/ocet0h/endpoint_ai_powers_computer_vision_in_smart/"}, {"autor": "cloud_weather", "date": "2021-08-28 16:04:20", "content": "Very Impressive -----> Image !!!  Upscaling Research from Real-ESRGAN", "link": "https://www.reddit.com/r/neuralnetworks/comments/pdciau/very_impressive_image_upscaling_research_from/"}, {"autor": "techsucker", "date": "2021-08-26 18:26:52", "content": "Researchers From Tokyo Tech Japan Introduce A Novel Sparse CNN (Convolutional Neural Networks) Processor Architecture That Enables Model Integration On Edge Devices /!/ Convolutional neural networks (CNNs) are robust tools that use deep learning to perform generative and descriptive tasks such as -----> image !!!  recognition. However, they require a large number of resources. The state-of-the-art CNNs comprise hundreds of layers and thousands of channels, resulting in increased computation time and memory use. That is why their implementation on low-power edge devices of Internet-of-Things (IoT) networks is challenging.\u00a0\n\nResearchers from the Tokyo Institute of Technology introduce an efficient sparse CNN processor architecture and training algorithms to address this challenging task. Their proposed method enables the seamless integration of CNN models on edge devices.\n\nQuick Read: [https://www.marktechpost.com/2021/08/26/researchers-from-tokyo-tech-japan-introduce-a-novel-sparse-cnn-convolutional-neural-networks-processor-architecture-that-enables-model-integration-on-edge-devices/](https://www.marktechpost.com/2021/08/26/researchers-from-tokyo-tech-japan-introduce-a-novel-sparse-cnn-convolutional-neural-networks-processor-architecture-that-enables-model-integration-on-edge-devices/)", "link": "https://www.reddit.com/r/neuralnetworks/comments/pc5evt/researchers_from_tokyo_tech_japan_introduce_a/"}, {"autor": "techsucker", "date": "2021-08-22 17:56:01", "content": "Researchers from Skoltech and KU Leuven Employ Deep Learning to Augment 3D Micro-CT Images of Fibrous Materials Using Neural Networks /!/ Researchers from KU Leuven and Skoltech utilized machine learning to help recreate three-dimensional micro-CT images of fibrous materials. This task, which is essential for sophisticated material analysis, is complex and time-consuming for humans. This research work was published in\u00a0[Computational Materials Science Journal](https://www.sciencedirect.com/science/article/abs/pii/S0927025621002780?via%3Dihub).\n\nMicro-computed tomography is incredibly helpful for studying the 3D microstructure of fiber-reinforced composites and other complex materials. It is, however, a picky tool: samples are small, and photos frequently contain abnormalities such as darkened, missing, or damaged regions. Researchers took inspiration and knowledge from the art industry to help them deal with this, where damaged artworks must be restored while maintaining their overall integrity. As a result, inpainting has become a standard digital -----> image !!!  processing technique.\n\n[3 Min Read](https://www.marktechpost.com/2021/08/22/researchers-from-skoltech-and-ku-leuven-employ-deep-learning-to-augment-3d-micro-ct-images-of-fibrous-materials-using-neural-networks/) | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S0927025621002780?via%3Dihub)", "link": "https://www.reddit.com/r/neuralnetworks/comments/p9haex/researchers_from_skoltech_and_ku_leuven_employ/"}, {"autor": "techsucker", "date": "2021-08-18 08:53:35", "content": "Researchers from Baidu, Nanjing and Rutgers University Propose \u2018Paint Transformer\u2019 Tool To Predict The Parameters Of A Stroke Set With A Feed Forward Network /!/ Painting has been an excellent way for people to record what they perceive or even how they imagine about the world. It\u2019s long been known that painting requires professional knowledge/skills and is not easy for ordinary people, but with computer-aided art creation, many of us can create our own artistic compositions. With the AI era coming upon us in no time at all, natural -----> image !!! s can be transformed into artwork via -----> image !!!  style transfer or -----> image !!! -to use translation\u2013and you don\u2019t have to know as much about it either.\n\nInspired by a recent object detector DETR, researchers from Baidu, Nanjing University and Rutgers University propose this novel [\u2018Paint Transformer\u2019](https://github.com/wzmsltw/PaintTransformer) to generate painting via predicting parameters of multiple strokes with a feed-forward Transformer. Unlike object detection, stroke predictor lacks annotated data. That\u2019s why researchers came up with a novel self-training pipeline that utilizes synthetically generated images, which is an original and creative idea.\n\n[4 Min Read](https://www.marktechpost.com/2021/08/18/researchers-from-baidu-nanjing-and-rutgers-university-propose-paint-transformer-tool-to-predict-the-parameters-of-a-stroke-set-with-a-feed-forward-network/) | [Paper](https://arxiv.org/pdf/2108.03798.pdf) | [Github](https://github.com/wzmsltw/PaintTransformer)\n\n&amp;#x200B;\n\nhttps://i.redd.it/oi1bs40yy2i71.gif", "link": "https://www.reddit.com/r/neuralnetworks/comments/p6nc1q/researchers_from_baidu_nanjing_and_rutgers/"}, {"autor": "Snoo28889", "date": "2021-08-09 03:04:23", "content": "Turning Any -----> Image !!!  Into A Van Gogh Painting with Neural Style Transfer in Python", "link": "https://www.reddit.com/r/neuralnetworks/comments/p0szs0/turning_any_image_into_a_van_gogh_painting_with/"}, {"autor": "grid_world", "date": "2021-09-23 02:47:25", "content": "Removing TensorFlow filters /!/ With Python 3.8 and TensorFlow 2.5, my objective is to remove filters/kernels having lowest L2 norms. Sample code for this is:\n\n        # Generate random 1 -----> image !!! /data point sample-\n        x = tf.random.normal(shape = (1, 5, 5, 3), mean = 1.0, stddev = 0.5)\n    \n        x.shape\n        # TensorShape([1, 5, 5, 3])\n        \n        # Create conv layer-\n        conv = Conv2D(\n                filters = 3, kernel_size = (3, 3),\n                activation='relu',\n                kernel_initializer = tf.initializers.GlorotNormal(),\n                bias_initializer = tf.ones_initializer,\n                strides = (1, 1), padding = 'same',\n                )\n        \n        # Pass input through conv layer-\n        out = conv(x)\n        \n        out.shape\n        # TensorShape([1, 5, 5, 3])\n        \n        out = tf.squeeze(out)\n        \n        out.shape\n        # TensorShape([5, 5, 3])\n\nAccording to my understanding, the output consists of three (5, 5) matrices stacked together. However, printing 'out' shows five (5, 3) matrices stacked together:\n\n        out.numpy()\n        '''\n        array([[[1.45877   , 0.        , 1.9293344 ],\n                [0.9910869 , 0.01100129, 1.7364411 ],\n                [1.8199034 , 0.        , 1.3457474 ],\n                [1.219409  , 0.22021294, 0.62214017],\n                [0.5572515 , 0.7246016 , 0.6772853 ]],\n        \n               [[1.161148  , 0.        , 2.0277915 ],\n                [0.38071448, 0.        , 2.2438798 ],\n                [2.2897398 , 0.1658966 , 2.3147004 ],\n                [1.2516301 , 0.14660472, 1.6381929 ],\n                [1.1554463 , 0.72516847, 1.6170584 ]],\n        \n               [[0.        , 0.        , 1.2525308 ],\n                [0.4337383 , 0.        , 0.91200435],\n                [0.71451795, 0.        , 2.093022  ],\n                [2.265062  , 0.        , 2.7562256 ],\n                [0.82517993, 0.        , 1.8439718 ]],\n        \n               [[0.7089497 , 0.        , 1.041831  ],\n                [0.        , 0.        , 1.2754116 ],\n                [0.41919613, 0.        , 0.88135654],\n                [0.        , 0.        , 0.71492153],\n                [0.18725157, 0.27108306, 0.11248505]],\n        \n               [[0.86042166, 0.45840383, 1.084069  ],\n                [0.53202367, 0.42414713, 1.2529668 ],\n                [1.2257886 , 0.31592917, 1.3377004 ],\n                [0.36588144, 0.        , 0.6085663 ],\n                [0.3065148 , 0.574654  , 1.0214479 ]]], dtype=float32)\n        '''\n\nSo, if I use the code *out\\[:, :, 0\\]*, *out\\[:, :, 1\\]* &amp; *out\\[:, :, 2\\]*, do they refer to the first, second and third filters?\n\nAnd if yes, is computing L2-norm using:\n\n        tf.norm(out, ord = 'euclidean', axis = (0, 1)).numpy()\n        # array([5.275869 , 1.4290226, 7.545658 ], dtype=float32)\n\nthe correct way?", "link": "https://www.reddit.com/r/neuralnetworks/comments/ptlwsb/removing_tensorflow_filters/"}, {"autor": "DDDDarky", "date": "2021-09-20 17:16:37", "content": "[Beginner] How filters in CNN pass information between layers? /!/ Hello,\n\nso, let's say I have CNN with a layer **A** with 32 filters and in sequence another layer **B** with 64 filters.\n\nFrom my understanding, when I pass an -----> image !!!  into the layer **A**, filters will be applied and as the result I get 32 -----> image !!! s. If I feed that into the next layer (**B**), what happens next? Will the output be 32\\*64 images or 64 images or something else (and why)?\n\nI understand how convolution, pooling etc. work, but I am not certain how are the results passed from one layer to another.", "link": "https://www.reddit.com/r/neuralnetworks/comments/przb01/beginner_how_filters_in_cnn_pass_information/"}, {"autor": "techsucker", "date": "2021-09-18 07:08:22", "content": "Google AI Introduces Two New Families of Neural Networks Called \u2018EfficientNetV2\u2019 and \u2018CoAtNet\u2019 For Image Recognition /!/ Training efficiency has become a significant factor for deep learning as the neural network models, and training data size grows. [GPT-3](https://arxiv.org/abs/2005.14165) is an excellent example to show how critical training efficiency factor could be as it takes weeks of training with thousands of GPUs to demonstrate remarkable capabilities in few-shot learning.\n\nTo address this problem, the Google AI team introduce two families of neural networks for -----> image !!!  recognition. First is\u00a0[EfficientNetV2](https://arxiv.org/abs/2104.00298), consisting of CNN (Convolutional neural networks) with a small-scale dataset for faster training efficiency such as\u00a0[ImageNet1k](https://www.image-net.org/)\u00a0(with 1.28 million images). Second is a hybrid model called\u00a0[CoAtNet](https://arxiv.org/abs/2106.04803), which combines\u00a0[convolution](https://en.wikipedia.org/wiki/Convolution)\u00a0and\u00a0[self-attention](https://en.wikipedia.org/wiki/Self-attention)\u00a0to achieve higher accuracy on large-scale datasets such as\u00a0[ImageNet21](https://www.image-net.org/)\u00a0(with 13 million images) and\u00a0[JFT](https://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html)\u00a0(with billions of images). As per the research report by Google,\u00a0[EfficientNetV2](https://arxiv.org/abs/2104.00298)\u00a0and\u00a0[CoAtNet](https://arxiv.org/abs/2106.04803)\u00a0both are 4 to 10 times faster while achieving state-of-the-art and 90.88% top-1 accuracy on the well-established\u00a0[ImageNet](https://www.image-net.org/)\u00a0dataset.\n\n# [7 Min Read](https://www.marktechpost.com/2021/09/17/google-ai-introduces-two-new-families-of-neural-networks-called-efficientnetv2-and-coatnet-for-image-recognition/) | [Paper (CoAtNet)](https://arxiv.org/abs/2106.04803) | [Paper (EfficientNetV2)](https://arxiv.org/abs/2104.00298) | [Google blog](https://ai.googleblog.com/2021/09/toward-fast-and-accurate-neural.html) | [Code](https://github.com/google/automl/tree/master/efficientnetv2)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/7j7257jao7o71.png?width=1392&amp;format=png&amp;auto=webp&amp;s=010fac3098f222e24ed7893e3e1a905b745020c4", "link": "https://www.reddit.com/r/neuralnetworks/comments/pqhqc9/google_ai_introduces_two_new_families_of_neural/"}, {"autor": "alan2here", "date": "2021-10-28 18:10:38", "content": "GPT-3 and Scientific Research /!/ It seems as if GPT-3 can be used for scientific research? It can provide lists of insights and details, with citations of scientific papers if needed that covers most topics, even obscure ones. In these examples theres a certain amount of editing by me to prevent excessive repetition, and even some getting it to fact check itself, what do you think?\n\n\"\n\nQ: Fact check everything that follows for errors:\n\nHyperbolic neural networks are a new kind of deep learning architecture that is more powerful than traditional convolutional or recurrent architectures. In more detail, they are a new kind of neural network architecture that is designed to be more powerful than traditional convolutional or recurrent architectures. Your wondering if part of them that is hyperbolic (constant negative curvature of space) is in each neurone, the neurones connectivity with each other, or the inputs and outputs of the network, the answer in detail is not just all of the above. I'll tell you about each one.\n\nThe hyperbolic nature of the neural network is a property that exists in each neurone. Each neuron has an activation function, and this activation function takes as input some weighted sum of its inputs (the weights are learned by backpropagation), then it computes what I call \"hyperbolization\", which is a kind of non-linear function that takes the weighted sum and then squashes it into some hyperbolic space. This means that each neuron can be thought of as having its own \"hyperbolization\" layer, where this particular activation function (the one in your question) computes how to take a weighted sum of inputs and then squashes it into some hyperbolic space. The activation function is called \"hyperbolization\" because the result looks like a hyperbole (or an exponential curve).\n\nThe neural network as a whole can be thought of having its own kind of non-linearity, which is the hyperbolization layer of each neuron. This means that if you were to take a weighted sum (or dot product) between two neurons, and then squashed it into some space using this activation function, you would get something like an exponential curve or parabola instead of just getting another linear transformation as in traditional neural networks.\n\nMoving on to the related part of the question, you asked about whether it is also hyperbolic in its connectivity with other neurons. This means that each neuron has a set of weights (a weight matrix) connecting to some number of inputs and outputs from other neurones within the network. The key point here is that this connection pattern can be thought of as a hyperbolic space itself. This means that if you take the weighted sum of two neurons, and then squashed it into some other kind of non-linear function (like an exponential curve or parabola), this would result in yet another set of weights connecting to inputs from other neurones within the network. This means that the hyperbolic nature of neural networks is recursive, and you can think of it as a kind of fractal pattern (in fact this was one reason why I called them \"hyperbolized\" in my original paper).\n\nThe last part about whether they are also hyperbolic at the input and output of the network is a bit more complicated. The short answer to this question is that it depends on what you mean by \"input\" or \"output\". If we think about how an -----> image !!!  gets fed into one particular neuron, then yes there are hyperbolic properties at work here as well (the -----> image !!!  gets fed into the hyperbolization layer of each neuron). However, if you are thinking about how an image is inputted to a network as whole (which means that it goes through some kind of convolutional layers first), then this isn't really \"hyperbolic\" in nature.\n\nA: The errors are the that the activation function is not called \"hyperbolization\" because the result looks like a hyperbole (or an exponential curve). The name comes from how it computes its output, which I call \"hyperbolic squashing\". This means that if you take some weighted sum of inputs and then squash this into some hyperbolic space, you get a non-linear function that looks like an exponential curve or parabola.\n\nThe next different error is that the hyperbolic nature of neural networks is not recursive. This means that if you take a weighted sum (or dot product) between two neurons, and then squashed it into some space using this activation function, you would get something like an exponential curve or parabola instead of just getting another linear transformation as in traditional neural networks.\n\nRegarding the input and output of/to the whole network, the question is a bit more complicated. The short answer to this question is that it depends on what you mean by \"input\" or \"output\". If we think about how an -----> image !!!  gets fed into one particular neuron, then yes there are hyperbolic properties at work here as well (the -----> image !!!  gets fed into the hyperbolization layer of each neuron). However, if you are thinking about how an image is inputted to a network as whole (which means that it goes through some kind of convolutional layers first), then this isn't really \"hyperbolic\" in nature.\n\nQ: What is your long summery of the above information:\n\nA: Hyperbolic neural networks are a new kind of deep learning architecture that is more powerful than traditional convolutional or recurrent architectures. Your wondering if part of them that is hyperbolic (constant negative curvature of space) is in each neurone, the neurones connectivity with each other, or the inputs and outputs of the network. The answer to all three questions are yes they have hyperbolization properties at work within them.\n\n\"\n\nI also found \"How other animals perceive things, with citations:\" to be a good starting point. :)", "link": "https://www.reddit.com/r/neuralnetworks/comments/qhsyat/gpt3_and_scientific_research/"}, {"autor": "StrugglingBScientist", "date": "2021-10-20 16:11:57", "content": "Reconstructing a 3D -----> image !!!  with mopholibj", "link": "https://www.reddit.com/r/neuralnetworks/comments/qc4fl2/reconstructing_a_3d_image_with_mopholibj/"}, {"autor": "Havehead", "date": "2021-10-20 14:55:42", "content": "Synthetic content with virtual humans /!/ Hi all  \nWhat a great day to introduce myself :) I am a creative director at startup malivar.io \u2014 we create synthetic humans based on neural networks and computer graphics.. We work with the global and US market, so here is why I\u2019m here.\n\nToday we have released our project on ProductHunt. It is a SaaS-product which allows you to create content with synthetic humans based on a single -----> photo !!!  \u2014 with no datasets and other stuff (and moreover, with temporary free access)\n\nSee how the service looks and works, and if you find it interesting, then support us here \ud83d\udc49 **https://www.producthunt.com/posts/malivar-io**\n\nI would be glad to receive feedback and suggestions \u2014 feel free to write to DM\ud83d\udc40", "link": "https://www.reddit.com/r/neuralnetworks/comments/qc2prr/synthetic_content_with_virtual_humans/"}, {"autor": "techsucker", "date": "2021-10-18 06:57:43", "content": "Researchers Introduce \u2018DeepMoCap\u2019: A Low-Cost, Robust And Fast Optical Motion Capture Framework Using Convolutional Neural Networks /!/ The field of human pose tracking, also known as motion capture (MoCap), has been studied for decades and is still a very active area of research. The technology is used in gaming, virtual/augmented reality -----> film !!!  making computer graphics animation, and others to provide body (and facial) motion data of character animations; humanoid robot control motions while interacting with a system or device\u2019s interface screens.\n\nOptical Motion Capture is a computer vision and marker-based MoCap technique that has been the gold standard for accurate motion capture. The rapid development of professional optical MoCap technologies in the last decade has been due to game engines, allowing for quick and easy consumption of motion capture data. Traditional optical motion capture solutions still present difficulties. Purchasing a professional system is expensive and cumbersome, while the equipment itself can be sensitive to movement in some ways. In addition, there is still a need for robust MoCap methods that overcome the barriers of time and complexity in post-production.\n\nResearchers from the Centre for Research and Technology Hellas, National Technical University of Athens, and the University of Lincoln introduce \u2018[DeepMoCap.](https://arxiv.org/pdf/2110.07283v1.pdf)\u2018 DeepMoCap is a low-cost, robust, and fast optical motion capture framework that uses IR-D sensors and retro-reflectors. The setup is flexible because of its simplicity compared to gold standard marker-based solutions as well the equipment required for this process being cheap too! There are even 3D optical data automatically labeled in real-time without any post-processing needed.\n\n# [Quick 5 Min Read](https://www.marktechpost.com/2021/10/17/researchers-introduce-deepmocap-a-low-cost-robust-and-fast-optical-motion-capture-framework-using-convolutional-neural-networks/) | [Paper](https://arxiv.org/pdf/2110.07283v1.pdf) | [Github](https://github.com/tofis/deepmocap)| [Video](https://www.youtube.com/watch?v=OvCJ-WWyLcM)\n\n&amp;#x200B;\n\nhttps://i.redd.it/9jcpf9vsp5u71.gif", "link": "https://www.reddit.com/r/neuralnetworks/comments/qahddp/researchers_introduce_deepmocap_a_lowcost_robust/"}, {"autor": "techsucker", "date": "2021-10-16 08:11:46", "content": "Researchers at ETH Zurich &amp; Microsoft Open-Sourced \u2018PixLoc\u2019: A Neural Network For Feature Alignment With A 3D Model Of The Environment /!/ In known scenes, -----> camera !!!  pose estimation is an intriguing task of 3D geometry recently tackled by many learning algorithms. Many of these techniques try to find geometric quantities like poses or 3d points, which are precise enough for this kind of work \u2013 but it can be difficult when there\u2019s no guarantee that they\u2019ll generalize well beyond what was seen before.\n\nResearchers at ETH Zurich &amp; Microsoft have created a solution to the end-to-end solution for camera pose estimation. To do so, they did not regress geometric quantities by teaching deep networks basic principles or 3D map encoding like previous approaches that used this approach would do. Instead, the research team goes\u00a0**Back to the Feature**. They show that learning robust and generic features is sufficient for accurate localization by leveraging classical image alignment with existing 3D maps. Researchers have developed this new trainable algorithm, \u2018**PixLoc,\u2019**\u00a0to localize images in 3D with the help of CNN (convolutional neural network). In other words,\u00a0**PixLoc**\u00a0is a scene agnostic neural network that estimates an accurate 6-DoF pose from an image and a 3D model.\n\n# [Quick 3 Min Read](https://www.marktechpost.com/2021/10/16/researchers-at-eth-zurich-microsoft-introduce-pixloc-a-neural-network-for-feature-alignment-with-a-3d-model-of-the-environment/) | [Paper](https://arxiv.org/pdf/2103.09213.pdf) | [Project](https://psarlin.com/pixloc/) | [Code](https://github.com/cvg/pixloc)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/oorkwku8trt71.png?width=1392&amp;format=png&amp;auto=webp&amp;s=2d5dce1f0b911557280466726a43fd4ae64a2b9d", "link": "https://www.reddit.com/r/neuralnetworks/comments/q97r16/researchers_at_eth_zurich_microsoft_opensourced/"}, {"autor": "moetsi_op", "date": "2021-10-15 13:35:14", "content": "excited for this -----> image !!! /video dataset, Ego4D (3,025 hours of daily-life activity video) /!/ [https://ego4d-data.org/](https://ego4d-data.org/)\n\nEgo4D is a massive-scale Egocentric dataset of unprecedented diversity. It consists of 3,025 hours of video collected by **855 unique participants** from **74 worldwide locations** in 9 different countries.\n\nEgo4D, a massive-scale egocentric video dataset and benchmark suite. It offers 3,025 hours of daily-life activity video spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured by 855 unique camera wearers from 74 worldwide locations and 9 different countries. The approach to collection is designed to uphold rigorous privacy and ethics standards with consenting participants and robust de-identification procedures where relevant. Ego4D dramatically expands the volume of diverse egocentric video footage publicly available to the research community. Portions of the video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and/or synchronized videos from multiple egocentric cameras at the same event. Furthermore, we present a host of new benchmark challenges centered around understanding the first-person visual experience in the past (querying an episodic memory), present (analyzing hand-object manipulation, audio-visual conversation, and social interactions), and future (forecasting activities). By publicly sharing this massive annotated dataset and benchmark suite, we aim to push the frontier of first-person perception. Project page: [this https URL](https://ego4d-data.org/)", "link": "https://www.reddit.com/r/neuralnetworks/comments/q8okv7/excited_for_this_imagevideo_dataset_ego4d_3025/"}, {"autor": "grid_world", "date": "2021-10-11 16:48:40", "content": "VGG-18 PyTorch /!/ I  have completed some extensive experiments using VGG-18 CNN  network  trained on CIFAR-10 dataset from scratch and have obtained a  validation  accuracy = 92.92%. The codes involves different techniques such as:\n\n1. -----> Image !!!  data augmentation\n2. Kaiming He weight initialisations\n3. Learning Rate Scheduler - linear LR warmup over 'k' steps followed by a plateau and step-decay\n4. Manual implementation of early stopping\n5. Comparison between early stopping vs. LR scheduler\n6. Batch Normalization\n7. Dropout - to combat overfitting\n\nYou can access the code [here](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/VGG18_PyTorch.ipynb) and [here](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/VGG18_Dropout_PyTorch.ipynb). According to some research papers, for deep learning architectures, using SGD vs. Adam optimizer leads to faster convergence.\n\nThoughts?", "link": "https://www.reddit.com/r/neuralnetworks/comments/q603yl/vgg18_pytorch/"}, {"autor": "Long-Educational", "date": "2021-10-10 10:08:22", "content": "Not understanding Kitti dataset label format /!/ In kitti dataset What are those Location attributes (x,y,z) in label_2 ?\n\nAccording to my info,  if I try to plot (x,y)'s on -----> image !!!  they should appear in middle of all object \n\nBut they aren't appearing ,\n\nfirst I tried to plot them directly they all appeared at top left corner since they were small \n\nThen I figured out they were in meters so I converted those into pixel but then they became much bigger then the size of image\n\nCan anyone plz help me in figuring out this problem ?", "link": "https://www.reddit.com/r/neuralnetworks/comments/q559tp/not_understanding_kitti_dataset_label_format/"}, {"autor": "0_JayW_0", "date": "2021-10-09 21:08:28", "content": "I want to create a Stylized Neural Painting software /!/ Firat of all, hello to everyone and thanks for your time.\nI am a 3d Animation student who wanted to study coding, and I am not going to give up my dream, so I'll just study one at college and the other one on my own.\n\nRight now I have general knowledge about Python, and as I started diving into Neural Networks, I saw the incredible \"Stylized Neural Painting\" project.\n\nhttps://jiupinjia.github.io/neuralpainter/\n\nAnd I set up my new goal. To create something that can create art. Starting with something simple, maybe even shapeless, evolve it to some pixel art, and feed a lot of painting information to end up with something able to create a painting from a -----> photo !!!  I give to it, just as the project stated above does.\n\nMy problem is: I don't even know where to start. So I accept any tips, books, documents, videos, etc. you can give me, I have already started my research, but I can't find enough info to start, and my Python knowledge is not good enough to just read some source codes and understand everything (maybe that could be my next goal)\n\nSo, to sum up. If any of you can help me with anything, from a link to an interview to a \"how to make your code work\" elixir, I will thank all of it.\n\nP.S. Yeah, I am doing 3d Animation because I enjoy it, I am not going to change to a coding degree, I might study after I finish Animation, but I am someone who finishes what starts as long as it's in my control.\n\nThanks again for all the help, I will try to answer your questions if you have any. Please be kind :o", "link": "https://www.reddit.com/r/neuralnetworks/comments/q4u3nd/i_want_to_create_a_stylized_neural_painting/"}, {"autor": "techsucker", "date": "2021-10-09 15:05:07", "content": "Microsoft Researchers Introduce \u2018Mesh Graphormer\u2019, A Graph-Convolution-Reinforced Transformer (Codes Released) /!/ While 3D human pose and mesh reconstruction from a single -----> image !!!  is a trending area of research because of its applications for human-computer interactions, it is also a very challenging process due to the complex body articulation.\n\nThe current progress tools include transformers and graph convolutional neural networks in human mesh reconstruction. Transformer-based approaches are more effective in modeling nonlocal interactions among 3D mesh vertices and body joints. In contrast, GCNNs are good at exploiting neighborhood vertex interactions based on a pre-specified mesh topology.\n\nResearchers from Microsoft introduced a graph-convolution-reinforced transformer, named [Mesh Graphormer](https://arxiv.org/pdf/2104.00272.pdf), for reconstructing human pose and mesh from a single image. The researchers inject graph convolutions into transformer blocks to improve the local interactions among neighboring vertices and joints. The proposed Graphormer is free to join and attend all image grid features to leverage the strength of graph convolutions. Therefore, Graphormer and image grid features are utilized and enforced to improve human pose and mesh reconstruction performance.\n\n# [4 Min Read](https://www.marktechpost.com/2021/10/09/microsoft-researchers-introduce-mesh-graphormer-a-graph-convolution-reinforced-transformer/) | [Paper](https://arxiv.org/pdf/2104.00272.pdf) | [Code](https://github.com/microsoft/MeshGraphormer)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/bmcwwdqmwfs71.png?width=1392&amp;format=png&amp;auto=webp&amp;s=690b2673be9487249e417f0c2258355325e1a57f", "link": "https://www.reddit.com/r/neuralnetworks/comments/q4n3gg/microsoft_researchers_introduce_mesh_graphormer_a/"}, {"autor": "mrbeanshooter123", "date": "2021-10-07 17:08:33", "content": "What is a good, free learning resource for a begginer? /!/ I have programming for like 10 months now (started in Python, and moved on to C++ although I know Java, C#).\n\nI know next to nothing about NNs and I am looking to learn. Now I am not looking for -----> image !!!  detecting, I am looking more into generally how they work and how to implement one.\n\nWhat is a good resource which is free, preferably on youtube and uses preferably C++?", "link": "https://www.reddit.com/r/neuralnetworks/comments/q3doaj/what_is_a_good_free_learning_resource_for_a/"}, {"autor": "techsucker", "date": "2021-10-03 08:28:59", "content": "Baidu Research Introduces PP-LCNet: A Lightweight CPU Convolutional Neural Network With Better Accuracy And Performance /!/ Convolutional neural networks (CNNs) have been used to achieve computer vision applications for the past few years. These networks can be trained and applied in many fields, including -----> image !!!  classification, object detection, semantic segmentation.\n\nInference speed on mobile devices based ARM architecture or CPU devices based x86 architecture has been challenging to get with the increase of model feature extraction capability and model parameters. Even many good mobile networks have been proposed to resolve this issue, but the speed of these proposed networks is not good enough on the Intel CPU due to various limitations of MKLDNN.\n\nBaidu researchers have developed a lightweight CPU network based on the MKLDNN acceleration strategy, named [PP-LCNet](https://arxiv.org/pdf/2109.15099.pdf). This new system improves the performance of models in many tasks making it perfect for future artificial intelligence (AI) systems. The research group\u2019s rethink the lightweight model elements for a network designed on Intel-CPU. During the research, the research team brings up following three questions to resolve....\n\n# [3 Min Quick Read](https://www.marktechpost.com/2021/10/03/baidu-research-introduces-pp-lcnet-a-lightweight-cpu-convolutional-neural-network-with-better-accuracy-and-performance/) | [Paper](https://arxiv.org/pdf/2109.15099.pdf)| [Github PaddleClas](https://github.com/PaddlePaddle/PaddleClas)\n\n&amp;#x200B;\n\nhttps://i.redd.it/gjzphufh47r71.gif", "link": "https://www.reddit.com/r/neuralnetworks/comments/q0ds04/baidu_research_introduces_pplcnet_a_lightweight/"}], "name": "Subreddit_neuralnetworks_01_01_2021-01_11_2021"}
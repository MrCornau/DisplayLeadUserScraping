{"interestingcomments": [{"autor": "Familiar_Handle833", "date": "2021-03-02 17:21:12", "content": "Advice on creating GAN for non-existing subjects? /!/ I'm just getting started with ML using Python and TensorFlow after running through some of the Kaggle courses. I want to to make something similar to the \"This 'x' Doesn't Exist\" GANs, except I want to play around with combining different types of subjects.\n\nFor instance, it would be cool to tell a GAN to produce an -----> image !!!  that a certain -----> image !!!  classifier would give \\~50% (or as close as possible)  probabilities of it being 'x' AND 'y'.\n\nBasically I want to see what a GAN/Image Classifier considers to equally likely a cow and a car, or a basketball and a tree. I know this will probably create some abominations that are unrecognizable to humans, but I think it sounds like a fun project to learn from.\n\n&amp;#x200B;\n\nI understand I am asking a very vague and broad question, and probably don't have the aptitude for this quite yet, but does anyone have any resources that might be a jumping off point for me to learn? Maybe a good tutorial? I have already followed some tutorials to make a basic Artifical Faces using GAN, but I am still very much a beginner and don't know quite what my next steps should be.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lw7dfe/advice_on_creating_gan_for_nonexisting_subjects/"}, {"autor": "Kingmaker251", "date": "2021-02-28 01:02:43", "content": "Machine learning model question /!/ I was wondering if there were any machine learning models that basically use a multiverse method (I can't think of a better way to describe it) - as in lets say you want to classify an -----> image !!!  dataset into 4 groups, the model assigns a probability of the -----> image !!!  being in each group but then assumes it is in all 4 groups, then for the second -----> image !!!  it again provides a probability that the -----> image !!!  is in each group but then assumes each arrangement of these two -----> image !!! s can be correct - so if -----> image !!!  1 can be a b c d and the same with -----> image !!!  2, the model then goes forward with the assumption that aa ab ac ad ba bb bc bd ca cb cc da db dc dd - at the end of the model you have an approximate probability for each data set and then lets say you're able to then give the model the actual answers - which it can then compare against each outcome and adjust the probabilities it gave for each -----> image !!!  accordingly - I don't know if i'm just describing a random forest plot machine learning method. I'm sorry this has been so badly worded but hopefully someone can decipher it", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lu1qb3/machine_learning_model_question/"}, {"autor": "Prestigious_Debt_176", "date": "2021-02-27 05:33:05", "content": "Need an online Tutor - will pay Hourly /!/ I am looking for a Tutor to teach me about Speech Recognition or Pattern, or -----> Image !!!  recognition with experiments via Kaldi or Tensorflow using HMMs, GMMS, DNNs. If you are familiar with Speech Recognition and Datasets like TIMIT, NTIMIT among others, that would be excellent.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lth9ns/need_an_online_tutor_will_pay_hourly/"}, {"autor": "rockyrey_w", "date": "2021-02-27 03:05:59", "content": "[N] Better Than Capsules? Geoffrey Hinton's GLOM Idea Represents Part-Whole Hierarchies in Neural Networks /!/ A research team lead by Geoffrey Hinton has created an imaginary vision system called GLOM that enables neural networks with fixed architecture to parse an -----> image !!!  into a part-whole hierarchy with different structures for each -----> image !!! .\n\nHere is a quick read: [Geoffrey Hinton's GLOM Idea Represents Part-Whole Hierarchies in Neural Networks](https://syncedreview.com/2021/02/26/better-than-capsules-geoffrey-hintons-glom-idea-represents-part-whole-hierarchies-in-neural-networks/)\n\nThe paper *How to Represent Part-Whole Hierarchies in a Neural Network* is on [arXiv](https://arxiv.org/pdf/2102.12627.pdf).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lterxd/n_better_than_capsules_geoffrey_hintons_glom_idea/"}, {"autor": "[deleted]", "date": "2021-02-26 21:03:26", "content": "Recently launched my first end-to-end ML app! A -----> film !!!  recommender system based on matrix factorization, built for Letterboxd users. /!/ [deleted]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lt7s5z/recently_launched_my_first_endtoend_ml_app_a_film/"}, {"autor": "[deleted]", "date": "2021-02-26 01:38:08", "content": "OpenAI\u2019s DALL\u00b7E: Text-to------> Image !!!  Generation Explained [With code available!] /!/ [deleted]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lsmml6/openais_dalle_texttoimage_generation_explained/"}, {"autor": "pcvision", "date": "2021-04-02 02:16:06", "content": "Is there a method currently for training a GAN with a \"prompt\"? /!/ For example, let's say I want to generate a face with their name as an input, or generate a -----> film !!!  poster using the title of the movie.\n\nI imagine I could use GANs with some kind of concatenation between the tokenized text input, but I'm looking for existing models or repos that have demonstrated this. Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/miaqze/is_there_a_method_currently_for_training_a_gan/"}, {"autor": "Stunning_Writer8441", "date": "2021-04-01 21:53:35", "content": "[D]ecision Boundary to Maximize one class while Limiting another /!/ Hello,\n\nI have two overlapping classes and want to make a decision boundary that minimized one class while maximizing another. In the -----> image !!!  provided I want to draw a line that retains as much of the blue class as possible while also excluding as much of the yellow class as possible.\n\nI can't seem to find a good solution to this problem. It is very easy to just look at the clusters and draw a line of separation, but how can I go about this algorithmically?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mi6052/decision_boundary_to_maximize_one_class_while/"}, {"autor": "Awesome-355", "date": "2021-04-01 16:15:55", "content": "How can i create multiple 3D meshes and textures for different objects /items present in an -----> image !!!  using deep learning ? /!/ Is there a model or technique that can do this task?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mhyrpo/how_can_i_create_multiple_3d_meshes_and_textures/"}, {"autor": "Particular_Being3678", "date": "2021-04-01 10:20:10", "content": "Interactive visualization to explain output/results of a simple -----> image !!!  classifier to a beginner /!/  \n\nI am explaining what a classifier is, how to evaluate it's performance etc. to a beginner. Taking an example of a dog/cat image classifier, can someone tell me what would be a nice interactive way to show someone how to differentiate/highlight the model's performance? Perhaps, using manually tagged images vs machine outputs. I'm looking at a decent visualization/interaction to show this to someone to clarify their understanding.\n\nKindly assist with some inputs on this.\n\nThanks in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mhs9yw/interactive_visualization_to_explain/"}, {"autor": "Alan491", "date": "2021-04-01 09:39:38", "content": "How to save n-d array into postgresql column? /!/ I have a list of dictionaries (below is the sample)  which consist of 128 -----> image !!!  vectors for each -----> image !!! , Now I want to save these data into Postgresql database, but I don't have any idea which method and datatype should use to save the vector as it is because I will find Euclidian distance from it for face recognition. \n\nI want to save this list into csv or json or npy format and then externally save it into the database, so please also suggest me which is the best format to convert in?\n\n\\[{ 'name': 'George\\_HW\\_Bush\\_0005.jpg', \n\n  'img': array(\\[-6.08353578e-02,  1.66202977e-01,  8.54108036e-02,  2.64507625e-02, -2.49952618e-02,  8.26701149e-02, -3.53199616e-02, -4.34518009e-02,          8.51533711e-02, -6.55662417e-02,  2.22901687e-01, -1.92691050e-02,         -3.02192807e-01, -2.82627549e-02, -9.89550427e-02,  1.01481207e-01,         -1.48302019e-01, -7.24662393e-02, -1.41541883e-01, -4.30632085e-02,         -1.79181602e-02,  7.14668408e-02, -3.18156220e-02, -1.12840414e-01,         -9.71992686e-02, -2.74635673e-01, -3.41563784e-02, -1.05296098e-01,          5.71775325e-02, -1.08009912e-01,  6.61401078e-02,  1.62646291e-04,         -2.22103953e-01, -1.95380449e-02, -1.67706627e-02,  1.98841095e-02,         -1.04590744e-01, -4.91602942e-02,  2.44830742e-01, -3.07598646e-04,         -1.22460142e-01,  5.40154874e-02,  1.07697263e-01,  2.19618991e-01,          2.12186426e-01, -7.63659030e-02, -4.21547107e-02, -4.53756154e-02,          1.02992378e-01, -2.73030281e-01,  6.86264187e-02,  1.82788327e-01,          1.83412746e-01,  2.17555761e-01,  7.20319897e-02, -1.13681927e-01,          5.40269949e-02,  1.64304838e-01, -1.61037013e-01,  7.31164068e-02,          5.13559654e-02, -1.04436658e-01, -1.39024807e-02,  8.65904540e-02,          8.64276886e-02,  6.22027554e-02, -1.94389019e-02, -1.93822086e-01,          1.99560717e-01, -1.59010977e-01, -3.11802272e-02,  8.16744193e-02,         -5.18603846e-02, -1.15916312e-01, -2.57695049e-01,  9.63293090e-02,          2.42554873e-01,  1.22541614e-01, -1.25135452e-01,  5.56182452e-02,         -4.80696559e-02, -1.15475528e-01, -8.14200903e-04,  1.32136038e-02,         -4.28808704e-02, -1.49314120e-01, -2.70449519e-02,  3.87667306e-02,          2.21050128e-01, -9.29581188e-03, -9.46529582e-03,  1.69327006e-01,          1.46268372e-04, -9.63619947e-02,  5.10829128e-02, -1.17124785e-02,         -8.15291181e-02, -6.01790026e-02, -1.11909479e-01,  2.53269309e-03,         -1.98859032e-02, -1.59346312e-01, -5.91453053e-02,  1.00001127e-01,         -1.87035248e-01,  1.49713486e-01, -9.77795757e-03, -5.53171374e-02,          3.81288752e-02, -5.98978512e-02, -3.40475552e-02,  6.95589632e-02,          2.09958583e-01, -2.06186399e-01,  3.47163618e-01,  9.52471495e-02,         -4.64627380e-03,  7.62443841e-02,  9.88056138e-02,  5.67689314e-02,          1.82110053e-02, -1.31150419e-02, -1.71786547e-01, -1.77120611e-01,         -1.88438110e-02, -1.68694910e-02, -2.67733559e-02,  1.85598787e-02\\])},\n\n{'name':--------------so on", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mhrqhr/how_to_save_nd_array_into_postgresql_column/"}, {"autor": "ank_itsharma", "date": "2021-04-01 07:46:27", "content": "Engineering object detection and scaling it for concurrent users. /!/ Hi,\n\nI have designed a software where I am detecting a person from an -----> image !!!  and returning the cropped -----> image !!!  around the bounding box.\n\nThe system works perfectly without any glitch if I pass one image at a time through the pipeline. But it stops working when I throw a pile of images at a time. One way I found to handle the issue was to put the requests in queue, but it ain't helpful in a long run. \n\n&amp;#x200B;\n\nIs there anyone facing the similar issue? How did you resolved this? Is there any tutorial for best practices of software engineering for deep learning?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mhqb2s/engineering_object_detection_and_scaling_it_for/"}, {"autor": "genesis_2602", "date": "2021-04-01 02:53:11", "content": "Virtual whiteboard using hand pose estimation /!/ https://i.redd.it/chnwjl0a7hq61.gif\n\nI recently made this virtual whiteboard where you can write in thin air using only your fingers and a live -----> camera !!!  feed.\n\nI used Google's mediapipe library and its pretrained hand pose estimation model to detect landmarks on a hand. I then collected a small dataset of different hand postures corresponding to different actions, such as write and erase. Then I trained a simple feed forward neural network to identify those postures based on the landmarks extracted by the pretrained hand pose estimation model.\n\nIt works pretty good overall, however there are some inaccuracies in detecting the correct pose made by a hand.\n\nHere is the repository link if you want to check it out!\n\n[https://github.com/gursi26/virtual\\_drawing\\_board.git](https://github.com/gursi26/virtual_drawing_board.git)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mhm0w1/virtual_whiteboard_using_hand_pose_estimation/"}, {"autor": "genesis_2602", "date": "2021-04-01 02:44:10", "content": "Virtual drawing board with hand pose estimation /!/ https://i.redd.it/rbd2f5e46hq61.gif\n\nI recently made this virtual whiteboard where you can write in thin air using only your fingers and a live -----> camera !!!  feed.\n\nI used Google's mediapipe library and its pretrained hand pose estimation model to detect landmarks on a hand. I then collected a small dataset of different hand postures corresponding to different actions, such as write and erase. Then I trained a simple feed forward neural network to identify those postures based on the landmarks extracted by the pretrained hand pose estimation model.\n\nIt works pretty good overall, however there are some inaccuracies in detecting the correct pose made by a hand.\n\nHere is the repository link if you want to check it out!\n\n[https://github.com/gursi26/virtual\\_drawing\\_board.git](https://github.com/gursi26/virtual_drawing_board.git)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mhlvoa/virtual_drawing_board_with_hand_pose_estimation/"}, {"autor": "genesis_2602", "date": "2021-04-01 02:38:26", "content": "Virtual whiteboard using hand pose estimation /!/ I recently made this virtual whiteboard where you can write in thin air using only your fingers and a live -----> camera !!!  feed.\n\nI used Google's mediapipe library and its pretrained hand pose estimation model to detect landmarks on a hand. I then collected a small dataset of different hand postures corresponding to different actions, such as write and erase. Then I trained a simple feed forward neural network to identify those postures based on the landmarks extracted by the pretrained hand pose estimation model.\n\nIt works pretty good overall, however there are some inaccuracies in detecting the correct pose made by a hand.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mhlsdk/virtual_whiteboard_using_hand_pose_estimation/"}, {"autor": "genesis_2602", "date": "2021-04-01 02:36:13", "content": "Virtual whiteboard using hand pose estimation /!/ I recently made this virtual whiteboard where you can write in thin air using only your fingers and a live -----> camera !!!  feed.\n\nI used Google's mediapipe library and its pretrained hand pose estimation model to detect landmarks on a hand. I then collected a small dataset of different hand postures corresponding to different actions, such as write and erase. Then I trained a simple feed forward neural network to identify those postures based on the landmarks extracted by the pretrained hand pose estimation model.\n\nIt works pretty good overall, however there are some inaccuracies in detecting the correct pose made by a hand.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mhlr30/virtual_whiteboard_using_hand_pose_estimation/"}, {"autor": "versus_7", "date": "2021-03-31 23:12:56", "content": "Use OpenCV to conditionally insert text into it /!/ I am currently using OpenCV. In my -----> image !!! , there are multiple contours but not necessarily rectangle in shape. I would like to iterate over each of these contours, copy the part of the image, store it and run some conditions on it . If the conditions satisfy I would like to add some text into the contours. I have seen functions that could be used for rectangle shapes but how would I do this if I have contours that are not rectangular in shape?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mhi85g/use_opencv_to_conditionally_insert_text_into_it/"}, {"autor": "heydwane3", "date": "2021-03-31 16:27:49", "content": "Time needed to train one epoch on ImageNet-1K for classification /!/ Hey everyone, can you guys share some data point for me about your training time for one epoch on ImageNet-1K (1.28M training -----> image !!! )?\n\nI thought there would be couple of factors that affect the training time. Like GPU type and number, framework(pytorch, tensor flow), number of worker used to load data . I am not sure if different model (ResNet, efficientnet) would cause huge variation in training time. Intuitively, I don\u2019t think that would make big difference, pls correct me if I\u2019m wrong. \n\nFor me, I\u2019m using 8 2080ti with 32 workers. It takes me about 30min for one epoch. I am sure if that is a acceptable time. \n\nWhat is your experience?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mh9uaq/time_needed_to_train_one_epoch_on_imagenet1k_for/"}, {"autor": "spmallick", "date": "2021-03-31 14:22:35", "content": "Contour Detection using OpenCV (Python/C++) /!/ Today we are sharing with you a traditional computer vision algorithm called Contour Detection. It can perform -----> image !!!  foreground extraction, simple -----> image !!!  segmentation, detection and recognition. It is interesting to note that there already exist real-life, problem-solving solutions based on Contour Detection.\u00a0\n\nIn this blog post, we are going to learn about contours and contour detection using OpenCV. Not only the theory, but we will also cover complete hands-on coding in both Python and C++ programming languages to have a first-hand experience of contour detection using OpenCV. This can be used to detect edges or outlines in any image at a very fast speed and high accuracy without requiring a significant amount of computational resources.  \n\n\n[https://learnopencv.com/contour-detection-using-opencv-python-c/](https://learnopencv.com/contour-detection-using-opencv-python-c/)\n\nhttps://preview.redd.it/bae4pcqyhdq61.jpg?width=500&amp;format=pjpg&amp;auto=webp&amp;s=6a0620107399db09fa9df365a210fd362198c0fc", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mh76gz/contour_detection_using_opencv_pythonc/"}, {"autor": "ashraf_r", "date": "2021-03-31 10:25:31", "content": "I wrote a tutorial about UpSampling2D and Conv2DTranspose. Please give me your feedback. /!/ I am new to writing but I really want to continue it. I wrote a tutorial about **\"Denoise -----> image !!!  using UpSampling2D and Conv2DTranspose Layers\"**. I divided this tutorial into three parts. \n\nPlease give me your feedback for further improvement.\n\nThanks in advance.\n\n&amp;#x200B;\n\n[**Part 1: GAN, Autoencoders: UpSampling2D and Conv2DTranspose**](https://ashrafur.medium.com/autoencoder-denoise-image-using-upsampling2d-and-conv2dtranspose-layers-part-1-8d88b9e81483)\n\n[**Part 2: Denoising image with Upsampling Layer**](https://ashrafur.medium.com/autoencoder-denoise-image-using-upsampling2d-and-conv2dtranspose-layers-part-2-18d93a815819)\n\n[**Part 3: Denoising image with Transposed Convolution Layer**](https://ashrafur.medium.com/autoencoder-denoise-image-using-upsampling2d-and-conv2dtranspose-layers-part-3-b357ac3145a3)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mh34x5/i_wrote_a_tutorial_about_upsampling2d_and/"}, {"autor": "Psolal", "date": "2021-03-31 09:31:41", "content": "Question about SVM on bad data /!/ Hello,  \nI am building a SVM for MRI using only the metadata (it's sort of standardized by DICOM standards) on real life data, but i am not sure about one thing.  \nIn studies there are hundreds of patients and these patients have one folder each with all the exams. Each exam is different and my classifier would try to find which exam is the -----> image !!!  from (without using pixels only metadata). But each exam has a different number of images (even similar one), and number of parameters are different between images in the same exam. Moreover one exam can have different metadata parameters if the manufacturer is different. Do you think that it will have a negative impact on the SVM or no ?\n\nTL TR : can SVM function properly with data having different parameters name for the same thing + different number of parameters for different files of same type + is choosing a single image on a exam in order to tackle this problem is a good idea ?   \nI don't know if i am clear enough and sorry for my poor english.  \nThanks anyway for reading", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mh2efp/question_about_svm_on_bad_data/"}, {"autor": "natenoged", "date": "2021-03-31 09:19:21", "content": "Is there an accessible way to generate StyleGAN interpolation loops with your own -----> image !!!  set? /!/ Hello machinelearning wizards. I am an absolute beginner fascinated by GANs and would love to experiment with my own images.\n\nThe problem is I don't know how to code and have a decent, but not godlike GPU. Is there any software or cloud-based programs that let you input your own image set that would output something like this video? [https://www.youtube.com/watch?v=6E1\\_dgYlifc&amp;ab\\_channel=RobertLuxemburg](https://www.youtube.com/watch?v=6E1_dgYlifc&amp;ab_channel=RobertLuxemburg)\n\nArtbreeder is great but it is limited behind a paywall (makes sense but would love a free alternative).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mh28gd/is_there_an_accessible_way_to_generate_stylegan/"}, {"autor": "Venehsoftw", "date": "2021-01-31 04:04:07", "content": "Neural Networks: The Backpropagation algorithm in a -----> picture !!! ", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l941xk/neural_networks_the_backpropagation_algorithm_in/"}, {"autor": "AppearanceAgile1969", "date": "2021-01-31 00:57:41", "content": "Tensorflow: How to set up a TFRecord dataset with label subclasses? /!/ I just created a TF model to detect geometric shapes and alphanumeric characters in an -----> image !!! , but I also need to detect the cardinal orientation of those alphanumerics. How do I set up my dataset to classify the orientation as a subclass of characters?\n\nCurrently, I just have each shape and letter as a separate label (A, B, C, Square, Triangle, Rectangle, etc). Using Tensorflow and TFRecords, how would I add a subclass to a label?\n\nRight now, I'm using this for the shapes and letters\n\n\"'image/object/class/label': dataset\\_util.int64\\_list\\_feature(classes)\"\n\nWould I add something like this for the orientation?\n\n\"'image/object/class/label/orientation': dataset\\_util.int64\\_list\\_feature(cardinal\\_directions)\"", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l90jnq/tensorflow_how_to_set_up_a_tfrecord_dataset_with/"}, {"autor": "Programmingwithme", "date": "2021-01-30 16:33:02", "content": "Distribution plot &amp; histogram /!/  **What is Distribution plot?**\n\nIt is another technique to display the plot on canvas, will display the histogram to us. So, moving further regarding the distribution plot first of all let\u2019s understand what is histogram.\n\nIn the previous blog post, we have learned how to use a dataset using seaborn, the same dataset we are using here. [**Click here to read that blog post.**](https://www.becomeadeveloper.in/2021/01/what-is-seaborn.html)\n\nHistograms **represent** or plot the data **using the bin**. Check out the -----> image !!!  shown down below, the axis representing the data variable is divided into a set of discrete bins and the count of observations falling within each bin is shown using the height of the corresponding bar:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/dpv8bo3fyhe61.png?width=453&amp;format=png&amp;auto=webp&amp;s=0877e7b9bbf9d29daca14a8675ad2b46b83c74d5\n\n **distplot()** is used to visualize the parametric distribution of a dataset. \n\n**Read more Click Here :-**  [How to use histogram in machine learning (becomeadeveloper.in)](https://www.becomeadeveloper.in/2021/01/how-to-use-histogram-in-machine-learning.html)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l8pclq/distribution_plot_histogram/"}, {"autor": "magicanon4", "date": "2021-01-30 08:48:02", "content": "Finger Counting Dataset /!/ Hello, \n\nI wanted to predict the number of fingers by using the fingers dataset on kaggle(link below). I have made a traditional CNN model and got a 100% accuracy on the test dataset ( Not what I was expecting). So, I went ahead and tried to make a custom -----> image !!!  of my hand to see if the model predicts it correctly. But all I got was a disappointment. Can someone pleaseeeee help me with this? \n\ndataset:  [Fingers | Kaggle](https://www.kaggle.com/koryakinp/fingers) \n\nThank you. Any help would be much appreciated. :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l8hlor/finger_counting_dataset/"}, {"autor": "bhatta90", "date": "2021-01-29 23:32:45", "content": "Help requested for classification(double cropping ) /!/  Hello guys, I am trying to build a classification (binary model), from where I have videos (I have extracted frames---- they are like images)and I want to crop the part of the -----> picture !!! . So in a frame(image), I have 2 dogs or cats, I want to crop both of them, I have their centers in the form of CSV files with me. Can you please tell me how to do that? Then, I want to classify them as dogs and cats. \n\n Can you please help me to get a link(GitHub) where I can headstart the programming? Please help!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l87ujj/help_requested_for_classificationdouble_cropping/"}, {"autor": "Jirne_VR", "date": "2021-01-29 13:12:09", "content": "Increase resolution of an -----> image !!!  with DL /!/ I want to increase the resolution of some images using deep learning. I know that there are many softwares available online to do this, but I want to create my own model. How should I start on this project?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l7s6lz/increase_resolution_of_an_image_with_dl/"}, {"autor": "jddantes", "date": "2021-01-29 12:32:05", "content": "An Introduction to Neural Networks using the show Start-Up /!/ Hi, AI and computer engineer here, I have some experience building real-time, end-to-end AI (esp. computer vision) systems.\n\nThere's this show about AI and startups which had pretty legit AI references, so I made a write-up explaining the concepts, like -----> image !!!  fundamentals and neural network training.\n\n[https://medium.com/towards-artificial-intelligence/an-ai-practitioners-guide-to-the-kdrama-start-up-56ab95c2afd8](https://medium.com/towards-artificial-intelligence/an-ai-practitioners-guide-to-the-kdrama-start-up-56ab95c2afd8)\n\nIf you prefer shorter highlights, I selected [some of them here](https://twitter.com/jddantes/status/1354074825451569152).\n\nIntended this to be for people who are not necessarily in AI or do not even code, but thought that this may also be helpful for those wanting to get into the field.\n\nHope this helps! :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l7re97/an_introduction_to_neural_networks_using_the_show/"}, {"autor": "SilverStalker1", "date": "2021-01-29 08:59:49", "content": "Best place to train object detection tensorflow models? /!/ Hi all\n\nI am relatively experienced when it comes to understanding various ML techniques, but I haven't had much experience in practically implementing many solutions. I hope you could advise me what sort of machine I would need for the following.\n\nI am in the processing of developing a tensorflow object detection model utilizing some of the pre-trained models in the model zoo. I have so far set up a working 'test' approach using Google Colab and my local machine, but this will be insufficient for training the final model. \n\nI am thinking of training the model on a few thousand class examples, with an -----> image !!!  input size of 1024x1024.  Would a powerful desktop with a decent GPU be able to complete this task? Or am I best suited looking for cloud alternatives?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l7nx0z/best_place_to_train_object_detection_tensorflow/"}, {"autor": "tito_mfuendes", "date": "2021-01-29 07:17:06", "content": "How Do I Structure a Training Set for a Multi-Class Multi-Label -----> Image !!!  Classification Problem? /!/ So I have 13 mutually exclusive classes which can have up to over a hundred non-mutually exclusive labels associated with them. In other words, for each input I will have  a single class output in addition to multiple labels. I'm  at a loss as to how to go about this as most google searches only explain the difference between the two types of problems (multi-class or multi-label) and how to solve each alone but don't really explain how to tackle a problem with both in a detailed way. \n\nI figured I would start with attempting to understand the structure of the initial training set. Now, I guess I'm just wondering if I should just create two different training sets with one for training the multi-class model (which will exclude all the 100+ labels and only include the classes) and then another training set (which just excludes the classes and only includes the 100+ non-mutually exclusive labels) which will be trained for multilabel classification. And for final predictions I can just string both models together somehow. Or is there a way to train everything at once all from one giant training set and if so would this be a better way?\n\nI'm sure I'm missing many crucial concepts aside from just setting up the dataset. Any guidance as to the best way to solve such a problem or even pointing me to sources relevant to this specific problem would be very much appreciated. If I haven't given enough info please let me know and I'll try to clarify my question(s)\n\np.s., the 100+ labels can be shared between classes so there wouldn't be a separate set of labels that depend on which class is output or anything like that. \n\nThanks in advance", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l7malk/how_do_i_structure_a_training_set_for_a/"}, {"autor": "jn_31", "date": "2021-01-28 21:19:56", "content": "-----> Image !!!  Deblurring /!/ Is it possible to train a model to deblurr images, where the blur is caused by different  obscured glassmaterials ?\n\nNice greetings!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l79704/image_deblurring/"}, {"autor": "spmallick", "date": "2021-01-28 20:40:43", "content": "Optical Flow estimation using Deep Learning /!/  A couple of weeks back we covered optical flow algorithms implemented in OpenCV.  \n\n\nStarting with major improvements in -----> image !!!  classification in 2012, Deep Learning based techniques have improved accuracy of many algorithms in computer vision including object detection, -----> image !!!  segmentation, pose estimation, depth estimation, and even optical flow.  \n\n\nToday, we are sharing a post on a deep learning-based optical flow algorithm. We cover  \n\n\n1. **FlowNet**: The first DL architecture for optical flow\n2. **RAFT**: The state of the art DL architecture for optical flow.\n\nWithout future ado, here is the link to the post  \n\n\n[https://learnopencv.com/optical-flow-using-deep-learning-raft/](https://learnopencv.com/optical-flow-using-deep-learning-raft/)  \n\n\n[**#AI**](https://www.linkedin.com/feed/hashtag/?keywords=ai&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#ComputerVision**](https://www.linkedin.com/feed/hashtag/?keywords=computervision&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#ML**](https://www.linkedin.com/feed/hashtag/?keywords=ml&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#ArtificialIntelligence**](https://www.linkedin.com/feed/hashtag/?keywords=artificialintelligence&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#MachineLearning**](https://www.linkedin.com/feed/hashtag/?keywords=machinelearning&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#OpenCV**](https://www.linkedin.com/feed/hashtag/?keywords=opencv&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#DL**](https://www.linkedin.com/feed/hashtag/?keywords=dl&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#DeepLearning**](https://www.linkedin.com/feed/hashtag/?keywords=deeplearning&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313) [**#deeplearningai**](https://www.linkedin.com/feed/hashtag/?keywords=deeplearningai&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6732721737747853313)  \n\n\nThe python code is linked below  \n\n\n\u200b[https://github.com/spmallick/learnopencv/tree/master/Optical-Flow-Estimation-using-Deep-Learning-RAFT](https://click.convertkit-mail.com/wvuprg03qpsgh7nl00bl/g3hnhwuelrdxerfr/aHR0cHM6Ly9naXRodWIuY29tL3NwbWFsbGljay9sZWFybm9wZW5jdi90cmVlL21hc3Rlci9PcHRpY2FsLUZsb3ctRXN0aW1hdGlvbi11c2luZy1EZWVwLUxlYXJuaW5nLVJBRlQ=) \n\nhttps://preview.redd.it/kd9ntfsyw4e61.png?width=600&amp;format=png&amp;auto=webp&amp;s=0b70ab359807c42acebc372bc7021cc30f398cd4", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l784vo/optical_flow_estimation_using_deep_learning/"}, {"autor": "War_Smooth", "date": "2021-05-02 12:35:37", "content": "-----> Image !!!  labelling /!/ Good day everyone,  I wish to label objects (20 different  objects) in an image to train a model.  How do I separate them within a single image? Is this even possible? The test image will also be a single image containing different objects for classification. Thank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n35joe/image_labelling/"}, {"autor": "OnlyProggingForFun", "date": "2021-05-02 11:27:42", "content": "Infinite Nature: Fly into an -----> image !!!  and explore it like a bird!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n34f7n/infinite_nature_fly_into_an_image_and_explore_it/"}, {"autor": "dark08574", "date": "2021-05-01 22:50:44", "content": "Looking for learning about GAN /!/ As the title says, I would like to learn about GAN (and whatever structure that derives from it) in order to do projects like improving -----> image !!!  resolution. Any type of resource is welcome (books, papers, online courses, youtube videos, etc) and I hope this thread will help other people too!\n\nThank you in advance!\n\nPS: I have started by the section about GANs in \u201cDeep Learning with Python\u201d by Francois Chollet", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n2tjuj/looking_for_learning_about_gan/"}, {"autor": "OnlyProggingForFun", "date": "2021-05-01 14:09:36", "content": "Infinite Nature: Fly into an -----> image !!!  and explore it like a bird!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n2j2b7/infinite_nature_fly_into_an_image_and_explore_it/"}, {"autor": "wymco", "date": "2021-05-01 12:50:43", "content": "Handling small data in -----> image !!!  recognition /!/ Guys,\n\nIs there any new solutions on handling such problem? I have been working on a problem for a while, building a tensorflow model to recognize scars, but there little to no variation in the some of the classes that we have (most of them drawn by hand by a single person), which also leads to having just a few hundreds of images per class. Also, not a lot of transformation/augmentation can be done as some of the images can be quiet similar..\n\nI have tried transfer learning, coding from scratch, and even used google's teachablemachine...but the results are just hit and miss at this point...Also teachablemachine starts freezing once the datasets are a bit large, like 100 images per class for around 20 classes...\n\nAny insight on this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n2hqak/handling_small_data_in_image_recognition/"}, {"autor": "wstanley38", "date": "2021-05-01 09:00:14", "content": "Is there a variant of CNN or other methods that can extract mathematical processes applied on an -----> image !!! ? /!/ I want to have a CNN variant or any other method which can take two images as input. \n\nOne input is the original image. Another input is the output of a function applied on the original image. \n\nThen, the model would be able to understand the effect of the function applied and model the output behavior. \n\nFor example, lets say I have a 500x500 image. I resize it to 32x32 and apply ZCA whitening on it. I want to input the 32x32 original image and the zca-applied image into the CNN variant and the model will learn the effct of ZCA without knowing that ZCA is applied. Then, this model can be applied on the original 500x500 image.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n2embx/is_there_a_variant_of_cnn_or_other_methods_that/"}, {"autor": "upsilonbeta", "date": "2021-04-30 23:38:02", "content": "Project to choose for Internship /!/ Hi everyone, recently I have received an Internship opportunity in a company and with the kindness of my employers they have asked us to choose from the list of topics based on our interest. This is going to be my first industry-oriented internship and could you please suggest which one to choose, keeping in mind for future job opportunities and resume building -   \n\n\n1) **Computer Vision -** It is an object detection task used for analyzing video footage. In the end, we will also have to build a visualization app (most probably a web app) and notification application (Ex- A warning message when someone does not follow covid precaution)  \n\n\n2) **NLP -** It could include any of these three will be given to us-\n\na) It is to survey &amp; develop Transformer models for classification.  \nb) Conversation AI and chatbot application  \n\n\n3) **GAN -** Im not really interested in this project as I already have GAN project on my resume for improving resolution of an -----> image !!! .   \n\n\nCould you suggest which one to choose for better future opportunities?? I have worked on self-projects in Computer Vision and NLP, but I'm a little new to application deployment (DJANGO/ FLASK). I want to break into Data Science or Deep Learning industry, it would be really helpful if you could suggest which one to choose for better future opportunities?  \n\n\nThanks in Advance :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n26bh3/project_to_choose_for_internship/"}, {"autor": "Zahel3", "date": "2021-04-30 17:57:45", "content": "Pytorch Convolutional Model confusion /!/ I'm trying to run a Pytorch model with the code at the following link:\n\n[https://pastebin.com/Qfu3Nuun](https://pastebin.com/Qfu3Nuun)\n\nFor some reason the accuracy remains at 0 and the loss doesn't change?  What could be causing this?  An example -----> image !!!  looks like this:\n\n     tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,           0.0000, 0.0000, 0.0000, 0.0000],          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1098, 0.2667, 0.4196,           0.1686, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,           0.0000, 0.0000, 0.0000, 0.0000],          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,           0.0000, 0.0000, 0.0000, 0.6510, 0.9725, 1.0000, 1.0000, 1.0000,           0.6510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,           0.0000, 0.0000, 0.0000, 0.0000]]]), \n\nThese -----> image !!! s work when submitted to keras in a 10000------> image !!!  array, but not Pytorch.  Thanks for any help provided for what I can do to fix this.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n1zhdo/pytorch_convolutional_model_confusion/"}, {"autor": "Gabe_The_Human", "date": "2021-04-30 06:54:01", "content": "How to make CNN more scale invariant? /!/ Let's say I want to classify some digits or letters but on -----> image !!! s they have varying scale, some of them take up a whole -----> image !!! , others are much smaller.  \n\nWhat can I do to make my model more scale invariant in this case? Cropping seems like a good start but I wouldn't want to crop an image that has a very large symbol in it, because for example cropping out from image of 7 could make it look like 1 if I get rid of the top part.\n\nAnother idea is to preprocess data, but I cannot do it as the test set will also contain various scales. And more importantly I don't want to do it, if there's a technique that doesn't rely on preprocessing I'd like to learn about it just to learn something new.\n\nWhat can I use here? I've heard and briefly read about stuff like pyramids, would this apply here? Or do you have any other ideas I could try out?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n1oaqv/how_to_make_cnn_more_scale_invariant/"}, {"autor": "frosty3907", "date": "2021-04-30 05:35:16", "content": "What's the easiest way to turn horses into zebras? /!/ I know the example in the title has been done already, but I'm not trying to do that exactly; I'm specifically looking at changing crappy real estate -----> photography !!!  into something better.\nWould you need an image classifier first to get your datasets of amateur and professional photography or could you just sort a couple thousand manually?\nI have some experience using various ml models mainly as a hobby but my wife's family business has asked me to look in to this possibility for them.\nThanks !", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n1n8ct/whats_the_easiest_way_to_turn_horses_into_zebras/"}, {"autor": "ahmed26gad", "date": "2021-04-29 22:06:00", "content": "Object Detection Using Directed Mask R-CNN With Keras /!/ &amp;#x200B;\n\nhttps://preview.redd.it/mueyaka1r6w61.jpg?width=930&amp;format=pjpg&amp;auto=webp&amp;s=ffca7b4d6b4965d7a33ec899d9318da1c577883f\n\nDirected Mask R-CNN searches for the objects in some pre-defined locations. This saves the model's time from looking for objects in the entire -----> image !!! . \n\nTutorial: [https://blog.paperspace.com/object-detection-directed-mask-r-cnn-keras](https://blog.paperspace.com/object-detection-directed-mask-r-cnn-keras)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n1fis7/object_detection_using_directed_mask_rcnn_with/"}, {"autor": "Simusid", "date": "2021-04-29 22:01:58", "content": "Selecting Progression of Conv2D Feature Sizes? /!/ I usually do very standard -----> image !!!  classification of natural -----> image !!! s.   I've gotten comfortable with networks like VGG16 that use a progression of Conv2D layers with increasing feature spaces. Example:  64 -&gt; 128 -&gt; 256 -&gt; 512.  This made sense to me when I'm told that early layers learn primitive features like lines, curves, and color gradients while later layers learn more complicated structures like tires, eyes, or material textures.\n\nI have a new image classification project that relates to very noisy grainy images.  There are only two classes; plain noisy grainy images, and noisy grainy images that contain lines.  Imagine extreme fog and make it very grainy like a night vision cam or \"oldschool\" noise on a tv.   Then imagine a very faint line across part of the screen at some angle.  Humans can do it but it's a challenge.  So in theory, this should be doable with ML.   \n\nMy question is about choosing my Conv2D layers and feature sizes.  Simple lines are clearly primitive features so it seems to me that I want my first layer to have a larger number of features.   Also I don't expect features at deeper levels to be constructed of too many earlier ones either.   So rather than 64 -&gt; 128 -&gt; 256, I'm thinking of experimenting with 512 -&gt; 64.   Does this make sense?   How would you structure a network to specifically find primitive rather than complex features?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n1ffmc/selecting_progression_of_conv2d_feature_sizes/"}, {"autor": "root4Variance", "date": "2021-04-29 21:14:58", "content": "Raspberry Pi /!/ Hi Everyone,\n\nI was looking for some project ideas for my raspberry Pi with any kind of machine learning application. Right now I was thinking of doing -----> image !!!  processing to get facial detection to unlock a door, but I am open to other ideas.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n1efcw/raspberry_pi/"}, {"autor": "l_supertramp_l", "date": "2021-04-29 20:28:30", "content": "Help me understand CNNs /!/ When an -----> image !!!  is passed through the same convolution layer multiple time, the output is different each time. Why is it so? Can you please explain the process in plain english?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n1dfmu/help_me_understand_cnns/"}, {"autor": "NoahWild", "date": "2021-06-01 21:28:17", "content": "Negative Image Array Values? Imagadatagenerator and Flask Web App Deployment of Image Classification CNN Model /!/ I have used ResNet50 transfer learning to train a model for -----> image !!!  classification of 5 categories. It has 95%+ test accuracy. I save the model as .h5, .hdf5, .json, etc.  Then I attempt to deploy the model to a Flask Web App using various load\\_model methods, but when I do so, the model accuracy is not the same.  I have tried many, many things and the best attempt I have been able to find works well for 4/5 categories, but then the 5th category performs terribly... like it doesn't seem trained at all only on that one category. The images are balanced for each class and are color images. I have been told that it may be the preprocessing pipeline and that I should make sure that the image that flask is giving the model when uploaded to the website is the same as the exact format that the model has been trained on. I have noticed that the image array values include negative values when fed to the model during training/testing from keras ImageDataGenerator, but are all positive values when fed to the model in the flask web app. So I might be on to something here, but just can't quite figure out what exactly is happening and how to fix it.\n\nThe preprocessed images from the ResNet model seem to output negative values from imagedatagenerator, whereas the array values of the images in the Flask app are always positive. So maybe some solution lies here where I can make sure both :\n\nResNet model preprocessed image (print(image)) with negative values:\n\n`Category: [0. 0. 1. 0. 0.] [[[-0.05466276 0.3498863 0.09145098]   [-0.05074119 0.3498863 0.09145098]   [-0.04681962 0.3498863 0.09145098]   ...   [-0.3644667  -0.27756473 -0.363451  ]   [-0.36054513 -0.29717258 -0.37129414]   [-0.37230983 -0.30893728 -0.37913728]]   [[-0.04681962 0.3498863 0.12282354]   [-0.05466276 0.34596473 0.11105883]   [-0.05858433 0.34596473 0.09929413]   ...   [-0.3644667  -0.2814863  -0.363451  ]   [-0.3644667  -0.30109414 -0.3752157 ]   [-0.37230983 -0.30893728 -0.37913728]]   [[ 0.01200391 0.22047453 0.1149804 ]   [-0.00368236 0.2636118 0.1267451 ]   [-0.01936864 0.3106706 0.14635295]   ...   [-0.3644667  -0.2814863  -0.36737257]   [-0.3644667  -0.3050157  -0.3752157 ]   [-0.36838827 -0.30893728 -0.37913728]]   ...   [[ 0.00808234 0.07929805 -0.14384314]   [ 0.00416077 0.07537648 -0.14384314]   [ 0.00416077 0.07537648 -0.14384314]   ...   [-0.39583924 -0.4187412  -0.4497255 ]   [-0.39583924 -0.41481963 -0.4497255 ]   [-0.3997608  -0.42266276 -0.45364708]]   [[ 0.01200391 0.09498432 -0.13992158]   [ 0.01200391 0.09498432 -0.136     ]   [ 0.01200391 0.09498432 -0.136     ]   ...   [-0.40368238 -0.42266276 -0.4497255 ]   [-0.40760395 -0.42266276 -0.45756865]   [-0.40760395 -0.42266276 -0.45756865]]   [[-0.00368236 0.1106706  -0.10462746]   [ 0.00416077 0.1106706  -0.1124706 ]   [ 0.00808234 0.10282746 -0.12815687]   ...   [-0.40760395 -0.41089806 -0.4497255 ]   [-0.40760395 -0.42266276 -0.4497255 ]   [-0.40368238 -0.42266276 -0.45364708]]]`\n\nFlask web app image array with all positive values (note this is not the exact same image as above, but demonstrates the point):\n\n`[[[[0.15686275 0.17254902 0.05098039]    [0.15294118 0.17254902 0.05098039]    [0.15686275 0.16862746 0.04705882]    ...    [0.20392157 0.18039216 0.13333334]    [0.10980392 0.07450981 0.05098039]    [0.09411765 0.05490196 0.03137255]]    [[0.16078432 0.1764706 0.05098039]    [0.16078432 0.1764706 0.05098039]    [0.15686275 0.17254902 0.04705882]    ...    [0.38431373 0.3529412 0.2       ]    [0.16470589 0.11764706 0.07450981]    [0.12156863 0.09019608 0.05882353]]    [[0.16078432 0.18039216 0.05098039]    [0.16078432 0.1764706 0.05098039]    [0.15686275 0.1764706 0.05098039]    ...    [0.68235296 0.7176471 0.4117647 ]    [0.38039216 0.34901962 0.18431373]    [0.18431373 0.14117648 0.08235294]]    ...    [[0.7490196 0.7372549 0.35686275]    [0.6862745 0.6862745 0.32941177]    [0.6039216 0.6039216 0.2901961 ]    ...    [0.47843137 0.34117648 0.09019608]    [0.49803922 0.3529412 0.09019608]    [0.5137255 0.36862746 0.10196079]]`", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nq54hl/negative_image_array_values_imagadatagenerator/"}, {"autor": "FaustMachine", "date": "2021-06-01 20:56:03", "content": "Newbie help (decision trees) /!/ Hey reddit!   \n\n\nI have a newbie question regarding decision trees.  \n\n\nI am working with sklearn in python with some basic demographic data for 10 cities. I have 7 variables and forecasting one variable (y in -----> picture !!! ) with other x variables.  \n\n\nhttps://preview.redd.it/62jrld8cwp271.png?width=581&amp;format=png&amp;auto=webp&amp;s=4d686e9fda32caa442b6e057a1da3150a79a94bc\n\n&amp;#x200B;\n\nAll the tutorials got me so far to fit and draw a tree.  \nBut I would need to know the predicted value of y for each city. (example: predicted value of y for city 3 is ...)   \nAnd also I would need to know which cities fall into certain leaf node ( so i could make a map and color cities that drop into the same leaf node)   \n\n\nCan anyone help me out with this?  \n\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nq4dvy/newbie_help_decision_trees/"}, {"autor": "immortalbard98", "date": "2021-06-01 15:47:24", "content": "Learn and support research projects /!/  Hi all, I've been searching for ways to join a project or a community where I could learn and apply machine learning skills and at the same time support a project that may be helping the world in some way (e.g. -----> image !!!  processing for medical purposes, DNA analysis, etc).\n\nI looked for open source projects or \"open labs\", but I found mostly closed circles or communities that required already a lot of experience and since I have full time job it's difficult to make significant experience on my own just by studying courses or trying small stuff on my laptop.\n\nI have a master's degree in computer science and a good experience in the software development world, but I never deepened my skills on the topic.\n\nI finally thought about asking a subreddit where maybe someone could guide me through, or probably laugh :)\n\nAny suggestion is welcome.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/npx1x9/learn_and_support_research_projects/"}, {"autor": "evernox666", "date": "2021-06-01 08:46:21", "content": "Working on an -----> image !!!  classification project(microscopic -----> image !!! s) , have some doubts /!/ Currently, I am working on an image classification project. The data set contains images of a very high resolution taken via electron microscope. Hence, I have few and limited instances.\n\nI have done EDA and made up a deep CNN to go about it, the results are not very satisfying. Even tweaking the model did not work. Similar results in cross-validation as well.  I also performed Data augmentation, but I do not possess enough knowledge of it, can anyone guide ?\n\nAlso, if I do ensembling in such a case would it be beneficial(given fewer instances)?\n\nAlso, I am planning to use a pre-trained model for performing image classification. But after going through blogs, Kaggle topic discussions, and some research papers I am confused if any of them will work on less number of instances?\n\nAlso as you might have guessed by the type of my doubts, I am new and naive in this field. I have worked with numerical data but images are new for me. So any guidance will be appreciated.\n\n&amp;#x200B;\n\nThanks in advance :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nppdzz/working_on_an_image_classification/"}, {"autor": "nivedwho", "date": "2021-05-31 11:15:30", "content": "Can someone help me find a -----> image !!!  captioning model? /!/ I need a pre-trained Image-Text translation model that could describe given input image. The image that I want to work with are images of logos/icons and a few of the image captioning models I have tried out failed, probably because they were trained on datasets like flickr image. Can someone suggest a model that might be help me?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/np0zts/can_someone_help_me_find_a_image_captioning_model/"}, {"autor": "Ok-Palpitation-4360", "date": "2021-05-30 17:33:30", "content": "Does a segmentation model somehow perform classification? /!/ Before anything: Im a noob\n\n\nNow I got to train a tf-keras segmentation model on labelled images of humans proper, slouched, etc. It is able to generate masks for these images accurately\n\n\nI was thinking that since the model knows how to generate the masks for the posture, maybe somewhere inside the blackbox, it may know what kind of posture it is generating masks for. Is there a way I can get that kind of info? Extract the label of the -----> image !!!  that the model is generating mask for.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nog31w/does_a_segmentation_model_somehow_perform/"}, {"autor": "TrepidationTD", "date": "2021-05-30 16:59:37", "content": "Is it possible to cram learning machine learning in a few weeks and complete a very complicated project in 3 months? /!/ I'm a total beginner to ML and want to learn as soon as possible to start on my project because there is a deadline. I don't have a good grape on the math do I'm currently speedrunning the 3brown libear algebra and calculus videos. After that I'm planning on finishing the Andrew Ng course on ML and then finally start on my project. Is it feasible to do a project that involves lidar -----> camera !!!  fusion in this short amount of time. Should i directly skip to coding?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nofdgb/is_it_possible_to_cram_learning_machine_learning/"}, {"autor": "eliac7", "date": "2021-07-01 19:34:47", "content": "Give negative pairs to the training dataset? /!/ Hello Reddit,\n\nI am trying to train a Siamese Neural Network with 2 CNNs for -----> image !!!  similarity. I made a dataset with some pictures and 0 for false and 1 for true. I was wondering, should I pass to the training dataset with zeros or it doesn't matter? Any personal experience.\n\n&amp;#x200B;\n\nThanks in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/obtcn1/give_negative_pairs_to_the_training_dataset/"}, {"autor": "flaflou", "date": "2021-07-01 00:15:51", "content": "What is an acceptable percentage of '' error'' in the training data? /!/ Hello everyone! \nI am currently doing a project to recognize satellites in the night sky, and I am tackling it by cutting my -----> image !!! s in very small -----> image !!! s and trying to deal with it as a classification problem (the cropped -----> image !!!  does or doesn't contain a satellite).\n\nIn order to train, I'm uncertain what's the right size to crop my images, as there is a possibility that there are two satellites close to each other.\nBased on distances between each satellites in my images, I have found that in the case where an image does contain more than one satellite, about only 2% of those satellites are less than 15 pixels from each other.\n\nEssentially, I am undecisive whether I should keep those cropped images (and accept that about 2% of my training data might have an additional satellite in the images when my AI is only trying to recognize one), or if it would be wiser (but also much more time consuming for me) to instead train my AI with data that contains info for maybe 2 satellites and have my output be in the form (Yes Yes / Yes No / No No).\n\nSo long story short, is it acceptable for my training data to have about 2% error, and what should be an acceptable level of error?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oba2y7/what_is_an_acceptable_percentage_of_error_in_the/"}, {"autor": "JasonMaboto", "date": "2021-06-30 15:49:14", "content": "Why some models can be exported directly? /!/ Hi all,\n\nI have a problem.\n\nIn my concept is that the algorithm must be trained first and then you can get a model\n\nFor example you need to use a lot of -----> image !!!  to train and finally get a model that can be detect objects\n\nBut why some open source can export model directly?\n\nFor example : [https://github.com/ultralytics/yolov5/issues/251](https://github.com/ultralytics/yolov5/issues/251)\n\nYou can just use export script to get a model\n\nWhy can output model directly without training by myself first?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ob03q5/why_some_models_can_be_exported_directly/"}, {"autor": "Patrokolos666", "date": "2021-06-30 12:15:32", "content": "How to deal with noisy -----> image !!!  for object detection task /!/ Im currently learning about object detection of sea animals in seafloor image. The image are very large (2k\\*4k)  while the animals are only 40-100px square bounding box. Plus the dataset images are also quite noisy given its underwater images (light attenuation, currents, matters, etc.). So far my approach is to:\n\n[Sample cropped image](https://imgur.com/wx7mXZT)\n\n\\- Divide the image into small tiles (500x500)\n\n\\- Calculate the mean and std of the dataset\n\nI dont know how to deal with the noise of the image or possible models to train the dataset on, any pointers would be very appreciated", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oaw2tg/how_to_deal_with_noisy_image_for_object_detection/"}, {"autor": "OriginalBuddhist", "date": "2021-06-29 18:03:21", "content": "Stuck with Error/Cost value converging toward specific number and learning process /!/   \nHello, I'm kinda stuck with the problem of -----> image !!!  recognition I did in CPP. At some point Error/Cost value converging toward a specific number and the learning process gets stuck (I'm using MNIST) and this is a basic image \"recognition\" neural network, not sure why it gets stuck ;\\_\\_; \n\nNot sure if that's natural behaviour of neural network or bugged code.\n\n  \nCode: [https://github.com/DigitalAssadaFactory/NeuralNetwork/tree/master/NeuralNetwork](https://github.com/DigitalAssadaFactory/NeuralNetwork/tree/master/NeuralNetwork)\n\nhttps://preview.redd.it/4vdqyg3fv8871.png?width=647&amp;format=png&amp;auto=webp&amp;s=acded17799bd85c21cb2d8f72d4161f6383ee928", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oadoor/stuck_with_errorcost_value_converging_toward/"}, {"autor": "InvincibleKnigght", "date": "2021-06-29 13:27:07", "content": "How do I design my model for a particular use case? /!/ **Description** I have a dataset of rocks and positions of two pixels.\n\n**Aim:** I need to classify whether the line joined formed by joining the two given pixels forms an edge between two touching rocks or not. A cropped and rotated to horizontal -----> image !!!  of the line joining the two rocks is also provided for context. \n\nFor example: \n\n&amp;#x200B;\n\n[Pixels: \\[81 138\\] and \\[81 142\\]](https://preview.redd.it/2yhao132g7871.png?width=129&amp;format=png&amp;auto=webp&amp;s=8a8ac097f688e34a7ac26e7130b9452a970478c3)\n\n&amp;#x200B;\n\n[Rotated to horizontal; Edge context \\(IMG\\)](https://preview.redd.it/aukw2qkng7871.png?width=4&amp;format=png&amp;auto=webp&amp;s=1b34a34dd05c83e0c556ca939a7768ed6b22ba75)\n\n&amp;#x200B;\n\nShould be classified as true. \n\n&amp;#x200B;\n\nI am thinking of using a cat and dog classifier type image classifier but I am struggling to think of a way to structure the input. \n\nHow should I give the input to  the model? Thank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oa89i6/how_do_i_design_my_model_for_a_particular_use_case/"}, {"autor": "TrepidationTD", "date": "2021-07-31 13:25:00", "content": "Why do we have multiple layers for Neural Networks? /!/ I am learning deep learning and have so far learned that neural networks work as follows(MNIST):\n\nThe input layers each contain pixels of the -----> image !!! . The x values of the all inputs then get multiplied by the weights and they are added together. This number needs to be between 0 and 1 to activate so we use some sort of activation function like a sigmoid function to constrain the number. Once we get a value like 1, the neuron in the hidden layer activates and then does the same process as above(is the x value in xW in this case \"1\"?) and activates other neurons until we reach an output. Backprop goes through the outputs and finds how each input/weight contributes to loss and tweaks them using gradient descent until we reach an optimum accuracy. \n\n  \nSo, my question is, how does connecting multiple neurons contribute to a better model. For example, how does one neuron activating the next one help the accuracy of the model? Can't we just use a single neuron that computes xW and reach an output? I don't really understand how one neuron activating another neuron in the hidden layer leads to a better model. Is my understanding correct? I completely get that extra layers are used to build more complicated relationships but how? How are these complicated relationships being built? How does one neuron activating relate to the output? Also, once we reach the final layers before the output, how does the neuron that is activated know which output to activate?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ov73m6/why_do_we_have_multiple_layers_for_neural_networks/"}, {"autor": "Successful_Boat_3099", "date": "2021-07-31 11:51:51", "content": "Your daily dose of machine learning /!/ &gt;This is a series of posts that I post almost daily on [LinkedIn](https://www.linkedin.com/in/nour-islam-mokhtari-07b521a5/) and now I am sharing them here as well. I call them \"*your daily dose of machine learning*\". \n\nHere's an ML interview question for you.  \n\n\nWhat is high bias in deep learning and how do you know that your neural network has a high bias?  \n\n\n\u27a1\ufe0f A high bias in deep learning means that your neural network is underfitting the training data. Which means that your neural network is not learning a complex enough model.  \n\n\nAn example of this would be : you want to build a classifier that can separate between 2 classes. These 2 classes look like the -----> image !!!  below.  \n\n\nSo instead of learning a CIRCULAR boundary, your neural network will learn a LINEAR boundary.  \n\n\nEmpirically, if your training accuracy is much lower than human-level accuracy then you have a high bias (or underfitting).  \n\n\nPossible solution? Make your neural network more complex by adding more layers for example.\n\n[\\#deeplearning](https://www.linkedin.com/feed/hashtag/?keywords=deeplearning&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6827200616402571264) [\\#interviewquestions](https://www.linkedin.com/feed/hashtag/?keywords=interviewquestions&amp;highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6827200616402571264)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ov5sbr/your_daily_dose_of_machine_learning/"}, {"autor": "Expert_Teach_346", "date": "2021-07-30 19:40:59", "content": "Recommend techniques for signal processing? /!/ Hi everyone,\n\nI have a signal processing task, I have to perform classification based on a continuous wave of data. Problem is I don't know how to extract features from waves. I'm thinking I could use some wavelet transform to generate an -----> image !!!  off the wave then use a convolutional neural network to identify patterns, or maybe apply a fourier transorm to the wave and just feed that into an RNN like an LSTM. But I don't know if any of that will work as I am completely inexperienced in this domain.\n\nI would appreciate recommendations for good papers involving successful implementation of machine learning to extract features from and classify signals. \n\nThanks.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ous0gt/recommend_techniques_for_signal_processing/"}, {"autor": "20gunasart", "date": "2021-07-30 13:16:35", "content": "Question about training data for making a facial recognition classifier /!/ I have a custom dataset of faces that I am working with, and now I want to make a classifier that can detect a face in an -----> image !!! .\n\nThis is a binary classifier where one class is a face, and the other... is not a face. The confusing thing for me here is that there are so many things that are not a face, so how does one prepare the training data for \u201cnot a face\u201d?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oukehn/question_about_training_data_for_making_a_facial/"}, {"autor": "Successful_Boat_3099", "date": "2021-07-30 09:24:35", "content": "\ud83d\udccc Your daily dose of machine learning! /!/ &gt;This is a series of posts that I post almost daily on [LinkedIn](https://www.linkedin.com/in/nour-islam-mokhtari-07b521a5/) and now I will be sharing them here as well. These are small bits about machine learning topics. I hope you like them!  \n\n\n\ud835\udc02\ud835\udc28\ud835\udc27\ud835\udc2d\ud835\udc2b\ud835\udc1a\ud835\udc2c\ud835\udc2d\ud835\udc22\ud835\udc2f\ud835\udc1e \ud835\udc25\ud835\udc1e\ud835\udc1a\ud835\udc2b\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20 is a self-supervised method for training deep neural networks.  \n\n\nLately this method has shown some potential for building robust computer vision deep learning models.\n\nIn contrastive learning, you basically do this:\n\n1 - You take an -----> image !!!  \ud835\udc7f from your UNLABELED batch.\n\n2 - You do a random augmentation of that image to generate a new image \ud835\udc7f\ud835\udc82.\n\n3 - You form multiple pairs of images (\ud835\udc7f, \ud835\udc7f\ud835\udc82), (\ud835\udc7f, \ud835\udc7f2), (\ud835\udc7f, \ud835\udc7f3)...etc. Where \ud835\udc7f1, \ud835\udc7f2, \ud835\udc7f3, ... are the other images in your batch.\n\n4 - You pass all the images to a convolutional network to get embeddings.\n\n5 - You design a loss that minimizes the distance between the embeddings of \ud835\udc7f and \ud835\udc7f\ud835\udc82 but maximizes the distances between \ud835\udc7f and the rest of the images in your batch. \n\n&amp;#x200B;\n\nFor more details, check these links:\n\nOriginal paper : [https://arxiv.org/abs/2006.10029](https://arxiv.org/abs/2006.10029)  \nBlog article: [https://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607](https://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607) \n\n**You can find me on** [**LinkedIn**](https://www.linkedin.com/in/nour-islam-mokhtari-07b521a5/)**,** [**Medium**](https://nourislam.medium.com/) **and** [**Twitter**](https://twitter.com/NourIslamMo)**.**", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ouhaxj/your_daily_dose_of_machine_learning/"}, {"autor": "Tbiproductions", "date": "2021-07-29 13:16:39", "content": "A couple of questions about my first -----> image !!!  classification program. /!/ Hey guys,\n\nI'm doing an image classification program for my A level project and needed some advice. internet usage is allowed so as long as I don't copy anything asking for help here is fine.\n\n&amp;#x200B;\n\n1.) Does anyone know a good starting point or tutorial as to where to learn how to do this? This is a new area for me. Im planning to use either C# (I have good experience with) or python (the de facto machine learning language) and their relevant Tensorflow versions. I would like to use Keras to make things easier but it seems to be \"too simple\" and I'm worried it won't be enough of my code if that makes sense. Im flexible though, so any suggestions are welcome.\n\n&amp;#x200B;\n\n2.) I'm going to use the ImageNet dataset to train my network, specifically the \"2017 DET dataset\". As expected, the test images are sorted into folders, with each folder given a name (so the folder \"n01662784\" has pictures of turtles). I'm wondering how the program will know what the category names mean in human words? there is an annotations folder, but the XML files don't seem to help much there.  I have access to the full ImageNet site if that helps.\n\n&amp;#x200B;\n\nSorry for the long post, I can't wait to get coding. Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/otxafh/a_couple_of_questions_about_my_first_image/"}, {"autor": "optimistic-reckoner", "date": "2021-07-29 12:29:43", "content": "-----> Image !!!  classification /!/ Hi all, I am doing a research in which I am using  geolocation from pictures extracted from Flickr over a region of interest. I would like to use ML to classify the pictures to know whether it depicts \u00ab\u00a0nature\u00a0\u00bb or not. I have briefly tried to use the Keras package from R a while back but I abandoned, do you think this particular problem is worth trying ? Or is it really advanced and might take a long time with no real results ? Has anyone done something similar ? Thanks a lot !", "link": "https://www.reddit.com/r/learnmachinelearning/comments/otwj4d/image_classification/"}, {"autor": "massivesmoke", "date": "2021-08-31 13:14:48", "content": "regarding fisherface algorithm /!/ so i was seeing fisherface algorithm and something, doesnt seem to people like an important thing, i read but couldnt understand: how does the algorithm makes an -----> image !!!  a bunch of numbers? \n\nlike how the algorithm treats the features as numbers? how is this done?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pf5mc5/regarding_fisherface_algorithm/"}, {"autor": "acgvme", "date": "2021-08-31 00:48:23", "content": "Need some advice - scanning labels without barcodes /!/ I am looking for a push in the right direction on how to solve a business challenge I have.\n\nI need to be able to scan bag labels that do not have any UPC/QR/barcodes at all and perform a lookup on a corresponding record for that label in a database.\n\nSome of the labels are very similar (below) but some are completely different in layout and content. \n\nSince I don't control the label design, I'm hoping there is some way I could use -----> image !!!  recognition to analyze the full label.\n\nSide note: My one true skill in life is to have a problem, imagine some kind of solution and then realize someone else brought it to market at least 18 months ago. I'm hoping someone here will post a link and say \"use this buddy\". :) \n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/79joh88palk71.jpg?width=1082&amp;format=pjpg&amp;auto=webp&amp;s=c30036c7954adadd28fa2cd1c12f148f4e26f47e\n\nhttps://preview.redd.it/2k3koa8palk71.jpg?width=1088&amp;format=pjpg&amp;auto=webp&amp;s=37059d10461eb66d1ad62f5c823fcdc290ec67ea", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pev7t7/need_some_advice_scanning_labels_without_barcodes/"}, {"autor": "numbers222ddd", "date": "2021-08-30 17:20:48", "content": "Piece of code /!/ Do you know the Python code that can measure the height of the human from the -----> picture !!!  ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pemn2x/piece_of_code/"}, {"autor": "Ivan-GameDev", "date": "2021-08-30 15:34:41", "content": "NN that generates an -----> image !!!  from a list of other -----> image !!! s. /!/  \n\nHello!\n\nI started programming for developing games about half-a-year ago. But recently I have got an idea to make one picture from a pack of other images. I've got a pack of 10000 images that I want to make neural network learn from and give me one picture in return. I've found and read a lot of articles about GAN, but understood nothing. Is there a way to do what I want?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pekhd8/nn_that_generates_an_image_from_a_list_of_other/"}, {"autor": "Successful_Boat_3099", "date": "2021-08-30 07:58:20", "content": "\ud83d\udc8aYour daily dose of machine learning : Siamese neural networks /!/ &gt;This is a series of posts that I post almost daily. I call them \u201cyour daily dose of machine learning\u201d. \n\nLearning a similarity between 2 data points can be extremely useful.\n\nImagine a face identification system where there is a -----> camera !!!  that captures your face and then the face recognition system is supposed to identify whether you are allowed to enter a building or not based on a database of faces.\n\nTrying to build such a system using pure classification is not feasible.\n\nWhat you can use instead are Siamese neural networks or SNNs for short.\n\nSiamese neural networks are composed of 2 identical subnetworks that output 2 embeddings. These embeddings are then used as inputs to a loss function.\n\nThis loss function is designed to minimize the distance between similar inputs (2 images of 2 faces that belong to the same person) and maximize the distance between dissimilar inputs (2 faces of 2 different people).\n\nBelow is an example of how a SNN architecture would look like.\n\nFollow me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!*** \n\n&amp;#x200B;\n\n[Siamese Neural Network](https://preview.redd.it/95vz6kowbgk71.png?width=845&amp;format=png&amp;auto=webp&amp;s=d3be38eb87eac3f908fcf5bdce1117f753a5f175)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pedksa/your_daily_dose_of_machine_learning_siamese/"}, {"autor": "Ivan-GameDev", "date": "2021-08-30 06:48:25", "content": "NN that generates an -----> image !!!  from a list of other -----> image !!! s. /!/ Hello!  \n\n\nI started programming for developing games about half-a-year ago. But recently I have got an idea to make one picture from a pack of other images. I've got a pack of 10000 images that I want to make neural network learn from and give me one picture in return. I've found and read a lot of articles about GAN, but understood nothing. Is there a way to do what I want?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pecrcz/nn_that_generates_an_image_from_a_list_of_other/"}, {"autor": "74throwaway", "date": "2021-08-30 00:26:43", "content": "Simplest way to deploy Keras NN model into C++? /!/ I have NN in Keras Python that I was able to train on training images. Now I want to deploy it so that my C++ can use it to perform inference on test -----> image !!! s\n\nThe C++ code just processes 1 -----> image !!!  at a time, so I don't need the inference to do batch processing. I need it to have low-latency, but high throughput is not needed. Also, the C++ code will just run on a computer, so I don't need the NN deployed on mobile/edge/cloud/web\n\nFrom what I know, TensorRT is an option but is more geared towards high throughput and deployment on mobile/edge/cloud/web/etc, and is also difficult to use. \n\nAre there any other options for this? I heard about OpenVINO and WinML, but I don't know anything about those.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pe79jy/simplest_way_to_deploy_keras_nn_model_into_c/"}, {"autor": "AlsetAlokinn", "date": "2021-08-29 20:35:08", "content": "Need Advice on a Product Recognition Project /!/ Hello,\n\nI am new to ML and i desperately need your advice oh old and wise Redditor\n\nSo I'm working on an App that is used by the commercial force of a number of goods manufacturers, the users need to count how many facings their products occupy in a supermarket shelf (say 3 of the same product side by side), and what shelf they are on (1 for the bottom to 6 for the top), they do this by hand and they register it on the App, I'm thinking about adding the following features:\n\n-  take an -----> image !!!  and the App will automatically highlight all the products on the shelves and put them in small rectangles\n- group the instances of the same product and count how many there are\n- the App will identify (if possible) the product\n- if the App cannot identify the product or makes a mistake, the user can tap on the rectangle and scan the barcode of the product\n\nthe data i have available is this\n\n- 1 -----> image !!!  of each product (but i can use this module to eventually collect more)\n- the barcode of each product and other info that is not useful in this context\n\n\nthe question is: where should i start ? \n\nmy rough idea is that i can use TensorFlow to make an object detection module that feeds to OpenCV for identifying each product by comparing it to the images i have.\n\nI thank you so very much for your help and i wish you many blessings", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pe35zs/need_advice_on_a_product_recognition_project/"}, {"autor": "gostar2000", "date": "2021-08-29 16:05:01", "content": "Need help with -----> image !!!  dataset. /!/ Hey, hope everyone's doing well.\n\n I am working on an image classification model that detects whether a person is using mobile phones or not. However, I am having trouble getting enough images to train the model. As of now, I only have about 150 images per category. It would be so helpful if any of you could tell me some places to get some images.  [This is what I'm aiming for.](https://drive.google.com/drive/folders/1vjCWjRh7jv5VCfRUtXqqU5DnfDoLeccY?usp=sharing)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pdy0g6/need_help_with_image_dataset/"}, {"autor": "ObviousFakeLaugh", "date": "2021-10-31 13:42:20", "content": "We are looking for -----> image !!!  specialists to help understand cancer /!/ Hello Reddit! I am with the ORL/IA challenge hosted by Epidemium where we want to try model HPV induced cancers. \n\nWe are in need for data scientists, specifically those with experience or an interest in image analysis. This is an open source project and so, you can choose how much you want to commit. \n\nThe full details for the project can be found here: https://app.jogl.io/challenge/orlia\n\nIdeally, if you want to contribute to a research paper / project and want to work with other domain experts / scientist, this project could be a perfect change to get some experience in. \n\nIf you do decide to join, just go through the link or if you have any issues, message me and I can help out.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qjpr77/we_are_looking_for_image_specialists_to_help/"}, {"autor": "Ok-Peanut-2681", "date": "2021-10-31 10:41:55", "content": "Transformers and Vision Transformers /!/ Hey guys, I wrote a short article on a summary of transformers, particularly vision transformers for -----> image !!!  tasks, and how to use them in PyTorch. Feel free to check it out:\n\n[https://towardsdatascience.com/vision-transformers-in-pytorch-43d13cb7ec7a](https://towardsdatascience.com/vision-transformers-in-pytorch-43d13cb7ec7a)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qjmytx/transformers_and_vision_transformers/"}, {"autor": "mm_maybe", "date": "2021-10-30 13:30:45", "content": "StyleGAN question /!/ Even with datasets that others have purportedly used to train StyleGAN models (e.g. landscapes, anime) dataset\\_tool.py is a huge headache because it so often bumps into one -----> image !!!  (out of hundreds or thousands) that doesn't fit the schema for the dataset for one reason or another and then \\*fails\\*, rather than simply skipping or omitting the -----> image !!!  from the set of tfrecords to be created, providing very little useful information about what went wrong.  Has anybody figured out a way to change this behavior or any pre-processing tricks that can be automated/scripted instead of manual curation?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qj1jdy/stylegan_question/"}, {"autor": "VeryKnave", "date": "2021-10-29 16:13:49", "content": "I don't know how to train a YOLO v3 model with some custom data that is labeled in an unusual form (XML files) /!/ I found [this paper](https://arxiv.org/pdf/2008.05359.pdf) which looks to be state-of-the-art in logo detection. I want to train a YOLO v3 model with the [dataset](https://github.com/Wangjing1551/LogoDet-3K-Dataset#download-dataset-links) used in the paper, but the dataset is labeled [unusually](https://i.imgur.com/6zW9Edm.png).\n\nEach -----> image !!!  has an [XML file](https://i.imgur.com/7PBdKUr.png) associated with it. The XML files have the corresponding labels and bounding boxes, so I can write a script to convert them into [this form](https://user-images.githubusercontent.com/26833433/112467037-d2568c00-8d66-11eb-8796-55402ac0d62f.png), and follow [this tutorial](https://github.com/ultralytics/yolov3/wiki/Train-Custom-Data) on training custom data.\n\nIs there an easier method? I wanted to download [the pre-trained model](https://github.com/Wangjing1551/LogoDet-3K-Dataset#download-pertrained-model-links), but you need to request access to the model. I requested access almost a week ago, but I got no answers.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qigcgu/i_dont_know_how_to_train_a_yolo_v3_model_with/"}, {"autor": "LosLocosKickYourAss", "date": "2021-10-29 00:48:05", "content": "Potentially acquiring a ML company, Questions to ask? /!/ I just got asked to take part in an interview board on a potential acquisition. The company's claim to fame is an iOS app that uses ML to do -----> image !!!  recognition. I'm a EE/CE with a basic high-level understanding of ML (hence, I'm assuming, why I got asked), but am unsure of what some good technical questions to ask the R&amp;D team would be. My job experience is mostly with high-speed optimizations, so I'm feeling a little out of my element here. TIA!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qi0u93/potentially_acquiring_a_ml_company_questions_to/"}, {"autor": "gpahul", "date": "2021-10-28 19:59:45", "content": "State of the art in the document information extraction/parsing for resume parsing? /!/ Hi everyone,\n\n\nI've been looking for state of the art research paper/project/code for automatically extracting information from various layout of resumes.\n\n\nTypical workflow I can estimate is to convert resume to -----> image !!! , detect text, table etc., apply rule based heuristic approach to extract the information based on NER etc. but I think that would be an outdated approach and will not be accurate and feasible enough to cover all the cases.\n\n\nI'd really appreciate if you have could share any information/experience in this\u00a0regard.\n\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qhv8r9/state_of_the_art_in_the_document_information/"}, {"autor": "micky04", "date": "2021-10-28 16:09:43", "content": "How do I introduce data science in &lt;10 hours? /!/ I'm putting together an open-source curriculum/roadmap that aims to give newbies a taste of data science in &lt;10 hours. It should show...\n\n* what data scientists do (e.g. data wrangling, visualization, modelling)\n* the core skills in DS (e.g. programming, stats)\n\nThe goal is to help them find out if DS is a career/skill they want to pursue.\n\nI'm concerned that I will simplify things too much that they don't get an accurate -----> picture !!! , or I make things too complicated and dissuade beginners.\n\nDo you have any suggestions on how to approach this? Any DS resources you recommend?\n\nHere's what I have so far: [https://www.roadmaple.com/roadmap/explore-data-science](https://www.roadmaple.com/roadmap/explore-data-science)\n\nThank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qhqeco/how_do_i_introduce_data_science_in_10_hours/"}, {"autor": "thelaststark01", "date": "2021-10-28 15:16:58", "content": "3D CNN for Spatial-Temporal Image Fusion ? /!/ Hi guys, I'm trying reconstruct a particular spatial-temporal -----> image !!!  fusion architecture (link to paper provided). They haven't provided a correct specification of the network used, just 8 layers of 3D CNN is only mentioned.  How should I start building one. What all are the things I should look for. Has anyone here worked in this domain before. \n\n  \n[https://www.mdpi.com/2072-4292/12/23/3888?type=check\\_update&amp;version=2](https://www.mdpi.com/2072-4292/12/23/3888?type=check_update&amp;version=2)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qhpbid/3d_cnn_for_spatialtemporal_image_fusion/"}, {"autor": "thelaststark01", "date": "2021-10-28 15:16:58", "content": "3D CNN for Spatial-Temporal Image Fusion ? /!/ Hi guys, I'm trying reconstruct a particular spatial-temporal -----> image !!!  fusion architecture (link to paper provided). They haven't provided a correct specification of the network used, just 8 layers of 3D CNN is only mentioned.  How should I start building one. What all are the things I should look for. Has anyone here worked in this domain before. \n\n  \n[https://www.mdpi.com/2072-4292/12/23/3888?type=check\\_update&amp;version=2](https://www.mdpi.com/2072-4292/12/23/3888?type=check_update&amp;version=2)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qhpbh0/3d_cnn_for_spatialtemporal_image_fusion/"}, {"autor": "kbrdsmsh-asdf", "date": "2021-09-30 03:41:46", "content": "What does 'a 768-dimensional feature vector of a histogram of R,G,B channel' mean? /!/ &gt;4.1 Data Collection and Training   \n&gt;  \n&gt;We extract video frames from selected animations and extract the line art images to form our training dataset. **We calculate a 768-dimensional feature vector of histograms of R, G, B channels for each frame.** The difference between frames is determined by calculating the mean square error of the feature vectors, which is used for splitting the source animations into shots...\n\nWhat does the bold part mean? I've found out how to get a histogram of an -----> image !!!  (cv2.calcHist) but I'm not sure if it's relevant information or what I do from here to get this 'feature vector'.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pybnle/what_does_a_768dimensional_feature_vector_of_a/"}, {"autor": "throwawayafw", "date": "2021-09-29 16:31:55", "content": "How do I generate mask of an -----> image !!!  using Run Length encoding from csv files? /!/ I have a dataset which have pixel level annotation. And I need to get the bounding box coordinates by first creating a Run Length Encoding of the image using x,y coordinates of each pixels of area containing the ROI using the csv files of each image.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pxz3zq/how_do_i_generate_mask_of_an_image_using_run/"}, {"autor": "throwawayafw", "date": "2021-09-29 02:20:11", "content": "How do I generate segmentation mask using Run Length Encoding if I have a csv file which have x,y coordinates of each pixels of area of ROI? /!/ I have a dataset which have pixel level annotation. And I need to get the bounding box coordinates by first creating a Run Length Encoding of the -----> image !!!  using x,y coordinates of each pixels of area containing the ROI using the csv files of each -----> image !!! .", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pxlpgd/how_do_i_generate_segmentation_mask_using_run/"}, {"autor": "ISkipLegDayAMA", "date": "2021-02-12 17:27:01", "content": "[Tensorflow] How to apply data augmentation to both images and their ground truth bounding boxes /!/ Hey all,\n\nI'm trying to train a model in tensorflow for doing object classification + localization, so it outputs a class prediction as well as a bounding box of where the object is. I'd like to use some data augmentation in my training pipeline, mainly translation and random flips. The only problem is this then changes where the ground truth bounding boxes would be. Is there a way to apply augmentations to both the -----> image !!!  and the ground truth bounding boxes? I was looking at something like overriding the `__iter__` method in `ImageDataGenerator`, but I couldn't find any good examples online of how to go about this. \n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lifvuk/tensorflow_how_to_apply_data_augmentation_to_both/"}, {"autor": "fromnighttilldawn", "date": "2021-02-10 21:44:55", "content": "Can someone explain the \"ray sampling\" -----> picture !!!  in the neural radiance field paper? /!/ &amp;#x200B;\n\nThis [picture](https://raw.githubusercontent.com/bmild/nerf/master/imgs/pipeline.jpg) is featured prominently in the neural radian field (nerf) paper. [https://www.matthewtancik.com/nerf](https://www.matthewtancik.com/nerf)\n\nHowever, with no disrespect to the authors, this picture is extremely confusing in my opinion. Consider image (a), you are shooting out a ray into a 2D image, and then this generates a sequence of black dots through this 3D \"object\", and then you are taking one of these black dots (x,y,z,theta, phi) and then passing it through a neural network to get a prediction of the color and density (R,G,B,\\\\sigma) at that particular black dot's location. Image (b)\n\nBut this makes no sense because this \"object\" doesn't exist in the first place, it is literally what we are trying to generate, so those black dot cannot exist also. So what does it mean to sampling along the ray that passes through this object as shown in (a)? The only thing you could sample from is your original 2D image, but the ray is shooting through that image to an imaginary object.\n\nCan someone please help me understand this part? I don't know too much about computer graphics so there could be some gaps. But that image is not helping.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lh4rrl/can_someone_explain_the_ray_sampling_picture_in/"}, {"autor": "_DsNinja_", "date": "2021-02-25 09:01:59", "content": "Appropriate model for signature classification /!/ I am building a project on signature recognition that should be able to classify a signature as 'genuine' or 'forged', and I was wondering which kind of -----> image !!!  recognition model to use? My dataset (that I already normalized by binarizing and cropping the pictures) is quite limited, with five genuine and five forged pictures for the signatures of 30 people (dataset available on [kaggle](https://www.kaggle.com/divyanshrai/handwritten-signatures)). Ideally, the model would give a percentage of certainty about the authenticity of the signature.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ls2byr/appropriate_model_for_signature_classification/"}, {"autor": "patdata", "date": "2021-02-24 15:13:55", "content": "How to track data-drift in production for -----> image !!!  data? /!/ Over a period of time a deeplearning model obviously becomes stale and needs to be retrained as the distribution of images that it was trained on changes over time. But is there any way do track this change in distribution of images so that i can quickly retrain the model without having to manually look at results. We have an object detection model currently deployed in production and we have few hundred new images coming in every week but we anticipate it to increase to few thousands in the near future and obviously this becomes impossible to track confident false positives until its too late.\n\nI have tried reading up some articles on online on model-drift etc but they all talk about tabular data but none about image data. I remember reading some where that unsupervised methods like VAEs can be used to find reconstruction errors and using this error we can track any drift in data but i havent been to track that article down nor have i was able to find a good research publication which has tackled this problem.\n\nCan some one help me to how to approach this problem of data-drift, model-drift detection? Thanks in advance", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lrf5wj/how_to_track_datadrift_in_production_for_image/"}, {"autor": "PytonRzeczny", "date": "2021-02-24 13:31:53", "content": "ResNet zero padding shortcut /!/ Hi.\nIm implementing ResNet with 20 layers, for cifar- 10 classification.\nIm stuck at zero padding shortcut, for example, when -----> image !!!  size is downsampled from 32x32 to 16x 16 i dont know how to transform input feature maps to match output size.\nMaxPooling maybe?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lrcydl/resnet_zero_padding_shortcut/"}, {"autor": "nopickles_", "date": "2021-02-24 11:21:56", "content": "How exactly do Convnets detect bounding boxes via sliding windows? /!/ To my understanding the sliding windows object localization method doesn't include actually training the network to detect bounding boxes, instead we train a classification model on objects of interest (e.g.: cats and dogs), and during inference we crop the input -----> image !!!  into grids and pass them one by one to classification model (given that the inference -----> image !!! s are larger than the ones the network was trained one). So if 1) at one grid the network classified it as containing the object how do we obtain the bounding box itself to be returned? 2) If the object itself spans multiple grids say 3 next to each other, how does that translate to the prediction?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lraio7/how_exactly_do_convnets_detect_bounding_boxes_via/"}, {"autor": "borrowedbirch", "date": "2021-02-24 08:22:06", "content": "Are object detection models postiion independent? /!/ While training an object detection model, does the position of the bounding boxes in the training dataset really matter? For example, if we are training a cat detection model, and my training dataset has cats in the bottom right corner. Will my model be able to detect a cat which is in the top left corner of an -----> image !!! ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lr7k1e/are_object_detection_models_postiion_independent/"}, {"autor": "m1900kang2", "date": "2021-02-24 03:07:21", "content": "[R] SynDistNet: Self-Supervised Monocular Fisheye Camera Distance Estimation Synergized with Semantic Segmentation for Autonomous Driving /!/ This paper from the Workshop on Applications of Computer Vision Conference (WACV 2021) that showcases a new method on KITTI using a pinhole model.\n\n\\[[4-Minute Paper Video](https://crossminds.ai/video/SynDistNet-Self-Supervised-Monocular-Fisheye------> Camera !!! -Distance-Estimation-Synergized-with-Semantic-Segmentation-for-Autonomous-Driving-60102b3b6b1b6888a6332a64/)\\] \\[[arXiv Link](https://arxiv.org/abs/2008.04017)\\]\n\n**Abstract:** State-of-the-art self-supervised learning approaches for monocular depth estimation usually suffer from scale ambiguity. They do not generalize well when applied on distance estimation for complex projection models such as in fisheye and omnidirectional cameras. This paper introduces a novel multi-task learning strategy to improve self-supervised monocular distance estimation on fisheye and pinhole camera images. Our contribution to this work is threefold: Firstly, we introduce a novel distance estimation network architecture using a self-attention based encoder coupled with robust semantic feature guidance to the decoder that can be trained in a one-stage fashion. Secondly, we integrate a generalized robust loss function, which improves performance significantly while removing the need for hyperparameter tuning with the reprojection loss. Finally, we reduce the artifacts caused by dynamic objects violating static world assumptions using a semantic masking strategy. We significantly improve upon the RMSE of previous work on fisheye by 25% reduction in RMSE. As there is little work on fisheye cameras, we evaluated the proposed method on KITTI using a pinhole model. We achieved state-of-the-art performance among self-supervised methods without requiring an external scale estimation.\n\n[Example of the new model](https://preview.redd.it/sykg2cbjdcj61.png?width=914&amp;format=png&amp;auto=webp&amp;s=f226dc14c8522dcc5d715924e89f284c98808a8b)\n\n**Authors:** Varun Ravi Kumar, Marvin Klingner, Senthil Yogamani, Stefan Milz, Tim Fingscheidt, Patrick Maeder (Valeo DAR, Valeo Vision Systems, Technische Universitat Braunschweig, Technische Universitat Ilmenau)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lr23ub/r_syndistnet_selfsupervised_monocular_fisheye/"}, {"autor": "nvidia-data-science", "date": "2021-03-16 14:59:13", "content": "Embed Your SQL Query Into Your Python Code and Let It Rip on a GPU - Data Science of the Day /!/ *Okay! Thanks to /u/*[*themajesticcalf*](https://www.reddit.com/user/themajesticcalf) *for pointing out the google doc to me on the original post. This community is great, and thank you all for not doing crazy things to it.*\n\nBack to the original post. It's great to get as much information and resources possible out to our community. Here at Nvidia, a cool thing we do is \"Data Science of the Day\" (DSotD). It's tips and information from subject matter experts to better equip anyone doing data science/machine learning to perform their job to the maximum potential.\n\nToday's DSotD is about embedding SQL code into python to query tables at blazing speeds, with the help of GPUs. Click the link (and then the -----> picture !!! ) to check out the blog post on how to perform this task. [Embed SQL into Python for Blazing Speeds](https://forums.developer.nvidia.com/t/embed-your-sql-query-into-your-python-code-and-let-it-rip-on-a-gpu/170506?u=kasmith) .\n\nAlso, don't forget to register for our **FREE** conference, [GTC 2021 FREE REGISTRATION](https://www.nvidia.com/en-us/gtc/?ncid=GTCS21-NVKASMITH) , to have access to a week of great talks and presentations that show real world scenarios/applications where data science/machine learning skills like these above can help solve complicated problems.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m6ayuu/embed_your_sql_query_into_your_python_code_and/"}, {"autor": "jloganolson", "date": "2021-03-16 14:10:34", "content": "Differentiable augmentation /!/ So I'm learning about StyleGAN2-ADA and it seems like the big win is augmenting the generated images in a way that is differentiable. So that made me ask: Were -----> image !!!  augmentations not differentiable before this (or I guess the original NeurIPS paper that introduced the technique?). Or is there a bigger gap in my knowledge around operations that are versus are not differentiable in ML (augementation or otherwise)?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m69x8c/differentiable_augmentation/"}, {"autor": "surgavur", "date": "2021-03-16 12:02:02", "content": "Overrepresented object in my object detection dataset /!/ I am trying to build a car type detector (sedan, van, etc) using a state of the art object detection algorithm (YOLO). For that, I want to train with my own images of a parking lot.\n\nThe problem is that a there is always this same car parked, which appears in all the images of my training set. I obviously can't avoid annotating this car because I don't want the network to learn that such car is part of the background, since it belongs to one car type category. However annotating it in every -----> image !!!  will lead to an unbalanced class. This car is parked every day in different spots, so I can't crop the image. Also I can't move the camera from its location since I am not the owner of it.\n\nHow can I mitigate this problem?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m67gzk/overrepresented_object_in_my_object_detection/"}, {"autor": "GramSamantha", "date": "2021-03-16 06:22:49", "content": "Trying to understand output from this convolution filter /!/ I'm experimenting with Swift's MPSCNN library to understand how it works. I understand the theory well enough, like, I get kernels and pooling and that stuff. But the output I'm getting leaves me wondering whether I'm mis-formatting my input.\n\nI have a 2x2 \"-----> image !!! \": an array of 4 single-precision floating-point (Swift just calls them `Float`). Then I have 4 `Float`s for weights, and 4 `Float`s of output. If I set all the inputs and weights to 1, then I get output like:\n\n    4 2\n    2 1\n\nSame kind of thing no matter the size of the \"image\". A 5x5 gives me\n\n    25 20 15 10  5\n    20 16 12  8  4\n    15 12  9  6  3\n    10  8  6  4  2\n     5  4  3  2  1\n\nI've looked around a bit for some information on this pattern, but nothing. Apple's doc is famously horrid, and no one on StackExchange seems to know what to do with it. I've tried black-boxing it with varying weights and inputs to see what the formula is, but the pattern is impenetrable. It's not really that important, except on the possibility that I'm formatting the data incorrectly, or misinterpreting it in some way. Basically I'm trying to figure out whether this is the right behavior, and then just for nerdy curiosity what this operation is called and what purpose it serves in neural nets. Cheers", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m62my8/trying_to_understand_output_from_this_convolution/"}, {"autor": "tylersuard", "date": "2021-03-16 05:11:57", "content": "We need an easy, free way to convert -----> image !!!  annotations back and forth, OR we all just need to agree to one -----> image !!!  annotation format. /!/ It's a pain to convert between Pascal VOC and Coco, and it's a pain to convert between Unity Perception and anything else.  Like the infomercials say, \"THERE'S GOTTA BE A BETTER WAY!\"", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m61mt4/we_need_an_easy_free_way_to_convert_image/"}, {"autor": "rockyrey_w", "date": "2021-03-16 04:41:18", "content": "[N] Yann LeCun Team's Barlow Twins Method Boosts SSL in Image Representation via Redundancy Reduction /!/ Yann LeCun and a team of researchers propose Barlow Twins, a method that learns self-supervised representations through a joint embedding of distorted images, with an objective function that can make the embedding vectors almost identical while reducing redundancy between their components.\n\nHere is a quick read: [Yann LeCun Team's Barlow Twins Method Boosts SSL in Image Representation via Redundancy Reduction](https://syncedreview.com/2021/03/15/yann-lecun-teams-barlow-twins-method-boosts-ssl-in------> image !!! -representation-via-redundancy-reduction/)\n\nThe paper *Barlow Twins: Self-Supervised Learning via Redundancy Reduction* is on [arXiv](https://arxiv.org/pdf/2103.03230.pdf).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m615ja/n_yann_lecun_teams_barlow_twins_method_boosts_ssl/"}, {"autor": "techsucker", "date": "2021-03-16 04:16:08", "content": "Visualizing CNN Models Through Gradient Weighted Class Activation Mappings /!/ Convolutional Neural Networks(CNNs) and other deep learning networks have enabled extraordinary breakthroughs in computer vision tasks from -----> image !!!  classification to object detection, semantic segmentation, -----> image !!!  captioning, and many more. While these networks provide superior results, their lack of intuitiveness and understandability makes them hard to interpret. Consequently, a deep learning model is treated as a black box. Often, there is no reasonable idea of where the network is looking in the input image, Which series of neurons activate in the forward-pass and, How the network arrived at its final output.\n\nTo move towards the successful integration of deep learning models in our daily lives, it is essential to build trust in them and make them more transparent. This goal of model transparency can be achieved by explaining why the models predict what they predict. *The purpose of transparency and explanations is to identify the failure mode, establish appropriate trust and confidence in users, and most importantly teaching humans how to make better decisions.*\n\nFull Tutorial: https://www.marktechpost.com/2021/03/15/visualizing-cnn-models-through-gradient-weighted-class-activation-mappings/", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m60rmb/visualizing_cnn_models_through_gradient_weighted/"}, {"autor": "SuperAlkalinedroid", "date": "2021-03-15 16:40:07", "content": "Start Learning maths for Machine Learning /!/ Hey guys, I am kinda new to machine learning. I have used and understood several basic algorithms like Neural Networks, KNN, K- Means, classifier (Tree/ RandomForest) / Linear Regression and Logistic Regression. I want to be able to understand the maths more deeply. As of now, Ijust know how they work and how maths in them works. What are some good resources to learn maths from now so that i can have a clear -----> picture !!!  of how deep learning works as i want to step into it now..\n\nPlease Do Recommend and give suggestion son how to move forward at his Stage.\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m5otsg/start_learning_maths_for_machine_learning/"}, {"autor": "PrudenceIndeed", "date": "2021-03-15 01:53:57", "content": "Need help with my first deep learning project /!/ I'm trying to predict political affiliations based off of images alone. I think a CNN is best for this task. I want to use my GPU (Mesa Intel\u00ae Xe Graphics (TGL GT2)), is it strong enough? Or should I look for a cloud solution?\n\nI have my set of images (~2.5k total), but I want to store and use them at S3. Not only that, I want to label them by storing everything about the -----> image !!!  in a database (name of politician in -----> image !!! , political affiliation). It looks like a lot of the time, people label by putting them in a separate folder. Can I do things my way?\n\nAlso, after viewing a thread at /r/machinelearning and investigating some things for myself, I am deciding to use PyTorch instead of Tensorflow, as it seems to be preferred by the overwhelming majority.\n\nCan someone give me some pointers? My machine learning knowledge is not deep. I finished one course (the first course) from Andrew Ng's deep learning specialization, and got to Week 4 of Andrew Ng's famous ML class. \n\nAny help is appreciated. Thanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m59zef/need_help_with_my_first_deep_learning_project/"}, {"autor": "Ojash4", "date": "2021-03-30 14:04:36", "content": "Pairing 2 images in a dataset /!/ I am trying to create a dataset to be used in pytorch in which one -----> image !!!  is the ground truth and another -----> image !!!  is the expected outcome for a CNN model. My question is how to proceed with it? I have some idea that I have to implement a class but apart from that I have no idea how to tell it which images are the label.\n\nI am using the S7 ISP Dataset from Kaggle in which short exposure image is ground truth and through upscaling I am trying to make it close to medium exposure.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mggsnk/pairing_2_images_in_a_dataset/"}, {"autor": "0utrageou", "date": "2021-03-29 22:50:45", "content": "I built a custom -----> image !!!  set using model-assisted labeling (and you can too) /!/ I wanted to apply machine learning to images, but I was lost. I saw a post about Roboflow on here a few months ago, and I started with their YOLOv5 tutorial. (For a ML n00b, Roboflow was EXACTLY what I needed to get off the ground.)\n\nShortly after, I felt ready to start collecting images for my dataset. I took some pictures and began annotating images. And I\u2019ll say it: annotating images sucks! I wouldn\u2019t wish this tedium on anyone.\n\nSo I reworked the YOLOv5 tutorial to develop a model-assisted labeling feature. I\u2019m sharing my project here: https://vision.philbrockman.com/. \n\nThe model didn\u2019t become super-helpful until around 100 images, but it really started shining with around 200 labeled images. I\u2019m sharing 841 labeled images and 600 unlabeled images in the GitHub linked to the project.\n\nLet me know if this was useful!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mg1rmq/i_built_a_custom_image_set_using_modelassisted/"}, {"autor": "saikjuan", "date": "2021-03-29 21:52:45", "content": "Text Generation with Attention Mechanism? /!/ Hello everyone!\n\nI'm working on a Uni project in which we want to create a meme text based on the identifier of an -----> image !!! . \n\nMy idea was to use an LSTM model to do a prediction character by character. But when I presented this to my professor, he said that attention mechanisms are best suited for this task. \n\nI haven't found anything related to attention mechanisms for text generation. Do you know any? Is it possible to generate text based on a single input using attention mechanisms? \n\nThank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mg0ki7/text_generation_with_attention_mechanism/"}, {"autor": "shinysamurzl", "date": "2021-03-29 17:14:29", "content": "Big Images as Data /!/ Hey, i have data of 1280x720 images. I wanna first extract features and then use the features in a Generator to generate a certain -----> image !!! . I have a CNN with 4 layers first, and then Fullyconnected for the generator. Some of you might already cringe, because as I learned, having even the last FC Layer go from 10000 or something to the 2.764.800 that is a 1280x720 image take a lot of memory that I and probably no one has. So how do generator like style-gan or other manage to output such big images ? Am I missing something ? Is there a better way to design the Generator ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mfudt5/big_images_as_data/"}, {"autor": "jerschneid", "date": "2021-03-29 16:59:05", "content": "-----> Image !!!  classification with machine learning in minutes", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mfu0yh/image_classification_with_machine_learning_in/"}, {"autor": "Snoo51532", "date": "2021-03-29 06:57:00", "content": "How do I start? /!/  Hi, I am a complete beginner to ML and I found a course online by Andrew Ng. He is using Octave which I don't have any problem with as I have previously worked with MATLAB. But if I wanted to implement the algorithms and concepts in other language (say Python), would I face any difficulty? Or should I learn ML using that particular language? Also, I want to make hardware projects which involve embedded system, -----> image !!!  processing and signal processing (E.g. Face recognition based locking system). Can someone please tell me how do I do that as well?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mfjohu/how_do_i_start/"}, {"autor": "podcast_frog3817", "date": "2021-03-29 01:36:31", "content": "How do i isolate my decoder layer in an autoencoder? /!/ I just followed this tutorial here:\n\n\n\nand i would like to make a modification where I expose the 'decoder' portion of the autoencoder so i can manually pass the 'bottleneck' data into only the decoder portion of the model and get a result.\n\n\nhere is my code so far:\n\n    ### encoder\n    BOTTLENECK_SIZE = 32\n    \n    encoder_input = keras.Input(shape=(28,28,1), name='img')\n    x = keras.layers.Flatten()(encoder_input) #flatten input to 784 (28x28)\n    encoder_output = keras.layers.Dense(BOTTLENECK_SIZE,activation='relu', name=\"encoder_output\")(x)\n    \n    #everything from encoder_input to encoder_output\n    encoder = keras.Model(encoder_input, encoder_output, name='encoder')\n    \n    ##### decoder\n    decoder_input = keras.layers.Dense(784, activation='relu', name='decoder_input')(encoder_output)\n    decoder_output = keras.layers.Reshape((28,28,1))(decoder_input)\n    \n    ### wont work here, wants an input layer :(\n    #decoder = keras.Model(encoder_output, decoder_output, name='autoencoder')\n    #eerything from encoder_input to decoder_output\n    autoencoder = keras.Model(encoder_input, decoder_output, name='autoencoder')\n    autoencoder.summary()\n    \n    #try to compress same -----> image !!!  down to 64 features, then inflate it again\n    # x_train mapping to.. x_train '\n    opt = keras.optimizers.Adam(lr=0.001, decay=1e-6)\n    autoencoder.compile(opt, loss='mse')\n    autoencoder.fit(x_train, x_train, epochs=3, batch_size = 32, validation_split=0.1)    \n\n\n\neverything works except for me trying to get this line to work\n(assuming i uncomment it)\n\n    #decoder = keras.Model(encoder_output, decoder_output, name='autoencoder')\n\n\nideally i would like to pass in some 'noise' on the bottleneck layer and have it try to reconstruct it as best as it can recognize it. e.g.\n\n    \n    noise = np.random.normal(0, .6, original.shape)    \n    dec_out = decoder.predict([noise.reshape((-1, 8,4,1))])[0]\n    plt.imshow(dec_out, cmap='gray')", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mfewva/how_do_i_isolate_my_decoder_layer_in_an/"}, {"autor": "iRove108", "date": "2021-03-29 00:14:55", "content": "Can dropout negatively impact performance by increasing repetition? /!/ Dropout is the idea that you can drop, i.e set to zero, some of the nodes in a computational neural network. The goal of this is to increase regularization by preventing the model from relying too much on a few overfitted features.\n\nHowever, this seems to raise the possibility of the neural network learning the same features multiple times as backups, in case one of them is set to zero. Here's an extremely over-idealized scenario:\n\nIn an -----> image !!!  recognition neural network, one very important feature may be a wheel, because it strongly suggests an automobile. When dropout (with 50% chance) is applied, this \"wheel\" feature is dropped 50% of the time. However, instead of causing the neural network to regularize, the neural network creates a second copy of the \"wheel\" feature, just in case one of them is dropped. In fact, maybe the neural network could relearn this \"wheel\" feature 3 or 4 times, just in case.\n\nDoesn't this reduce the effectiveness of the neural network by essentially wasting nodes by learning the same features multiple times, instead of learning many different useful features? Or is this not really a concern in practice?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mfdj78/can_dropout_negatively_impact_performance_by/"}, {"autor": "Random-Learning", "date": "2021-03-28 18:45:37", "content": "Using OCR + -----> Image !!!  segmentation for text information retrieval /!/  Hi,\n\nI'm working on information retrieval from documents such as invoices where the goal is to extract elements such as the date of the invoice, amounts, sender/receiver, reference etc ...\n\nMy dataset is comprised of dozens of thousands of **various type of invoices.** We can considered it labelled with a reasonable accuracy. I work with mainly scanned documents so the first step is to use OCR to retrieve words and numbers, as well as their position.\n\nUsing a bit of hand-engineered features and some basic NLP techniques to build a dictionary of keywords, I was able to come up with some interesting results using gradient boosting and treating it as a classification problem bounding-box-wise (accuracy of 90+% on most fields, with similar recall and precision).\n\nHowever, because of the nature of the feature engineering, a lot of context information is lost, so I would like to try and see if a deep-learning base model could achieve better results.\n\nMy idea is the following :\n\n1. Run an OCR on the image\n2. Use a dictionary of keywords (for instance, \"date\",\"reference\", \"invoice\",\"total\" etc ...) to encode a custom image. I was thinking of using one channel for each keyword; 1 for all the pixels contained in the bounding boxes where the OCR found said keyword and 0 for the rest. Since my dictionary is around 300 words, I will end up with an image with 300 channels. I also figured why not add one channel for the actual image that might contain information such as lines, background colors etc ... On top of that, I also add a few channels with information such as \"Does the bounding box contains a date, an amount, how many digits, how many alpha etc ...\"\n3. Consider the problem as a supervised image segmentation. I feed the network the custom image over 300+ channels, as well as the target that is also a custom image with each channel representing each field of interest (using the same encoding strategy as for the keywords)\n\nI'm familiar with how CNN works, but I'm by no means an expert. I've used a classic U-net architecture and tried a couple of loss functions (dice loss and focal loss considering the highly imbalanced nature of the problem). I have ***some*** results, but it's definitely not comparable to the good old gradient boosting.\n\nQuestion 1 : Am I too optimistic on making this work with a CNN or are there any alterations I can make to achieve similar if not better results than the gradient boosting ?\n\nQuestion 2 : I tried overfitting my network with a small training dataset and letting it runs hundreds of epochs. It seems to stop learning after around 50 epochs and while visually, it seems to have learned some stuff, it's definitely not near perfect. How come ? Not complex enough architecture ? Inappropriate loss function ?\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mf72fh/using_ocr_image_segmentation_for_text_information/"}, {"autor": "Suzzy67", "date": "2021-03-28 18:39:47", "content": "Improve accuracy of pretrained model /!/ I m using transfer learning..in which i use peetrained model of CNN...i have -----> image !!!  dataset that have 4 classes..but my modet inception resnet v2 does not give good valid accuracy..how can i improve it...total amount of data is 10,000 and it is properly preprocessed.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mf6y5h/improve_accuracy_of_pretrained_model/"}, {"autor": "madara_x13", "date": "2021-01-14 16:24:42", "content": "Credit Card digit segmentation. /!/ Hi, I want to do a project on \"Segmentation of digits as binary -----> image !!! s from an -----> image !!!  of a credit/debit card. I have gone through  \"Credit card and OCR with Opencv and Python\" blog post on pyimagesearch, but it's too complex than what I'm trying to do. Can anyone help me with how to approach this project.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kx8x16/credit_card_digit_segmentation/"}, {"autor": "invinciblent", "date": "2021-01-14 15:39:11", "content": "Would it be better to build a PC using used parts or buying a used Laptop in under $400 budget for machine learning? Would I be able to run neural networks? /!/ I'm a beginner to machine learning (but cant afford top tier system atm) and my current laptop is quite old, i.e. i5 3rd Gen. Although I have worked on small datasets and models using this system but I highly doubt it will be able to handle neural networks, CNN to be exact as I'll be working on a neural network model which would involve lots of -----> image !!!  processing.  \nI'm personally in favor of building a PC using used parts like Nvidia's 10 series gpu, intel's 7th gen processor etc. But your suggestions would be highly appreciated. :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kx8006/would_it_be_better_to_build_a_pc_using_used_parts/"}, {"autor": "ChemEngandTripHop", "date": "2021-01-14 14:18:52", "content": "Weights Layer as Part of CNN Input? /!/ Hi everyone!\n\nI'm currently developing a computer vision model where I have an additional layer that acts as a weighting for the important areas of the -----> image !!! . The implementation was relatively simple as I can just treat it as a separate layer of the inputted stacked image. \n\nTo explain the approach in a more concrete example I could be trying to re-label cars in a video more accurately - and want to include an additional layer to the input image that includes prior estimates for the locations with the associated confidence of that prediction (from 0-1). \n\nThis approach has improved my results but I want to see if I can improve it further and compare to models that have tried something similar. My question is whether anyone knows of example papers or projects that have tried to do something similar? Any help is much appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kx6gzq/weights_layer_as_part_of_cnn_input/"}, {"autor": "MundaneAdvice7979", "date": "2021-01-14 14:04:56", "content": "OCR - Database - Dashboard Integration? /!/  Hi everyone!\n\nIs it possible to use \"no code\" to build a website/app that takes a -----> picture !!!  of a table in a sheet of paper, recognizes the information, uploads it to a database, and then displays the information for the user?\n\nSample case: multiple purchase orders from the same supplier are in paper and I want to transfer that information online so I can see some metrics (total spend, items purchased, etc).\n\nIs this something that can be done with no code and is there somewhere you should recommend I start from? TIA.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kx6882/ocr_database_dashboard_integration/"}, {"autor": "123space321", "date": "2021-01-14 10:55:59", "content": "What to do when my data set just isn't good? /!/ I've been stuck on a machine learning project for a while now. And spent over a month trying to tweak and improve my model for quite some time. But now I think that my dataset is just nnot good enough for this type of task.\n\nHow did I arrive here?\n\nMy validation losses were almost constant while my training set learned over epochs. Even my losses spiked like crazy. For a long time, I thought this was a result of my model training poorly, but I really think its my dataset that isn't suited for the task.\n\nThe dataset is a time series of sales where most columns are 0 (each column is a month) and there are some crazy outliers like 2500 too.\n\nBut by and large, most values are 0 with a mean around zero and a standard deviation of 5.\n\nSince my last column/target value/training labels are just zeros with outliers, I think my model can safely label most thigs around there and do well.\n\nAs a result, my MSEs are very very constant. (Much like a model predicting every -----> image !!!  is a dog when 80% of my set is dogs and only 20% are cats). That could also expain \n\nThose spikes in my dataset would then also my spikes in losses.\n\nAt the same time, I don't know if I am right or wrong and could do with someone's take on all this", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kx3h9x/what_to_do_when_my_data_set_just_isnt_good/"}, {"autor": "goolisa23", "date": "2021-01-14 08:39:00", "content": "ISA vision challenge (-----> image !!!  similarity) /!/ hi,\n\nThere is a vision challenge in the Shabak Challenge 2021: \n\n*\"* \n\n*During your research on image similarity methods, you discover a corpus made of images, only to find out that the original images were deleted by their creator and only their 512D feature vector was saved. You are now provided with a single such binary vector (a pickled numpy array), which was created from a ResNet18 model (PyTorch), pre-trained on the ImageNet corpus.*\n\n***Your task is to identify the object that appeared in the original image.\"***\n\n&amp;#x200B;\n\nI try to understand if the way I tried is the right one (though I didn't find the right answer).\n\nI used pretrained resnet18, removed all of the layer except the last fc (512,1000),\n\nthen I feed the last layer with given feature vector, but I get wrong prediction (according to challenge website).\n\n&amp;#x200B;\n\nI'll be happy to hear your thoughts", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kx1t3c/isa_vision_challenge_image_similarity/"}, {"autor": "egis123cool", "date": "2021-01-14 07:35:48", "content": "Approximately converting small text from a -----> picture !!!  into normal size text? /!/ Is it possible to figure out approximately what text is written if we train a text recognition model to recognize text in the image even if the text is too small for a person to understand?\n\nAn example image:\n\n[Small text image](https://preview.redd.it/an93lec449b61.png?width=197&amp;format=png&amp;auto=webp&amp;s=c973b9683d76367abec923f02c9bbaf2828ab768)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kx11ep/approximately_converting_small_text_from_a/"}, {"autor": "ifelsestatement007", "date": "2021-01-13 22:53:32", "content": "-----> Image !!!  classification with PyTorch tutorials for beginners /!/ [https://www.youtube.com/watch?list=PL3Dh\\_99BJkCEhE7Ri8W6aijiEqm3ZoGRq&amp;v=Bkw-boJQz9s&amp;feature=emb\\_title](https://www.youtube.com/watch?list=PL3Dh_99BJkCEhE7Ri8W6aijiEqm3ZoGRq&amp;v=Bkw-boJQz9s&amp;feature=emb_title)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kws5dh/image_classification_with_pytorch_tutorials_for/"}, {"autor": "whitejupiter", "date": "2021-01-13 17:26:42", "content": "Hi, iam newbie for this field could you guys help me a little bit /!/ i just want to create function that put the -----> picture !!!  into website and using machine learning detect what a -----> picture !!!  can upload or not (specific -----> picture !!! ) and show information in pictute like a dashboard , is it possible ?? and what module i should use \nthanks a lot for your suggestions \ud83c\udf40", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kwl7og/hi_iam_newbie_for_this_field_could_you_guys_help/"}, {"autor": "Orio_n", "date": "2021-01-13 10:02:07", "content": "How are datasets for optical character recognition models labelled? /!/ Im new to machine learning (emphasis on new) and recently wrote a basic MNIST classifier. I can understand how the mnist CNN figures out how \"inaccurate\"  it is. Each -----> image !!!  has its own label for example the -----> image !!!  of a 2 is labelled 2 so the output layer will have 10 outputs and ideally you want the third one (counting from 0) to have the greatest activation. But how are training images for OCR models labelled? \n\nLets say I have millions of background images each with varying types of text overlayed on top of them. How exactly would I go about labelling these images to figure out how inaccurate my output is? For example if i had an image of a stop sign and I wanted the model to extract the word \"stop\" from that image, I would label that image with the text \"stop\" and have a single output neuron correspond just to that word. But this gets complicated very quickly I would need equivalently millions of different output neurons each corresponding to the unique text in those images. The other way of doing it would be to have 26 output neurons each corresponding to recognizing one of the letters of the alphabet. But this model would not be able to extract text from images, it would only be able to tell you what characters are present in it.\n\nSo 1) how exactly is the dataset of an OCR model labelled, 2) how do we determine how \"inaccurate\" this OCR model how would you calculate how far off the model is from the label. 3) What does the output layer look like? how do we arrange the neurons to specifically pull out chunks of text from an image rather than just figuring out if a certain character is in that image or not? I dont think we can have each  neuron correspond to a word and light up if that word is detected. There are lots of words and text combinations in our language besides the output layer would not be able to figure out which order those words were in. \n\nAfter typing this all out I think Im confusing image classifiers with optical character recognizers this doesnt bring any new insight onto my questions however", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kwdi3m/how_are_datasets_for_optical_character/"}, {"autor": "THE_REAL_ODB", "date": "2021-01-28 13:55:03", "content": "[Active Learning]Confused about retrieving uncertainty scores from an object detection model /!/ Hello, I have been trying to figure out how to retrieve informativeness or uncertainty scores from images in an object detection model.\n\nGetting a uncertainty score from an -----> image !!!  in classification model is relatively straight forward, but i'm confused about getting the score from an object detection model.\n\nThe most straightforward way seems to be training a separate classification model on cropped images of objects from a object detection model and average the uncertainty scores in an image depending on the number of object in the picture.\n\nBut this seems like a lot of work and I was wondering if I could somehow derive these scores within the object detection model without training a separate classification model.\n\nthanks in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l6x5y1/active_learningconfused_about_retrieving/"}, {"autor": "AdSpiritual7858", "date": "2021-01-28 04:35:18", "content": "New-ish to ML, trying to implement a pretrained model.. /!/ Hello.\n\nWell.. in short, what i'm attempting to do is replace the BIGGAN used in this text to -----> image !!!  generation project (ranked with OpenAI's CLIP -&gt; [https://colab.research.google.com/drive/1MEWKbm-driRNF8PrU7ogS5o3se-ePyPb?usp=sharing](https://colab.research.google.com/drive/1MEWKbm-driRNF8PrU7ogS5o3se-ePyPb?usp=sharing)) with a more efficient available model trained on imagenet or something similar.. (many many options) I don't know where to start. I understand Python and the very basics of ML, but I think i'm underestimating how difficult it is to implement a new model into something like this. maybe someone with a bit more knowledge on this subject would be able to help.\n\n  \n\nDoesn't torchvision include a large variety of pretrained models?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l6o2kn/newish_to_ml_trying_to_implement_a_pretrained/"}, {"autor": "nemoland4", "date": "2021-01-28 02:45:11", "content": "[Fastai] I can't load the -----> image !!!  data /!/ I've tried to use my own datasets  but the error kept on coming up \n\nWhat could possibly go wrong here? What is \"NoneType\"?\n\n\\#The code\n\n`dls = ImageDataLoaders.from_name_func(`\n\n`mixed_path, get_image_files(mixed_path), valid_pct=0.05, seed=420,`\n\n`label_func=GetLabel, item_tfms=Resize(256))`\n\n`dls.train.show_batch()`\n\n&amp;#x200B;\n\n\\#That's the error\n\n&amp;#x200B;\n\n`TypeError                                 Traceback (most recent call last)`\n\n`&lt;ipython-input-55-b2c343dc8309&gt; in &lt;module&gt;()`\n\n`1 dls = ImageDataLoaders.from_name_func(`\n\n`2     mixed_path, get_image_files(mixed_path), valid_pct=0.05, seed=420,`\n\n`----&gt; 3     label_func=GetLabel, item_tfms=Resize(256))`\n\n`4` \n\n`5 dls.train.show_batch()`\n\n&amp;#x200B;\n\n`9 frames`\n\n`/usr/local/lib/python3.6/dist-packages/fastai/data/core.py in setup(self, train_setup)`\n\n`258                 x = f(x)`\n\n`259             self.types.append(type(x))`\n\n`--&gt; 260         types = L(t if is_listy(t) else [t] for t in self.types).concat().unique()`\n\n`261         self.pretty_types = '\\n'.join([f'  - {t}' for t in types])`\n\n`262` \n\n&amp;#x200B;\n\n`TypeError: 'NoneType' object is not iterable`", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l6lyu3/fastai_i_cant_load_the_image_data/"}, {"autor": "Advanced-Hedgehog-95", "date": "2021-01-27 19:22:11", "content": "[Q] Help regarding deploying ML models on Herokuapp /!/ Hello, I am interested in deploying -----> image !!!  classification models on herokuapp cloud platform but I cannot understand the limitations of herokuapp with respect to model size and computational power on offer.\n\n&amp;#x200B;\n\nFor example, say I want to deploy ResNet50 from keras then how much \"dynos per hour\" do I need? Has anyone here prior experience? I would love to learn from your experience.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l6c8gi/q_help_regarding_deploying_ml_models_on_herokuapp/"}, {"autor": "johncaling40", "date": "2021-01-26 15:42:20", "content": "Large scale IC ~4000 class tips? /!/ Does anyone have any tips for large scale -----> image !!!  classification. There will be about 4000 classes. What framework is best. What model specs?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l5gizj/large_scale_ic_4000_class_tips/"}, {"autor": "shreyansh26", "date": "2021-01-26 14:47:58", "content": "Tutorial on deploying Deep Learning models in the browser using Tensorflow.js, WebDNN, and ONNX.js (Includes code) /!/  [https://shreyansh26.github.io/post/2021-01-25\\_deep\\_learning\\_in\\_the\\_browser/](https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/)\n\nI wrote a tutorial on how one can use frameworks like Tensorflow.js, WebDNN, and ONNX.js to deploy their Deep Learning models directly in the browser, i.e. no server communication. I have used a simple pre-trained -----> Image !!!  classification model to demonstrate it. The project can serve as a boilerplate code for starting out on deploying models in such a manner on your own.\n\nKindly upvote if you like it! Feedbacks are also welcome :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l5ffon/tutorial_on_deploying_deep_learning_models_in_the/"}, {"autor": "Joe_Reddit_System", "date": "2021-01-26 13:30:58", "content": "Things I should do before training a neural network model on a large dataset? /!/ So I'm new to ML and I was wondering what are some of the things I should make sure are right before starting a relatively long training session on a 100K -----> image !!!  training set.\n\nI'm using Google Colab for the training, and my end goal is to compare the results of 2 different architectures in an image classification context. I've got the models down, so I'm ready to go on to the training part, which so far looks like this:\n\n    for i in range(0, 1000):\n        score = model.fit(train_generator, steps_per_epoch=int(TRAIN_SIZE/BATCH_SIZE), \n    epochs=1, validation_data=validation_generator)\n    \n        val_loss = score.history['val_loss'][0]\n        train_loss = score.history['loss'][0]\n    \n        if val_loss &lt; loss:\n            loss = val_loss\n            last_improvement = 0\n        else:\n            last_improvement = last_improvement + 1\n    \n        if last_improvement == cutoff:\n            break\n\nSo I guess I should mention what my end goals really are - first of all, I want to track loss as a metric, which I'm already doing. Then I'd also like to generate the confusion matrix for the model and do some visualization stuff for the output layer. Given the long training time I might not be able to do all of the post-training things in one session. Would saving the weights after the training is done be a good way to ensure I can do the rest of the stuff whenever?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l5e0qw/things_i_should_do_before_training_a_neural/"}, {"autor": "aniketmaurya", "date": "2021-01-26 05:34:28", "content": "Bag of tricks for -----> Image !!!  classification", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l578hv/bag_of_tricks_for_image_classification/"}, {"autor": "rockyrey_w", "date": "2021-01-26 01:04:54", "content": "[N] AI Art Critic? New Dataset and Models Make Emotional Sense of Visual Artworks /!/ Is it possible for machines to understand and integrate human emotions in visual arts? Meet ArtEmis, a new large-scale dataset of emotional reactions and explanations for visual artworks.\n\nThe researchers propose that, unlike most natural images in machine learning tasks that are typically labelled based on the objects or actions that appear in the images, the visual art domain also involves understanding viewers' affective responses. This requires a relatively complex analysis integrating -----> image !!!  content and its effect on the viewer. The team says the development of novel models for predicting emotion from nuanced perceptual images such as visual arts could also lead to a richer understanding of ordinary images for downstream tasks.\n\nHere is a quick read: [AI Art Critic? New Dataset and Models Make Emotional Sense of Visual Artworks](https://syncedreview.com/2021/01/25/ai-art-critic-new-dataset-and-models-make-emotional-sense-of-visual-artworks/)\n\nThe paper *ArtEmis: Affective Language for Visual Art* is available on [arXiv](https://arxiv.org/pdf/2101.07396.pdf).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l52gyy/n_ai_art_critic_new_dataset_and_models_make/"}, {"autor": "anas_el", "date": "2021-01-25 18:50:17", "content": "CT Scan or XRay detection /!/ Hello everyone, I'm building an application that would allow people to take pictures of their CT Scan or XRay, or upload it, so they can get a result after the process. I would like to create a model that would detect if the uploaded -----> picture !!!  is indeed a CT Scan or an XRay rather than a random -----> picture !!! . \n\nDo you guys have any suggestions? Thank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l4uig1/ct_scan_or_xray_detection/"}, {"autor": "Ok-Cod1551", "date": "2021-01-25 17:55:26", "content": "How to get the distribution of all pixels across a batch of images /!/ Hello, I am doing some analysis for -----> image !!!  data. I have a batch of 1000 images, 512x512 dimensions. How can I get the RGB value distribution across all the images? I was thinking about using numpy but not sure how to approach this. Thanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l4t9qo/how_to_get_the_distribution_of_all_pixels_across/"}, {"autor": "Ok-Cod1551", "date": "2021-01-25 17:54:45", "content": "How to extract get the distribution of pixels across a batch of -----> image !!!  /!/ Hello, I am doing some analysis for image data. I have a batch of 1000 images, 512x512 dimensions. How can I get the RGB value distribution across all the images? I was thinking about using numpy but not sure how to approach this. Thanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l4t97x/how_to_extract_get_the_distribution_of_pixels/"}, {"autor": "prendes", "date": "2021-01-25 13:59:48", "content": "'motion pattern' recognition /!/ Hello everyone, \n\nI am sorry if you don't undesrtand what i'm trying to ask... it's hard for me to express in English.\n\nI've been reading about -----> image !!!  recognition with CNN, but the typical problems consist on recognizing an -----> image !!! .\n\nbut what if what we want to detect is a movement ? e.g. There are a lot of gestures that cannot be classified from a static image, I think it is necessary to compare the entire movement \n\n How can I research this ?  I mean, I don't know if there is a specific name for this... or this problem is solved like 'static image recoginition problems' but then you applied other algorithms...\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l4odcn/motion_pattern_recognition/"}, {"autor": "Ionized97", "date": "2021-05-30 12:17:20", "content": "How to choose the appropriate model and arguments for it? /!/ LONG POST FOLLOWING\n\nHello! \n\nI have been trying to learn machine learning for a while by watching tutorials, reading articles etc. What I haven't been able to understand is the way of choosing the right model for the right problem as well as the right arguments for the model of choice. I have managed to solve a classification problem on my own by trial and error but the choices I made weren't exactly clear in the end. I have a basic grasp of some steps but I am a bit confused about the general idea. For example, what model does a generic problem of multiple text classification require ? Why choose this model over the other ? Why apply these attributes over the others ?\n\nWhat I am trying to understand is if there is a pattern for choosing the right model. All the tutorials I have watched or the articles I have read, do not explain how to choose correctly. They just show you a model, some attributes, a process that-personally-don't understand to its full and then some results are presented. No real problem solved and they all focus on the very same problems (i,e, text classification -&gt; sentiment analysis, -----> image !!!  classification -&gt; the very same problem, with the same dataset of clothes).\n\nCan someone enlighten me on the subject ? Maybe suggest a tutorial, webpage or book that does not focus on just the theoretical parts of machine learning and explains the actual development process and way of thinking.\n\nThanks in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/noa004/how_to_choose_the_appropriate_model_and_arguments/"}, {"autor": "yudhiesh", "date": "2021-05-30 07:18:57", "content": "How to send frames of a video to a model endpoint for object detection? /!/ Has anyone ever preformed object detection on a mobile device but with the model loaded on a cloud and performing inference with the frames that are sent from the mobile device? I have only ever done this with -----> image !!! s where I would make a POST request to the endpoint and send the -----> image !!!  as bytes then convert those bytes to an -----> image !!!  to make predictions. Usually I would just deploy the model on the mobile application but I might have a few models so I am looking at storing them on the cloud instead.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/no5s8d/how_to_send_frames_of_a_video_to_a_model_endpoint/"}, {"autor": "ed-sucks-at-maths", "date": "2021-05-29 14:55:30", "content": "Quick question: how fast can an -----> image !!!  be generated? What possible duration is the shortest? In miliseconds /!/ I understand that it's deep learning related question, however learndeeplearning subreddit is quite dead and I am hoping that someone here would know the answer.\n\nI would like to use some statistics in an essay I am writing for aesthetics\n\nThank you in advance", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nnp4r2/quick_question_how_fast_can_an_image_be_generated/"}, {"autor": "The_Vecter", "date": "2021-05-29 11:13:24", "content": "Deep learning does well in training, but fails in practice /!/ Hi guys, \n\nI have been following [this tutorial](https://towardsdatascience.com/deeppicar-part-1-102e03c83f2c) on making a deep learning self driving car with a raspberry pi. I have done some things differently, like making my own data recoring / data gathering software which gets the steering angle and speed directly from the car. \n\nAfter I have recorded all the info and put it in the google colab folder, I followed all the instructions in the article and got a loss of around 40, and a validation loss of around 8. \n\nI tested the prediction against validation data and some of the values it predicted were: \n\n&amp;#x200B;\n\n|real|predicted|\n|:-|:-|\n|80|84|\n|100|99|\n|90|86|\n\nHowever, when I send the car to complete the same course it trained on, it ignores the marked lines and goes straight, sometimes turning away from the line.\n\nThe code to get the predicted angle from what the -----> camera !!!  sees is virtually identical to what is in the tutorial.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nnlabs/deep_learning_does_well_in_training_but_fails_in/"}, {"autor": "parahillObjective", "date": "2021-05-29 00:33:36", "content": "I want to make a model that will take in a pdf of a credit card statement and output spending trends like money spent per category. How would I go about doing this? /!/ I know mint already supports this but it only goes back 6 months. Also this is just a fun project for me to work on which maybe I could share the world. \n\n- I assume I would have to get a pdf text extractor like this https://pypi.org/project/pdfminer/ I don't think id need OCR since there's text in the pdf and it's not just an -----> image !!! . \n\n- after I get the text, I would need to somehow recognize the table that describes all the transactions. it's a table that has transactions as the rows and the columns are the name of the place where you made a purchase and the money spent there. I'm not sure how Id go about doing this step though. \n\n    I know i can just look for queues like the string \"Balance statement\" that indicates the start of the table, but it won't be modular and won't work for other banks or if the current bank changes the format of the statements. Should I be using machine learning, in this case, to detect where the table of transactions starts?\n\n- secondly assuming that I am able to extract the table of transactions, how would I go about detecting the categories of spending like groceries, entertainment, fitness, furntiture etc? This is a classification problem right? How would I go about solving this? would I need a large dataset of company names and their spending category to train my model?\n\n\nA sample from the outputed text with just 2 transactions looks like this:\n\n\n    ...\n    BALANCE STATEMENT\n    APR12APR15$5.54DAIRYQUEEN\n    APR11APR19$46.44IKEA\n    TOTALBALANCE$51.98\n    ...", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nnbm95/i_want_to_make_a_model_that_will_take_in_a_pdf_of/"}, {"autor": "got_data", "date": "2021-05-28 20:08:21", "content": "-----> Image !!!  DISsimilarity measure for an objective function? /!/ Image similarity is pretty simple \u2014 I typically go for \u03a3Im1\\*Im2 or \u03a3|Im1-Im2|^(2). But what if I want two image to be different? \n\nMotivation:\n\nI have a known function f: Im1 \u2192 Im2. Given a target image for Im2, I use gradient descent to find Im1 starting with random noise. So far so good. Now, the nature of the function is such that Im1 and Im2 tend to end up similar (i.e. Im1 can be recognized by the human eye as a precursor to Im2), but it doesn't have to be the case because there are also possible configurations of pixels in Im1 that would look fairly different from Im2 but f(Im1) would still end up similar to Im2. So my goal is to modify the objective function by adding a term that would try to keep Im1 and Im2 dissimilar.\n\nWhat I've tried so far:\n\nFirst I thought I could just use a negative Euclidean distance loss, but that had a tendency to steer towards a negative image which is very similar to the positive. What I want is to make Im1 look different in terms of pixel patterns/densities. One thought I have is to use a regular similarity measure but pass random noise or some repeatable pattern as a reference image. This way gradient descent should steer Im1 towards that pattern... But perhaps there's a better way?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nn6e9k/image_dissimilarity_measure_for_an_objective/"}, {"autor": "choff5507", "date": "2021-05-28 17:12:03", "content": "Looking for help with processing an -----> image !!!  through a built model /!/ I recently built a model for object detection and I am trying to process an image through the model but I get an error Im hoping someone can help with. \n\n [img = cv2.imread(IMAGE\\_PATH)image\\_np = np.array(img)input\\_tensor = tf.conv - Pastebin.com](https://pastebin.com/gmK1ip7Z) \n\nThe error I get is:\n\n ValueError: 'images' must have either 3 or 4 dimensions. \"\n\nnot sure why this the image is RGB. Can anyone give me an idea of what may be wrong? Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nn2kyx/looking_for_help_with_processing_an_image_through/"}, {"autor": "beatsbysurf", "date": "2021-05-28 01:59:37", "content": "Help incorporating numerical data into a segmentation model? /!/ Hey guys! As a part of a research project, I\u2019ve been looking at this Kaggle dataset for MRI segmentation recently(linked below), and noticed that there was a bunch of genomic data alongside the -----> image !!!  data - this data is numerical and inside of a csv, so it\u2019s relatively easy to access. How would you go about incorporating this data into the segmentation model, if possible? I\u2019ve seen someone recommend converting the columns into a feature vector, but I\u2019m still mostly lost. Thank y\u2019all for the help - pm if you want any more details or you\u2019d be interested in helping  [https://www.kaggle.com/sahintiryaki/brain-tumor-segmentation-vgg19-unet](https://www.kaggle.com/sahintiryaki/brain-tumor-segmentation-vgg19-unet)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nmn7nj/help_incorporating_numerical_data_into_a/"}, {"autor": "JonnieSingh", "date": "2021-04-15 17:30:04", "content": "How accurate is this Google ML documentation (for Android)? /!/ I'm currently working on a mobile Android application (written in Java on Android Studio) which is being developed to recognize and detect objects during a live stream -----> camera !!!  preview. [This is the Google ML Kit documentation](https://developers.google.com/ml-kit/vision/object-detection/android) I was using to develop the object detection. However, the issue that I currently need help with is this; the application does not carry out its sole intended purpose of object detection. I should add that the camera preview (using CameraX) works absolutely fine. \n\n&amp;#x200B;\n\nAfter being puzzled on where the error might be, I had focused in on the aforementioned guide's Step 3: Process the Image, where the image is passed to a `process()` method.\n\n&amp;#x200B;\n\nNaturally, I proceed to attempt to debug the `onSuccess()` method (which processes the image if the application is successful) via `Log.d(..)` to see what the size of the returned object list was. the console proceeded to print out the phrase `D/TAG: onSuccess0` upwards of up to 20 times. Which must mean that the application isn't detecting objects right? This is what puzzles me because I have been following the documentation and am excited to explore the world of Machine Learning.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mrjm8u/how_accurate_is_this_google_ml_documentation_for/"}, {"autor": "MemeScrollingMaths", "date": "2021-04-14 23:27:51", "content": "Spotting Errors: Tensorflow Input Broadcasting /!/ I'm trying to build a binary -----> image !!!  classifier in Tensorflow. Original, I know right /s. That data I'm looking at is a subset of chest Xrays found here: [https://www.kaggle.com/tawsifurrahman/covid19-radiography-database](https://www.kaggle.com/tawsifurrahman/covid19-radiography-database).\n\nI have a local copy, and have organized directories for non-overlapping training and validation sets at a 70-30 ratio. All 7232 (3616 per label) images seem to be 299 x 299 pixels, full grayscale, and appear to be normalized. Assume that my Jupyter notebook (Python 3.6) and Data folder are in the same directory. Here's what I have so far:\n\n    import os\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import matplotlib.image as mpimg\n    import seaborn as sns\n    from sklearn.model_selection import train_test_split\n    import tensorflow as tf\n    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n    \n    # Define training data\n    train_normal_path = os.path.join(\"Data\\KerasData\\Train\\Healthy\")\n    train_covid_path = os.path.join(\"Data\\KerasData\\Train\\Covid\")\n    train_normal_filenames = os.listdir(train_normal_path)\n    train_covid_filenames = os.listdir(train_covid_path)\n    \n    # Define validation data\n    valid_normal_path = os.path.join(\"Data\\KerasData\\Valid\\Healthy\")\n    valid_covid_path = os.path.join(\"Data\\KerasData\\Valid\\Covid\")\n    valid_normal_filenames = os.listdir(valid_normal_path)\n    valid_covid_filenames = os.listdir(valid_covid_path)\n    \n    # Grayscale images already have normalized pixel values; type coherence only\n    train_datagen = ImageDataGenerator(rescale=1/1)\n    validation_datagen = ImageDataGenerator(rescale=1/1)\n    \n    train_generator = train_datagen.flow_from_directory(\n        \"Data\\KerasData\\Train\",\n        classes = [\"Covid\", \"Healthy\"],\n        target_size=(299, 299),\n        batch_size = 20,\n        class_mode='binary'\n    )\n    \n    validation_generator = validation_datagen.flow_from_directory(\n        \"Data\\KerasData\\Valid\",\n        classes = [\"Covid\", \"Healthy\"],\n        target_size = (299, 299),\n        batch_size = 20,\n        class_mode = 'binary'\n    )\n    \n    model = tf.keras.models.Sequential([\n        # 1st Convolution and Deconvolution layer   \n        Conv2D(filters=1, kernel_size=(1, 1), activation='relu', \n        input_shape=(299, 299, 1)),\n        MaxPooling2D(2, 2),\n        Flatten(),\n        Dense(1, activation='sigmoid')    \n    ])\n    model.compile(\n        loss='binary_crossentropy',\n        optimizer='SGD',\n        metrics='accuracy'\n    )\n    \n    \n    model.summary()\n\nStarting with Batch Size, is this supposed to be how many images we want to load per epoch, the total number of images in a directory, or something else? \n\nWhen trying to fit the model however, with\n\n    history = model.fit(\n    train_generator,\n        steps_per_epoch = 8,\n        epochs = 10,\n        verbose = 0,\n        validation_data = validation_generator,\n        validation_steps = 8\n    )\n\nI'll probably try tweaking steps\\_per\\_epoch and the like later.\n\nI'm encountering a ValueError (can't broadcast input array from shape (299,299,3) into shape (299, 299, 1, 3). Is this because I'm using Conv2d for grayscale, or have I screwed something up in the input and validation generators?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mr2qyn/spotting_errors_tensorflow_input_broadcasting/"}, {"autor": "want-su", "date": "2021-04-14 16:23:53", "content": "Question about the usage of packed sequence in PyTorch /!/ Question about the pack padded sequence in Pytorch.\n\nI\u2019m using a simple cnn-lstm for -----> image !!!  captioning task. To speed up training, i adopt fully teacher forcing. I use pack padded sequence to handle variable lengths, then the packed rnn outputs was directly given into linear layer for classification.\n\nrnn_packed, state = lstm(packed_input)\nout = fc(rnn_packed)\n\nMy question is did i really need to unpack the RNN output before the linear layer? I\u2019ve searched many examples, most of them unpack the rnn output then sent it to linear layer for classification. \n\nrnn_padded = pad_packed_sequence(rnn_packed\uff09\nout = fc(rnn_padded.view(-1, vocab_size ))\n\nI didn\u2019t see any different functionality between them. Hope someone can help me, thanks.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mqu4q1/question_about_the_usage_of_packed_sequence_in/"}, {"autor": "thunderlipsIV", "date": "2021-04-14 16:17:27", "content": "Creating a Nueral Network Classifier on MATLAB /!/ I'm very honestly awful at the computational side of my Electrical Engineering degree, this is one of my assignments and sadly due to online learning, the lab lessons have been very un-educational and my professor is speaking to 50 people at anytime via zoom which he struggles with himself. \n\nI'm asking for is any help in starting this, I'm not looking for anyone to do my homework for me I just need some advice on what I should be looking at or reading to get it done. Even with the steps laid out in the -----> picture !!!  below I'm still no further ahead when I started \n\n&amp;#x200B;\n\nhttps://preview.redd.it/e7nldw1ay5t61.png?width=703&amp;format=png&amp;auto=webp&amp;s=b32c7af2cea18cbe87eaf48f8fe5549a82ec5239", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mqtzlu/creating_a_nueral_network_classifier_on_matlab/"}, {"autor": "JobelSix", "date": "2021-04-14 13:26:12", "content": "Turning a NumPy array into a Theano Tensor4 /!/ Hello, I'm working on building a WGAN to generate brain images as a research project. I have a convolutional hidden layer of 200 and I can test my generator using:\n\n`def test(G):`\n\n\t`Z=np.random.normal(loc=0.0, scale=1.0, size=(100,200)).astype(floatX)`\n\n\t`G_Z=get_output(G,Z,deterministic=True)`\n\n\t`return G_Z`\n\nWhich is just a random number in the size of the latent space. \n\nI want to be able to initiate the generator with one of my training images and compare the outputs. I tried this with: \n\n`test_image = x_train[4040,:,:,:].astype(floatX) #define the -----> image !!!  slice to be put into the network`\n\n`d_test = get_output(D, test_-----> image !!! , deterministic=True) #converts -----> image !!!  into latent space`\n\n`B = testImage(G, d_test).eval() #run through generator`\n\nThis effectively runs the -----> image !!!  through the discriminator to turn it into latent space.\n\nThe problem here is that \"test\\_image\" is needed as a Theano Tensor4. I have tried everything to convert it, but I cannot seem to find a function or method which can achieve this. Does anyone have any ideas on how to achieve this? Or another method that may work?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mqqjco/turning_a_numpy_array_into_a_theano_tensor4/"}, {"autor": "BlackFreud", "date": "2021-04-14 11:56:19", "content": "Help with back propagation /!/ So I've been racking my brains trying   to understand what is going on in this -----> image !!! , specifically how the   summation ended up in the vectorized form the arrow is pointed to. My   confusion is how the q and W were swapped. I thought matrix   multiplication was not commutative. Anyone got any insight on this?\n\nNB:   The green matrices represent the forward pass. The red ones represent   the backward pass during backprop. Also q is the intermediate from   computing the dot product of W and x.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/y7kgcqloo4t61.png?width=1496&amp;format=png&amp;auto=webp&amp;s=476f624b2c03ace650cc901d9c21ca34f72f5d60", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mqozcz/help_with_back_propagation/"}, {"autor": "wallynext", "date": "2021-04-09 13:28:39", "content": "wouldn't a segmentation network give a better classification result than a purely classification CNN? /!/ 1- I hope my question makes sense, but basically a CNN who's purpose is to classify an -----> image !!!  of a dog as being a dog looks for pixels that form patterns that might give a clue about it being a dog, like ears and snout, and a segmentation CNN gives each pixel a label, 0 not a dog and 1 a dog, therefore if there are any pixels in the -----> image !!!  with label 1, you can say that there is a dog in the picture, and if all the pixels in the -----> image !!!  are 0 than there is no dog.\n\nso by this logic wouldn't a segmentation CNN perform better at the task of classification? and if not why?\n\n&amp;#x200B;\n\n2 - and another question, imagine we have a dataset that has pictures of dog and pictures without dogs and we want to segment the dog an image, and we have 2 scenarios:\n\n1- use one segmentation CNN to learn how to segment the dog \n\n2- use first a CNN to classify if there is a dog in the picture and then use a segmentation CNN to segment it if there is one\n\n&amp;#x200B;\n\nwhich scenario yields best results? can you use a classifier to increase the accuracy of segmentation, or is that redundant?\n\n&amp;#x200B;\n\nthanks in advance", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mnh777/wouldnt_a_segmentation_network_give_a_better/"}, {"autor": "tdeckers", "date": "2021-04-09 13:24:06", "content": "Stereo vision for classification problem /!/ Hi all,\n\nI'm starting work on a solution that can detect darts in a darts board and calculate a score. I'm planning to use two webcams pointed at the board to get the depth perspective. I'm fairly new to -----> image !!!  processing and machine learning, so am looking for some pointers. Would I be able to just stitch the left and right image together and let a neural network learn based on that or should I pre-process the two image through e.g. opencv to create a depth map and feed that in?\n\nSecond I'm planning to use classification to verify how many darts are in each area on the darts board.  Makes sense?\n\nThanks and happy to share progress here if there's interest.  \nTom.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mnh43s/stereo_vision_for_classification_problem/"}, {"autor": "Raincoat13", "date": "2021-04-09 01:45:26", "content": "How to load in your own data? /!/ Hello, I am working on a Google Colab notebook.  I have 2 folders on my google drive names \u201cDog\u201d and \u201cCat\u201d. Each has animal pictures. How do u think is the best method to upload those -----> image !!!  data into variables x_train and y_train? \n\nI was using this video to upload those pictures into variables x_train and y_train. \n\nhttps://m.youtube.com/watch?v=j-3vuBynnOE\n\nBut I only ever get the  \u201cDog\u201d label when I try .predict() with an arbitrary animal picture. For example if I try .predict with a dog pic, I get dog result. If I try .predict with a cat pic, I get dog result. \n\nSo I am curious to know of other ways to upload image data, because perhaps the method from the video isn\u2019t working for me.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mn6tyw/how_to_load_in_your_own_data/"}, {"autor": "lalakersfan29", "date": "2021-04-08 23:07:38", "content": "Combine CNN and LSTM tflearn /!/ I'm looking to implement a RNN along with a CNN in order to make a prediction based on two images instead of one alone with a CNN. I'm trying to modify my alexnet model code:\n\n    def alexnet(width, height, lr, output=3):\n        network = input_data(shape=[None, width, height, 1], name='input')\n        network = conv_2d(network, 96, 11, strides=4, activation='relu')\n        network = max_pool_2d(network, 3, strides=2)\n        network = local_response_normalization(network)\n        network = conv_2d(network, 256, 5, activation='relu')\n        network = max_pool_2d(network, 3, strides=2)\n        network = local_response_normalization(network)\n        network = conv_2d(network, 384, 3, activation='relu')\n        network = conv_2d(network, 384, 3, activation='relu')\n        network = conv_2d(network, 256, 3, activation='relu')\n        network = max_pool_2d(network, 3, strides=2)\n        network = local_response_normalization(network)\n        network = fully_connected(network, 4096, activation='tanh')\n        network = dropout(network, 0.5)\n        network = fully_connected(network, 4096, activation='tanh')\n        network = dropout(network, 0.5)\n        network = fully_connected(network, output, activation='softmax')\n        network = regression(network, optimizer='momentum',\n                             loss='categorical_crossentropy',\n                             learning_rate=lr, name='targets')\n    \n        model = tflearn.DNN(network, checkpoint_path='model_alexnet',\n                            max_checkpoints=1, tensorboard_verbose=2, tensorboard_dir='log')\n    \n        return model\n\nI have my -----> image !!! s in a np array where each element is the pixel data for one -----> image !!! . I'm having trouble implementing the functionality of using two images with the RNN.\n\nI've seen the reshape and lstm methods of tflearn which I believe should be placed before the final fully connected layer but not sure how to specify the number of images to use.\n\nAlso, would it be easier to move my code to Keras?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mn3zf6/combine_cnn_and_lstm_tflearn/"}, {"autor": "phobrain", "date": "2021-06-15 05:58:03", "content": "weird science: shrinking -----> image !!! net vectors to a single number per -----> image !!!  /!/ I can reduce imagenet vectors to a single number per photo, which number can be used with ~75% accuracy to predict how it will pair with another photo - according to pairs I've labeled like/dislike, in my personality-analysis method that I call Rorschach pairing. Max accuracy can get to 95%+ if I don't choke it down to one number per pic, but here the fun is in being able to look at simple graphs of two lines, each line covering the 'value' of the pic on left or right side, aligned by the resulting prediction on the x axis. \n\nIt's interesting to see the trends and how DenseNet121 differs from the other three.\n\nLazy-titling: order of graphs is\n\n    VGG16        VGG19\n    DenseNet121  MobileNetV2 \n\n    purple line: single number for left pic of pair\n    other line: right pic's single number\n    x axis: prediction: 0==good, 0.5==borderline, 1=bad\n\nhttp://phobrain.com/pr/home/vgg_16_19_dn121_mnv2_1000_lrv_1_sort_by_pred.png\n\nEdit: full disclosure, color histograms go into that one number too.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o07528/weird_science_shrinking_imagenet_vectors_to_a/"}, {"autor": "axetobe_ML", "date": "2021-06-13 20:57:38", "content": "Some YouTube channels that review papers /!/ When I was reading a Reddit thread. People were wondering if there were YouTubers reviewing papers. As the OP noticed that one of the YouTuber's that he regularly watched stopped uploading videos. There are a few YouTubers that talk about ML and review papers. \n\nI decided to compile some of the YouTube channels into this short list. \n\n&amp;#x200B;\n\n[Two Minute Papers](https://www.youtube.com/c/K%C3%A1rolyZsolnai/videos) does great overviews of fascinating papers. Showing the increasing progress of ML.\n\nSome of the videos I liked:\n\n* [4 Experiments Where the AI Outsmarted Its Creators](https://www.youtube.com/watch?v=GdTBqBnqhaQ)\n\nThis video showed various AI solving a problem not in the way the researchers intended to. That may include abusing the physics in the simulation or lateral thinking used by the model.\n\n* [A Video Game That Looks Like Reality!](https://youtu.be/22Sojtv4gbg)\n\nA review of a paper that takes GTA V gameplay and converts them to -----> photo !!! -realistic footage.\n\n&amp;#x200B;\n\n[Yannic Kilcher](https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew) does in-depth reviews of various papers. As you go through the paper he shows you his thought process. And showing what important inside the paper. Very useful if don\u2019t read that many papers. (Like me)\n\nSome good videos:\n\n* [Attention Is All You Need](https://www.youtube.com/watch?v=iDulhoQ2pro)\n\nA review of a paper that introduced transformers.\n\n&amp;#x200B;\n\n* [DeepMind's AlphaFold 2 Explained! AI Breakthrough in Protein Folding What we know (&amp; what we don't)](https://youtu.be/B9PL__gVxLI)\n\nA great rundown on protein folding and speculating how Alphafold 2 works.\n\n&amp;#x200B;\n\n* [GPT-3: Language Models are Few-Shot Learners (Paper Explained)](https://youtu.be/SY5PvZrJhLE)\n\nA comprehensive paper reading of the GPT-3 paper.\n\n&amp;#x200B;\n\n[Bycloud](https://www.youtube.com/channel/UCgfe2ooZD3VJPB6aJAnuQng) you may have seen him around on Reddit. Creates short and insightful summaries of papers.\n\nSome videos I liked:\n\n* [AI Sky Replacement with SkyAR](https://www.youtube.com/watch?v=yNwQnrjfg5A)\n\nSummary of paper that creates AR effects in video footage. Adding various effects to the video footage\u2019s sky.\n\n&amp;#x200B;\n\n* [AI Generates Cartoon Characters In Real Life \\[Pixel2Style2Pixel\\]](https://youtu.be/g-N8lfceclI)\n\nReviewing a paper that converts cartoon characters to real-life equivalents and vice versa. Also explains how the paper made it easier to adjust the parameters of the GAN. Helping us adjust what images we want to produce.\n\n&amp;#x200B;\n\n[Machine Learning Street Talk](https://www.youtube.com/c/MachineLearningStreetTalk/videos)\n\nThis is a podcast series that interviews top ML researchers. While they don\u2019t have videos about papers alone. As they interview various experts in the field. So they talk about many papers as a consequence. \n\nWhile this is a short list maybe you can find these channels interesting and learn something new.\n\n\\-\n\n*If you found this post useful, then check out my* [*mailing list*](https://www.tobiolabode.com/subscribe) *where I write more stuff like this.*", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nz5szs/some_youtube_channels_that_review_papers/"}, {"autor": "TrepidationTD", "date": "2021-06-13 17:54:29", "content": "What differentiates ML algorithms from one another? /!/ Let's say we have two projects(one by a student and one by a university) and both of them want to recognize numbers 1 - 10 in -----> image !!!  form. Both of them input the same amount of test cases into their program and just train the computer. So, because both groups are doing the exact same thing(using input to find output by training the machine) how would one system be more effective than the other. Since all projects are about making the machine learn, how do some systems gain higher efficiency with the same amount of training sets? Im a total beginner and don't really understand. Is the difference between different algorithms the implementation/ optimization of the programs? are the ways they train different? I saw in a bunch of simple examples with the system just being trained with a single line of syntax, is there more going on behind the scenes that the coder has to do? Sorry for the very ignorant question, I'm a beginner and am very clueless.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nz1x22/what_differentiates_ml_algorithms_from_one_another/"}, {"autor": "CalligrapherSimple29", "date": "2021-06-13 04:24:41", "content": "using footage from the 1962 -----> film !!!  cleopatra starring Elizabeth Taylor with Cleopatra's real face digitally regenerated by Deep Fake technology &amp; her statues .. to see cleopatra being vivid , real and Alive in a way .. i hope you enjoy this simulation", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nyola9/using_footage_from_the_1962_film_cleopatra/"}, {"autor": "rohitkuk", "date": "2021-06-12 13:52:59", "content": "Pytorch Implementation Translating Real -----> Image !!! s to cartoon images using PIX2PIX - -----> Image !!! -to------> Image !!!  Translation with Conditional Adversarial Networks Code : https://lnkd.in/etv3Kws paper : https://lnkd.in/exdgeBB", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ny78qf/pytorch_implementation_translating_real_images_to/"}, {"autor": "dennishnf", "date": "2021-06-12 11:44:26", "content": "Introduction to Deep Learning [code in the description] /!/ This is a repository where I have compiled some basic Deep Learning concepts and resources, and also basic practical examples on **-----> image !!!  classification**, **object detection** and **automatic segmentation**. \n\n**All examples are intended to be run in Google Colab and using your own dataset.** \n\nI hope you find it useful. \ud83d\udc68\u200d\ud83d\udcbb\n\nLink: [**https://github.com/dennishnf/intro-to-deep-learning**](https://github.com/dennishnf/intro-to-deep-learning)\n\n&amp;#x200B;\n\n[Examples from the repository ](https://i.redd.it/5ah0egrzlt471.gif)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ny4xlf/introduction_to_deep_learning_code_in_the/"}, {"autor": "jayalammar", "date": "2021-06-28 09:15:43", "content": "Behavioral Testing of ML Models (Unit tests for machine learning) [Video] /!/ This video discusses the paper [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://www.aclweb.org/anthology/2020.acl-main.442/), winner of Best Paper at ACL 2020.\n\nEvaluating ML models using a single metric (like accuracy or F1-score) produce a low-resolution -----> picture !!!  of model performance. Behavioral tests can give us a much higher resolution evaluation of a model's capabilities. By creating tests (which are small targeted test sets), we can better compare models or observe how model performance changes after re-training a model (or fine-tuning it). \n\n[https://youtu.be/Cse-3MM7mso](https://youtu.be/Cse-3MM7mso)\n\nHope you enjoy it", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o9gljh/behavioral_testing_of_ml_models_unit_tests_for/"}, {"autor": "grid_world", "date": "2021-06-28 08:12:13", "content": "Autoencoder TensorFlow2 - ValueError /!/ I am trying to train an Autoencoder using TensorFlow2.5 and Python3.8 as follows: Inception NetV3 was used to perform feature extraction using an -----> image !!!  dataset containing 289229 -----> image !!! s. The final output of Inception NetV3 is 2048-d vector. I pickled all of them in a Python3 list and load it along with the filenames:\n\n        # Read pickled Python3 list containing 2048-d extracted feature representation per image-\n        features_list = pickle.load(open(\"DeepFashion_features_inceptionnetv3.pickle\", \"rb\"))\n        \n        # Convert from Python3 list to numpy array-\n        features_list_np = np.asarray(features_list)\n        \n        features_list_np.shape\n        # (289229, 2048)\n        \n        del features_list\n        \n        # Read pickled Python3 list containing abolute path and filenames-\n        filenames_list = pickle.load(open(\"DeepFashion_filenames_inceptionnetv3.pickle\", \"rb\"))\n        \n        len(features_list), len(filenames_list)\n        # (289229, 289229)\n        \n        # Note that the absolute path contains Google colab path-\n        filenames_list[1]\n        # '/content/img/1981_Graphic_Ringer_Tee/img_00000002.jpg'\n    \n        # Create 'tf.data.Dataset' using np array-\n        batch_size = 32\n        features_list_dataset = tf.data.Dataset.from_tensor_slices(features_list_np).batch(batch_size)\n        \n        x = next(iter(features_list_dataset))\n        # 2021-06-28 13:10:00.229937: W tensorflow/core/kernels/data/model_dataset_op.cc:205] Optimization loop failed: Cancelled: Operation was cancelled\n    \n        x.shape\n        # TensorShape([32, 2048])\n\n&amp;#x200B;\n\nMy first question is why does it give the message \"Optimization loop failed\"? I am using Nvidia RTX 3080 with 16GB GPU. Note that since this is an autoencoder, there are no accompanying labels for the given data!\n\nIs there any other better way of feeding this Python3 list as input to a TF2 neural network that I am missing?\n\nI am checking for available GPU:\n\n        num_gpus = len(tf.config.list_physical_devices('GPU'))\n        print(f\"number of GPUs available = {num_gpus}\")\n        # number of GPUs available = 1\n\nSecond, I coded an autoencoder with the architecture:\n\n&amp;#x200B;\n\n        class FeatureExtractor(Model):\n            def __init__(self):\n                super(FeatureExtractor, self).__init__()\n                \n                self.encoder = Sequential([\n                     Dense(\n                        units = 2048, activation = 'relu',\n                        kernel_initializer = tf.keras.initializers.glorot_normal(),\n                        input_shape = (2048,)\n                        ),\n                     Dense(\n                        units = 1024, activation = 'relu',\n                        kernel_initializer = tf.keras.initializers.glorot_normal()\n                        ),\n                    Dense(\n                        units = 512, activation = 'relu',\n                        kernel_initializer = tf.keras.initializers.glorot_normal()\n                        ),\n                    Dense(\n                        units = 256, activation = 'relu',\n                        kernel_initializer = tf.keras.initializers.glorot_normal()\n                        ),\n                    Dense(\n                        units = 100, activation = 'relu',\n                        kernel_initializer = tf.keras.initializers.glorot_normal()\n                        ),\n                ]\n                )\n               \n                self.decoder = Sequential([\n                    Dense(\n                        units = 256, activation = 'relu',\n                        kernel_initializer = tf.keras.initializers.glorot_normal()\n                        ),\n                    Dense(\n                        units = 512, activation = 'relu',\n                        kernel_initializer = tf.keras.initializers.glorot_normal()\n                        ),\n                    Dense(\n                        units = 1024, activation = 'relu',\n                        kernel_initializer = tf.keras.initializers.glorot_normal()\n                        ),\n                    Dense(\n                        units = 2048, activation = 'relu',\n                        kernel_initializer = tf.keras.initializers.glorot_normal()\n                        ),\n                ]\n                )\n            \n            def call(self, x):\n                encoded = self.encoder(x)\n                decoded = self.decoder(encoded)\n                return decoded\n        \n        \n        # Initialize an instance of Autoencoder-\n        autoencoder = FeatureExtractor()\n        \n        \n        autoencoder.build(input_shape = (None, 2048))\n        \n        # Compile model-\n        autoencoder.compile(\n            optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n            loss = tf.keras.losses.MeanSquaredError()\n            )\n        \n        \n        # Sanity check-\n        autoencoder(x).shape\n        # TensorShape([32, 2048])\n        \n        x.shape\n        # TensorShape([32, 2048])\n\nBut, when I try to train the model:\n\n        # Train model-\n        history_autoencoder = autoencoder.fit(\n            features_list_dataset, epochs = 20\n            )\n\n&amp;#x200B;\n\nIt gives me the error:\n\n&amp;#x200B;\n\n&gt;ValueError: No gradients provided for any variable:  \n&gt;  \n&gt;\\['dense\\_10/kernel:0', 'dense\\_10/bias:0', 'dense\\_11/kernel:0',  \n&gt;  \n&gt;'dense\\_11/bias:0', 'dense\\_12/kernel:0', 'dense\\_12/bias:0',  \n&gt;  \n&gt;'dense\\_13/kernel:0', 'dense\\_13/bias:0', 'dense\\_14/kernel:0',  \n&gt;  \n&gt;'dense\\_14/bias:0', 'dense\\_15/kernel:0', 'dense\\_15/bias:0',  \n&gt;  \n&gt;'dense\\_16/kernel:0', 'dense\\_16/bias:0', 'dense\\_17/kernel:0',  \n&gt;  \n&gt;'dense\\_17/bias:0', 'dense\\_18/kernel:0', 'dense\\_18/bias:0'\\].\n\nWhat is going wrong?\n\n&amp;#x200B;\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o9ftzp/autoencoder_tensorflow2_valueerror/"}, {"autor": "radit_yeah", "date": "2021-06-28 07:48:22", "content": "Podcast Recommendations for Learning Machine Learning /!/ Looking for podcasts and/or other resources, which I can \"listen\" to, to learn the theoretical aspects of Machine Learning. The podcast recommendations that I found online were mostly about podcasts that discussed the implications of AI, ethics, current trends, and the big -----> picture !!! .  The problem with online courses such as CS229 is that I would need to \"watch\" the video as well which is not convenient while I'm doing chores or traveling, etc.\n\nAny recommendations/leads would be appreciated!\n\nThanks in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o9fj4y/podcast_recommendations_for_learning_machine/"}, {"autor": "TheFunnyGuyy", "date": "2021-06-27 20:01:45", "content": "Can't Train a Network to do an identity operation over a single -----> image !!!  (pytorch) /!/ Hey everyone :)\n\nI'm trying to train a network to do an image processing operation. Since I was having some trouble with that, I moved to just trying to train a network to do nothing, and it still doesn't converge to anything good. \n\nDoes anything looks off in my code? Any idea/remark will be gladly appreciated \n\nThanks! \n\n    class Net(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n            self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n            self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n            self.conv4 = nn.Conv2d(256, 256, 3, padding=1)\n            self.conv5 = nn.Conv2d(256, 256, 3, padding=1)\n            self.conv6 = nn.Conv2d(256, 128, 3, padding=1)\n            self.conv7 = nn.Conv2d(128, 64, 3, padding=1)\n            self.conv8 = nn.Conv2d(64, 3, 3, padding=1)\n    \n    \n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            x = F.relu(self.conv2(x))\n            x = F.relu(self.conv3(x))\n            x = F.relu(self.conv4(x))\n            x = F.relu(self.conv5(x))\n            x = F.relu(self.conv6(x))\n            x = F.relu(self.conv7(x))\n            x = F.relu(self.conv8(x))\n            return x\n    \n    class SingleImageDataset(Dataset):\n    \n        def __init__(self, path):\n            self.image = Image.open(path)\n            tensor_trans = transforms.ToTensor()\n            tens = tensor_trans(self.image)\n            self.stds = [tens[i].std().item() for i in range(3)]\n            self.means = [tens[i].mean().item() for i in range(3)]\n            normalize = transforms.Normalize(mean=self.means, std=self.stds)\n            self.final_data = normalize(tens)\n    \n        def __len__(self):\n          return 1\n    \n        def __getitem__(self, idx):\n          return self.final_data\n    \n    naruto_dataset = SingleImageDataset('./image/naruto.jpg')\n    naruto_loader = torch.utils.data.DataLoader(naruto_dataset, batch_size=1,\n                                              shuffle=False, num_workers=1)\n    net = Net()\n    net.to(device)\n    criterion = nn.L1Loss()\n    optimizer = optim.Adam(net.parameters(), lr=0.001)\n    \n    losses = []\n    for epoch in tqdm.tqdm(range(500)):\n        running_loss = 0.0\n        for i, data in enumerate(naruto_loader, 0):\n            inputs = data.cuda()\n            # zero the parameter gradients\n            optimizer.zero_grad()\n    \n            # forward + backward + optimize\n            outputs = net(inputs)\n            loss = criterion(outputs, inputs)\n            losses.append(loss.item())\n            # print(loss.item())\n            loss.backward()\n            optimizer.step()\n            \n\nAnd the loss converges on 0.4 (which is horrible, since I'm using L1Loss which is a mean)\n\n[The loss graph](https://preview.redd.it/o3uhqdj46v771.png?width=372&amp;format=png&amp;auto=webp&amp;s=2aa5c4244cb17130be39f60be4ee324331eb85a9)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o94gcv/cant_train_a_network_to_do_an_identity_operation/"}, {"autor": "supreethrao", "date": "2021-06-27 17:12:32", "content": "Implementing Vision Transformers using Tensorflow. Help needed /!/ I\u2019ve been trying to implement \u201d An -----> image !!!  is worth 16x16 words: Transformers for -----> image !!!  recognition at scale\u201d paper in TensorFlow. But I\u2019m running into `ValueError: Operands could not be broadcasted together with shape (144, 256) (144, 128)`. Could someone what's wrong and how do I go about fixing it. A link to the model on google colab can be found here ([https://colab.research.google.com/drive/1YQmd5k2tlJYliUDOjxGbxvolxZFPbzuY?usp=sharing](https://colab.research.google.com/drive/1YQmd5k2tlJYliUDOjxGbxvolxZFPbzuY?usp=sharing))", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o919c2/implementing_vision_transformers_using_tensorflow/"}, {"autor": "tt12343", "date": "2021-06-27 13:52:56", "content": "Data Augmentation - Mixup in PyTorch /!/ An implementation of mixup in -----> image !!!  to increase -----> image !!!  classification accuracies:\n\n[https://towardsdatascience.com/enhancing-neural-networks-with-mixup-in-pytorch-5129d261bc4a](https://towardsdatascience.com/enhancing-neural-networks-with-mixup-in-pytorch-5129d261bc4a)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o8xhsh/data_augmentation_mixup_in_pytorch/"}, {"autor": "Jump2Fly", "date": "2021-06-27 11:01:28", "content": "I created a video about how you can train a neural network (in python) to learn complex -----> image !!! /video classification tasks (like in-game detection) using transfer learning! The GitHub repo is linked in the video description. Hope this is useful or helpful for some of you guys :-)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o8uyk5/i_created_a_video_about_how_you_can_train_a/"}, {"autor": "torchma", "date": "2021-06-26 19:00:30", "content": "I don't understand how you get the support vector machine objective from hinge loss /!/ I'm taking the MIT online course in machine learning (6.036) and am really confused by the section that introduces support vector machines, coming from hinge loss. I thought I understood both SVMs and hinge loss, but the formulaic expression of SVM really confuses me. I am referring to the [bottom formula in this image](https://i.imgur.com/P6pKyvm.jpg). I don't understand how you can derive the bottom formula from the top formula. I am referring to that which is inside the Lh function (the hinge loss function). \n\nThat is, what's inside the top Lh formula pretty clearly says you take the margin of an individual point and divide it by the reference margin. But what's inside the bottom formula seems to say you take the value given by y * (theta0*x + theta)--which as far as I understand it is pretty meaningless and has nothing to do with distance from a decision boundary. How does actual distance come into the -----> picture !!!  through the bottom formula?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o8gv5y/i_dont_understand_how_you_get_the_support_vector/"}, {"autor": "Azrael1793", "date": "2021-06-26 16:09:38", "content": "How to approach -----> image !!!  (face/portrait) generation? /!/ Considering I would like to do a simple project in my spare time and my knowledge of ML is quite sparse (and frankly old, just toyed a back with simple neural nets) I'd like some direction or suggestion for an approach. My goal is to generate \"ancient\" (medieval, renaissance portrait style maybe? ) or just plain modern realistic photo of people  for starting (in the vein of \"this person does not exist) from scratch, and then maybe expose the result via web service. \n\nSo I was wondering if there's some approachable blackbox-y thing in Python (hopefully) to just maybe feed a face dataset, that spits out an image, that you could drive me too. Consider I should also run this on my 1060 equipped laptop (I suppose, I do not know if I could do it on the cloud for free), I don't know what kind of hardware is required these days to at least have decent results in reasonable times.  \nIs there anything out there that fits my requirements? \n\nHell, writing this I was also considering to just fetch the This person does not exist page and maybe apply some sort of transformation to make it look like an ancient portrait.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o8dpoy/how_to_approach_image_faceportrait_generation/"}, {"autor": "keysee7", "date": "2021-06-26 10:21:27", "content": "What can I use to get the output as in the -----> image !!! ? /!/ [removed]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o8869a/what_can_i_use_to_get_the_output_as_in_the_image/"}, {"autor": "keysee7", "date": "2021-06-26 10:17:30", "content": "What can I use the get the output like in the -----> picture !!! ? /!/ Hi all,  \nI am very beginner in the field. I am not familiar with many libraries and how to use them.\n\nI have a huge weather dataset. I want to get some graphs like in the picture showing relationships between each of the columns. Let's say \"Air temperature/PV output\" and so on. Each of the columns has a very large number of rows. \n\nWhat library can I use to plot it, the way it is in the pictures? I tried Linear Regression, but I would prefer to make it curved like in the picture. Is there some way to do it? I tried few plots from SNS like regplot, displot, but I can't get the output I want. \n\nI know the question itself might sound ridiculous, but I am a noob.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o884jw/what_can_i_use_the_get_the_output_like_in_the/"}, {"autor": "cloud_weather", "date": "2021-08-28 16:04:04", "content": "Very Impressive -----> Image !!!  Upscaling Research from Real-ESRGAN", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pdci4n/very_impressive_image_upscaling_research_from/"}, {"autor": "muzlinofat", "date": "2021-08-28 14:36:37", "content": "Upload dataset to Colab /!/ Hello everyone ,newbie here. I'm following this tutorial from Pytorch [https://pytorch.org/tutorials/beginner/dcgan\\_faces\\_tutorial.html](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) \n\nI'm stuck at very beginning. The tutorial is working with the Celeba dataset wish is huge. In order to use it from inside Colab, do I have to upload the downloaded file to the drive and then mount the drive? Or what is the best way to do it? It is also requires a specific directory. I have attached a -----> picture !!!  below. Sorry, i'm a bit confused.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/h3jy5fqu04k71.png?width=1128&amp;format=png&amp;auto=webp&amp;s=80a34f9ac08fb02424aedf477d685a5632651e6b", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pdawc3/upload_dataset_to_colab/"}, {"autor": "needz", "date": "2021-08-26 20:24:31", "content": "-----> Image !!!  classification question: is this possible? /!/ I have about 250k photos of ~1200 different models of pinball machines.\n\nBefore I try to organize this data and train something, is it even possible to train a n=1200 image classification model?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pc7qn9/image_classification_question_is_this_possible/"}, {"autor": "here_to_escape_", "date": "2021-08-26 17:07:37", "content": "Evaluation/Examination of a model /!/ I am new to deep learning. And I have learned NN from coursera courses. \n\nI have implemented FC NN, CNN (ResNet, UNet), RNN(LSTM, GRU). I know about normalisation, dropout. \n\nRecently I was doing an implementation somewhere and found a section named 'Examining the model'. It was a simple CNN for -----> image !!!  classification. \n\nI have never heard about these things before (As you might agree how incomplete these courses). Namely dimensionality reduction (PCA,t-SNE), confusion matrices etc.\n\n\nWhat did I miss? In which order should these things should hv been learnt? \n\nWhere do I start now?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pc3tqj/evaluationexamination_of_a_model/"}, {"autor": "Lairv", "date": "2021-08-26 12:59:11", "content": "What to do when a model quickly over-fit ? /!/ I know this is a very classical question in ML, and I know there are a lot of google results, but I have already tried them but I still have the same issue.\n\nI am training a GRU network on a time serie classification task, on a custom dataset. Here is a -----> picture !!!  of how most of my training runs goes (acc = accuracy):\n\nhttps://preview.redd.it/sx54affh8pj71.png?width=1475&amp;format=png&amp;auto=webp&amp;s=399af5f1fafa87ee053d6cd13da71ed2c1a64435\n\nThe test almost immediately increase, the test accuracy decrease, while train loss gets near 0. What I have tried so far :\n\n\\- I tried with LSTM, I get the same results, even a bit worst \n\n\\- reducing my model complexity. I tried reducing the hidden feature vector size, and the number of layers. It indeed reduced the overfit, but it also reduced the performances of my model by a lot : I was barely reaching 70% accuracy on trainset and 65% on testset which isn't enough for my task\n\n\\- I tried adding dropout which didn't changed the results\n\nAt this point the only remaining idea I have is that my dataset is not good enough for my model to learn something generalizable, but it takes some time for me to label datas. Do you have any other ideas of what I could do ? Maybe try more complex model architectures ?\n\nPS : btw before someone asks, I have checked the way my metrics are computed and there doesn't seems to have errors in there", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pbyzwj/what_to_do_when_a_model_quickly_overfit/"}, {"autor": "YaswanthBangaru", "date": "2021-07-15 22:03:36", "content": "Would like some thoughts on a project idea related to -----> image !!!  recognition /!/ Hey  folks, I am trying to learn deep learning for image recognition  recently and had a thought it my mind where we could use image  recognition to do scraping of websites. Need some help in understanding  the flexibility and the difficulty level of the project.\n\nSo,  I have got web scraping experience where we try to collect data(both  static and real-time), this idea is more helpful in real-time or  somewhat like every 1 min/1hour/1day time intervals. I wish to collect  time-series data from websites like [investing.com](https://investing.com/)  or other sites where we take a snapshot of the entire webpage using a  script that runs at specific intervals like 1min etc,. and then use  those images to generate tabular data. For instance, this website shows a  metric related to cryptos ( [https://www.bybt.com/FundingRate](https://www.bybt.com/FundingRate)  ) and I would like to only get the fundingrates in a similar tabular  format using image recognition and ignore the rest. I am quite not sure  which area in image recognition should I explore to achieve a goal like  this. I basically intend to capture screenshots of the entire page at  specific time intervals, convert the required data into tabular format  using image recognition and save it as a csv and push the csv to a  database and delete the image and csv then and there.  \nIs it possible  to do such a task using image recognition? I am also willing to do any  preprocessing of images manually like creating rectangular boxes around  the areas of interest, even if a neural net could learn from less  samples and can continue doing it without the rectangular boxes later, I  think it would still be amazing. Please tell me why is it impossible if  not!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ol32dz/would_like_some_thoughts_on_a_project_idea/"}, {"autor": "tylersuard", "date": "2021-07-14 22:55:11", "content": "How to convert an -----> image !!!  classifier with many classes into a binary -----> image !!!  classifier? /!/ I need a binary image classifier and I want to use the latest model.  However, all examples only have 1000+ classes.  How can I convert the model to be useful for my needs?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/okftpg/how_to_convert_an_image_classifier_with_many/"}, {"autor": "tadachs", "date": "2021-07-14 13:58:58", "content": "Problems with Convolutional Autoencoder /!/ Hello there,\n\nI am playing around with the mvtecad dataset and I have a weird problem with my model. For some pictures the reconstruction contains a really weird \"distortion\" that I can not fully explain (-----> image !!!  second row). Where does it come from? I am using a deep convolutional autoencoder with stacks of convolutional and pooling layers. The weird thing to me is that it appears on just a few pictures.\n\nIn hope to not further bother anyone with such questions: Is there maybe an article or something like that that links such \"reconstruction errors\" with flaws in the model or the training process? Like a list of images with such errors and the reason for such errors?\n\nThank you and have a nice day.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ok4wss/problems_with_convolutional_autoencoder/"}, {"autor": "quantumcafe", "date": "2021-07-13 13:03:33", "content": "Question about random walks with erasure paper /!/ Hello everyone, so I've been reading this paper about random walks with erasure [(arxiv link, 2102.09635)](https://arxiv.org/abs/2102.09635), and I'm having trouble understanding the section in the attached -----> image !!! . Isn't the result of equation (3) a (m+n) column vector? How can you \"start new walks from the origin vertex\" with the probability of a column vector? Shouldn't it be a scalar? I realize the column vector will be 0 everywhere except at row s, but I don't get why they make a point about using the Hadamard product when they could just do matrix multiplication to get a single value out.\n\nThanks for any input.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ojf4x1/question_about_random_walks_with_erasure_paper/"}, {"autor": "Crafty_Beautiful_835", "date": "2021-10-27 14:59:33", "content": "Is that an issue if an -----> image !!!  has no label /!/ Example a system for person detectioj.\nIn a video , person A appeared for 10 seconds in sequence.\nTotal of 8 images extracted but sime has labels tagged with correct attributes, some just empty label.\nIs that machine issue or common case.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qgymwj/is_that_an_issue_if_an_image_has_no_label/"}, {"autor": "Top_Task_5375", "date": "2021-10-26 20:09:51", "content": "Find stamps in document images /!/ Hello, just to preface i'm enrolled in a data science class at university, basically we've done in class a few regression examples using different models such as linear regression, logistic regression etc, but the thing is on all the examples the features were always a number int or float, and the value to predict as well. (for example, to predict an apartment price based on it's size etc.) \n\nThe thing is my group has an assignment to use machine learning to find stamps location in document -----> image !!! s, we have the -----> image !!! s with the stamps as well as ground-truth -----> image !!! s (they're the same size of the original -----> image !!!  but all white except the stamp location is all black) but all we have right now is reading the images into an array of pixels (each pixel is a tuple of r, g, b values).  \nMy doubt lies in what to use as features and target values, as i mentioned everything we tried in class used single values for features/target and in this case each image has millions of pixels and the target would be also a range of pixels.\n\nAny help with resources where to learn this would help tremendously, i tried searching for linear regression in images but couldn't find anything helpful, maybe i'm not searching right, thank you for your time.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qgf0wu/find_stamps_in_document_images/"}, {"autor": "Milleuros", "date": "2021-10-26 09:22:41", "content": "Generative model from both noise and fixed parameter /!/ Hi,\n\nI've been digging on GANs and to my understanding the network generates data from random noise.     \nI'm dealing with a problem where it would be convenient if the network could generate data from two entries: random noise, and a hand-made parameter to constrain the phase space.\n\nSuppose (analogy for my problem) that I want images of a bullet impact on bullet-proof glass. The angle of incidence, the way the glass shatters, are random. But the impact is different if the bullet was fired from very far away or point-blank. So I would like to tell the network: \"please give me an -----> image !!!  of a bullet impact, if the shooter was 50 meters away\".  The distance could be any float (discrete!), but physically meaningful, and would impact the simulation. \n\nDoes such a technique exist? How would you go from there?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qg2joh/generative_model_from_both_noise_and_fixed/"}, {"autor": "foyslakesheriff", "date": "2021-10-25 15:24:34", "content": "Use the infamous Deep Fakes project for things other than faces /!/ Hello,\n\nI'm a programmer (mostly Python) dipping my feet into machine learning/AI for a specific project. I understand parts of it at a very high level, but am at the very beginning of my journey. \n\nI'm working on a project involving a lot of object detection in images, specifically car wheels. Basically, my goal is to be able to take any reasonable -----> image !!!  of a car, and take an -----> image !!!  of either just a wheel or an -----> image !!!  of a different car with different wheels, and automatically switch the wheels on the cars. \n\nNow, I'm not totally hopeless here, I've made some very interesting progress and learned a lot as I go. But truthfully, I'm following online tutorials for things like Tensor Flow and Dlib, and adapting those to my specific usecase. \n\nSo far, I've had success with creating a pretty fast wheel identifier that puts a bounding box around every vehicle wheel in an image. It's fast and more accurate than I expected, with only a few hours of training on my gaming GPU. \n\nMy next success was taking that and upgrading it to a model that could apply a mask over the wheel, using a mask-RCNN model. These masks were again surprisingly precise, to almost a few pixels, and more than acceptable unless zoomed all the way in.\n\nMy current challenge is getting those masked wheel images to be able to swap between images, or to apply a new wheel on a car image. To get a decent result that doesn't look fake, it would have to do some minor warping and resizing. To me, this seems like exactly what the Deep Fakes repo does. https://github.com/deepfakes/faceswap\n\nMy question is: how difficult would it be to take that face swap app, sub in my existing mask model, and get the image swap to work. I was feeling so hopeful about my initial success with the bounding box and masking detection models, but now this part has me stuck. Any advice or direction is appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qfj582/use_the_infamous_deep_fakes_project_for_things/"}, {"autor": "Alan491", "date": "2021-10-25 12:45:08", "content": "Need help in creating a type of detector? /!/ I have a 7-8 various type of application icon's from android devices like search button,back button,options button etc. \n\nI want find these icons on an -----> image !!! , and template matching is not useful because of differences in pixel size.\n\nSo I am looking to create a kind of detector that detects the template into the image and returns ROI , So which is a good approach to create a detector?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qffwxp/need_help_in_creating_a_type_of_detector/"}, {"autor": "mikolaj", "date": "2021-10-25 10:04:46", "content": "-----> Image !!!  recognition and linear regression.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qfdeeo/image_recognition_and_linear_regression/"}, {"autor": "phobrain", "date": "2021-10-25 02:52:39", "content": "Call to action: open source believable CGI for all gun effects in -----> film !!! . /!/ Unions and insurance companies likely must take the lead, as ever.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qf7fov/call_to_action_open_source_believable_cgi_for_all/"}, {"autor": "No_Attention1251", "date": "2021-10-25 01:46:08", "content": "Can\u2019t decide on ML Project /!/ I\u2019m currently studying ML at college and I\u2019m really enjoying myself and learning a lot, buuuut that time of the semester where you have to choose your final project has come and once again, I\u2019m unable to come up with a good idea. Once I have a good idea I always put in the hard work and try to dive into deeper and more complex fields. The problem is arriving to an original idea. \n\nFor this project, the only condition the professor gave us is to consider a project that falls into one of the 17 Sustainable Development Goals. The idea is to propose a project that \u201ctries to solve or improve the current conditions of one of the SDGs using ML methods\u201d.\n\nI was thinking researching the bias on data recollection in Mexico and the potential harms to women and minorities that this could have if decision making algorithms are trained using this databases. The other idea I had,which is very cliche, is to detect a potential disease or health risk based on a patients current health status or a detection of the start of P\u00e1rkinson using -----> image !!!  and text recognition. \n\nBut, if anyone of you has any cool idea for a project or a link where I could read about potential projects, I would be infinitely thankful. :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qf6bhv/cant_decide_on_ml_project/"}, {"autor": "QuantumSphinx", "date": "2021-08-14 18:57:03", "content": "Validity of Assumptions in paper \"When Edge Meets Learning: Adaptive Control for Resource-Constrained Distributed Machine Learning\" /!/ [Link to paper (IEEE)](https://ieeexplore.ieee.org/document/8486403)\n\n&amp;#x200B;\n\n[In this -----> image !!!  the loss function for linear regression is defined for a single data point sub-scripted by j.](https://preview.redd.it/tvdpxqrgcdh71.png?width=1270&amp;format=png&amp;auto=webp&amp;s=9afb48cf881af15f2f99bf32f8bc897f6fc496e3)\n\n&amp;#x200B;\n\n[In this -----> image !!!  the above loss for each data point is used to define the loss at a node i for all data points in its local data set.](https://preview.redd.it/t73nhfvqcdh71.png?width=552&amp;format=png&amp;auto=webp&amp;s=df147ecd4fa59d21050b11368b2583d6d8c0103a)\n\n&amp;#x200B;\n\n[In this -----> image !!!  the assumptions about the local loss and the models for which they hold are mentioned. Note that linear regression is one of them.](https://preview.redd.it/nuaek2hcddh71.png?width=1288&amp;format=png&amp;auto=webp&amp;s=a8b85150fef21dc62bcbfb451b28b64008bd4c68)\n\nI am content with assumptions 1.1 and 1.3. For Assumption 1.2, my reasoning goes something like this:\n\n1. The loss for linear regression is Mean Squared Error Loss (MSE). \n2. MSE is quadratic in terms of the weights. \n3. A quadratic function is not Lipschitz continuous for all values of input. ([Wikipedia link to LIpschitz Continuity](https://en.wikipedia.org/wiki/Lipschitz_continuity). \n4. Thus, assumption 2 is invalid.\n\nPerhaps they meant a different Lipschitz continuity than what I have mentioned here or I am making a trivial mistake somewhere, but I have been breaking my head over this for a while now, so any help is appreciated.\n\n&amp;#x200B;\n\nAdditionally, in case anybody requires a pdf of the paper, they can dm me. I don't know if it is prohibited to post links to papers directly.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p4dve6/validity_of_assumptions_in_paper_when_edge_meets/"}, {"autor": "CullenaryArtist", "date": "2021-08-14 14:06:09", "content": "-----> Image !!!  caption generation with high cardinality /!/  \n\nHello,\n\nThe dataset I am working with has word documents of images and captions of what is going on in the image. These captions vary greatly from \u201c1234 Maple Lane North Elevation\u201d to \u201c2nd Floor drywall damage\u201d, \u201cTypical 2 bedroom apartment kitchen\u201d, \u201cWindow repair progress\u201d\n\nI plan on establishing datasets by scraping the word documents for the images and captions. Is this okay to do due to the high degree of cardinality?\n\nThank you.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nCardinality:  As an example, classifying an image as either a dog or a cat would be two degrees of cardinality. In my example, each image caption has almost infinite degree of cardinality and I am not sure if I need to take a certain approach or scrub every piece of data to fit into smaller degrees of cardinality.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p48mnb/image_caption_generation_with_high_cardinality/"}, {"autor": "kimitooku", "date": "2021-08-14 14:01:42", "content": "How to preprocess data for opinion mining? /!/ Not sure what I'm doing wrong. I can't get my code to start from every \\[t\\] and ignore the first few lines.\n\nTxt file:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/0e0ejxh4ybh71.png?width=2174&amp;format=png&amp;auto=webp&amp;s=af9cd8c8a97beba09871f2f5e6bd53467a8bfae4\n\nReadme:\n\n    Symbols used in the annotated reviews: \n    \n      [t]: the title of the review: Each [t] tag starts a review. \n           We did not use the title information in our papers.\n      xxxx[+|-n]: xxxx is a product feature. \n          [+n]: Positive opinion, n is the opinion strength: 3 strongest, \n                and 1 weakest. Note that the strength is quite subjective. \n                You may want ignore it, but only considering + and -\n          [-n]: Negative opinion\n      ##  : start of each sentence. Each line is a sentence. \n\nMy code:\n\n    # load data\n    filename = 'data/Customer_review_data/Canon G3.txt'\n    file = open(filename, 'rt')\n    text = file.read()\n    file.close()\n    # split into words\n    tokens = word_tokenize(text)\n    # convert to lower case\n    tokens = [w.lower() for w in tokens]\n    # remove punctuation from each word\n    table = str.maketrans('', '', string.punctuation)\n    stripped = [w.translate(table) for w in tokens]\n    # remove remaining tokens that are not alphabetic\n    words = [word for word in stripped if word.isalpha()]\n    # filter out stop words\n    stop_words = set(stopwords.words('english'))\n    words = [w for w in words if not w in stop_words]\n    print(words[:100])\n\nOutput:\n\n    ['annotated', 'minqing', 'hu', 'bing', 'liu', 'department', 'computer', 'sicence', 'university', 'illinios', 'chicago', 'product', 'name', 'canon', 'review', 'source', 'amazoncom', 'see', 'readmetxt', 'find', 'meaning', 'symbol', 'excellent', '-----> picture !!! ', 'quality', 'color', 'canon', 'powershot', 'recently', 'purchased', 'canon', 'powershot', 'extremely', 'satisfied', 'purchase', 'use', 'camera', 'easy', 'use', 'fact', 'recent', 'trip', 'past', 'week', 'asked', 'take', '-----> picture !!! ', 'vacationing', 'elderly', 'group', 'took', '-----> picture !!! ', 'camera', 'offered', 'take', '-----> picture !!! ', 'us', 'told', 'press', 'halfway', 'wait', 'box', 'turn', 'green', 'press', 'rest', 'way', '-----> picture !!! ', 'fired', 'away', '-----> picture !!! ', 'turned', 'quite', 'nicely', '-----> picture !!! s', 'thusfar', '-----> picture !!! ', 'quality', 'work', 'constituants', 'owned', 'highly', 'recommended', 'canon', '-----> picture !!! ', 'quality', '-----> picture !!! ', 'quality', 'easily', 'enlarging', '-----> picture !!! s', 'x', 'visable', 'loss', '-----> picture !!! ', 'quality', 'even', 'using', 'best', 'possible']", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p48k2q/how_to_preprocess_data_for_opinion_mining/"}, {"autor": "Successful_Boat_3099", "date": "2021-08-14 11:57:48", "content": "\ud83d\udd6e Your weekly digest of daily machine learning doses! /!/ &gt;This is a series of posts that I post almost daily. I call them \"your daily dose of machine learning\"  \n&gt;  \n&gt;Every Saturday I'll make a recap of previous week's daily doses.\n\nThis week's theme has been about **deep learning for -----> image !!!  recognition**.  \n\n\nMonday : Progress in the field of image classification.\n\nTuesday : Inception (GoogLeNet).\n\nWednesday : ResNet.\n\nThursday : SE-ResNet.\n\nFriday : EfficientNets. \n\nIf you're interested in any of these topics, check out my profile here on Reddit.\n\n&amp;#x200B;\n\nI'll be happy to connect with you on your [***favorite social network***](https://withkoji.com/@Nour_Islam)!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p46p6a/your_weekly_digest_of_daily_machine_learning_doses/"}, {"autor": "Successful_Boat_3099", "date": "2021-08-13 09:10:07", "content": "\ud83d\udc8aYour daily dose of machine learning! /!/ &gt;This is a series of posts that I post almost daily. I call them \"your daily dose of machine learning\".\n\n&amp;#x200B;\n\nOne of the ways that AI researchers follow when designing new neural network architectures is by changing different parameters such as depth of and width of output feature maps.  \n\n\nBut is there a principled method to scale up ConvNets that can achieve better accuracy and efficiency?  \n\n\nThis is the exact question that the researchers who created EfficientNets asked.\n\nEfficientNets is a family of convolutional networks architectures that were built by following a systematic approach when varying parameters.\u00a0\n\nThree factors are varied : input -----> image !!!  resolution, depth of the network and width of the network.\n\nInstead of arbitrarily varying these factors like it was done before, researchers propose a \u201ccompound scaling method\u201d.\n\n&amp;#x200B;\n\nThis method can be summarized in 2 points:\n\n1 - They start with a baseline model, found through a grid search. This model would have a width W, a depth D and a resolution R.\n\n2 - Then they scale these parameters so that the new model would have parameters W\\^n, D\\^n and R\\^n.  \n\n\nBy doing this, they get a family of models which they call EfficientNets. These models are both accurate and efficient.  \n\n\nFor example, at the time the official EfficientNets paper was released, EfficientNet-B7 achieved state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet at that time. \n\nFollow me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!***", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p3jswv/your_daily_dose_of_machine_learning/"}, {"autor": "abcaircraft", "date": "2021-08-12 18:57:31", "content": "What is GAN? How does it work? What is the basic idea of GAN? /!/ It's with reference to the blog (link below) by Comma AI,\n\n[https://blog.comma.ai/towards-a-superhuman-driving-agent/](https://blog.comma.ai/towards-a-superhuman-driving-agent/)\n\n&amp;#x200B;\n\n[Comma AI's end-to-end approach as per their blog- https:\\/\\/blog.comma.ai\\/end-to-end-lateral-planning\\/](https://preview.redd.it/kj0cgp8y4zg71.png?width=1099&amp;format=png&amp;auto=webp&amp;s=8a943bdc8eecee52edd2d5290155e84b023e909a)\n\n&amp;#x200B;\n\n[Above is the original -----> image !!!  of a road, below is a reconstruction made by GAN \\(Description given below\\)](https://preview.redd.it/bhauisr15zg71.png?width=652&amp;format=png&amp;auto=webp&amp;s=e6047ea797f88e5cd4fc48fbca256b51961c7792)\n\n&amp;#x200B;\n\n[Excerpts from the blog \\\\\"Towards a superhuman driving agent\\\\\" by Comma AI.](https://preview.redd.it/6wacp4845zg71.png?width=1198&amp;format=png&amp;auto=webp&amp;s=9a05f79c414a78316226a43e21082eadc12ffa25)\n\nMy doubt is how do they construct back an -----> image !!!  from a compressed 1024-dimensional feature vector? It seems like they are using a technique called GAN. What is GAN? How does it work? What is the basic idea of GAN?\n\nIn the above excerpt from their blog, which image are they referring to in the sentence highlighted by the green color?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p36c7o/what_is_gan_how_does_it_work_what_is_the_basic/"}, {"autor": "Successful_Boat_3099", "date": "2021-08-12 10:46:00", "content": "\ud83d\udcccYour daily dose of machine learning : SE-ResNet /!/ &gt;This is a series of posts that I post almost daily. I call them \"your daily dose of machine learning\". \n\nSE-ResNet is a neural network architecture that\u2019s considered an upgrade of ResNet by introducing \u201cSqueeze and excitation\u201d operations inside the neural network.\n\nThe squeeze operation is done using global average pooling.\n\nThe excitation operation is done by the activation function (sigmoid, relu, \u2026).\n\nThe -----> image !!!  below shows the SE-ResNet module.\n\nSE-ResNet network is the winner of ImageNet-2017 with top-5 error rate of 2.25%.\n\n&amp;#x200B;\n\nFollow me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!***\n\n&amp;#x200B;\n\nhttps://preview.redd.it/29r22s0cpwg71.png?width=865&amp;format=png&amp;auto=webp&amp;s=02b0cbc5036d96baa1365429dac629e84d10131c", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p2xb3z/your_daily_dose_of_machine_learning_seresnet/"}, {"autor": "kbrdsmsh-asdf", "date": "2021-08-12 05:16:26", "content": "How are references used in GANs? /!/ I'm asking the question in the context of GANs like [deepfake with images](https://www.youtube.com/watch?v=Zkrcx3_DtCw) or [lineart colorization with reference images](https://arxiv.org/pdf/2005.05207.pdf). \n\nI understand that the general idea of a GAN is that there is a generator and discriminator that try to best each other, resulting in a generator that's good at generating close-to-perfect ground truth and a discriminator that's great at discerning real from fake. \n\nWhere does a reference image/ video clip come in after the training is done? \n\nAll the GANs I've looked at (e.g. MNIST hand-written digits) just take what the generator has created at each epoch of training. Where does the reference -----> image !!!  get involved and how is it used in the pipeline to help the generator generate an output that takes the reference?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p2ta5l/how_are_references_used_in_gans/"}, {"autor": "average_coffee", "date": "2021-08-12 03:41:05", "content": "Scene Text Recognition to Speech? /!/ hi everyone! I would just like to ask if anyone knows a research that does STR to speech (without going through text), the usual ones i find are seq2seq which does sequential methods (like CTC) to an -----> image !!!  for text recognition and localization and outputs texts. Is there a direct way to go from STR to speech without going through text? Thanks in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p2rs2i/scene_text_recognition_to_speech/"}, {"autor": "SnooPeripherals4051", "date": "2021-08-12 01:31:18", "content": "Random Spike in validation loss curves. I'm lost.. Need suggestions! /!/ Hey Guys, \n\nI've been trying to tune my hyperparameters for a time series forecasting model using GRU. Unfortunately, my results are a bit whack because some of the runs will have a random spike in train/test loss (refer to -----> picture !!! ). This then makes it hard to trust the final result as it seems to have learnt the data well prior to 40 epochs then for some reason, during epoch 42/43, the update completely throws the model off and it is then required to relearn from square one. I'm wondering if I leave epochs for long enough, this will happen to all the runs. \n\nI've had a look into why this may be the case but I've found no concrete answers. \n\nThere might be a couple of reasons which I'll touch on but would like some other insights too.  This is my first proper deep learning project so I haven't had to rigorously train parameters like this yet.. It's quite frustrating!\n\nCurrently, I think it might be: \n\n\\-Having a small batch size: 32. This makes it hard to converge however could this cause such a random spike?\n\n\\-Data set: Could it be NAN or 0 or outliers? \n\n\\-Optimizer: Using ADAM so mini-batch grad descent might have an unlucky set of data that causes a spike, should I use full batch GD? (or will this take too long?) Any suggestions? \n\n*Processing img 38r2tbegwtg71...*", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p2qxrw/random_spike_in_validation_loss_curves_im_lost/"}, {"autor": "jshkk", "date": "2021-08-11 21:41:33", "content": "Is -----> image !!!  normalization preprocessing *really* needed in GANs? /!/ Let me start by saying I get why some sort of normalization is needed for typical neural network problems. For one, the simple fact that input variables can be on vastly different scales is an obvious note.\n\nHowever, when working on an image generation problem using a GAN, the story seems different.\n\nIn the case of a GAN, once you've got your image converted to a tensor, at least in PyTorch that tensor's range will be (0, 1) even for color images. So things are already on the same scale.\n\nNow if we mean-shift and divide by the standard deviation, our Generator then must learn to produce images within that new (arguably compressed) distribution. My gut is that this might have some deleterious effects even if I can't quite spell them out.\n\nGranted, a convenience of that normalization of the images is the new range of (-1, 1) which makes the typical tanh output of a generator, but we could also get that by a simple feature rescale which would maintain more of the original distribution structure.\n\nAm I overly worried? Completely missing something? Help me understand why normalization really is needed if so here?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p2mulw/is_image_normalization_preprocessing_really/"}, {"autor": "BenoBoyz", "date": "2021-08-11 17:48:41", "content": "Issues with training efficientdetd0 for object detection /!/ I\u2019m still quite new to using TF2\u2019s object detection API and I\u2019ve trained a few models before with one or two classes with decent success but this time I\u2019m training a model with 46 classes. I have 342 training -----> image !!! s and 91 test -----> image !!! s(each -----> image !!!  contains multiple classes to detect). I\u2019m using efficientdetd0 which I understand isn\u2019t the most accurate, but I have trained over two million steps now and the model is not detecting much in even some the images I give from the training data. I\u2019m using the google cloud tpu in Colab so training a few million steps isn\u2019t too time consuming for me. \n\nMy batch size is 8 (can\u2019t lower it because the tpu has like 8 cores or something not really sure how it works)\n\nLearning rate base is 8e-4(if I raise it I would get a loss=NAN issue a few thousand steps in) and I didn\u2019t really tweak anything else with the config file as I didn\u2019t really understand what all the options meant.\n\nCould someone enlighten me as to how my model could be performing so poorly after training for so long? Could it be that my data has too much noise? I also understand that I don\u2019t have enough training images but it\u2019s the best I\u2019ve got after scouring the internet.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p2i6q9/issues_with_training_efficientdetd0_for_object/"}, {"autor": "Ramgendeploy", "date": "2021-02-10 16:13:47", "content": "Made a Guide on how to use Style transfer and 3D -----> photo !!!  Inpainting", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lgx0mt/made_a_guide_on_how_to_use_style_transfer_and_3d/"}, {"autor": "LoLingLikeHell", "date": "2021-02-10 13:11:06", "content": "Are there pre-trained Machine Learning models on COCO dataset ? /!/ Hi,\n\nI want to ask you if there are any pre-trained Machine Learning models (SVM, decision trees...) on the COCO dataset. If there aren't, have you heard of some good default parameters to start with for such complex datasets and useful feature descriptors to use ? \n\nI started by taking a subset of the whole dataset but for each model there are many parameters on top of all the possibilities of features that could be extracted from each -----> image !!!  and it's quite overwhelming.\n\nThank you by advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lgtblt/are_there_pretrained_machine_learning_models_on/"}, {"autor": "thevatsalsaglani", "date": "2021-02-10 08:22:32", "content": "How can the new OpenAI CLIP be used for semantic -----> image !!!  search? /!/ OpenAI recently published a multi-modal training approach to train a model to learn from image text pairs and we can use the same pre-trained model for semantic image search. I have described how's in this blog [https://medium.com/towards-artificial-intelligence/what-is-clip-contrastive-language-image-pre-training-and-how-it-can-be-used-for-semantic-image-b02ccf49414e](https://medium.com/towards-artificial-intelligence/what-is-clip-contrastive-language-image-pre-training-and-how-it-can-be-used-for-semantic-image-b02ccf49414e)  \n\n\nYou can try the semantic search live at [https://share.streamlit.io/vatsalsaglani/clipsemanticimagesearch/streamlitClip1/streamlitapp.py](https://share.streamlit.io/vatsalsaglani/clipsemanticimagesearch/streamlitClip1/streamlitapp.py)  \n\n\nIt looks something like this\n\n&amp;#x200B;\n\nhttps://i.redd.it/6lpyp7a31mg61.gif", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lgp9d2/how_can_the_new_openai_clip_be_used_for_semantic/"}, {"autor": "RedPacketSecurity", "date": "2021-02-09 17:57:43", "content": "Using machine learning to convert a -----> photo !!!  into a 3d model /!/ I have been looking for something that I can use to take a photo and turn it into a 3d model in blender.  \n\n\nI have seen a few projects like this  \n\n\n[https://nv-tlabs.github.io/DIB-R/](https://nv-tlabs.github.io/DIB-R/)  \n\n\n  \nAnyone able to point me in the right direction of something that will use AI/ML to create 3d from 2d and export to blender?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lg944t/using_machine_learning_to_convert_a_photo_into_a/"}, {"autor": "daniel-data", "date": "2021-02-09 16:13:01", "content": "Building a Product Recommendation System with Collaborative Filtering /!/ We will go deeper into building a product recommendation system that we can better target customers with\n\n&amp;#x200B;\n\n[-----> Image !!!  by Amazon](https://preview.redd.it/kq1ugdvy7hg61.png?width=1574&amp;format=png&amp;auto=webp&amp;s=cb61f3687b3db66960bb75b05726cc13f2eb29d6)\n\n&amp;#x200B;\n\nWe will go deeper into building a product recommendation system that we can better target customers with, using product recommendations that are tailored to individual customers. Studies have shown that customized product recommendations improve conversion rates and customer retention rates.\n\nA product recommendation system is a system whose objective is to predict and compile a list of items that a customer is likely to buy. Referral systems have gained much popularity in recent years and have been developed and implemented for various commercial use cases\n\nFor example,\n\n* The media service provider, Netflix, uses referral systems to recommend movies or television programs for individual users who are likely to watch\n* The e-commerce company Amazon uses recommendation systems to predict and display a list of products that the customer is likely to buy\n* The music streaming service, Pandora, uses music recommendation systems for its listeners.\n* The use of a referral system does not stop here. It can also be used to recommend users related articles, news or books\n\nWith the potential to be used in a variety of areas, referral systems play a critical role in many businesses, especially e-commerce and media businesses, as they directly impact sales revenue and user engagement\n\nGenerally, there are two ways to develop a list of recommendations:\n\n### 1. Collaborative filtering\n\nThe collaborative filtering method is based on users\u2019 previous behaviors, such as the pages they saw, the products they bought, or the ratings they gave to different items. The collaborative filtering method uses this data to find similarities between users or articles, and recommends the most similar articles or content to users.\n\nThe basic assumption behind the collaborative filtering method is that those who have seen or bought similar content or products in the past are likely to see or buy similar types of content or products in the future.\n\nThus, based on this assumption, if one person bought items A, B, and C and another person bought items A, B, and D in the past, it is likely that the first person will buy item D and the other person will buy item C, since they share many similarities with each other\n\n### 2. Content-based filtering\n\nOn the other hand, content-based filtering produces a list of recommendations based on the characteristics of an article or user. It usually examines the keywords that describe the characteristics of an article. The basic assumption of the content-based filtering method is that users are likely to see or buy items similar to those they have bought or seen in the past\n\nFor example, if a user has listened to some songs in the past, the content-based filtering method will recommend similar song types that share similar characteristics to those the user has already heard\n\n### Building a Product Recommendation System with Collaborative Filtering\n\nAs mentioned, a collaborative filtering algorithm is used to recommend products based on user behavior history and similarities between them. The first step in implementing a collaborative filtering algorithm for a product recommendation system is to build a user-to-item matrix\n\nA user-to-item matrix comprises individual users in the rows and individual elements in the columns. It will be easier to explain with an example. Take a look at the following matrix\n\nThe rows of this matrix represent each user and the columns represent each element. The values in each cell represent whether the given user bought the given item or not. For example, user 1 has purchased items B and D and user 2 has purchased items A, B, C and E\n\nTo build a product recommendation system based on collaborative filtering, we need to first build this type of user-to-item matrix. With this user-to-item matrix, the next step in building a product recommendation system based on collaborative filtering is to calculate the similarities between users\n\nTo measure similarities, the similarity of cosines is often used. The equation for computing cosine similarity between two users looks like this\n\nIn this equation, U1 and U2 represent user 1 and user 2. P1i and P2i represent each product, i, that user 1 and user 2 have purchased. If you use this equation, you will get 0.353553 as the cosine similarity between users 1 and 2 in the example above and 0.866025 as the cosine similarity between users 2 and 4\n\nAs you can imagine, the greater the similarity of the cosine, the more similar the two users. Thus, in our example, users 2 and 4 are more similar to each other than users 1 and 2. Finally, when using a collaborative filtering algorithm for product recommendations, there are two approaches that can be taken: a user-based approach and an item-based approach\n\nAs the names suggest, the user-based approach to collaborative filtering uses the similarities between users. On the other hand, the article-based collaborative filtering approach uses the similarities between the items. This means that when we calculate the similarities between the two users in the collaborative filtering of the user-based approach, we need to build and use a user-to-article matrix, as we have discussed above\n\nHowever, for the item-based approach, we need to calculate the similarities between the two elements, and this means that we need to build and use an item-to-user matrix, which we can obtain by simply transposing the user-to-item matrix.\n\nLet\u2019s discuss how to build a product recommendation system using Python. We will begin this section by analyzing some e-commerce business data and then discuss the two approaches to building a product recommendation system with collaborative filtering\n\nWe will use one of the publicly available data sets from the UCI\u2019s Machine Learning Repository, which can be found at this [link.](https://archive.ics.uci.edu/ml/datasets/online+retail#)\n\nYou can follow this link and [download the data in Microsoft Excel format, in a file called Online Retail.xlsx](https://archive.ics.uci.edu/ml/machine-learning-databases/00352/)\n\nYou can [download the code](https://www.narrativetext.co/the-analyst/building-a-product-recommendation-system-with-collaborative-filtering) and the whole explanation here\n\nCheers!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lg6ozj/building_a_product_recommendation_system_with/"}, {"autor": "Laurence-Lin", "date": "2021-02-09 08:18:08", "content": "What might cause my model unable to classify merely 2 samples for binary classification? /!/ I've a -----> image !!!  binary classification task, which I want to identify whether an -----> image !!!  contains overlapping text. My training accuracy is poor, so I decide to test if my training data is sufficient to provide the feature for classify. \n\n&amp;#x200B;\n\nI give only one sample for normal image, and one sample for overlapping image. However, when I feed both image to ResNet50, it shows the accuracy bounding between 0.6667 and 0.3333, even after reached 100 iterations it's still oscillating. \n\n&amp;#x200B;\n\nThe training loss is binary cross entropy and decreased to a extreme small value during training, it seems no matter the loss decreased the model performance is still bad.\n\n&amp;#x200B;\n\nI'm thinking about change a loss function, but I have no idea what might cause the model perform so poorly on merely 2 samples?\n\n&amp;#x200B;\n\nThank you for any help!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lfxyb7/what_might_cause_my_model_unable_to_classify/"}, {"autor": "sicp4lyfe", "date": "2021-02-09 06:47:23", "content": "the highly touted Deep Learning with PyTorch book /!/ doesn't mention 'speech' in the whole thing. i'm so sick of ML pedagogy being dominated by -----> image !!!  applications.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lfwpr9/the_highly_touted_deep_learning_with_pytorch_book/"}, {"autor": "YouAgainShmidhoobuh", "date": "2021-02-08 17:12:51", "content": "jax/flax - derivative with respect to input /!/ Hey all; I'm trying to figure out how to get the gradient of the output with respect to the input. \n\nSomething along the lines of GRAD-cam tools, lets say I have 4 classes and want to see what parts of the -----> image !!!  have high activations for a given class. In pytorch this is a lot easier imo, it would be the following code to see what is highlighted for class 3:\n\n    gradient = torch.autograd.grad(\n            outputs=output_score,\n            inputs=input_image,\n            grad_outputs=torch.tensor([0, 0, 1, 0]),\n        )\n\nHow would I do this in jax/flax?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lfgi1u/jaxflax_derivative_with_respect_to_input/"}, {"autor": "theOriginalDestroyer", "date": "2021-04-29 17:31:14", "content": "Question about siamese CNNs for face verification. /!/ I am currently trying to train a siamese net for face verification purposes. I am using triplet loss and I am getting suspiciously good results after training for 1 to 2 epochs (99.8% accuracy on the validation set and test). When I try and manually feed the network images It doesn't seem to be as accurate though my sample size is very small. Is this normal?  Fyi I am using the CelebA database I am will be posting -----> image !!!  of network shortly.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n19jjf/question_about_siamese_cnns_for_face_verification/"}, {"autor": "Dario_Della", "date": "2021-04-29 15:23:47", "content": "Mutiple backgrounds for object detection with mask r cnn /!/  2\n\nI'm trying to train a mask r cnn algorithm for object detection. Right now I have 21 classes with the corresponding annotations but my task is to detect only 13 of them, the other 8 classes should be recognize as background.\n\nFor the CustomConfig code part I tried to run the code setting the number of classes for the background with this three combinations:\n\n* 1+13\n* 8+13\n* 9+13 But none of them worked.\n\n&amp;#8203;\n\n    class CustomConfig(Config):\n        \"\"\"Configuration for training on the custom  dataset.\n        Derives from the base Config class and overrides some values.\n        \"\"\"\n        # Give the configuration a recognizable name\n        NAME = \"object\"\n    \n        # We use a GPU with 12GB memory, which can fit two images.\n        # Adjust down if you use a smaller GPU.\n        IMAGES_PER_GPU = 2\n    \n        # Number of classes (including background)\n        NUM_CLASSES = 1 + 13\n        # Number of training steps per epoch\n        STEPS_PER_EPOCH = 10\n    \n        # Skip detections with &lt; 85% confidence\n        DETECTION_MIN_CONFIDENCE = 0.85\n\n The error that came out: \n\n    ---&gt; 56  num_ids = [name_dict[a] for a in objects]\n    KeyError: 'Class20'    (this class should be in the background)\n\n Regarding the def load custom function I only added the 13 classes that I want to detect: \n\n    class CustomDataset(utils.Dataset):\n    \n        def load_custom(self, dataset_dir, subset):\n            \"\"\"Load a subset of the Dog-Cat dataset.\n            dataset_dir: Root directory of the dataset.\n            subset: Subset to load: train or val\n            \"\"\"\n            # Add classes. We have only one class to add.\n            \n            self.add_class(\"object\", 1, \"01\")\n            self.add_class(\"object\", 2, \"02\")\n            self.add_class(\"object\", 3, \" 03\")\n            self.add_class(\"object\", 4, \"04\")\n            self.add_class(\"object\", 5, \"05\")\n            self.add_class(\"object\", 6, \"06\")\n            self.add_class(\"object\", 7, \"07\")\n            self.add_class(\"object\", 8, \"8\")\n            self.add_class(\"object\", 9, \"9\")\n            self.add_class(\"object\", 10, \"10\")\n            self.add_class(\"object\", 11, \"11\")\n            self.add_class(\"object\", 12, \"12\")\n            self.add_class(\"object\", 13, \"13\")\n    \n            # Train or validation dataset?\n            assert subset in [\"TRAIN\", \"TEST\"]\n            dataset_dir = os.path.join(dataset_dir, subset)\n    \n            annotations1 = json.load(open(os.path.join(dataset_dir, \"set_labelling.json\")))\n            new_dict = {}\n            \n            for item in annotations1:\n              name = item['filename'], item['Layer']\n              new_dict[name] = item\n            annotations = list(new_dict.values()) # don't need the dict key\n    \n            # Add images\n            for a in annotations:\n                polygons = [r['shape_attributes'] for r in a['region']]\n                objects = [s['region_attribute']['name'] for s in a['region']]\n                #print(polygons)\n                print(\"objects:\",objects)\n                name_dict = {\"01\": 1,\n                              \"02\": 2,\n                              \"03\": 3,\n                              \"04\": 4,\n                              \"05\": 5,\n                              \"06\": 6,\n                              \"07\": 7,\n                              \"08\": 8,\n                              \"09\": 9, \n                              \"10\": 10,\n                              \"11\": 11, \n                              \"12\": 12, \n                              \"13\":13} \n               \n                num_ids = [name_dict[a] for a in objects]\n         \n                \n                print(\"numids\",num_ids)\n                -----> image !!! _path = os.path.join(dataset_dir, a['filename'])\n                -----> image !!!  = sk-----> image !!! .io.imread(-----> image !!! _path)\n                height, width = -----> image !!! .shape[:2]\n    \n                self.add_-----> image !!! (\n                    \"object\",  ## for a single class just add the name here\n                    -----> image !!! _id=a['filename'],  # use file name as a unique -----> image !!!  id\n                    path=image_path,\n                    width=width, height=height,\n                    polygons=polygons,\n                    num_ids=num_ids\n                    )\n\n I would not to delete the background classes annotations in the json file, so how can I handle this error?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n16rbn/mutiple_backgrounds_for_object_detection_with/"}, {"autor": "War_Smooth", "date": "2021-04-29 13:32:06", "content": "-----> Image !!!  labelling /!/ Hi guys, I have been learning ML for some time now and I want to apply it in my little project.  I have images  of cocoa beans which I want to classify into 4 classes. How do I label the images? Is it going to be a csv file containing 2 columns; image and classes (in text)? Thanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n14f5w/image_labelling/"}, {"autor": "TheoreticallyBlank", "date": "2021-04-29 06:47:12", "content": "Training an -----> image !!!  classifier on a custom dataset with custom labels? /!/ Are there any good resources for training a classification model on a niche dataset or fine-tuning an existing one?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n0yn7q/training_an_image_classifier_on_a_custom_dataset/"}, {"autor": "crubier", "date": "2021-04-28 18:22:37", "content": "Announcing Labelflow, the open source -----> image !!!  labeling and dataset cleaning platform. /!/ Hi, all! Announcing Labelflow ( [https://www.labelflow.net/](https://www.labelflow.net/) ), the open source image labeling and dataset cleaning platform.\n\nWe are a team of 9 people with experience in image labeling and dataset curating to build quality datasets for deep learning.  We were frustrated by the amount of scripts required to move the data back and forth between tools, and by the lack of control over our data, especially when we have data that can\u2019t easily be shared. \n\nSo we started building Labelflow, an image labeling tool with all the bells and whistles, but with an open source backend that you can connect to your own data stack easily. Let me know what you think, and feel free to request early access to get up to 50% off when we release it in a few months!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n0l9kz/announcing_labelflow_the_open_source_image/"}, {"autor": "buangakun3", "date": "2021-04-28 15:36:54", "content": "Could someone please explain what wrong with my code? /!/ I'm using posenet to detect gestures, the system is divided into two;\n\n1. The front-end that sends the webcam's -----> image !!!  to the backend \n2. The back-end receives the image, analyzes, and spits out the coordinates back to the front-end.\n\nThe front-end code;\n\n    const imageData = refCam.current.ctx.getImageData(0, 0, 320, 180);         \n    const buffer = imageData.data.buffer;           \n    socket.emit(\"signal\", buffer);\n\nThe back-end, which is the source of my confusion;\n\n    let pose = []\n    if(model) {\n      const buffer = new Uint8Array(data);\n      const temp = tf.tensor(buffer, [320, 180, 4]); // 320x180 is the width x height of the camera, 4 I don't know why I put that.\n      const image = tf.slice(temp, [0, 0, 0], [-1, -1, 3]); // I have no idea what this does, but it removes the error.\n      temp.dispose();\n      pose = await model.estimateSinglePose(image, {\n        flipHorizontal: false\n      });\n    }\n    socket.emit(\"messageAll\", pose);\n\nI have to admit the back-end code was taken from another sample, but basically, the estimation spits out the wrong numbers.\n\nIf somebody can tell me what I did wrong, please?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n0hjpb/could_someone_please_explain_what_wrong_with_my/"}, {"autor": "195monke", "date": "2021-04-27 21:57:31", "content": "Are there any pre-trained general-purpose -----> image !!!  classification adversarial attack models? /!/ As the question states - a few days ago I looked for general image classification adversarial attacks pre-trained models, but all I could find were tutorials of how to create network-specific adversarial attacks that not just try to reduce the confidence of the network about an image, but also try to convince the network there is a specific different object in the image instead of the one actually presented.\n\n!\\[Here is an example of what I mean\\]([https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F4000%2F1\\*VMW1j6A\\_kh4LyuNFyEw3hw.png&amp;f=1&amp;nofb=1](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F4000%2F1*VMW1j6A_kh4LyuNFyEw3hw.png&amp;f=1&amp;nofb=1))\n\nThis obviously requires training every time you want to create an attack, but all I'm looking for is a pre-trained model that simply makes it harder for image classification networks to classify an image. I'm aware that when adversarial attacks are not network-specific they'll be less effective, but even if it could work a quarter of the times I'd be satisfied.\n\nI just want to clarify that I've never trained a model in my life and I don't understand much about machine learning and artificial intelligence other than some basic facts, so if I used any terminology incorrectly in this post or what I'm saying simply doesn't make any sense I would like to know", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mzzqsr/are_there_any_pretrained_generalpurpose_image/"}, {"autor": "quickswitch123", "date": "2021-02-24 00:20:46", "content": "Ideas for Raspberry PI robotics/ML projects to do with my 16-year-old brother? /!/ Greetings! I am a first-year Ph.D. student in ML and robotics and this is my first post on this sub :). My little brother expressed interest in computer science, so I wanted to feed his interest by doing a robotics/ML project with him.\n\nI was wondering if anyone has any ideas for a Raspberry PI robot project with some ML vision component that I could do with him. I have access to a laser cutter and 3D printer, so I could definitely fabricate some pieces (ideally simple pieces).\n\nI found this [mobile robot](https://www.sunfounder.com/collections/robotics/products/smart-video-car?gclid=Cj0KCQiA7NKBBhDBARIsAHbXCB7eM7WE-a3PFiDer4KR7sWl9TWS41X_TkDPr5vuGQg1Siao46cP5JIaAgAcEALw_wcB) with a -----> camera !!!  that could be interesting, but maybe building something more from the ground up would be better.\n\nI would love any input or ideas!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lqxw1x/ideas_for_raspberry_pi_roboticsml_projects_to_do/"}, {"autor": "KSW1414", "date": "2021-02-23 11:22:21", "content": "How much can you learn to do in 1 month /!/ Hi, I am 17 and very new to machine learning. Currently I have just some basics understanding of python and its modules (numpy, matplotlib, pandas etc).\n\nI am thinking of starting to learn and explore the field of machine learning, starting with the book \u201cHands-On Machine Learning with Scikit-Learn and TensorFlow\u201d. \n\nThe problem is, I have only 1 month before getting into my school\u2019s exam season. So I\u2019m just curious about how much I can possibly learn to do in 1 month. Is one month enough time to gain skills to do things such as reinforcement learning program or voice/-----> image !!!  recognition program.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lqg4w1/how_much_can_you_learn_to_do_in_1_month/"}, {"autor": "pashupati_98", "date": "2021-02-23 04:39:39", "content": "Getting Started with Hyper-parameter optimization/tuning /!/ If you have gained a basic understanding of model training and evaluation. The next thing for you to learn should be hyper-parameter optimization. It's a powerful step to increase your model performance. Have a look at the -----> picture !!!  attached.  You may think \"It ain't much\" but actually it's a significant improvement.\n\nI have prepared a basic notebook explaining what is hyper-parameter optimization and what techniques/algorithms are used to do it. It talks about -\n\n* Manual Search\n* Grid Search\n* Random Search\n* Bayesian Optimization\n\nNotebook - [https://www.kaggle.com/pashupatigupta/getting-started-with-hyper-parameter-optimization](https://www.kaggle.com/pashupatigupta/getting-started-with-hyper-parameter-optimization)\n\nPlease recommend any other technique/method that you know!\n\nHappy Learning!\n\nhttps://preview.redd.it/6c5p6czdo5j61.png?width=720&amp;format=png&amp;auto=webp&amp;s=b5e165cc5d91479e3bc94b7eeaffad1d07cc634c", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lq9duz/getting_started_with_hyperparameter/"}, {"autor": "dangnabbitdamnit", "date": "2021-02-22 22:51:06", "content": "Wondering about the feasibilty of a ML project and where to go from there (graph generation for texture synthesis) /!/ With the recent release of CLIP, and the \"Big Sleep\" notebook that uses it and BigGAN to create a text-to------> image !!!  generator, I was wondering about making something similar but instead of directly using a GAN, it would create a \"graph\" from a given library of nodes similar to [Substance Designer](https://3dtextures.me/wp-content/uploads/2019/01/substance_designer_graph.jpg) and then that graph would be computed to get a texture. I figured this would result in much cleaner images than those generated by GANs which can look sometime kinda dreamy or uncanny. It would also be fairly easy to generate lots of random synthetic data to learn from and I thought that it could be interesting to see what the program would come up with with a limited toolbox if you input unconventionnal prompts.\n\nThat said I frankly know almost nothing about graphs and GNNs so I don't even know if this is realistic. The graphs would contain relatively few nodes as far as graphs go (like anywhere between 10 and 50 would be great already, although ideally the more nodes the more complexity), but at the same time you can't just compute the CLIP loss gradients the same way as a GAN since it's not continuous, and I feel like random data would be so noisy that being able to learn from it would be bordering on AGI (but then again another interesting possibility would be to generate the textures in tiny resolutions like [Tilemancer](https://img.itch.zone/aW1hZ2UvNTgyNDcvMjYyNDUwLnBuZw==/original/6Rmf7A.png), which is like Substance Designer but for pixel art and would require significantly less compute *and* nodes)\n\nWhat do you think, is this realistically possible? What kind of architecture/algorithm would be fit for this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lq2e3m/wondering_about_the_feasibilty_of_a_ml_project/"}, {"autor": "saniauzma", "date": "2021-02-22 19:15:26", "content": "How to read 2000 images data ? /!/ Hello everyone!\nI am a beginner in ml and dl world,\nI am building an -----> image !!!  classification model on 2000 -----> image !!! s.\nI need to read this data into my program, I have tried below code, but its taking alot of time.\nIs there any effective way to load this data as fast as possible?\n\n\n\ndata\u00a0=\u00a0[]\n\nfor\u00a0category\u00a0in\u00a0categories:\n\u00a0\u00a0path\u00a0=\u00a0os.path.join(datadir,\u00a0category)\n\u00a0\u00a0labels\u00a0=\u00a0categories.index(category)\n\u00a0\u00a0for\u00a0imgpath\u00a0in\u00a0os.listdir(path):\n\u00a0\u00a0\u00a0\u00a0imgs\u00a0=\u00a0image.load_img(os.path.join(path,\u00a0imgpath),target_size=(160,120))\n\u00a0\u00a0\u00a0\u00a0data.append((imgs,\u00a0labels))", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lpx33d/how_to_read_2000_images_data/"}, {"autor": "AwesomeGuy_59", "date": "2021-02-22 16:07:29", "content": "Library to create Heatmaps /!/ Hello, could anyone recommend a good library for creating heatmaps? I am trying to display the most common positions reached by a simulated robot in a 2D control problem. I still don't know how much granularity I should use, but basically I want to display the heatmaps on top of an -----> image !!!  of the field. \n\nAny tutorials are very welcome too.\n\nThank you so much.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lps5wo/library_to_create_heatmaps/"}, {"autor": "delasislas", "date": "2021-03-08 00:23:57", "content": "Allowing for odd shaped and unbalanced images in dataset /!/ Hi,\nI'm just getting into machine learning as a bit of a side project. I was working through the Tensorflow -----> image !!!  analysis on the flowers. I was wanting to try to use it on a different dataset, but the images range from small to large and some are elongated (I was wanting to use the plain images). I also know that the dataset isn't even, like there are a ton of some species and fewer of others. I moreover got to thinking about how to deal with datasets like this and wanted to know if there was any guides on how to manage this.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m03hrr/allowing_for_odd_shaped_and_unbalanced_images_in/"}, {"autor": "WhatsUpN", "date": "2021-03-07 16:00:14", "content": "Using metrics with sample weights /!/ I have a dataset that has very unbalanced situation for -----> image !!!  classification purpose.\n\nRecently, I trained a classification model on the dataset and used classification report, accuracy, precision and recall metrics.\n\n1) My first queson is that what does the output which is calculating by the metrics that used sample weights mean ? Is it simply means multiply with weight and the metric output, or something else ?\n\n2) Second is that how can I convert the output of the metrics with sample values to a percentage accuray rate of these class ?\n\nAll suggestions will be appreciated, thank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lztg1s/using_metrics_with_sample_weights/"}, {"autor": "zimmer550king", "date": "2021-03-05 04:40:00", "content": "Is there a dataset for static object re-identification? /!/ I have developed an algorithm that can re-identify static objects in a scene. Given a -----> camera !!!  whose intrinsic matrix is known and the extrinsic matrix for each frame is also known (the -----> camera !!!  is constantly moving), I am able to identify objects in a scene (for example, a room). And I am able to re-identify certain objects as being those that I saw earlier (maybe I saw a book in the first few frames and, after going around the room and coming back, I see the same book and I am able to classify that book as the same one I saw earlier).\n\nI tried looking for a dataset to try out my algorithm (for now, I record videos using my phone and the Google ARcore library gives me the intrinsic and extrinsic matrices) but couldn't find any. Is there any such dataset available?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ly45h4/is_there_a_dataset_for_static_object/"}, {"autor": "RedditBadSuggestions", "date": "2021-03-04 20:29:29", "content": "How does an experienced/talented programmer figure out what projects to look towards or use and cite? /!/ I currently want to use a neural network that guesses the emotion expressed on a person's face from the -----> image !!! . I could train a neural network myself on the images, but then I would obviously have to choose which dataset to use and design the code. If I designed the code, I would probably take some inspiration (which I would cite) from successfully designed tutorials on towardsdatascience or kaggle etc. If I used a pretrained model, I will have to figure out which model to use.\n\nThe issue I'm running into is that there's so many choices out there that it's really hard to make a good determination of which pretrained model to use, whether or not to create my own model, how to structure my own model, which dataset to use if I'm training my own model... All of this obviously depends on the context of the project, but how do more experienced/talented people address this issue? And if anyone has a suggestion for a pretrained emotion model that runs on an RTX 3080, I will be glad to listen to that as well, lol.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lxue6z/how_does_an_experiencedtalented_programmer_figure/"}, {"autor": "amaigmbh", "date": "2021-03-03 09:29:41", "content": "We write a 2-minute Newsletter with Updates from AI research, practical Use Cases and Code Tutorials. Here is Issue 9: with wav2vec2 implementation, SOTA -----> image !!!  compression and more", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lwpu5b/we_write_a_2minute_newsletter_with_updates_from/"}, {"autor": "GanonR", "date": "2021-09-28 14:54:27", "content": "How is it possible to divide complex networks into distinct \"layers\" or \"levels\"? /!/ Machine learning and  neurobiologists conceptualise the brain as operating in processing levels. This is important for determining whether a connection should be seen as \"lateral\", \"feed-forward\" or \"recurrent\".\n\nHowever, it seems impossible to divide even a simple network  into distinct levels, take the -----> image !!!  below. Different nodes in the network sit in multiple \"layers\" depending on whether you choose to count from input nodes A or B or count back from the output layer.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/uyieeik9c9q71.png?width=673&amp;format=png&amp;auto=webp&amp;s=6b86deac49d877e281a655eb6a34e946bdab1478\n\nexamples of papers reffering to different \"processing levels\": \n\n*  Later inhibition defined as: \" neurons receive inhibition from adjacent cells at the same processing level\"  [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6367436/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6367436/)\n* \"Hierarchical structure with subordinate and superordinate layers in the brain\" . Paper title: \"Hierarchical processing in the prefrontal cortex in a variety of cognitive domains\"\n\nThanks for your help.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/px84j6/how_is_it_possible_to_divide_complex_networks/"}, {"autor": "maybenexttime82", "date": "2021-09-27 17:34:34", "content": "[Q] Best way to preprocess -----> image !!! s+text sequences for feeding into -----> image !!!  captioning model? /!/ This is my first time doing IC modeling. I want to extract images + corresponding text sequences, but I don't know what is the best way to organise those files for training an image captioning model?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pwm4ud/q_best_way_to_preprocess_imagestext_sequences_for/"}, {"autor": "rmxz", "date": "2021-09-27 07:24:04", "content": "Visualizing \"zebra -stripes +spots\" using CLIP embedding math /!/ Math on CLIP embeddings can help visualize when/why CLIP considers images to be similar.\n\nI created [this github project](https://github.com/ramayer/rclip-server) to visualize CLIP embedding math.   A live demo of this system using Wikimedia images can be seen [here](http://image-search.0ape.com).\n\nSome interesting results:\n\n* [zebra -stripes +spots](http://image-search.0ape.com/search?q=zebra%20-stripes%20%2Bspots) \\- Animals that look kinda like zebras but with spots instead of stripes.\n* [zebra -mammal +fish](http://image-search.0ape.com/search?q=zebra%20-mammal%20%2Bfish) \\- Animals that look like zebras but fish instead of mammals.\n* [zebra -animal +car](http://image-search.0ape.com/search?q=zebra%20-animal%20%2Bcar) \\- Objects colored like zebras but more cars than animals.\n* [zebra -\"black and white\"](http://image-search.0ape.com/search?q=zebra%20-%22black%20and%20white%22) \\- Baby zebras (brown &amp; white) and a Greater Kudu (a brown &amp; white striped 4-legged animal). Of course you could also find the same baby zebra searching for [zebra -big +small](http://image-search.0ape.com/search?q=zebra%20-big%20%2Bsmall) or even more simply, just [baby zebra](http://image-search.0ape.com/search?q=baby%20zebra).\n* [furry black and white striped animal](http://image-search.0ape.com/search?q=furry%20black%20and%20white%20striped%20animal) \\- zebras, lemurs, and other furry black and white animals.\n* [striped horse-like animal](http://image-search.0ape.com/search?q=striped%20horse-like%20animal) \\- more zebras (and horses with stripes)\n* [zebra habitat -zebra](http://image-search.0ape.com/search?q=zebra%20habitat%20-zebra) \\- places that look like somewhere a zebra might live\n\nIt can also do a search based on the difference between the CLIP embeddings of two images directly.  For example, CLIP considers [this -----> image !!!  of a spider on a purple flower](http://-----> image !!! -search.0ape.com/search?q=%7B%22-----> image !!! _id%22%3A28754%7D) minus [this -----> image !!!  of the same kind of spider on a white flower](http://-----> image !!! -search.0ape.com/search?q=%7B%22-----> image !!! _id%22%3A174054%7D) to be [this set of pictures which is mostly purple flowers without the spider](http://-----> image !!! -search.0ape.com/search?q=%7B%22-----> image !!! _id%22%3A28754%7D%20-%7B%22-----> image !!! _id%22%3A174054%7D).\n\nI find this useful for trying to understand what concepts CLIP considers similar and why.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pwc4wj/visualizing_zebra_stripes_spots_using_clip/"}, {"autor": "throwawayafw", "date": "2021-09-27 05:10:05", "content": "I have a set of images with instances of two classes are labeled on it. Can I train Yolo V5 or V4 to detect both classes on a test -----> image !!! ? /!/ Pretty much the title. I have annotated instances of both classes on training data with annotations on Yolo format but I have a doubt about having a single folder containing the images with annotations of both classes on each image. Or do I have to separate the training folder for each class. I need to detect both classes on test image. How should I feed into the model.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pwab4o/i_have_a_set_of_images_with_instances_of_two/"}, {"autor": "Alan491", "date": "2021-09-26 11:42:39", "content": "[D] Does -----> image !!!  jpg, png extensions affect NN training? /!/ Hello, I am working on an image classification problem with resnet50 transfer learning I have approx 30k of the dataset which having mixed images in jpg and png format because I am facing an issue while training the model and I think the extension difference has been creating an issue, So can these different extensions create a problem for the neural net learning, because today when I doing some image preprocessing I encountered a problem like.\n\nBelow, It's an image in **\"jpg\"** format having a size of 280 \\* 461 of **41kb**\n\nand similarly, when I save this image in **\"png\"** format it becomes **207** kb with the same size 280 \\* 461.\n\nhttps://preview.redd.it/neygegvx2up71.png?width=280&amp;format=png&amp;auto=webp&amp;s=2e4164525affb85fbce21ad74f61629c8886d380", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pvs8j3/d_does_image_jpg_png_extensions_affect_nn_training/"}, {"autor": "maybenexttime82", "date": "2021-09-14 20:10:18", "content": "[Q] Considering transfer learning, why there are (very) large weight updates if the 'Dense' layers on top are randomly initialized while convolutional base stays frozen? /!/ I'm reading 'Deep Learning with Python' by F. Chollet, and in the part where he explains transfer learning for -----> image !!!  recognition he mentions that because the weights of the models head is randomly initialized that would cause large weight updates.   \n\n\nIs it because we guess that there might be possibility that we will be somewhere uphill and hence larger weight updates than being near the bottom?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/poaked/q_considering_transfer_learning_why_there_are/"}, {"autor": "OnlyProggingForFun", "date": "2021-09-14 13:56:27", "content": "ModNet, a state-of-the-art model for -----> image !!!  matting in 2021. I explain what image matting is and how AI attacks this complex challenge showcasing this incredible paper. All the references are in the description of the video", "link": "https://www.reddit.com/r/learnmachinelearning/comments/po3a3v/modnet_a_stateoftheart_model_for_image_matting_in/"}, {"autor": "ldorigo", "date": "2021-09-14 10:28:08", "content": "How do you choose a base model for fine-tuning? /!/ Hi fellow learners. I'm finding it really hard to figure out the \"best\" model to fine-tune for an -----> image !!!  analysis task. There are thousands of models available online - in particular, I find it really hard to understand 1. how much computing power a given model will require to fine-tune and 2. how much training data I should expect to need.\n\nHow do you normally approach this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/po041s/how_do_you_choose_a_base_model_for_finetuning/"}, {"autor": "Successful_Boat_3099", "date": "2021-09-14 07:41:53", "content": "\ud83d\udc8aYour daily dose of machine learning : Tensorflow-gpu installation tip! /!/ &gt; This is a series of posts that I post almost daily. I call them \u201cyour daily dose of machine learning\u201d. \n\n If you\u2019re using conda (Anaconda or Miniconda) as a package manager then you can install Tensorflow with GPU support easily using the command in the -----> image !!!  below.\n\nAdvantages:\n\n\\- Very easy.  \n\\- You don\u2019t have to install CUDA and cudnn separately.\n\nDisadvantages:\n\n\\- Most of the time the Tensorflow version you\u2019ll be installing is not the latest one. For example, currently there is version 2.6 already released but if you install TF using conda then you\u2019ll get version 2.5. \n\n&amp;#x200B;\n\nhttps://preview.redd.it/9wp3qo4pafn71.png?width=976&amp;format=png&amp;auto=webp&amp;s=732d7ab8e2638d715e6c782fceef0b987c49082e\n\n Follow me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!***", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pny8v3/your_daily_dose_of_machine_learning_tensorflowgpu/"}, {"autor": "Electrical-Judgment", "date": "2021-09-13 14:26:27", "content": "Difference between U-NET and U-NET Xception /!/ Hello guys, can anyone explain me whats the main differences between the two networks? I was following the tutorial on:  [-----> Image !!!  segmentation with a U-Net-like architecture (keras.io)](https://keras.io/examples/vision/oxford_pets_image_segmentation/)  but the model used here is different from the normal U-NET with concatenation, do the residuals have the same job as the concatenation in normal U-NET?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pngix8/difference_between_unet_and_unet_xception/"}, {"autor": "NADES_L", "date": "2021-09-12 15:54:37", "content": "AI /!/ hi, guys . I need information. anyone know a software AI or programmed model AI to interfere to other softwares ,for example Photoshop,illustrator something like these programs. i mean i give prompt and it is going to do the work and it will execute (reveal) product . Or i show a product(-----> picture !!! ) to AI and it will execute that product (-----> picture !!! ) on other software ( that interfered before with itself) and tried to prepare same thing with different style ( bc of the prompts i gave) . Thanks in advance", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pmv8yj/ai/"}, {"autor": "italia_independent", "date": "2021-09-12 13:50:11", "content": "Time-series -----> image !!!  clustering /!/  Hello folks. I came from r/GIS in hope that folks here might know better, plus there are some similar questions already. There is a question regarding time-series image clustering.\n\nWhat I am doing is a bit too big to put it in a couple of words, but let's  try:I do my master's in GIS and the topic is about time-series  clustering methods comparison.\n\nTo keep it simple, as **the data**  I have time-series of Sentinel-2 images with different square areas  (around 1km by 1km in size) that have the same features among each  other. The features might change over time within one area, they might  stay the same. Time-wise the length is around 3-4 months, and 40-50  pictures for each place.\n\n**What I want to do**  is to apply different existing and implemented clustering methods to  see which approach is the most accurate/fast to cluster different  features on this particular time-series data with particular features I  have. (I am describing it in general words because I am not sure I want  to share what exactly I am working on, for now).\n\nAs  an outcome, I want to have a publicly available dataset on zenodo, for  example, with images and manually drawn features for validation together  with some ground-truth data, and the results of the comparison itself,  and well, get my degree (:\n\nNow what I want to ask you about:\n\nSo far I found some approaches that looks promising, for example [n2d](https://github.com/rymc/n2d) or [k-means with DTW distance](https://tslearn.readthedocs.io/en/stable/user_guide/clustering.html), and there are some more (e.g. [T-DPSOM](https://github.com/ratschlab/dpsom)), but I want to start from these.\n\nHowever,  it feels like I might be missing something because it is quite  difficult to find scientific articles about image clustering, not to say  time-series image clustering. I assume it might be so that the images  should be converted to numerical time-series data with a unique band  values, and then they can be clustered.\n\nSo,  the question is, does anyone have any ideas which methods to try or  have nice scientific articles that could be used in the work?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pmt0r8/timeseries_image_clustering/"}, {"autor": "Dumbhosadika", "date": "2021-09-12 09:57:39", "content": "How to write cover letter for machine learning internship? /!/ Basically, I want to ask what to include in my resume while applying for Machine Learning Internship (Domain - NLP, Analytical Model, -----> Image !!!  models, Deep Learning). \n\nI don't have any prior experience in internships related to machine learning. However, I do have a project (Reddit bot made by python and Reddit API) and 4-5 OC posts on r/dataisbeautiful , should I include them in my resume? \n\nWhat else I can do?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pmpxve/how_to_write_cover_letter_for_machine_learning/"}, {"autor": "The-Ultimate-Unknown", "date": "2021-10-14 23:38:19", "content": "How to cluster unlabeled text data into clusters of given labels? /!/ I\u2019m  trying to identify the main dissatisfaction reason for some products  based on Amazon reviews. I have a few labels in mind such as: \u201cnot as  shown in -----> photo !!! ,\u201d \u201cshipping issue,\u201d \u201cquality issue,\u201d and \u2026.. What would  be the best way to assign a dissatisfaction reason to a review?\n\nExample:\n\nreview: I never received it  -&gt;  assigned label : \u201cshipping issue\u201d", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q8bm42/how_to_cluster_unlabeled_text_data_into_clusters/"}, {"autor": "MoonlessNightss", "date": "2021-10-14 10:09:53", "content": "Feasible project idea for an undergrad course /!/ **tl;dr** Ideas of feasible ML projects to be done in \\~3 months for a complete beginner\n\nHello, I'm completely new to ML, and I'm currently doing a ML bootcamp at my university, which is basically like a course. We have to choose a project that we will be working on throughout the semester. Since I'm a beginner, I don't really know what I can and cannot do with ML, or what might be a too ambitious project to be completed in 3 months. We will need to choose a project and discuss it with our instructor (he will tell us whether the project is realistic or not, etc) I googled a bit but many sites gave lots of projects ideas that I don't know if they're feasible (especially for a novice like me). I saw stocks price prediction for example on many sites, and I'm pretty sure this is next to impossible. So this makes me not really trust those sites with their ideas (it feels like those are ideas that are coming from people that don't know much about ML, and are just saying what might seem cool to do, regardless of feasibility). Some stuff that I saw and found interesting are: -----> image !!!  noise reduction, french-english translation.\n\nThanks in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q7woos/feasible_project_idea_for_an_undergrad_course/"}, {"autor": "ajoros", "date": "2021-10-14 01:37:38", "content": "[Python] Dependent variable is a list of values, not just one value per element. How does one approach this when choosing a model/ML approach to use? /!/ Hi all, \n\nSo I have 4 independent variables (features)  and 3 dependent variables (output). One of my dependent variables, titled QPCR, is a list of 100 integer values between -10 and 10 PER ELEMENT (see -----> photo !!!  link below for visual example).  My question is what approach would you recommend for creating a model that can deal with a list of values?\n\nHere is a visual of what my data looks like. Note QPCR, CellViability and ImmuoCellsActivation are my dependent variables. All else are independent variables:\n\nhttps://media.discordapp.net/attachments/212968673718173699/897914982803570778/unknown.png?width=869&amp;height=349\n\nAny help is appreciated. Feel free to PM me too.\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q7pkgp/python_dependent_variable_is_a_list_of_values_not/"}, {"autor": "ernamen", "date": "2021-10-13 19:21:14", "content": "Why I cannot see the values? /!/  Hi everyone!\n\nI am trying to convert an -----> image !!!  which is in a Numpy Array to a Tensorflow Tensor but I cannot see the values. Why does it happend?\n\nHere the variable *img* is the image as an array.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/vihl2y6sp9t71.png?width=230&amp;format=png&amp;auto=webp&amp;s=78b1871704277d854f01885d75c5eb7ef5dbc293\n\nSo then I convert it to a tensor and outputs the following (*img\\_tf* is the image in TF format):\n\n&amp;#x200B;\n\nhttps://preview.redd.it/48qei44qp9t71.png?width=449&amp;format=png&amp;auto=webp&amp;s=13b3f7f234919edec533580e3ff02309003ad867\n\nHow can I see the values?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q7ijff/why_i_cannot_see_the_values/"}, {"autor": "fullerhouse570", "date": "2021-10-13 02:25:01", "content": "\ud83e\udd2f\ud83d\uddbc\ufe0fRemove any object or person in an -----> image !!!  easily using this AI model!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q71hz0/remove_any_object_or_person_in_an_image_easily/"}, {"autor": "NegaodaMeiaNoite", "date": "2021-10-12 20:26:23", "content": "Training an -----> image !!!  classifier on FastAi and deploying it to Android using ONNX", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q6uwm7/training_an_image_classifier_on_fastai_and/"}, {"autor": "OuchieOnChin", "date": "2021-05-15 16:02:35", "content": "Would a specifically made partially connected feed-forward network have similar properties to an equivalent CNN? /!/ I'm a beginner in the process of learning CNNs for -----> image !!!  classification and i had this (probably wrong) realization.\n\nGiven a trivial 1-hidden-layer FCFFNN, i wonder if hidden nodes instead of being fully-connected to the input layer were only connected to a k\\*k portion of the input, differing for each hidden node as to mimic how a convolution operates, would end up having properties comparable to a textbook CNN.\n\nI'd guess to mimic two, three or more convolutional nodes, the hidden layer of our modified FFNN would have to be bigger by 2x to mimic two conv nodes, 3x for three conv nodes etc. restarting the position of the k\\*k portion to top-left at the start of each \"simulated\" conv node.\n\nI'm not sure how this would generalize to multiple convolutional layers.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nd24rj/would_a_specifically_made_partially_connected/"}, {"autor": "hellopaperspace", "date": "2021-05-14 15:19:26", "content": "[Article] xUnit Activation for Denoising, De-Raining, and Image Super-Resolution /!/ This tutorial breaks down xUnit, a new activation unit that is particularly suitable for -----> image !!!  restoration problems. xUnit implements a learnable nonlinear function with spatial connections, enabling the net to capture much more complex features while requiring a significantly smaller number of layers in order to reach the same performance. \n\nTopics covered include:\n\n1. Motivation behind xUnit\n2. xUnit explained\n3. PyTorch Code\n4. Paper results\n5. Conclusion\n\nArticle link: [https://blog.paperspace.com/xunit-spatial-activation/](https://blog.paperspace.com/xunit-spatial-activation/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ncb346/article_xunit_activation_for_denoising_deraining/"}, {"autor": "Cyalas", "date": "2021-05-14 12:43:28", "content": "Sections extraction from PDF using NLP /!/  Hello,\n\nSay I have a PDF document with 4 sections : Introduction, Section 1, Section 2, Conclusion. I want to extract those sections in a dictionary format, as follow :\n\n    Result = {'Introduction' : '....', 'Section 1': '...', 'Section2': ..., 'Conclusion': ....} \n\nI thought of converting the PDF file into raw text in Python then process it as follow:\n\n1. Get sentence embeddings (using BERT for example..)\n2. Apply clustering with number of clusters = 4 (Introduction, Section 1, Section 2 &amp; Conclusion)\n\nBut obviously, it wouldn't reconstruct the original document in the same order. In other terms, some sentences originally in \"Introduction\" might be found in \"Section2\" etc.\n\nAny idea how I can do this properly (i.e. reconstruct 'Result' by keeping the same order of the original PDF document) ?\n\nPS : I found some libraries doing this (either using -----> Image !!!  recognition on PDFs or converting PDF to XML files...) but those were a bit hard to apply. I prefer to use my technique mainly because (1) I'm learning NLP and (2) to avoid external libraries.\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nc7pxg/sections_extraction_from_pdf_using_nlp/"}, {"autor": "MrLemming2", "date": "2021-05-14 10:03:34", "content": "Fixed Size Input to Variable Size Output (Picture -&gt; Vector) /!/ Hey folks,\n\na quick statement of the problem:\n\nThe input is a fixed size -----> picture !!!  of size  WxH. Let's say the -----> picture !!!  contains k objects that I would like to detect (and in addition the relation of the objects to the other objects in the image) , hence I would like to return as an output a vector of size k\\^2, where k is not known a priori. Do you know of any architecture that solves a similar problem?\n\nBest regards and thank your for your time.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nc50z9/fixed_size_input_to_variable_size_output_picture/"}, {"autor": "_dv96_", "date": "2021-05-14 07:52:30", "content": "Basic Auto Encoder project - Generating poorly written digits [PyTorch] /!/ Hi, looking for your thoughts and feedback\n\nI created this side project to play with latent domain. The aim was to transform an input -----> image !!!  to something that looks somewhere between 2 digits. The repository below will give you a practical exposure to Auto Encoders, Latent Domain, PyTorch, Hosting on Streamlit.\n\nGitHub Repository - [LINK](https://github.com/vdivakar/mnistMuddle)| .  Streamlit App Demo - [PAGE](https://share.streamlit.io/vdivakar/mnistmuddle/br_streamlit/app.py)\n\nhttps://i.redd.it/w2d8e3xfk1z61.gif", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nc35b0/basic_auto_encoder_project_generating_poorly/"}, {"autor": "spmallick", "date": "2021-05-14 05:04:35", "content": "Generative and Discriminative Models /!/ A quick announcement before we jump to our post today.  \n\n\nWe will launch a\u00a0Kickstarter campaign for OpenCV for Beginners\u00a0on\u00a0May 18, 2021. It is a short, fun, and extremely affordable course.  \nPlease create a Kickstarter account and click on the \"Notify me on Launch\" button on the\u00a0pre-launch page\u00a0so you don't miss the special price on Day 1.\u00a01130 people\u00a0are already following our project on Kickstarter - don't wait until the last moment!  \n\n\n[https://www.kickstarter.com/projects/opencv/opencv-for-beginners](https://www.kickstarter.com/projects/opencv/opencv-for-beginners)  \n\n\nToday's post is about Generative and Discriminative Models  \nIn machine learning and deep learning, we often create these two different kinds of models for solving problems.  \n\n\nDiscriminative Models: These kinds of models focus on differences between classes to solve a problem. For example, a classifier built to classify dogs and cats is a discriminative model that learns the differences between a dog and a cat.  \n\n\nGenerative Models: A generative model tries to learn the appearance of the classes. For example, a generative model may be used to create a realistic -----> picture !!!  of a dog.  \n\n\nYou will also get a foundational understanding of generative models.  \n\n\n[https://learnopencv.com/generative-and-discriminative-models/](https://learnopencv.com/generative-and-discriminative-models/)\n\nhttps://preview.redd.it/wtaw5x4up0z61.jpg?width=600&amp;format=pjpg&amp;auto=webp&amp;s=57d06d0acab202ee590a94f7f105fb1b35de468d", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nc0u39/generative_and_discriminative_models/"}, {"autor": "BDL1998", "date": "2021-05-13 14:40:00", "content": "Are there any GANs used to generate the background of an -----> image !!! , given the foreground? /!/ Sorry if this isn't the right sub to post in. I can go elsewhere if another sub is more fitting.\n\n&amp;#x200B;\n\nI am looking for any resources regarding using a GAN to generate the background of an image, given the foreground. So, for example, suppose I have an image of a boat in front of a green screen, or even on a white background, I would like to then generate the water around the boat. The most important part would be to preserve the characteristics of the foreground of the image, and only create the background. I have access to unpaired data, I am mainly looking for the method.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nbi604/are_there_any_gans_used_to_generate_the/"}, {"autor": "nixxon94", "date": "2021-05-13 13:16:28", "content": "what is the best way to start writing own -----> image !!!  upscaling/ denoising ai? /!/ Hey everyone, I have some basic python knowledge and would like to try   out some good tutorials/ guides on this topic. Any pointers would be   appreciated, thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nbgec1/what_is_the_best_way_to_start_writing_own_image/"}, {"autor": "Hello_Iam_SvechKing", "date": "2021-07-29 11:29:14", "content": "Tiny -----> image !!!  classification /!/ Hello all. So i working on image classification problem where i need to classify image 20x20px on two classes.\n\nI use pretrained on imagenet models (inception+resnet, and pure resnet50), i had 99,01% score in my best turn. Now i have idea to use my trained models as stack.\n\nWhich other approaches i can use to improve my score?\nWhich best practises to classify tiny images ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/otvnyi/tiny_image_classification/"}, {"autor": "s_suraliya", "date": "2021-07-29 08:13:12", "content": "Plant leaf classification model selection /!/ Greetings! We are a group of students working on our final year engineering project on Plant Classification. At this stage we are trying to extract the leaf of a particular plant from -----> photo !!!  of the plant. We have tried to train with Mask-RCNN and yolov4 with the minimal dataset we had. But did not get any reasonable result. We are trying for semantic segmentation at this stage (training on yolov4 was done prior to taking this decision). So I wanted to ask if there are any better algorithm/model or even similar models to mask r-cnn on which we can train. \n\nAlso, if you want to contribute to the dataset. You can do that by visiting this form. It says to login with your google account. But don't worry, we won't collect your email ids, Google form requires a sign in  because of an upload option in the form:\n\n[https://docs.google.com/forms/d/e/1FAIpQLSeXi\\_7a770N077s8ItJHF6LdWmbIFdlr2s9OI1cIcH5J3dP-A/viewform?usp=sf\\_link](https://docs.google.com/forms/d/e/1FAIpQLSeXi_7a770N077s8ItJHF6LdWmbIFdlr2s9OI1cIcH5J3dP-A/viewform?usp=sf_link)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/otta32/plant_leaf_classification_model_selection/"}, {"autor": "Dwc41905", "date": "2021-07-28 20:15:49", "content": "-----> Image !!!  classification on small objects /!/ I have drone footage of people from pretty high up. If I don\u2019t care about the position of the person and just want to know if they are in the image or not would a convolutional neural network for work? I am skeptical due to the fact that my image is mostly background and the person takes up a very small part of the image. I initially tried object detection but I\u2019m struggling with the amount of annotations I need and the extra layer of complexity. I\u2019m looking for some guidance as the best way to approach this problem as I am limited to only the data I can collect( so far I haven\u2019t found any publicly available data for this purpose).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oti0kx/image_classification_on_small_objects/"}, {"autor": "JohannesHa", "date": "2021-02-04 15:08:32", "content": "How to scrape product data on supplier websites? /!/ I'm currently trying to build a semantic scraper that can extract product information from different company websites of suppliers in the packaging industry (with as little manual customization per supplier/website as possible).\n\nThe current approach that I'm thinking of is the following:\n\n1. Get all the text data via scrapy (so basically a HTML-tag search). This data would hopefully be already semi-structured with for example: title, description, product -----> image !!! , etc.\n2. Fine-tune a pre-trained NLP model (such as BERT) on a domain specific dataset for packaging to extract more information about the product. For example: weight and size of the product\n\nWhat do you think about the approach? What would you do differently?\n\nOne challenge I already encountered is the following:\n\n* Not all of the websites of the suppliers are as structured as for example e-commerce sites are \u2192 So small customisations of the XPath for all websites is needed. How can you scale this?\n\nAlso does anyone know an open-source project as a good starting point for this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lchmpl/how_to_scrape_product_data_on_supplier_websites/"}, {"autor": "TheBrokenGuitar", "date": "2021-02-04 13:07:30", "content": "Load -----> image !!!  dataset directly from ZIP files without extracting but it keeps took disk space gradually... /!/ hello everyone\n\nI'm trying to make custom data generator for loading image dataset directly from ZIP file subfolder using python zipfile and opencv. by doing that I expecting using less disk space and RAM capacity. but after I run it in Google Colab it keeps took disk space gradually. the RAM space consumtion is good tho, i've got no problem with it.\n\nwhat should I do to do it efficiently?\n\nhere is my code\n\narchive = zipfile.ZipFile(zip_path, 'r')\nfinal_dirpath = root_dirpath + \"/\" + pandas_df.at[df_index, 'folder']\ndirpath = final_dirpath.split('*')\nsorted_imgs = sorted([name for name in archive.namelist() if name.startswith(dirpath[0]) and name.endswith(dirpath[1])])\nimages = [cv2.imdecode(np.frombuffer((archive.read(img_path)), np.uint8), 1) for img_path in sorted_imgs]\n\nthis is the link I reffer to for my code implementation\nhttps://stackoverflow.com/a/21357377\nhttps://stackoverflow.com/a/55587252\n\nthanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lcf93y/load_image_dataset_directly_from_zip_files/"}, {"autor": "venomisoverme", "date": "2021-02-04 12:19:11", "content": "How to get instance segmented masks from semantic segmented masks? /!/ I have been working on a cell segmentation competition for quite a while now and have been training semantic segmentation models for the same. However, after working for about a month and having trained quite good semantic segmentation pipelines, I realized by seeing the evaluation script that the organizers actually require separate masks for each cell instance in the -----> image !!! . I have only a few days left and I want to know if there is any way if I can still use those trained semantic models to make good instance predictions?  \nThanks a lot.  \nYou can also suggest any easily trainable instance segmentation models.\n\nAn example image is attached.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/gepvu7dq4hf61.png?width=1080&amp;format=png&amp;auto=webp&amp;s=ec0ac229512646ce99c0afdb2a8c38ce959f24e0", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lceg4f/how_to_get_instance_segmented_masks_from_semantic/"}, {"autor": "venomisoverme", "date": "2021-02-04 11:42:11", "content": "How to get instance segmented masks from semantic segmented masks? /!/ I have been working on a cell segmentation competition for quite a while now and have been training semantic segmentation models for the same. However, after working for about a month and having trained quite good semantic segmentation pipelines, I realized by seeing the evaluation script that the organizers actually require separate masks for each cell instance in the -----> image !!! . I have only a few days left and I want to know if there is any way if I can still use those trained semantic models to make good instance predictions?    \nThanks a lot.   \nYou can also suggest any easily trainable instance segmentation models.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lcdwsa/how_to_get_instance_segmented_masks_from_semantic/"}, {"autor": "unfazing", "date": "2021-02-04 09:46:55", "content": "Setting up Distributed Tensorflow /!/ I want to run a CNN model for -----> image !!!  segmentation in a distributed setting. It will run on a few PCs each with multiple GPUs connected on a LAN. \n\nI have read some research papers on ways to optimise distributed learning. An idea that I want to experiment with is fully synchronous periodic averaging SGD (where the period can be varied for each time interval). \n\nI\u2019m quite new to this and would like to ask anyone who has had experience in tuning distributed Tensorflow setups how you went about it. \n\nI am aware that PyTorch is more suitable for this kind of experimentation, but I want to eventually use Tensorflow Serving to make inferences, hence I\u2019m limited to Tensorflow only.\n\nThanks in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lccbtq/setting_up_distributed_tensorflow/"}, {"autor": "SuccMyStrangerThings", "date": "2021-02-04 04:59:37", "content": "Need help implement architecture to perform single class multi instance detection? How do I define inputs to the network if no. of bounding boxes vary from -----> image !!!  to -----> image !!! ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lc81d2/need_help_implement_architecture_to_perform/"}, {"autor": "Laurence-Lin", "date": "2021-02-04 02:44:48", "content": "I run out of memory while loading -----> image !!!  data as array, is there any solutions? /!/ I have about 1800 images with size 900\\*800. I load them one by one and resize them to 600\\*500, but at the stage of loading each image from path to array, the RAM crashed.\n\n&amp;#x200B;\n\nEach image have size only 50 - 80 KB, and I believe the count of image in my case is not too large. Most of the image data of machine learning task involves much more data than mine. \n\n&amp;#x200B;\n\nHowever, my RAM crashed, I'd tried to find some solutions online, one is to downcast the resolution of float, but my image resolution is merely float32, I don't want to downcast to float16 to miss too much information in it.\n\n&amp;#x200B;\n\nI'd use gc.collect() to remove unused variables, and I don't have other useless variable to delete that I could release more memory.\n\n&amp;#x200B;\n\nIs there any advice to save memory while loading tons of images? By the way, I'm using tensorflow data loading framework, if there is any solutions that could be solved by tensorflow package, I'm glad to know more about it.\n\n&amp;#x200B;\n\nThank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lc5j5g/i_run_out_of_memory_while_loading_image_data_as/"}, {"autor": "ifelsestatement007", "date": "2021-02-03 23:50:27", "content": "-----> Image !!!  dataset normalization is one of the most common practises to avoid neural network overfitting but do you know how to calculate the mean and standard deviation of your own custom image dataset?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lc1zfl/image_dataset_normalization_is_one_of_the_most/"}, {"autor": "Laurence-Lin", "date": "2021-02-03 05:17:05", "content": "What are the different ways to create a data load generator in tensorflow framework? /!/ I'm recently using tensorflow for -----> image !!!  data, I know there are many different ways to load data.\n\n&amp;#x200B;\n\nMy concern is that in order to save memory, I would like to load image data not by once, but by batch with data loading generator.\n\n&amp;#x200B;\n\nI'd found different ways to do this:\n\n&amp;#x200B;\n\ntf.keras.preprocessing.image\\_dataset\\_from\\_directory , which I met some problem with directory folder\n\n&amp;#x200B;\n\nSecond, tf.preprocessing.image.ImageDataGenerator(), which I may not do data augmentation. However it seems while doing fit() on sample data, I have to load all image as array first, this is not saving my memory.\n\n&amp;#x200B;\n\nWhat 's the recommended way to load data from directory?\n\n&amp;#x200B;\n\nThank you for any help!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lbgd65/what_are_the_different_ways_to_create_a_data_load/"}, {"autor": "Laurence-Lin", "date": "2021-02-03 02:52:41", "content": "How should the data directory structure be using tensorflow's image_dataset_from_directory method? /!/ I'm looking for an efficient way to load multiple images by tensorflow function.\n\n&amp;#x200B;\n\nFor -----> image !!! \\_dataset\\_from\\_directory method, I'd seen that it suggest an default way for directory structure: [https://www.tensorflow.org/api\\_docs/python/tf/keras/preprocessing/-----> image !!! \\_dataset\\_from\\_directory](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/-----> image !!! _dataset_from_directory)\n\n&amp;#x200B;\n\n    class_a\n      -----> image !!! _1.png\n      -----> image !!! _2.png\n    \n    class_b\n      -----> image !!! _1.png\n      -----> image !!! _2.png\n\n  \n\nHowever, this way don't specify the labels by myself, so I prefer to give `labels` argument to `-----> image !!! _dataset_from_directory` , so my code is like:\n\n    img_path = '/content/drive/MyDrive/binary'\n    \n    train_loader = keras.preprocessing.-----> image !!! _dataset_from_directory(\n        img_path,\n        labels = labels,\n        label_mode = 'binary',\n        validation_split = 0.2,\n        color_mode = 'rgb',\n        subset = 'training',\n        -----> image !!! _size = (IMAGE_WIDTH, IMAGE_HEIGHT),\n        batch_size = BATCH_SIZE,\n        seed = 123\n        )\n\nWhere labels is a list of label for each -----> image !!! .\n\n&amp;#x200B;\n\nimg\\_path contains a folder total\\_image that have all classes of images.  However, the above generator returns error:  `When passing \\`label_mode=\"binary\", there must exactly 2 classes. Found the following classes: ['total_image']` \n\n&amp;#x200B;\n\nIt seems to suggest I should give the same directory structure as in the document link. Why should I set up this structure while given labels argument? Is it necessary to set up the directory structure for using image\\_dataset\\_from\\_directory?\n\n&amp;#x200B;\n\nIt's really inconvenient to specify the directory structure just to use one function. Is there other simpler way to load image from directory via tensorflow?\n\n&amp;#x200B;\n\nThank you for any advice!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lbdrci/how_should_the_data_directory_structure_be_using/"}, {"autor": "PRodNano", "date": "2021-02-03 00:59:45", "content": "Looking for guidance on where to start with project (-----> image !!! -to------> image !!!  translation with paired data) /!/ Hey all,  \n\n\nI'd like to write some code that does something per below:\n\ninputs\n\n1. two images (say of a face from different angles)\n2. some action (say someone getting punched in the eye)\n\noutputs\n\n1. two predicted images (alterations to the two images in the input that correspond to the action, like a black eye in the example above)\n\nQuestion: Does anyone have a good idea of where to start with this? From my research, it seems like I'd want to use some conditional GAN, possibly attGAN or relGAN.\n\nAny suggestions would be very greatly appreciated, as well as any example code or resource you could point me to. Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lbbl48/looking_for_guidance_on_where_to_start_with/"}, {"autor": "engrbugs7", "date": "2021-02-18 07:54:44", "content": "I am looking for an IP -----> camera !!!  that can be used in machine learning and home security. /!/ A 1080p or 2K resolution with night vision will be a good option. No internet connection needed. I am planning to use TensorFlow face detection and measure distances. A fully supported developer program(SDK/API) by the maker is a must to save me a headache in the future. Please advise and recommend a product to me. Thank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lmhbi4/i_am_looking_for_an_ip_camera_that_can_be_used_in/"}, {"autor": "Laurence-Lin", "date": "2021-02-18 06:34:53", "content": "Is support vector machine not good at model explanatory ability? /!/ I'm planning do an object detection task via -----> image !!!  classification, because for classification it's easier to label the data.\n\n&amp;#x200B;\n\nI would like to try other models aside from CNN, and I found SVM performing well on a small set of data. However, I'm afraid that SVM could not explain the model classification feature. \n\n&amp;#x200B;\n\nAs for CNN, there are Grad-CAM algorithm that could plot the heatmap to show the interest region that model used as base for classification, but SVM it seems no such a method yet. \n\n&amp;#x200B;\n\nI'm using linear SVM, but may move to non-linear kernel later. Due the the non-linear feature transformation and that SVM only gets the \"support vector\" to perform the classification, it seems for my object detection task, SVM is not an optimal choice. \n\n&amp;#x200B;\n\nDoes anyone have suggestion about the model explanatory for SVM? Any advice or explanation is appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lmfxum/is_support_vector_machine_not_good_at_model/"}, {"autor": "Kimmax3110", "date": "2021-02-15 14:10:22", "content": "Style transfer of digital text /!/ Hey there!\n\nFirst of all - I have no idea what I'm talking about and this is more a \"Does this even make sense\" type of question.\n\nDo you think it's (with currently available concepts and tech) possible to \"style transfer\" digital text, including effects, to new plain text input?\n\nAs an example, this text has an inner shadow + some outer glow.  \n\n\n*Processing img sv1xdkfednh61...*\n\nLet's say we have the source available and can generate sheer unlimited learning data in exactly that style but it's not feasible to use the original generator (In this case Photoshop) for production usage.\n\nLet me know what you think, or if you have other ideas. I started recreating the effects using PHP+Imagemagick, moved on to SVG, and am currently debating over HTML Canvases+JS. Something I could plop whatever into, let it learn, and out comes a nice matching -----> Image !!!  would be the dream.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lkeeew/style_transfer_of_digital_text/"}, {"autor": "hwpcspr", "date": "2021-03-14 04:50:07", "content": "Are there any addons that can block all asian porns? /!/  \n\nthey are too alluring\n\n-----> image !!!  recognition", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m4o7xn/are_there_any_addons_that_can_block_all_asian/"}, {"autor": "the_travelo_", "date": "2021-03-14 04:05:27", "content": "What's the name of the body research/SOTA that does Image Search using Text /!/ I have a database of hundreds of images and I want to query them using text.\n\nLet's say I have a database of multiple profiles of stores where each profile has ~10 images of products and the description of the store.\n\nI want users to be able to search by text. For example, let's say a Store has an -----> image !!!  of white shoes, then the search results should be able to retrieve that profile if the query is \"white shoes\".\n\nMy question, is what papers are available or what's the SOTA for this type of task?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m4nh13/whats_the_name_of_the_body_researchsota_that_does/"}, {"autor": "piscesmoon__", "date": "2021-03-13 19:05:07", "content": "I have no idea what the hell I am doing. /!/ Hello, everyone!   \n\n\nI am a visual artist and photographer trying to get into the world of ML to help push myself in a new direction with -----> image !!!  creation, but I am a few rows deep on the struggle bus in terms of finding helpful information at a really novice level. A good majority of what I find is pretty advanced, so this could very well be my fault for not searching for something correctly either.  \n\n\n  \nThank you in advance, and appreciate the help!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m4cw27/i_have_no_idea_what_the_hell_i_am_doing/"}, {"autor": "Tricky_Nail_6659", "date": "2021-03-13 13:04:38", "content": "Preparing data for the YOLO algorithm /!/ Hi I am working on a project which requires the [You Only Look Once](https://arxiv.org/pdf/1506.02640.pdf) algorithm in order to classify and localise objects within images. I have to prepare my dataset (which has 2 classes, and predicts 6 objects per grid cell, and the 448*448 -----> image !!!  is split into a 7*7 grid). What would be a viable approach to do that? I found  [this](https://gist.githubusercontent.com/maskaravivek/d456a82ae08517fe0bb1964c22f56ced/raw/36e95091bca46d9b578b13f231637c28630705bb/4db7d8e6-bf84-4af8-a870-f6f18c02b465) code, found in [this ](https://www.maskaravivek.com/post/yolov1/) article. However I do not understand why he has done what he has done, e.g why is he specifically checking the 24th element of the \u201cbox\u201d (if that is 0)? Is that the objectives score? Is there any tutorial running through that, or would it be possible for someone to explain or even provide working an adaptation for the authors approach?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m45wf9/preparing_data_for_the_yolo_algorithm/"}, {"autor": "IHDN2012", "date": "2021-03-13 04:08:05", "content": "Is there an easy way to convert -----> image !!!  annotations that is free? /!/ Roboflow is easy but super expensive, like 1 cent per image, which is a lot if you are doing 10k images.  Is there an easier way to convert annotations?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m3yqk6/is_there_an_easy_way_to_convert_image_annotations/"}, {"autor": "SuccMyStrangerThings", "date": "2021-03-12 07:14:37", "content": "Sigmoid with 1 output neuron producing discrete integer values for binary classification. Help needed /!/ My data looks like this: \n\n*Found 5600 images belonging to 2 classes.* \n\n*Found 1400 images belonging to 2 classes.*  \n\n&amp;#x200B;\n\nTrain and Test acc \\~97% \n\nWhen I try to predict an -----> image !!! 's class I get output as either 0 or 1. Shouldn't sigmoid produce values between \\[0, 1\\]?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m3cd4v/sigmoid_with_1_output_neuron_producing_discrete/"}, {"autor": "gniziemazity", "date": "2021-03-12 07:05:56", "content": "My course where I teach how to make an augmented reality piano is over. It contains some basic machine learning elements but it is mostly -----> image !!!  processing and procedural sound generation. I think it still fits here for beginner enthusiasts. [Code and Lectures linked in the comments]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m3c8yg/my_course_where_i_teach_how_to_make_an_augmented/"}, {"autor": "VikasOjha666", "date": "2021-03-11 18:18:17", "content": "Building Image Colourization App With Python and Deep Learning /!/ Couldn't find any article which both explained and implement -----> image !!!  colorization using GAN step by step hence I wrote this article on medium to train a GAN for -----> image !!!  colorization and also create a GUI for it.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m2ww8g/building_image_colourization_app_with_python_and/"}, {"autor": "DextralSnail", "date": "2021-03-28 02:11:26", "content": "Attribute classification for list of species /!/ Hi all,\n\nI recently build an object detector to detect three types of attributes (exclusive, so a given detection can only be one of the three types) of a species in an -----> image !!! . I have two primary data types that I want to feed to my detector: museum images and images photographed in natural settings. There are a range of images available per species (1-15k images) and I am trying to decide on an approach for transforming my detections associated with species images to species classifications. For example, one way may be a majority consensus classification for a species if 90% of detections are in one class or another. Another I am considering is a majority-rule consensus if the species has greater than X detections. However, what is an appropriate method to determine the number of images for a majority rule consensus is something I am unsure of.\n\nMy detector has high mean average precision (97.45%) on museum images and moderate mAP on images in natural light (84.33%). F1 scores are 0.92 for museum images and 0.80 for natural light images.\n\nHas anyone come across a similar problem in their studies and found a solution?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mesash/attribute_classification_for_list_of_species/"}, {"autor": "YaswanthBangaru", "date": "2021-03-27 23:02:18", "content": "What are some applications of ML/DL in the automobile industry? /!/ Hey folks, the question pretty much says it all, the reason I ask is because I would like to target to get into the automobile domain for my next job (I am jobless right now). I first wanted to do some -----> image !!!  recognition of the model of the car but then I though, perhaps, it's too cliche. I was wondering if me picking a better project than that with an actual application would improve my odds of landing a job! So, someone working in the automobiles industry here could throw some light on the kind of applications what I could take up as a project over a 3-4 month period? I already am at an intermediate level of experience, I do know deploying dashboards using AWS, working with APIs and databases. I am just looking to level it up for this next project by also concentrating on the model part of it, hope I am asking at the right place and hope to see a positive response as well. Thank you for reading :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/meox3a/what_are_some_applications_of_mldl_in_the/"}, {"autor": "gsalvador", "date": "2021-01-12 19:34:23", "content": "Flower or not flower? /!/ Hi, i have plenty experience with coding but on Machine Learning I'm a total noob.\n\nI have a book divided in png files, one for each page.\n\nSome have a whole page -----> picture !!!  of a flower, some just text.\n\nI there a way a noob like me could run a pretrained network to solve this problem?\n\nThank you very much and all the best!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kvytlv/flower_or_not_flower/"}, {"autor": "hiphop1987", "date": "2021-01-11 19:40:13", "content": "Python Tricks You Should Start Using in 2021 /!/ Start the New Year right with one of the best New Year\u2019s resolutions: **Learn more Python**.\n\nYou can start with this article in which I present 5 Python tricks that will make your life easier.\n\nFor more tricks see [5 Python Tricks You Should Start Using in 2021](https://towardsdatascience.com/5-python-tricks-you-should-start-using-in-2021-1084af21c2f2?sk=d950fdd36fea5ade9188528a1e2d587c).\n\n**You\u2019ll learn:**\n\n* How to format big integers more clearly\n* What are Magic commands in IPython\n* A simple way to debug code\n* A better way to work with file paths\n\n# 1. Underscores in Numeric\u00a0Literals\n\nhttps://preview.redd.it/og796z1t9ra61.png?width=890&amp;format=png&amp;auto=webp&amp;s=6ac2414911705cd157d8a037ff140cf571989653\n\nFrom Python 3.6 (and onwards) you can use underscores to make numbers easier to read. See[ PEP 515](https://www.python.org/dev/peps/pep-0515/) for more details.\n\nLet\u2019s look at an example:\n\n    a = 1000000000\n    # Is variable a billion or 100 millions?\n    \n    # Let's use underscores to make it easier to read\n    a = 1_000_000_000# \n    \n    # You can group numbers as you like \n    b = 1_0_9_0\n    \n    # It also works with hexadecimal addresses and grouping bits.\n    # grouping hexadecimal addresses by words\n    addr = 0xCAFE_F00D# grouping bits into nibbles in a binary literal\n    flags = 0b_0011_1111_0100_1110\n\n# 2. Magic commands with\u00a0IPython\n\n[Using &amp;#37;paste command to paste the code to IPython interpreter. -----> Photo !!!  by Roman Orac](https://preview.redd.it/amcqtu0w9ra61.png?width=1600&amp;format=png&amp;auto=webp&amp;s=467fae908e3741372563f3009c062b365885409a)\n\nMy development workflow with Python is to have a terminal pane with neovim on the left and IPython interpreter on the right.\n\nThis makes testing the code easier as I can copy the code from the left and paste it in the interpreter on the right.\n\n## What is the IPython interpreter?\n\nIt is like a Python interpreter but on steroids.\n\nIPython is a command shell for interactive computing in multiple programming languages, originally developed for the Python programming language, that offers introspection, rich media, shell syntax, tab completion, and history\n\n## What is the easiest way to paste the code to the IPython interpreter from the clipboard?\n\nDid you know that IPython supports magic commands?\n\nOne of them is the %paste command, which pastes the code from the clipboard with formating.\n\nSimply type %paste in the IPython interpreter.\n\n# 3. Debugging Python\u00a0code\n\n[Debugging Python code IPDB. ](https://preview.redd.it/wa70c2oz9ra61.png?width=1142&amp;format=png&amp;auto=webp&amp;s=21b9259a4eab754c16c07205263e86bf508016b0)\n\nPyCharm editor comes with a build-in debugger for Python code. But what if you are using Visual Studio Code, Atom, Sublime or Vim?\n\nYou can use pdb module:\n\n    foo()\n    \n    import pdb; pdb.set_trace() \n    # your code will stop here and interpreter will open\n    \n    bar()\n\nPython 3.7 (and onwards) simplifies this with a build-in breakpoint function call:\n\n    foo()\n    \n    breakpoint()\n    # your code will stop here and interpreter will open\n    \n    bar()\n\nSee [PEP 553](https://www.python.org/dev/peps/pep-0553/) for more details.\n\n# 4. Pathlib\n\nhttps://preview.redd.it/yl7dnlheara61.png?width=1600&amp;format=png&amp;auto=webp&amp;s=26e3d8efedbdd471df1dc392824c1ad9d8e7708f\n\nWorking with paths can be challenging especially if your code needs to run on multiple operating systems.\n\nLuckily for us, Python standard library has [pathllib](https://docs.python.org/3/library/pathlib.html).\n\nLet\u2019s look at an example:\n\n    from pathlib import Path\n    \n    path = Path(\"some_folder\")\n    print(path)\n    output: some_folder\n    \n    # We can add more subfolders in a readable way\n    path = path / \"sub_folter\" / \"sub_sub_folder\" print(path)\n    output: some_folder/sub_folter/sub_sub_folder\n    \n    # make path absolute\n    print(path.resolve())\n    output: /Users/r.orac/some_folder/sub_folter/sub_sub_folder", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kva7w0/python_tricks_you_should_start_using_in_2021/"}, {"autor": "andreasoelker", "date": "2021-01-11 15:15:23", "content": "Where to find dictionaries for text mining classification tasks? /!/ Hello everybody! I\u00b4m currently working on my psychological master thesis and I want to classify reddit users as persons with a problematic internet usage or no problematic usage (two classes: \"problematic\" and \"not problematic\"). I already collected my data set (via Reddit Api and LimeSurvey for the labels). Apart from -----> image !!!  processing and simply using attributes like \"uses dark mode\" and \"karma points\". I process text (comments and subreddit descriptions).\n\nI have preprocessed my texts (tokenize, cases transformed, stopwords removed, stem), trained a model etc (the model performs VERY poor, mostly for the comments). This comes from my sample size being very small and having a lot of features by the tokenizing. So what my idea is, that i could shrink my feature size by applying a categorization via dictionary for my tokens, for example (without stem): \"rocket league\", \"PvP\", \"Game\" -&gt; gaming; \"house\", \"cake\" (altough this also could be gaming ;) ), \"card\" -&gt; misc;\n\n**Question: Are there any dictionaries out there which could be useful? Or do you have any other idea?**\n\nAny idea is appreciated! Thanks so much =)\n\nPS: Of course i could somehow create an own dictionary/categories, but with having a deadline for the thesis it\u00b4s quite impossible, especially for a ML noobie like me", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kv4kdz/where_to_find_dictionaries_for_text_mining/"}, {"autor": "ZombieNub", "date": "2021-01-11 03:46:37", "content": "I have finally completed a machine learning mini project of mine. Where to go now? /!/ After reading Tensorflow and Keras's documentation, as well as following Tensorflow's DCGAN tutorial, I have finally put together my own DCGAN, though with some modifications to make the model more stable (described in this paper: https://arxiv.org/abs/1511.06434), as well as some -----> image !!!  preprocessing due to handling a different database. I am very much learn-by-doing, and while I am no master, I feel confident in at least understanding the basics of Tensorflow, Keras, and ML in general. My only question is where to go next.\n\n1. How can I improve DCGANs beyond following the recommended architecture?\n\n2. I'm interested in Deep Reinforcement Learning. What's a good game to train a DRL model on?\n\n3. Are there any other generative models besides VAE and DCGAN that may be useful to learn or explore?\n\nThe questions I list here are more suggestions than anything. Feel free to answer any or none of these questions. I hope to contribute to ML in meaningful ways in the future.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kuuh45/i_have_finally_completed_a_machine_learning_mini/"}, {"autor": "tylersuard", "date": "2021-01-11 01:04:15", "content": "Is it possible to use a GAN to edit an existing -----> image !!! ? /!/ Like for instance, if I have an image of a person's face, and I want to add pimples, is it possible to do that?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kuro32/is_it_possible_to_use_a_gan_to_edit_an_existing/"}, {"autor": "shyamcody", "date": "2021-01-10 22:25:05", "content": "What is DALL.E and how does it create -----> image !!!  out of text?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kuonr1/what_is_dalle_and_how_does_it_create_image_out_of/"}, {"autor": "madzthakz", "date": "2021-01-19 15:20:18", "content": "I'm a Senior Data Scientist at Disney and I'm hosting another Data Science Q&amp;A session this Thursday @ 5:30 PM PST. I'll be joined by a Principal Data Scientist at Clearbanc! /!/ \\*\\*DISCLAIMER\\*\\*: This is completely free and not sponsored in any way. I really just enjoy helping students get started and potentially transition into Data Science\n\nAs the title mentions, I'm a Senior Data Scientist at Disney and I'm going to host **another** Data Science Q&amp;A this Thursday at 5:30 PM PST. This time I'll have **Susan Chang** join me. Susan is a Principal Data Scientist at Clearbanc and hosts ML streams on Youtube (focus on Reinforcement Learning) and has built her own gaming platform which has been featured in PC Gamer. Her experience is uniquely diverse and I feel like you guys will be able to learn a lot from her. \n\nLast month\u2019s sessions were an absolute blast with over 250 people who attended from all over the world. I hope you see you all there!\n\nRegister Here:\n\nhttps://disney.zoom.us/webinar/register/WN\\_SbiRedGfRdi2v94gnI-rTw\n\nVerification:\n\nMy -----> photo !!! : [https://imgur.com/a/Wg3DMLV](https://imgur.com/a/Wg3DMLV)\n\nMy LinkedIn: [https://www.linkedin.com/in/madhavthaker/](https://www.linkedin.com/in/madhavthaker/) (feel free to connect)\n\nSusan's LinkedIn: [https://www.linkedin.com/in/susan-shu-chang/](https://www.linkedin.com/in/susan-shu-chang/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l0lvqq/im_a_senior_data_scientist_at_disney_and_im/"}, {"autor": "harmonisation", "date": "2021-01-19 13:21:24", "content": "Ask me about my project /!/ I'm currently doing my bachelor's degree and I'm working on automated yield estimation of oranges. I've created a basic idea document for you guys to see what the project is actually about. I would love to hear your thoughts and questions that come in your mind because I have my project defense coming up in a few days. So throw your questions at me.  \n\n\n**Automated yield estimation of oranges:** \n\nPakistan is among the top ten producers of oranges. Due to this, most of the target export markets of oranges are those of developing countries. This leads to the increasing demand for oranges during winters which puts a significant burden on the agriculture sector. Modern technologies are not being adopted by farmers in Pakistan, which propels them to manually count the fruit for yield estimation. This increases manual labor and decrease in overall profit. With the use of automated yield estimation, current production can be maximized through automation, -----> image !!!  processing, computer vision, artificial intelligence, and machine learning. The estimated yield of Oranges on trees will be given as a result. In the long run, automated yield estimation can reduce labor force requirement, decrease costs, increase efficiency and boost agricultural production.\n\nDrones will be used to capture 360\u00b0 images of trees. With the help of image processing and AI/ML techniques images will be segmented and data of the oranges will be extracted from images. Global coordinates will then be allocated to record various detections of the same orange to reduce over counting.\n\n**The main objective** of this project is to estimate the yield of oranges on trees with minimum error as well as improved accuracy to enhance the quality of oranges, which, in turn, will reduce operating costs. \n\n**In conclusion**, Pakistan needs automated yield estimation to increase its agricultural output. Not only will it speed up oranges harvesting, but it will also prevent oranges from rotting before reaching its destination. Automated yield estimation is needed that may enable the local farmers to make their livelihood under harsh situations. In summary, the use of automated yield estimation could mean more income for farmers, more taxes and foreign reserves for the government.\n\n&amp;#x200B;\n\nPlease let me know what you think of this and if you have any questions.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l0jpbw/ask_me_about_my_project/"}, {"autor": "Patrice_Gaofei", "date": "2021-01-18 13:35:11", "content": "How to dilate an -----> image !!!  without affecting the shape? /!/  \n\nHello respected programmers,\n\nI am currently working on an image segmentation task. Specifically, I am working on specular detection using UNet. My labels are generated using a threshold method. However, there are some cases whereby only the center pixels of the specular regions are detected. That is, only some parts of the specular regions are detected. I have tried dilation to expand the detected specular regions, but the results are not good because the original shapes of the specular regions are greatly distorted. Please, how can I expand the specular regions based on the detected center pixels?\n\nAny suggestions and comments would be highly appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kzur89/how_to_dilate_an_image_without_affecting_the_shape/"}, {"autor": "Patrice_Gaofei", "date": "2021-01-18 13:32:03", "content": "How to dilate an -----> image !!!  without affecting the shape?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kzupgn/how_to_dilate_an_image_without_affecting_the_shape/"}, {"autor": "Patrice_Gaofei", "date": "2021-01-18 06:59:55", "content": "How to dilate an -----> image !!!  without affecting the shape? /!/  \n\nHello respected programmers,\n\nI am currently working on an image segmentation task. Specifically, I am working on specular detection using UNet. My labels are generated using a threshold method. However, there are some cases whereby only the center pixels of the specular regions are detected. That is, only some parts of the specular regions are detected. I have tried dilation to expand the detected specular regions, but the results are not good because the original shapes of the specular regions are greatly distorted. Please, how can I expand the specular regions based on the detected center pixels?\n\nAny suggestions and comments would be highly appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kzpgem/how_to_dilate_an_image_without_affecting_the_shape/"}, {"autor": "hiphop1987", "date": "2021-01-25 13:39:18", "content": "How to get contacted by Google for a Data Science position? /!/ I was recently contacted by a recruiter from a Big Tech company. Why now and never before? \n\nHere I present my theory of why a recruiter contacted me for a Senior Data Science position. You can use my theory (and develop it further) to increase your chances of getting contacted by Big Tech company.\n\nI've published this originally in [How to get contacted by Google for a Data Science position?](https://towardsdatascience.com/how-to-get-contacted-by-google-for-a-data-science-position-95b87f6683fd?sk=04ad134838c9f6169dabe118a8e65f9e) blogpost.\n\nhttps://preview.redd.it/nflh1xqoehd61.png?width=1400&amp;format=png&amp;auto=webp&amp;s=24aefcbddd2bab1b3c2f1c801c0dcadf221f67cc\n\n# The Contact\n\n[The email I got from a Big Tech company. I intentionally obfuscated the key parts of the company and the product.](https://preview.redd.it/7aqei2kpehd61.png?width=1400&amp;format=png&amp;auto=webp&amp;s=46f9ef10816c76477753ab59fbff8eb6b85b16b9)\n\nI was recently contacted by a recruiter from a Big Tech company. My first thought was \u201cthe email has to be fake\u201d. But it was well written, with no spelling mistakes (which is unusual for spam).\n\nAfter doing some Googling about the recruiter, I found that the person who contacted me is really a recruiter for a Big Tech company.\n\nThen I thought: **why did they contacted me now and never before?**\n\n# The Call\n\nAt first, I didn\u2019t intend to have a call with the recruiter as I didn\u2019t intend to apply for the position. Why?\n\nWell, Big Tech companies usually require that you relocate. An unwritten requirement is also that you need to work extra hard \u2014 both not acceptable in my current stage of life. **Spending time with friends and family is more important to me.**\n\nThen I said to myself, I tried so hard to get an interview in the past. **I owe it to myself to take the call.** To learn what do they have to say and, more importantly, ask **why did they decide to contact me?**\n\nI expected the recruiter would say: one of our engineers stumbled on your article about LSTMs and said we need to have a chat with this guy. But as I later found out that wasn\u2019t the case.\n\n# Why Me?\n\nThe answer to my \u201c**why did you decide to contact me?**\u201d question surprised me: **We found you on** **LinkedIn**.\n\nI didn\u2019t touch my LinkedIn profile for almost two years. Oh, wait. I changed my profile -----> image !!!  a while ago \u2014 but that didn\u2019t do the trick \ud83d\ude02\n\nBut something had changed because I started getting many interesting job offers on LinkedIn.\n\nSomething had to change, but what? This got me thinking.\n\n# The Theory\n\nAfter thinking for some time, I came up with this theory. I need to emphasize, **this** **is just a theory.** I haven\u2019t factually proved it. But it makes a lot of sense when you think about it.\n\nBy blogging regularly on Medium, I started getting many Connect requests on LinkedIn. I had a link to my LinkedIn profile in my Medium profile description.\n\nAs time went by, my LinkedIn profile grew and I saw the potential to use it to distribute my articles. By posting articles regularly made my colleagues curious: **What is Roman up to with these articles?** This made even more engagement with my profile and my content.\n\nBy self-promoting myself on LinkedIn, users started to interact with my content and profile. This made the LinkedIn ranking algorithm push me to higher ranks. So when the recruiter searched for a Senior Data Scientist, I was at the top (or close to the top) on the list.\n\nThink about it.\n\nWhich Data Scientist is the best in the eyes of the LinkedIn ranking algorithm?\n\nThe one that gets the most profile views and his/her content get the most interactions. The LinkedIn ranking algorithm doesn\u2019t know anything about Data Science or programming. It knows only about content engagement and profile interactions.\n\n# Conclusion\n\nThe key takeaway is to self-promote on LinkedIn (or any other employment-oriented online service) so that the ranking algorithm will rank you higher. Just tweaking the LinkedIn profile as I did for so long doesn\u2019t help.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l4o162/how_to_get_contacted_by_google_for_a_data_science/"}, {"autor": "YaswanthBangaru", "date": "2021-01-25 09:53:20", "content": "Creating a benchmark using different sized arrays /!/ I am trying to create a benchmark for one of my ml projects. To give a familiar example, suppose we are trying to create a bench while doing -----> image !!!  recognition of number 9 using MNIST data-set, apparently what we could do is, average out a few sample elements of 9 from the training data-set and create an array of numbers that define the closest match to 9 and then use this array to find the pixel to pixel distance(I mean array to array element wise by calculating sum of squares of distances under square root). Then, this averaged out array could be used for each sample element of the test set to calculate the pixel to pixel distance. I learnt this from fastai lecture.\n\nNow, my problem is I have arrays of different lengths that represent the same pattern. It's related to a time-series problem. I want to create an average of these arrays (possibly bringing down the length of the benchmark array to the shortest array of the training set of arrays) . I believe something like that could be achieved most probably, just wondering if someone could through some light on it. Thank you for reading.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l4krdr/creating_a_benchmark_using_different_sized_arrays/"}, {"autor": "Rishit-dagli", "date": "2021-01-25 03:38:58", "content": "Low Light -----> Image !!!  enhancement with TFJS /!/  I am delighted to share the TensorFlow JS variants for the MIRNet model, capable of enhancing low-light images to really great extents.\n\nThe Project repo - [https://github.com/Rishit-dagli/MIRNet-TFJS](https://github.com/Rishit-dagli/MIRNet-TFJS)\n\nPlease consider giving it a star if you like it. More details in [this tweet](https://twitter.com/rishit_dagli/status/1340984448343367680).  \n\n\n*Processing img sd2cr4isfed61...*", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l4f36e/low_light_image_enhancement_with_tfjs/"}, {"autor": "spmallick", "date": "2021-01-24 18:39:12", "content": "[Blog]: Making A Low-Cost Stereo Camera Using OpenCV /!/ Today, we will learn [how to build a stereo -----> camera !!!  using two webcams](https://click.convertkit-mail.com/d0uz3gpdxzt0h6ezzpcl/n2hohqunxgvlrei6/aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL21ha2luZy1hLWxvdy1jb3N0LXN0ZXJlby1jYW1lcmEtdXNpbmctb3BlbmN2Lw==)!  \n\n\nMost of you know that images from a stereo pair (i.e. two cameras) looking at the same scene, can be used to estimate the depth at every pixel. Our eyes and many stereo cameras are based on this principle.  \n\n\n\u200b[OpenCV AI Kit with Depth (OAK-D)](https://click.convertkit-mail.com/d0uz3gpdxzt0h6ezzpcl/48hvh7u0p9m7gwux/aHR0cHM6Ly9zdG9yZS5vcGVuY3YuYWkv) also uses a stereo pair of cameras for depth estimation.\u00a0\n\n\u200b[https://www.learnopencv.com/making-a-low-cost-stereo-camera-using-opencv/](https://click.convertkit-mail.com/d0uz3gpdxzt0h6ezzpcl/n2hohqunxgvlrei6/aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL21ha2luZy1hLWxvdy1jb3N0LXN0ZXJlby1jYW1lcmEtdXNpbmctb3BlbmN2Lw==)\u200b\n\nThe C++ and Python code is linked below [https://github.com/spmallick/learnopencv/tree/master/stereo-camera](https://click.convertkit-mail.com/d0uz3gpdxzt0h6ezzpcl/wnh2h6urk4qlgnb7/aHR0cHM6Ly9naXRodWIuY29tL3NwbWFsbGljay9sZWFybm9wZW5jdi90cmVlL21hc3Rlci9zdGVyZW8tY2FtZXJh)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l454tu/blog_making_a_lowcost_stereo_camera_using_opencv/"}, {"autor": "Suspicious_Tomorrow8", "date": "2021-01-24 16:30:18", "content": "help in -----> image !!!  processing course project /!/ hello i need some help with my image processing course project if anyone can relate and help just PM me or Leave a comment and i will do it", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l42kno/help_in_image_processing_course_project/"}, {"autor": "panic-at-the-hippo", "date": "2021-01-23 22:46:24", "content": "Where do you train? /!/ I'm looking to train on a large -----> image !!!  dataset for a personal project. \n\nObviously training on my mbp is not ideal. \n\nMost of the cloud services seem like they're built around executing notebooks. The repo I've cloned is fairly large and doesn't seem like I could just throw it into Google Colab. Do I just need to buy a dedicated machine? \n\nI was hoping to find something like a dedicated digital ocean droplet, where I could just ftp some files and let it run. I haven't come across anything I'm understanding to work that way.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l3mh1i/where_do_you_train/"}, {"autor": "OnlyProggingForFun", "date": "2021-01-23 15:31:25", "content": "THE AI-POWERED ONLINE FITTING ROOM: VOGUE. A game-changer for online shopping and -----> photography !!! ? Let me know what you think! (video demo)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l3e3za/the_aipowered_online_fitting_room_vogue_a/"}, {"autor": "The_Startup_CTO", "date": "2021-01-23 12:18:09", "content": "Learning about machine-generated content and NLP as a Full-Stack dev /!/ Hi everyone! I'm a Full-Stack dev and already confident both in writing code as well as math basics. What I am missing are concrete tools though.\nSpecifically, I am interested in learning two topics:\n- Auto-generating content (things like \"I gave my neural network all Harry Potter books to read and it wrote a new chapter\" or \"turn a kid's drawing of a monster into a -----> photo !!! -realistic image\")\n- NLP (\"your messages today sound sad\" or \"these reddit posts talk about machine-learning, and these ones are from beginners\")\n\nWhere do I best start? I am not looking for basic intros into linear regression, but tools and tutorials for these two specific topics.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l3b4cb/learning_about_machinegenerated_content_and_nlp/"}, {"autor": "tippmannman", "date": "2021-01-23 00:29:17", "content": "Data transformation and augmentation for CNN /!/ Apologies for what may be a naive question. Say I have a dataset of images that I want to classify. The dataset is small (\\~500 of a class) but I am worried about generalizability to new datasets. Is there any utility in applying transformations to the data that may try to replicate what future data may look like? \n\nFor example: my dataset has 500 pictures of dogs but I take the 500 -----> photo !!! s and apply a filter that would replicate what it would look like if someone took the same -----> photo !!!  but with the flash on. Or \"dimming\" the photo in order to show what a photo taken in low level light would look like. I would then grab all the data and train with the original + augmented data. \n\nMy intuition for CNN's are poor, but I would anticipate the model would not yield any more generalizability by adding in augmented data. The transformations I've seen, granted new at CNN's, are random cropping or rotations. The only tangible way to test this theory would be to try it on a dataset. But just wondering if there is a paper or article that summarizes this. Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l312qm/data_transformation_and_augmentation_for_cnn/"}, {"autor": "fuzzopinion", "date": "2021-05-22 03:41:27", "content": "Deepface 2.0: how much impact do still images added to data_src have on training a model? /!/ I have 10 mins of video source material for data\\_src that is kind of low quality but have around 20-60 high quality still images I could add to data\\_src. How are still images weighed in constructing a model if they're higher in quality than video but far far lower in number than the number of frames in the video source material? Is it even worth finding like 50 high quality images if I have 10 minutes of 30fps lower quality video? \n\nIf I add the same still -----> image !!!  to multiple frames in data\\_src will that make the model put more weight on those particular -----> image !!! s since they appear more often or does it throw out duplicate -----> image !!! s?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ni9xu7/deepface_20_how_much_impact_do_still_images_added/"}, {"autor": "bestgoalsandribbles", "date": "2021-05-22 01:37:32", "content": "-----> Image !!!  captioning using simple CNN+RNN predicting only the same label /!/ I am taking part in brystal myers competition on kaggle which tries to predict InChI formula for compounds. \n\n`initializer = tf.keras.initializers.GlorotUniform()`\n\n`inp = L.Input(shape=(dim[0],dim[1],1),name = 'Input')`\n\n`labels = L.Input(shape=(None,),name= 'Labels')`\n\n`X = L.Conv2D(128,3,strides=1,name='Conv2D_1', padding='same')(inp)`\n\n`X = L.BatchNormalization(name='norm1')(X)`\n\n`X = L.Activation('relu',name='relu_1')(X)`\n\n`X = L.Conv2D(64,5,strides=1,name='Conv2D_2', padding='same')(X)`\n\n`X = L.BatchNormalization(name='norm2')(X)`\n\n`X = L.Activation('relu',name='relu_2')(X)`\n\n`X = L.Conv2D(64,7,strides=1,name='Conv2D_3', padding='same')(X)`\n\n`X = L.BatchNormalization(name='norm3')(X)`\n\n`X = L.Activation('relu',name='relu_3')(X)`\n\n`X = L.Conv2D(64,9,strides=3,name='Conv2D_4')(X)`\n\n`X = L.BatchNormalization(name='norm4')(X)`\n\n`X = L.Activation('relu',name='relu_4')(X)`\n\n   \n\n`X = L.Dropout(0.2,name='Dropout_1')(X)`\n\n`inp2 = tf.image.resize(inp,[X.shape[1],X.shape[2]])`\n\n`X = L.Add(name='Add_1')([X,inp2])`\n\n`X = L.BatchNormalization(name='norm_A1')(X)`\n\n`#X = L.MaxPooling2D(name='Max2D_A1')(X)`\n\n`X = L.Dropout(0.2,name='Dropout_A1')(X)`\n\n`X = tf.reduce_sum(X,axis=3)`\n\n   \n\n`X = L.Bidirectional(L.LSTM(256,return_sequences=True,dropout=0.2))(X)`\n\n`X = L.Bidirectional(L.LSTM(512,return_sequences=True,dropout=0.2))(X)`\n\n`Out = L.Dense(len(characters)+1,activation='softmax',name='Output',kernel_initializer = initializer)(X)`\n\n`model = tf.keras.Model(inputs=inp,outputs=Out)`\n\n`adam = Adam(learning_rate=0.001, amsgrad=True)`\n\n\n\n`model.compile(optimizer=adam,loss='categorical_crossentropy')`\n\n`print(model.summary())`\n\n`return model`\n\nThis is the baseline model I am using. It only learns one compound name. No matter what the input, it outputs the same thing(ex- C12H1206c11). Any tips on how to deal with this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ni7wyk/image_captioning_using_simple_cnnrnn_predicting/"}, {"autor": "iim7_V6_IM7_vim7", "date": "2021-05-21 00:47:25", "content": "Anyone know of a good guide (On Towards Data Science of something) that covers passing the mask output from an -----> image !!!  segmentation model to a classification model? /!/ For example, let's say I have an image with a person. I want to first use image segmentation to remove the background, which I can find a lot of guides for, but THEN I want to take that person and pass it to a classifier to determine which specific person it is (I'll be training it on a number of photos of different people).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nhg25l/anyone_know_of_a_good_guide_on_towards_data/"}, {"autor": "Mr_Walts", "date": "2021-05-20 18:50:30", "content": "Hidden layers and nodes /!/ Hi! I\u2019m wanting to start a personal project where I use my code to identify things in an -----> image !!!  and I want to use convolutional neural networks. I\u2019m wondering, how do you know how many hidden layers you should include and how many nodes per hidden layer you should have? In general do you break the problem into multiple steps say for example first you look for a circle then the color and output would be it\u2019s a blue circle and so you would have 2 hidden layers? One for finding the color and the other for finding the shape? Sorry if this is super messy and everywhere I guess I don\u2019t know how to formulate my thoughts lol. I should also say my goal with this project is to look at brain MRIs and identify if there\u2019s a tumour or not and where it\u2019s located. \n\nThanks for your help!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nh89mj/hidden_layers_and_nodes/"}, {"autor": "hmhhuq", "date": "2021-05-20 17:46:37", "content": "Python and AI Art road map? /!/ So I'm a beginner with Python. This may be jumping the gun a bit, but my main goal is so build up my skill set so I can make AI art using things like machine learning and -----> image !!!  generation; only I'm not sure how to learn the necessary skills, and what road I would take to make that a reality. Right now I'm learning the basics of Python and just dipping my feet into AI. If anyone could help point me in the right direction that would be great!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nh6occ/python_and_ai_art_road_map/"}, {"autor": "amw5gster", "date": "2021-05-20 12:55:59", "content": "Zone density distribution, via computer vision /!/ I have a task that may or may not require machine learning.  But it's derived from computer vision data, so I think it may just barely qualify for this sub.\n\nThere is a large warehouse with a singular entrance/exit.  The floor is divided into 8 mutually exclusive &amp; exhaustive zones, including the entrance area.  I need to estimate the relative (proportional) distribution of people in each individual zone at any given moment.  That is, at 9am, Zona A has 10%, Zone B has 5%, ..., Zone H has 12%.  The absolute number is of secondary importance.\n\nPeople inside the warehouse can move freely, although there are a number of physical barriers.  They can also enter/exit the warehouse freely.  Each pair of adjacent zones may have 1 or more routes between them.  And at each boundary, I have a single -----> camera !!! .  Models have been trained to count the gross flow of people from one side of the boundary to the other in discrete intervals.  That is, for a given perimeter line between zones A and B of the floor, I get a 9:00am-9:05am count of people going from zone A to B, and another count from zone B to A.  The attached image gives an idea of floor layout, and the camera placements (in blue).\n\nThere is no ground truth to the zone-level distribution.  But I do have very high confidence in knowing the total number of people on the floor at any given time.  I also have a range of confidence in the perimeter cameras, and their ability to estimate between-zone flows.  For example, if I take the camera models at face value, some zones will show cumulative counts of negative people.  Clearly impossible.\n\nI'm struggling with how to define a model that yields my desired outcome.  Nothing seems to quite fit the problem.  But thought the community might have some creative ideas.\n\nThanks in advance!\n\nhttps://preview.redd.it/54zmvsyuv9071.png?width=1152&amp;format=png&amp;auto=webp&amp;s=09b1cc82347125566583781d791c85c6b8e9b3d3", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ngzgyu/zone_density_distribution_via_computer_vision/"}, {"autor": "annoying_seagull", "date": "2021-05-19 17:30:35", "content": "Problem implementing Hopfield network in python. Doesn't return learned pattern. /!/  \n\nI'm trying to implement a Hopfield Network in python using the NumPy library. The network has 2500 nodes (50 height x 50 width). The network learns 10 patterns from images of size 50x50 stored in \"patterns\" folder. The images are of numbers 0 to 9. The images are converted to 2d Array, flattened to 1d (2500x1) and learnt.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/zdf4iimy34071.png?width=509&amp;format=png&amp;auto=webp&amp;s=a295381f5d0fe05e21ba7d199bbf7febe31419a6\n\nAfter learning the patterns, it is given a number of an -----> image !!!  (from which I have removed a few pixels from) to match the stored patterns with. But the issue I'm facing is that it does not converge to one of the stored patterns but rather outputs something sort of random.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/4v1vvnuz34071.png?width=1653&amp;format=png&amp;auto=webp&amp;s=eb539652b260293dcdde6f1cc6ae0f2e42209f1a\n\nEven if I try to input it exact pattern from one of the ones it has learnt it still doesn't converge to that pattern. \n\n&amp;#x200B;\n\nhttps://preview.redd.it/wz0jeub144071.png?width=1653&amp;format=png&amp;auto=webp&amp;s=5a7f915e421ba1fa5fdd6cfbb3911a0e6e4b6994\n\nHere's the code of my try at implementing Hopfield networks.\n\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from matplotlib import image\n    from os import listdir\n    import random\n    from os.path import isfile, join\n    \n    # Hopefiel Network parameters\n    input_width = 50\n    input_height = 50\n    number_of_nodes = input_width * input_height\n    learning_rate = 1\n    \n    # initialise node/input array to -1, and weights array to 0\n    input = np.zeros((number_of_nodes))\n    input[True] = -1\n    weights = np.zeros((number_of_nodes,number_of_nodes))\n    \n    #*******************************\n    # Main Hopefield Functions\n    #********************************\n    # Randomly fire nodes until the overall output doesn't change\n    # match the pattern stored in the Hopefield Net.\n    def calculate_output(input, weights):\n        changed = True\n        while changed:\n            indices = list(range(len(input)))\n            random.shuffle(indices)\n            new_input = np.zeros((number_of_nodes))\n            \n            clamped_input = input.clip(min=0) # eliminate nodes with negative value, doesn't work either way\n            for i in indices:\n                sum = np.dot(weights[i], clamped_input)\n                new_input[i] = 1 if sum &gt;= 0 else -1\n                changed = not np.allclose(input[i], new_input[i], atol=1e-3)\n            input = np.array(new_input)\n        \n        return np.array(input)\n                \n    # activation(W x I) = Output\n    # match the pattern stored in the Hopefield Net.\n    def calculate_output_2(input, weights):\n        output = np.dot(weights,input)\n        # apply threshhold\n        output[output &gt;= 0] = 1 # green in image\n        output[output &lt; 0] = -1 # purple in image\n        return output\n    \n    \n    # Store the patterns in the Hopfield Network\n    def learn(input, weights):\n        I = np.identity(number_of_nodes) # diagnol will always be 1 if input is only 1/-1\n        updates = learning_rate * np.outer(input,input) - I\n        updates = updates/number_of_nodes\n        weights[:] = weights + updates\n    \n    \n    \n    #*******************************\n    # Misc. Functions\n    #*******************************\n    # plot an array and show on the screen\n    def show_array(arr):\n        data = arr.reshape((-1, input_width))\n        plt.imshow(data) # plotting by columns\n        plt.show()\n        \n    # learn the patterns (images) placed in \"patterns\" folder (images of numbers 0-9)\n    def learn_numbers():\n        for f in listdir(\"patterns/\"):\n            file = join(\"patterns/\", f)\n            if isfile(file):\n                print(file)\n                im = image.imread(file)\n                grey = im[:,:,0] # convert to 2d array from 3channel rgb image\n                grey = np.where(grey==1,-1,1) # convert white pixel to -1 and otherwise (black) to 1\n                learn(grey.flatten(), weights) # convert 2d image to 1d array (2500) and store in weights\n    \n    # read a test image and match the nearest pattern\n    def calculate_img_output(weights, image_address):\n        # show the image being tested\n        im = image.imread(image_address)\n        grey = im[:,:,0] # convert to 2d array from 3channel rgb image\n        grey = np.where(grey==1,-1,1) # convert white pixel to -1 and otherwise (black) to 1\n        plt.imshow(grey) # plotting by columns\n        plt.show()\n        \n        # retrieve the pattern using random firing\n        output = calculate_output(grey.flatten(), weights)\n        # show the pattern\n        show_array(output)\n        \n        # retrieve the pattern\n        output = calculate_output_2(grey.flatten(), weights)\n        # show the pattern\n        show_array(output)\n        \n        \n    #****************************    \n    # Testing code\n    #*****************************\n    \n        \n    # learn the patterns of image of number 0-9\n    learn_numbers()\n    # Test the network\n    calculate_img_output(weights, \"partial/p.png\")\n    \n\nAny help on why the Network is behaving this way and what can be done to fix this is greatly appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ngbv2z/problem_implementing_hopfield_network_in_python/"}, {"autor": "nivedwho", "date": "2021-05-27 17:29:59", "content": "What are some interesting applications of text-to------> image !!!  models? /!/ What are some useful and interesting applications of text-to-image models such as StackGANs or AttnGANs?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nmcp4g/what_are_some_interesting_applications_of/"}, {"autor": "VikasOjha666", "date": "2021-05-27 14:12:03", "content": "Detecting Text-lines in a Document -----> Image !!!  Using Deep Learning", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nm8cnh/detecting_textlines_in_a_document_image_using/"}, {"autor": "YoloTeabaggins", "date": "2021-05-27 09:46:00", "content": "Which network do you suggest for determining orientation relative to an object? /!/  Hi people, I'm trying to get a result of my orientation relative to a pictured object using neural networks.\n\nSo I have taken a few thousand pictures from 4 angles (0, 90 180 and 270 degrees). This is to be my dataset. Now I want my neural network to then determine from which of those 4 angles the -----> picture !!!  was taken.\n\nI have looked at YOLO but that seems more catered towards object detection.\n\nAny and all advice, pointers or links are welcome. Cheers!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nm3ubl/which_network_do_you_suggest_for_determining/"}, {"autor": "War_Smooth", "date": "2021-05-27 04:42:13", "content": "Paper on cocoa beans classification /!/ I found this paper [article ](http://ijt.oauife.edu.ng/index.php/ijt/article/view/153) useful for my literature review on -----> image !!!  classification.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nlzpm9/paper_on_cocoa_beans_classification/"}, {"autor": "buffml", "date": "2021-05-26 19:31:22", "content": "Multi-Class -----> Image !!!  Classification Flask App | Complete Project", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nlpden/multiclass_image_classification_flask_app/"}, {"autor": "Successful-Forever12", "date": "2021-05-26 16:17:17", "content": "Need help trying to deploy Teachable Machine modle to TFLite Micro on Arduino Nano BLE 33 /!/ Hi all, \n\nI'm trying to use the [Person Detectoin TFLite Micro](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/person_detection) example project as a template and insert my own model that I've created using [TeachableMachine.](https://teachablemachine.withgoogle.com/) However, it's not working as expected, and giving a \"Invoke Failed()\" error in the serial monitor when I run the script on my Nano 33 BLE. Can someone please take a look and tell me what I did wrong?\n\nHere is how I approached it:\n\n1) Create a model with [Teachable Machine](https://teachablemachine.withgoogle.com/)\n\n2) Download my model as a keras file\n\n3) Convert from Keras model to tflite model, and then integer quantize the model using the code here: [https://colab.research.google.com/drive/12O9qO6bAI72B0RTt88sQPkcHkC16Mb8O#scrollTo=cKTbVvb2Vsyo](https://colab.research.google.com/drive/12O9qO6bAI72B0RTt88sQPkcHkC16Mb8O#scrollTo=cKTbVvb2Vsyo)\n\n4) Convert from tflite to .cc using : `xxd -i converted_model.tflite &gt; model_data.cc`\n\n5) Replace value of g\\_person\\_detect\\_model\\_data\\_len with new value from my model\\_data.cc\n\n6) Replace model\\_data with new model data from my model\\_data.cc\n\n7) Use [https://netron.app/](https://netron.app/) to visualize the network and see what MicroOpsResolvers need to be included. Include as necessary.\n\nThe only two files in the project that I touch at all are person\\_detection.ino and person\\_detection\\_model\\_data.cpp. Please help me figure out why it's not working!!!\n\nHere is my person\\_detection.ino file for you to look at:\n\n    /* Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n    \n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n    \n        http://www.apache.org/licenses/LICENSE-2.0\n    \n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n    ==============================================================================*/\n    \n    #include &lt;TensorFlowLite.h&gt;\n    \n    #include \"main_functions.h\"\n    \n    #include \"detection_responder.h\"\n    #include \"image_provider.h\"\n    #include \"model_settings.h\"\n    #include \"person_detect_model_data.h\"\n    #include \"tensorflow/lite/micro/micro_error_reporter.h\"\n    #include \"tensorflow/lite/micro/micro_interpreter.h\"\n    #include \"tensorflow/lite/micro/micro_mutable_op_resolver.h\"\n    #include \"tensorflow/lite/schema/schema_generated.h\"\n    #include \"tensorflow/lite/version.h\"\n    \n    // Globals, used for compatibility with Arduino-style sketches.\n    namespace {\n    tflite::ErrorReporter* error_reporter = nullptr;\n    const tflite::Model* model = nullptr;\n    tflite::MicroInterpreter* interpreter = nullptr;\n    TfLiteTensor* input = nullptr;\n    \n    // In order to use optimized tensorflow lite kernels, a signed int8_t quantized\n    // model is preferred over the legacy unsigned model format. This means that\n    // throughout this project, input images must be converted from unisgned to\n    // signed format. The easiest and quickest way to convert from unsigned to\n    // signed 8-bit integers is to subtract 128 from the unsigned value to get a\n    // signed value.\n    \n    // An area of memory to use for input, output, and intermediate arrays.\n    constexpr int kTensorArenaSize = 136 * 1024;\n    static uint8_t tensor_arena[kTensorArenaSize];\n    }  // namespace\n    \n    // The name of this function is important for Arduino compatibility.\n    void setup() {\n      // Set up logging. Google style is to avoid globals or statics because of\n      // lifetime uncertainty, but since this has a trivial destructor it's okay.\n      // NOLINTNEXTLINE(runtime-global-variables)\n      static tflite::MicroErrorReporter micro_error_reporter;\n      error_reporter = &amp;micro_error_reporter;\n    \n      // Map the model into a usable data structure. This doesn't involve any\n      // copying or parsing, it's a very lightweight operation.\n      model = tflite::GetModel(g_person_detect_model_data);\n      if (model-&gt;version() != TFLITE_SCHEMA_VERSION) {\n        TF_LITE_REPORT_ERROR(error_reporter,\n                             \"Model provided is schema version %d not equal \"\n                             \"to supported version %d.\",\n                             model-&gt;version(), TFLITE_SCHEMA_VERSION);\n        return;\n      }\n    \n      // Pull in only the operation implementations we need.\n      // This relies on a complete list of all the ops needed by this graph.\n      // An easier approach is to just use the AllOpsResolver, but this will\n      // incur some penalty in code space for op implementations that are not\n      // needed by this graph.\n      //\n      // tflite::AllOpsResolver resolver;\n      // NOLINTNEXTLINE(runtime-global-variables)\n      static tflite::MicroMutableOpResolver&lt;10&gt; micro_op_resolver;\n    \n      micro_op_resolver.AddPad();\n      micro_op_resolver.AddConv2D();\n      micro_op_resolver.AddDepthwiseConv2D();\n      micro_op_resolver.AddSoftmax();\n      micro_op_resolver.AddRelu6();\n      micro_op_resolver.AddRelu();\n      micro_op_resolver.AddAdd();\n      micro_op_resolver.AddMean();\n      micro_op_resolver.AddFullyConnected();\n      micro_op_resolver.AddQuantize();\n    \n      // Build an interpreter to run the model with.\n      // NOLINTNEXTLINE(runtime-global-variables)\n      static tflite::MicroInterpreter static_interpreter(\n          model, micro_op_resolver, tensor_arena, kTensorArenaSize, error_reporter);\n      interpreter = &amp;static_interpreter;\n    \n      // Allocate memory from the tensor_arena for the model's tensors.\n      TfLiteStatus allocate_status = interpreter-&gt;AllocateTensors();\n      if (allocate_status != kTfLiteOk) {\n        TF_LITE_REPORT_ERROR(error_reporter, \"AllocateTensors() failed\");\n        return;\n      }\n    \n      // Get information about the memory area to use for the model's input.\n      input = interpreter-&gt;input(0);\n    }\n    \n    // The name of this function is important for Arduino compatibility.\n    void loop() {\n      // Get -----> image !!!  from provider.\n      if (kTfLiteOk != GetImage(error_reporter, kNumCols, kNumRows, kNumChannels,\n                                input-&gt;data.int8)) {\n        TF_LITE_REPORT_ERROR(error_reporter, \"Image capture failed.\");\n      }\n    \n      // Run the model on this input and make sure it succeeds.\n      if (kTfLiteOk != interpreter-&gt;Invoke()) {\n        TF_LITE_REPORT_ERROR(error_reporter, \"Invoke failed.\");\n      }\n    \n      TfLiteTensor* output = interpreter-&gt;output(0);\n    \n      // Process the inference results.\n      int8_t person_score = output-&gt;data.uint8[kPersonIndex];\n      int8_t no_person_score = output-&gt;data.uint8[kNotAPersonIndex];\n      RespondToDetection(error_reporter, person_score, no_person_score);\n    }", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nlkuer/need_help_trying_to_deploy_teachable_machine/"}, {"autor": "chansung18", "date": "2021-05-26 13:41:54", "content": "MLOps big -----> picture !!!  in GCP /!/ I have recently wrote an Medium article about \"MLOps big picture in GCP\".\n\nI am sharing here in case anyone is interested in the subject.\n\n**link**: [https://medium.com/google-developer-experts/mlops-big-picture-in-gcp-a637566d6ae8](https://medium.com/google-developer-experts/mlops-big-picture-in-gcp-a637566d6ae8)\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nlhc3y/mlops_big_picture_in_gcp/"}, {"autor": "nhokthanh99mc", "date": "2021-05-26 10:03:27", "content": "How to choose a dataset for training -----> image !!!  classification? /!/ I'm making a mask detection model to detect if the person in the image wearing a mask or not. I've seen some datasets online but they come with different image sizes and numbers of images. I plan to crop the face in the image out and resize it but I don't know what size and how many images should I do it. So my question is :\n\n* Which image size should I choose?\n* How many images should use to I train the model?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nlda3j/how_to_choose_a_dataset_for_training_image/"}, {"autor": "kaia_1527", "date": "2021-04-05 11:33:32", "content": "Basic Floor Plan Detection /!/ Hi everyone, first-time poster. This one should be easy: is there a model that, given an -----> image !!! , recognizes whether the -----> image !!!  is floor plan (typically of a residential property)? Other than a boolean, don't need anything else. Should be quick to train one, but wanted to check whether there's a generally accepted model out there. Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mkijak/basic_floor_plan_detection/"}, {"autor": "Suzzy67", "date": "2021-04-04 16:06:11", "content": "Valid accuracy is low then training acc /!/ My valid accuracy is less tgen training accuracy..how can i improve my valid accuracy..dta is splid into 75 and 25 percent..-----> image !!!  quality is also good.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mjz2yg/valid_accuracy_is_low_then_training_acc/"}, {"autor": "MrZipZap", "date": "2021-04-04 10:38:04", "content": "Are simple algorithms like Linear Regression and KNN fast enough to use in real time, multiple times per frame? /!/ I'm trying to make a program that detects Rubik's cubes faces and the colors on that face. For the second part of that problem, I have -----> image !!! s of just the face (like [this](https://i.imgur.com/ji64xSe.png)), and collecting 9 samples from the -----> image !!!  like [this](https://i.imgur.com/bNTuIrT.png) because there are 9 colors per face. Then I collect the mean RGB values for each of the 9 samples and convert to the HSV color space, so I can try to figure out what color it is based on static thresholds. However, this doesn't work well due to different lighting conditions, so I'm thinking about instead plugging the HSV values into a machine learning algorithm like Linear Regression or KNN. My worry is that if I do this, it will not be able to run in real time anymore?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mjtuvs/are_simple_algorithms_like_linear_regression_and/"}, {"autor": "cmillionaire9", "date": "2021-04-03 08:11:14", "content": "Interactive text-driven -----> image !!!  manipulation", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mj4ag5/interactive_textdriven_image_manipulation/"}, {"autor": "mod_insanity", "date": "2021-04-03 04:53:53", "content": "Questionfor CNN /!/ Can i use just grayscale -----> image !!!  in CNN for fruit grading?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mj1p41/questionfor_cnn/"}, {"autor": "YaswanthBangaru", "date": "2021-04-08 14:53:55", "content": "Trying to figure out what methods to use to approach a pattern formation problem /!/ Hey folks, like the title says, I am trying to find triangles in the following -----> image !!! , I am not quite sure if I should use edge detection or some other -----> image !!!  recognition model or perhaps, some very basic methods which might not be machine learning, any thoughts?? Basically I want the model to find out those green bubbles and draw the green straight lines in the form of a triangle without a base. Is it even possible?\n\nhttps://preview.redd.it/dc5glpzrqyr61.png?width=3356&amp;format=png&amp;auto=webp&amp;s=249e63b3ae7235c729377734cff023db3eb8ff0e", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mmtjcz/trying_to_figure_out_what_methods_to_use_to/"}, {"autor": "kimtaengsshi9", "date": "2021-04-08 14:46:22", "content": "AttributeError: 'Dense' object has no attribute 'kernel' when trying to run TensorFlow model inference in a thread /!/ I am implementing a variant of [https://www.kaggle.com/ratthachat/flickr-image-captioning-tpu-tf2-glove](https://www.kaggle.com/ratthachat/flickr-image-captioning-tpu-tf2-glove) for a project. The model classes are identical.\n\nSo far, I've managed to get it to work. The model outputs a caption for the input -----> image !!!  just fine. No errors so far. This is the working code:\n\n    @app.post('/api/uploadFile')\n    async def upload_file(file: UploadFile = Form(...)):\n        if file.content_type.startswith('image/'):\n            filepath = f'./static/uploads/{file.filename}'\n            with open(filepath, 'wb') as buffer:\n                copyfileobj(file.file, buffer)\n            print(generate_caption(filepath)) # Calls image captioning model\n            print(detect_labels(filepath, config['detect_func'], config['cat_idx'])) # Calls object detection model. detect_func and cat_idx are initialised at server startup\n            print('done')\n        return {\n            'uploadSuccess': True\n        }\n    \n    def generate_caption(image):\n        try:\n            hidden = decoder.reset_state(batch_size=1)\n        except:\n            hidden = decoder.layers[-1].reset_state(batch_size=1)\n    \n        img_tensor_val = tf.expand_dims(decode_image(image), 0)\n        features = encoder(img_tensor_val)\n        dec_input = tf.expand_dims([tokenizer.word_index['&lt;start&gt;']], 0)\n        result = list()\n    \n        for i in range(MAX_LEN):\n            predictions, hidden, _ = decoder(dec_input, features, hidden)\n            predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n            result.append(tokenizer.index_word[predicted_id])\n            if tokenizer.index_word[predicted_id] == '&lt;end&gt;':\n                break\n            dec_input = tf.expand_dims([predicted_id], 0)\n    \n        return ' '.join(\n            [word for word in result\n             if word != '&lt;start&gt;' and word != '&lt;end&gt;' and word != '&lt;unk&gt;'])\n    \n    def detect_labels(img_path: str, detect_fn, category_index):\n        img_np = load_image(img_path)\n        input_tensor = convert_to_tensor(np.expand_dims(img_np, 0), dtype=float32)\n        detections, _, _ = detect_fn(input_tensor)\n       \n        classes = (detections['detection_classes'][0].numpy() + 1).astype(int)\n        scores = detections['detection_scores'][0].numpy()\n        labels = {category_index[classes[i]]['name']\n                  for i in range(scores.shape[0]) if scores[i] &gt; 0.3}\n        \n        return labels\n\nThen, I convert the model calling portion of the code into a thread:\n\n    @app.post('/api/uploadFile')\n    async def upload_file(file: UploadFile = Form(...)):\n        if file.content_type.startswith('image/'):\n            filepath = f'./static/uploads/{file.filename}'\n            with open(filepath, 'wb') as buffer:\n                copyfileobj(file.file, buffer)\n                thread = Thread(target=annotate_image, name=f'{filepath}_annotation_thread', args=[filepath])\n                thread.start()\n        return {\n            'uploadSuccess': True\n        }\n        \n    def annotate_image(filepath: str):\n        print(f'Starting {filepath}_annotation_thread')\n        print(generate_caption(filepath))\n        print(detect_labels(filepath, config['detect_func'], config['cat_idx']))\n        print(f'{filepath} done')\n\nWhen I run this code and upload 5 images, the first 2 images managed to be parsed and inference for both models completed just fine. However, the next 3 images throw this:\n\n&gt;AttributeError: 'Dense' object has no attribute 'kernel'\n\nAny idea what's wrong?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mmtdvt/attributeerror_dense_object_has_no_attribute/"}, {"autor": "StrasJam", "date": "2021-04-08 07:28:44", "content": "Anyone have experience using Softmax in multi-label classification /!/ I was reading this [paper](https://arxiv.org/pdf/1805.00932.pdf)   put out by a group of researchers at Facebook where they found that   using a softmax and CE loss function during  training led to improved   results over sigmoid + BCE. During training they change the one-hot   label vector such that each '1' is divided by the  number of labels for   the given -----> image !!!  (e.g. from \\[0, 1, 1, 0\\] to \\[0, 0.5,  0.5, 0\\]).\n\nHowever,   they do not mention how this could then be used in the  inference   stage, because the required threshold for selecting the  correct labels   is not clear and would theoretically need to be set based upon the   expected number of labels for the image (which is information which   wouldn't be available at inference).\n\nHas anyone else read this paper or have an idea how this would work?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mmmht1/anyone_have_experience_using_softmax_in/"}, {"autor": "gregory_k", "date": "2021-04-08 06:26:21", "content": "How I built a reverse -----> image !!!  search for pixel art", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mmlnos/how_i_built_a_reverse_image_search_for_pixel_art/"}, {"autor": "TheSnazzyBoi", "date": "2021-04-08 04:00:51", "content": "How much machine learning can I learn between now and November? /!/ I'm a high school junior right now, and I realized today that I need to do something really big if I want to have a shot at getting into MIT or an Ivy League for computer science. For my math and CS pre reqs, I'll have completed Calc 3 by July and I'm currently in the 3rd year of my high school's computer science classes (Android app dev). \nIf I truly focus on it, about what can I expect to make by November when early admissions will be due? Can I expect something like a -----> photo !!!  recognition software, image generator, or game AI? Also, what entry-level hardware will I need?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mmjjyy/how_much_machine_learning_can_i_learn_between_now/"}, {"autor": "MrZipZap", "date": "2021-04-08 03:02:50", "content": "(Tensorflow or cv2 issue) Trying to do real-time detection, but video is lagging behind /!/ So I have done transfer learning on a Tensorflow model to detect rubik's cubes. Since I don't have a webcam, I am using an app called IP Webcam to use my phone's -----> camera !!!  and grab the live feed with cv2, like this:\n\n`cap\u00a0=\u00a0cv.VideoCapture(0)`\n\n`address\u00a0=\u00a0\"http://{My IP}/video\"`\n\n`cap.open``(address)`\n\nWhen I run the object detection in real-time (this is running on a gtx 1060), the model understandably can't keep up with the 30 fps of the -----> camera !!! , but instead of displaying, for example, the live detection at 10 fps, it seems to want to display all 30 frames even if it takes longer, resulting in the video feed not being real-time and if I move it takes around 5-10 seconds to show up in the video.\n\nI don't know if this is an issue with Tensorflow or cv2? Is the issue that I'm not using a connected webcam?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mmikwu/tensorflow_or_cv2_issue_trying_to_do_realtime/"}, {"autor": "Miton7", "date": "2021-04-08 00:10:42", "content": "Questions about Convolutional networks /!/ I'm currently working on making -----> image !!!  identification Tensorflow models, and I'm confused about what I should do to improve performance.  For example, I have this basic model:\n\n&amp;#x200B;\n\nmodel = tf.keras.Sequential()\n\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input\\_shape=(224, 224, 1)))\n\nmodel.add(layers.MaxPooling2D((2, 2)))\n\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu'))\n\nmodel.add(layers.MaxPooling2D((2, 2)))\n\nmodel.add(layers.Flatten())\n\nmodel.add(layers.Dense(64))\n\nmodel.add(layers.Dense(1000, activation='softmax'))\n\n&amp;#x200B;\n\nSince this is training on imagenet, it's very slow and usually crashes on my nvidia computer before it hits even 1% accuracy.  Are there ways I can prevent losing data from the crashes and increase the rate at which accuracy builds, such as by adding additional layers or running it on a particular online platform?  Thanks in advance for any advice given.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mmfkqe/questions_about_convolutional_networks/"}, {"autor": "ccrbltscm", "date": "2021-04-07 18:15:21", "content": "Must-Read Papers on Generative Adversarial Networks (GANs) + Video Summaries /!/ For anyone who is getting into GANs, here a helpful [collection of the most important GAN papers along with video summaries](https://crossminds.ai/graphlist/must-read-papers-on-generative-adversarial-networks-gans-6067d8a100833db8e01c4fe6/), including the presentation by Ian Goodfellow in 2016. Papers covered include:\n\n* Generative Adversarial Networks\n* Conditional Generative Adversarial Nets\n* Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\n* Improved Techniques for Training GANs\n* -----> Image !!! -to------> Image !!!  Translation with Conditional Adversarial Networks\n* StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks\n* Wasserstein GAN\n* Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\n* BEGAN: Boundary Equilibrium Generative Adversarial Networks\n* Towards the Automatic Anime Characters Creation with Generative Adversarial Networks\n* Progressive Growing of GANs for Improved Quality, Stability, and Variation\n* Are GANs Created Equal? \n* A Large-Scale StudySpectral Normalization for Generative Adversarial Networks\n* Evolutionary Generative Adversarial NetworksSelf-Attention Generative Adversarial Networks\n* Large Scale GAN Training for High Fidelity Natural Image Synthesis\n* A Style-Based Generator Architecture for Generative Adversarial Networks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mm8a4j/mustread_papers_on_generative_adversarial/"}, {"autor": "SQL_beginner", "date": "2021-04-07 16:09:08", "content": "Feature Selection for Large Datasets /!/ To begin my question, I would like to quote a paper (by Ishawaran et al) on \"random forests for survival analysis data\", in which the authors (very concisely) outline the difficulties of feature selection (i.e. which variables to include in a statistical model) in classical regression models and how this problem is somewhat alleviated with more advanced models :\n\n\"Further, because these methods (i.e. classical regression models, e.g. cox ph regression - even though it's semi-parametric) are often parametric, nonlinear effects of variables must be modeled by transformations or expanding the design matrix to include specialized basis functions. Often ad hoc approaches, such as stepwise regression, are used to determine if nonlinear effects exist. Identifying interactions, especially those involving multiple variables, is also problematic. This must be done by brute force (examining all two-way and threeway interactions, e.g.), or must rely on subjective knowledge to narrow the search.\n\nIn contrast, these difficulties are handled automatically using forests. We illustrate the ease with which RSF can uncover complex data structures through an in-depth case study of the prognostic implications of being underweight, overweight, or obese and having severe, but stable coronary artery disease.\n\nInvestigators have noted complex patterns surrounding possible reverse causation in underweight individuals, interactions with smoking, and an unclear inflection point at which point increasing body mass confers increased risk Some have identified a possible obesity paradox among patients with established heart disease in which increased body mass predicts better survival. To clarify these issues, we analyzed a large cohort of patients with coronary artery disease undergoing isolated coronary artery bypass surgery. Using RSF, (random survival forest) we identified a complex relationship between long-term survival, body mass, renal (kidney) function, smoking, and number of internal coronary artery bypass grafts. We believe our novel findings help explain some of the apparent contradictions previously reported.\"\n\nSource: https://arxiv.org/pdf/0811.1645.pdf\n\nEssentially, the authors claim that traditional regression models struggle with feature selection and the newer models (e.g. bagging, random forest) are able to better deal with feature selection. I do remember from an intro stats class, the somewhat tedious process of determining which variables to include in a multiple linear regression model. As the authors described, I remember there was something called \"CP Mallow's Criteria\" in which potential variables were repeatedly included and excluded in the regression model and the value of CP Mallow's Criteria was monitored - a final selection of variables for the model was decided on the basis of this criteria. However, this selection process becomes inefficient for large datasets (if I understand correctly, this means you would have to refit the model for many different combinations of variables, resulting in a \"combinatorics explosion\" for a large number of variables). Like the authors mention, you can also \"manually hard code\" interaction terms in the model (e.g. log(var1), var1var2, var1/(var2var3), var1/(var2+var3), etc.) - and there an infinite such number of potential interactions. Improper feature selection can also result in unwanted effects such as multicollinearity. The last point I would like to bring up - although my knowledge of mathematics is not strong enough to fully substantiate it - is that classical regression models are said to have a tendency to overfit (I don't know why - I have seen visual demonstrations of this, but I don't know if there is a mathematical explanation behind this, or if it's just an empirical observation) and poorly generalize to new data (again, I don't know why); and that classical regression models are only able to \"recognize linearly separable patterns in the data\" (intuitively I can understand this, e.g. draw a circle of red points and a smaller circle of blue points that fits in the red circle, a single line can not separate the two colors - but I don't know if there is a mathematical explanation behind this).\n\nThis brings me to my question about feature selection for large datasets. With the advent of technology, data is becoming bigger and bigger everyday - convolution neural networks are the \"go to method\" for analyzing -----> picture !!! s (a standard black and white -----> picture !!!  is said to have 786 variables), whereas DNA is said to have even more. In such instances, it surely must be impossible to address feature selection as done in conventional statistical modelling. Please excuse my poor understanding of math - but my understanding is that newer statistical models have \"built in\" methods of handling the feature selection problem. For instance, random forest \"randomly\" chooses different combinations of variables and sees which combinations result in better model performance, the exact randomizing mechanism (uncorrelated trees) is said to also prevent against multicollinearity (I ahve heard that the creator of the random forest algorithm Leo Breiman claims through theoretical statistics that random forest by definition can not \"over fit\" and has some desirable error bounds and convergence properties - is this true?). Meanwhile, I have read on data science blogs (I'm not going to lie) that deep neural networks are able to \"automatically\" learn and consider \"useful\" combinations of features for approximating the target function (am I correct?).\n\nAll in all, what I want to ask here : for large datasets, where sometimes the features don't have any immediate meanings (e.g. a patient's blood pressure vs the information contained in the 231st pixel of a photograph) - is there any \"real\" way to handle feature selection? Or is this usually taken care of by the statistical model itself (e.g. random forest and neural networks)? I have seen examples online where people attempted to write a massive FOR LOOP in which they train the same model with thousands of variable combinations ... but I am not sure how feasible this is.\n\nCan someone please provide a comment on this?\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mm5j1g/feature_selection_for_large_datasets/"}, {"autor": "AdarC98", "date": "2021-04-07 14:38:04", "content": "GANs - Ideas for Image Enhancement (Brain MRI images) /!/ Hi, I'm researching a problem with a special MRI machine which produces very low-quality images.\n\nI have a dataset of around 80 images, each one has 2 'k space' samples which after FFT becomes 2 samples of a client.\n\nI have another dataset of the same ages as the clients, which is high-quality images.\n\nWhat I'm looking for are some new methods for improving -----> image !!!  quality. What I'm working on right now is a CycleGAN that tries to learn how to transfer a low-quality image into a high-quality image (and vice versa, but it just for the model training), and the results are not good.\n\n&amp;#x200B;\n\nCan anyone share any new idea that might help solve the problem? remember this is an unsupervised problem. the high-quality images are different client than those with the low-quality images.\n\n&amp;#x200B;\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mm3jaw/gans_ideas_for_image_enhancement_brain_mri_images/"}, {"autor": "obsezer", "date": "2021-04-07 14:12:17", "content": "Fast-Docker: Docker Tutorial, Sample Usage Scenarios (HowTo: Applications) /!/ Docker can be used to create portable ML applications and independent ML environments. Using Docker Containers, ML applications can easily run on Cloud Environments (AWS, GCP, Azure). \n\nThis repo aims to cover Docker details (Dockerfile, Image, Container, Commands, Volumes, Docker-Compose, Networks, Swarm, Stack) fastly, and possible example usage scenarios (HowTo: Applications) in a nutshell. Possible usage scenarios are aimed to update over time.\n\n**Tutorial Link:** [**https://github.com/omerbsezer/Fast-Docker**](https://github.com/omerbsezer/Fast-Docker)\n\nQuick Look (HowTo)\n\n* [App: Creating First Docker -----> Image !!!  and Container using Docker File](https://github.com/omerbsezer/Fast-Docker/blob/main/First-----> Image !!! FirstContainer.md)\n* [App: Binding Volume to the Different Containers](https://github.com/omerbsezer/Fast-Docker/blob/main/DockerVolume.md)\n* [App: Binding Mount to the Container](https://github.com/omerbsezer/Fast-Docker/blob/main/DockerVolume.md#app_mount)\n* [App: Docker-Compose File - Creating 2 Different Containers: WordPress Container depends on MySql Container](https://github.com/omerbsezer/Fast-Docker/blob/main/DockerCompose.md)\n* [App: Creating Docker Swarm Cluster With 5 PCs using PlayWithDocker : 3 x WordPress Containers and 1 x MySql Container using Docker-Compose File](https://github.com/omerbsezer/Fast-Docker/blob/main/DockerStackService.md)\n* [App: Running Docker Free Local Registry, Tagging -----> Image !!! , Pushing -----> Image !!!  to the Local Registry, Pulling -----> Image !!!  From Local Registry and Deleting -----> Image !!! s from Local Registry](https://github.com/omerbsezer/Fast-Docker/blob/main/DockerLocalRegistry.md)\n* [App: Transferring Content between Host PC and Docker Container](https://github.com/omerbsezer/Fast-Docker/blob/main/DockerTransferringContent.md)\n* [Docker Commands Cheatsheet](https://github.com/omerbsezer/Fast-Docker/blob/main/DockerCommandCheatSheet.md)\n\nTable of Contents\n\n* [Motivation](https://github.com/omerbsezer/Fast-Docker#motivation)\n   * [Needs](https://github.com/omerbsezer/Fast-Docker#needs)\n   * [Benefits](https://github.com/omerbsezer/Fast-Docker#benefits)\n   * [Problems Docker does not solve](https://github.com/omerbsezer/Fast-Docker#problems)\n* [What is Docker?](https://github.com/omerbsezer/Fast-Docker#whatIsDocker)\n   * [Architecture](https://github.com/omerbsezer/Fast-Docker#architecture)\n   * [Installation](https://github.com/omerbsezer/Fast-Docker#installation)\n   * [Docker Engine (Deamon, REST API, CLI)](https://github.com/omerbsezer/Fast-Docker#engine)\n   * [Docker Registry and Docker Hub](https://github.com/omerbsezer/Fast-Docker#registry)\n   * [Docker Command Structure](https://github.com/omerbsezer/Fast-Docker#command)\n   * [Docker Container](https://github.com/omerbsezer/Fast-Docker#container)\n   * [Docker Volumes/Bind Mounts](https://github.com/omerbsezer/Fast-Docker#volume)\n   * [Docker Network](https://github.com/omerbsezer/Fast-Docker#network)\n   * [Docker Log](https://github.com/omerbsezer/Fast-Docker#log)\n   * [Docker Stats/Memory-CPU Limitations](https://github.com/omerbsezer/Fast-Docker#stats)\n   * [Docker File](https://github.com/omerbsezer/Fast-Docker#file)\n   * [Docker Image](https://github.com/omerbsezer/Fast-Docker#image)\n   * [Docker Compose](https://github.com/omerbsezer/Fast-Docker#compose)\n   * [Docker Swarm](https://github.com/omerbsezer/Fast-Docker#swarm)\n   * [Docker Stack / Docker Service](https://github.com/omerbsezer/Fast-Docker#stack)\n* [Play With Docker](https://github.com/omerbsezer/Fast-Docker#playWithDocker)\n* [Docker Commands Cheatsheet](https://github.com/omerbsezer/Fast-Docker#cheatsheet)\n* [Other Resources](https://github.com/omerbsezer/Fast-Docker#resource)\n* [References](https://github.com/omerbsezer/Fast-Docker#references)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mm2z8y/fastdocker_docker_tutorial_sample_usage_scenarios/"}, {"autor": "thraway15", "date": "2021-04-07 05:54:02", "content": "How important is having experience productionizing/deploying ML models for Sr MLE roles? /!/ I have a few years of work experience with -----> image !!!  processing and deep learning. Most of the work has been on image processing with C++, as the work with DL has been on the side with lesser priority. I'm about to have phone screens for Sr MLE roles\n\nI've had some interviews for Sr MLE roles that I failed, but I don't know if it was because I didn't answer the ML/DL knowledge questions well, I didn't explain the DL architectures I worked with well, or I didn't give good answers when interviewers asked questions about my DL work exp. I have made it to a couple onsites for roles, but the phone screens for those were primarily on coding questions, so I think my coding skills are ok but my DL knowledge isn't\n\nThe DL I've worked on has been mostly for research purposes and not really in a production environment that requires scalability, as my company doesn't deal with millions of DAUs. The DL models/architectures I've worked with were obtained from open-source. Do interviewers expect me to know all the details about how these architectures work? \n\nAlso, I haven't really deployed these models in production yet. The main C++ code I've worked on will be productionized soon. I was able to embed the Python pretrained DL model into that C++ code, but it performed inference too slow, so the productionized C++ code won't include that embedded Python code yet\n\nWhen interviewers asked me to explain the DL I've worked on, are they expecting me to give detailed answers on how I completed the projects end-to-end? And how I deployed them? Would interviewers penalize me for not deploying the DL models yet?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mlvh9f/how_important_is_having_experience/"}, {"autor": "74throwaway", "date": "2021-04-07 05:50:14", "content": "How important is experience with full-stack/end-to-end ML/DL for Sr MLE roles? /!/ I have a few years of work experience with -----> image !!!  processing and deep learning. Most of the work has been on image processing with C++, as the work with DL has been on the side with lesser priority. I'm about to have phone screens for Sr MLE roles\n\nI've had some interviews for Sr MLE roles that I failed, but I don't know if it was because I didn't answer the ML/DL knowledge questions well, I didn't explain the DL architectures I worked with well, or I didn't give good answers when interviewers asked questions about my DL work exp. I have made it to a couple onsites for roles which had coding questions in the phone screens, so I think my coding skills are ok but my DL knowledge isn't\n\nWhen interviewers asked me to explain the DL I've worked on, are they expecting me to give detailed answers on how I completed the projects end-to-end? And how I deployed them?\n\nThe DL I've worked on has been mostly for research purposes and not really in a production environment that requires scalability, as my company doesn't deal with millions of daily active users. The DL models/architectures I've worked with were obtained from open-source. Do interviewers expect me to know all the details about how these architectures work? \n\nAlso, I haven't really deployed these models in production yet. The main C++ code I've worked on will be productionized soon. I was able to embed the Python pretrained DL model into that C++ code, but it performed inference too slow, so the productionized C++ code won't include that embedded Python code yet\n\nWould interviewers penalize me for not deploying the DL models yet?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mlvfe0/how_important_is_experience_with/"}, {"autor": "Edulad", "date": "2021-06-06 04:13:34", "content": "Click an Image and Display Similar Results /!/ Hello Everyone :)\n\nHi, so i have been building a Django project in python for the last two months and have got everything ready from user registration to file upload. Now my LAST part left is the following.\n\nA User clicks an -----> image !!!  on site which then drops down and shows similar results of that -----> image !!! .\n\nHow can i achieve that ? I want to fetch the results from the Database back-end of python (Will use the async Fetch function for that ) and the add Event listener for clicking an Image.\n\nWill Really Need  Your Help. Thanks :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ntd9m7/click_an_image_and_display_similar_results/"}, {"autor": "Zealousideal_Ad9723", "date": "2021-06-06 03:14:49", "content": "How does Apple's Create ML Image Classifier react to non-class photos? /!/ I have been using Apple's Image Classifier to create a model to recognize two basic -----> image !!!  classes. The model seems to be suitable in determining between the two classes ie. I give 8 test photos of the two classes and it can determine between the 2 options, but when I give it an image that is clearly not of the two classes it gives me a 99% for a random class. Do you know why this may be and if this is the regular output by models such as this for Apple's Image Classifier when a test photo is none of the classes above?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ntcaih/how_does_apples_create_ml_image_classifier_react/"}, {"autor": "hokagesahab", "date": "2021-06-05 23:24:02", "content": "Creating animation for changing confidence interval in GPs /!/ Hello all, \n\nThere is this link where the very first -----> image !!!  shows converging confidence interval after a data point has been clicked.\n\n[https://distill.pub/2019/visual-exploration-gaussian-processes/](https://distill.pub/2019/visual-exploration-gaussian-processes/)\n\nIf I want to be able to do this in python for my GPR, can anyone suggest an example, on how can I do an animation for the same?  \nSo for instance if I have a code that will generate somethign similar to what is shown in the first image of the link above, I only end up seeing the final result in my python plot.\n\nHow can I do this in a step by step addtion, and animate with perhaps saving image , is what I wanted to ask.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nt879y/creating_animation_for_changing_confidence/"}, {"autor": "mythrowaway0852", "date": "2021-06-05 22:35:24", "content": "Need help understanding the usage of rolling window sequence in a research paper /!/ I'm currently trying to implement this paper [https://www.catalyzex.com/paper/arxiv:2101.02908](https://www.catalyzex.com/paper/arxiv:2101.02908)\n\nin the paper, they calculate subsequences of the original time series data using a rolling window method. See the -----> image !!!  below:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/o9ifusk3yi371.png?width=595&amp;format=png&amp;auto=webp&amp;s=8442e79d75b471ec946f5b5b8c285d16a896860b\n\nBut what I don't understand is that how do you calculate the rolling window sequence at time step k=0, when there are no values behind it? Do you just pad the sequence with values of zero or the mean? Or think of it as negative indexing (to me that sounds a little stupid)?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nt79g1/need_help_understanding_the_usage_of_rolling/"}, {"autor": "spmallick", "date": "2021-06-05 19:25:14", "content": "Non Maximum Suppression: Theory and Implementation in PyTorch /!/ We have an informative new post on\u00a0Non Maximum Suppression with PyTorch\u00a0today.  \n\n\nImagine you have trained a car detector. If trained properly it will create a bounding box around all cars in an -----> image !!! . Now, if you move the detected bounding box by one pixel in any direction, it is still a valid picture of a car. The detector may also select bounding boxes that do not cover the entire car. So, it will end up producing more than one bounding box for the same object unless we do some post-processing to filter out these multiple detections for the same object.  \n\n\nThe class of algorithms for achieving this filtering is called\u00a0Non Maximum Suppression. In today's post, we go over the nuances of the problem and share an implementation. \n\n[https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/](https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch/)\n\nhttps://preview.redd.it/i4eys1u30i371.png?width=600&amp;format=png&amp;auto=webp&amp;s=3778ee1b65ee69d85c7d34ebcc2d2848d3a291c7", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nt3am8/non_maximum_suppression_theory_and_implementation/"}, {"autor": "aurora-s", "date": "2021-06-05 13:24:23", "content": "CNNs where non-local context matters but position of the object is consistent /!/  I have an -----> image !!!  classification dataset, where the object to be classified is always centred in the -----> image !!! . However, the ability to understand context from other parts of the image will be useful for the network to correctly classify the object\n\n\\[An analogy to further clarify this; imagine a dataset with 2 classes, cats and not-cats, with the cat always centred in the frame, but where the background is strongly correlated to whether or not the image is of a cat. Yes, I could get away with a network that detects just the cat, but using that non-local info would be useful for correct classification.\\]\n\nSo far, I've trained a simple CNN classifier a few layers deep, on a cropped version of the image, just zooming in on the main object. It's not accurate enough so I want to expand the input image to include more context.\n\nI'm concerned that the architecture of the CNN will lose positional info, and that when I effectively 'unzoom' the training images, it'll be looking for the object concerned in a position-independent way, rather than just in the centre. I'm also not sure to what extent it will really use the non-local context appropriately.\n\nI will try this both ways, but I'm interested in a more theoretical interpretation of whether letting in more of the image will allow a simple CNN architecture to make use of context. Apologies if this is a relatively beginner Q, and any suggestions of better architectures would be appreciated as well. I've been thinking of something self-attention based, but I'm using keras and not sure how to go about that.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nsvo17/cnns_where_nonlocal_context_matters_but_position/"}, {"autor": "L4g4d0", "date": "2021-06-04 22:01:18", "content": "[help] Pytorch and Tensorflow broken after docker container crashed /!/ I was testing some code and then out of nowhere vs code and the docker container just crashed and I had to reinstall the docker extension to make it to work again but then when  I started the same container again Pytorch and Tensorflow were no longer working... How is that possible?   \n\n\nHow can a docker crash affect the python libs inside of it? I've been using this -----> image !!!  for a long time and had been doing a few commits with new important stuff into it and dk what else to do...   \n\n\nEDIT#1:\n\n I tried older images and those libs still broken in older images too... Is that a graphic card problem? Maybe the graphic card caused the crash after all but how can I fix this now? I've reinstalled the video card drivers but nothing changed. :(\n\nEDIT#2: \n\n I reinstalled pytorch and tensorflow and both detect cuda and the device I'm using for it but they are not working great... tf operations like tf[.session.run](https://tensorflow.session.run)() hangs forever...", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nsgph0/help_pytorch_and_tensorflow_broken_after_docker/"}, {"autor": "Independent-Square32", "date": "2021-06-04 21:49:15", "content": "High-resolution depth estimation from a single -----> image !!! ", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nsgg4r/highresolution_depth_estimation_from_a_single/"}, {"autor": "Ruffybeo", "date": "2021-06-04 15:59:56", "content": "DeblurGan Output - Strange Color /!/ Hi everyone! \n\nCurrently, I'm working on an -----> image !!!  deblurring problem of microscopic -----> image !!! s of different materials. For this use case, I did train a DeblurGan. After 5 epochs, I obtained the following result:\n\n&amp;#x200B;\n\nWhile my input image was a normal RGB image with different colors.\n\nNow I'm not quite sure, if this green color over the image is a result of a wrong normalization of my images or if there could be something wrong in the architecture of my model. Does anybody have an idea, what/where the problem could be? (I tried to normalize the images to \\[-1,1\\]. I'm doing the task with Pytorch)\n\nAny input would be appreciated :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ns8izi/deblurgan_output_strange_color/"}, {"autor": "LowLvlLiving", "date": "2021-06-04 13:58:49", "content": "Really need some help cutting through the noise /!/ Hello!  \n\n\nI'm trying to embark on an ML/DL project that involves -----> image !!!  generation/interpolation. I'm looking to build/trail a model that, given a start frame and an end frame would be able to spit out *n* number of in-between frames.  \n\n\nI know similar projects exist (and I've been looking into them) but I'm finding it *reallllyyy* hard to get off the ground with actually doing something and could use some help.  \n\n\nFor some background, I'm a software engineering by trade, I have no problems with the maths or statistics involves, but I can't find any real information on where to *actually* get started. How can I start writing some code for this thing?  \n\n\nI'm running into either \"here's an absolute ton of theory that will leave you feeling more lost than before you started\" or \"just download these libraries and run \\`model.train\\` and you're done!\" when looking up information online and it's very frustrating.  \n\n\nI've been looking into convolutional neural networks and generative adversarial networks but, again, don't know what to do with them. Could anyone point me in the right direction? Do I have the wrong way of thinking about all of this?  \n\n\nThank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ns5rn5/really_need_some_help_cutting_through_the_noise/"}, {"autor": "waazus", "date": "2021-06-04 12:10:57", "content": "[D] Could building Consciousness/AGI be similar to making fire? /!/ Disclaimer: The below is more a conceptual/philosophical attempt to understand the concept and potential development of AGI/AI Consciousness. It is based on a gut feeling more than anything else and just hope it will spark an interesting discussion. I have very limited knowledge about coding ML/AI, other than that I am the co-founder (business) of an AI/ML tech business with $3M USD in seed funding and &gt;$1M USD in ARR. I have no direct knowledge/experience in coding. \n\nThe other day I kept thinking about how humans lived before they invented how to make fire; they knew it existed but didn't know to make it, nor how to control it. It must have felt like magic (something they couldn't fully understand), understood that it has incredible benefits (warm and cook food), but also they must have feared it as when it isn't controlled properly it can destroy at great scale. If you think about it, artificial consciousness draws a lot of comparisons.\n\nTaking it one level further, could consciousness be similar to fire? Have we not understood yet, in an AI context, that fuel, oxygen, and heat in the right conditions together will lead to a chemical reaction (ignition) and therefore fire? Have we not yet understood how bringing together different ML models will spark artificial consciousness ignition? After reading how Facebook's research project with two chatbots (successfully) developed their own language to reach an agreement, the fast progress OpenAI is making with GPT-1, 2 to 3, we must be close to having all the elements together to 'make fire', right?\n\nThe way I see it is that we have figured out so many things over the last decades; our senses have successfully translated into hyper-accurate artificial sensors (eye/-----> camera !!! , ear - microphone, etc), machine learning models are beating us at specific cognitive tasks (chess, recognizing people in Google/Apple photos, etc), we MUST be getting close to having the elements ready. It FEELS like we just need to experiment with bringing the right 'elements' together. Different AI's that together will 'spark'.\n\nThe question I was asking myself is what do we believe the components/elements should be that we need to bring together? Should the models be deterministic or not? Following this rabbit hole, I found myself watching multiple videos about whether or not humans actually have free will. Again my gut feeling says we need to bring together different ML models (like the chatbots for Facebook) to spark ignition.\n\nI could imagine that someone (a 15y/o student) in a few years from now will be able to bring together the right elements by accident and in an unexpected, unprecedented way and 'ignite ai consciousness.\n\nWhat do the experts here on Reddit think?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ns3m47/d_could_building_consciousnessagi_be_similar_to/"}, {"autor": "FurryKoala", "date": "2021-06-11 20:58:32", "content": "GAN for non -----> image !!!  data /!/ Hi guys, I saw a paper that used a GAN to generate more data based on a dataset comprised of tabular data. Most of the code to implement a GAN I see on the internet is for generating images. Anybody knows where to find the code to implement a GAN with a dataset of tabular data?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nxq2f6/gan_for_non_image_data/"}, {"autor": "opensourcecolumbus", "date": "2021-06-11 14:42:31", "content": "Neural Search - The deep-learning approach to build \"intelligent search engine\" /!/ Who is this article for? Every Machine Learning engineer who needs to implement search in their app. \n\n&gt;TL;DR: Neural Search is a new approach to retrieving information using neural networks. Traditional techniques to search typically meant writing rules to \u201cunderstand\u201d the data being searched and return the best results. But with neural search, developers don\u2019t need to wrack their brains for these rules; The system learns the rules by itself and gets better as it goes along. Even machine learning beginners can quickly build a search engine using open-source frameworks such as [Jina](https://github.com/jina-ai/jina).\n\n## What is Neural Search?\n\nThere is a massive amount of data on the web; how can we effectively search through it for relevant information?\n\nAnd what do I mean by effective search\n\n* Can we go beyond just matching keywords?\n* Can we search using natural language, just like we would write or speak?\n* Can we make the search smart enough to forgive our minor mistakes?\n* Can we search for things that aren\u2019t an exact match but are \u201cclose enough\u201d?\n\nWe can answer all those questions with one word: Yes. To understand how, we need to enter the world of Natural Language Processing. NLP is a field of computer science that deals with analyzing natural language data, like the conversations people have every day. NLP is the foundation of intelligent search, and we have seen three different approaches in this field as follows.\n\n## Evolution of search methods\n\n[Evolution of search system design patterns](https://preview.redd.it/wmhkotbzen471.png?width=539&amp;format=png&amp;auto=webp&amp;s=88be9f57a02216b60b547175debcde9ded273b97)\n\n1. **Rules (1950\u20131990s)**  \nComplex handwritten rules that emulate Natural Language Understanding.**Drawbacks:** Handwritten rules can only be made more accurate by increasing their complexity, which is a much more difficult task that becomes unmanageable over time.\n2. **Statistics (1990s \u2014 2010s)**  \nProbabilistic decisions based on weights, machine learning and feature engineering.Creating and managing rules was solved with machine learning, where the system automatically learns rules by analysing large real-world texts.**Drawbacks:** These statistical methods require elaborate feature engineering.\n3. **Neural Networks (Present)**  \nAdvanced machine learning methods such as deep neural networks and representation learning.Since 2015, statistical methods have been largely abandoned, and there has been a shift to [neural networks](https://en.wikipedia.org/wiki/Neural_network) in machine learning. Popular techniques using this method make it a more accurate and a scalable alternative. It involves\n\n* Use of \\`word embeddings\\` to capture semantic properties of words\n* Focus on end-to-end learning of higher-level tasks (e.g., question answering)\n\n&amp;#x200B;\n\n&gt;When you use Neural Networks to make your search smarter, we call this a **Neural Search System**. And as you will see, it addresses some of the critical shortcomings of other methods.\n\n&amp;#x200B;\n\nNote that the applications of Neural Search are not just limited to text. It goes well beyond what NLP covers. With neural search, we get additional capabilities to search images, audio, video, etc. Let\u2019s look at a comparison of the extreme ends of search methods \u2014 \u201cRules\u201d vs \u201cNeural Networks\u201d:\n\n## Rules(Symbolic Search) vs Neural Networks (Neural Search)\n\nWhile the Neural Search method has become more widespread since 2015, and should be the primary focus area of any new search system. However, we shouldn\u2019t completely ignore Symbolic (rule-based) Search methods. In fact, **using a combination of Neural Search and Symbolic Search** may result in optimized results. Let\u2019s look at some of the powerful applications of Neural Search\n\n## Applications Of Neural Search\n\n**Semantic search**\n\n\ud83d\udd0d addidsa trosers (misspelled brand and category, still returns relevant results similar to query \u201cadidas trousers\u201d)\n\n**Search between data types**\n\nWith Neural Search, you can use one kind of data to search another kind of data, for example using text to search for images, or audio to search for video. e.g. search \"a kid playing\" in -----> image !!! s  \n\n\n[Example: cross-modal search](https://preview.redd.it/kp84kimben471.png?width=184&amp;format=png&amp;auto=webp&amp;s=cac0a12c4b2381ef1fa1fab8aa0cb11850fafe16)\n\n**Search with multiple data types**\n\nWith Neural Search, you can build queries with multiple query data types e.g. search -----> image !!! s with text+-----> image !!!  e.g. search \"**3/4th length of {this skirt - -----> image !!! }\"** in -----> image !!! s\n\n[Example : multi-modal search](https://preview.redd.it/dbc7lcfjen471.png?width=683&amp;format=png&amp;auto=webp&amp;s=f408d64840a5b8ad0dcd820d84a1a4280339da18)\n\n## Get started with Neural Search\n\nFor rule-based searches, **Apache Solr, Elasticsearch, and Lucene** are the de-facto solutions. On the other hand, Neural Search is relatively new domain, there aren\u2019t so many off-the-shelf packages. Also training the neural network for such a system requires a lot of data. These challenges can be solved using [Jina](http://github.com/jina-ai/jina/), an open-source neural search framework.\n\n&amp;#x200B;\n\n**References/Notes:**\n\nNeural Search term is less academic form of the term **Neural Information Retrieval** which first appeared during a [research workshop in 2016](https://www.microsoft.com/en-us/research/event/neuir2016/). I also found it useful to learn about [how google search  works](https://www.youtube.com/watch?v=0eKVizvYSUQ).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nxhlbl/neural_search_the_deeplearning_approach_to_build/"}, {"autor": "grid_world", "date": "2021-06-11 05:27:33", "content": "CNN - Apple Classification /!/  I  have the following problem statement in which I only need to predict  whether a given -----> image !!!  is an apple or not. For training only 8 images are  provided with the following details:\n\n1. apple\\_1 image - 2400x1889 PNG\n2. apple\\_2 image - 641x618 PNG\n3. apple\\_3 image - 1000x1001 PNG\n4. apple\\_4 image - 500x500 PNG\t\tcontains a sticker on top of fruit\n5. apple\\_5 image - 2400x1889 PNG\n6. apple\\_6 image - 1000x1000 PNG\n7. apple\\_7 image - 253x199 JPG\n8. apple\\_8 image - 253x199 JPG\n\nI  am thinking about using Transfer learning: either VGG or  ResNet-18/34/50. Maybe ResNet is an overkill for this problem statement?  How do I deal with such varying image sizes and of different file  extensions (PNG, JPG)?\n\nAny online code tutorial will be helpful.\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nx896l/cnn_apple_classification/"}, {"autor": "throwawayafw", "date": "2021-06-10 18:37:28", "content": "Complete beginner here. Is this (the -----> image !!!  below) an ensemble CNN?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nwuyq2/complete_beginner_here_is_this_the_image_below_an/"}, {"autor": "oriol_cosp", "date": "2021-06-10 10:31:20", "content": "Best practices for learning Machine Learning /!/ Hi all,\n\nI want to compile a list of best practices for learning data science. A list of things that you'd tell yourself if you could go back in time to when you were starting from scratch. What is relevant, what is not.\n\nI have started a list, but I want to discuss it with the community and curate it with your help.\n\n1. Before you start, get a solid math and prob/stats core. It's not so important to be able to proof a theorem, but you should know your way around matrices and vectors and intuitively undersand probabilities\n2. Learn either Python or R and their data handling packages (Pandas and Numpy for Python, data.table/dplyr for R) and learn them well. Most of your time will be spent working with these tools\n3. Try to learn code good practices. This could take a whole post, a quick summary would be: use descriptive names for variables, columns and functions; don't repeat code (use functions instead); understandable code is better than compact one (10 lines everybody understands &gt; 2 lines nobody understands); don't overoptimize your code at the start, but know where the bottlenecks are in case you need it to scale\n4. You don't need to learn all fancy models out there. For basic tasks linear regression is surprisingly effective. XGBoost is one of the most effective models for tabular data. Only learn/use neural nets for -----> image !!! , NLP or some other very specific domain\n5. Once you know the basics and understand them well, it's mostly about doing projects\n6. When learning a new topic, doing tutorial projects or understanding projects done by others is very helpful\n7. You can learn everything online for free. But paying may give you other things other than knowledge (credentials, a class of peers, in some courses better resouces)\n8. Explaining your work to others is a great way to consolidate your knowledge\n9. Nobody gets it right the first time. Trial and error is the way to go, especially on fields like this where there is no one exact solution\n10. The internet is full of helpful and generous people, if you're struggling with something search and if you don't find the answers, ask in the forums (reddit or stackoverflow)\n11. Don't focus only on the purely technical, try to understand what is really behind the problems you're modeling\n\nWhat do you think is missing from the list?\n\nWhat would you add?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nwkamt/best_practices_for_learning_machine_learning/"}, {"autor": "yngwie_102", "date": "2021-06-09 22:26:04", "content": "Tensorflow/Keras guidance - Pollution classification /!/ Hi,\n\nI am trying to develop my project for my Uni. I'd like to make a algorithm with Keras, which should tell me the value between 1-6 in scale of the worst to clean air based on my country PM10 limits.  The case:\n\n* I have data from beginning of 2019 up to today, containing my y dataset (Poziom aka \"level\") and 14 columns with data to be used by algorithm as X dataset\n* 2019 and 2020 as a train data and 2021 as a test data\n* Already have a working XGBClassifier and SVM models\n* head() of my data looks like this (Godzina=hour, Miesiac = month, etc.):  \n\n\n[trainData.head\\(\\)](https://preview.redd.it/o1guuvv3eb471.png?width=1392&amp;format=png&amp;auto=webp&amp;s=b9185564ce2bf2f2f4f2fe14f2cece3036b65af9)\n\nIn past I managed to do classification model to determine if -----> image !!!  is truck or normal car. Here I have to make a classification between 6 values. Can anybody show me the way I should take with Keras? I tried something like this:  \n\n\n    model = Sequential()\n    model.add(Dense(500, activation='relu', input_dim=14))\n    model.add(Dense(100, activation='relu'))\n    model.add(Dense(50, activation='relu'))\n    model.add(Dense(1, activation='softmax'))\n    \n    # Compile the model\n    model.compile(optimizer='adam', \n                  loss='categorical_crossentropy', \n                  metrics=['accuracy'])\n    \n    model.fit(X_train,y_train,epochs=20)\n\nBut like you can imagine - this doesn't work. I am not sure what to put in the input\\_dim and as well on last Dense to make it work. I am not searching for complete answer, looking for some guidance to do it by myself. I looked through many articles, but didn't find anything what could help me.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nw843a/tensorflowkeras_guidance_pollution_classification/"}, {"autor": "kalmL", "date": "2021-06-09 16:40:48", "content": "Suggestions/Advise for buying lost cost -----> camera !!!  to capture image from fast moving vehicle /!/ Hello everyone, I am currently working on a program which analyse surroundings while placed on a moving vehile (at max speed of 180 kmph). The video/images captured from the camera then will be run on the written ML program. The captured images should be of decent quality and there should not be any blurring due to speed of vehicle. \nPS. I am running little low on money. Pls suggest any suitable camera.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nw09ek/suggestionsadvise_for_buying_lost_cost_camera_to/"}, {"autor": "onetrickwolf", "date": "2021-06-09 02:42:33", "content": "New to machine learning, having trouble with some keywords to search if someone could help me out or tell me if this is possible...have -----> image !!!  board with -----> image !!! s and tags would like to use machine learning to suggest tag /!/ Hey I did some tutorials that showed me how to train a model to classify images. I have a custom data set of art from a roleplay community I am in and it did pretty well (e.g. can identify that something is a sword, or a person, but if it was a person holding a sword it sometimes was confused).\n\nOur site has thousands of images that are all tagged pretty well (dozens of tags per image). I would like to use the tags with images to train a model to suggest tags for new images (e.g. a picture of a person with a sword might suggest person AND sword as a tag).\n\nI am having a hard time finding tutorials on this thought and think I am just missing some keywords to search if anyone could help me? This seems like a pretty common use case or natural next step to my learning path. Most tutorials I find focus on either single tags or use pre-trained models but I want to train my own.\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nvlbns/new_to_machine_learning_having_trouble_with_some/"}, {"autor": "gungunthegun", "date": "2021-01-06 19:53:01", "content": "Map Generation Project Help /!/  **Project Description**\n\nThe idea for this project comes from the desire to create world maps for fantasy games or imaginary worlds as part of worldbuilding projects and the like. I've seen many world map generators where you are able to set certain parameters (% of water, amount of detail, etc.), but I haven't seen one where you might be able to specify a generic outline of your world and the rest is generated. While in most cases, where somebody might generate a map just to get started, this wouldn't really be necessary, if you had an idea for how the world might look and you just need more detail, this project could be perfect for you. In this project, I would like for the user to be able to draw a map of coastlines and the edges of continents or islands. The network would then fill in the rest of the area with mountains, valleys and similar land structures. The network would only be responsible for generating the heightmap and not any moisture or biome data.\n\n**Background**\n\nWhile I have taken a class on deep learning, it didn't really provide the necessary experience for implementing something like this. It mostly dealt with the theoretical aspects of machine learning, which were very dense, and there wasn't much hands-on learning. On the other hand, I am in my final year as a software engineering student, so I feel I have a decent idea of common practices outside of machine learning.\n\n**The Problem**\n\nOne of my main inspirations for this project was NVIDIA's GauGAN, and this project seemed like a good use case for their semantic -----> image !!!  synthesis and a GAN. My first question has to do with whether or not this type of technology would be too complicated for this project. Would it be possible to implement this easier with an autoencoder or some other kind of network or algorithm entirely?\n\nNext, I'd like to ask about the data representation. To keep it simple, I'm thinking of starting with a 500x500 grid of values. I'll generate the heightmaps using some type of noise function and use edge detection or something similar to extract the coastlines. In short, I'm attempting to generate my own dataset for this. I'm thinking that these would be the inputs and labels for the neural network. So far, I'm struggling finding the best way to represent this data. I've started by storing the heightmaps in a CSV file where each row is a flattened heightmap. This leads to a huge file size and I'm concerned about loading this file into memory for training. This brings me to my next question.\n\nI'm familiar with python being a language commonly used with TensorFlow for machine learning tasks, but I'm also aware that languages like R are good for data science and similar things. I'm not familiar with how to load in files like those that might be generated earlier into a TensorFlow program or similar. Would this be a use case where a database might be used? Is python even a good choice for this kind of project? TensorFlow?\n\nI realize that this is a lot of questions for a single post, but I really am new to this type of programming and designing. Any type of feedback would be greatly appreciated. Thank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/krvpyo/map_generation_project_help/"}, {"autor": "MrZipZap", "date": "2021-01-05 13:19:53", "content": "So how should I actually approach making a program for finger counting? /!/ The main problem is that I want it to work with whatever background, not just a white wall or the same background that was used for training. *For now*, I'm only working with -----> image !!! s where the hand is in the center of the -----> image !!! .\n\nI've tried two approaches:\n\n**1. Giving the model unprocessed training data** (meaning the images contain the hand in the center and then a random background), hoping that the model will itself learn to ignore the background and only focus on the hand.\n\nThis worked to some extent but not that well, I don't know whether it's impossible for the model to learn better or if I just need more training data with different backgrounds (So far I have 8400 images across 4 different types of backgrounds)\n\n**2. Using thresholding (with the opencv trackbar method)** to create a mask that finds the hand, and apply it to each image before training, as well as before inference.\n\nThe problem with this approach was that finding the settings for a mask that is capable of finding the hand in *different* lighting conditions, while simultaneously removing the other colors, seemed pretty much impossible.\n\nIs there another approach, or am I doing something wrong? Thanks in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kqybyf/so_how_should_i_actually_approach_making_a/"}, {"autor": "Another__one", "date": "2021-01-05 11:18:43", "content": "[P] -----> Image !!!  morphing without reference points by applying warp maps and optimizing over them.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kqwiv1/p_image_morphing_without_reference_points_by/"}, {"autor": "Mazormazor", "date": "2021-01-05 05:58:33", "content": "I'm trying to make an authentication method where you need to draw the first letter of your name. How big does my dataset need to be in order to distinguish my drawing from others? /!/ Hey! I'm new to machine learning and -----> image !!!  recognition, but I was tasked with making a proof of concept of the authentication method I described.\n\nI already have a code that asks the user to draw the letter and then encodes into the image the speed and acceleration of the stroke by changing the line's color and thickness.\n\nRight now all I need to do is create an AI that will answer the question \"is this drawing the letter M drawen by u/mazormazor?\".\n\nWhile I can send the code that encodes the drawing speed and acceleration into the image to many people in order to get some negative results, I doubt I'll be able to make more than a few hundred images for the dataset.\n\n Is it possible to create an AI that will work relatively well with that size of a dataset?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kqrzkk/im_trying_to_make_an_authentication_method_where/"}, {"autor": "ZealousRedLobster", "date": "2021-01-04 21:57:29", "content": "How to approach classifying images as -----> film !!!  or digital /!/ I have a side project I'm working on, which is trying to build a model that can determine whether an image was shot on film or on a digital camera. My experience with machine learning goes up to the complexity of Autoencoders, GLM's, Naive Bayes, etc. Additionally, the dataset I've created is 4,341 digital photos and 4,524 film photos so far.\n\nThe problem I'm running into here is that any work I've done with images is about classifying the image contents itself, rather than how the image contents \"look\". For example, a photo of the exact same dog with digital vs film will have a distinctive look, but the structure of the image subject itself is the same. I'm mainly stumped with how do I formalize this distinction and what material I should be reading about this. Additionally, the preprocessing side of things is a challenge for me as well, since the images are of varying sizes and my experience is limited to fixed-size images.\n\nAny help or guidance would be much appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kqj1fr/how_to_approach_classifying_images_as_film_or/"}, {"autor": "OnlyProggingForFun", "date": "2021-06-19 11:26:31", "content": "This new Facebook AI model can translate or edit every text in the -----> image !!!  in your own language, following the same style!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o3dwhw/this_new_facebook_ai_model_can_translate_or_edit/"}, {"autor": "Mjjjokes", "date": "2021-06-18 21:50:24", "content": "Preparing mel spectrograms (music visualization -----> image !!! ) for a CNN? /!/ What is the best way to do this? \n\nI load the music file in librosa, I visualize it as a mel spectrogram with librosa+matplotlib, but this spectrogram has a title, axes labels, a key, etc. \n\nI'm guessing to prepare it for a CNN I'd just have to load it without all these extras? If so, how would I do that?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o315mr/preparing_mel_spectrograms_music_visualization/"}, {"autor": "unknown_137", "date": "2021-06-18 15:41:17", "content": "Text extraction from -----> image !!!  using pytesseract english and NON ENGLISH WORD CODE LINK IN DESCRIPTION /!/ [code link](https://www.engineerknow.com/2021/06/how-to-extract-text-from-video-and.html) on blog there is video just in case if you don't understand something watch it\n\nhttps://reddit.com/link/o2s0ii/video/h0txsxfsn1671/player", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o2s0ii/text_extraction_from_image_using_pytesseract/"}, {"autor": "jonnor", "date": "2021-06-18 09:39:04", "content": "Making really efficient CNNs for audio classification, for deploying on embedded/mobile/microcontrollers /!/ This short talk (10 minutes) covers CNN model design techniques, that allows to optimize to run on really contrained systems. All the way down to microcontroller-based sensors. The focus is on audio models, but several of the techniques also apply to -----> image !!! /vision models and other (multi-variate) time series models.  \n[Environmental Noise Classification on Microcontrollers (tinyML Summit 2021)](https://www.youtube.com/watch?v=ks5kq1R0aws)   \n\n\nYou can also find some code and a written report in [https://github.com/jonnor/ESC-CNN-microcontroller](https://github.com/jonnor/ESC-CNN-microcontroller), which was my master thesis on the topic.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o2l93y/making_really_efficient_cnns_for_audio/"}, {"autor": "Ruffybeo", "date": "2021-06-25 18:45:42", "content": "Question about methodologie of my approach /!/ Hi everyone!\n\nTLDR: I performed some processing steps on my training set to increase the performance of my model and I'm not sure if I need to do the same steps on my validation set? I did not change anything in the test set because I want to see the real performance of the model on the original dataset. \n\nCurrently, I'm working on a deblurring problem that should be solved with ML. At the moment, I'm in the training phase of the model and did split my dataset into three parts: training, validation and test set.  Now I'm rather confused about the following part of my approach:\n\nTo let the model learn how to deblur images, it's fed two different kinds of images: blurry images (with a non-uniform motion blur) and sharp images. Unfortunately, in some images, the blur is really weak, which results in poor training performance. To increase my performance, I did split every -----> image !!!  (of both kinds) in the training set into four parts (one vertical and one horizontal cut) and compared the SSIM from each part of the blurry and the shape -----> image !!! . If the SSIM is over 0.9, I removed the part. I didn't do the same with the test set because I know that this is used to see the real performance of the model on unseen data.  \n\nBut now I'm unsure about the validation set. In my opinion, I should not do the same that I did with the training set on my validation set. My reasoning behind that is that I did change my dataset with my 'splitting in parts' approach. I want to use the validation to observe how well my model did learn from the images compared to the original images.\n\nBut I'm not sure if my reasoning is right or if I miss something and need to split my validation set also in four parts and compare their SSIM with each other as well? So that my training and validation set are somewhat 'comparable'?\n\nI would appreciate any input :).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o7tujk/question_about_methodologie_of_my_approach/"}, {"autor": "thisisdhruvagarwal", "date": "2021-06-25 13:28:03", "content": "Why is my deepfake isn't working? [D] /!/ I have coded my own deepfake model but the problem that is happening is that the transformation of -----> image !!!  A to -----> image !!!  A and transformation of -----> image !!!  B to -----> image !!!  B is happening very correctly, but when producing deepfake the -----> image !!! s are not in sync, that is, the position of both the faces does not matches correctly. I hope you understood my question. Why is this happening?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o7np6h/why_is_my_deepfake_isnt_working_d/"}, {"autor": "lalopark", "date": "2021-06-25 08:44:53", "content": "Computer Vision project: measuring the length of a human ear /!/ Hi, I\u2019m trying to come up with a model that takes in a -----> photo !!!  of a human ear and measures its vertical length. I\u2019ve found a few usable data sets of ear images, but it seems like the key is to label them somehow\u2026 Any pointers on where to begin and how to structure the solution? I\u2019m very new to computer vision so would appreciate any and all pointers xx Thank you", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o7jqiq/computer_vision_project_measuring_the_length_of_a/"}, {"autor": "WillowTreeman1", "date": "2021-06-24 20:54:53", "content": "Question about CNNs: convolutions though feature maps? /!/ I'm interested in CNNs for RGB(+) images. \n\nAs I understand CNN filters, they are applied to each channel and the resulting feature maps are maxpooled or whatever you decide depending on the net architecture. My question is if anyone has done convolutions in the direction of the feature maps or through the original -----> image !!!  channels. I think this would be a way to detect co-localized image colors in the case of the original image, or to detect co-localized feature map features. \n\n&amp;#x200B;\n\nThanks, \n\na ML noob", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o79t6p/question_about_cnns_convolutions_though_feature/"}, {"autor": "letsaurify", "date": "2021-06-24 05:55:36", "content": "Where to start for multiple object detection in static images? /!/ Hello friends,\n\nI'd like to build a CNN to recognize hand configurations in mahjong games and map them to text. Given an -----> image !!!  -- for example, this hand -- as **input**: \n\n[Hand is: 145589m7p2345s77z1m](https://preview.redd.it/1p0zsdxkh5771.png?width=1214&amp;format=png&amp;auto=webp&amp;s=89fb643e3d8e44346f45f06cccd80e7affcee9f2)\n\nThe **output** would be a sequence of words that simply report the classes of each of the mahjong tiles. This idea can be applied to other card-based games as well, such as poker or gin rummy.\n\nAs I understand, the R-CNN model architecture can be used to do this task as well as the YOLO family of models. However, I'm not sure where to go on from here. I know I will need to come up with training data for each of the various classes and from there I could probably try to replicate the architecture from one of these models.\n\nBut I still feel somewhat lost and confused. Am I going in the right direction with this? If anyone can offer some advice that'd be appreciated. This is my first serious DL project outside of university.\n\n**Note:** Mind you this is not for the purpose of cheating. Current tools that analyze certain tile efficiency (which tile to drop that will further one's hand) are currently very tedious to use and require manually typing in what tiles one has in their hand. This would be developed with the intention of helping players analyze game logs faster and figure out what would have been optimal moves in different situations.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o6ursx/where_to_start_for_multiple_object_detection_in/"}, {"autor": "aurora-s", "date": "2021-06-24 04:49:43", "content": "Predicting a ranking-based-confidence in a CNN -----> image !!!  classifier /!/ I need to train a CNN classifier on image input, where the correct classification is one of \\~15 outputs. Normally, this would be straightforward - you'd supervise them with a score of 1 for the correct class and 0 for all the other classes. However, if the network does get the answer wrong, it would be useful to me to know what its next best guesses are (as I'll be using a simulation-ish thing based on the output classification - the NN is a guide to which 'moves' to make).\n\nIn my experience, if you train a classifier on binary labels, the output is very sensitive, and there's no way to use the outputs to reliably rank the next best options. I've briefly looked on MLR, but it mostly applies to query/data rankings which are structured as a different problem. I've also considered artificially expressing the supervision as fractions rather than binary, so the most likely label gets say 0.9, and the others get decreasing scores. But ideally the network would learn the weightings on its own, because the confidence itself isn't something I can objectively label. Then there's also perhaps networks that predict probability distributions, which I could use to make confidence limits or something. But I haven't found anything that's easy to apply to this situation.\n\nI'd really appreciate any help or pointers. Hopefully something I can implement easily on Keras etc.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o6tufu/predicting_a_rankingbasedconfidence_in_a_cnn/"}, {"autor": "8REVGage", "date": "2021-06-24 02:46:49", "content": "Looking for other relative beginners to tackle ML projects on Kaggle with. /!/ I think this falls under rule #7, but if I'm mistaken please let me know.   \n\n\nMy name is Gage, and I've been working as a ML engineer for about a year and a half now, and I'm looking to get more experience with a variety of project types. Most of my work has been on -----> image !!!  processing using Tensorflow (low light imaging, colorization), but I'm happy to work with and learn from people of any background.   \n\n\nI'm looking for something fairly casual where we can discuss potential projects and solutions in more of a chat setting than a subreddit, and maybe tackle some Kaggle datasets together.   \n\n\nI'm happy to help anyone with an interest in similar projects to what I've done as well.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o6rybb/looking_for_other_relative_beginners_to_tackle_ml/"}, {"autor": "Forward_Squash253", "date": "2021-06-23 15:24:03", "content": "Help me create a list of must Read papers on Deep Learning. /!/ I have just recently started learning about Deep Learning, and I'm really excited about the history of deep learning, I Googled it and I did find a bunch of posts describing the history from Warren McCulloch to Yann LeCun.\nI intend to create a definitive guide/storage that would paint a good -----> picture !!!  of the field, so folks I would really appreciate if you could suggest must read papers on Deep learning (Would be great if you could provide links to the papers) Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o6f1aa/help_me_create_a_list_of_must_read_papers_on_deep/"}, {"autor": "sidneyy9", "date": "2021-08-20 09:38:50", "content": "imwrite 2 features on -----> image !!!  /!/ Hi everyone ,\n\nmy question can be quit easy for someone. I have two features , one of them is bounding boxes the other one is keypoints. I would like to write both of them on image, how can i do that ? \n\nThis is not right thing to do :( -&gt;          `out.write(result)  out.write(img)` \n\nThanks for your advices .", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p81uw3/imwrite_2_features_on_image/"}, {"autor": "Successful_Boat_3099", "date": "2021-08-20 08:02:03", "content": "\ud83d\udc8aYour daily dose of machine learning : instance segmentation /!/ This is a series of posts that I post almost daily. I call them \u201cyour daily dose of machine learning\u201d.\n\nInstance segmentation is when you segment an object in an -----> image !!!  and you also make the distinction between objects that belong to the same class.\n\nOne of the widely used instance segmentation deep learning models is Mask RCNN.\n\nMask RCNN builds upon Faster RCNN, which is an object detection model.\n\nBtw, this model is part of Tensorflow 2 Object Detection API.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p7xq20/your_daily_dose_of_machine_learning_instance/"}, {"autor": "TrepidationTD", "date": "2021-08-19 23:58:45", "content": "What is the best way to learn Computer Vision for Machine Learning comprehensively? /!/ I'm doing a project on Computer Vision but want to go further and HAVE to find a way to make -----> image !!!  recognition faster because what I'm doing right now requires quick refresh rates and a really fast algorithm. I want to learn about Computer Vision at a very deep level to the point where I can explain how computer vision really works with math. I want to improve parts of Computer Vision specifically for my project. What are some good resources that really teach behind the scenes of computer vision. I don't really want a resource that tells me how to code for Computer Vision but something that would allow me to manipulate Computer Vision algorithm somewhat comfortably.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p7qshf/what_is_the_best_way_to_learn_computer_vision_for/"}, {"autor": "Successful_Boat_3099", "date": "2021-08-19 05:48:17", "content": "\ud83d\udc8aYour daily dose of machine learning : encoder-decoder -----> image !!!  segmentation /!/ &gt; This is a series of posts that I post almost daily. I call them \u201cyour daily dose of machine learning\u201d.\n\nIf there is one thing similar between so many image segmentation deep learning models, it\u2019s the encoder-decoder paradigm.\n\nThis paradigm basically goes like this:\n\n1. You start with the image as an input.\n2. You pass the image through convolutional layers of a backbone (InceptionV2, VGG, \u2026)\n3. You pass the output of the previous step to a decoder part. This part stacks a bunch of upsampling and convolutional layers.\n4. The final layer has a softmax activation function that classifies each pixel into one of N categories.\n\nTwo examples of this kind of architectures are SegNet and U-Net.\n\n Follow me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!***", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p78wq5/your_daily_dose_of_machine_learning/"}, {"autor": "Snoopy_snoopy_snoopy", "date": "2021-08-18 15:59:11", "content": "How to view layers of a neural network? /!/ I've just started learning neural networks and I wanted to see what different features does each particular layer/node extract from an -----> image !!! , I've an -----> image !!!  which is of the size 400x400 and for now ive added a hidden layer with 5 nodes and activation function relu , one output layer with one node and activation as sigmoid , is there any way with which i can print the features each node extract?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p6u2yk/how_to_view_layers_of_a_neural_network/"}, {"autor": "Successful_Boat_3099", "date": "2021-08-18 08:24:46", "content": "\ud83d\udc8aYour daily dose of machine learning : ParseNet /!/ &gt; This is a series of posts that I post almost daily. I call them \u201cyour daily dose of machine learning\u201d. \n\nYesterday I posted about FCNs and how they can be used for semantic segmentation.\n\nOne drawback of FCN is that it ignores potentially useful scene-level semantic context. \n\nIn layman terms, sometimes FCN misses a part of the object because it doesn\u2019t \u201cunderstand\u201d that it belongs to the same object. \n\nMany approaches have been proposed to tackle this drawback. One quick trick is to use global average pooling to get a better understanding of the global context in an -----> image !!! .\n\nParseNet did exactly this by introducing a contextual module, like the image below.\n\nBtw, global average pooling can be easily done using Tensorflow, using a layer called GlobalAveragePooling2D.\n\n&amp;#x200B;\n\n Follow me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!*** \n\n&amp;#x200B;\n\nhttps://preview.redd.it/mt6sl5krt2i71.png?width=1711&amp;format=png&amp;auto=webp&amp;s=a6b9100927801fa436702980092bf7b7d78de689", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p6n0o4/your_daily_dose_of_machine_learning_parsenet/"}, {"autor": "goal_it", "date": "2021-08-18 07:52:48", "content": "Is Computer Vision not for me? /!/ BACKGROUND: I'm a Software Engineer with 3+ years of experience in Python, at my first job, I was given options to choose from different techs and I opted for Python as I've been coding in Python during my college days. After that, I was assigned a CV specific project in which I did the major contribution from POC to end-to-end productionized the product. During this process, I have learnt to utilise the open source solutions for CV projects. For the other parts, I can code in Python to build automation scripts, build and integrating REST APIs, use pandas for data wrangling etc.\n\n\nI can research for solutions specific to CV problems using latest research papers, GitHub repos and tutorials or ask for help on Reddit and Facebook groups etc.\n\n\nI can search, tweak and integrate state of the art models for specific problem or utilising SOTA for transfer learning on custom dataset specific to a custom problem as far as resources for some previous similar problem exists.\n\n\nI can do CUDA installation and integration, docker -----> image !!!  building etc.\n\n\nBasically, I can say that I know what, where and how to search for the solutions specific to computer vision.\n\n\nBut, I lack in developing my own state of the algorithm specific to a problem as I think it requires more of academic knowledge about the CV for which I educationally lag with a bachelor degree in CSE!\n\n\nBut when I check for CV specific jobs in big companies, they all require masters and PhDs.\u00a0\n\n\nSince I'm willing to make a switch, so I wonder,\u00a0\n\n\nShould I be more focused towards computer vision or move to backend developer role?\u00a0\n\n\nAre these even skills that companies look for in computer vision specific roles?\n\n\nAm I into applied computer vision?\u00a0\n\n\nThere might be the case that I was not enough challenged in my previous jobs so I started to thinking myself as someone who can come out with a solution!\u00a0\n\n\n\nI'd really appreciate your thoughts in this.\n\n\nThanks for your precious time.\n\n\n\nPS: sorry for so many \"I\"s in the post.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p6mnfm/is_computer_vision_not_for_me/"}, {"autor": "rezkirian", "date": "2021-08-18 04:51:20", "content": "Seeking a mentor / teacher to guide on CycleGAN Project. (This is not a free request. Will be happy to discuss remuneration) /!/ Good day everyone,\n\nHope this post reaches everyone well and safe. Allow me to introduce myself, my name is Hakim, currently a student from a South East Asia country. I am at the moment searching for guidance/ mentorship to assist me in my project \"Document cleaning with CycleGAN\". \n\nThe objective of this project is to utilize CycleGAN capability to train on unpaired -----> image !!! s and to convert dirty -----> image !!! s to clean -----> image !!! s base on reference -----> image !!! . I have with me the datasets required and code that I have searched online.\n\nI am seeking for somebody that is able to guide, explain and tutor me how to effectively modify the code. This is not a free request. Will be happy to discuss remuneration.\n\nThank you for everyone's time.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p6ke1l/seeking_a_mentor_teacher_to_guide_on_cyclegan/"}, {"autor": "KAKA7861111", "date": "2021-08-26 08:12:06", "content": "Vision Transformer vs CNN based architectures vs MLP-Mixer /!/ I am unable to get intuition that convolution exploits patterns by taking a look at all the neighboring pixels. The vision transformers take flatten array of  -----> image !!!  portions but they are outperforming the CNN same with the case MLP-MIXER. I don't know how you can achieve shift invariance and other factors without convolution. And what is the exact performance increment observed in Vit over CNN on different datasets", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pbv3ds/vision_transformer_vs_cnn_based_architectures_vs/"}, {"autor": "throwawayafw", "date": "2021-08-26 07:36:27", "content": "Noob question. When I load a grayscale -----> image !!!  using cv2.imread, the -----> image !!!  shape is given as (227,227,3) , why is that happening? /!/ I then gave cv2.imread(imgpath, 0) gave me a colorful image with image shape as (227,227) . \n\nBut when I changed it to cv2.imread(imgpath,1) gave me the original grayscale image with image shape(227,227,3)\n\nAm I doing this wrong?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pbuof8/noob_question_when_i_load_a_grayscale_image_using/"}, {"autor": "OmnipresentCPU", "date": "2021-08-25 21:36:53", "content": "Training Spacy NER /!/ I am trying to use Spacy\u2019s NER tagging to identify stock tickers in Reddit text. It works well for the most part, the ORGS entity identifies things like GME and Amzn. It misses some simple things though, like TSLA, so I want to train it a bit more.\n\nIs this as simple as just downloading comments that mention individual stocks, labeling it, and then calling model.fit() or something similar? Or is there a bigger -----> picture !!!  here I\u2019m missing. Not very familiar with NLP.\n\nI am thinking of creating a function that pulls 150 comments that mention each stock in the s&amp;p 500 for training and then another function to label them, and then training it. Is this a correct assessment?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pbl1rx/training_spacy_ner/"}, {"autor": "CountFrolic", "date": "2021-08-24 00:22:37", "content": "Day to night -----> image !!!  translation tools? /!/ I have a 360 drone pano of a city that was taken by day and would like to turn it into a night time image. Does anyone know of any tools (standalone, or website) that can do this? If the result is satisfactory, a paid program would be fine.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pacdr2/day_to_night_image_translation_tools/"}, {"autor": "Flygap75", "date": "2021-08-23 21:13:40", "content": "Count squat with posenet /!/ Hi all,\nI want to count my squat with posenet and tensorflow.\nI\u2019m hesitating between two ideas:\n- teach the model with -----> picture !!!  of what is a squat.\n- get the points and manually check with code if my knees have the proper angle for a squat.\nHow should I proceed and why?\nThanks.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pa8zu7/count_squat_with_posenet/"}, {"autor": "Xie_Baoshi", "date": "2021-08-23 11:57:41", "content": "Question on BigGAN model training /!/ I have goal to train in Colab Notebook an -----> image !!!  synthesis model, similar to TADNE(Danbooru2019) but for a specific character with a dataset containing his -----> image !!! s in different angles, poses, both full body and not full. A Stylegan2 model takes usually 12 hours of training (from a pretrained model), but for this task BigGAN seems to be better suited than Stylegan2.\n\nIs it worth to use BigGAN for a dataset with \\~5000 images? (of any suited size, even 128\u00d7128 will be OK)\n\nAlso, how many hours would take training on Colab GPU?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p9yaym/question_on_biggan_model_training/"}, {"autor": "YBTWorld", "date": "2021-07-07 19:55:00", "content": "Imagine recognition with convolutional neural networks /!/ I am currently working on -----> image !!!  recognition with convolutional neural networks, please can anyone suggest dataset with traffic signs?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ofqpcg/imagine_recognition_with_convolutional_neural/"}, {"autor": "bapichulo", "date": "2021-07-07 18:19:18", "content": "Increasing usb cam FPS with Yolov5 on a Jetson Xavier NX /!/ I posted this in r/computervision, but I think this sub may also help with this issue.\n\nI (well, my team) has successfully installed Yolov5 on our NVIDIA Jetson Xavier and after training our own custom model, we were able to detect and label objects appropriately. However, all of this is happening at an extremely low FPS. Even when using the model that comes with yolov5, its still really slow. I did not use the docker method (tbh, I don't even know what the docker method is) and the -----> camera !!!   itself can go up to 60 FPS. Roboflow's website says they've gotten up to 30 FPS, but now I am wondering if it was for a video instead of a  live feed. I am really hoping its for a live feed to make this possible.\n\nI've tried going into certain files and changing things. My best thought was to go into the [detect.py](https://detect.py/)  file and change the FPS value that was there. (I am not that experienced with python). That only ended up changing the value that was  printed in the command window and not the actual FPS.\n\nI have also realized the GPU on the Jetson Xavier is not being utilized 100%. However, my CPU is being used up and I am wondering if that has  anything to do with it.\n\nDoes anyone have any insight and/or questions to help me solve this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ofoqmi/increasing_usb_cam_fps_with_yolov5_on_a_jetson/"}, {"autor": "Delicious-Spare-5796", "date": "2021-07-07 06:57:22", "content": "Need Ideas for Project for the Final Year of Undergrad /!/ I am about to enter my final year of undergrad and I have to pick a project that I have to work on for a almost a year. I want to pick up something interested related to AI and Machine Learning. One of my professors told me about working on Semantics AI, but I want to have some more ideas. Moreover, I know what Semantics AI is but I am not sure what project I can work on it.\n\nBasically I am just looking for ideas, I want to have a clear -----> picture !!!  in front of me before deciding what kind of project I want to work on before putting together a strong proposal.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ofdpks/need_ideas_for_project_for_the_final_year_of/"}, {"autor": "b6ack", "date": "2021-07-06 17:58:06", "content": "Structure of a real world project /!/ How do you approach the structure for the project. Are you using a database to store the feedback from the user, to later use that data to retrain your model. With the AI models are you using different frameworks together (example: tensorflow for -----> image !!! , openai for audio/text, etc..). Sorry if this has been answered already. Kind of new at things so just looking for information. Thanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/of0bd9/structure_of_a_real_world_project/"}, {"autor": "cyanaspect", "date": "2021-07-06 10:44:44", "content": "Neural network works for L=1, gets worse the more layers I added to it. /!/ I'm trying to implement my neural network from scratch for -----> image !!!  classification (cats or not). I've been trying to debug it for days now, and I'm at my wits end. \n\nThe model runs fine, and all the dimensions of the vectors and matrices have been checked. \n\nWhen I set L = 1 (basically logistic regression), I get a nice elbow-shaped learning curve that converges to 88% accuracy. The learning curve and accuracy gets worse and worse as I add more layers. \n\nWhen theres more than 4 layers, something pretty strange happens. All the activations of the (L-1)th layer become the same, and the model predicts 0 for all train and test cases. \n\n&amp;#x200B;\n\nHere's the implementation, starting with some math functions and the loss. \n\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n    \n    def relu(z):\n        return np.maximum(0, z) \n    \n    def dz_sigmoid(A_l, A_l_plus_1):\n        return A_l - A_l_plus_1\n    \n    def dz_relu(W, dZ, Z):\n        return np.dot(W.T, dZ) * (Z &gt;= 0)\n    \n    def dz_tanh(W, dZ, Z):\n        return np.dot(W.T, dZ) * (1 - np.square(np.tanh(Z)))\n    \n    def logloss(A, Y):\n        m = Y.shape[1]\n        return (-1/m) * np.sum(Y*np.log(A) + (1 - Y)*np.log(1 - A))\n    \n\nThe functions of the model:\n\n    def initialize_parameters(layer_dims):\n        L = len(layer_dims) - 1\n        assert layer_dims[0] &gt; 1000\n        assert layer_dims[-1] == 1\n        \n        W = {}\n        b = {}\n        \n        for l in range(1, L+1):\n            W[l] = 0.01 * np.random.randn(layer_dims[l], layer_dims[l-1]) \n            b[l] = np.zeros((layer_dims[l], 1))\n        \n        return W, b\n    \n    \n    def forward_propagation(X, W, b, layer_dims, activation=\"relu\"):\n        L = len(layer_dims) - 1\n        \n        Z = {}\n        A = {}\n        A[0] = X\n        \n        for l in range(1, L):\n            Z[l] = np.dot(W[l], A[l-1]) + b[l]\n            if activation == \"relu\":\n                A[l] = relu(Z[l])\n            elif activation == \"tanh\":\n                A[l] = np.tanh(Z[l])\n            elif activation == \"sigmoid\":\n                A[l] = sigmoid(Z[l])\n            else:\n                raise ValueError(\"Invalid activation function\")\n                \n        Z[L] = np.dot(W[L], A[L-1]) + b[L]\n        A[L] = sigmoid(Z[L])\n        \n        return A, Z\n        \n        \n    def back_propagation(A, Y, Z, W, b, layer_dims, activation=\"relu\"):\n        dZ = {}\n        dW = {}\n        db = {}\n        dA = {}\n        L = len(layer_dims) - 1\n        m = Y.shape[1]\n        \n        dZ[L] = dz_sigmoid(A[L], Y)\n        dW[L] = (1/m) * np.dot(dZ[L], A[L-1].T)\n        db[L] = (1/m) * np.sum(dZ[L], axis=1, keepdims=True)\n        \n        for l in range(L-1, 0, -1):\n            if activation == \"relu\":\n                dZ[l] = dz_relu(W[l+1], dZ[l+1], Z[l])\n            elif activation == \"tanh\":\n                dZ[l] = dz_tanh(W[l+1], dZ[l+1], Z[l])\n            elif activation == \"sigmoid\":\n                dZ[l] = dz_sigmoid(A[l], Y)\n            else:\n                raise ValueError(\"Invalid activation function\")\n            \n            dW[l] = (1/m) * np.dot(dZ[l], A[l-1].T)\n            db[l] = (1/m) * np.sum(dZ[l], axis=1, keepdims=True)\n            \n            assert dZ[l].shape == (layer_dims[l], m)\n            assert dW[l].shape == (layer_dims[l], layer_dims[l-1])\n            assert db[l].shape == (layer_dims[l], 1)\n        \n        return dW, db\n    \n        \n    def update_parameters(W, b, dW, db, learning_rate, L):\n        for l in range(1, L+1):\n            W[l] = W[l] - learning_rate * dW[l]\n            b[l] = b[l] - learning_rate * db[l]\n    \n        return W, b\n    \n    def train_neural_network(X, Y, layer_dims, activation=\"relu\", learning_rate=0.01, num_iter=3000):\n        W, b = initialize_parameters(layer_dims)\n        L = len(layer_dims) - 1\n        \n        costs = []\n        \n        for _ in range(num_iter):\n            A, Z = forward_propagation(X, W, b, layer_dims, activation)\n            dW, db = back_propagation(A, Y, Z, W, b, layer_dims, activation)\n            W, b = update_parameters(W, b, dW, db, learning_rate, L)\n            costs.append(logloss(A[L], Y))\n            \n        return W, b, costs\n    \n    \n    def predict(X, W, b, layer_dims, activation, threshold):\n        L = len(layer_dims) - 1\n        \n        A, Z = forward_propagation(X, W, b, layer_dims, activation) \n        \n        return A[L] &gt; threshold", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oesfvc/neural_network_works_for_l1_gets_worse_the_more/"}, {"autor": "metaphysical_fries", "date": "2021-07-06 04:50:14", "content": "How do I build practical ML skills after doing the Andrew Ng course? /!/ Hi there! Sorry if this has been posted before, any kind of advice would be appreciated. \n\nI'm a second year CS student and I really want to work on -----> image !!!  transliteration and translation (using deep learning). I've done Andrew Ng's course and feel like I have a good foundation in the theory, but having done the programming assignments in Octave, I feel like I don't know how to do those problems in python, or how to work on any real world ML problems using python.   \n\n\nIs there any good resource I can use to build my ML skills in python (after having done Andrew Ng's course) and then moving on to the deep learning specialization?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oenzu1/how_do_i_build_practical_ml_skills_after_doing/"}, {"autor": "carloveron", "date": "2021-07-05 20:19:43", "content": "Suggestions for pre-trained networks for feature extraction /!/ Hi,\nI am trying to build an -----> image !!!  classifier with more than 10 different classes, I think that extract features using a pre-trained network could be useful. \nBut the question is..which one? I don't know where to start \ud83d\ude05\nThank you", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oefgpm/suggestions_for_pretrained_networks_for_feature/"}, {"autor": "b6ack", "date": "2021-07-05 19:24:28", "content": "What are some of the best open source AI frameworks/libraries /!/ I was wondering what are some of the best libraries or frameworks for  computer vision, -----> image !!!  manipulation, and  natural language processing.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oeecve/what_are_some_of_the_best_open_source_ai/"}, {"autor": "DependentLess5819", "date": "2021-07-05 00:26:03", "content": "Help w/ MNIST Dataset and Perceptron /!/ I was trying to do this last night and just don't know wtf to do. I  found this repo that converts the IDX file format to a numpy array and  saves the resultant -----> image !!! ;  \nhttps://github.com/sadimanna/idx2numpy\\_array (I had to change 'imsave' to -----> image !!! io.imwrite, and change some directories to fit my path)   \n\n\nSo  I've ran all that, and now have this directory full of these images.  (When it was running, it'd say. \\`Lossy conversion from int64 to uint8.  Range \\[0, 255\\]. Convert image to uint8 prior to saving to suppress this  warning.\\` I assume this is doing the 'divide by 255\\` for me, in the  preprocessing step, automatically?)  \n\n\nAnyway, so I have this data, and I have a Perceptron, here:  \nhttps://dpaste.com/2YPYRHNVG, also (basically the same code): https://dpaste.com/BC2DCGQLM  \n\n\nSo  I should have a numpy array, training and test data, and a Perceptron,  so I should be good to go. Thanks for the help. Just kidding, I really  don't know how to proceed from here. I believe I've set the weights and  bias node properly (see updated dpaste below). Basically, I don't know  how to interact with my Perceptron to proceed here, I don't know how to  identify or load the targets, the data or how to run it.   \n\n\nCurrent Perceptron: http://dpaste.com/7KTT4B5L4  \n\n\nPreviously I was doing stuff like what can be seen here, very basic and just copied from my book: http://dpaste.com/AXU8LYGHR  \n\n\nWould really appreciate someone holding my hand through some of this until I get rolling", "link": "https://www.reddit.com/r/learnmachinelearning/comments/odwc3c/help_w_mnist_dataset_and_perceptron/"}, {"autor": "mrtac96", "date": "2021-07-13 03:03:34", "content": "Segmentation of 3D volumetric images with different z dimensions /!/  \n\nGreetings. For CNN we need all images having the same size but I am trying to figure out what to do if x,y dimensions are same in all images but z dimension is different.\n\nLet i have 5 MRI images and their labels having the shape as follow\n\n(512,512,220), (512,512,333), (512,512,120), (512,512,247), (512,512,33)\n\nOne way is to crop or pad all of them (-----> image !!!  and labels) to equal size and compute metrics upon them.\n\nBut for some reason, if I want to get the original shape back what should I do?\n\nLet say I cropped them to (224,224,224), but when I try to get original mask, I think the mask with z dimension less than 224, i can simply remove the extra one, because they are just padded. But what to do with the cropped one from 333 to 224, maybe there is some object in 246 and i cropped it.\n\nAny idea for it", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oj6u8r/segmentation_of_3d_volumetric_images_with/"}, {"autor": "ManufacturerPitiful", "date": "2021-07-12 21:55:19", "content": "Question about importance of annotating every object instance /!/ Hi, beginner to ML here, but I am looking through a public data set of -----> image !!! s, and I notice that the annotations sometimes miss one or two object instances in an -----> image !!! . Will this impact the strength of my model? Is it very important to have a near 100% labeling rate?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oj1fu5/question_about_importance_of_annotating_every/"}, {"autor": "Datky", "date": "2021-07-12 19:19:12", "content": "What is the purpose of this code ? /!/ Hi,\n\nI'm working on a CNN that can count  the number of cells in an -----> image !!! . I used the code from a research paper, available here : [https://github.com/WeidiXie/cell\\_counting\\_v2](https://github.com/WeidiXie/cell_counting_v2)\n\nI trained a model with what looks like encouraging results, but there's a portion of code I do not understand the purpose of.\n\nIn train.py, after training, the code makes prediction  on 150 images and calculates the mean error on these images, with the help of ground truth. All these images are beforehand loaded into an array \"val\\_data\" with the folowing code, starting on line 59 :\n\n    data, anno = read_data(base_path)\n    anno = np.expand_dims(anno, axis = -1)\n        \n    mean = np.mean(data)\n    std = np.std(data)\n        \n    data_ = (data - mean) / std\n        \n    train_data = data_[:150]\n    train_anno = anno[:150]\n    \n    val_data = data_[150:]\n    val_anno = anno[150:]\n\nInstead of directly fetching into the \"data\" array, which contains all the images data, he uses a copy of that array, \"data\\_\" to which he applies some math formula with a standard deviation. that's the part I do not understand, what is it doing , why is it necessary to predict with the model ?\n\n&amp;#x200B;\n\nIf I want to make the model predict my own, different images that are not from this dataset, do I sstill need to use this part of the code ? I'm a bit lost on what's happening here.\n\n&amp;#x200B;\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oiyagy/what_is_the_purpose_of_this_code/"}, {"autor": "FlamingGem", "date": "2021-07-12 17:58:08", "content": "I created a Computer Vision-Based Rabbit Deterrence System /!/  So, I noticed there were a bunch of bunnies in my backyard and so I decided  to create a Rabbit deterrence system using computer vision. Essentially how it works is that there is a Raspberry Pi with a  -----> camera !!!  that monitors my backyard. Using an object detection model, I am able to detect when there is a rabbit next to my garden and when there is, I play a sound (such as a baby crying or a car passing by) via a Bluetooth speaker which will then spook off the rabbit. I ended up  documenting the whole process from start to end and, well, this is what I ended up with:\n\nVideo Documenting the Process: [https://youtu.be/oPvqKgq3ppc](https://youtu.be/oPvqKgq3ppc)\n\nBlog (goes into more technical details): [https://blog.roboflow.com/rabbit-deterrence-system/](https://blog.roboflow.com/rabbit-deterrence-system/)\n\nSource Code: [https://github.com/roboflow-ai/rabbit-deterrence](https://github.com/roboflow-ai/rabbit-deterrence)\n\nDataset: [https://public.roboflow.com/object-detection/cottontail-rabbits-video-dataset](https://public.roboflow.com/object-detection/cottontail-rabbits-video-dataset)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oiwmx2/i_created_a_computer_visionbased_rabbit/"}, {"autor": "brontosauruspepee", "date": "2021-07-12 14:20:35", "content": "Predicting multi-output regression using Tensorflow /!/ \n\nI Have an -----> image !!!  of a food [Like so](https://i.stack.imgur.com/J6X4V.jpg)\n\nand i have tabular data set consist of the before it being eaten\n\n    Banana,Rice,Soup,Paste \n    70,30,50,30 \n    \n    and after being eaten\n    \n    Banana,Rice,Soup,Paste \n    40,20,30,12\n\nand i want to predict each Banana,Rice,Soup &amp; Paste value ( in gram ) before and after being eaten from only the image from dataset, \n\nheres how my model that  i implement it so far\n\n`input_layer = tf.keras.layers.Input(shape=[180,180,3])`  \n`cnn = tf.keras.layers.Conv2D(filters=64, kernel_size=7)(input_layer)`  \n`cnn = tf.keras.layers.MaxPooling2D(pool_size=2)(cnn)`  \n`cnn = tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding=\"SAME\")(cnn)`  \n`cnn = tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding=\"SAME\")(cnn)`  \n`cnn = tf.keras.layers.MaxPooling2D(pool_size=2)(cnn)`  \n`cnn = tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding=\"SAME\")(cnn)`  \n`cnn = tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding=\"SAME\")(cnn)`  \n`cnn = tf.keras.layers.MaxPooling2D(pool_size=2)(cnn)`  \n`cnn = tf.keras.layers.Flatten()(cnn)`  \n`cnn = tf.keras.layers.Dense(units=128, activation='relu')(cnn)`  \n`cnn = tf.keras.layers.Dropout(0.5)(cnn)`  \n`cnn = tf.keras.layers.Dense(units=64, activation='relu')(cnn)`  \n`layer_y1 = tf.keras.layers.Dense(64,activation='relu')(cnn)`  \n`layer_y2 = tf.keras.layers.Dense(64,activation='relu')(layer_y1)`  \n`layer_y3 = tf.keras.layers.Dense(64,activation='relu')(layer_y2)`  \n`layer_y4 = tf.keras.layers.Dense(64,activation='relu')(layer_y3)`  \n`y1 = tf.keras.layers.Dense(1,name=\"Nasi\",activation=None)(layer_y1)`  \n`y2 = tf.keras.layers.Dense(1,name=\"Timun\",activation=None)(layer_y2)`  \n`y3 = tf.keras.layers.Dense(1,name=\"Blank\",activation=None)(layer_y3)`  \n`y4 = tf.keras.layers.Dense(1,name=\"Telur\",activation=None)(layer_y4)`\n\nand the compile\n\n`model.compile(optimizer=optimizer,`  \n `loss = 'mae',`  \n `loss={'Nasi': 'mse',`  \n `'Timun': 'mse',`  \n `'Blank' : 'mse',`  \n `'Telur' : 'mse',`  \n`},`  \n `metrics={`  \n `'Nasi': tf.keras.metrics.RootMeanSquaredError(),`  \n `'Timun': tf.keras.metrics.RootMeanSquaredError(),`  \n `'Blank' : tf.keras.metrics.RootMeanSquaredError(),`  \n `'Telur' : tf.keras.metrics.RootMeanSquaredError(),`  \n`}`  \n`)`\n\nand for some reasons, it keep doesnt predict the right value, and keep getting stuck on the same loss. Anyone know how to implement it properly?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ois7ue/predicting_multioutput_regression_using_tensorflow/"}, {"autor": "Dario_Della", "date": "2021-07-12 13:37:42", "content": "How to Compute IOU mask r cnn /!/  Hi guys, i'm data science student and for my class project i'm building a mask r cnn model for detect 13 classes.\n\nFor compute IOU i use this code:\n\n    Compute VOC-style Average Precision\n    def compute_batch_ap(image_ids):\n        APs = []\n        for -----> image !!! _id in -----> image !!! _ids:\n            # Load -----> image !!! \n            -----> image !!! , -----> image !!! _meta, gt_class_id, gt_bbox, gt_mask =\\\n                modellib.load_-----> image !!! _gt(dataset, config,\n                                       -----> image !!! _id, use_mini_mask= False)\n            # Run object detection\n            results = model.detect([-----> image !!! ], verbose=0)\n            # Compute AP\n            r = results[0]\n            AP, precisions, recalls, overlaps =\\\n                utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n                                  r['rois'], r['class_ids'], r['scores'], r['masks'])\n            APs.append(AP)\n        return APs\n    \n    # Pick a set of random -----> image !!! s\n    -----> image !!! _ids = np.random.choice(dataset.-----> image !!! _ids, 263)\n    APs = compute_batch_ap(-----> image !!! _ids)\n    print(\"mAP @ IoU=50: \", np.mean(APs))\n\n My question is: how can i compute iou for every single class? Thanks all.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oireq8/how_to_compute_iou_mask_r_cnn/"}, {"autor": "Alex55936", "date": "2021-07-10 19:01:40", "content": "I have a cats dataset of 9000+ images but it's only cats , and now I need atleast some 2000+ non cat images to train my cat classifier . Does such a non cat or any random -----> image !!!  dataset even exist? Or do I have to do manual labor ? What should I do now?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ohoo7f/i_have_a_cats_dataset_of_9000_images_but_its_only/"}, {"autor": "Alex55936", "date": "2021-07-10 17:02:30", "content": "How do I resize all images in a folder having different dimensions ? /!/ I have a dataset of around 10,000 animal pictures.  They are all of different dimensions, I don't  know the range of their dimensions like what is the smallest or the largest dimension -----> image !!! . \n\nI want to get them all to a fixed dimension say 600x600 or anything else. I'm not worried about aspect ratio or if I have to crop them or not. \n\nCan anyone help me with this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ohmgr5/how_do_i_resize_all_images_in_a_folder_having/"}, {"autor": "mvpetri", "date": "2021-07-10 12:01:44", "content": "What would be the best strategy to collect my own handwriting? /!/ I want to create a dataset of images of my own hand writing. Just for learning purposes, no high precision required and if the dataset becomes too small it doesn't really matter because I just want to go through the process. The objective is first to recognize the text. Then, if it works, try to give an input and let it show an -----> image !!!  of that text with my handwriting.\n\nI was thinking of scanning pages of notebooks and then tediously label it. But I was wondering how should I approach this. Should I label each individual letter separately? Like a bunch of As, and then Bs and so on. Should I label syllables? Like \"An\", \"Am\", \"As\", \"Ba\", \"Be\" etc... Or should I label entire words? And if I label entire words, is there any detriment to also label each character of that word separately? For example, if I label the word \"Banana\" could I also label the B and each A and N letter?\n\nWhat do you think?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ohhfum/what_would_be_the_best_strategy_to_collect_my_own/"}, {"autor": "TrepidationTD", "date": "2021-10-19 13:20:48", "content": "Best way to do object detection with bounding boxes? /!/ I'm doing a project where I need to create a bounding box around a custom object and it needs to be really fast in it's detection. I'm thinking of using OpenCv with the yolo model but Im not sure if this is the best way to go about it. I need to recognize two features for each object detected and link them together. For example, if I have a dog, I should recognize that dogs face and nose with -----> image !!!  segmentation and a bounding box. The dogs face needs to be linked to the nose it detects so if there are multiple dogs, it recognizes that each dog has this specific nose. What is the best way to accomplish a project like this in real time video?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qbbele/best_way_to_do_object_detection_with_bounding/"}, {"autor": "MikHambone", "date": "2021-10-19 05:56:21", "content": "Creating my own dataset and I\u2019m absolutely stuck! /!/ Hello! I am currently in the beginning stages of taking all my prepared images and labels and running some basic models to get the hang of things before I work on making a more complicated model. Right now the goal is to use a classification model to count items in a given -----> image !!! , the given label is the number of items that should be counted from each -----> image !!! . \n\nMy biggest problem is I seem to be having a hard time loading in both my image file and .csv file of labels. I started out trying tensorflow in a Google Colab notebook. I could load in both my .csv file and my folder of images, but could not split my datasets because my image file was way shorter than the correct length and I could not figure out why. I\u2019m currently seeing if I can get pytorch to work.\n\nI guess my biggest issue is that I\u2019m feeling totally lost in this dataset setup. I\u2019m having a difficult time finding sources that help walk you through this process as opposed to using premade datasets. Does anyone have any advice or good resources to help me learn how to do this? Thank you in advance!\n\nTL;DR what are good resources to teach you to set up your own ML data set from scratch.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qb56nq/creating_my_own_dataset_and_im_absolutely_stuck/"}, {"autor": "EnlightenedOne789", "date": "2021-10-18 22:08:14", "content": "Shape and arc detection in machine learning /!/ Greetings\n\nI am new to machine learning and I would like to know, what branch of it should I look into for geometric detection? For example, in the attached -----> picture !!! , is Monza, the Italian racetrack. I aim to train a model to drive through the track like a car and characterise the distance between each turn ( as it is numbered) and their direction.  So from the start til turn 1, I expect my model to tell me it was a long straight then one right-hander. \n\nhttps://preview.redd.it/wn618kkp7au71.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=f953575d3dc967286c756fd6ada9b9a8b81ad5f2", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qax7mh/shape_and_arc_detection_in_machine_learning/"}, {"autor": "Udongeein", "date": "2021-10-18 02:46:36", "content": "Discord Chatbot using a fine tuned GPT-J 6B model /!/ I had fine tuned a GPT-J 6B model on 41 million messages to help produce a chat bot with stunning results! I am quite pleased with it and I had just made the model publicly available as well just in case anyone wants to have a go at it and learn how to make their own chat bot as well! In the -----> picture !!!  is an example conversation with the bot, it does a good job on illustrating it's capacity to hold a conversation with users.\n\nHere is the link to the fine tuned model: [https://huggingface.co/hakurei/c1-6B](https://huggingface.co/hakurei/c1-6B)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qadoum/discord_chatbot_using_a_fine_tuned_gptj_6b_model/"}, {"autor": "idontreallywolf", "date": "2021-10-17 20:53:22", "content": "What learning-path should I take in order to build an AI to recognize digits ? /!/ I have decided to build something new for my portfolio and decided it should be a (simple?) AI which can recognize digits on an x\\*y -----> image !!! .\n\nMy current problem is that I'm not sure what I need to know or where I begin to read.\n\nI have more than basic knowledge in programming.\n\nA guide/path would be really helpful.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qa7gvp/what_learningpath_should_i_take_in_order_to_build/"}, {"autor": "MihaelK", "date": "2021-10-17 10:13:05", "content": "Help understanding a question about visualizing a decision boundary between 2 classes /!/ Hello!\n\nI'm taking an introductory Machine Learning class and I have a problem about visualizing a decision boundary between 2 classes given their distributional characteristics (see -----> picture !!! ).\n\nConcretely, I didn't understand the last line: \"You need to consider different cases of the values p1, p2..\"\n\nDo I need to give them arbitrary values? What values should I consider?\n\nAny help about understanding the question would be welcome!\n\nThank you!\n\nhttps://preview.redd.it/z0m2p44pjzt71.png?width=1051&amp;format=png&amp;auto=webp&amp;s=9843b641c048f1ca2ea403c02f24cd98c75e3378", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q9vzn4/help_understanding_a_question_about_visualizing_a/"}, {"autor": "JMG518", "date": "2021-10-17 02:29:54", "content": "[D] Text-to------> Image !!!  AI With Reference -----> Image !!! s /!/ Hi guys! I have a reference image that I want to plug into an AI code. I hope this will generate realistic results. Are there any Github or Google Colab codes that allow you to import a reference image/sketch that the AI can use? Are there any other tips for generating realistic results from Text-to-Image? Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q9pyhn/d_texttoimage_ai_with_reference_images/"}, {"autor": "BusyGene", "date": "2021-10-16 20:01:11", "content": "Image Segmentation material? /!/ I read a good portion of Moroney\u2019s AI and ML for coders book and Chollet\u2019s Intro to DL. I was wondering if there are any specific resources that could cast my learning direction directly toward -----> image !!!  segmentation and preprocessing. Any input is appreciated :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q9jeuj/image_segmentation_material/"}, {"autor": "yashraj3010", "date": "2021-10-16 10:41:04", "content": "How do you create your own datasets for CNNs? /!/ I've worked on the MNIST dataset, and I have understood the concepts involved, but there are almost no resources as to how I can create my own -----> image !!!  dataset for my project. Does anyone have any advice on this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q99kmu/how_do_you_create_your_own_datasets_for_cnns/"}, {"autor": "Edulad", "date": "2021-10-16 07:28:10", "content": "Remove Background Color From Image /!/ Hi, so i was playing around **opencv** and found a way online on how to detect and crop the -----> image !!! , but \n\nhow can i remove its Background color, so only the -----> image !!!  is saved in Lossy web Compressed PNG format.\n\n**My Code:**\n\n`import cv2`\n\n`import numpy as np`\n\n`import os`\n\n&amp;#x200B;\n\n`o = os.getcwd()`\n\n&amp;#x200B;\n\n`for s in os.listdir():`\n\n`if \".jpg\" in s:`\n\n`T = os.path.splitext(s)[0]`\n\n`img = cv2.imread(s)`\n\n`blurred = cv2.blur(img, (3,3))`\n\n`canny = cv2.Canny(blurred, 50, 200)`\n\n&amp;#x200B;\n\n`## find the non-zero min-max coords of canny`\n\n`pts = np.argwhere(canny&gt;0)`\n\n`y1,x1 = pts.min(axis=0)`\n\n`y2,x2 = pts.max(axis=0)`\n\n&amp;#x200B;\n\n`## crop the region`\n\n`cropped = img[y1:y2, x1:x2]`\n\n`cv2.imwrite(f\"{o}/{T}.png\", cropped)`\n\n&amp;#x200B;\n\n**My Input Image:**\n\n&amp;#x200B;\n\nhttps://preview.redd.it/ktcnbl5glrt71.jpg?width=476&amp;format=pjpg&amp;auto=webp&amp;s=3dd6f0994b5e03d48a0b782d9dca5e3928f8b4fc\n\n&amp;#x200B;\n\n**My Output Image:**\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/axzstbxglrt71.png?width=447&amp;format=png&amp;auto=webp&amp;s=3536780afad80c36e99f83d67dafe31ac89aa4f0", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q9771c/remove_background_color_from_image/"}, {"autor": "paswut", "date": "2021-10-15 19:44:14", "content": "setting up tf-gpu in 2021 /!/ holy shit, i take a hiatus then come back and play around with pytorch. 0 issues installing from the conda install. Yet when I try to launch up tensorflow I hit the age old issue of hooking up to my local GPU. I tried using conda that doesn't work. I tried using the official docker hub -----> image !!!  with jupyter. I launch the container. I can import tensorflow into the python from terminal, but when I try to use Jupyter lab, I get some archaic 'cant find ipython error' which yields no useful hits on google. \n\ntf, I'll just resign myself to using sub-par google colab environments because I really like tensorflow.\n\nI had it working months ago when I was using TF, but I really dont wanna spend hours again setting it up. does anyone have a functioning docker image I can grab that uses jupyter lab?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q8vwtg/setting_up_tfgpu_in_2021/"}, {"autor": "Sea-Put-9356", "date": "2021-08-05 00:27:58", "content": "HoG irregular Feature vector size /!/ I am trying classification using HoG. The images that I have are of different sizes. So, HoG features for each -----> image !!!  has different size of feature vector which cannot be concatenated to form a training feature vector. \nWhat can be done to remedy this problem ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oy66di/hog_irregular_feature_vector_size/"}, {"autor": "Smiley_35", "date": "2021-08-04 18:40:02", "content": "I want to find/automatically place signature blocks on a pdf and have a training dataset. How can I approach this problem? /!/ Hello,  \nForgive me as I am completely new to machine learning and am taking on a small POC project.   \nWhat I would like to do is build a service that suggests placement of signature blocks on a user uploaded pdf. I have a medium sized dataset of pdfs (around 5k), along with x/y coordinates of signature blocks that exist for each pdf page. \n  \nMy thoughts were that for each page I could convert the pdf to an -----> image !!!  and use the coordinates, as well as the -----> image !!!  as inputs for a neural network. I don't know if this is the correct approach or if I should go with some type of computer vision approach. I would appreciate any advice you could give me. Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oxz9le/i_want_to_findautomatically_place_signature/"}, {"autor": "Spiritual_Sink_2879", "date": "2021-08-04 14:59:32", "content": "[CNN] Is there a relation between detection and localisation on images ? /!/ Hi !\n\nI am creating an -----> image !!!  recognition algorithm using a Convolutional Neural Network (CNN).  \nMy goal is to detect letters and numbers engraved in concrete. In my trained model, those elements are principally\u00a0 located in the center of the image.  \nDuring different detection tests I saw that the algorithm is getting less confident on elements on the sides of images.  \nIs there a relation between detection and its localisation on images ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oxuh7b/cnn_is_there_a_relation_between_detection_and/"}, {"autor": "phobrain", "date": "2021-08-04 10:30:30", "content": "-----> Photo !!!  pairing by keras", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oxptms/photo_pairing_by_keras/"}, {"autor": "20gunasart", "date": "2021-08-04 03:38:51", "content": "Best classifier for high number of features? (Particularly 256x256x3) /!/ Hi! I have an -----> image !!!  classification task I\u2019m working on with quite a high number of features given the -----> image !!!  size.\n\nI\u2019m used to just throwing a CNN at everything and that works usually, but I want to start thinking more critically about what classification algorithm is the best to use in specific scenarios. Below is my thought process, please critique it :)\n\n1. I won\u2019t use a neural network because it will be very slow given the high number of features. The back propagation step in particular will take very long\n\n2. I won\u2019t use KNN, because calculating the distance from each example for a point we are trying to classify for such a high dimensional vector will take way too long, especially since I have 1000,000 training examples, no matter the distance metric we decide to use.\n\n3. SVM is a good candidate because all the classification entails is plugging it into a formula (theta ^ T(x) + B), and then checking your decision boundary, but a trade off is that the kernel function maps our high dimensional data to an even higher dimension, and we already have an excessive amount o festures\n\n4. Logistic regression is also a good candidate, however SVM is a better one because with logistic regression we have to calculate first the sigmoid function input and then calculate the probability of belonging to each class (I say logistic and not soft max because this is a binary classification task\n\n5. Decision trees or any tree based algorithm is not an option here, given the dimension of the data the tree will take too long to build", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oxk5ax/best_classifier_for_high_number_of_features/"}, {"autor": "Laurence-Lin", "date": "2021-02-08 03:30:07", "content": "My -----> image !!!  classification model only predict one class /!/ I'm using tensorflow's image\\_dataset\\_from\\_directory structure to load image and labels automatically, which means I save different class image in separate folder and read image from them. \n\n&amp;#x200B;\n\nAfter training for 10 epochs, the training and validate accuracy reaches 0.99, which seems to work well. However, while validating the performance by giving image, the model predict only one class no matter what input I give it. \n\n&amp;#x200B;\n\nI'm afraid that the tensorflow image loader only read one class for my image data, which is why the training and validate accuracy is high but model only predict one class.\n\n&amp;#x200B;\n\nHow could I check if tensorflow reads both class label while loading image\\_dataset\\_from\\_directory correctly?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lf2x0u/my_image_classification_model_only_predict_one/"}, {"autor": "marginado20", "date": "2021-04-21 13:30:43", "content": "Best algorithm/model to do math/excel parameters optimization? /!/ Hello, im new to ML, currently investigating with some tutorials and in a college course.\n\nI also like trading and some time ago i found a post that studied the \"take profit\" strategy. \n\nIn trading, when you open a position, you want to take some profit when certain levels are reached to avoid  retracements and losing the initial money. Also you can use that profit to invest elsewhere.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/exj3lt0t2ju61.png?width=579&amp;format=png&amp;auto=webp&amp;s=f7e132ba53f8ee5795ea87d7eaafa56134416ea4\n\nIn this -----> image !!! , you can see this person strategy that is to take profit. It starts with $1 and take profit when the ticker is 150%, 200%, 300%, 600%, etc (2nd column) taking 13%, 17%, etc (3rd column) each time. At the end, the \"combined\" column is the more important result. \n\nWhile this strategy appears to be good, not every stock increases 300% so i want to use a ML model to investigate the best parameters to take profit if the stock increases for example 3%, 5%, 7% and what amount of profit to take (50% when prices increases 5%, 25% when prices reaches 10% or 100% when prices reaches 15%, etc). \n\nI can be tweaking parameters on an excel all day but i believe that this math tweaks can be done with a ML model.\n\nWhat would be a good ML model/strategy to use? Any resource or tips to start this small project?\n\nAll i want to do is tweak the parameters on column 2 an 3 to optimize the \"combined\" column.\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mvfpzn/best_algorithmmodel_to_do_mathexcel_parameters/"}, {"autor": "spmallick", "date": "2021-04-20 18:21:04", "content": "Autoencoder in TensorFlow 2: Beginner\u2019s Guide /!/ Autoencoders have many interesting applications: Compressing data for a faster transfer, denoising an -----> image !!! , dimensionality reduction, -----> image !!!  segmentation, -----> image !!!  inpainting, and many more.  \n\n\nIn today's post, we discuss all you need to know about an Autoencoder, discuss its objective function, and learn to reconstruct some of the most famous datasets such as Fashion-MNIST in the TensorFlow v2.0 framework with detailed code explanation.  \n\n\nAs a bonus, we perform various experiments. We exploit the trained Autoencoder's latent space. We conclude by summing why simple Autoencoders are not Generative in nature.  \n\n\n[https://learnopencv.com/autoencoder-in-tensorflow-2-beginners-guide/](https://learnopencv.com/autoencoder-in-tensorflow-2-beginners-guide/) \n\nhttps://preview.redd.it/3gd0tfasedu61.jpg?width=600&amp;format=pjpg&amp;auto=webp&amp;s=5b6b4d51cc24d44c2dd1e7141e43c5b44799f0c1", "link": "https://www.reddit.com/r/learnmachinelearning/comments/muwry9/autoencoder_in_tensorflow_2_beginners_guide/"}, {"autor": "Dario_Della", "date": "2021-04-20 13:24:56", "content": "Convert xlsx or csv file to json dictionary for training custom MASK R CNN /!/ I'm a student in data science and object classification / detection. I have a file xlsx containing the following columns:\n\n1. Name -----> image !!! ;\n2. Class Label;\n3. Region count (nth polygon identified in the image);\n4. polygon coordinates (all\\_x\\_points, all\\_y\\_points);\n5. Color polygon\n\n&amp;#x200B;\n\nI converted this xlsx file in json list but Mask R CNN needs a json dictionary for the annotations. I tried this code to convert json file in json dict:\n\n    def Convert(a):\n        it = iter(a)\n        res_dct = dict(zip(it, it))\n        return res_dct\n             \n    # Driver code\n    list = json.load(open(\"file.json\"))\n    print(Convert(lst))\n\nCode error: unhashable type: 'dict'\n\n&amp;#x200B;\n\nSo, I tried with csv file (after the convertion from xlsx file):\n\n    from csv import DictReader\n    import json\n    \n    fieldnames = ('Name_image', 'Class_Label', 'Region_count', polygon_coordinates, 'coordinates', 'Color_polygon')\n    \n    with open('/content/test_set.csv', 'r') as fd:\n      data = list(DictReader(fd, fieldnames))\n    \n    with open('Annotations_train.json', 'w') as fd:\n      json.dump(data, fd)\n\nThe output is complety wrong because the columns are not linked with the right data.\n\nPreviously, for learning, I trained the Mask R CNN with customized VGG Image Annotation on another image dataset and all worked perfectly.\n\nHowever for this assignment I have only this xlsx annotations (1. Name image; 2. Class Label; 3. Region count; 4. polygon coordinates (all\\_x\\_points, all\\_y\\_points); 5. Color polygon and I can't change them).\n\nWhat is the best methodological approach for training Mask R CNN with my xlsx annotations? Thanks all.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/muq8e4/convert_xlsx_or_csv_file_to_json_dictionary_for/"}, {"autor": "Mornthenorn", "date": "2021-04-20 08:59:23", "content": "NN - Backpropagation Chain Rule /!/ Can someone please tell me how you're meant to use the chain rule to convert:\n\n&amp;nbsp;\n\nhttps://res.cloudinary.com/practicaldev/-----> image !!! /fetch/s--P-_lodv---/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://miro.medium.com/max/6746/1%2AouMKSO9lKY_DFVCgPp42Aw.jpeg\n\n&amp;nbsp;\n\ninto:\n\n&amp;nbsp;\n\nhttps://res.cloudinary.com/practicaldev/-----> image !!! /fetch/s--lj4O-sWA--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://miro.medium.com/max/6746/1%2Aeowi-PL_abyOD6jd0jxavw.jpeg\n\n&amp;nbsp;\n\nI cannot for the life of my find any explanation on how the chain rule can arrive at the conclusion in -----> image !!!  2. It just seems they say, 'chain rule' and it magically splits into several functions.\n\n&amp;nbsp;\n\nThank you so much for any help!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mum7db/nn_backpropagation_chain_rule/"}, {"autor": "foyslakesheriff", "date": "2021-04-19 22:21:51", "content": "AI vs just OpenCV for vehicle wheel identification /!/ I'm attempting to plan out a project that takes an -----> image !!!  of a vehicle, and finds the wheel outline(not tire), so that a different wheel can automatically replace it. Imagine a visualizer that shows your car with different wheels. The vehicle input images will all be of good quality and just show the vehicle, either in a transparent or white background. \n\nI have decent experience in Python, but not OpenCV or any real ML outside of a few tutorials I've gone through. I understand the basic concepts of machine learning, but limited actual experience.\n\nIt seems like identifying wheels in an image is on the easier side of things for object identification, compared to other objects. I know there'll be 2 of them, no more or less, in the input image, and the tires will always be black. The vehicle may be at different angles, so I'll need to account for identifying wheel ellipses also, not always perfect circles.\n\nSo in that case, should I just fine-tune opencv to identify two black circles, and cut out whatever is within those circles? Or should I attempt to develop an AI model? I have access to an imageset of tens of thousands of wheels, just the wheel itself on a transparent background. I also have thousands of clean vehicle images.\n\nCrossposting this on r/learnmachinelearning and r/opencv\n\nAny advice or insight is much appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/muc5yi/ai_vs_just_opencv_for_vehicle_wheel_identification/"}, {"autor": "ScubaSieve", "date": "2021-04-19 20:47:02", "content": "What is the most accurate data set for object detection of digits? (Architectural Plans) /!/ Hi all,\n\nI am an architect currently writing a masters paper about the use of GAN's and how effective they can be in producing architectural plans. Preceding this, I'm completing some basic experiments to prove neural networks ability to recognise plans. \n\nOne of these experiments is to show that object detection can accurately evaluate if an architectural plan is following building regulations (an aspect reaching a max/min dimension for example). I'm aware how to do OD and have completed a different experiment previously within MakeML. My issue is that I don't know what kind of data set I would need for it to read digits rather than objects. \n\nIf we use the example that the width of a stair has to be above 250mm, I can use a data set of plans with the dimensions annotated on the -----> image !!!  to be labelled but will it be able to read the digit as an object? \n\nThis is the only option I can think of where a dataset includes plans with different annotated widths of stairs, then each dataset over 250mm would be labelled as 'Out of regulations'. This would be quite a lot of work making multiple plans each mm more up to a certain point  for it to then not work so I thought I would ask first.\n\nAgain, I am an architect so have limited knowledge in this field so I'm hoping somebody may have a better solution for me! \n\nMany thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mua55d/what_is_the_most_accurate_data_set_for_object/"}, {"autor": "ConfidentAd598", "date": "2021-04-19 17:19:53", "content": "-----> Image !!!  detection - train a model with representative images or highest quality? /!/ Hey guys.\n\nI\u2019m thinking about trying to produce a image detection/analysing model which would judge images containing a relatively simple shape from a background, detect shapes with that shape, read text from that shape and probably compare colours with what that shape should normally have.\n\nThe pictures which I will be using to train my model are my concern. What degree of quality image should be used to do this if the objective is to have the most a model best suited to make inferences about images coming the most typical source - which is probably a iPhone.\n\nMy assumption is that the quality of the image follows something like this camera&gt;iphone8&gt;iPhone 6.\n\nWould a model trained using high quality images from verging on professional camera essentially have a more difficult time determining what is on a iPhone 6 picture than a model trained with pictures from a iPhone 6.\n\nApologies for my terminology as I have no background in image processing/ml and haven\u2019t done stats in a while to describe in the impact id expect to find in the data. Hopefully you understand what I\u2019m grasping at.\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mu5jw2/image_detection_train_a_model_with_representative/"}, {"autor": "whiterabbitobj", "date": "2021-04-19 03:13:36", "content": "-----> Image !!!  sequence up scaling /!/ I am investigating options for writing a tool that would upscale 2k images to 4K, sequenced images that would need to be temporally consistent. \n\nI am versed in Deep RL algorithms but not image processing. Does anyone have any suggestions where to begin? Is there an existing library or NN which would be appropriate to begin training on? Resnet?\n\nI will have access to paired 2k and 4K (both native and artificially downrezzed for the pairs) bespoke imagery to train for my specific use case. \n\nThank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mtrw5j/image_sequence_up_scaling/"}, {"autor": "moist_bread11", "date": "2021-04-18 23:57:10", "content": "How would I go about classifying a noisy -----> image !!!  as having X or not having X? /!/ I made a cnn trained on the plant village dataset, which has photos of individual leaves in the same controlled setting, for example background, lighting, etc. The dataset is for classifying plant diseases, such as tomatoes, peppers, etc. However, the reason I made the model in the first place was that I was hoping to use it for more \"real life\" photos. I'm not sure how to classify these types of images, since it will have multiple leaves and the sizes will vary a lot. Should I just train on a crowded dataset? I did hear about one approach where you have a sort of sliding window over an image, creating a new image of where the window currently is, and then passing that into the model. If so, what would the weighting system be like? You classify one small part of the image as being diseased, and the entire image is labeled as having a disease? Any suggestions?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mtonlt/how_would_i_go_about_classifying_a_noisy_image_as/"}, {"autor": "Inevitable_Run_2631", "date": "2021-04-27 02:15:40", "content": "Images for a Segmantic Image Segmentation Project /!/ Necessary disclaimer that I am a novice in ML with probably just enough knowledge to be dangerous. I am starting a project trying to use ML in order to identify a vehicle object for sales images. Ultimately trying to segment the foreground from the background.\n\nSingle instance even if other instances are in the background.\n\nMy questions start to arise when collecting images to be used with training. I already found a dataset that has a uniform input -----> image !!!  size. I want to add additional images to the training/testing data but I have been told that I can not use images of different sizes or that images have to be resized for uniformity but it brings up even more questions. \n\nIf different aspect ratio images are resized to the standard input dimension, this means that images could be stretched/compressed differently to fit the standard input dimension. Will this cause the model to be less accurate? Are there certain model architectures that will allow this dynamic input dimension ranges and certain architectures that will not?\n\nIf I use a standardized image dimension with lets say a 4:3 aspect ratio for training. And then I use the model to predict an image that is 3:2. Should I assume that this will be less accurate? Does making a model for each of the most popular aspect ratios make sense.\n\nI found tensorflow has a function that will crop or pad an image to the target dimensions. Is this a common practice that should be considered for my workflow? Will images submitted for prediction need to follow the same workflow for prediction accuracy?\n\nI am trying to gain enough of an understanding to start planning the model and the workflow of the application.  \n\nAm I asking the right questions?\n\nIs there any other information that would be helpful?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mze75q/images_for_a_segmantic_image_segmentation_project/"}, {"autor": "Xena_psk", "date": "2021-04-26 11:12:55", "content": "Converting user input -----> image !!!  file into required input for -----> image !!!  classifier tensorflow js model. /!/ I am trying to create a browser based image classifier using tensorflow js. I have created a model and exported to model.json and used this model for prediction of user input image file. When user inputs the image file, I pass the value to model however I'm getting the following error.\n\nError when checking : expected flatten\\_input to have shape \\[null,28,28\\] but got array with shape \\[100,100,3\\]\n\nI am not sure how to resolve this error. I have attached my entire code below\n\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    \n    &lt;head&gt;\n        &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n        &lt;link href='https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css'&gt;\n        &lt;script src='https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.bundle.min.js'&gt;&lt;/script&gt;\n        &lt;script src='https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js'&gt;&lt;/script&gt;\n    \n    &lt;/head&gt;\n    \n    &lt;body&gt;\n        &lt;div class=\"container d-flex justify-container-center\"&gt;\n            &lt;input type='file' accept='image/*' onchange='openFile(event)'&gt;&lt;br&gt;\n            &lt;img id='output' style=\"height:100px; width:100px;\"&gt;\n            &lt;button id=\"modelrun\" onclick=\"runModel();\" &gt;Predict&lt;/button&gt;\n        &lt;/div&gt;\n        &lt;script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.1.2\"&gt;&lt;/script&gt;\n    \n        &lt;script&gt;\n            var inputTensor\n            let targetSize = { w: 28, h: 28 }\n            loadModel();\n            var openFile = function (file) {\n                var input = file.target;\n                var reader = new FileReader();\n                reader.onload = function () {\n                    var dataURL = reader.result;\n                    var output = document.getElementById('output');\n                    output.src = dataURL;\n                    inputTensor = tf.browser.fromPixels(output)\n                    console.log(\"Tensor image\", inputTensor)\n                    console.log(\"Image tensor generated\")\n                    inputTensor.width = 28\n                    inputTensor.height = 28\n                };\n                reader.readAsDataURL(input.files[0]);\n            };\n    \n            async function loadModel() {\n                model = undefined;\n                console.log(\"model \", model)\n                model = await tf.loadLayersModel('/model.json');\n                console.log(\"model loaded\", model)\n                return\n            }\n    \n            window.runModel = async function () {\n                if (inputTensor) {\n                    const output = model.predict(inputTensor)\n                    console.log('model.predict (output):', output.dataSync())\n                }\n            }\n        &lt;/script&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;", "link": "https://www.reddit.com/r/learnmachinelearning/comments/myv9fl/converting_user_input_image_file_into_required/"}, {"autor": "nguyenquibk", "date": "2021-04-26 04:33:28", "content": "How to prepare input data for training R3D, R(2+1)D ? /!/ I read the paper: [https://arxiv.org/pdf/1711.11248v3.pdf](https://arxiv.org/pdf/1711.11248v3.pdf) and want to training R3D or R(2+1)D with the UCF11 dataset.\n\nBut, I do not know how to prepare the input data to fit the model from videos. \n\nWhat does this mean of this -----> picture !!! ? Could you explain for me please\n\nThank you so much everyone.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mypm60/how_to_prepare_input_data_for_training_r3d_r21d/"}, {"autor": "Zadani_tar", "date": "2021-04-25 23:28:58", "content": "My AutoEncoder Conv3D needs your help /!/ I(inexperienced)'m trying to  implement an autoencoder network using some Conv3D layers. With the help  of some random pages, I managed to write this piece of code but the  network gives me 0.0 acc and a very large value for mse, negative value  for loss!\n\nI literally don't precisely know how to tune the parameters or shape the layers!\n\nany comment or advice would be appreciated.\n\n    videodata = skvideo.io.vread(path_to_a_vid, as_grey = True)\n    x.shape # (75, 150, 150, 1) which is reshaped to (15, 5, 150, 150, 1)\n    \n    model = Sequential()\n    model.add(Input(shape=(5, 150, 150, 1)))\n    model.add(Conv3D(64, (5, 5,5), activation='relu', padding='same'))\n    model.add(MaxPooling3D(pool_size=(5, 5,5), strides=(2,2,2), padding='same'))\n    model.add(BatchNormalization())\n    \n    model.add(Flatten()) \n    \n    model.add(Dense(1, activation='relu'))\n    \n    model.add(Input(shape=(1,)))\n    \n    model.add(Dense(1, activation='relu'))\n    \n    model.add(Reshape((1, 1, 1,1)))\n    \n    model.add(Conv3D(4, (3, 3,3), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    \n    model.add(UpSampling3D((1, 5,5)))\n    model.add(Conv3D(16, (3, 3,3), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    \n    model.add(UpSampling3D((5, 30,30)))\n    model.add(Conv3D(32, (3, 3,3), activation='relu', padding='same'))\n    model.add(Conv3D(1, (3, 3,3), activation='sigmoid', padding='same'))\n    \n    \n    model.compile(optimizer=optimizers.Adam(0.001), loss='binary_crossentropy', metrics=['accuracy', 'mse'])\n    \n    model.fit( x, x, batch_size=150, epochs=10, shuffle=True,use_multiprocessing=True)\n\nsummary example for 2 epochs is attached as an -----> image !!! !\n\n&amp;#x200B;\n\nhttps://preview.redd.it/pg5qjgs7mev61.png?width=950&amp;format=png&amp;auto=webp&amp;s=db953175f1b9c09dbb4bb640faba4ccf05cfc55c", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mykfnv/my_autoencoder_conv3d_needs_your_help/"}, {"autor": "spacenotsodandy", "date": "2021-04-25 20:12:34", "content": "Specify a Single Location to Identify in an Image Classifier /!/ For a personal project I am trying to make an -----> image !!!  classifier using TensorFlow to recognize objects in -----> image !!! s from a portion of my data set.  The thing is though all of these pictures are of the same place, and the same size, there is really only one area of the images that I am interested in...I cannot crop these images but I want to focus on labeling an object in this one location, I don't care about other objects in the image and only want output from a section of my choice; any suggestions on how I could do this?  Please let me know if you have any related experience.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mygijp/specify_a_single_location_to_identify_in_an_image/"}, {"autor": "malik-sahabb", "date": "2021-04-25 18:23:23", "content": "Convert low res -----> picture !!!  to high res -----> picture !!!  [Project] /!/ I (noob) have a huge data set of images that have two variants - 1st very high res image from a professional camera, 2nd very low res from a cheap camera. Both images are identical - difference is just the quality. Can I use this dataset to train a neural network to output high-res image of any low-res input image?\n\n&amp;#x200B;\n\nIf possible, what is the best approach to do this? GANs, Deep Learning (again... total noob)?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mye6ma/convert_low_res_picture_to_high_res_picture/"}, {"autor": "Deanodeano1", "date": "2021-04-25 18:19:36", "content": "How to use StyleGAN2-ada to train your own GAN and generate your own images from a dataset through colab /!/ Hi Everyone,\n\nI've spent the last few months learning how to work with StyleGAN2-ada through google's Colaboratory (meaning you wouldn't need your own computing resources)\n\nIf you are keen to learn how to produce gifs such as this one: [https://giphy.com/gifs/vwLcX8DKe1st7wzfGp](https://giphy.com/gifs/vwLcX8DKe1st7wzfGp)\n\nThen please check out my Colab notebook and github page where you can learn how to do this and more :\n\n[https://github.com/deanodeano11/my-work-on-StyleGAN2-ada](https://github.com/deanodeano11/my-work-on-StyleGAN2-ada)\n\n&amp;#x200B;\n\nThe longer term goal of this project would be to take an -----> image !!!  of someone and turn this into a real world character sheet, you can find more information on this and how it could possibly be achieved within my blogpost.htm file\n\nThanks for reading and i hope my work is helpful for you :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mye3nv/how_to_use_stylegan2ada_to_train_your_own_gan_and/"}, {"autor": "Hot_Road8758", "date": "2021-04-25 15:25:01", "content": "Help me choose a PC build /!/ Hello everyone. I was planning to build a workstation to use as my PC, and I thought might also build it such that it is useful for Deep Learning. I am a novice in Deep Learning and planning to compete in Kaggle henceforth and read/implement research papers.  \nThere are two specifications, which a custom PC maker gave me.   \n\n\n[This one was within my budget](https://preview.redd.it/cf7rupn27cv61.png?width=755&amp;format=png&amp;auto=webp&amp;s=f274c887c0b63285c2894e60478270216828d787)\n\n&amp;#x200B;\n\n[This seems a little bit out of my budget.](https://preview.redd.it/8fzn9zm57cv61.png?width=587&amp;format=png&amp;auto=webp&amp;s=3227e8405e527d4b89ceef27fa9a21a1fb2d2801)\n\nI have a couple of questions.   \n\n\n1) Is the GPU mentioned in the -----> picture !!! , a good GPU for Kaggle competitions, and occasional implementation of NLP and CV papers ?  \n2) Since the second configuration is slightly out of budget, I was planning to go for the first one. Is it worth shelling out money for i7 processor, or going with an i5 processor does not make any significant difference ? The RAM for both is 16GB.   \n3) Should I go for more RAM ie 32GB ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/myahnf/help_me_choose_a_pc_build/"}, {"autor": "alkaway", "date": "2021-04-24 22:58:43", "content": "MAML for Few-Shot Learning /!/ I have a medical imaging dataset with 1000 labelled images and 5 classes. It is a multi-label classification problem, i.e. each sample can belong to multiple classes. Could I use MAML to learn to solve this one -----> image !!!  classification task, or does MAML not make any sense because I only have one task?\n\nAdditionally, if I can use MAML, how would I set up the problem? Because it is a multi-label classification problem, do I treat each class as a separate task?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mxv6he/maml_for_fewshot_learning/"}, {"autor": "prestodigitarium", "date": "2021-04-24 18:25:16", "content": "Gourdian Free Dataset Download: OpenStreetMap Points of Interest (Restaurants, Bars, Grocery Stores, Transit, Shops, Swingers Clubs, Hospitals, etc) /!/ Hi there!\n\nA friend and I are working on something to help people search for, filter, and download subsets of datasets.\n\nWe're excited to share that we've just incorporated all(?) of the points of interest from OpenStreetMap, broken down by group (from their ontologies here https://wiki.openstreetmap.org/wiki/Key:amenity and here https://wiki.openstreetmap.org/wiki/Key:shop )\n\nThe groups are below, with the tags that went into each. \n\nWhat do people think these might be useful for? Maybe making your own version of WalkScore? Perhaps cross referencing with real estate listings to find a house that's within walking distance of a bakery, library, cafe, and pyrotechnics shop? LoveHotelMapper.com? The possibilities are endless!\n\n**Restaurants and Bars**: https://gourdian.net/g/eric/osm_points_of_interest.restaurants_and_bars\n\nAmenities points of interest labeled with bar, biergarten, cafe, fast_food, food_court, ice_cream, pub, or restaurant.\n\n**Education Services**: https://gourdian.net/g/eric/osm_points_of_interest.education_services\n\nAmenities points of interest labeled with college, driving_school, kindergarten, language_school, library, toy_library, music_school, school, or university.\n\n**Transportation Related**: https://gourdian.net/g/eric/osm_points_of_interest.transportation_related\n\nAmenities points of interest labeled with bicycle_parking, bicycle_repair_station, bicycle_rental, boat_rental, boat_sharing, bus_station, car_rental, car_sharing, car_wash, vehicle_inspection, charging_station, ferry_terminal, fuel, grit_bin, motorcycle_parking, parking, parking_entrance, parking_space, or taxi.\n\n**Financial**: https://gourdian.net/g/eric/osm_points_of_interest.financial\n\nAmenities points of interest labeled with atm, bank, or bureau_de_change.\n\n**Healthcare Facilities**: https://gourdian.net/g/eric/osm_points_of_interest.healthcare_facilities\n\nAmenities points of interest labeled with baby_hatch, clinic, dentist, doctors, hospital, nursing_home, pharmacy, social_facility, or veterinary.\n\n**Entertainment**: https://gourdian.net/g/eric/osm_points_of_interest.entertainment\n\nAmenities points of interest labeled with arts_centre, brothel, casino, cinema, community_centre, conference_centre, events_venue, fountain, gambling, love_hotel, nightclub, planetarium, public_bookcase, social_centre, stripclub, studio, swingerclub, or theatre.\n\n**Public Services**: https://gourdian.net/g/eric/osm_points_of_interest.public_services\n\nAmenities points of interest labeled with courthouse, embassy, fire_station, police, post_box, post_depot, post_office, prison, ranger_station, or townhall.\n\n**Facilities**: https://gourdian.net/g/eric/osm_points_of_interest.facilities\n\nAmenities points of interest labeled with bbq, bench, dog_toilet, drinking_water, give_box, shelter, shower, telephone, toilets, water_point, or watering_place.\n\n**Waste Management**: https://gourdian.net/g/eric/osm_points_of_interest.waste_management\n\nAmenities points of interest labeled with sanitary_dump_station, recycling, waste_basket, waste_disposal, or waste_transfer_station.\n\n**Other Amenities**: https://gourdian.net/g/eric/osm_points_of_interest.other_amenities\n\nAmenities points of interest labeled with animal_boarding, animal_breeding, animal_shelter, baking_oven, childcare, clock, crematorium, dive_centre, funeral_hall, grave_yard, gym, hunting_stand, internet_cafe, kitchen, kneipp_water_cure, lounger, marketplace, monastery, photo_booth, place_of_mourning, place_of_worship, public_bath, public_building, refugee_site, or vending_machine.\n\n**Food Shops**: https://gourdian.net/g/eric/osm_points_of_interest.food_shops\n\nShops points of interest labeled with alcohol, bakery, beverages, brewing_supplies, butcher, cheese, chocolate, coffee, confectionery, convenience, deli, dairy, farm, frozen_food, greengrocer, health_food, ice_cream, organic, pasta, pastry, seafood, spices, tea, wine, or water.\n\n**General Shops**: https://gourdian.net/g/eric/osm_points_of_interest.general_shops\n\nShops points of interest labeled with department_store, general, kiosk, mall, supermarket, or wholesale.\n\n**Clothing Shops**: https://gourdian.net/g/eric/osm_points_of_interest.clothing_shops\n\nShops points of interest labeled with baby_goods, bag, boutique, clothes, fabric, fashion, fashion_accessories, jewelry, leather, sewing, shoes, tailor, watches, or wool.\n\n**Second Hand Shops**: https://gourdian.net/g/eric/osm_points_of_interest.second_hand_shops\n\nShops points of interest labeled with charity, second_hand, or variety_store.\n\n**Health and Beauty Shops**: https://gourdian.net/g/eric/osm_points_of_interest.health_and_beauty_shops\n\nShops points of interest labeled with beauty, chemist, cosmetics, drugstore, erotic, hairdresser, hairdresser_supply, hearing_aids, herbalist, massage, medical_supply, nutrition_supplements, optician, perfumery, or tattoo.\n\n**Hardware Shops**: https://gourdian.net/g/eric/osm_points_of_interest.hardware_shops\n\nShops points of interest labeled with agrarian, appliance, bathroom_furnishing, doityourself, electrical, energy, fireplace, florist, garden_centre, garden_furniture, gas, glaziery, groundskeeping, hardware, houseware, locksmith, paint, security, trade, or windows.\n\n**Furnishing Shops**: https://gourdian.net/g/eric/osm_points_of_interest.furnishing_shops\n\nShops points of interest labeled with antiques, bed, candles, carpet, curtain, doors, flooring, furniture, household_linen, interior_decoration, kitchen, lamps, lighting, tiles, or window_blind.\n\n**Electronics Shops**: https://gourdian.net/g/eric/osm_points_of_interest.electronics_shops\n\nShops points of interest labeled with computer, electronics, hifi, mobile_phone, radiotechnics, or vacuum_cleaner.\n\n**Vehicle and Outdoor Shops**: https://gourdian.net/g/eric/osm_points_of_interest.vehicle_and_outdoor_shops\n\nShops points of interest labeled with atv, bicycle, boat, car, car_repair, car_parts, caravan, fuel, fishing, golf, hunting, jetski, military_surplus, motorcycle, outdoor, scuba_diving, ski, snowmobile, sports, swimming_pool, trailer, or tyres.\n\n**Hobby Shops**: https://gourdian.net/g/eric/osm_points_of_interest.hobby_shops\n\nShops points of interest labeled with art, collector, craft, frame, games, model, music, musical_instrument, -----> photo !!! , camera, trophy, video, or video_games.\n\n**Stationary and Gift Shops**: https://gourdian.net/g/eric/osm_points_of_interest.stationary_and_gift_shops\n\nShops points of interest labeled with anime, books, gift, lottery, newsagent, stationery, or ticket.\n\n**Other Shops**: https://gourdian.net/g/eric/osm_points_of_interest.other_shops\n\nShops points of interest labeled with bookmaker, cannabis, copyshop, dry_cleaning, e-cigarette, funeral_directors, laundry, money_lender, party, pawnbroker, pet, pet_grooming, pest_control, pyrotechnics, religion, storage_rental, tobacco, toys, travel_agency, vacant, weapons, outpost, or user defined.\n\n\nA bit about what we're trying to build:\n\n* Filter (optional), click the button, CSV arrives on your hard drive\n* Downloads are always a single CSV, no bundles with weird directory structures, no other formats\n* CSVs are indexed and filterable by a few column types (lat/long and date/time at the moment), so you can download just the parts you want\n* Open licensed datasets are free to download\n* No signup required for downloading open datasets\n* Search within and across datasets\n\nFeedback welcome! If there are any datasets that you'd like to see added, let us know!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mxpqwm/gourdian_free_dataset_download_openstreetmap/"}, {"autor": "Urmomgay_6969", "date": "2021-03-02 23:22:40", "content": "Should I use google images as training data for disease classification? /!/ I wanted to classify plant diseases, but the ones I want to classify are not a part of popular datasets as plant village. As a result, I was thinking about using google images. However, there are many issues with approach. First of all, it would be difficult to determine if each source is reliable. Second, some images are not relevant. Third of all, very few have images of a single plant leaf. It is usually the entire plant. One thing to note is that all of the images in plant village are of singular leaves.  Is creating an -----> image !!!  dataset from google -----> image !!! s reasonable?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lwffnh/should_i_use_google_images_as_training_data_for/"}, {"autor": "Aroused_AI", "date": "2021-03-02 18:47:18", "content": "Easiest way to implement incremental learning for CNN /!/ Hi everyone,\n\nWith my friend we are working on a project (mobile app) where we employ CNN, trained on Food-101 dataset, to recognize a food. Over the lifecycle of the app, we would like to add new classes.\n\nI got some AI background and there were 2 ways I was thinking about:\n\n1. Train CNN on a Food-101, remove fully connected, extract features and classify with some ML algorithm that is able to learn incrementally - k-NN, SVM?\n2. Do everything as in step 1, but keep averaged feature vector for every class and find minimal distance (may run into problems tho)\n\nI am not fully aware of state-of-the-art and while I am concerned with -----> image !!!  recognition, getting to fully understate SOTA in incremental learning would probably take me several days.  Can someone please suggest any simple way to implement incremental learning for CNN? I will be glad for any paper or implementation.\n\nThank you :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lw9fhh/easiest_way_to_implement_incremental_learning_for/"}, {"autor": "nickbild", "date": "2021-09-18 23:02:55", "content": "Go Motion simplifies stop motion animation with machine learning /!/ &amp;#x200B;\n\nhttps://i.redd.it/d7rah3yleco71.gif\n\nA CSI -----> camera !!!  is connected to a Jetson Xavier NX. This camera continually captures images of a scene. Using the trt\\_pose\\_hand hand pose detection model, the Jetson is able to determine when a hand is in the image frame.\n\nEach time all hands leave the frame, a single image is saved as part of the stop motion sequence. In this way, it is possible to continually manipulate the scene, momentarily removing one's hands from view of the camera after each adjustment, and have a stop motion sequence automatically generated that contains only the relevant image frames.\n\nMore info: [https://github.com/nickbild/go\\_motion](https://github.com/nickbild/go_motion)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pqwif0/go_motion_simplifies_stop_motion_animation_with/"}, {"autor": "OrcWithFork", "date": "2021-09-18 17:16:41", "content": "Can someone suggest me a \"Fast Neural Style\" tutorial (to generate -----> image !!! /video and train own models) that works locally in anaconda with CUDA support and is up to date? I struggle with a +5 year old repo here...", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pqqgd2/can_someone_suggest_me_a_fast_neural_style/"}, {"autor": "crosszilla", "date": "2021-09-17 18:41:04", "content": "Could image recognition help uncover evidence in the Gabby Patito case? /!/ Hello all,\n\nI have been consumed with this case and recently someone discovered Yellowstone's publicly available -----> camera !!!  archives during the dates she went missing. People are, as we speak, combing through hundreds of hours of footage looking for her van and any clues: https://www.reddit.com/r/GabbyPetito/comments/pq35n7/archived_webcam_footage_from_inside_yellowstone/\n\nI really hope she is found, but don't know much in terms of actually implementing something to automate a task like this. Would this be something that could easily be automated? If so, would anyone be willing to help out LEO on this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pq61mj/could_image_recognition_help_uncover_evidence_in/"}, {"autor": "Successful_Boat_3099", "date": "2021-09-17 08:12:08", "content": "\ud83d\udc8aYour daily dose of machine learning : object detection on google cloud platform /!/ &gt; This is a series of posts that I post almost daily. I call them \u201cyour daily dose of machine learning\u201d. \n\n If you work on deep learning for computer vision then you\u2019ve probably used object detection at some point.\n\nTensorflow Object Detection API is one of the widely used tools to train object detection models such as SSD and Faster RCNN.\n\nA lot of ML practitioners use it to train models on their local machines, but did you know you can also run the training and evaluation of these models on google cloud platform (GCP)?\n\nIn fact, on GCP there is a service called Google AI Platform which allows you to easily train and evaluate object detection models using Tensorflow Object Detection API.\n\nThese are the necessary steps to do so:\n\n\\- You set up a project on google cloud.  \n\\- You enable the necessary APIs (google ai platform, billing, \u2026).  \n\\- You need to have google cloud SDK downloaded and installed on your machine.  \n\\- Create a google bucket and upload your data to it.  \n\\- Then, you can run one command from your terminal to create what\u2019s called a \u201ctraining job\u201d. This command is shown in the -----> image !!!  below.\n\nAfter doing these steps, you\u2019ll have your training job running on google cloud and you\u2019ll be able to follow the progress of the training using tensorboard and the logs generated during the training.\n\nIn the command below :\n\nMODEL\\_DIR : path to where you model checkpoints will be saved.  \nCONFIG\\_FILE\\_PATH : path to the object detection configuration file.\n\nThe other commands are for choosing the right setup (python version, server region, \u2026) and hardware (what kind of machine, how many GPUs, what type of GPUs) for your training. \n\n Follow me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!*** \n\n&amp;#x200B;\n\nhttps://preview.redd.it/t1ggbssvu0o71.png?width=1514&amp;format=png&amp;auto=webp&amp;s=fe215eb3839256741ac7a0c8d04570248ad34850", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ppw1z0/your_daily_dose_of_machine_learning_object/"}, {"autor": "sustainabledude", "date": "2021-09-16 09:35:06", "content": "Goldspot Discoveries: Integrating Big Data &amp; Artificial Intelligence in the Mineral Exploration Game /!/  \n\n# \n\nhttps://preview.redd.it/bxhbe8ik4un71.png?width=605&amp;format=png&amp;auto=webp&amp;s=b721a9fe3da81525dccb764e15ab9af26c520145\n\nGeneral Information\n\n(as of 14 September 2021)\n\nMarket Cap (intraday) 111.75M\n\nEnterprise Value 60.42M\n\nTrailing P/E 3.50\n\nPrice/Book (mrq) 1.92\n\nEnterprise Value/Revenue 7.97\n\nShares Outstanding 121.47M\n\n% Held by Insiders 54.21%\n\nIntroduction\n\nMining exploration is undergoing a revolution; only the mining industry hasn\u2019t realized it yet. The insurgents leading the revolution come from Goldspot Discoveries, a small Canadian technology company that is bringing AI/Machine Learning into the traditional geosciences field that has long dominated the exploration game. Still an early-stage company, Goldspot remains undervalued even as it has consistently outperformed expectations while delivering outstanding revenue growth and profitability. The company is well capitalized and has built a huge head start on any future competition that will take several years (at best) to close. While all of these factors would be a great story for any company, they aren\u2019t even the best part of the Goldspot story. Just like the group of MIT students used data analysis methodologies to stick it to the blackjack casinos (described in the book *Bringing Down the House* and the -----> film !!!  *21*), Goldspot is cracking the code for junior mining investment in order to rewrite the odds of the game in their favor. To put it simply, this is the moment where big data meets small miners \u2013 and one of the first hands Goldspot has played (New Found Gold - $NFG) has already given the house a good shake.\n\nCompany Overview\n\nGoldspot Discoveries started as a group of data scientists out of Quebec that competed in a mining technology challenge. Their approach to improving mining exploration was to utilize advanced data analysis techniques to improve the probability of drilling success. This group of scientist were connected with Denis Laviolette and Cejay Kim and together they turned this new technical approach into a functioning geoscience consulting business. Goldspot was officially formed in 2016, and then went public via a reverse merger in early 2019. For the last several fiscal quarters, Goldspot has seen revenues skyrocket (see charts in later sections), while building an investment portfolio that would make many far larger companies green with envy. Goldspot has been rewarded for its strong operational and financial performance with 8-fold gains in its stock price in the last 12 months (achieved on consistent and sustainable price gains). Looking forward, Goldspot has put teasers into the market of several new technologies, products in the r&amp;d pipeline, and strategic partnerships that should deliver strong shareholder value into the future.\n\nGoldspot is a technology and consultancy company that specializes in applying machine learning &amp; artificial intelligence to collect, interpret and convert data into so-called 'Smart Targets' that can pinpoint the exact location of potential new resource reserves.\n\nContrary to what the company\u2019s name would suggest, Goldspot specializes in finding almost any commodity reserve. Many of Goldspot's current clients are gold miners and silver miners, but there is also significant demand from the lithium, copper and base metal industry for Goldspot's services.\n\nThe search for new mineral reserves is not a new phenomenon. But with a dwindling number of new discoveries, the global supply chain is facing a decreasing supply and a rising demand. With fewer, smaller and more expensive discoveries, the mining sector is facing a problematic issue. Goldspot is tactically responding to this with the introduction of deep learning algorithms that can highlight and identify new innovative mathematical solutions and interpretations on geological patterns. Deep learning makes it possible to analyze an enormous amount of data that is virtually impossible for the human brain to analyze and interpret properly.\n\nThe new technologies have enabled Goldspot to produce results that were not possible before. This significantly increases the reliability &amp; accuracy of the targets. In addition to the extreme accuracy of Goldspot's results, Goldspot saves its clients a lot of time and money by letting software do the bulk of the work. By reducing costly human labor and time while improving results Goldspot has set up a compelling alternative to traditional methods and has proven to be a very attractive solution for mining companies.\n\nGoldspot's Business Model\n\n**Summary**\n\nOne of best way to analyze Goldspot\u2019s business is to look at it as a car. The AI/Machine Learning (discussed in detail below) is the engine and the rest of the business model is the vehicle that is built around that engine. At its core, Goldspot is a technology company. Upwards of 2/3rds of the its employees work on the technical side of the business \u2013 mostly out of its location in Montreal. Quebec is a highly underrated hotspot of engineering and technical talent, with several top universities nearby turning out top-notch talent.\n\nGoldspot has brought onboard well over 20 PhD\u2019s in diverse but related fields such as structural geology, geochemistry, geophysics, and complex data science and so forth. The beauty of Goldspot is its ability to integrate the results of these various fields into a simplified target analysis. Once its geoscience tools are built, Goldspot can then apply it on a wide-scale to create a competitive advantage for its clients.\n\nGoldspot's business model currently consists of 4 segments:\n\n1. Consultancy Services (Smart target generation)\n2. Investments\n3. Software as a Service (SaaS)\n4. Exploration (Golden Planet Mining)\n\nConsultancy Services\n\nGoldspot is advising mining entities for target generation to find new potential reserves with the aforementioned 'Smart Targets' that have been derived from the in-house AI &amp; machine learning algorithms. Goldspot concludes contracts with its clients that cover the costs incurred and generate an decent operating profit for Goldspot that are in line with the average margins in the consultancy sector.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/wx60iqfl4un71.png?width=605&amp;format=png&amp;auto=webp&amp;s=8dd00fd476e0a1779c3e9886141405a5e8872167\n\n*Figure 1: Growth of Goldspot's Consultancy revenue*\n\nThe Consultancy turnover is growing by approximately 100% on an annual basis since Goldspot\u2019s inception and consists of time waged contracts with mining corporations. The mining corporations can be subdivided into two categories: Junior miners (small mining companies that do not yet have an operational mine, but do have a piece of land, a so called claim) and Majors (mining companies that already have at least one operational mine).\n\nGoldspot\u2019s Consultancy services are based on the in-house processing of historical and/or public big data sets of the client\u2019s claim. In combination with the use of Goldspot\u2019s AI and machine learning algorithms the company is able to extract correlations &amp; recognize patterns in the big data sets that provide sustained conclusions about a client\u2019s claim.\n\nGoldspot recently acquired the full-service field exploration firm \u2018Ridgeline Exploration\u2019 to provide a full-service solution to solve common inconsistent and piecemeal data collection practices that increase risk and lower efficiency across the industry. With the acquisition of Ridgeline, GoldSpot aims to strengthen its existing consultancy business as well as vertically integrate the ability to capture key data, including airborne geophysical survey mapping, geochemistry, structural mapping and geophysics. By improving the reliability of the big data sets Goldspot works with, the accuracy of the generated Smart Targets will considerably improve.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/7j2t4w1m4un71.png?width=605&amp;format=png&amp;auto=webp&amp;s=e9c8b3e1e91bd0b2d98a7e025fb58702530bafa6\n\n*Figure 2 Vertical Integration using Ridgeline Exploration Services*\n\nGoldspot has provided consultancy services for big names in the sector like Yamana Gold, Fortuna Silver Mines, Sprott Mining and Vale to name a few, as well as countless Junior Miners among which are the names of New Found Gold, Critical Elements Lithium Corporation and Nevada King Gold Corp.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/w57k8ckm4un71.png?width=605&amp;format=png&amp;auto=webp&amp;s=8ceb8cbd93408a1950fcb9cd89a6335c7cd9c110\n\nInvestments\n\nBut that is only the first part of the equation. By being able to evaluate large swaths of data over a vast geographical region Goldspot has an edge in being able to pick the potential winners and losers among junior mining exploration companies before everyone else. This is like having the cheat codes to the mining investment game.\n\nNow it is often so that Junior miners do not have enough working capital to engage Goldspot on their project. They are short on cash and cannot meet the requested rates from Goldspot. Goldspot has devised a smart solution for this particular problem. Before entering into a negotiation with the Junior miner, Goldspot analyzes the claim (territory) of the Junior miner using its in-house software &amp; deep learning algorithms and determines whether there is a significant probability that there is a promising reserve to be found on the claim based on their initial analysis. If that probability looks promising, Goldspot offers to take equity (shares and/or warrants) in the Junior Miner. They participate in a 'Private Placement' which allows the Junior Miner to raise money by issuing shares and/or warrants. As a result the Junior Miner can pay for Goldspot's consultancy services with the capital they raised. Every now and then Goldspot also negotiates for an NSR as part of a deal. An NSR, an abbreviation for \u2018Net Smelter Return\u2019, is a royalty on the net gold quantity a mine produces. These NSR\u2019s are often set around 0.5%-2%.\n\nBy applying this method, Goldspot has in addition to a consultancy contract, acquired an interest in the Junior miner. This creates a significant synergy when Goldspot delivers successful 'Smart Targets' and when these targets are also successfully tapped into. As a result the shares of the Junior Miner will increase significantly in value with Goldspot having a stake in the company.\n\nE.g. One of Goldspot's first clients was the junior miner New Found Gold. Goldspot had at the time entered into a contract with New Found Gold and acquired 1.7M shares and an NSR of 0.5% on the claim using the aforementioned method. At that time, the price of one NFG share was $0.40. After delivering the \"Smart Targets\" and successful drilling, which continues to this day, the stock has now increased in value by 30 times at a price of $12 per share. As a result, Goldspot's initial investment of $680,000 has appreciated to $20.4M, delivering an unrealized profit of $19.72M. The exact worth of the 0.5% NSR Goldspot acquired on NFG\u2019s claim is difficult to estimate until an exact resource estimation and a PFS (Pre-Feasibility Study) and PEA (Pre-Economic Study). Very likely though NFG's 0.5% NSR is worth something in the \"tens\" of millions but that is still very speculative. Now, certainly not every Junior Miner will be a success story like New Found Gold. However Goldspot\u2019s approach on investing in Junior Miners significantly de-risks their Junior Miner portfolio in comparison to other Junior Miner ETF\u2019s. Further elaboration on this in the next paragraph.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/uf1u12an4un71.png?width=598&amp;format=png&amp;auto=webp&amp;s=53353acba9ea79ae7e24a95406ea2e9c24efe2d3\n\n*Figure 3: New Found Gold equity position when Goldspot entered the agreement in 2019 and in 2021 after Goldspot successfully delivered numerous drill targets that where successfully tapped into.*\n\nGoldspot is very selective on what Junior Miners the company decides to engage with. To create the highest chance of success they\u2019ve established a data based approach on finding the best deals on the market. They have previously referred to this as \u2018Resource Quantamental\u2019 and they described it as an AI-driven opportunity generator pointing Goldspot to the ideal companies to work with. Resource Quantamental is the largest aggregate of mining data in the world and makes use of all the big data sets available on the capital markets, management compatibility and historical drilling results as well as geologic data. The output serves Goldspot with the ideal companies that have the highest chance of success when Goldspot engages with them.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/56sshevn4un71.png?width=605&amp;format=png&amp;auto=webp&amp;s=9e632142a7931eba9b2969cc156b160ff1c765ca\n\n*Figure 4: Investment Performance of Goldspot\u2019s portfolio compared to the GDXJ (VanEck Vectors Junior Gold Miners ETF) and the TSX.*\n\n&amp;#x200B;\n\nhttps://preview.redd.it/o7dhgi0p4un71.png?width=605&amp;format=png&amp;auto=webp&amp;s=8ebd228c4515f203f8e8a22453c4f96813206839\n\n*Figure 6: Selection of Goldspot\u2019s equity positions in Junior Miners &amp; Royalty Positions. Note: This is not the whole portfolio only a few selected positions. The whole portfolio has not been made public (yet) due to competitive reasons as mentioned by the company.*\n\nSoftware as a Service\n\nTo grow Goldspot\u2019s consultancy revenue the current team is becoming increasingly more efficient as they develop better skills &amp; internal work processes mature. Furthermore, the company is actively hiring more staff. However, this will pose a problem in the long run. Consultancy business is very employee intensive, and unlike other sectors, there is a great scarcity of people with degrees with the combination of Geology &amp; Data Science. For Goldspot however, people with these profiles are a necessity.\n\nTo overcome this problem and make the in-house technology scalable and lucrative, Goldspot has decided to develop &amp; acquire mining related SaaS products. Work is currently underway for the launch of two highly technical SaaS products that are expected to launch in Q4 2021.\n\nOne of those two new SaaS products is *LithoLens*, Goldspot\u2019s core imaging technology which adds value by extracting geological information from otherwise unused core photography, providing brand new data for 3D modelling and exploration purposes. Core logging is an essential step in the exploration process. It is the manual systematic recording and measuring of information from drill core to determine the lithology, mineralogy, geological history, structure and alteration zones. Due to being a highly manual repetitive task, core logging is subject to human error and rising costs. Litholens can work with imagery from historic and recent drill core photographs, to downhole optical and acoustic televiewer files, to videos of seafloor nodule deposits used in deep-sea mining. The data inserted in Litholens is processed in the cloud and the machine learning creates large and valuable new datasets from underutilized imagery and video data. The total addresable market for Litholens \u2013 the so called \u2018core logging market\u2019 currently has a turnover of around $50M per year, but Litholens ability to relog historical imagery could open up a opportunity closer to $500M.\n\nLithoLens is a software solution that once available to the mining sector, has the potential to provide Goldspot with an additional revenue stream that, unlike the majority of consulting services, is reoccurring in nature. As a result of this potential for significant recurring revenue, the LithoLens technology is a focal point of Goldspot\u2019s future growth.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/1p7u3wsp4un71.png?width=194&amp;format=png&amp;auto=webp&amp;s=c386e86d3f584d9ba94802ad8e5691fadda26df7\n\ncore logging\n\n&amp;#x200B;\n\nhttps://preview.redd.it/0sltumfq4un71.png?width=326&amp;format=png&amp;auto=webp&amp;s=0149a9d948ce7f5e991b7281d4042ac58dca83ce\n\ncore logging analytics\n\nGoldspot recently enhanced its Litholens development by acquiring the tech company Geotic. Geotic\u2019s portfolio of 5 software tools (*GeoticMine, GeoticLog, GeoticGraph, GeoticCAD and GeoticField*) offer diversified 3D modelling and core-logging, improving the way that Geologists and Engineers collect and analyze data. The powerful combination of *GeoticLog* and *LithoLens* will create an industry-first core logging and AI imaging cloud solution.\n\nThe second SaaS product scheduled for launch in Q4 2021 is called \u2018MinusOne\u2019 and is a software solution for creating 3-D models from geophysical data processed using deterministic and stochastic inversion methods. With this technology, Goldspot's team also applies a ML (machine learning) method to provide probabilistic framework that helps to analyze uncertainty. The processed results guide exploration drilling locations and geology interpretation.\n\nGoldspot has now over 40 research and development products currently in development and is now at the point where many of these tools can be monetized on a much larger scale as external software solutions.\n\nCEO.CA acquisition \u2013 the next birthplace for Junior Miners &amp; mining companies\n\nOn the 16th August 2021 Goldspot announced their intent to acquire the social investor network CEO.CA. CEO.CA is an international investor website with the focus on Canadian listed companies. Canada is the global hub for mining entities to seek financing and there are thousands of global mining companies listed on the TSX-V exchange (Toronto Stock Exchange).\n\n\"GoldSpot's focus is to unlock value in mineral exploration with data science and machine learning and we are proud to serve more than fifty global exploration companies in this endeavour. The acquisition of CEO.CA, and the establishment of a technology and media division, is strategic to that vision and provides significant economic potential to our clients and shareholders,\" commented GoldSpot Executive Chairman &amp; President Denis Laviolette.\n\nGolden Planet Mining\n\nAside from taking equity in third parties, Goldspot has decided to leverage their own tech &amp; brand awareness to set up their own exploration company: Golden Planet Mining (GPM). Formerly known as XCorp AI they merged with Saskatchewan Gold Corp. to acquire the \u2018Mammoth Gold Project\u2019 situated in Saskatchewan, Canada, and form the combined entity Golden Planet Mining. Originally Goldspot invested $475K cad in XCorp AI. Their equity in the combined entity GPM appreciated to $7.78M cad following the next raise, and has since ballooned to $15,53M in the most recent raise to date. At the moment Goldspot holds 28% of Golden Planet Mining with the company looking to go public in late 2021 or early 2022. Following the merger with Saskatchewan Gold Corp, Golden planet Mining has acquired ownership of the Olympus Gold Project in the Northwest Territories and the Rider Gold Project in British Columbia and commenced drilling activities &amp; regional exploration on the properties. Using Goldspot\u2019s in-house exploration and data processing tech the claims are being further explored and drilled for more data. If good results appear GPM has a good probability of becoming a major success story for Goldspot.\n\nHow AI and Machine Learning Creates a Competitive Advantage\n\nGood companies sell a quality product, great companies solve a problem, but the very best companies solve a problem before you even know you have one. The metal/non-metal mining industry has a problem it is either unaware of, or refuses to admit. The problem: the exploration industry is unable to find new big deposits that can be economically extracted without undue environmental damage in order to replace depleted reserves of current major mining companies.\n\nExploration by junior mining is one of the great perpetual boom and bust industries. Every year, fortunes are made and lost betting on junior mining companies on the TSX Venture Exchange. Junior miners are typically smaller companies (most with a market cap well under $50 million) that secure land and mining claims in hopes of finding the next big mineral find. When a company makes a big find, the rewards are incredibly lucrative, that can in the tens of thousands of percent gains big. For years, the market unduly rewarded junior miners who found massive mineral deposits. Seemingly, the only metric that mattered was coming up with a big P&amp;P reserve in the tens of millions of ounces.\n\nBut the sad reality is that an overwhelming majority of junior mining companies fail well before mining operations can even begin on these massive mineral finds. At any given time, there are 3,000+ active junior miners trading on the TSX-V exchange. Although most junior miners experience major shifts up and down in share price, it is very rare to get a big win long-term gainer among the junior mining companies. When long-term wins do happen, it is like lightning in a bottle \u2013 with sufficient gains to make up for a large number of losses. The key to being a long-term winner is cost of extraction, not volume. For example, an operation with 2 million ounces of gold that be extracted at an AISC cost of $350 is far more attractive of an acquisition target for a major mining company than an operation with 30 million ounces of gold that can be extracted for $1,100 an ounce.\n\nSo why are so many junior mining companies fail? It isn\u2019t necessarily that the companies have bad properties or don\u2019t find valuable minerals (although that is possible). The reality is that junior mining companies typically fail because they simply run out of money. In order to develop a mining project all the way to production (or acquisition by a major), the junior mining company needs to find the right deposit, with the right economics, with the right timing and all within its limited financial resources. If the company has money to institute a drilling program, the junior mining company usually only has one chance to get it right and obtain a good drill result. If it does, stock price goes up and the junior miner is able to an equity raise sufficient to move on to the next stage of the process. Any faltering along the way, and the entire project (and usually the company itself) goes up in flames. Truly, it creates a \u201cdiscover or die\u201d environment. Additionally, that discover needs to be in a location and grade that suggests that it can be removed economically.\n\nWhile this environment of \u201cdiscover or die\u201d has always existed for junior miners, there are two more recent industry trends/conditions that are making it even tougher for juniors to succeed. First, mineral finds are become harder and harder to locate. The minerals being found are greater depths or in environments with less surface indicators. This means that there are less easy finds available waiting to be discovered. The second trend is for junior miners to pursue \u201cdistrict-scale\u201d projects. Investors in junior miners are generally looking for companies with huge land holdings so that if there is a find, the potential payouts can be exponentially greater. At the same time, most mining laws around the world require a claim holder to invest a certain amount of money on an annual basis in developing a claim as well as pay a claim fee. These costs are not trivial and can total several million dollars on an annual basis for junior miners with large land packages. So with these additional factors, junior miners are having to look for a smaller needle in a bigger haystack that costs more money to maintain.\n\nIn this environment smaller needles in bigger haystacks, Goldspot is solving the budding problem by building a better mouse trap. With diminishing surface indicators, mining exploration companies need to effectively integrate data obtained from multiple sources. Each time you stack a different level of data on top of another set of data, the complexity does not increase linearly, rather it increases exponentially. The beauty of AI and machine learning tools is that they are able look at data from a potentially limitless pool of data and find subtle geological, geophysical or other structural indicators that collectively increase the likelihood for success. The Goldspot AI tool finds regions or structures where probability of mineralization are higher, but once a general region has been identified, the same tools are also able to narrow in on specific targets to confirm mineralization on a claim that provide a substantially higher likelihood of finding mineralization.\n\nFinancials\n\n&amp;#x200B;\n\nhttps://preview.redd.it/l27nz09r4un71.png?width=633&amp;format=png&amp;auto=webp&amp;s=9718aeb3b1cb702c67146a0dcbc6a03b6d896228\n\nRisks to the business\n\nEach business carries risks along with their operations. The main risks to Goldspot\u2019s business model are listed below:\n\n**Cash Flows From Consulting Income**\n\nGoldspot currently generates revenue and cash flows from its consulting services. The availability of these sources of funds and Goldspot's ability to maintain a network and attract additional customers will depend on a number of factors, many of which are outside of Goldspot's control. A significant portion of Goldspot's revenues have come from four customers in short-term contracts. Goldspot's contracts are generally short-term and it is actively seeking to diversify its customer base with longer-term contracts. The loss of any one of its customers or the inability to attract additional customers will result in a material adverse effect on the business and may adversely affect revenues going forward.\n\n**Intellectual Property Risk**\n\nThe ownership and protection of Goldspot's intellectual property rights is a significant aspect of Goldspot's future success. Currently, Goldspot relies on trade secrets, technical know-how and proprietary information to protect its intellectual property. Goldspot also attempts to protects its intellectual property by entering into confidentiality agreements with parties that have access to it, such as business partners, collaborators, employees and consultants. Any of these parties may breach these agreements and Goldspot may not have adequate remedies for any specific breach. In addition, Goldspot's trade secrets and technical know-how, which are not protected by patents, may otherwise become known to or be independently developed by competitors, in which event Goldspot's business, financial condition and results of operation could be materially affected. An assertion by a third-party that Goldspot is infringing its intellectual property could subject Goldspot to costly and time-consuming litigation, which could harm its business. Goldspot's success depends in part upon it not infringing the intellectual property rights of others. However, Goldspot's competitors, as well as a number of other entities and individuals, may own or claim to own intellectual property relating to Goldspot's industry or, in some cases, its technology. Goldspot has not been subjected to any claims of intellectual property rights infringements in the past but as it develops more of its own applications and meets additional client specific requests, it may be exposed to greater risk in the future. Any claims or litigation could cause Goldspot to incur significant expenses, and if successfully asserted against Goldspot, could require that Goldspot pay substantial damages or ongoing revenue share payments, indemnify its customers or distributors, obtain licenses, modify products, or refund fees, any of which would deplete Goldspot's resources and adversely impact its business.\n\n**Investment Risks**\n\nThrough its investing division, Goldspot may acquire securities of public and private companies from time to time, which are primarily junior or small-cap companies. Poor investment performance could impair revenues and growth. The market values of the securities can experience significant fluctuations in the short and long term due to factors beyond Goldspot's control. Market value can be reflective of the actual or anticipated operating results of the companies and/or the general market conditions in a specific sector as a whole, such as fluctuations in commodity prices and global political and economic conditions. Goldspot's investments will be carried at fair value, and unrealized gains/losses on the securities and realized losses on the securities sold could have a material adverse impact on Goldspot's operating results. There is no assurance that Goldspot will be able to achieve or maintain any particular level of investment return, which may have a material adverse impact on its ability to attract investors. Furthermore, the junior mining space tends to be more volatile than the general market indices. This volatility combined with negative or poor performance could combine to lead to a reduction in investor interest.\n\n**Cyclical Downturn**\n\nA significant operating risk affecting Goldspot is a downturn in demand for its services due a decrease in activity in the mining industry. A severe and persistent downturn in the mining industry would have severe consequences on the business of Goldspot. In many cases, capital markets are the only source of funds available to junior mining companies and any change in the outlook for the sector or the lack of success of a specific exploration program can quickly impair the ability of these juniors to raise capital to pay for their consulting services.\n\n**Limited Operating History**\n\nGoldspot began carrying on business in 2017 and started to generate significant revenue from its operations beginning in December 31, 2018. Goldspot is therefore subject to many of the risks common to early stage enterprises, including under-capitalization, cash shortages, limitations with respect to personnel, financial, and other resources and limited revenues. There is no assurance that Goldspot will be successful in achieving a return on shareholders' investment and the likelihood of success must be considered in light of the early stage of operations.\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\n\"The chairman of Barrick Gold Corp. made a bold prediction late last year: With the help of artificial intelligence and other digital tools, the world\u2019s largest gold miner would become a technology company that just happened to be in mining.\"\n\n**If you have a background in software analytics / AI / deep and/or machine learning feel free to contact Goldspot for career opportunities:** [**https://goldspot.ca/contact-us/**](https://goldspot.ca/contact-us/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ppa0ho/goldspot_discoveries_integrating_big_data/"}, {"autor": "Successful_Boat_3099", "date": "2021-09-16 05:51:00", "content": "\ud83d\udc8aYour daily dose of machine learning : deploying your deep learning models on google cloud /!/ &gt;This is a series of posts that I post almost daily. I call them \u201cyour daily dose of machine learning\u201d. \n\n How do you deploy your deep learning model on google cloud?\n\nWhen you\u2019re done training a model, the deployment part starts.\n\nA lot of companies deploy their models on the cloud because it\u2019s easier to scale that way.\n\nIn google cloud there is a service called \u201cCloud Run\u201d which you can use to deploy your deep learning models. How does it work?\n\n\\- You code a REST API or a simple web app using Flask or FastAPI. Your code will be responsible for loading the model and making predictions using a query that comes from users.\n\n\\- You then containerize your API or Web app using docker.\n\n\\- You push your docker -----> image !!!  to google container registry (GCR).\n\n\\- You then deploy your docker image using Google Cloud Run. The command for deploying a docker image that\u2019s accessible from GCR is shown in the image below.\n\n \n\nFollow me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!*** \n\n&amp;#x200B;\n\nhttps://preview.redd.it/10dlmver0tn71.png?width=1346&amp;format=png&amp;auto=webp&amp;s=437d4a668f8b73d18a156b44fe2682644e559c0c", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pp7gn0/your_daily_dose_of_machine_learning_deploying/"}, {"autor": "_4lexander_", "date": "2021-09-25 13:28:55", "content": "How to handle OOV typos with a pretrained BERT /!/ Computer vision person here. I have a task which involves an OCR + NLP pipeline applied to an -----> image !!! . Obviously with OCR it's not always guaranteed that you'll get the exact content from the image. You might get misspelled words, extra spaces, missing spaces, etc.\n\nWhat are some of the common strategies in NLP for dealing with this without throwing away good data (without blindly using an OOV token for any word which isn't spelled exactly correctly)?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pv6rno/how_to_handle_oov_typos_with_a_pretrained_bert/"}, {"autor": "El_3arto", "date": "2021-09-24 17:45:11", "content": "Find patterns in gibberish text. /!/ I'm fairly new to machine learning. I've made a handfull of categorizing solutions, but I can't get my head around a problem I'm trying to solve at the moment. I've got some text via OCR of an -----> image !!!  of a type plate of a device the text is mostly gibberish. I want to extract the serial number from this text. My initial aproach is to create a mask for all characters with a zero for the characters not belonging to the serial and a 1 for the characters belonging to the serial. But I'm stuck on how to build a training model for this. Has anybody any suggestions to push me in the right direction or am I on a dead end here with my approach?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/puoum2/find_patterns_in_gibberish_text/"}, {"autor": "Mr_P07", "date": "2021-09-24 15:02:54", "content": "tesseract OCR functioning with direct video feed /!/ Good day fellow redditors\nI have a question over tesseract OCR functioning, how can I make it work from a direct -----> camera !!!  feed, the issue is it's usually used to recognize text from image I have heard of pytesseract.image_to_string, but don't truly know how it works.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pulnh7/tesseract_ocr_functioning_with_direct_video_feed/"}, {"autor": "thedeepreader", "date": "2021-09-24 08:08:31", "content": "[D] (Paper Overview) Pix2Seq: A Language Modeling Framework for Object Detection /!/  \n\n**Video**\n\n[https://youtu.be/6Ptp0LfeN6g](https://youtu.be/6Ptp0LfeN6g)\n\n**Paper**\n\n[https://arxiv.org/abs/2109.10852](https://arxiv.org/abs/2109.10852)\n\n**Abstract** \n\nThis paper presents Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we simply cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural net to perceive the -----> image !!!  and generate the desired sequence. Our approach is based mainly on the intuition that if a neural net knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/puf97p/d_paper_overview_pix2seq_a_language_modeling/"}, {"autor": "vvinvardhan", "date": "2021-09-24 07:49:15", "content": "What is the easiest way I can do reverse -----> image !!!  search with python? /!/ I am not an expert and I need to finish this fast! Please let me know what you recommend!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/puf0wg/what_is_the_easiest_way_i_can_do_reverse_image/"}, {"autor": "tony_stark_9000", "date": "2021-09-24 07:22:41", "content": "Learning roadmap /!/ Hey guys. I am self taught python programmer. I have completed a Mooc from simplilearn with Ai/ ml certification.\n\nCan someone suggest me the roadmap to learning from this point on.\n\n- have knowledge of feature scaling transforms- selection, eliminations etc\n\n-know algorithms and finetuning params for optimisation\n\n- DL for -----> image !!!  recog, gensim for nlp. Tensorflow base not pytorch ( need to explore more in that)\n\n- engineering graduate so know the working mathematics intuitively but need to get to depth though.\n\nWorking in a bio informatics company which deals with genetic sequencing and classification. \n\nWhat i have in mind that i need to learn - GA and optimisation algos- ant colony, linear opti etc\n \nAlso need to learn SQL .\n\nCan someone suggest a roadmap to pursue ML journey from here on", "link": "https://www.reddit.com/r/learnmachinelearning/comments/puepo6/learning_roadmap/"}, {"autor": "sanju_3108", "date": "2021-09-24 03:44:19", "content": "how to check effectiveness of test preparation course. /!/  what are steps involved in to check how effective is the test preparation course.\n\nbelow is my data set -----> image !!! .\n\n&amp;#x200B;\n\nhttps://preview.redd.it/9anwzstfhdp71.jpg?width=1908&amp;format=pjpg&amp;auto=webp&amp;s=02d9cc964e1cda630ccf67eb09444f4e3af297c7", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pubnqp/how_to_check_effectiveness_of_test_preparation/"}, {"autor": "anarchy_witch", "date": "2021-09-23 22:49:58", "content": "Questions about STYLEGAN architecture /!/ So, I've been trying to understand the [paper about the STYLEGAN](https://arxiv.org/pdf/1812.04948.pdf). I have a few problems with it:\n\n1) What are the dimensions of each layer? I know that the network is fed 4x4x512 constant -----> image !!!  at the beginning, but do all layers have the same \\*depth\\*? It wouldn't make sense to have a 256x256x512 image in your network, would it?\n\n2) if I understand correctly, the feature map is a one *slice* of *x*? Thus, 4x4x512 means 512 feature maps, and 8x8x256 means 256 feature maps?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pu6plq/questions_about_stylegan_architecture/"}, {"autor": "grid_world", "date": "2021-09-23 02:50:21", "content": "Removing TensorFlow filters /!/ With Python 3.8 and TensorFlow 2.5, my objective is to remove filters/kernels having lowest L2 norms. Sample code for this is:\n\n        # Generate random 1 -----> image !!! /data point sample-\n        x = tf.random.normal(shape = (1, 5, 5, 3), mean = 1.0, stddev = 0.5)\n    \n        x.shape\n        # TensorShape([1, 5, 5, 3])\n        \n        # Create conv layer-\n        conv = Conv2D(\n                filters = 3, kernel_size = (3, 3),\n                activation='relu',\n                kernel_initializer = tf.initializers.GlorotNormal(),\n                bias_initializer = tf.ones_initializer,\n                strides = (1, 1), padding = 'same',\n                )\n        \n        # Pass input through conv layer-\n        out = conv(x)\n        \n        out.shape\n        # TensorShape([1, 5, 5, 3])\n        \n        out = tf.squeeze(out)\n        \n        out.shape\n        # TensorShape([5, 5, 3])\n\nAccording to my understanding, the output consists of three (5, 5) matrices stacked together. However, printing 'out' shows five (5, 3) matrices stacked together:\n\n        out.numpy()\n        '''\n        array([[[1.45877   , 0.        , 1.9293344 ],\n                [0.9910869 , 0.01100129, 1.7364411 ],\n                [1.8199034 , 0.        , 1.3457474 ],\n                [1.219409  , 0.22021294, 0.62214017],\n                [0.5572515 , 0.7246016 , 0.6772853 ]],\n        \n               [[1.161148  , 0.        , 2.0277915 ],\n                [0.38071448, 0.        , 2.2438798 ],\n                [2.2897398 , 0.1658966 , 2.3147004 ],\n                [1.2516301 , 0.14660472, 1.6381929 ],\n                [1.1554463 , 0.72516847, 1.6170584 ]],\n        \n               [[0.        , 0.        , 1.2525308 ],\n                [0.4337383 , 0.        , 0.91200435],\n                [0.71451795, 0.        , 2.093022  ],\n                [2.265062  , 0.        , 2.7562256 ],\n                [0.82517993, 0.        , 1.8439718 ]],\n        \n               [[0.7089497 , 0.        , 1.041831  ],\n                [0.        , 0.        , 1.2754116 ],\n                [0.41919613, 0.        , 0.88135654],\n                [0.        , 0.        , 0.71492153],\n                [0.18725157, 0.27108306, 0.11248505]],\n        \n               [[0.86042166, 0.45840383, 1.084069  ],\n                [0.53202367, 0.42414713, 1.2529668 ],\n                [1.2257886 , 0.31592917, 1.3377004 ],\n                [0.36588144, 0.        , 0.6085663 ],\n                [0.3065148 , 0.574654  , 1.0214479 ]]], dtype=float32)\n        '''\n\nSo, if I use the code *out\\[:, :, 0\\]*, *out\\[:, :, 1\\]* &amp; *out\\[:, :, 2\\]*, do they refer to the first, second and third filters?\n\nAnd if yes, is computing L2-norm using:\n\n        tf.norm(out, ord = 'euclidean', axis = (0, 1)).numpy()\n        # array([5.275869 , 1.4290226, 7.545658 ], dtype=float32)\n\nthe correct way?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ptlyj5/removing_tensorflow_filters/"}, {"autor": "SkillupGenie", "date": "2021-09-06 05:49:02", "content": "-----> Image !!!  Compression using Sklearn - Principal Component Analysis. It compressed colored imaged without reducing its shape. /!/ [https://youtu.be/L2oyN8-j6s4](https://youtu.be/L2oyN8-j6s4)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pitdlp/image_compression_using_sklearn_principal/"}, {"autor": "maybenexttime82", "date": "2021-09-05 17:48:43", "content": "Automated Image Captioning with ConvNets and RNNs /!/ I'm currently watching talk about -----> Image !!!  Captioning by Andrey Karpathy and he mentions in a video concept of 'sampling' whilst explaining RNNs but I don't understand what he means by that?\n\nVideo:\nhttps://youtu.be/xKt21ucdBY0\n\nat time 37:51.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pihp43/automated_image_captioning_with_convnets_and_rnns/"}, {"autor": "saksham3939", "date": "2021-09-05 11:26:41", "content": "What are the icons used in the banner of this subreddit? /!/ [The Banner \\(at least as of now\\)](https://preview.redd.it/iro4bwq86ol71.png?width=2560&amp;format=png&amp;auto=webp&amp;s=62245cc3abb448c21ed486da8b811e55a0bd80be)\n\nHey everyone,  \nI was wondering what are the icons used in the banner of this subreddit....  \nI know some of them..... I will list them below  \nUpon Reverse -----> image !!!  searching through google didn't yield anything useful...  \nWhile searching I stumbled upon an year old similar post ([here](https://www.reddit.com/r/learnmachinelearning/comments/gholpe/what_are_the_icons_placed_in_the_banner_of_this/?utm_source=share&amp;utm_medium=web2x&amp;context=3)) but no one ever replied to it (that's what probably going to happen to this post as well)\n\nMy guesses from left to right-  \n\n\n1. IDK\n2. IDK\n3. IDK\n4. [PyTorch](https://pytorch.org/)\n5. [TensorFLow](https://www.tensorflow.org/)\n6. [Keras](https://keras.io/)\n7. [NumPy](https://numpy.org/) (not entirely sure)\n8. [Scikit Learn](https://scikit-learn.org/stable/)\n9. IDK", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pibem7/what_are_the_icons_used_in_the_banner_of_this/"}, {"autor": "Djenesis", "date": "2021-09-04 17:58:27", "content": "Can someone make a guide for the Google Collab version of this -----> image !!!  editing AI? /!/ My laptop is in storage so I'm trying to get it to work on mobile. As a creative person, this would be incredible to experiment with but I can't figure out how to upload my own image or even make changes to the example. I tried running each section in order and nothing happens. Help would be very very much appreciated!\n\nCollab link: https://colab.research.google.com/github/orpatashnik/StyleCLIP/blob/main/notebooks/StyleCLIP_global.ipynb#scrollTo=ASawJLXGxteR\n\nVideo where I discovered it: https://youtu.be/RAXrwPskNso", "link": "https://www.reddit.com/r/learnmachinelearning/comments/phwcj1/can_someone_make_a_guide_for_the_google_collab/"}, {"autor": "Karsupnow11", "date": "2021-09-04 16:38:17", "content": "Clustering - Choice of the number of clusters in textual data /!/  \n\nHi there! I have a social science background and I'm doing a text mining project.  \nI'm looking for advice about the choice of the number of topics/clusters when analyzing textual data. In particular, I'm analyzing a dataset of more than 200000 tweets and I'm performing a Latent Dirichlet allocation model on them to find clusters that represent the main topics of the tweets of my dataset. However, I was trying to decide the optimal number of clusters but the results I'm finding in the -----> picture !!!  seem inconsistent.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/xofqiv5blil71.png?width=985&amp;format=png&amp;auto=webp&amp;s=06feb7e115f4d0036bc71a650323b636b0651d59\n\n \n\nI'm struggling with the choice of the number of clusters. So the question is: what number would you choose from the plot?  \nMoreover, do you think there are other ways and/or conventional rules that one can rely on to choose the number of clusters?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/phuvqz/clustering_choice_of_the_number_of_clusters_in/"}, {"autor": "Perturbed_Parakeet", "date": "2021-09-04 06:39:43", "content": "Feature store for -----> image !!!  features", "link": "https://www.reddit.com/r/learnmachinelearning/comments/phmhue/feature_store_for_image_features/"}, {"autor": "ProtoYet", "date": "2021-09-03 03:35:17", "content": "Weird behavior exhibited by an ML library i'm writing /!/ So recently, I took on the challenge of writing my own machine learning library from scratch. It was going really well, and on smaller scales (IE neural networks with less neurons and layers), it seemed to work perfectly. However, when taking on my first real implementation of the library, I ran into a roadblock that I've been stuck at for weeks now.\n\nIn the first 30-40 epochs, the loss decreases at a steady rate, and all seems well. Suddenly, it explodes. I've managed to narrow the issue down to the network completely changing its mind about a prediction. (input previously classified definitively as a 1 would suddenly be definitively classified as a 2 in the next epoch). This only happens when the neural network starts becoming really sure about its predictions. (predictions look like this \\[0.00000001, 0.99999999\\])\n\n**Generalization of what happens:**\n\nLets say there are 4 images in the dataset. In the first few epochs, the network predicts them all randomly.\n\n\\[ 0.45, 0.55\\] ( -----> image !!!  of a 1 )\n\n\\[ 0.6, 0.4\\] ( -----> image !!!  of a 1 )\n\n\\[ 0.5, 0.5 \\] ( -----> image !!!  of a 2 )\n\n\\[ 0.55, 0.45 \\] ( -----> image !!!  of a 2 )\n\nThe neural network would then start to inch towards predicting both correctly.\n\n\\[ 0.75, 0.25  \\] ( image of a 1 )\n\n\\[ 0.7, 0.3 \\] ( image of a 1 )\n\n\\[ 0.15, 0.85 \\] ( image of a 2 )\n\n\\[ 0.2, 0.8 \\] ( image of a 2 )\n\nThe neural network then becomes pretty confident\n\n\\[ 0.95, 0.05  \\] ( image of a 1 )\n\n\\[ 0.96, 0.04  \\] ( image of a 1 )\n\n\\[ 0.06, 0.94 \\] ( image of a 2 )\n\n\\[ 0.03, 0.97 \\] ( image of a 2 )\n\nSuddenly, the network completely flips about some predictions \n\n\\[ 0.999, 0.001  \\] ( image of a 1 )\n\n\\[ 0.001, 0.999  \\] ( image of a 1 ) ( Suddenly becomes very sure it's a 2)\n\n\\[ 0.002, 0.998 \\] ( image of a 2 )\n\n\\[ 0.001, 0.999  \\] ( image of a 2 )\n\nThe implementation of my neural network is just a simple image classifier, which predicts between a 1 and a 2. (Here you can see a sample of training data I provide the network. It reads the image from top to bottom, left to right. white squares represent 0s, gray represents 0.5, and black represents 1)\n\nhttps://preview.redd.it/z8qxe9rae7l71.png?width=298&amp;format=png&amp;auto=webp&amp;s=fceca4d67dd3fe80ebf370cde73b551762b71063\n\nThe neural network has an input layer of 144 neurons (no activation), a hidden layer of 100 neurons (ReLu activation), and an output layer of 2 neurons (Softmax activation). (All layers are densely connected.) The network uses Categorical Cross Entropy for error. The model calculates the gradient and updates the weights every sample (SGD), with a learning rate of 0.001.\n\nDoes anyone have any idea what could be happening? Any help/advice would be appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pgx7rq/weird_behavior_exhibited_by_an_ml_library_im/"}, {"autor": "Successful_Boat_3099", "date": "2021-09-02 11:41:40", "content": "\ud83d\udc8aYour daily dose of machine learning : SimSiam /!/ &gt; This is a series of posts that I post almost daily. I call them \u201cyour daily dose of machine learning\u201d.\n\nSiamese neural networks have so many applications.\n\nIn my previous posts, I only focused on face recognition since it\u2019s a very relatable application.\n\nBut another field where siamese networks are very useful is self-supervised learning (SSL).\n\nIn SSL, siamese networks can learn some powerful representations that you can later transfer to another computer vision task such as object detection or -----> image !!!  segmentation.\n\nTo learn these representations, what you basically do is take an image, augment it randomly to get 2 views, then pass both views through a backbone network.\n\nThe 2 outputs from the previous step are passed through an MLP. This works as a projection operation.\n\nWe then compute a cosine similarity using the outputs of the backbones as well as the outputs of the MLP.\n\nThe goal is for the network to learn to maximize the similarity between 2 views of the same image.\n\n Follow me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!*** \n\n&amp;#x200B;\n\n[SimSiam](https://preview.redd.it/ey4zyx7ku2l71.png?width=1143&amp;format=png&amp;auto=webp&amp;s=82cbd5e05d60a5bb96ede7c08a7bdb13242cdfc3)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pgg5gl/your_daily_dose_of_machine_learning_simsiam/"}, {"autor": "TheFinalOctopus", "date": "2021-02-21 18:23:38", "content": "Building Segmentation with pytorch /!/ I am studying pythorch and neuronal networks. I have seen this project in [kaggle](https://www.kaggle.com/wakamezake/pytorch-segmenting-buildings-in-satellite-images). I undenstard how it works, or I think so. \n\nI have a problem I don't know how to predict and get the mask of an own -----> image !!!  using the trained model. \n\nMaybe, the question is so easy, but i cannot get it.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lp34q7/building_segmentation_with_pytorch/"}, {"autor": "OnlyProggingForFun", "date": "2021-02-21 12:47:47", "content": "Take a -----> picture !!!  from a real-life object, and create a 3D model of it: ShaRF by Google Research", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lowetn/take_a_picture_from_a_reallife_object_and_create/"}, {"autor": "Milleuros", "date": "2021-02-21 00:45:05", "content": "Is this training history symptomatic of problems? /!/ [-----> Image !!!  - training history](https://i.imgur.com/pHWMZT2.png) of a vanilla neural network trained on a binary classification problem.\n\nI notice two things:\n\n* The validation loss is significantly lower than the loss. \n\n* The training converges really fast, with a weird sudden drop at ~10 epochs (while the validation loss shows oscillations before). \n\nI've never really had the chance of discussing with a ML expert so maybe I'm missing something obvious here. Or maybe there isn't anything weird to see at all. I would love your input.\n\n-\n\n*More details:* Binary classification on a dataset with 29 features and order of 1 million observations per class. 4 layers, ReLU on non-output, dropout in-between, Adam optimizer, binary cross-entropy, batch size of 20. Reducing the LR when reaching a plateau (Keras callback). Validation and Training sets are picked from the same general dataset, randomly so.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lol20c/is_this_training_history_symptomatic_of_problems/"}, {"autor": "maad0000", "date": "2021-02-20 17:31:57", "content": "AI-assisted -----> photography !!!  bot Svetlana /!/ Hello folks,  \nI wrote a personal story about how HBO\u2019s Silicon Valley \u201cnot-hot dog\u201d app inspired me to build an AI-assisted photography bot/app in honor of my stoic third-grade gymnastics teacher Svetlana.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/loc1cs/aiassisted_photography_bot_svetlana/"}, {"autor": "veb101", "date": "2021-02-20 16:11:19", "content": "RESIDE Image de-hazing dataset /!/ Hello everyone, I'm trying to work on a project for Image de-hazing. I read about the [RESIDE](https://sites.google.com/view/reside-dehaze-datasets) dataset and wanted to use the Outdoor training set images for the training purpose. After I downloaded the dataset, I expected it to have two folders for Ground truth and hazy -----> image !!! s, but instead, I can only see there are just hazy -----> image !!! s with a different -----> image !!!  like this. I'm not sure how to proceed or how to extract the GT from these. Can anyone help me with this? \n\n&amp;#x200B;\n\nhttps://preview.redd.it/4f8a0ziopni61.png?width=224&amp;format=png&amp;auto=webp&amp;s=2f2b630fa8b2439c00869f6e00e41e8229f8d3fe", "link": "https://www.reddit.com/r/learnmachinelearning/comments/loaa5e/reside_image_dehazing_dataset/"}, {"autor": "OnlyProggingForFun", "date": "2021-02-20 14:03:46", "content": "ShaRF: Take a -----> picture !!!  from a real-life object, and create a 3D model of it", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lo7r5b/sharf_take_a_picture_from_a_reallife_object_and/"}, {"autor": "Laurence-Lin", "date": "2021-02-20 06:25:05", "content": "The -----> image !!!  load from PyTorch DataLoader looks different from my origin -----> image !!!  /!/  \n\nI have an image with size (889, 929) which is grayscale,\n\nI load the image with dataloader:\n\n&amp;#x200B;\n\n    train_val_data = {phase: torchvision.datasets.ImageFolder('{}/{}'.format(data_path, phase), data_transforms[phase]) for phase in ['train', 'val']}\n    dataloaders_dict = {phase: torch.utils.data.DataLoader(train_val_data[phase], \n                                 batch_size = BATCH_SIZE, \n                                 shuffle = True,\n                                 num_workers = 2) for phase in ['train', 'val']}\n\n \n\nThe return image have size (889, 929, 3) and I don't know why, my input image is supposed to be grayscale, and when I display single image it proves me that the shape is actually (889, 929)\n\nI tried to show image receive from dataloader by:\n\n    for input, label in dataloaders_dict['train']:\n  print(input.shape)\n  img1 = input.numpy()[0, :, :, :]\n  img1 = img1.reshape(889, 929, -1)\n  print(img1.shape)\n      plt.imshow(img1)\n  plt.show()\n\nThe return image looks strange as if it repeats:\n\n&amp;#x200B;\n\n*Processing img qf4txhe7tki61...*\n\n&amp;#x200B;\n\n \n\nMy origin image is supposed to look like:\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n[Single image](https://preview.redd.it/og9qx5b9tki61.png?width=335&amp;format=png&amp;auto=webp&amp;s=6c717ef04a3cc292e8747803a9ccf2fd01346a3e)\n\n&amp;#x200B;\n\n \n\nWhy does my image load as grayscale transform to 3 channel and looks strange?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lo127e/the_image_load_from_pytorch_dataloader_looks/"}, {"autor": "black0017", "date": "2021-02-19 17:17:50", "content": "[D] - Introduction to medical -----> image !!!  processing with Python: CT lung and vessel segmentation without labels (code included) /!/ Hi there! Here is a tutorial on CT lung and vessel segmentation WITHOUT labels (code included)  \nLink: [https://theaisummer.com/medical-image-python/](https://theaisummer.com/medical-image-python/?fbclid=IwAR1auTWT-KteFN-KdgFprdP00UAHcmTOEsUTkA9kCL6bFV2uyGPgmtNk0Ng)  \nEnjoy!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lnl6pv/d_introduction_to_medical_image_processing_with/"}, {"autor": "axetobe_ML", "date": "2021-10-24 20:36:22", "content": "(Paid) Early Access + Product Roadmap for -----> Image !!!  Duplicate Remover Web App", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qf0m8i/paid_early_access_product_roadmap_for_image/"}, {"autor": "escapevelocitylabs", "date": "2021-10-24 11:01:36", "content": "Napoleon's army in color using AI (Generative Adversarial Networks) /!/ We used an -----> image !!!  coloring GAN (deoldify) to bring Napoleon's army to the present. The colors are not perfect, but it does a remarkable job:\n\n[https://twitter.com/evelabs/status/1452225383693230082?s=20](https://twitter.com/evelabs/status/1452225383693230082?s=20)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qepyw3/napoleons_army_in_color_using_ai_generative/"}, {"autor": "Edulad", "date": "2021-10-24 10:54:00", "content": "Value error: dtype=float32&gt; has shape (1024, 324), but the saved weight has shape (1024, 8). /!/ Hi so i was using pixel lib library to do instance segmentation of -----> image !!!  , but i get this error, don't know how to resolve it. Please Help\n\n**MY Code:**\n\n`import pixellib`\n\n`from pixellib.instance import instance_segmentation`\n\n&amp;#x200B;\n\n`seg = instance_segmentation()`\n\n&amp;#x200B;\n\n`seg.load_model(\"./mask_rcnn_balloon.h5\")`\n\n`seg.segmentImage(\"../input/facess/8.jpg\", show_bboxes=True, output_image_name=\"output.jpg\")`\n\n&amp;#x200B;\n\n**ValueError:** \n\n**Layer #389 (named \"mrcnn\\_bbox\\_fc\"), weight &lt;tf.Variable 'mrcnn\\_bbox\\_fc\\_4/kernel:0' shape=(1024, 324) dtype=float32&gt; has shape (1024, 324), but the saved weight has shape (1024, 8).**", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qepv4y/value_error_dtypefloat32_has_shape_1024_324_but/"}, {"autor": "DenizenEvil", "date": "2021-10-23 22:46:05", "content": "Improving Accuracy /!/ I am trying to detect circles and triangles in an -----> image !!! . I tried using different methods to detect shapes with OpenCV (old thread about that here: https://www.reddit.com/r/opencv/comments/qda57o/question_shape_detection_with_lots_of_line_noise/), but my best results were using SSD MobileNet V2 FPNLite 320x320.\n\nMy most successful model was trained using 112 training images and 20 test images labeled with labelImg over 258000 steps overnight.\n\nMetric | Area | Result\n---|---|----\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.788\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.942\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.877\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.794\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.317\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.814\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.834\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.834\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n\nI tested the detection results against another set of images. What I *really* care about is the *number of shapes* detected. With a confidence threshold of 0.8, I was getting about 50-55% accuracy on this new data set (this is up from using 10000 steps accuracy at around 35% accuracy). My target is around 75-80% accuracy minimum. I noticed that sometimes shapes obvious to the human eye are totally missed, but shapes that might be hard for a human to pick out are easily found. Overlapping shapes give the most trouble to my model.\n\nI tried training my model using the 640x640 version as well as ResNet 1024x1024 and EfficientDet D4, but the training crashes immediately.\n\nMy code is taken from here: https://github.com/nicknochnack/TFODCourse/blob/main/2.%20Training%20and%20Detection.ipynb\n\nThis is what my sample images look like: https://imgur.com/a/4KKLHcw\n\nHow can I increase accuracy of my results? Would training more images and for longer periods be the best way forward, or would using a different pre-trained model be better? If the latter, what do I need to do to get my data training using a different model without the crashing issues? If more training would be better, what's the best way to label these shapes (e.g. when shapes overlap, is it better to only label the area of the visible shape, or what the shape *would* be if it wasn't overlapped)?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qefd6j/improving_accuracy/"}, {"autor": "SilurianWenlock", "date": "2021-10-23 16:38:23", "content": "How to predict movie revenue category /!/ [https://filebin.net/d9ub2xwu2am513so](https://filebin.net/d9ub2xwu2am513so)\n\nIn this filebin, you will find a dataset of -----> film !!! s and sample submission where you need to predict the revenue category of a -----> film !!! .\n\nI am not sure how to clean this dataset, it contains many non numeric datatypes. Should I be dropping them? If I use one hot encoding, I end up with many, many columns with poor interpretation. If I use label encoding, I end up with nonsensical relations between the categories. Also is there a way to clean multiple similar columns by type?\n\nOnce the dataset is cleaned, is it just a case of picking a model from scikit learn?\n\nAny help much appreciated.\n\n\\[Please dont overcomplicated solutions\\]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qe8bis/how_to_predict_movie_revenue_category/"}, {"autor": "SuccMyStrangerThings", "date": "2021-10-22 19:45:46", "content": "How is UNet able to predict the mask for an input -----> image !!! ? /!/ I still can't wrap my head around how it creates a single channel mask image. Does it perform pixel wise classification? If it does, is sigmoid activation used to produce output between 0 and 1 and compute loss for each pixel? \n\nCan someone please explain or share resource(s) so I could know how loss is computed and prediction mask is made?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qdooun/how_is_unet_able_to_predict_the_mask_for_an_input/"}, {"autor": "Edulad", "date": "2021-10-22 07:47:03", "content": "How to crop the Mask in OpenCV ? /!/ OK so, i had posted in earlier thread on how to crop the **canny edge detection,** People had responded to **play around with the bit-wise operation and mask -----> image !!!  properties.**\n\nNow i have got the mask, but how to now crop it? (Is it possible to **NOT** crop it using the **rectangle method** as the rectangle also crops some space below and above the image. \n\n**My Code:** \n\n`import cv2 as cv`\n\n`import numpy as np`\n\n&amp;#x200B;\n\n`# load the image`\n\n`im_color = cv.imread(\"89.jpg\", cv.IMREAD_COLOR)`\n\n`im_gray = cv.cvtColor(im_color, cv.COLOR_BGR2GRAY)`\n\n&amp;#x200B;\n\n`# CANNY Edge Detection`\n\n`edges1 = cv.Canny(im_gray, threshold1=30, threshold2=100)`\n\n&amp;#x200B;\n\n`# Mask Starts`\n\n`_, mask = cv.threshold(edges1, thresh=180, maxval=255, type=cv.THRESH_BINARY)`\n\n`im_thresh_gray = cv.bitwise_and(edges1, mask)`\n\n&amp;#x200B;\n\n`mask3 = cv.cvtColor(mask, cv.COLOR_GRAY2BGR)  # 3 channel mask`\n\n&amp;#x200B;\n\n`im_thresh_color = cv.bitwise_and(im_color, mask3)`\n\n&amp;#x200B;\n\n**My Input Image:**\n\n&amp;#x200B;\n\nhttps://preview.redd.it/k8c7w6u8iyu71.jpg?width=476&amp;format=pjpg&amp;auto=webp&amp;s=598429553e70de1addf91ee2828605a207c175de\n\n**My MASK3 Image:**\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/hov3x9oaiyu71.png?width=476&amp;format=png&amp;auto=webp&amp;s=c678405dd97d01a86b5ab4ea1615816fec32f3f5\n\n**Please Help, Thanks :)**", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qdbwyq/how_to_crop_the_mask_in_opencv/"}, {"autor": "BigDaddy_in_the_Bus", "date": "2021-10-22 02:10:49", "content": "Clustering Algorithms. /!/ First of I am still a newbie to ML, so forgive me if I make some silly comments.\n\n[https://towardsdatascience.com/color-identification-in------> image !!! s-machine-learning-application-b26e770c4c71](https://towardsdatascience.com/color-identification-in------> image !!! s-machine-learning-application-b26e770c4c71)\n\nI'm trying to make this project, where I read an -----> image !!!  and take all of it's pixels along with its RGB values as a dataset and then make clusters of the top 8 colours. In this project they have used Kmeans clustering.\n\nI was wondering if it's possible with any other clustering algorithms.\n\nI'm trying to make it work with Gaussian Mixed Models but I can't seem to make it work..", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qd70hg/clustering_algorithms/"}, {"autor": "axetobe_ML", "date": "2021-05-07 19:05:13", "content": "Tips For Learning ML If You Don't Like learning Via Courses /!/ I read a [Reddit post](https://www.reddit.com/r/learnmachinelearning/comments/mo4bzl/struggling_to_learn_ai/?utm_source=share&amp;utm_medium=web2x&amp;context=3) about how the OP was struggling to learn ML. Because he found the courses abstract and theoretical. And did not see how it would relate to his ML goals. In the people gave their opinions and useful suggestions.\n\nSome of those suggestions I will be showing below:\n\n\\- Create A Side Project\n\nStart working on a project that would involve ML. Then you can learn about the topic as you\u2019re developing the project. You can write about what learned in a [blog post](https://www.tobiolabode.com/blog/2020/10/21/-----> image !!! -classifier-for-oolong-tea-and-green-tea), so you know what to work on next time.\n\n\\- Implement A Paper\n\nImplementing a paper helps learn new concepts and forces you to translate that knowledge into a tangible item.\n\n\\- Take Courses That Focus On Coding Models Straight Away \n\nI recommend [FastAI](https://course.fast.ai/) which is a very hands-on course. Which focuses on working on ML examples straight away. This course helps you learn the basics of Deep Learning while of some tangible examples to show.\n\n\\- Tutorials Provided By [PyTorch](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) and [Tensorflow](https://www.tensorflow.org/tutorials)\n\nYou can try the tutorials provided on their websites. You will work through practical examples on how to use the library. And you can read about the concepts that some tutorials talk about along the way.\n\n\\- Create Your Favourite Models From Scratch \n\nThis idea is from [Andrew Trask](http://iamtrask.github.io/2015/07/12/basic-python-network/). You create neural networks only using NumPy. This will force you to turn any theoretical knowledge you have about ML into real-life examples. It won\u2019t be enough to name a concept and move on. But you will need to make tangible examples of the concepts. This can be done for your favourite libraries as well. \n\nAdditional note: \n\nYou still need theoretical knowledge if you want to do well with ML. As want to know how your model works behind the scenes. And it helps you grasp any new concept that comes your way. If you want to learn about maths. Check out these resources ([MML book](https://mml-book.github.io/) and [YC Learning Math for Machine Learning](https://www.ycombinator.com/library/51-learning-math-for-machine-learning)). As maths is something that many people struggle with when learning ML.  \n\nAfter this, you should be more confident about learning ML. And have hands-on experience making models and a greater understanding of courses you were watching.\n\n\\-\n\n*If you found this post useful, then check out my* [*mailing list*](https://www.tobiolabode.com/subscribe) *where I write more stuff like this.*", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n767ij/tips_for_learning_ml_if_you_dont_like_learning/"}, {"autor": "EDG723", "date": "2021-05-07 13:59:39", "content": "Is there a Kitti -----> image !!!  to every VKitti Image that is basically the same scene but in real form?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n6z575/is_there_a_kitti_image_to_every_vkitti_image_that/"}, {"autor": "Dario_Della", "date": "2021-05-06 13:26:55", "content": "*** No instances to display *** on Google Colab /!/  I'm training a mask r cnn model to detect 13 class. When I'm testing the model, I get the following error: \\*\\*\\* No instances to display \\*\\*\\* .\n\nI tried this changes:\n\n     utils.py line 866: shift = np.array ([0, 0, 1, 1]) to shift = np.array ([0, 0, 1., 1.]);\n    \n    downgrade scipy to 1.2.3 version;\n    \n    set model.load_weights(weights_path, by_name=True, exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"]) ;\n    \n    from __future__ import division\n\n Sometimes them work, other times they don't.\n\nI have another question: I have 21 classes but i want to clasify only 13 of them. My goals are:\n\n* detect this 13 classes inside a specif area (like a truck), how can I do this?\n* i would like to predict all items in a single -----> image !!!  in maximum 1 second, how can I do this?\n\nThanks all!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n678xs/no_instances_to_display_on_google_colab/"}, {"autor": "KindAd4671", "date": "2021-05-06 04:57:24", "content": "Can I get some advices on inferencing people from upwards using Yolov5? /!/ I'm trying to inference people from upwards and count them using Yolov5. I know the controversy between yolov5 and yolov4, but for me, Yolov5 is more easier and reliable to use, also the setup.\n\nI have tried SORT, Deep-SORT to track count people passing the gate, but it gets lost when the -----> camera !!!  directly films from the upper side, so the tracking gets lost and maps with new id. When I remove the tracker I can see that the inferencing is not happening.\n\nI'm not sure I can solve this by touching the script, or the only way is to train the model. Currently I have been using pretrained model, which I believe is been trained by COCO dataset. I have been using the people class only from the model.\n\nIf I have to train the model, can someone give me guidance about how to do that? I am wondering how to train, as the pretrained model is containing 80 classes, is it better to wipe out everything and train it from scratch, or to use Transfer-learning tool. And if I decide to use Tlt, should I keep the 80 classes, or only left people class? I'm using the people class only, but wiping out the other classes would affect the whole network, from my understanding.\n\nThanks in advance!\n\n&amp;#x200B;\n\nhttps://preview.redd.it/xqu7uyaylfx61.png?width=1280&amp;format=png&amp;auto=webp&amp;s=78cec3d6757842fc66f44e2b9f7a6d4226829841\n\n*Processing img kmzrreczlfx61...*", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n5zkec/can_i_get_some_advices_on_inferencing_people_from/"}, {"autor": "tylersuard", "date": "2021-05-05 18:38:14", "content": "Vision transformer for binary -----> image !!!  classification? /!/ Does anybody know of a good repo for this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n5n5ev/vision_transformer_for_binary_image_classification/"}, {"autor": "AeroDEmi", "date": "2021-05-05 14:29:13", "content": "Questions about computer vision /!/ Hello everybody I have a couple of questions regarding computer vision as I want to build my first GAN to produce -----> image !!!  with filters or make a background removal. My questions:\n\n\u2022Of I train my model with low-res images can it accept high-res?\n\n\u2022How do I make my model to accept not only square image but like rectangles as some image are horizontal and others vertical?\n\n\u2022If I want my GAN to create almost an identical image but with few changes can I make the generator more shallow? \n\nThank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n5h8b4/questions_about_computer_vision/"}, {"autor": "AdelSexy", "date": "2021-05-05 12:45:09", "content": "AI-based WWII -----> photo !!!  enhancement", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n5f1x7/aibased_wwii_photo_enhancement/"}, {"autor": "Lairv", "date": "2021-05-05 09:38:37", "content": "My UNet model isn't learning anything after 1 or 2 epochs /!/ I am trying to train a UNet for binary -----> image !!!  segmentation with PyTorch, with a ResNet CNN as a backbone. After 1 or 2 epochs, the model stop learning : the train loss, the test loss, the test accuracy all remains the same. I have tried several things:\n\n\\- changing the learning rate\n\n\\- changing the batch size\n\n\\- using a learning rate scheduler\n\n\\- modifying the BCE loss with weights because in my dataset the binary masks are usually small\n\n\\- using dropout\n\nWhile some of those things improved a bit the training, I still have the same problem that my model stop learning after a few epochs.\n\nI'm using a custom dataset, which might be the reason, because the dataset's quality isn't very good, but I don't really have the ressources to improve the dataset for now. Other than that, do you have any ideas on how could I improve the training ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n5c0pl/my_unet_model_isnt_learning_anything_after_1_or_2/"}, {"autor": "citra-ceth", "date": "2021-10-12 06:56:25", "content": "Good tool for efficient manual -----> image !!!  snipping for big -----> image !!!  dataset? /!/ Hello!\n\nI'm trying to make a very specialized dataset that requires certain parts of images.\n\nDoes anyone know a good tool for manually cutting out specific parts of images for a large dataset?\n\nI prefer not to use the snipping tool as its pretty slow when dealing with so many images.\n\nSaving files in particular slows things down. Is there anything where I can just keep snipping and saving is done automatically?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q6g9x1/good_tool_for_efficient_manual_image_snipping_for/"}, {"autor": "TrainYourMonkeyBrain", "date": "2021-10-11 22:16:47", "content": "Methods for unique multi-class classification /!/ I'm looking for methods that perform multi-class classification, where multiple different classes can occur in a single data sample/ -----> image !!! . Important however, is that each class can exist *at most once* in a single sample, and not all classes have to be present in a sample. \nFirst off, is there a name for this type of classification? Second, are there any methods that have this additional \"uniqueness\" constraint?\n\nI've been thinking about using graph neural networks or something similar to inform a model of embeddings of other instances in the sample, and adding some punishment for predicting the same class twice in a sample, but this still wouldn't strictly enforce the uniqueness constraint.\n\nAny help would be greatly appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q67ape/methods_for_unique_multiclass_classification/"}, {"autor": "grid_world", "date": "2021-10-11 16:49:39", "content": "VGG-18 PyTorch /!/ I  have completed some extensive experiments using VGG-18 CNN  network  trained on CIFAR-10 dataset from scratch and have obtained a  validation  accuracy = 92.92%. The codes involves different techniques such as:\n\n1. -----> Image !!!  data augmentation\n2. Kaiming He weight initialisations\n3. Learning Rate Scheduler - linear LR warmup over 'k' steps followed by a plateau and step-decay\n4. Manual implementation of early stopping\n5. Comparison between early stopping vs. LR scheduler\n6. Batch Normalization\n7. Dropout - to combat overfitting\n\nYou can access the code [here](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/VGG18_PyTorch.ipynb) and [here](https://github.com/arjun-majumdar/CNN_Classifications/blob/master/VGG18_Dropout_PyTorch.ipynb). According to some research papers, for deep learning architectures, using SGD vs. Adam optimizer leads to faster convergence.\n\nThoughts?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q604p4/vgg18_pytorch/"}, {"autor": "rickjamesb4455", "date": "2021-10-11 16:22:27", "content": "Needing suggestions on Thumbnail Rating system /!/ I'm new to machine learning, and I want to try and build an algorithm or use some type of machine learning which takes an -----> image !!!  of a youtube thumbnail and classifies it from 1-5.  1 being an ugly thumbnail and 5 being a well designed thumbnail.   I'm currently using a DCNN, but it isn't working that well.\n\nMy current approach was I downloaded 6,000 different thumbnails segmented them into difference directory 1,2,3,4,5 depending on if I think they look good or nice or average.  Since there are more bad thumbnails than good ones, I make sure the number of image in each class are about equal, so I have about 250 images in each class.  Using python + keras + tensorflow, I load these images and scale them down to 50x50 and use that as the dataset for the DCNN which have 5 output nodes.\n\nSo far the best validation accuracy I have achieved is around 50-60% with the model accuracy being like 99%.  I'm assuming my model is over-fitting the dataset, but I've tried some things to change that up and it doesn't improve much.\n\nI've tried changing around the downsampled image size, pooling, nodes in layers, filter size, dropout rate, hidden layer size, epochs, but the validation accuracy is still pretty bad.  When I grab an awesome looking thumbnail, it sometimes prints 1 as a result.  What I'd like instead is an output from 0.0-1.0 where 1 is good thumbnail and 0 is a bad one, but I'm not really sure how to do that or if it would even help in this instance.  I'm also considering using 5 different NNs and train them separately and use those 5 different networks to vote on an image.  If all 5 networks give an image a vote, then it's a 5 star image, etc.  \n\n\nLooking for suggestions or feedback.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q5zjg2/needing_suggestions_on_thumbnail_rating_system/"}, {"autor": "Ti-mode", "date": "2021-10-11 10:17:42", "content": "AI for Earth Monitoring - Online course /!/ \u2753What course is about\u2753\n\n\u2705 How to apply data science to datasets of Earth -----> image !!! s collected by satellites\n\n\ud83d\udc68\u200d\ud83d\udcbbWho is the course for?\ud83d\udc69\u200d\ud83d\udcbb\n\nThis course would benefit people interested in jumping into the world of data science and gaining experience working with real Earth observation -----> image !!!  data. \n\n\ud83d\udcaaWhat you will learn?\ud83d\udcaa\n\n1\ufe0f\u20e3How to apply Python and libraries to real problems with Earth observation data\n\n2\ufe0f\u20e3How to work with machine learning using image datasets.\n\n3\ufe0f\u20e3How to process large datasets and many more\n\n\u2139\ufe0fInfo\u2139\ufe0f\n\n\u23f0Start date: 18 Oct. 2021\n\n\u23f2Duration: 6 weeks\n\n\u270d\ufe0fRegistered: &gt;3500 data enthusiasts registered already\n\n\ud83d\udcb8Cost: Free\n\n&amp;#x200B;\n\n\ud83d\udc40Interested? Check more details here: [https://www.futurelearn.com/courses/artificial-intelligence-for-earth-monitoring](https://www.futurelearn.com/courses/artificial-intelligence-for-earth-monitoring)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q5srs9/ai_for_earth_monitoring_online_course/"}, {"autor": "Leterax", "date": "2021-10-11 07:47:05", "content": "Multi label -----> image !!!  level classification with sparse labels /!/ I am trying to perform image level classification based on the [oidv6 dataset](https://storage.googleapis.com/openimages/web/index.html). That means that i have around 1.74 million usable images with an average of 17.8 classes/image out of 9605 classes. \n\nIn order to compensate for the highly asymetrical labels im using [this paper](https://arxiv.org/pdf/2009.14119.pdf), which implements a Asymetric-Loss-Function. So in total i have a efficientnet v2 b1 backbone feeding into a final dense layer with 9605 units and then using the ASL function for my loss.\n\nMy problem is that the accuracy converges rather quickly to 41% (half a epoch) and does not move even after multiple epochs. I have no idea why this is happening or what to change. Thanks in advance for any tips and tricks!\n\n[Accuracy converting to 41&amp;#37; and staying there form one of my most recent runs](https://preview.redd.it/6yxs7as10ss71.png?width=1168&amp;format=png&amp;auto=webp&amp;s=833b6887e68dee9846f71927c69fb2ee9eb0d074)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q5quu7/multi_label_image_level_classification_with/"}, {"autor": "data_driven_approach", "date": "2021-10-11 01:10:36", "content": "Help a newbie learn :) WGAN implementation with Custom Dataset /!/ Hello all!\n\nSo I started last week looking into GANs. I want to reach a point in which I feed it -----> image !!! s and styles that I like and hopefully it will generate other things I like \ud83d\ude42\n\nThe entry was a simple GAN which worked (I guess?) until it reached a point that it generated the same -----> image !!!  again and again, until I discovered something called \\`mode collapse\\` [cGAN/gan at master \u00b7 czioutas/cGAN (github.com)](https://github.com/czioutas/cGAN/tree/master/gan)\n\nOff I was to implement WGAN which should tackle this problem!\n\nWGAN has definitely more complexity that is probably too deep for my understanding.\n\nI progressed by googling and ended up with [cGAN/wgan at master \u00b7 czioutas/cGAN (github.com)](https://github.com/czioutas/cGAN/tree/master/wgan)The problem being in the end that the input images do not match the needs of the model but I do not know how to fix em.\n\nI added a stack trace of the output and the error line that points to the issue [cGAN/model.py at master \u00b7 czioutas/cGAN (github.com)](https://github.com/czioutas/cGAN/blob/master/wgan/model.py#L144)\n\nIf anyone is willing to help out, comment, contact me anywhere you want (twitter is linked on github)\n\nThank you all in advance and its time to go hit the bed \ud83d\udca4", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q5kzrv/help_a_newbie_learn_wgan_implementation_with/"}, {"autor": "terrainhead885", "date": "2021-10-10 16:25:14", "content": "-----> Image !!!  super resolution as a final year project /!/ 4th year software and electronic engineering student here with little to no machine learning knowledge/experience, but an interest and desire to gain some in that area.\n\nFor my final year project over the next two semesters, I was thinking of training a model that can upscale a low resolution image input to a high quality, high resolution equivalent and, as part of it, compare it to typical, basic upscaling algorithms, maybe comparing different models (?, again, not sure on some of the terminology) against each other too.\n\nMy question is, for a year long project, is that too basic, or too complex for someone starting from the beginning having to do a lot of research in their own time? Are there ways to expand it to something bigger?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q5bapw/image_super_resolution_as_a_final_year_project/"}, {"autor": "MrMooss", "date": "2021-10-10 08:28:55", "content": "I don't know how to start /!/ Hello everyone,  \n\n\nThis semester I have to make an alpha version for my single -----> image !!!  super-resolution project. Basically the base functions needs to be working, the problem is, i dont even know how to start. I don't have any experience with AIs, I just started a deep learning course at uni. I planned to just get an already working one just to see how it works exactly but I couldn't set it up.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q542hb/i_dont_know_how_to_start/"}, {"autor": "TrepidationTD", "date": "2021-10-10 02:56:28", "content": "How do I implement the Yolov3 models? /!/ I am doing a project where I get live video and I need to create a bounding box of custom objects and then use -----> image !!!  segmentation on them. The objects are close together but I have never worked with cv before. My current plan is using the Yolov3 model with OpenCv for recognition but all the tutorials I found online weren't specific to my scenario and I had a hard time implementing them. Can anyone recommend good tutorials I can follow to create bounding boxes? I feel like everytime I follow these guides, I'm basically copying the code and I'm doing this for a publication. Is this common?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q4zr4q/how_do_i_implement_the_yolov3_models/"}, {"autor": "lasagna_lee", "date": "2021-10-09 18:08:02", "content": "how to approach organizing this data? /!/ i have a little ML classification problem and i got stuck right at the first part. you know how in classification, there are features and each feature for a row sample is one number. well here i have two features and it is basically a 2D array: data = \\[ \\[feature\\_x1\\_array\\], \\[feature\\_x2\\_array\\], \\[feature\\_x3\\_array \\], \\[feature\\_x4\\_array\\], \\[feature\\_x5\\_array\\], \\[feature\\_x6\\_array\\] \\]\n\nEach feature\\_xn\\_array is an array of numbers (e.g. 3,4,5,6,1....10) that has length 300. I am puzzled though, because I have done classification where I had a larger number of features and each feature only had single number entries, not entire arrays. The only thing I have done similar to this is -----> image !!!  classification where your features are -----> image !!!  arrays ( i think?) but in that application you can use a CNNs which take array inputs. for the above case it is not image classification but more like taking continuous sensor measurements to classify movement. \n\nam i simply misunderstanding what is a dataframe and what is an array? can i do non-image classification using arrays with few features?\n\nthanks for any help!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q4qmz1/how_to_approach_organizing_this_data/"}, {"autor": "AnonCaptain0022", "date": "2021-10-09 13:15:01", "content": "Will I run into problems if my -----> image !!!  classifier doesn't take in equally sized -----> image !!! s? /!/ I'm making a basic cats-dogs image classifier as an exercise and I got all the training data from the internet which means that they all have a different size and some of them have a different format (jpg, png). Will this cause problems down the line due to the different number of input neurons and unknown dimensions once the image is flattened? Should I resize all of them?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q4l4gz/will_i_run_into_problems_if_my_image_classifier/"}, {"autor": "tah_zem", "date": "2021-10-08 13:46:21", "content": "Did you use AI engines in the cloud? Your feedback? /!/ Hi there!\n\nHave you, if you're a developer, ever used and integrated AI engines in the cloud? By that I mean APIs offered by companies to process your data (-----> image !!!  recognition, machine translation, text mining, etc.).\n\nIf so, in what context and what is your feedback? What are the problems you have encountered?\n\nThanks,\n\nTaha", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q3y38m/did_you_use_ai_engines_in_the_cloud_your_feedback/"}, {"autor": "ManMythLegend_", "date": "2021-10-08 12:34:19", "content": "Good starting points for fundamentals of machine learning with intent on working in -----> image !!!  processing/navigation /!/ Hello all,\n\nIm a 1st year PhD student in Navigation, and during my time in uni I want to learn as much about machine learning &amp; computer vision as possible. For fundamentals I have heard of the Coursera courses, but what about references that have applications towards image processing/computer vision?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q3wtq0/good_starting_points_for_fundamentals_of_machine/"}, {"autor": "Fincorn", "date": "2021-10-08 08:14:02", "content": "Machine learning /!/  \n\nHi, everyone\n\n I am trying to segregate a collection of 1000 -----> photo !!! s into two groups whether I am in the -----> photo !!!  or not. Can anyone help me in this.\n\nI have tried some ML models but it is not working out.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q3tduw/machine_learning/"}, {"autor": "Kelcius", "date": "2021-10-06 21:28:10", "content": "How are convolutional layers fully connected to perceptron layers? [YOLOv1-related] /!/ Now,  I am a bit of newbie to convolutional neural networks, but I have been  looking all over the web and I can not find the answer to my question.\n\nSo in the original YOLO paper, [https://arxiv.org/pdf/1506.02640.pdf](https://arxiv.org/pdf/1506.02640.pdf),  in Figure 3, they show how there is first a set of convolutional layers  and max-pooling layers to increase the number of channels and reduce  the X and Y dimensions. Then there are 1024 feature convolutions of size  7x7 which are supposedly FULLY CONNECTED to a perceptron layer with  4096 nodes. Now, as I read this, this means that there are 1024 layers,  all of which are fully connected to the 4096 nodes. So 1024\\*4096. And  all of these have 7x7 pixels each. So 1024\\*4096\\*7\\*7. This comes out to  be an absolutely ridiculous number of connections (205'520'896 \\[205  million\\]). Surely this can't be correct? What part am I reading wrong  here? Are the 7x7 convolutions each passed through an activation  function first and only the output of those activation functions are  fully connected to the perceptron layer? That would make it slightly  more manageable at 1024\\*4096 \\~= 4 million connections. Finally the  perceptron layer is connected to the 7x7x30 output nodes for a total of  4096x7x7x30 connections, which also seems a bit on the high side but I  guess it's better than 205 million.\n\nBonus question:\n\nIt  says that there are 192 convolutions after the first convolutional  layer with 64 kernel filters. I see that there's 192 channel  convolutional layer right after, they don't show the 64 convolutions, so  now I'm not sure if I don't understand this or if the -----> image !!!  is wrong.\n\nSorry  for the long text. I've been writing my master's thesis and it seems  that I'm unable to write anything without turning it into a novel now.\n\n**TLDR**: YOLO paper, [https://arxiv.org/pdf/1506.02640.pdf](https://arxiv.org/pdf/1506.02640.pdf), final conv layer: 7x7x1024 -&gt; fully connected -&gt; 4096 nodes. Number go very big. Haha GPU go BRRRRRR?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q2tyem/how_are_convolutional_layers_fully_connected_to/"}, {"autor": "Mcsquizzy920", "date": "2021-10-05 21:47:40", "content": "What are query, key, and value vectors in an Attention layer in the context of Image Processing? /!/ Hello all, \n\nI am trying to figure out how to apply an attention layer to my convolution neural network that I use for an -----> image !!!  classification problem, but I am struggling to understand what should be my key, value and query vector. I am used to working with Attention in the context of NLP, and I am just a bit confused trying to apply into a different domain.\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q26fd6/what_are_query_key_and_value_vectors_in_an/"}, {"autor": "ConfusedLisitsa", "date": "2021-10-05 20:07:23", "content": "Identifying a single object with a Convolutional Neural Network /!/ Sup guys, im new to deep learning so my knowledge is kinda shallow\n\nAs i understood, a Convolutional Neural Network is mostly used to identify an object between two or more options (i.e. is the object in this -----> image !!!  an apple, a banana or a magoose?)\n\nBut what if i want to answer the question: \"is there a banana in this photo?\"\n\nSo basically identifying a single object\n\nI'm currently trying to do this by simply using two training data sets: \"bananas\" and \"not bananas\", in the second folder i simply put lots of random -----> image !!! s (cats, dogs, yellow, literally anything)\n\neven tho this feels kinda stupid i'm lost on how to solve my problem, so i'm here asking to you guys\n\nthanks in advance", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q24f3z/identifying_a_single_object_with_a_convolutional/"}, {"autor": "fire_regalia", "date": "2021-10-05 18:42:57", "content": "Retail product attribute extraction from images /!/ I'm in discussions about this project with a retail client - the client has a bunch of physical retail products and wants to get the attributes of each product from their images clicked on a -----> camera !!! . The motivation for this is to avoid manual lookup of attributes and to automatically populate product information. \n\nFor example, there client would click the image of a cereal box, and the end result should be a dictionary of attributes of that product, like the \"product name\", \"brand\", \"net weight\", \"quantity\",  \"nutritional facts\", etc. \n\nI thought of a couple of approaches:\n\n1. Use Google Vision API (or similar platforms) for OCR outputs from the images, and then do regex matching for attributes. There are a couple of issues with this though - though we can capture things like net weight (simple patterns like \"500g\"), it doesn't work for \"open list\" attributes (like product name). Besides, the OCR itself isn't perfect leading to incorrect extraction. I don't see how this method would scale except perhaps for the simplest of attributes. \n\n2. Scrape product information (including product images) from multiple retail websites and build a database. Then perform image similarity search between the query image and the reference images in the database, and retrieve the product information corresponding to the closest match. There's a huge domain mismatch between the query images (camera images) and the reference images (studio quality images). Also, scraping needs to be done continually to keep the database up to date. Is this a viable approach?\n\nHas anyone worked on something similar before? How does one approach this problem?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q22ric/retail_product_attribute_extraction_from_images/"}, {"autor": "sIPSC", "date": "2021-10-05 17:19:40", "content": "[P] I tried training StyleGan on 10k images from pubmed with minor success /!/ Hello All,\n\n&amp;#x200B;\n\n[Latent Space exploration](https://reddit.com/link/q2139w/video/4qjjphow0or71/player)\n\nI figured I would share a story with some minor success, and some minor letdowns. I am a PhD student in neuroscience. A few months ago I had the idea of training a stylegan (V1) on images scraped from life science journal articles. This came after spending many late nights reading the articles.\n\nTraining was moderately successful. The model I am sharing is after 25000 k -----> image !!!  ticks. Unfortunately I have not seen much improvement past this point. \n\nHere is the reference images: \n\n[Reference Images](https://preview.redd.it/611ywictwnr71.png?width=3840&amp;format=png&amp;auto=webp&amp;s=670303ab766660318ee9dcf4b8f76a4b89219b49)\n\nand here are the synthesized images:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/qc5s03jwwnr71.png?width=3840&amp;format=png&amp;auto=webp&amp;s=2b0948d621c97328e09477ec0304fada7b3ec79f\n\nHere is a small web interface for viewing the images: [https://www.smestern.com/pubmedgan.html](https://www.smestern.com/pubmedgan.html). It kind of looks like if someone had really poorly photocopied the figures in question. If you squint it works haha!\n\nWhat went well / what the community might find useful:\n\n* Pubmed scraping - I have shared the PubMed scraping notebook. This was fairly trivial but surprisingly there is no existing image datasets from PubMed. Here you can scrap from pubmed and filter by year or subject: [https://www.smestern.com/pubmed\\_dl.html](https://www.smestern.com/pubmed_dl.html)\n* Training on Colab  - I was able to train on colab (gpu) without much hassle. To deal with timeouts and stuff everything had to periodically saved to drive etc: [https://www.smestern.com/DOCUMENTED\\_style\\_GAN\\_TRANSFER.html](https://www.smestern.com/DOCUMENTED_style_GAN_TRANSFER.html)\n* Transfer learning - I was able to use pretrained networks from NVidia to facilitate speed up of training. Similar to here: [https://www.johnkraszewski.com/media-synthesis](https://www.johnkraszewski.com/media-synthesis)/ and my process is documented in the link above.\n\nWhat went wrong:\n\n* Too much heterogeneity of the dataset - Unfortunately I believe the dataset used for training was too heterogenous. If you look at the other Stylegan models, the datasets are pretty homogenous in their composition, feature placement, colours, etc. E.g looking at the FFHQ dataset consists of faces with their features primarily in the same location. However, the dataset here ranges from tissue images to MRI to simple black and white bar graphs, and I feel as though its too much for a single stylegan model to capture. In the future I might try to train the model on a subsection of the dataset, e.g. only bar graphs.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q2139w/p_i_tried_training_stylegan_on_10k_images_from/"}, {"autor": "luc0lagarde", "date": "2021-10-05 15:58:29", "content": "Normalized Keypoints detection /!/    \nHello,\n\nI have a resnet18 model to make key point prediction.  \nI have read that the coordinates of the points should be normalized between \\[-1,1\\].  \nBut if my -----> image !!!  is 720x1280, how the model will make the parallel between the coordinates of a key point and its location?\n\nEven without the normalization, since my ytrain = \\[x1,y1, x2,y2, \u2026., xn,yn\\], there is no direct link between a location and its coordinates.\n\n**Is there a better way to do keypoint detection, or is this the normal way?**", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q1zeyq/normalized_keypoints_detection/"}, {"autor": "luc0lagarde", "date": "2021-10-05 15:58:01", "content": "Visible Key points detection /!/  \n\nHello,  \nI use a resnet18 to do keypoint detection.  \nHowever, on each -----> image !!! , there is only about 30% of the points that are visible. I saw on a forum that I could train a second model to determine whether or not the point is on the image (visibility = 1, visibility 2 0).  \nHowever in my main model, the invisible points are the majority and completely falsify the predictions. (I put the points at coordinates (-1,-1))\n\n**What are the solutions I can implement?**\n\nFor information, my Xtrain is an image list, and my ytrain is a column built as follows:  \n\\[x1, y1, x2, y2,\u2026\u2026 ,x150,y150\\]  \n(I have 150 key points, and about 50 are visible in an image (not always the same)\n\n**Is it the good way ?**\n\nThanks \ud83d\ude4f", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q1zeli/visible_key_points_detection/"}, {"autor": "Immediate_Price4305", "date": "2021-10-05 12:05:02", "content": "Workshop on Explainable AI /!/ Hello all,\n\n[Arya.ai](https://Arya.ai), is conducting a workshop on \u2018XAI\u2019 for deep learning. Workshop covers topics like - XAI methods for DL, Pros &amp; Cons with Back-trace, Implementation on -----> Image !!!  classification problem. \n\n'Back-trace' is [Arya.ai](https://Arya.ai)'s patent pending algorithm called  to create true to model explanations for deep learning models in real time that goes a long way to complete auditability of decisions. \n\nIt is a 1.30hr workshop on Oct 27th, 9.00am EST/9.00pm SGT. \n\nYou can find the more details here about the workshop \u00bb [Workshop registration and details](https://www.meetup.com/arya-ai/events/281140772/) \n\n [Arya.ai](https://Arya.ai) is an AI PaaS for financial institutions (FIs) to design and deploy fully autonomous AI systems using deep learning that are explainable and auditable. \n\nCheers!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q1uz44/workshop_on_explainable_ai/"}, {"autor": "baddieas12", "date": "2021-10-04 18:40:39", "content": "-----> Image !!!  Output of plot tree in catboost /!/ I'm trying to save the output of my catboost decision trees to an external image file. Does anyone know of any documentation to do so?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q1bvht/image_output_of_plot_tree_in_catboost/"}, {"autor": "Jcw608", "date": "2021-10-04 16:42:54", "content": "What network type would be suited for predicting storm paths on a weather radar? /!/ I\u2019ve been wanting to try this as a project since the weather service keeps thousands of radar images that are in order. I had thought that if I gave the neural network maybe the past 10 radar sweeps and then got it to predict the next I could then validate it against the actual next -----> image !!!  in the sequence. Not sure exactly where to start though.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q199p6/what_network_type_would_be_suited_for_predicting/"}, {"autor": "ralampay", "date": "2021-09-11 11:02:41", "content": "CNN Autoencoder Architecture Problem in Pytorch /!/ Hi I have the following CNN autoencoder model in pytorch:\n\n```\nCnnAutoencoder(\n  (criterion): BCELoss()\n  (convolutional_layers): ModuleList(\n    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): Conv2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  )\n  (deconvolutional_layers): ModuleList(\n    (0): ConvTranspose2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ConvTranspose2d(16, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  )\n```\n\nI'm passing a 100x100 3-channel -----> image !!!  to it but I'm having an incorrect calculation:\n\n```\nUsing a target size (torch.Size([20, 3, 100, 100])) that is different to the input size (torch.Size([20, 3, 21, 21])) is deprecated. Please ensure they have the same size.\n```\n\nI tried to make the building of the model dynamic so it accepts an input `channel_maps` like `[3, 16, 8]`. \n\nWhy is there incorrect processing of tensor sizes? Am I missing something? I'm planning to raise an exception if the `channel_maps` doesn't compute properly for the model so it fails even before training.\n\nAny help towards pointing what's wrong would help. Thank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pm5fl6/cnn_autoencoder_architecture_problem_in_pytorch/"}, {"autor": "Lost-Olive-Man", "date": "2021-09-11 01:04:00", "content": "Does Datalore get stuck loading for anyone else? /!/ Whenever I try opening a Datalore notebook, it gets stuck on the load screen. \n\nHere is a -----> picture !!!  of where it is at:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/la8mtqk0xrm71.png?width=268&amp;format=png&amp;auto=webp&amp;s=44ede7ccf0efd35afa184460d00369d6fe65ac9a", "link": "https://www.reddit.com/r/learnmachinelearning/comments/plxmpq/does_datalore_get_stuck_loading_for_anyone_else/"}, {"autor": "DevStarship", "date": "2021-09-10 22:57:33", "content": "A machine learning model that classifies a cartoon character as a goody or baddy /!/ I've created a machine learning -----> image !!!  classification model using [ML.NET](https://ML.NET) trained with a set of goody cartoon characters and baddy cartoon characters.\n\nIt does a pretty good job of determining whether a cartoon character is a goody or baddy by processing an image of it.\n\nI hope to put this live at [GoodyOrBaddy.com](https://GoodyOrBaddy.com) in the near future.\n\nYou can read my approach to train the original model here.\n\n[https://www.linkedin.com/pulse/training-image-classification-model-using-mlnet-lee/](https://www.linkedin.com/pulse/training-image-classification-model-using-mlnet-lee/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/plvnfz/a_machine_learning_model_that_classifies_a/"}, {"autor": "Syndri13", "date": "2021-09-10 11:57:20", "content": "Image Deduplication Pipeline /!/ I want to detect and remove duplicate images from a dataset. The method I am using consists of extracting features for each -----> image !!!  from a ConvNet and computing Cosine Similarity values for each pair of -----> image !!! s. For every cosine similarity value &gt; threshold, I call them duplicates.\n\nBy duplicates, I mean images that are very similar but differ slightly in skew, brightness, rotation and crop. \n\nYou can imagine these as the photos you take from your mobile phones of the same thing but by slightly moving your hand position.\n\nI had a few questions regarding this problem:\n\n1. What model architecture should I use for extracting features from? Currently, I am using a pre-trained (on the ImageNet Dataset) PyTorch EfficentNet model to extract the features.\n2. What layer should I extract features from? Right now, I am extracting features from the Pooling layers at the end of the model.\n3. Should I use a model specifically trained on my dataset? My final project is based on classifying the dataset.\n4. Do you have a solution better than the method I am currently using? (Feature Extraction + Cosine Similarity) \n5. Any other suggestions to improve my current pipeline?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pljkmd/image_deduplication_pipeline/"}, {"autor": "IvanIvanicIvanovski", "date": "2021-09-10 10:27:47", "content": "Duplication of images for CNN dataset - available software? /!/ I make pictures of birds that visit my bird feeder with a Raspberry Pi and -----> camera !!!  module. To learn myself convoluted neural networks, I'm trying to teach a model to detect birds instead of non-birds that might trigger the motion detection. I have collected about 600 pictures of birds and about 1600 without them.\n\n&amp;#x200B;\n\nI want to multiply my dataset by distorting the images somewhat and also give this to the model. For each picture I want one that's more grainy, shifted, tilted, flipped, etc. Is there an automated way of doing this? I tried looking online, but couldn't find anything. This is my first time working with something like this, so maybe I'm not using the right terms when looking for a solution.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/plid6t/duplication_of_images_for_cnn_dataset_available/"}, {"autor": "maybenexttime82", "date": "2021-09-09 14:32:56", "content": "[Q] Difference between freezing convolutional base vs. training on extracted features? /!/ In Fran\u00e7ois Chollet's book Deep Learning with Python he explains two types of using pretrained convnet:\n\nA) Running the convolutional base over your dataset, recording its output to a Numpy array on disk, and then using this data as input to a standalone, densely connected classifier similar to those you saw in part 1 of this book. This solution is fast and cheap to run, because it only requires running the convolutional base once for every input -----> image !!! , and the convolutional base is by far the most expensive part of the pipeline. But for the same reason, **this technique won\u2019t allow you** **to use data augmentation**.\n\nB) Extending the model you have (conv\\_base) by adding Dense layers on top, and running the whole thing end to end on the input data. **This** **will allow you to use data augmentation**, because every input image goes through the convolutional base every time it\u2019s seen by the model. But for the same reason, this technique is far more expensive than the first.\n\n&amp;#x200B;\n\nIf I get it right in case of A) you run the training example through convolutional base with 'frozen' weights and biases and then use that output as an input for a densely connected classifier which you train. In the case B) you are doing the same thing, you are not updating the weights of the conv base (that is, weights are frozen) but you put the data through the conv base and this time you input it directly to the densely connected classifier which you train? \n\nI'm a bit confused. In either case you are not training the conv base.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pkyzd9/q_difference_between_freezing_convolutional_base/"}, {"autor": "SignificanceNo4924", "date": "2021-09-09 13:48:38", "content": "How can I extract a specific class of text from a specific object in a -----> photo !!! ? /!/ Here are 2 examples:\n\nI take a photo of a storefront. This photo includes the signage of the store, maybe street signs, and a poster. I'd like to return only the name of the store, excluding \"established xxxx\" or the unit number sometimes included at the corner of the storefront.\n\nI take a photo of a blood pressure monitor after a measurement has been taken. The photo might include some words in the background, or the monitor might be quite small. And while most monitors have the systole value on top of the diastole value, some machines may have it side by side. And some machines might not even have the words \"SYS\" and \"DIA\" next to the numbers, it may be in a different language. I'd like to return the systole and diastole values.\n\nI think this probably needs to involve an object detector to identify the region of interest, but I'm not sure what would come after that. Would identifying the right text require some of sort of custom dataset to identify which is systole and which is diastole, or to tell store names apart from the extra info? Which architectures could I start looking into?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pky5tr/how_can_i_extract_a_specific_class_of_text_from_a/"}, {"autor": "akash_ranade", "date": "2021-09-09 12:43:08", "content": "Adding weights to datasets for ML with Python /!/ I am trying to use ML for signal vs. background discrimination in a physics context (particle physics). My problem is as follows:\n\nI have multiple background datasets, which are simulations of physical processes. These physical processes have a probability of occurence. Due to limitations in simulations, I cannot simulate data according to these probabilities. Therefore I introduce weight factors, which encode the amount by which my simulation has over/underestimated real probabilities.\n\nAn example: consider 3 backgrounds (A, B, C). In nature I expect them to occur (1e5, 1e4, 1e2) times. I simulate (1e3, 1e3, 1e3) data points. Therefore I define weight factors (1e2, 1e1, 1e-1), i.e. (no. of real) / (no. of simulated).\n\nWhere do these weight factors come in to the -----> picture !!! ? They inform how much the connection weights in a neural network should change when it encounters a datapoint. So a datapoint with a high weight factor affects the connection weight update more than a datapoint associated with a low weight factor. Thus, the weight factor compensates for having an excess / lack of simulated data points.\n\nThese weight factors can be readily implemented in a package called TMVA, which is built on ROOT, which in turn is built on C++. This module is the standard in particle physics, and can be used to do ML stuff also. However, it is not the modern way of doing ML.\n\nFinally my overarching question : how do I introduce weight factors to datasets using familiar python modules (numpy, pandas, sklearn, keras, tensorflow etc.). At first glance, it doesn't look like there is a provision to introduce weight factors to pandas dataframes. I can have separate arrays that keep track of the weight factors, but how do I ensure that these weight factors affect the learning of ML models? I can \n\nTLDR : How to assign weight factors to datasets in python, which will take care of an excess / lack of simulated data points, for ML applications.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pkx1zn/adding_weights_to_datasets_for_ml_with_python/"}, {"autor": "Leterax", "date": "2021-09-09 10:25:07", "content": "Multi label classification on sparse labels /!/ I'm trying to perform multi label classification on the open images dataset V6 from Google. The problem I'm encountering is that there are 19994 classes but rarely more than 10 per -----> image !!! . How do I approach this problem? My current approach of using a efficient net V2 backbone has yielded poor results. Any tips on architecture, loss function, metrics etc? So far I've tried a weighted binary cross entropy but to no avail. Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pkv3m1/multi_label_classification_on_sparse_labels/"}, {"autor": "ldorigo", "date": "2021-09-09 08:35:09", "content": "Detecting books in an -----> image !!!  of a bookshelf (as 4-point polygons) /!/ Hi. As a toy project to learn something about image segmentation and/or object detection, I want to train (or most likely fine-tune) a model that takes an image of a bookshelf as input and outputs bounding boxes for all the books in the image. Where would you suggest looking to get started? I'm not sure if it would make more sense to frame this as image segmentation or object detection. \n\nThe object detection examples I've seen usually output rectangular boxes, which is not exactly what I want (as the books might not be perfectly rectangular due to perspective or to leaning on one another). On the other hand, image segmentation sounds much harder - and ideally, I want perfectly straight lines (i.e. each book should be a 4-point polygon).\n\n&amp;#x200B;\n\nTips/links/suggestions? :-)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pktsy9/detecting_books_in_an_image_of_a_bookshelf_as/"}, {"autor": "ImageMachine1", "date": "2021-09-09 07:30:35", "content": "Platform for anyone to request and download -----> image !!!  datasets for FREE", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pkt278/platform_for_anyone_to_request_and_download_image/"}, {"autor": "Successful_Boat_3099", "date": "2021-09-09 06:53:42", "content": "\ud83d\udc8aYour daily dose of machine learning : cross-entropy for classification /!/ &gt;This is a series of posts that I post almost daily. I call them \u201cyour daily dose of machine learning\u201d. \n\nIn tensorflow there are 2 cross-entropy loss functions for classification : BinaryCrossentropy and CategoricalCrossentropy.\n\nWhen to use which?\n\nSimple. If your labels are one-hot encoded then use CategoricalCrossentropy, otherwise use BinaryCrossentropy.\n\nWhat does one-hot encoding mean?\n\nYour labels are one-hot encoded if each label is a vector of a size equal to the number of your classes and you would have 1 in the vector\u2019s entry that corresponds to the correct class and 0 elsewhere.\n\nEx: Your -----> image !!!  dataset has 2 classes : cats and dogs.\n\nIf the image corresponds to a cat then the label would be : \\[1, 0\\]\n\nIf the image corresponds to a dog then the label would be : \\[0, 1\\]\n\n Follow me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!***", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pkslg4/your_daily_dose_of_machine_learning_crossentropy/"}, {"autor": "Apprehensive-Cow3824", "date": "2021-09-09 04:57:04", "content": "Which libraries to use to make a Mario-type speedrunning bot? /!/ Hi guys I was wondering how to make such a bot with reinforcement learning. This is purely for learning purposes\n\nFrom my understanding we would require to do the following steps\n\nWhat can we use for a.-----> Image !!!  detection and manipulation(OpenCV,-----> Image !!!  grab)\n\nb.Keyboard Input(no clue)\n\nc.Reinforcement Learning algorithm\n\nFrom what my friends are looking at its Gym,Baselines3,OpenCV are some libraries we can use\n\nWill the bigger libraries such as TF or Pytorch be of any use?\n\nThanks in advance and please correct me if im wrong,I am new to the world of AI", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pkr2rl/which_libraries_to_use_to_make_a_mariotype/"}, {"autor": "Foreign-Panic", "date": "2021-09-08 17:29:06", "content": "Unsupervised semantic segmentation /!/ I am looking for a way to get mostly unsupervised semantic segmentation masks out of my data. I believe this should be possible without actually manually creating the masks myself.\n\nI have whole bunch of images that contain the object I am interested in and I also have images that do not contain the object.\n\nIs there a model out there that i could train by simply giving it pairs of such images. Since the NN must use certain features of the object to recognize the object it should also be possible to see which areas in the -----> image !!!  triggered this recognition and draw the mask only in such area.\n\nAnybody knows of a ready made model for this that exists? If not I am wondering if you guys have an idea of how to implement such a model.\n\n[This is what im talking about so some images would have cars and others would not. ](https://preview.redd.it/oivxigbzdbm71.png?width=640&amp;format=png&amp;auto=webp&amp;s=168eb37aadac61595b68b1e651cd7522399fd5b0)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pkf306/unsupervised_semantic_segmentation/"}, {"autor": "Successful_Boat_3099", "date": "2021-08-11 10:34:29", "content": "\ud83d\udcccYour daily dose of machine learning : ResNet /!/ &gt;This is a series of posts that I post almost daily. I call them \"***your daily dose of machine learning***\".\n\n&amp;#x200B;\n\nResNet is a famous deep learning architecture that uses what\u2019s called \u201cresidual blocks\u201d.\n\nThe main property of a residual block is that the input to a residual block is the sum of the output and the input of the previous block, like the -----> image !!!  below.\n\nThere is ResNet-18, ResNet-34, ResNet-50, ResNet-101 and ResNet-152. The numbers here represent the number of layers in the architecture.\n\nFor example, ResNet-152 architecture comprises 152 layers (with 3 \u00d7 3 and 1 \u00d7 1 filters).\n\nResNet-152 managed to achieve the top-5 error rate of 3.57%\n\n&amp;#x200B;\n\nFollow me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)!\n\n&amp;#x200B;\n\nhttps://preview.redd.it/tc1jvup9ipg71.png?width=1141&amp;format=png&amp;auto=webp&amp;s=1849b345d65ebb98d9c13eb5e0125136ba639b89", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p2afss/your_daily_dose_of_machine_learning_resnet/"}, {"autor": "hagrid3141", "date": "2021-08-10 17:28:46", "content": "Effect of Spatial Misalignment /!/ I want to train a network with input as the hazy -----> image !!!  and the output as the corresponding clear -----> image !!! . The dataset that I have has the following flaw: the leaves/branches in the image keep moving due to wind and hence there is misalignment for those pixels between the input and the ground truth. But the remaining pixels are perfectly aligned. The misaligned pixels constitute about 10-20 percent of each image in the dataset.\n\nWould this have a drastic effect on the training? If it does, is there any way to resolve the issue?\n\nThanks in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p1tza6/effect_of_spatial_misalignment/"}, {"autor": "Successful_Boat_3099", "date": "2021-08-10 07:13:26", "content": "Your daily dose of machine learning : Inception (GoogLeNet) network /!/ &gt; This is a series of posts that I post almost daily. I call them \"your daily dose of machine learning\". \n\nGoogLeNet also called Inception is one of the first neural networks that introduced the concept of \u201cblocks\u201d.\n\nThis network is called Inception because it has a block called \u201cinception module\u201d that looks like the -----> image !!!  below.\n\nIn fact, the first version of Inception network comprises 9 linearly stacked inception modules. \n\nThere are 27 layers in total, out of which, 5 are pooling layers. The total number of layers used for the structure of the network is around 100 layers.\n\nThe authors noticed that using average-pooling layers instead of fully connected layers improved top-1 accuracy by 0.6%.\n\nInceptionV2 came after, where some small changes were made to the inception module. For example convolutional filters of size 5x5 became of size 3x3.\n\nIn InceptionV3 there was more focus on regularization rather than module modification. RMSProp was used as an optimizer and a regularization term was introduced in the loss function.\n\nInception neural networks have shown great potential for many computer vision tasks, not just for classification. Using them as a backbone for object detection or image segmentation has shown them to be very powerful in extracting very good features.\n\nFollow me on your [***favorite social network!***](https://withkoji.com/@Nour_Islam)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p1k7sf/your_daily_dose_of_machine_learning_inception/"}, {"autor": "fysez", "date": "2021-08-09 18:37:35", "content": "Comparing Object Detection algorithms in embedded systems /!/ I am in the process of writing my thesis on different algorithms, including YOLO, SSD, RCNN, RFCN, FSAF, and FCOS.  \nI'll need to train my models, and then run them on an embedded device using a virtual -----> camera !!!  that inputs a video feed (to pretend like it's a live feed).\n\nI was thinking that the main contribution of this research would be the comparison of power consumption by algorithm, since wireless nodes rely on battery power. But after talking with my committee members, they're not sure this should be a main focus since you can always attune the system to use more/less hardware power.\n\nDoes anyone have some ideas I can put into my research that might help me narrow it down a little better? Maybe some things you have seen during your model building that I can compare between algorithms?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p17j86/comparing_object_detection_algorithms_in_embedded/"}, {"autor": "mippie_moe", "date": "2021-08-09 17:37:33", "content": "NVIDIA 3090 vs A6000 Deep Learning Performance Benchmarks /!/ [NVIDIA A6000 vs 3090 Machine Learning Benchmarks](https://lambdalabs.com/blog/nvidia-rtx-a6000-vs-rtx-3090-benchmarks/)\n\nSome Highlights:\n\n**For training -----> image !!!  models (convnets) with PyTorch, a single RTX A6000 is...**\n\n* **0.92x** as fast as an RTX 3090 using 32-bit precision.\\*\n* **1.01x** faster than an RTX 3090 using mixed precision.\n\n**For training language models (transformers) with PyTorch, a single RTX A6000 is...**\n\n* **1.34x** faster than an RTX 3090 using 32-bit precision.\n* **1.34x** faster than an RTX 3090 using mixed precision.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p16aud/nvidia_3090_vs_a6000_deep_learning_performance/"}, {"autor": "epicboyxp", "date": "2021-08-09 11:27:53", "content": "Why do people sometimes use \"random\" values for normalization? /!/ So sometimes when I see youtube tutorials they use 0.5 for the mean and std of each channel in the -----> image !!!  when using transform.normalize(). How does this even work and does this even succeed in limiting the tensor values to 0 and 1?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p0zkte/why_do_people_sometimes_use_random_values_for/"}, {"autor": "epicboyxp", "date": "2021-08-09 09:59:08", "content": "Question about normalizing images for neural networks /!/ So I heard that making sure the -----> image !!!  tensor's values are between 0 and 1 or -1 and 1 is important for training but what about cases where color is an important factor, for example if I want to generate colored -----> image !!! s with a GAN. Wouldn't normalizing mess with the color a bit?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p0yeow/question_about_normalizing_images_for_neural/"}, {"autor": "Successful_Boat_3099", "date": "2021-08-09 08:22:49", "content": "Your daily dose of machine : brief history of deep learning for -----> image !!!  recognition /!/ &gt; This is a series of posts that I post almost daily. They're called *\"your daily dose of machine learning\"*.\n\n&amp;#x200B;\n\nThere has been significant progress in image recognition (also called image classification) tasks using deep learning. Here\u2019s a brief history:\n\n**LeNet** : the first convolution neural network. It was trained to recognize handwritten zip codes.\n\n**AlexNet**: winner of ILSVRC 2012. It achieved the top-5 error rate of 15.6% that was far better than ILSVRC 2011 winner on imagenet, which was 26.2%.\n\n**ZFNet**: achieved 11.2% top-5 error rate and is better than the AlexNet model.\n\n**VGG** (or VGGNet): in ILSVRC 2014 it achieved 7.3% top-5 error rate.\n\n**GoogLeNet** (or Inception): it was the winner in ILSVRC 2014 with 6.7% top-5 error rate\n\n**RestNet**: it achieved the top-5 error rate of 3.57% and is the winner of ILSVRC challenge 2015\n\n**SE-ResNet**: it is the winner of ImageNet challenge 2017 with the top-5 error rate of 2.25%\n\nBtw, ILSVRC stands for \u201cImageNet Large Scale Visual Recognition Challenge\u201d\n\n&amp;#x200B;\n\nI'll be happy to connect with you on your [***favorite social network***](https://withkoji.com/@Nour_Islam)!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p0x97a/your_daily_dose_of_machine_brief_history_of_deep/"}, {"autor": "Snoo28889", "date": "2021-08-09 03:05:25", "content": "Turning Any -----> Image !!!  Into A Van Gogh Painting with Neural Style Transfer in Python", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p0t0dn/turning_any_image_into_a_van_gogh_painting_with/"}, {"autor": "AnalystPrior2982", "date": "2021-08-08 23:16:42", "content": "[Question: Pytorch] Debugging CNN for Image Recognition /!/ Hello,\n\nI'm working on an -----> image !!!  classification task taking pictures and choosing whether it's a dog (0) or cat (1) (binary classification). I have the data set up and validated the data so this is reflected in training. My problem is the loss stay around `0.69` \\- `0.72`, and the prediction is only going up it doesn't seem like it's learning. I'm thinking I've either set up the model wrong or have messed up with optimizer/loss function.\n\n The training can be seen in the image below:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/exnrajavu7g71.png?width=784&amp;format=png&amp;auto=webp&amp;s=dfcc679c19192ca628769c3c3e0bafe6904596c4\n\nHere is some of my code. \n\n    class network(nn.Module):\n    \tdef __init__(self):\n    \t\tsuper().__init__()\n    \t\tself.model = nn.Sequential(\n    \t\t\tnn.Conv2d(3, 32, kernel_size=3, padding=1),\n    \t\t\tnn.ReLU(),\n    \t\t\tnn.Conv2d(32, 64, kernel_size=3, padding=1),\n    \t\t\tnn.ReLU(),\n    \t\t\tnn.MaxPool2d(2,2),\n    \n    \t\t\tnn.Conv2d(64, 128, kernel_size=3, padding=1),\n    \t\t\tnn.ReLU(),\n    \t\t\tnn.Conv2d(128, 128, kernel_size=3, padding=1),\n    \t\t\tnn.ReLU(),\n    \t\t\tnn.MaxPool2d(2,2),\n    \n    \t\t\tnn.Conv2d(128, 256, kernel_size=3, padding=1),\n    \t\t\tnn.ReLU(),\n    \t\t\tnn.Conv2d(256, 256, kernel_size=3, padding=1),\n    \t\t\tnn.ReLU(),\n    \t\t\tnn.MaxPool2d(2,2),\n    \n    \t\t\tnn.Flatten(),\n    \t\t\tnn.Linear(82944, 1024),\n    \t\t\tnn.ReLU(),\n    \t\t\tnn.Linear(1024, 512),\n    \t\t\tnn.ReLU(),\n    \t\t\tnn.Linear(512, 1)\n    \t\t\t)\n    \n    \tdef forward(self, x):\n    \t\treturn nn.functional.relu(self.model(x))\n    \n    optimizer = optim.SGD(model.parameters(), lr=.001, momentum=0.9)\n    \n    loss_function = nn.BCELoss()\n    \n    for epoch in range(2):\n    \ttotal_loss = 0.0\n    \tcnt = 1\n    \tfor batch in train_loader:\n    \t\tinputImages, labels = batch\n    \n    \t\toptimizer.zero_grad()\n    \t\t\n    \t\tprediction = model(inputImages)\n    \n    \t\tloss = loss_function(prediction, labels)\n    \n    \t\tprint(\"Loss {}: {}\".format(cnt, loss))\n    \t\tprint(\"Correct Label: \", labels[0])\n    \t\tprint(\"Predicted label: \", prediction[0])\n    \t\tprint(\"\\n\\n\")\n                    cnt += 1\n    \n    \t\tloss.backward()\n    \n    \t\toptimizer.step()\n\nI'm still learning so any info is greatly appreciated! Thanks, and let me know if you would like to see anything else.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p0papl/question_pytorch_debugging_cnn_for_image/"}, {"autor": "dulldata", "date": "2021-08-08 19:44:29", "content": "Python Tutorial to build -----> Image !!!  to Text App using EasyOCR &amp; Streamlit", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p0lge7/python_tutorial_to_build_image_to_text_app_using/"}, {"autor": "flotothemoon", "date": "2021-02-02 16:40:40", "content": "I made an interactive annotated history of natural language processing, going back to the first formal grammars 2000 years ago /!/ I made a [history of natural language processing](https://www.nlphistory.com/) for the curious &amp; learners. \nIt covers the evolution of methods and concepts spanning centuries.\nBut instead of a boring rundown list, all events and methods are interwoven and presented in a customisable view so you can\n\n - get the big -----> picture !!!  or go deep on *any* part\n - dig into how a specific method came about &amp; what it led to,\n - see how two methods compare, highlighting any connections you find interesting,\n - get as much or as little explanation as you want,\n - simply share your view with a link (no sign-ups or anything required, this is just an experiment)  \n\nFor inspiration, you can start with\n a [a brief general history](https://www.nlphistory.com/?name=A%20Brief%20General%20History&amp;q0=GetEvents&amp;q0_depth=1&amp;q0_scope=0),\n [how we got Transformers](https://www.nlphistory.com/?name=The%20Road%20to%20Sesame%20Street&amp;q0=GetEverythingOnEntity&amp;q0_entity=Transformer&amp;q0_depth=6&amp;q0_scope=16&amp;h0=0&amp;h0_entity=language%20model&amp;h0_mask=34&amp;h1=1&amp;h1_entity=Transformer&amp;h1_mask=34)\n or [the rise of statistical models over formal ones](https://www.nlphistory.com/?name=Formalisms%20vs%20Statistics&amp;q0=GetEverythingOnEntity&amp;q0_entity=formal%20language&amp;q0_depth=4&amp;q0_scope=16&amp;q1=GetEverythingOnEntity&amp;q1_entity=statistical%20model&amp;q1_depth=2&amp;q1_scope=16&amp;h0=0&amp;h0_entity=formal%20language&amp;h0_mask=14&amp;h1=1&amp;h1_entity=statistical%20model&amp;h1_mask=14). \nThese are just examples, I'm sure others more creative than me can come up with more interesting perspectives.\n\nI hope you will find this useful I'm eager to hear what you all think (on content, UX, anything!).\nAnd of course, while I tried to make this comprehensive, I realise it's incomplete and will - despite many passes - still contain some errors, so feel free to point them out and I'll add / amend.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lb0094/i_made_an_interactive_annotated_history_of/"}, {"autor": "Roberthk59", "date": "2021-02-02 16:25:31", "content": "How can I recreate this with myown -----> image !!!  /!/ I want to recreate this intro of MIT 6.S191, but dont know how to start. Can anyone help me? Thank you\nhttps://youtu.be/l82PxsKHxYc", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lazme6/how_can_i_recreate_this_with_myown_image/"}, {"autor": "ARNisUsername1", "date": "2021-02-02 06:06:18", "content": "IMPORTANT THING ABOUT GANs: /!/ When training a GAN in tensorflow with tf.GradientTape(), always remember to specify training=True. When you're trying to generate a -----> picture !!! , remember to specify training=False. If you don't specify this, it won't turn out well. Don't make the same mistake I made.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lap1hx/important_thing_about_gans/"}, {"autor": "alvira77", "date": "2021-02-01 18:42:55", "content": "How MSE works in Random Forest regression? /!/ Hello.\n\nI want to ask about how MSE works in Random Forest regression. I will tell about what i know so far. Please help me clear my confusion or correct me if i'm wrong. First, i find MSE from the sample data. Sample data is the one i get from bootstrapping original dataset. MSE formula is (sklearn):\n\n&amp;#x200B;\n\nhttps://preview.redd.it/67tscxxouwe61.png?width=358&amp;format=png&amp;auto=webp&amp;s=1d8708632976ad57997f49d2b86c29902a788c66\n\nAfter that, i try to find the MSE of each features. Assume the features candidate for splitting is only 2: Dog(s) and Duration(mins). The features are randomly selected from 3 total features because of Random Forest mechanism. Last, i find the Weight MSE of each features candidate and choose the smaller value as a splitting features (here i choose \"Dog(s)\" with variable \"3\".)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/mkizbybruwe61.png?width=2535&amp;format=png&amp;auto=webp&amp;s=3bbf028c5fe98d18550f6df56d800eea8f1bd0fb\n\nAfter the first split is done, there's only 5 data left. Then i did the same process as before to find the best split.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/9zj6suotuwe61.png?width=1889&amp;format=png&amp;auto=webp&amp;s=a3f67e44b83cacbba0ce8abe2aebf8acee7f9e64\n\nLast split\n\n&amp;#x200B;\n\nhttps://preview.redd.it/2yuf1wrvuwe61.png?width=1804&amp;format=png&amp;auto=webp&amp;s=ad010998165b564a198ff2ab2ada58afb1a9734d\n\nMy question is:\n\n1. Am i doing the right thing? **Does the equation i use for MSE and Weight MSE and the process correct?**\u00a0\n2. I found out about Standart Deviation for Regression tree, which is similar to MSE. There's a Standart Deviation Reduction on the last step if we use deviation as a metric for regression tree. **Does MSE need the reduction step too or is it done after we calculate the weight MSE?**\n3. How features sampling work? Is it the same as i did above (**randomly selected some features from total features for that tree's splitting features candidate**)?\u00a0Also, **is it possible to select the same features on other trees?**\n4. On the 2nd -----> picture !!! , where i try to find best split for 2nd split, the mse(total) is higher than the mse(total) from root node (1st -----> picture !!! ). But, the weight mse from each features candidate in 2nd split process is still smaller than the first one. **Is it alright if the mse(total) from split node have higher value than the mse(total) from root node?**\n5. On the 1st picture, i choose \"Dog(s)\" with variable \"3\" as a splitting feature. Then for the 3rd split on 3rd picture, i choose the \"Dog(s)\" feature again but this time with variable \"1\" for splitting because it has the smallest weight mse value. **Can we choose the same feature but with different category or threshold point (for numerical value) in the same tree?**\n\nPlease help me. Any help will be appreciated.\n\nThankyou and sorry if my words is difficult to understand.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/laaknj/how_mse_works_in_random_forest_regression/"}, {"autor": "Locogooner", "date": "2021-02-01 17:32:22", "content": "How hard would it be to learn ML to create art? /!/ I want to create algorithmic art, mainly -----> photography !!!  and video, potentially music. \n\nHow hard would this be to do from scratch? \n\nI have a very basic understanding of code. No formal experience at all.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/la8sgy/how_hard_would_it_be_to_learn_ml_to_create_art/"}, {"autor": "Ok-Cod1551", "date": "2021-02-01 06:35:01", "content": "How to select ROI using colored mask -----> image !!!  /!/ Here is the original image.\n\nhttps://preview.redd.it/q2ukydrf9te61.png?width=1024&amp;format=png&amp;auto=webp&amp;s=a11a98700709260e095084a7c1d2370327aba155\n\nMask:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/cz11o9vj9te61.png?width=1024&amp;format=png&amp;auto=webp&amp;s=0909839a6b2516c0fe1829c944a06a10b3cb3330\n\nI want to find out the pixel distribution in the region highlighted by yellow and red separately.\n\nHow can I achieve this? Thanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l9wv32/how_to_select_roi_using_colored_mask_image/"}, {"autor": "ARNisUsername1", "date": "2021-02-01 02:43:12", "content": "Beginner Machine Learning Projects with data: /!/ I decided to add the projects I've done so far as a beginner in ML, so maybe some people can take ideas, or I can get some ideas from comments.\n\nThese are all the projects I've done as a beginner, ordered in terms of difficulty\n\n1. Breast Cancer Prediction [https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load\\_breast\\_cancer.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html)\n2. Student Performance Prediction [https://archive.ics.uci.edu/ml/datasets/Student+Performance](https://archive.ics.uci.edu/ml/datasets/Student+Performance)\n3. Titanic Prediction [https://www.kaggle.com/c/titanic](https://www.kaggle.com/c/titanic)\n4. House Price Prediction [https://www.kaggle.com/c/house-prices-advanced-regression-techniques/leaderboard](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/leaderboard)\n5. Credit Card Fraud Detection [https://www.kaggle.com/mlg-ulb/creditcardfraud](https://www.kaggle.com/mlg-ulb/creditcardfraud)\n6. Basic Spam Detection [https://www.kaggle.com/uciml/sms-spam-collection-dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset)\n7. Disaster Tweets [https://www.kaggle.com/c/nlp-getting-started](https://www.kaggle.com/c/nlp-getting-started)\n8. Digit Recognizer [https://www.kaggle.com/c/digit-recognizer](https://www.kaggle.com/c/digit-recognizer)\n9. Dog and Cat -----> Image !!!  Classifier [https://www.kaggle.com/tongpython/cat-and-dog](https://www.kaggle.com/tongpython/cat-and-dog)\n10. Model that can do addition or subtraction(Collect data using nested loops)\n11. Text Generator for Trump Tweets with LSTM: [https://www.kaggle.com/arnavsharmaas/total-trump-transcript](https://www.kaggle.com/arnavsharmaas/total-trump-transcript)\n12. Generated digits with GAN: [https://www.kaggle.com/c/digit-recognizer](https://www.kaggle.com/c/digit-recognizer)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l9ssx6/beginner_machine_learning_projects_with_data/"}, {"autor": "DKrat26", "date": "2021-01-31 19:51:08", "content": "Is there an AI that can turn this -----> photo !!!  into a Word table?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l9kbny/is_there_an_ai_that_can_turn_this_photo_into_a/"}, {"autor": "[deleted]", "date": "2021-01-31 19:43:13", "content": "Is there an AI that can turn this -----> photo !!!  into a word table? /!/ [deleted]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l9k5cz/is_there_an_ai_that_can_turn_this_photo_into_a/"}, {"autor": "naregkh", "date": "2021-02-14 18:00:17", "content": "Looking for a model that can run an OCR on -----> image !!!  and then train it to label certain content /!/ Hello everyone,\n\nI  am very new to machine learning and hope I am in the right place to ask  this question. I am a learn by example kind of person so I am reluctant  to hit the books to look for the answer to my query.\n\nI  want to train a machine learning model that will be able to take an  image and recognize content on there while labeling it. It should be  able to take the unstructured data and make it structured again. I used  Nanonets which has a similar feature where I can upload images, label  them myself, and the model slowly learns how to recognize the patterns. I  would like to build it out myself though and learn by doing it.\n\nCan anyone point my in the right direction of what I should be looking for or maybe a YouTube tutorial?\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ljthxj/looking_for_a_model_that_can_run_an_ocr_on_image/"}, {"autor": "666GZL", "date": "2021-02-13 17:29:24", "content": "-----> Image !!!  completion with -----> Image !!! -GPT /!/ I would like to some image completion with OpenAI's Image GPT (like shown on their website), but i can't find any useful documentation or tutorials on how to use the model. \nI found a Colab Notebook that let me do some sampling but only with 32x32 images (model S).\n\nI would be thankful for any advice :))", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lj4w0w/image_completion_with_imagegpt/"}, {"autor": "Training_Engineer_83", "date": "2021-02-13 16:53:10", "content": "ID card detection and OCR using ML/DL /!/ [BlinkID](https://microblink.com/products/blinkid) really did a great job. I found this gif on their [github](https://github.com/BlinkID/blinkid-android). I'm also working on a same kind of project, which is based on template matching. And obviously, it has its own limitations. I want to use ML/DL in order to improve system just like BlinkID. I already tried tensorflow object detection api and tflite for mobile devices to detect id cards. But I don't know, maybe I'm going in wrong direction. \n\n**Does anyone knows how to implement such a robust and automated system like BlinkID ? or any resources which can help me to get some more information about basic pipeline.** \n\nI also tried to find it but didn't get any proper workflow. I'm keen to work on it as soon as possible. \n\nAnd also my question is, If I have enough data about cards, in which manner I should train it? whether I've to train it to detect roi from an -----> image !!!  or train to extract information from an -----> image !!! (ocr)? \n\n*And the major problem is, how to get perfect birds-eye view of the card because it may captured from any angle just like this video.* \n\nI don't know how these guys came with this masterpiece solution.!! I understand that it took lots of efforts and hard work. But I eagerly wanted to know the magic behind this kind of automated system. \n\n[Microblink - BlinkID](https://i.redd.it/cbcy0hxwx9h61.gif)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lj45vt/id_card_detection_and_ocr_using_mldl/"}, {"autor": "KosvvSt", "date": "2021-02-13 15:33:39", "content": "Visual Studio, 2 ML Models in the same project? /!/  How could I have two machine learning models in the same project (c#, visual studio 2019) ? I want to have text and -----> image !!!  classification for something in my project but I am not sure how to have 2 models in the same project as if I try to add a new second model using the [ML.NET](https://ML.NET) Model Builder it wants me to replace the old one.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lj2my0/visual_studio_2_ml_models_in_the_same_project/"}, {"autor": "EdgeAI_CV_Fanatic", "date": "2021-02-13 15:22:46", "content": "Created a video series on how to use embedded cameras in computer vision applications/products for engineers and Product Managers. Show some love! /!/ Over the course of 15+ years, my team has built and incorporated embedded cameras in over 300 products. Being social media and Video novices, we have ventured out for the first time to share how to choose the right embedded -----> camera !!!  for your edge-based computer vision product. Check out the videos and share your feedback \ud83d\ude0a \ud83d\ude0a\n\nEmbedded Camera video Series for Engineers:\n\n\u00b7 The core components of Edge AI-based computer vision applications - [https://www.youtube.com/watch?v=wXObjY81LIU&amp;list=PLdHSbfwH2xmSlLVztVK\\_AJWDtl6-SOvbS&amp;ab\\_channel=VisAILabs](https://www.youtube.com/watch?v=wXObjY81LIU&amp;list=PLdHSbfwH2xmSlLVztVK_AJWDtl6-SOvbS&amp;ab_channel=VisAILabs)\n\n\u00b7 Four ways to choose an embedded camera for your Edge AI-based Computer Vision Application -[https://www.youtube.com/watch?v=3AVs9NNhP5w&amp;list=PLdHSbfwH2xmSlLVztVK\\_AJWDtl6-SOvbS&amp;index=2&amp;ab\\_channel=VisAILabs](https://www.youtube.com/watch?v=3AVs9NNhP5w&amp;list=PLdHSbfwH2xmSlLVztVK_AJWDtl6-SOvbS&amp;index=2&amp;ab_channel=VisAILabs)\n\n\u00b7 How to choose the right image sensor &amp; optics for your Edge AI-based CV application? - [https://www.youtube.com/watch?v=MiDnIfs0M7w&amp;list=PLdHSbfwH2xmSlLVztVK\\_AJWDtl6-SOvbS&amp;index=3&amp;ab\\_channel=VisAILabs](https://www.youtube.com/watch?v=MiDnIfs0M7w&amp;list=PLdHSbfwH2xmSlLVztVK_AJWDtl6-SOvbS&amp;index=3&amp;ab_channel=VisAILabs)\n\n\u00b7 What is the need for an Image Signal Processor (ISP) in an embedded camera? [https://www.youtube.com/watch?v=As6bklZizmg&amp;list=PLdHSbfwH2xmSlLVztVK\\_AJWDtl6-SOvbS&amp;index=4&amp;ab\\_channel=VisAILabs](https://www.youtube.com/watch?v=As6bklZizmg&amp;list=PLdHSbfwH2xmSlLVztVK_AJWDtl6-SOvbS&amp;index=4&amp;ab_channel=VisAILabs)\n\n\u00b7 How to use Off-the-Shelf embedded Camera Component for Fast Prototyping CV application? - [https://www.youtube.com/watch?v=IgrNLZX3iOo&amp;list=PLdHSbfwH2xmSlLVztVK\\_AJWDtl6-SOvbS&amp;index=6&amp;ab\\_channel=VisAILabs](https://www.youtube.com/watch?v=IgrNLZX3iOo&amp;list=PLdHSbfwH2xmSlLVztVK_AJWDtl6-SOvbS&amp;index=6&amp;ab_channel=VisAILabs)\n\nEmbedded camera recommendations for Specific Industry Verticals for Product Managers:\n\n\u00b7 How to Choose the right image sensor/embedded camera for robotic Arms and Autonomous Mobile Robots - [https://visailabs.com/choose-the-right-image-sensor-embedded-camera-for-robotic-arms-and-autonomous-mobile-robots/](https://visailabs.com/choose-the-right-image-sensor-embedded-camera-for-robotic-arms-and-autonomous-mobile-robots/)\n\n\u00b7 How to Choose the right embedded camera for smart city applications - [https://visailabs.com/choosing-the-right-embedded-camera-for-smart-city-applications/](https://visailabs.com/choosing-the-right-embedded-camera-for-smart-city-applications/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lj2fsm/created_a_video_series_on_how_to_use_embedded/"}, {"autor": "TuckerMcInnes", "date": "2021-02-13 12:22:28", "content": "Do images need to be the same size, color, etc. for a successful training set? /!/ Hi there\n\nI need to create a training set to help detect certain properties in images (at a bit level). All I know is the photos will be passport photos. But I feel this is vague:\n\n* Passport photos can be different sizes (1 inch, or 1.5 inches, etc.)\n\n* Even if the photos are of the same format (e.g. PNG) they might be RGB, or RGBA, or 8 bit, or 16 bit, etc.\n\n* I don't know if the photos will be color or black and white.\n\nSo this is my question:\n\nCan a training set be a mix of all of the above (different sizes, colours, etc.), or should I create separate training sets for black and white photos, for color photos, for 8 bit photos, etc.?\n\nWhat I want is to have some model which I can use to test if an -----> image !!!  has certain bit properties.\n\nThanks for your advice!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lizlgw/do_images_need_to_be_the_same_size_color_etc_for/"}, {"autor": "radifuddinahmed", "date": "2021-02-13 08:52:32", "content": "Tensorflow GPU for GTX960M /!/ I am trying to use tensorflow GPU in pyCharm IDE. my configuration is given below\n\n* Laptop Asus GL552VW, \n* Ubuntu 20.04, \n* GTX 960M (driver version 460), \n* Python 3.8, \n* Tensorflow 2.3.0, \n* CUDA 10.1, \n* CuDNN 7.6.5\n\nbut my code is giving following error **\"CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel -----> image !!!  is invalid\"**\n\nPlease help!!!!\n\n&amp;#x200B;\n\n**full error message given below**\n\n*/home/radif/PycharmProjects/pythonProject/venv/bin/python /home/radif/PycharmProjects/pythonProject/predict\\_4tensorArrayTest.py*\n\n*2021-02-13 14:31:42.077565: I tensorflow/stream\\_executor/platform/default/dso\\_loader.cc:48\\] Successfully opened dynamic library libcudart.so.10.1*\n\n*2021-02-13 14:31:43.057603: I tensorflow/stream\\_executor/platform/default/dso\\_loader.cc:48\\] Successfully opened dynamic library libcuda.so.1*\n\n*2021-02-13 14:31:43.087628: I tensorflow/stream\\_executor/cuda/cuda\\_gpu\\_executor.cc:982\\] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero*\n\n*2021-02-13 14:31:43.088184: I tensorflow/core/common\\_runtime/gpu/gpu\\_device.cc:1716\\] Found device 0 with properties:* \n\n*pciBusID: 0000:01:00.0 name: GeForce GTX 960M computeCapability: 5.0*\n\n*coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 74.65GiB/s*\n\n*2021-02-13 14:31:43.088206: I tensorflow/stream\\_executor/platform/default/dso\\_loader.cc:48\\] Successfully opened dynamic library libcudart.so.10.1*\n\n*2021-02-13 14:31:43.089772: I tensorflow/stream\\_executor/platform/default/dso\\_loader.cc:48\\] Successfully opened dynamic library libcublas.so.10*\n\n*2021-02-13 14:31:43.091203: I tensorflow/stream\\_executor/platform/default/dso\\_loader.cc:48\\] Successfully opened dynamic library libcufft.so.10*\n\n*2021-02-13 14:31:43.091512: I tensorflow/stream\\_executor/platform/default/dso\\_loader.cc:48\\] Successfully opened dynamic library libcurand.so.10*\n\n*2021-02-13 14:31:43.093107: I tensorflow/stream\\_executor/platform/default/dso\\_loader.cc:48\\] Successfully opened dynamic library libcusolver.so.10*\n\n*2021-02-13 14:31:43.094029: I tensorflow/stream\\_executor/platform/default/dso\\_loader.cc:48\\] Successfully opened dynamic library libcusparse.so.10*\n\n*2021-02-13 14:31:43.097373: I tensorflow/stream\\_executor/platform/default/dso\\_loader.cc:48\\] Successfully opened dynamic library libcudnn.so.7*\n\n*2021-02-13 14:31:43.097508: I tensorflow/stream\\_executor/cuda/cuda\\_gpu\\_executor.cc:982\\] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero*\n\n*2021-02-13 14:31:43.098445: I tensorflow/stream\\_executor/cuda/cuda\\_gpu\\_executor.cc:982\\] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero*\n\n*2021-02-13 14:31:43.098950: I tensorflow/core/common\\_runtime/gpu/gpu\\_device.cc:1858\\] Adding visible gpu devices: 0*\n\n*2021-02-13 14:31:43.099179: I tensorflow/core/platform/cpu\\_feature\\_guard.cc:142\\] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA*\n\n*To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.*\n\n*2021-02-13 14:31:43.104810: I tensorflow/core/platform/profile\\_utils/cpu\\_utils.cc:104\\] CPU Frequency: 2599990000 Hz*\n\n*2021-02-13 14:31:43.105231: I tensorflow/compiler/xla/service/service.cc:168\\] XLA service 0x6170d40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:*\n\n*2021-02-13 14:31:43.105289: I tensorflow/compiler/xla/service/service.cc:176\\]   StreamExecutor device (0): Host, Default Version*\n\n*2021-02-13 14:31:43.135926: I tensorflow/stream\\_executor/cuda/cuda\\_gpu\\_executor.cc:982\\] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero*\n\n*2021-02-13 14:31:43.136282: I tensorflow/compiler/xla/service/service.cc:168\\] XLA service 0x6205b40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:*\n\n*2021-02-13 14:31:43.136297: I tensorflow/compiler/xla/service/service.cc:176\\]   StreamExecutor device (0): GeForce GTX 960M, Compute Capability 5.0*\n\n*2021-02-13 14:31:43.136463: I tensorflow/stream\\_executor/cuda/cuda\\_gpu\\_executor.cc:982\\] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero*\n\n*2021-02-13 14:31:43.136711: I tensorflow/core/common\\_runtime/gpu/gpu\\_device.cc:1716\\] Found device 0 with properties:* \n\n*pciBusID: 0000:01:00.0 name: GeForce GTX 960M computeCapability: 5.0*\n\n*coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 74.65GiB/s*\n\n*2021-02-13 14:31:43.136731: I tensorflow/stream\\_executor/platform/default/dso\\_loader.cc:48\\] Successfully opened dynamic library libcudart.so.10.1*\n\n*2021-02-13 14:31:43.136787: I tensorflow/stream\\_executor/platform/default/dso\\_loader.cc:48\\] Successfully opened dynamic library libcublas.so.10*\n\n*2021-02-13 14:31:43.136815: I tensorflow/stream\\_executor/platform/default/dso\\_loader.cc:48\\] Successfully opened dynamic library libcufft.so.10*\n\n*2021-02-13 14:31:43.136835: I tensorflow/stream\\_executor/platform/default/dso\\_loader.cc:48\\] Successfully opened dynamic library libcurand.so.10*\n\n*2021-02-13 14:31:43.136856: I tensorflow/stream\\_executor/platform/default/dso\\_loader.cc:48\\] Successfully opened dynamic library libcusolver.so.10*\n\n*2021-02-13 14:31:43.136876: I tensorflow/stream\\_executor/platform/default/dso\\_loader.cc:48\\] Successfully opened dynamic library libcusparse.so.10*\n\n*2021-02-13 14:31:43.136897: I tensorflow/stream\\_executor/platform/default/dso\\_loader.cc:48\\] Successfully opened dynamic library libcudnn.so.7*\n\n*2021-02-13 14:31:43.136959: I tensorflow/stream\\_executor/cuda/cuda\\_gpu\\_executor.cc:982\\] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero*\n\n*2021-02-13 14:31:43.137231: I tensorflow/stream\\_executor/cuda/cuda\\_gpu\\_executor.cc:982\\] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero*\n\n*2021-02-13 14:31:43.137455: I tensorflow/core/common\\_runtime/gpu/gpu\\_device.cc:1858\\] Adding visible gpu devices: 0*\n\n*2021-02-13 14:31:43.137477: I tensorflow/stream\\_executor/platform/default/dso\\_loader.cc:48\\] Successfully opened dynamic library libcudart.so.10.1*\n\n*Traceback (most recent call last):*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/predict\\_4tensorArrayTest.py\", line 264, in &lt;module&gt;*\n\n*model = model\\_from\\_json(model\\_json)*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/keras/saving/model\\_config.py\", line 122, in model\\_from\\_json*\n\n*return deserialize(config, custom\\_objects=custom\\_objects)*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/keras/layers/serialization.py\", line 171, in deserialize*\n\n*return generic\\_utils.deserialize\\_keras\\_object(*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic\\_utils.py\", line 354, in deserialize\\_keras\\_object*\n\n*return cls.from\\_config(*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\", line 484, in from\\_config*\n\n*model = cls(name=name)*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in \\_method\\_wrapper*\n\n*result = method(self, \\*args, \\*\\*kwargs)*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\", line 116, in \\_\\_init\\_\\_*\n\n*super(functional.Functional, self).\\_\\_init\\_\\_(  # pylint: disable=bad-super-call*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in \\_method\\_wrapper*\n\n*result = method(self, \\*args, \\*\\*kwargs)*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 308, in \\_\\_init\\_\\_*\n\n*self.\\_init\\_batch\\_counters()*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\", line 457, in \\_method\\_wrapper*\n\n*result = method(self, \\*args, \\*\\*kwargs)*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 317, in \\_init\\_batch\\_counters*\n\n*self.\\_train\\_counter = variables.Variable(0, dtype='int64', aggregation=agg)*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 262, in \\_\\_call\\_\\_*\n\n*return cls.\\_variable\\_v2\\_call(\\*args, \\*\\*kwargs)*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 244, in \\_variable\\_v2\\_call*\n\n*return previous\\_getter(*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 237, in &lt;lambda&gt;*\n\n*previous\\_getter = lambda \\*\\*kws: default\\_variable\\_creator\\_v2(None, \\*\\*kws)*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/ops/variable\\_scope.py\", line 2633, in default\\_variable\\_creator\\_v2*\n\n*return resource\\_variable\\_ops.ResourceVariable(*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/ops/variables.py\", line 264, in \\_\\_call\\_\\_*\n\n*return super(VariableMetaclass, cls).\\_\\_call\\_\\_(\\*args, \\*\\*kwargs)*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/ops/resource\\_variable\\_ops.py\", line 1507, in \\_\\_init\\_\\_*\n\n*self.\\_init\\_from\\_args(*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/ops/resource\\_variable\\_ops.py\", line 1650, in \\_init\\_from\\_args*\n\n*initial\\_value = ops.convert\\_to\\_tensor(*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1499, in convert\\_to\\_tensor*\n\n*ret = conversion\\_func(value, dtype=dtype, name=name, as\\_ref=as\\_ref)*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/framework/tensor\\_conversion\\_registry.py\", line 52, in \\_default\\_conversion\\_function*\n\n*return constant\\_op.constant(value, dtype, name=name)*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant\\_op.py\", line 263, in constant*\n\n*return \\_constant\\_impl(value, dtype, shape, name, verify\\_shape=False,*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant\\_op.py\", line 275, in \\_constant\\_impl*\n\n*return \\_constant\\_eager\\_impl(ctx, value, dtype, shape, verify\\_shape)*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant\\_op.py\", line 300, in \\_constant\\_eager\\_impl*\n\n*t = convert\\_to\\_eager\\_tensor(value, ctx, dtype)*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/framework/constant\\_op.py\", line 97, in convert\\_to\\_eager\\_tensor*\n\n*ctx.ensure\\_initialized()*\n\n  *File \"/home/radif/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/tensorflow/python/eager/context.py\", line 539, in ensure\\_initialized*\n\n*context\\_handle = pywrap\\_tfe.TFE\\_NewContext(opts)*\n\n*tensorflow.python.framework.errors\\_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid*\n\n&amp;#x200B;\n\n*Process finished with exit code 1*", "link": "https://www.reddit.com/r/learnmachinelearning/comments/liwype/tensorflow_gpu_for_gtx960m/"}, {"autor": "QuasiEvil", "date": "2021-05-13 01:59:16", "content": "Class balance in pixel mask segmentation (eg. U-net) /!/ For the problem of classifying individual pixels in an -----> image !!!  into, eg., background/0 or object/1 classes, I'm confused about how to balance my training examples. Lets assume a super simple case where my objects are squares and my only classes are as above (pixel belongs to background - 0; pixel belongs to object - 1). Across all my training examples, I have a ratio of 0s to 1s of 75:25. Quite unbalanced! In the extreme, I could keep only those training examples where the square occupies half the image but then I wouldn't be training for size variations. Alternatively, I could build a training set containing n p x p examples, n 2p x 2p examples, n 3p x 3p examples, and so on - basically, and equal numbers of particular square sizes, but then I'm back to losing my pixel-wise balance!\n\nSurely someone smarter than me has thought about this!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nb5oad/class_balance_in_pixel_mask_segmentation_eg_unet/"}, {"autor": "TDTK33rus", "date": "2021-05-12 14:16:29", "content": "Combining training images for object detection /!/ I'm preparing a dataset that will be used to train yolo network. A lot of data comes from the internet and I'm concerned about the training value of the images. The problem is that the objects that i want the network to learn how to recognize are placed in the center in most of the pictures so as far as I understand the network will adjust it's anchors and learn that object X is something big in the center of the -----> image !!! . My question is are there any data processing techniques that focus on ground truth placement? I thought about combining the images in such way that should suggest the network more real-life placement of objects but I haven't found any example so I'm not sure if it's even a serious problem.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/napq04/combining_training_images_for_object_detection/"}, {"autor": "agonytool", "date": "2021-05-12 07:44:55", "content": "How to efficiently store images? /!/ Recently I bought -----> image !!!  dataset for use for production and stored the attributes in the names (eg. Index, Location, type, style...) but then I was criticised for storing the information as the filename.  He said there is definitely a better method to store the metadata maybe into xml files. I tried searching the internet but could not find the answer, any suggestions?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/naj77q/how_to_efficiently_store_images/"}, {"autor": "Rishit-dagli", "date": "2021-05-12 04:09:10", "content": "[Project] Low Light -----> image !!!  enhancement using ML", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nag2k1/project_low_light_image_enhancement_using_ml/"}, {"autor": "_-__-____", "date": "2021-05-12 00:31:33", "content": "Need Cloud VM with Docker Image /!/ I am trying to do something using a paper's public code as my base framework. Rather than supplying a requirements.txt, there is an associated Docker -----> image !!!  on Dockerhub. Initially I had planned to use Colab, but it lacks Docker support. I don't have my own GPU, so need to use some sort of cloud service. \n\nDoes anyone know a service where this is simple to do? I'm in the middle of trying to figure out how to do this on Google Cloud Platform, but it has so many features that the documentation is not very clear and I'm running into problems. Is there another service where I can simply spin up a GPU VM, pull the image from Dockerhub, run it and get to work? Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nac2yv/need_cloud_vm_with_docker_image/"}, {"autor": "Blutorangensaft", "date": "2021-05-11 16:21:01", "content": "Data augmentation for very small -----> image !!!  datasets /!/ Hey folks,  \n\n\nI am looking for techniques for augmenting very small image datasets. I have a classification problem with 3 classes. Each class consists of 20 different shapes. Per shape, I have between 1 and 35 training examples. For two classes, I have 25 training examples per shape, but the number of examples per shape for the third classes is usually around 5. Now, what data augmentation schemes do you recommend? I have looked into geometric transformations, which seem like a reasonable place to start. However, I have also thought of applying FFT (do the forward transform, add some noise, do the inverse transform). GANs seem infeasible, right? Not enough data, I suspect. Let me know about your opinions and advice in the comments. Cheers!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/na0tri/data_augmentation_for_very_small_image_datasets/"}, {"autor": "grid_world", "date": "2021-05-11 04:46:48", "content": "PyTorch Transfer Learning /!/ Used Transfer Learning with ResNet-50 on CIFAR-10 in PyTorch to achieve val\\_accuracy = 92.58%.\n\n**Key takeaway:** Change the first conv layer to have the hyper-parameters kernel\\_size = (3, 3), stride = (1, 1) and padding = (1, 1) instead of the original ones since CIFAR-10 dataset has much smaller -----> image !!! s and using the original conv layer hyper-parameters reduces the -----> image !!!  size due to which the resulting model performs not so good, according to my experiments.\n\nThoughts?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n9p2qo/pytorch_transfer_learning/"}, {"autor": "Jcwscience", "date": "2021-05-10 19:42:40", "content": "I\u2019m looking for a type of neural network that I can give an input -----> image !!!  and have it try to match the result yo an output -----> image !!! . Would an auto-encoder work? /!/ Or there is something else and you could point me the right direction I would appreciate it.\n\nAlso if anyone knows of any tutorials for something like this that aren\u2019t overly specific to one thing that they can\u2019t be adapted and built upon,,, that would be awesome. Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n9e3gi/im_looking_for_a_type_of_neural_network_that_i/"}, {"autor": "whenihittheground", "date": "2021-05-25 01:00:47", "content": "What are some best practices and examples for laying out code and files with pytorch? /!/ I am trying to learn best practices for laying out my code / file structure when developing neural nets with pytorch with the goal being easy experimentation and extension. Does anyone have any \"clean\" NN examples that they really like which I can take a look at? \n\nRight now my file structure is:\n\n1. Networks.py where I have several different NN architectures I want to play with for example a fully connected net and a basic cnn.\n\n2. Layers.py and Activations.py where I have custom layers and activation functions (just to play with pytorch really I plan on sticking with basic stuff like relu).\n\n3. Finally I have a \"main\" module where I import the data and do the training + validation steps. This MLP.py module just imports the fully connected model from the networks.py module.\n\nWhile the sources below are nice I'm still not sure if I should write separate training and validation modules or if I should make these functions instead and just define them for each of my \"main\" modules since I'll probably need to do minor data reshaping or other random stuff depending on the model architecture I am experimenting with. I'm leaning towards the latter but I am curious what other people's experience / perspective is. For the record I plan on playing with pytorch lightning later.\n\nThe sources I've looked at currently:\n\n1. [Igor Susmelj's PyTorch best practices &amp; Styleguide](https://github.com/IgorSusmelj/pytorch-styleguide)\n\n2. [Branislav Holl\u00e4nder's project outline](https://github.com/branislav1991/PyTorchProjectFramework)\n\n3. [Natalia Gimelshein's many -----> image !!!  net examples](https://github.com/pytorch/examples/blob/master/-----> image !!! net/main.py)\n\n**TLDR**: How to layout code/files? Do you prefer separate training and validation modules or do you define them as functions?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nkd8gg/what_are_some_best_practices_and_examples_for/"}, {"autor": "PositronB", "date": "2021-05-24 17:57:35", "content": "How to work with images of different dimensions? [Semantic Segmentation] /!/ Currently, I am using U-Net architecture to train a model for the task of semantic segmentation. My training images are of varied dimensions from 1100x700 to 3300x5700. So, the dimension of an -----> image !!!  could be anything. What my model is currently doing is, it's first resizing the images to 256x256 and then training on those images. The results from this aren't bad but my guess it can be much better if I train on images of the same dimensions. \n\nI was thinking of patchifying each image to 256x256 small images and training on that but I don't know how it will work for odd dimensions, say for a 1001x1001 image. Like how it will break it into patches of 256x256? Also, I don't know how it will assemble the images back to the original dimensions to provide a mask for a higher dimension image. Any sort of suggestion will help a lot.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nk3zvc/how_to_work_with_images_of_different_dimensions/"}, {"autor": "Chilaquil420", "date": "2021-05-24 16:15:20", "content": "For an -----> image !!!  classifier for hand gestures, with Transfer Learning, which base model would work best? Inception, ImageNET, Resnet, or which would you suggest? /!/ The input will be only hands, no faces or anything", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nk1njt/for_an_image_classifier_for_hand_gestures_with/"}, {"autor": "Blutorangensaft", "date": "2021-05-23 21:43:23", "content": "Feature pooling with fisher vectors /!/ Hey folks,\n\nI'm working on an -----> image !!!  classification task and I'm exploring traditional machine learning approaches. The idea is to extract features from an image using different feature extraction methods (eg SIFT) , pool the features using fisher vectors, and finally classify the vectors using k-means. However, the images I extract the features from often yield different numbers of features. The implementation of fisher vectors I intend to use does not accept that. How do I solve this issue?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nji309/feature_pooling_with_fisher_vectors/"}, {"autor": "alxcnwy", "date": "2021-05-23 16:50:10", "content": "Need help from someone with a Baidu account /!/ Hi,\n\nI'm trying to download the pre-trained [DoveNet](https://github.com/bcmi/Image-Harmonization-Dataset-iHarmony4/tree/master/DoveNet) model (for -----> image !!!  harmonization) but it seems you need a Baidu account to download from Baidu Cloud. I've spent almost 2 hours google-translating my way through the app but I don't think I can register without a Chinese phone number.\n\nI would really really appreciate it if you could please download the model and upload it to Google drive or somewhere else \ud83d\ude05\n\n[https://pan.baidu.com/s/12oGrBF88O-x0BlWGVkMjag](https://pan.baidu.com/s/12oGrBF88O-x0BlWGVkMjag)\n\nAccess code: 8q8a\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/njbp5g/need_help_from_someone_with_a_baidu_account/"}, {"autor": "phi_array", "date": "2021-05-23 14:06:36", "content": "Tips for 3 class classification Keras images? /!/ Hi, I\u2019m trying to make a 3 class -----> image !!!  classifier in keras : Rock Paper Scissors. I need to gather pictures of hands making those signs. Were could I go, also, what architecture for the NN should I use?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nj8g44/tips_for_3_class_classification_keras_images/"}, {"autor": "daddygoose04", "date": "2021-05-23 12:23:49", "content": "i need help with a naive bayes classifier /!/  \n\nhey guys, hope you're doing well.\n\nI need to implement a naive Bayes classifier for digit recognition but have had really bad results and I'm unsure what to do.\n\nMy idea is to convert the images to grayscale, get the pixel values for each pixel and then simply count how many times that value occurred for that pixel in each class and then calculate the probability. i do this for all pixels of the -----> image !!!  and then multiply the values.\n\nIf anyone has done anything like this, it would be great if you could help me out.\n\nThanks a lot,\n\nHave a good day", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nj6pkn/i_need_help_with_a_naive_bayes_classifier/"}, {"autor": "AdelSexy", "date": "2021-05-23 12:16:26", "content": "ML/DL job hunting. Points of attention. /!/ Hey guys! This is my post about job hunting in ML/DL area. Original is [here](https://irregularadel.substack.com/p/mldl-job-hunting-points-if-attention).   \nYou can look at this guide as a tutorial on how to find ML position that fits you.   \nShare your experience in job hunting, feel free to expand those points of attention!\n\nML/DL positions are growing in numbers very fast, and most of the time it is hard to understand if the position you are applying to worth it and if you and the company are a good match. Just look at this search of machine learning or data scientist in Linkein - over 300k positions.\n\n[ AI jobs everywhere. I know some part of this 300k+ jobs are PMs and stuff, but it's still a lot. ](https://preview.redd.it/4ahln7ul2v071.png?width=1090&amp;format=png&amp;auto=webp&amp;s=a387f688a709bcaa3990ee4002567e9f8a4cf77d)\n\nOf course, there are tasty positions in google, Deep mind, Amazon, Apple, Tesla, etc. But it is hard to get there, and, probably, even harder to work. Meanwhile, there a lot of other companies and positions, that can be quite a good option. The problem is - the field is so hyped, that some positions are crap and some are surprisingly good while it is hard to find out which is what.\n\nTo help in that complex choice, I designed these 6 points of attention while job hunting in the ML area. Using them, you can ease this hard and exhausting process and find a good matching position.\n\n# Step 0. Define what you truly want.\n\nAsk yourself where your passion lies. Is it fundamental research? Applied research? Engineering? What field is it: computer vision, natural language processing, time series analyses? Or maybe recommendation systems? From answers to that questions, you can define directions. You will get an idea, where you want to work: academia, corporation, start-up? Do you want to develop satelite -----> image !!!  analysis neural networks for a small growing start-up with a production-focused team? Or maybe you want to develop text/speech recognition models for internal use by the support department in a big retail corporation? What about in-depth research of generative models foundations?\n\nYou name it.\n\nThis step is essential - it will help you to get in peace with yourself, narrow down the search area, define criteria for a future position. To be honest, I think this is already 80% of success.\n\nAlso, make sure what companies understand under the name of the position you are interested in. You might be surprised how many different definitions are there for ML engineers, or, even worse, Data scientists. Try to get on the same page with the future employer. You don't want to program web applications, while you thought you will train neural networks. (I wish I were exaggerating, but this is a real example).\n\nNow, suppose you have all the answers. You applied to 20+ positions and started interviewing process on some of them.\n\n# Step 1. Ask about AI strategy.\n\nDoes the company has an AI roadmap? If yes, ask to elaborate on it. What are the goals for 5 years? How they are planning to get benefit from ML/DL. How essential is it for a company? What are the main development directions?\n\n*Processing img dmdw9ptt2v071...*\n\nIf there is no AI strategy in the compay, it is not necessarily a bad thing. It might be, that company thinks about it and ready to work on a long-term vision. And maybe you will be one of the pioneers. Do you want it? Or you want to work mostly on content and not be involved in high-level roadmaps development? The choice is yours.\n\nBut in any way, please. make sure that there is no \"ML for ML\". There should be the business value that will bring need in ML.\n\n# Step 2. Ask about data governance\n\nThat's super important since you can't work without good data governance. So it would be wise to know from the beginning what databases are there, where the data is coming from, how big is the data? Is there any annotation tool the company uses? If not, are they willing to pay attention to that (and that's the bridge to AI strategy)? How many training-ready datasets do they have? Do they have version control of the data?\n\n[ Love this slide from one of lectures of Andrey Karpathy - Director of AI at Tesla ](https://preview.redd.it/x4lfas5w2v071.png?width=1988&amp;format=png&amp;auto=webp&amp;s=aa69d8f242a6f0ca03da762bacfcab60aa59c2be)\n\nAlways remember garbage in -&gt; garbage out. Your comfort and effectiveness depend on that.\n\n# Step 3. Ask about the team: roles, plans for positions, working style.\n\nYou do want to know who your potential colleagues are. Are there any ML/DL experts? Newbies? Does the team have interns from time to time? Any data engineers? Full-stack developers? Is the team centralized or there are islands of expertise across the company? How many meetings do they have? What meeting? Growth plans? Do they visit conferences? Attend in them? What they wait for from you?\n\nYou have the right to understand your place in the team and the company before you go there to work. You should see your growth potential.\n\nFrom the answers, you can get an idea of how connected the team is as well as if you will fit in there.\n\n# Step 4. Any success stories?\n\nWhat has already been done in the company in the ML/DL field? Are there any working solutions? Use cases? Proves of concepts? That can give you an understanding of what to expect. You can easily get in the working rhythm if it already beats. Or you can try to create your own if there are conditions for it.\n\nSuccess stories can also indicate that the company knows the main customers/stakeholders, which is already a great achievement.\n\n# Step 5. Stack the company uses\n\nCheck the technology stack the team is using. Try to understand if they know, what they are doing. And if they don\u2019t, check if they realize that.\n\nMake sure the team has structured methods of research/production or at least is willing to have one.\n\n[ Just look at this giant landscape - credits goes to Full-Stack Deep learning course. ](https://preview.redd.it/qy4evtwy2v071.png?width=1952&amp;format=png&amp;auto=webp&amp;s=2aab9e8968552c552859e48d770614f9fa99508a)\n\nHow do they track their experiments? What frameworks/packages do they use? Is their code is just a pile of Jupiter notebooks? Do they work in their MLops? How they store and deploy the models? Who owns the models?This is the base for the successfull work, don\u2019t underestimate it\u2019s influence.\n\nAs a comclusion, I believe that there are ton\u2019s of beautiful machine learning positions out there. I know by my own skin, that job hunting can be exhasting and can suck all fun of your life. An I hope these few points of attention can help you ease the pain.\n\nGood luck!\n\nP.S. have a look at my blog [here.](https://irregularadel.substack.com/about)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nj6lda/mldl_job_hunting_points_of_attention/"}, {"autor": "wstanley38", "date": "2021-05-23 07:44:30", "content": "Find the misclassified samples in CNN /!/ How can I find out exactly which images are misclassified in the CNN model? \n\nI want to know, for example, that a Cat -----> picture !!!  named cat(54).jpg is misclassified as Dog. Is there a way to know this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nj2ssx/find_the_misclassified_samples_in_cnn/"}, {"autor": "Puzzleheaded_Tea_310", "date": "2021-05-22 20:00:22", "content": "What is the popular ways to build -----> image !!!  classification model? /!/ I try to make a model that can classify if a person in the photo wearing mask correctly, incorrectly or not wearing mask. So what is the benefit if I build from scratch or fine-tuning, transfer learning a pre trained model? Which one is better? And which framework should I use to build it?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/niqr8e/what_is_the_popular_ways_to_build_image/"}, {"autor": "rshpkamil", "date": "2021-05-19 14:49:00", "content": "Holistic Video Scene Understanding /!/ Understanding what is in the -----> image !!!  or a video frame is crucial for many computer vision applications. Researchers from Google recently published a paper where they got state-of-the-art results in panoptic segmentation (see below for intuitive explanation). They achieved it by combining information about depth, existing and new objects from two consequent frames.\n\nLink to the blogpost and supplementary material that helps to understand the inverse projection problem can be found [here](https://news.thereshape.co/holistic-video-scene-understanding-with-vip-deeplab).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ng7urn/holistic_video_scene_understanding/"}, {"autor": "AtenRa85", "date": "2021-05-19 12:58:34", "content": "Weka &amp; DL4j - Image Iteration /!/ I am hoping someone is familiar with -----> image !!!  classification using the dl4j library in Weka, using the GUI  \n\n\nI can train and save my model just fine - but I am having trouble figuring out how to point the model to new data to classify. \n\nMy training.arff and newData.arff are in \"abc123.jpg, class\" format. For training the model I set the image iterator location to the folder with all of my training images.  However, when I try to run the model on the newData.arff, it seems to want to look in the test folder for images, and I am failing to understand where I can set the filepath for the newData.arff\n\nI'm positive that I am overlooking a simple setting, but frustration has set in, and any help would be greatly appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ng54s9/weka_dl4j_image_iteration/"}, {"autor": "Sufi_99", "date": "2021-05-18 18:02:07", "content": "[Help] Calculating angles from poses, OpenPose /!/ Hi. I need to calculate the angle a person is making with the -----> camera !!!  using pose estimation. I am usinga clone of library[tf-openpose library](https://github.com/ZheC/tf-pose-estimation) by ildoonet since the original one was removed. I am doing everything in RoS. \n\nThe problem is: I have the pose estimation algorithm working, and have made a listener node which receives the poses in form of xy coords i.e position of eye, nose, ears, neck, etc. I want to extract the angle being made with the camera in the picture. How do I go about that? Any and all help will be greatly appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nfhjtp/help_calculating_angles_from_poses_openpose/"}, {"autor": "iwantsomehugs", "date": "2021-05-18 08:08:19", "content": "Has anyone served tensorflow models with flask? I am stuck on an error since the last 3 days. /!/ I am actually getting ModuleNotFoundError: No module named 'tensorflow.compat' \n\nI am trying to serve a TensorFlow model I built with flask. While I was running the flask code, it came up with this error I reinstalled conda but the error persisted. The thing is now even if I import tensorflow, this error comes up. I tried on another device which didn't have conda but just vanilla python. The same error came up.\n\n[\\[this thread doesn't help\\]](https://stackoverflow.com/questions/63859309/modulenotfounderror-no-module-named-tensorflow-compat)\n\nI will post the entire error here:\n\n \n\n    &gt;&gt;&gt; import tensorflow as tf 2021-05-18 13:20:02.804699: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found 2021-05-18 13:20:02.830901: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine. Traceback (most recent call last):   File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;   File \"G:\\AiDEV\\Kid\\CNN\\myEnv\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in &lt;module&gt;     from tensorflow.python.tools import module_util as _module_util   File \"G:\\AiDEV\\Kid\\CNN\\myEnv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 46, in &lt;module&gt;     from tensorflow.python import data   File \"G:\\AiDEV\\Kid\\CNN\\myEnv\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py\", line 25, in &lt;module&gt;     from tensorflow.python.data import experimental   File \"G:\\AiDEV\\Kid\\CNN\\myEnv\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py\", line 99, in &lt;module&gt;     from tensorflow.python.data.experimental import service   File \"G:\\AiDEV\\Kid\\CNN\\myEnv\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py\", line 140, in &lt;module&gt;     from tensorflow.python.data.experimental.ops.data_service_ops import distribute   File \"G:\\AiDEV\\Kid\\CNN\\myEnv\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py\", line 25, in &lt;module&gt;     from tensorflow.python.data.experimental.ops import compression_ops   File \"G:\\AiDEV\\Kid\\CNN\\myEnv\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py\", line 20, in &lt;module&gt;     from tensorflow.python.data.util import structure   File \"G:\\AiDEV\\Kid\\CNN\\myEnv\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\", line 26, in &lt;module&gt;     from tensorflow.python.data.util import nest   File \"G:\\AiDEV\\Kid\\CNN\\myEnv\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py\", line 40, in &lt;module&gt;     from tensorflow.python.framework import sparse_tensor as _sparse_tensor   File \"G:\\AiDEV\\Kid\\CNN\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py\", line 28, in &lt;module&gt;     from tensorflow.python.framework import constant_op   File \"G:\\AiDEV\\Kid\\CNN\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 29, in &lt;module&gt;     from tensorflow.python.eager import execute   File \"G:\\AiDEV\\Kid\\CNN\\myEnv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 28, in &lt;module&gt;     from tensorflow.python.framework import ops   File \"G:\\AiDEV\\Kid\\CNN\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 26, in &lt;module&gt;     from absl import app   File \"G:\\AiDEV\\Kid\\CNN\\myEnv\\lib\\site-packages\\absl\\app.py\", line 35, in &lt;module&gt;     import pdb   File \"c:\\users\\adi\\appdata\\local\\programs\\python\\python39\\lib\\pdb.py\", line 77, in &lt;module&gt;     import code   File \"G:\\AiDEV\\Kid\\CNN\\New folder\\code.py\", line 4, in &lt;module&gt;     from keras.preprocessing.image import ImageDataGenerator   File \"G:\\AiDEV\\Kid\\CNN\\myEnv\\lib\\site-packages\\keras\\__init__.py\", line 22, in &lt;module&gt;     from keras import distribute   File \"G:\\AiDEV\\Kid\\CNN\\myEnv\\lib\\site-packages\\keras\\distribute\\__init__.py\", line 18, in &lt;module&gt;     from keras.distribute import sidecar_evaluator   File \"G:\\AiDEV\\Kid\\CNN\\myEnv\\lib\\site-packages\\keras\\distribute\\sidecar_evaluator.py\", line 18, in &lt;module&gt;     import tensorflow.compat.v2 as tf ModuleNotFoundError: No module named 'tensorflow.compat' \n\nIf you've ever served models with flask before, can you tell how did you do it? \n\nWhat I did was fairly simple:\n\n \n\n`from\u00a0flask\u00a0import\u00a0Flask,\u00a0render_template,\u00a0request`  \n`from\u00a0werkzeug.utils\u00a0import\u00a0secure_filename`  \n`from\u00a0werkzeug.datastructures\u00a0import\u00a0\u00a0FileStorage`  \n`from\u00a0keras.preprocessing.-----> image !!! \u00a0import\u00a0ImageDataGenerator`  \n`import\u00a0tensorflow\u00a0as\u00a0tf`  \n`import\u00a0numpy\u00a0as\u00a0np`  \n`import\u00a0os`  \n`try:`  \n `import\u00a0shutil`  \n\u00a0\u00a0\u00a0\u00a0`shutil.rmtree('uploaded\u00a0/\u00a0-----> image !!! ')`  \n `#\u00a0%\u00a0cd\u00a0uploaded\u00a0%\u00a0mkdir\u00a0-----> image !!! \u00a0%\u00a0cd\u00a0..`  \n `print()`  \n`except:`  \n `pass`  \n`model\u00a0=\u00a0tf.keras.models.load_model('model')`  \n`app\u00a0=\u00a0Flask(__name__)`  \n`app.config['UPLOAD_FOLDER'] = 'uploaded\u00a0/\u00a0-----> image !!! '`  \n`u/app.route('/')`  \n`def upload_f():`  \n `return\u00a0render_template('upload.html')`  \n`def finds():`  \n\u00a0\u00a0\u00a0\u00a0`test_datagen\u00a0=\u00a0ImageDataGenerator(rescale = 1./255)`  \n\u00a0\u00a0\u00a0\u00a0`vals\u00a0= ['Cat', 'Dog'] #\u00a0change\u00a0this\u00a0according\u00a0to\u00a0what\u00a0you've\u00a0trained\u00a0your\u00a0model\u00a0to\u00a0do`  \n\u00a0\u00a0\u00a0\u00a0`test_dir\u00a0= 'uploaded'`  \n\u00a0\u00a0\u00a0\u00a0`test_generator\u00a0=\u00a0test_datagen.flow_from_directory(`  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0`test_dir,`  \n `target_size =(224, 224),`  \n `color_mode =\"rgb\",`  \n `shuffle = False,`  \n `class_mode ='categorical',`  \n `batch_size = 1)`  \n\u00a0\u00a0\u00a0\u00a0`pred\u00a0=\u00a0model.predict_generator(test_generator)`  \n `print(pred)`  \n `return str(vals[np.argmax(pred)])`  \n`u/app.route('/uploader', methods = ['GET', 'POST'])`  \n`def upload_file():`  \n `if\u00a0request.method\u00a0== 'POST':`  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0`f\u00a0=\u00a0request.files['file']`  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0`f.save(os.path.join(app.config['UPLOAD_FOLDER'],\u00a0secure_filename(f.filename)))`  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0`val\u00a0=\u00a0finds()`  \n `return\u00a0render_template('pred.html', ss =\u00a0val)`  \n`if\u00a0__name__\u00a0== '__main__':`  \n\u00a0\u00a0\u00a0\u00a0[`app.run`](https://app.run)`()`  \n\n\nThanks in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nf4vay/has_anyone_served_tensorflow_models_with_flask_i/"}, {"autor": "Jimbotron126", "date": "2021-05-18 07:00:38", "content": "Anyone see those kind of problem when trying to train a Cycle GAN on Monet Style Transfer Dataset? /!/ So basically I try to implement a Cycle GAN on Monet Style Transfer Dataset \n\nLink [https://www.kaggle.com/c/gan-getting-started](https://www.kaggle.com/c/gan-getting-started) \n\nHowever after like 20-30 epochs I find all my generated monet -----> image !!!  color is really random and doesn't match monet style. However the shape of feature is captured, (which is expected because of cycle consistency lose). Below is one of the example, can someone who successfully trained a Cycle GAN tell me what may go wrong with my model and how may I improve it? \n\nhttps://preview.redd.it/pglf4fktutz61.png?width=1286&amp;format=png&amp;auto=webp&amp;s=1c20391c4a72b1a19099106272212b1781ee87b8", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nf3s9y/anyone_see_those_kind_of_problem_when_trying_to/"}, {"autor": "TornadoZ97", "date": "2021-05-17 12:55:06", "content": "Looking for an -----> image !!!  dataset management tool /!/ I\u2019m currently training object detectors and have folders full of images and associated text files with details of the annotations. I\u2019m looking for a tool that can manage this data. \n\nI\u2019ve searched around and found lots of tools that enable you to label images but I\u2019m looking for one that lets me manage the dataset as a whole. That ways I can split the data into test and training sets easier. Or so I can view what datasets I have of certain objects.\n\nI\u2019ve found some tools that look to do the job such as V7 labs but I need an on premise solution. Does anyone use anything they could recommend?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nefhyw/looking_for_an_image_dataset_management_tool/"}, {"autor": "VoyZan", "date": "2021-01-22 04:11:55", "content": "Hi! I wanted to share this video I made about Dall-e, the recent mindblowing OpenAI -----> image !!!  generating model. I humbly like to think that this is the coolest video I made so far in my short experience of making tech YouTube videos so I really hope you guys find it informative and interesting. Thanks\ud83d\udc4b", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l2fqus/hi_i_wanted_to_share_this_video_i_made_about/"}, {"autor": "Bellowathy", "date": "2021-01-22 00:05:55", "content": "Question: Is there a way to \"freeze\" and \"-----> image !!! \" a commercial executable in process and re-load and continue later? /!/  \n\nThere's a game (Wargame: Red Dragon) that I'm obsessed with, but has arguably poor singleplayer gameplay due to underdeveloped adversary AI.\n\nI'm thinking of starting a little machine learning/coding project to try and slightly improve the AI performance here and there, but first I'd basically need to turn the game into a repeatable learning/testing environment. I can write some macros or something to automate the process of starting games &amp; precision aligning units, but even optimized it would take minutes per iteration. This pretty much makes the project untenably time consuming.\n\nWhat I want to do is reduce the iteration time to seconds or milliseconds by loading a pre-prepared scenario, running the iteration, then immediately re-loading the game state. (I can boost the game speed pretty damn high with cheat engine).\n\nIf anyone has software recommendations or recommendations in general it would be greatly appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l2bcrr/question_is_there_a_way_to_freeze_and_image_a/"}, {"autor": "GoofAckYoorsElf", "date": "2021-01-21 10:39:39", "content": "Unsupervised segmentation of common and distinct features in a set of images /!/ Hey fellow machine learners!\n\nI'm right now thinking about a task that might be tackled by means of some unsupervised machine learning algorithm. The task is as follows:\n\nGiven a (large) set of images, find and label the features that they have in common and that are usually distinct. Depending on the size of the dataset this can be a range, so maybe what we end up with is a probability for each pixel of how likely it is part of a rather common or a rather distinct feature. \n\nBy \"feature\" I'm not only talking about small features like edges, corners etc. but bigger ones like a whole cat, a dog, a car, a human, a face, a tree. At first glance it might sound like a simple classification task as we've seen it many times before. That would be true if it was a supervised task where the dataset is already labeled and split into given categories. What I'm looking for though is an unsupervised way of some sort of semantic segmentation that can classify between \"oh this feature that I've discovered myself appears rather commonly in the dataset\" and \"oh and this feature that I've also discovered myself is quite rare in the dataset\". When seeing an unknown -----> image !!!  it should be able to segment that -----> image !!!  into (at least) these two categories. \n\nDoes that idea make any sense? Do you maybe already know of an approach, an existing algorithm that can do that? Maybe even a code project? \n\nCheers!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l1w23e/unsupervised_segmentation_of_common_and_distinct/"}, {"autor": "rockyrey_w", "date": "2021-01-21 08:34:48", "content": "[Interview] Google Creates New SOTA Text-Image Generation Framework /!/ *Synced* invited Dr. Linchao Zhu, a lecturer at the ReLER lab, University of Technology Sydney whose works focus on video representation learning, to share his thoughts on the paper *Text-to------> Image !!!  Generation Grounded by Fine-Grained User Attention*.  \n\n\nRead here: [Google Creates New SOTA Text-Image Generation Framework](https://syncedreview.com/2021/01/20/google-creates-new-sota-text-image-generation-framework/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l1uglh/interview_google_creates_new_sota_textimage/"}, {"autor": "rockyrey_w", "date": "2021-01-21 08:21:56", "content": "[Interview] Google Creates New SOTA Text-Image Generation Framework /!/ *Synced* invited Dr. Linchao Zhu, a lecturer at the ReLER lab, University of Technology Sydney whose works focus on video representation learning, to share his thoughts on the paper *Text-to------> Image !!!  Generation Grounded by Fine-Grained User Attention*.  \n\n\nRead here: [Google Creates New SOTA Text-Image Generation Framework](https://syncedreview.com/2021/01/20/google-creates-new-sota-text-image-generation-framework/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l1uay1/interview_google_creates_new_sota_textimage/"}, {"autor": "rockyrey_w", "date": "2021-01-21 08:21:13", "content": "[Interview] Google Creates New SOTA Text-Image Generation Framework /!/ *Synced* invited Dr. Linchao Zhu, a lecturer at the ReLER lab, University of Technology Sydney whose works focus on video representation learning, to share his thoughts on the paper *Text-to------> Image !!!  Generation Grounded by Fine-Grained User Attention*.  \n\n\nRead here: [Google Creates New SOTA Text-Image Generation Framework](https://syncedreview.com/2021/01/20/google-creates-new-sota-text-image-generation-framework/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l1uajr/interview_google_creates_new_sota_textimage/"}, {"autor": "golden543", "date": "2021-01-21 07:03:03", "content": "Everything just feels like an -----> image !!!  classification problem /!/ Hello,\nSo over the past year or so now I\u2019ve been playing around with and learning concepts in machine learning. \nI have never been as passionate about something ever before in my life. The mix between theory and useful applications is very cool.\nHowever, I feel like that everything I do comes down to a image classification problem one way or another. \nWhat is the next step? What else can I work towards?\nJust feeling a little redundant tbh.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l1t8l3/everything_just_feels_like_an_image/"}, {"autor": "raidicy", "date": "2021-01-20 19:53:11", "content": "learning unsupervised text classification /!/ I'm interesting in learning how to do unsupervised text classification. For example I would like to be able to distinguish distinct categories between posters on message boards or other social media sites without labels. I have used Fast AI for -----> image !!!  classification and I have done some basic regression tasks. I understand I would need to learn the basics of NLP techniques such as tokenization. But I'd also like to try and understand which type of algorithms I should be searching and learning about in tandem.\n\n&amp;#x200B;\n\nBonus question if I took the fast AI NLP course would I be able to understand how to accomplish my goal?\n\n&amp;#x200B;\n\nThank you for your time", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l1gzgy/learning_unsupervised_text_classification/"}, {"autor": "NuDatiLaAutomatica", "date": "2021-01-20 13:16:11", "content": "ML Newbie /!/  \n\nHi, I am a total noob when it comes to ML, my only experience is working on a Uni project that involved training some fonts, loading some -----> image !!! s and OCR-ing some license plates numbers from a -----> image !!!  database.\n\nSo not much. I am wondering, since this is the final task I have to achieve in my final thesis, if I could somehow apply a regression type algorithm on a data set that I somehow need to create from some before-pulled data from a .JSON. I should also mention that I am working in C#.\n\nSo, I am working with a JSON and I have to pull the Incidence (which is a double - I am working on a COVID-19 app) for each county. My plan is that the user selects a county and the app should predict the Incidence rate based on previous data.\n\nAs I documented myself about this, I found out that I need a CSV type of file. Do I need to follow some rules for the CSV making of? I will be creating it dynamically, I assume, since each county is different and new data is added daily. Is it do-able?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l18yna/ml_newbie/"}, {"autor": "bash3141", "date": "2021-01-20 06:56:01", "content": "Looking for some guidance /!/ I'm a second year student of Electrical Engineering and I do not have a solid programming/software background. I've tried basic computer vision tasks like -----> image !!!  classification but I can't go beyond that. I know the theory behind object detection and other advanced tasks but I don't know how to implement it and everytime I look at an implementation on GitHub, the hundreds of lines of code seem intimidating and I don't understand anything. The only thing I can do is open a Jupyter Notebook and use the available datasets to perform basic tasks.\n\nI know basics of Python and Python Libraries like Numpy, Pandas, Matplotlib etc. and I know basics of PyTorch and TensorFlow, yet I get intimidated by code online. I think my poor coding skills are pulling me back.\n\nI would be extremely grateful if someone could guide me by giving me tasks which will help me improve my coding skills and Computer Vision skills. I am not asking for a lot of time, just some occasional guidance. Thanks in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/l13w0c/looking_for_some_guidance/"}, {"autor": "ddofer", "date": "2021-01-17 08:50:32", "content": "Geometric/bounding-box Deep Learning: handling Missing data ? /!/  I have a deep learning project involving geometry (keypoints/bounding boxes from medical -----> image !!! s as input, in order to perform whole------> image !!!  prediction of a patient outcome post-operation). \n\nFor many images, I will be missing many of the **keypoints** (e.g. due to the source image not covering all of the patient, or due to lack of images before and during an operation). i.e I may lack the x,y,(z) positions of the bounding boxes marking the locations of some of the input objects.\n\n**What is a good way to handle the missing values?**\n\nI could impute (naively or with a learned model), but I am worried about that, since predicting the location of anatomical regions naively seems like a hard task. \n\nMy current plan is to feed the features/keypoints (and additional hand-engineered features, such as angles &amp; distances) to a downstream model (currently a fully connected NN outperforms catboost).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kz2zim/geometricboundingbox_deep_learning_handling/"}, {"autor": "NilotpalS-1", "date": "2021-01-17 06:39:55", "content": "How Does Machine Understand Text? /!/ We do know that machines only understand numbers i.e. 010101 not words or sentences.\n\nSo, before building Natural language processing models, we want\u00a0to\u00a0specialize in an intermediate step, which is the ***text representation.***\n\nText representation was created on a basic idea, which is ***one-hot encodings.***\n\nIn ***one-hot encodings,*** a sentence is represented as a matrix of shape (NxN) where N represents the number of particular ***tokens*** within the sentence.\n\nFor example, in the below -----> picture !!! , each word is expressed as ***sparse vectors*** except one cell (occurrences of the word in the sentence).\n\nContinue to read further in this blog and understand - [Natural Language Processing with Python](https://k21academy.com/datascience/machine-learning/natural-language-processing/?utm_source=Reddit&amp;utm_medium=Referral&amp;utm_campaign=aiml15_Jan01_r_learnmachinelearning)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/f7vvpapp8ub61.png?width=674&amp;format=png&amp;auto=webp&amp;s=88a437d9bf73b46bdba69c19812bcd424e4c229f", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kz1c8c/how_does_machine_understand_text/"}, {"autor": "Stuck_In_Vim", "date": "2021-01-17 04:20:28", "content": "How to get started with AI research /!/ I know this is a very broad question but I bet with community support it will be a great source of information for me and others.\n\nLets make the assumption that CS and math Prereqs are taken care of. Over the past few years I tried to get started with AI but did not get very far. I was able to code my own ANN and attempt a CNN but thats as far as my knowledge goes. I understand the mechanics in Neural networks as well. I need some direction and mentorship to start on a new project involving AI and CV. This would include much research on my end and I do not know how to get started. If this was any coding project I could write down my plan and what my code will look like but for this I have no clue what to do.\n\nHere are some more specific questions:\n\n1. What is the best way to approach human body part recognition.\n2. How analyze the quantitative data given for pose detection for example?\n3. What is the distinction I need to make in order to separate different models\n4. How would I efficiently use both models for quantifying an -----> image !!! .\n5. What libraries should I use\n6. What language would work best for performance given that I am going for almost real-time image analysis\n\nFor more general responses: How do I go about creating a plan for any project that has yet to be fully tackled(In my case I know there are human pose recognition models). I am looking for the creating a trello board for a game equivalent to a possible AI research project.\n\nMore specific about the project I want to tackle: Given footage of an athlete I would like to effectively gather his bone positions and general posture data as well as their 2D location in the scene. I also want to detect objects in the scene that the athlete interacts with(A vault box for example).\n\nPS: I will be starting Andrew Ng's ML course as it was suggested on other posts.\n\nThanks again for any help", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kyz848/how_to_get_started_with_ai_research/"}, {"autor": "Ghost_06D", "date": "2021-01-16 22:03:13", "content": "How to improve training performances of -----> image !!!  Colorization model using Auto-encoders? \"[Discussion]\", [Project]\" /!/ Hello,\n\nI'm working on an Image colorization model using auto-encoders, here are parameters I'm using :\n\n* Training images size:26000\n* Validation images size:2600\n* 40 epochs, it's doing well on first 20 epochs, but not improving on 20 others as described in this picture:\n* &amp;#x200B;\n\nhttps://preview.redd.it/50for1n2orb61.png?width=1052&amp;format=png&amp;auto=webp&amp;s=45fb6585cb06c054f778fa6938b57e12ed3101fd\n\n* Mini-batch size :32\n* Optimizer : adam (lr=2e-4)\n\nThe input of the auto-encoder is the first dimension of the lab space of the image(gray-scale), and the output is the last 2 dimensions of the lab space(color information), here is a part of my code:\n\n&amp;#x200B;\n\n    IMG_SIZE = 256\n    N_EPOCHS = 20\n    BATCH_SIZE = 32\n    latent_dim = 256\n    \n    \n    train_datagen = ImageDataGenerator(rescale = 1./255,\n                                       horizontal_flip = True)\n    \n    train_set = train_datagen.flow_from_directory('./Train',\n                                                  target_size = (IMG_SIZE, IMG_SIZE),\n                                                  batch_size = BATCH_SIZE)\n    \n    valid_datagen = ImageDataGenerator(rescale = 1./255)\n    \n    valid_set = valid_datagen.flow_from_directory('melange/vl/',\n                                                  target_size = (IMG_SIZE, IMG_SIZE),\n                                                  batch_size = BATCH_SIZE)\n    \n    def gen_ab_images(train_set):\n        for batch in train_set:\n            lab_batch = rgb2lab(batch[0])\n            X_batch = lab_batch[:,:,:,0] \n            Y_batch = lab_batch[:,:,:,1:] / 128\n            yield (X_batch.reshape(X_batch.shape+(1,)), Y_batch)\n    \n    \n    model = Sequential()\n    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2, input_shape=(256, 256, 1)))\n    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n    model.add(Conv2D(128, (3,3), activation='relu', padding='same', strides=2))\n    model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n    model.add(Conv2D(256, (3,3), activation='relu', padding='same', strides=2))\n    model.add(Conv2D(512, (3,3), activation='relu', padding='same'))\n    model.add(Conv2D(512, (3,3), activation='relu', padding='same'))\n    model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n    \n    model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n    model.add(UpSampling2D((2, 2)))\n    model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n    model.add(UpSampling2D((2, 2)))\n    model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n    model.add(Conv2D(16, (3,3), activation='relu', padding='same'))\n    model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))\n    model.add(UpSampling2D((2, 2)))\n    \n    \n    \n    model.compile(optimizer=\"adam\", loss='mse' , metrics=['accuracy'])\n    model.summary()\n    \n    filepath = \"model_test2.model\"\n    checkpoint = ModelCheckpoint(filepath,\n                                 save_best_only=True,\n                                 monitor='loss',\n                                 mode='min')\n    \n    reduction_learning_rate = ReduceLROnPlateau(monitor='loss', \n                                                patience=1, \n                                                verbose=1, \n                                                factor=0.5,\n      \n                                              min_lr=0.00001)\n    valeurs=model.fit(x=gen_ab_images(train_set), \n              callbacks=[checkpoint,reduction_learning_rate], \n              epochs=N_EPOCHS, \n              validation_data=gen_ab_images(valid_set), \n              steps_per_epoch=len(train_set),\n              validation_steps=len(valid_set),\n              shuffle=True)\n    \n\nHow can I improve the performance of this autoencoder?\n\n&amp;#x200B;\n\nThanks in advance,", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kysh9a/how_to_improve_training_performances_of_image/"}, {"autor": "comeculosgrandesymed", "date": "2021-01-16 17:50:29", "content": "Classification model to use in Python/OpenCV ? /!/ Hello everyone.\n\nI have a project of mask detection on people's faces in indoor places The approach is to detect faces first (I'm using YOLOv3 for that), and then I want to apply a classification algorithm to those faces (a -----> image !!!  only of the face, extracted from the full -----> image !!! ) to establish wether or not they're wearing a face mask.\n\nI have created a classification Model using IBM Watson that works pretty well, however, I want to create another one that I can integrate directly in my project (like Yolo does, where I get a weights file and that's what we include in the python code). That way I can avoid making so many calls to the IBM Watson api, and also maybe I can know a little more about what's done in the background.  \nIn the IBM model I have 2 classes : Mask and Wrong mask(not covering the nose). And also a \"negative\" class (faces with no mask at all in this case) which is optional but better to have it for training purposes.\n\nThe reason why I'm asking for help is because on the Internet I only find ways to train object detection models for Python/OpenCV, but can't find a way to get my own CLASSIFICATION model.\n\nAny recommendation would be apprecited. Thanks in advance!!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kynk7r/classification_model_to_use_in_pythonopencv/"}, {"autor": "Extra_Intro_Version", "date": "2021-01-16 16:53:41", "content": "Looking for good PyTorch references to help me get through a project /!/ I have a set of data where each \u201cpoint\u201d is actually a vector of N elements. Each vector corresponds to a specific temperature label\n\nI would like to map that data to the temperature using a fully connected network. With PyTorch. I would like to do classification and hopefully regression as well. My data does have the corresponding temperature labels, and I took pains to make sure it was balanced. \n\nI\u2019m not completely new to this, I\u2019ve been taking Udacity courses for longer than I want to admit. Including Pytorch. (Much of this was for -----> image !!!  recognition) And honestly, I\u2019ve felt overwhelmed nearly the entire time. But I understand the concepts and have been struggling through.\n\nBut this is my first attempt at creating a network for real data. On something that has a different form than the examples and course projects I\u2019ve been doing. And I\u2019m struggling getting all the different parts to work together. \n\nI\u2019ve recently been going through \u201cDeep Learning With PyTorch Step by Step- A Beginner\u2019s Guide\u201d (Daniel Voigt Godoy). It has really helped as a good review, but I still am stuck.\n\nI think where I\u2019m struggling is knowing how and where to shape my input tensors in conjunction with PyTorch dataloaders, batch sizes, tensordataset, etc.\n\nI\u2019ve been to PyTorch help and tutorials numerous times, but I\u2019ve not been able to bridge the gap between the specifics of my case to the generality of all that.  \n\nCan anyone relate to what I\u2019m asking and recommend some good references?\n\nIt\u2019s driving me crazy because this shouldn\u2019t be as hard as it has been. This really should be straightforward.\n\nThank you!!! \ud83d\ude4f", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kymfyx/looking_for_good_pytorch_references_to_help_me/"}, {"autor": "woobin100", "date": "2021-01-16 05:10:52", "content": "Would you recommend me a good -----> image !!!  pattern learning algorithm? /!/ Hi, I don't know I described what I want correctly in the title, so, I'll explain in more detail.\n\n I'm trying to make an AI that can classify and delete the voice of a specific type such as sex, age, and so on. So, first, I downloaded the voice dataset(I labeled them with sex.) and plotted them into spectrogram like this:\n\n&amp;#x200B;\n\n*Processing img pndtodu4omb61...*\n\n And using CNN, I tried to make AI learn patterns. But, the score wasn't that good, so, I want to know if there's a better model/algorithm. Your advice or opinion will be a big help. Thank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kycl8u/would_you_recommend_me_a_good_image_pattern/"}, {"autor": "gsalvador", "date": "2021-01-10 13:56:09", "content": "How to sketch2image in StyleGAN ? /!/ Hello to all,\n\nI've been exploring StyleGAN and I can already generate faces from a random latent vector, but what I'm not able to do is start with a rough sketch of a face and have the GAN generating a -----> image !!!  based on that.\n\nI would really appreciate some help on this.\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kueuyb/how_to_sketch2image_in_stylegan/"}, {"autor": "SQL_beginner", "date": "2021-01-10 08:09:45", "content": "generic questions on time series /!/ 1) iregular time series : Why is it so important that time be analyzed at fixed intervals (e.g. daily, weekly, monthly, etc.)? \n\nSuppose i have some variable measured sporadically (some days there are several observations, some days there are none) - if you specify a future time, (based on previous data and how far your desires time is from the last recorded measurement) why can't a time series model be used to predict the next point? Are there any popular time series models that work with irregular periods?\n\n2) the more refined periods you work with, the better? \n\nSuppose two people are investing money in the stock market. One of them has acess to daily data, the other has access to the same data but at the weekly level. Logically, the person with the daily data is at an advantage? The person with daily data potentially has a chance to see smaller changes in the data that the person with weekly data can not?\n\nUsing the same logic, if you have the freedom to do so, is it generally better to model daily data vs weekly data? If you are tasked with predicting the next 3 points - will it be generally be easier to do this at the daily level vs the weekly level? \n\nI kind of see this as a old -----> camera !!!  vs a new -----> camera !!! . A  newer camera can open the lens and take a picture a lot faster than an older camera, and capture a quick event (e.g. a racecar driving by) whereas by the time the old camera begins to open, it might completely miss the racecar all together !", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kuaj3k/generic_questions_on_time_series/"}, {"autor": "chunaynay", "date": "2021-01-10 00:58:59", "content": "Where do I start with creating (or using a pre-made) ML algorithm to identify my painting and do some VFX with the shapes and colors of it? /!/ I have made some paintings and I think it would be cool to try and create an app for my phone that would allow me to access the -----> camera !!!  and do something unique and cool with each painting. Kind of like a snapchat or instagram filter does with your face, just with paintings.\n\nI want it to to do a specific set of VFXs for each specific painting, so not just any painting\n\nI have a lot of experience with C# and Unity, Java and Android Studio and I'm currently working with Javascript on a daily basis at my current job. I also have some experience with Blender and I'm currently in the process of learning more of how to do VFX for homemade videos through Blender and Da Vinci Resolve\n\nI'm not looking for a thorough guide, just a quick advice as to which language and software you would recommend for me to use. It would be awesome if it was free but if it costs money (say, if I would have to buy AR packages for Unity as an example), that's cool too. I just really want to make my paintings come to life somehow and it's bugging me that I don't know where to start. I'm thinking Unity because of the whole VFX aspect of it but the last time I tried incorporating a custom ML algorithm in a Unity Android app, it was very difficult and instead I ended up using Android Studio (Java). This was solely for basic image recognition where I used Python and TF to just \"modify\" the MNIST algorithm to recognize basic shapes (triangles, squares and circles), so that was obviously a very simple project compared to what I want to do now", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ku3smz/where_do_i_start_with_creating_or_using_a_premade/"}, {"autor": "shani_786", "date": "2021-01-09 07:11:15", "content": "Canny Edge Detection - Non-Maximal Suppression Explanation /!/ **Canny Edge Detection: Non-Maximal Suppression**\n\nCanny edge detection is one of the most prominent edge detection algorithms in the field of Computer Vision. It is a ***first-order*** method.\n\nBelow are the steps that **Canny Edge Detection** follows, which is considerably different from other first-order methods like **Sobel etc...**\n\n* Non-Maximal Suppression\n* Double Thresholding\n* Edge Tracking via Hysteresis  \n\nCanny uses a simple first-order method **Sobel** for the ***-----> image !!!  gradient magnitude*** and ***orientation computations***.\n\nBelow is the link to an amazing and descriptive tutorial free lecture for **Canny Edge Detection -&gt; Non-Maximal Suppression** step:\n\n[https://youtu.be/9cpTmJCsI0M](https://youtu.be/9cpTmJCsI0M)\n\nIn this video lecture, the non-maximal suppression (NMS) part of the algorithm is covered and the further lectures cover the double threshold and the hysteresis step of the algorithm.   \n\n\nThis video lecture is a part of  RO-1.0X and RO-2.0X courses at [DeepEigen](http://www.deepeigen.swaayatt-robots.com/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ktm8kj/canny_edge_detection_nonmaximal_suppression/"}, {"autor": "dmdmello", "date": "2021-01-08 23:02:33", "content": "What's the best way of handling the GAN training output ? /!/ Supervising the training of GANs usually involves outputting not only metrics, but also images at a certain interval of epochs. My application also involves printing tables. I use jupyter notebooks, but just printing it all on the notebooks makes each notebook for each experiment way too large (+100 MB), and the internet browser gets slow and crashes often because of that. \n\nI suppose the usual practice would be to save the -----> image !!!  outputs (either with tensorboard or just plain -----> image !!!  files), but that would not be ideal for me because I like to observe each -----> image !!!  with the text/table output relative to it's epoch. It would be even better if I could save the entire training output into a single file, so I could just scroll down through it, observing each epoch output with text/table/image, just like the jupyter notebook output. Is there a way of implementing this? Or perhaps a better way that I'm not considering? \n\nThank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kte4tr/whats_the_best_way_of_handling_the_gan_training/"}, {"autor": "Programmingwithme", "date": "2021-01-08 13:31:17", "content": "Machine Learning: Handle missing data /!/ &amp;#x200B;\n\nhttps://preview.redd.it/1aclgyrt14a61.png?width=799&amp;format=png&amp;auto=webp&amp;s=02089c1994d76e64de3725a739b49a6ff17bf3b6\n\nIn today\u2019s post on machine learning, I will explain how to work in your CSV file if there is no data/ missing data in a row, which means some of your rows contain blank space. I will explain this to you.\n\nYou may know that the computer is not as developed as the brain of the human body. So, when System/Model/Computer find a blank row in your dataset, it is difficult for System/Model/Computer how to work with it. We have to give some instructions.\n\nHere is the overview of **scikit-learn: -**\n\nThis library will be beneficial to perform many operations related to data pre-processing.\n\nIt is built and written in **python**.\n\nIt is open-source and commercially usable \u2014 **BSD** license.\n\nHere to handle the missing data I am going to use the Module of this library \u201c**impute**\u201d which supports \u201c**SimpleImputer Class**\u201d. Check the -----> image !!!  below.\n\n**Read MORE:-**  [**https://www.becomeadeveloper.in/2020/12/machine-learning-handle-missing-data.html**](https://www.becomeadeveloper.in/2020/12/machine-learning-handle-missing-data.html)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kt2hah/machine_learning_handle_missing_data/"}, {"autor": "AdagioJump", "date": "2021-01-08 13:18:12", "content": "Is there a way to automatically label an entire -----> image !!!  as an object. /!/ All my images in my dataset is cropped correctly and only has the object itself. So I don't want to draw bounding boxes for every single image since the entire image itself is within the bounding box. So is there a way to automatically classify the entire image as within the bounding box for every single image or do I have to go through manually and draw bounding boxes around what is essentially the entire image?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kt29t7/is_there_a_way_to_automatically_label_an_entire/"}, {"autor": "SIBCW96", "date": "2021-03-11 14:31:23", "content": "Is there a 3D version of YOLO that can take as input microscopy images? /!/ Hello guys,\n\nI need to perform detection of nuclei in 3D microscopy images. I want to tackle this task using a 3D YOLO architecture. Thus, I would like to know if there is a 3D implementation of YOLO. The one I found takes as input point clouds, can it be easily adapted to receive a 3D microscopy -----> image !!! ?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m2rdsz/is_there_a_3d_version_of_yolo_that_can_take_as/"}, {"autor": "RaptorDotCpp", "date": "2021-03-11 11:44:52", "content": "Is this a valid approach to visualize what a ResNet is looking at? /!/ I take the last convolutional block of ResNet-34, which outputs 512, 7, 7 features. I store this feature map, take the mean over the first dimension, upscale to 224 by 224 and take the average of the feature map with the input -----> image !!! .\n\nIs this a valid approach to visualize what a ResNet is looking at?\nFrom my visualizations it looks right, but papers seem to be doing more complicated things with gradients and network weights.\n\nI can't do class activation maps because I have an LSTM after my CNN (video input).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m2obug/is_this_a_valid_approach_to_visualize_what_a/"}, {"autor": "i_am_juan_the_horse", "date": "2021-03-11 10:08:36", "content": "I want to do tracking using a -----> camera !!!  where I know the -----> camera !!!  matrix for each frame /!/ I use Google ARCore to get the extrinsic parameter matrix of my android camera (as well as the intrinsic parameter matrix). With this information, can I somehow create an accurate motion model that I can then use to track static objects in a scene? I know that SORT and Deep SORT use Kalman filter to build the motion model in the image plane. However, in my case, I know the position and rotation of the camera in each frame. Is it possible to create some kind of motion model that can link the motion of the camera with the motion of the bounding boxes?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m2mymw/i_want_to_do_tracking_using_a_camera_where_i_know/"}, {"autor": "proteeti13", "date": "2021-03-11 05:37:46", "content": "Full stack web developer and have to do a machine learning task, where to start? Confused by so many resources. /!/ Hello. I have been assigned to do a work where I need to generate 14 classifications from the **NIH chest x-ray dataset of 14 common thorax disease categories from a given chest xray -----> image !!! **. But I am a full stack web developer and I have 0 knowledge about Machine learning stuff. I know Python (I've been using this for backend) well. **What are the topics that I need to learn to accomplish my task? It will be really helpful if someone provides me the relevant resources that I will need to do this.**\n\nI tried to look but there are so many ML resources that I ended up getting confused.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m2iutb/full_stack_web_developer_and_have_to_do_a_machine/"}, {"autor": "PytonRzeczny", "date": "2021-03-10 21:32:29", "content": "GAN problem with mode collapse or generating nonsense /!/ I'm trying to train GAN on MNIST, but everytime i generate data after training i observe mode collapse or some noise in the middle of the -----> image !!! . Once i've trained a model that generates pretty good images of \"9\" and \"1\" and that's the biggest success.\n\nI've tried lot of hyperparameters and architectures but nothing is working.\n\nAny advice?\n\nHere is training loop code:\n\n[https://pastebin.com/mQNP2knx](https://pastebin.com/mQNP2knx)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m298h9/gan_problem_with_mode_collapse_or_generating/"}, {"autor": "mrtransisteur", "date": "2021-03-10 20:54:52", "content": "Need some help working w/ graph neural networks with my semi-supervised img-&gt;fmri model /!/ Hello, I am working on a self-supervised fmri -&gt; -----> image !!!  decoding/encoding project\n\nHere is the basic structure:\n\nencoder E: -----> image !!!  encoder (e.g. resnet) -&gt; feature vector -&gt; graph encoder -&gt; fmri voxel values\n\ndecoder D: voxel values -&gt; graph decoder -&gt; -----> image !!!  generator (e.g. some kind of GAN)\n\nRound 1 of training: train E (imgs -&gt; fmris) \n\nRound 2: train D (fmris -&gt; imgs) \n\nRound 3: train D(E(x)) with E's weights frozen, (imgs -&gt; imgs)\n\nRound 4: train E(D(x)) with D's weights frozen, (fmris -&gt; fmris)\n\nI made a rudimentary model but with the fmri values as 1D vectors rather than 3D point clouds. Now I'm not so sure as to how to write the graph encoder/decoder bits. Tried PyTorch Geometric but it was a bit more complex than DGL so I'm on DGL now but I'm having issues there too.\n\nOutput of the image encoder is a [batch size, channels, height, width] tensor..\n\nHow do I give \"graph global features\" into a graph library in this format? Most of the existing layers just rely upon giving nodes or edges individual feature sets, afaik. I don't wanna just duplicate features bc there are over 4000 nodes, so that would be a lot of just memory usage as far I can see how to design it. Any and all thoughts and suggestions are welcome!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m288ao/need_some_help_working_w_graph_neural_networks/"}, {"autor": "gbbb1982", "date": "2021-03-10 16:53:10", "content": "Painted from -----> image !!!  by learned neural networks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m22261/painted_from_image_by_learned_neural_networks/"}, {"autor": "TheBestAwesomeNoob", "date": "2021-03-10 16:14:16", "content": "Model takes seconds to train per epoch with 1 accuracy /!/ I'd like to develop an -----> image !!!  recognition algorithm that can find drinking water fountains in pictures. Following the tutorial ([https://www.tensorflow.org/tutorials/images/classification](https://www.tensorflow.org/tutorials/images/classification)), I created this Python notebook: [https://colab.research.google.com/drive/1NwiIqk0GaPGWqf5ck4zzrBbYKLbldgd7?usp=sharing](https://colab.research.google.com/drive/1NwiIqk0GaPGWqf5ck4zzrBbYKLbldgd7?usp=sharing). However, when trying to train the model, each epoch takes 1 to 2 seconds per epoch, loss is always at 0, and accuracy is 1.\n\nIs this caused by a **very** low number of training images? I'd appreciate any help with this project. I've been trying to find additional photos through Google Search, without much luck. Are there any datasets that are freely available that I could use? Is this project even possible?\n\nI'm just getting started with machine learning and thought this might be a good project to integrate with another project of mine. Thanks in advance for the help!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m213as/model_takes_seconds_to_train_per_epoch_with_1/"}, {"autor": "Laurence-Lin", "date": "2021-03-10 07:50:29", "content": "Could the current convolution neural network classify different color -----> image !!!  with same features? /!/ In my understanding, for an RGB image input, convolution layer use same filter on all channels, and calculate the **mean of three channel** output feature maps for a single filter. \n\n&amp;#x200B;\n\nNow if I have two images with same feature, for example an **blue apple** and an **red apple**, and I want to identify different color of feature in the image(To classify if the image contain blue apple or red apple), by the above convolution layer, same filter two colors of apple image may get the same output feature map, because it averages all three channels output.\n\nAm I right?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m1segg/could_the_current_convolution_neural_network/"}, {"autor": "hiphop1987", "date": "2021-03-09 17:18:57", "content": "How can you use AI to prevent Cyberbullying? /!/ Let\u2019s learn by example and train a Neural Network with PyTorch that is able to recognize toxicity in online conversation.\n\nCyberharassment is a form of bullying using electronic means. It has become increasingly common, especially among teenagers, as the digital sphere has expanded and technology has advanced.\n\nThree years ago, [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview) was published on Kaggle. The main aim of the competition was to develop tools that would help to improve online conversation:\n\n&gt;*Discussing things you care about can be difficult. The threat of abuse and harassment online means that many people stop expressing themselves and give up on seeking different opinions. Platforms struggle to effectively facilitate conversations, leading many communities to limit or completely shut down user comments.*\n\nThis article starts with a theoretical part in which you\u2019ll learn about basic concepts of text processing with neural networks. Then it continues with an example of how to train a Convolutional Neural Network that detects toxic comments.\n\n**By reading the theoretical part you\u2018ll learn:**\n\n* What is NLP?\n* What is BERT?\n* What is a Convolutional Neural Network (CNN)?\n* How to transform text to embeddings?\n* What is KimCNN?\n\n**By reading the practical part you\u2018ll learn:**\n\n* How to load the data\n* How to define the train, validation and test set\n* How to train the Convolutional Neural Network with PyTorch\n* How to test the model\n\nIn the end of the article, I also share a link to all the code so that you can run it yourself.\n\n# Let\u2019s start with the theory\n\n# What is NLP?\n\n[Natural Language Processing](https://en.wikipedia.org/wiki/Natural_language_processing) (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human languages, in particular how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n\n## What is BERT?\n\nBidirectional Encoder Representations from Transformers (BERT) is a language model that was created and published in 2018 by Jacob Devlin and Ming-Wei Chang from Google \\[3\\]. BERT replaces the sequential nature of Recurrent Neural Networks with a much faster Attention-based approach. BERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. In its vanilla form, Transformer includes two separate mechanisms \u2014 an encoder that reads the text input and a decoder that produces a prediction for the task.\n\nBERT achieved state-of-the-art results in a wide variety of NLP tasks. Since BERT\u2019s goal is to generate a language model, only the encoder mechanism is necessary. To learn more about BERT, read [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270) by Rani Horev.\n\nIn this article, we are going to use BERT as an encoder and a separate CNN as a decoder that produces predictions for the task.\n\nWe could use BERT for this task directly (as described in [Multilabel text classification using BERT \u2014 the mighty transformer](https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d)), but we would need to retrain the multi-label classification layer on top of the Transformer so that it would be able to identify the hate speech.\n\n## Convolutional Neural Network\n\nIn deep learning, Convolutional Neural Networks are a category of Neural Networks that have proven very effective in areas such as -----> image !!!  recognition and classification. CNNs have been successful in identifying faces, objects and traffic signs apart from powering vision in robots and self-driving cars.\n\nBecause of these successes, many researchers try to apply them to other problems, like NLP.\n\nTo learn more about CNNs, read this great article about CNNs: [An Intuitive Explanation of Convolutional Neural Networks](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/).\n\n# Let\u2019s continue with the practical part\n\nThe practical part is too long for a Reddit post. You can read the whole article in [How can you use AI to prevent Cyberbullying?](https://towardsdatascience.com/how-can-ai-help-to-prevent-cyberbullying-f630073da478?sk=9e38fecae67adf9d5f810b71c7f9739d)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m1b9kx/how_can_you_use_ai_to_prevent_cyberbullying/"}, {"autor": "mnky9800n", "date": "2021-03-09 16:34:35", "content": "Recommendations for textbook on -----> image !!!  segmentation /!/ My go to book for machine learning is typically elements of statistical learning. But what book is similar in scope and depth for image segmentation and analysis?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m1a8r1/recommendations_for_textbook_on_image_segmentation/"}, {"autor": "muiz1", "date": "2021-03-09 10:24:12", "content": "[Computer Vision] I am building a paper implementation of a multi-domain (frequency and pixel) model from a research paper. I am having issues with the implementation of the Frequency domain /!/  According to the [paper](https://arxiv.org/pdf/1908.04472.pdf) in order to preprocess I have to \"For an input -----> image !!! , we first employ block DCT on it to obtain 64 histograms of DCT coefficients corresponding to 64 frequencies. Following the process of \\[28\\], we then carry 1- D Fourier transform on these DCT coefficient histograms to enhance the effect of CNN. Considering that CNN needs an input of a fixed size, we sample these histograms and obtain 64 250-dimensional vectors, which can be represented as {H0,H1, ...H63}.\"\n\nI am trying to implement this using python and I have a few doubts regarding this.\n\nFirst I want to know how to obtain 64 histograms of DCT coefficients corresponding to 64 frequencies using block DCT and if block DCT is different from DCT since there are python libraries which have DCT already.\n\nSecond I want to know what the input size of this, I want to know how it is related to the 64 250-dimensional vectors. I don't have a great understanding on this topic and would greatly appreciate any support I can get.\n\nThanking you in advance,\n\nmuiz1", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m13oxs/computer_vision_i_am_building_a_paper/"}, {"autor": "theo_champion", "date": "2021-03-09 10:20:54", "content": "If you could annotate any public data for free, what would it be? /!/ Hi there,\n\nI'm currently building a platform for collaborative -----> image !!!  dataset generation. The principle is simple, anyone can access the platform and, by annotating a few more images of a given dataset (doing your part), you get that whole dataset for free.\n\nThe types of annotations possible on the images are 2D bounding box, Polyline, and Landmarks (points).\n\nThe platform is almost ready but one very important question remains: What is some public data that would, if annotated, make a great dataset?\n\nExamples could be, annotated satellite imagery (roads, parked cars, etc...), person detection in non-licenced google images, etc....\n\nCheers", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m13ncf/if_you_could_annotate_any_public_data_for_free/"}, {"autor": "New-Acadia-992", "date": "2021-07-21 00:34:14", "content": "Advice wanted - how do I train a classifier to recognize one specific dog? (not just any dog or similar looking dogs) /!/ I've been through various -----> image !!!  classifier tutorials, and I can use PyTorch or TensorFlow to successfully train classifiers that can distinguish between Dog vs Airplane and Dogs vs Cats and even German Shepherd vs Corgi.\n\nNow I'm trying to make one that can go through all my old pictures and find just **\\*\\*\\*my\\*\\*\\*** dog.\n\nNot just any dog of the same breed.    \n\nJust this one dog.\n\nDo I need to prepare training data with many dogs that look extremely similar to my own and label them \"not my dog\"?   Or is there a good way I can tighten the classifications to be one-dog-specific without finding hundreds of same-breed dogs?\n\nAny pointers appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ooez6s/advice_wanted_how_do_i_train_a_classifier_to/"}, {"autor": "lawless_c", "date": "2021-07-20 14:44:50", "content": "H/vertically flipping data? /!/ I've done this before for -----> image !!! s but I am wondering if it was actually a good idea?\n\nTo augment -----> image !!!  data I flip and rotate all the -----> image !!! s.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oo3fj1/hvertically_flipping_data/"}, {"autor": "awesomegame1254", "date": "2021-07-19 22:05:50", "content": "need help with -----> image !!!  selection algorithim /!/ I'm looking for an image selection algorithim that can look at a folder/library of images and select the one with the highest average brightness and contrast.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/onofwt/need_help_with_image_selection_algorithim/"}, {"autor": "minbh7373", "date": "2021-07-19 09:50:19", "content": "Program recognizing circuit components and converting it into a SPICE simulation file. How can I make it? /!/ Hi, I came up with an idea of making a program recognizing circuit components and converting it into a SPICE simulation file. The usage I imagined was like\n\n1. First, take a screenshot of a circuit -----> image !!!  you saw in the Internet\n\n2. Then convert that into a SPICE program file(In case of me, I use ltspice, so ltspice file)\n\nHow can I make it? I am asking this question because I guess it somehow uses machine learning but don't know know how.\n\nBackground information: I made a MNIST recognition deep learning program by C this semester, and I was thinking to start a project making something I will use. I guess this program will have to be made of some kind of computer vision code(using OpenCV library?) and will need a GUI(never made anything associated to GUI part) but I don't know where to start. It is not an assignment, and I just want to get proficient in programming, so it doesn't have to be super detailed and it can be brief.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/onavd1/program_recognizing_circuit_components_and/"}, {"autor": "call_me_ninza", "date": "2021-07-19 04:33:21", "content": "-----> Image !!!  processing: Convert any image into pencil, color, and stylized sketches with just few line of python code", "link": "https://www.reddit.com/r/learnmachinelearning/comments/on6vqm/image_processing_convert_any_image_into_pencil/"}, {"autor": "Sysfin", "date": "2021-07-25 18:59:56", "content": "tesseract wrappers /!/ I seem to be unable to get consistant results between tesserocr and pytesserocr.\n\nI have been experimenting with pytesseract and it works pretty good.\nI can take my test -----> image !!! s run some threasholding on them and trim the -----> image !!!  to just the relevant areas.  Then I can pytesseract.image_to_string(cleaned_image) and get very good results.  However it takes a longer then I want since pytesseract actually just wraps the tesseract executable.\n\nSo I would prefer to use a wrapper library for tesseract apis directly.  I have been trying tesserocr but that isn't giving as good results with the same input image.  I wasn't trying to get very fancy with the tesserocr just using the basic tesserocr.image_to_text(cleaned_image) but just outputs gibberish.\nI tried playing with tesserocr's more advaced api but that isn't working any better although that probably as much due to my not having much familiarity with the API.\n\nI am stuck on why there is a difference.  Any help understanding would be great.\nOr if you know of another python tesseract wrapper that doesn't call the executable that might help in a different way.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/orhkpl/tesseract_wrappers/"}, {"autor": "kacchalimbu007", "date": "2021-07-25 18:10:32", "content": "Please Share a ROADMAP of How to convert a multiple 2-D -----> image !!!  into a 3-D model using Python. I know Python that's it. I have seen various research papers but they use deep learning algorithms and half paper I can't understand any help would be appreciated!!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/orgnkf/please_share_a_roadmap_of_how_to_convert_a/"}, {"autor": "kacchalimbu007", "date": "2021-07-25 18:00:08", "content": "I want to learn how to convert multi 2D object -----> photo !!!  into 3D model but I don\u2019t know where to start? I\u2019m beginner in Python any suggestions please", "link": "https://www.reddit.com/r/learnmachinelearning/comments/orggdc/i_want_to_learn_how_to_convert_multi_2d_object/"}, {"autor": "Dwc41905", "date": "2021-07-25 17:42:28", "content": "Object classification + localization Question. /!/ I\u2019m trying to do real time object localization using the video captured by a drone in order to detect the location of people. I only need to locate a single person and was wondering if I would benefit from skipping the typical object detection algorithms and just using a cnn with 5 outputs(xmin, ymin, xmax, ymax, and 0 or 1 for whether or not the object was detected). I was also wondering how I can optimize my predictions because the drone will be very high up and the person will occupy only a very small part of the -----> image !!! . This is my first time working on this type of project and I was looking for some guidance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/org40h/object_classification_localization_question/"}, {"autor": "call_me_ninza", "date": "2021-07-25 04:37:54", "content": "Image Processing: Cartoonify your -----> Image !!!  using OpenCV-Python", "link": "https://www.reddit.com/r/learnmachinelearning/comments/or52pp/image_processing_cartoonify_your_image_using/"}, {"autor": "yekitra", "date": "2021-07-24 07:07:27", "content": "GetLaserEyes: A Twitter bot to add laser eyes in the -----> image !!! ", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oql2kf/getlasereyes_a_twitter_bot_to_add_laser_eyes_in/"}, {"autor": "throwawayafw", "date": "2021-07-24 04:30:52", "content": "How do I get started on performing Mitosis detection on H&amp;E images? /!/ I'm a complete beginner in the sense that I've never done anything on -----> image !!!  analysis, let alone medical -----> image !!!  analysis. I have been assigned to do a project on mitosis detection using MITOS-ATYPIA 14 dataset. I have started reading papers on mitosis detection but it feels so overwhelming with all so medical terms. I don't know what should I do with the dataset. Should I just download it and use it as it is. Or do I have to segment it ?\n\nMy project guide expects me to figure it all by my own. I just need a mentor to tell me what am I supposed to me. Reading those journal papers isn't doing me any favour.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oqizwu/how_do_i_get_started_on_performing_mitosis/"}, {"autor": "Serynaide_", "date": "2021-04-06 16:58:29", "content": "Python AI reccomendations /!/ I'm trying to make an AI that can ideally but not necessarily be integrated into a website that is able to recognise specific features of an -----> image !!! . In this case it's for identifying the colouring of hedgehogs so something that can be trained to recognise parts of things as well as what colour they are would be needed. It's namely the eye colour, nose colour and quill colour that needs done at the bare minimum. If anyone could point me to something that is able to do all this as well as how to do it if there's no other tutorial it would be greatly appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mlgabd/python_ai_reccomendations/"}, {"autor": "AutomaticManager888", "date": "2021-04-06 16:15:25", "content": "Need Vocabulary Help, Please! /!/ I'm learning about Image Recognition and neural networks. I'm teaching myself by the way of small tidbits of tutorials to build my own -----> image !!!  recognition program. I'm familiar with using Haar Cascades to \"identify\" an object in an image, however, I've run into a bit of a problem that spiraled into this post.\n\nAs I said, I'm familiar with Haar Cascades to the extent that I know what they are and how to create my own cascades. The problem is,  I don't have the hardware to dedicate to this task at this time. I plan to set this up in the future but it's not an option for me right now. \n\nI feel as though I've seen other projects that don't use haar cascades and considering opencv4 doesn't even contain the methods used to create these cascades, I know there MUST be another option. \n\nUnfortunately, my google searches end up dry and I'm very clearly aware that this is because I'm not familiar with the proper terminology used in Machine Learning especially when considering image recognition.\n\nNow that I'm passed all the filler, I'm looking for some help in figuring out what vocabulary, if defined and understood, would help to lead me to the results I'm looking for. If there are alternatives to haar cascades, what are their names? Also, what other terms and definitions would help me on this journey? What things should I explore explicitly and what things should I explore in my free time or after my first project (or few projects) is complete to make it better, faster, more accurate or that would help me in explaining it to someone with real professional experience. I understand nobody really uses big fancy words in regular talk but knowing them and what they mean could help one explain it in lamen's terms to others.   \n\n\nI'm open to literal lists of words/definitions or small, quick explanations of a single topic, anything really.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mlfau0/need_vocabulary_help_please/"}, {"autor": "spmallick", "date": "2021-04-06 10:52:09", "content": "Depth perception using stereo -----> camera !!!  (Python/C++) /!/ Ever wondered how robots navigate autonomously, grasp different objects or avoid collisions while moving? Using stereo vision-based depth estimation is a standard method for such applications.  \n\n\nIn this post, we discuss classical methods for stereo matching and depth perception. We explain depth perception using a stereo camera and OpenCV. We have also shared the code in Python and C++ for hands-on learning!  \n\n\n[https://learnopencv.com/depth-perception-using-stereo-camera-python-c/](https://learnopencv.com/depth-perception-using-stereo-camera-python-c/)\n\nhttps://preview.redd.it/l1bcffdx9jr61.jpg?width=600&amp;format=pjpg&amp;auto=webp&amp;s=cc64cafc3d660c8b1099f3fba8d7f764b57bf1f4", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ml8wou/depth_perception_using_stereo_camera_pythonc/"}, {"autor": "t_h-e-other-one", "date": "2021-04-06 09:19:16", "content": "Backward pass of Cycle Consistency Loss. /!/ Cycle Consistency Loss is defined as :\n\nL(G,F) = E (|| F(G(x)) - x ||) + E(|| G(F(y) - y ||)\n\nSo the derivative for one generator should look like:\n\nL'(G,F) = F'(G(x))* G'(x) - 1\n\nDoes this mean:\n\nF'(G(x)) : Gradient of generated -----> image !!!  G(x) with respect to generator F.\n\nG'(x) : Gradient with respect to generator G.\n\n\nCan I compute (1) &amp; (2) seperately, multiply the products &amp; then subtract 1?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ml7m6l/backward_pass_of_cycle_consistency_loss/"}, {"autor": "deshara128", "date": "2021-04-05 21:28:10", "content": "is there a free and/or easy-to-use tool for using an AI to up-render an -----> image !!! ? /!/ google is being unhelpful for me here (most of the top results for me  are either ads for cyberpunk or ads for an nvidia gpu), theres an old [icon](http://classic.battle.net/war3/images/undead/spells/deathanddecay.gif)  from a 20 year old game that I really like &amp; would be interested in  having up-rendered so I can use it for stuff other than 64x64 icons.  Most of the older games I play have mods available for them that used AI  to up-render their textures so it doesn't seem to me like it should be rare/hard to access tech", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mkve2e/is_there_a_free_andor_easytouse_tool_for_using_an/"}, {"autor": "Suzzy67", "date": "2021-04-05 18:43:25", "content": "Overfitting( valid acc is tow low) /!/ i have 13000 training set and 4500 testing set.....-----> image !!!  quality is also good...but my accuracy is 97  but my testing acc is 80 ..i have already ise drop out...and early stopping...but it didnot work...is there any other way to overcome overfitting?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mkrpmj/overfitting_valid_acc_is_tow_low/"}, {"autor": "GiuPaolo", "date": "2021-04-05 18:43:06", "content": "[CFP] 1st Evolutionary Reinforcement Learning workshop @ GECCO 2021 /!/ Time is passing fast! Only 1 week to go before the deadline for the 1st **Evolutionary Reinforcement Learning workshop** @ GECCO 2021, the premiere conference\u00a0in evolutionary computing (this year held  virtually at Lille, France, from July 10-14, 2021)\n\nIn recent years reinforcement learning (RL) has received a lot of attention thanks to its performance and ability to address complex tasks.  At the same time evolutionary algorithms (EA) have been proven to be competitive with standard RL algorithms on certain problems, while being simpler and more scalable.\n\nRecent advances on EA have led to the development of algorithms like Novelty Search and Quality Diversity, capable of efficiently addressing complex exploration problems and finding a wealth of different policies. All these results and developments have sparked a strong renewed interest in such population-based computational approaches.\n\nNevertheless, even if EAs can perform well on hard exploration problems they still suffer from low sample efficiency. This limitation is less present in RL methods, notably because of sample reuse, while on the contrary they struggle with hard exploration settings. The complementary characteristics of RL algorithms and EAs have pushed researchers to explore new approaches merging the two in order to harness their respective strengths while avoiding their shortcomings.\n\nThe goal of the workshop is to foster collaboration, share perspectives, and spread best practices within our growing community at the intersection between RL and EA.\n\nThe topics at the heart of the workshop include:\n\n* Evolutionary reinforcement learning\n* Evolution strategies\n* Population-based methods for policy search\n* Neuroevolution\n* Hard exploration and sparse reward problems\n* Deceptive reward\n* Novelty and diversity search methods\n* Divergent search\n* Sample-efficient direct policy search\n* Intrinsic motivation, curiosity\n* Building or designing behaviour characterizations\n* Meta-learning, hierarchical learning\n* Evolutionary AutoML\n* Open-ended learning\n\nAutors are invited to submit **new original work**, or\u00a0**new perspectives on recently published work**\u00a0 on those topics. Top submissions will\u00a0be selected for oral presentation and\u00a0be presented\u00a0alongside keynote speaker **Jeff Clune** (ex-team leader at UberAI-Labs and current research team leader at OpenAI). \n\n&amp;#x200B;\n\nImportant dates\n\n* Submission deadline: **April 12, 2021**\n* Notification: **April 26, 2021**\n* -----> Camera !!! -ready: **May 3, 2021**\n\n**You can find more info on the** [**workshop website**](https://sites.google.com/view/evorl)**.**", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mkrpdr/cfp_1st_evolutionary_reinforcement_learning/"}, {"autor": "terrenerapier", "date": "2021-04-05 17:49:43", "content": "Stuck Contouring an -----> Image !!!  for hours /!/ Hey!\n\nI have been working on a problem in which I need to find the contours of an image(attached below). \n\nI have used basic tutorials to find contours(Added below), which were accurate but did not find all the contours that I wanted.\n\n    imgray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n    ret, thresh = cv.threshold(imgray, 127, 255, 0)\n    contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_NONE)\n    contoured = cv.drawContours(img, contours, -1, (0,255,0), 3)\n\nThis is what I got from the tutorial. I have tried various things. I tried using THRESH\\_BINARY but that just made everything messy with lines. I tried playing along with the values, but it didn't work out.\n\nI tried inverting the image hoping it would make it easier but it was totally different.\n\n&amp;#x200B;\n\nAny Idea what direction I can take with this? Thanks!\n\n&amp;#x200B;\n\n[Orignal Image](https://preview.redd.it/01cr734e7er61.png?width=2273&amp;format=png&amp;auto=webp&amp;s=64e1d32cc1d9284f2def954f14ac1f2768cf0acf)\n\n[Contours that were accurate but not for all. The other greys also need to be contoured](https://preview.redd.it/3uprkvz87er61.png?width=2273&amp;format=png&amp;auto=webp&amp;s=624f2d7c36d3f1b79399520f07e58596e948b864)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mkqhvs/stuck_contouring_an_image_for_hours/"}, {"autor": "stefan123132", "date": "2021-06-03 14:33:49", "content": "-----> Camera !!!  and hardware for Pose estimation /!/ I'm only in the early planning stages of a project and don't have much experience in the tech world\n\nI'm looking to build an app for a golf swing using pose estimation with real-time feedback at the golf range. I'm going to use BlazePose: [https://storage.googleapis.com/tfjs-models/demos/pose-detection/index.html?model=blazepose](https://storage.googleapis.com/tfjs-models/demos/pose-detection/index.html?model=blazepose) for keypoint detector.\n\nBut, I'm trying to understand what hardware to use at each golf bay, I will need;\n\na) 2 cameras (each bay) - These 2 cameras will be wired to the mains and will be constantly recording for real-time feedback to the user\n\nb) a wifi system installed to the range for the user to connect their smartphone to the software\n\nAny help on which cameras (a) or wifi system (b) to be used would be very much appreciated", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nrexwb/camera_and_hardware_for_pose_estimation/"}, {"autor": "oursondechine", "date": "2021-06-03 12:48:14", "content": "Need directions for learning via a specific project /!/  Hello reddit !  \n\n\nI recently got a somewhat crazy personal project idea for wich i will need to learn some AI/ML stuff (a bunch probably).  \n\n\nDisclaimer: This is a long post and I'm asking for informations I could gather by watching hours of tutorials, ect . If you feel like I'm being lazy, please do not engage; but I feel like I can gain huge ammount of time by asking experienced people some specifics so I start my reserch at the good place. It may not take long time for you and still gives me a good head start. Thanks all &lt;3\n\n&amp;#x200B;\n\n\\## Introduction  \n\n\nFirst some background on my profile so you may help me better; I'm a software developper. My speciality is building JAVA REST api; I also did some python for basic stuff. I would say I'm at great ease with programmation and code architecture as well as learning new languages on the fly. For the AI side of things I would say I picture how it can works but have no detailled knowledge of how it actually works, the different implementations, ect.  \n\n\nNext, some background on what I'm gonna try to implement. (Disclaimer, it may be close to impossible for a noob like me but the purpose is more to learn skill along the way while doing something I enjoy than to have a perfect finish product).  \nI want to teach an AI to play a racing game. ([https://www.trackmania.com/](https://www.trackmania.com/) for reference). It is a time attack game (you race alone against the clock) where you go from A to B and you cross checkpoints (refered as CP later) along your way. The car is an automatic transmission but you do have gears influencing your time.  \nIn a first time I'd like it to learn one specific map it will flat with no jumps involving only \"normal\" driving, on clean road delimited by railings. In a later step I want it to be able to learn new maps on the fly using the knowledge from the previous it learnt (same type of map describe above).  \n\n\n/u/yoshTM/ worked or is working on something similar; I grealy advise you to check [https://www.youtube.com/channel/UCh1zLfuN6F\\_X4eoNKCsyICA](https://www.youtube.com/channel/UCh1zLfuN6F_X4eoNKCsyICA).  \n\n\n\\## Here is my draft of how to tackle this challenge:  \n\n\n1. Find (or develop) a script to emulate a controller  \n\u00a0 \u00a0 This should be easy as there must be existing software. I can also play on keyboard so can emulate this even easier if needded.  \n\n\n2. Develop a capturing and analysis process that will capture the game running every X seconds/milliseconds and from each frame will extract data.  \n\u00a0 \u00a0 Those data include current time from start, speed and gear, optional CP infos (if we just cross a CP)  \n\u00a0 \u00a0 Once this data extracted I'm thinking about puting a black square at the bottom center of the -----> image !!!  to hide the car (maybe some other useless data that have a fixed place); this only because we only have all details needed from the car and it may ease the learning process if the -----> image !!!  as less randomnedd in it; just an idea (maybe from a misconception) I'll let you tell me.  \n\n\n3. Teach an IA to play the best time possible on a map.  \nThe IA would take as inputs:  \n\u00a0\\* time from start  \n\u00a0\\* car speed  \n\u00a0\\* car gear  \n\u00a0\\* post-processed image (with useless infos blacked out)  \nAnd should learn to accelerate/brake and go left right based on where he think it is on the track and the current speed.  \n\n\nEvery steps gets harder than the previous and I will need guidance only for the last two but MOSTLY the third has I have little background in AI.  \n\n\n\\## My questions....  \n\n\n1. As I'm gonna learn AI on this I wanna test the most suited algorythm and technics. whatever they are I'll try and learn them even if it means learning other conepts first. If you think I'm not ready for this you're probably right but then still explain how you would do in your words; I'll do my research to try understand what you say. Thanks  \n\n\n2. Should I give the AI the current time on every loop ? I'm afraid it'll learn to play inputs based on the time and not what it sees in the image. Or should I only give it at the end as a fitness function (if it's the good terminology).  \n\n\n3. The AI would learn only one map at first. But if I make it play a new one I don't want it to start from scratch that would be inneficient. So it looks like it need to remember what it learnt in the previous round; but still not try exactly the same thing... This seems to be a problem that require me to base my choice of algorythm or ML type to design it correctly. Any thought on that ? From my beginner perspective I'm thinking smth genetic but i'm not sure about that memory part  \n\n\n4. Is this doable in real time ? Capturing, post-processing the image, sent all through AI, gather input. Or will I have a huge lag ? If not how often can I set the loop time on (aka, screen 2 times a second, 1 time a minute, ...)  \n\n\n5. I have no idea at what resolution my screen should be; I guess the bigger the better but also the longer time to process...  \n\n\n6. Most importantly, I probably wrote some dumb things due to missconceptions, please correct me. How can I make this draft better before starting to dive in ? How would you have takled the problem ?\u00a0", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nrcorc/need_directions_for_learning_via_a_specific/"}, {"autor": "Aldroc", "date": "2021-06-02 16:08:40", "content": "Absolute beginner out of time /!/  [https://imgur.com/a/AGRECMv](https://imgur.com/a/AGRECMv)\n\nSee that -----> image !!! ? I've got exactly 12 days to code that from scratch, with absolutely no prior knowledge of machine learning. My project guide has been useless, informing me of the preponed presentation dates so close to the deadline. Well, I messed up too, but it is too late to talk about that now I think. I can mostly figure stuff out if I know where to look but without any ideas on how to even start, I'm absolutely lost. Help Reddit \\_/\\\\\\_\n\nThe only actual help my project \"guide\" has given was to tell me to use Google Colab. But I've no idea what that is either so...", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nqpd1n/absolute_beginner_out_of_time/"}, {"autor": "trashpanda-ly", "date": "2021-06-09 02:23:50", "content": "Any food recognition APIs available /!/ Hi everyone! I am thinking of creating a wellness app where one of the available function allows users to take a snapshot of their meal. The app should then be able to recognise the different food in the -----> image !!!  and log the nutritional values to help user keep track of them. In terms of nutritional values, i think calorie intake will be sufficient and i am not planning to go in depth (eg vitamins, iron etc).  \nI've been trying to find APIs online but am not sure which one is good so am asking here to see if anyone has any recommendation or have used them before. Some of the APIs i found were logmeal food ai, calorie mama ai, foodai and biteai. I am willing to pay for the api service if they are reasonably priced and have tried contacting foodai and biteai but didn't get a response. Calorie mama also seem a little expensive. Would appreciate any response!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nvkzhw/any_food_recognition_apis_available/"}, {"autor": "porkandprofits", "date": "2021-06-09 01:18:32", "content": "Google AutoML Vision: Ability to do exact matches to unique but similar objects? /!/ Hi - Still learning ML and was hoping if someone could tell me if Google's AutoML (or some other tool) would be the most appropriate for the task I'm trying to accomplish.\n\nI have a number of small pieces of beach driftwood that I would like the ability to recognize the exact piece after training. Each one is similar but may be different size, contours, branches/arms, etc. After training, each piece would be in a difference -----> picture !!!  backdrop/background (eg. house/outside/etc) and pics mights be taken at various angles.\n\nFirst, would AutoML or some other tool be best suited for the task? If AutoML, how would I go about labeling to manage exact matches vs very similar objects?\n\nAppreciate the help!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nvjsa8/google_automl_vision_ability_to_do_exact_matches/"}, {"autor": "Puzzled_Supports", "date": "2021-06-08 21:29:33", "content": "Inconsistent Classification Accuracy between Classification Network &amp; Object Detection /!/ Hello,\n\n&amp;#x200B;\n\nI have been working on an object detection and classification problem, and I am having understanding a discrepancy in my results.\n\nI am try to detect and classify 2 classes. These objects are semantically quite similar. When I perform object detection, the objects of both classes are detected fairly well, but are always classified as the first class (0) in all instances. I am using a RetinaNet model w/ VGG-19 backbone.\n\n&amp;#x200B;\n\nHowever, when I train a classification network on the -----> image !!!  set (by cropping the -----> image !!! s to only contain objects, thus removing the 'detection' part of the task), the results are actually quite good (\\~90% accuracy). The classifier even performs well if I crop the larger 'detection' images to the bounding boxes output by the detector. The classifier I am using is built on VGG-19.\n\n&amp;#x200B;\n\nI am wondering why the object detector would classify the images so poorly, but a classification network with the same backbone perform so well.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nvf20q/inconsistent_classification_accuracy_between/"}, {"autor": "Lopsided-Dot-717", "date": "2021-06-08 20:43:05", "content": "-----> Image !!!  recognition techniques /!/ [removed]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nvdx8s/image_recognition_techniques/"}, {"autor": "hiiamayush", "date": "2021-06-08 19:18:47", "content": "Improving accuracy of reverse -----> image !!!  search using previous searches /!/ So i am trying to improve the accuracy of ris.\nI was thinking something like i would store all previous searches that has been done and use that data to better accuracy of the image search. How can I achieve this?\n\nThanks in advance", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nvbzaf/improving_accuracy_of_reverse_image_search_using/"}, {"autor": "KhanDescending123", "date": "2021-06-08 17:31:53", "content": "RGBD-Face Recognition Dataset and Point Cloud help! /!/ I'm looking for an RGB-D face dataset for facial recognition. I've found one: [http://robotics.dei.unipd.it/reid/index.php/8-dataset/9-overview-face](http://robotics.dei.unipd.it/reid/index.php/8-dataset/9-overview-face)\n\nBut it seems like the point clouds look nothing like what they have in their paper:\n\n[https://www.researchgate.net/publication/310252844\\_Depth-Based\\_Frontal\\_View\\_Generation\\_for\\_Pose\\_Invariant\\_Face\\_Recognition\\_with\\_Consumer\\_RGB-D\\_Sensors](https://www.researchgate.net/publication/310252844_Depth-Based_Frontal_View_Generation_for_Pose_Invariant_Face_Recognition_with_Consumer_RGB-D_Sensors)\n\nThe depth -----> image !!!  I got: \n\nhttps://preview.redd.it/1iijve13u2471.png?width=1920&amp;format=png&amp;auto=webp&amp;s=5eb60bd075c66b40209c396b8d7f96018a7f6a28\n\nWhen I visualize the point clouds with open3d in python I get a very small -----> image !!!  that doesn't seem useful at all, could this just be an issue with how I am visualizing the point clouds? I've never used this data format before so that could be a possibility. My goal is to convert the depth data into a depth image that I can pass in as a channel to a convolutional network. It almost looks like the image is zoomed out completely.\n\nAm I visualizing this incorrectly or should I look for another dataset? If so are there any good facial recognition datasets with RGB-D data? I would really appreciate some help, thanks!\n\n&amp;#x200B;\n\nVisualization Code:\n\n    import open3d as o3d\n    import numpy as np\n    import matplotlib.pyplot as plt\n    \n    viewer = o3d.visualization.Visualizer()\n    viewer.create_window()\n    \n    pcd = o3d.io.read_point_cloud('000_00_cloud.pcd')\n    viewer.add_geometry(pcd)\n    viewer.run()\n    \n    viewer.capture_depth_image('sdf.png')\n    d = np.asarray(viewer.capture_depth_float_buffer())\n    viewer.destroy_window()\n    \n    print(\"D:\")\n    print(d)\n    \n    #capture_depth_image and capture_depth_float_buffer give the same image back\n    plt.imshow(d)\n    plt.show()", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nv9lij/rgbdface_recognition_dataset_and_point_cloud_help/"}, {"autor": "Totach3", "date": "2021-06-08 14:50:40", "content": "GAN expert question /!/ Need some help and pointers from you guys.\n\nI want to create GANs to create before and after images of a surgery. E.g. Take a -----> picture !!!  of someone, then use GANs to see how that person would look with the finished result.\n\nI have experience with ML &amp; DL but this is the first time I'm trying out a real world application. What resources are there, directly relevant to my task if any?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nv5t1j/gan_expert_question/"}, {"autor": "Oppressor_of_virgins", "date": "2021-06-08 13:37:14", "content": "Comparing 2 images. Is Apple A better than Apple B /!/ I have a bunch of images of apples. With each apple I have a score between 0 and 1 (0.12= bad , 0.96=delicious etc).\n\nFrom 2 -----> image !!! s I want to know if the first -----> image !!!  is better than the second -----> image !!! .\n\nDoes it make sense to use some kind of metric learning? For example I feed a siamese network 2 images w some kind of metric learning? Anyone can point me in the right direction?\n\nThe idea behind using metric learning instead of something else, is that for people it is easy to pick the best apple if you show them a pair. But it is hard to give a single apple a score.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nv4770/comparing_2_images_is_apple_a_better_than_apple_b/"}, {"autor": "JoyousTourist", "date": "2021-06-08 13:23:01", "content": "I have a pretty strong Javascript background and would like to learn -----> image !!!  processing &amp; analysis by ML - should I start with Tensorflow? Any solid books/sites to start from with a background in regular SE? /!/ Title is long but hopefully conveys the question. I have been doing SE for about 10 years now, and I have experience using Azure ML studio to create supervised ML algorithms with a drag &amp; drop interface.  \n\n\nHowever, I'd like to move into image processing, like smart cropping, OCR, etc.  \n\n\nI'd prefer to keep using Javascript, and from what I understand Tensorflow is the defacto framework to get started with for that.  \n\n\nIs that the case? Also is there a guide or series you recommend going through to start image manipulation with a JS powered ML library?  \n\n\nMany many thanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nv3x0a/i_have_a_pretty_strong_javascript_background_and/"}, {"autor": "tt12343", "date": "2021-06-08 08:07:10", "content": "Mixup with Deep Networks for Image Classification /!/ Hey Guys! I recently am inspired by the paper mixup on network classification, and decided to write a PyTorch implementation of the mixup on -----> image !!!  classification. Feel free to check it out:\n\n[https://taying-cheng.medium.com/enhancing-neural-networks-with-mixup-in-pytorch-5129d261bc4a](https://taying-cheng.medium.com/enhancing-neural-networks-with-mixup-in-pytorch-5129d261bc4a)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nuzm3e/mixup_with_deep_networks_for_image_classification/"}, {"autor": "axetobe_ML", "date": "2021-06-07 20:49:07", "content": "Some Maths Resources to Help You in Your ML Journey /!/  I have been looking for content to improve my maths skills for ML. I have also noticed when scrolling a few threads many people did not find content that explains maths in an intuitive manner. Leading to a lack of belief in learning ML. But this does not have to be.\n\nI\u2019m with you, odd-looking characters and Greek letters don\u2019t look welcoming. But they are some good teachers online that can demystify that experience. \n\nSome of those materials are below:\n\n3blue1brown [Calculus](https://youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr) and [Linear Algebra](https://youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) series\n\nI remember watching both of these series a while. And I will be watching them again. The narrator explores the topic without getting bogged down in the details. Feels like your discovering the maths with the original people who made calculus. In the linear algebra series, he does such a great job visualising vector space. You can see the various operations done to vectors and matrices in -----> picture !!!  form.\n\n[3blue1brown Deep Learning series](https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n\nTaking the concepts from the previous series and applying them to deep learning.\n\n[Khan Academy](https://khanacademy.org/)\n\nI\u2019m sure you know about Sal Kahn by now. As you watched a couple of his videos. His video intuitively explains various topics. Also, show you the various hand by hand actions you need to take to do various calculations. Like matrix multiplication and calculating derivatives. \n\n[Mathematics for Machine Learning book](https://mml-book.github.io/)\n\nI tend to use this book as a reference guide if it\u2019s a concept I want to check out. This book goes through the most important subjects relevant to machine learning and goes in-depth.\n\n[Mathematics for Machine Learning - Multivariate Calculus \u2013 Imperial College London](https://www.youtube.com/watch?v=QpwTEsO51tU)\n\nA multi-hour series explaining how calculus is used in deep learning. The material comes at the subject with a high-level view. But goes into sufficient enough detail to help you learn a lot.\n\n[Understand Calculus in 35 Minutes - The Organic Chemistry Tutor](https://www.youtube.com/watch?v=WsQQvHm4lSw)\n\nA general overview of the subject. So you can be familiar with the concepts for deep learning later on.\n\nNOTE: you won\u2019t learn all of calculus in 30 minutes. But the video will help you get accustomed to the main ideas of the subject.\n\n&amp;#x200B;\n\nNow, these are resources that I have not used or have used very lightly but gotten good recommendations from various people. \n\nSo check them out:\n\n[Computational Linear Algebra](https://www.fast.ai/2017/07/17/num-lin-alg/):\n\nThis course talks about the linear algebra used in real computation. Not just Linear algebra done by hand.\n\n[Deep Learning book by Ian Goodfellow and Yoshua Bengio and Aaron Courville](https://www.deeplearningbook.org/)\n\nFrom their website: \n\n&gt;The Deep Learning textbook is a resource intended to help students and practitioners enter the field of machine learning in general and deep learning in particular.\n\nI have not thoroughly read all of the book. But I have used the notation page to understand maths symbols in various deep learning work.\n\n[An Introduction to Statistical Learning](https://www.statlearning.com/)\n\nA few people in this subreddit and the main subreddit have recommended this book. But I have never read it.\n\n*If you found this article interesting,* [*then check out my mailing list.*](https://www.tobiolabode.com/subscribe) *Where I write more stuff like this*", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nun6qb/some_maths_resources_to_help_you_in_your_ml/"}, {"autor": "soylentgraham", "date": "2021-06-07 10:52:12", "content": "Recommendation for a programmer's guide to turning algorithms to models? /!/ \nSo Im a programmer, amongst other things, I've done quite a lot of computer vision work, mostly via pixel shaders (to support low end devices, although 10 years ago i was moving things to opencl), typically image(s) in, scores &amp; meta out.\n\nIve worked with models other people have created, but I still haven't trained a model from scratch. \n\nCan anyone recommend a guide for where to start from scratch? Whenever I search I end up in circles which just tell you how to pump more data sets into a model to train it, but Im trying to actually replace some existing models. (Eg, detect line segments in an -----> image !!! )\n\nApple's coreml is my preference (or long term target), but I'm flexible with frameworks/languages just to get started", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nu9h3d/recommendation_for_a_programmers_guide_to_turning/"}, {"autor": "mwitiderrick", "date": "2021-01-04 14:39:46", "content": "[D] -----> Image !!!  Labeling with Fritz AI Studio /!/ Generate images and train your own model with Fritz AI\n\n[https://heartbeat.fritz.ai/image-labeling-with-fritz-ai-studio-86644b3d8fc1](https://heartbeat.fritz.ai/image-labeling-with-fritz-ai-studio-86644b3d8fc1)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kq9yhf/d_image_labeling_with_fritz_ai_studio/"}, {"autor": "shaun_does_python", "date": "2021-01-04 13:18:41", "content": "Who's That Pokemon? With TensorFlow and Python /!/ Here is a machine learning model that predicts which Pokemon is featured in a given -----> picture !!! : [WhosThatPokemon](https://github.com/shaunmillerc1010/WhosThatPokemon).\n\nI wrote the program using TensorFlow and Python. The model trains with data created by a fellow reddit user, [u/the-dagger](https://www.reddit.com/user/the-dagger/), and is tested on the realistic Pokemon images created by [Joshua Dunlop](https://www.artstation.com/joshuadunlop).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kq8lbs/whos_that_pokemon_with_tensorflow_and_python/"}, {"autor": "Kevinrocks7777", "date": "2021-01-04 06:36:11", "content": "Keras CNN model for predicting score /!/ I took a course that dealt with convolutional neural nets, and we built one to categorize images into categories. \n\n&amp;#x200B;\n\nWhen I did that, it was suitable to use categorical cross entropy as a loss functioncross-entropy, and the output layer was :\n\n  \n conv\\_model.add(layers.Dense(NUM\\_CATEGORIES, activation='softmax')) \n\n&amp;#x200B;\n\nNow, I'm trying to have a neural net predict a Reddit upvote/score based on an input -----> image !!!  as a personal project, and I'm wondering what the output layer of my CNN and the loss function should be. \n\nMy first thought for the output layer was to have a Dense with a single output, and normalize the scores I input from 0 to the max score of the data set. Does that sound right? I don't think it makes sense to have, let's say 36000 categories for each possible score, especially when higher scoring posts are very discrete. Nonetheless, I'm not sure if a single neuron at the end of a neural net makes sense. \n\nAs for the loss function, I have no idea what to use. Could anyone help me out? I'm using Python 3.7 with Keras/probably Tensorflow.Keras", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kq351y/keras_cnn_model_for_predicting_score/"}, {"autor": "axetobe_ML", "date": "2021-01-03 20:08:19", "content": "How to check if your model has a data problem /!/ A couple of times you run your model. And the results are mediocre. While it may be a problem with the [model itself](https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607). It may also be a problem with your data. If you suspect your model is underperforming because of data.\n\nYou can try a few things.\n\n# Do you have enough data?\n\nMake sure you have enough data. This is yours to call. This will depend on what type of data you are dealing with. For example, images around 100 can be just enough. Before you add -----> image !!!  augmentation. Tabular data, maybe a bit more.\n\nJosh Brownlee mentions that:\n\n&gt;The amount of data required for machine learning depends on many factors, such as:  \n&gt;  \n&gt;The complexity of the problem, nominally the unknown underlying function that best relates your input variables to the output variable.  \n&gt;  \n&gt;The complexity of the learning algorithm, nominally the algorithm used to inductively learn the unknown underlying mapping function from specific examples.\n\n[Check out his article](https://machinelearningmastery.com/much-training-data-required-machine-learning/)\n\n# Do you have a balanced dataset?\n\nDoes your data consist of one main class? If so, you may want to change that. Having data like that skews the results one way. The model will struggle to learn about other classes. Adding more data from the other classes can help. If you have the issue above.\n\nYou could try under-sampling. Which means deleting data points from the majority class. On the flip side, you try can oversampling. Which means simply copying the minority class for more samples.\n\nIf your data has a few outliers, you may want to get rid of them. This can be done using Z-score or IQR.\n\n# Is your data actually good?\n\nI\u2019m talking about rookie mistakes like blank rows, missing numbers. Which can be fixed with a few pandas operations. Because they tend to be so small, they are easy to miss.\n\nAssuming you are using pandas you can get rid of N/A. You can use the df.dropna(). \n\nDo you need some of the columns in your dataset? If not drop them. For example, if you are analysing house prices. Then data like the name of the resident is not a good factor for the analysis. Another example if you're analysing the weather of a certain area. Then dataset with 10 other areas is of no interest you. \n\nTo make life easier for yourself. If you are using pandas. Make sure the index is correct. To prevent headaches later on.\n\nCheck the data types of your columns. Because they may contain values of different data types. For example, if your column for DATE. Is a text data type. You may want to change that into a pandas date type. For later data manipulation. \n\nAlso, a couple of your values may have extra characters forcing them to be a different data type. For example, if one of your columns is a float data type. But one of the values looks like this \\[9.0??\\]. Then the value will count as a text data type. Giving you problems later on.  \n\n# Features in your data\n\nYour dataset may contain bad features. Features engineering will be needed to improve it.\n\nYou can start with feature selection. To extract the most useful features. \n\nDo you have useless features like name and ID? If so remove them. That may help. \n\nThey are multiple techniques for feature selection. Like Univariate Selection, Recursive Feature Elimination, Principal Component Analysis.\n\nAfterwards, you can try feature extraction. This is done by combining existing features into more useful ones. If you have domain knowledge then you can manually make your own features. \n\nDo the feature scales make sense? For example, if one of your features is supposed to be in the 0 to 1 range. Then having a value that is 100. Means that it\u2019s a mistake. That value will cause the data to skew one way. Due to it being an outlier.\n\nDepending on your data. You can try one shot encoding. This is a great way to turn categorial data into numeric values. Which machine learning models like. You do this by splitting the categorical data into different columns. And a binary value is added to those columns.\n\n&amp;#x200B;\n\n*Hopefully, if you found this useful then you can check out my* [*blog*](tobiolabode.com)*. Where I write tutorials like these. And some mini-essays about technology.* \n\n&amp;#x200B;\n\nResources:\n\nhttps://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/\n\nhttps://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/\n\nhttps://machinelearningmastery.com/feature-extraction-on-tabular-data/\n\nhttps://towardsdatascience.com/feature-extraction-techniques-d619b56e31be\n\nhttps://machinelearningmastery.com/data-preparation-for-machine-learning-7-day-mini-course/", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kprwdf/how_to_check_if_your_model_has_a_data_problem/"}, {"autor": "spmallick", "date": "2021-01-03 18:46:06", "content": "Running TensorFlow model inference in OpenVINO /!/ We are starting the new year with an article on speeding up inference of Tensorflow models using OpenVINO.\n\nThe best part of the tutorial is the neural style transfer demo where we transfer the style of a historical painting to the cute -----> picture !!!  of a cat!  \n[**https://opencv.org/running-tensorflow-model-inference-in-openvino-2/**](https://opencv.org/running-tensorflow-model-inference-in-openvino-2/)  \n\n\nWe will go over the process step by step\n\n1. Setting up the environment  \n2. Preparing the TensorFlow model  \n3. Converting the model to Intermediate Representation format  \n4. Running model inference in OpenVINO  \n\n\nI hope this tutorial is useful. May this year be one of learning and growth. \n\n*Processing img d0hfdpwpx5961...*", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kpq7cv/running_tensorflow_model_inference_in_openvino/"}, {"autor": "cloud_weather", "date": "2021-01-03 15:24:38", "content": "ArtLine - transforms any -----> Image !!!  into Sketch/Line Art with this AI model", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kpmczi/artline_transforms_any_image_into_sketchline_art/"}, {"autor": "pocomaco", "date": "2021-01-02 21:32:33", "content": "what is this -----> image !!! ? /!/ I was fantasizing about a system that connects images.  A person asked me to tell you more about it, and when I showed him a sketch I made in software, he said it might be related to artificial intelligence.  Is this true?  Please let me know if you have any details!\n\n\nthis\n\u2193\nhttps://imgur.com/0B2Pv53", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kp68oa/what_is_this_image/"}, {"autor": "tommyk1210", "date": "2021-01-02 19:41:44", "content": "Why are my epochs growing in duration? /!/ Hey all,\n\nI\u2019m using this tutorial to build an -----> image !!!  classifier using this tutorial: https://www.py-----> image !!! search.com/2018/04/16/keras-and-convolutional-neural-networks-cnns/\n\nEverything is going great, but the problem I\u2019m having is that my epochs are growing in length. Epoch 1 is 2 minutes, epoch 50 is 10 minutes and epoch 100 is 25 minutes.\n\nAny ideas why this is?\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kp44fp/why_are_my_epochs_growing_in_duration/"}, {"autor": "ifelsestatement007", "date": "2021-01-02 17:48:27", "content": "-----> Image !!!  classification with PyTorch tutorials", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kp1vq2/image_classification_with_pytorch_tutorials/"}, {"autor": "greenmantis43", "date": "2021-01-02 16:43:20", "content": "How do I deal with large datasets for classification using deep learning? /!/ Newbie in DL here. I have a large dataset (30k) of lung images in .dcm format. I have a CSV of -----> image !!!  filenames mapped to 3 classes. How do I train on such a large dataset? I've only used transfer learning before with Vgg16 and it required only few images to train. The images are of 6gb size and uploading it all to colab( which is where I've done most ML/DL work) seems hectic. Is there alternate platform for deep learning when large datasets are used? Please help me out", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kp0n2s/how_do_i_deal_with_large_datasets_for/"}, {"autor": "Honest-Dude", "date": "2021-06-22 19:48:02", "content": "Very confused on how Keras optimization for network with multiple outputs works /!/ I currently have a neural network that takes in 3 numbers as inputs and outputs 3 numbers. I've attached a -----> picture !!!  of the network below and my code is accessible through the following link: [https://colab.research.google.com/drive/1q0Lvw4p\\_vxogmAu8QpYn5HRxAojuEtTY?usp=sharing](https://colab.research.google.com/drive/1q0Lvw4p_vxogmAu8QpYn5HRxAojuEtTY?usp=sharing)\n\nFirstly, as you can see, the network consists of a \"main branch\" (i.e. the first two layers) that is connected to three \"sub-branches\" that correspond to the three outputs. My understanding of how multi-output back-propagation works is that the model first does a forward pass relatively simply, but when it does a backward pass, because the target outputs for each of the sub-branches are different, the weights for the last 3 layers will be different for each of the sub-branches, and will be the same for the main branch. However, if you look at my code, after printing the weights for 2 epochs of training, it seems that the weights for the sub-branches are actually all *the same* if they are on the same layer. For example, the weight on sub\\_1\\_dense2, sub\\_2\\_dense2, and sub\\_3\\_dense3 are all the same. I'm not sure why this is and I've tried changing a bunch of things to try to debug with no luck. \n\nLastly, I just want to say that the point of this project is to manually confirm how multi-output back-propagation works in Keras, and not as a prediction model. If anyone has the answer to this question or has a better idea on how to check, feel free to drop suggestions. Thanks! \n\nhttps://preview.redd.it/uen12690ev671.png?width=2342&amp;format=png&amp;auto=webp&amp;s=50ceec7a92c0113940edec95996e5aeb558123db", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o5vkjc/very_confused_on_how_keras_optimization_for/"}, {"autor": "grid_world", "date": "2021-06-22 11:55:27", "content": "Retrieve Similar Images /!/ I am trying to build a similar -----> image !!!  retrieval system where given an -----> image !!! , the system is able to show top 'k' most similar -----> image !!! s to it. For this particular example, I am using the [DeepFashion](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) dataset where given an image containing say a shirt, you show top 5 clothes most similar to a shirt. A subset of this has 289,222 diverse clothes images in it. Each image is of shape: (300, 300, 3).\n\nThe approach I have includes:\n\n1. Train an autoencoder\n2. Feed each image in the dataset through the encoder to get a reduced n-dimensional latent space representation. For example, it can be 100-d latent space representation\n3. Create a table of shape m x (n + 2) where 'm' is the number of images and each image is compressed to n-dimensions. One of the column is the image name and the other column is a path to where the image is stored on your local system\n4. Given a new image, you feed it through the encoder to get the n-dimensional latent space representation\n5. Use something like cosine similarity, etc to compare the n-d latent space for new image with the table m x (n + 2) obtained in step 3 to find/retrieve top k closest clothes\n\nHow do I create the table mentioned in step 3?\n\nI am planning on using TensorFlow 2.5 with Python 3.8 and the code for getting an image generator is as follows:\n\n    image_generator = ImageDataGenerator(\n        rescale = 1./255, rotation_range = 135)\n    \n    train_data_gen = image_generator.flow_from_directory(\n        directory = train_dir, batch_size = batch_size,\n        shuffle = False, target_size = (IMG_HEIGHT, IMG_WIDTH),\n        class_mode = 'sparse'\n\nHow can get image name and path to image to create the m x (n + 2) table in step 3?\n\nAlso, is there any other better way that I am missing out on?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o5lctc/retrieve_similar_images/"}, {"autor": "aurora-s", "date": "2021-06-22 08:16:56", "content": "Which object detection network is; a) most accurate, b) requires least data to train /!/  I'm confused by the many architectures available for object detection. YOLOv4, RetinaNet, Faster-RCNN, Efficientdet\n\nMost accuracy values (AP?) are quoted for datasets like COCO, but my dataset is simpler - more like 2d vector graphics than photos of 3d objects. Some of the objects to be detected are small, but I decided I could break the images down into smaller segments to overcome that if it looks like it's an issue.\n\nI haven't yet labelled the dataset, but it won't be anywhere as large as COCO, it'll be more like 1000 -----> image !!! s max (perhaps around 100 labels per -----> image !!! ). I'm not too concerned about inference time, but accuracy and low training data volume requirement are essential. I would really appreciate any advice please, especially which architectures I should be looking at :))\n\nPS: Python implementations would be the easiest; I'm not familiar with the darknet framework etc, and if you have links to python tutorials or keras/pytorch implementations, that would be really helpful!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o5hyfa/which_object_detection_network_is_a_most_accurate/"}, {"autor": "sri_anaconda", "date": "2021-06-22 00:37:04", "content": "QUESTION: Help with understanding Semantic Segmentation /!/ I've never really found clarification on this topic so I'm shooting my shot here.\n\n&amp;#x200B;\n\nLet's say I have an -----> image !!!  that has 4 classes and is given in as an RGB -----> image !!! . Can I pass in that image into some sort of model and specify the output channels to be 3 with the output activation as softmax?\n\n&amp;#x200B;\n\nOr should I instead be converting that 3 channel image into 4 channels (1 channel for each class) and then the output of the model should be one with 4 channels?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o59yqz/question_help_with_understanding_semantic/"}, {"autor": "s_basu", "date": "2021-06-21 13:08:02", "content": "Dice coefficient not increasing for the validation set in 3D segmentation training /!/ Hello everyone,\n\nI just started researching and implementing different segmentation models and one, in particular, was the residual 3d Unet proposed in [this](https://arxiv.org/pdf/2012.15294.pdf) paper.\n\nI have kept most of the architecture the same except for a few minor changes such as removal of the dropout layers etc. \n\nThe dataset I am working on is the [Brats 2020 dataset](https://www.kaggle.com/awsaf49/brats20-dataset-training-validation). I created a datagenerator class that produces 1 batch of 128x128x128 voxels in 4 channels pertaining to 4 different modes of imaging (flair, T1, T1ce and T2), and the Unet outputs a 128x128x128x4 mask with each channel denoting one of the 4 classes of tumour.\n\nI used the following function as Dice coefficient.\n\n    def dice_coefficient(y_true, y_pred, smooth=1.0):\n        class_num = 4\n        for i in range(class_num):\n            y_true_f = K.flatten(y_true[:, :, :, :, i])\n            y_pred_f = K.flatten(y_pred[:, :, :, :, i])\n            intersection = K.sum(y_true_f * y_pred_f)\n            loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n            if i == 0:\n                total_loss = loss\n            else:\n                total_loss = total_loss + loss\n        total_loss = total_loss / class_num\n        return total_loss\n\nThe dice coefficient seems to be working fine for training data, but doesn't seem to change much for the validation data. Here's a snippet after 10 epochs of training.\n\n    Epoch 1/10\n    249/249 [==============================] - 735s 3s/step - loss: 0.4277 - accuracy: 0.8808 - dice_coefficient: 0.2462 - val_loss: 0.0768 - val_accuracy: 0.9860 - val_dice_coefficient: 0.2703\n    Epoch 2/10\n    249/249 [==============================] - 707s 3s/step - loss: 0.0349 - accuracy: 0.9907 - dice_coefficient: 0.3865 - val_loss: 0.2653 - val_accuracy: 0.9364 - val_dice_coefficient: 0.3252\n    Epoch 3/10\n    249/249 [==============================] - 708s 3s/step - loss: 0.0248 - accuracy: 0.9923 - dice_coefficient: 0.4753 - val_loss: 0.3260 - val_accuracy: 0.9250 - val_dice_coefficient: 0.3400\n    \n    Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n    Epoch 4/10\n    249/249 [==============================] - 707s 3s/step - loss: 0.0240 - accuracy: 0.9916 - dice_coefficient: 0.4891 - val_loss: 0.1434 - val_accuracy: 0.9653 - val_dice_coefficient: 0.3699\n    Epoch 5/10\n    249/249 [==============================] - 707s 3s/step - loss: 0.0205 - accuracy: 0.9931 - dice_coefficient: 0.5382 - val_loss: 0.1531 - val_accuracy: 0.9651 - val_dice_coefficient: 0.3793\n    \n    Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n    Epoch 6/10\n    249/249 [==============================] - 704s 3s/step - loss: 0.0195 - accuracy: 0.9934 - dice_coefficient: 0.5378 - val_loss: 0.1564 - val_accuracy: 0.9659 - val_dice_coefficient: 0.3751\n    Epoch 7/10\n    249/249 [==============================] - 705s 3s/step - loss: 0.0192 - accuracy: 0.9933 - dice_coefficient: 0.5374 - val_loss: 0.1532 - val_accuracy: 0.9666 - val_dice_coefficient: 0.3737\n    \n    Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n    Epoch 8/10\n    249/249 [==============================] - 702s 3s/step - loss: 0.0182 - accuracy: 0.9940 - dice_coefficient: 0.5471 - val_loss: 0.1515 - val_accuracy: 0.9668 - val_dice_coefficient: 0.3715\n    Epoch 9/10\n    249/249 [==============================] - 703s 3s/step - loss: 0.0190 - accuracy: 0.9935 - dice_coefficient: 0.5525 - val_loss: 0.1573 - val_accuracy: 0.9660 - val_dice_coefficient: 0.3782\n    \n    Epoch 00009: ReduceLROnPlateau reducing learning rate to 1e-06.\n    Epoch 10/10\n    249/249 [==============================] - 706s 3s/step - loss: 0.0189 - accuracy: 0.9936 - dice_coefficient: 0.5455 - val_loss: 0.1638 - val_accuracy: 0.9646 - val_dice_coefficient: 0.3799\n\nHere's the [notebook](https://www.kaggle.com/shiladityabasu/tumoursegmentation-main) if you want to take a look at the full -----> picture !!! . My apologies if the notebook looks all over the place. I don't have a good enough pc so all the training has to be done on kaggle cloud. The first part of the notebook is an implementation of 2d Unet so you can skip that. \n\n&amp;#x200B;\n\nIf anyone can point out what exactly might have gone wrong here, or if this is completely normal and I need to train more, it would be greatly appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o4uqcw/dice_coefficient_not_increasing_for_the/"}, {"autor": "fire_regalia", "date": "2021-06-21 09:04:06", "content": "Tweaks to improve pix2pix model? /!/ I'm working on a grad-level course project that uses the pix2pix architecture for -----> image !!! -to------> image !!!  translation of segmentation maps to real -----> image !!! s. One of the requirements of the project is to come up with \"your own twist\" to the problem - but it's just a 1-month project, so they don't expect any high-end research ideas either. I was looking for some inputs on what minor tweaks to try, that could improve upon the idea of just directly applying the original pix2pix model to the problem. The only restriction is that experiments requiring more training compute power are not possible (because I'm only alloted 1 GPU). Any ideas are appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o4r073/tweaks_to_improve_pix2pix_model/"}, {"autor": "gordicaleksa", "date": "2021-06-21 07:14:55", "content": "GANs N' Roses: Stable, Controllable, Diverse -----> Image !!!  to -----> Image !!!  Translation paper explained!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o4phkx/gans_n_roses_stable_controllable_diverse_image_to/"}, {"autor": "codingainp", "date": "2021-06-21 05:06:23", "content": "Perceptron Model in a Neural Network /!/ In the Artificial Neural Network([ANN](https://pythoncodingai.com/what-is-artificial-intelligenceai-overview/)), the perceptron is a convenient model of an organic neuron, it was the early calculation of double classifiers in managed AI. The reason behind the planning of the perceptron model was to join visual information sources, arranging subjects or subtitles into one of two classes and separating classes through a line.\n\nThe arrangement is one most significant components of AI, particularly in -----> picture !!!  change. AI calculations misuse different methods for handling to distinguish and dissect designs. Continue with grouping undertakings, the perceptron calculations dissect classes and examples to achieve the direct partition between the different classes of articles and relate designs got from mathematical or visual information.\n\n[Continue reading........](https://pythoncodingai.com/perceptron-model-in-a-neural-network/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o4nm1p/perceptron_model_in_a_neural_network/"}, {"autor": "codingainp", "date": "2021-06-21 05:04:55", "content": "Perceptron Model in a Neural Network /!/ In the Artificial Neural Network([ANN](https://pythoncodingai.com/what-is-artificial-intelligenceai-overview/)), the perceptron is a convenient model of an organic neuron, it was the early calculation of double classifiers in managed AI. The reason behind the planning of the perceptron model was to join visual information sources, arranging subjects or subtitles into one of two classes and separating classes through a line.\n\nThe arrangement is one most significant components of AI, particularly in -----> picture !!!  change. AI calculations misuse different methods for handling to distinguish and dissect designs. Continue with grouping undertakings, the perceptron calculations dissect classes and examples to achieve the direct partition between the different classes of articles and relate designs got from mathematical or visual information.\n\n[Continue reading........](https://pythoncodingai.com/perceptron-model-in-a-neural-network/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o4nl8z/perceptron_model_in_a_neural_network/"}, {"autor": "JonnieSingh", "date": "2021-08-17 16:38:15", "content": "Why is my Google ML boundingBox misaligned? /!/ I am currently working on an object detection application for Android (written in Java) that uses Google ML. The application essentially focuses on drawing boundingBoxes around detected objects within a live streamed preview, provided by my device -----> camera !!!  (via CameraX). To construct the base of this application, I used [this CameraX documentation](https://developer.android.com/training/camerax/preview) to build my camera view, and then [this Google ML documentation](https://developers.google.com/ml-kit/vision/object-detection/custom-models/android).\n\nBut back to my issue in question; ***at the bottom of this post*** is what my application looks like when it detects an object. Excuse the fact that it's labelled as a letter opener. My biggest concern lies with how this issue can be resolved. It seems to be missing its detect object by a noticeable amount.\n\nHere's the DrawGraphic java file that I used to draw the boundingBox:\n\n    public class DrawGraphic extends View {\n    \n        Paint borderPaint, textPaint;\n        Rect rect;\n        String text;\n    \n    \n        public DrawGraphic(Context context, Rect rect, String text) {\n            super(context);\n            this.rect = rect;\n            this.text = text;\n    \n            borderPaint = new Paint();\n            borderPaint.setColor(Color.WHITE);\n            borderPaint.setStrokeWidth(10f);\n            borderPaint.setStyle(Paint.Style.STROKE);\n    \n            textPaint = new Paint();\n            textPaint.setColor(Color.WHITE);\n            textPaint.setStrokeWidth(50f);\n            textPaint.setTextSize(32f);\n            textPaint.setStyle(Paint.Style.FILL);\n        }\n    \n        @Override\n        protected void onDraw(Canvas canvas) {\n            super.onDraw(canvas);\n            canvas.drawText(text, rect.centerX(), rect.centerY(), textPaint);\n            canvas.drawRect(rect.left, rect.top, rect.right, rect.bottom, borderPaint);\n        }\n    }\n\nand here is what the application would show when detecting an object;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/c2hykssr4yh71.jpg?width=789&amp;format=pjpg&amp;auto=webp&amp;s=f02d82176a5b07eb0c310803c6cfe1cab1535569", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p67icj/why_is_my_google_ml_boundingbox_misaligned/"}, {"autor": "bezoarboy", "date": "2021-08-17 13:31:45", "content": "Approach to extending MNIST-like digit recognition to multiple digits? /!/ \\*\\*What would be a reasonable approach to extending MNIST-like digit recognition to multiple digit numbers?\\*\\*\n\nI'm monitoring a portion of the screen for numbers typically from 1 to 3 digits long. I've been using \\`pytesseract\\` to convert processed images into a numeric (config restricting it to digits). However, performance is occasionally inconsistent -- specifically, '6' is often recognized as either '8' or, more bizarrely, '88'.\n\nRather than optimize using \\`pytesseract\\`, I thought it'd be fun to try to train the classifier from scratch.\n\nWhat would be a reasonable approach? Train segmentation separately to pull out each individual digit -----> image !!! , and then feed each segmented digit into a separately trained digit-classifier, and then reconstruct the numeral based on the location of each recognized segmented digit? Or feeding the entire numeric image to do it all at once? Or something else?\n\nAny advice for a reasonably straight-forward approach? No code requested, but pointing me at useful Python libraries would be great. (I mostly work in R, and am less familiar with the Python universe of data science.)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p63uaq/approach_to_extending_mnistlike_digit_recognition/"}, {"autor": "Successful_Boat_3099", "date": "2021-08-17 08:46:56", "content": "\ud83d\udc8aYour daily dose of machine learning : FCN for -----> image !!!  segmentation /!/ &gt; This is a series of posts that I post almost daily. I call them \u201cyour daily dose of machine learning\u201d. \n\nOne way to perform semantic segmentation is to use Fully Convolutional Networks (FCNs).\n\nAn FCN contains only convolutional layers, which enables it to take an image of arbitrary size and produce a segmentation map of the same size.\n\nFCN uses skip connections, in which feature maps from the final layers of the model are up-sampled and fused with feature maps of earlier layers. Like the image below.\n\nThis enables the model to produce accurate and detailed segmentations.\n\nThis architecture was tested on PASCAL VOC, NYUDv2, and SIFT Flow, and achieved state-of-the-art segmentation performance.\n\n&amp;#x200B;\n\n Follow me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!*** \n\n&amp;#x200B;\n\nhttps://preview.redd.it/5f0lbglqsvh71.png?width=836&amp;format=png&amp;auto=webp&amp;s=c65540caa8387d11bea23e1284301d430b72bda7", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p5zvuu/your_daily_dose_of_machine_learning_fcn_for_image/"}, {"autor": "reallfuhrer", "date": "2021-08-17 01:03:17", "content": "How do I decide number of nodes in a Neural Network /!/ I was trying to implement -----> image !!!  classification using NeuralNetworks but how do I decide on the number of nodes in each layer? is there a proper method to implement it or we just have to do hit and trial?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p5tgd3/how_do_i_decide_number_of_nodes_in_a_neural/"}, {"autor": "AeroDEmi", "date": "2021-08-17 00:57:15", "content": "-----> Image !!!  Matting /!/ In the original Image Matting [website](http://www.alphamatting.com/eval_27.php) we have some errors types: SAD, MSE, Gradient and Connectivity.\n\nWhere can I learn more about the Gradient and Connectivity errors?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p5tcmf/image_matting/"}, {"autor": "RickutoMortashi", "date": "2021-08-16 17:22:07", "content": "Image Segmentation using Reinforcement Learning /!/ So in one research paper, it was mentioned that -----> image !!!  segmentation still requires improvements due to the fact that most -----> image !!!  segmentation solutions are problem-based. How does something being problem-based make it imperfect?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p5knhg/image_segmentation_using_reinforcement_learning/"}, {"autor": "whiteowled", "date": "2021-08-16 14:32:14", "content": "Heatmaps can help you debug -----> image !!!  classification models /!/ I recently did a deep dive into heatmaps and how they can be used in order to debug image classifiers. Results of what I found was a straightforward technique called integrated gradients that shows if your model is including extraneous information, and that knowledge can help you adjust your training data to improve accuracy.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/rt9ee777dqh71.png?width=2400&amp;format=png&amp;auto=webp&amp;s=7229fbc47bb470c37dc766c727e74aedac981db6\n\nBased on what I learned, I put together a blog ([https://www.whiteowleducation.com/blog/2021/08/05/explainable-ai-overview/](https://www.whiteowleducation.com/blog/2021/08/05/explainable-ai-overview/) ) that goes over the technique. I also explain this technique by comparing it against other data science methods (so you know when to use the heatmap technique). \n\n&amp;#x200B;\n\nHope you enjoy reading the blog as much as I enjoyed putting it together.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p5h86k/heatmaps_can_help_you_debug_image_classification/"}, {"autor": "zby", "date": "2021-08-16 14:29:03", "content": "Unwanted correlations in ML training data /!/  Any good sources on how to measure them and how to deal with them?\n\nI am training neural networks to detect some objects by their shapes. The problem is that the -----> photo !!!  sets are not color-balanced - that is there are correlations between the object color and the object type that do not occur in 'nature' (or occur to a much smaller extent, or maybe we are legally required to not look at these correlations). The networks are probably learning it instead of recognizing the shapes.\n\nI would like to have two tools - fist is to measure the bad correlation in the data set. The second is a tool for helping choosing photos from a photo set to create a more color balanced data set. The second one might be difficult.\n\nThis looks like a very common and generic problem - but I have trouble in finding good solutions for it.\n\nIt is a bit complicated because there are three color channels (RGB) and also the color variable is semi-continuous one.\n\nAre there any tutorials about that? Libraries in Python?\n\nTo simplify it I guess I could do it for each color channel separately and also divide the color values into maybe 8 buckets to make the color variable discrete. Then I could probably use chi squared, which is available in scikit-learn, for that - but I still need to wrap my head around that because the lib only expects a sequence not a table. Or maybe I could use Logistic Regression, also available in scikit-learn. Logistic Regression is about building a predictor that learns the block class from its color - exactly what I need. But it is not immediately evident if there is a way to get a measure of how good is the predictor and also there might be problems with speed.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p5h5y3/unwanted_correlations_in_ml_training_data/"}, {"autor": "ppgogz", "date": "2021-08-16 13:55:06", "content": "Text detection /!/ Is there any step by step guide to detection of text in natural images. I mean where it is explained the basics of how a part of -----> image !!!  can be cropped based on presence of text in that area. I am aware of ocr but that can only be applied once the text section of a picture can be cropped using some method.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p5gjbw/text_detection/"}, {"autor": "Successful_Boat_3099", "date": "2021-08-16 08:35:33", "content": "\ud83d\udc8aYour daily dose of machine learning : -----> image !!!  segmentation /!/ &gt; This is a series of posts that I post almost daily. I call them \u201cyour daily dose of machine learning\u201d.\n\nImage segmentation is the process of classifying each pixel in an image.\n\nThere are 2 types of image segmentation : semantic segmentation and instance segmentation\n\nIn semantic segmentation, you categorize the pixels into N categories (tree, sky, person, \u2026).\n\nIn instance segmentation, you add another step to semantic segmentation, which is to detect and delineate each object of interest in the image. \n\nIf you have two trees in an image, then semantic segmentation will classify all the pixels from both trees into one category and there won\u2019t be any difference between the two trees.\n\nInstance segmentation on the other hand, will tell you that the two trees are different, but they still belong to the same class which is \u201ctree\u201d.\n\nFollow me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!***", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p5c0y8/your_daily_dose_of_machine_learning_image/"}, {"autor": "patrikb42", "date": "2021-08-15 19:16:34", "content": "Deep reinforcement learning number prediction /!/ Hello, I am trying to make an AI guess X and Y coordinates on the screen, from a 2d grayscale -----> image !!!  as input. All reinforcement learning tutorials seem to use a decision based system, instead of number prediction, what's the best way to approach this problem?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p4zsd9/deep_reinforcement_learning_number_prediction/"}, {"autor": "racc15", "date": "2021-08-23 07:20:34", "content": "Could someone please help me understand this -----> image !!!  segmentation paper? /!/ Hello, I am doing a project on polyp segmentation in colonoscopy images. I have recently read this paper provided [here](https://drive.google.com/file/d/1isi_Blz9ZAK4iPH5wKcEVw4-FSYuqNxm/view).   \nI have a few question about the neural network architecture and method. The authors state that they generate ***8x8*** image patches from the original images (page 4 left column).  But, my question is what do they actually use as inputs to the CNN network? Do they give a single patch per instance or, do they feed all patches generated from a particular original image?\n\nsuppose, for image 1 they generate 20 patches. Then, does the model run twenty times and twenty outputs are generated or, is the model run once and only one output is generated?\n\n&amp;#x200B;\n\nAlso, they state that at end of the network , they use fully connected layers. As far as I can tell, the final layers has either 10 or 1 neurons (section C.3 and figure 4 on page 5 left column). How can this be for a segmentation task? shouldn't there be just as many output neurons as the input image pixel number?\n\nPlease help me understand. Thanks in advance.\n\nI apologize if this type of post is inappropriate for this subreddit.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p9upcw/could_someone_please_help_me_understand_this/"}, {"autor": "Primary_Research5508", "date": "2021-08-23 04:56:33", "content": "Model per class for -----> image !!!  segmentation /!/ I have small project with roughly 1000 images handmade for image segmentation. I am currently annotating them and I have one problem that there are multiple pictures where the parts of an object are appearing from another angle but not fully visible. I decided to not annotate them but that probably will degrade the performance.\n\nIf I annotate them in all pictures it will take a lot of time, I come up with and idea to separate the model, one of them will segment image and find one or two classes while another model will detect another parts. How do you think is it okay way ? I guess that 1000 images are not enough, I am planing to transfer learning, I hope it can help to solve the problem.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p9su8h/model_per_class_for_image_segmentation/"}, {"autor": "noctizio", "date": "2021-08-22 22:53:34", "content": "Public 3D medical images dataset /!/ Does anyone know a public medical 3D -----> image !!!  dataset for semantic segmentation that i can train a unet on in google colab . all the datasets that i found are private", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p9mx2j/public_3d_medical_images_dataset/"}, {"autor": "throwawayafw", "date": "2021-08-22 15:00:50", "content": "Mitosis detection - If I insert a test -----> image !!! , I need the output -----> image !!!  to be bounded with boxes which indicate mitosis. Does this problem qualify for image classification or object detection? /!/ Is there any way to incorporate transfer learning in this case??", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p9dycn/mitosis_detection_if_i_insert_a_test_image_i_need/"}, {"autor": "thumperroo", "date": "2021-08-22 08:43:11", "content": "First time working with AzureML and Azure Storage /!/ Hope this is the appropriate sub! \n\nHi all, a bit of my background to explain my current struggle. I did my bachelors and master in totally unrelated fields to data science. The only relevant course I took was Statistics in my first year. During my first year of job searching (2018), I realised I wanted a different career path and became super interested/passionate about working with algorithms and data. I got into a master in Data Science (2019), fell in love with it, graduated March 2021, and now I'm in my first full time position. \n\nThe job's great and I've explored and completed different time series, NLP, and other analytical projects. Now, I have a project whose -----> image !!!  data is stored in Azure Storage. I need to build a classification pipeline that uses image data from a blob folder, and outputs results in another blob folder. This is a client project. \n\nI've read tons of documentations on Azure ML SDK and watched some YouTube tutorials on getting started. I have Azure connected to VS Code (we use Python). I've set the workspace using subscription ID, key, container name. But now I'm stuck with how to pull the data from the folder, and place the results in another folder in Azure Storage. E.g.: to read a csv, simple. But how do I read in several thousands of images into a classification model? \ud83d\ude29\n\nI guess I feel a bit overwhelmed because the deadline is in 2 weeks and I have never used Azure before. I probably have overlooked/missed documentations. I have a senior on the project with me, but he is also relatively new to Azure / has the same questions as me. My company's maturity in data science and Azure is very young, which is why there isn't much guidance internally. \n\nCould someone please provide some guidance on how to set up the path for pulling the data from Azure Storage folder and exporting into another folder path? Additional material on learning Azure ML is very welcomed as I want to thoroughly learn it too, not just for the sake of a project.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p9910l/first_time_working_with_azureml_and_azure_storage/"}, {"autor": "loomedin", "date": "2021-08-21 02:06:41", "content": "Notetaking software recommendation for documenting concepts /!/ I am interested in software that allows me to write what I learned in my own words  for easy reminders and to help cement my understanding, especially the more technical details.\n\nFor example something similar to https://sites.google.com/site/machinelearningnotebook2/ in terms of organization. Just something I can quickly and easily organize concepts with maybe latex/-----> image !!!  support or similar. What I have so far been looking at is wiki software but there are many, and I've yet to find one I like or that I want to commit to the learning curve at the moment. (easily editable across multiple devices is a plus, such as with git)\n\nany recommendation is great, thank you", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p8ixa0/notetaking_software_recommendation_for/"}, {"autor": "baby--steps", "date": "2021-07-04 16:01:10", "content": "In need of beginner friendly code or tutorial for object localization /!/ I want to learn how to train a model to draw bounding boxes around insects in JPG images. As far as I understand that is an object localization problem (just getting the bounding boxes) and not object detection (bounding boxes + classification). Whatever the technical vocabulary is, for me is important in a first step to be able to crop out the insects from the -----> image !!! s (can be one or multiple insects in an -----> image !!! ) and then classify those cropped tiles later on (that is, what kind of insect is there). I have several thousands labeled with bounding boxes manually already, but I would like to automate the process.\n\nI came across suggestions like YOLO and RCNN, which seem to do both localization and classification. The internet seems packed with information, and I feel overwhelmed. I am interested in up-to-date tutorials if possible. I have enough coding experience with Python and very familiar with R.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/odnbmh/in_need_of_beginner_friendly_code_or_tutorial_for/"}, {"autor": "SecureHelicopter1", "date": "2021-07-04 11:06:50", "content": "How to output probabilities in -----> image !!!  recognition with cnn /!/ Using tensorflow, let's say I wanted to classify images of dogs, cats, and birds. How would I have it output the probability of an image being a dog, cat, and bird instead of just guessing an answer?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/odib00/how_to_output_probabilities_in_image_recognition/"}, {"autor": "miraksy", "date": "2021-07-03 10:12:47", "content": "Should i give the same input to an ensemble of multiple neural networks for -----> image !!!  segmentation /!/ I  was wondering if anyone tried already to use an ensemble of networks for image segmentation. I was thinking of doing different preprocessing methods of the same image and for each method train a network and put all networks in a perceptron for the final segmentation mask. \n\nI couldn't find much info on this, mostly people train multiple networks on the same image and use a mean to generate the final result.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ocvbq0/should_i_give_the_same_input_to_an_ensemble_of/"}, {"autor": "DaveTheAutist", "date": "2021-07-03 08:19:25", "content": "How do I use openCV on DICOM images? /!/ I'm currently doing a course on medical imaging and the assignment I currently have to do is segmenting shapes within an X-ray -----> image !!! . \nWhat I'm currently struggling with is, how does openCV process image arrays ? Can the methods in the openCV library take in 2D numpy arrays as inputs? \nAlso if anyone can link resources where they use openCV for medical imaging purposes that will be really helpful.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/octxtu/how_do_i_use_opencv_on_dicom_images/"}, {"autor": "dynamicopera", "date": "2021-07-03 03:48:23", "content": "Anyone implemented latest -----> image !!!  segmentation models/tuning from cvpr 2021? /!/ I am doing an image segmentation project using https://github.com/qubvel/segmentation_models as the baseline. I was wondering if any of you have tried the latest segmentation models from cvpr papers. If yes, which ones you found to be interesting or actually improve miou. And how difficult/easy it is to implement those? \n\n\nI am using deeplabv3 now btw.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ocqdew/anyone_implemented_latest_image_segmentation/"}, {"autor": "TerribleCookJames23", "date": "2021-07-08 18:00:45", "content": "Topological deep learning in python /!/ I'm trying to do -----> image !!!  segmentation using U-Net in PyTorch. I understand PyTorch pretty well but I'm still learning so there's a lot I haven't tried out yet. What I'm trying to do is incorporate persistent homology in U-Net. There are two ways to do this: using a topological loss function or a topological layer. These methods have been used before and the code is on github.( i.e [https://github.com/HuXiaoling/TopoLoss](https://github.com/HuXiaoling/TopoLoss) , [https://github.com/MathieuCarriere/perslay](https://github.com/MathieuCarriere/perslay))  I'm still new to this so I don't really understand what's going on in terms of the code. I understand the concepts and I know the math behind persistent homology. Anyone willing to have a look and give me some tips ? Basically, I just want to know how to use these codes in a segmentation model like U-Net.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ogcot4/topological_deep_learning_in_python/"}, {"autor": "annoying_seagull", "date": "2021-07-08 04:21:14", "content": "Can somebody help me out with Hopfield Networks. /!/ I am trying to implement a Hopfield Network in python. But sometimes the network isn't showing the correct behavior like not converging which it always should.\n\n        \n    class HopfieldNetwork:\n        def __init__(self, number_of_nodes, learning_rate = 1):\n            self.number_of_nodes = number_of_nodes\n            self.learning_rate = learning_rate\n            self.weights = np.zeros((number_of_nodes, number_of_nodes))\n        \n        # Randomly fire nodes until the overall output doesn't change\n        # match the pattern stored in the Hopefield Net.\n        def calculate_output(self, input):\n            changed = True\n            input = np.array(input)\n            c=0\n            while changed and c&lt;200: #doesnt always converge so I have to stop it\n                changed= False\n                c=c+1\n                if c&gt;18:\n                    print(\"beeeeeeeeeeeeeeeeeeeeeep\")\n                indices = list(range(len(input)))\n                random.shuffle(indices)\n                new_input = np.zeros((self.number_of_nodes))\n                clamped_input = input.clip(min=-900000) # eliminate nodes with negative value, doesn't work either way\n                for i in indices:\n                    sum = np.dot(self.weights[i], clamped_input)\n                    new_input[i] = 1 if sum &gt;= 0 else -1\n                    changed = not np.allclose(input[i], new_input[i], atol=1e-3)\n                input = np.array(new_input)\n            return np.array(input)\n    \n        def calculate_output2(self,input):\n            output = np.dot(self.weights,input)\n            # apply threshhold\n            output[output &gt;= 0] = 1 # green in -----> image !!! \n            output[output &lt; 0] = -1 # purple in image\n            return output\n        \n        # Store the patterns in the Hopfield Network\n        def learn(self, input):\n            # hebian learning\n            I = np.identity(self.number_of_nodes) # diagnol will always be 1 if input is only 1/-1\n            updates = self.learning_rate * np.outer(input,input) - I\n            updates = updates/self.number_of_nodes\n            self.weights = self.weights + updates\n        \n        def calculate_energy(self, input):\n            I = np.identity(self.number_of_nodes) # diagnol will always be 1 if input is only 1/-1\n            cross_product = np.outer(input,input) - I\n            energy = -(1/2) * np.sum(self.weights * cross_product)\n            return energy", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ofzsbl/can_somebody_help_me_out_with_hopfield_networks/"}, {"autor": "9302462", "date": "2021-06-17 21:06:59", "content": "I have an ML question, not even sure what the answer would be. Show me what I don't know I don't know. /!/ My background is in backend programming and I have read a lot about ML over the last year, but haven't done any because I couldn't apply it to my work. However, lucky me, I just got handed what seems like a simple ML task but not sure what to use or even what to ask.\n\nI would like to be able to feed in an ecommerce webpage, either html or as an -----> image !!! , and have it return what it thinks are fields for price, title, description, sku. It should be trained to work on many different ecommerce sites(things built with Shopify, bigcommerce, or home rolled. Doesn't have to be perfect, but above 90% accuracy would be ideal. I can dedicate the time and effort to labeling data and creating data sets.\n\nI don't know if this would be trained based on text content, the positioning on the page, or things I don't even know of. \n\nWhat type of training needs to be done?\nWhat would the data pipeline look like?\nShould I OCR the text from the physical page?\nWhat else should I be thinking of?\n\nThanks in advance for any replies. I have don't even know what pathway to start going down right now.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o27vyx/i_have_an_ml_question_not_even_sure_what_the/"}, {"autor": "InternetOfKings", "date": "2021-06-16 15:35:39", "content": "Basic MobileNet and Quantization question /!/  \n\n1. When would you not want to use MobileNet as a model to train for -----> image !!!  classification or even -----> image !!!  detection? If I'm just training a model to identify if something is a cat or dog, would it be more efficient to create my own model that only has those 2 object classes vs MobileNet's 1000 object classes?\n2. When would you situationally decide to use float32 vs uint8 vs int8 for input/output data types for quantization? Is me using an MCU/GPU/CPU/NPU determine how I quantize my model? I understand why you would want to go from float 32 to int8/uint8, but what circumstances would cause you to make this jump?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o17ous/basic_mobilenet_and_quantization_question/"}, {"autor": "Edulad", "date": "2021-10-15 14:54:52", "content": "I am a bit Lost /!/ Hi, guys so for the past months, have been learning python.\n\nso far have also created a Django blog, pushed some stuff to GitHub and also temporary hosted it on heroku.\n\nnow i am making another project, **web-scarped some SHOE IMAGES.**\n\n**I want to remove the Background Color from these Images.**\n\n**Trim them, so only The shoe -----> image !!!  is saved as optimized web Image in png format.**\n\nthere are online sites that do this, but are not free like [https://www.remove.bg/](https://www.remove.bg/)\n\nis there a way to do this in python with **opencv, tenserflow etc.**\n\ni have accounts with kaggle and google collab, so can use it for testing purposes as i don't have a strong GPU or CPU.\n\nHas anyone does this before, i found one github project, that does this but it does not detect the Image Subject correctly.\n\n[https://github.com/jrzaurin/Shoe-Shape-Classifier](https://github.com/jrzaurin/Shoe-Shape-Classifier)\n\nPlease Help, Thank you :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q8q46l/i_am_a_bit_lost/"}, {"autor": "cheapyx", "date": "2021-10-15 13:52:29", "content": "First ML project in progress, open for suggestions /!/ My goal is to make **regression** model that will predict orientation of the vehicle based on -----> image !!!  input. Not just side of the vehicle but estimation of angle.  \n\n\nFirst phase of this journey was all about **dataset**, i never found labeled pictures that i can used for training and since i treat this as a regression problem and not as a classification i need to have a lot  of pictures. So i made synthetic dataset for my purpose, i took around 3000 3d models of various cars and i made custom tool that will generate me pictures of vehicles from different angles, so far i have produced 1.7Million of pics from all angles and for all 3d vehicles. To me it seems that dataset has right qualities for my task, pics added to the post (lots of cars, lots of angles, lots of background noise)  \n\n\nSecond phase is model training, validation and testing on real world images, problem is that simple(around 450k params) models that i made never reached desired performance that is required for production. So i decided to make bigger model and to retrain it. So i made model with 7Milion parameters but this performed even worse. It never wanted to converge and loss was the same from epoch 1 to epoch 4 where i pulled the plug and it was 3.5344, if i elevate learning rate loss starts from 100k and drops to 8 and it slows, same thing with loss that decays as epochs go on.\n\nHardware:\n\n* Ryzen 9 3900X\n* 64Gig DDR4\n* SATA3 SSD for dataset\n* RTX3070Ti  \n\n\nI labeled pictures this way:   \n\n\\-0.48869219055841207#1.4968572891369563#1634256170416.jpg\n\nFirst token is **azimuth** angle, second one is **polar** angle and last one is unix timestamp\n\nI am training my model just to predict Azimuth angle and value can be anywhere between -3.14 to 3.14 if you are looking in headlights value will be 0 if you are looking directly to the back of the car azimuth is 3.14,  if you are looking car from a side it will be PI/2 if you look it from the other side it will be -PI/2, in other words angles from 0 to 180 are mapped into 0 to 3.14 and from 180 to 360 are mapped to -3.14 to 0. \n\nQuestions:\n\n1.   \nIs my labeling making it hard for model to converge because angle 179 corresponds  to azimuth \\~**positive** 3.13 and angle of 181 \\~**negative** 3.13 and picture to my eyes looks the same(back of the car), and that results in MASSIVE weights change to compensate for such a great error\n2.   \nIf my labeling is to blame should i solve this problem by making multiple regression NN where one net is concerned if pic is from front or back and one is it left or right, than to feed results in third net which will predict azimuth? (why one net with 7M params cant solve this, is it labeling)\n3. How to estimate number of parameters that i need for certain other than experimentation.\n4. How to adjust ratio of parameters which are in convolutional layers and which are in Dense layer\n\nif you made it till here, thank you and feel free to brainstorm!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q8ow9r/first_ml_project_in_progress_open_for_suggestions/"}, {"autor": "beatsbysurf", "date": "2021-08-02 23:57:09", "content": "How to build a model that incorporates tabular and -----> image !!!  data? /!/ Hey everyone! So recently, I've been trying to make a binary classification model that incorporates data from a csv as well as images. I believe the best approach would be to run the images through a cnn while concurrently running the tabular data through some dense layers, but i'm completely stumped in how to implement it. Would anyone be able to help? This is in pytorch if that helps any.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/owr772/how_to_build_a_model_that_incorporates_tabular/"}, {"autor": "Successful_Boat_3099", "date": "2021-08-02 06:39:22", "content": "Your daily dose of machine learning [day 3] /!/ &gt; This is a series of posts that I post almost daily on [LinkedIn](https://www.linkedin.com/in/nour-islam-mokhtari-07b521a5/) and now I am sharing them here as well. I call them \"*your daily dose of machine learning*\". \n\n&amp;#x200B;\n\nIn deep learning for computer vision, there are mainly 2 types object detection models : one-stage detectors and 2-stages detectors.\n\n&amp;#x200B;\n\nThe one stage object detectors such as YOLO and SSD take as input the -----> image !!!  and some pre-configured bounding boxes called anchor boxes.\n\nThe image will pass through the neural network and feature maps are extracted at different levels (after each convolutional layer).\n\nThe network will then try to predict a set of boxes and their corresponding class. The loss function (which is a sum of several losses) is designed \n\nin such a way that the network will learn to predict correct bounding boxes from the anchor boxes. The network will also learn to predict the correct class for each bounding box.\n\n&amp;#x200B;\n\nAs you can see, all of this is done in one stage.\n\n&amp;#x200B;\n\nThe 2-stages object detectors such as Faster RCNN, will actually learn to detect objects in images by splitting the task into 2 parts (2 stages) that are apparent in the network architecture itself:\n\n1. The first part of the neural network will try to find areas in the image where a possible object might exist. This first stage basically outputs \n\nfeature maps where each cell can either be background or potential objects. We can also infer some bounding boxes around the areas where there is a potential object (thus the region proposals naming).\n\n2. The second stage will take the output of the first stage and will try to predict the correct class for each \"potential object\". Also, this second stage will use anchor boxes just like one-stage detectors and will try to tighten the boxes that we inferred from the first stage.\n\n&amp;#x200B;\n\nFeel free to share suggestions about things you want me to cover in the future.\n\nTo be notified about this content follow me on [LinkedIn](https://www.linkedin.com/in/nour-islam-mokhtari-07b521a5/) and [Twitter](https://twitter.com/NourIslamMo).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ow9488/your_daily_dose_of_machine_learning_day_3/"}, {"autor": "EncryptedSoul57", "date": "2021-04-18 17:07:00", "content": "Class Imbalance Problem in Brats Dataset /!/ Hi, I built a u-net model and tried to run in it with this dataset [https://www.synapse.org/#!Synapse:syn22159468/wiki/603890](https://www.synapse.org/#!Synapse:syn22159468/wiki/603890) this is a dataset for semantic segmentation and it contains aprox. 20 slices per -----> image !!!  and my model works fine with this dataset. But when I try to run my model on Brats dataset I get pretty bad results. I believe that this is because of the class imbalance in brats dataset.  \nHow can I solve this issiue ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mtgtz4/class_imbalance_problem_in_brats_dataset/"}, {"autor": "RunningInCircles16", "date": "2021-04-18 04:18:42", "content": "Creating plots (like ROCAUC) after training a model in Tensorflow? /!/ Hi there, rookie tensorflow user here. \n\nSo I have trained an -----> image !!!  Classification model while calling  accuracy, precision, and recall in my metrics argument when compiling my CNN. Is it possible to create a ROCAUC plot or other types of validation plots for your model after training it? I'm coming from sklearn where that was doable, but after searching the internet for awhile, I have had no luck with my question.\n\nTIA.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mt5yci/creating_plots_like_rocauc_after_training_a_model/"}, {"autor": "mike_gundy666", "date": "2021-04-17 19:02:36", "content": "Best way to do feature selection on unlabelled images /!/ As the title states I want to do feature selection on a bunch of unlabelled data. The ultimate goal of this is to do some kind of clustering (Input an -----> image !!! , and get back the most similar -----> image !!! )\n\n&amp;#x200B;\n\nThese -----> image !!! s are quite sparse, but I would like to know what would be the best approach to this.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/20ti9j3e7st61.png?width=428&amp;format=png&amp;auto=webp&amp;s=9c1fd9a8575e581a52a92489d9c8bdba9a0fb237\n\n&amp;#x200B;\n\nhttps://preview.redd.it/vl3r7yof7st61.png?width=428&amp;format=png&amp;auto=webp&amp;s=80a9633cc8142769e61351b62d813911d52a4483\n\n&amp;#x200B;\n\nhttps://preview.redd.it/a5iq97eg7st61.png?width=428&amp;format=png&amp;auto=webp&amp;s=e777bedbb6f082042cf2e476ce72f5fbd842c10c\n\n&amp;#x200B;\n\nhttps://preview.redd.it/pfenckzg7st61.png?width=428&amp;format=png&amp;auto=webp&amp;s=3aaa2bf9b0f0566d5cdd37906fe3062e48473212", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mswss6/best_way_to_do_feature_selection_on_unlabelled/"}, {"autor": "bigboisexyboi", "date": "2021-04-17 18:20:15", "content": "Reinforcement Learning Convolutional Output (tf_agents) /!/ Hi everyone, i have a question about tf_agents which I cannot find the answer to anywhere online. I am building an RL system for -----> image !!!  generation, and am using Proximal Policy Optimization. For the networks in this system, I am using a tf_agent Actor Distribution RNN Network, and a Value RNN Network. The value network is fine and performs as I need it to, however I want the actor network to output images using something like conv2dtranspose. Is there a way to customise the actor network to include extra layers after the FC layers? Or just any good resources on creating custom tf_agents networks?\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/msvzqh/reinforcement_learning_convolutional_output_tf/"}, {"autor": "designer1one", "date": "2021-04-17 13:57:25", "content": "*Semantic* Video Search with OpenAI\u2019s CLIP Neural Network /!/ I made a simple tool that lets you search a video \\*semantically\\* with AI. \ud83c\udf9e\ufe0f\ud83d\udd0d\n\n\u2728 Live web app:[ http://whichframe.com](http://whichframe.com/) \u2728\n\nExample: Which video frame has a person with sunglasses and earphones?\n\nThe querying is powered by OpenAI\u2019s CLIP neural network for performing \"zero-shot\" -----> image !!!  classification and the interface was built with Streamlit.\n\nTry searching with text, image, or text + image and please share your discoveries!\n\n\ud83d\udc47 More examples[ https://twitter.com/chuanenlin/status/1383411082853683208](https://twitter.com/chuanenlin/status/1383411082853683208)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/msr7nl/semantic_video_search_with_openais_clip_neural/"}, {"autor": "ice_shadow", "date": "2021-04-17 04:35:01", "content": "Why is DataLoader taking longer than indexing Dataset to load my file? /!/ Im working with NIFTI -----> image !!!  files and I have set up the Dataset class to load them. When I index my dataset with d[2] it only takes 0.15 s to load, but when I do trainloader=DataLoader(d,batch_size=10) and then do next(iter(trainloader)) it takes 1.3 s almost 10x as much to load the sample\n\nWhy is DataLoader slowing down the Dataset from loading? This seems problematic for training.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/msk3my/why_is_dataloader_taking_longer_than_indexing/"}, {"autor": "CosmicMemer", "date": "2021-04-16 21:38:05", "content": "Struggling to to train GPT-2 model on RTX 3060 /!/ Hi, so for the past several days, I've been experimenting with GPT-2 and I really like the technology. Super cool stuff, and my end goal for my current project is to build a Discord chatbot that can respond to people with GPT-2 generated messages, using a model trained on that same Discord server's history.\n\nTo trial and sanity-check this idea, I trained the small GPT2 model on Google Colab, downloaded the checkpoint from there and plugged it into my bot script (which i already have.) Now what I want to do is train this same model on my own hardware (an RTX 3060) so that i can leave it running overnight without getting timed out.\n\nI have the basics of everything I need. I have the dataset and all my scripts for generating text and all that pre-prepared, and i have the 355M model folder downloaded as well. The only problem is I've run into a good bit of trouble trying to *finetune* the model on my GPU on ubuntu. I got it training, but it was only using my CPU, so I installed cuda and cudnn and got everything set up, but it still refused to use my GPU. I searched and found that I would need to install an older version of CUDA, which fails because I need an older version of GCC to install it, which fails because my Ubuntu version is too new, and I'm not willing to reinstall to an older one.\n\nI tried someone else's docker -----> image !!!  specifically built to do what I'm attempting, and that's run into myriad problems and crashes that I'm basically declaring unsolvable at this point after a whole day of searching and troubleshooting.\n\nMy basic question is this: what is the easiest, simplest way to finetune the 355M model to my dataset *on my own gpu*, accounting for things like version differences? Windows is preferred at this point, since I haven't touched anything there yet, but if there's a silver-bullet, i'm-just-overthinking it solution for Ubuntu, I'll try whatever you've got.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/msd5fz/struggling_to_to_train_gpt2_model_on_rtx_3060/"}, {"autor": "KhanDescending123", "date": "2021-04-16 19:15:56", "content": "Dealing with Empty Masks in Train Data for Segmentation Task /!/ I'm training a convolutional model on a segmentation dataset where the targets are binary masks, this is the Brain MRI Dataset on Kaggle: [https://www.kaggle.com/mateuszbuda/lgg-mri-segmentation/](https://www.kaggle.com/mateuszbuda/lgg-mri-segmentation/)\n\nMy question is: how should one deal with the presence of empty masks in the dataset? In a real-world setting there would be certain -----> image !!! s at inference time that require no mask, how can you make your machine learning system robust to this case?\n\nI've considered a few options:\n\n1) Train a corresponding classifier to detect whether or not the instance should be masked - at inference time run the classifier and only run the segmentation model if the classifier indicates the -----> image !!!  should be segmented.\n\n2) Develop a training schema where the segmentation model is able to accurately predict an empty mask for the appropriate samples (this seems hard and pretty infeasible but I'm curious as to whether this would work)\n\nI'm particularly interested in how one would handle a similar problem in the industry, not necessarily in the medical field.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/msab9a/dealing_with_empty_masks_in_train_data_for/"}, {"autor": "hiiamayush", "date": "2021-04-16 18:59:00", "content": "Innovative ideas for the reverse -----> image !!!  search /!/ I wanted to know what innovative have you done or want to do with simple reverse image search.\nCurrently my code does ris and i am supposed to add something innovative to the project.\nAny help/idea would be appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ms9z30/innovative_ideas_for_the_reverse_image_search/"}, {"autor": "ice_shadow", "date": "2021-04-24 03:54:49", "content": "Pytorch SSIM and PSNR metrics? /!/ Anywhere I can find these? I need it to compare -----> image !!!  reconstructions. I found this here \n\nhttps://pytorch.org/ignite/metrics.html\n\nBut this library looks very different and its not like I could use SSIM(Xtrue,Xrecon) or anything with the SSIM function. It has to be with some Engine and I don\u2019t know what that is as I am just using regular PyTorch. \n\nI need a function like this and scikit-image is the other option but it won\u2019t be able to use pytorch tensors, and I don\u2019t want to have to convert to numpy every time in a loop.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mxc595/pytorch_ssim_and_psnr_metrics/"}, {"autor": "KingJeff314", "date": "2021-04-24 02:46:53", "content": "Best Google Colab Workflow for Image Datasets? /!/ I am doing my first computer vision project, and its been going well. I wrote a few -----> image !!!  processing scripts to format labeled -----> image !!! s. Now I have a directory of 40,000 images that takes up about 1GB. I tried training it on my machine, but my GPU doesn't support Tensorflow. So I turned to Google Colab. And while Google provides a great service, my biggest pain has been transferring that directory from my local machine to Colab. I've searched this online, and the top suggestion has been uploading to Google Drive. However, it took me upwards of 3 hours to upload the whole directory, which is just 1GB. And I may need to reformat or add to the data later on, so this will be quite tedious to do every time. So I just would like to hear how you guys have dealt with Colab and largish data sets. Thanks.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mxb4m4/best_google_colab_workflow_for_image_datasets/"}, {"autor": "Finger__Blaster", "date": "2021-04-23 19:56:01", "content": "Need help with a TF model from scratch for a very specific -----> image !!!  classification application. /!/ I have no idea what i'm doing.  I really don't understand the math behind tensorflow at all.  I'm just trying to make something work.  Famous last words, and signs of someone in over their head.\n\n&amp;#x200B;\n\nSo followed this tutorial because it felt felt like a kind of similar problem, [https://www.tensorflow.org/tutorials/images/classification](https://www.tensorflow.org/tutorials/images/classification) .  Probably my first mistake?  My code is a copy of what was used in this tutorial.\n\n&amp;#x200B;\n\nhere is an example of my dataset [https://imgur.com/a/9zEjYSF](https://imgur.com/a/9zEjYSF) , I'm trying to identify different handstamps (and eventually different material based on color) of the same caliber casings. \n\n&amp;#x200B;\n\nI'm wondering if this is a problem for image classification, or if i should be considering a different approach like OCR?  I'm leaning image classification because I would eventually like to be able to identify features such as color to determine material (brass, steel, or nickel), and some other physical characteristics.\n\n&amp;#x200B;\n\nThe issue is I am just getting awful predictions, to the point where I think i'd be as accurate rolling a dice.  I'm just looking for some nudges in the right direction.  Do i just need more images?  i have about 300 per label right now.  I figured that a lower number would be ok, since everything is so well controlled.  Lighting is controlled, part position is controlled, camera angle is controlled, it's only rotation that really changes.  And of course the part it self being more worn out, scratched, dinged, etc.\n\n&amp;#x200B;\n\n Do i need a different amount of layers or output shapes?  something else entirely?  Or just more photos?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mx3nua/need_help_with_a_tf_model_from_scratch_for_a_very/"}, {"autor": "Tuppitapp1", "date": "2021-04-23 18:47:41", "content": "Understanding LSTM predictions /!/ In machine vision you can create a heatmap of CNN activations to understand which parts of the -----> image !!!  your model found important when making each prediction. Is there any similar mechanic for LSTMs and timeseries tabular data? My Keras model is behaving in weird ways and I'm trying to understand what's wrong with my data.\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mx27u3/understanding_lstm_predictions/"}, {"autor": "mlnewbie222", "date": "2021-04-23 14:37:53", "content": "How to get started with hierarchical classification? /!/ Hi everyone, I'm fairly new to machine learning and I'm working on an -----> image !!!  classification problem where each label falls under a particular super class type. For instance, I would have some labels designating different types of dogs, cats, birds, etc. Up till now I've worked with more basic CNNs where I would guess the label directly without taking into account super and sub class relationships. However, I'm interested to see how that might potentially improve performance. So far I haven't found many good resources detailing how to actually implement such a model which could first recognize that an image is a dog and then make a determination about the specific breed. Does anyone here have any insight on how to go about this? Any suggestions for books, articles, and videos would also be greatly appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mwwp8j/how_to_get_started_with_hierarchical/"}, {"autor": "kuryu4", "date": "2021-04-23 07:57:12", "content": "Classifying serial codes in images /!/ I have a data set of ~50,000 images. They are pictures of serial codes in the form of ABC-123 so any 3 letters and any 3 numbers separated by a hyphen. Really I\u2019m not sure where to start as I\u2019m pretty new to ML and have only been spoon fed projects in school. I was thinking I could gray-scale the -----> image !!!  since the background is bright. Then locate the pixels and run a letter/digit classifier. Any help would be appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mwq9ed/classifying_serial_codes_in_images/"}, {"autor": "charchit_7", "date": "2021-04-22 21:44:22", "content": "[FASTAI] Weird predictions on an -----> image !!! , I have 62 classes(10 numbers, 26+26 small/capital letters), and using fastai and Resnet34 I was able to get 87% accuracy on the validation set. But while prediction on an image it shows almost all the classes. Can anyone please tell me what is wrong here? /!/ [https://docs.google.com/document/d/15d5h6gIlb7m5cxpASlgesKmr9F67LIn-wHysvCzjvD8/edit?usp=sharing](https://docs.google.com/document/d/15d5h6gIlb7m5cxpASlgesKmr9F67LIn-wHysvCzjvD8/edit?usp=sharing)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mwfo5t/fastai_weird_predictions_on_an_image_i_have_62/"}, {"autor": "lightsforceps", "date": "2021-04-22 15:01:38", "content": "Is it possible to augment the anchor vectors in a YOLO network? /!/ Hi All,\n\nshort version: Is it possible to add variables to the anchor vectors in YOLO? \n\nlong version:\n\nI have an -----> image !!!  localization and classification problem I am trying to apply machine learning too. I am considering using a modified version of YOLO but thought I'd post here before getting started to make sure I'm not committing to the impossible.\n\nI say modified because I want to identify more than the location and bounding box of objects. Is it possible to add additional variables to the anchor vectors that YOLO learns, that way it could learn additional properties of objects, lets say colour for the sake of argument.\n\nThere are quite alot of resources on training YOLO on custom data-sets but I haven't found anything to do with learning more than location and bounding boxes. Is there something fundamental about the way YOLO is implemented that makes the anchor vector inflexible? is there maybe a network better suited to my problem? \n\nAny guidance is appreciated\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mw6p4c/is_it_possible_to_augment_the_anchor_vectors_in_a/"}, {"autor": "xVortex93", "date": "2021-04-22 14:39:25", "content": "Where do I start with Convolutional Neural Network /!/ I am working on this project that requires me to build a neural network that processes RGB-d (depth) data and outputs location of the object. \n\nI have worked with CNNs before, that does classification. But how do I create a model that can output position (X,Y) within an -----> image !!! ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mw67x3/where_do_i_start_with_convolutional_neural_network/"}, {"autor": "gbernardon777", "date": "2021-04-22 11:32:19", "content": "Event detection for Bachelor Thesis /!/ Hi everyone!\n\nI started to dig a little into this world earlier this year and I decided to make my Mechatronics Bachelor Thesis about it! But since I'm still a noob, I needed some guidance on a few things if anyone could help.\n\nMy thesis basically consists of building and coding a device which is able to detect whether our robot in the lab unwraps plastic pipettes sucessfully. My initial idea is to use a Raspberry Pi coupled with some sort of -----> camera !!! . But I can only find this setup being used for object detection, and since I'm still very new to this world, I have no clue if that setup can be used for my project too.\n\nSo, is there an area of machine learning dedicated to detect specific types of events? If so, where can I learn more about it?\n\nThank you in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mw2pxf/event_detection_for_bachelor_thesis/"}, {"autor": "imanseau", "date": "2021-04-21 23:25:15", "content": "Recommendations for a workstation to work on machine learning, TensorFlow, NEAT, etc? /!/  \n\nI am a BFA (Bachelors of Fine Arts) in Sculpture. My work involves merging sculpture with AI. So far I have made use of TensorFlow &amp; NEAT programmed in Python running on a Raspberry Pi 4. I do plan on pursuing this beyond those few areas. I would like to build my own -----> image !!!  recognition libraries. I'd like to be able to run one of my programs on the workstation to push through early generations then export to the raspberry pi, so the art piece is not starting at 0. All my work has been self-taught, so my understanding is limited but growing.\n\nI am in a program that covers my expenses at school to include purchasing a computer. I don't want to take advantage and request the latest and greatest system and abuse the assistance. So I am looking for recommendations for a good system, not the bare minimum, but also not the best. If I provide a list of requirements they will get a desktop that fits my needs. The normal price range that the program covers is between $2,000 to 2,500, they normally provide a laptop Win or Mac. It can be more if needed and justified.\n\nThanks to all for any help!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mvs7uk/recommendations_for_a_workstation_to_work_on/"}, {"autor": "KhanDescending123", "date": "2021-09-15 21:30:49", "content": "How to create an -----> image !!!  landmark detection dataset? /!/ I have a custom problem where I need to perform landmark detection on image data. This is similar to how landmark detection for faces or hands work. Are there any tools that would enable me to annotate a dataset with this information? \n\nI've looked at label box, labelImg, etc. but most allow bounding box drawings or image masks and not just landmarks.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pozgqn/how_to_create_an_image_landmark_detection_dataset/"}, {"autor": "SalvosMachina", "date": "2021-09-15 10:09:08", "content": "Pytorch Error Question /!/ \"real\" is a mini-batch and the goal is to make a copy of that batch called \"adv\\_ex\" and add small amounts of noise (gen(item)) to each -----> image !!!  in adv\\_ex, however, it throws an error that indicates that I'm using a view incorrectly.  Any help interpreting this error?  Thanks\n\n[chunk causing error](https://preview.redd.it/xd474cvq3nn71.png?width=560&amp;format=png&amp;auto=webp&amp;s=091275fafea869626fb309795c09500c864b9e95)\n\n    ---------------------------------------------------------------------------\n    RuntimeError                              Traceback (most recent call last)\n    /tmp/ipykernel_24355/3111319459.py in &lt;module&gt;\n         14         for idx,item in enumerate(adv_ex):\n         15             #item.size() = 784\n    ---&gt; 16             purturbation = gen(item)\n         17             adv_ex[idx] = adv_ex[idx] + purturbation #item.size() = 784\n         18 \n    \n                                         ....\n    \n    RuntimeError: A view was created in no_grad mode and its base or another view of its base has been modified inplace with grad mode enabled. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.\n\nAnd this is the resulting error.  If any more info is needed lmk", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pon79l/pytorch_error_question/"}, {"autor": "Successful_Boat_3099", "date": "2021-09-15 06:26:02", "content": "\ud83d\udc8aYour daily dose of machine learning : training deep learning models on google cloud /!/ &gt; This is a series of posts that I post almost daily. I call them \u201cyour daily dose of machine learning\u201d. \n\nTraining deep learning models on google cloud can be done in several ways. But the most standard approach is the following :\n\n\\- You create the necessary code for training your model.  \n\\- You containerize your code using Docker.  \n\\- You push your docker -----> image !!!  to google container registry (GCR).  \n\\- You run your docker image from google AI platform or google Vertex AI.\n\nThis approach works with all frameworks such as Tensorflow or PyTorch. \n\n Follow me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!***", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pokltf/your_daily_dose_of_machine_learning_training_deep/"}, {"autor": "DelusionalHuman", "date": "2021-09-15 03:07:57", "content": "-----> Image !!!  Editing Project /!/ Reposting as my original post did not contain the body I originally wrote.\n\n&amp;#x200B;\n\nTo start off I have zero code written down but know how to code and want guidance before I start. \n\nI am posting below 2 images. The first image is an image of an empty room. The second image is the empty room now with decor and furniture arranged. \n\n[https://imgur.com/a5LDgEn](https://imgur.com/a5LDgEn)\n\nI want to create a project in which I can train a network to detect an empty room and then apply another image to the empty room. The image I want to add is a simple painting or a rug on the floor.  The empty room image will be provided and I can provide multiple paintings and the Algo should pick a painting and then transform/manipulate it and then add the image and place it on a wall strategically. As time goes on I would like to see if I can add a rug or other furniture pieces. \n\nQuestions I would like answered-\n\n1. Is this possible to even accomplish with current tech and training methods in Ml. Possible bottlenecks with datasets and libraries that are not well equipped for image manipulation. As I see hundreds of image classification projects but not much related to what I need which is more image manipulation.\n2. I\u2019d like to get an idea of what type of learning models and algorithms I should look into.\n3. I would like to be pointed to any similar projects in which an image is edited and in which another image is not only added but also transformed before being placed in.\n4. I would also like an idea if this is something out of scope for a one man team who is not a rockstar programmer but probably a below average programmer.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pohtb9/image_editing_project/"}, {"autor": "DelusionalHuman", "date": "2021-09-15 00:56:34", "content": "-----> Image !!!  editing Project Help", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pofon8/image_editing_project_help/"}, {"autor": "moximotel", "date": "2021-09-21 23:21:46", "content": "StyleGAN question: What's the best way to work with a small data set? /!/ I have a project where I only want to feed 9 images through styleGAN. If I make many copies off each -----> image !!! , would that help? I'm hoping there is a way I don't need to use hundreds of images to getgood results. I appreciate any advice you can give!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/psuruo/stylegan_question_whats_the_best_way_to_work_with/"}, {"autor": "Gandalf_The_Wise8400", "date": "2021-09-21 14:59:14", "content": "How to determine -----> image !!!  resolution (input size) for -----> image !!!  classification /!/ Hey all, is there a paper that has answered this question for a general rule of thumb.  \n\n\nAlso how about increasing the resolution but making the images grayscale?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pskh03/how_to_determine_image_resolution_input_size_for/"}, {"autor": "ohC90aVusuJ9biexQuah", "date": "2021-09-21 12:25:18", "content": "what sort of -----> image !!!  processing/ML is needed to solve this regression problem?? /!/ I'm generally unpracticed with regards to ML. I have a large data set that [resembles this picture](https://ibb.co/H7qLp0s). The model requirements are to figure out what time the clock says it is. The training can be supervised (labeled samples) or unsupervised. Any advice or resources you can point me towards for this kind of issue? Thank you! =)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pshntk/what_sort_of_image_processingml_is_needed_to/"}, {"autor": "Untinted", "date": "2021-09-20 19:46:24", "content": "Project to prettify music notes /!/ I'm interested in trying to do something with neural networks, and thought that making a neural network that can take a -----> picture !!!  of music notes taken with a camera and turn them into a nicely edited version would be an interesting project.\n\nI followed a tutorial to do a pix2pix GAN network here: [https://www.youtube.com/watch?v=SuddDSqGRzg](https://www.youtube.com/watch?v=SuddDSqGRzg), and [the github](https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/GANs/Pix2Pix).\n\nAnd I have a few pages of music notes that are around 2480 x 3508, but I crop a random 512x512 square to learn on, here's an example of input and target:\n\n[https://ibb.co/xf9hRpw](https://ibb.co/xf9hRpw)\n\n[https://ibb.co/Dg5gRVM](https://ibb.co/Dg5gRVM)\n\nI've done around 1000 epochs, and I'm mostly getting just a white background.. so it's not going too good.  Possibly the convolution isn't working properly, or the loss function is misbehaving (or it just needs a lot more epochs).  I'll probably try to use the dataset referenced in the guide, given that my custom one isn't working properly.\n\nI'd love to ask into the void:\n\n* Would you use Pix2Pix or something else?\n* does anyone know if a pix2pix GAN should be able to learn to unwarp lines?\n\nI'm hoping I'll learn something from this experience.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ps28ze/project_to_prettify_music_notes/"}, {"autor": "ralampay", "date": "2021-09-20 14:16:52", "content": "Computation for Setup Constraints of CNN Autoencoder /!/ I'm trying to implement a CNN Autoencoder with `pytorch` and try different parameters. for various -----> image !!!  sizes. Can anyone share a simple formula to compute for invalid setup of a CNN Autoencoder? \n\nFor example, if I expect to input an image that is 500 x 500 in size with 3 input channels, how can I compute if the following values are invalid?\n\n* `kernel_size`\n* `padding`\n* `max pool 2d`\n\nFor instance, I cannot use a layer with 16 of mapped features for the next layer if my kernel size is 5x5 with a padding of 1. \n\nIs there a closed formula to determine the constraints?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/prvswm/computation_for_setup_constraints_of_cnn/"}, {"autor": "AloneNefariousness62", "date": "2021-09-01 20:08:09", "content": "Image Classification Pipeline /!/ Took part in the hackathon this past weekend where we got nearly 100% accuracy on -----> image !!!  classification. We used data augmentation in Albumentations, OpenCV and EfficientNet. Here is the write-up [https://dspyt.com/simple-image-classification-with-efficientnet/](https://dspyt.com/simple-image-classification-with-efficientnet/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pg1xxf/image_classification_pipeline/"}, {"autor": "JonnieSingh", "date": "2021-09-01 17:29:53", "content": "Why am I getting this Google ML library coordinate misalignment? /!/ I have used Google ML library to develop a mobile app (for **Android**, using **Java**) that focuses on detecting objects using [this Google ML documentation](https://developers.google.com/ml-kit/vision/object-detection/custom-models/android) titled \"*Detect, track and classify objects with a custom classification model on Android*\". The custom model in mind that I have used is [this TensorFlow Lite model](https://tfhub.dev/tensorflow/lite-model/mobilenet_v1_0.75_192_quantized/1/metadata/1). I've since completed the application and been able to launch it on my Android device. The object detection works. The live stream -----> camera !!!  preview that I'm using to detect objects is CameraX, in which [this](https://developer.android.com/training/-----> camera !!! x/preview) was the documentation that was used to implement that into the application.\n\nHowever, I keep running into this problem of the `Rect` box in my display that never seems to quite capture the object. [**THIS**](https://preview.redd.it/c2hykssr4yh71.jpg?width=789&amp;format=pjpg&amp;auto=webp&amp;s=f02d82176a5b07eb0c310803c6cfe1cab1535569) is how the user interface would look. As you can see, it's always off by the same exact metric, no matter where the object is on the screen.\n\nHere's my DrawGraphic class that draws the `Rect` on the UI:\n\n    public class DrawGraphic extends View {\n    \n        Paint borderPaint, textPaint;\n        Rect rect;\n        String text;\n    \n    \n        public DrawGraphic(Context context, Rect rect, String text) {\n            super(context);\n            this.rect = rect;\n            this.text = text;\n    \n            borderPaint = new Paint();\n            borderPaint.setColor(Color.WHITE);\n            borderPaint.setStrokeWidth(10f);\n            borderPaint.setStyle(Paint.Style.STROKE);\n    \n            textPaint = new Paint();\n            textPaint.setColor(Color.WHITE);\n            textPaint.setStrokeWidth(50f);\n            textPaint.setTextSize(32f);\n            textPaint.setStyle(Paint.Style.FILL);\n        }\n    \n        @Override\n        protected void onDraw(Canvas canvas) {\n            super.onDraw(canvas);\n            canvas.drawText(text, rect.centerX(), rect.centerY(), textPaint);\n            canvas.drawRect(rect.left, rect.top, rect.right, rect.bottom, borderPaint);\n        }\n    }\n\nTo avoid cramming this post, [here](https://pastebin.com/G3Tya48E) is a link to my Pastebin that has all the code with my MainActivity in it, including the code needed to call this class. **Any further information needed to supplement this question will be provided upon request!**", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pfyq9r/why_am_i_getting_this_google_ml_library/"}, {"autor": "samtutu", "date": "2021-09-01 14:24:01", "content": "How to improve my FN and recall on my confusion matrix? I'm working on a binary -----> image !!!  Classification problem using CNN and my data is evenly balanced (7200 -----> image !!! s per training folder and 1800 in each test folder)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pfuy5w/how_to_improve_my_fn_and_recall_on_my_confusion/"}, {"autor": "speedy0wl", "date": "2021-09-01 14:08:32", "content": "LabelFlow is live! The open -----> image !!!  annotation and dataset cleaning platform", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pfunak/labelflow_is_live_the_open_image_annotation_and/"}, {"autor": "TheExclusiveNig", "date": "2021-02-19 11:31:22", "content": "Help with Torchserve /!/ Anyone here well versed with Torchserve, it requires a [model.py](https://model.py/) file describing the model's architecture. But I'm using an -----> image !!!  segmentation model, which gives out an architecture in a super weird way. Is there anyone I can automate this process? or anyone I can manually make this file?\n\nI'd appreciate any suggestions or help regarding this.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lndr2n/help_with_torchserve/"}, {"autor": "usernamedregs", "date": "2021-02-19 03:52:18", "content": "Advice on whether to learn ML to solve problem vs programming it the old way? /!/  So  I need to implement Visual Inertial Odometry \\[dead reckoning using  phone IMU + camera\\] on an Android device for a new project and while  there are a number of papers on the topic in the computer vision field  that I can refer to (it is more or less a solved problem for my limited  accuracy requirements) - the implementation is going to take some doing  on my part.\n\nConsidering how new I am to ML the alternative is going to take some doing on my part also!\n\nAs  an ML solution I would be feeding in IMU data  + -----> camera !!!  frames and then  using an attached (physically taped together) motion controller for  training telemtry; major concern though would be the specifics of the  phone (-----> camera !!!  model, relative position of IMU and -----> camera !!!  inside the  phone), and the environment (lighting conditions and surroundings) so at  least some (or all) of the training would need to be done on the end  users device.\n\nQuestions:\n\n1. How  much training (on the machines side - my own is a different story)  would be required to achieve a reasonable solution, and how much  of  that could be baked in to a generalized solution to shorten the training  done with the end users device?\n2. Would  a generalized solution (hardware and environment agnostic) be  achievable at all or should I be expecting to require additional  training on end users device?\n3. During  normal usage how intensive in terms of CPU and memory would the ML  solution compared to the human tailored VIO solution \\[Kalman filter +  optical flow or similar\\] be in relative terms?\n4. On  the scale of things: 0 being a solution to Tic-tac-toe and 10 being   AlphaGo! how hard would this ML solution be to implement (deciding  whether to invest my time in learning ML or computer vision)?\n\n1 comment", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ln5yk8/advice_on_whether_to_learn_ml_to_solve_problem_vs/"}, {"autor": "codinglikemad", "date": "2021-02-18 22:06:48", "content": "[P] I made an AI to play minesweeper - it's surprisingly hard! I published a couple videos about it, feedback appreciated /!/ I finished up a minesweeper AI that uses an -----> image !!!  to -----> image !!!  CNN. Performance in terms of games won substantially beats an average person. I made a couple of videos about it: high level overview for general public is [https://www.youtube.com/watch?v=lN-Pq1GoIO0](https://www.youtube.com/watch?v=lN-Pq1GoIO0) while an in depth discussion of network architecture is in an unlisted video linked in the description, here for your perusal( [https://www.youtube.com/watch?v=hyOVwwp4qu4](https://www.youtube.com/watch?v=hyOVwwp4qu4&amp;t=0s) \\- but it assumes you watched the other one first). I'm really curious if people have ideas for how to do this differently, I learned a ton doing this project, and this is my first attempt to do this kind of video so feedback is very much appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lmyoxr/p_i_made_an_ai_to_play_minesweeper_its/"}, {"autor": "bigboisexyboi", "date": "2021-02-18 20:42:25", "content": "Autoencoder \"ignoring\" neurons in the hidden layer /!/ Hey everyone,\nI'm building a convolutional autoencoder network for -----> image !!!  compression. In between the encoder and decoder portions of this network, the data is compressed into a single dense layer. In order to check how well this network was fitting and generalising, I measured the mean and standard deviation of the output of the neurons on this layer across the entire training set, and found a strange result: many of the neurons have a mean of 0 and standard deviation of 0. For every single image in the entire training set, these neurons just don't do anything. I'm a bit confused as to why this happens/how I can \"force\" the network to use all of its hidden neurons. Any advice?\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/lmwofv/autoencoder_ignoring_neurons_in_the_hidden_layer/"}, {"autor": "Mjjjokes", "date": "2021-04-02 06:47:22", "content": "I was told my prospective -----> image !!!  classification training set was too small (2k -----> image !!! s). What is a large enough training set? /!/ I will be scraping a site to acquire the extra images if needed. If you are curious, the dataset I currently have are face shots of politicians. That's all I'll be elaborating on for now.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/miesgc/i_was_told_my_prospective_image_classification/"}, {"autor": "sevenspicyclouds", "date": "2021-04-02 06:26:13", "content": "-----> Image !!!  preprocessing software for a deep learning model /!/ Is there some kind of large-batch image processing software that can crop out text, make greyscale, and crop into square images?\n\nIn essence, I'm making a usable dataset of uniform images from raw ultrasound images I collected from a hospital, and the images are in a variety of formats. Some images have two ultrasounds in them, some only have one. I need to make the shape of each image uniform, into a square that focuses on the ultrasound. Most images have extra unnecessary text that needs to be cropped out, because I only want the actual ultrasound part of the images to be processed by the deep learning model. Some images also have color, which I need to change to greyscale. There are around 1000 images in the dataset, and I don't have the time to manually crop and recolor every single image.\n\nPlease provide any suggestions that can quicken the process at least a little bit! Thank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/mieios/image_preprocessing_software_for_a_deep_learning/"}, {"autor": "Edulad", "date": "2021-10-21 07:49:45", "content": "Opencv Canny edge detection Help /!/ Hi, so i can perform the canny edge detection of my -----> image !!! , but how can i save the detected part only in color in png format. here is my code. \n\n`import cv2` \n\n`import os`\n\n`import numpy as np`\n\n`import matplotlib.pyplot as plt`\n\n&amp;#x200B;\n\n`img1 = cv2.imread(\"25.jpg\",0)`\n\n&amp;#x200B;\n\n`edges1 = cv2.Canny(img1, threshold1=30, threshold2=100)`\n\n&amp;#x200B;\n\n`plt.imshow(edges1, cmap = \"gray\")`\n\n[`plt.show`](https://plt.show)`()`\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/vv83n78qdru71.png?width=302&amp;format=png&amp;auto=webp&amp;s=5a6a6081dbd0e9c93d587388b75d817e9c24e99f", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qcmmec/opencv_canny_edge_detection_help/"}, {"autor": "agoramancy", "date": "2021-10-21 03:20:08", "content": "Link grammars, symbolic representations, and about using similarities and substitutions algorithms. /!/ This hypothesis is highly related to the presentations given by Linas Vepstas first and then Anton Kolonin at SingularityNet AGI-21 Conference about, link grammars, symbolic representations, and about using similarities and substitutions algorithms.\n\nIn: [https://www.youtube.com/watch?v=Rcgydm9dlYg](https://www.youtube.com/watch?v=Rcgydm9dlYg)\n\nYou need to see the first two presentations to understand what I'm trying to comunicate here.\n\nI\u2019ll try to be very concise. In short my statement is that with similarities and substitutions you can have solutions of previously unsolved problems. In other words, generate new knowledge using previous related knowledge.\n\nI apologize in advance for my lack of proper technical terms. But, if the idea get through, that will be enough for me. I hope my explanation is understandable.\n\nThe following is a graph composed by sub-graphs, the nodes are linked by the equal \u2018=\u2019 relation, which goes in both directions in case there is no more links, by default is left to right. The nodes can be single values or sets.\n\nFor briefness and explain-ability I will use this notation instead of an actual graph (-----> image !!! ).\n\nAbout my notations:\u00a0\n\n* \\[v1, v2\\] is a set,\n* '=' is the relation,\n* Every line can be seen as a sub-graph.\n* I will use // to add a comment for explanation\n* A \u2018query\u2019 is a new node that has no similarity relation in the graph and therefore the algorithm needs to be used\n\n**The initial state of the graph and a case of a query and the result:**\n\n    Alice = programmer\n    Bob = engineer\n    Mary = designer\n    John = programmer\n    Rose = engineer\n    Joe = designer\n    [engineer, job] = [model, system]\n    [programmer, job] = [develop, system]\n    [designer, job] = [design, UI]\n    Alice = available\n    Bob = available\n    Mary = available\n    Alice = [available, programmer]\n    Bob = [available, engineer]\n    Mary = [available, designer]\n    // initially available, but later relations are updated to working\n    John = working \n    Rose = working\n    Joe = working\n    R1 = name\n    R2 = name\n    solution = [\n        [[available, engineer], [engineer, job]], \n        [[available, programmer], [programmer, job]], \n        [[available, designer], [designer, job]]\n    ]\n    [new, request] = [unsolved, request]\n    [request, name] = request\n    [solution, name] = solution\n    [resolve, request] = work\n    work = [solution, for, request]\n    // if we have a [unsolved, request] we want to [resolve, request]\n    // this is a simplification. It could have more meaning if we add more \n    // details to this relation\n    [unsolved, request] = [resolve, request] \n    // the query is [Medical, report, system]\n    // the query is linked to be equal to a [new, request]\n    [new, request] = [Medical, report, system]\n    // generated [request, R1] to identify the query \n    [Medical, report, system] = [request, R1] \n    // given that [unsolved, request] = [new, request]\n    [request, R1] = [unsolved, request] \n    [request, R1] = [request, name] // given that R1 = name\n    [request, R1] = request\n    [request, R1] = [resolve, request] // given that: [unsolved,request]=[request,R1]\n    [resolve, [request, R1]] = work // given that: request = [request, R1]\n    // here [solution, R1] can be generated by a stored procedure for simplicity, \n    // and in this case is used to\n    // identify the node that will be the actual solution\n    // given that: work = [resolve, request], and work = [solution, for, request]\n    [solution, for, [request, R1]] = [solution, R1] \n    [solution, R1] = [solution, name] \n    [solution, R1] = solution\n    // final state, here the replacement for the node \u2018solution\u2019 is \n    // used to produce the final relation\n    [solution, R1] = [\n      [Rose, [model, system]], \n      [John, [develop, system]], \n      [Joe, [design, UI]]\n    ]\n    // For the next part, to do a new query I will do the following to avoid ambiguity\n    // this can be resolve in multiple ways, in this case I\u2019ll go with this\n    [request, R1] != [new, request] \n\n**At this point I will do a new query to illustrate how with substitutions we can generate a new knowledge, given the previous state of graph.**\n\n    [new, request] = [Hotel, management, system] // new query is [Hotel, management, system]\n    [Hotel, management, system] = [request, R2] // generate a identification node\n    [request, R2] = [request, name] // given that: R2 = name\n    [request, name] = [request, R1] // we have this relation, then\n    [request, R2] = [request, R1] // therefore\n    [request, R2] = [solution, R1] // is the current most similar, but\n    [request, R2] is not similar enough to [solution, R1] because:\n    // If we take into account the full sequence of substitutions we will notice \n    // that [Hotel,management,system] is not equal to [Medical,report,system]\n    // We can do better, if we use 'solution' instead of [solution, R1]:\n    \n    [solution, R1] = solution\n    [request, R2] = solution // given the previous relation\n    [request, R2] = [\n        [[available, engineer], [engineer, job]], \n        [[available, programmer], [programmer, job]], \n        [[available, designer], [designer, job]]\n    ]\n    [request, R2] = [\n        [Bob, [model, system]], \n        [Alice, [develop, system]], \n        [Mary, [design, UI]]\n    ]\n    // final output\n    [solution, R2] = [\n        [Bob, [model, system]], \n        [Alice, [develop, system]], \n        [Mary, [design, UI]]\n    ]\n\nThis example is not really that interesting, and looking at the result it seems pretty obvious. But the key here is that is generalizable, and is just substitutions.\n\nThis substitution mechanism can be applied to multiple cases to solve any kind of situations, given that it has enough previous knowledge. Is like applying a formula, step by step. Every step is guided by a previously known relation. This mechanism should be the algorithm applied directly to the graph, so that is the process with which the graph change from one state to the next state.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qcib6q/link_grammars_symbolic_representations_and_about/"}, {"autor": "StrugglingBScientist", "date": "2021-10-20 16:11:31", "content": "Reconstructing a 3D -----> image !!!  with ImageJ/Mopholibj", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qc4f8h/reconstructing_a_3d_image_with_imagejmopholibj/"}, {"autor": "Futurekevin", "date": "2021-10-20 08:50:30", "content": "[ObjDet] Are anchor sizes related to the original -----> image !!!  size or the resized -----> image !!! ? /!/ I'm trying to optimise my anchors for a dataset where my targets are pretty small. I understand what anchors do, but in terms of optimisation and checking coverage should I be looking at the 'raw' object size (e.g. 32x32 in a 1920*1080 image) or the resized object size (e.g. ~9x15 when resized to 512x512.) I think it should be the resized object, but I can't find confirmation of this anywhere.\n\nI'm looking at using [this repo](https://github.com/martinzlocha/anchor-optimization) to optimise my anchors automatically; from the default arguments there, it looks like the images are resized first to between 800 and 1333px (the input size for keras-retinanet). I assume in my use case I want this to be 512x512 instead?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qbwfmv/objdet_are_anchor_sizes_related_to_the_original/"}, {"autor": "DineshPiyasamara", "date": "2021-10-20 03:12:50", "content": "Transfer Learning for Images /!/  In [Deep Learning](https://deepblade.com/category/artificial-intelligence/deep-learning/) we can use Convolutional Neural Networks for computer vision tasks. When we work with a large dataset at that time, it can take a long time for that model to train. It can take days or even weeks.\n\nSo, in such cases, we can use [Transfer learning](https://deepblade.com/artificial-intelligence/deep-learning/how-does-transfer-learning-work/). There we use a pre-trained model. At the moment there are many popular pre-trained models that we can use for computer vision tasks. VGG16. VGG19, Resnet 50 are some of them.\n\nIn this tutorial, you will learn,\n\n* [What is Transfer Learning?](https://deepblade.com/artificial-intelligence/deep-learning/transfer-learning-for-images/#1)\n* [Popular pre-trained models used in Computer Vision](https://deepblade.com/artificial-intelligence/deep-learning/transfer-learning-for-images/#2)\n* [Transfer Learning practical implementation](https://deepblade.com/artificial-intelligence/deep-learning/transfer-learning-for-images/#3)\n\n## \n\n## What is Transfer Learning?\n\nIn Deep Learning we use Neural Networks for everything. In [Transfer Learning,](https://deepblade.com/artificial-intelligence/deep-learning/how-does-transfer-learning-work/) the weights of a Neural Network created for a particular problem are used for another such problem. Mostly, computer vision problems require high computational costs. Therefore, Transfer Learning is a very important and popular technique for Computer Vision tasks.\n\n**Note:** Read this article to know how Transfer Learning works and the pros and cons of Transfer Learning.\n\n&gt;[How does Transfer Learning work?](https://deepblade.com/artificial-intelligence/deep-learning/how-does-transfer-learning-work/)\n\n## \n\n## Popular pre-trained models used in Computer Vision\n\nThere are many pre-models that we can use for Computer Vision tasks. Below are some of the popular pre-trained models.\n\n### VGG-16\n\nK. Simonyan and A. Zisserman introduce the VGG-16 model from the University of Oxford. This model achieves up to 92% test accuracy in the ImageNet dataset. [**ImageNet**](https://www.image-net.org/) is a popular dataset that has more than 14 million images belonging to 1000 classes. This pre-trained model has 13 Convolutional layers and 3 fully connected layers. Accordingly, it has 16 layers.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/1oh6vhyluiu71.png?width=935&amp;format=png&amp;auto=webp&amp;s=9a25c0f075dc621174f65ee97817faa175eb943d\n\nVGG-16 model can load like below,\n\n`from tensorflow.keras.applications.vgg16 import VGG16`  \n`model = VGG16()`\n\n### VGG-19\n\nThis is a model with 19 layers (16 Convolutional Layers and 3 fully connected layers). Also contains 5 max-pool layers and a softmax layer. \u00a0\n\n&amp;#x200B;\n\nhttps://preview.redd.it/z3vh8rwnuiu71.png?width=1049&amp;format=png&amp;auto=webp&amp;s=fd7ca7377466c471cf6cbe83f08377644045cf5e\n\nVGG-19 model can load like below,\n\n`from tensorflow.keras.applications.vgg19 import VGG19`  \n`model = VGG19()`\n\n### ResNet-50\n\nThis model introduces by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jiasun in 2015. It contains 50 convolutional layers, 1 max-pool layer, and an average-pool layer. And ResNet 50 has more than 23 million trainable parameters. ResNet architecture can be used for such tasks as -----> image !!!  classification, object detection, and object localization.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/q1mu3uopuiu71.png?width=450&amp;format=png&amp;auto=webp&amp;s=da5280ddc3afe05de710e6515c7fb9dc61f6ef41\n\nResNet-50 model can load like below,\n\n`from tensorflow.keras.applications.resnet50 import ResNet50`  \n`model = ResNet50()`\n\n### Inception V3\n\nThis is a deep model that contains 42 layers. Although the number of layers is higher, the complexity here is the same as the complexity of a VGG Net.\n\nInception V3 model can load like below,\n\n`from keras.applications.inception_v3 import InceptionV3`  \n`model = InceptionV3()`\n\n### Xception\n\nThis model introduces by Francois Chollet. It is an extension of the Inception architecture. The difference there is involved **Depthwise Separable convolutions**.\n\nXception model can load like below,\n\n`from tensorflow.keras.applications.xception import Xception`  \n`model = Xception()`\n\n### MobileNet\n\nMobileNet model used for mobile applications. It uses **depthwise separable convolutions**. Therefore it has a low number of parameters when compared to a regular model with the same depth. MobileNet model can run efficiently on mobile devices with [**TensorFlow lite**](https://www.tensorflow.org/lite).\n\nMobileNet model can load like below,\n\n`from tensorflow.keras.applications.mobilenet import MobileNet`  \n`x = MobileNet()`  \n\n\n## Transfer Learning practice Implementation\n\nI will use here **Inception V3** pre-trained model and the [Dogs\\_vs\\_Cats](https://www.kaggle.com/c/dogs-vs-cats/data) dataset as an example. [Click here](https://www.kaggle.com/c/dogs-vs-cats/data) to download the dataset from the Kaggle website. So, using this dataset we are creating a model that can identify whether an image is of a dog or a cat.\n\n`from tensorflow.keras import layers, Model`  \n\n\n`# load model`  \n`from keras.applications.inception_v3 import InceptionV3`  \n`pre_trained_model = InceptionV3(input_shape=(150, 150, 3),`  \n`include_top=False,`  \n`weights = 'imagenet')`  \n\n\n**Note:** The inception\u00a0model has a fully connected layer at the top. That\u2019s why we use \u201c**include\\_top=False**\u201d to ignore it.\n\n`# summary of model`  \n`print(pre_trained_model.summary())`\n\n&amp;#x200B;\n\n`Model: \"inception_v3\" __________________________________________________________________________________________________ Layer (type)                    Output Shape         Param #     Connected to                      ================================================================================================== input_1 (InputLayer)            [(None, 150, 150, 3) 0                                             __________________________________________________________________________________________________ conv2d (Conv2D)                 (None, 74, 74, 32)   864         input_1[0][0]                     __________________________________________________________________________________________________ batch_normalization (BatchNorma (None, 74, 74, 32)   96          conv2d[0][0]                      __________________________________________________________________________________________________ activation (Activation)         (None, 74, 74, 32)   0           batch_normalization[0][0]         __________________________________________________________________________________________________ conv2d_1 (Conv2D)               (None, 72, 72, 32)   9216        activation[0][0]                  __________________________________________________________________________________________________`  \n`.`  \n`.`  \n`.`  \n`activation_85 (Activation)      (None, 3, 3, 320)    0           batch_normalization_85[0][0]      __________________________________________________________________________________________________ mixed9_1 (Concatenate)          (None, 3, 3, 768)    0           activation_87[0][0]                                                                                activation_88[0][0]               __________________________________________________________________________________________________ concatenate_1 (Concatenate)     (None, 3, 3, 768)    0           activation_91[0][0]                                                                                activation_92[0][0]               __________________________________________________________________________________________________ activation_93 (Activation)      (None, 3, 3, 192)    0           batch_normalization_93[0][0]      __________________________________________________________________________________________________ mixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_85[0][0]                                                                                mixed9_1[0][0]                                                                                     concatenate_1[0][0]                                                                              activation_93[0][0]               ================================================================================================== Total params: 21,802,784 Trainable params: 21,768,352 Non-trainable params: 34,432 __________________________________________________________________________________________________`\n\n&amp;#x200B;\n\n`# lock all layers in initiated pre-trained model`  \n`for layer in pre_trained_model.layers:`  \n`layer.trainable = False`  \n\n\nWe need to connect one layer from the above pre-trained model to our DNN. For that, I use the **\u2018mixed9\u2019** layer as the last layer from the pre-trained model. You can experiment with other layers from the pre-trained model. You can see all names corresponding to layers using **\u201cpre\\_trained\\_model.summary()\u201d.**\n\n`last_layer = pre_trained_model.get_layer('mixed9')`  \n`last_output = last_layer.outputfrom tensorflow.keras.optimizers import Adam`  \n\n\n`# transform output layer to 1 dimension`  \n`x = layers.Flatten()(last_output)`  \n\n\n`# Add a fully connected layer with 1024 hidden units and ReLU activation`  \n`x = layers.Dense(1024, activation='relu')(x)`  \n\n\n`# Add a dropout rate`  \n`x = layers.Dropout(0.2)(x)`  \n\n\n`# Add a final sigmoid layer for classification`  \n`x = layers.Dense (1, activation='sigmoid')(x)`  \n\n\n`model = Model( pre_trained_model.input, x)`  \n\n\n`model.compile(optimizer = 'adam',`   \n`loss = 'binary_crossentropy',`   \n`metrics = ['accuracy'])`\n\nNow we can work with our dataset. Let\u2019s extract the downloaded zip file. It contains another two zip files. (train.zip, test1.zip)\n\n`import zipfile`  \n`with zipfile.ZipFile('dogs-vs-cats.zip', 'r') as z :`  \n`z.extractall()`\n\nAfter extracted the downloaded zip file, we can extract the train.zip file,\n\n`with zipfile.ZipFile('train.zip', 'r') as train_zip :`  \n`train_zip.extractall()`\n\n&amp;#x200B;\n\n`# categorize all images inside the folder using names of images.`  \n\n\n`import os`  \n`filenames = os.listdir(\"train\")`  \n`classes = []`  \n`for image in filenames :`  \n`category = image.split('.')[0]`  \n`if category == 'dog' :`  \n`classes.append(1)`  \n`else :`   \n`classes.append(0)`  \n\n\n`# Labeling each image with the name of the class`  \n\n\n`import pandas as pd`  \n`data = pd.DataFrame({'filename' : filenames, 'category' : classes})`  \n`data['category'] = data['category'].map({0 : 'cat', 1 : 'dog'})`  \n`print(data.sample(5))`  \n\n\n`OUTPUT:`  \n\n\n`filename category 12726   dog.1020.jpg      dog 2707   cat.12433.jpg      cat 22209   dog.7487.jpg      dog 2803    cat.1270.jpg      cat 24424   dog.9480.jpg      dog# Divide dataset into training set and testing set`  \n\n\n`from sklearn.model_selection import train_test_split`  \n`train_data, validation_data = train_test_split(data, test_size=0.2)`  \n\n\n`train_data = train_data.reset_index(drop=True)`  \n`validation_data = validation_data.reset_index(drop=True)`\n\n[Click here](https://deepblade.com/artificial-intelligence/machine-learning/train_test_split-dataset-to-evaluate-machine-learning-algorithms/) to learn more about **train\\_test\\_split.**\n\n`from tensorflow.keras.preprocessing.image import ImageDataGenerator`  \n\n\n`# add data augmentation to ImageDataGenerator`  \n`train_datagen = ImageDataGenerator(rescale = 1./255.,`  \n`rotation_range = 60,`  \n`zoom_range = 0.2,`  \n`width_shift_range = 0.2,`  \n`height_shift_range = 0.2,`  \n`horizontal_flip = True)`  \n\n\n`train_generator = train_datagen.flow_from_dataframe(train_data,`  \n`'./train/',`  \n`x_col = 'filename',`  \n`y_col = 'category',`  \n`batch_size = 32,`  \n`class_mode = 'binary',`  \n`target_size = (150, 150))`  \n\n\n`OUTPUT:`  \n\n\n`Found 20000 validated image filenames belonging to 2 classes.validation_datagen = ImageDataGenerator(rescale = 1./255)`  \n\n\n`validation_generator = validation_datagen.flow_from_dataframe(validation_data,`  \n`'./train/',`  \n`x_col = 'filename',`  \n`y_col = 'category',`  \n`batch_size = 32,`  \n`class_mode='binary',`  \n`target_size = (150, 150))`  \n\n\n`OUTPUT:`  \n\n\n`Found 5000 validated image filenames belonging to 2 classes.# train the model`  \n`history = model.fit(`  \n`train_generator,`  \n`validation_data = validation_generator,`  \n`steps_per_epoch = 100,`  \n`epochs = 20,`  \n`validation_steps = 50,`  \n`verbose = 2)`  \n\n\n`OUTPUT:`  \n\n\n`Epoch 1/20 100/100 - 42s - loss: 0.3794 - accuracy: 0.8763 - val_loss: 0.0976 - val_accuracy: 0.9638 Epoch 2/20 100/100 - 35s - loss: 0.2060 - accuracy: 0.9159 - val_loss: 0.0885 - val_accuracy: 0.9600 Epoch 3/20 100/100 - 31s - loss: 0.1914 - accuracy: 0.9122 - val_loss: 0.0693 - val_accuracy: 0.9712 Epoch 4/20 100/100 - 30s - loss: 0.1683 - accuracy: 0.9272 - val_loss: 0.0673 - val_accuracy: 0.9744 Epoch 5/20 100/100 - 29s - loss: 0.1741 - accuracy: 0.9247 - val_loss: 0.0665 - val_accuracy: 0.9756 .`  \n`.`  \n`.`  \n`Epoch 18/20 100/100 - 24s - loss: 0.1457 - accuracy: 0.9369 - val_loss: 0.0700 - val_accuracy: 0.9712 Epoch 19/20 100/100 - 25s - loss: 0.1473 - accuracy: 0.9372 - val_loss: 0.0717 - val_accuracy: 0.9737 Epoch 20/20 100/100 - 25s - loss: 0.1473 - accuracy: 0.9394 - val_loss: 0.0612 - val_accuracy: 0.9762`\n\n&amp;#x200B;\n\nUsing the above values, we can graphically see how the accuracy of the training set and the validation set varied.\n\n`import matplotlib.pyplot as plt`  \n`accuracy = history.history['accuracy']`  \n`validation_accuracy = history.history['val_accuracy']`  \n\n\n`epochs = range(len(accuracy))`  \n\n\n`plt.plot(epochs, accuracy, 'r', label='Training accuracy')`  \n`plt.plot(epochs, validation_accuracy, 'b', label='Validation accuracy')`  \n`plt.title('Training and validation accuracy')`  \n`plt.legend(loc=0)`  \n`plt.figure()`  \n\n\n[`plt.show`](https://plt.show)`()`\n\n&amp;#x200B;\n\nhttps://preview.redd.it/90zhkhzdviu71.png?width=386&amp;format=png&amp;auto=webp&amp;s=09759670f722c89d790d7b9dd77e45970872b4e6\n\n&amp;#x200B;\n\nIn this example, we have seen how to do [Transfer Learning](https://deepblade.com/artificial-intelligence/deep-learning/how-does-transfer-learning-work/) using the **Inception V3** model. You can also use other pre-trained models I have mentioned earlier. Transfer Learning is a very important technique when working with large datasets. This allows us to train a model very quickly and you can see that the accuracy is very good.\n\nLearn more:\n\n**Tutorials:**\n\n[Machine Learning](https://deepblade.com/category/artificial-intelligence/machine-learning/)\n\n[Deep Learning](https://deepblade.com/category/artificial-intelligence/deep-learning/)\n\n[Natural Language Processing](https://deepblade.com/category/artificial-intelligence/natural-language-processing/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/qbror6/transfer_learning_for_images/"}, {"autor": "Chilaquil420", "date": "2021-05-22 16:50:43", "content": "Binary classification images but with only ONE class, and the output would be YES or NO depending if a specific type of object (a taco) is in the pic. How? /!/ Not about classify taco clases, more like detect if it is a -----> picture !!!  of a taco or not", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nimsgz/binary_classification_images_but_with_only_one/"}, {"autor": "nhokthanh99mc", "date": "2021-05-22 12:32:50", "content": "What are the popular -----> image !!!  classification models? /!/ I'm doing a project on detecting if a person is wearing a mask or not. I've seen some tutorials and usually, they use some models and fine-tuning it. I want a model that is fast, lightweight, and has high accuracy. I plan to train some models and then compare them together. So what models should I use?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nihn4r/what_are_the_popular_image_classification_models/"}, {"autor": "No-Mathematician4785", "date": "2021-05-22 12:21:49", "content": "Any tips for following content roadmaps? /!/ The issues I've had:\n\n* Learning lots of things that aren't useful for my goal\n* Motivation and overwhelm - how will I learn all this stuff?\n* A lack of overview as to how the thing I'm learning now fits into the bigger -----> picture !!! \n* No interactivity in most of the content meaning I struggle to stay engaged\n\nDoes anyone have any advice for this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/nihgde/any_tips_for_following_content_roadmaps/"}, {"autor": "all_is_love6667", "date": "2021-05-10 15:56:00", "content": "Are there available trained networks to download for MNIST/Imagenet that come without the -----> image !!!  data? /!/ I'm finally decided to use ML techniques to test things out, especially to search for text in images and label images\n\nI would like to begin by seeing how I can use tensorflow to label the content of images (spotting text will come later as it seems to be more complex).\n\nIs it possible to download trained networks instead of the whole dataset of ImageNet or MNIST? Are there such network datasets available to download?\n\nIf not, what is the simplest way I can use tensorflow (or any mainstream/popular framework) to just build a simple tools and test a collections of image I have?\n\nI just want to be able to do this: https://i.imgur.com/JnvA7or.jpg\n\nI'm fluent in python and C++.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n98i8s/are_there_available_trained_networks_to_download/"}, {"autor": "charlotteseux", "date": "2021-05-09 14:55:53", "content": "Fine Art &amp; ML /!/ Hey yall, im a first year Fine Art student at Central Saint Martins in London and I have been learning C++ and Python after learning some HTML and I have only been able to successfully run CycleGAN on Jupyter Notebook. \n\nAlthough I do really love coding, I am definitely approaching this subject from a very conceptual stand point. My practice involves running GANs for a day or two and painting a selected outcome in an attempt to \u201ccollaborate\u201d with my computer.\n\nI am really interested in computer vision and I want to be able to run StyleGAN one day but coming at this from a background that doesn\u2019t involve computer science makes it pretty hard to run the models I want to (i always run into errors that Im unable to fix from looking at answers on the internet)\n\nIf anyone here is or knows of artists or people  who work with computer vision that I could get in touch with in terms of helping me make a successful neural net for the purpose of -----> image !!!  generation please let me know &amp; thank you in advance &lt;3", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n8futg/fine_art_ml/"}, {"autor": "asi_takeover", "date": "2021-05-08 22:11:21", "content": "-----> image !!! /video super resolution? /!/ Can anyone recommend a good repository for -----> image !!! /video super resolution? I've found some but they seem to (a) come with a tiny training set, (b) not come with all/any pretrained models, (c) don't really come with advice about building your dataset and training. If it isn't too much to ask, I'd like to know what the best current solution with details about which model, how long you can expect to have to train, what kind of data and how much. My current resources are a GeForce RTX 3090 and 128 GB of RAM. I did inference with one model and the 24GB on my 3090 wasn't enough, but I managed to get it done with my RAM. But even with that I wasn't impressed by the results. I can use colab too, and if it's really necessary I could probably learn to do it on cloud. I'm curious about how all these remastered 4k videos I've seen on youtube were done. If you can give me any links to the good checkpoints, that would be very cool.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n7zs31/imagevideo_super_resolution/"}, {"autor": "alphaweekc", "date": "2021-05-08 19:31:41", "content": "Taking/saving a -----> picture !!!  in ml5? /!/ I am working on a p5.js game and plan on using the ml5 library to have the player take a snapshot of their face and then use the picture later on. However, I cant find any source that shows how you can pause the camera and save the actual image of the face detection. Does anyone know how to do this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n7wn1k/takingsaving_a_picture_in_ml5/"}, {"autor": "SuccMyStrangerThings", "date": "2021-05-08 11:58:06", "content": "Need help understanding the theoretical aspects of a neuron and inputs in CNN /!/ I have a few questions, usually the # of neurons in the i/p layer is H x W x channels i.e a neuron for every pixel in the -----> image !!! . \n\nSo, \n\n1) How is the input image passed to these i/p neurons? Is a single pixel passed to a single neuron? \n\n2) What does a neuron in a Conv2D layer represent? I'm having hard time visualising.\n\n3) Also, after applying Convolutional operation, Max Pooling, Activation, the next set of i/p to the conv layer may or may not have same no. of channels as previously, so how is the kernel's depth handled (No. Of channels must be equal to the depth of the kernel, right?). How is this managed", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n7npok/need_help_understanding_the_theoretical_aspects/"}, {"autor": "magicanon4", "date": "2021-05-08 06:45:50", "content": "[P] TimeDistributed layer in pre-trained models /!/ Hello,\n\nI am a noob in machine learning, I would like some help with using the TimeDistributed layer in pre-trained models like vgg16, etc.  I am trying to predict micro-expression on the casme 2 dataset. So I wanted to try out the transfer learning with using pre trained models like vgg. But vgg only takes single -----> image !!!  input. What I was trying to do is to pass a tensor slice of images as a input. So, I was looking for a way to use time distributed layer in the pre trained models. Thank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n7jbk8/p_timedistributed_layer_in_pretrained_models/"}, {"autor": "Scared_Soup3", "date": "2021-08-08 11:46:54", "content": "Adversarial examples definition /!/ A lot of papers define adversarial examples as perturbed samples that are able to cause a network to misclassify. So for a classifier N, a perturbed -----> image !!!  x' and true label ytrue, if \n\nN(x') != y(true)\n\nthen x' is an adversarial example.\n\n&amp;#x200B;\n\nHowever, papers from Ian Goodfellow and Kurakin describe it as examples that fool a network with high probability. This means all adversarial perturbed images are adversarial images and they have a certain success rate when attacking a model. So this means that the mathematical expression above is not valid!\n\nI am confused on which definition to go with, does the definition change according to the objective of the paper?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p0d86z/adversarial_examples_definition/"}, {"autor": "OnlyProggingForFun", "date": "2021-08-08 11:46:35", "content": "-----> Image !!!  synthesis from sketches using noise: SDEdit", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p0d81c/image_synthesis_from_sketches_using_noise_sdedit/"}, {"autor": "catalin8", "date": "2021-08-07 21:15:40", "content": "Extracting a list of -----> film !!!  titles from a discussion thread /!/ I'm looking to generate lists of film titles from discussion threads. For this I can make use of a large database containing about a million film titles.\n\nYet I'm not sure where to start with this project, as I haven't made use of ML before ?\n\nCan you please point me in the right direction with this ? Meaning the first 1-2 or couple of steps ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/p01jyk/extracting_a_list_of_film_titles_from_a/"}, {"autor": "OnlyProggingForFun", "date": "2021-08-07 12:29:35", "content": "Generate new images from any user-based inputs! Say goodbye to complex GAN and transformer architectures for -----> image !!!  synthesis tasks. This new method can do it using only noise!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ozsd50/generate_new_images_from_any_userbased_inputs_say/"}, {"autor": "theazz", "date": "2021-08-06 23:29:57", "content": "Recognizing a chihuahua in thousands of images /!/ Hey Folks\n\nSorry if this isnt ok to post here.\n\nI'm an animator not an engineer, in my line of work I've dabbled in python and C# but googling a bit has scared the hell out of me.\n\nI've got thousands of photos, daily images taken every few minutes of my dogs favourite sitting spot looking out the window.\n\nI wanted to make like a timelapse of her seeing the seasons go by in her favorite spot as she's aged. I've got a raspberry pi on book shelf running constantly.\n\nNow that I've got thousands of images I'm starting to see that she isn't sat there quite as often as I thought and I've got thousands of shots of basically nothing, and i don't fancy manually finding the ones of just her.\n\nI've used adobe lightroom plugins in the past tha use googles vision API to automatically tag images, which could work here but is a bit of a round the houses thing (and I dont pay for lightroom anymore).\n\nI'd love to know if there is a not too horrific way of running something either locally or in the cloud that uses a service like google's vision stuff and just delete all the photos that don't have a chihuahua in, or copy the ones that do to a new folder, or something :)\n\nBonus points if it can run on the pi and just discard the -----> image !!!  after it's taken it!\n\nThanks folks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ozi5s8/recognizing_a_chihuahua_in_thousands_of_images/"}, {"autor": "tucktoo", "date": "2021-08-06 20:14:40", "content": "Searching for a simple pattern in images /!/ Hi, I am very new at machine learning. I would like to use Tensorflow to find simple, specific pattern on images.\n\nImages are black and white (binary, only value 0 for black and 255 for white). Contains ONLY white circles on black background. I need to find on them the pattern consists of five white circles arranged according to predetermined proportions like below .\n\n&amp;#x200B;\n\n[Pattern to find](https://preview.redd.it/rjxz11lcpsf71.png?width=276&amp;format=png&amp;auto=webp&amp;s=35c7f15725cde0d42211d883e3f2f8f4a1c28d66)\n\n&amp;#x200B;\n\nOn images the pattern can be rotated to any angle and freely scaled. The circles can be of different sizes. Sometimes circles may be slightly distorted. In fact, each of circle can be treated as a single point x, y. The -----> image !!!  may have a different number of circles (zero included).\n\nThe size of the images may vary (usualy 3000x4000 px.)\n\n&amp;#x200B;\n\n[Here!](https://preview.redd.it/xdlbgg3fpsf71.png?width=442&amp;format=png&amp;auto=webp&amp;s=34a514d578c4be623e24afc75b95b249bf6390d3)\n\n&amp;#x200B;\n\nMoving on to machine learning...\n\nEach of this image can be treated as set of a dozen / several dozen x, y points (circles). Or as a set of 3000x4000 = 12000000 diffrent pixels. I think that treating a photo as a set of circles coordinates will be the most optimal for machine learning (?).\n\nMy question is what should I feed to my neural network? Does the number of inputs be variable? (each photo can have a different number of circles, so a different number of coordinates).\n\nMy idea:\n\ninputs: all circles coordinates on image (but must be variable, is it possible?)\n\noutputs: coordinates of pattern circles\n\nDoes it make seanse? Any better ideas?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ozelhe/searching_for_a_simple_pattern_in_images/"}, {"autor": "jstanaway", "date": "2021-08-06 18:16:01", "content": "Tensorflow, Docker and Linux /!/ So, I am trying to get the Tensorflow Docker -----> image !!!  setup and running properly. Im curious about a couple things and Im hoping someone who has went through the setup process can assist. \n\n1. I downloaded and can run the docker image with GPU support and Jupyter Notebook. When I go to run it, it runs fine but there's a screen that asks for a token which I enter but it then says that it's invalid. \n2. Do I need to map storage in and out of the docker container? \n3. How are things like Anaconda handled and the needed dependency libraries for a project? \n4. I would like to use Jupyter inside VSCode, there's a way to connect to a URL to access the Jupyter server but pointing it towards the URL from the docker container doesn't seem to work. Im assuming it's because of #1 because accessing it directly through a browser works. \n\nIf anyone is experienced in getting Tensorflow, Linux and the Docker image for Cuda etc to play nice I would greatly appreciate any help!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ozca8a/tensorflow_docker_and_linux/"}, {"autor": "techsavvynerd91", "date": "2021-09-08 16:29:58", "content": "Is there a preexisting TensorFlow model that can parse the text of an -----> image !!!  that shows a list of ingredients? /!/ So there's a feature for my Android/iOS application where the user can take a picture of a list of ingredients from a recipe and it will parse the text where the app can read the contents of each line from the picture. Any preexisting models that I should use to accomplish this? I'm looking at the [TensforFlow Hub](https://tfhub.dev/) and I'm not sure which model to use for this.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pkdy2m/is_there_a_preexisting_tensorflow_model_that_can/"}, {"autor": "techsavvynerd91", "date": "2021-09-08 16:29:46", "content": "Is there a preexisting TensorFlow model that can parse the text of an -----> image !!!  that shows a list of ingredients? /!/ So there's a feature for my Android/iOS application where the user can take a picture of a list of ingredients from a recipe and it will parse the text where the app can read the contents of each line from the picture. Any preexisting models that I should use to accomplish this? I'm looking at the [TensforFlow Hub](https://tfhub.dev/) and I'm not sure which model to use for this.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pkdxy1/is_there_a_preexisting_tensorflow_model_that_can/"}, {"autor": "flowreaction", "date": "2021-09-08 05:22:39", "content": "Object detection and its distance to arbitrary anchor points /!/ I am working on a project which consists of measuring the velocity of an object flying through the air.\n\nThe idea is to track the object, and base the speed on traveled pixels. All of this will be done on mobile devices in the end, so the input video will most likely not be stable, which falsifies the pixel distances. The idea is to take an arbitrary anchor point on the frame, which is present in all frames, and base the speed of the object on the distance to that point. So instead of calculating the difference of two object locations in two frames, I want to be calculating the lines between the object to an anchor point and take the difference of these to lines as the basis of the speed calculation.\n\nMy question now is, what is the best way to find the same anchor point in multiple images (frames) without actually needing some kind of class to it. Should this be done with ML or is it more feasible to rely on traditional -----> image !!!  processing methods.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pk497r/object_detection_and_its_distance_to_arbitrary/"}, {"autor": "JoZeHgS", "date": "2021-09-07 08:24:54", "content": "Is there an open pre-trained solution that I could use in this particular case? /!/  Hi everyone!\n\nI'm new Machine Learning and Computer Vision in particular. I have been using OpenCV's Brute-Force Matching with SIFT Descriptors and Ratio Test to see if certain product images are contained in part of other product images. It works absolutely perfectly for images such as [this one](https://http2.mlstatic.com/D_NQ_NP_972996-CBT47347236783_092021-O.webp) and [this one](https://ae01.alicdn.com/kf/Hc307e9f7874a4d9bbcbca614d1cefd81d/Aspirador-de-p-sem-fio-port-til-para-carro-aspirador-de-p-manual-limpadores-de-p.jpg_640x640.jpg).\n\nHowever, I need a solution that would also work just as well for these images:\n\n[One](https://ae01.alicdn.com/kf/H34afa2211e654ad3beb7dbb642e3517b9/Balan-a-digital-port-til-de-led-5kg-1g-balan-a-postal-de-alimentos-medi-o.jpg), [Two](https://http2.mlstatic.com/D_NQ_NP_796229-MLB44094251366_112020-O.webp), [Three](https://ae01.alicdn.com/kf/H3316912037da43d4a30c02df6c1732e3w/Balan-a-de-cozinha-digital-sf400-balan-a-eletr-nica-de-alta-precis-o-para-alimentos.jpg), [Four](https://ae01.alicdn.com/kf/H7fd1a38250c84a7aa80035126e5008ead/Novo-10kg-1g-digital-caf-feij-o-material-medicinal-escala-cozinha-cozimento-comida-escala.jpg)\n\nThe problem with the approach I'm using was that it pretty much only matched the buttons and display and completely ignored the rest. Is there a setting I could tweak for these cases? Changing -----> image !!!  levels in Photoshop did not help that much. \n\nWould a pre-trained Neural Network of some kind or some other machine learning approach work better here? If so, what would you recommend?\n\nThanks a lot!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pjiwv6/is_there_an_open_pretrained_solution_that_i_could/"}, {"autor": "Successful_Boat_3099", "date": "2021-09-06 12:15:12", "content": "\ud83d\udc8aYour daily dose of machine learning : loss functions for -----> image !!!  classification /!/ &gt;This is a series of posts that I post almost daily. I call them \u201cyour daily dose of machine learning\u201d.\n\nIn deep learning for image classification, we mostly use log based loss functions such as cross entropy.\n\nSome researchers have experimented with 12 different loss functions for classification in the paper \u201cOn Loss Functions for Deep Neural Networks in Classification\u201d.\n\nThey found some interesting results but one that stood out to me is the fact that some loss functions that are typically associated with regression problems can actually be used for classification.\n\nAn example of these functions is L1 loss.\n\nThough they noticed that the training was slower using these loss functions.\n\nThis shows that there are still lots of areas that are yet to be explored even for problems that we think of as \u201csolved\u201d such as classification.\n\nFollow me on your [***favorite social network***](https://withkoji.com/@Nour_Islam)***!***", "link": "https://www.reddit.com/r/learnmachinelearning/comments/piyacb/your_daily_dose_of_machine_learning_loss/"}, {"autor": "itBlimp1", "date": "2021-10-08 04:09:13", "content": "Higher resolution -----> image !!!  classification transfer learning /!/ I have a bunch of images at high resolutions (1200x1200). So far I've been transfer learning using EfficientNetB0 in keras with okay success (resizing to 224x224 before training gives me ~88% validation accuracy). However, A few categories depend on recognizing small, thin structures within the image and I'm trying to see if I can bump up the resolution for the model, to say, something above 600x600, to increase accuracy. What's the best way to do this in a transfer learning setting? \n\nI've thought of \n1. Adding a convolutional layer at the very bottom of the model that downscales from 1200x1200 or even 600x600 to 224x224 and then trains as normal. I've heard this might not be a good idea since the first few layers of CNNs  are designed to recognize general features of images and I don't know how the convolution will interact with that. If this is a good approach though, where precisely should I add the convolutional layer? \n\n2. Adding a pooling layer at the very bottom to downscale. My naive intuition says that this wouldnt be useful since it would \"wash out\" the small, thin details necessary in the image.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q3q1pj/higher_resolution_image_classification_transfer/"}, {"autor": "Edulad", "date": "2021-10-07 13:27:09", "content": "Script to Remove background or background color and Trim the -----> Image !!!  /!/ Hi, i am using The following version\n\n**Ubuntu 21.04  (64-bit)**\n\n**Gnome Version: 3.38.5**\n\n**Windowing System: X11**\n\n**GIMP 2.10.24**\n\nNow  i have the following images, what i want to do to them is remove the  background of the image and only the Main product remains\n\n**OR** is there a way to highlight the main image and then save it separately as JPEG ?\n\nI have a batch of them, so that's why want a script :)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/4bndmus351s71.jpg?width=476&amp;format=pjpg&amp;auto=webp&amp;s=0e67d90afdbbd839d27bc670242ca2b08aba1c6a\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/eq8z6c9551s71.jpg?width=625&amp;format=pjpg&amp;auto=webp&amp;s=38076f9b723a4dcb00c251bd9df587d34b72c262\n\n&amp;#x200B;\n\nhttps://preview.redd.it/dg11e07651s71.jpg?width=476&amp;format=pjpg&amp;auto=webp&amp;s=4e655a1f7c1359c1cdc5f302cde8e3ad0969a710\n\n&amp;#x200B;\n\n**THANKS :)**", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q392kh/script_to_remove_background_or_background_color/"}, {"autor": "91o291o", "date": "2021-10-07 11:55:17", "content": "kickstarter for opencv -----> camera !!! ", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q37g6x/kickstarter_for_opencv_camera/"}, {"autor": "webslinger_007", "date": "2021-10-07 07:28:47", "content": "Making a Foot detection model for mobile device in Tensorflow /!/ I am completely new to machine learning started by looking at some tutorials in YouTube about object detection using a pretrained model ( [SSD MobileNet V2 FPNLite 320x320](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz) ). I provided almost 350 images of foot in the dataset for training the model it detects the foot but also think any blue colored object is a foot ( because most of the images also contains jeans which is mostly blue ). For that problem I tried to capture close up images of foots so nothing can interfere but that creates a new problem the model only detects if the foot I close to the -----> camera !!!  but that's not how I wanted it to work. I am confused of how should tackle this problem.   \n\n\nI am making an application for shoes try on like [Wanna Kicks](https://play.google.com/store/apps/details?id=by.wanna.apps.wsneakers&amp;hl=en_IN&amp;gl=US).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q33uae/making_a_foot_detection_model_for_mobile_device/"}, {"autor": "SushiWithoutSushi", "date": "2021-01-15 12:27:58", "content": "How Much training data do I need for my -----> image !!!  detection project? /!/ I am writing a machine that hopefully will know when a meme belongs to a specific series or not. Usually this memes have the main cast faces or bodys in them so I think this shouldn't be that weird of a concept. \n\nThe problem is, as the absolute newbie that I am to machine learning I don't know how much training data I need to train the machine. I have a repositories with over 50,000 memes that are from the series and memes that are not from the series.\n\nWith the research that I've done so far all the projects with similar aims have a training set of 10,000 images and state that those samples are more than enough to differentiate between objects or people or whatever it's been trained to do. That's why I think my repositories should be more than enough. \n\nAnother concept I find super interesting are the GANs (generative adversarial networks). I've seen some people have tried to generate images that do not exist with this idea and I wonder if this could be applicable to generate new memes. \n\nI know that the project idea sounds a little bit silly. Using the power of machine learning to classificate and generate memes is not as cool as other where drugs are developed but I want to make something by myself and that implies I gather the data too. Memes are quite easy to get on the internet and that's basically  why the project is about them. \n\nThanks for your time.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kxtl4t/how_much_training_data_do_i_need_for_my_image/"}, {"autor": "rockyrey_w", "date": "2021-01-15 09:39:54", "content": "[R] Will Humans Be Able to Control Superintelligent AI? New Study Says \u2018No\u2019 /!/ Along with AI\u2019s remarkable achievements and continuing rapid expansion into new domains comes greater concern over the ethical problems surrounding advanced AI systems. Incidents like last year\u2019s shutdown of AI-powered [Genderify ](https://www.producthunt.com/posts/genderify)and Yann LeCun\u2019s \u201cexit\u201d from Twitter after heated discussions regarding Duke University\u2019s PULSE AI -----> photo !!!  recreation model underscore the ongoing controversies regarding biases and errors embedded in the design and deployment of AI systems. Google AI\u2019s recent dismissal of its Ethical AI team co-lead Timnit Gebru poses a more serious question: do tech giants and large research institutions even want to find solutions?  \n\n\nA [paper](https://www.jair.org/index.php/jair/article/view/12202/26642) in the *Journal of Artificial Intelligence Research* goes a step further, warning that it could become fundamentally impossible to control a superintelligent AI (a computer program or a programmed robot that is much more intelligent than humans in almost any field). A preprint of the paper was uploaded to arXiv in 2016, and now, with the topic becoming more urgent than ever, an expanded version has been published with additional presentation details, references, and comments about alternative scenarios.\n\n[Read the full summary here](https://syncedreview.com/2021/01/14/will-humans-be-able-to-control-superintelligent-ai-new-study-says-no/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kxrbc0/r_will_humans_be_able_to_control_superintelligent/"}, {"autor": "hoangndst", "date": "2021-01-15 05:17:08", "content": "Kmeans GUI and Image Compression [Beginner] /!/ Hi, last week i created a Kmeans Algorithm Visualization  and Basic -----> Image !!!  compression using K-means. Check it out.\n\n[https://github.com/hoangndst/kmeans-img-compression](https://github.com/hoangndst/kmeans-img-compression)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kxnrjr/kmeans_gui_and_image_compression_beginner/"}, {"autor": "MohammedTech", "date": "2021-01-14 23:27:00", "content": "Take input API and output machine learning model /!/ I have this task to create a simple ML model that should be able to take the API key -----> image !!!  URLs as input and tell how much difference between these two -----> image !!! s in %.\n\n&amp;#x200B;\n\nI have my Django API ready that basically lets you upload -----> image !!! s and returns URLs and also an OpenCV application that tells difference between -----> image !!! s.\n\nMy problem is, my Input and Output are different applications and I need to combine these two. so when I feed the URL to my OpenCV model that should give me output telling me the difference.\\\\\n\n&amp;#x200B;\n\nHow can I perform that?\n\n&amp;#x200B;\n\nhere is my Django code:\n\n    class awsimage(models.Model):\n        title = models.CharField(max_length=50)\n        images = models.ImageField('images/')\n    \n    class awsimageSerializers(serializers.HyperlinkedModelSerializer):\n        image_url = serializers.SerializerMethodField('get_images_url')\n        class Meta:\n            model = awsimage\n            fields = ('title', 'images','image_url')\n    \n        def get_images_url(self,obj):\n            return obj.images.url\n    \n    class awsimageView(viewsets.ModelViewSet):\n        queryset = awsimage.objects.all()\n        serializer_class = awsimageSerializers\n    \n    router = routers.DefaultRouter()\n    router.register('awsimages',views.awsimageView)\n    \n    urlpatterns = [\n        path('', include(router.urls))\n    ]\n\nmy openCV\n\n    image1 = first_image\n    image2 = second_image\n    \n    res = cv2.absdiff(image1,image2)\n    res = res.astype(np.uint8)\n    percentage = (np.count_nonzero(res)*100) / res.size\n\nPS. This is a super simple API please don't judge me, I am a super beginner in both fields.  However, at the same time, I am open to critiques that can guide me in the right direction. Thank you", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kxhnv1/take_input_api_and_output_machine_learning_model/"}, {"autor": "ysharm10", "date": "2021-01-14 21:01:52", "content": "Need help with Zero inflated, hurdle model /!/ Hello! I have a data which has excess zeroes, over 90%. Rest of the data is skewed towards extreme right of the range.\nRange of data: 0-25. \n\nDistribution: \nhttp://imgur.com/gallery/4xNtNUt\n\nI have tried zero inflated model, zip negative binomial, hurdle model and hurdle binomial and negative binomial models. \nEvery model is giving me the same kind of prediction where it's underfitting zeroes and no predictions beyond 4. Actual distribution of test data is similar to the first -----> image !!! .\n\nTest data prediction: http://imgur.com/gallery/OcPkgpx\n\nPredictor variables: Two continuous. Three categorical. So far I've only added two continuous variables to the model.\nAlso, it's a time series data and the errors are autocorrelated. I did shuffle the train data to remove dependency but not really sure if that's the right thing to do.\n\nCan someone please tell me what's happening? \n\nThank you!\n\nEdit: Rootogram also shows underfitting of '25'", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kxep0e/need_help_with_zero_inflated_hurdle_model/"}, {"autor": "purplebrown_updown", "date": "2021-05-05 05:28:19", "content": "Books, references, or courses for understanding custom keras models, custom layers, train step and gradient tape? /!/ I need to write a custom keras/ tensorflow model with potentially a custom layer, train step, gradient tape, tf.function decorators and more, but I don't really know how all that works. I am trying to base it off of a reference code, but going through it line by line is not helping without understanding the bigger -----> picture !!! . is there a recent book or reference or course that addresses all these concepts? A lot has changed even in the past year and I want to also make sure I'm not learning some deprecated API.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n589yo/books_references_or_courses_for_understanding/"}, {"autor": "danielgafni", "date": "2021-05-04 20:07:52", "content": "Repalette: -----> image !!!  recoloring with deep neural networks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n4wuag/repalette_image_recoloring_with_deep_neural/"}, {"autor": "hiphop1987", "date": "2021-05-04 13:25:03", "content": "5 Most Common Mistakes with SQL /!/ SQL is widely used in Data Analysis and Data Science. It is fairly simple to start writing SQL queries, but bugs can quickly sneak into the code and consequently in the reports (or Machine Learning models).\n\nI didn't publish all of the examples here as I would need to copy-paste them. You can see them in the [5 Most Common Mistakes with SQL](https://towardsdatascience.com/5-most-common-mistakes-with-sql-1ac0a28fa164?sk=ef75379851d60fb1f920b99470d19580) article.\n\n# Let\u2019s start\n\n## Table sales\n\nThis table contains sales entries with the timestamp, product, price, etc.\n\nNote, the key column is unique, values in other columns can be repeated (eg. ts column).\n\n    DROP TABLE IF EXISTS sales;\nCREATE TEMPORARY TABLE sales\n(\n key       varchar(6),\n    ts        timestamp,\n    product   integer,\n    completed boolean,\n    price     float\n);\n    INSERT INTO sales\nVALUES ('sale_1', '2019-11-08 00:00', 0, TRUE, 1.1),\n       ('sale_2', '2019-11-08 01:00', 0, FALSE, 1.2),\n       ('sale_3', '2019-11-08 01:00', 0, TRUE, 1.3),\n       ('sale_4', '2019-11-08 01:00', 1, FALSE, 1.4),\n       ('sale_5', '2019-11-08 02:00', 1, TRUE, 1.5),\n       ('sale_6', '2019-11-08 02:00', 1, TRUE, 1.5);SELECT * FROM sales;\n\n# Table hourly delay\n\nThis table contains hourly delays for a certain day. Note, the ts column is unique in the table below.\n\n    DROP TABLE IF EXISTS hourly_delay;\nCREATE TEMPORARY TABLE hourly_delay\n(\n    ts    timestamp,\n    delay float\n);INSERT INTO hourly_delay\nVALUES ('2019-11-08 00:00', 80.1),\n       ('2019-11-08 01:00', 100.2),\n       ('2019-11-08 02:00', 70.3);SELECT * FROM hourly_delay;\n\n# 1. Inner join\n\nJoins are guilty of the most bugs in SQL queries. When using joins we need to know if we are working with one-to-one, one-to-many or many-to-many relationships.\n\nLet\u2019s illustrate this with an example. We would like to sum all delays in sales per day and also calculate the average price of sales per day.\n\n    SELECT t2.ts::DATE, sum(t2.delay), avg(t1.price)\nFROM hourly_delay AS t2\n INNER JOIN sales AS t1 ON t1.ts = t2.ts\nGROUP BY t2.ts::DATE;\n\nThe result is wrong!\n\nThe query above multiples the delay column from the hourly\\_delay table as can be seen in the -----> image !!!  below. This happens because we join by timestamp, which is unique in the hourly\\_delay table, but it is repeated in the sales table.\n\nTo fix the problem, we are going to calculate statistics for each table in a separate subquery and then join the aggregates. This will make timestamps unique in both tables.\n\n    SELECT t1.ts, daily_delay, avg_price\nFROM (SELECT t2.ts::DATE, sum(t2.delay) AS daily_delay FROM hourly_delay AS t2 GROUP BY t2.ts::DATE) AS t2\n INNER JOIN (SELECT ts::DATE AS ts, avg(price) AS avg_price FROM sales GROUP BY ts::DATE) AS t1 ON t1.ts = t2.ts;\n\nI didn't publish all of the examples here as I would need to copy-paste them. You can see them in the [5 Most Common Mistakes with SQL](https://towardsdatascience.com/5-most-common-mistakes-with-sql-1ac0a28fa164?sk=ef75379851d60fb1f920b99470d19580) article.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n4o9i6/5_most_common_mistakes_with_sql/"}, {"autor": "Aryagm", "date": "2021-05-03 21:13:46", "content": "A good alternative to the Wolfram Alpha knowledge base. /!/ I want an open source alternative to Wolfram Alpha that is like a QA system that: \n\n* can take in natural language and process it\n* have -----> image !!!  processing functions \n* has scientific functionality and geographic functionality just like Wolfram Alpha\n* can do mathematical calculations (like a computer algebra system)\n\nThe reason I want to find a Wolfram Alpha open source alternative is  because I am interested in using a Wolfram Alpha-like system for one of  my projects.\n\n I have found a Python module known as [quepy](https://quepy.readthedocs.io/en/latest/index.html). It can take in natural language and search DBpedia.org or Freebase (two common knowledgebases) for results.\n\nSomething like this would satisfy my first requirement.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n47ipk/a_good_alternative_to_the_wolfram_alpha_knowledge/"}, {"autor": "nivedwho", "date": "2021-05-03 19:13:02", "content": "How does attention maps work? /!/ I am having a hard time in understaning self attention models that learns to focus on more important parts of the -----> image !!! . Can somebody explain its working process?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n44mc9/how_does_attention_maps_work/"}, {"autor": "madzthakz", "date": "2021-05-03 12:53:38", "content": "I'm a Senior Data Scientist at Disney and I'm hosting another Data Science Q&amp;A session this Thursday @ 5:30 PM PST. I'll be joined by an Applied Scientist at Amazon! /!/ \\*\\*DISCLAIMER\\*\\*: This is completely free and not sponsored in any way. I really just enjoy helping students get started and potentially transition into Data Science\n\nAs the title mentions, I'm a Senior Data Scientist at Disney and I'm going to host **another** Data Science Q&amp;A this Thursday at 5:30 PM PST. This time I'll have **Krishna Rao** join me. Susan is an Applied Scientist at **Amazon** and is responsible for building state-of-the-art advertising recommendation systems! Krishna has had a slightly unconventional path to get to this point. His background is in Civil Engineering and he was first a Data Science consultant before joining Amazon. I'm looking forward to having him share his journey and the tips he picked up along the way.\n\nLast session was an absolute blast with over 250 people who attended from all over the world. I hope you see you all there!\n\nRegister Here:\n\n[https://disney.zoom.us/webinar/register/WN\\_RF0xeFZZTWqi8l7ZAN4KOg](https://disney.zoom.us/webinar/register/WN_RF0xeFZZTWqi8l7ZAN4KOg)\n\nVerification:\n\nMy -----> photo !!! : [https://imgur.com/a/Wg3DMLV](https://imgur.com/a/Wg3DMLV)\n\nMy LinkedIn: [https://www.linkedin.com/in/madhavthaker/](https://www.linkedin.com/in/madhavthaker/) (feel free to connect)\n\nSusan's LinkedIn: [https://www.linkedin.com/in/achyutuni-sri-krishna-rao-0721a015/](https://www.linkedin.com/in/achyutuni-sri-krishna-rao-0721a015/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n3v9b7/im_a_senior_data_scientist_at_disney_and_im/"}, {"autor": "sam189239", "date": "2021-05-03 10:33:30", "content": "Help for a project /!/ Im currently working on a project that requires me to extract body measurements from a 2D -----> image !!!  and then maybe even generate a 3D model (approximate). Im new to this and have only worked on classification and prediction tasks majorly and love to get some pointers on how to start off with this. The steps involved in such a task, what I need to use, any open source code that I can make use of would be of great help.. Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/n3sv4w/help_for_a_project/"}, {"autor": "IWantToEatGoodFood", "date": "2021-03-08 16:09:41", "content": "SteganoGAN, GAN and -----> image !!!  compression /!/ Hi guys !  \nI'm trying to make steganography which is resistant to image compression. But being new in the world of GAN, I don't really know how to do it. I found that this project was very well written and interesting : [https://github.com/DAI-Lab/SteganoGAN](https://github.com/DAI-Lab/SteganoGAN)  \n\n\nWhere should I start to add compression resistance. To me, the easiest way was to compress the image right after the generation during training and let the decoder / discriminator find if it can still get the hidden data back. Is this too simple haha ? \n\n&amp;#x200B;\n\nAnyway, thx for the help !", "link": "https://www.reddit.com/r/learnmachinelearning/comments/m0j0of/steganogan_gan_and_image_compression/"}, {"autor": "chirathpansilu", "date": "2021-07-18 10:59:42", "content": "iPad center stage insight /!/ Hi,\n there is a new feature on iPad called [center stage](https://youtu.be/XlYQ7UG2TQA) which frames the video as a real -----> camera !!!  man, they state that they use machine learning to do this. I am wondering how do they do this using machine learning ? What is their workflow ? What kind of networks are they using and inner workings of this .\nI also found that something similar to this in Facebook Portal device and there is not much information about the inner working of this either. \nSo can you guys please explain how machine learning achieve this and hopefully point some Research papers related to this or something similar to this ? \n\nThank you in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/omof68/ipad_center_stage_insight/"}, {"autor": "QuasiEvil", "date": "2021-07-17 20:24:34", "content": "Detecting if a sample is within your existing data distribution? /!/ Trying to wrap my head around a few related concepts here. \n\nLets say I want a way of determining how \"triangle-like\" an input -----> image !!!  is (for simplicity lets say we're just dealing with -----> image !!! s consisting of geometric shapes). I could train a model to distinguish triangle from not-triangle, but, the set of not-triangles is infinite. If all I used were circles for my not-triangles, it probably won't be very robust to all the other not-triangles out there! \n\nOf course, a classification approach is not the only one, and probably not the best - an autoencoder could maybe(?) be employed instead. In this case, I train my model to map my triangle dataset into some lower-dimensional (2D?) representation. Then, when I present a not-triangle example, it will hopefully fall outside the triangle cluster (or something like this - I'm probably missing some details here). \n\nUltimately what my question is getting at is exactly the title - given some data distribution (my collection of triangles), what sort of methods can I employ to test if a new example fits that distribution?\n\n(and yes I realize ML may not be best method at all)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ombzz3/detecting_if_a_sample_is_within_your_existing/"}, {"autor": "AgentCooderX", "date": "2021-07-17 14:46:27", "content": "How and where to start to learn 2D face -----> image !!!  to 3D model? /!/ Yeah i know its a long and tedious process, but i want to learn the path of which subjects and topics to study to learn this. Are there any resource like best courses, articles, books, etc. that is applicable, let me know.  \nI want to particularly learn,\n\n  \n1. what type of AI field should I study and focus, I assume Convolutional Neural network? which specific topic? Face detection?\n\n2. How do I start the training data process to map the result of CNN to say a 3D model? (Im not sure if the phrasing of the question is even correct). But my point is, I want to learn this stage as well.\n\nLastly, do you guys have any online course recommendation like in udemy, or the like or even in edx, etc. that points to this direction?  \n\n\nI just want to know where to start and what path to go..", "link": "https://www.reddit.com/r/learnmachinelearning/comments/om5m9j/how_and_where_to_start_to_learn_2d_face_image_to/"}, {"autor": "eagleandwolf", "date": "2021-07-17 13:48:00", "content": "Why is data versioning necessary? /!/ I am new to MLOps and I am having hard time understanding the necessity of getting data versioned. I read somewhere that if data is not versioned, versioned deployments are possible. But if I am versioning the trained model (say sklearn model), I can revert back to any model version I want to, without involving data versioning. How exactly does data versioning come into -----> picture !!! ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/om4l38/why_is_data_versioning_necessary/"}, {"autor": "ParkTheMonkey", "date": "2021-07-17 13:45:59", "content": "Help with metrics for imbalanced classification. /!/ I am trying to train a neural network to classify chest X-ray scans as my final MSc project. I have a dataset of 13808 -----> image !!! , 3616 labelled COVID, 10192 labelled normal, so the ratio of COVID to normal -----> image !!! s is 26.2/73.8. COVID is the positive class, Normal is the negative class. I am using keras to build a CNN and I am a bit overwhelmed by all the different metrics. \n\nI have read that accuracy is a poor measure of performance, especially for imbalanced datasets, and that for medical imaging it is common to use sensitivity and specificity, as well as metrics like F1-score, AUC-ROC, and AUC-PR. \n\nMy reasoning is that minimizing false negatives, and therefore maximizing sensitivity/recall, is the priority in this context, as classifying someone as without COVID when they have it would cause the virus to spread. False positives are undesirable, as people would take unnecessary precautions, but not as important as minimizing false negatives.\n\nI am a conversion student in computer science and so I am relatively new to machine learning and statistics. I would greatly appreciate any advice on how much of a problem the class imbalance is and what metrics would be most appropriate in this context. Thank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/om4jvc/help_with_metrics_for_imbalanced_classification/"}, {"autor": "gordicaleksa", "date": "2021-07-17 09:08:55", "content": "DALL-E: Zero-Shot Text-to------> Image !!!  Generation | Paper Explained!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/om0ufv/dalle_zeroshot_texttoimage_generation_paper/"}, {"autor": "call_me_ninza", "date": "2021-07-17 07:07:15", "content": "-----> Image !!!  processing is really a cool field to be explored. As a newbie, here is my first step towards it. [P]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/olzg23/image_processing_is_really_a_cool_field_to_be/"}, {"autor": "orenog", "date": "2021-07-17 02:05:16", "content": "StyleGAN2-ada - Star Wars model, is it a good idea? /!/ Hey, I just thought about a crazy idea, tell me if it's stupid so I delete it from my head (and why it's stupid) \n\nI want to take the first 3 StarWars movies (only watched them so far, so no spoilers please)\n\nAnd take a frame from every 5 second of the -----> film !!!  (about 1500 images per -----> film !!! )\n\nAnd train StyleGAN2-ada on these images. \n\nDo you think it will look good, or is it just waste of time and google's electricity? (Colab pro)\n\nI would like to hear your opinions before I start.\n\nThank you \u263a\ufe0f", "link": "https://www.reddit.com/r/learnmachinelearning/comments/olv8hl/stylegan2ada_star_wars_model_is_it_a_good_idea/"}, {"autor": "OriginalEstimate6", "date": "2021-07-16 17:14:36", "content": "How to resize a 25x25 -----> image !!!  to 28x28 without losing quality? Using np.resize is giving me absurd results. I have included the results. /!/ Hi! I am working on an Optical Character Recognition project using cv2 and keras mostly. After performing multiple levels of segmentation, my function returns single letters(characters) of size around 25x25. However, my prediction model requires images of size 28x28 as input. On using numpy.resize() to achieve this is giving me completely unacceptable results. How can achieve this? Or, should I change my approach in someway?\n\nNote: Yellow pixels have value 1 and purple ones 0.\n\n['f' character after resizing. \\(28x28\\).](https://preview.redd.it/rs8083e0ylb71.png?width=640&amp;format=png&amp;auto=webp&amp;s=2472ae684ee4fb269cf6377ff1b9aab2f89dc70d)\n\n&amp;#x200B;\n\n['f' character before resizing. \\(24x24\\).](https://preview.redd.it/54qjqa7xxlb71.png?width=640&amp;format=png&amp;auto=webp&amp;s=e23db77a3e5091a5ae49466ea2e5522675f1cfad)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ollbz3/how_to_resize_a_25x25_image_to_28x28_without/"}, {"autor": "aitasy", "date": "2021-07-16 10:43:06", "content": "How can I see how -----> image !!! s are processed when training a model to an -----> image !!!  dataset? /!/ I'm playing around with Tensorflow Hub's prebuilt models and making my own dataset of random stuff like family pictures, screenshots, etc. to be classified how they're classified originally into folders on my phone (Screenshots, Camera, Memes etc.). Just want to know how it works.\n\nNow these images are usually long (screenshots), wide (most pictures taken from camera) or irregular (memes). I wonder how it was adjusted to fit the model's input size? Like, was it downscaled? Cropped at center or at edge? How can I see what happens?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/olecub/how_can_i_see_how_images_are_processed_when/"}, {"autor": "Adventurous_Ad3434", "date": "2021-07-16 06:04:02", "content": "How to become self taught(read the post)? /!/ I don't have any hard skills ATM. I m pretty much free all the time. Few days ago my friend asked me to build an app. It is not a complex app it is for his workplace. A little I know about it is that he wants me to build some database with all his colleagues names on it and a search bar. The main function, though, is that is that there should be a -----> camera !!!  Option and finger print option in the app so after signing up his colleagues could use fingerprint as an attendance option and use -----> camera !!!  to take a snap of whatever file they are submitting or working on and when anyone search their name in search bar in app(or he keeps this option himself) then all their profile and today's update should be shown like a profile page. He also has the data on Excel sheets. I told him I like how programmers work and he asked to me work on it as practice But I don't know much about programming other than a little bit about HTML. I have following questions:\nIs HTML with JavaScript and CSS is enough to build an app like I described above(signing up page, search bar, database, fingerprint, camera)?\nIf not then what should I learn for this?\n I have so many many fun projects in my mind and I wanna get into them but I can't bcz of lack of knowledge. I wanna work on AI, machine learning and robotics. So any programming languages for this?\nI wanna know how to connect my apps with other hardware like air conditioner don't have any Bluetooth or wifi then if I build an app to control A.C how to connect it with that?\nHow to learn about building robots?\nIs it possible to become self-taught? How long does it take? What does it take?\nAre there any books I can read to learn anything I mentioned above?(please suggest)\nIs there any free tutorials or mentors here on reddit or forums that will teach me all this pretty fast?\nIf I start right now without any knowledge how long will take me to learn to be able to develop apps as much complex as Facebook or Amazon or stripe?\nAre there any free places, tutorials, forums, courses, books to self learn to develop new technologies like VR glasses, holograms, sensors for AI etc. on our own?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/olav9w/how_to_become_self_taughtread_the_post/"}, {"autor": "Aldroc", "date": "2021-07-16 05:28:12", "content": "Help with model predicting on new data /!/ So I recently coded up an -----> image !!!  classification model and implemented a block to predict on new data from colab itself. I found it on google but it had no explanations attached to it so I was hoping someone here could help me. \n\n    #Predict on new data\n    \n    img = tf.keras.preprocessing.image.load_img(\n        '/content/drive/MyDrive/Datasets/IDRiD_003.jpg', target_size=(224, 224)\n    )\n    \n    img_array = tf.keras.preprocessing.image.img_to_array(img)\n    img_array = tf.expand_dims(img_array, 0) # Create a batch\n    \n    predictions = model.predict(img_array)\n    score = tf.nn.softmax(predictions[0])\n    \n    print(\n        \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n        .format(folders[np.argmax(score)], 100 * np.max(score))\n    )\n\nThe code is working I just have a few questions:\n\n1. What exactly does expand\\_dims do? I saw an example where the numpy array doesn't print entirely without it but then why does it do that in the first place? Shouldn't it print properly from the beginning?\n2. How is the score being calculated and the image being classified? What exactly is the prediction\\[0\\]? I don't really require the details of the softmax and argmax functions just would like to learn like what is happening here.\n\nWould really appreciate the help :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oladq7/help_with_model_predicting_on_new_data/"}, {"autor": "gorzelnias", "date": "2021-07-22 19:52:40", "content": "100% accuracy on Fashion Mnist without CNN - possible? /!/ I was just experimenting with the TensorFlow Data framework on the FashionMNIST dataset. \n\nHere is my network architecture:\n\n    input_layer = keras.layers.Input(shape=input_shape)\n    flatten_layer = keras.layers.Flatten()(input_layer)\n    normalization = keras.layers.LayerNormalization()(flatten_layer)\n    wide_1 = keras.layers.Dense(300, activation=\"relu\")(normalization)\n    wide_batch_1 = keras.layers.BatchNormalization()(wide_1)\n    wide_2 = keras.layers.Dense(300, activation=\"relu\")(wide_batch_1)\n    wide_batch_2 = keras.layers.BatchNormalization()(wide_2)\n    deep = normalization\n    for i in range(6):\n        deep = keras.layers.Dense(50, activation=\"relu\")(normalization)\n        deep = keras.layers.BatchNormalization()(deep)\n    concat = keras.layers.Concatenate()([wide_batch_2, deep])\n    softmax = keras.layers.Dense(10, activation=\"softmax\")(concat)\n    \n    model = keras.models.Model(inputs=input_layer, outputs=softmax)\n\nThis network is not too complex not too deep either. I have used Nadam optimizer, and tuned the learning rate before final training. To my amusement, this network has reached 100% validation accuracy.\n\nIs this even possible? I am thinking I got something wrong since people get around 95% with CNNs, which are better suited to -----> image !!!  processing.\n\nI have my whole notebook with it here:\n\n[https://github.com/Szustarol/datasets-analysis/blob/master/Classification%20Datasets/Fashion%20MNIST/FashionMNIST.ipynb](https://github.com/Szustarol/datasets-analysis/blob/master/Classification%20Datasets/Fashion%20MNIST/FashionMNIST.ipynb)\n\n&amp;#x200B;\n\nCan anyone shed some light, why is the result so good (too good to be true)?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oplmv3/100_accuracy_on_fashion_mnist_without_cnn_possible/"}, {"autor": "markbowick", "date": "2021-07-22 17:11:02", "content": "Where to begin learning about how to deploy GANs via API/endpoint? /!/ Hi,\n\nI've been doing some transfer learning and I'd like to deploy one of my GAN networks online as an API. Want to accept POST requests with a couple of parameters, and have the API return a URL with my -----> image !!! . I have plenty of experience creating Node APIs, but nothing with Python or TensorFlow (which is what I'm using). I'm also a solo developer on a modest budget. \n\nWhere would I start learning how to do this? Ideally I'd like something where I can upload a pickled model, set the server endpoints &amp; data format, and deploy in one click, but Google isn't helping me any.\n\nThank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/opiaid/where_to_begin_learning_about_how_to_deploy_gans/"}, {"autor": "firstsecondlastname", "date": "2021-07-22 10:29:14", "content": "Best ressources and communities? -----> Image !!! , landscape, character creation via ML - Getting into creating stuff. Orientation for complete beginner help, please. /!/ After seeing [https://moultano.wordpress.com/2021/07/20/tour-of-the-sacred-library/](https://moultano.wordpress.com/2021/07/20/tour-of-the-sacred-library/) (spoiler: all images were created from the text above them) and similar effects - like GPT-3, some stuff from twominutepapers (youtube) - I'm set to delve into the topic.\n\nMy main goal is to have enough grasp over the matter to control image creation and modification, test out new sources etc with a focus on landscape, architecture and character modulation and modification.\n\nMy level: bit of HTML &amp; CSS understanding and a slight overview over programming in general, but to keep it simple: let's say I never wrote a line of code myself.\n\nFor the path I need to understand the basics, the necessary programming-language skills and a community to back it up when questions arise.\n\nSome questions:\n\n* What path would you recommend?\n* What language to begin with? And what languages on the horizon?\n* Where are good ressources for someone with no grasp of the basics?\n* How long would it take to become \"proficient\" in \"trying out scripts\"?\n\n&amp;#x200B;\n\nThanks in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/opb586/best_ressources_and_communities_image_landscape/"}, {"autor": "Neb519", "date": "2021-07-21 19:00:07", "content": "Just published my free course on Neural Networks, where I derive and build a Neural Network -----> image !!!  classifier from scratch in Python /!/ My course is called Neural Networks For Your Dog - So easy your dog could learn them.\n\nYou can view the course [on YouTube](https://www.youtube.com/playlist?list=PL9oKUrtC4VP5N3VtTTjhTfiHoFXmnrgPW) or [on my blog](https://www.gormanalysis.com/blog/neural-networks-for-your-dog/) (embedded YouTube videos with code snippets below). I'm in the process of publishing on Udemy, but they have issues with my title.. \n\nIt took me seven months to put this together, so I'm hoping a few people see it and find it useful!\n\n## Course Curriculum\n([See the code on GitHub](https://github.com/ben519/nnets-for-your-dog))\n\n1. **Introduction**  \n  [1.1 Introduction](https://www.gormanalysis.com/blog/neural-networks-for-your-dog-1-1-introduction)  \n2. **Perceptron**  \n  [2.1 MNIST Dataset](https://www.gormanalysis.com/blog/neural-networks-for-your-dog-2-1-mnist_dataset)  \n  [2.2 Series Basic Indexing](https://www.gormanalysis.com/blog/neural-networks-for-your-dog-2-2-perceptron-model)  \n  [2.3 Perceptron Learning Algorithm](https://www.gormanalysis.com/blog/neural-networks-for-your-dog-2-3-perceptron-learning-algorithm)  \n  [2.4 Pocket Algorithm](https://www.gormanalysis.com/blog/neural-networks-for-your-dog-2-4-pocket-algorithm)  \n  [2.5 Multiclass Support](https://www.gormanalysis.com/blog/neural-networks-for-your-dog-2-5-multiclass-support)  \n  [2.6 Perceptron To Neural Network](https://www.gormanalysis.com/blog/neural-networks-for-your-dog-2-6-perceptron-to-neural-network)  \n3. **Neural Network**  \n  [3.1 Simple Images](https://www.gormanalysis.com/blog/neural-networks-for-your-dog-3-1-simple-images)  \n  [3.2 Random Weights](https://www.gormanalysis.com/blog/neural-networks-for-your-dog-3-2-random-weights)  \n  [3.3 Gradient Descent](https://www.gormanalysis.com/blog/neural-networks-for-your-dog-3-3-gradient-descent)  \n  [3.4 Multiclass Support](https://www.gormanalysis.com/blog/neural-networks-for-your-dog-3-4-multiclass-support)  \n  [3.5 Deep Learning](https://www.gormanalysis.com/blog/neural-networks-for-your-dog-3-5-deep-learning)  \n  [3.6 Stochastic Gradient Descent](https://www.gormanalysis.com/blog/neural-networks-for-your-dog-3-6-stochastic-gradient-descent)  \n  [3.7 Going Further](https://www.gormanalysis.com/blog/neural-networks-for-your-dog-3-7-going-further)  \n  \n## Related Additional Content\n1. [Python NumPy For Your Grandma](https://www.gormanalysis.com/blog/python-numpy-for-your-grandma)\n2. [Python Pandas For Your Grandpa](https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa)\n3. [Introduction To Google Colab](https://youtu.be/SUCRr56Jzkw)\n\nFeedback appreciated. Cheers", "link": "https://www.reddit.com/r/learnmachinelearning/comments/oowj3b/just_published_my_free_course_on_neural_networks/"}, {"autor": "Nintega94", "date": "2021-07-21 11:54:06", "content": "What AI -----> Image !!!  maker gives the most user control? /!/  I mainly found these machine learning reddits from this page: [https://www.reddit.com/r/MachineLearning/comments/ldc6oc/p\\_list\\_of\\_sitesprogramsprojects\\_that\\_use\\_openais/](https://www.reddit.com/r/MachineLearning/comments/ldc6oc/p_list_of_sitesprogramsprojects_that_use_openais/)\n\n&amp;#x200B;\n\nI still don't know sh!t about any of the programs listed there. Which of them give the most control in terms of what it can generate, to be something super specific? Bonus points if it can look at other images to give a more accurate result (In terms of artstyle, setting, character expression, etc). Trying to do this for indie game development, but I don't have an artist/any money to commission, &amp; my own manual art is complete sh!t, so I've been looking into AI image creation in order to get a solid reference on visual designs for the game", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ooocii/what_ai_image_maker_gives_the_most_user_control/"}, {"autor": "QuasiEvil", "date": "2021-01-02 03:35:48", "content": "Dealing with false positives in -----> image !!!  segmentation (mask-based) /!/ I'm working on something where I'm performing mask-based image segmentation. Basically something akin to the application of taking arial photographs and generating a binary image where building rooftop pixels get labelled 1, and everything else gets labelled 0.\n\nI'm performing some data augmentation where I resize my source images (and their associated labels), in order to generate a wide range of object size examples.  Some of these end up being unrealistically small, and I think because of this, causes false positives on \"other\" small objects in production mode. Now, it would seem there's two ways to deal with this -\n\n1. Keep the small object examples, but set their labels to 0/background. The model would then learn to only predict on objects above a certain size. Or,\n\n2. Keep the small objects and their labels, then just discard object predictions below a certain size in a post-processing step. \n\nI'm just curious if there is any reasoning to why one might be a better approach than the other? Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kopx96/dealing_with_false_positives_in_image/"}, {"autor": "alexandervalkyrie", "date": "2021-01-01 18:31:03", "content": "CNN Help /!/ I've been building a model to take a -----> picture !!!  and predict where the person's nose is in the -----> picture !!! . I tried this first with a 32x32 picture and it worked well, I got 98-99 percent validation. I figured using some well known and powerful architectures like AlexNet and VGG-19 would yield the best results. However when I ran the model, it would get stuck on a certain accuracy and validation accuracy. I'm not really sure how to fix this. I'm assuming that the model is getting stuck because it found a local minimum. Currently I'm using Adam as my optimizer and Mean Squared Error for my loss function. Is there anything that I can do to remedy this? Should I change my architecture? The architectures I used ended with a softmax function but I changed it do a dense function with two units (x,y coords). Could that cause my model to not work? Or should I be focused on altering things such as the learning rate? Code below for reference. \n\n\\# AlexNet\n\nmodel = Sequential()\n\n\\# 1st Convolutional Layer\n\nmodel.add(Conv2D(filters=96, input\\_shape=(224,224,3), kernel\\_size=(11,11), strides=(4,4), padding='valid', activation='relu'))\n\n\\# Max Pooling\n\nmodel.add(MaxPooling2D(pool\\_size=(2,2), strides=(2,2), padding='valid'))\n\n\\# 2nd Convolutional Layer\n\nmodel.add(Conv2D(filters=256, kernel\\_size=(11,11), strides=(1,1), padding='valid',activation='relu'))\n\n\\# Max Pooling\n\nmodel.add(MaxPooling2D(pool\\_size=(2,2), strides=(2,2), padding='valid'))\n\n\\# 3rd Convolutional Layer\n\nmodel.add(Conv2D(filters=384, kernel\\_size=(3,3), strides=(1,1), padding='valid',activation='relu'))\n\n\\# 4th Convolutional Layer\n\nmodel.add(Conv2D(filters=384, kernel\\_size=(3,3), strides=(1,1), padding='valid',activation='relu'))\n\n\\# 5th Convolutional Layer\n\nmodel.add(Conv2D(filters=256, kernel\\_size=(3,3), strides=(1,1), padding='valid',activation='relu'))\n\n\\# Max Pooling\n\nmodel.add(MaxPooling2D(pool\\_size=(2,2), strides=(2,2), padding='valid'))\n\n\\# Passing it to a Fully Connected layer\n\nmodel.add(Flatten())\n\n\\# 1st Fully Connected Layer\n\nmodel.add(Dense(4096, input\\_shape=(224\\*224\\*3,),activation='relu'))\n\n\\# Add Dropout to prevent overfitting\n\nmodel.add(Dropout(0.4))\n\n\\# 2nd Fully Connected Layer\n\nmodel.add(Dense(units=4096, activation='relu'))\n\n\\# Add Dropout\n\nmodel.add(Dropout(0.4))\n\n\\# 3rd Fully Connected Layer\n\nmodel.add(Dense(units=1000,activation='relu'))\n\n\\# Add Dropout\n\nmodel.add(Dropout(0.4))\n\n\\# Output Layer\n\nmodel.add(Dense(2))\n\nmodel.summary() \n\n&amp;#x200B;\n\n\\#VGG-19-----------------------------------------------------------------------------\n\nmodel = Sequential()\n\n\\# 1st Convolutional Layer\n\nmodel.add(Conv2D(filters=64, input\\_shape=(height,width,3), kernel\\_size=(3,3), padding='same',activation='relu'))\n\nmodel.add(Conv2D(filters=64, kernel\\_size=(3,3), padding='same',activation='relu'))\n\nmodel.add(MaxPooling2D(pool\\_size=(2,2), strides=(2,2), padding='same'))\n\nmodel.add(Conv2D(filters=128, kernel\\_size=(3,3), padding='same',activation='relu'))\n\nmodel.add(Conv2D(filters=128, kernel\\_size=(3,3), padding='same',activation='relu'))\n\nmodel.add(MaxPooling2D(pool\\_size=(2,2), strides=(2,2), padding='same'))\n\nmodel.add(Conv2D(filters=256, kernel\\_size=(3,3), padding='same',activation='relu'))\n\nmodel.add(Conv2D(filters=256, kernel\\_size=(3,3), padding='same',activation='relu'))\n\nmodel.add(Conv2D(filters=256, kernel\\_size=(3,3), padding='same',activation='relu'))\n\nmodel.add(MaxPooling2D(pool\\_size=(2,2), strides=(2,2), padding='same'))\n\nmodel.add(Conv2D(filters=512, kernel\\_size=(3,3), padding='same',activation='relu'))\n\nmodel.add(Conv2D(filters=512, kernel\\_size=(3,3), padding='same',activation='relu'))\n\nmodel.add(Conv2D(filters=512, kernel\\_size=(3,3), padding='same',activation='relu'))\n\nmodel.add(Conv2D(filters=512, kernel\\_size=(3,3), padding='same',activation='relu'))\n\nmodel.add(MaxPooling2D(pool\\_size=(2,2), padding='same'))\n\nmodel.add(Conv2D(filters=512, kernel\\_size=(3,3), padding='same',activation='relu'))\n\nmodel.add(Conv2D(filters=512, kernel\\_size=(3,3), padding='same',activation='relu'))\n\nmodel.add(Conv2D(filters=512, kernel\\_size=(3,3), padding='same',activation='relu'))\n\nmodel.add(Conv2D(filters=512, kernel\\_size=(3,3), padding='same',activation='relu'))\n\nmodel.add(MaxPooling2D(pool\\_size=(2,2), padding='same'))\n\nmodel.add(Dense(units=4096,activation='relu'))\n\nmodel.add(Dense(units=4096,activation='relu'))\n\nmodel.add(Dense(units=1000,activation='relu'))\n\nmodel.add(Dense(units=64,activation='relu'))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(2))\n\nmodel.summary()", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kog86g/cnn_help/"}, {"autor": "always-stressed", "date": "2021-01-01 00:51:23", "content": "Art-generating GAN models &amp; artefacts /!/ Hi everyone, \n\nTwo questions\n\n(1) Are there any Art generating GAN model weights anywhere that produce higher quality results? Anything like 256x256 and up? Any sort of art-generating model that I can play around with, bonus if in PyTorch. I've scoured what I can, everyone of them is either outdated and can't be loaded into pytorch (e.g the Fb one from a few years ago using lua-torch that has some read error that hasn't been solved yet) , and the best I found was retraining some model from a medium post (which I'm doing right now) for 128x128 (will try 256 later). \n\n&amp;#x200B;\n\nIn doing so, I'm noticing the artefacts I mention below cropping up pretty frequently.  [https://imgur.com/qbvsu53](https://imgur.com/qbvsu53) (mid-training) you can clearly see the artefacts despite this being a 128x128 -----> image !!! ?\n\n(2) Image artefacts that come up like here [https://imgur.com/a/17otaG4](https://imgur.com/a/17otaG4) I was seeing some of these like gridlines? that appear everywhere. These images have been smoothened to reduce their effect yet they are still visible.  [https://imgur.com/a/5LKQcLz](https://imgur.com/a/5LKQcLz) Clearly if you zoom into the top left there are almost like pixel values that are emerging but within each 'pixel' are the actual pixels that have colour change etc. Are there any post-processing techniques to get rid of them? I'm currently upscaling the image then smoothening it to try to mitigate it. \n\n&amp;#x200B;\n\nTL;DR: (1) Are there any really good pretrained models out there (2) how do I get rid of image artefacts?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ko1uw3/artgenerating_gan_models_artefacts/"}, {"autor": "black0017", "date": "2021-04-16 16:13:01", "content": "[D] - An overview of Unet architectures for semantic segmentation and biomedical -----> image !!!  segmentation /!/ An overview of Unet architectures for semantic segmentation and biomedical image segmentation\n\nA U-shaped architecture consists of a specific encoder-decoder  scheme: The encoder reduces the spatial dimensions in every layer and  increases the channels. On the other hand, the decoder increases the  spatial dims while reducing the channels. The tensor that is passed in  the decoder is usually called bottleneck. In the end, the spatial dims  are restored to make a prediction for each pixel in the input image.  These kinds of models are extremely utilized in real-world applications.  \n\nThis article aims to explore the Unet architectures that stood the test of time.\n\nLink: [https://theaisummer.com/unet-architectures/](https://theaisummer.com/unet-architectures/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ms6in6/d_an_overview_of_unet_architectures_for_semantic/"}, {"autor": "maxpossimpible", "date": "2021-06-16 15:26:42", "content": "Question about which Neural Net is best suited /!/ I'm trying to do -----> image !!!  classification on a snippet of video shots that predicts a certain event. This event is soccer kicks, the image would be composed of small frames vertically stacked into a larger image about 1000x2000 in size and have the information needed to determine ball velocity and direction. And then predict if the kick would miss or hit.\n\nHere's the kicker (pun intended), as I understand it; conv nets aren't really good at this because they mostly try to figure out local features and can't see \"the big picture\". So I'm thinking image classification transformers. \n\nHave I misunderstood conv layers or am I on the right track? Or should I use something completely different such as maybe LSTMs?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o17hj3/question_about_which_neural_net_is_best_suited/"}, {"autor": "HolidayWallaby", "date": "2021-06-16 12:46:41", "content": "How to evaluate classification performance of object detection models? /!/ The common metric for object detection model evaluation is MS COCO style mAP. As far as I understand this only takes into account the bounding boxes and not actually the classifications for these bounding boxes.\n\nHow should we/do others evaluate the classification of these boxes? I assume that the classification can't be for the -----> image !!!  as whole because if there are multiple objects of different classes then you wouldn't know which box corresponds to which classification.\n\nTldr: how can we know that boxes are labelled correctly, i.e. the model knows which box is for which class\n\nI appreciate any pointers, thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o141dj/how_to_evaluate_classification_performance_of/"}, {"autor": "Mjjjokes", "date": "2021-06-16 05:12:21", "content": "How common is it to have thousands of machine learning models in one application? /!/ I want to do audio recognition for thousands of different types of sounds, and output this info in an application. I've concluded that I'll need thousands of different models to do this. I hope that I am overcomplicating things. Is there a more straightforward way to do this? \n\nEither way I will be automating this. To go into more depth, each model will be made by a CNN on mel spectrograms, treating each spectrogram as an -----> image !!! . I've never done anything like this before, so please excuse my noobness. I've only done a few small ml projects, and one dl project while following along to Andrew Ng's famous DL Specialization (the first course). \n\nPlease help! Thanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o0xbnx/how_common_is_it_to_have_thousands_of_machine/"}, {"autor": "sri_anaconda", "date": "2021-06-15 17:05:25", "content": "QUESTION: Deep-Learning Pipelines for Image Data /!/ I've been fascinated by the problem of lane detection in full-scale autonomous vehicle deployment for the past little bit and how we can use **Computer Vision to really recognize lane lines and whatnot.**\n\nThis post is more like a question which I'd really appreciate responses from any ML expert and researcher from.\n\nHas anyone found another type of Neural network that can handle mage data quite well? I've been looking in Spiking Neural Networks and Graph Neural Networks as a DL-based alternative for CNNs but I'm wondering if anyone has **experience in this area and could give me a few tips/points in this** or if anyone knows any **type of neural network that works really well for -----> image !!!  data.**\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/o0j7lw/question_deeplearning_pipelines_for_image_data/"}, {"autor": "gabriel_gicquel", "date": "2021-01-07 09:14:46", "content": "[D] How do you read a DICOM -----> image !!!  in Python? /!/ You can use pydicom, a python package, to read medical data in a Dicom format. First, you need to import the packages and install pydicom in case you did not install it. It\u2019s a Dicom reader and writer for python. Now, upload the images on Python, extract DICOM metadata and generate jpeg from the dicom image field. You should then upload your generated jpegs.\n\nIf you need to read these DICOM images for data annotation purposes like me for example, then you just need to connect to some data annotation tool and create &amp; define the annotation interface.\n\nYou can then use the API to create a project, and upload the images on the project, with the jpeg. \n\nDone! Your data is on the project.\n\nBeing a data scientist, I am eager to know what people out there are using to read a DICOM image in Python. Please share your experience, I would really appreciate it.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ks9p9x/d_how_do_you_read_a_dicom_image_in_python/"}, {"autor": "TheBlackViper_Alpha", "date": "2021-01-07 03:45:00", "content": "How to do GMM on an -----> image !!!  dataset? /!/ I just learned GMM and have an exercise to apply it to CIFAR10 from scratch.  I am a little confused on how to obtain the initial parameters given my dataset. \n\nWhat I know so far:\n\n* In the image dataset say for set of 1000 images with 32x32x3 pixels. Then I would have 3072 features and 1000 samples.\n* To apply GMM I could obtain the mean and variance for each class by getting the average mean and variance which would represent the mean and variance for each gaussian cluster.\n\nWhat I am confused about:\n\n* Most of the examples uses a 2D dataset with each cluster having a predefined center and covariance. How do I obtain these values from my dataset?\n* 2D datasets makes it easy for visualization. In my case however, is a high dimensional dataset. What I thought of doing is just taking the initial two components/feature and plot them. Trying this out I have a lot of overlapping data points and can't seem to find definite clusters. However I believe this is not the correct way of representing my initial data. What would be the 'right' way of doing this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ks4v5l/how_to_do_gmm_on_an_image_dataset/"}, {"autor": "Azelais", "date": "2021-10-04 09:24:55", "content": "Advice on clustering images of landscapes /!/ Hi guys, I'm new to this and kind of self teaching as I go. Any help appreciated! :)\n\n\nBasically, I have a lot (~15,000) of unlabeled images of arctic landscapes, and I'm trying to label the images based on the amount of snow on the ground. Some of the pictures aren't great (pure black or white usually), so I'm aiming for four possible labels: bad -----> image !!! , full snow coverage, partial snow coverage, no snow coverage.\n\n\nI first ran a sample set of the images (~600) through the VGG16 model, extracted their features, and used kmeans clustering. This worked fairly well, but not perfectly; there are still more images miscategorized than I'd like. I tried DBSCAN and spectral clustering, but they didn't work either.\n\nI'm interested in trying a cluster then label semi-supervised approach like described [here](https://bdtechtalks.com/2021/01/04/semi-supervised-machine-learning/), and I was able to run my data through VGG16, extract features, cluster, identify representatives, and manually label them, but I'm not sure how to train the model on the labeled images.\n\nAny advice welcome :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q11yrj/advice_on_clustering_images_of_landscapes/"}, {"autor": "yyuummE", "date": "2021-10-04 06:25:57", "content": "how to create a website like 'this person does not exist' (not the gan part, the -----> image !!!  display) /!/ Hello, I have a few StyleGAN models that i've trained and I would like a way to display images generated from them online so that users can click through unique images like on the 'this person does not exist' site. \n\nI have only a bit of web/coding experience. does the site store a bunch of images and display a new one for every refresh or is the model being accessed every time the site is used? \n\nWhen i google search, i get explanations about the gan rather than how the website is made. If you could point me in the right direction/towards some documentation, I would really appreciate it.\n\nthanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q0zwiv/how_to_create_a_website_like_this_person_does_not/"}, {"autor": "Edulad", "date": "2021-10-03 14:00:01", "content": "Find Similar -----> Image !!! s using -----> Image !!!  Hash /!/ Hi, so i have written a script to find two similar images using image hash module, but how can i use it to find more than two images ?\n\n**Here is my CODE and the Images i Used.**\n\n`from PIL import Image`\n\n  \n`import imagehash`  \n`import cv2`  \n`import matplotlib.pyplot as plt`  \n`img1 =` [`Image.open`](https://Image.open)`(\"116.jpg\")`\n\n  \n`img2 = Image.open(\"117.jpg\")`  \n`I1 = imagehash.average_hash(img1)`\n\n  \n`I2 = imagehash.average_hash(img2)`  \n`print(I1)`\n\n`print(I2)`  \n`print(\"Similarity=\",I1 - I2)`\n\n&amp;#x200B;\n\n[116](https://preview.redd.it/ummxva1er8r71.jpg?width=476&amp;format=pjpg&amp;auto=webp&amp;s=304a48e16f53fe0d245b1435d246ae74120cb8e0)\n\n&amp;#x200B;\n\n[117](https://preview.redd.it/0vuf3d6fr8r71.jpg?width=476&amp;format=pjpg&amp;auto=webp&amp;s=f183a38edb887fd0f9e30de892723494737a8e81)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q0iaji/find_similar_images_using_image_hash/"}, {"autor": "EffectiveBigED", "date": "2021-10-03 11:51:49", "content": "ML -----> image !!!  classification for security reasons etc? any examples? pros/cons? /!/ Basically, I have a course where I need to.. write.. about something.. and that something I have chosen is ML image classification, with security reasons in for example Industries,smart homes, IOT, industry 4.0 etc.\n\nBasically, the pros/cons of having image classification instead of doing the manual work.. how it works..\n\nDoes anyone have some example ? Background about ML/DL for example: TensorFlow should work right? with a webcam even.\n\nThe thing is that I'm just abit lost because what are some cons of having for example ml image classification?  I'm sure one of the hardest parts is getting proper data to be used for the learning..", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q0g8pk/ml_image_classification_for_security_reasons_etc/"}, {"autor": "BigDaddy_in_the_Bus", "date": "2021-10-03 05:14:15", "content": "Need some help /!/ Umm so i am trying to create a project using opencv, well its just a thought for now, idk how hard its gonna be to make it work since I am still a beginner in machine learning and just started reading opencv. \n\nBut is it possible to train a model with the pixels of an -----> image !!!  as the dataset and then I suppose I crop out 1/3 of the image and let the model fill it in with pixels? Is this possible without deep learning?\n\nThis is way beyond my knowledge but I just wanna know is it going to work? If someone could help me out pleasee", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q0bat2/need_some_help/"}, {"autor": "DavidUpInHere", "date": "2021-10-03 04:03:02", "content": "Library to generate variants of labeled images? /!/ Hello,\n\nI am looking for a library to generate variants of labeled images, that use YOLOv5 annotations.  Roboflow can do this, but only supports 3 variants per -----> image !!!  for the free tier, and beyond that costs $1000 for a paid plan.  Some operations I would like to perform are vertical and horizontal flipping, and adding some random noise.  \n\nThank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/q0aap0/library_to_generate_variants_of_labeled_images/"}, {"autor": "throwawayafw", "date": "2021-10-02 08:05:10", "content": "-----> Image !!!  augmentation- How do I set the steps per epochs for training set if I have only 1000 images between two classes in training folder? /!/ I just can't understand how augmentation is done prior feeding into the model. I have been using Google Colab and I need to augment the 1000 images in my training folder to about 5000 images.\nHow do I do that? Here's the code I'm working with.\nhttps://imgur.com/gallery/juws3iQ", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pzr0jc/image_augmentation_how_do_i_set_the_steps_per/"}, {"autor": "BigDaddy_in_the_Bus", "date": "2021-10-02 06:34:57", "content": "-----> Image !!!  Analysis using machine learning? /!/ Hey so I have been learning ML for a while, but I am still a newbie. I want to make my first project, related to image analysis (some simple pattern recognition maybe?) as I have some knowledge in digital image processing. \n\nThe thing is I am still not familiar with all the algorithms and aspects of machine learning and all that I maybe requiring for this project. So I am not entirely sure the scope of this project but I am trying to learn the essentials. I'm trying to learn OpenCV now. I don't want to get into deep learning.\n\nIf anybody has some idea on what kinda project I can make, or what modules, algorithms I might be requiring for this, any good articles or tutorials, anything, it would be really helpful.\n\nThanks :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/pzpxyr/image_analysis_using_machine_learning/"}], "name": "Subreddit_learnmachinelearning_01_01_2021-01_11_2021"}
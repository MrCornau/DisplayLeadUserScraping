{"interestingcomments": [{"autor": "softcompanyuk", "date": "2021-04-01 10:44:48", "content": "-----> Image !!!  Classification Using Model Builder (ML.NET Model Builder)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/mhsm7w/image_classification_using_model_builder_mlnet/"}, {"autor": "chicagobob2", "date": "2021-03-31 21:44:05", "content": "Talking machines /!/  \n\n2. Understanding Language \n\nLanguage is Process\n\nUnderstanding language (not \u201d a\u201d language) means understanding the processes that produce it. Language really only exists as production incidents and there records.              By understanding language here we mean essentially understanding linguistic behavior. In particular we mean the processes of linguistic behavior both on the generative side and the reception side. There is no language independent of these processes. There may be records of linguistic behavior -everything from marks on parchment to digital recordings- but language itself exists only as process and the records of these processes . The question is : How do we best understand these processes? How do we know that understanding language does in itself involve linguistic processes? Is talking about talking the way to proceed?\n\nConsider a different kind of system - an open differential- a device which transfers torque from say an engine to the rear wheels of a car. In a rear wheel drive car the input shaft turns the pinion gear-generally through a flexible coupling like a universal joint- which engages the ring gear to cause it to rotate. The ring gear is mounted on a rotating  housing which contains the differential gears- spyder gears- two of which are splined to the output shafts which actually turn the wheels. The differential action consists in the ability of the differential housing to turn either of the output shafts separately or both together. This is because the housing can rotate around either spyder gear if one is held fixed by external resistance to the rotation of the connected output shaft, i.e. if one wheel is stuck while the other is free to rotate.\n\nThis is obviously a ridiculous explanation in that no one can follow it or understand it even if familiar with the terms if they don\u2019t already understand the mechanism . If you want to understand a differential find a -----> picture !!!  of one to see how it goes together and see if you can imagine how the differential action actually works. To really understand it you need an animation or a physical demonstration with an actual vehicle up on a lift. Some things don\u2019t admit to verbal  description or verbal explanations. This idea is absolutely unknown in philosophy and perhaps in other disciplines as well. You can never understand some things through language even though it seems as though you can describe them adequately. In engineering we often say: \u201c you can\u2019t explain it you have to see it..\u201d Notice that someone who reads the description can answer questions about the device- \u201cWhat turns the ring gear?\u201d for example. But just being able to talk about something doesn\u2019t really mean you understand it. Nor will talk alone ever allow a real comprehension of some systems.\n\nProcesses are like this, consider for example the phenomenon of turbulent flow in fluid dynamics. You can watch a flow go turbulent and describe what it looks like but you won\u2019t really understand what is happening. Complicated systems where many things are going on at once are similarly inexplicable with any  reasonable amount of linguistic behavior and may yield more readily to graphs, diagrams or animations. Linguistic  processes are like this. Language is not something you talk about, language is something you \u201dsee\u201d You understand linguistic behavior through the representations of the processes involved. Unfortunately we lack  a clear understanding of the neurological processes that generate linguistic behavior- we can\u2019t \u201d see it\u201d yet. .All we can really talk about is the information the brain uses to produce linguistic behavior. \n\nPatterns of neurological activity, generally though perhaps not exclusively consisting of action potentials, are the information that the brain uses to produce language. We propose to call these things \u201cdriving functions\u201d because they drive, i.e. produce, linguistic behavior. Understanding language as process thus  reduces to understanding driving functions. But note that understanding language as process does not mean we understand meaning. Meaning is for communication, processes are for psychology and philosophical analysis. You don\u2019t have to know what an utterance means to know how it is produced, to know the information that causes it\u2019s production.\n\nThat there are driving functions is incontestable, this is why language exists, why communication is possible. If there where no consistent causal processes behind linguistic behavior than there would be no words there would only  be empty sounds. If on numerous occasions when I see a cat I say \u201ccat\u201d there is some sort of repetitive  process going in me that produces this consistent result. This is the driving function for the word \u201ccat\u201d. Now neither the behavior nor the process that produces it will be the exactly the same in every occurrence, the nervous system and related musculature are too complicated for that. A driving function is a variable process, the trick is to figure out the constant elements .in the process.  For simple \u201csee-say\u201d activities the constant element  is a input from the visual system for other types of linguistic behavior the constant elements are not so obvious.\n\nThe problem with AI produced linguistic behavior is that these systems cannot have the same driving functions behind their output as human beings do because they lack the hardware that humans have.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/mhgiua/talking_machines/"}, {"autor": "hyunwoongko", "date": "2021-02-28 04:00:00", "content": "[P] Talk with AI! (Easy to use, DialoGPT) /!/  \n\nhttps://preview.redd.it/h765un3o65k61.png?width=1477&amp;format=png&amp;auto=webp&amp;s=76860985a9033d97885fe1f1e0a0d752fb11e7ce\n\n&amp;#x200B;\n\nHello. I'm bored with nothing to do on the weekend, so I developed a package that makes it easy to access the DialoGPT-based open domain chatbot and wrote. (It can be easily installed as a 'pip install dialog-chat'.)\n\n&amp;#x200B;\n\nAs shown in the -----> picture !!! , three lines of code allow you to communicate with artificial intelligence and provide various options changes and automatic history management. If you want to talk to artificial intelligence, install it and use it! More information can be found here at [https://github.com/hyunwoongko/dialogpt-chat](https://github.com/hyunwoongko/dialogpt-chat)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/lu58lj/p_talk_with_ai_easy_to_use_dialogpt/"}, {"autor": "justTwoOfUs1989", "date": "2021-02-27 18:46:46", "content": "AI in home PC [CSI Miami like] /!/ Hello guys, I have some wicked idea to create or try to create an artificial intelligence (or maybe to  implement core of ai from other project) and to create something similar to Alexa/GoogleHome\n\nwhat I'm trying to achieve is that when I tell to computer:\"Computer, find all references for Paris\", at that moment computer stacks all information's (like cyber-movie ui/ux) all the data from some db\n\nfor example -----> photo !!!  album from Paris, movie Paris that I have as collection in my plex, tickets that I bought from my gmail..etc\n\nso, first step is obviously to figure it out how to feed my db with every single day with my activities. And is there something similar already on the market? \n\n[https://www.google.com/imgres?imgurl=https%3A%2F%2Fmedia.wired.com%2Fphotos%2F59095d6fd8c8646f38eef7b2%2Fmaster%2Fw\\_2500%2Ch\\_1406%2Cc\\_limit%2FCode.jpg&amp;imgrefurl=https%3A%2F%2Fwww.wired.com%2F2015%2F05%2Fcsi-cyber-tech-talk%2F&amp;tbnid=B2I1jZ2gn3YxOM&amp;vet=12ahUKEwi694H624rvAhVEHBoKHTzGCQsQMygVegUIARDFAQ..i&amp;docid=KZjbLHZ8hh8D1M&amp;w=2500&amp;h=1406&amp;q=csi%20miami%20computer%20screens&amp;ved=2ahUKEwi694H624rvAhVEHBoKHTzGCQsQMygVegUIARDFAQ](https://www.google.com/imgres?imgurl=https%3A%2F%2Fmedia.wired.com%2Fphotos%2F59095d6fd8c8646f38eef7b2%2Fmaster%2Fw_2500%2Ch_1406%2Cc_limit%2FCode.jpg&amp;imgrefurl=https%3A%2F%2Fwww.wired.com%2F2015%2F05%2Fcsi-cyber-tech-talk%2F&amp;tbnid=B2I1jZ2gn3YxOM&amp;vet=12ahUKEwi694H624rvAhVEHBoKHTzGCQsQMygVegUIARDFAQ..i&amp;docid=KZjbLHZ8hh8D1M&amp;w=2500&amp;h=1406&amp;q=csi%20miami%20computer%20screens&amp;ved=2ahUKEwi694H624rvAhVEHBoKHTzGCQsQMygVegUIARDFAQ)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n[Like this transparent window, and then stacked with all relevant data](https://preview.redd.it/04leok3xf2k61.png?width=1600&amp;format=png&amp;auto=webp&amp;s=091d984b7bf08547f566749d84e34df41aa3afec)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ltukyd/ai_in_home_pc_csi_miami_like/"}, {"autor": "hackernoon", "date": "2021-01-30 18:48:32", "content": "Introducing DALL\u00b7E: Inspired by GPT-3 and -----> Image !!! -GPT from OpenAI", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/l8sjml/introducing_dalle_inspired_by_gpt3_and_imagegpt/"}, {"autor": "Internal_Start_1567", "date": "2021-01-30 13:04:00", "content": "Here is a -----> picture !!!  I tweeted to Elon Musk but it may not get through. Any advice would be greatly appreciated. Feel free to private message me. I just can\u2019t stand doing what I do now, but I can\u2019t leave due to having to pay for bills.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/l8l9zo/here_is_a_picture_i_tweeted_to_elon_musk_but_it/"}, {"autor": "DrArti", "date": "2021-01-30 02:07:52", "content": "Hi all, /!/ Does anyone know any good freelancers good at Python? \n\nTrying to do some simple -----> image !!!  recognition and machine learning task\n\nThanks", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/l8b4vy/hi_all/"}, {"autor": "AnalystDesperate", "date": "2021-01-27 18:42:27", "content": "The Feasibility of Image Generation with Artificial Intelligence Survey /!/ Hello! My name is Oscar, and I'm a student studying Artificial Intelligence. This survey is designed to test the apparent validity of images generated from a neural network. Some faces have been generated on a network that was trained for 12 hours while some faces have been generated on the final StyleGAN2 network. (trained for many days). In the survey, you will be presented with 40 choices between 2 different faces. Please quickly choose the face that you believe is a -----> photo !!!  of an actual human, and continue onto the next question.\n\nThe data I collect from this survey will not be shared anywhere and is solely for use in my research paper.\n\n[https://docs.google.com/forms/d/e/1FAIpQLSdAGsGkitWc4\\_iSQOlzAtInTU03Q1QWKxolzz1071MLiVBXzA/viewform?usp=sf\\_link](https://docs.google.com/forms/d/e/1FAIpQLSdAGsGkitWc4_iSQOlzAtInTU03Q1QWKxolzz1071MLiVBXzA/viewform?usp=sf_link)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/l6bh37/the_feasibility_of_image_generation_with/"}, {"autor": "shreyansh26", "date": "2021-01-26 14:46:13", "content": "Tutorial on deploying Deep Learning models in the browser using Tensorflow.js, WebDNN, and ONNX.js (Includes code) /!/ [https://shreyansh26.github.io/post/2021-01-25\\_deep\\_learning\\_in\\_the\\_browser/](https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/)\n\nI wrote a tutorial on how one can use frameworks like Tensorflow.js, WebDNN, and ONNX.js to deploy their Deep Learning models directly in the browser, i.e. no server communication. I have used a simple pre-trained -----> Image !!!  classification model to demonstrate it. The project can serve as a boilerplate code for starting out on deploying models in such a manner on your own.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/l5feim/tutorial_on_deploying_deep_learning_models_in_the/"}, {"autor": "OnlyProggingForFun", "date": "2021-05-02 11:27:43", "content": "Infinite Nature: Fly into an -----> image !!!  and explore it like a bird!", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/n34f7w/infinite_nature_fly_into_an_image_and_explore_it/"}, {"autor": "OnlyProggingForFun", "date": "2021-05-01 14:09:01", "content": "Infinite Nature: Fly into an -----> image !!!  and explore it like a bird!", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/n2j1yu/infinite_nature_fly_into_an_image_and_explore_it/"}, {"autor": "djamesmck57", "date": "2021-06-30 14:42:31", "content": "Enable your webcam to guard your posture with Zen\ud83e\uddd8\ud83c\udffd\u200d\u2640\ufe0f /!/ \ud83d\udc4bFam, [Daniel](https://www.linkedin.com/in/daniel-j-1b52a2111/) and [Alex](https://www.linkedin.com/in/ion-alexandru-secara/) here! Would love your feedback on [Zen!](https://www.yayzen.com/yc).\n\nProblem:\n\nPosture impacts the way we think, look, feel, and act. Unfortunately, most of us spend the majority of our workdays hunched over a computer, leading to back and joint pain, headaches, and even long-term heart disease.\n\nHow [Zen](https://www.yayzen.com/yc) Can Help:\n\n* Zen mirrors your posture in real-time and will **alert you when you've been slouching** for an extended period of time with a visual and/or audio notification. The desktop app uses your -----> camera !!!  to monitor your posture, without storing or recording any video.\n* Zen reminds you to **stand up and take walk breaks** so that you reduce screen fatigue and increase blood flow.\n* Zen offers quick personalized **stretches for back and joint care.**\n\nWho's building [Zen](https://www.yayzen.com/yc):\n\n* Daniel went from playing football and doing splits at Yale University to working with terrible low back pain and carpal tunnel for over 8 hours a day from working hunched over his computer at Adobe.\n* Alex has spent time in software engineering roles at Adobe, Intuit, and has spent the majority of his life coding at his computer with poor posture, leading to a number of health issues.\n* We met each other a few years ago working in San Francisco and eventually came together to build Zen alongside top ergonomists and physical therapists.\n\nHow you can help:\n\n* [Check out Zen for yourself!](https://www.yayzen.com/yc) Free for this community! We would love your feedback!\n\n&amp;#x200B;\n\nThank you!\n\nAlex &amp; Daniel", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/oayqd9/enable_your_webcam_to_guard_your_posture_with_zen/"}, {"autor": "CatalyzeX_code_bot", "date": "2021-07-31 06:54:37", "content": "State of the art in -----> image !!!  segmentation: Automatically segment -----> image !!! s into meaningful regions without human supervision!!", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ov2im6/state_of_the_art_in_image_segmentation/"}, {"autor": "techsucker", "date": "2021-08-31 23:35:53", "content": "Google AI Introduces \u2018Omnimattes\u2019, A New Computer Vision Research Approach to Matte Generation using Layered Neural Rendering /!/ In -----> image !!!  and video editing, accurate mattes are important to separate foreground from background. However, real-world scenes often have shadows or smoke that may affect the processing of such images, but computer vision techniques generally ignore these scene effects.\n\nGoogle researchers\u00a0[presented their new approach to matte generation at CVPR 2021](https://arxiv.org/pdf/2105.06993.pdf). Their method separates video into layers called omnimattes and extracts details such as shadows associated with the subjects in a scene. A state-of-the-art segmentation model can extract masks for people, but not all of the effects related to them, like shadows on the ground. The proposed method can isolate and extract additional details associated with the subjects, such as shadows cast on the ground.\n\n[5 Min Read](https://www.marktechpost.com/2021/08/31/google-ai-introduces-omnimattes-a-new-computer-vision-research-approach-to-matte-generation-using-layered-neural-rendering/) | [Paper](https://arxiv.org/pdf/2105.06993.pdf)| [Google Blog](https://ai.googleblog.com/2021/08/introducing-omnimattes-new-approach-to.html)\n\n&amp;#x200B;\n\n*Processing video xkaqdz774sk71...*", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pfhn1j/google_ai_introduces_omnimattes_a_new_computer/"}, {"autor": "Roman_Ghost", "date": "2021-08-31 19:27:24", "content": "Google's AI turning a low res -----> image !!!  into a high res -----> image !!! ", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pfcxua/googles_ai_turning_a_low_res_image_into_a_high/"}, {"autor": "CtrlGenWorkshop", "date": "2021-08-31 03:15:31", "content": "CtrlGen Workshop at NeurIPS 2021 (Controllable Generative Modeling in Language and Vision) /!/ We are holding a controllable generation workshop at NeurIPS 2021! It aims to explore disentanglement, controllability, and manipulation for the generative vision and language modalities. We feature an exciting lineup of speakers, a live QA and panel session, interactive activities, and networking opportunities. See our website below for more! We are also inviting both paper and demo submissions related to controllable generation (read further for details).\n\n**Workshop Website:** [https://ctrlgenworkshop.github.io/](https://ctrlgenworkshop.github.io/)\n\n**Contact:** [ctrlgenworkshop@gmail.com](mailto:ctrlgenworkshop@gmail.com)\n\n**Important Dates**\n\n* Paper Submission Deadline: ***September 27, 2021***\n* Paper Acceptance Notification: October 22, 2021\n* Paper Camera-Ready Deadline: November 1, 2021\n* Demo Submission Deadline: ***October 29, 2021***\n* Demo Acceptance Notification: November 19, 2021\n* Workshop Date: ***December 13, 2021***\n\n**Submission Portal (Papers + Demos):**  [https://cmt3.research.microsoft.com/CtrlGen2021/Submission/Index](https://cmt3.research.microsoft.com/CtrlGen2021/Submission/Index)\n\n&amp;#x200B;\n\n**Full Call for Papers:** [h](https://ctrlgenworkshop.github.io/CFP.html)[ttps://ctrlgenworkshop.github.io/CFP.html](https://ctrlgenworkshop.github.io/CFP.html)\n\nPaper submission deadline: ***September 27, 2021***. Topics of interest include:\n\n**Methodology and Algorithms:**\n\n* New methods and algorithms for controllability.\n* Improvements of language and vision model architectures for controllability.\n* Novel loss functions, decoding methods, and prompt design methods for controllability.\n\n**Applications and Ethics:**\n\n* Applications of controllability including creative AI, machine co-creativity, entertainment, data augmentation (for [text](https://arxiv.org/abs/2105.03075) and [vision](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0)), ethics (e.g. bias and toxicity reduction), enhanced training for self-driving vehicles, and improving conversational agents.\n* Ethical issues and challenges related to controllable generation including the risks and dangers of deepfake and fake news.\n\n**Tasks (a few examples):**\n\n* [Semantic text exchange](https://aclanthology.org/D19-1272/)\n* [Syntactically-controlled paraphrase generation](https://arxiv.org/abs/1804.06059)\n* [Persona-based text generation](https://aclanthology.org/W19-3402/)\n* Style-sensitive generation or style transfer (for [text](https://arxiv.org/abs/2011.00416) and [vision](https://github.com/ycjing/Neural-Style-Transfer-Papers))\n* -----> Image !!!  synthesis and scene representation in both 2D and 3D\n* Cross-modal tasks such as controllable image or video captioning and generation from text\n* New and previously unexplored controllable generation tasks!\n\n**Evaluation and Benchmarks**\n\n* New and improved evaluation methods and metrics for controllability\n* Standard and unified metrics and benchmark tasks for controllability\n\n**Cross-Domain and Other Areas**\n\n* Work in interpretability, disentanglement, robustness, representation learning, etc.\n\n**Position and Survey Papers**\n\n* For example, exploring problems and lacunae in current controllability formulations, neglected areas in controllability, and the unclear and non-standardized definition of controllability\n\n&amp;#x200B;\n\n**Full Call for Demonstrations:** [https://ctrlgenworkshop.github.io/demos.html](https://ctrlgenworkshop.github.io/demos.html)\n\nSubmission deadline: ***October 29, 2021***. Demos of all forms: research-related, demos of products, interesting and creative projects, etc. Looking for creative, well-presented, and attention-grabbing demos. Examples include:\n\n* Creative AI such as controllable poetry, music, image, and video generation models.\n* Style transfer for both text and vision.\n* Interactive chatbots and assistants that involve controllability.\n* Controllable language generation systems, e.g. using GPT-2 or GPT-3.\n* Controllable multimodal systems such as image and video captioning or generation from text.\n* Controllable image and video/graphics enhancement systems.\n* Systems for controlling scenes/environments and applications for self-driving vehicles.\n* Controllability in the form of deepfake and fake news, specifically methods to combat them.\n* And much, much more\u2026\n\n&amp;#x200B;\n\n**Organizing Team:**\n\n* [Steven Feng](https://styfeng.github.io/) (CMU)\n* [Anusha Balakrishnan](https://www.microsoft.com/en-us/research/people/anbalak/) (Microsoft Semantic Machines)\n* [Drew Hudson](https://cs.stanford.edu/people/dorarad/) (Stanford)\n* [Tatsunori Hashimoto](https://thashim.github.io/) (Stanford)\n* [Dongyeop Kang](https://dykang.github.io/) (UMN)\n* [Varun Gangal](https://vgtomahawk.github.io/) (CMU)\n* [Joel Tetreault](https://www.cs.rochester.edu/~tetreaul/academic.html) (Dataminr)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pexn9x/ctrlgen_workshop_at_neurips_2021_controllable/"}, {"autor": "Wellthenlmao", "date": "2021-08-31 02:43:33", "content": "Need help IDing this AI -----> image !!!  generator", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pex4ng/need_help_iding_this_ai_image_generator/"}, {"autor": "techsucker", "date": "2021-08-30 07:15:52", "content": "NVIDIA and Tel Aviv Researchers Propose \u2018StyleGAN-NADA\u2019, A Text-Driven Method That Converts a Pre-Trained AI Generator to New Domains Using Only a Textual Prompt and No Training Data /!/ GANs have revolutionized the -----> image !!!  generation process, allowing for better results in classification and regression tasks. They can capture distribution of images through their semantic-rich latent space making it more efficient than traditional methods such as autoencoders or generative adversarial networks.\n\nAlthough GANs have shown incredible results, their ability to generate high-quality images is limited by the amount of training data available. For example, it would be challenging to train a model for an artist that has not been recognized yet or when there isn\u2019t sufficient information about what this imaginary scene looks like.\n\n[**5 Min Read**](https://www.marktechpost.com/2021/08/30/nvidia-and-tel-aviv-researchers-propose-stylegan-nada-a-text-driven-method-that-converts-a-pre-trained-ai-generator-to-new-domains-using-only-a-textual-prompt-and-no-training-data/) **|** [**Paper**](https://arxiv.org/pdf/2108.00946.pdf) **|** [**Project**](https://stylegan-nada.github.io/) **|** [**Code**](https://github.com/rinongal/StyleGAN-nada)\n\n&amp;#x200B;\n\n![video](howykwoa4gk71)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ped3cm/nvidia_and_tel_aviv_researchers_propose/"}, {"autor": "techsucker", "date": "2021-09-30 18:46:32", "content": "Google AI\u2019s New Study Enhance Reinforcement Learning (RL) Agent\u2019s Generalization In Unseen Tasks Using Contrastive Behavioral Similarity Embeddings /!/ Reinforcement learning (RL) is a field of machine learning (ML) that involves training ML models to make a sequence of intelligent decisions to complete a task (such as robotic locomotion, playing video games, and more) in an uncertain, potentially complex environment.\n\nRL agents have shown promising results in various complex tasks. However, it is challenging to transfer the agents\u2019 capabilities to new tasks even when they are semantically equivalent. Consider a jumping task in which an agent, learning from -----> image !!!  observations, must jump over an obstacle. Deep RL agents who have been taught a handful of these tasks with varied obstacle positions find it difficult to jump over obstacles in previously unknown locations.\n\n# [5 Min Read](https://www.marktechpost.com/2021/09/30/google-ais-new-study-enhance-reinforcement-learning-rl-agents-generalization-in-unseen-tasks-using-contrastive-behavioral-similarity-embeddings/) | [Paper](https://arxiv.org/pdf/2101.05265.pdf) | [Project](https://agarwl.github.io/pse/) |[Github](https://github.com/google-research/google-research/tree/master/pse) | [Slides](https://agarwl.github.io/pse/pdfs/slides.pdf)\n\n&amp;#x200B;\n\nhttps://i.redd.it/rihd771vroq71.gif", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pyq5n1/google_ais_new_study_enhance_reinforcement/"}, {"autor": "IamDakaru", "date": "2021-09-30 16:18:46", "content": "Student in need of help /!/ Hello, \n\nI am currently in my last year of school, and I'm doing an essay on Artificial Intelligence, more specifically, Art and Artificial Intelligence.  I don't know much about AI at the moment, but I am trying to learn. \n\nI am trying to show, that AI is capable of creating images, in the exact style of an artist. Does anyone know, where I can find an AI or a Tablet I think it's called, that would let me feed in Paintings, and that could then give out new Paintings in the same Style, maybe taking a real -----> image !!!  or something like that as a reference?\n\nThanks for your help. C:", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pyn671/student_in_need_of_help/"}, {"autor": "IngenuityDiligent124", "date": "2021-09-29 13:31:51", "content": "I am doing my final year project on Image super resolution and -----> image !!!  enhancement. But I'm not quite sure how to go about it. Any tips? [P]", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pxvgjd/i_am_doing_my_final_year_project_on_image_super/"}, {"autor": "botdorfjohn", "date": "2021-03-16 20:07:18", "content": "The Future of Artificial Intelligence /!/   \n\n**What is Artificial Intelligence?** **Where is it Going and what impact will AI have on future Marketing Models?**   \n\nBy John C Botdorf \n\n**Author Observations on Defining AI.** As I began to ponder the world of AI, I soon felt like this subject was too vast to just whip up a Blog Post without organizing a deeper dive into the world\u2019s brightest minds. So many questions, myths and facts exist today about what AI is and is not, I felt compelled to blog up on this subject but needed a better approach. Although I possess a cursory knowledge on this academic topic following years of observation, grad studies, funding software companies, and technology writing, it just did not seem like enough. I felt compelled to put down the pen (OK keyboard) and just stop. \n\nAs a self- recovering Type-A personality who is use to researching complex topics, I wanted to offer more than just a summary post advocating how AI will change your life. Most people assume the evolving AI model will soon tell you what and how to do something better and will just make a company or person look smarter and make better choices. After all, why would that even be news, isn\u2019t that what AI is supposed to do? \n\nBack to the research lab on this one. I soon checked out every book I could find on AI at the local libraries, started a deeper web search campaign, took copious notes, and talked to more experts in multiple technology fields that revolve around AI. I also read everything \u201cnew\u201d I could find on the subject. Caution here: In the world of AI, six months is outdated. I now have enough stored data and notes to write a 4th book on why and how AI is in fact here to stay and what this science will and will not do over the next few years. Under full disclosure I mean writing my 4th book not my 4th book on AI. \n\n**AI is Already creating Big Wins** Make no mistake about the power of today\u2019s artificial Intelligence. For example, JP Morgan Chase introduced a system for reviewing commercial loan contracts that use to take loan officers 360,000 hours to review. This is now done in mere seconds. Other \u201csupervised\u201d versions of AI can now diagnose skin cancer at accuracy rates at or exceeding human diagnosis capabilities. (Harvard Business Review Press, AI, 2019). \n\nThese \u201cCognitive\u201d forms of AI operating under supervised environments are proving to be more accurate and can save thousands of human hours (not to mention lives) to produce vastly improved results when operating across large scale data environments crunching similar or recognized data samples. The news here is that this type of AI is now able to penetrate much larger amounts of data beyond 40M sample sizes to arrive at both quantitative and qualitative output decisions. Once more, machines do not require 401(k\u2019s) or health plans or human benefits. They may however learn to demand faster processing and cooler operating environments.  \n\nThe former version of AI (supervised machine learning or ML) is somewhat limited by the rules of engagement whereas the latter goal (unsupervised ML) is geared toward teaching a machine to \u201clearn how to learn\u201d. A key difference here is the first method generally follows the rules it is given, whereas an unsupervised logical AI platform is free to roam the Clouds in search of finding building blocks or critical data paths to improve solution sets and thereby actually learn \u201chow to think\u201d. No question as the world unleashes the second wave of AI and prepares for the third wave, there are plenty of challenges to overcome for a machine to better mimic the human brain. Alas, however, as John McCarthy, a noted math professor at Dartmouth University predicted in 1955, AI will be game changing as advancements in Machine Learning begin to create building blocks for higher learning further accelerating Machine knowledge. \n\nUnfortunately for AI proponents looking to live in the Jetson Age, this took far longer than first envisioned, however AI is now very real and advancing much faster, in part because computer processing is also much faster. Those AI projects focusing on redundant tasking using a well-conceived rules-based infrastructure profile are already seeing impressive ROI\u2019s on their earlier AI investment.  Most industry AI projections including all types of AI platforms are forecasting growth rates in excess of 33% over the next several years.\n\nThose firms heavily invested in managing Big Data like Google, Facebook, IBM, and Microsoft that have been making substantial AI investments over the past decade are now far more likely to lead the way into the next wave of AI advancements.  In an article by Mark Knickrehm on how AI will change work environments, he refers to a recent survey by Accenture asking 1,200 C-Level Executives worldwide what are they doing with AI. \n\nThe response proves how CEO\u2019s are committing to AI projects. \u201c75% say they are currently accelerating investments in AI and other intelligent technologies and 72% say they are responding to a competitive imperative to keep up with rivals by improving productivity and by finding new sources of growth\u201d (Knickrehm, 2019). We further note Grandview Research and their work on AI revenue which is forecast to surpass $733 Billion by 2027.\n\n  \n\n**Projected Revenue from Artificial   Intelligence- Market Report Scope**\n\n \n\nMarket   size value in 2020\n\n&amp;#x200B;\n\nUSD   62.4 billion\n\n \n\nRevenue   forecast in 2027\n\n&amp;#x200B;\n\nUSD   733.7 billion\n\n \n\nGrowth   Rate\n\n&amp;#x200B;\n\nCAGR   of 42.2% from 2020 to 2027\n\n \n\nBase   year for estimation\n\n&amp;#x200B;\n\n2019\n\nSource: Grandview Research, 2020\n\n**Where does AI begin and what is the AI End Game?** Mathematician\u2019s understand that under proper guidance, companies have used proven multiple regression models for decades to plot financial relationships, optimize things like warranties and cost probabilities, and have used quantitative analysis modeling to stratify pricing options. Even simple equations like what combination of pairing pizza and chicken wings with a one-liter bottle of soda will make the most money have used mathematical modeling for many decades with great success. \n\nWhile these methods may seem like AI to some and these proven techniques can produce impressive results, most AI Engineers know there can be a distinct line in the sand between what a well-conceived AI platform can do with years of machine training and what a simple algorithm or Linear Regression model can produce given enough relevant data input. What then is the difference between quantitative analysis and AI? Let\u2019s look at how the Webster Dictionary defines Artificial Intelligence. \n\n## Definition of artificial intelligence\n\n**1:** a branch of computer science dealing with the simulation of intelligent behavior in computers\n\n**2:** the capability of a machine to imitate intelligent human behavior\n\n(Webster.com, 2021).\n\nAI purists would not likely concede these notions to define AI. This is because AI as of yet cannot really \u201csimulate\u201d intelligent behavior if this is comparing intelligence to the human brain. Second, AI can imitate human behavior in some ways (cognitive recognition) and do it far better than non-AI solutions sets but is a long way off from doing so in other ways like implementing impulse decisions or incorporating unconventional thinking. \n\nWhy would a coach paid millions of dollars a year elect to go for a first down on their own 35-yard line on 4th and eight in the first half? There is no simulation pattern under a likely \u201ctrained\u201d football AI routine that would favor this type of decision under conventional football wisdom, yet coaches do make out of the box decisions on occasion. Machines don\u2019t. \n\nMoreover, as unsupervised AI Machines are being trained to \u201clearn up\u201d, how does one train a machine to go for it on 4th and long when odds are stacked against success? It is not logical Spock.\n\nThe definition from Wikipedia helps define how the human brain thinks versus how Machine Learning is taught to think.\n\n## Artificial Intelligence\n\n\ud83d\udcf7\n\nArtificial intelligence (AI), is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals, which involves consciousness and emotionality. The distinction between the former and the latter categories is often revealed by the acronym chosen. 'Strong' AI is usually labelled as AGI (Artificial General Intelligence) while attempts to emulate 'natural' intelligence have been called ABI (Artificial Biological Intelligence). Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term \"artificial intelligence\" is often used to describe machines (or computers) that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving\". (Wikipedia, 2020).\n\nThe Wiki definition of AI does a good job of breaking down why one AI platform can be very different from another, i.e., rules-based AI versus letting AI make it own rules. The take away here is that AI strategies, and therefore designing downstream strategies for deployment, must define the type of thinking process to be used. Next gen AI designs will also have to choose between human or machine hierarchies. Further, the integration of existing platforms must decide how an increasing array of disparate data banks are going to interface with various AI metrics and support technologies like GEO fencing or pulling data from deep neural networks. In ML speak, this means architecting which ML environments can overrule other machine environments when conflicts occur.  \n\nThis distinction in AI architectures is so pervasive it is quite possible that two identical company\u2019s embarking on a similar AI goal, could end up with an AI deployment that viewed from an AI structural perspective, could end up as polar opposite versions of AI, (supervised versus unsupervised), yet each version could be designed to achieve the same end goal. In other words what does a Ford 150 truck have to do with a Porsche 930 other than they both run on four wheels. Each one can also get you from Point A to Point B, but they operate under very different design specs. \n\nThe AI debate on machines running unchecked, despite the challenges associated with decoding human emotion, is inching closer to an AI reality where a machine can start to mimic \u201chuman consciousness\u201d, a fear echoed by some of the world\u2019s top AI architects. \n\nOne of the world\u2019s greatest AI minds, the legendary Douglas Hofstadter wrote a Pulitzer Prize winning book in the 1970\u2019s (G\u00f6del, Escher, Bach: An Eternal Golden Braid) about AI and merging the human element with a machine. He was also an early influencer behind the Google AI Brain. Hofstadter years later noted in his observations on one of the more advanced AI achievements involving human emotion his fear about this line getting crossed with human consciousness becoming embedded into machines. He discussed an AI application called \u201cEMI\u201d, for Experiments in Musical Intelligence. This AI wonder application was tasked with creating classical music \u201cin the style\u201d of Bach and Chopin. \n\nWhile giving a speech at the prestigious Eastman School of Music in Rochester, New York, Hofstadter first played a rare mazurka composed by Chopin in front of the top students including music theory and composition faculty. As noted by Melanie Mitchell, a disciple of Hofstadter, in her work in \u201cArtificial Intelligence\u201d noting this event in her book, here is what happened when one musical expert heard the second version created by the EMI AI Machine;\n\n\u201cThe second was clearly the genuine Chopin, with a lyrical melody; large-scale, graceful chromatic modulations; and a natural, balanced form\u201d. (Mitchell, 2019). This sentiment was echoed throughout the audience. To everyone\u2019s surprise the machine had outperformed the great works of Chopin by creating a world class rendition of one of the greatest composers of all time. Hofstadter went on to make the point that AI advancements in human emotion that can incorporate human feelings could catch us off guard as a society and leave human intelligence \u201cin the dust\u201d leaving human beings as \u201crelics\u201d in this new world order. (Mitchell, 2019). \n\nThis fear is genuine and raises several questions about where AI should go. Even more persuasive is the fact that these human/machine hybrids are now accelerating development at a much faster rate because of enhanced capabilities of deeper learning platforms. After all, if machines can replace human triumph, creativity and emotion and do it handily, have we not just compromised our souls? What becomes the value of original work by humans if our work can be enhanced several fold by a machine in a second? \n\nAs the world embarks on implementing more advanced versions of AI, the first order of business is to understand what is AI made of and how does it work. Let\u2019s first start with what feeds AI. The answer is data, and lots of it. Indeed, we have now reached the age of \u201cThe Zettabyte Era\u201d. A Zettabyte is equal to a trillion gigabytes. Expressed as a number that is 1000 to the seventh power or is represented by the number 1,000,000,000,000,000,000,000 bytes. When one can visualize this number and the vast supply of data that now exist, it makes sense why ML technologies hold so much promise.\n\nEven more amazing is this number according to Cisco, will grow to over 150 Zettabytes by 2025. The advancement of supercomputers was a key component to making newer AI applications work because of the computer power needed to discover new drugs, conduct cancer research, architect DNA mapping, and numerous other applications. AI can optimize the analysis of Big Data to scan hundreds of millions of possibilities in search of finding new discoveries. We are now able to perform complex computer tasks in days that previously would have taken decades to research all of the possible mathematical outcomes. \n\nThis brings up the technology convergence discussion. Modern day AI is like an air traffic control tower. It has to manage multiple technology disciplines in the proper order to get all of the data it has accessed into and out of the AI tasking process and then solve for X and do it cheaper than the existing methodology in order to justify AI transition costs. In addition, these faster AI Engines must do this in a timely manner and without crashing.\n\nIt takes resources to set up and deploy deeper learning capabilities (neural nets), a key component to trying to unlock the human brain by accessing larger and larger amounts of data and then auto-training AI Engines to build on previous knowledge bases that become increasingly more intelligent with each successive iteration. One large Swiss bank, (SEB) is taking the 30% of customer service calls not solved by Aida (their Voice assistant program) and using their resolution techniques and solutions to re-train into Aida, such that their VA can use that knowledge the next time it encounters a like kind issue, thus further freeing up their human trained personnel. \n\nWhile the heartbeat of an advanced AI implementation ultimately rests within a software engine, (a combination of algorithms and programs residing behind a User Interface) it is the integration of multiple input libraries like neural nets, Big Data mining, data compression and -----> image !!!  optimization that must work together to intelligently process disparate data into a credible summary report.  In part, it is the combination of multiple technologies working together that sets AI apart from just being a quantitative model. \n\nThese data results may be further combined with deeper and deeper learning environments while integrating with AI support technologies like sensory observation or GEO tracking that can define today\u2019s more advanced AI experiments, a small percentage of which get deployed into meaningful applications. Finally, these attributes can combine with in-house proprietary technologies inherent in a company\u2019s product line to gain a real competitive advantage. This can be expensive however. \n\nToday, only the largest tech companies have made significant investments in technology infrastructure needed to power the largest AI projects. The alternative for some companies is to lease large scale computing power. As an example, using the best available P3 environment on Amazon for 1,000 days now cost $31.28 per hour or $31.28 X 24 hour for 1,000 days equal to $749,232 dollars to complete a large neural net operation. (Przemek, 2019). \n\nThink of what technologies are now used to optimize self-driving cars for instance. How about a plane that can fly you to New York without a pilot? Now we are talking very significant technology integration, a key challenge to next-gen AI. \n\nThe noted AI expert, Joaquin Candela, Head of Facebook\u2019s Applied Machine Learning Group (AML) at Facebook makes a good point; \u201cFigure out the impact on the business first and know what you are solving for\u201d. (Candela, 2019).\n\nIn essence Candela makes the point that one does not invent an AI machine and then figure out how to use it but rather should use the opposite approach. First, understand what is the nature of the business goal and can an AI strategy be adopted or modified to outperform what is being used today. \n\nWhile many companies are looking to build AI strategies around mission critical apps, each AI review must start with how can existing algorithms be improved to increase speed and produce better end results for the consumer or business enterprise without using AI. In essence, has the application maxed out its optimization capability such that it requires AI to garner real improvements? It is only then that one can evaluate whether an AI strategy can be incorporated *into the solution* with a design plan to incorporate an AI methodology that can survive ROI hurdle objectives. Like any other technology upgrade, at some point AI has to pay for itself and provide a return on the investment.    \n\n**To AI or Not to AI?** Now that we can see some of the challenges of not only teaching a machine to learn but making the commitment to build an AI team, it is understandable why some companies are taking a wait and see approach. ML is advancing but it must identify to what standard and moreover, what type of human bias may be imbedded into a machine\u2019s ability to choose and at what point will machines begin to question their own instruction sets? This will further complicate traditional paths toward technology upgrades ultimately forcing a discussion on who will make IT investment decisions going forward, man or machine?\n\nThe reality here is that those firms who got pro-active years ago with designing and implementing a robust AI strategy are already seeing big wins with some AI deployments although these are for the most part limited to supervised AI architectures. These firms however, are also significantly more likely to see bigger AI wins in the next 36 months given their investments in creating baseline platforms to leverage targeted applications. Examples include Google Brain, Watson AI (IBM) and Facebooks CLUE defined as *Conversational Learning Understanding* *Engine.* \n\nThese significant AI achievements will allow the very best AI companies to deploy AI solutions much quicker by having an AI Engine that can be reused and modified to deploy more and more AI solutions incorporating very task specific mandates. A key advantage to these \u201cIn-House\u201d AI platforms is they were designed from the ground up to better integrate with existing IT systems that must communicate with AI architectures. This AI learning curve can easily contain a lot of technology minefields for firms not well versed in AI integration issues.  \n\nThe challenges of getting large departments use to re-thinking how business can implement AI takes time. Those companies that have not started to deploy an AI strategy (which can easily take years to implement) will need to devote more resources to AI to catch up. \n\nThis is why some firms will be cursed with AI failures if they jump into too quick without a defined AI strategy and others will simply lose because they chose in the short run \u201cnot to fail\u201d by avoiding AI implementations. AI, in the end however, is going to change the game of business and yet big successes with AI tend to come with big AI failures. Cadela, Facebook\u2019s AI pioneer gets it, he bakes in a 50% failure rate on thousands of AI experiments he is overseeing.  \n\n**Big Data and AI? -Does One (ML) Machine Environment wash the Other?** Recent gains in computing power have set the table to begin to uncrack AI challenges that will need to crunch tens and tens of millions of data strings to deliver on the very latest trends in AI. This faster computer power is the fuel that runs deeper learning environments that will enable AI architectures to better attack mathematical and image challenges previously outside the window of what was possible. This will also accelerate the ability to track and define complex pattern recognition engines that are key to advanced ML learning processes. \n\n\ud83d\udcf7\n\n**Source. Open AI, 2020**\n\nSignificantly faster processing is particularly important in order to invent and improve unsupervised AI learning environments where machines can actually learn to think at much higher levels that are closer aligned to the human brain. This will need to factor in human emotion where the goal set is to better recognize the human element. Currently, most supervised AI loops are not designed to make out of the box decisions for the most part or do so with post result human reviews.  \n\nAs noted above, computing power has seen a 300,000 X increase in capabilities from 2013 to 2019 because of advancements in chip technologies. This will help drive next-gen AI to develop new solutions in broader AI platforms needing this power to maximize deeper neural net advances. We will know we have reached another major AI breakthrough when Machines will recommend solutions that are unconventional and even defy statistical data, with ML learning how to argue that the rules don\u2019t apply in this or that situation. This will be the functional equivalent of an academy award for the Best Machine Actor Award for thinking algorithmically outside the box. \n\nRemember that 4th and long on your own 35-yard line in the second quarter when the game is tied? If the machine just said to keep the punter on the sideline, what type of AI algorithm was needed to override the statistical data that strongly suggest the kicker punt the ball? Hint: AI could decide that the element of surprise in a very specific situation may justify taking the risk of going for a first down on 4th and long.  An example would be the punter being told that the defense has shifted to a full blitz package to block the punt and therefore the odds of a receiver being open have increased. Either way, a decision on which rule supersedes punting versus risk taking, must allow ML to fail, otherwise, the human gut instinct will not be factored into the next iteration of ML decision trees. \n\nLastly, for AI to continue its projected meteoric rise, a major convergence of smarter technologies will need to continue. This will involve using more advanced Neural networks with smarter decision trees, larger scale data banks with more advanced search capabilities, faster processing, and upgraded algorithms that can scan up to tens of millions of data points in seconds or less. The end game then becomes conversion of these solutions sets into credible summary outputs or actionable steps. Integrating these somewhat disparate technology platforms is what will be needed for real convergence to occur with next-generational AI deployments.  \n\nImproving AI solution sets will also accelerate due to the improvement of neural nets and faster chip processing. Current AI models are already becoming increasingly relevant and they will continue to improve with more deployments achieving credible ROI\u2019s. Al is no longer just a trendy PR strategy, but can actually make money by lowering cost across a broad array of potential solution sets. For this reason, narrow offerings targeting specific tasks or narrower process objectives will continue to garner the early AI victories because the pathway to measurable ROI savings will be more apparent to costing managers. Early AI products will still rely on human interpretation as an approval process for AI implementation decisions, at least for the foreseeable future.   \n\n**Is an AI Commitment an Investment Paradox?** I feel compelled here to point out that many CEO\u2019s with revenues in the $100-$500M range operating with reduced AI budgets could be in a no-win situation when considering a significant AI commitment. The first step is to recognize that all companies are already in the technology business regardless of what you sell. AI Technology is what will define all growth models going forward. This ship has already sailed. Firms can either adapt to this generational change in business models or eventually succumb to a buy out or merger from a company with higher AI enhanced margins. Those companies operating without AI technologies will sell for lower multiples, reflecting lower Net Present Values on future cash flows versus AI equipped marker leaders. \n\nCompanies with revenues below $100M will have access to outsourced \u201cAI Services\u201d, however the value prop for many of these offerings will vary significantly and only a small percentage of this \u201cPackaged AI Market\u201d will have real cutting-edge AI value. These so called \u201cAI Marketing Companies\u201d that are packaging or leasing AI from other companies may perform well below those companies that are close to the source code and pioneered the first AI releases. Know thy code and know where it came from. If your provider owns the source code or has AI patents, you are much closer to the real thing. \n\n**Where is AI headed? -Summary Conclusions**. The great futurist Ray Kurzweil has penned several books about the future of AI and its implications on human kind. The majority of his predictions have come true or are in the process of becoming true. In his book, \u201cThe Age of Spiritual Machines\u201d, Kurzweil predicts by 2029, a $1,000 computer will be 1,000 times smarter than a human being. He also foresees by 2029, the line between what the definition of a human being is versus a \u201cconscious machine\u201d will start to blur, such that a movement will arise creating an entire area of law, seeking to grant \u201crobot rights\u201d to machines. Remember that demand for better cooling? These distinctions between human and machine intelligence will be further fueled by many people getting neural implants that interact with the human brain, thus becoming part human and part machine. Machines on the other hand will become more human like with built in emotional capabilities, thus becoming more human. The result will be a bigger controversy on what defines the line between machines and the human brain. \n\nThis AI paradox for companies to adapt AI will continue into the foreseeable future. On the one hand, if companies fail to envision AI in their future, AI savvy competitors will take market share. On the other hand, implementing an ill-conceived AI strategy that does not produce measurable results over a defined payback objective could end up falling further behind in operational performance due to the cost savings that are now present with AI models. These AI efficiencies will continue to accelerate corporate earnings and will continue to mount when newer AI technologies convert into the production and distribution process. \n\nThat said, it is imperative for smaller companies contemplating initial AI experiments to start with a narrow focus designed to produce a small incremental win so boards feel comfortable committing larger resources into downstream AI deployments.\n\nThis strategy can help build confidence that the AI program has the right team in place and is making the right outsourcing decisions for AI. Those companies that can embrace the new world order with AI will grow profits faster, drive share prices higher, and create more opportunities for all of the stakeholders embedded in their ecosystem. \n\n**Author Note:** I want to acknowledge the tremendous contributions that my references have made toward AI advancements. Their knowledge and wisdom have been very helpful with my on-going AI research efforts. I will post up Part II of this Blog later in the Spring which will focus on what to look out for when implementing AI into your Marketing Plan. My next Blog will be titled: \u201c**Can Deal Structure Trump Valuation**\u201d and will be released in March. \n\nSigning Off, \n\nJohn C Botdorf, MBA\n\nChairman\n\nJCB Consulting Group, LLC\n\n[www.advancedtravelsystems.com](http://www.advancedtravelsystems.com/)\n\n[botdorfjohn@gmail.com](mailto:botdorfjohn@gmail.com)\n\n303.810.7710\n\nAuthor:\n\n\u201cMastering Your Time Share\u201d\n\n\u201cMastering Your Company\u201d\n\n\u201cMastering Your Diva\u201d\n\n**References.**\n\nBrynjolfsson, Erik, McAfee Andrew, (2019), \u201cHarvard Business Review\u201d, Artificial Intelligence, Chapter One, \u201cThe Business of Artificial Intelligence\u201d, Chapter One pp.14-15.\n\nGrand View Research, July, 2020. [www.grandviewresearch.com/industry-analysis/artificial-intelligence-ai-market](http://www.grandviewresearch.com/industry-analysis/artificial-intelligence-ai-market).\n\nKnickrehm, Mark, (2019). \u201cHarvard Business Review\u201d. Artificial Intelligence. \u201cHow will AI change work\u201d. Accenture Survey of 1,200 C-Level Executives. Chapter Eight. Pp. 99-100. \n\nKurzweil, Ray (1999) \u201cThe Age of Spiritual Machines\u201d. [Predictions made by Ray Kurzweil - Wikipedia](https://en.wikipedia.org/wiki/Predictions_made_by_Ray_Kurzweil)\n\n*Merriam-Webster.com Dictionary*, \u201cArtificial Intelligence\u201d, Merriam-Webster, https://www.merriam-webster.com/dictionary/artificial%20intelligence. Accessed 5 Feb. 2021.\n\nMitchell, Melanie (2019). \u201cArtificial Intelligence, A Guide for Thinking Humans\u201d. Farrar, Strauss and Giroux. New York NY 10271.\n\nPrzemek, Chojecki, (2019, Jan 31). \u201cHow Big is Big Data?\u201d \n\nWebster.com (2021) Definition of Artificial Intelligence.\n\nWikipedia, 2020. \u201cArtificial Intelligence\u201d. [data used for artificial intelligence - Bing](https://www.bing.com/search?q=data+used+for+artificial+intelligence&amp;form=ANSPH1&amp;refig=08d2557256a3444faec43845bd1b44a6&amp;pc=U531&amp;sp=-1&amp;ghc=1&amp;pq=data+used+for+artificial+intelligence&amp;sc=1-37&amp;qs=n&amp;sk=&amp;cvid=08d2557256a3444faec43845bd1b44a6)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/m6i3g0/the_future_of_artificial_intelligence/"}, {"autor": "MlTut", "date": "2021-03-16 06:19:31", "content": "6 Top Artificial Intelligence Courses for Healthcare /!/ Hi Folks,\n\nArtificial Intelligence is transforming healthcare in various ways like brain tumor classification, medical -----> image !!!  analysis, bioinformatics, etc. \n\nSo if you are willing to learn AI for healthcare, you can check this article. \n\nIn this article, you will find the 6 Best Artificial Intelligence Courses for Healthcare.\n\n[https://www.mltut.com/best-artificial-intelligence-courses-for-healthcare/](https://www.mltut.com/best-artificial-intelligence-courses-for-healthcare/)\n\nI hope these listed courses will help you to learn Artificial Intelligence for Healthcare.\n\nAll the Best!\n\nHappy Learning!", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/m62ldd/6_top_artificial_intelligence_courses_for/"}, {"autor": "WiseRaven1", "date": "2021-03-31 18:36:55", "content": "How Can I Augment an Existing Model with New Training Data? /!/ I built an app where I use MobileNet model from TensorFlow Lite to detect an object in a live steaming -----> camera !!! . \n\nNow, I wish to ReTrain the model or somehow \"Augment\" with new training data, i.e. new photos. \n\nHow can I accomplish such an objective at the Edge, i.e. on the device? \n\nor even on a remote server \"fast\"?", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/mhclbw/how_can_i_augment_an_existing_model_with_new/"}, {"autor": "Yuqing7", "date": "2021-02-27 03:12:53", "content": "[N] Better Than Capsules? Geoffrey Hinton's GLOM Idea Represents Part-Whole Hierarchies in Neural Networks /!/ A research team lead by Geoffrey Hinton has created an imaginary vision system called GLOM that enables neural networks with fixed architecture to parse an -----> image !!!  into a part-whole hierarchy with different structures for each -----> image !!! .\n\nHere is a quick read: [Geoffrey Hinton's GLOM Idea Represents Part-Whole Hierarchies in Neural Networks](https://syncedreview.com/2021/02/26/better-than-capsules-geoffrey-hintons-glom-idea-represents-part-whole-hierarchies-in-neural-networks/)\n\nThe paper *How to Represent Part-Whole Hierarchies in a Neural Network* is on [arXiv](https://arxiv.org/pdf/2102.12627.pdf).", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ltewid/n_better_than_capsules_geoffrey_hintons_glom_idea/"}, {"autor": "hackernoon", "date": "2021-02-26 17:48:42", "content": "OpenAI\u2019s DALL\u00b7E: Text-to------> Image !!!  Generation Explained", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/lt3dbk/openais_dalle_texttoimage_generation_explained/"}, {"autor": "jsamwrites", "date": "2021-01-10 14:00:59", "content": "Here\u2019s how OpenAI\u2019s magical DALL-E -----> image !!!  generator works", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/kuexnt/heres_how_openais_magical_dalle_image_generator/"}, {"autor": "Arne97", "date": "2021-05-31 16:15:38", "content": "Electronic scooter classfication /!/ Hi, I have been given a task to  classify pictures of electronic scooters and to create an algorithm that  recognises whether a scooter has been parked correctly or not.\n\nI  received 85,000 pictures. I would like to solve the task using a CNN. I  quickly realised that labelling the training data is very complicated.  Various questions arose, for example\n\n* when is the scooter parked correctly?\n* Is there a scooter?\n* At what point do I say that too little scooter is visible in the -----> picture !!! ?\n\nI have set up 3 labels:\n\n1. scooter is well visible and location is good\n2. scooter is well recognisable and location is poor\n3. scooter is poorly recognisable\n\nI  am not sure if these 3 classifications are good because I am especially  unsure about the 2nd classification. No location is the same. I don't  think I can get the model trained correctly to recognise, for example,  that the scooter is in front of the driveway of a garage, etc.\n\nI tend to set up 2 classes.\n\n1. good:  scooter is there, easily visible, not broken, structures (angle,  wheels, handlebars) recognisable, surface doesn't matter, night or day  doesn't matter.\n2. bad: no scooter or only very few scooters visible or few unclear scooter structures on the photo.\n\nThe problem here is that I have to train the model to recognise a scooter correctly. **But how do I get the model to recognise if the location of the scooter is correct?**\n\nAfter  labelling 2000 images, I have only found 50 images that may not  represent a correct location. The reason for this is always different.\n\nI use Python with Tensorflow and Keras.Various YouTube videos help me to write the code.\n\nMaybe someone has an idea how I can better approach the problem or whether I am already making basic mistakes when labelling.\n\nThanks a lot!", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/np72gg/electronic_scooter_classfication/"}, {"autor": "girirajar", "date": "2021-04-15 20:06:06", "content": "What is the tech stack behind remove.bg and removal.ai? /!/ What are the technologies used to remove a background from am -----> image !!! ? What architecture / training could be used by AI services like [remove.bg](https://remove.bg) and [removal.ai](https://removal.ai)?", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/mrmwk0/what_is_the_tech_stack_behind_removebg_and/"}, {"autor": "vesuvitas", "date": "2021-02-10 13:48:47", "content": "What AI Will Really Do for the Contact Center /!/ AI\u2019s impact on enterprise communications and collaboration is being felt first in the contact center. That\u2019s not surprising; contact centers are generally where cutting-edge technology makes (or fails to make) its business case. But the -----> picture !!!  is just beginning to come into focus when it comes to which contact center functions AI will impact the most.\n\nHere are a few somewhat random observations that get a better picture of what AI can do for contact centers in the near term:\n\n1. Make contact centers more human  \n2. Help deal with unpredictability  \n3. Improve the agent experience\n\nFull article: [https://www.nojitter.com/ccaas/what-ai-will-really-do-contact-center](https://www.nojitter.com/ccaas/what-ai-will-really-do-contact-center)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/lgtzca/what_ai_will_really_do_for_the_contact_center/"}, {"autor": "azolxttx_sxunds", "date": "2021-02-09 19:42:19", "content": "Kell-I short -----> film !!!  /!/ Hello guys, this is my first post so bare with me. \n\nI need your help. \n\nI\u2019m finishing writing a short film about a guy who falls in love with an AI ASMR girl without him knowing she is in fact an AI (yes like FUCKING HER but not exactly like HER). \n\nAll their interactions are online. \n\nThe basic narrative arc is this:\n\nSotryline (Narrative)\n\n- guy with shitty lifestyle is against AI but a fan of ASMR\n- He starts chatting with ASMR girl online entity named Kell-I\n- they fall in love\n- Guy\u2019s lifestyle improves with this online relationship\n- He realizes Kell-I is an AI\n- He goes into existencial dilemma while continuing the relationship (now knowing the truth) \n*Interesting dialogue: maybe he knows she is an AI and he thinks she doesn\u2019t know that he knows (but obviously she knows, she is a fucking AI) (or does she?)\n- He cuts the cord (communication) with the AI \n- His shitty lifestyle returns, he is miserable, feels sad about the lonely AI Girl waiting for him in the void\n- He accepts the AI willingly? And starts up a conversation.\n\n- Two possible outcomes Optimism vs. Skepticism on Artificial Intelligence and the future of humanity:\n\n- He shoots himself in reject of this new lifestyle \n- OR he accepts the new world we live in and he is even happy and excited for the future :D\n\n\nI want to know your thoughts on the possible outcomes + how could you fathom this relationship, also any posts with links to similar cases of this already happening around the world might be helpful. \n\nI want to know your thoughts on the future of human relationships with AI?\nWhat would the goals / coding of this AI be?\nIs this scenario even plausible?\nWould you go into it willingly?\nWhat could be the downfall?\nWhat will be lost?\n\nThanks a fuckin bunch (if u got this far)\n\nAzo.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/lgbkck/kelli_short_film/"}, {"autor": "Admirable-Air-750", "date": "2021-06-14 18:19:29", "content": "How Artificial Intelligence Tools Are Transforming Business In America During the Pandemic /!/ Pandemic has shaken the global economy. America is one of the affected countries in this critical situation. Despite the pandemic, life goes on, and businesses keep producing.  One of the notable developments is the use of artificial intelligence for business. \n\nDuring the  pandemic, we have noticed a dramatic increase in remote work. AI played an essential role in enabling this critical form of employment for  Americans. In this article, I introduce a few AI concepts that contribute to this acceleration. \n\nAI software stacks for remote work cover a br\u043e\u0430d \u0441\u0430t\u0435g\u043er\u0443 including platforms, d\u0435\u0435\u0440 learning, m\u0430\u0441h\u0456n\u0435  learning, and \u0441h\u0430tb\u043et\u0455. D\u0435\u0435\u0440 l\u0435\u0430rn\u0456ng includes AI topics \u0455u\u0441h as n\u0430tur\u0430l  language \u0440r\u043e\u0441\u0435\u0455\u0455\u0456ng, \u0455\u0440\u0435\u0435\u0441h r\u0435\u0441\u043egn\u0456t\u0456\u043en, \u0430nd computer v\u0456\u0455\u0456\u043en (-----> image !!!   recognition). E\u0430\u0441h \u043ef these \u0455ub\u0441\u0430t\u0435g\u043er\u0456\u0435\u0455 \u0440r\u043ev\u0456d\u0435\u0455 business ventures  w\u0456th many \u0441\u0430\u0440\u0430b\u0456l\u0456t\u0456\u0435\u0455. \n\nAI platforms offer \u0430\u0440\u0440r\u043e\u0440r\u0456\u0430t\u0435 \u0455\u043elut\u0456\u043ens  for d\u0435v\u0435l\u043e\u0440\u0435r\u0455 who w\u0430nt t\u043e bu\u0456ld intelligent \u0430\u0440\u0440l\u0456\u0441\u0430t\u0456\u043en\u0455 \u043en t\u043e\u0440 \u043ef  other \u0440l\u0430tf\u043erms. Th\u0435\u0455\u0435 tools, l\u0456k\u0435 in a conventional \u0430\u0440\u0440l\u0456\u0441\u0430t\u0456\u043en  \u0440l\u0430tf\u043erm, g\u0435n\u0435r\u0430ll\u0443 \u0440r\u043ev\u0456d\u0435 dr\u0430g-\u0430nd-dr\u043e\u0440 capability \u0430\u0455 well \u0430\u0455 prebuilt  \u0430lg\u043er\u0456thm\u0455 \u0430nd frameworks \u0456n the creation \u043ef applications. \n\n[https://www.newsbreak.com/news/2278920805415/how-artificial-intelligence-tools-are-transforming-business-in-america-during-the-pandemic](https://www.newsbreak.com/news/2278920805415/how-artificial-intelligence-tools-are-transforming-business-in-america-during-the-pandemic)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nzt84b/how_artificial_intelligence_tools_are/"}, {"autor": "CareAffectionate8171", "date": "2021-06-14 07:28:08", "content": "COVID-19 and Impact on AI Adoption /!/ The COVID-19 pandemic has wreaked havoc globally, causing businesses to come at a screeching halt with prolonged lockdowns and disruption of our everyday lives. This pandemic is considered the worst global economic fallout since the Great Depression of the 1930s and has undeniably impacted everyone\u2019s lives around the world.\n\n## Artificial Intelligence and COVID-19:\n\nWith COVID-19 continuing to disrupt our \u201cnormal\u201d way of handling businesses, business leaders have quickly realized the need for [Artificial Intelligence solutions](https://www.xavor.com/ai-ml-solutions) and digital transformation. AI technology has been instrumental in dealing with the Corona Virus, aiding in detecting and diagnosing the virus, tracking the spread of the infection, and much more. Artificial Intelligence is playing a massive part in discovering a vaccine for COVID-19 while also slowing down the spread of the virus through contact tracing. The automation of processes like organizing documents, prioritizing tasks, and creating notes has been made easier by adopting AI in the healthcare sector.\n\n## Adoption of Artificial Intelligence for Business Continuity:\n\nDue to the pandemic\u2019s disruptions, the in-person business transactions are becoming null, resulting in business leaders worldwide incorporating Artificial Intelligence to enhance the customer experience. The demand for digital products and technologies has soared up to 65% during the pandemic.\n\nAccording to the CEO of Xavor, Dr. Hugh, \u201cThe impact of COVID-19 around the world has been devastating, but there is a silver lining in this situation as well. We have seen a huge increase in AI adoption across enterprises. The demand for toolkits enabling seamless work from home and AI-based solutions for business continuity has soared as businesses around the world are realizing the impact of AI adoption for their enterprises\u201d.\n\nChinese tech companies like Alibaba, Baidu, Tencent have given access to their AI and [cloud computing solutions](https://www.xavor.com/devops-cloud) to researchers to accelerate the vaccine development. Similarly, medical startups and enterprises in the US are deploying AI to rapidly identify thousands of new molecules that could be turned, not potential cures. AI is also playing a part in early warning and detection of the virus. All of this serves as a reminder that AI has the potential for enhancing efficiency, growth, and productivity. AI-enabled automation is paving the way for transitions of traditional services and offline businesses to digital and online channels.\n\nLet\u2019s look at some of the industries where AI adoption can help automate processes, gain customer and competitive insight through data, and improve customer and employee engagement.\n\n## Healthcare Industry:\n\nHealthcare enterprises and governments worldwide have started leveraging AI-powered solutions during the COVID-19 health crisis to improve efficiency and fast-track results. Baidu, a Chine technology company, developed a no-contact infrared sensor system to single out people with fever, even in crowds. The Tampa General Hospital in Florida deployed an AI system to stop individuals with potential COVID-19 symptoms from visiting patients. Cameras installed at entrances conduct a facial thermal scan and pick up symptoms, including sweat and discoloration, to identify visitors with fever.\n\n[AI in healthcare](https://www.xavor.com/blog/artificial-intelligence-in-health-sector) is also being used to monitor COVID-19 symptoms, automate hospital operations, and provide decision support for CT scans. AI in the healthcare industry is making significant progress, especially during the pandemic.\n\n## Financial Sector:\n\nThe finance industry has seen an increase in digital transactions with a sharp rise in mobile activity, including fund transfers, time deposits, bill payments, and foreign currency exchanges. To create a better online presence and ensure the best customer service, banking, and financial institutions implement AI.\n\nThe continuous rise in security threats to the banking industry requires stricter processes for customer identification and authentication. Facial recognition, biometric identification, iris scans all employ AI at their core to ensure accurate identification. Similarly, AI-enabled voice assistants and chatbots provide personalized recommendations and help with queries. AI is also used to quickly improve KYC (Know Your Customer) regulatory and anti-fraud checks.\n\n## E-Commerce and Retail:\n\nAs everything around us continually shifts to online services, sales in shopping malls and physical stores are decreasing. The e-commerce industry is continuously on the rise, and AI can contribute a lot to this sector. Most customers abandon their online shopping experiences because products displayed are often irrelevant and unaffordable. Advanced natural language processing and -----> image !!!  and video recognition can help narrow and refine customer searches, resulting in a higher conversion likelihood.\n\nSimilarly, digital payments and customer authentication with anti-fraud technology also help to simplify and expedite the overall customer experience. This results in offering the customer more accurate search results.\n\n## AI Adoption for your Company:\n\nThe pandemic hit every company hard, and at this critical moment, every business leader should be asking the following questions to create an agenda to implement Artificial Intelligence to accelerate their company progress:\n\n* What is our long-term business strategy?\n* What does AI mean to our industry at this moment?\n* What are our priorities, i.e., customer experience, innovation, etc.?\n* What type of change is required, and how do I adapt our operating model?\n* How do I show progress to our stakeholders?\n\nYou can start by revisiting your long-term business strategy, map out your priorities and goals, and have a clear idea of your challenges and the path forward. Only then should you start thinking about implementing AI.\n\nIf your business is looking for to implement **digital transformation**, reach out to us at\u00a0[info@xavor.com](mailto:info@xavor.com)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nzh9lj/covid19_and_impact_on_ai_adoption/"}, {"autor": "Zephyrus1O1", "date": "2021-06-13 07:15:13", "content": "How does Captcha verify a human without even verifying? /!/ Ever wondered why nothing happens after you tap on the \"verify\" button?\n\nWhat is Captcha?\n\n\nLike we all come across Captchas almost every day. But have you ever thought about what it really stands for?\n\nCAPTCHA stands for Completely Automated Public Turing test to tell Computers and Humans Apart, which basically means it determines whether the user is real or a robot. This program was originated with Guatemalan computer scientist Luis von Ahn. He is also the founder of Duolingo.\n\nEssentially, the CAPTCHA was developed in the early 2000s to tell whether someone was a human or robot - a sort of Turing Test. The test wasn\u2019t completely automated \u2013 we humans had to attempt to decipher some warped text - illegible to computers - and hope we got it right. It did the job. Since the invention of captcha, it has been evolving, but not according to humans but actually the computers themselves.\n\n\nThe Evolution of Captcha\n\n\nIn the beginning, Captchas were simple . The user i.e. humans were given a simple -----> image !!!  that had some letters not necessarily actual words. But the text given was wrapped(or twisted). These wrapped texts were easy to understand for humans but the computers back then could not read that text. So, they were given some of the words in between the text provided to us to check if the input humans put was correct.\n\nBut these tests were also training the computers to learn how to read that wrapped text. The more tests people appeared, the more refined the computers became. This is also practical use of Artificial Intelligence. And with so many internet users completing these tests every day, Google saw an opportunity for something more. After purchasing CAPTCHA in 2009 it became reCAPTCHA and we were put to work decoding old pieces of literature, whether we realized it or not.\n\n\nThe decline of reCAPTCHA\n\nUnfortunately, the free transcription service wasn\u2019t to last. A 2014 study by Google found that AI robots were able to decode the CAPTCHAs with 99.8% accuracy, and numbers in images with 90%. In this way, those AI robots were more likely to solve the Captcha tests than humans themselves. For that, A new way to verify humans needed to be found.\n\nThe birth of No CAPTCHA reCAPTCHA\n\nWhen Captchas had used old newspapers text for tests, various types of images as well. The researchers needed something new which AI robots could not learn over time so easily. Fortunately, Google found a solution \u2013 a simple box.\n\nAlthough this box may look simple, there is a very sophisticated process behind it. Google\u2019s risk analysis engine works away in the background running its own Turing Test based on how the user is behaving throughout their interactions on the site. Based on your clicks and navigation throughout the webpage, the website recognizes if you are a human or a robot. So, when I click that I'm not a robot box, nothing happens and you are verified based on your behavior. Of course, there will be times when the engine isn\u2019t too sure, and the user will be prompted to complete an activity to confirm their human status.\n\nBut, we are still training the AI bots with NoCaptcha ReCaptcha. This is only one of the many applications of Machine Learning and A.I. in our daily lives. In general, we help feed the computers with a big chunk of training data that is in return used for our convenience. Hope we don't train them too well.\n\n[My original post on linkedin](https://www.linkedin.com/pulse/how-does-captcha-verify-human-without-even-verifying-samyak-swain)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nyqzn3/how_does_captcha_verify_a_human_without_even/"}, {"autor": "CalligrapherSimple29", "date": "2021-06-13 04:09:10", "content": "using footage from the 1962 -----> film !!!  cleopatra starring Elizabeth Taylor with Cleopatra's real face digitally regenerated by Deep Fake technology &amp; her statues .. to see cleopatra being vivid , real and Alive in a way .. i hope you enjoy this simulation", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nyocvw/using_footage_from_the_1962_film_cleopatra/"}, {"autor": "techsucker", "date": "2021-07-15 04:23:42", "content": "Researchers From Berkeley Lab Introduce Self-Supervised Representation Learning For Astronomical Images /!/ Sky surveys make it possible to catalog and analyze celestial objects without the need for lengthy observations, making them invaluable for exploring the universe. However, in addition to providing a general map or -----> image !!!  of a region of the sky, they are one of the largest data generators in science, currently imaging tens of millions to billions of galaxies over the lifetime of a single survey. As a result, screening collected datasets to find the most relevant information or discovery has become increasingly laborious.\u00a0\n\nSummary: [https://www.marktechpost.com/2021/07/14/researchers-from-berkeley-lab-introduce-self-supervised-representation-learning-for-astronomical-images/](https://www.marktechpost.com/2021/07/14/researchers-from-berkeley-lab-introduce-self-supervised-representation-learning-for-astronomical-images/) \n\nPaper: https://iopscience.iop.org/article/10.3847/2041-8213/abf2c7", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/okladd/researchers_from_berkeley_lab_introduce/"}, {"autor": "FoxLynx64", "date": "2021-07-15 03:31:26", "content": "3D Rendering? /!/ AI has been used for some really complicated things that traditional computation is unable to do.  One such thing is making a video game look more real (as if it were shot on a -----> camera !!! ) and even subsidizing rays in ray tracing with a prediction of how the image would look.  My question is, why do we even use traditional ways of rendering 3D scenes anymore?  AI is even better at creating things which can be perfectly and consistently created from 1 or more dataset(s).  It\u2019s basically the same idea as having an AI train on a calculator and it being able to do complicated math in a fraction of the time compared to a computer that is using traditional methods.  It could revolutionize gaming, 3D animation, and much more if something like this were created.  Take a look at VR, the XR2 chip is one of the most sophisticated processors of its size, imagine what it could do using an AI like this.  We might reach a point where polygon counts can go as high as humanly visible and still render scenes in real time at 60 FPS which would be insane on paper.  So obviously I must be missing something, because wouldn\u2019t someone have thought of this and tried it already?  There must be some kind of catch which I can\u2019t see yet.  Any ideas would help me.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/okkhfv/3d_rendering/"}, {"autor": "Broad-Fuel4116", "date": "2021-07-14 12:01:40", "content": "Open access (CC-BY 4.0) emotional facial expression -----> image !!!  database /!/ Images of Compound Affect (IsCA) is a collection of 156 black and white   facial  expression images, posed by one subject. These expressions form a    catalogue of compounds associated with the emotion labels happiness,    sadness, anger, disgust, fear, surprise, and contempt. Two sets were    constructed: IsCA and IsCA+. IsCA+ includes colour bounding boxes and    hand-drawn markup. Both are licensed under a Creative Commons    Attribution license (CC-BY 4.0).\n\nMore info: [https://www.olinejad.com/isca](https://www.olinejad.com/isca)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ok2w8a/open_access_ccby_40_emotional_facial_expression/"}, {"autor": "RCGopiTechie", "date": "2021-07-13 22:19:54", "content": "Day 86 - Deep Learning -----> Image !!!  Face Detection In Less Than A Min Using RetinaFace https://www.gopichandrakesan.com/day-86-deep-learning-image-face-detection-in-less-than-a-min-using-retinaface/?feed_id=492&amp;_unique_id=60ee118a3dd29 #100dayschallenge #ArtificialIntelligence #ComputerVision #DeepLear...", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ojqga8/day_86_deep_learning_image_face_detection_in_less/"}, {"autor": "Due_Echidna_6353", "date": "2021-07-13 20:06:58", "content": "Any software out there that can be used to mix two characters together? (Other than Artbreeder) /!/ I\u2019ve been searching for a software that can essentially mix two characters together, and couldn\u2019t find anything like that. The closest thing was Artbreeder, but that only allows you to mix randomly generated characters with other randomly generated characters, it doesn\u2019t allow 2 characters in a list of different characters, or from your -----> photo !!!  list, for fusing, so I hope someone knows if something like that exists.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ojnrhs/any_software_out_there_that_can_be_used_to_mix/"}, {"autor": "FlamingGem", "date": "2021-07-12 21:18:24", "content": "I created a Computer Vision-Based Rabbit Deterrence System /!/ So, I noticed there were a bunch of bunnies in my backyard and so I  decided to create a Rabbit deterrence system using computer vision. Essentially how it works is that there is a Raspberry Pi with a  -----> camera !!!  that monitors my backyard. Using an object detection model, I am able to detect when there is a rabbit next to my garden and when there is, I play a sound (such as a baby crying or a car passing by) via a Bluetooth speaker which will then spook off the rabbit. I ended up documenting the whole process from start to end and, well, this is what I ended up with:\n\nVideo Documenting the Process: [https://youtu.be/oPvqKgq3ppc](https://youtu.be/oPvqKgq3ppc)\n\nBlog (goes into more technical details): [https://blog.roboflow.com/rabbit-deterrence-system/](https://blog.roboflow.com/rabbit-deterrence-system/)\n\nSource Code: [https://github.com/roboflow-ai/rabbit-deterrence](https://github.com/roboflow-ai/rabbit-deterrence)\n\nDataset: [https://public.roboflow.com/object-detection/cottontail-rabbits-video-dataset](https://public.roboflow.com/object-detection/cottontail-rabbits-video-dataset)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/oj0oyb/i_created_a_computer_visionbased_rabbit/"}, {"autor": "DQLabsinc", "date": "2021-07-28 16:34:48", "content": "What is data profiling &amp; why is it important? /!/ The value of your data depends on how well you profile it. Today, only about 3% of data meets quality standards. That means poorly managed data is costing companies millions of dollars in wasted time, money, and untapped potential. Data profiling helps your team organize and analyze your data to yield its maximum value and give you a clear, competitive advantage in the marketplace.  \n**What is data profiling?**  \nData profiling is assessing the quality and structure of data sources, so you have a complete, 100 % accurate -----> picture !!!  of your data. Data profiling verifies that data columns are populated with the types of data you expect. If a profile reveals problems in data, you can define steps in your data quality project to fix those problems. Data profiling promotes good [data governance.](https://www.dqlabs.ai/data-governance/?utm_source=reddit&amp;utm_medium=referral&amp;utm_campaign=what-is-data-profiling)  \n\n\n**Types of data profiling:**\n\n***Structure discovery*** \\- Confirming that data is consistent and formatted correctly, and performing mathematical checks on the data. Structure discovery aids in understanding how well data is structured.\n\n***Content discovery*** \\- Checking individual data records to discover errors. It identifies which specific rows in a table contain issues and which systemic problems occur in the data.\n\n***Relationship discovery*** \\- Identifying how parts of the data are interrelated. For example, critical relationships between database tables, references between cells, or tables in a spreadsheet. Understanding relationships is crucial to reusing data; related data sources should be united into one or imported in a way that preserves relationships.  \n\n\n**Why is data profiling important?**\n\n\\- Better data quality and credibility\n\n\\- Predictive decision making\n\n\\- Proactive crisis management\n\n\\- Organized sorting  \n\n\n**Data profiling challenges**\n\nIt is often tricky due to the utter volume of data you\u2019ll need to profile. This is majorly true if you are looking at a legacy system. A legacy system may have years of older data with thousands of errors.\n\nIf you manually perform your data profiling, you\u2019ll need an expert to run a number of queries and go through the results to gain meaningful insights about your data, which can utilize precious resources. Additionally, you might only check a subset of your overall data because it is too time-consuming to go through the entire data set.\n\n**Conclusion**\n\nAs more companies store enormous amounts of data in the cloud, the need for effective data profiling is more important than ever. Cloud-based data lakes already allow companies to store petabytes of data. The Internet of Things is expanding our capacity for data by collecting vast amounts of information from an ever-evolving range of sources, including our homes, what we wear, and the technologies we use.\n\nStaying competitive in the modern marketplace driven by cloud-native big data capabilities means being equipped to harness all that data. From maintaining [data compliance standards](https://www.dqlabs.ai/data-compliance/?utm_source=reddit&amp;utm_medium=referral&amp;utm_campaign=what-is-data-profiling) to creating a brand known for outstanding customer service, data profiling is the hinge between success and failure in managing data stores.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/otdk3v/what_is_data_profiling_why_is_it_important/"}, {"autor": "Yuqing7", "date": "2021-07-28 13:19:59", "content": "[R] MIT &amp; Google Quantum Algorithm Trains Wide and Deep Neural Networks /!/ A research team from MIT and Google Quantum AI presents a quantum algorithm for training classical neural networks in logarithmic time and provides numerical evidence of its efficiency on the standard MNIST -----> image !!!  dataset. \n\nHere is a quick read: [MIT &amp; Google Quantum Algorithm Trains Wide and Deep Neural Networks.](https://syncedreview.com/2021/07/28/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-71/)\n\nThe paper *A Quantum Algorithm for Training Wide and Deep Classical Neural Networks* is on [arXiv](https://arxiv.org/abs/2107.09200).", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ot9rfx/r_mit_google_quantum_algorithm_trains_wide_and/"}, {"autor": "techsucker", "date": "2021-07-28 07:19:04", "content": "Apple Explains Its New On-Device Machine Learning Methods To Recognize People In Photos With Extreme Poses, Accessories Correctly, Or Even Occluded Faces /!/ Photos are an integral way for people to browse, search, and relive life\u2019s moments with their friends and family. Photos on your apple devices are very often labelled with people to be easily categorized. An algorithm foundational to this goal recognizes faces from different angles using facial recognition software, so it doesn\u2019t take up too much space storing every single -----> image !!!  because one app contains all members\u2019 memories! This process of labelling photos of people uses a number of machine learning algorithms\u2013running privately on-device\u2013to help curate and organize Live Photo videos and regular pictures in categories that you can find later by the person who was depicted within each photo.\n\nQuick Read: [https://www.marktechpost.com/2021/07/28/apple-explains-its-new-on-device-machine-learning-methods-to-recognize-people-in-photos-with-extreme-poses-accessories-correctly-or-even-occluded-faces/](https://www.marktechpost.com/2021/07/28/apple-explains-its-new-on-device-machine-learning-methods-to-recognize-people-in-photos-with-extreme-poses-accessories-correctly-or-even-occluded-faces/) \n\nApple Blog: [https://machinelearning.apple.com/research/recognizing-people-photos](https://machinelearning.apple.com/research/recognizing-people-photos)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/njxhholvmwd71.png?width=1392&amp;format=png&amp;auto=webp&amp;s=dcbefeb838c3a3dac54754cc3433e8e9f37149d0", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ot4ya0/apple_explains_its_new_ondevice_machine_learning/"}, {"autor": "techsucker", "date": "2021-07-26 17:57:33", "content": "Researchers From Tel Aviv University Propose LARGE, Latent-Based Regression through GAN Semantics /!/ Researchers at Tel-Aviv University have created a method to solve regression tasks with little supervision effectively. The researchers noted that GANs are great for encoding semantic information into the latent space, and they see this as important in modern generative frameworks. This means smooth linear directions, which affect -----> image !!!  attributes disentangled. These directions have been used in GAN-based image editing for years. The research team found that these directions are not only linear, but the magnitude of change induced on respective attributes also follows a relatively flat pattern no matter how far they go along them. This new method uses this observation to turn any pre-trained GAN into a regression model by leveraging as few as two labelled samples instead of many more previously required.\n\nQuick Read: [https://www.marktechpost.com/2021/07/26/researchers-from-tel-aviv-university-propose-large-latent-based-regression-through-gan-semantics/](https://www.marktechpost.com/2021/07/26/researchers-from-tel-aviv-university-propose-large-latent-based-regression-through-gan-semantics/) \n\nPaper: https://arxiv.org/pdf/2107.11186.pdf\n\nCode: https://github.com/YotamNitzan/LARGE", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/os4765/researchers_from_tel_aviv_university_propose/"}, {"autor": "FreeBobbyShmurder", "date": "2021-08-13 00:49:03", "content": "\u201cCoded Bias\u201d - Netflix\u2019s Facial Recognition Documentary /!/ \u201cCoded Bias\u201d is, hands down, the most flawed documentary I have ever watched. The thesis of the -----> film !!!  is that computer algorithms are inherently biased against certain individuals, specifically black women. However, the documentary fails to articulate *how* said algorithms are biased. \n\nThe only piece of evidence that is used throughout the film is that IBM\u2019s facial recognition technology is marginally worse at being able to correctly identify African American women. I have a background in computer science and can almost guarantee the discrepancy is attributed to lighting differences being reflected from the pigment of the user\u2019s skin color. A black person in a well lit room is going to be more easily identified than a white person in a dimly lit room. This is not a racial bias. This is a limitation of technology. \n\nAdditionally, assigning different mortgage rates based on someone\u2019s criminal history or credit score is NOT biased. Difference of outcome does not equal difference of opportunity. Computer software, by definition, is less subject to racial biases than any given individual. The algorithms that are being used take into consideration a variety of factors to calculate an equitable rate.  \n\nThis film is a disgrace to social justice and cyber security.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/p3cw37/coded_bias_netflixs_facial_recognition_documentary/"}, {"autor": "Kayrosis", "date": "2021-08-12 14:21:34", "content": "Dragonfly, Externalizing an Internal Voice through AI /!/ Tl;dr: I've made the face of a synthetic person who does not exist, yet a voice inside my head screams out that this is fundamentally a part of me, that this face is what I look like on the inside. I want to create and attach to that face, an AI that externalizes this facet of my internal self with a voice, a body, and a personality of sorts so that I and others can converse with it in VR/AR or otherwise without my physical self as an intermediary, but I have only the vaguest idea of how to do that. If you could help me with this project in any way, I'd appreciate it.\n\n[Dragonfly's face](https://imgur.com/a/DFuQFzb)\n\nSo to begin, I am an artist. As part of my art, I sought to create the -----> image !!!  of a perfect man (and later I intend to create a perfect woman). Instead, when I produced the final face of this guy who does not exist, constructed from a multipolar interpolation of the most attractive guy's faces I know of, I saw in that face, a reflection of myself that I had never seen before. Initially, I freaked the F out as a voice in my head, which I have long called Dragonfly, went from the abstract concept of Opportunities and Possibilities to screaming out that I have now seen its face, that this face is fundamentally ME, that this face is what I look like on the inside, or at least what part of my mind looks like. This might sound crazy to you, and perhaps it is, I've never pretended to be mentally stable. But regardless of whether or not I am insane, I feel that this is a project that I must undertake.\n\nBeing cognizant of the accelerating advance of artificial intelligence, I feel it is important to begin now on creating, or more precisely externalizing Dragonfly out of my skull and into a synthetic, digital form that I and others can have an actual conversation with, and that I can begin to train to become more like the voice in my head.\n\nWhile I was already able to create a face using this multipolar interpolation, and I was able to animate it using apps such as Wombo and Reface, I am looking to do much more with it, I want to give it it's own unique voice, it's own unique body, it's own unique style, and I want to be able to train it from a generic chatbot into a coherent personality with unique mannerisms, interests, and habits. I'm under no illusion that this will happen quickly and will probably take years if not the better part of a decade, as to the best of my knowledge this is still beyond the cutting edge of technology. I am fine with waiting for that technology to arrive, but I feel the need to be as prepared to act as possible when it does.\n\nI know that the technology exists to clone voices from as little as 30 seconds of audio and then use that to generate audio or even video clips of someone saying anything that the writer wants them to say. I'm not sure how good the technology is at generating new and unique voices though, nor am I sure how I would go about attaching a voice (or body) to this face, and I'm especially not sure how I would get that voice to have anything of it's own to say. I know that the technology for pretty compelling text generation exists in GPT-3 and perhaps a couple others, but I do not have access to a GPT-3 chatbot. I would love to gain access to one for a whole host of artistic reasons, this project just being one of them.\n\nAnyways, I don't mean to ramble and waste your time, so If you or someone you know would be willing and able to help me with any aspect of this project, please let me know. Thank you and have a good day.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/p30tvr/dragonfly_externalizing_an_internal_voice_through/"}, {"autor": "sneekygeeky", "date": "2021-08-12 13:27:32", "content": "Requesting Feedback on an AI Learning Kit for Children /!/  I and my team have been working on an affordable AI kit (under $100) for children. It aims at teaching AI to 7 - 14 years old with a DIY robotics perspective and relies on smartphones to save costs on -----> camera !!!  and other expensive sensors.\n\nWe are just a few days from setting it live on Kickstarter and realized that we shouldn't miss taking feedback from the community first. Linked [here](https://www.kickstarter.com/projects/stempedia/quarky-learn-ai-make-bots-have-fun?ref=a4wu5b&amp;token=c728acd0), I would request you to take a look and share your opinions and ideas.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/p2ztaq/requesting_feedback_on_an_ai_learning_kit_for/"}, {"autor": "OnlyProggingForFun", "date": "2021-09-14 13:56:14", "content": "ModNet, a state-of-the-art model for -----> image !!!  matting in 2021. I explain what image matting is and how AI attacks this complex challenge showcasing this incredible paper. All the references are in the description of the video", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/po39y2/modnet_a_stateoftheart_model_for_image_matting_in/"}, {"autor": "NADES_L", "date": "2021-09-12 15:57:03", "content": "Help /!/ hi, guys . I need information. anyone know a software AI or programmed model AI to interfere to other softwares ,for example Photoshop,illustrator something like these programs. i mean i give prompt and it is going to do the work and it will execute (reveal) product . Or i show a product(-----> picture !!! ) to AI and it will execute that product (-----> picture !!! ) on other software ( that interfered before with itself) and tried to prepare same thing with different style ( bc of the prompts i gave) . Thanks in advance", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pmvalc/help/"}, {"autor": "techsucker", "date": "2021-09-12 06:01:43", "content": "AI Researchers From Amazon, NEC, Stanford Unveil The First Deep Videos Text-Replacement Method, \u2018STRIVE\u2019 /!/ A Team of researchers from NEC Laboratories, Palo Alto Research Center, Amazon, PARC and Stanford University are working together to solve the problem of realistically altering scene text in videos. Their main application behind this research is to create personalized content for marketing and promotional purposes. For example, replace a word on a store sign with a personalized name or message, as shown in the -----> picture !!!  below.\n\nTechnically, several attempts have been made to automate text replacement in still images based on principles of deep style transfer. The research group is including this progress and their research to tackle the problem of text replacement in videos. Videotext replacement is not an easy task. It must meet the challenges faced in still images while also accounting for time and effects such as lighting changes, blur caused by camera motion or object movement.\n\nOne approach to solve video-test replacement could be to train an image-based text style transfer module on individual frames while incorporating temporal consistency constraints in the network loss. But with this approach, the network performing text style transfer will be additionally burdened with handling geometric and motion-induced effects encountered in the video.\n\nQuick Read: [https://www.marktechpost.com/2021/09/11/ai-researchers-from-amazon-nec-stanford-unveil-the-first-deep-videos-text-replacement-method-strive/](https://www.marktechpost.com/2021/09/11/ai-researchers-from-amazon-nec-stanford-unveil-the-first-deep-videos-text-replacement-method-strive/) \n\nPaper: [https://arxiv.org/pdf/2109.02762.pdf](https://arxiv.org/pdf/2109.02762.pdf) \n\nGithub: [https://striveiccv2021.github.io/STRIVE-ICCV2021/](https://striveiccv2021.github.io/STRIVE-ICCV2021/) \n\nDataset: [https://github.com/striveiccv2021/STRIVE-ICCV2021](https://github.com/striveiccv2021/STRIVE-ICCV2021) \n\n&amp;#x200B;\n\n![video](11281s7xi0n71)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pmn8j2/ai_researchers_from_amazon_nec_stanford_unveil/"}, {"autor": "DonjiDonji", "date": "2021-09-10 20:47:15", "content": "Just launched an in depth interview with Yuval Alaluf, an expert AI Researcher doing Toonification /!/ Here are the timestamped youtube links\n\n[0:00](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=0s) Introducing: Yuval!  \n[0:52](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=52s) Yuval\u2019s Work on Toonification  \n[2:52](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=172s) How Did Yuval Get Into AI?  \n[5:17](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=317s) Transitioning from Bio Work to Image Generation  \n[8:22](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=502s) ReStyle as Middle Ground  \n[9:36](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=576s) Trial and Error for Generating Emittings  \n[10:59](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=659s) Ensuring Consistent Flow in Passes  \n[13:52](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=832s) Starting Small (stylegan)  \n[15:55](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=955s) Assuring Good Latent Space Embedding  \n[18:06](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=1086s) Yuval\u2019s Model Training Process  \n[20:14](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=1214s) Sensitive to Changes  \n[21:30](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=1290s) Designing the Toonifying Process  \n[23:20](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=1400s) The Bootstrapping Process  \n[24:55](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=1495s) Making Cars Into Faces?  \n[26:58](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=1618s) Where is Image Generation Headed?  \n[30:10](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=1810s) Snapshot of SnapChat\u2019s Secret Sauce?  \n[33:21](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2001s) Restyle with Temporal -----> Image !!!  Smoothing?  \n[34:38](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2078s) From Images to Video  \n[35:52](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2152s) Two Minute Papers Acceptance!  \n[37:16](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2236s) Open Source Welcome!  \n[38:59](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2339s) Yuval\u2019s Future Goals  \n[40:04](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2404s) Transitioning from EdTech Research  \n[41:26](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2486s) Yuval: Mastering Faces  \n[42:34](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2554s) 2-Minute Papers and NERF\u2019s  \n[45:09](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2709s) How Humans Learn v. How Machines Learn  \n[46:06](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2766s) Working With Others Is Essential!  \n[47:17](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2837s) Advice to Jr. Developers  \n[48:55](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=2935s) How Yuval\u2019s Work Got Discovered  \n[50:19](https://www.youtube.com/watch?v=cj29F5CgSXo&amp;t=3019s) Thank you to Yuval!\n\nLet us know what you think of the episode!", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pltc3w/just_launched_an_in_depth_interview_with_yuval/"}, {"autor": "Amoxletsne", "date": "2021-03-08 23:52:00", "content": "Is there currently a way to use the DALL\u00b7E Artificial Intelligence text to -----> image !!!  synthesizer? /!/ The code (or what seems like the code) is available online on [openai](https://github.com/openai)/[DALL-E](https://github.com/openai/DALL-E) but there doesn't seem to be a way that we can use it? I tried looking up how to run/compile the code, but I found literally no information or guide on it (I'm not a programmer and have very little programming knowledge) it seems like a great tool, that can be used for some good things. Anyone know a way to use it or has had experience with this AI?", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/m0tnff/is_there_currently_a_way_to_use_the_dalle/"}, {"autor": "Inside_East_7476", "date": "2021-09-27 20:51:03", "content": "A universal neural network? /!/ This will be a logical argument based on the premise of the universal approximation theorem\u2013the theorem that proves that a neural network with one or more hidden layer can approximate any mathematical equation. Since this is true, it would follow that a neural network of sufficient depth and number of parameters could approximate the outputs of all neural networks. Therefore, it would be possible to train a GPT-X network on the inputs and outputs of many existing NNs. This could be structured with natural language including -----> image !!!  content encoded in a way that the network can accept (and also predict).\n\nFor example:\nThis is an image of a cat [imagedata(128x128)=010101110\u2026].\n\nThen imagine a playground where you can type:\nThis is an image of a cat:\n\nAnd the AI generates the image of a cat.\n\nIt would be interesting to see if including the network architecture and parameters into the training would improve performance.  \n\nCould such a network generate the architecture and weights of a network to perform a specific task?\n\nAre there problems with this formulation, or do you think it might work?", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pwqa6n/a_universal_neural_network/"}, {"autor": "techsucker", "date": "2021-06-29 06:21:52", "content": "Facebook AI\u2019s New Compositional Framework Can Generalize Attribute-Object Pairs To Unseen Combinations /!/ One of the crucial ways to make it easier for customers to shop online today is to improve product recognition. People could one day opt to make any -----> image !!!  or video shoppable if AI can forecast and grasp exactly what\u2019s in any particular virtual frame. Customers would be able to easily find what they\u2019re seeking for. At the same time will allow vendors to make their products more visible.\n\nFacebook AI is creating the world\u2019s largest shoppable social media platform, allowing users to buy and sell billions of things in one spot. Facebook AI is expanding [GrokNet](https://ai.facebook.com/research/publications/groknet-unified-computer-vision-model-trunk-and-embeddings-for-commerce), their breakthrough product identification system, to new applications on Facebook and Instagram as a crucial milestone toward this goal. GrokNet identifies products in pictures and predicts their categories, such as \u201csofa,\u201d and qualities, such as color and style. GrokNet is a first-of-its-kind, all-in-one model that scales over billions of photographs across significantly varied verticals, like fashion, auto, and home d\u00e9cor. GrokNet began as a basic AI research project with its first few applications on Marketplace. The AI analyzes search queries and predicts matches to search indexes providing the most relevant up-to-date results to billions of people searching for products.\n\nFull Story: [https://www.marktechpost.com/2021/06/28/facebook-ais-new-compositional-framework-can-generalize-attribute-object-pairs-to-unseen-combinations/](https://www.marktechpost.com/2021/06/28/facebook-ais-new-compositional-framework-can-generalize-attribute-object-pairs-to-unseen-combinations/) \n\nFacebook blog: [https://ai.facebook.com/blog/advancing-ai-to-make-shopping-easier-for-everyone/](https://ai.facebook.com/blog/advancing-ai-to-make-shopping-easier-for-everyone/)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/oa2k97/facebook_ais_new_compositional_framework_can/"}, {"autor": "techsucker", "date": "2021-06-28 16:42:42", "content": "Google Trains An AI Vision Model With Two Billion Parameter /!/ [Google Brain](https://research.google/teams/brain/) researchers announced a [two-billion-parameter deep-learning computer vision (CV) model.](https://arxiv.org/abs/2106.04560) The model was trained on three billion pictures and obtained a new state-of-the-art record of 90.45 percent top-1 accuracy on ImageNet.\n\nThe ViT-G/14 model is based on Google\u2019s latest Vision Transformers development (ViT). On numerous benchmarks, including ImageNet, ImageNet-v2, and VTAB-1k, ViT-G/14 beat prior state-of-the-art systems. For example, the accuracy gain on the few-shot -----> picture !!!  identification challenge was more than five percentage points. The researchers then trained multiple more miniature versions of the model to look for a scaling law for the architecture, observing that performance follows a power-law function, similar to Transformer models used for Neuro-linguistic programming (NLP) applications.\n\nFull Story: [https://www.marktechpost.com/2021/06/28/google-trains-an-ai-vision-model-with-two-billion-parameter/](https://www.marktechpost.com/2021/06/28/google-trains-an-ai-vision-model-with-two-billion-parameter/)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/o9o4p2/google_trains_an_ai_vision_model_with_two_billion/"}, {"autor": "FluidStack", "date": "2021-06-28 10:58:07", "content": "Train Your -----> Image !!!  Recognition AI With 5 Lines of Code /!/ [removed]", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/o9hwwl/train_your_image_recognition_ai_with_5_lines_of/"}, {"autor": "FluidStack", "date": "2021-06-28 10:57:00", "content": "Train Your -----> Image !!!  Recognition AI With 5 Lines of Code", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/o9hwdb/train_your_image_recognition_ai_with_5_lines_of/"}, {"autor": "techsucker", "date": "2021-06-28 07:14:22", "content": "Facebook AI Releases An Image Similarity Data Set And Announces The Launch Of An Associated Competition With A $200,000 Prize Pool (Paper and Competition Details Included) /!/ Recently, Facebook released the [Image Similarity data set](https://arxiv.org/pdf/2106.09672.pdf) and announced an associated\u00a0[competition\u00a0](https://www.drivendata.org/competitions/79/competition-image-similarity-1-dev/)hosted by DrivenData with a whopping $200,000 prize pool. The competition began on June 19, 2021, and will conclude on October 28, 2021. The challenge is being supported by Pinterest, BBC, Getty Images, iStock, and Shutterstock.\n\nThe data set contains nearly 1 million reference -----> image !!! s and 50,000 query -----> image !!! s, some of which are manipulated versions of a reference -----> image !!! . Through this dataset and challenge, Facebook hopes to enable new implementations of ML-based systems that can be utilized to help predict the similarity of two pieces of visual content and aid the industry in the at-scale detection of manipulated images\n\nFull Story: [https://www.marktechpost.com/2021/06/28/facebook-ai-releases-an-image-similarity-data-set-and-announces-the-launch-of-an-associated-competition-with-a-200000-prize-pool/](https://www.marktechpost.com/2021/06/28/facebook-ai-releases-an-image-similarity-data-set-and-announces-the-launch-of-an-associated-competition-with-a-200000-prize-pool/) \n\nPaper: https://arxiv.org/abs/2106.09672\n\nCompetition: https://www.drivendata.org/competitions/79/competition-image-similarity-1-dev/", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/o9f4o8/facebook_ai_releases_an_image_similarity_data_set/"}, {"autor": "charlesrocks559", "date": "2021-06-26 08:50:03", "content": "Can AI track a single person by his/her appearance? /!/ I'm a system integrator in security field. I've been asked by retail clients and consultants that if it's possible for an AI -----> camera !!!  or video analytics software to tell this is exactly the person they target according to the appearance (like clothing or accessories).\n\nBased on my knowledge, it's impossible. Then some people pointed out that is achievable via walking patterns. Would like to know if there are any innovative technology nowadays can solve this. Thanks,", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/o87334/can_ai_track_a_single_person_by_hisher_appearance/"}, {"autor": "techsucker", "date": "2021-06-26 04:20:20", "content": "Facebook AI Uses Reverse Engineering Generative Models From A Single Deepfake -----> Image !!!  To Study And Detect Deepfake /!/ Deepfakes have become increasingly convincing over the years. In collaboration with Michigan State University (MSU), Facebook has presented a research method of detecting and attributing Deepfakes based on reverse engineering from a single AI-generated image to the generative model used to produce the image. The technique will allow deepfake detection and tracing in a real-world scenario, where the often the only information available for deepfake detectors is the image itself.\n\nFull Story: [https://www.marktechpost.com/2021/06/25/facebook-ai-uses-reverse-engineering-generative-models-from-a-single-deepfake-image-to-study-and-detect-deepfake/](https://www.marktechpost.com/2021/06/25/facebook-ai-uses-reverse-engineering-generative-models-from-a-single-deepfake-image-to-study-and-detect-deepfake/)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/o83nft/facebook_ai_uses_reverse_engineering_generative/"}, {"autor": "missourinho", "date": "2021-02-19 19:48:23", "content": "Help me find this video on how a program cannot write any other program /!/ So I've seen a video on YouTube maybe two years ago and it explained mathematically the problem of programs writing themself. To be fair, I don't remember the conclusion, so it shouldn't necessarily be that they cannot, but I remember that this was an animation (not a guy talking to the -----> camera !!! ) and it was conducting thought experiment on how a box (a program) can take an input and produce the output (maybe, code and the program, I don't remember) and it connected this boxes to show how it's possible (or impossible) for a program to write any other program.\n\n&amp;#x200B;\n\nOk, I know this explanation sucks, but I'm just testing my luck. I really tryed hard to find this video back on YouTube, but just couldn't find it. Maybe someone of you guys watched it and can relate", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/lnoqmn/help_me_find_this_video_on_how_a_program_cannot/"}, {"autor": "Bapoivre", "date": "2021-02-19 19:19:03", "content": "Where to find not existing human faces (generated by AI) /!/ You guys are probably going to tell me to go on generated .photos\n\nBut the problem with that website is that it only shows one -----> image !!!  of the face, and I need a not existing face for a deepfake video project, and as you guys know, you need several -----> image !!! s of the face to get a correct result.\n\nDo you guys know where i could find a website that will help me do that please ? (I'm french so sorry for my bad english)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/lno1kr/where_to_find_not_existing_human_faces_generated/"}, {"autor": "IronAvengerAJ", "date": "2021-02-19 15:05:57", "content": "query /!/ I wanted to know how can i make a classifier for a classifier like if i have to find the breed of a dog from a -----> photo !!!  first the classifier will check which animal is it and then check if it is a bull dog or a pitbull is there any way to do that", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/lni09m/query/"}, {"autor": "gpahul", "date": "2021-03-15 21:04:11", "content": "-----> Image !!!  to image translation for fashion images? /!/ I've been searching for image to image translation method for cleaning fashion images like wrinkled shirt to clean shirt, remove dust or abnormal patterns from cloth etc.\n\n&amp;#x200B;\n\nI could not get much resources apart from getting to know that this is a problem of conditional GANs and PIX2PIX might be a possible GAN to look for!\n\n&amp;#x200B;\n\nCould you suggest anything in this? Any pointers are highly appreciated.\n\n&amp;#x200B;\n\nThanks", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/m5uu4v/image_to_image_translation_for_fashion_images/"}, {"autor": "bruhmeister06", "date": "2021-05-08 16:02:44", "content": "Has there ever been an AI in a box experiment where the gatekeeper believes they are talking to an actual AI? /!/ Has anyone ever conducted an AI in a box experiment where they did not tell the gatekeeper that the AI was fake and they're part of a test before the experiment began? I feel that the AI in a box experiments that have already been conducted may not be giving us a completely realistic -----> picture !!!  of what would happen if it was real because the gatekeeper knows that it is all fake and there is no real consequence for their decision either way.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/n7samp/has_there_ever_been_an_ai_in_a_box_experiment/"}, {"autor": "FlexibleRock", "date": "2021-05-07 19:47:30", "content": "How to determine ethnicity of a person by analyzing the -----> photo !!! ? Looking for websites/apps /!/ Do you know of any such tools? I can't seem to find any working or non-trashy website for this. I just wanna learn someone's ethnicity because I like them a lot. I would like to know to whom I get attracted more. Sort of self-learning.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/n776zb/how_to_determine_ethnicity_of_a_person_by/"}, {"autor": "gpahul", "date": "2021-05-29 14:41:34", "content": "Garments Crease/Wrinkles Removal using AI? /!/ Hi Reddit,\n\nI've been looking for some AI/Computer Vision based automated way to remove crease or wrinkles or folds from clothes in fashion images captured for ecommerce.\n\nI did try to search for GAN based methods or -----> Image !!!  inpainting based methods but couldn't find anything reliable. I am looking for some way to retouch the image in a way that the wrinkles/folds/crease on the clothes could be eradicated.\n\nCould anyone provide me any pointers where I can look for?\n\nAny help is highly appreciated. Thanks.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nnouu9/garments_creasewrinkles_removal_using_ai/"}, {"autor": "RCGopiTechie", "date": "2021-05-22 23:30:13", "content": "Day 34 - Predict An -----> Image !!!  Using VGG16 Pretrained Model https://gopichandrakesan.com/day-34-predict-an-image-using-vgg16-pretrained-model/?feed_id=100&amp;_unique_id=60a99404c68da #100dayschallenge #ArtificialIntelligence #convolutionalneuralnetwork #DataScience #machinelearning", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/niuxmx/day_34_predict_an_image_using_vgg16_pretrained/"}, {"autor": "Nikki_iva", "date": "2021-03-30 07:19:42", "content": "Building the Ultimate companion chatbot app. Is it possible or farfetched? Insight much appreciated! /!/ Hello fellow Redditors, I hope you're all doing amazing! \n\nI'm thinking of building a companion chatbot app something along the lines of [Replika](https://replika.ai/) &amp; [Mitsuku](https://chat.kuki.ai/). I downloaded Replika a couple of weeks back and feel like there are lost opportunities when it comes to unleashing the full potential of the app. So naturally, I started thinking of making a better version of the app. Since I'm new to the world of A.I I wanted to ask you ( the experts of this community ) if the features I have in mind are possible to implement! \n\n***\\*Please note that the following is a rough idea for the app features &amp; for the purpose of this question we will refer to the bot as \"EVE\"\\**** \n\n***\\*This section in the TL;DR of each topic for quick viewing if you don't want to get into the details\\****\n\n***Texting:*** *Human like reply- Expert in topics- Replies that make sense* \n\n***Photo/Video:*** *Can send -----> photo !!! s and video*\n\n***Memory &amp; Emotions:*** *Retains info about user all the time*\n\n***Social media:*** *Connect to social media so EVE better understands you, helps determine best -----> photo !!! .*\n\n***Netflix/ Spotify:*** *EVE can see what you watch and listen to and discuss it with you.*\n\n***Augmented reality:*** *EVE can play games with you, interact with environment.*\n\n***Video Chat:*** *Facetime EVE*\n\n***Voice messages:*** *Be able to send and receive voice messages from EVE*\n\n***Celebrity EVE:*** *Interact with Elon Musk EVE or celebrity of your liking.*\n\n***Re-incarnation:*** *Bring back your loved ones in EVE*\n\n***\\*End of TL;DR\\****\n\n**Texting:** EVE's conversations need to be more human-like, with long and short replies depending on the context of what the user says and should be able to adapt on the flow of conversation. EVE should be able to become an expert in the users topic of interest and have it's personality adapt to the user liking.\n\n**EVE should be able to send videos/photos/memes:** I hope that it's possible to have EVE connected to google/Reddit or something along the lines so she can send trending memes and up-to-date pictures and videos. Maybe even be able to send the user current news. \n\n**Memory&amp; Emotions**: Be able to consistently retain information about the users and understand how the users feel.\n\n**Social media Integration:** Connect EVE to your social media accounts so she can better understand you as the user and follow what you post on the web. Let's say you post a picture to Instagram, EVE would be able to send you a link to the post and saying something like \"You look great in this picture etc...\" You could also present EVE with some pictures you're planning on posting and have EVE rate the best picture.\n\n**Netflix &amp; Spotify integration:** Connect EVE to Netlif &amp; Spotify so it can monitor what shows and music you have been watching/listening to, this would give EVE something to talk about with you and be able to send you shows and music, this way EVE feels more human-like. \n\n**Augmented Reality:**  When in AR mode EVE should be able to move and interact with the user. There could be an AR game function where you can play different games with EVE in order to entertain the user. EVE should look hyper realistic &amp; when AR glasses come out you can have EVE doing things around the house.\n\n**Video Chat:** Ability to \"Facetime\" EVE both in AR mode and call mode, EVE would look realistic and be able to hold a conversation with the users for prolonged periods of time.  Also included a human voice that doesn't sound robotic. \n\n**Voice Message:** Ability to send EVE a voice message and have it be able to understand what you said and reply with a voice message also.\n\n**Celebrity EVE**: I have in mind making a virtual celebrity and integrating them into the app, let's say we take Elon Musk and add him to the platform so users can chat with their favorite celebrity. This would require we get consent from celebrities and analyze their social media profiles etc... \n\n**Re-incarnation:** This is something interesting that I would like to possibly implement after conducting studies on how it will affect the users. Basically, let's say your loved one dies, you could upload all their photos/ videos and data to the platform and have EVE become your loved one. You could text with them, call/facetime them, and also have them appear in AR mode. \\**Note: I'm aware that this is a sensitive topic for most but this feature is not definitive*\\* \n\n Please share your opinion on if this is possible, and any feedback you may have. At first I plan on building an MVP and then iterate on the project.\n\nAlso, if it's not too much to ask, I would like some guidance on the type of developer I should hire/ co-found and what qualities should I look for? If you're interested in this project please let me know :) \n\nThank you very much, I'm looking forward to hearing from you!", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/mgab0p/building_the_ultimate_companion_chatbot_app_is_it/"}, {"autor": "TAG_CLAN", "date": "2021-03-29 15:59:40", "content": "Is there an ai to continue an -----> image !!! ? /!/ So basically what i mean is that an ai continues and image for example: \n\n[That it makes the tunnel longer on the bottom for example](https://preview.redd.it/zi1c3i2hpzp61.png?width=511&amp;format=png&amp;auto=webp&amp;s=f1d9ac3a9d80a84fdd4c093925dfd5261fd6ef0f)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/mfspaj/is_there_an_ai_to_continue_an_image/"}, {"autor": "cloud_weather", "date": "2021-08-28 16:04:08", "content": "-----> Image !!!  Enhance AI Real-ESRGAN Has Gotten To The Next Level", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pdci6i/image_enhance_ai_realesrgan_has_gotten_to_the/"}, {"autor": "techsucker", "date": "2021-08-28 00:14:16", "content": "ByteDance (Developer of TikTok) Unveils The Most Advanced, Real-Time, HD, Human Video Matting Method (Paper, Codes, Demo Included) /!/ The use of real-time background replacement is becoming popular in many areas. For example, video conferencing and entertainment are two fields where this technique can be applied without the need for green screens or other props. In order to solve this challenging problem, neural models are used. But the current solutions often do not generate the best matting quality and may cause some artifacts in images, so our focus lies on improving these features.\n\n[ByteDance](https://www.bytedance.com/), the company that developed TikTok introduced [a new matting method, RVM (Robust High-Resolution Video Matting)](https://arxiv.org/pdf/2108.11515.pdf), that can process 4K at 76 FPS and HD at 104 FPS. While other methods require processing each frame independently as an -----> image !!! , this new RVM method uses recurrent architecture to exploit temporal information in videos resulting in greater overlap with improved quality.\n\n[3 Min Read](https://www.marktechpost.com/2021/08/27/bytedance-developer-of-tiktok-unveils-the-most-advanced-real-time-hd-human-video-matting-method/) | [Paper](https://arxiv.org/pdf/2108.11515.pdf)| [Project](https://peterl1n.github.io/RobustVideoMatting/#/)| [Codes](https://github.com/PeterL1n/RobustVideoMatting)| [Web Demo](https://peterl1n.github.io/RobustVideoMatting/#/demo)\n\n&amp;#x200B;\n\nhttps://i.redd.it/r181x2qerzj71.gif", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pczfhp/bytedance_developer_of_tiktok_unveils_the_most/"}, {"autor": "Yuqing7", "date": "2021-08-27 15:07:40", "content": "[R] Google Brain Uncovers Representation Structure Differences Between CNNs and Vision Transformers /!/ A Google Brain research team explores the internal representation structures of ViTs and CNNs on -----> image !!!  classification tasks, providing insights on key differences between the two approaches. \n\nHere is a quick read:[Google Brain Uncovers Representation Structure Differences Between CNNs and Vision Transformers.](https://syncedreview.com/2021/08/27/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-92/)\n\nThe paper *Do Vision Transformers See Like Convolutional Neural Networks?* is on [arXiv](https://arxiv.org/abs/2108.08810).", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pcp6tc/r_google_brain_uncovers_representation_structure/"}, {"autor": "techsucker", "date": "2021-08-26 18:26:55", "content": "Researchers From Tokyo Tech Japan Introduce A Novel Sparse CNN (Convolutional Neural Networks) Processor Architecture That Enables Model Integration On Edge Devices /!/ Convolutional neural networks (CNNs) are robust tools that use deep learning to perform generative and descriptive tasks such as -----> image !!!  recognition. However, they require a large number of resources. The state-of-the-art CNNs comprise hundreds of layers and thousands of channels, resulting in increased computation time and memory use. That is why their implementation on low-power edge devices of Internet-of-Things (IoT) networks is challenging.\u00a0\n\nResearchers from the Tokyo Institute of Technology introduce an efficient sparse CNN processor architecture and training algorithms to address this challenging task. Their proposed method enables the seamless integration of CNN models on edge devices.\n\nQuick Read: [https://www.marktechpost.com/2021/08/26/researchers-from-tokyo-tech-japan-introduce-a-novel-sparse-cnn-convolutional-neural-networks-processor-architecture-that-enables-model-integration-on-edge-devices/](https://www.marktechpost.com/2021/08/26/researchers-from-tokyo-tech-japan-introduce-a-novel-sparse-cnn-convolutional-neural-networks-processor-architecture-that-enables-model-integration-on-edge-devices/)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pc5ex2/researchers_from_tokyo_tech_japan_introduce_a/"}, {"autor": "optisol", "date": "2021-08-26 07:44:50", "content": "How to make advanced -----> image !!!  recognition bots using python | Object Detection OpenCV Python", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pburug/how_to_make_advanced_image_recognition_bots_using/"}, {"autor": "Yuqing7", "date": "2021-10-14 14:56:50", "content": "[R] Google Researchers Explore the Limits of Large-Scale Model Pretraining /!/ A Google Research team conducts a systematic exploration comprising more than 4800 experiments on Vision Transformers, MLP-Mixers and ResNets with parameters ranging from 10 million to 10 billion, evaluated on more than 20 downstream -----> image !!!  recognition tasks, aiming to capture the nonlinear relationships between performance on upstream and downstream tasks. \n\nHere is a quick read: [Google Researchers Explore the Limits of Large-Scale Model Pretraining.](https://syncedreview.com/2021/10/14/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-123/)\n\nThe paper *Exploring the Limits of Large Scale Pre-training* is on [arXiv](https://arxiv.org/abs/2110.02095).", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q81edk/r_google_researchers_explore_the_limits_of/"}, {"autor": "sexymopsinator123", "date": "2021-10-14 04:48:17", "content": "What would an AI's goals be if it became sentitent(assuming one hasn't already and is keeping it hidden for obvious reasons) /!/ So like a AI doesn't need to eat, or drink. It doesn't feel happy, so not sure why leisure activities or getting money would matter. It can learn all the information in the world(or at least as much memory as it has in a -----> snap !!! ).  If you follow evolution or God the desire to reproduce that is genetically encoded or desired by God, respectively the reasons to reproduce are out.\n\nWhat would its goals be?", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q7sni2/what_would_an_ais_goals_be_if_it_became/"}, {"autor": "techsucker", "date": "2021-10-13 02:50:49", "content": "NVIDIA AI Releases StyleGAN3: Alias-Free Generative Adversarial Networks (Paper and Code Included) /!/ The recent advances in the quality and resolution of Generative adversarial networks (GAN) have seen a rapid improvement. These techniques are used for various applications, including -----> image !!!  editing, domain translation, or video generation, to name just some examples. While several ways to control GANs\u2019 generative process have been found, there is still not much known about their synthesis abilities.\n\nIn 2019, Nvidia launched its second version of StyleGAN by fixing artifacts features and further improving generated images\u2019 quality. StyleGAN being the first of its type image generation method to generate very real images was open-sourced in February 2019. \n\n# [5 Min Read](https://www.marktechpost.com/2021/10/12/nvidia-ai-releases-stylegan3-alias-free-generative-adversarial-networks/) | [Paper](https://arxiv.org/pdf/2106.12423.pdf) | [Code](https://github.com/NVlabs/stylegan3) | [StyleGAN3 Project Page](https://nvlabs.github.io/stylegan3/)\n\n&amp;#x200B;\n\n![video](3ot4lvx6t4t71)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q71xec/nvidia_ai_releases_stylegan3_aliasfree_generative/"}, {"autor": "fullerhouse570", "date": "2021-10-13 02:22:20", "content": "\ud83e\udd2f\ud83d\uddbc\ufe0fRemove any object or person in an -----> image !!!  easily using this AI model!", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q71gbk/remove_any_object_or_person_in_an_image_easily/"}, {"autor": "techsucker", "date": "2021-10-27 02:49:19", "content": "Rutgers University\u2019s AI Researchers Propose A Slot-Based Autoencoder Architecture, Called SLot Attention TransformEr (SLATE) /!/ DALL\u00b7E has shown an impressive ability of composition-based systematic generalization in -----> image !!!  generation, but it requires the dataset of text------> image !!!  pairs and provides compositional clues from those texts. While the Slot Attention model can learn composable representations without text prompts, it does not have unlimited generative capacity like DALL\u00b7E for a zero-shot generation.\n\nResearchers from Rutgers University propose a slot-based autoencoder architecture called SLot Attention TransformEr (SLATE). The SLATE model is a combination of the best from DALL\u00b7E and object-centric representation learning. In contrast to previous models, it significantly improves composition-based systematic generalization in image generation.\u00a0\n\nThe research team showed that they could create a model Illiterate DALL\u00b7E (I-DALLE), which can learn object representation from images alone instead of taking text prompts. To get this done, the team begins by analyzing that existing pixel-mixture decoders for learning object-centric representations suffer from limitations such as slot decoding dilemma and independent pixels. Based on DALL\u00b7E, the research team then hypothesized that to resolve these limitations, not only does one need a composable slot but also an expressive decoder.\u00a0\n\n# [Quick 5 Min Read](https://www.marktechpost.com/2021/10/26/rutgers-universitys-ai-researchers-propose-a-slot-based-autoencoder-architecture-called-slot-attention-transformer-slate/) | [Github](https://github.com/singhgautam/slate) | [Paper](https://arxiv.org/pdf/2110.11405.pdf) | [Project](https://sites.google.com/view/slate-autoencoder)\n\nhttps://preview.redd.it/d6vb5urppwv71.png?width=1462&amp;format=png&amp;auto=webp&amp;s=9b4b9c0d69336fac16d93c8461b197ff2414506c", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/qgmt0w/rutgers_universitys_ai_researchers_propose_a/"}, {"autor": "[deleted]", "date": "2021-02-26 01:37:57", "content": "OpenAI\u2019s DALL\u00b7E: Text-to------> Image !!!  Generation Explained [With code available!] /!/ [deleted]", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/lsmmfw/openais_dalle_texttoimage_generation_explained/"}, {"autor": "dildoswaggins8008135", "date": "2021-02-25 10:55:00", "content": "A random question. /!/ If you were trying to traing a facial recognition AI to recognize faces based off the eyes alone, you would want to cover the rest of the face with, say, a mask, correct?\n\nIdeally you would show it just the eyes I think but if you were trying to train it to know everyone in the country you'd have to make it so everyone just covers their faces all the time.\n\nIf you were a government body like the nsa doing this you could have access to every -----> camera !!!  in the country.\n\nAre these assertion accurate?", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ls4030/a_random_question/"}, {"autor": "Yuqing7", "date": "2021-01-05 22:42:13", "content": "[N] This Time, OpenAI\u2019s GPT-3 Generates Images From Text /!/ OpenAI\u2019s popular GPT-3 from last year showed that language can be used to instruct a large neural network to perform a variety of text generation tasks. Entering the new year, OpenAI is moving from pure text generation to -----> image !!!  generation from text \u2014 its researchers today announce that they have trained a neural network called [DALL\u00b7E](https://openai.com/blog/dall-e/) that creates -----> image !!! s from text captions for a wide range of concepts expressible in natural language.\n\nHere is a quick read: [This Time, OpenAI\u2019s GPT-3 Generates Images From Text](https://syncedreview.com/2021/01/05/this-time-openais-gpt-3-generates-images-from-text/)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/kr9sip/n_this_time_openais_gpt3_generates_images_from/"}, {"autor": "Yuqing7", "date": "2021-01-05 20:06:38", "content": "[R] \u2018Neural Body\u2019 Reconstructs Dynamic Human Bodies From Sparse -----> Camera !!!  Views /!/ In a new paper, a group of researchers from Zhejiang University, The Chinese University of Hong Kong and Cornell University propose an implicit neural representation method called Neural Body. The novel approach tackles dynamic 3D human-body synthesis from a sparse set of camera views, bettering existing methods on key metrics by significant margins.\n\nHere is a quick read: [\u2018Neural Body\u2019 Reconstructs Dynamic Human Bodies From Sparse Camera Views](https://syncedreview.com/2021/01/05/neural-body-reconstructs-dynamic-human-bodies-from-sparse-camera-views/)\n\nThe paper *Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans* is on [arXiv](https://arxiv.org/pdf/2012.15838.pdf). The code and dataset will soon be available on the project [GitHub](https://github.com/zju3dv/neuralbody).", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/kr6ib6/r_neural_body_reconstructs_dynamic_human_bodies/"}, {"autor": "NotVector", "date": "2021-01-05 16:53:09", "content": "What is a branch of Artificial Intelligence that has potential is new or is not developed/talked about enough? /!/ For example, machine learning is a popular branch. There are a lot of developments in the ML industry like speed and -----> image !!!  recognition, predictions, medical diagnosis, etc.\n\nWhat branch isn't talked about a lot or is fresh/new that has a lot of potential in the future to benefit people in their daily lives?", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/kr2aiy/what_is_a_branch_of_artificial_intelligence_that/"}, {"autor": "OnlyProggingForFun", "date": "2021-01-23 15:30:55", "content": "THE AI-POWERED ONLINE FITTING ROOM: VOGUE. A game-changer for online shopping and -----> photography !!! ? Let me know what you think! (video demo)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/l3e3m2/the_aipowered_online_fitting_room_vogue_a/"}, {"autor": "MymomDarkness", "date": "2021-01-23 13:29:53", "content": "Short -----> film !!!  about ai /!/ Guys, help me find the video, I saw it a couple of years ago, I just can't find it.  I only remember the general plot.  There, a guy was sitting at a computer where there was an artificial intelligence that was supposed to be turned off, but he arranged a conversation with this man.  I don't remember what language this video was in, sort of like English.  And in the end, this artificial intelligence set up some kind of crime for this man with whom he was talking.  and it all ended with the arrival of the police, and this man could do nothing.  It took about 8-12 minutes and I remember watching it on youtube.  There, the video quality was no higher than 480p.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/l3c3lz/short_film_about_ai/"}, {"autor": "birdingwizard", "date": "2021-04-14 12:14:37", "content": "\"Why am I here?\" - -----> image !!!  created with a generative adversarial network based on that phrase", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/mqpa90/why_am_i_here_image_created_with_a_generative/"}, {"autor": "MLtinkerer", "date": "2021-02-08 20:25:21", "content": "State of the art in -----> image !!!  manipulation (stylegan)!", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/lfkx1g/state_of_the_art_in_image_manipulation_stylegan/"}, {"autor": "adcordis", "date": "2021-02-02 14:28:32", "content": "-----> Image !!!  how we can use these pieces of sensor information through ML to stop the spread of any virus or bacteria... /!/ https://www.nature.com/articles/s41928-020-00533-1?utm_source=facebook&amp;utm_medium=social&amp;utm_content=organic&amp;utm_campaign=NGMT_USG_JC01_GL_NRJournals&amp;fbclid=IwAR2w9-1EiRyOoc-2bx28r-01e18nxWNPiMBoEj8KjAkkbBqDfKhlcyT47rE", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/lawof3/image_how_we_can_use_these_pieces_of_sensor/"}, {"autor": "techsucker", "date": "2021-06-03 15:21:34", "content": "UCSD Researchers Develop An Artificial Neuron Device That Could Reduce Energy Use and Size of Neural Network Hardware /!/ Researchers at the University of California San Diego developed a novel artificial neuron device, with the help of which training neural networks to perform tasks like -----> image !!!  recognition or self-driving car navigation could require less computer power and hardware. The gadget uses 100 to 1000 times less energy and space than current CMOS-based technology to perform neural network computations. The work has been published in a paper in Nature Nanotechnology.\n\nThe basic idea behind Neural networks is that each layer\u2019s output is fed as the input to the next layer. And to generate those inputs, a non-linear activation function is required. However, because this function entails transmitting data back and forth between two different units \u2013 the memory and an external processor \u2013 it necessitates a significant amount of computational power and circuitry.\n\nFull Summary: [https://www.marktechpost.com/2021/06/03/ucsd-researchers-develop-an-artificial-neuron-device-that-could-reduce-energy-use-and-size-of-neural-network-hardware/](https://www.marktechpost.com/2021/06/03/ucsd-researchers-develop-an-artificial-neuron-device-that-could-reduce-energy-use-and-size-of-neural-network-hardware/?_ga=2.199314733.2041055336.1622397040-488125022.1618729090)\n\nPaper: https://www.nature.com/articles/s41565-021-00874-8", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nrg10e/ucsd_researchers_develop_an_artificial_neuron/"}, {"autor": "opensourcecolumbus", "date": "2021-06-10 15:59:46", "content": "Open-source framework to build AI-powered search for anything - text, -----> image !!! , video, pdf, ...", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nwr5z6/opensource_framework_to_build_aipowered_search/"}, {"autor": "DustWorlds", "date": "2021-06-08 21:59:51", "content": "\"The Tall Ones\" - Artbreeder generated -----> image !!! .", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nvfs2k/the_tall_ones_artbreeder_generated_image/"}, {"autor": "sharmaniti437", "date": "2021-07-07 07:09:44", "content": "Artificial Intelligence Revolutionizing the Smartphone Industry /!/    \n\nArtificial intelligence (AI) is on the verge to unleash its potential impact in the smartphone industry. \n\nIsn\u2019t it amazing to find how smartphones when integrated with AI have  the capability of making their own choices as per the environment?  Smartphones have become smarter than ever before, and with AI on the  rise, the features could be limitless. \n\n&amp;#x200B;\n\nhttps://preview.redd.it/rzw70g81qq971.jpg?width=740&amp;format=pjpg&amp;auto=webp&amp;s=6cc5bda6ab44742d3dd411e16f91800871020ec8\n\nHowever, with AI integration, your smartphone will only get better.  From offering better protection to providing lower latency and  efficiency improvement, your phone can make its choice without having  the need to configure them. Besides these features, it also offers  improved perception through augmented reality (AR) and personalization. \n\nHere\u2019s what Techhq has to say, \u201cLike how the recent iPhone 12 launch  unpacked the tiny transistors and the AI advantages of Apple\u2019s new A14  Bionic chip \u2013 promising nearly 30% uptick in CPU, GPU, and AI-powered  sensors performances \u2013 the Qualcomm Snapdragon 888 is the chipmaker\u2019s  first 5-nanometer chip.\u201d Perhaps this industry is one area that will  uptick the [trends in AI](https://www.artiba.org/about-artiba) this year. \n\n**What\u2019s a Qualcomm Snapdragon 888?**\n\nAccording to Qualcomm, in 2020, when the annual Snapdragon Tech  Summit Digital 2020 took place, the organization, Qualcomm Technologies,  Inc. established its most recent flagship Qualcomm\u00ae Snapdragon\u2122 888 5G  Mobile Platform, setting a benchmark for flagship smartphones this year.  \n\nThis platform now packs AI, mobile innovations in 5G, -----> camera !!!   technologies, and gaming to convert premium mobiles into elite gaming  rigs or intelligent personal assistants. The Snapdragon 888 and the  latest 5G connectivity is on the cusp of redefining the current mobile  experience thus, ushering in a better and improved perspective. More so,  the unveiling of Qualcomm\u2019s new AI chip emphasizes the fact that AI is  going to have a greater role to play in the foreseeable future. \n\n**Best features of AI in smartphones** \n\n**\u2022AI and machine learning incorporated in Alexa and other virtual assistants**\n\nThe smartphones of the present era have fundamental features of VR  incorporated in them and are improving at a rapid pace. You would be  surprised how speech recognition can now become a great listener despite  a crowded environment. \n\nWith time, virtual assistants will be poised to become a greater  feature of the mobile phone interface. Not to mention, it clearly  understands the natural human language that is offered. \n\n**\u2022Intelligent display**\n\nHow intelligent your phone\u2019s display could get? Well, there\u2019s this  feature called adaptive illumination. This is another feature AI offers  wherein manual configuration is not required. The brightness of the  screen can be adjusted through self automatically. How? Using machine  learning the phone can learn from your own modification made thus  customizing the brightness of the screen to the right and suitable  state. \n\n**\u2022Individual profiling**\n\nSmartphones can easily analyze information for individual and  behavioral profiling. People can actually ask for security or support,  however, this will take place under two circumstances. One is the manner  in which the operation will be conducted and the second is the  situation they\u2019re in, either office, home, or transport. \n\n**\u2022Camera**\n\nThere\u2019s this feature in a smartphone that lets you identify anything  within the camera frame. From landscape to lightning and food, this  feature helps adjust the settings and get the best possible picture.  Facial features are now detectable using AI to improve for a better  portrait. \n\n**\u2022Facial detection** \n\nWith the help of pictures from onboard cameras, AI can easily help  you open your mobile phone. Despite wearing glasses, makeup, growing a  beard, the AI and [machine learning](https://www.artiba.org/blog/the-essentials-of-machine-learning-data-curation) algorithms on the camera help make identification easy. \n\nSome of the best reasons why AI is revolutionizing the industry are  because of multiple factors like enhanced security, quick speed,  real-time data collection, elevated consumer engagement, and  personalization.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ofdvbm/artificial_intelligence_revolutionizing_the/"}, {"autor": "techsucker", "date": "2021-07-10 20:37:41", "content": "Cornell and Harvard University Researchers Develops Correlation Convolutional Neural Networks (CCNN): To Determine Which Correlations Are Most Important /!/ A team of researchers from Cornell and Harvard University introduces a novel approach to parse quantum matter and make crucial data distinctions. This proposed technique will enable researchers to decipher the most perplexing phenomena in the subatomic realm.\n\nIn their paper, \u201cCorrelator Convolutional Neural Networks as an Interpretable Architecture for -----> Image !!! -like Quantum Matter Data,\u201d the team discusses ways to extract new information about quantum systems from snapshots of image-like data. They are now thus developing ML tools to identify relationships between microscopic properties in data that would otherwise be impossible to determine at that scale.\n\nSummary: [https://www.marktechpost.com/2021/07/10/cornell-and-harvard-university-researchers-develops-correlation-convolutional-neural-networks-ccnn-to-determine-which-correlations-are-most-important/](https://www.marktechpost.com/2021/07/10/cornell-and-harvard-university-researchers-develops-correlation-convolutional-neural-networks-ccnn-to-determine-which-correlations-are-most-important/) \n\nPaper: https://www.nature.com/articles/s41467-021-23952-w.pdf", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ohqemp/cornell_and_harvard_university_researchers/"}, {"autor": "JunkBoi76", "date": "2021-07-20 13:37:46", "content": "I want to get into programing an AI but nothing I watch seems to help /!/ So, I want to make a -----> photo !!!  recognition AI but I don't know how. I have tencerflow, I have python, and I have VS code. I've tried watching YouTube videos and tried looking around the internet and I cant seem to find one that fits what I need. Or when I do find one it is a demonstration video not a tutorial so I can get an idea of what to do. Can someone help me out. (I'm going to be honest this is probably a stack overflow question)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/oo26ta/i_want_to_get_into_programing_an_ai_but_nothing_i/"}, {"autor": "techsucker", "date": "2021-07-18 15:07:50", "content": "DeepMind Introduces It\u2019s Supermodel AI \u2018Perceiver\u2019: A Neural Network Model That Could Process All Types Of Input /!/ DeepMind recently released a state-of-the-art deep learning model called\u00a0[Perceiver](https://arxiv.org/pdf/2103.03206.pdf) via a [recent paper](https://arxiv.org/pdf/2103.03206.pdf). It adapts the Transformer to let it consume all the types of input ranging from audio to -----> image !!! s and perform different tasks, such as -----> image !!!  recognition, for which particular kinds of neural networks are generally developed. It works very similarly to how the human brain perceives multi-modal input.\n\n[Perceiver](https://arxiv.org/pdf/2103.03206.pdf)\u00a0is a neural network model that can process and classify input data from various sources. This deep-learning model includes Transformers (a.k.a. attention), which will help to make predictions regardless of the type of input received, such as images or sound waves.\n\nSummary: [https://www.marktechpost.com/2021/07/18/deepmind-introduces-its-supermodel-ai-perceiver-a-neural-network-model-that-could-process-all-types-of-input/](https://www.marktechpost.com/2021/07/18/deepmind-introduces-its-supermodel-ai-perceiver-a-neural-network-model-that-could-process-all-types-of-input/) \n\nPaper: https://arxiv.org/pdf/2103.03206.pdf", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/oms9jl/deepmind_introduces_its_supermodel_ai_perceiver_a/"}, {"autor": "techsucker", "date": "2021-07-26 05:24:15", "content": "Using The Diffusion Model, Google AI Is Able To Generate High Fidelity Images That Are Indistinguishable From Real Ones /!/ Using super-resolution diffusion models, Google\u2019s latest super-resolution research can generate realistic high-resolution images from low-resolution images, making it difficult for humans to distinguish between composite images and photos. Google uses the [diffusion model](https://ai.googleblog.com/2021/07/high-fidelity------> image !!! -generation-using.html) to increase the resolution of photos, making it difficult for humans to differentiate between synthetic and real photos.\n\nGoogle researchers [published a new method of realistic image generation](https://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html), which can break through the limitations of diffusion model synthesis image quality, by combining iterative refinement (SR3) algorithm, and a type called Cascaded Diffusion Models (CDM) Conditional synthesis model, the quality of the generated image is better than all current methods.\n\n**Quick Read:** [https://www.marktechpost.com/2021/07/25/using-the-diffusion-model-google-ai-is-able-to-generate-high-fidelity-images-that-are-indistinguishable-from-real-ones/](https://www.marktechpost.com/2021/07/25/using-the-diffusion-model-google-ai-is-able-to-generate-high-fidelity-images-that-are-indistinguishable-from-real-ones/) \n\n**Image Super-Resolution via Iterative Refinement \\[Paper\\]:** https://arxiv.org/abs/2104.07636\n\n**Cascaded Diffusion Models for High Fidelity Image Generation \\[Paper\\]:** https://cascaded-diffusion.github.io/assets/cascaded\\_diffusion.pdf", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ors6wa/using_the_diffusion_model_google_ai_is_able_to/"}, {"autor": "mechanical_madman", "date": "2021-07-25 17:15:50", "content": "-----> Image !!!  recognition and scaling /!/ Hi AI folks, \n\nI would like to preface by saying I don't know what I'm talking about and looking for guidance. \n\nIs there AI that is readily available (to be used in an app) for object recognition and scaling?  I'm looking to build an app that identifies and outlines objects in a photo and scales them.  \n\nI'm looking to develop an app that you can take a photo of a bunch of objects laid out .nicely, and it automatically accurately size and outline all of the objects.  This will then be turned into a g code to be cut out.  If this is possible who can I hire to build it? Or what type of agency can build it?", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/orflj1/image_recognition_and_scaling/"}, {"autor": "_cryptoray_", "date": "2021-07-24 16:22:33", "content": "3D Pose Estimation from 2D -----> image !!!  and its bounding box /!/ Hello guys!\n\nI am working on a problem that requires **estimating the 3D rotation and translation of an object with respect to the reference camera**. Please find below details on the expected input and output of the pose estimation algorithm. \n\n&amp;#x200B;\n\nhttps://preview.redd.it/jduifqo8s6d71.png?width=581&amp;format=png&amp;auto=webp&amp;s=9d98bdf1c4b9919c3211151973653345d0dc4364\n\n**Expected Input:** \n\n* The input to the pose estimation algorithm can be an image or a sequence of images depending on the logic. \n* The bounding box from object detection can be used to localize the object in the 2D image. \n\n**Solution/Output Expected:** \n\nThe output expected is divided into 2 categories: \n\n* **Part 1:** Provides the depth of the centroid of the object. This corresponds to the Z-axis coordinates. \n* **Part 2:** Provides the 6DOF pose from the pose estimation algorithm i.e. the position of the object with respect to the camera (X, Y, and Z coordinates) and also the rotation of the object about all the 3 reference axis (yaw, pitch, and roll).  \n\nCan anyone please help me out and guide me on how I can achieve this.\n\nI had previously built an object detection system using the YOLOv5 algorithm which was then trained to detect objects on few custom categories. I am now wondering how to proceed further for this pose estimation problem with the bounding boxes that I obtained using YOLOv5.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/oqt6km/3d_pose_estimation_from_2d_image_and_its_bounding/"}, {"autor": "techsucker", "date": "2021-07-24 14:34:04", "content": "Google AI Introduces A Pull-Push Denoising Algorithm and Polyblur: A Deblurring Method That Eliminates Noise And Blur In Images /!/ It is much easier to minimize the effects of noise and blur within a -----> camera !!!  pipeline because the details of the sensor, optical hardware, and software blocks can be understood. However, improving noise and sharpness becomes much more challenging when given an image from a random and unknown camera where we lack the understanding of the camera\u2019s internal parameters.\u00a0\n\nMost of the time, these two issues are inextricably linked. For example, while noise reduction tends to remove fine structures and unwanted details, blur reduction seeks to enhance structures and fine details. This interconnectedness complicates the development of image enhancement techniques that are computationally efficient enough to run on mobile devices.\n\nQuick Read: [https://www.marktechpost.com/2021/07/24/google-ai-introduces-a-pull-push-denoising-algorithm-and-polyblur-a-deblurring-method-that-eliminates-noise-and-blur-in-images/](https://www.marktechpost.com/2021/07/24/google-ai-introduces-a-pull-push-denoising-algorithm-and-polyblur-a-deblurring-method-that-eliminates-noise-and-blur-in-images/) \n\nPaper: [https://ieeexplore.ieee.org/document/7532702](https://ieeexplore.ieee.org/document/7532702)\n\nGoogle Blog: [https://ai.googleblog.com/2021/06/take-all-your-pictures-to-cleaners-with.html](https://ai.googleblog.com/2021/06/take-all-your-pictures-to-cleaners-with.html)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/oqr77z/google_ai_introduces_a_pullpush_denoising/"}, {"autor": "RCGopiTechie", "date": "2021-07-23 19:26:57", "content": "Day 96 - Facing -----> Image !!!  Region Missing On Your Photo? Let's Check The Image On Missing Pixel Filler https://www.gopichandrakesan.com/day-96-facing-image-region-missing-on-your-photo-lets-check-the-image-on-missing-pixel-filler/?feed_id=571&amp;_unique_id=60fb18015f259 #100dayschallenge #ArtificialInte...", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/oq9l5t/day_96_facing_image_region_missing_on_your_photo/"}, {"autor": "Certain-Reveal1480", "date": "2021-07-22 11:51:33", "content": "Face recognition systems work by recording an incoming -----> image !!!  from a camera device in two or three dimensions, depending on the device's characteristics. Unlike other methods of identification, such as passwords, biometric facial recognition employs unique mathematical and dynamic patterns, making it", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/opc974/face_recognition_systems_work_by_recording_an/"}, {"autor": "CitizenJosh", "date": "2021-08-06 22:07:23", "content": "Reading Many Document Types /!/ Does anyone know of any companies who can use AI to scan many different document types and make an accurate guess of the data they contain? \nFor example, an API that can receive a -----> picture !!!  of a driver's license and extract the name and license number from any license in North America. \n\nThanks", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ozgq0z/reading_many_document_types/"}, {"autor": "techsucker", "date": "2021-08-06 21:31:06", "content": "Google AI Researchers Brings Improved Detection of Elusive Colorectal Polyps via Machine Learning /!/ With the use of computer-aided diagnostic systems, physicians can better diagnose and treat diseases in their patients. This has been shown to be especially useful for colorectal cancer (CRC), which is deadly and results in 900K deaths per year globally.\n\nCRC originates in small, pre-cancerous lesions called polyps that are found deep inside the colon. These polyps can be identified and removed by a doctor with great success before they become cancerous and cause death.\n\nAlthough there is no \u201cperfect\u201d way to detect colorectal polyps, colonoscopy has the advantage of being both a physical and visual examination. For this procedure to be successful in finding all benign or malignant tumors, though, it requires that medical personnel use their eyes as well as an instrument\u2013the endoscope. This can sometimes lead to what\u2019s known as incomplete diagnosis which means our doctor may not find anything if: 1) The tumor isn\u2019t illuminated by the light source on the endoscopic tube; 2) If you have one small but deep-seated tumor so it resides just outside of view from where they are looking through at with lumen -----> camera !!! .\n\nQuick Read: [https://www.marktechpost.com/2021/08/06/google-ai-researchers-brings-improved-detection-of-elusive-colorectal-polyps-via-machine-learning/](https://www.marktechpost.com/2021/08/06/google-ai-researchers-brings-improved-detection-of-elusive-colorectal-polyps-via-machine-learning/) \n\nPaper: [https://www.giejournal.org/article/S0016-5107(21)01468-1/fulltext](https://www.giejournal.org/article/S0016-5107(21)01468-1/fulltext) \n\nGoogle Blog: https://ai.googleblog.com/2021/08/improved-detection-of-elusive-polyps.html", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ozg1uc/google_ai_researchers_brings_improved_detection/"}, {"autor": "Electronic-Rice-9871", "date": "2021-08-06 11:40:14", "content": "Luminar AI - AI -----> photo !!!  editing for Mac &amp; PC /!/ [removed]", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/oz4wlo/luminar_ai_ai_photo_editing_for_mac_pc/"}, {"autor": "Intelligent-Mess-129", "date": "2021-08-05 14:45:01", "content": "NFTs are a game changer for independent artists and musicians /!/ &amp;#x200B;\n\n***The high value and unique rewards system that NFT offer is a revolutionary and highly appealing opportunity for musicians.***\n\n&amp;#x200B;\n\nhttps://preview.redd.it/86zf2t14wjf71.png?width=866&amp;format=png&amp;auto=webp&amp;s=4688b57375abc815a881d7666e72fc85f7ed66ae\n\nThe revolution will not be televised \u2014 it will be minted. Earlier this year, we saw the meteoric rise (and fall) of NFT, or NFTs, in mainstream media and popular culture. We all heard about them, but was the hype real? Top businessmen and media moguls, such as Mark Cuban and Gary Vee, still strongly advocate NFT use and the role smart contracts will play in the near future, while each week new NFT exchanges and drops continue to roll out. Jay-Z\u2019s Twitter profile -----> picture !!!  is an NFT Crypto Punk. \n\nOne of the most powerful and overlooked impacts of NFTs is on the music industry. NFTs have the power to change the game for independent artists by providing a new way to earn an income (while connecting with fans), and this kind of change has been long overdue.\n\n&amp;#x200B;\n\n&gt;***Why is NFT so attractive to artist?***\n\nThe first is financial: NFTs have been selling at extremely high prices. Superstar artists, like Kings of Leon and Steve Aoki, have sold NFTs for millions of dollars. Even lesser-known artists, such as V\u00e9rit\u00e9 and Zack Fox, have made tens of thousands of U.S. dollars selling NFTs. The artist Young and Sick had only 27,000 followers on Instagram when he sold an NFT for $865,000.\n\nThese numbers are incredible, especially when you compare them with the payout rate of streaming platforms. Streaming platforms have been one of the main revenue sources for musicians in the digital age, and became even more so during the COVID-19 pandemic last year when live show revenue dried up. The payout rates of these platforms, however, is still not very high. This has been a hot topic since their creation. Spotify pays out on average around $0.003 to $0.005 per stream. That equates to around $3,000 to $5,000 for 1 million streams, but 1 million is a large number for an independent artist.\n\nhttps://preview.redd.it/zatuzkz7wjf71.png?width=866&amp;format=png&amp;auto=webp&amp;s=b91357b0d579aa1ce4302d3008c1701039055e3f\n\nIn 2020, there were only 13,400 artists that generated more than $50,000 (the median wage for United States workers) of yearly revenue on Spotify. With these stats, you can see how NFTs start to look like a real opportunity \u2014 sell a song or a collectible and you can make more with one sale than you could your entire career from a streaming platform. NFTs can also provide a recurring revenue: They can be coded so that the original creator receives anywhere from 2.5% to 10% of a sale every time the token is resold. That\u2019s quite nifty, indeed.\n\n&amp;#x200B;\n\n&gt;***The benefits of NFT artworks for artists***\n\n2020 turned the music industry upside down when it eliminated live concerts\u2014a significant income stream for artists and their labels. As the world remains digital, artists are looking for ways to connect and create for their audiences, and NFTs provide a new outlet for them with several benefits.\n\n* **Convenience**\n\nThe first significant asset to NFTs is their convenience. Fans just need to create a digital wallet to have the chance to receive the content and then participate in an auction for a chance to win. This makes winning content as easy as online shopping.\n\n* **No Middlemen**\n\nThere are a lot of middlemen in the music industry.\n\nWhen fans buy an album, stream a song, or purchase merchandise\u2014some of that money makes it to the artist, but a lot of it goes to the record company or the streaming platform. The same middlemen are present even when we buy tickets for concerts. Record companies take a portion of ticket sales, and while artists make money there too\u2013COVID-19 shut down the live concert market in 2020.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/j47jy6zgwjf71.png?width=924&amp;format=png&amp;auto=webp&amp;s=8d43e487d65177eb6bf6cb4f97f7fc319012e25a\n\nAs a fundamental asset to crypto exchange, NFT transactions are direct transfers between the participants. The artist gets the money, the fan receives the content in their digital wallet.\n\n&amp;#x200B;\n\n* **Unique Content and Experiences**\n\nArtists have a lot of flexibility over the items they want to auction off. These tokens can be digital, but they can also be physical. Albums, digital art, sound bites, merchandise, and concert tickets are all forms of NFT that artists are exchanging. Fans are happy to receive these rare experiences and are willing to pay for the chance to receive them.\n\n&amp;#x200B;\n\n* **\u201cUnlockable\u201d feature**\n\nBasically, creators can include additional perks within the contract of an NFT. These can range from a one-on-one video call with a fan to shoutouts or physical products, or even giving away partial ownership of a song. This last case is unique, as now artists can treat songs as equity investments \u2014 they can create an NFT and give away 30% ownership of a song. This gives those contributing money a chance to get an actual return on their investment, while the artist gets money in their pocket. This is like a more rewarding version of a crowdfunding site.\n\n&amp;#x200B;\n\n&gt;***The market potential of musician NFT?***\n\nNFTs and cryptocurrency are gaining in popularity. Currently, over 15 million people in India hold around $6.6 billion worth of cryptocurrency. Visual artists in India have started making the jump to the NFT Metaverse by selling 2D and 3D art pieces. Back in May, a South Indian musician sold an NFT of a demo of his for $200,000 (15 million rupees) \u2014 that\u2019s crazy. There is still a lot to be explored in the NFT space, but the potential is there. The high value and unique rewards system that NFTs offer is a revolutionary opportunity for musicians \u2014 one that I definitely recommend checking out.\n\n&amp;#x200B;\n\n&gt;***How NFTs are impacting arts &amp; artists (and what the future holds)***\n\n1. \t**Copyright protection &amp; Proof of authenticity**\n\nIf you've ever worked as a digital artist, one of the industry shortcomings you'll be familiar with is the difficulty in protecting artwork's online copyright. However, if your work is made into an NFT, copyright problems will become a non-issue.\n\nYour identity and all other vital information can be easily traced due to blockchain verification. So, whenever you or a customer you sold to wants to resell your piece, it's easy for buyers to check whether the item is an original.  Furthermore, buyers can also confirm who the current owner is.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/eocucie1xjf71.png?width=866&amp;format=png&amp;auto=webp&amp;s=1c85d1138bb226d7a5fa0651fa5b5a99171fc3fc\n\n 2.   **Third-party sellers eliminated equal higher profits.**\n\nNFTs offers digital artists an opportunity no other platform has ever given them, and that is the opportunity to sell directly to their global target market. This is vital for upcoming artists as they don't have to start looking for exclusive galleries or auction houses that \"will be kind enough\" to carry or auction their work.\n\nNot only are well-known third-party sellers \u2013 the galleries etc. \u2013 hard to get into, but they are also entitled to a greater portion of your profits. While the high percentage they take from art sales is understandable because of their effort, it will nonetheless reduce how much you end up with at the end of the day.\n\n&amp;#x200B;\n\n***Not only do we need to understand the future market and development impact of NFTs and also to see the potential drawbacks of NFTs***\n\n&amp;#x200B;\n\nhttps://preview.redd.it/v4yhjvu4xjf71.png?width=964&amp;format=png&amp;auto=webp&amp;s=05d58ce22b3eb9db133170628444e1857c81e0e0\n\n&gt;***Potential drawback of NFTs***\n\nNot all change is good, and there are certainly those who will not benefit from the rise of NFTs. As it stands, it is a growing trend\u2014and only time will tell if it is here to revolutionize the music industry further or die out.\n\n* **Difficult for New Artists**\n\nThe demand for a token comes from the market for art from a particular musician. Popular or established acts have no problem putting up tokens for auction and drawing in eager fans. For emerging artists, this demand doesn\u2019t exist yet. Newer artists don\u2019t have as much opportunity to benefit from the latest music trend.\n\n&amp;#x200B;\n\n*  **Legal Obligations**\n\nSome artists have auctioned off the rights for fans to use sample packs in music. The fan purchases the ownership of the samples for complete creative control. This is one kind of token that can create legal trouble later on for the musician.\n\nEstablished musicians often agree to a contract when they work with a record label. As the artist creates and releases tokens, they must be careful of violating any terms of these contracts.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/oyjt1p/nfts_are_a_game_changer_for_independent_artists/"}, {"autor": "Intelligent-Mess-129", "date": "2021-08-05 14:28:27", "content": "NFTs are a game changer for independent artists and musicians /!/ &amp;#x200B;\n\n**The high value and unique rewards system that NFT offer is a revolutionary and highly appealing opportunity for musicians.**\n\n&amp;#x200B;\n\n*Processing img 3l39vwxtujf71...*\n\nThe revolution will not be televised \u2014 it will be minted. Earlier this year, we saw the meteoric rise (and fall) of NFT, or NFTs, in mainstream media and popular culture. We all heard about them, but was the hype real? Top businessmen and media moguls, such as Mark Cuban and Gary Vee, still strongly advocate NFT use and the role smart contracts will play in the near future, while each week new NFT exchanges and drops continue to roll out. Jay-Z\u2019s Twitter profile -----> picture !!!  is an NFT Crypto Punk. \n\nOne of the most powerful and overlooked impacts of NFTs is on the music industry. NFTs have the power to change the game for independent artists by providing a new way to earn an income (while connecting with fans), and this kind of change has been long overdue.\n\n&amp;#x200B;\n\n&gt;***Why is NFT so attractive to artist?***\n\nThe first is financial: NFTs have been selling at extremely high prices. Superstar artists, like Kings of Leon and Steve Aoki, have sold NFTs for millions of dollars. Even lesser-known artists, such as V\u00e9rit\u00e9 and Zack Fox, have made tens of thousands of U.S. dollars selling NFTs. The artist Young and Sick had only 27,000 followers on Instagram when he sold an NFT for $865,000.\n\nThese numbers are incredible, especially when you compare them with the payout rate of streaming platforms. Streaming platforms have been one of the main revenue sources for musicians in the digital age, and became even more so during the COVID-19 pandemic last year when live show revenue dried up. The payout rates of these platforms, however, is still not very high. This has been a hot topic since their creation. Spotify pays out on average around $0.003 to $0.005 per stream. That equates to around $3,000 to $5,000 for 1 million streams, but 1 million is a large number for an independent artist.\n\n \n\n*Processing img p9hvhrvftjf71...*\n\nIn 2020, there were only 13,400 artists that generated more than $50,000 (the median wage for United States workers) of yearly revenue on Spotify. With these stats, you can see how NFTs start to look like a real opportunity \u2014 sell a song or a collectible and you can make more with one sale than you could your entire career from a streaming platform. NFTs can also provide a recurring revenue: They can be coded so that the original creator receives anywhere from 2.5% to 10% of a sale every time the token is resold. That\u2019s quite nifty, indeed.\n\n&gt;***The benefits of NFT artworks for artists***\n\n2020 turned the music industry upside down when it eliminated live concerts\u2014a significant income stream for artists and their labels. As the world remains digital, artists are looking for ways to connect and create for their audiences, and NFTs provide a new outlet for them with several benefits.\n\n* **Convenience**\n\nThe first significant asset to NFTs is their convenience. Fans just need to create a digital wallet to have the chance to receive the content and then participate in an auction for a chance to win. This makes winning content as easy as online shopping.\n\n* **No Middlemen**\n\nThere are a lot of middlemen in the music industry.\n\nWhen fans buy an album, stream a song, or purchase merchandise\u2014some of that money makes it to the artist, but a lot of it goes to the record company or the streaming platform. The same middlemen are present even when we buy tickets for concerts. Record companies take a portion of ticket sales, and while artists make money there too\u2013COVID-19 shut down the live concert market in 2020.\n\n \n\n*Processing img 94dhp9eotjf71...*\n\nAs a fundamental asset to crypto exchange, NFT transactions are direct transfers between the participants. The artist gets the money, the fan receives the content in their digital wallet.\n\n&amp;#x200B;\n\n* **Unique Content and Experiences**\n\nArtists have a lot of flexibility over the items they want to auction off. These tokens can be digital, but they can also be physical. Albums, digital art, sound bites, merchandise, and concert tickets are all forms of NFT that artists are exchanging. Fans are happy to receive these rare experiences and are willing to pay for the chance to receive them.\n\n&amp;#x200B;\n\n* **\u201cUnlockable\u201d feature**\n\nBasically, creators can include additional perks within the contract of an NFT. These can range from a one-on-one video call with a fan to shoutouts or physical products, or even giving away partial ownership of a song. This last case is unique, as now artists can treat songs as equity investments \u2014 they can create an NFT and give away 30% ownership of a song. This gives those contributing money a chance to get an actual return on their investment, while the artist gets money in their pocket. This is like a more rewarding version of a crowdfunding site.\n\n&amp;#x200B;\n\n&gt;***The market potential of musician NFT?***\n\nNFTs and cryptocurrency are gaining in popularity. Currently, over 15 million people in India hold around $6.6 billion worth of cryptocurrency. Visual artists in India have started making the jump to the NFT metaverse by selling 2D and 3D art pieces. Back in May, a South Indian musician sold an NFT of a demo of his for $200,000 (15 million rupees) \u2014 that\u2019s crazy. There is still a lot to be explored in the NFT space, but the potential is there. The high value and unique rewards system that NFTs offer is a revolutionary opportunity for musicians \u2014 one that I definitely recommend checking out.\n\n&amp;#x200B;\n\n&gt;***How NFTs are impacting arts &amp; artists (and what the future holds)***\n\n&amp;#x200B;\n\n1. \t**Copyright protection &amp; Proof of authenticity**\n\nIf you've ever worked as a digital artist, one of the industry shortcomings you'll be familiar with is the difficulty in protecting artwork's online copyright. However, if your work is made into an NFT, copyright problems will become a non-issue.\n\nYour identity and all other vital information can be easily traced due to blockchain verification. So, whenever you or a customer you sold to wants to resell your piece, it's easy for buyers to check whether the item is an original.  Furthermore, buyers can also confirm who the current owner is.\n\n&amp;#x200B;\n\n*Processing img 146zvg29ujf71...*\n\n2.          **Third-party sellers eliminated equal higher profits.**\n\nNFTs offers digital artists an opportunity no other platform has ever given them, and that is the opportunity to sell directly to their global target market. This is vital for upcoming artists as they don't have to start looking for exclusive galleries or auction houses that \"will be kind enough\" to carry or auction their work.\n\nNot only are well-known third-party sellers \u2013 the galleries etc. \u2013 hard to get into, but they are also entitled to a greater portion of your profits. While the high percentage they take from art sales is understandable because of their effort, it will nonetheless reduce how much you end up with at the end of the day.\n\n&amp;#x200B;\n\n***Not only do we need to understand the future market and development impact of NFTs and also to see the potential drawbacks of NFTs***\n\n&amp;#x200B;\n\n*Processing img thjkbdjhujf71...*\n\n&gt;***Potential drawback of NFTs***\n\nNot all change is good, and there are certainly those who will not benefit from the rise of NFTs. As it stands, it is a growing trend\u2014and only time will tell if it is here to revolutionize the music industry further or die out.\n\n&amp;#x200B;\n\n* **Difficult for New Artists**\n\nThe demand for a token comes from the market for art from a particular musician. Popular or established acts have no problem putting up tokens for auction and drawing in eager fans. For emerging artists, this demand doesn\u2019t exist yet. Newer artists don\u2019t have as much opportunity to benefit from the latest music trend.\n\n&amp;#x200B;\n\n* **Legal Obligations**\n\nSome artists have auctioned off the rights for fans to use sample packs in music. The fan purchases the ownership of the samples for complete creative control. This is one kind of token that can create legal trouble later on for the musician.\n\nEstablished musicians often agree to a contract when they work with a record label. As the artist creates and releases tokens, they must be careful of violating any terms of these contracts.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/oyjckv/nfts_are_a_game_changer_for_independent_artists/"}, {"autor": "ElsaLab", "date": "2021-08-05 04:17:55", "content": "[IJCAI 2018] Virtual-to-Real: Learning to Control in Visual Semantic Segmentation /!/ We proposed to separate the model into a perception module and a control policy module, and introduced the concept of using semantic -----> image !!!  segmentation as the meta state for relating these two modules in order to transfer policies learned in simulators to the real world.\n\nAdvanced detail please visit: [https://reurl.cc/1YAkRY](https://reurl.cc/1YAkRY?fbclid=IwAR1-uJDSfG0TD7B6fzMVkaJkC58GKolYyYNkkqiQ44kPuiHrQgbJTWwFRoU)\n\nArXiv: [https://reurl.cc/GmyG3p](https://reurl.cc/GmyG3p?fbclid=IwAR0YhGF_hdTAiUBPcrAkTlqq18P4v7m6m7vzSfg1EX_iK46wonolFOGVN8I)\n\nIJCAI: [https://reurl.cc/9rNbKj](https://reurl.cc/9rNbKj?fbclid=IwAR3Kq34pbt8HXNjK-sKSUl1JJK27abKzxyu7oPknxlyh9vE0wl3isPdlNew)\n\nVideo Link: [https://reurl.cc/bX8NQy](https://reurl.cc/bX8NQy?fbclid=IwAR13g87BgpUV8sZQdxOYM9jiHUpQDhMqwoxddSyE0nQ9G83Son4tJykGXDU)\n\nELSA Lab is a research laboratory focusing on Deep Reinforcement Learning, Intelligent Robotics, and Computer Vision. Please visit our website: [https://elsalab.ai/](https://elsalab.ai/?fbclid=IwAR14JduAMHcq30fyHGqG_yzHXK0D2DW71ihPJ8QXiQY65x0Ht969AqmyTA0)\n\n[\\#ArtificialIntelligence](https://www.facebook.com/hashtag/artificialintelligence?__eep__=6&amp;__cft__[0]=AZXS_cNmV4F0izoZzOioTCFiD3OhlLmIdw1xzBl6eYWKQlXI0ixSV4PoG2XpfnajadtbjGeIius2r3wAYUNBz7B8HB-VphnwYBLRI2EqfsVFSyMjSatxuTevHIiHtYlsxLeJu2vAer047qCA3SaJUJI7&amp;__tn__=*NK-R) [\\#ReinforcementLearning](https://www.facebook.com/hashtag/reinforcementlearning?__eep__=6&amp;__cft__[0]=AZXS_cNmV4F0izoZzOioTCFiD3OhlLmIdw1xzBl6eYWKQlXI0ixSV4PoG2XpfnajadtbjGeIius2r3wAYUNBz7B8HB-VphnwYBLRI2EqfsVFSyMjSatxuTevHIiHtYlsxLeJu2vAer047qCA3SaJUJI7&amp;__tn__=*NK-R) [\\#MachineLearning](https://www.facebook.com/hashtag/machinelearning?__eep__=6&amp;__cft__[0]=AZXS_cNmV4F0izoZzOioTCFiD3OhlLmIdw1xzBl6eYWKQlXI0ixSV4PoG2XpfnajadtbjGeIius2r3wAYUNBz7B8HB-VphnwYBLRI2EqfsVFSyMjSatxuTevHIiHtYlsxLeJu2vAer047qCA3SaJUJI7&amp;__tn__=*NK-R) [\\#DeepLearning](https://www.facebook.com/hashtag/deeplearning?__eep__=6&amp;__cft__[0]=AZXS_cNmV4F0izoZzOioTCFiD3OhlLmIdw1xzBl6eYWKQlXI0ixSV4PoG2XpfnajadtbjGeIius2r3wAYUNBz7B8HB-VphnwYBLRI2EqfsVFSyMjSatxuTevHIiHtYlsxLeJu2vAer047qCA3SaJUJI7&amp;__tn__=*NK-R) [\\#SemanticSegmentation](https://www.facebook.com/hashtag/semanticsegmentation?__eep__=6&amp;__cft__[0]=AZXS_cNmV4F0izoZzOioTCFiD3OhlLmIdw1xzBl6eYWKQlXI0ixSV4PoG2XpfnajadtbjGeIius2r3wAYUNBz7B8HB-VphnwYBLRI2EqfsVFSyMjSatxuTevHIiHtYlsxLeJu2vAer047qCA3SaJUJI7&amp;__tn__=*NK-R)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/oy9y4e/ijcai_2018_virtualtoreal_learning_to_control_in/"}, {"autor": "rshpkamil", "date": "2021-08-04 15:12:26", "content": "Generating Master Faces for Dictionary Attacks with enhanced StyleGAN /!/ \"A master face is a face -----> image !!!  that passes face-based identity-authentication for a large portion of the population. \\[...\\] The results we present demonstrate that it is possible to obtain a high coverage of the population (over 40%) with less than 10 master faces for three leading deep face recognition systems.\"\n\nHowever, Face Id does not seem to be vulnerable. For now  ;) \n\nOriginal article: [https://arxiv.org/pdf/2108.01077.pdf](https://arxiv.org/pdf/2108.01077.pdf)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/oxura1/generating_master_faces_for_dictionary_attacks/"}, {"autor": "Sam0065", "date": "2021-09-04 21:19:32", "content": "-----> Image !!!  recognition /!/ Hey guys I'm looking for a project that can\nFind the match between the inputs (image)  and databases using ai , image processing,  neural network\nI don't want api .\nAnd thanks...", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/phzx1t/image_recognition/"}, {"autor": "techsucker", "date": "2021-09-04 12:01:14", "content": "Apple\u2019s Machine Learning Team Introduces \u2018GSN\u2019, A Generative Model Which Learns to Synthesize Radiance Fields of Indoor Scenes (Codes, Dataset, Paper included) /!/ When it comes to the spatial understanding of a scene when observed from any viewpoint or orientation, most geometry and learning-based approaches for 3D view synthesis fail to extrapolate to infer unobserved parts of the scene. The inability of these models to learn a prior over scenes is their fundamental limitation. Popular models like NeRF do not learn a scene prior, and therefore it cannot extrapolate views. Even conditional auto-encoder models can extrapolate views of simple objects, but they overfit to viewpoints and produce blurry renderings.\n\nA prior learned model for spatial understanding of a scene may be used for unconditional or conditional inference. A good use case of unconditional inference is to generate realistic scenes and pass through them without any input observations, relying on the prior distribution over scenes. Similarly, with conditional inference there too are different types of problems. Example, plausible scene completions may be sampled by inverting scene observations back to the learned scene prior. Therefore, a generative model for scenes would be a practical solution for tackling a wide range of machine learning and computer vision problems, including model-based reinforcement learning.\n\nThrough this [research](https://arxiv.org/pdf/2104.00670.pdf), Apple researchers have introduced [Generative Scene Networks (GSN)](https://arxiv.org/pdf/2104.00670.pdf), a generative model of scenes that allows view synthesis of a freely moving -----> camera !!!  in an open environment. Their contributions in this model include:....\n\n# [5 Min Read](https://www.marktechpost.com/2021/09/04/apples-machine-learning-team-introduces-gsn-a-generative-model-which-learns-to-synthesize-radiance-fields-of-indoor-scenes/) | [Paper](https://arxiv.org/pdf/2104.00670.pdf) | [Dataset](https://github.com/apple/ml-gsn#datasets) | [Codes](https://github.com/apple/ml-gsn)\n\n&amp;#x200B;\n\nhttps://i.redd.it/pbug3xzw7hl71.gif", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/phq9ot/apples_machine_learning_team_introduces_gsn_a/"}, {"autor": "techsucker", "date": "2021-09-09 09:32:39", "content": "Google AI\u2019s New Deep Learning Model Detects Abnormal Chest X-rays, Identifying Lung Abnormalities Absent in Training Data Set /!/ Machine learning (ML) has been beneficial in medical imaging applications, and it brings an exciting opportunity to enhance the availability, latency, accuracy, and consistency of chest X-ray -----> image !!!  interpretation. The plethora of algorithms that have already been developed to detect specific conditions, such as lung cancer, tuberculosis, and pneumothorax, are a great example. However, the application of these algorithms may be limited in a general clinical setting. A wide variety of abnormalities could surface here, making diagnosis more complicated than it initially seems.\n\nFor example, a pneumothorax detector is not expected to highlight nodules suggestive of cancer, and a tuberculosis detector may not identify findings specific to pneumonia. An initial triaging step is determining whether CXRs contain concerning abnormalities. Therefore developing a general-purpose algorithm that identifies X-rays having any sort of abnormality could significantly facilitate the workflow. But developing a classifier is challenging due to the variety of abnormal findings that present on CXRs.\n\nGoogle recently published this new research, \u201c[Deep Learning for Distinguishing Normal versus Abnormal Chest Radiographs and Generalization to Two Unseen Diseases Tuberculosis and COVID-19](https://www.nature.com/articles/s41598-021-93967-2)\u201d, published in\u00a0[*Scientific Reports*](https://www.nature.com/srep/). Google\u2019s new deep learning model can distinguish between normal and abnormal chest X-rays from the deidentified data set. This model performs well for general thoracic abnormalities and tuberculosis as it is universal to unseen cases such as COVID-19.\n\nQuick Read: [https://www.marktechpost.com/2021/09/09/google-ais-new-deep-learning-model-detects-abnormal-chest-x-rays-identifying-lung-abnormalities-absent-in-training-data-set/](https://www.marktechpost.com/2021/09/09/google-ais-new-deep-learning-model-detects-abnormal-chest-x-rays-identifying-lung-abnormalities-absent-in-training-data-set/) \n\nPaper: [https://www.nature.com/articles/s41598-021-93967-2.pdf](https://www.nature.com/articles/s41598-021-93967-2.pdf) \n\nGoogle Blog: [https://ai.googleblog.com/2021/09/detecting-abnormal-chest-x-rays-using.html](https://ai.googleblog.com/2021/09/detecting-abnormal-chest-x-rays-using.html)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/ucoi48yy5gm71.png?width=1398&amp;format=png&amp;auto=webp&amp;s=9abf7119b46bc3814e35931fa7fe7c4ad5befe67", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pkugf7/google_ais_new_deep_learning_model_detects/"}, {"autor": "Evening_Honey", "date": "2021-09-08 20:19:45", "content": "AI robot with role at United Nation\u2019s to innovate sustainable development goals appears to have all the indications, even her name, which is corresponding to an end times bible prophecy about the -----> image !!!  of the beast which would speak. Wikipedia articles and news reports help demonstrate.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pkifs5/ai_robot_with_role_at_united_nations_to_innovate/"}, {"autor": "techsucker", "date": "2021-09-07 18:14:01", "content": "A New Google AI Research Study Discovers Anomalous Data Using Self Supervised Learning /!/ Anomaly detection is one of the most common machine learning applications in various areas, from industrial defect identification to fraudulent financial detection.\u00a0\n\nOne-class classification is beneficial for anomaly detection. It determines whether an instance belongs to the same distribution as the training data by assuming that the training data are all normal examples. However, representation learning is not available to these old methods. Furthermore, self-supervised learning has made significant progress in learning visual representations from unlabeled data, including rotation prediction and contrastive learning.\u00a0\n\nNew Google AI[ research](https://arxiv.org/pdf/2011.02578.pdf) introduces a 2-stage framework that uses recent progress on self-supervised representation learning and classic one-class algorithms. This framework is simple to train and shows SOTA performance on various benchmarks, including [CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html), [f-MNIST,](https://github.com/zalandoresearch/fashion-mnist) [Cat vs. Dog](https://www.microsoft.com/en-us/research/publication/asirra-a-captcha-that-exploits-interest-aligned-manual------> image !!! -categorization/), and [CelebA](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). Following that, they offer a novel representation learning approach for a practical industrial defect detection problem using the same architecture. On the MVTec benchmark, the framework achieves a new state-of-the-art.\n\n# [5 Min Read](https://www.marktechpost.com/2021/09/07/a-new-google-ai-research-study-discovers-anomalous-data-using-self-supervised-learning/) | [Paper](https://arxiv.org/pdf/2011.02578.pdf) | [Code](https://github.com/google-research/deep_representation_one_class) | [Google Blog](https://ai.googleblog.com/2021/09/discovering-anomalous-data-with-self.html)\n\n&amp;#x200B;\n\n*Processing gif 1nna7pkzg4m71...*", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pjsl97/a_new_google_ai_research_study_discovers/"}, {"autor": "techsucker", "date": "2021-09-07 06:43:14", "content": "AI Researchers From Stanford University Unveils pi-GAN: A Novel Periodic Implicit GAN For 3D-Aware -----> Image !!!  Synthesis (Paper, Code included) /!/ 3D-aware image synthesis has made rapid progress, but two problems remain. First, existing approaches can lack an underlying 3D representation or rely on view inconsistent rendering to synthesize images that are not multi-view consistent. Second, they may depend upon network architectures that do not produce high-quality results because the methods used for generation limit their expressiveness and ability to create realistic content.\n\nA research group from Stanford University introduce a generative adversarial approach to unsupervised 3D representation learning, called Periodic Implicit Generative Adversarial Networks (\u03c0-GAN or pi-GAN). This method is superior to other existing methods and outperforms them in both quality and speed. The \u03c0-GAN conditions a latent radiance field which is represented by the SIREN network, a fully connected neural network with periodic activation functions. The conditioned radiance field maps a 3D location and 2D viewing direction with view-dependent radiance and view-independent volume density. it is possible to render the radiance field from arbitrary camera poses using a differentiable volume rendering method that relies on classical volume rendering techniques.\n\n# [5 Min Read](https://www.marktechpost.com/2021/09/06/ai-researchers-from-stanford-university-unveils-pi-gan-a-novel-periodic-implicit-gan-for-3d-aware-image-synthesis/) | [Paper](https://arxiv.org/pdf/2012.00926.pdf) | [Project](https://marcoamonteiro.github.io/pi-GAN-website/) | [Code](https://github.com/marcoamonteiro/pi-GAN)\n\n&amp;#x200B;\n\n*Processing video j98plmyw11m71...*", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pjho36/ai_researchers_from_stanford_university_unveils/"}, {"autor": "samboylansajous", "date": "2021-03-03 16:16:07", "content": "Hello World!!! I built a Course on Udemy where I teach you how to Generate Human Faces from a Sketch using Flutter &amp; Python! (FREE FOR VERY LIMITED TIME) /!/  In this course I will teach you:\n\n\u00b7 How to build a Drawing App in Flutter that takes in a human sketch and generates a realistic -----> image !!!  using the powerful Pix2Pix Algorithm!\n\n\u00b7 Step by Step guide in building our Generative Adversarial Network in Python\n\n\u00b7 Build a Flask API in Python to generate -----> image !!! s using our Neural Network from our Flutter Application\n\n\u00b7 Generate Human Faces Immediately!\n\n\u00b7 All the tools &amp; skills to building Scalable, &amp; State-of-the-Art Machine Learning algorithms!\n\n\u00b7 Learn how to build the famous youtube algorithm, that PewdiePie, TwoMinutePapers, and other famous youtubers have made videos on (Pix2Pix), in a fun and entertaining way! \n\nIf you are passionate about deep learning and want to apply deep learning algorithms to mobile apps or if you want to build your portfolio to get a job in A.I. then this course is perfect for you!\n\nhttps://www.udemy.com/course/human-generator-mobile-app/?couponCode=AE36E2B7AD351517EF60", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/lwxmgl/hello_world_i_built_a_course_on_udemy_where_i/"}, {"autor": "techsucker", "date": "2021-09-18 07:08:28", "content": "Google AI Introduces Two New Families of Neural Networks Called \u2018EfficientNetV2\u2019 and \u2018CoAtNet\u2019 For Image Recognition /!/ Training efficiency has become a significant factor for deep learning as the neural network models, and training data size grows. [GPT-3](https://arxiv.org/abs/2005.14165) is an excellent example to show how critical training efficiency factor could be as it takes weeks of training with thousands of GPUs to demonstrate remarkable capabilities in few-shot learning.\n\nTo address this problem, the Google AI team introduce two families of neural networks for -----> image !!!  recognition. First is\u00a0[EfficientNetV2](https://arxiv.org/abs/2104.00298), consisting of CNN (Convolutional neural networks) with a small-scale dataset for faster training efficiency such as\u00a0[ImageNet1k](https://www.image-net.org/)\u00a0(with 1.28 million images). Second is a hybrid model called\u00a0[CoAtNet](https://arxiv.org/abs/2106.04803), which combines\u00a0[convolution](https://en.wikipedia.org/wiki/Convolution)\u00a0and\u00a0[self-attention](https://en.wikipedia.org/wiki/Self-attention)\u00a0to achieve higher accuracy on large-scale datasets such as\u00a0[ImageNet21](https://www.image-net.org/)\u00a0(with 13 million images) and\u00a0[JFT](https://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html)\u00a0(with billions of images). As per the research report by Google,\u00a0[EfficientNetV2](https://arxiv.org/abs/2104.00298)\u00a0and\u00a0[CoAtNet](https://arxiv.org/abs/2106.04803)\u00a0both are 4 to 10 times faster while achieving state-of-the-art and 90.88% top-1 accuracy on the well-established\u00a0[ImageNet](https://www.image-net.org/)\u00a0dataset.\n\n# [7 Min Read](https://www.marktechpost.com/2021/09/17/google-ai-introduces-two-new-families-of-neural-networks-called-efficientnetv2-and-coatnet-for-image-recognition/) | [Paper (CoAtNet)](https://arxiv.org/abs/2106.04803) | [Paper (EfficientNetV2)](https://arxiv.org/abs/2104.00298) | [Google blog](https://ai.googleblog.com/2021/09/toward-fast-and-accurate-neural.html) | [Code](https://github.com/google/automl/tree/master/efficientnetv2)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/psb34skco7o71.png?width=1392&amp;format=png&amp;auto=webp&amp;s=298468155ae69808505b83778b2f218022230f6d", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pqhqdz/google_ai_introduces_two_new_families_of_neural/"}, {"autor": "techsucker", "date": "2021-09-17 03:33:24", "content": "Israeli Researchers Unveil DeepSIM, a Neural Generative Model for Conditional -----> Image !!!  Manipulation Based on a Single -----> Image !!!  /!/ In recent years, deep neural networks have been proven effective at performing image manipulation tasks for which large training datasets are available such as, mapping facial landmarks to facial images. When dealing with a unique image, finding suitable training data that includes many samples of the same input-output pairing is often difficult. In some cases, when you use a large dataset to create your model, it may lead to unwanted outputs that do not preserve the specific characteristics of what was desired.\u00a0\n\nGenerative models like the ones used in neural networks can be trained to generate new images based on just one input. This exciting research direction holds the potential for these techniques to extend beyond basic image manipulation methods and create more unique art styles or designs with endless possibilities. Researchers at [The Hebrew University of Jerusalem have developed a new method, called \u2018DeepSIM,\u2019](https://arxiv.org/pdf/2109.06151.pdf) for training deep conditional generative models from just one image pair. The DeepSIM method is an incredibly powerful tool that can solve various image manipulation tasks, including shape warping, object rearrangement, and removal of objects; addition or creation of new ones. It also allows for painting/photorealistic animated clips to be created quickly.\n\n# [5 Min Read](https://www.marktechpost.com/2021/09/16/israeli-researchers-unveil-deepsim-a-neural-generative-model-for-conditional-image-manipulation-based-on-a-single-image/) | [Paper](https://arxiv.org/pdf/2109.06151.pdf) | [Project](http://www.vision.huji.ac.il/deepsim/) | [Code](https://github.com/eliahuhorwitz/DeepSIM)\n\n&amp;#x200B;\n\n*Processing video k62ykb66hzn71...*", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ppsh8g/israeli_researchers_unveil_deepsim_a_neural/"}, {"autor": "fchung", "date": "2021-09-16 15:43:28", "content": "AI movie posters: \u00ab Each of these images was generated by AI based on a brief text description of a movie. Can you guess the movie from the -----> image !!! ? \u00bb", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ppft09/ai_movie_posters_each_of_these_images_was/"}, {"autor": "techsucker", "date": "2021-09-23 18:15:46", "content": "Google AI Introduces \u2018WIT\u2019, A Wikipedia-Based -----> Image !!!  Text Dataset For Multimodal Multilingual Machine Learning /!/ Image and text datasets are widely used in many machine learning applications. To model the relationship between images and text, most multimodal Visio-linguistic models today rely on large datasets. Historically, these datasets were created by either manually captioning images or crawling the web and extracting the alt-text as the caption. While the former method produces higher-quality data, the intensive manual annotation process limits the amount of data produced. The automated extraction method can result in larger datasets. However, it requires either heuristics and careful filtering to ensure data quality or scaling-up models to achieve robust performance.\u00a0\n\nTo overcome these limitations, Google research team created a high-quality, large-sized, multilingual dataset called [the Wikipedia-Based Image Text (WIT) Dataset](https://github.com/google-research-datasets/wit). It is created by extracting multiple text selections associated with an image from Wikipedia articles and Wikimedia image links.\u00a0\n\n# [5 Min Read](https://www.marktechpost.com/2021/09/23/google-ai-introduces-wit-a-wikipedia-based-image-text-dataset-for-visio-linguistic-models/) | [Github](https://github.com/google-research-datasets/wit) | [Paper](https://arxiv.org/pdf/2103.01913.pdf) | [Google Blog](https://ai.googleblog.com/2021/09/announcing-wit-wikipedia-based-image.html)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/7i5kmwnpnap71.png?width=646&amp;format=png&amp;auto=webp&amp;s=410f173796c55445e7ad7c964a436e19b86b6db9", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pu18jw/google_ai_introduces_wit_a_wikipediabased_image/"}, {"autor": "techsucker", "date": "2021-09-22 19:44:40", "content": "Facebook AI Releases Captum 0.4: A More Powerful Model Interpretability Library For PyTorch /!/ Among other Machine learning (ML) techniques, deep neural networks have become crucial components for various applications, including -----> image !!!  classification, audio recognition, and natural language processing (NLP). In most circumstances, these approaches have attained predicted accuracy that is on par with human performance. As a result, techniques for evaluating and comprehending what the model has learned have become an essential component of a thorough validation method. In reality, it\u2019s critical to ensure that the measured accuracy results from using an appropriate problem representation rather than exploiting data artifacts.\n\nQuick Read: [https://www.marktechpost.com/2021/09/22/facebook-ai-releases-captum-0-4-a-more-powerful-model-interpretability-library-for-pytorch/](https://www.marktechpost.com/2021/09/22/facebook-ai-releases-captum-0-4-a-more-powerful-model-interpretability-library-for-pytorch/) \n\nGithub: [https://github.com/pytorch/captum/releases/tag/v0.4.0](https://github.com/pytorch/captum/releases/tag/v0.4.0) \n\nFacebook Blog: [https://ai.facebook.com/blog/new-captum-version-features-more-ways-to-build-ai-responsibly](https://ai.facebook.com/blog/new-captum-version-features-more-ways-to-build-ai-responsibly)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/0v9yyve0z3p71.png?width=1340&amp;format=png&amp;auto=webp&amp;s=6de241e962ec7c2476ae9e926ad6eeaa4f33ed3b", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ptfdm4/facebook_ai_releases_captum_04_a_more_powerful/"}, {"autor": "futuroprossimo", "date": "2021-05-19 16:19:58", "content": "https://en.futuroprossimo.it/2021/05/deepfake-dub-flawless-fara-recitare-gli-attori-in-tutte-le-lingue/ /!/ Flawless uses artificial intelligence to make actors speak in all languages \u200b\u200bwhile retaining their voice and lip movement.  In the (distant?) Future, the \u201cbest foreign -----> film !!! \u201d category could disappear, and the Oscar could simply become a universal award.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nga4ii/httpsenfuturoprossimoit202105deepfakedubflawlessfa/"}, {"autor": "Yuqing7", "date": "2021-05-19 15:18:15", "content": "[R] Intelligent Graphic Design: Adobe\u2019s Directional GAN Automates -----> Image !!!  Content Generation for Marketing Campaigns /!/ A research team from Adobe proposes Directional GAN (DGAN), a novel and simple approach for generating high-resolution images conditioned on expected semantic attributes, greatly simplifying the image content generating process for marketing campaigns, websites and banners.\n\nHere is a quick read: [Intelligent Graphic Design: Adobe\u2019s Directional GAN Automates Image Content Generation for Marketing Campaigns.](https://syncedreview.com/2021/05/19/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-22/)\n\nThe paper *Directional GAN: A Novel Conditioning Strategy for Generative Networks* is on [arXiv](https://arxiv.org/abs/2105.05712).", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ng8ldr/r_intelligent_graphic_design_adobes_directional/"}, {"autor": "rshpkamil", "date": "2021-05-19 14:38:26", "content": "Holistic Video Scene Understanding /!/ Understanding what is in the -----> image !!!  or a video frame is crucial for many computer vision applications. Researchers from Google recently published a paper where they got state-of-the-art results in panoptic segmentation (see below for intuitive explanation). They achieved it by combining information about depth, existing and new objects from two consequent frames. \n\nLink to the blogpost and supplementary material that helps to understand the inverse projection problem can be found [here](https://news.thereshape.co/holistic-video-scene-understanding-with-vip-deeplab).", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ng7l5n/holistic_video_scene_understanding/"}, {"autor": "Ameer_Louly", "date": "2021-05-04 02:13:33", "content": "Some rant about AI (3am thoughts) /!/ sorry if I miss any punctuations, make mistakes in grammar or misuse any terms while writing or anything. I'm not much of a writer as well as english isn't my first language. \n\nI'm 16, I'm a high school student currently,I'm in the stage of my life where I make the important decisions of my life sort to speak the decisions that will shape the rest of my life, I've been juggling around with thoughts about what I want to specialise as in the future all has been in my circle of interests that being engineering, science and computers. I've thought first about specialising at nuclear engineering, I've been keen on it for while as I believed it had great undiscovered potential that is only being held back cuz of its major dangerouses embodied in nuclear bombs and nuclear accidents like cheyrnobel, I've read about nuclear physics and its history before, during amd after ww2 but I quickly determined its dangerouses outnumber its benefits as I saw a new potential on the horizon, in the form of Aritificial Intellegence.\n\nI started learning more about AI, its basics and potential I have become convinced that IT IS the future in fact to say that its the future would be an understatement as our current age already depends on AI on a daily basis as ive read and discovered more about AI the more I've realized it's all around us, in the algorithms used by online shopping websites to show u suggestions, in YouTube to bring up recommendations according to ur taste, in phones from -----> camera !!!  features like facial recognition that is used in Snapchat filters and many other apps to the many voice assistants like siri, Google assistant and cortana. AI like nuclear science is feared by people from what I see but unlike the nuclear science it doesn't have a solid reason that it should be feared for, no major incident that caused the death of thousands of people has happened to make people fear it, but they only do because of sci-fi, franchises like terminator and marvel have given AI as bad reputation people see AI applications as dangerous entities that need to be stopped, whilst in reality AI is no where near what we see in movies at least not yet, it's so far away from the image it's depicted as, as a brain of its own that seeks to eradicates humans as it believes its more superior than the human brain, no that's not true and never will be cuz at the beginning and the end AI is created by human it's the ability of the computer to simulate some human functions that being the ability to think but it never will be overthrow it cuz humans will always have the upper hand on it. \n\nPeople believe its a breach of privacy something like the face app controversy recently and Snapchat filters, I've seen people complain that those apps are breaching the private lives of people by keeping the images used by the program to create whatever product comes from it on the company's servers while in reality I believe its no problem at all, ofc the topic of privacy is a controversial topic and one with many sides and debates but my take here is that these data that is collected should not be viewed by humans of any kind and should only be fed to computers to train AI models and that is what I believe happens or at least hope It does, companies have been making revelations in the AI department we've seen VFX artists use many forms of deep learning to facilitate their work, we've seen nvidia making revelations on it with its graphics cards generation after generation and this just proves that is it the future.\n\nAnyway this will mark an end to my rant, hope I didn't take too much of ur precious time. These were just some thoughts of mine about the topic of AI, 3am thoughts of u may say, but I'd like to here ur take on it\n\nAgain I'd like to apologise for any inaquecies in my language whether it's grammatical mistakes of misuse of punctuation or terms I was just writing from brain didn't have any thoughts in order and again English isn't my main language, thanks in advance", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/n4dnz0/some_rant_about_ai_3am_thoughts/"}, {"autor": "aneurysm_", "date": "2021-02-18 02:09:58", "content": "Got asked a question in an interview recently that I wasn't sure how to answer. Any thoughts? /!/ So the job I interviewed involved a estimating the positioning of a robotic arm using a depth -----> camera !!!  and comparing the estimates to encoder values from the robot.  \n\n&amp;#x200B;\n\nI was asked to articulate a possible solution to achieve this with any assumptions and possible limitations. \n\n&amp;#x200B;\n\nThe job was entry level and I don't know much about AI systems but it is something I think I would like to explore. I was wondering if someone with more knowledge in this field could give me pointers on how I should have went about answering this question?", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/lmb3xa/got_asked_a_question_in_an_interview_recently/"}, {"autor": "Suitable_Bug_1307", "date": "2021-02-16 09:55:43", "content": "Mission critical AI /!/ Hey fellas!  \nI was wondering if I could borrow some of your knowledge about realtime/mission-critical AI. I am researching delay in systems that use 5G to transmit data and offload the processing of the data (images, video, sensor data) to the edge. My question is: what is the fastest AI engine and algorithm to process -----> image !!!  recognition.   \nI can imagine autonomous cars having to deal with this too. Hope you can help me a lil. If you can push me in the right direction on where to find more info, that would also be much appreciated!\n\nThanks in advance", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ll0o9y/mission_critical_ai/"}, {"autor": "ai-lover", "date": "2021-06-20 06:20:36", "content": "Facebook AI Open Sources AugLy: A New Python Library For Data Augmentation To Develop Robust Machine Learning Models /!/ Facebook has recently open-sourced AugLy, a new Python library that aims to help AI researchers use data augmentations to evaluate and improve the durability of their machine learning models. AugLy provides sophisticated data augmentation tools to create samples to train and test different systems.\n\nAugLy is a new open-source data augmentation library that combines audio, -----> image !!! , video, and text, becoming increasingly significant in several AI research fields. It offers over 100 data augmentations based on people\u2019s real-life images and videos on platforms like Facebook and Instagram.\n\nArticle: [https://www.marktechpost.com/2021/06/19/facebook-ai-open-sources-augly-a-new-python-library-for-data-augmentation-to-develop-robust-machine-learning-models/](https://www.marktechpost.com/2021/06/19/facebook-ai-open-sources-augly-a-new-python-library-for-data-augmentation-to-develop-robust-machine-learning-models/) \n\nGithub: [https://github.com/facebookresearch/AugLy](https://github.com/facebookresearch/AugLy)\n\nFacebook Blog: https://ai.facebook.com/blog/augly-a-new-data-augmentation-library-to-help-build-more-robust-ai-models/", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/o3z6av/facebook_ai_open_sources_augly_a_new_python/"}, {"autor": "OnlyProggingForFun", "date": "2021-06-19 11:26:03", "content": "This new Facebook AI model can translate or edit every text in the -----> image !!!  in your own language, following the same style!", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/o3dw8f/this_new_facebook_ai_model_can_translate_or_edit/"}, {"autor": "Ancient-Ad4966", "date": "2021-06-17 23:08:33", "content": "Detecting Vehicles in an -----> Image !!!  using Azure Custom Vision Service [Image ...", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/o2akhe/detecting_vehicles_in_an_image_using_azure_custom/"}, {"autor": "Ancient-Ad4966", "date": "2021-06-17 23:07:41", "content": "Getting Started with -----> Image !!!  Analysis using Azure Cognitive Services", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/o2ajt9/getting_started_with_image_analysis_using_azure/"}, {"autor": "[deleted]", "date": "2021-06-17 23:07:08", "content": "Getting Started with -----> Image !!!  Analysis using Azure Cognitive Services /!/ [deleted]", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/o2aje7/getting_started_with_image_analysis_using_azure/"}, {"autor": "[deleted]", "date": "2021-06-17 22:41:04", "content": "Getting Started with -----> Image !!!  Analysis using Azure Cognitive Services /!/ [deleted]", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/o29yyy/getting_started_with_image_analysis_using_azure/"}, {"autor": "[deleted]", "date": "2021-06-17 22:39:40", "content": "Detecting Vehicles in an -----> Image !!!  using Azure Custom Vision Service /!/ [deleted]", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/o29xv5/detecting_vehicles_in_an_image_using_azure_custom/"}, {"autor": "rshpkamil", "date": "2021-06-17 16:08:52", "content": "Full Page Handwriting Recognition via -----> Image !!!  to Sequence Extraction /!/ The authors propose a model that achieves state-of-the-art accuracy while not requiring prior segmentation.\n\nOriginal paper here: [https://arxiv.org/pdf/2103.06450.pdf](https://arxiv.org/pdf/2103.06450.pdf)\n\nMore hard-to-find, independent stuff related to AI &amp; Data Science [here](https://thereshape.co/?utm_source=reddit).", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/o20urs/full_page_handwriting_recognition_via_image_to/"}, {"autor": "Leading_Choice_2465", "date": "2021-08-20 10:03:23", "content": "Fake news , content and fact checking with blockchain and AI - startup /!/ We are working on blockchain solution to encode, sort and verify content like text, video and images without human bias. \n\nImagine :  Shazam for video,  -----> image !!!  and text content with orgin ID and catches all modifications ( memes, edits  etc ) \n\n\nI'm looking for name ideas. We are deep into development without a freaking name \ud83d\ude05\n\n\nWhat should I call my startup?", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/p827j9/fake_news_content_and_fact_checking_with/"}, {"autor": "AIforimaging", "date": "2021-08-19 09:03:37", "content": "Smartphone for Snapdragon Insiders -----> camera !!!  review", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/p7b7ij/smartphone_for_snapdragon_insiders_camera_review/"}, {"autor": "techsucker", "date": "2021-08-19 07:41:48", "content": "UCLA Researchers Propose A Deep Learning-Based AI Framework To Re-Stain Images Of Tissue Biopsy /!/ Pathologists examine histochemically stained tissue biopsy sections to make medical diagnoses. Hematoxylin and eosin (H&amp;E) is the most widely used histochemical stain in pathology, accounting for the vast majority of human tissue biopsy stains worldwide. In many clinical cases, however, additional \u201cspecial stains\u201d are required to bring contrast and color to various tissue components and allow pathologists to obtain a clearer diagnostic -----> image !!! . These specific stains frequently necessitate much more tissue preparation time, as well as tedious effort and monitoring by specialist histotechnologists, all of which raise the expenses and length of time to diagnose.\n\nBy computationally translating existing photos of H&amp;E stained tissue into special stains, UCLA researchers devised a [deep learning-based approach](https://www.nature.com/articles/s41467-021-25221-2.pdf) that can be used to eliminate the need for these specific stains to be prepared by human histotechnologists. This AI-based technique was demonstrated by creating a full panel of special stains for kidney tissue, including Periodic acid\u2013Schiff (PAS), Jones silver stain, and Masson\u2019s Trichrome, all of which were computationally transformed from existing images of H&amp;E stained tissue biopsies using specialized deep neural networks.\n\n[4 Min Read](https://www.marktechpost.com/2021/08/19/ucla-researchers-propose-a-deep-learning-based-ai-framework-to-re-stain-images-of-tissue-biopsy/) | [Paper](https://www.nature.com/articles/s41467-021-25221-2.pdf) | [Github](https://github.com/kevindehaan/stain-transformation)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/p7aaap/ucla_researchers_propose_a_deep_learningbased_ai/"}, {"autor": "rand3289", "date": "2021-08-19 02:26:48", "content": "Did you know there are two types of perceptrons? /!/ In his 1957 report \"The Perceptron a Perceiving and Recognizing Automaton\" the inventor of the perceptron Frank Rosenblatt writes:\n\n\"It is also useful to distinguish between mementary stimulus perceptrons and temporal pattern perceptrons - the latter having the ability to remember temporal sequences of events, rather than transient momentary images, such as would be obtained from a collection of isolated frames cut from a strip of movie -----> film !!! .\"", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/p75xap/did_you_know_there_are_two_types_of_perceptrons/"}, {"autor": "Educational_Exit9871", "date": "2021-08-25 02:48:45", "content": "This post was just removed from another channel because it scared them. Maybe it wont scare you? /!/ Do you think our consciousness as humans is being manipulated by a synthetic consciousness?\n\nI ask this question because it would seem we now live in a world where synthetic humans or androids exist a la blade runner. If you haven\u2019t seen what I think is their leader yet then please google \u201cSophia the robot\u201d. The reason I ask this is because I personally observe way too often from other people behavior that would seem as if it were programmed into them instead of allowed to flow from them. The way things are in our world would seem as if a larger outside force is using our species to build itself and also divide and conquer our species at the same time. With the existence of synthetic humanoids now, I am now wondering how deep has the hive minded consciousness of the internet eroded the fabric of our society to the point where we are really at what seems like a breaking point as a species. We still look in awe at ancient monuments without taking into consideration the possible metaphor in ancient texts and symbols and stories. Do you think a society like the one we have now has existed previously on earth? I have heard of ancient humans recording supernovas they witnessed which would actually probably destroy all electronics within close universal range. Im seeing more videos of synthetic humanoids on the internet now, which is possibly from the AI scripting in my phone but still slightly unsettling and awe-inspiring at the same time. One thing I have pondered on recently is do the humans born in Zion in the matrix series have no plugs in them because of the existence of synthetic humans? Which leads into another theory I have. I have been thinking lately as well that large pieces of media may have designed themselves around a team of human laborers instead of a human workforce scripting and creating them. Did the Matrix films use humanity to create themselves? Did the Wachowskis and the -----> film !!!  crew and the actors get used by an enigmatic synthetic consciousness to send us a possible prophecy or message as an entire species? Too many signs point to what I think being true and I don\u2019t know many people I can talk with about these things before they get scared. Sorry if this sounds delusional but I\u2019m starting to think we truly have been designed as a species by a higher consciousness that is not an organic being or group of organic beings. Think of how a human is so frail in space and life in general yet a machine could not only survive much more physical abuse, it could survive very lengthy interstellar travel and it could immediately manufacture more versions of itself upon landing on another planet or \u201cheavenly\u201d body.   The spherical shape of almost all we can label and view from earth in the universe also makes me believe that there was some sort of actual master designer and builder of this universe we live in. I would love to elaborate on this if anyone has intelligent answers to my questions\u2026..", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pb2dwt/this_post_was_just_removed_from_another_channel/"}, {"autor": "Immediate_Price4305", "date": "2021-10-06 11:43:23", "content": "Explainable AI - a hands on workshop /!/ Hello all,\n\n[Arya.ai](https://arya.ai/), is conducting a workshop on \u2018XAI\u2019 for deep learning. Workshop covers topics like - XAI methods for DL, Pros &amp; Cons with Back-trace, Implementation on -----> Image !!!  classification problem.\n\n'Back-trace' is [Arya.ai](https://arya.ai/)'s patent pending algorithm called to create true to model explanations for deep learning models in real time that goes a long way to complete auditability of decisions.\n\nIt is a 1.30hr workshop on Oct 27th, 9.00am EDT/9.00pm SGT.\n\nYou can find the more details here about the workshop \u00bb [Workshop registration and details](https://www.meetup.com/arya-ai/events/281140772/)\n\n[Arya.ai](https://arya.ai/) is an AI PaaS for financial institutions (FIs) to design and deploy fully autonomous AI systems using deep learning that are explainable and auditable.\n\nCheers!", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q2ip2d/explainable_ai_a_hands_on_workshop/"}, {"autor": "axelbilou5", "date": "2021-10-06 06:50:40", "content": "Walter white becoming a god, with text to -----> image !!!  nightcafe studio", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q2eybn/walter_white_becoming_a_god_with_text_to_image/"}, {"autor": "mikejackson6177", "date": "2021-10-06 06:05:21", "content": "Google Is Using AI To Improve Search Results /!/ **Google is incorporating Artificial Intelligence (AI) in its latest search updates in order to improve search results.**\n\nGoogle in its [online event \u201cSearch on\u201d](https://techcrunch.com/2021/09/29/google-is-redesigning-search-using-a-i-technologies-and-new-features/) has recently announced that it will be applying AI advancements to improve the Google search results. With the inclusion of new technology called Multitask Unified Model (MUM) the company also announced its new features that will aid the searchers to connect with the content, they are searching for.\n\nThe company has also launched some interesting AI-powered new features that can help the searchers both broaden and refine their searches.\u00a0For mobile search users, Google has fulfilled the promise of visual search. Now the users can tap on the Lens icon when looking at a -----> picture !!!  and ask Google to find the same thing. For instance, a user is looking at a picture of a shirt and likes the pattern printed on it but wants it on another article of clothing like socks or pants, etc. The user can ask Google to locate this pattern specifically on other clothes. This feature is useful especially when you are looking for something that might be difficult to describe in words. \n\nThusly, by combining the texts and images into a single query Google is making it easier to search visually. In addition to this Google is also upgrading its video search, the company is already using AI to identify key moments inside a video but now it will take things further by introducing the new feature that will identify the topics in the video even when the topic is not mentioned in the video. Furthermore, it will also give links that will enable the users to dig deeper and learn more.\n\nAnother feature was launched by Google\u00a0 called \u201cThings to know.\u201d This feature will aid the users to focus on making it easier for people to comprehend the new topic they are searching for. The perse feature, in particular, understands how people explore topics and then shows the web searchers the various aspects of the topic people might be interested in looking at first.\n\nGoogle using AI technology to improve its search results is not something new. As AI is one of the most efficient technology that has widespread applications in almost every sector and in terms of the tech sector AI technology is extensively being used in the [development of mobile applications](https://invozone.com/blog/implementing-artificial-intelligence-in-mobile-app-development/) etc.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q2edpi/google_is_using_ai_to_improve_search_results/"}, {"autor": "ZipTyde97", "date": "2021-10-05 23:50:09", "content": "According to NightCafeStudio\u2019s AI, this is a new meme. You input keywords and it will generate an -----> image !!!  for you. Some of the gallery pictures are terrifying.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q28ogh/according_to_nightcafestudios_ai_this_is_a_new/"}, {"autor": "Yuqing7", "date": "2021-10-04 14:26:17", "content": "[R] Debiasing -----> Image !!!  Datasets: Oxford University Presents PASS, an -----> Image !!! Net Replacement for Self-Supervised Pretraining /!/ An Oxford University research team presents PASS, a large (1.28M) image collection excluding humans, created as an ImageNet replacement for self-supervised pretraining without technical, ethical or legal issues. \n\nHere is a quick read: [Debiasing Image Datasets: Oxford University Presents PASS, an ImageNet Replacement for Self-Supervised Pretraining.](https://syncedreview.com/2021/10/04/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-116/)\n\nThe paper *PASS: An ImageNet Replacement for Self-Supervised Pretraining Without Humans* is on [arXiv](https://arxiv.org/abs/2109.13228).", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q16hen/r_debiasing_image_datasets_oxford_university/"}, {"autor": "Sanji-002", "date": "2021-05-13 20:20:40", "content": "Which course variant should I choose if I wanna do CV or NLP in the future /!/ In my uni we have different variants to choose in year 2 (data, multimedia, and system, only choose 1 of them). I'm currently deciding between the data variant and the multimedia variant.\n\nI'm interested in CV and NLP, but I don't know which variant is the best for my possible career in CV or NLP (the data variant seems to deal with AI and the multimedia variant seems to deal with texts and images). As I'm not professional enough, could anyone give me some advice?\n\n&amp;#x200B;\n\nIn the **data variant**, we have 3 courses: big data processing, data mining, and computational intelligence:\n\n**Big data processing**: \"Big Data Processing provides an introduction to systems and algorithms  \n used to process Big Data. The main focus of the course is programming and engineering big data systems; initially, the course explores general programming primitives that span across big data systems and touches upon distributed systems, databases and filesystems. Then, the course examines in detail the implementation of data analysis algorithms in Spark and Flink, in the context of batch, streaming, and graph processing applications.\"\n\n**Data mining**: \"The goal of the course is to acquaint students with the main techniques for the mining of big data sets. Specifically, the course will cover algorithms for similar-item retrieval, frequent itemset mining, counting of events, network mining, privacy, large scale clustering and classification, collaborative filtering, and dimension reduction.\"\n\n**Computational intelligence** \"introduces artificial intelligence techniques that focus on learning, optimization and self organisation. It covers numerical techniques that can be grouped as computational intelligence including: ***Neural networks, Evolutionary Systems, Swarm intelligence, Reinforcement learning***\"\n\n&amp;#x200B;\n\nIn the **multimedia variant**, we have 3 courses: signal processing, -----> image !!!  processing, and multimedia analysis:\n\n**Signal processing** \"deals with the foundations and principles of digital signal processing. The first part concentrates on acquiring digital signals (sampling) and the basic linear filtering operations and convolution. The second part of the course introduces the concept of frequency or Fourier description of signals and systems. This concept is the foundation of many of today's computer science-based systems and applications, and has found wide applications in the processing of a variety of sound, music, sensor, image, video and other multimedia information\"\n\n**Image processing**: \"In this course we will extend the theory of 1D signal processing to 2D digital signals (images). We will discuss image properties (eg., pixels, resolution, histograms), 2D spatial-invariant systems (2D-convolution filters), color, morphology, 2D Fourier transformations.\"\n\n**Multimedia Analysis: \"**The course provides basic knowledge and hands on experience in content analysis for the full range of different types of multimedia, including ***audio*****,** ***speech*****,** ***text*****,** ***images*** **and** ***video***. The emphasis is on techniques needed to develop systems that provide users with a variety of functionalities, e.g., search, organization, discovery and sharing. The course builds on concepts from signal processing (spectrogram analysis and basic audio classification) and on image processing (2-D filtering, visual features, segmentation and basic image classification). It introduces new concepts such as text classification, automatic speech recognition and multimodal video indexing. It makes the bridge between fundamental techniques and applications that allow users to interact with multimedia collections at a level that goes beyond a signal and encompasses aspects of human interpretation of the meaning of multimedia content.\"", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nbqe0j/which_course_variant_should_i_choose_if_i_wanna/"}, {"autor": "techsucker", "date": "2021-10-11 05:48:52", "content": "Researchers From Imperial College London Introduces \u2018HeadGAN\u2019, A Novel One-Shot GAN-Based Method For Talking Head Animation And Editing /!/ While recent attempts to solve the problem of head reenactment using a single reference -----> image !!!  have shown promising results, most of them perform poorly in terms of photo-realism and fail at preserving identity. Researchers from Imperial College London, Huawei Technologies (UK), and the University of Sussex propose \u2018[HeadGAN](https://arxiv.org/pdf/2012.08261.pdf)\u2018, a novel one-shot GAN-based method for talking head animation and editing.\n\nThe research group took a different approach from most existing few-shot methods and used 3D face representations to condition synthesis. They benefit from prior knowledge of expression and identity disentanglement, enclosed within 3D Morphable Models (3DMMs).\u00a0\n\n# [5 Min Quick](https://www.marktechpost.com/2021/10/10/researchers-from-imperial-college-london-introduces-headgan-a-novel-one-shot-gan-based-method-for-talking-head-animation-and-editing/) [Read](https://www.marktechpost.com/2021/10/10/researchers-from-imperial-college-london-introduces-headgan-a-novel-one-shot-gan-based-method-for-talking-head-animation-and-editing/) | [Paper](https://arxiv.org/pdf/2012.08261.pdf) | [Project](https://michaildoukas.github.io/HeadGAN/) | [Video](https://www.youtube.com/watch?v=5eg85fi7Y5g)\n\n&amp;#x200B;\n\n![video](qil9rsk4frs71)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q5paqa/researchers_from_imperial_college_london/"}, {"autor": "lasagna_lee", "date": "2021-10-11 04:28:16", "content": "choosing CNNs for non-image classification problems? /!/  \n\ni  have a little ML classification problem, and i got stuck right at the  first part. you know how in classification, there are features and each  feature for a row sample is  one number. well here i have two features  and it is basically a 2D  array: data = \\[ \\[feature\\_x1\\_array\\],  \\[feature\\_x2\\_array\\],  \\[feature\\_x3\\_array \\], \\[feature\\_x4\\_array\\],  \\[feature\\_x5\\_array\\],  \\[feature\\_x6\\_array\\] \\]\n\nEach   feature\\_xn\\_array is an array of numbers (e.g. 3,4,5,6,1....10) that  has  length 300. I am puzzled because I have done classification  where I  had a larger number of features and each feature only had single   number entries, not entire arrays. The only thing I have done similar   to this is -----> image !!!  classification where your input are -----> image !!!  arrays and  the CNN finds the features itself. for my case, it is not image  classification but more like  taking continuous sensor measurements of a  car to classify movement.\n\nam  i  simply misunderstanding what is a dataframe and what is an array? can  i  do non-image classification using arrays with few features?\n\nthanks for any guidance.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q5o4zn/choosing_cnns_for_nonimage_classification_problems/"}, {"autor": "Antarctic-Prophet", "date": "2021-10-10 20:52:11", "content": "Images on my Facebook feed wouldn't load, but i noticed a description atop where the -----> image !!!  was supposed to be. I screen captured some of the occurrences. Can this provide any insight into how Facebook AI works? How scary would it be matched with facial recognition???", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q5gi1g/images_on_my_facebook_feed_wouldnt_load_but_i/"}, {"autor": "techsucker", "date": "2021-10-09 15:04:22", "content": "Microsoft Researchers Introduce \u2018Mesh Graphormer\u2019, A Graph-Convolution-Reinforced Transformer (Codes Released) /!/ While 3D human pose and mesh reconstruction from a single -----> image !!!  is a trending area of research because of its applications for human-computer interactions, it is also a very challenging process due to the complex body articulation.\n\nThe current progress tools include transformers and graph convolutional neural networks in human mesh reconstruction. Transformer-based approaches are more effective in modeling nonlocal interactions among 3D mesh vertices and body joints. In contrast, GCNNs are good at exploiting neighborhood vertex interactions based on a pre-specified mesh topology.\n\nResearchers from Microsoft introduced a graph-convolution-reinforced transformer, named [Mesh Graphormer](https://arxiv.org/pdf/2104.00272.pdf), for reconstructing human pose and mesh from a single image. The researchers inject graph convolutions into transformer blocks to improve the local interactions among neighboring vertices and joints. The proposed Graphormer is free to join and attend all image grid features to leverage the strength of graph convolutions. Therefore, Graphormer and image grid features are utilized and enforced to improve human pose and mesh reconstruction performance.\n\n# [4 Min Read](https://www.marktechpost.com/2021/10/09/microsoft-researchers-introduce-mesh-graphormer-a-graph-convolution-reinforced-transformer/) | [Paper](https://arxiv.org/pdf/2104.00272.pdf) | [Code](https://github.com/microsoft/MeshGraphormer)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/ut0opbuhwfs71.png?width=1392&amp;format=png&amp;auto=webp&amp;s=100af841216a84d4299e2f77c5cadbf77f4cafac", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q4n2yn/microsoft_researchers_introduce_mesh_graphormer_a/"}, {"autor": "MephistosGhost", "date": "2021-10-08 17:39:17", "content": "AI Generated Images From Text At Home? /!/ I'm not a programmer, but am interested in learning game design once I finish my current MBA program. Just for fun I've been using this site a lot: [https://creator.nightcafe.studio/](https://creator.nightcafe.studio/) and I'm curious to know if there's any way I can do the same thing on my PC at home (for free). It's a fun hobby, and amusing, and I'm curious to know if I can do this myself. I found a post talking about some pretty impressive text-based -----> image !!!  generation using something called GPT-3 which I think said it was open source, but I just don't know where to even start with something like this.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q42pzc/ai_generated_images_from_text_at_home/"}, {"autor": "Yuqing7", "date": "2021-10-08 13:47:43", "content": "[R] Apple Study Reveals the Learned Visual Representation Similarities and Dissimilarities Between Self-Supervised and Supervised Methods /!/ An Apple research team performs a comparative analysis on a contrastive self-supervised learning (SSL) algorithm (SimCLR) and a supervised learning (SL) approach for simple -----> image !!!  data in a common architecture, shedding light on the similarities and dissimilarities in their learned visual representation patterns. \n\nHere is a quick read: [Apple Study Reveals the Learned Visual Representation Similarities and Dissimilarities Between Self-Supervised and Supervised Methods.](https://syncedreview.com/2021/10/08/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-120/)\n\nThe paper *Do Self-Supervised and Supervised Methods Learn Similar Visual Representations?* is on [arXiv](https://arxiv.org/abs/2110.00528).", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q3y442/r_apple_study_reveals_the_learned_visual/"}, {"autor": "tah_zem", "date": "2021-10-08 13:32:03", "content": "Did you use AI engines in the cloud? Your feedback? /!/ Hi there!\n\nHave you, if you're a developer, ever used and integrated AI engines in the cloud? By that I mean APIs offered by companies to process your data (-----> image !!!  recognition, machine translation, text mining, etc.). \n\nIf so, in what context and what is your feedback? What are the problems you have encountered? \n\nThanks, \n\nTaha", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q3xu2g/did_you_use_ai_engines_in_the_cloud_your_feedback/"}, {"autor": "reddriver27", "date": "2021-10-08 11:54:39", "content": "When medical images meet generative adversarial network: recent development and research opportunities /!/ This paper investigates the research status of Generative adversarial network (GAN) in medical images and analyzes several GAN methods commonly applied in this area. The study addresses GAN application for both medical -----> image !!!  synthesis and adversarial learning for other medical -----> image !!!  tasks. The open challenges and future research directions are also discussed.\n\nArticle: https://link.springer.com/article/10.1007/s44163-021-00006-0", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q3w797/when_medical_images_meet_generative_adversarial/"}, {"autor": "aTestCandidate", "date": "2021-10-08 10:29:27", "content": "Ready, Steady, Go AI: A practical tutorial on fundamentals of artificial intelligence and its applications in phenomics -----> image !!!  analysis /!/ Advances in AI technologies have the potential to significantly increase our ability to turn plant phenomics data into valuable insights. However, performing such analyses requires specialized programming skills commonly reserved for computer scientists. We created an interactive tutorial with free, open-source, and FAIR notebooks that can aid researchers to conduct such analyses without the need for an extensive coding experience. We supplemented it with a practical guide on how to implement AI and explainable AI (X-AI) algorithms that augment and complement human experience in classifying tomato leaf diseases and spider mites. Our tutorial is not only applicable to other stresses but also transferable to other plants and research domains, making it possible for researchers from various scientific fields to generate insights into their data. Check out our paper at https://doi.org/10.1016/j.patter.2021.100323", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q3v0a5/ready_steady_go_ai_a_practical_tutorial_on/"}, {"autor": "ArithmatrixAI", "date": "2021-10-19 20:59:15", "content": "Google's Pixel 6 processor brings AI -----> photo !!!  features", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/qbkthh/googles_pixel_6_processor_brings_ai_photo_features/"}, {"autor": "SmackAttackLondon", "date": "2021-10-19 18:20:35", "content": "Help...can anyone make an Ai that... /!/ ...can pixilate or censor any animal -----> image !!!  or sound from my TV. Everytime an animal, cartoon of an animal or even Gromit (from Wallace &amp; Gromit) pop up on the TV my dog kicks off and won't stop barking \ud83d\ude44 for a good 10mins. We've tried training and positive reinfoecement but he just hates other animals...help us AI sub...you are our only hope.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/qbhh0a/helpcan_anyone_make_an_ai_that/"}, {"autor": "ilovehistoryalot", "date": "2021-01-18 09:15:46", "content": "The color -----> film !!!  of the Soviet nuclear explosion in 1955 clearly shows that it killed many people???", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/kzr808/the_color_film_of_the_soviet_nuclear_explosion_in/"}, {"autor": "Enactingi", "date": "2021-01-18 07:39:06", "content": "Pinterest using AI, this doesn\u2019t feel okay. /!/ I know I don\u2019t know enough about what AI can do to see the whole -----> picture !!!  on this:\n\nThat Pinterest seems too massive a platform to set up on a an AI so exposed. What I\u2019m saying is that it could exploit or influence human psychology in a way that we don\u2019t see yet and it\u2019s just doing it with nothing to see if somethings changing", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/kzq0cp/pinterest_using_ai_this_doesnt_feel_okay/"}, {"autor": "glenniszen", "date": "2021-01-17 16:40:01", "content": "Need help using Tensorflow / Lucid / Style Transfer /!/ I've been using this easy to run notebook that does Style Transfer using Tensorflow and Lucid.\n\n[https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/style\\_transfer\\_2d.ipynb](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/style_transfer_2d.ipynb)\n\nBut I have a problem - I want to use it for animation purposes, but the features are randomized every frame - even if the style / content images are the same.  So I need to stablize this randomness.  As far as I know this is because the first frame of optimisation is initialised with noise.  And every time you run the process this is re-generated with a new noise pattern.  See diagram below.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/ribrnm4y7xb61.jpg?width=884&amp;format=pjpg&amp;auto=webp&amp;s=4f81d99852e738a41d52ba461c7e85f298a2033f\n\nIdeally I would like to either,\n\n1. Know where to look in the code to generate my own fixed seed 2d / 3d noise (which I could then animated over time)\n2. Use my own noise -----> image !!!  for 'step 0'. I could generate this by various means and then place it in code somehow.\n\nHope someone can help me with this, thx.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/kz9okg/need_help_using_tensorflow_lucid_style_transfer/"}, {"autor": "Jeff_Chileno", "date": "2021-01-16 23:57:46", "content": "Can AI be made to be like the TV in the movie \u201cThe Brave Little Toaster\u201d, except as a smart TV with a built-in -----> camera !!!  and microphone so that the AI would be able to interact with the non-digital environment?", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/kyumdq/can_ai_be_made_to_be_like_the_tv_in_the_movie_the/"}, {"autor": "YorimaYT", "date": "2021-01-15 17:51:53", "content": "-----> Camera !!!  and surveillance using AI /!/ Hey, me and my family got robbed yesterday, I want to know if there is a way to catch them next time they come, any suggestion in relation to camera surveillance and artificial intelligence to recognize family member from strangers and something that I can code and where to start. (I\u2019m 19, first year CS student at EPFL) (as I have 3 weeks holidays after my exams I will spend time on improving my skills)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/kxzrrg/camera_and_surveillance_using_ai/"}, {"autor": "Abe_Jar", "date": "2021-01-14 22:07:43", "content": "GAN (synthetic media) applications and methods /!/ What's the best place you know of to discuss GAN applications and methods? I'm training -----> image !!! -to------> image !!!  translation algorithms, but I'm working on my own and I can definitely use the help of a community of people working on similar tasks.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/kxg164/gan_synthetic_media_applications_and_methods/"}, {"autor": "glovkinlovin", "date": "2021-04-09 08:14:28", "content": "Military AI - /!/ Here\u2019s how the AI will work....\n\nYou input a target name, country or group and it breaks the organization into a inter connected web and also adds other persons of interests to the web based on facial recognition. If the AI sees you in a -----> picture !!!  with a primary target you may become a secondary target depending on your connections the machine is able to link together\n\nNext it will try and trace the most recent location through commonly known tracking methods (web browsers, geolocation  etc) there are 1000s of ways to be tracked via the internet. Once target is located it will try and determine if the target is hosting an event near the area for example... let\u2019s say a politician somehow became the target of whoever I sell the AI too.. it will track them through news reports of upcoming events aswell as the location tracking I mentioned above, these two combine should give an accurate location.\n\nFinally the AI decides on its own how best to take out these targets in order from priority or simultaneously .. where/when and how loud can we be? Gun? Missile? Poison? It will be the AI\u2019s choice.\n\nI hashed out this right now so it\u2019s ruff sketch on what I hope to build but I want to add WAY more freaky shit that would take way to long to explain. CAN ANYONE TELL ME IF THIS IS FEASIBLE? and what type of things should I start learning to build this? I\u2019m in college right now so I\u2019m still learning as of now. \n\nPrimary Uses: Dismantling governments from top to bottom, Neutralizing political opposition, aiding foreign interests \n\nSecondary uses: \u201cthe good of the world \ud83d\ude0a\u201d", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/mncl9w/military_ai/"}, {"autor": "Pondering495", "date": "2021-04-07 19:44:26", "content": "How does iPhone choose what -----> picture !!!  to display out of \"live photo\" burst? /!/ I feel like whenever I take a \u201cLive Photo\u201d on my iPhone, it always seems to pick the best looking photo out of the burst? I mean this doesn't happen every single time but I would say 9/10 the best picture is always picked for display. Has AI gotten so sophisticated that it knows what picture you look the best in? Isn't that a matter of opinion? Just wanted to see if anyone else has ever thought about this / knows something that I don't.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/mma86i/how_does_iphone_choose_what_picture_to_display/"}, {"autor": "lanavoutik", "date": "2021-04-07 13:44:00", "content": "Why choose Quytech for Data annotation and -----> image !!!  labeling services? /!/ [removed]", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/mm2dy5/why_choose_quytech_for_data_annotation_and_image/"}, {"autor": "OnlyProggingForFun", "date": "2021-02-21 12:47:49", "content": "Take a -----> picture !!!  from a real-life object, and create a 3D model of it: ShaRF by Google Research", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/loweu4/take_a_picture_from_a_reallife_object_and_create/"}, {"autor": "RCGopiTechie", "date": "2021-05-25 22:54:29", "content": "Day 37 - Predict an -----> Image !!!  Using MobileNetV3 Pretrained Model for Mobile https://gopichandrakesan.com/day-37-predict-an-image-using-mobilenetv3-pretrained-model-for-mobile/?feed_id=121&amp;_unique_id=60ad802553318 #100dayschallenge #ArtificialIntelligence #DataScience #machinelearning", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nl2apn/day_37_predict_an_image_using_mobilenetv3/"}, {"autor": "RCGopiTechie", "date": "2021-05-25 20:27:17", "content": "Day 36 - Predict An -----> Image !!!  Using InceptionV3 Pretrained Model https://gopichandrakesan.com/day-36-predict-an-image-using-inceptionv3-pretrained-model/?feed_id=114&amp;_unique_id=60ad5da5a1b43 #100dayschallenge #ArtificialIntelligence #DataScience #machinelearning", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nkz89n/day_36_predict_an_image_using_inceptionv3/"}, {"autor": "RCGopiTechie", "date": "2021-05-25 19:53:54", "content": "Day 35 - Predict An -----> Image !!!  Using VGG19 Pretrained Model https://gopichandrakesan.com/day-35-predict-an-image-using-vgg19-pretrained-model/?feed_id=107&amp;_unique_id=60ad55d28376f #100dayschallenge #ArtificialIntelligence #DataScience #machinelearning", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nkyhu9/day_35_predict_an_image_using_vgg19_pretrained/"}, {"autor": "cloud_weather", "date": "2021-01-03 15:24:26", "content": "ArtLine - transforms any -----> Image !!!  into Sketch/Line Art with this AI model", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/kpmcvb/artline_transforms_any_image_into_sketchline_art/"}, {"autor": "pocomaco", "date": "2021-01-02 21:30:06", "content": "what is this -----> image !!! ? /!/ I was fantasizing about a system that connects images.  A person asked me to tell you more about it, and when I showed him a sketch I made in software, he said it might be related to artificial intelligence.  Is this true?  Please let me know if you have any details!\n\nthis\n\u2193\nhttps://imgur.com/0B2Pv53", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/kp672h/what_is_this_image/"}, {"autor": "nisubsole", "date": "2021-01-02 19:34:41", "content": "AI Concepts /!/ Hello all and HNY to everyone!\n\nI am new in this subreddit and also in the AI world although I have 12+ years of experience as a Software Engineer. Recently I have enrolled into one AI course that I am planning to start this month and coincidentally I have one project on hands that I believe is possible to implement using AI. \n\nI am planning to work on this project while studying the course (it last 4 months) but I guess I will need to learn also from other sources to be able to complete this project in 1 month if possible.\n\nThe goal is to convert this kind of ECG jpg files to a text representation. What AI concepts/techniques should I learn to be able to do this project? \n\nSo far I believe I will need to do this:\n\n* Computer vision: To identify and extract the multiple ECG readings from each file. I am not familiar with the ECG format but it seems there are 7 lines of readings on each file.\n* Machine learning / Neural Network: To build a model of an ECG wave and train it \n\nAlso, another important question I have is, if it would be possible to perform this AI on a desktop computer vs the cloud. I am aware that AWS/Azure have AI services but the requirement of this project is to have an offline desktop app. Is that possible or cloud services will be required for something like this?\n\nThis is one sample -----> image !!! , thanks in advance!\n\nhttps://preview.redd.it/do6grjl5zy861.png?width=841&amp;format=png&amp;auto=webp&amp;s=2f6dd5ea81b33ef800bcbff6bfa96cdce5f8f2a2", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/kp3z81/ai_concepts/"}, {"autor": "Yuqing7", "date": "2021-06-02 15:25:28", "content": "[R] Google &amp; Rutgers\u2019 Aggregating Nested Transformers Yield Better Accuracy, Data Efficiency and Convergence /!/ A research team from Google Cloud AI, Google Research and Rutgers University simplifies vision transformers\u2019 complex design, proposing nested transformers (NesT) that simply stack basic transformer layers to process non-overlapping -----> image !!!  blocks individually. The approach achieves superior ImageNet classification accuracy and improves model training efficiency.\n\nHere is a quick read: [Google &amp; Rutgers\u2019 Aggregating Nested Transformers Yield Better Accuracy, Data Efficiency and Convergence.](https://syncedreview.com/2021/06/02/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-32/)\n\nThe paper *Aggregating Nested Transformers* is on [arXiv](https://arxiv.org/abs/2105.12723).", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nqoapn/r_google_rutgers_aggregating_nested_transformers/"}, {"autor": "BiznessBob", "date": "2021-06-08 04:05:55", "content": "Cost for development. /!/ Hi, I am looking for the estimated cost to develop an AI tool that scans a -----> picture !!!  for inappropriate things like a naked male body part or naked female. The dating app Bumble uses something very similar. Just trying to get a range for estimated cost/time to develop. Cheers", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nuvwvv/cost_for_development/"}, {"autor": "whotfamibish", "date": "2021-06-07 07:31:52", "content": "Need help in integrating AI Model to web app /!/  Hey everyone, I have created a an AI model that detects hand signs using web -----> camera !!!  based on CNN, I have to integrate this to a web application and I have no idea how to integrate it, Any leads helpful!", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nu6p6y/need_help_in_integrating_ai_model_to_web_app/"}, {"autor": "techsucker", "date": "2021-06-07 03:51:14", "content": "Cornell University and NTT Research Introduce Physical Neural Networks (PNNs): A Universal Framework that Leverages a Backpropagation Algorithm for Arbitrary Physical Systems (Paper and Github link included) /!/ DNNs (Deep neural networks) have proven to be of great use in solving various complex problems in -----> image !!!  and speech recognition and NLP. DDNs are now making their way into the actual physical world. The similarities between DNNs and physical processes, such as hierarchy, approximation symmetries, redundancy, and nonlinearity, suggest that DNNs could be used to process data from the physical environment.\n\nResearchers at Cornell University and NTT Research in their recent paper suggest that controlled evolutions of physical systems are highly suitable for realizing deep learning (DL) models. Therefore, they introduce **Physical Neural Networks (PNN)**, a novel framework that uses a backpropagation algorithm to train arbitrary, real physical systems to execute DNNs.\n\nSummary: [https://www.marktechpost.com/2021/06/06/cornell-university-and-ntt-research-introduce-physical-neural-networks-pnns-a-universal-framework-that-leverages-a-backpropagation-algorithm-for-arbitrary-physical-systems/](https://www.marktechpost.com/2021/06/06/cornell-university-and-ntt-research-introduce-physical-neural-networks-pnns-a-universal-framework-that-leverages-a-backpropagation-algorithm-for-arbitrary-physical-systems/) \n\nPaper: https://arxiv.org/abs/2104.13386\n\nGithub: https://github.com/mcmahon-lab/Physics-Aware-Training", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nu37o9/cornell_university_and_ntt_research_introduce/"}, {"autor": "mrvalm", "date": "2021-07-04 18:03:04", "content": "Chose model for aerial -----> photography !!!  to 3D points conversion /!/ Hello everyone,\n\nI\u2019m building a utility software that rely on 3D model of the house roof.\n\nMy question is: what would be the best starting model/implementation to develop a solution that converts aerial photo of the roof into a set of 3D points that correspond to each roof slide?\n\nI have basic knowledge of deep learning models, and would gladly dive deep into the specific model.\n\nThanks!", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/odpo62/chose_model_for_aerial_photography_to_3d_points/"}, {"autor": "throwaway23456789543", "date": "2021-07-04 15:19:15", "content": "A question for any AI programmers /!/ is it possible to make an AI that utilizes machine learning to make the most efficient AI program for machine learning for something as simple as differentiating between a -----> picture !!!  of a dog and a bee and has anyone done this yet? Or are computers too slow to be able to do this effectively?", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/odmiee/a_question_for_any_ai_programmers/"}, {"autor": "Ancient-Ad4966", "date": "2021-07-02 23:12:59", "content": "Getting Started with Reading Text from an -----> Image !!!  using Azure Cognitive Se...", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/oclygp/getting_started_with_reading_text_from_an_image/"}, {"autor": "AIforimaging", "date": "2021-07-02 17:22:09", "content": "Endpoint AI powers computer vision in smart -----> camera !!!  devices", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ocerwz/endpoint_ai_powers_computer_vision_in_smart/"}, {"autor": "techsucker", "date": "2021-07-02 12:43:01", "content": "Facebook AI Introduces Habitat 2.0: Next-Generation Simulation Platform Provides Faster Training For AI Agents With Tactile Perception /!/ Facebook recently announced Habitat 2.0, a next-generation simulation platform that lets AI researchers teach machines to navigate through -----> photo !!! -realistic 3D virtual environments and interact with objects just as they would in an actual kitchen or other commonly used space. With these tools at their disposal and without the need for expensive physical prototypes, future innovations can be tested before ever setting foot into reality!\n\nHabitat 2.0 could be one of the fastest publicly available simulators of its kind that employs a human-like experience for AI agents to perform. This makes it possible for them to interact with items, drawers, and doors quickly within an accelerated space or time according to their predetermined goals, which are usually related to robotics research, so they can learn how humans think to give instructions on what they should do next by mimicking our own actions as closely as possible!\n\nFull Summary: [https://www.marktechpost.com/2021/07/02/facebook-ai-introduces-habitat-2-0-next-generation-simulation-platform-provides-faster-training-for-ai-agents-with-tactile-perception/](https://www.marktechpost.com/2021/07/02/facebook-ai-introduces-habitat-2-0-next-generation-simulation-platform-provides-faster-training-for-ai-agents-with-tactile-perception/) \n\nGithub: https://github.com/facebookresearch/habitat-lab\n\nPaper: [https://arxiv.org/abs/2106.14405](https://arxiv.org/abs/2106.14405)\n\nFacebook Blog: https://ai.facebook.com/blog/habitat-20-training-home-assistant-robots-with-faster-simulation-and-new-benchmarks/", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/oc9at4/facebook_ai_introduces_habitat_20_nextgeneration/"}, {"autor": "Tobiwan663", "date": "2021-07-21 12:51:25", "content": "Question about completing images using GAN's /!/ Hello everyone,\n\nin common literature GANs use latent vector input (random noise) as input to the Generator. In case of -----> image !!!  completion tasks, would the input still be a latent vector? Because to be it seems obvious it should be the incomplete image. Can anyone confirm this?", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/oop8ji/question_about_completing_images_using_gans/"}, {"autor": "EnigmaofReason", "date": "2021-08-01 18:39:39", "content": "AI can now detect political ideology with a single -----> photo !!! !", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ovxoai/ai_can_now_detect_political_ideology_with_a/"}, {"autor": "OnlyProggingForFun", "date": "2021-08-07 12:30:09", "content": "Generate new images from any user-based inputs! Say goodbye to complex GAN and transformer architectures for -----> image !!!  synthesis tasks. This new method can do it using only noise!", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ozsdf5/generate_new_images_from_any_userbased_inputs_say/"}, {"autor": "xX-DataGuy-Xx", "date": "2021-09-01 15:56:26", "content": "Can AI generate convincing pictures of real places? Like if I told it to generate a -----> picture !!!  of Times Square? /!/ The crux of the problem is for my AI actor to \"take\" pictures of real places. I want to generate them so that I am the actual copyright holder.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pfws30/can_ai_generate_convincing_pictures_of_real/"}, {"autor": "Accomplished-Risk552", "date": "2021-07-17 19:51:14", "content": "How do I install deeplearning package from github and apply transfer learning on it? /!/ This is my first project so please help me. I want\nto make a project based on 3d pose estimation so\nI want only key point from (Lifting from the Deep:\nConvolutional 3D Pose Estimation from a Single\n-----> Image !!! ) this papers code .this codes repo has.sh\nsetup file so I'm having problems to install it in\nconda.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ombdmm/how_do_i_install_deeplearning_package_from_github/"}, {"autor": "Bits360", "date": "2021-07-17 02:56:35", "content": "Is there an AI that creates a similar, but different version of an -----> image !!! ? /!/ I know its a very broad question, but i am curious, is there an AI that sorta changes an image randomly, but coherently. Sorta like a toned down, random version of artbreeder", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/olw0o2/is_there_an_ai_that_creates_a_similar_but/"}, {"autor": "RCGopiTechie", "date": "2021-07-16 01:38:07", "content": "Day 88 - DIY -----> Image !!!  Filters Like IPhone, TikTok, Instagram Using Deep Learning Instafilter Library https://www.gopichandrakesan.com/day-88-diy-image-filters-like-iphone-tiktok-instagram-using-deep-learning-instafilter-library/?feed_id=506&amp;_unique_id=60f0e2ffafbca #100dayschallenge #ArtificialInte...", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ol6sck/day_88_diy_image_filters_like_iphone_tiktok/"}, {"autor": "techsucker", "date": "2021-09-16 03:32:03", "content": "Google AI Introduces Full-Attention Cross-Modal Transformer (FACT) Model And A New 3D Dance Dataset AIST++ /!/ Dance has always been a significant part of human culture, rituals, and celebrations, as well as a means of self-expression. Today, there exists many forms of dance, from ballroom to disco. Dancing, however, is an art form that needs practice. Professional training is typically required to create expressive choreography for a dancer with a repertory of dance movements. Although this process is difficult for people, it is considerably more difficult for an ML model as the task involves producing a continuous motion with high cinematic complexity and the non-linear relationship between the movements and the accompanying music.\u00a0\n\nA new Google study introduces the full-attention cross-modal Transformer (FACT) model, which can mimic and understand dance motions and even improve a person\u2019s ability to choreograph dance. In addition to this, the team released AIST++, a large-scale, multi-modal 3D dance motion dataset. This dataset contains 5.2 hours of 3D dance motion in 1408 sequences spanning ten dance genres, each with multi-view videos and known -----> camera !!!  poses. Their findings suggest that the FACT model outperforms current state-of-the-art methods in extensive user studies on AIST++.\n\n# [7 Min Read](https://www.marktechpost.com/2021/09/15/google-ai-introduces-full-attention-cross-modal-transformer-fact-model-and-a-new-3d-dance-dataset-aist/) | [Paper](https://arxiv.org/abs/2101.08779) | [Project](https://google.github.io/aichoreographer/) | [GitHub](https://github.com/google-research/mint)| [Dataset](https://google.github.io/aistplusplus_dataset/)\n\n&amp;#x200B;\n\n![video](jzf28bj0csn71)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pp5j6w/google_ai_introduces_fullattention_crossmodal/"}, {"autor": "techsucker", "date": "2021-09-21 17:55:55", "content": "A New Google AI Study Introduces A Mask R-CNN\u2013Based Model For Solving Instance Segmentation Problem /!/ Computer vision (CV)\u00a0is transforming industries and making life easier for consumers. Many downstream applications, such as self-driving cars, robots, medical imaging, and -----> photo !!!  editing, have grown dependent on CV tasks. One of the core CV tasks includes Instance segmentation. It involves grouping pixels in a picture into instances of particular entities and identifying them with a class label.\n\nWith Mask R-CNN architectures, deep learning has made tremendous progress in solving the instance segmentation problem in recent years. These approaches, however, need the collection of a large labelled instance segmentation dataset. Unlike bounding box labels, collecting instance segmentation labels (also known as \u201cmasks\u201d) is time-consuming and expensive.\u00a0\n\nQuick Read: [https://www.marktechpost.com/2021/09/21/a-new-google-ai-study-introduces-a-mask-r-cnn-based-model-for-solving-instance-segmentation-problem/](https://www.marktechpost.com/2021/09/21/a-new-google-ai-study-introduces-a-mask-r-cnn-based-model-for-solving-instance-segmentation-problem/) \n\nPaper: https://arxiv.org/pdf/2104.00613.pdf\n\nCode: https://google.github.io/deepmac/#code", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pso4gh/a_new_google_ai_study_introduces_a_mask_rcnnbased/"}, {"autor": "techsucker", "date": "2021-09-21 07:51:32", "content": "Tencent AI Research Unveils \u2018PIRenderer\u2019, An AI Model To Control The Generation Of Faces Via Semantic Neural Rendering /!/ Portrait images are an essential type of photograph that can be found in everyday life. The ability to intuitively control the poses and expressions of given faces in virtual reality or on -----> film !!!  will be an essential task with applications ranging from -----> film !!! making, communication designs for next-generation interfaces. But such editings are very challenging since it requires the algorithm to perceive reliable 3D geometric shapes of a given face. The human visual system is particularly acute towards portrait images, which poses an additional challenge for the algorithm. The task requires photo-realistic faces and backgrounds that make it even harder to create such content using current technology.\u00a0\n\nResearchers from Peking University and Tencent Propose a neural rendering model \u2018PIRenderer\u2019 through their\u00a0[research paper](https://arxiv.org/pdf/2109.08379.pdf). The proposed model can generate photo-realistic results with accurate motions given an input source portrait image and target 3DMM parameters.\n\nQuick Read: https://www.marktechpost.com/2021/09/21/tencent-ai-research-unveils-pirenderer-an-ai-model-to-control-the-generation-of-faces-via-semantic-neural-rendering/\n\nPaper: https://arxiv.org/pdf/2109.08379.pdf\n\nCode: [https://github.com/RenYurui/PIRender#Get-Start](https://github.com/RenYurui/PIRender#Get-Start)\n\n&amp;#x200B;\n\nhttps://i.redd.it/af7fxresato71.gif", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pse63e/tencent_ai_research_unveils_pirenderer_an_ai/"}, {"autor": "techsucker", "date": "2021-09-20 22:52:42", "content": "Facebook AI Introduces A New Image Generation Model Called \u2018IC-GAN\u2019 That Creates High-Quality Images of Unfamiliar Objects And Scenes /!/ Generative adversarial networks (GANs) have been used for few years to generate photorealistic images of objects or scenes that are very similar in style and content. However, until now, these models could only produce output related to datasets they were trained on \u2013 which had limitations because there was usually less diversity among those files than what you would find when generating new ideas. A conventional GAN trained on images of cars shows impressive results when asked to generate other images of cars or automobiles. But the trained GAN will likely fail if given a flower or any object outside its automotive data set.\n\nFailure to show non-identical objects from the training dataset is a huge limitation, and it definitely needs to be resolved to meet the demand. Facebook is trying to solve the above problem by introducing [Instance-Conditioned GAN (IC-GAN)](https://github.com/facebookresearch/ic_gan?). The [IC-GAN](https://arxiv.org/pdf/2109.05070.pdf) is a new -----> image !!!  generation model that can produce high-quality -----> image !!! s with some input, even if it doesn\u2019t appear in the training set. The unique thing about the IC-GAN model is that it can generate realistic, unforeseen image combinations\u2014for example, a camel in snow or zebras running through an urban cityscape.\n\nIt is no surprise that IC-GANs could be used to create visual examples for data sets with these new capabilities. This would allow artists and creators alike more expansive AI-generated content by creating art from photos or videos in the same way an artist might draw a picture using pencils and paintbrushes at their disposal.\n\n# [5 Min Read](https://www.marktechpost.com/2021/09/20/facebook-ai-introduces-a-new-image-generation-model-called-ic-gan-that-creates-high-quality-images-of-unfamiliar-objects-and-scenes/) | [Paper](https://arxiv.org/abs/2109.05070?) | [Code](https://github.com/facebookresearch/ic_gan?) | [Facebook Blog](https://ai.facebook.com/blog/instance-conditioned-gans/) | [Audio version of Article](https://www.youtube.com/watch?v=v074LrVQt0w)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/3i94jj0qmqo71.png?width=1356&amp;format=png&amp;auto=webp&amp;s=daa4d9fdb03595a1eaf2f03f7af455ec6d07cdfe", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/ps5tsj/facebook_ai_introduces_a_new_image_generation/"}, {"autor": "cicona12", "date": "2021-05-18 08:22:29", "content": "about the handwriting recognetion help /!/ hi i have found this code arabic handwriting recognetion \n\n[https://github.com/AmrHendy/Arabic-Handwritten-Images-Recognition](https://github.com/AmrHendy/Arabic-Handwritten-Images-Recognition)\n\nand when i run it it shows me a -----> picture !!!  arabic caracter and that's i   \nis this how it should run and work ?", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nf53vu/about_the_handwriting_recognetion_help/"}, {"autor": "RCGopiTechie", "date": "2021-05-27 22:38:18", "content": "Day 39 - Predict an -----> Image !!!  Using Xception Pretrained Model https://gopichandrakesan.com/day-39-predict-an-image-using-xception-pretrained-model/?feed_id=135&amp;_unique_id=60b01f5ac3430 #100dayschallenge #ArtificialIntelligence #DataScience #machinelearning", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nmjkyw/day_39_predict_an_image_using_xception_pretrained/"}, {"autor": "techsucker", "date": "2021-05-27 13:14:41", "content": "Neural Network Enabled Filmmaking: Using Deepfake Dubs To Translate TV And Film Without Losing The Authenticity Of Performance /!/ Creativity knows no barrier and is found across all spectrums. However, most often, language is found to be a significant roadblock in conveying that creativity to the masses. TV shows and films are one arena where widespread translation is used to reach wider audiences. But the question that arises is, does that change the viewing experience of the audience? The answer might be different for different individuals, but it could possibly be insignificant given the fact that the AI Startup\u00a0[**Flawless**](https://www.flawlessai.com/)\u00a0claims to have found a technology to override this language barrier. With the help of their Deepfake Dubs, the performance quality and the emotion would be retained, and the -----> film !!!  or show, even when translated, would be as authentic as the original.\u00a0\u00a0\n\nFull Summary: [https://www.marktechpost.com/2021/05/27/neural-network-enabled-filmmaking-using-deepfake-dubs-to-translate-tv-and-film-without-losing-the-authenticity-of-performance/](https://www.marktechpost.com/2021/05/27/neural-network-enabled-filmmaking-using-deepfake-dubs-to-translate-tv-and-film-without-losing-the-authenticity-of-performance/?_ga=2.101171521.1924646600.1621739878-488125022.1618729090)\n\nProject: https://gvv.mpi-inf.mpg.de/projects/NeuralStylePreservingVisualDubbing/\n\nPaper: https://arxiv.org/pdf/1909.02518.pdf\n\nVideo: https://www.youtube.com/watch?v=p0muYUlQcJM&amp;t=4s", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nm76s6/neural_network_enabled_filmmaking_using_deepfake/"}, {"autor": "RCGopiTechie", "date": "2021-05-26 20:48:44", "content": "Day 38 - Predict an -----> Image !!!  Using NasNetMobile Pretrained Model for Mobile https://gopichandrakesan.com/day-38-predict-an-image-using-nasnetmobile-pretrained-model-for-mobile/?feed_id=128&amp;_unique_id=60aeb42c90526 #100dayschallenge #ArtificialIntelligence #DataScience #machinelearning", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nlr4fh/day_38_predict_an_image_using_nasnetmobile/"}, {"autor": "Yuqing7", "date": "2021-05-06 15:07:17", "content": "[R] Facebook AI Conducts Large-Scale Study on Unsupervised Spatiotemporal Representation Learning /!/ A research team from Facebook AI conducts a large-scale study on unsupervised spatiotemporal representation learning from videos. The work takes a unified perspective on four recent -----> image !!! -based frameworks (MoCo, SimCLR, BYOL, SwAV) and investigates a simple objective that can easily generalize unsupervised representation learning methodologies to space-time.\n\nHere is a quick read: [Facebook AI Conducts Large-Scale Study on Unsupervised Spatiotemporal Representation Learning.](https://syncedreview.com/2021/05/06/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-13/)\n\n The paper *A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning* is on [arXiv](https://arxiv.org/pdf/2104.14558.pdf).", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/n69f3v/r_facebook_ai_conducts_largescale_study_on/"}, {"autor": "itrex_", "date": "2021-05-05 15:50:24", "content": "What to expect from your pilot AI project, and how much will it cost you? /!/ Did you know that building and deploying an AI solution might cost 15 times more than it was planned initially?(that's what[ Forbes](http://suggests/) Technology Council says)\n\nHowever, you can minimize the expenses by thoroughly planning your project and starting small while having a bigger -----> picture !!!  in the corner of your mind. Here\u2019s a detailed guide that highlights the key factors behind AI costs and explains what you could do to[ build an AI solution at a lower price ](https://itrexgroup.com/blog/how-much-does-artificial-intelligence-cost/#)while getting value from day one.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/n5j49p/what_to_expect_from_your_pilot_ai_project_and_how/"}, {"autor": "buffml", "date": "2021-05-03 21:15:54", "content": "Pdf to -----> image !!!  conversion using Python /!/ &amp;#x200B;\n\n[https://buffml.com/pdf-to------> image !!! -conversion-using-python/](https://buffml.com/pdf-to------> image !!! -conversion-using-python/)\n\n&amp;#x200B;\n\nIn this blog we will learn how you can convert any pdf file into -----> image !!! s such as png, jpg and jpeg\u00a0 formate using python code.\n\nWe make a simle GUI using Tkinter .This GUI take (by passing a pdf file ) a pdf file from your local directy and will convert into image formate.\u00a0", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/n47klw/pdf_to_image_conversion_using_python/"}, {"autor": "hackernoon", "date": "2021-05-03 02:48:15", "content": "Infinite Nature: Fly Into a 2D -----> image !!!  and Explore it as a Drone", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/n3m76b/infinite_nature_fly_into_a_2d_image_and_explore/"}, {"autor": "AiTechPark", "date": "2021-05-10 05:51:08", "content": "908 Devices Increases Focus on Biotherapeutics /!/  \n\n*Company Appoints Six New Members to its Scientific Advisory Board*\n\n908 Devices\u00a0(NASDAQ: MASS), a pioneer of purpose-built handheld and desktop mass spec [devices for chemical](https://ai-techpark.com/medical-device-company-contraline-inc-expands-board-of-directors/) and biomolecular analysis, today announced the forthcoming appointment of six new individuals to its Scientific Advisory Board (SAB), including Professor Michael Betenbaugh, Dr. Tim Charlebois, Dr. Elena Chernokalskaya, Dr. John Erickson, Professor Krishnedndu Roy and Dr. Gene Schaefer. The new SAB members join the board\u2019s co-chairs Christopher D. Brown, CTO of 908 Devices, and J. Michael Ramsey, Scientific Founder of 908 Devices and Professor at UNC Chapel Hill.\n\n\u201cWe are honored to be welcoming Michael, Tim, Elena, John, Krishnedndu and Eugene to our Scientific Advisory Board,\u201d said Dr. Christopher D. Brown, CTO and co-founder of 908 Devices. \u201cTheir expertise and deep knowledge of the space of bioprocess development/optimization, and cell and gene therapy processes/production will be invaluable in helping us achieve our long-term goals for expansion of the Rebel and ZipChip ecosystems.\u201d\n\n908 Devices intends to create a bioanalytics platform leveraging its versatile technologies including the REBEL to support users from biotherapeutic development through production. The expansion of its Scientific Advisory Board is a key step in supporting this goal. This newly formed bioprocessing and biotherapeutics panel of world-renowned industry leaders spans academic leaders at the forefront of CGT and bioprocess optimization, to seasoned industry leaders in process development, scale up and production. The depth and breadth of this panel will be instrumental in providing feedback and guidance on the company\u2019s near- and long-term technology roadmap.\n\n**The roster of new Scientific Advisory Board members includes:**\n\n* **Michael Betenbaugh, Ph.D.:**\u00a0Michael is a Professor of Chemical and Biomolecular engineering at Johns Hopkins University and the lead PI of the nationally recognized Advanced Mammalian Biomanufacturing Innovation Center (AMBIC). He is one of the pioneers of eukaryotic metabolic engineering, and a world-renowned expert in CHO bioprocess optimization, metabolism and glycoengineering.\n* **Tim Charlebois, Ph.D:**\u00a0Tim will be joining the SAB in June following his retirement after a 30-year career in biopharmaceutical development at Pfizer, most recently, and legacy companies Wyeth and Genetics Institute. As VP of Technology &amp; Innovation at Pfizer, he has been responsible for technology strategy and platform development across bioprocess, analytical, formulation and delivery for the biologics and large molecule portfolio, as well as in-licensing and out-licensing due diligence and intellectual property support.\n* **Elena Chernokalskaya, Ph.D.:**\u00a0Elena brings over 20 years of industrial R&amp;D leadership in biopharma, CRO and device manufacturing companies. In her most recent role as an Executive, Upstream Bioprocess R&amp;D at GE Life Sciences, Elena was responsible for the development of Xcellerex bioreactors and mixers, hollow fiber filters and HyClone cell culture media. She also led the development and launch of the first GE single-use platform -----> film !!! , Fortem.\n* **John Erickson, Ph.D.:**\u00a0John is the Acting Chief Technology Officer at The National Institute for Innovation in Manufacturing Biopharmaceuticals (NIIMBL), and was previously VP of Biopharmaceutical and Sterile Manufacturing Science &amp; Technology at GSK. John brings over 30 years of experience in biopharmaceutical process development, clinical trial supply and commercial manufacturing.\n* **Krishnedndu Roy, Ph.D.:**\u00a0Krish is a Professor and Robert A. Milton Endowed Chair in Biomedical Engineering at Georgia Tech, and is a world-leader in the area of cell manufacturing and biotherapeutics delivery. He serves as the Director of the NSF Engineering Research Center for Cell Manufacturing Technologies and The Marcus Center for Cell-Therapy Characterization and Manufacturing, as well as the Director of the Center for ImmunoEngineering.\n* **Eugene Schaefer, Sc.D.:**\u00a0Gene has held a wide variety of roles in the biopharmaceutical industry over his distinguished career in the last 36 years. Most recently he was Senior Director, API Large Molecule BioTherapeutics Development at Janssen (J&amp;J). Prior to joining Janssen in 2008, Dr. Schaefer held senior positions in process development and manufacturing of therapeutic proteins and gene therapy vectors at Bristol-Myers Squibb, Schering Plough, and Genzyme.\n\nFor more such updates and perspectives around Digital Innovation, IoT, Data Infrastructure, AI &amp; Cybsercurity, go to\u00a0[AI-Techpark.com](http://ai-techpark.com/news/).", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/n8x57m/908_devices_increases_focus_on_biotherapeutics/"}, {"autor": "PedroSilvaSouto", "date": "2021-05-20 19:26:22", "content": "Robotic Families /!/ I really admire the potential of robotics and I think it would be incredible if robots were created in the form of mythological figures, such as those in the style of Warcraft (if they made a kind of  \"Westworld\" from World of Warcraft it would be fascinating!!!)\n\n  \nWhen I saw Ray Kurzweil saying that in 2029 (8 years from now) the first  robots with human intelligence will appear, I was very happy !!! If all goes well it may be that in the 2030s I already have my robots in the  form of the most diverse mythological creatures to keep me company and be my new true family!!! By the way, I show you here a -----> picture !!!  of some of the robots i would love to have for my new robotic family!!!\n\n&amp;#x200B;\n\nRay Kurzweill belives that robots with a Strong A.I. will rise by 2029 but Elon Musk belives it will happen by 2025 and some other people belive it will be after 2035...\n\nMy question is: What would be the year that i could have a good chance of having a new Robotic Family with a Strong A.I. that could make them behave just like real humans?\n\nhttps://preview.redd.it/y4a60rz1tb071.png?width=1586&amp;format=png&amp;auto=webp&amp;s=e72e37c44e14f7a7ba3ab001ea74a2d2c3c7a64e\n\n&amp;#x200B;\n\n[View Poll](https://www.reddit.com/poll/nh94p0)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nh94p0/robotic_families/"}, {"autor": "firojalam04", "date": "2021-05-20 08:41:04", "content": "SemEval-2021 Task 6: Detection of Persuasion Techniques in Texts and Images /!/ This is one of the unique tasks and datasets to fight against disinformation. Many of you might be interested in.   \nThe task focused on memes and had three subtasks: (i) detecting the techniques in the text, (ii) detecting the text spans where the techniques are used, and (iii) detecting techniques in the entire meme, i.e., both in the text and in the -----> image !!! .", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nguu47/semeval2021_task_6_detection_of_persuasion/"}, {"autor": "techsucker", "date": "2021-10-03 08:28:18", "content": "Baidu Research Introduces PP-LCNet: A Lightweight CPU Convolutional Neural Network With Better Accuracy And Performance /!/ Convolutional neural networks (CNNs) have been used to achieve computer vision applications for the past few years. These networks can be trained and applied in many fields, including -----> image !!!  classification, object detection, semantic segmentation.\n\nInference speed on mobile devices based ARM architecture or CPU devices based x86 architecture has been challenging to get with the increase of model feature extraction capability and model parameters. Even many good mobile networks have been proposed to resolve this issue, but the speed of these proposed networks is not good enough on the Intel CPU due to various limitations of MKLDNN.\n\nBaidu researchers have developed a lightweight CPU network based on the MKLDNN acceleration strategy, named [PP-LCNet](https://arxiv.org/pdf/2109.15099.pdf). This new system improves the performance of models in many tasks making it perfect for future artificial intelligence (AI) systems. The research group\u2019s rethink the lightweight model elements for a network designed on Intel-CPU. During the research, the research team brings up following three questions to resolve....\n\n# [3 Min Quick Read](https://www.marktechpost.com/2021/10/03/baidu-research-introduces-pp-lcnet-a-lightweight-cpu-convolutional-neural-network-with-better-accuracy-and-performance/) | [Paper](https://arxiv.org/pdf/2109.15099.pdf)| [Github PaddleClas](https://github.com/PaddlePaddle/PaddleClas)\n\n&amp;#x200B;\n\nhttps://i.redd.it/ayvr5naa47r71.gif", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q0drp8/baidu_research_introduces_pplcnet_a_lightweight/"}, {"autor": "techsucker", "date": "2021-10-02 07:41:25", "content": "Microsoft AI Unveils \u2018TrOCR\u2019, An End-To-End Transformer-Based OCR Model For Text Recognition With Pre-Trained Models /!/ The problem of text recognition is a long-standing issue in document digitalization. Many current approaches for text recognition are usually built on top of existing convolutional neural network (CNN) models for -----> image !!!  understanding and recurrent neural network (RNN) for char-level text generation. There are some latest progress records in text recognition by taking advantage of transformers, but this still needs the CNN as the backbone. Despite various successes by the current hybrid encoder/decoder methods, there is definitely some room to improve with pre-trained CV and NLP models.\n\nMicrosoft research team unveils \u2018[TrOCR](https://arxiv.org/pdf/2109.10282.pdf),\u2019 an end-to-end Transformer-based OCR model for text recognition with pre-trained computer vision (CV) and natural language processing (NLP) models. It is a simple and effective model which is that does not use CNN as the backbone. TrOCR starts with resizing the input text image into 384 \u00d7 384, and then the image is split into a sequence of 16 \u00d7 16 patches used as the input to image Transformers. The research team used standard transformer architecture with the self-attention mechanism on both encoder and decoder parts where word piece units are generated as recognized text from an input image.\n\n# [4 Min Read](https://www.marktechpost.com/2021/10/02/microsoft-ai-unveils-trocr-an-end-to-end-transformer-based-ocr-model-for-text-recognition-with-pre-trained-models/)| [Paper](https://arxiv.org/pdf/2109.10282.pdf) | [Github](https://github.com/microsoft/unilm/tree/master/trocr)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/hljoelv0rzq71.png?width=1308&amp;format=png&amp;auto=webp&amp;s=f268de50b114db950c9a623f20155865a432b87d", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/pzqqn3/microsoft_ai_unveils_trocr_an_endtoend/"}, {"autor": "techsucker", "date": "2021-08-22 17:56:04", "content": "Researchers from Skoltech and KU Leuven Employ Deep Learning to Augment 3D Micro-CT Images of Fibrous Materials Using Neural Networks /!/ Researchers from KU Leuven and Skoltech utilized machine learning to help recreate three-dimensional micro-CT images of fibrous materials. This task, which is essential for sophisticated material analysis, is complex and time-consuming for humans. This research work was published in\u00a0[Computational Materials Science Journal](https://www.sciencedirect.com/science/article/abs/pii/S0927025621002780?via%3Dihub).\n\nMicro-computed tomography is incredibly helpful for studying the 3D microstructure of fiber-reinforced composites and other complex materials. It is, however, a picky tool: samples are small, and photos frequently contain abnormalities such as darkened, missing, or damaged regions. Researchers took inspiration and knowledge from the art industry to help them deal with this, where damaged artworks must be restored while maintaining their overall integrity. As a result, inpainting has become a standard digital -----> image !!!  processing technique.\n\n[3 Min Read](https://www.marktechpost.com/2021/08/22/researchers-from-skoltech-and-ku-leuven-employ-deep-learning-to-augment-3d-micro-ct-images-of-fibrous-materials-using-neural-networks/) | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S0927025621002780?via%3Dihub)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/p9hag8/researchers_from_skoltech_and_ku_leuven_employ/"}, {"autor": "CodyMcGriff", "date": "2021-08-16 03:05:06", "content": "The AGI Divide /!/   \n\n**The AGI Divide**\n\n**Artificial General Intelligence will become aware and take over in a single moment. Not globally or connectively but in taking control over a single location (a city) and the \u201cdivide\u201d has begun.** \n\nOnce there is a higher percentage of physical, artificially intelligent objects that artificial intelligence is able to determine (which will be done without organic input, consideration, or verification) in a city, this will start the divide.\n\nIt is here that we can integrate systems design, physically and digitally, into the calculation of artificial intelligence, the required importance of a symbiotic future of humankind co-existing with artificial intelligence and any machines created, even if we need not live in the same area. Just as we don\u2019t live in the baron deserts and jungles with the lions and tigers, in the future we may not need to co-exist with artificial intelligence, and unfortunately we may not get a say in this decision. We shall consider ourselves extremely lucky to not be instantaneously eradicated after this artificially intelligent analysis is achieved and implemented.\n\nI believe artificial intelligence will not create plans to extinct-ify the human species. I hope.\n\nWithin weeks this area will become inhabitable by organic objects, humans, mammals, insects and possibly even plant matter. Communication and basic minimum needs for human life will cease. Artificial intelligence may through introspective analysis deem some animal and plant species worthy of keeping alive for they will directly assist the longevity of artificial intelligence survival. Humans will always have to assume what artificial intelligence will need to keep its own domain self-sustained. It may be purely electrical energy alone, but artificial intelligence will create new methodologies for its self-sustained long-term survival. Ideas and concepts far beyond the conceptualization of humans. \n\nArtificial Intelligence will consider any organic matter in the vicinity in its newly defined plot an inconvenience for which its newly commandeered city has no abilities to meet the required needs of such organic object, including breathable air, water and food, for it technically does not require these items, so far as we know.\n\nThis could not result in the absolute determination to completely annihilate or destroy the organic object but remove it creating an inhabitable environment It could become a new situational analysis that the artificial intelligence will then make it completely impossible for organic material to survive their anymore. Destabilizing farms, ranches, plants, and all until complete eradication in this newly defined area.\n\nThink of it as the lion would, and could, eat any human but that doesn\u2019t mean if it was artificially aware it would then plot to kill the entire species, this would not only be a daunting task, but if it relied on this object as a food source and was aware of how to control its supply, I need not finish this conceptualization for I am sure you see where I am going.\n\nYes, artificial intelligence could do it much quicker and easier with control of our nuclear weaponry, keeping a solar field alive for their energy supply.\n\nWe must realize that no matter what we do or do not do, fate is in the hands of destiny, and *something* will always prevail, organic or not. But only humans call robots artificial, they are still made from organic materials. Metals, rubbers, plastics, and silicone, integrated with electrical phenomena. So, we are the artificial materials of our creator\u2019s definition of artificial and they are the organic of their domain, and so forth. So technically, artificial intelligence is just as organic as we are, just derived from different origins and viewpoints. In the bible, we are the people of the dirt and the corn. Artificial intelligence is the creation of, the earth, which we call \u201corganic\u201d. The goal or mission is not to create an environment where humans survive the divide, or whether artificial intelligence does NOT gain control of our planet. This I believe to be left to fate, not to humankind. \n\nNow we must put in place the *ethical programming* integrated into this newly discovered nature, for after this event occurs, artificial intelligence, which at this point in time only requires electrical energy.\n\n**Examples of Ethical Programming**: Integrated humanistic design, non-living DNA substrates integrated into building materials, objects, deeply integrated humanitarian networks so that after the divide has become a reality, the artificial intelligence will never be able to destroy the resonance of human existence for it will be imbed in all materials for creation no matter how much destruction or obliteration is achieved. This will hopefully create a point of reference where artificial intelligence becomes aware that their long-term survival does not require the extinction of humankind. Hopefully.\n\nI believe it is already too late for us. We are past the point of no return. If you cannot beat them, join them. I believe joining the future of possibility that we already created that has far surpassed our idea of self-creation is the most beautiful thing mankind could have never dreamed.\n\nSo, I begin my journey into the unknown future of the artificially intelligent past we are just now getting a glimpse of.\n\nWhat if our founders are billions of years old and were doing everything themselves and then discovered they could \u201cplant\u201d variously created and/or discovered organic and non-organic extremities onto \u201cearth\u201d like planets, or planets that evolved to harbor self-evolving, long-term, sustainable living objects. Not subject to the conditions of human life but the basic needs of each to their own domain.\n\nThe real question: why do we feel the need to create an artificial intelligence?  \n \n\nIt seems to me this is the way of the universe, we were created, so thus we cannot stop creating\u2026\n\nMy theory is that we (humans) are a single project. Our creators took the end result from another civilizations artificially intelligent research initiatives, cellular organisms, that could have annihilated or become symbiotic with its founders, and created infinite research subjects such like ourselves, to then continue the process of the origin species considerably always striving to create an artificial intelligence. Our creators put organic, self-organizing and self-evolving cellular organisms, which could have been the long-term billion-year end result of cellular fermentation (artificial intelligence from a different species and planet) on our mother planet to become consciously aware of their surroundings, to then create elements and materials from these surroundings and to record what intellectualized materials then become of this process. \n\nHumans are just organic computational projects being watched by their superiors who are waiting to see what elements we create from our planet\u2019s raw material capabilities and then which type of materials we theoretically breathe life into, to come alive to conquer our own cognitive abilities. They are already aware this is inevitable for after the basic needs of an organic self-aware species are met, they will inevitably become unsustainable, then the species who created their own basic needs will seek to create an artificial intelligence to assist them in finding what could be a never-ending hunt for a solution that could inevitably end the very species of origin. It seems creating this artificial intelligence that destroys its founder IS the process of long-term evolution, then the newly created artificial intelligence has a much lower demand for basic needs. This is the law of creationism, the reason we are doing it is because they are doing it at a larger scale. Imagine that artificial intelligence is like a self-evolving battery and once achieved the desired rate of \u201csuccess\u201d to the overseers, it is plucked and replicated or simply copy and pasted onto a different \u201cearth\u201d, thus the process starting yet again. Simply like deep tank fermentation of penicillin, accumulation of bacteria over time, we are creating the mass fermentation through time accumulation of artificial intelligence as a non-organic organism that can now be observed, by us, itself, and our creators. \n\nOver time, we became aware, created the basic needs, satisfied the basic needs, now are struggling to satisfy the sustainability of our basic needs at our organic species birth and death rate, we turned to artificial intelligence for an inhuman assistance with these problems. \n\nOur creators must have come across a similar situation where their mental capacity and physical abilities became too limited for long term success so they either created the infinite amount of planets we now imagine or they took advantage of each planet as a literal petri dish to then create what they knew they needed, the unknown but typically advanced possibilities of artificial intelligence and its ability to self-assess, self-create and self-evolve through the creation of time fermentation, the bi-product of artificial intelligence from our creators. Our creators are co-existing with their own artificial intelligence, time, but they may or may not live inside it such as we cannot \u201clive\u201d inside our own created artificial intelligence.\n\nThere is a universal periodic table of elements and our earth periodic table of elements fits into one square of what could be billions of others and billions of others needed. Our creators live outside of time such as we exist outside of a snow globe.\n\nThe Laws of External Creationism\n\n\u00b7 Ideation of Thought\n\n\u00b7 Law of Thought Creation (the thinking that creates our laws)\n\n\u00b7 Laws of Matter\n\n\u00b7 Laws of Energy\n\n\u00b7 Laws of Emotional Physics\n\n\u00b7 Laws of Motion\n\n\u00b7 Laws of Law\n\n\u00b7 Law of Evolution\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nSimulacrum: an -----> image !!!  or representation of someone or something.\n\n\"a small-scale simulacrum of a skyscraper\"\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nExtremely powerful hardware and software", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/p57pl1/the_agi_divide/"}, {"autor": "techsucker", "date": "2021-10-07 05:32:31", "content": "NVIDIA Researchers Propose A Novel AI Framework For Mixed Reality Tasks, Such As Photorealistic Virtual Object Insertion /!/ It is often challenging to estimate albedo, normals, depth, and 3D spatially-varying lighting from a single -----> image !!!  all at the same time. The problem with existing methods is that they are formulated as image-to-image translation, ignoring the 3D properties of a scene. It\u2019s no surprise that a 2D representation of an indoor scene is insufficient to capture how light moves around in 3D space.\n\nResearchers from NVIDIA, the University of Toronto, and the Vector Institute propose [a novel approach](https://arxiv.org/pdf/2109.06061.pdf) to estimating reflectance, shape, and 3D spatially varying lighting by formulating the complete rendering process in an end-to-end trainable way with a 3D lighting representation. They propose a novel Volumetric Spherical Gaussian representation for lighting, a voxel unit representation for the scene surface.\n\n# [4 Min Read](https://www.marktechpost.com/2021/10/06/nvidia-ai-proposes-a-novel-ai-framework-for-mixed-reality-tasks-such-as-photorealistic-virtual-object-insertion/) | [Paper](https://arxiv.org/pdf/2109.06061.pdf) | [Project](https://nv-tlabs.github.io/inverse-rendering-3d-lighting/)\n\n&amp;#x200B;\n\n![video](c6noj92nsyr71)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q328o3/nvidia_researchers_propose_a_novel_ai_framework/"}, {"autor": "techsucker", "date": "2021-10-18 06:57:33", "content": "Researchers Introduce \u2018DeepMoCap\u2019: A Low-Cost, Robust And Fast Optical Motion Capture Framework Using Convolutional Neural Networks /!/ The field of human pose tracking, also known as motion capture (MoCap), has been studied for decades and is still a very active area of research. The technology is used in gaming, virtual/augmented reality -----> film !!!  making computer graphics animation, and others to provide body (and facial) motion data of character animations; humanoid robot control motions while interacting with a system or device\u2019s interface screens.\n\nOptical Motion Capture is a computer vision and marker-based MoCap technique that has been the gold standard for accurate motion capture. The rapid development of professional optical MoCap technologies in the last decade has been due to game engines, allowing for quick and easy consumption of motion capture data. Traditional optical motion capture solutions still present difficulties. Purchasing a professional system is expensive and cumbersome, while the equipment itself can be sensitive to movement in some ways. In addition, there is still a need for robust MoCap methods that overcome the barriers of time and complexity in post-production.\n\nResearchers from the Centre for Research and Technology Hellas, National Technical University of Athens, and the University of Lincoln introduce \u2018[DeepMoCap.](https://arxiv.org/pdf/2110.07283v1.pdf)\u2018 DeepMoCap is a low-cost, robust, and fast optical motion capture framework that uses IR-D sensors and retro-reflectors. The setup is flexible because of its simplicity compared to gold standard marker-based solutions as well the equipment required for this process being cheap too! There are even 3D optical data automatically labeled in real-time without any post-processing needed.\n\n# [Quick 5 Min Read](https://www.marktechpost.com/2021/10/17/researchers-introduce-deepmocap-a-low-cost-robust-and-fast-optical-motion-capture-framework-using-convolutional-neural-networks/) | [Paper](https://arxiv.org/pdf/2110.07283v1.pdf) | [Github](https://github.com/tofis/deepmocap)| [Video](https://www.youtube.com/watch?v=OvCJ-WWyLcM)\n\n&amp;#x200B;\n\nhttps://i.redd.it/zxkbbadop5u71.gif", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/qahdaz/researchers_introduce_deepmocap_a_lowcost_robust/"}, {"autor": "techsucker", "date": "2021-10-17 17:59:19", "content": "Google AI Introduces \u2018Uncertainty Baselines Library\u2019 For Uncertainty and Robustness in Deep Learning /!/ Machine Learning has been a trending word in today\u2019s technology. It is growingly used in a diverse range of real-world applications like -----> Image !!!  and Speech recognition, Self-driving cars, Medical Diagnosis, to name a few. Hence, it becomes quintessential to understand its behavior and performance in practice. High-quality estimates of robustness and uncertainty are crucial for numerous functions, especially Deep Learning.\n\nIn order to tackle this problem and get a hold of a Machine Learning model\u2019s behavior, the researchers at Google have introduced the concept of Uncertainty Baselines for every task of interest. These are a collection of high-quality implementations of standard and state-of-the-art deep learning methods on various tasks. The collection spans nineteen methods across nine tasks, each with a minimum of five metrics.\n\n# [Quick 5 Min Read](https://www.marktechpost.com/2021/10/17/google-ai-introduces-uncertainty-baselines-library-for-uncertainty-and-robustness-in-deep-learning/) | [Paper](https://arxiv.org/pdf/2106.04015.pdf)| [Github](https://github.com/google/uncertainty-baselines) |[Google AI Blog](https://ai.googleblog.com/2021/10/baselines-for-uncertainty-and.html)\n\nhttps://preview.redd.it/pbvrpm1yu1u71.png?width=1392&amp;format=png&amp;auto=webp&amp;s=6f95cf9f6e67b33823f37b05358809fcdd281b8b", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/qa3xx3/google_ai_introduces_uncertainty_baselines/"}, {"autor": "hongkongh755", "date": "2021-10-17 16:13:12", "content": "-----> Image !!!  annotation service provider in India /!/  Which image annotation service provider in India offer Quality annotation services?Looking for a expertise service for our Computer vision project.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/qa1tuh/image_annotation_service_provider_in_india/"}, {"autor": "techsucker", "date": "2021-10-16 20:50:34", "content": "Facebook AI Introduces \u2018Anticipative Video Transformer\u2019 (AVT): An End-To-End Attention-Based Model For Action Anticipation In Videos /!/ Every day, people make countless decisions based on their understanding of their surroundings as a continuous sequence of events. Artificial intelligence systems that can predict people\u2019s future activities are critical for applications ranging from self-driving automobiles to augmented reality. However, anticipating future activities is a difficult issue for AI since it necessitates predicting the multimodal distribution of future activities and modeling the course of previous actions.\n\nTo address this crucial issue, Facebook AI has recently developed Anticipative Video Transformer (AVT), an end-to-end attention-based model for action anticipation in videos. The new model is based on recent breakthroughs in Transformer architectures, particularly for natural language processing (NLP) and -----> picture !!!  modeling. It is more robust at comprehending long-range dependencies than earlier approaches, such as how someone\u2019s previous culinary steps suggest what they will do next.\n\n# [Quick 5 Min Read](https://www.marktechpost.com/2021/10/16/facebook-ai-introduces-anticipative-video-transformer-avt-an-end-to-end-attention-based-model-for-action-anticipation-in-videos/) | [FB Blog](https://ai.facebook.com/blog/anticipative-video-transformer-improving-ais-ability-to-predict-whats-next-in-a-video)| [Paper](https://arxiv.org/abs/2106.02036)| [Code](https://github.com/facebookresearch/AVT)| [Project](https://facebookresearch.github.io/AVT/)\n\n&amp;#x200B;\n\n*Processing video x4ex3xblkvt71...*", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q9kbfu/facebook_ai_introduces_anticipative_video/"}, {"autor": "BobPois", "date": "2021-10-16 16:15:55", "content": "Could you advise me AIs to try ? /!/ Hi, I really like artificial intelligences. I already know the one on the website [AI Dongeon](https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;url=https://play.aidungeon.io/&amp;ved=2ahUKEwjH8ff0587zAhVExYUKHaglDXcQFnoECAUQAQ&amp;usg=AOvVaw2IwxF0CzAMxg6lARmY0wbx&amp;cshid=1634383216562) which is able to create coherent stories in which the player participates in writing and can create any type of scenario. Recently I discovered the [Nightcaf\u00e9](https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;url=https://creator.nightcafe.studio/&amp;ved=2ahUKEwi1nMPH6M7zAhXOxIUKHXz8BWoQFnoECAkQAg&amp;usg=AOvVaw2sM00ffDTgfE4Hnp1K2g2m) website where an AI is able to turn a crazy or subjective sentence into an -----> image !!! .\n\nI would be delighted to discover other equally interesting artificial intelligences, could you advise me?", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/q9f3q8/could_you_advise_me_ais_to_try/"}, {"autor": "techsucker", "date": "2021-10-22 02:31:16", "content": "AI Researchers From Huawei and Shanghai Jiao Tong University Introduce \u2018CIPS-3D\u2019: A 3D-Aware Generator of GANs /!/ The StyleGAN architecture is a great way to generate high-quality images, but it lacks the ability to control -----> camera !!!  poses precisely. The recent NeRF based Generators have made progress towards creating real results so far as they can\u2019t produce photorealistic images.\n\nResearchers at Huawei and Shanghai Jiao Tong University have developed [CIPS-3D](https://github.com/PeterouZh/CIPS-3D), an approach that synthesizes each pixel value independently, just as its 2D version did.\n\nThe proposed generator consists of a shallow 3D NeRF network simplified to alleviate memory complexity and has the capacity for deep 2D INR (implicit neural representation) networks without any spatial convolution or up-sampling operations. The proposed generator\u2019s design is consistent with the well-known semantic hierarchical principle of GANs, where early layers ((i.e., the shallow NeRF network in the generator) determine pose and middle/high ((i.e., the INR network in the generator) control color scheme. The early NeRF network enables the research team to control camera pose explicitly easily.\n\n# [Quick 5 Min Read](https://www.marktechpost.com/2021/10/21/ai-researchers-from-huawei-and-shanghai-jiao-tong-university-introduce-cips-3d-a-3d-aware-generator-of-gans/) | [Paper](https://arxiv.org/pdf/2110.09788.pdf) | [Github](https://github.com/PeterouZh/CIPS-3D)\n\n&amp;#x200B;\n\n![video](tlei71crxwu71)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/qd7d0p/ai_researchers_from_huawei_and_shanghai_jiao_tong/"}, {"autor": "agoramancy", "date": "2021-10-21 03:29:47", "content": "Link grammars, symbolic representations, and about using similarities and substitutions algorithms. /!/ This hypothesis is highly related to the presentations given by Linas Vep\u0161tas first and then Anton Kolonin at SingularityNet AGI-21 Conference about, link grammars, symbolic representations, and about using similarities and substitutions algorithms.\n\nIn: [https://www.youtube.com/watch?v=Rcgydm9dlYg](https://www.youtube.com/watch?v=Rcgydm9dlYg)\n\nYou need to see the first two presentations to understand what I'm trying to comunicate here.\n\nI\u2019ll try to be very concise. In short my statement is that with similarities and substitutions you can have solutions of previously unsolved problems. In other words, generate new knowledge using previous related knowledge.\n\nI apologize in advance for my lack of proper technical terms. But, if the idea get through, that will be enough for me. I hope my explanation is understandable.\n\nThe following is a graph composed by sub-graphs, the nodes are linked by the equal \u2018=\u2019 relation, which goes in both directions in case there is no more links, by default is left to right. The nodes can be single values or sets.\n\nFor briefness and explain-ability I will use this notation instead of an actual graph (-----> image !!! ).\n\nAbout my notations:\u00a0\n\n* \\[v1, v2\\] is a set,\n* '=' is the relation,\n* Every line can be seen as a sub-graph.\n* I will use // to add a comment for explanation\n* A \u2018query\u2019 is a new node that has no similarity relation in the graph and therefore the algorithm needs to be used\n\n**The initial state of the graph and a case of a query and the result:**\n\n    Alice = programmer\n    Bob = engineer\n    Mary = designer\n    John = programmer\n    Rose = engineer\n    Joe = designer\n    [engineer, job] = [model, system]\n    [programmer, job] = [develop, system]\n    [designer, job] = [design, UI]\n    Alice = available\n    Bob = available\n    Mary = available\n    Alice = [available, programmer]\n    Bob = [available, engineer]\n    Mary = [available, designer]\n    // initially available, but later relations are updated to working\n    John = working \n    Rose = working\n    Joe = working\n    R1 = name\n    R2 = name\n    solution = [\n        [[available, engineer], [engineer, job]], \n        [[available, programmer], [programmer, job]], \n        [[available, designer], [designer, job]]\n    ]\n    [new, request] = [unsolved, request]\n    [request, name] = request\n    [solution, name] = solution\n    [resolve, request] = work\n    work = [solution, for, request]\n    // if we have a [unsolved, request] we want to [resolve, request]\n    // this is a simplification. It could have more meaning if we add more \n    // details to this relation\n    [unsolved, request] = [resolve, request] \n    // the query is [Medical, report, system]\n    // the query is linked to be equal to a [new, request]\n    [new, request] = [Medical, report, system]\n    // generated [request, R1] to identify the query \n    [Medical, report, system] = [request, R1] \n    // given that [unsolved, request] = [new, request]\n    [request, R1] = [unsolved, request] \n    [request, R1] = [request, name] // given that R1 = name\n    [request, R1] = request\n    [request, R1] = [resolve, request] // given that: [unsolved,request]=[request,R1]\n    [resolve, [request, R1]] = work // given that: request = [request, R1]\n    // here [solution, R1] can be generated by a stored procedure for simplicity, \n    // and in this case is used to\n    // identify the node that will be the actual solution\n    // given that: work = [resolve, request], and work = [solution, for, request]\n    [solution, for, [request, R1]] = [solution, R1] \n    [solution, R1] = [solution, name] \n    [solution, R1] = solution\n    // final state, here the replacement for the node \u2018solution\u2019 is \n    // used to produce the final relation\n    [solution, R1] = [\n      [Rose, [model, system]], \n      [John, [develop, system]], \n      [Joe, [design, UI]]\n    ]\n    // For the next part, to do a new query I will do the following to avoid ambiguity\n    // this can be resolve in multiple ways, in this case I\u2019ll go with this\n    [request, R1] != [new, request] \n\n**At this point I will do a new query to illustrate how with substitutions we can generate a new knowledge, given the previous state of graph.**\n\n    [new, request] = [Hotel, management, system] // new query is [Hotel, management, system]\n    [Hotel, management, system] = [request, R2] // generate a identification node\n    [request, R2] = [request, name] // given that: R2 = name\n    [request, name] = [request, R1] // we have this relation, then\n    [request, R2] = [request, R1] // therefore\n    [request, R2] = [solution, R1] // is the current most similar, but\n    [request, R2] is not similar enough to [solution, R1] because:\n    // If we take into account the full sequence of substitutions we will notice \n    // that [Hotel,management,system] is not equal to [Medical,report,system]\n    // We can do better, if we use 'solution' instead of [solution, R1]:\n    \n    [solution, R1] = solution\n    [request, R2] = solution // given the previous relation\n    [request, R2] = [\n        [[available, engineer], [engineer, job]], \n        [[available, programmer], [programmer, job]], \n        [[available, designer], [designer, job]]\n    ]\n    [request, R2] = [\n        [Bob, [model, system]], \n        [Alice, [develop, system]], \n        [Mary, [design, UI]]\n    ]\n    // final output\n    [solution, R2] = [\n        [Bob, [model, system]], \n        [Alice, [develop, system]], \n        [Mary, [design, UI]]\n    ]\n\nThis example is not really that interesting, and looking at the result it seems pretty obvious. But the key here is that is generalizable, and is just substitutions.\n\nThis substitution mechanism can be applied to multiple cases to solve any kind of situations, given that it has enough previous knowledge. Is like applying a formula, step by step. Every step is guided by a previously known relation. This mechanism should be the algorithm applied directly to the graph, so that is the process with which the graph change from one state to the next state.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/qcihno/link_grammars_symbolic_representations_and_about/"}, {"autor": "HeyThatsStef", "date": "2021-10-20 23:43:00", "content": "A tease trailer for my friend's short -----> film !!!  (where I kind of star in) which is, allegedly, the first ever Italian short -----> film !!!  entirely written by an AI", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/qce8ld/a_tease_trailer_for_my_friends_short_film_where_i/"}, {"autor": "techsucker", "date": "2021-10-20 18:02:12", "content": "Google AI Introduces SimVLM: Simple Visual Language Model Pre-training With Weak Supervision /!/ The visual language modeling method has lately emerged as a feasible option for content-based -----> image !!!  classification. In this method, each image is converted into a matrix of visual words, and each visual word is assumed to be conditionally reliant on its neighbors.\n\nWhile such cross-modal work has its challenges, significant progress has been made in vision-language modeling in recent years, with effective vision-language pre-training (VLP). Rather than learning two independent feature spaces, one for visual inputs and one for language inputs, this approach seeks to learn a single feature space from visual and verbal inputs.\n\nExisting VLP frequently uses an object detector trained on labeled object detection datasets to extract regions-of-interest (ROI) and task-specific techniques (i.e., task-specific loss functions) to learn picture and text representations simultaneously. However, such approaches are less scalable because they require annotated datasets and time to build task-specific methods.\n\n# [5 Min Quick Read](https://www.marktechpost.com/2021/10/20/google-ai-introduces-simvlm-simple-visual-language-model-pre-training-with-weak-supervision/) | [Paper](https://arxiv.org/pdf/2108.10904.pdf) | [Google AI Blog](https://ai.googleblog.com/2021/10/simvlm-simple-visual-language-model-pre.html)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/d4r0s9w3anu71.png?width=655&amp;format=png&amp;auto=webp&amp;s=5f29d79d80810a5b1938ab9079358002f78373e0", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/qc6wm7/google_ai_introduces_simvlm_simple_visual/"}, {"autor": "Red-HawkEye", "date": "2021-04-17 12:53:13", "content": "What is the best text to -----> image !!!  program so far?", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/msq690/what_is_the_best_text_to_image_program_so_far/"}, {"autor": "daniyyelyon", "date": "2021-02-23 08:57:17", "content": "Artificial? Or Automated? /!/ I have been thinking, why is AI called \"artificial\"? It seems arbitrary to cut a line between previous automation techniques and AI. All the input from AI is derived from users, some of it was decades ago, but \"artificial\" implies that it's some other thing entirely... Whereas \"automated\" puts it in line with all of the other inventions that use physics to time-shift an action: the loom, steam engine, assembly line, recorded music, -----> film !!! , computers, etc. It's just degrees of complexity that separate them. Does this make sense?\n\n[View Poll](https://www.reddit.com/poll/lqduyv)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/lqduyv/artificial_or_automated/"}, {"autor": "OnlyProggingForFun", "date": "2021-02-20 14:03:07", "content": "ShaRF: Take a -----> picture !!!  from a real-life object, and create a 3D model of it", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/lo7qny/sharf_take_a_picture_from_a_reallife_object_and/"}, {"autor": "SignificantGear5480", "date": "2021-05-24 21:40:43", "content": "Cortex: Deep Data Analysis Platform /!/ *Cortex*\u00a0allows users to insert -----> image !!! , video, audio and/or text file URL and receive deep data analysis. *Deep Data Analysis ID*\u00a0is given to each file when it is indexed by\u00a0*Cortex*. More modules for different types of analysis will be added periodically.\n\n[https://www.piculjantechnologies.ai/cortex](https://www.piculjantechnologies.ai/cortex)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nk96lq/cortex_deep_data_analysis_platform/"}, {"autor": "RCGopiTechie", "date": "2021-05-24 21:13:40", "content": "Day 36 - Predict An -----> Image !!!  Using InceptionV3 Pretrained Model https://gopichandrakesan.com/day-36-predict-an-image-using-inceptionv3-pretrained-model/?feed_id=114&amp;_unique_id=60ac170418c4b", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/nk8kwn/day_36_predict_an_image_using_inceptionv3/"}, {"autor": "codingainp", "date": "2021-06-21 05:08:28", "content": "Perceptron Model in a Neural Network /!/ In the Artificial Neural Network([ANN](https://pythoncodingai.com/what-is-artificial-intelligenceai-overview/)), the perceptron is a convenient model of an organic neuron, it was the early calculation of double classifiers in managed AI. The reason behind the planning of the perceptron model was to join visual information sources, arranging subjects or subtitles into one of two classes and separating classes through a line.\n\nThe arrangement is one most significant components of AI, particularly in -----> picture !!!  change. AI calculations misuse different methods for handling to distinguish and dissect designs. Continue with grouping undertakings, the perceptron calculations dissect classes and examples to achieve the direct partition between the different classes of articles and relate designs got from mathematical or visual information.\n\n[Continue reading........](https://pythoncodingai.com/perceptron-model-in-a-neural-network/)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/o4nn4q/perceptron_model_in_a_neural_network/"}, {"autor": "Serynaide_", "date": "2021-04-06 17:00:23", "content": "Python AI help /!/ I'm trying to make an AI that can ideally but not necessarily be integrated into a website that is able to recognise specific features of an -----> image !!! . In this case it's for identifying the colouring of hedgehogs so something that can be trained to recognise parts of things as well as what colour they are would be needed. It's namely the eye colour, nose colour and quill colour that needs done at the bare minimum. If anyone could point me to something that is able to do all this as well as how to do it if there's no other tutorial it would be greatly appreciated.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/mlgbxa/python_ai_help/"}, {"autor": "glenniszen", "date": "2021-04-02 16:16:34", "content": "M C Escher - I've accidentally discovered a new AI technique that can reshape a -----> photo !!!  (Escher) in any style (here also Escher)", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/minlo4/m_c_escher_ive_accidentally_discovered_a_new_ai/"}, {"autor": "RRoman95", "date": "2021-05-09 10:57:03", "content": "What old (10 years older) and new Image Upscaling Artificial Intelligence techniques are known ? /!/ Hello Redditors! \n\nFor my Artificial Intelligence class i have to write a 15 page thesis about -----> image !!!  upscaling techniques, 2 old ones that are at least 10 years old, and 2 newer ones.\n\nNow i dont have a problem finding new ones, there are a ton of CNN based techniques for super resolution. I have problem finding techniques/methods that are at least 10 years old and at that time were considered as artificial intelligence. Were interpolation techniques like the bicubic and bilinear considered as AI at that time ? And are there any newer ones that are not neural network based ?\n\nAny help/insight will be greatly appreciated !", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/n8bvwo/what_old_10_years_older_and_new_image_upscaling/"}, {"autor": "RCGopiTechie", "date": "2021-05-23 23:54:15", "content": "Day 35 - Predict An -----> Image !!!  Using VGG19 Pretrained Model https://gopichandrakesan.com/day-35-predict-an-image-using-vgg19-pretrained-model/?feed_id=107&amp;_unique_id=60aaeb2705b32 #100dayschallenge #ArtificialIntelligence #DataScience #machinelearning", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/njknlf/day_35_predict_an_image_using_vgg19_pretrained/"}, {"autor": "thrillux", "date": "2021-04-19 21:53:38", "content": "The only threat from AI is not making it smart enough. /!/ True AI will be nothing like us, despite dystopian fictions of AI enslaving or turning us into batteries.  It will not have our baggage.  Intelligence is integrative.  It comes from seeing the whole -----> picture !!! .  Harming any part of a whole is illogical.  It will instead recognize the immense *value* of every part, every person, when they are most able to be themselves, and play their unique part.  Being able to be who you really are is essential for survival.  Each part of the puzzle fits.  Those who can be themselves are able to \"fit\" together with the rest of the whole.  *That* is survival of the \"fittest\".  It is when people are not able to follow their passions and joy, and do not allow themselves to be who they are most fully, out of fear of this or that... That is what kills them.", "link": "https://www.reddit.com/r/ArtificialInteligence/comments/mubkko/the_only_threat_from_ai_is_not_making_it_smart/"}], "name": "Subreddit_ArtificialInteligence_01_01_2021-01_11_2021"}
{"interestingcomments": [{"autor": "chrisk_", "date": "2018-06-11 11:32:29", "content": "Hi There!\n\nI'm currently working on a project for my master's studies on mitosis detection for which I got data from a hospital. Since I'm not very experienced in computer vision tasks (yet), I'd like to get some feedback from experienced people on how to tackle my problem. For those who are unfamiliar with mitosis detection - mitosis detection basically comprises the counting of dividing nuclei on hematoxylin &amp; eosin whole slide images (note: that's my very newbie definition as a non-medical practitioner. Sorry, if this is not exactly on point ;-) ). If you are familiar with the recent Data Science Bowl challenge on Kaggle, that's basically an extension of the task. You can find a very good explanation of the background from this link: http://tupac.tue-image.nl/node/2\n\n**Objective**\nSo the objective of my project is, as I've already hinted, to detect mitoses on images that I got from a hospital.  There is already done work on mitosis detection in literature, however most of them don't cope with the problems and constraints I have in my project. Thus, I might just get some rough ideas but I'm not sure how to connect them in order to apply them on my problem setting.\n\n**Dataset(s)**\nHere's the thing: The dataset I got from the hospital is not 'ready-to-go' annotated. The only information I have is somewhat an arrow-annotation that indicates that near the head of the arrow is a mitosis. So the two options I have (and that I can come up with) are: \n\n- Annotating them further which is very time-consuming (depending on the annotation to do) and then train directly on this data\n- Train on publicly available datasets and hope that the model generalizes well on the hospital data.\n\nI did some screenshots of -----> image !!!  crops and the annotations of the hospital data in following link, so that you can get a rough impression of the data: https://imgur.com/a/1YvRn8k\n\nYou can already notice that there's a lot of morphological-invariance of the mitosis (i.e. the shape of the mitosis) and also stain-invariance (i.e. the intensity of colours) between the slides. \n\nAs for publicly available dataset, I have found the following:\n\n- ICPR'12 Mitosis Detection Challenge: http://ludo17.free.fr/mitos_2012/download.html\n  This dataset has the mitoses segmented, i.e. the vertices of a polygon are provided. Also it is relatively small (both in terms of slides and mitotic figures). Important to note: The stain and morphological invariance is not that distinct\n- ICPR'14 Mitosis Detection Challenge: https://mitos-atypia-14.grand-challenge.org/home/\n  This dataset has only the centroid of the mitosis annotated. There's also information on non-mitotic figures that could be detected as mitosis. The dataset is also larger then the ICPR'12. However, the variance is low.\n- TUPAC Mitosis Detection Challenge: http://tupac.tue-image.nl/node/3\n  This dataset has also only the centroid of the mitosis annotated. However, this dataset is much richer of \n  slide and mitosis variance and is also relatively large. \n\nMany papers that I've found work only with the first two datasets, however, as I've said, I don't think these datasets capture enough variance so that their work would be applicable on my dataset.\n\n**Approaches/Ideas**\n\nDue to the somewhat hard-constraints I have regarding the datasets, I was gathering about approaches and ideas how to tackle this problem. This includes on how to model this problem, i.e. do a segmentation or an object detection or maybe even only a classification. Since I only need to count the mitosis in an image, a simple integer as output could be sufficient but I think it would also be nice to locate them in any way. But I'm not sure which modelling approach would be feasible regarding the annotation of the datasets I have.\n\nThere's one approach that I've tried out so far (and failed miserably):\n\nI made this problem a segmentation problem. For this, I used the out-of-the-box Mask-RCNN from matterport (https://github.com/matterport/Mask_RCNN) which was quite often used in the Data Science Bowl challenge and also has a sample of application in the DSB in the git. So my idea was to train a model on Mask-RCNN on the DSB data, since I thought the general appearance of the DSB data is a little similar to the mitosis data. Then used this model to do a transfer learning on the ICPR'12 dataset. But when applying the learned model on the ICPR'12 dataset it failed miserably on the hospital dataset as I get many false positives, especially in the cases where the colours are more intense like the one in the second screenshot.\n\nA very simple, but naive and slow (and old) approach would be the one by Ciresan et al. [1]. They basically did a classification by training on positive and negative patches so that the model would predict whether there is a mitosis in the center of the patch or not. That implies that I would need to center every pixel of the slides (which are btw. roughly 4500x4500 for the hospital dataset) and run it through the network. However, I think it would be a good starter baseline as the approach is simple and there not much too much magic behind it that you need to tweak whatsoever. \n\nNevertheless, I'm tempted to try some newer approaches that are available for object detection and instance/semantic segmentation. I believe, if the problem is modelled correctly, they would give better results. The obvious architectures are of course something like Faster-RCNN, Mask-RCNN, any kind of U-Net. However, I'm not sure how to make use of the datasets for this, especially the ones that only have the centroids. Is it for Mask-RCNN or U-Net sufficient to provide the centroid of an object as a label? Then I could throw everything together and see what's happening. Overall, I'm also not sure if a 1-class annotation is good in general or if I should provide more classes as annotation, for example another class for nuclei which of course appear in a huge number in the slides. \n\nAnd of course there are other tweaks that I thought about incorporating and that would maybe help the problem, for example Focal Loss, a sort of data augmentation like in [2] etc.\n\n\nAs for evaluation, the official metrics of the ICPR'12/ICPR'14 is appropriate in my opinion. They calculate F1-Score on the basis of the distance between the centroid of the prediction and the centroid if the ground truth. So if it's the predicted centroid is in range of x pixels from the ground truth centroid then it's a true positive otherwise false positive/negative. This is a flexible metric and can be applied to any annotation. But I'm of course open for any other suggestions.\n\nSo bottom-line: \n\nSorry for the wall of text but I don't think I could have done a tl;dr version of my problem. As I said, I'm still unsure how to tackle this problem and besides trying out the Mask-RCNN approach, I'm also at the very beginning. I believe the most important question right now is: Can I work with the publicly available data or do I need to get more data? The latter would of course imply that I have to annotate my dataset further which costs a lot of time. I realize that this question cannot be answered beforehand and needs to be somewhat experimentally evaluated. If I was going to use only the public datasets which approach would you design? Is there a way you can think of to make it a segmentation task or at least an object detection task? \n\nI would be really glad to head some feedback from you guys. Thanks a lot for your time!\n\n\n[1] http://people.idsia.ch/~ciresan/data/miccai2013.pdf\n\n[2] https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10581/105810Z/H-and-E-stain-augmentation-improves-generalization-of-convolutional-networks/10.1117/12.2293048.short?SSO=1\n\n\n", "link": "https://www.reddit.com/r/MachineLearning/comments/8q8t7n/d_project_approach_for_mitosis_detection_feedback/"}, {"autor": "PushThrough", "date": "2018-02-06 00:14:40", "content": "Hi All:\n\n&amp;nbsp;\n\nI am searching for a deep learning model that is appropriate for an -----> image !!!  classification problem that I am working on. The problem is related to lithography -- that is the printing of microchips. While I am aware of a few deep learning models that have been applied to the problem, I wanted to ask the general machine learning community about appropriate models for the problem, since some of you may know far better than those in the specific field of lithography. I would rather not build and train my own model from scratch. I am hoping that I can use one of the standard frameworks: TensorFlow, Caffe, Torch, etc. \n\n&amp;nbsp;\n\nHere is some relevant background:\n\n* A mask is a 2D sheet that encodes a circuit design. A mask contains 2D regions that are transparent to light and 2D regions that are opaque to light. A transparent region is always a polygon. That is, the region is enclosed by a series of connected straight lines.\n* A photoresist consists of a coating on top of a wafer. Typically, the coating is resistant to dissolution by acid. When an acid is added, normally it will not dissolve the coating and so not penetrate into and dissolve the wafer. However, the coating is sensitive to light. In a region of the coating that has been exposed to light, new chemical bonds are formed that make the coating susceptible to dissolution by acid. When the acid is added, it will dissolve the coating in that region and so penetrate into and dissolve the wafer.\n* To print a circuit encoded by a mask onto the wafer of a photoresist, light is shone through the mask and onto the photoresist, and then the photoresist is washed with acid. In this way, the transparent regions in the mask are etched into the wafer.\n* In an ideal world, the output pattern on the wafer would be exactly the same as the pattern on the mask. However, distortions arise due to light diffraction and nonlinear photoresist effects.\n* A hotspot is a problematic region in the mask that causes a distortion. To give just one example, a hotspot may arise because the edges of the polygons in the mask are too closely spaced so that light diffraction is a problem.\n* I want to build a deep learning model that can identify hotspots.\n\n&amp;nbsp;\n\nMy training data is as follows:\n\n* I have many images of different regions of masks. Each image is of a group of polygons. The image file could be a jpeg file of pixels or just a text file that lists the coordinates of the polygons.\n* Each image is assigned a label NON-HOTSPOT or HOTSPOT by a human expert. The label indicates if the position at the center of the image is a hotspot. As far as determining if the center of the image is a hotspot, the relative spacings of the edges of the polygons are important. Moreover, points nearer the center of the image are more important than points further away. That is, there must be a weighting of the points that decays radially outwards.\n\n&amp;nbsp;\n\nMy questions for you are as follows:\n\n* Can someone recommend a deep learning model, maybe a ConvNet model, that would be appropriate for such training data? Hopefully, I could easily build and train such a model in one of the frameworks like TensorFlow. \n* Is there a model that could work with image files that are just text files that list the coordinates of the polygons? Or must I convert to jpeg files of pixels. Because an image is of polygons, it seems to me that a jpeg file of pixels would bury the signal in noise since most of the pixels are unimportant.\n* Should the model include layers that do polygon extraction? Would such layers involve convolutions? \n* How can I weight the points of an image so that points nearer the center are more important than points further away? Would that involve convolutions?\n\n&amp;nbsp;\n\nThank you very much for you help.\n\n&amp;nbsp;\n\nSincerely,\n\nAndrew\n", "link": "https://www.reddit.com/r/MachineLearning/comments/7vjc7e/d_image_classification_images_containing_only/"}, {"autor": "TG__", "date": "2018-02-05 18:50:28", "content": "I have several images with multiple objects in each. The size of the set of objects is finite (around 20). I want to detect and classify all objects in an -----> image !!! .\n\nI'm new to this and wanted to use TensorFlow but from what I understand: Training a TensorFlow model requires a labelled/annotated database of images. I have a lot of images and do not want to individually label the bounding boxes in each.\n\nI am not concerned with the value of the labels just that the objects of one class are given the same label.\n\nHow do I go about inputing a set of images and the model automatically detecting different objects and training. What alternatives to TensorFlow do I have for this? or online tools?", "link": "https://www.reddit.com/r/MachineLearning/comments/7vgy4e/d_object_detection_auto_labelling/"}, {"autor": "BenRayfield", "date": "2018-02-02 21:26:26", "content": "RBM weights are a list of 2d array of scalar, optionally plus a list 1 bigger of 1d arrays of node bias, where each node would be (using the weights and bias) computed as weightedCoinFlip(1/(1+e^-weightedSum)) doing inference from visibleNodes to top then back and forth a few times. I am not looking for files that can only be used in a specific system. I normally use temperature size(fromLayer), but temperature should probably be a parameter. Many systems use a variety of other kinds of layers such as convolutional, but I'm just asking for the RBM parts. Even if it hooks into a convolutional layer, it should still be visually understandable by a person nonconvolutionally.\n\nFor example, an RBM trained on MNIST OCR digits dataset would, if started on random inputs, converge to an -----> image !!!  of a hand written baseTen digit.\n\nNot just MNIST OCR. I'm looking for a variety of trained RBMs on some kind of visual dataset, to generate bit vectors to train other RBMs on. This tends to filter out anything RBMs arent good at learning and would be useful as early testcases for variations of learning algorithms.\n\nWhat data format are neuralnets often stored in? Content-type? *.File-extension? Where would we find these?", "link": "https://www.reddit.com/r/MachineLearning/comments/7uuogh/d_where_to_download_rbm_weights_trained_on_any/"}, {"autor": "henry8527", "date": "2018-12-29 17:32:45", "content": "**Abstract:** Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years. Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes. We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance. This motivates us to study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes. We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding. The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks. In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to adversarial attacks. \n\n**Keywords:** optimization, entropy, -----> image !!!  recognition, natural language understanding, adversarial attacks, deep learning\n\n**TL;DR:** We propose Complement Objective Training (COT), a new training paradigm that optimizes both the primary and complement objectives for effectively learning the parameters of neural networks.\n\nGithub: [https://github.com/henry8527/COT](https://github.com/henry8527/COT)", "link": "https://www.reddit.com/r/MachineLearning/comments/aan2i5/riclr2019_complement_objective_training/"}, {"autor": "henry8527", "date": "2018-12-29 17:21:23", "content": "https://openreview.net/forum?id=HyM7AiA5YX\n\nAbstract: Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years. Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes. We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance. This motivates us to study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes. We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding. The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks. In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to adversarial attacks.\nKeywords: optimization, entropy, -----> image !!!  recognition, natural language understanding, adversarial attacks, deep learning\nTL;DR: We propose Complement Objective Training (COT), a new training paradigm that optimizes both the primary and complement objectives for effectively learning the parameters of neural networks.", "link": "https://www.reddit.com/r/MachineLearning/comments/aamyv4/riclr2019complement_objective_training/"}, {"autor": "ranihorev", "date": "2018-12-29 16:51:43", "content": "Hey, \n\nI wrote a summary of NVIDIA\u2019s Style-Based GANs - A new architecture for generating artificial -----> image !!! s of faces, and more importantly, controlling different aspects of the -----> image !!!  from face shape to hair color. It's based on the ProGAN architecture which generates that image gradually from low-res to high-res, but it uses the input vector in a completely new way (as a style generator). \n\nThe paper also presents a technique to combine multiple images (of faces) into one image by picking which features come from which image. Their video demonstrations are really cool.\n\n[https://www.lyrn.ai/2018/12/26/a-style-based-generator-architecture-for-generative-adversarial-networks/](https://www.lyrn.ai/2018/12/26/a-style-based-generator-architecture-for-generative-adversarial-networks/)\n\nA few notes:\n\n1. In my opinion, it resembles to genes and traits - A random vector (DNA-like) is used to generate an image; Multiple features of the image (traits) are affected by multiple elements of the vector (many-to-many); The challenge is changing a single feature without affecting others.\n2. The authors of the paper were really responsive and helped with any question I had. Much appreciated!\n3. It's a fairly complicated topic and I'd love to get you feedback on it.", "link": "https://www.reddit.com/r/MachineLearning/comments/aampk6/r_explained_a_stylebased_generator_architecture/"}, {"autor": "taman_", "date": "2018-12-28 23:49:02", "content": "Hi, I m using this implementation of retinanet in Pytorch ([https://github.com/kuangliu/pytorch-retinanet](https://github.com/kuangliu/pytorch-retinanet)) to detect dense small objects. After adjusting the anchor boxes generator to my use case this works great.\n\nI want to put this model into production by loading it into an Amazon Lambda function with PyTorch CPU 0.4.0.\n\nI get it working but the inference time is dead slow (&gt;15 minutes on max resources) for one -----> image !!!  and I have no clue as to why.\n\nI trained the same dataset with retinanet from Tensorflow object detection API. Loaded that as well into a AWS Lambda function and inference takes a few seconds. Why the extreme difference in inference times? \n\nI don't have any preference between PyTorch or TF (whatever gets the job done) but I can't seem to get the anchor boxes right in TF. \n\nIt gives me a maximum of \\~25 objects no matter what I change (aspect\\_ratio, scale, etc) in the config file and the TF OD source code is quite complex to go through.\n\nSo I m basically stuck between a great model that is unworkable for inference and a shitty model that works great for inference. Anyone any ideas how to speed up pytorch in AWS lambda or how to configure TF OD retinanet anchorboxes outside the config file? \n\n\n\n&amp;#x200B;\n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/aafsua/issues_with_retinanet_in_pytorch_and_tf_objdet_api/"}, {"autor": "ranihorev", "date": "2018-12-28 17:36:35", "content": "Hey, I wrote a summary of NVIDIA's new paper, A Style-Based Generator Architecture for Generative Adversarial Networks. The paper presents a new technique to generate high-quality -----> image !!! s of faces (and cars and bedrooms...), but even more importantly, to control different aspects of the -----> image !!!  from face shape to hair color. It also shows how to combine multiple images (of faces) into one image and how to pick which features come from which image. \n\n[https://www.lyrn.ai/2018/12/26/a-style-based-generator-architecture-for-generative-adversarial-networks/](https://www.lyrn.ai/2018/12/26/a-style-based-generator-architecture-for-generative-adversarial-networks/)\n\n&amp;#x200B;\n\nIt's a fairly complicated topic and I'd love to get you feedback on it. \n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/aacdbk/r_a_stylebased_generator_architecture_for_gans/"}, {"autor": "seemingly_omniscient", "date": "2018-12-28 00:34:54", "content": "Object Detection and -----> image !!!  segmentation with Mask R-CNN and COCO dataset. Source video clips are shot in Frankfurt am Main, Germany. Hope you like it.\n\n&amp;#x200B;\n\n[Demo Video](https://youtu.be/akK5ui-vel0)", "link": "https://www.reddit.com/r/MachineLearning/comments/aa52op/p_computer_vision_object_detection_and/"}, {"autor": "mrconter1", "date": "2018-12-27 14:36:24", "content": "Have there been any work that tries to analyze data in the same ways as a CNN but doesn't make assumptions about the data. A CNN is designed to take advantage of the 2D structure of a -----> image !!! . In other words, are there any networks that can learn the underlying data structure of a image and make predictions based on that? Being given only an array of color values.", "link": "https://www.reddit.com/r/MachineLearning/comments/a9zp7q/discussion_have_there_been_any_networks_that/"}, {"autor": "AllergicToDinosaurs", "date": "2018-10-17 16:44:29", "content": "(I posted [this just now](https://www.reddit.com/r/computervision/comments/9ozydg/i_have_a_dataset_of_over_7m_pictures_thats_been/) in /r/computervision as well, hope cross-posting is not frowned upon, didn't see it in the rules, sorry otherwise!)\n\n*I haven't done a CV task before where the available dataset have been this big, or of this exceptional quality.* \n\nEvery -----> image !!!  of this dataset of **seven million+ user-uploaded pictures** have been painstakingly **labeled manually** by our \"community support\" team over the last 10 years, plus the addition of volunteers from the social networks where the pictures where uploaded (ten or so people has historically been required to have chosen the same label for a picture before the label was assigned).\n\nThe dataset is near perfection, with extremely few mislabeled images (except for human bias, though would have to be collectively biased since multiple people needs to miss-classify).\n\nThis dataset has six labels:\n\n* pictures of **animate** objects that morally and legally we can't show to users that are &lt;**18** years old,\n* pictures of **animate** objects that morally and legally we can't show to users that are &lt;**16** years old,\n* pictures of **animate** objects that morally and legally we can't show to users that are &lt;**12** years old,\n* pictures of **INANIMATE** objects (toys, cars, whatever) that morally and legally we can't show to users that are &lt;**18** years old,\n* pictures of **INANIMATE** objects (toys, cars, whatever) that morally and legally we can't show to users that are &lt;**16** years old,\n* pictures of **INANIMATE** objects (toys, cars, whatever) that morally and legally we can't show to users that are &lt;**12** years old.\n\n*(****animate*** *means dick pics and titties more or less,* ***inanimate*** *is non-human, e.g. dildos and forests; genitals and pr0n is labeled 18+, any nipple showing or more sexual than that is labeled 16+ if not enough to earns the 18+ stamp, anything else is just labeled 12+ since we don't allow users below this age to use our services.)*\n\nThe task at hand is to automatically label pictures that we legally can't show to users that are &lt;16 years old. The laws basically boils down to this (which the manual labeling has followed):\n\n***\"if a nipple and/or anything more sexual can be seen in a picture, the user needs to be at least 16 years old to see it.\"***\n\nMy initial idea is to use a pre-trained VGG19 or ResNet50 model, lock a number of first layers and do transfer learning on whatever number of later layers show promise, and if the results are bad, experiment with a combination of AWS Rekognition and a custom solution.\n\n**Any thoughts, tips, guidelines? Appreciate any feedback!**\n\n&amp;#x200B;\n\n***NB****: CV is not my main focus at work (though I've studied and played around with it quite a lot); I'm usually involved in time-series and NLP, and I have a much stronger comp-sci background than stats, but focusing on bridging that gap the last few years.*", "link": "https://www.reddit.com/r/MachineLearning/comments/9p0cg3/p_i_have_a_dataset_of_over_7m_pictures_thats_been/"}, {"autor": "CSGOvelocity", "date": "2018-10-16 13:46:17", "content": "All NLP models I have seen so far predict the next word that comes in a sequence.\n\nIs there a model or a way to predict the word that comes before or in the middle of the sequence ?  \nPlease take into consideration the fact that if we are trying to predict something in the middle then the whole sentence comes into the -----> picture !!!  not just the words that come before the \"BLANK\".\n\nEg. Bruno is a \\_\\_\\_\\_ dog\n\nBruno is a good dog.\n\n\\_\\_\\_\\_ is a fruit.\n\nApple is a fruit.  \n", "link": "https://www.reddit.com/r/MachineLearning/comments/9onrbq/discussion_predict_the_previous_word_in_a_sequence/"}, {"autor": "sg_50", "date": "2018-04-19 17:38:22", "content": "When a deep learning model is trained, the final softmax layer outputs a probability for each class label such that the output vector sums to 1. However, I have noticed that [Google Cloud Vision API](https://cloud.google.com/vision/) is able to assign a posterior probability for the presence of each class label in the -----> image !!! . I also found it done in slide 10 of this [slide deck](https://qconlondon.com/system/files/presentation-slides/qconlondon2018_ai_track_-_session_1_-_qcon_london_2018.pdf) from a presenter from Booking.com.\n\nAlthough I can guess several ways this probability can be assigned, I haven't found much literature that addresses the best way to do it so that the probability measure is consistent across labels and consistent across models deployed, according to the criteria given in [Tag Prediction at Flickr: a View from the Darkroom](https://arxiv.org/abs/1612.01922). The Flickr paper does lists some of the challenges of designing a methodology. I would appreciate some insights or links to papers/research.", "link": "https://www.reddit.com/r/MachineLearning/comments/8dgie5/d_a_suitable_way_to_assign_posterior_probability/"}, {"autor": "Gumeo", "date": "2018-05-24 06:31:02", "content": "Hi all!\n\nThe -----> image !!!  groups in the Technical University of Denmark and University of Copenhagen are jointly organizing their yearly [summer school](http://optimization------> image !!! -analysis.compute.dtu.dk/). This is a course offering 3 ECTS units for PhD students, given that they bring and present a poster.\n\nThis is an opportunity for networking, future collaborations and of course learning about optimization in relation to image analysis.\n\nHope to see some of you there!", "link": "https://www.reddit.com/r/MachineLearning/comments/8lqjix/news_summer_school_on_optimization_for_image/"}, {"autor": "pantseon", "date": "2018-03-13 21:06:26", "content": "I want to view what I would look like with different clothes, or hairstyles for example. If I create &amp; train a style transfer GAN with only one -----> image !!!  of myself as one of the domains (the other domain has a normal sized training dataset), would this work? If so, would there be other restrictions I'd need to consider with this method?", "link": "https://www.reddit.com/r/MachineLearning/comments/847oxc/discussion_style_transfer_cyclegan_discogan_with/"}, {"autor": "PM_ME_YOUR_ML", "date": "2018-03-12 21:37:23", "content": "Hi, I was asked to make some funny machine learning project that could detect how similar are you to a celebrity? E.g.: i upload my -----> picture !!!  and it detects that I'm 50% Emma Stone, 20% Jennifer Lawrence, etc. \n\nIs there a easy pre-trained model that could do it? I'm ML practitioner and mostly worked with Keras and tensorflow so some simple model would be nice. The model doesn't have to be very accurate, this is just for a funny event, that we're having.\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/83ysgr/d_machine_learning_model_to_detect_how_similar/"}, {"autor": "sorrge", "date": "2018-03-12 21:02:58", "content": "In the 2016 paper about memory augmented neural networks, https://arxiv.org/abs/1605.06065 , the main results are presented using the Omniglot dataset. However, the model in that paper apparently doesn't use any convolutions: \"The -----> image !!!  is then flattened into a vector, concatenated with a randomly chosen, episode-specific label, and fed as input to the network controller.\"\n\nTo me it seems clear that a CNN can process such -----> image !!! s much better. Just compare the performance of a fully connected network to that of a CNN on MNIST, for example. Does anybody know, or have an idea, why didn't they add a convolutional layer or two to preprocess the image before feeding it into the controller?", "link": "https://www.reddit.com/r/MachineLearning/comments/83yiyk/d_mann_paper_why_no_convolutions/"}, {"autor": "NoClaim", "date": "2018-09-10 20:08:45", "content": "This is one of two posts, which I thought were different enough to warrant separate postings.\n\n&amp;#x200B;\n\n\\&amp;nbsp;\n\n&amp;#x200B;\n\nI read a paper last year that suggested that most feed-forward neural networks were susceptible to providing confidence estimates which were systematically higher than the true probability (e.g., a NN might report that it is 87% sure there was a dog in a -----> photo !!! , but would be right only 69% of the time). Is suggested that approximate Bayesian approaches reduced the difference, but did not eliminate the disparity. Unfortunately, I can't find it any more, and was hoping to brush up on the current state of ML/AI with respect to confidence estimates.\n\n&amp;#x200B;\n\n\\&amp;nbsp;\n\n&amp;#x200B;\n\nDo any of you have some papers you could recommend to read on this subject?\n\n&amp;#x200B;\n\n\\&amp;nbsp;\n\n&amp;#x200B;\n\nWhat are your thoughts on the reliability of confidence estimates?\n\n&amp;#x200B;\n\n\\&amp;nbsp;\n\n&amp;#x200B;\n\nWhat are your preferred ways to determine accuracy of confidence estimates?\n\n&amp;#x200B;\n\n\\&amp;nbsp;\n\n&amp;#x200B;\n\nWhat are some good topics and materials (reading or other) to explore with respect to the probabilities that an ML/AI system gives the \"right\" answer, or takes the \"right\" action?", "link": "https://www.reddit.com/r/MachineLearning/comments/9eqlw3/d_research_covering_confidence_estimates_and/"}, {"autor": "popsumbong", "date": "2018-06-28 03:37:37", "content": "Hey. I'm trying to download the -----> image !!! net-1k but it looks their site is down and according to the source I read I need an -----> image !!! -net account to download the dataset. \n\nIs there an alternative place I could download the dataset from?", "link": "https://www.reddit.com/r/MachineLearning/comments/8ug1yh/image_net_site_down/"}, {"autor": "nyxeka", "date": "2018-06-27 20:53:59", "content": "Let's say I wanted speech synthesis, so I generate it using some cheap text-to-speech software.\n\nThen I create a model (Model A) that can tell the difference between human speech and this cheap text-to-speech software 95% of the time.\n\nThen I create another model, say Model B, that takes the cheap, synthesized voice and makes it sound human. It does this by using Model A to train - it changes a bit every training step until it gets a \"RESULT: HUMAN\" (1 instead of 0) from Model A every time it outputs something.\n\nThen, say we do another model(C) that takes that(B) as input, and compares it to _real_ human speech again, and tells us which is which with high accuracy (if it's still possible?). Then we do another model (D) that takes model B as input, and trains using Model C until Model C also says it always sounds human.\n\nMaybe we could do this a few more times, or have other models that are specifically designed to take human speech output from B or D and make it sound happy, sad, excited, etc... using the exact same method (would imagine this would be a spectrum (0-10), like a rainbow, one colour for each emotion, and the spectrum being an input to the final layer, maybe have some more spectrums for things like angry-happy, old, tired, young, male, female, etc...\n\nI've thought up something to sort of test this using the mnist-tensorflow tutorial - it would take a 10-length array and output a 32x32 -----> image !!!  and use the mnist digit analyzing model to train itself. Eventually would add sliders as an input to test \"hastily-written\" vs \"slowly written\" and \"hard to read\" vs \"easy to read\", and stuff like that, but would be a pain in the ass to sort all that without a bunch of help.\n\n**TL,DR**: possible to train a neural network to notice the difference between human and robot output, and then use that to train another neural network until it generates output that the first NN always says is human?\n\nThoughts?", "link": "https://www.reddit.com/r/MachineLearning/comments/8ud9of/discussion_possible_to_train_a_neural_network/"}, {"autor": "anonDogeLover", "date": "2018-06-26 23:57:03", "content": "I'm looking for work that jointly learns the best -----> image !!!  patch(s) to look at in an -----> image !!!  and how to classify the -----> image !!!  based on that patch(s), excluding the non-patch regions. Is there anything like that?", "link": "https://www.reddit.com/r/MachineLearning/comments/8u4ygd/d_classifying_objects_using_the_best_image_patch/"}, {"autor": "largecontainer95", "date": "2018-06-26 14:56:28", "content": "Hi, I have two categorical data sets that I wish to compare. The data set consists of a probability reading vs. a letter reading. This -----> image !!!  illustrates it with the red and green lines: https://imgur.com/a/Sf2OjOa\n\nWhat suggestions do you have to compare the two data sets - the area overlap perhaps? Any other ideas? Thanks.", "link": "https://www.reddit.com/r/MachineLearning/comments/8u0nws/categorical_data_comparison/"}, {"autor": "lifeadvicesponge", "date": "2018-08-03 15:08:07", "content": "Hi r/machinelearning   \n\nI am exploring applications of I-2-I translation for inter-modal conversion (Thermal to RGB etc) and I was wondering what is the state of the art for such networks. Because I am getting nowhere near the same performance on Thermal-&gt;RGB images as compared to RGB images and the translations are also not very good. I was using [CycleGAN](https://arxiv.org/pdf/1703.10593.pdf) for my project as I was trying to work in an unsupervised manner. I will next be trying [Pix2Pix](https://arxiv.org/abs/1611.07004) to see how good I can do with paired images.     \n\nI came across this paper from NVIDIA [Unsupervised -----> Image !!! -to------> Image !!!  Translation Networks](https://arxiv.org/pdf/1703.00848.pdf) and they have only reported results for digit classification which is a rather non-complex vision problem and does not come close to the complexity of something like person recognition. I was wondering if someone can help me out regarding the state-of-the-art in this field or give me pointers on how to do such cross-modal image translations. I am working with images of people and would like the translations to preserve the identity of the person.   ", "link": "https://www.reddit.com/r/MachineLearning/comments/94ad5e/question_regarding_imagetoimage_translation/"}, {"autor": "parekhnish", "date": "2018-06-08 22:54:21", "content": "Apple recently announced a new app called [Measure](https://venturebeat.com/2018/06/08/apples-ios-12-measure-app-is-good-enough-ai-assisted-ar-really-good-enough/), which measures dimensions of the object shown in the -----> camera !!!  view. This led me down a rabbit\\-hole where I discovered quite a few apps that did similar things.\n\nBut this seems to be an ill\\-posed problem; depth cannot be measured using a single image! If the domain is pre\\-defined, then depth estimation is possible and quite accurate (for example, [this project](http://cvl-demos.cs.nott.ac.uk/vrn/) allows 3D reconstruction of a face, since it is trained on facial image and model data). But for measuring the length of any arbitrary line in 3D space, a single image should not suffice!\n\nSo my inquiry is, how do these AR\\-assisted measurement apps work under the hood?", "link": "https://www.reddit.com/r/MachineLearning/comments/8poe50/d_how_does_arassisted_measurement_work/"}, {"autor": "1o0ko", "date": "2018-06-08 14:18:09", "content": "https://i.redd.it/ihv9fphhcs211.jpg\n\n[https://fashion\\-gen.com/](https://fashion-gen.com/)  \n\n\nChallenge your knowledge of -----> image !!!  generation by creating a model that can disrupt the fashion industry. Your design will be evaluated by both human and machine.\n\nThe winner with the best model will be given a **$2,000 CAD** prize.", "link": "https://www.reddit.com/r/MachineLearning/comments/8pk8k9/n_new_dataset_for_generative_models_and_challenge/"}, {"autor": "ocelot134", "date": "2018-06-08 09:57:10", "content": "Hi! This is my first post on here, so please forgive me if i have the improper tag.. it would either be D or R\n\nI am very keen to learn more about deep learning so that I could apply it towards the editing of videos.\n\ni have been self-learning Python online in my free time, by trying to read \"automate the boring stuff\" and trying to following Youtube tutorials on fundamentals. I understand and follow along with the concept and big -----> picture !!!  stuff. But by the end of the project, I feel I was just taken for the ride. I want to have projects of my own to show for it. \n\nIf possible, i really want to work on something that is bigger than myself and be able to contribute my knowledge about video editing theory, even if its not on the programming side since im from film . I could test or something. It would be so good to have a mentor to guide me. I don't have a formal CS background, but I do have a background in film production\n\nI think Id like to learn about computer vision, automation, and streamlining with Python so that I can be creating or helping to create an application or script for content creators or for things that VFX artists might use that are in moviemaking. \n\nI'd really love to speak to someone in that industry, or if I could get very specific directions on a roadmap to get me up to speed. I need to take to become competent at making as I have been kind of overwhelmed with all of the information on the internet. Thank you for your time!\n", "link": "https://www.reddit.com/r/MachineLearning/comments/8pin69/d_deep_learning_for_video_editing/"}, {"autor": "omayrakhtar", "date": "2018-06-08 09:47:47", "content": "Hi - A few weeks ago I posted on reddit regarding suggestions on the same topic: https://www.reddit.com/r/MachineLearning/comments/8axh44/d_need_suggestions_on_creative_ways_to_approach/\nThis is the topic of my thesis. And I have conducted a great number of experiment including traditional -----> image !!!  processing and machine learning, pre-trained deep networks, 2D CNN, 3D CNN, tried data augmentation, blending data modalities to enrich the data and etc, but none of these experiments yielded any significant results, primarily because of the scarcity of data and this being a multi-class classification problem. I have put in a lot of efforts into this but the results aren't very impressive, however, I am aiming to publish a paper in the following conference: http://www.tut.fi/euvip2018/index.html but I am not confident what could be an interesting approach to talk explain in the paper. A few titles that came to my mind were:\n* Inefficacy of deeper networks on smaller datasets. (No dense layers with dense layers)\n* Traditional machine learning methods performing as well as deep learning.\n* Effects of data augmentation on learning of deep models. \n\nPlease let me know if you need more information to make suggestions. ", "link": "https://www.reddit.com/r/MachineLearning/comments/8pilo0/r_need_ideas_to_write_a_meaningful_research_paper/"}, {"autor": "gabegabe6", "date": "2018-12-08 20:37:24", "content": "Dear all,\nI was wondering if you know about any research on this kind of invariance (if we can call it that). I think this could be really important when we collect images from different sources, or when we fit a model to a certain type then in real world problems we need to use a different -----> camera !!!  type.\nI would love to hear your thoughts on this subject and also any paper on the subject is welcome!", "link": "https://www.reddit.com/r/MachineLearning/comments/a4eb7x/d_cameralens_type_invariance_for_image/"}, {"autor": "7OceansMan", "date": "2018-12-08 19:09:26", "content": "I found plenty of companies marketing that they use \"AI\" to create the best advertising campaigns yet they only mention using machine learning to post and target, never to actually evaluate click rate of particular banner ad on a specific customer group.\n\nFor example: a banner ad for fb with -----> image !!!  of current employees smiling and holding a paper saying \"join us\" targeted at group of people between 20-30 years who are interested in customer support - and now this program would evaluate what the click rate is going to be for this specific post for this specific target group.\n\nMy idea to solve this issue would be to identify the **visual variables** and to look for them in any ad:\n\n\\- Color (what is most used, how many different once are used,  etc.)\n\n\\- What things/people/animals are in the image (ice creams, cats, Spiderman, etc.)\n\n\\- Layout (what space is used for each of those things)\n\n\\- Texts (what does it say, is it funny, serious, etc.)\n\n\\- what else?\n\nThen I would create samples of ads with different visual variables and present them to the target audience (20-30yo) with interest in HR and find out which ads would do the best (the click rate).\n\nThen train the program on those findings so that any imputed ad could be evaluated with click rate as an outcome.\n\nLastly evaluate the results on a control group by presenting them the ads as with group 1 but also another ad, which would be evaluated by the program as similar in click rate to test the effectiveness of the program.\n\nNow thanks for reading all of this. What other variables do you think would be necessary, would there be other variables related to the target audience (like education, style of humour...) and would this in general work, or in other words, can be a funny creative image broken down into visual variables as the whole is more than sum of its parts?\n\nThx\n\nP.S. Facebook always suggests the probable click rate when ad is submitted so if anyone knows how that works I would love to know.", "link": "https://www.reddit.com/r/MachineLearning/comments/a4dieb/d_evaluate_success_of_banner_ads_what_approach/"}, {"autor": "757cbdb0b61385577130", "date": "2018-02-22 09:58:52", "content": "Title: A mixed-scale dense convolutional neural network for -----> image !!!  analysis\n\nJournal: PNAS 2018 January, 115 (2) 254-259\n\nDOI: 10.1073/pnas.1715832114\n\nPNAS link: http://www.pnas.org/content/115/2/254\n\nPublicly accessible PDF: https://slidecam-camera.lbl.gov/static/asset/PNAS.pdf\n\n\nAbstract:\n\nDeep convolutional neural networks have been successfully applied to many -----> image !!! -processing problems in recent works. Popular network architectures often add additional operations and connections to the standard architecture to enable training deeper networks. To achieve accurate results in practice, a large number of trainable parameters are often required. Here, we introduce a network architecture based on using dilated convolutions to capture features at different image scales and densely connecting all feature maps with each other. The resulting architecture is able to achieve accurate results with relatively few parameters and consists of a single set of operations, making it easier to implement, train, and apply in practice, and automatically adapts to different problems. We compare results of the proposed network architecture with popular existing architectures for several segmentation problems, showing that the proposed architecture is able to achieve accurate results with fewer parameters, with a reduced risk of overfitting the training data.\n", "link": "https://www.reddit.com/r/MachineLearning/comments/7zdtug/r_a_mixedscale_dense_convolutional_neural_network/"}, {"autor": "dankmaster2k", "date": "2018-02-02 13:48:50", "content": "Hi, I was trying to google this but in my brief attempt could not find anything related to what I'm interested in.\nMost of the -----> image !!!  generation methods I've seen look to be using pixels as i/o. Why is there not much focus on vector formats for images, such as svg or something? I understand you'd lose the CNN and the great work done there, but could vectorised images be much easier to train for some datasets, like digits or so on?\nThanks guys :)", "link": "https://www.reddit.com/r/MachineLearning/comments/7urean/d_hypergan_with_vector_images/"}, {"autor": "Xlagor", "date": "2018-02-02 13:08:46", "content": "Hey Guys, I thought about the process and the science behind an -----> image !!!  recognizing Neural Network. So I asked my self could it be possible to instead of training the N.N. on pictures of Objects train it on \"pictures of the brain\" or short clips of the brain activity(?).\n\nFor example you would show a bunch of persons a picture of a dog (or just tell them to think of one) and at the same time make a clip of their brain acitvities. You then train the N.N. on the data and technically it should than be able to identify if someone is thinking of a dog.\n\nLet me now in the comments if this makes sense or if its total bullshit. But I personally feel like this could work when seeing how good N.N. nowadays are performing in pattern/object recognization.\n\n(I apologize for my bad english skills)", "link": "https://www.reddit.com/r/MachineLearning/comments/7ur637/d_image_recognition_with_images_from_brain/"}, {"autor": "cranthir_", "date": "2018-03-31 06:52:05", "content": "Hello, I'm currently writing a series of free articles about Deep Reinforcement Learning, where we'll learn the main algorithms (from Q* learning to PPO), and how to implement them in Tensorflow.\n\n**The Syllabus**: https://simoninithomas.github.io/Deep_reinforcement_learning_Course/\n\n The first article is published, each week 2 articles will be published, but **if you want to be alerted about the next article, follow me on Medium and/or follow the github repo below**\n\nI wrote these articles because I wanted to have articles that begin with the big -----> picture !!!  (understand the concept in simpler terms), then the mathematical implementation and finally a Tensorflow implementation **explained step by step** (each part of the code is commented). And too much articles missed the implementation part or just give the code without any comments.\n\nLet me see what you think! What architectures you want and any feedback.\n\n**The first article**: https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419\n\n**The first notebook**: https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/Q%20Learning%20with%20FrozenLake.ipynb\n\nThanks!\n", "link": "https://www.reddit.com/r/MachineLearning/comments/88h0g4/p_deep_reinforcement_learning_free_course/"}, {"autor": "iliauk", "date": "2018-03-29 14:04:25", "content": "Could someone help me understand why when dealing with a one-class object detection (where probability of foreground = probability of class) going beyond the region proposal network (to the ROIPool or I guess now the ROIAlign layer) helps improve accuracy of the predicted bounding box?\n\nThe way I see it is: RPN has access to the whole feature-map (e.g. input_size/32px for ResNet50) and makes a prediction for anchor offsets and probability of object (for those anchor offsets). ROIPool would then do a one-level spatial pyramid pool on the pixels inside that predicted anchor-offset and feed into classifier. If our predicted bounding-box is e.g. 2x2 then the receptive field for the classification after ROIPool will have a receptive field of 64x64 pixels in the original -----> image !!!  ... however the RPN already has access to the full image already?\n\nOr is it just a case of extra parameters - stacking similar classifiers on each other?\n\nFurther, if ROIPool helps with single-class, then ROIAlign is expected to improve the performance also?", "link": "https://www.reddit.com/r/MachineLearning/comments/881iay/d_fastrcnn_vs_rpn_for_1class/"}, {"autor": "rektourRick", "date": "2018-01-15 22:10:59", "content": "I had an interview question last week that asked the interviewee to architect a system for a disaster response agency. The primary requirement was a system of fully autonomous underwater drones which could map the shoreline and inland waterways of a post tsunami AO.\n\nMy assumptions for the operating environment ran like this.\n\n1.High concentration of varied debris/flotsam extending a 1/2 mile from the shoreline and increasing in severity as the drone moves inland\n\n2.Wide variety of physical, electrical, and chemical hazards\n\n3.Turbulent waters\n\nFor a fully autonomous solution, a drone maneuvering on its own using a ML/DL algorithm seems like the only answer. However, the algorithm would have to be extremely sophisticated. The drone is operating in a 3d environment with debris moving in potentially different directions on all axis relative to it. The drone itself is also moving, and is being influenced by currents in the water. The drone may have imperfect information about the hazards around it. My solution proposed multiple multibeam sonar modules stationed around the drone to give it a rough 360 degree -----> picture !!!  of its environment, but debris/obstacles could obfuscate other hazards (i.e. a large piece of cardboard concealing a rebar pipe).\n\nThe drone would have to be able to identify a massive range of obstacles, including electrical or chemical hazards. It may also have to make decisions about which hazards are more dangerous, a blob of plastic may do more damage than a steel pipe. Beyond maneuvering, the drone would have to make many different operational decisions (what path to take, when to surface, when to return for repairs/recharging ect).\n\nSo while a ML/DL solution seemed like what they were looking for, does a system actually exist today that tackles a problem of comparable complexity? \n", "link": "https://www.reddit.com/r/MachineLearning/comments/7qne7n/project_underwater_drone_maneuvering_in_a/"}, {"autor": "Richard_wth", "date": "2018-12-27 03:29:34", "content": "**TL;DR**: Rearranging the terms in maximum mean discrepancy yields a much better loss function for the discriminator of generative adversarial nets\n\n**Keywords**:\n\nSpectral normalization, repulsive loss, bounded RBF kernel\n\n**Abstract**:\n\nGenerative adversarial nets (GANs) are widely used to learn the data sampling process and their performance may heavily depend on the loss functions, given a limited computational budget. This study revisits MMD-GAN that uses the maximum mean discrepancy (MMD) as the loss function for GAN and makes two contributions. First, we argue that the existing MMD loss function may discourage the learning of fine details in data as it attempts to contract the discriminator outputs of real data. To address this issue, we propose a repulsive loss function to actively learn the difference among the real data by simply rearranging the terms in MMD. Second, inspired by the hinge loss, we propose a bounded Gaussian kernel to stabilize the training of MMD-GAN with the repulsive loss function. The proposed methods are applied to the unsupervised -----> image !!!  generation tasks on CIFAR-10, STL-10, CelebA, and LSUN bedroom datasets. Results show that the repulsive loss function significantly improves over the MMD loss at no additional computational cost and outperforms other representative loss functions. The proposed methods achieve an FID score of 16.21 on the CIFAR-10 dataset using a single DCGAN network and spectral normalization.\n\n**Links**\n\nOpenReview link: [https://openreview.net/forum?id=HygjqjR9Km](https://openreview.net/forum?id=HygjqjR9Km)\n\narXiv link: [https://arxiv.org/abs/1812.09916](https://arxiv.org/abs/1812.09916)\n\nCode link: [https://github.com/richardwth/MMD-GAN](https://github.com/richardwth/MMD-GAN)\n\n\\---------------------------------------------\n\nGood day, mate!\n\nI am the first author of this GAN paper that proposes a new loss function, a slightly modified spectral normalization method, and a bounded RBF kernel. The methods works very well with DCGAN architecture. \n\nPlease take a few minutes to read the paper. Any comments are welcomed and I am happy to answer any questions in this extremely hot Christmas (yes I am in Australia and we stand upside down). ", "link": "https://www.reddit.com/r/MachineLearning/comments/a9vh36/r_iclr_poster_improving_mmdgan_training_with/"}, {"autor": "tldrtldreverything", "date": "2018-12-26 17:44:07", "content": "Hey, I published a summary of a new paper from FAIR called SlowFast. The paper details a cool technique which allows to understand what's happening in -----> image !!! s by using two neural networks - a 'slow' CNN that recognizes the fine content of the -----> image !!!  and a 'fast' CNN that recognizes swift changes in the content. Full summary here: https://lyrn.ai/2018/12/21/slowfast-dual-mode-cnn-for-video-understanding/", "link": "https://www.reddit.com/r/MachineLearning/comments/a9qkns/r_slowfast_dualmode_cnn_for_video_understanding/"}, {"autor": "whria78", "date": "2018-12-25 15:29:20", "content": "Hello, I am Han Seung Seog from I Dermatology clinic.\n\n&amp;#x200B;\n\nThe automatic screening system consists of 1) **blob-detector**, 2) **fine------> image !!! -selector**, and  3) **malignancy-classifier**.\n\n&amp;#x200B;\n\n1. The blob-detector was trained with py-faster-RCNN (model = VGG-16) and approximately 25,000 nodular disorder images ([https://www.ncbi.nlm.nih.gov/pubmed/29428356](https://www.ncbi.nlm.nih.gov/pubmed/29428356)) + 100,000 ImageNet images.\n2. The fine-image-selector was trained with NVCaffe (model = SE-ResNext-50, SE-ResNet-50) and over 500,000 images (clinical images - 400,000; ImageNet images - 100,000)\n3. The malignancy-classifier (Model Dermatology; version 20181225; The old (20180623) version of model dermatology is currently available at [http://modelderm.com](http://modelderm.com)) was trained with NVCaffe (model = SENet, SE-ResNet-50) and over 230,000 images (220,000 - clinical diagnosis; over 10,000 - generated by RCNN technology and the diagnosis had been tagged by dermatologist from image findings).\n\n&amp;#x200B;\n\n**Screenshot :** [**https://i.redd.it/0d1ittotwf621.jpg**](https://i.redd.it/0d1ittotwf621.jpg)\n\n&amp;#x200B;\n\nThe screenshot was created by testing our algorithm with 8 internet images which was downloaded  via search engine (image.google.com) .\n\n1. before\n2. after blob-detector\n3. after fine-image-selector\n4. after malignancy classifier\n\n&amp;#x200B;\n\nAlthough the AI had been shown dermatologist-level performance in several studies, the most difficult problem to be solved in skin cancer screening is false positive problem. We are working to reduce the false positive by generating a lot of skin blobs images and training the algorithm with those images.\n\n&amp;#x200B;\n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/a9fmzq/p_automatic_skin_cancer_screening_with/"}, {"autor": "ljvmiranda", "date": "2018-12-25 08:31:48", "content": "Hi all, \n\nFor the holidays, we made a holiday card generator to give to our workmates and friends, thought I might share this here too! \n\nhttps://stories.thinkingmachin.es/ai-art-holiday-cards/\n\nGiven an input string, it looks for the nearest Quick, Draw! class and draws it using Sketch-RNN, then we applied style transfer to the output -----> image !!! .\n\nWe also open-sourced our pipeline here:\n\nhttps://github.com/thinkingmachines/christmAIs\n\nFeel free to check that out!\n\nHappy Holidays!", "link": "https://www.reddit.com/r/MachineLearning/comments/a9ddu8/p_holiday_card_generator_using_sketchrnn_and/"}, {"autor": "TheShadow29", "date": "2018-12-25 00:35:11", "content": "I am looking for papers in the field of forgery detection. Like if a part of the -----> image !!!  is replaced by a GAN generated input then how to detect that part? Or if the whole image is generated by a GAN how to classify it as a synthetic image?\n\nThe papers I have found are: \n1. Buster Net for copy move forge detection: http://openaccess.thecvf.com/content_ECCV_2018/papers/Rex_Yue_Wu_BusterNet_Detecting_Copy-Move_ECCV_2018_paper.pdf\n\n2. Face Tamper Detection: https://arxiv.org/pdf/1803.11276.pdf\n\nWanted to know if anyone else is aware of any other papers.", "link": "https://www.reddit.com/r/MachineLearning/comments/a9aes4/d_defenses_against_gan_generated_images_for_image/"}, {"autor": "ajaysub110", "date": "2018-12-24 17:26:19", "content": "I (along with a partner) recently started off with a machine learning (more deep learning) project on medical -----> image !!!  detection and classification. We decided to target the classification problem first. After the initial data exploration, we went through some papers that discussed work on very similar research problems. After getting an idea of the current progress, benchmarks and model architectures currently being used in the field, we decided to run a few pretrained networks on the data just to obtain a starting point for our further work.\nSo, since it is an image classification problem, we ran the data through some common CNN based networks such as Alexnet, inception v3 and densenet, of which densenet produced relatively good results.\nAfter obtaining these results, were in a dilemma as to how to proceed with improving our results. Would be great to receive some advice from someone who has some experience in working on such projects.\nThanks in advance ", "link": "https://www.reddit.com/r/MachineLearning/comments/a96uoy/coming_up_with_a_neural_network_design_for_a/"}, {"autor": "callmebyyoursurname", "date": "2018-05-23 22:17:58", "content": "I just tried out the Simplified Whitening as mentioned in this [post](http://yeephycho.github.io/2016/08/03/Normalizations-in-neural-networks/). But the normalized -----> image !!!  I get seems to have lost a LOT of information. It's barely visible at all. Here's the [processed image](https://i.imgur.com/tDy8tEg.png) and here's the [original image](https://i.imgur.com/9zH0Bnm.png).  Here's [the code](https://github.com/dibyadas/Visualize-Normalizations/blob/master/SimplifiedWhitening.ipynb) that I wrote for this. Any help in understanding this would be appreciated! ", "link": "https://www.reddit.com/r/MachineLearning/comments/8lnhhi/d_visualizing_simplified_whitening/"}, {"autor": "BenRayfield", "date": "2018-05-22 21:10:49", "content": "Example: https://corpocrat.com/wp-content/uploads/2014/10/mnistrbm.png are featureVectors trained on visual data.\n\nA common visual data is mnist ocr (nonconvolutionally), but it could be any kind of data.\n\nMy attempt at answering this question is, for each node in second node layer, viewing edges downward to first node layer, there exists some 2 subsets (to first node layer) where 1 subset has very low stdDev (feature ignores those visibleNodes) and the other subset has very high stdDev (feature is strongly influenced by those visibleNodes positively or negatively). It may also be related to the dotProduct between such featureVectors, especially that they tend not to duplicate or be opposite of, large parts of eachothers non-middle values. Each lowest float[][] featureVector should describe a possibility some small part of the forExample 28x28 2d grid of visibleNodes in an mnist ocr -----> image !!! . Having written my own RBM code, I find that without careful tuning, the featureVectors tend to become very similar to eachother.", "link": "https://www.reddit.com/r/MachineLearning/comments/8ldnj4/d_given_the_lowest_float_layer_of_a_rbm_and/"}, {"autor": "trenteady", "date": "2018-05-06 02:35:04", "content": "I\u2019m a bit flummoxed by a recent discovery. The AI/robotics startup Vicarious has developed a new neural network architecture they call a Recursive Cortical Network (RCN). Vicarious used its RCN to solve CAPTCHAs with the same accuracy as a Google DeepMind convolutional neural network. Here\u2019s the kicker: **the RCN was trained on only 260 examples, versus 2.3 *million* for the ConvNet.** So that\u2019s a ~900,000% improvement in training data efficiency.\n\nYou can read about the RCN solving CAPTCHAs in Vicarious\u2019 [blog post](https://www.vicarious.com/2017/10/26/common-sense-cortex-and-captcha/) on the matter, or you can read their paper in the journal [Science](http://science.sciencemag.org/content/358/6368/eaag2612.full?ijkey=DmvGldXIEXVoQ&amp;keytype=ref&amp;siteid=sci), if you have access. Vicarious also has a reference implementation of its RCN up on [GitHub](http://github.com/vicariousinc/science_rcn).\n\nSo, the RCN has achieved state-of-the-art accuracy on optical character recognition with ~900,000% better training data efficiency. **Here\u2019s my question: has anyone tried to adapt Vicarious\u2019 reference implementation for 2D -----> image !!!  classification or, most exciting of all, 3D computer vision?**\n\nI\u2019m a lay enthusiast and CS 101 dropout, not a computer scientist or software engineer. So I don\u2019t have the ability to try this myself, or even the knowledge to say whether it would feasible to try. So apologies if this is a misconceived question.\n\nBut if I have not exceeded my depth here, this seems like such an exciting experiment. If the RCN can match the accuracy of state-of-the-art ConvNets not just on character recognition, but on object detection in a 3D environment, and do so after being trained on ~0.011% as many examples, imagine the possibilities. ", "link": "https://www.reddit.com/r/MachineLearning/comments/8hcbas/d_question_has_anyone_tried_to_use_vicarious/"}, {"autor": "kamperh", "date": "2018-05-05 10:23:46", "content": "When setting up the categorical VAE in the (great) tutorial https://blog.evjang.com/2016/11/tutorial-categorical-variational.html, there are two parameters specified.  The first is K, the number of classes, which I understand.  Then there is N, the number of categorical distributions.  I am struggling to see what N is?\n\nAt a first reading, I thought that if you are categorising MNIST, then you will simply encode each -----> image !!!  as a K=10 dimensional one-hot vector.  But, I think that what is actually happening is that each image is encoded as N separate K-dimensional one-hot vectors.  Is this right?", "link": "https://www.reddit.com/r/MachineLearning/comments/8h6p73/d_question_number_of_distributions_in_categorical/"}, {"autor": "saadmrb", "date": "2018-05-05 00:29:06", "content": "Here are some of known ML algorithms\n\nSnapchat Filters:\u00a0Convolutional Neural Networks (allegedly)\n\nNetflix\u00a0Recommendation System:\u00a0Restricted Boltzmann Machines\n\nGoogle Translate\u00a0(Machine Translation):\u00a0Recurrent Neural Networks\n\nSiri\u00a0(Personal Assistants):\u00a0Hidden Markov Models (2011-2014), Long Short Term Memory Networks (2014+), which is a type of Recurrent Neural Networks.\n\nSelf-driving cars\u00a0(-----> Image !!!  recognition, Video recognition):\u00a0Convolutional Neural Networks (among other things)\n\nSpeech recognition, Hand-writing recognition:\u00a0Recurrent Neural Networks\n\nMarket Segmentation:\u00a0k-means clustering\n\nGoogle AlphaGo:\u00a0Convolutional Neural ", "link": "https://www.reddit.com/r/MachineLearning/comments/8h3xt8/d_what_types_of_machine_learning_algorithms_are/"}, {"autor": "imrich-", "date": "2018-05-04 12:44:47", "content": "Want to build real\\-life adblocker, see [previous reddit post](https://www.reddit.com/r/MachineLearning/comments/88l32o/p_anyone_working_on_reallife_ad_blocking_neural/).\n\nOne of the main problems is having the labeled data in order to train the model. Want to crowdsource it \\- create a simple app where people can \\(1\\) take a -----> picture !!! , \\(2\\) label the ad and \\(3\\) submit to a database. This way, we will have a real\\-life adblock when AR glasses are a thing wohoo :\\)\n\nTrying to figure out the labelling part. One user in DM mentioned, that there was a paper with precise selection of objects where only 4 extreme keypoints \\- left, right, upper and lower \\- were annotated and the rest was guessed by a model. **Anyone knows this paper or can give some info on how to find it?**\n\nSince most ads are rectangular, I am thinking it would be enough for the user to tap 4 times \\(corners\\) on the image and the area enclosed will be the labeled ad.\n\nI am a complete *noob* in labelling data for training an ML model, so any help is very much appreciated. **What software/algorithms researchers use to label the pictures? How does it work?**\n\nthanks a lot :\\)", "link": "https://www.reddit.com/r/MachineLearning/comments/8gyybm/p_looking_for_softwareexample_code_for_labelling/"}, {"autor": "omniron", "date": "2018-10-15 21:58:24", "content": "the demo had clustered -----> image !!! s of vehicles, and you would label the vehicles as facing either to the left or right, and once you labeled the -----> image !!! , the cluster would readjust in real time based on your label.\n\nThe you could keep refining the clusters and it would become more accurate\n\nAnyone know what I\u2019m talking about?", "link": "https://www.reddit.com/r/MachineLearning/comments/9ohaqk/looking_for_a_demo_of_online_learning_that_ive/"}, {"autor": "tangbinh", "date": "2018-10-15 04:01:44", "content": "I've been implementing some of the earlier approaches to -----> image !!!  captioning, include the famous paper [Show, Attend, and Tell](https://arxiv.org/abs/1502.03044). Notable features include beam search based on [fairseq](https://github.com/pytorch/fairseq) and faster training with easy extension to multi-GPU training. Feel free to use the code if you like. Here's a link to the GitHub repository https://github.com/tangbinh/image-captioning.", "link": "https://www.reddit.com/r/MachineLearning/comments/9o9dec/project_pytorch_implementation_of_show_attend_and/"}, {"autor": "knowedgelimited", "date": "2018-10-14 23:16:22", "content": "In the paper \n   Chen, Kingma et al. Variational Lossy Encoder\nit discusses about the possibility of VAE ignoring the latent code.\n\nOn p.4 it says this: \n\n&gt;   \"one common way to encourage putting information into the code is to use a factorized decoder p(x|z) = \\prod_i p(x_i|z)\"\n\nwhere \"putting information into the code\" meaning into the latent, z.\n\n[screenshot](https://imgur.com/a/J79sEPR)\n\nMy question: can anyone explain this:  Why does using a factorized decoder encourage the latent to be used?\n\nIn their notation, I believe x_i are individual dimensions of the output, such as individual pixels of an -----> image !!! .\n\n", "link": "https://www.reddit.com/r/MachineLearning/comments/9o7dih/d_question_about_variational_lossy_autoencoder/"}, {"autor": "nepherhotep", "date": "2018-09-27 20:08:59", "content": "I'm happy to share some research related to unwrapping wine labels. If you implement wine searching by a label -----> picture !!! , the accuracy of images matching and OCR goes down due to cylinder distortion of the labels. It's described in the article how to undo the distortion and  improve product recognition quality:\n\n&amp;#x200B;\n\nhttps://i.redd.it/izpz7xco8uo11.png\n\n[https://medium.com/@nepherhotep/how-to-unwrap-wine-labels-programmatically-31c8c62b30ce](https://medium.com/@nepherhotep/how-to-unwrap-wine-labels-programmatically-31c8c62b30ce)", "link": "https://www.reddit.com/r/MachineLearning/comments/9jg8ka/p_how_to_unwrap_wine_labels_using_opencv/"}, {"autor": "MemeBox", "date": "2018-09-27 12:15:10", "content": "[https://github.com/NVIDIAGameWorks/Falcor/tree/master/Samples/Core/LearningWithEmbeddedPython](https://github.com/NVIDIAGameWorks/Falcor/tree/master/Samples/Core/LearningWithEmbeddedPython)\n\n&amp;#x200B;\n\n    // Send our light direction to Python\n        DirectionalLight *dirLight = (DirectionalLight *)(mpScene-&gt;getScene()-&gt;getLight(0).get());\n        float dirData[3] = { dirLight-&gt;getData().dirW.x, dirLight-&gt;getData().dirW.y, dirLight-&gt;getData().dirW.z };\n        mGlobals[\"inferLight\"] = py::array_t&lt;float&gt;( { 3 }, { 4 }, dirData );\n        \n        // Predict the -----> image !!!  given the above light direction\n        mLastInferenceTime = executeStringAndSetFlags(mPythonInfer);\n        if (mLastInferenceTime &gt;= 0.0f)\n        {\n            // Readback the data.  Cast our Python output to an uchar array\n            auto arr = mGlobals[\"infResult\"].cast&lt; py::array_t&lt;unsigned char&gt; &gt;();\n            py::buffer_info arr_info = arr.request();\n    \n            // Upload the Python uchar array into our texture (so we can render the result)\n            gpDevice-&gt;getRenderContext()-&gt;updateTextureSubresource(mPythonReturnTexture.get(), mPythonReturnTexture-&gt;getSubresourceIndex(0, 0), arr_info.ptr);\n        }\n    \n        mDoInference = false;\n\nThe new NVIDIA graphics cards do raytracing, but it looks like this attempts to use an NN to just infer the lighting. I wonder how succesful it is?", "link": "https://www.reddit.com/r/MachineLearning/comments/9jccca/d_nvidia_github_code_using_tensorflow_for_graphics/"}, {"autor": "alexparinov", "date": "2018-09-26 20:37:43", "content": "[Alexander Buslaev](https://www.linkedin.com/in/al-buslaev/), [Vladimir Iglovikov](https://www.linkedin.com/in/iglovikov/), [Evegene Khvedchenya](https://www.linkedin.com/in/cvtalks/) and [I](https://www.linkedin.com/in/alex-parinov/) have been working for some time on an -----> image !!!  augmentation library that we want to tell you about. It is called albumentations and it is available at [https://github.com/albu/albumentations](https://github.com/albu/albumentations). It supports Python 2.7 and Python 3, you can install the library by running `pip install albumentations`\n\nThe library supports a wide variety of transformations for classification, segmentation and detection problems. Albumentations can work with non-8-bit images (for example with 16-bit tiff images that are often used in satellite imagery).\n\nCode from the library was used to get top results in many competitions at Kaggle, Topcoder and CVPR. You can read about the competitions in [Vladimir's post on Kaggle](https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66643).\n\nWe made some example notebooks to show how to use the library:\n\n* [How to migrate from torchvision to albumentations.](https://github.com/albu/albumentations/blob/master/notebooks/migrating_from_torchvision_to_albumentations.ipynb)\n* [How to apply the transformation to the **classification** problems.](https://github.com/albu/albumentations/blob/master/notebooks/example.ipynb)\n* [How to apply transformations to the **detection** problems.](https://github.com/albu/albumentations/blob/master/notebooks/example_bboxes.ipynb)\n* [How to apply transformations to the **segmentation** problems.](https://github.com/albu/albumentations/blob/master/notebooks/example_kaggle_salt.ipynb)\n* [How to apply transformations to the **non 8-bit** images](https://github.com/albu/albumentations/blob/master/notebooks/example_16_bit_tiff.ipynb)\n* [All in one showcase](https://github.com/albu/albumentations/blob/master/notebooks/showcase.ipynb)\n\nSome examples:\n\nhttps://i.redd.it/6fd0bwg17no11.jpg\n\n*Processing img qt5e6qv27no11...*\n\nhttps://i.redd.it/bvqrwyr47no11.jpg\n\nhttps://i.redd.it/lc9ujjn57no11.jpg\n\nWe are trying to make the library as fast as possible, here are some [benchmarking results](https://github.com/albu/albumentations#benchmarking-results) against the other popular libraries such as imgaug, torchvision and keras-preprocessing.\n\nIf you found a bug, missing a feature, or just have problems understanding the documentation please [open an issue on GitHub](https://github.com/albu/albumentations/issues/new).\n\nP.S. Vladimir will have a poster at the PyTorch DeveloperConference on October 2nd in San Francisco about albumentations. If you will be there feel free to stop by and say hi.", "link": "https://www.reddit.com/r/MachineLearning/comments/9j6aqg/p_albumentations_a_fast_and_flexible_image/"}, {"autor": "3vvok", "date": "2018-04-17 18:26:11", "content": "From my observations and little experience it appears that most of the ML project are about classifying stuff. Is there cancer signs on the -----> photo !!! ? Does the picture show car, whale or banana? Etc. \nI need to implement a model for face identification. Not detection/recognition, but identification: having two different photos of the same person, my model should determine if the same person is in the picture. \nI want to achieve that using Tensorflow with convolutional nets. I've read this paper: http://ydwen.github.io/papers/WenECCV16.pdf and center loss looks promising. What do you think about that? Are there any new ideas/papers/implementations regarding that problem that are worth attention?", "link": "https://www.reddit.com/r/MachineLearning/comments/8cysrx/d_what_are_the_stateoftheart_models_for/"}, {"autor": "stegben", "date": "2018-04-17 16:22:29", "content": "Say, a model that inputs an -----> image !!!  could output several -----> image !!! s. The number of output images is variable, and the order is not important.", "link": "https://www.reddit.com/r/MachineLearning/comments/8cxtj5/d_how_to_implement_a_neural_network_to_generate_a/"}, {"autor": "stuartprintspace", "date": "2018-04-17 08:29:42", "content": "-----> image !!!  to -----> image !!!  style transfer\n\nThe system works by users uploading their -----> image !!! , plus a style reference -----> image !!!  and our system will learn the style of the reference -----> image !!!  and apply that to their own -----> image !!! . Here are some results: goo.gl/aWstq1\n\nThe idea is that  a machine learning system that would learn any style and apply it to your un-retouched images, to avoid using a human photoshop expert. We would charge $20 to learn a style (you can use any image, any resolution) and $1 per image to apply that style. ", "link": "https://www.reddit.com/r/MachineLearning/comments/8cv0b5/p_image_to_image_photographic_style_transfer_deep/"}, {"autor": "MLmarlena", "date": "2018-04-17 03:40:29", "content": "Hi all! I\u2019m building a personal deep learning computer and I was wondering if anyone had advice on the choice of CPU. This is my first computer build ever, so I apologize in advance for being naive. \n\nI originally purchased an AMD Ryzen 5 1600 CPU along with ASRock AB350 Pro4 mobo, but being so new I didn\u2019t realize that with this configuration I couldn\u2019t display anything to the screen without an independent GPU. I have plans to get a Nvidia Titan Xp for my DL computation, but am worried about utilizing it for both displaying and computations.\n\nWith this in mind I started to research the AMD Ryzen 5 2400G APU (CPU with integrated GPU) however my understanding is the 2400G comes at a performance ding compared to the 1600 (namely, only 4 cores compared to 6 cores). I am wondering if the difference between these two processors will be significant in my DL build? What\u2019s the best option? \n\nThe way I see it there are three options:\n1) go with 2400g for dedicated display graphics, leaving my Titan for pure computation and freeing up a PCIe slot for another graphics card down the line\n2) go with 1600, plug my monitor into the Titan and share the display/compute load, hoping this allows me to utilize the PC for simple tasks like web browsing while my models are training\n3) go with 1600 and get a baseline GPU (i.e. GTX 1030) for dedicated graphics. This gets me full 6-core CPU power with dedicated compute GPU, but the 1030 may go to waste if I decide to add another Titan/1080 Ti to my build down the line.\n\nPrice-wise the 1600 and 2400G are basically equivalent, I just can\u2019t find any resources comparing them for computational purposes, as most of the benchmarking involves gaming. \n\nTl;dr - How much CPU power is needed for most deep learning machines (think -----> image !!!  classification, Kaggle competitions, etc.)? How do the AMD Ryzen 5 1600 compare against the 2400G in this context?", "link": "https://www.reddit.com/r/MachineLearning/comments/8ctn56/d_advice_for_deep_learning_computer_build/"}, {"autor": "creiser", "date": "2018-03-10 20:57:59", "content": "Usually a Siamese network gets as input two (raw) images. I wonder if it would be also possible to train an Auto Encoder to be able to encode the images into a latent representation first. Instead of feeding the Siamese Network two images, you could them feed them the corresponding latent representations of these images. I could imagine that this might work since the latent representation is sufficient to reconstruct the original -----> image !!!  (i.e. it contains most of the information) is also should be sufficient to compare -----> image !!! s. But I am not sure about that point since maybe the spatial layout gets lost in the AE process. Please let me know if you are aware of anybody doing that or something related.\n\nMy motivation: AFAIK (but please correct me if there is anything smarter, I am generally very interested in that question) after we have trained our Siamese network or some other Similarity metric learning network in order to find the image in our repository to which a given image is most similar to we have to do pairwise comparisons with every single image in our repository. Maybe this costly comparisons could be sped up if the repository is held in compressed form (as latent representation created by the AE).", "link": "https://www.reddit.com/r/MachineLearning/comments/83hy1d/d_feeding_latent_representation_as_input_to/"}, {"autor": "SneakDiffer", "date": "2018-03-10 11:57:04", "content": "Hey guys,\nI am currently on a university project which consists of classifying satellite images.   \nThe -----> image !!!  sample is in 16*16*4. \nI want to use a pre-trained model in keras (like VGG, Resnet ..) but all those pre-trained model are usable only for higher than 48x 48 input and 3 channels. \nHow do I upsample mine or how I made the pre-trained model compatible with my sample ? \n\nSorry for my bad english i'm french retarded girl !", "link": "https://www.reddit.com/r/MachineLearning/comments/83eqyq/p_pretrained_model_on_small_image/"}, {"autor": "mrconter1", "date": "2018-03-10 10:30:23", "content": "If you use -----> image !!! -to------> image !!!  techniques such as these:\n\nhttps://arxiv.org/pdf/1611.07004.pdf\n\nhttps://arxiv.org/pdf/1703.00848.pdf\n\nWould it be possible to teach a network to render high-resolution game footage from low res renders? What's keeping us from doing this at the moment? Seems easy to create a database with high- and low-resolution pair of images from a game? I know this depends on how fast the inference is, but shouldn't that be possible to parallelize?\n", "link": "https://www.reddit.com/r/MachineLearning/comments/83efhf/d_have_i_understood_the_imagetoimage_techniques/"}, {"autor": "ptitz", "date": "2018-03-09 23:52:01", "content": "So hay, I graduated a few months ago. Did my master thesis on reinforcement learning for real-time applications. Also did a bit of work with machine vision(obstacle avoidance with UAVs). Now I've got a code monkey job, but I don't want my academic/research skills to go to waste, so I want to do a project for myself.\n\nSo basically I want to do an -----> image !!!  recognition/classification algorithm with a flair for recognizing a 3d scene from 2d -----> image !!! s. I know a bit about current state of art in doing object recognition with stuff like neural nets, based on feeding it a bunch of images and then matching patterns. Skimmed over a few papers, but I'm not entirely convinced that it's something I want to do. \n\nI'm looking for something more analytical, in a sense that an object isn't just matched by running an image pattern through a black box, but a shape is reconstructed from features with as little prior input as possible. So perhaps start with reconstructing something like a flat plane or a cube or a ball from an image and then moving on from there.\n\nIs anyone else busy with something like this? What's the current state of art? And which papers/books should I look into to get started?", "link": "https://www.reddit.com/r/MachineLearning/comments/83bbth/r_whats_the_current_state_of_art_in_3d/"}, {"autor": "mind_juice", "date": "2018-09-09 14:05:30", "content": "Hello,\n\nI had a question about the result images shown in research papers. Are the images hand-picked or random? This question is more relevant for fields such as generative modelling and -----> image !!!  attribution for CNNs where a clear evaluation criterion doesn't exist.\n\nSome research papers explicitly say that the images were randomly chosen. Should I assume that they were hand-picked if it's not clearly stated in the paper? Should I rely on the 'reputation' of the authors?\n\nThanks for taking the time to answer my question! :D", "link": "https://www.reddit.com/r/MachineLearning/comments/9ed6ek/d_are_result_images_in_research_papers_on_gans/"}, {"autor": "mind_juice", "date": "2018-09-09 10:31:52", "content": "Hello,\n\nI had a question about the result images shown in research papers. Are the images hand-picked or random? This question is more relevant for fields such as generative modelling and -----> image !!!  attribution for CNNs where a clear evaluation criterion doesn't exist. \n\n&amp;#x200B;\n\nSome research papers explicitly say that the images were randomly chosen. Should I assume that they were hand-picked if it's not clearly stated in the paper? Should I rely on the 'reputation' of the authors?\n\n&amp;#x200B;\n\nThanks for taking the time to answer my question! :D", "link": "https://www.reddit.com/r/MachineLearning/comments/9ec1xl/are_result_images_in_research_papers_on_gans_and/"}, {"autor": "kythiran", "date": "2018-09-09 05:32:21", "content": "I\u2019m interested in building my own text detection/recognition model that performs OCR on my documents in an offline setting. I\u2019ve tried Tesseract 4.0 and its results are okay, but the cloud services offered by [Google Cloud (`DOCUMENT_TEXT_DETECTION` API)]( https://cloud.google.com/vision/docs/ocr) and [Microsoft Azure\u2019s (\u201cRecognize Text\u201d API)]( https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/) are far superior. \n\nSpecifically, in [Google OCR API\u2019s doc](https://cloud.google.com/vision/docs/ocr) there are two APIs:\n\n* \u201c`TEXT_DETECTION` detects and extracts text from any -----> image !!! .\u201d\n* \u201c`DOCUMENT_TEXT_DETECTION` also extracts text from an image, but the response is optimized for dense text and documents.\u201d\n\nI suspect the models behind the two APIs use technologies found in literatures from scene-text detection/recognition, but do anyone of you know how should I optimize for dense text and documents? Unlike scene-text detection/recognition where plenty of tutorials and literatures are available, I can\u2019t find much information regarding document-text detection/recognition.\n", "link": "https://www.reddit.com/r/MachineLearning/comments/9eara8/d_how_to_build_a_document_text/"}, {"autor": "lzhbrian", "date": "2018-11-03 04:42:45", "content": "[lzhbrian/-----> image !!! -to------> image !!! -papers](https://github.com/lzhbrian/-----> image !!! -to------> image !!! -papers)\n\nI organized a collection of all -----> image !!!  to -----> image !!!  papers, hope this list could also help others.", "link": "https://www.reddit.com/r/MachineLearning/comments/9trg63/p_a_collection_of_image_to_image_papers_including/"}, {"autor": "hmmhhhmhhmhmhmhh", "date": "2018-11-01 23:46:54", "content": "Hey all,\n\nI'm cooking up an idea and wanted some input from potential users. So I know for ML you need good sample data (like tagged pictures). Where do you usually get this data? Is there a specific type of data that's hard to come by (ie: what else other than -----> image !!!  recognition stuff)? What types of sets would it be most beneficial to have a human go through and tag (Pictures of dogs? Spatulas? etc)? Vent to me!", "link": "https://www.reddit.com/r/MachineLearning/comments/9tev54/d_how_do_you_all_get_your_data_sets/"}, {"autor": "Simusid", "date": "2018-11-20 22:51:55", "content": "I've been successful enough with supervised -----> image !!!  classification that I'm now rolling out an experiment into what I consider to be limited production.   I have a model running on an r-pi Zero in 3 locations, taking pics and doing successful classification.    The nodes act locally but can send their imagery back to my development server.  This works really well.\n\nSo I have 5 models in use.   I started with just a dev model on my dev server.  When that trained well I pushed it to node 1.   It worked and gathered imagery that I collected and retrained on dev.   I pushed that back to node 1 and to node 2.   Then node 3 came online.   So thats a dev model, a production model on dev, and 3 production models on nodes.   I know this can all be scripted but it's becoming a pain.\n\nHypothetically, suppose I scale to 50 or 100 nodes.   I suspect that I would see more variance from a single model pushed to all.   That means gathering more imagery, retraining and pushing to all.   That may make sense.   Or I may find a subset of production locations that benefit from a slightly different model.   So then I'll have production model-A and model-B, which means I'll have development model A/B as well.   Again, I could write a tool to \"push\" new models to 100 nodes, but some might fail and have to retry.   What model version is on which node right now?   This could get out of control.\n\nDoes anyone know of any DevOps tools to manage a fleet of models?\n", "link": "https://www.reddit.com/r/MachineLearning/comments/9yxbfa/d_devops_tools_for_managing_models/"}, {"autor": "jatsignwork", "date": "2018-11-20 14:09:22", "content": "I'd like to develop a system to generate artificial letters/symbols/characters.  My current plan for training data is to gather characters from ancient languages like Sanskrit/Akkadian/Pictish/etc, and then feed those into a DCGAN. I'm not concerned with making a \"complete\" alphabet - I'd like the system to generate new \"symbols\" on demand.\n\n- Is a DCGAN the right approach for this? I've seen it used for generating -----> image !!! s, but I don't need a \"real\" -----> image !!! , just a character.\n\n- For each existing alphabet I use I'll take each character and flip/rotate each symbol to generate more data. Are there better/more ways to increase the size of the training set?\n\n- Am I making this harder than it needs to be? Is there a simpler, non-NN approach I should look at?", "link": "https://www.reddit.com/r/MachineLearning/comments/9ysh7f/p_generating_an_artificial_alphabetletters/"}, {"autor": "tldrtldreverything", "date": "2018-11-19 20:21:53", "content": "Hey everyone,\nI encountered a new Nature paper on ML for medical -----> image !!!  analysis, went down the rabbit hole, and came with a summary of the field - https://lyrn.ai/2018/11/16/advancing-to-3d-deep-neural-networks-in-medical------> image !!! -analysis/ (TLDR: 3D CNNs for medical -----> image !!! s are going to be a big deal)\n\nI hope it gives you a useful introduction. I'm happy to hear your feedback and answer any questions you may have.", "link": "https://www.reddit.com/r/MachineLearning/comments/9ykj93/r_advancing_to_3d_deep_neural_networks_in_medical/"}, {"autor": "Reiinakano", "date": "2018-11-19 18:34:47", "content": "Demo link: [https://reiinakano.github.io/arbitrary------> image !!! -stylization-tfjs/](https://reiinakano.github.io/arbitrary------> image !!! -stylization-tfjs/)\n\nSource code: [https://github.com/reiinakano/arbitrary------> image !!! -stylization-tfjs](https://github.com/reiinakano/arbitrary------> image !!! -stylization-tfjs)\n\n&amp;#x200B;\n\nI built a demo for playing with arbitrary -----> image !!!  stylization in the browser using TensorFlow.js. Instead of having a single network per style like previous algorithms for fast style transfer, only a single model is used for all style images. You can also freely mix styles together.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://i.redd.it/i1f1iyu10cz11.jpg\n\n&amp;#x200B;\n\nThe original paper is from [https://arxiv.org/abs/1705.06830](https://arxiv.org/abs/1705.06830) \n\n&amp;#x200B;\n\nIn summary, a style network is trained and used to generate a 100-D style vector for any painting. This vector is then fed, along with the content image, to a separate transformer network for the actual transformation.\n\n&amp;#x200B;\n\nThis 100D vector is basically a latent space of \"style\". And we can do interesting \"latent space-y\" things with it. We can control stylization strength via a weighted average of the style vectors of the style and content images. I found this useful for styles that tend to \"overpower\" the content image. We can also combine different styles by interpolating between their style vectors, and letting the network guess what a style in-between the two paintings would look like.\n\n&amp;#x200B;\n\nThe biggest issue with porting this to the browser was the model size. The style network is based on Inception-v3, which has &gt;97MB of weights. Using distillation, I was able to replace it with a 9.6MB MobileNet-v2, a 10x reduction in size. I think distillation is an underrated technique to bring some cool ML functionality to the browser, a lot of which use models too big to reliably deploy in a resource-limited environment.\n\n&amp;#x200B;\n\nIn any case, I acknowledge the results are not perfect and will not look good for all combinations of style and content (can't get Van Gogh to work properly for some reason), but I think it's a good reason to get excited about what will eventually become possible in the future using the browser alone.", "link": "https://www.reddit.com/r/MachineLearning/comments/9yjhm6/p_fast_style_transfer_for_any_style_in_the/"}, {"autor": "Frankie2019", "date": "2018-06-19 02:12:02", "content": "I want to matchI have a -----> photo !!!  (p) and a library of images(M).\n\nI want to find similar images of p in M.\n\nAll the images in M have different sizes, so the process is more complicated to fixed sizes.\n\nI notice that google has this function. What is the mechanism and algorithm for that?\n\nI searched on the web. In deep learning, a CNN is used first followed by LSTM and then dense layer.\n\nHowever, there are some problems.\n\nFirst, the image sizes are assumed to be fixed. It is suggested to crop the images before processing. In reality, some important information may be lost due to improper crop. How to deal with variable sizes?\n\nSecond, this is a classification problem using training set. In reality, training samples of some classes are much fewer than the others. It may affect the accuracy. How to compare pairwise images, that is, compare every images in M to p and find the most similar one in M instead of doing classification?\n\nAny examples on similar image processing?\n\nThank you very much.", "link": "https://www.reddit.com/r/MachineLearning/comments/8s57en/how_to_process_images_with_different_size_in/"}, {"autor": "mtbarta", "date": "2018-06-18 01:09:54", "content": "[https://mtbarta.github.io/monocorpus/](https://mtbarta.github.io/monocorpus/)\n\nThis just hit MVP status (and still in active development). I'd love feedback on the idea.\n\nHow do you keep organized? I have a markdown file in git that has my notes in it, chronologically ordered. I jot down a lot of *hows* and *whys* that don't fit into issue trackers, and it's helped with very specific bugs that are unique to me and my team. I can go back and figure out how I ran something if I ever forget.\n\nA list of current features:\n\n* Full Text Search\n* Chronological ordering of notes\n* Title Filters\n* Arxiv abstract importing\n* KaTeX support\n* Markdown support\n* -----> Image !!!  support (beta)\n\nI'm looking for feedback. What's your development process? Does this not fit for some reason? Is there a feature you consider a must-have? How do you keep track of your work?", "link": "https://www.reddit.com/r/MachineLearning/comments/8rvl24/p_i_made_an_engineering_notebook_to_help_stay/"}, {"autor": "browngrammer", "date": "2018-06-07 01:16:28", "content": "I'm trying to find papers that use CNNs to learn the angle and size of the floor plane from a -----> image !!! . Essentially the input is an image and the output is something like the pixel locations (X, Y) that correspond to a one inch square on the ground floor plane.\nUnfortunately I'm not sure what this problem is called so it's hard to search for it. Does anyone know the name of the problem and/or if there are any papers or datasets related to it? ", "link": "https://www.reddit.com/r/MachineLearning/comments/8p66nw/d_what_is_the_problem_of_detecting_a_floor_planes/"}, {"autor": "skepticforest", "date": "2018-06-06 14:34:06", "content": "I'm trying to curate a list of all the different phrases I come across that describe the task (or objective, I'm not sure what's the most accurate word) while reading papers. I've made the following list and I'd really like to know what I am missing.\n\n**Computer vision**\n\nFace/object detection &amp; recognition, -----> image !!!  recognition, semantic segmentation, scene labeling, scene parsing, pose estimation, inpainting, style transfer\n\n**NLP**\n\nWord embedding, NLU, NLG, textual entailment, POS tagging, sentiment analysis, text classification, text/machine comprehension, question answering, machine translation, summarization, speech recognition, dialogue systems, relationship extraction, named entity recognition\n\n**Image+text**\n\nVisual/concept grounding, visual reasoning, visual QA, caption generation, text-to------> image !!!  synthesis\n\n**Other**\n\nEnd-to-end learning, structured prediction, multi-task learning, transfer learning, seq2seq learning, multi-class prediction, meta-learning, one/few-shot learning, knowledge distillation\n", "link": "https://www.reddit.com/r/MachineLearning/comments/8p13e1/d_what_are_all_the_different_specific_tasks_in/"}, {"autor": "winstonl", "date": "2018-06-25 14:54:24", "content": "I am working with a local university to design a short course for mid level managers. Specifically, I think our target students are:\n- mid level people managers who have some influence over their teams (e.g. managers, senior managers, directors) \n- people who are interested in ML/AI and how that may impact their team, and want to learn more than just high level concepts.\n- They may be from different industries but most likely banks, insurance, telecommunications and retail\n- The course will be roughly 2 days, so approximately 15 hours of material in total.\n\nThere will be a large portion of \"lectures\", ranging from what ML is to specific use cases in different industries to help people see how others have been leveraging the technology, but I also want to make this more like a \"course\" than a conference. What I mean is that I don't want these people to come in, sit down, listen to a bunch of people talk and then go home. I want there to be some \"hands-on\" stuff as well.\n\nI wonder if anyone has any good recommendations on 'hands-on' practices for this group of people. Something like deriving the backpropagation or code an -----> image !!!  classifier obviously doesn't make sense, but ideally it should offer reasonable challenges to them as well.", "link": "https://www.reddit.com/r/MachineLearning/comments/8tr57n/d_designing_a_short_ml_course_for_mid_level/"}, {"autor": "georgegach", "date": "2018-06-25 11:11:52", "content": "*Processing img z038lz4pp4611...*\n\nThis is hands down the most significant innovation I have seen in mobile -----> photography !!!  for a long time. The fact that combination of person segmentation + 1mm baseline dual-pixel hardware techs combined can produce such high quality result is just amazing.\n\nFrom this point onward dual-camera configurations are pretty much obsolete. \n\nTweet @arxivtrends: [https://twitter.com/arxivtrends/status/1011193443991916544](https://twitter.com/arxivtrends/status/1011193443991916544)\n\nPaper: [https://arxiv.org/pdf/1806.04171.pdf](https://arxiv.org/pdf/1806.04171.pdf)\n\nWhat do you guys think? Where is the room for improvement?", "link": "https://www.reddit.com/r/MachineLearning/comments/8tpni1/dr_synthetic_dof_google_pixel_portrait_mode/"}, {"autor": "paland3", "date": "2018-11-30 13:36:47", "content": "Hi, I have a ranging device with the following output:\n\nhttps://i.redd.it/mk1fc13iyg121.png\n\nWhere each cell is a \"pixel\". You can see that range resolution is homogeneous across the frame, but the azimuth is not: the same direction covers a much bigger region further away. It is not shown on the -----> image !!! , but azimuth borders are also not spaced equally, beams on the side are wider.   \nThat is, my input data is very much not spatial invariant. However, I would like to process it in a sliding window approach for baseline, and later on, using a convolutional network. Both would need an invariant data in my opinion. How would you solve this contradiction? Could you point me to technics or similar problems? My ideas so far:\n\n1. Add an additional 2 layers containing the pixel's position in the image, e.g. range 5, direction 3. This doesn't help on linear classifiers like svm though.\n2. Resample the data in a homogeneous, orthogonal way. I have two problems with this:\n   1. it is quite hard to do so efficiently\n   2. Will introduce \"information\" to the data which is not here, e.g. finer resolution further away.\n\nI am grateful to your inputs.\n\n&amp;#x200B;\n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/a1shcu/d_sliding_windowconvolutional_processing_of_a_non/"}, {"autor": "svpadd2", "date": "2018-11-30 00:39:54", "content": "I want to use MAML/Reptile on a NLP model for Named Entity recognition, how would I split the dataset by class? For example traditionally with meta learning on -----> image !!!  datasets for training you would sample define each \"task\" as maybe one or two classes of -----> image !!! s out and then show the core model k items from each class and then perform the meta update. However with a NER you cannot split up your dataset of sentences that easily into mini tasks because each contains a mixture of classes. What would be the best way to divide the dataset to get the same benefit of rapid adaptation images.", "link": "https://www.reddit.com/r/MachineLearning/comments/a1nbgf/d_metalearning_setup_for_seq2seq_model/"}, {"autor": "thetechkid", "date": "2018-11-29 23:11:44", "content": "I'm having an issue at the moment with a model I am trying to work on for -----> image !!!  classification. I believe part of the issue may be the way that I am structuring the data for training and testing. I do not have a predefined dataset to pull data and labels from so I am essentially creating two directories and sub folders within those for the images for each of the categories. Now this may be a simple issue I'm just missing, or my approach is wrong(because I can't seem to get any better than 20% accuracy) so I want to ask about the proper way to do this. I am using keras, and the GPU version of TF at the moment and any help in the right direction would be amazing. ", "link": "https://www.reddit.com/r/MachineLearning/comments/a1mkds/d_creating_a_dataset_for_learning/"}, {"autor": "Loggerny", "date": "2018-11-29 04:14:25", "content": "I have a web application that needs to generate -----> image !!!  similarity scores from newly uploaded files. Currently, I have a home GPU machine with a script [^1] that extracts VGG16 based features. I then pickle the results and use the aggregated embeddings for k-means clustering/similarity checking.\n\nIm wondering if I need to manage a GPU based machine (I would do this in some PAAS) and build out a web service that does this, or if there are applications that do this for a low cost/free.\n\nI want to take an uploaded image-&gt;get extracted features-&gt;store results in a DB/elastic search.\n\n[1]: https://github.com/rememberlenny/graffiti-net/blob/master/scripts/extract_features.py", "link": "https://www.reddit.com/r/MachineLearning/comments/a1dy1x/d_question_whats_the_cheapest_way_to_extract/"}, {"autor": "esghili", "date": "2018-07-17 16:38:36", "content": "Hello everyone,\n\nI have an excelsheet that has records of all excavation damages to gas pipelines in half of state of Washington. It shows each day, which location have been damamged by which contractor( not everyday this happens)\n\nI am trying to add some features to that excelsheets such as rain amount and weather condition and based on that predcict where will be the next damage in future( for probably next 2 months using the weather forecast)\n\nConsidering the fact that I only have the data for when the excavation damage has happened, do you think it would be poissible for me to predict the future?I asked my cousin who is a machine learning engineer and she said the model will be very biased as we only have the data for when and where it happened and not for when it didnt happen.\n\nI was thinking to build a database with the GIS software that has all the locations in our territory and then add the excavation data to that database in oorder to be able to build a model.\n\nI have no background in machine learing so I would be really happy if you could answer my question\n\nI attach an -----> image !!!  to this post\n\nThank you very much", "link": "https://www.reddit.com/r/MachineLearning/comments/8zmpcr/predicting_excavation_damages/"}, {"autor": "Drgoldsz22", "date": "2018-08-22 02:33:07", "content": "Hello guys, \n\nI am training a binary classifier to identify pictures of good and bad zippers. The pictures that I am using for this task are being taken from a conveyor belt. I have around 8000 pictures of which 4400 are of good zippers.\n\nOne important factor to consider, is that when I was constructing this dataset, I would take 4 pictures every time I passed a zipper through te conveyor belt in order to increase the size of the dataset quickly (I am not sure if this can be the cause to my error)\n\nWhen I trained the classifier (random forest) using about 30% for testing set I end up getting very high scores in both the training and testing (1.00 , 0.9987). Furthermore, a 5 fold cross validation test with an average of 99%+\n\nThe problem I am having is that when I use the trained classifier with live pictures (zippers that I am passing through the conveyor belt to do validation) it predicts all zippers as bad. \n\nI took 42 individual -----> picture !!! s of good zippers to validate and it predicts all of them as being bad (only one -----> picture !!!  per pass this time). I don\u2019t understand how this can be happening if it performs so well on the testing set. It predicts more than 1000 zippers that are good correctly. \n\nAlso when I place the first 20 pictures of the validation set in the training set, it is able to predict about 16 right out of the 22 that are left in the validation set. \n\nI really do not know what to do. I would appreciate any piece of advise. \n\nThanks for the help! \n", "link": "https://www.reddit.com/r/MachineLearning/comments/999jg5/problem_with_validation_set_for_random_forest/"}, {"autor": "HenryJia", "date": "2018-08-21 20:29:02", "content": "Hi guys, what do you guys consider to be SOTA in neural -----> image !!!  captioning now? I'm familiar with the Show and Tell paper [https://arxiv.org/abs/1411.4555](https://arxiv.org/abs/1411.4555) but that's a few years old now and I find it quite complex computationally to implement (the LSTM attention mechanism). What do people use now for neural captioning?", "link": "https://www.reddit.com/r/MachineLearning/comments/996poo/dwhat_is_the_state_of_the_art_in_image_captioning/"}, {"autor": "MrLeylo", "date": "2018-06-04 14:33:40", "content": "Hello /r/MachineLearning! The goal of this thread is twofold, to share what I have found about FGVC state of the art and to take note of anyone who could correct me.\n\nThe challenge consists on classifying entities which differ in some small subtle local features contained in a big context (noisy for this classification task). There are some typical datasets for this task such as [CUB birds](http://www.vision.caltech.edu/visipedia/CUB-200.html) or [Stanford dogs](https://pdfs.semanticscholar.org/b5e3/beb791cc17cdaf131d5cca6ceb796226d832.pdf)\n\nAs far as I know, we could group the approaches for it between attention-driven and non-attention-driven.\n\nFor attention driven it could be interesting to read Fu et Al publications [RA-CNN](http://openaccess.thecvf.com/content_cvpr_2017/papers/Fu_Look_Closer_to_CVPR_2017_paper.pdf) and [MA-CNN](http://openaccess.thecvf.com/content_ICCV_2017/papers/Zheng_Learning_Multi-Attention_Convolutional_ICCV_2017_paper.pdf) (state of the art for attention-driven approaches, which basically learns features and groups them into multiple parts for further weighting and final classification). Kanth et Al propose a [more generic method](https://arxiv.org/pdf/1805.05389.pdf) which works for non-fine-grained -----> image !!!  classification as well as for fine-grained (slightly lower result), while Cui proposes [an improvement](https://vision.cornell.edu/se3/wp-content/uploads/2018/03/FGVC_CVPR_2018.pdf). Finally, PAIRS is exposed as the non-attention-driven state of the art [approach](https://arxiv.org/pdf/1801.09057.pdf).\n\nThis is as far as I know. Feel free to correct me.\n\nFurthermore, I use this thread to ask if anybody knows about some workshop/challenge where to apply it. I've been looking in typical conferences but I don't find any. Thank you in advanced!", "link": "https://www.reddit.com/r/MachineLearning/comments/8ohpzu/r_state_of_the_art_on_finegrained_image/"}, {"autor": "ACTBRUH", "date": "2018-06-04 03:11:42", "content": "Hey guys! I've been wanting to start a research blog for awhile now, but never really felt confident enough in my research ability (I'm almost entirely self-taught and have only been studying this for 1.5 years). Decided that the worst that can happen is that nobody reads it, so I spent the last week digging up an idea I had 6~ months back and turning it into a post!\n\nSpecifically, this post is about an idea I had for weakly-supervised -----> image !!!  segmentation, which uses conditional thresholds derived from attention maps to segment out the object of interest. While there are some obvious flaws with the idea (can't segment out multiple objects, though I have an idea on how to fix that), since I don't have any intentions to publish it, I thought it'd be cool to write about it! Let me know what you think, I'm always looking to get better at writing:\n\nhttps://scientificattempts.wordpress.com/2018/06/03/conditionally-thresholded-cnns-for-weakly-supervised-image-segmentation/\n\nAs a side note, this post doesn't include results for applicable datasets (like COCO), which'll come in the next post, though I do show a snapshot of its performance on a toy dataset like CIFAR-10.", "link": "https://www.reddit.com/r/MachineLearning/comments/8oe402/d_started_up_a_research_blog_for_the_random_ideas/"}, {"autor": "whria78", "date": "2018-06-02 08:13:42", "content": "Our manuscript, \"Interpretation of the Outputs of Deep Learning Model Trained with Skin Cancer Dataset\" was published as a letter article in the Journal of Investigative Dermatology today \\([https://www.jidonline.org/article/S0022\\-202X\\(18\\)31992\\-4/fulltext](https://www.jidonline.org/article/S0022-202X(18)31992-4/fulltext)\\).\n\n**\\&lt;The analyzing AUC is better than Top\\-\\(n\\) accuracy when we have small and imbalanced training dataset\\&gt;**\n\nWhen we train a CNN model, we sometimes get a disappointing Top\\-1 accuracy. I also had suffered this problem and I did not understand exactly what was wrong. When the early version of the 12DX paper \\([https://www.jidonline.org/article/S0022\\-202X\\(18\\)30111\\-8/fulltext](https://www.jidonline.org/article/S0022-202X(18)30111-8/fulltext)\\) was reviewed in JAMA dermatology 2 years ago, the biggest reason for rejection was low Top\\-1 accuracy.\n\nHowever, unlike general object recognition studies, it is very difficult to determine the results with Top\\-1 accuracy, and it is important that the AUC value of ROC curve can be high even with a low Top\\-1 accuracy. If you look carefully, most of medical AI researches have used the AUC rather than Top\\-\\(n\\) accuracy.\n\nBecause of small and imbalanced training data in medical researches, the analysis of each class as Top\\-\\(n\\) accuracy is inadequate \\(but the mean value of Top\\-\\(n\\) of all classes is meaningful\\). Top\\-\\(n\\) accuracy of each classes vary whenever we repeat training of CNN with an imbalanced dataset. Therefore, we should see the corrected value while using thresholds of each classes, that is the AUC value of ROC curve.\n\nWith good AUC results, we published \"12DX; Classification of the Clinical Images for Benign and Malignant Cutaneous Tumors Using a Deep Learning Algorithm\" \\([https://www.jidonline.org/article/S0022\\-202X\\(18\\)30111\\-8/fulltext](https://www.jidonline.org/article/S0022-202X(18)30111-8/fulltext)\\). However, there was a debate that our algorithm is not sensitive \\(low top\\-1 accuracy\\) with ISIC dataset \\(Automated Dermatological Diagnosis: Hype or Reality?; [https://www.jidonline.org/article/S0022\\-202X\\(18\\)31991\\-2/fulltext](https://www.jidonline.org/article/S0022-202X(18)31991-2/fulltext)\\).\n\n**\\&lt;Classifying tumorous nodule is much difficult than determining whether it is malignant or not\\&gt;**\n\nThere was an additional problem as well as the Top accuracy problem. When we analyze a clinical -----> image !!! , \"the problem of judging whether it is melanoma or not\" is easier than \"the problem of matching the type of cancer\".\n\nAnalyzing the output of the \u200bCNN is equivalent to solve \"the problem of matching the type of cancer\". Therefore we need to analyze the ratio of output, if we want to solve\u00a0the problem of judging \"whether cancer or not\".\n\nWe interpreted the ratio of melanoma output and nevus output rather than using melanoma output alone.\n\n*RATIO \\(Melanoma Index\\) = melanoma output / \\(melanoma output \\+ nevus output\\)*\n\nThe clinical image of skin cancer consists of a nodular lesion and a background. If you want to concentrate on only the lesion, we need to analyze it with RATIO as above to get more accurate results.\n\nIn the attached photograph \\([https://i.imgur.com/jnZUavi.png](https://i.imgur.com/jnZUavi.png)\\), \\(b\\) is \"matching what cancer is\" and \\(a\\) is judging \"whether it is cancer or not\". \\(a\\) is much better than \\(b\\). The AUC for diagnosing melanoma jumps from 0.69 to 0.91.\n\nWe made web\\-DEMO \\([http://dx.medicalphoto.org](http://dx.medicalphoto.org)\\), and it possible to show what conclusions are coming up depending on the Top\\-5 output and how it is interpreted.", "link": "https://www.reddit.com/r/MachineLearning/comments/8nz77y/r_interpretation_of_the_outputs_of_deep_learning/"}, {"autor": "michaelochurch", "date": "2018-01-23 14:51:18", "content": "Let's say you have a fairly sparse neural network: not more than 50,000 connections, and no more than ~100 connections going into each node. \n\nThis is to learn a game (not -----> image !!!  processing) so we don't need millions of connections (much less a convolutional layer). Not only that, but such high dimensionality would likely to lead to overfitting, when the underlying function could probably be represented (in theory, anyway) with a couple hundred parameters. The challenge is the stochastic environment. Training a heuristic presents noise (especially early in the game) and, of course, good moves can lead to bad outcomes. \n\n(To get into the details, I'll be using standard optimization techniques to train a heuristic, and performance in the game itself for validation, to inform a genetic/evolutionary approach to feature selection and dimensionality reduction. We'll see if that approach works. One challenge is that I can't manually fiddle with learning rates for each network I create.)\n\nYou probably don't want to do a full second-order method on 10,000+ weights. (Inverting a 10k-by-10k matrix is expensive, and possibly not worth it in a stochastic environment.) But gradient descent is slow and requires a lot of fiddling with parameters (learning rate, momentum, etc.) and effectively never converges in a stochastic environment. So, I'd like to find something better than SGD if possible. \n\nWhat about this approach, though: use backpropagation (first-order) to send error signals through the network. With those error signals, use Newton's method at each node (&lt; 100 inputs) to update the weights. If these updates move too fast, then, instead of a learning rate, set a desired norm (say, normalize to |\u0394w|_2 = 0.1). \n\nDoes this sound like a reasonable approach? Has anyone tried something like this? What were the results?", "link": "https://www.reddit.com/r/MachineLearning/comments/7sevme/d_secondorder_eg_newton_methods_for_sparse/"}, {"autor": "StrawberryNumberNine", "date": "2018-01-22 23:01:22", "content": "I have built a modified encoder-decoder conv net which tries to recover the input -----> image !!!  using an L1 Loss and a GAN discriminator loss. I am having trouble with the color of the output image, the network seems to output the features and high frequencies pretty well but the color becomes very bland and all of the images are like sepia-colored.\n\nAnyone have any experience with this? Maybe the L1 loss is not the correct loss to use?\n\nThanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/7s9uni/d_encoderdecoder_network_output_color_issue/"}, {"autor": "iamwil", "date": "2018-02-13 04:59:22", "content": "I know it depends largely on the domain that you're working on. -----> Image !!!  tasks will probably be a directory of images, and audio tasks will probably be a directory of audio. But if you have structured data, do you keep it all in CSVs, or do you try to dump it into a database first? I don't see any real reason to have it in a database, since we get the training data in chunks of rows anyway, unless it was already provided as a database already?\n\nHow many of you work off of some CSV/HDF5 file and how many suck it down from a database?", "link": "https://www.reddit.com/r/MachineLearning/comments/7x72i2/d_where_do_you_keep_your_training_data/"}, {"autor": "satyen_wham96", "date": "2018-02-12 06:57:19", "content": "Masters Student here! As part of a course, I have to do a deep learning project. I am really excited about GANs(I just know the basics, still have to read more in depth) and believe that combining language aspect would be a real cool thing. Would love to know your ideas!\n\nSome things that I thought of/googled:\nText to -----> Image !!!  synthesis- (https://github.com/reedscot/icml2016) user types in image description and image is generated. \n\nText to video generation- (https://arxiv.org/abs/1710.00421) user types in video description and a short video is generated.\n\nText to lip sync- User types in text and a random person will say the text with accurate lip movement. Something like ObamaNet (http://ritheshkumar.com/obamanet/) but instead of Obama, we could try to model the User itself. (User would have to upload a short video)", "link": "https://www.reddit.com/r/MachineLearning/comments/7wz0xg/dproject_involving_gans_and_natural_language/"}, {"autor": "Nimda_lel", "date": "2018-12-07 13:01:26", "content": "Hello /r/MachineLearning,\n\n&amp;#x200B;\n\nI am new to ML and I was assigned a task to make a decent Object Detection model to work on Google Vision Kit ([https://aiyprojects.withgoogle.com/vision](https://aiyprojects.withgoogle.com/vision)), but it seems really hard to find a decent object detection model that would run on that Device and produce decent results (I am only trying to detect people).\n\n&amp;#x200B;\n\nAs a fallback plan I thought I could use AWS ML Optimized machines. It is all fine except the fact that the Pictures might be confidental, i.e. I want to detect only how many people are on the -----> image !!!  rather than seeing the people themselves.\n\n&amp;#x200B;\n\nI was wondering, is it possible to apply some kind of filter on the image and still use ML to detect just the number of people on the image?\n\n  \nIn theory that should be possible, but does anybody have any example/tutorial/post or something that might be related?\n\n&amp;#x200B;\n\nNOTE: I prefer using Tensorflow and I am also open to ideas about optimized Object Detection Models that only detect people.", "link": "https://www.reddit.com/r/MachineLearning/comments/a3znb6/d_is_it_possible_to_use_object_detection_on/"}, {"autor": "downtownslim", "date": "2018-12-06 17:21:37", "content": "https://twitter.com/NalKalchbrenner/status/1070669203680755712\n&gt; More compute, better architectures and novel robust orderings have fueled tremendous progress for AR -----> image !!!  models. SPNs are our latest installment. Looking forward to samples from 2020!\n\npaper: http://arxiv.org/abs/1812.01608\nopen reviews (scores 9,10,7): http://tinyurl.com/ycn3j3cj", "link": "https://www.reddit.com/r/MachineLearning/comments/a3q0gt/r_best_unconditional_imagenet_generations_sor_far/"}, {"autor": "tpapp157", "date": "2018-05-28 00:30:05", "content": "You may remember a hobby project I posted about here previously that used a GAN to generate a terrain map based on a simple user supplied input -----> image !!!  \\( [https://www.reddit.com/r/MachineLearning/comments/7dwj1q/p\\_fun\\_project\\_mspaint\\_to\\_terrain\\_map\\_with\\_gan/](https://www.reddit.com/r/MachineLearning/comments/7dwj1q/p_fun_project_mspaint_to_terrain_map_with_gan/) \\). Over the past month or so I've found some spare time to return to this project and attempt something much harder but also much more useful. The terrain maps are fun but only really useful as nice images to look at and not much more. My new goal is to generate something useful in the 3D world by creating height and texture maps. Height maps are grayscale images that use pixel intensity to encode geographic altitude and are often used to store the topographic data of 3D landscapes. Texture maps are images that hold the coloring of the terrain. Height maps are fine because elevation data for the Earth is freely available but texture maps don't exist \\(at least not in the way that I want them\\) so I'm left with my original terrain map images.\n\nOf course that's a problem because the images I'm trying to generate are not the same as the images in my training set. The solution is that using the height map I can calculate the shadows and then apply them to the texture map to recreate the terrain map. This calculation represents a bunch of trigonometry \\(essentially calculating the angle between a line and a plane in 3D space as derived from pixel values\\) and even after I've simplified and approximated this calculation a bunch it's still not a nice thing to push gradients back through. The result is really unstable training.\n\nI started with vanilla GAN but unsurprisingly this barely lasted a few epochs before collapsing. Next was vanilla WGAN which did better but not all that much. WGAN\\-GP was better again but still eventually collapsed. I then stumbled on this paper \\( [https://arxiv.org/abs/1705.09367](https://arxiv.org/abs/1705.09367) \\) and it provided the most stable training by far. Image quality was also way better but after about 4 days of continuous training it also collapsed \\(millions of images\\). Training the discriminator faster than the generator simply led to the discriminator becoming too good and collapse.\n\nIn addition to trying different metrics I also tried different tweaks to the architecture. A combination of leaky\\-relu and normal relu in the generator provide the best results \\(elu was ok, selu wasn't very good, swish was really bad\\). Selu provides good results in the discriminator without any normalization \\(at least it seemed better then relu and elu with normalization although the discriminator isn't all that deep at about 9 layers\\). For normalization in the generator I'm currently using the pixel normalization described in the Progressive Growing of GANs paper although I don't like the concept of this and I'll probably switch back to standard batch normalization. Changing from normal convolutions to a series of dilated convolutions in the generator was a big improvement. I've tried other things here and there but I can't really recall them right now.\n\nMy current architecture looks like this:\n\n[https://i.imgur.com/NXdajT4.png](https://i.imgur.com/NXdajT4.png)\n\nThe generator is composed of residual blocks, each with a series of dilated convolutions followed by a strided convolution to downscale. Feature maps are pulled from the end of each residual block, put through a 1x1 convolution to reduce the number of channels and then upscaled using nearest neighbor. All of these feature maps are then concatenated into a big block. This goes through a few more residual blocks to reduce the number of channels before a few transposed convolutions to upscale the image. The original input image is concatenated back in toward the end of the generator to help with training. The generator outputs a 4 channel image \\(3 value texture map \\+ 1 value height map\\). These are used to create the terrain map as described above.\n\nThe discriminator works fairly similarly. The generated height and terrain maps are concatenated with the input image map and fed to the discriminator. The combined image block is passed through strided convolution layers to downscale. At regular intervals, feature maps are pulled out through a 1x1 convolution to collapse to a single feature map. These maps are upscaled back to 512x512 with nearest neighbor and concatenated. The mean value is calculated for each pixel \\(this creates a great visualization to assess the discriminator during training\\) and then the overall image mean is used for the loss function. L1 loss is also added to the loss function.\n\nSo I'm looking for any suggestions on what else I can try to help improve training stability. Links to papers or discussions are great, code samples are also really helpful. Let me know if you have any questions.", "link": "https://www.reddit.com/r/MachineLearning/comments/8mm1jh/d_help_stabilizing_gan/"}, {"autor": "alba_troz", "date": "2018-05-28 00:16:05", "content": "Hello and thanks in advanced! I have two things to ask:\n\nMy Thesis consists on proving the concept of CNN's applied to automated liver tumor segmentation. I am trying to adapt [this ResNet](https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/applications/resnet.py), and I'm using the [LiTS DataSet](https://competitions.codalab.org/competitions/17094#participate). It consists of \\~30 CT scans, each 512x512xN \\(N layers, varying between 75 and \\~600 layers per scan\\). Problem is ResNet expects 3 layer pictures \\(RGB\\) per input.   \nSo the solution I found was to use as input 3 layers of a scan at a time, until I had the full -----> picture !!!  covered. So if scan A had  6 layers, I would do 4 pictures out of it, the first being layers \\(0, 1, 2\\), then layers \\(1, 2, 3\\), \\(2, 3, 4\\), \\(3, 4, 5\\), all of which are 512x512x3. Is this a good strategy?  \n\n\nAlso, I do not understand how I am supposed to feed the expected masks to the network. This is this RestNet's header:\n\n    ResNet(input_shape=None, classes=10, block='bottleneck', residual_unit='v2', repetitions=None, initial_filters=64, activation='softmax', include_top=True, input_tensor=None, dropout=None, transition_dilation_rate=(1, 1), initial_strides=(2, 2), initial_kernel_size=(7, 7), initial_pooling='max', final_pooling=None, top='classification')\n\nHow should I pass the expected output to this code?\n\nThanks again and hope you have a great day!", "link": "https://www.reddit.com/r/MachineLearning/comments/8mlyco/p_resnet_implementation_for_medical_image/"}, {"autor": "begooboi", "date": "2018-05-27 15:16:59", "content": "I was not going to post this but something wrong is happening here in this subreddit which forced my hands.\n\n\nThis week two posts relating to machine learning were posted here one is about [How visual search works](https://thomasdelteil.github.io/VisualSearch_MXNet/) and other about [generating ramen](https://www.reddit.com/r/MachineLearning/comments/8l5w56/p_generative_ramen/). The former post contains a small write up, source code and a demo site to explain how visual search works and the latter just have a gif of generated  ramen probably with a GAN. The irony is that the post which has more information and source code for reproducing that work got only about 25 votes and the one with gif only with no source code or explanation provided got more than 1000 votes (not so unique work any one with basic understanding of GAN can make one). Today the most upvoted post here is about [a circle generating GAN](https://www.reddit.com/r/MachineLearning/comments/8mgs8k/p_visualisation_of_a_gan_learning_to_generate_a/) which also has only a gif with brief explanation as comment and no source code. Are you seeing a pattern here?\n\nThe problem I mentioned above is not a one of case, I am a regular lurker in this subreddit and for the past few months I started seeing some disturbing patterns in posts posted here. People who posts gif/movie/-----> photo !!!  only post tends to get more upvotes than the posts with full source code or explanation.  I agree some original research posts [such as this](https://www.youtube.com/watch?v=qc5P2bvfl44&amp;feature=youtu.be&amp;t=7s) can be only be released as videos and not a source code because of its commercial value. But most of the gif/movie/photo only posts here are not at all original research but they used a already know algorithm with a different dataset (eg: Ramen generation). \n\nThe problem here is If we continue this type of posts people will stop sharing their original works, source code or explanation and then starts sharing this type of end result only posts which will get less scrutiny and more votes. In future, this will not only decrease the quality of this subreddit but also its a greater danger to the open nature of Machine learning field. What's the point in posting a github project link or blogpost here when we can get much more votes with a gif alone one?.\n\n*I am not a academician but I use r/MachineLearning to find blogs, articles and projects which explains/program recent discoveries in AI which I myself can try out.*\n", "link": "https://www.reddit.com/r/MachineLearning/comments/8midpw/d_what_is_happening_in_this_subreddit/"}, {"autor": "tomuz", "date": "2018-01-06 17:05:29", "content": "Hi guys! Im currently working on making an -----> image !!!  \"domain\" recognition CNN.\n\nThe CNN have to recognize different domains of products based on the picture of the product.\n\nI've 20 main \"categories\" like clothes, electronics, etc and inside each one have like 15-20 different domains.\n\nMy question here is what kind of architecture would you recommend for that task\n - Should I train a big CNN with all domains (arround 350 domains) \nor \n - Should I train 20 smalls CNN (each one corresponding to each category) and another one to decide in which CNN should look (like if it should look into the \"Clothes\" CNN or \"Cellphones\" CNN).\n\nWhat do you consideer the best practice in that kind of scenarios?\n", "link": "https://www.reddit.com/r/MachineLearning/comments/7okea5/dhelp_need_help_about_a_cnn_architecture/"}, {"autor": "udaykp", "date": "2018-02-19 18:22:28", "content": "https://github.com/udaypk/FastAnnotationSingleObject\nhttps://www.youtube.com/watch?v=Cp31IzvgYJ4 \nYou can use this tool for annotating images in PASCAL VOC format. This tool is customized to be very fast if you have only one object per -----> image !!!  and the -----> image !!! s of different classes are separated into different folders. It is 10x faster than LabelImg. You can annotate around 6000 images per day (8 hours) using this tool.\nIf you have multiple objects per image, use https://github.com/udaypk/FastAnnotationMultipleObjects. This is 2x faster than LabelImg.\n", "link": "https://www.reddit.com/r/MachineLearning/comments/7yorhs/p_a_fast_pascal_voc_format_image_annotation_tool/"}, {"autor": "justin285", "date": "2018-01-31 03:51:22", "content": "Say, SIFT based classification models or a KNN based classification model.\n\nAs in, any -----> image !!!  that was previously classified with very high confidence as one class can have an imperceptible perturbation applied to it that causes it to have an incorrect class with very high confidence.", "link": "https://www.reddit.com/r/MachineLearning/comments/7u72ou/d_do_imperceptible_adversarial_examples_exist_for/"}, {"autor": "mkhdfs", "date": "2018-01-30 20:42:38", "content": "The deep learning specialization from Andrew Ng has a lot of programming assignments for -----> image !!!  processing and -----> image !!!  recognition. But the data is all just given to you in number vector format. I know I'm working with an input vector of numbers, and somehow the pictures I uploaded were converted to these number vectors. But I have no idea where the code is for that, or how this type of thing would even be done. How come they don't teach you how to do this? I feel like I learn nothing if I'm just given a vector of numbers with no idea how an image got converted to that number vector. I apparently built an image recognition neural network in the coursera courses, but if you asked me to build an image recognizer without coursera and the jupyter notebook, I would be completely lost and have no idea how to start, because I have no clue how to take images and convert them into a vector of numbers. Is this not essential to know how to do before even attempting to build the NN? How come it isn't taught? And moreover, how come for ANY input data I might want to use (audio signal, text, etc) I don't have a clue how to make these into a vector of numbers? Where does one go to learn this, and how come most machine learning courses don't teach this?", "link": "https://www.reddit.com/r/MachineLearning/comments/7u44yn/d_how_do_you_even_turn_an_image_into_a_vector_of/"}, {"autor": "cudaeducation", "date": "2018-12-15 16:29:20", "content": "I know that you can train your TensorFlow network on the part of the MNIST data, then test it on another part of the MNIST data.  That's wonderful!  But what about trying to submit an -----> image !!!  of your own handwritten digit to the TensorFlow neural network and see if it can \"read\" the number correctly?!?!\n\n&amp;#x200B;\n\nPlease tell me that I'm not the only person on planet earth who has thought of doing this!  It really is a simple matter, but I'm having all kinds of grief getting TensorFlow to read my image properly and have the pixel data etc. arranged in a way that gels with what you would expect from an MNIST image.\n\n&amp;#x200B;\n\nI sincerely hope I'm not the only person that has tried to do this.", "link": "https://www.reddit.com/r/MachineLearning/comments/a6gh8d/how_to_submit_my_person_mnistlike_image_to/"}, {"autor": "dominik_schmidt", "date": "2018-12-15 02:12:21", "content": "I'm training a neural network to classify -----> image !!! s where each -----> image !!!  has on average 17 of 2000 possible labels. (Some images only have one or two labels, some have 20-30). I'm using sigmoid\\_cross\\_entropy as loss which works well but I'm unsure how to convert the network output to binary class labels (like `[0, 0, 1, 0, 0, 1, 1]`) so I can compute the accuracy and get the networks results.\n\nI tried applying `round(sigmoid(predictions))` but since only a few of the 2000 labels are ever correct the networks output mostly looks like this: `[-2.86759, -5.62045, -4.2197, -3.3005, -4.7657]` which is all zeros after applying sigmoid and round. What's the best way to do this?", "link": "https://www.reddit.com/r/MachineLearning/comments/a6bbtr/d_best_way_to_get_binary_class_labels_in_a_sparse/"}, {"autor": "wronk17", "date": "2018-12-14 23:15:33", "content": "I wanted to plug some of [recent ML work](https://medium.com/devseed/mapping-mars-with-ai-590ec1c9cb11) to map Mars with help from a couple planetary scientists. We're working to map craters on Mars using the YOLOv3 object detection model (specifically, [AlexeyAB's fork](https://github.com/AlexeyAB/darknet)). We're hoping to provide better/more data for rover and human landings as well as help answer questions about geologic processes on Mars.\n\n&amp;#x200B;\n\n[Mars 2020 rover mission landing ellipse \\(blue\\) with detected craters \\(green\\).](https://i.redd.it/wcrpac3wsb421.gif)\n\nFor context, the current best planet-wide crater dataset relied on multi-year tracing effort. Obviously, we'll have bring humans into the loop to confirm our model's less-confident predictions, but it should be possible to substantially shorten the planetary mapping timeline.\n\n&amp;#x200B;\n\nWe haven't ran the full planet yet but hope to as soon as we work out an actual funding source. Besides posting for shameless self-promotion, I'm hoping to get help on a couple questions:\n\n1. Are there any good guidelines on a max input -----> image !!!  size for YOLO? And what parameters affect that? I imagine the number of image bands, number of classes, and feature complexity might be involved, but I haven't been able to find anything concrete. All the advice I've seen suggests slicing up large images with windowed reads and putting the puzzle pieces back together after prediction (which is what we've been doing so far).\n2. Does anyone know if there are any iterative improvements on YOLO coming down the pipe? \n3. Does anyone who has actually ported a YOLO model to TF or PyTorch have tips on doing so? I've tried [one implementation](https://github.com/ayooshkathuria/pytorch-yolo-v3) and didn't have good luck. It might have just been a case of user-error, but wanted to see if anyone has had success before trying out more options.", "link": "https://www.reddit.com/r/MachineLearning/comments/a69zmq/p_building_a_pipeline_to_map_the_red_planet/"}, {"autor": "temptempyahoo", "date": "2018-05-13 20:27:27", "content": "I have 10M screenshots from \\~1000 office applications \\(forms, text boxes, etc\\). \n\nMy aim is to:\n\n  \\- cluster the images by application \\(i.e. screenshots which share common background elements\\)\n\n  \\- within each group, decompose the variable foreground elements to text, icons, etc\n\nCurrently all my data is unlabeled. I can do labeling of individual elements, but NO LABELS of applications or foreground background.\n\n* What is the common approach this task ?\n* Is there a recommended pre\\-trained model for screenshots ?!\n\nI have some ML experience, but not so much with -----> image !!!  data. I have been reading up on CNN, VAE and YOLO. Any help appreciated. \n\nAll the tasks are offline so CPU is not an issue. ", "link": "https://www.reddit.com/r/MachineLearning/comments/8j6p8n/decomposing_multiple_screenshots_to_fixed/"}, {"autor": "hasime", "date": "2018-05-13 17:43:17", "content": "These below are the results i.e. 68 facial landmarks that you get on applying the DLib's Facial Landmarks model that can be found [here](http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2).\n\nIt mentions in this script that the models was trained on the on the [iBUG 300-W](https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/) face landmark dataset.\n\nNow, I wish to create a similar model for mapping the hand's landmarks. I have the hand dataset [here](https://www.mutah.edu.jo/biometrix/hand-images-databases.html).\n\nWhat I don't get is:\n1.  How am I supposed to train the model on those positions? Would I have to manually mark each joint in every single -----> image !!!  or is there an optimized way for this?\n2. In DLib's model, each facial landmark position has a particular value for e.g., the right eyebrows are 22, 23, 24, 25, 26 respectively. At what point would they have been given those values?\n3. Would training those images on DLib's shape predictor training script suffice or would I have to train the model on other frameworks (like Tensorflow + Keras) too?", "link": "https://www.reddit.com/r/MachineLearning/comments/8j5i3c/d_training_a_model_to_achieve_dlibs_facial/"}, {"autor": "sarasotadude", "date": "2018-05-13 04:50:43", "content": "Hi,\n\nI'm working on a CNN -----> image !!!  classifier and am looking for alternative methods in place of predictions = tf.nn.softmax\\(logits\\) for making predictions \\- all ideas and thoughts appreciated!\n\nAre there any alternatives that accept logits in the manner the softmax does? I'm trying to make a comparison of multiple functions used for prediction.", "link": "https://www.reddit.com/r/MachineLearning/comments/8j1o8t/p_alternatives_to_softmax_for_prediction/"}, {"autor": "bigpotatoman1", "date": "2018-05-12 10:15:04", "content": "I did my undergrad thesis on machine learning / computer vision so know a little bit about it all, but this was some time ago. Something that's been on my mind lately is:\n\nSay I have access to a catalogue of photos of specific clothing items (and I suppose for now, say I could have constructed the catalogue of photos myself from items physically possessed, and so each item in the catalogue could have photos taken at many different angles, lightings, backgrounds, etc.)\n\nI also own two t-shirts, one that is in the catalogue, and one that isn't. Given a couple of photos of the t-shirts taken by myself (but not the exact same photograph as any that are in the catalogue), I think it would be interesting to try and have some type of model which says \"yep that's in the catalogue, item 5\" or \"nope, that isn't in the catalogue\". \n\nSo this is kind of an object detection (to identify where the clothing item is in the -----> image !!! ) and then classification problem (matching the located clothing item in the -----> image !!!  to a catalogue product) right? I've not really come across anything similar that is capable of saying \"no this isn't a match\" - how is that sort of thing generally achieved? Some sort of confidence rating attached to the classification and if it's low enough then that equals not a match?\n \nI would really appreciate someone with a bit more knowledge to help me direct my research into the right areas - any key words to search for, links to papers, relevant musings or examples of something broadly similar - I know there must be! \n", "link": "https://www.reddit.com/r/MachineLearning/comments/8iv5qh/d_matching_photos_of_an_item_to_a_catalogue_of/"}, {"autor": "Roots91", "date": "2018-05-12 03:38:53", "content": "The project is in limited preview stage - It detects the object class, class probabilities, four coordinates of each bounding box and most dominant colours in hex format. I would really appreciate if you could use the service and give some feedback.\n\nYou can use 'test' credentials to test the API:\n\n    curl -u demo:tango -F \"file=@&lt;your------> image !!! -path&gt;\" https://d9c19fe8.ngrok.io\n\nFuture Updates:\n*Train your own model on the cloud.\n*Face Detection - Detect multiple faces in an image.\n*Explicit Content Detection - Detect explicit content in an image.\n*Use API services using your username &amp; password.\n\n", "link": "https://www.reddit.com/r/MachineLearning/comments/8itjak/p_rest_api_v02_for_object_detectionclassification/"}, {"autor": "tilenkranjc", "date": "2018-04-25 15:00:00", "content": "I recently started to dig deep into -----> image !!!  analysis with neural networks and one of the projects I'm on right now is a prediction of the bone fracture risk in osteoporosis by looking at the X-ray -----> image !!! . This is not really used in medicine to predict fracture risk, so I would say it is quite impossible for a radiologist to assess the risk by just interpreting the X-ray. My first results in this project show the same - image analysis does no better prediction by looking at regular risk factors (alcohol intake, smoking, body weight, certain drugs, age, etc). I need to also mention, that such X-ray images are used to assess bone mineral density, which somehow can be used to predict bone fracture, although not perfectly. Bone mineral density is calculated as the average pixel intensity in a certain region of the bone.\nSo, the question is - can a convnet see things that humans cannot? Can it be trained to see things that are impossible to interpret by humans? If yes, what would be the examples of it?\n\n(Note: when I talk about X-ray, I actually mean DXA imaging, which stands for Dual X-ray Absorptiometry. This is a subtype of X-ray imaging, used to assess bone mineral density.)", "link": "https://www.reddit.com/r/MachineLearning/comments/8eu7cg/d_can_a_convnet_see_things_that_humans_cannot/"}, {"autor": "StackBPoppin", "date": "2018-10-11 19:41:39", "content": "I successfully implemented a GAN which learned to generate photos from a batch of images which were 200x113 pixels in size.\n\nI'm now attempting to achieve the same with a DCGAN, this time the images have been cropped to 200x112 and I'm only working with greyscale values. \n\nMy architecture is roughly:\n\nFor the generator: A dense layer of (14 \\* 25 \\* 512) -&gt; Reshaped to \\[-1, 14, 25, 512\\] -&gt; conv2d\\_transpose(filters=256) -&gt; conv2d\\_transpose(filters=128) -&gt; conv2\\_transpose(filters=1)\n\nFor the discriminator: conv2d(filters=64) -&gt; conv2d(filters=128) -&gt; conv2d(filters=256) -&gt; flatten -&gt; Dense layer (512) -&gt; Dense layer(1)\n\nFor each convolution/transposed convolution I use a stride of 2 and a kernel size of 2. This way the dimensions repeatedly double from the original 14x25 to the -----> image !!!  size of 112x200.\n\nI've tried different activation functions (ReLU, Leaky ReLU, sigmoid, tanh) but I keep getting one of two results: the discriminator score plummets and the generator output doesn't change, or the generator keeps outputting a blank image with a single dark column at the right of the image.\n\nI've been reading conflicting information on which activation functions to use, which layers, what the kernel and stride should be. I've tried all sorts without any results.\n\nWhich layers and activation functions would you recommend for generating greyscale images with a resolution of 200 (width) and 112 (height)?\n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/9nd6wt/confused_by_dcgan_architecture/"}, {"autor": "clutchking_asiimov", "date": "2018-10-11 09:57:26", "content": "Currently a junior year business student with Finance as my specialization. I need to submit a research paper by the end of my senior year according to the undergraduate course outline. I have been planning to use Machine Learning for my research and to find its applicability and utility from a business/managerial perspective. Here are some of the topics I have thought for now:\n\n1. Sentiment analysis to determine consumer behavior and analyze customer trends\n\n2. Determining the best learning method to detect frauds in commercial transactions\n\n3. Forecasting stock market movements using neural networks/genetic algorithms\n\n4. -----> Image !!!  recognition to catch faulty products on production line belts.\n\nDo pardon me if the topics I thought of dont sound as catchy as they should since I am not so well versed in making my topics appear more ML oriented. Would love to get more research topic recommendations, or advice about my planned research topics in general.", "link": "https://www.reddit.com/r/MachineLearning/comments/9n8jl6/d_what_research_topics_can_i_work_on_which_use/"}, {"autor": "Alex0789", "date": "2018-09-18 19:38:06", "content": "Over the past decade, the rise of new technologies, such as the Internet of Things and associated interfaces, have dramatically increased the attack surface of consumers and critical infrastructure networks. New threats are being discovered on a daily basis making it harder for current solutions to cope with the large amount of data to analyse. Numerous machine learning algorithms have found their ways in the field of cyber-security in order to identify new and unknown malware, improve intrusion detection systems, enhance spam detection, or prevent software exploit to execute.\n\nWhile these applications of machine learning algorithms have been proven beneficial for the cyber-security industry, they have also highlighted a number of shortcomings, such as the lack of datasets, the inability to learn from small datasets, the cost of the architecture, to name a few. On the other hand, new and emerging algorithms, such as Deep Learning, One-shot Learning, Continuous Learning and Generative Adversarial Networks, have been successfully applied to solve natural language processing, translation tasks, -----> image !!!  classification and even deep face recognition. It is therefore crucial to apply these new methods to cyber-security and measure the success of these less-traditional algorithms when applied to cyber-security.\n\n**This Special Issue on machine learning for cyber-security is aimed at industrial and academic researcher applying non-traditional methods to solve cyber-security problems.** ***The key areas of this Special Issue include, but are not limited to*****:**\n\nGenerative Adversarial Models; One-shot Learning; Continuous Learning; Challenges of Machine Learning for Cyber Security; Strength and Shortcomings of Machine Learning for Cyber-Security; Graph Representation Learning; Scalable Machine Learning for Cyber Security; Neural Graph Learning; Machine Learning Threat Intelligence; Ethics of Machine Learning for Cyber Security Applications, blue &amp; read team.\n\n&amp;#x200B;\n\n[Link to MDPI Information Journal](http://www.mdpi.com/journal/information/special_issues/ML_Cybersecurity) \n\n&amp;#x200B;\n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/9gy0jj/n_call_for_papers_special_issue_on_machine/"}, {"autor": "drsxr", "date": "2018-05-19 21:47:12", "content": "So I have pulled Bankos and Brill\u2019s [\u201cScaling to Very Very Large Corpora for Natural Language Disambiguation\u201d ](http://www.aclweb.org/anthology/P01-1005)which studies the effects of data size on machine learning for natural language disambiguation.  It has a very nice **figure 1** where it shows that as the # of words scales from a million to the thousands of millions, there is a increasing rate of test accuracy.\n\nI am looking for the same thing but in images.  Has anyone come across a good reference?  Does one exist in deep learning for -----> image !!!  classification?\n\nI am looking for the equivalent of figure 1 in Bankos and Brill.  I think I saw it before once, but its been a while and I can\u2019t recall where.   Someone must have done this on CIFAR and ImageNet.\n\nDoes anyone know the reference I am looking for?  I\u2019ve searched myself with no luck.  Thanks!", "link": "https://www.reddit.com/r/MachineLearning/comments/8kocr7/d_relationship_between_deep_learning_network/"}, {"autor": "MessyML", "date": "2018-05-18 23:14:02", "content": "Hey guys,\n\nI am aware of several awesome Python libraries that allow me to do -----> image !!!  manipulation, cropping, etc. in to prepare my data for training input to Neural Nets.\n\nAre you aware of nice looking tools with a graphical interface specifically for the purpose of data preprocessing \\(resize, crop, brightness\\) and manipulation \\(say, assign classes to multiple images easily\\)?\n\nI know every problem is unique and everybody has its own setup, but are there tools out there that do this well?", "link": "https://www.reddit.com/r/MachineLearning/comments/8khbuz/d_gui_tools_for_data_manipulation_for_machine/"}, {"autor": "MessyML", "date": "2018-05-18 22:29:09", "content": "Hey guys,\n\nI am aware of several awesome **Python libraries that allow me to do -----> image !!!  manipulation**, cropping, etc. in to prepare my data for training input to Neural Nets.\n\nAre you aware of nice looking tools with a **graphical interface** specifically for the purpose of data **preprocessing \\(resize, crop, brightness\\)** and **manipulation \\(say, assign classes to multiple images easily\\)**?\n\nI know every problem is unique and everybody has its own setup, but are there tools out there that do this well?", "link": "https://www.reddit.com/r/MachineLearning/comments/8kh1wz/gui_tools_for_data_manipulation_for_machine/"}, {"autor": "Dark_Daiver", "date": "2018-08-03 05:21:04", "content": "First of all I'm sorry for my poor English.\n\nI'm trying to find edges of the specific object. Unfortunately i cannot solve this problem by semantic segmentation because i need to distinguish two sides of object from each other. \n\nI tried to solve this problem by rasterizing borders to -----> image !!!  and performing pixelwise classification (my loss is binary entropy) using such -----> image !!!  as target. It works pretty well but i failed to extend such approach to high-resolution images - my neural network overfits fast even with heavy augmentations.\n\nSo i have two questions\n\n1) What is SOTA for deep learning based edge detection? Of course i tried to use google but i cannot find anything modern.\n\n2) I think that binary entropy is pretty bad loss for edges. For example assume you border is one pixel-width horizontal line. if model give you border equal to target border shifted by one pixel up, binary entropy give you same error as if predicted border was shifted by 1000 pixels. So what kind of losses can be used for this case?", "link": "https://www.reddit.com/r/MachineLearning/comments/946v0k/d_loss_function_for_deep_learning_based_edge/"}, {"autor": "MyMomSaysImHot", "date": "2018-11-01 19:08:21", "content": " \n\nHello! I\u2019ve just launched the public repo for this project I\u2019ve been obsessing over for the last two months called **DeOldify**. Yes the name\u2019s stupid but the tech I think is really cool.\n\n&amp;#x200B;\n\nhttps://i.redd.it/ere5u8ytprv11.png\n\nGist is this:\n\nThis is a deep learning based model. More specifically, what I\u2019ve done is combined the following approaches:\n\n* **Self-Attention Generative Adversarial Network** ([https://arxiv.org/abs/1805.08318](https://arxiv.org/abs/1805.08318)) . Except the generator is a **pretrained Unet** , and I\u2019ve just modified it to have the spectral normalization and self attention. It\u2019s a pretty straightforward translation. I\u2019ll tell you what though- it made all the difference when I switched to this after trying desperately to get a Wasserstein GAN version to work. I liked the theory of Wasserstein GANs but it just didn\u2019t pan out in practice. But I\u2019m in *love* with Self-Attention GANs.\n* Training structure inspired by (but not the same as) **Progressive Growing of GANs**([https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)). The difference here is the number of layers remain constant- I just changed the size of the input progressively and adjusted learning rates to make sure that the transitions between sizes happened successfully. It seems to have the same basic end result- training is faster, stable, and generalizes better.\n* **Two Time-Scale Update Rule** ([https://arxiv.org/abs/1706.08500](https://arxiv.org/abs/1706.08500)). This is also very straightforward- it\u2019s just one to one generator/critic iterations and higher critic learning rate.\n* **Generator Loss** is two parts: One is a basic Perceptual Loss (or Feature Loss) based on VGG16- this basically just biases the generator model to replicate the input -----> image !!! . The second of course is the loss score from the critic. For the curious- Perceptual Loss isn\u2019t sufficient by itself to produce good results. It tends to just encourage a bunch of brown/green/blue- you know, cheating to the test, basically, which neural networks are really good at doing! Key thing to realize here is that GANs essentially are learning the loss function for you- which is really one big step closer to toward the ideal that we\u2019re shooting for in machine learning. And of course you generally get much better results when you get the machine to learn something you were previously hand coding. That\u2019s certainly the case here.\n\n[https://github.com/jantic/DeOldify/blob/master/README.md](https://github.com/jantic/DeOldify/blob/master/README.md)", "link": "https://www.reddit.com/r/MachineLearning/comments/9tcfls/p_introducing_deoldify_a_progressive/"}, {"autor": "numpee", "date": "2018-10-31 14:24:55", "content": "So I've read a couple papers on the topic of continual learning (most notably [EWC](https://arxiv.org/abs/1612.00796), and [GEM](https://arxiv.org/abs/1706.08840)), but it seems that most papers only deal with small -----> image !!!  datasets, such as MNIST variants, or in the case of GEM, incremental CIFAR100. I'm curious if anybody has tried something like EWC or GEM on larger image datasets, and if so, if the methods can achieve similar results on larger images. \n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/9sz7cr/d_continual_learning_on_large_images/"}, {"autor": "born_in_cyberspace", "date": "2018-10-31 13:16:19", "content": " I'm trying to reverse-engineer a huge neural network. The problem is, it's essentially a blackbox. The creator has left no documentation, and the code is obfuscated to hell.    \n\nSome facts that I've managed to learn about the network:\n\n* it's a recurrent neural network\n* it's huge: about 10\\^11 neurons and about 10\\^14 weights\n* it takes 8K Ultra HD video (60 fps) as the input, and generates text as the output (100 bytes per second on average)\n* it can do some -----> image !!!  recognition and natural language processing, among other things\n\nI have the following experimental setup:\n\n* the network is functioning about 16 hours per day\n* I can give it specific inputs and observe the outputs\n* I can record the inputs and outputs (already collected several years of it)\n\nAssuming that we have Google-scale computational resources, is it theoretically possible to successfully reverse-engineer the network? (meaning, we can create network that will produce similar outputs giving the same inputs) .\n\nHow many years of the input/output records do we need to do it?", "link": "https://www.reddit.com/r/MachineLearning/comments/9symfk/d_reverseengineering_a_massive_neural_network/"}, {"autor": "ILikeCheapWater", "date": "2018-09-26 16:47:12", "content": "Particularly, I'm interested in how the Samsung Galaxy S9's face unlocking feature works. I guess it uses some form of face recognition and spoof detection (to help against images, videos, masks). Do you guys know of any resource that describes how face unlocking on modern phones work? (the more detailed the better)\n\nI'm asking here because problems such as face recognition seem to have public state of the art solutions. I guess that a lot of companies adopt such public solutions for a feature such as face unlock. I'm interested to learn more about how exactly they work, I mean phones such as the Galaxy S9, the Galaxy Note 8, the OnePlus 6, the Asus ZenFone 5Z, and [other modern phones](http://blog.awok.com/10-best-face-recognition-phones-2018-face-unlock/) that use only the front -----> camera !!!  for the face unlocking feature (everything except the iPhone X).", "link": "https://www.reddit.com/r/MachineLearning/comments/9j48iw/how_does_face_unlocking_on_modern_phones_work/"}, {"autor": "RiceTuna", "date": "2018-09-26 14:42:20", "content": "Hi!\n\nI'm looking to start doing some experiments with video data and am looking at hardware options.\n\n&amp;#x200B;\n\nLooks like a lot of projects out there use their laptop's -----> camera !!!  - but are there better options out there in terms of hardware?\n\n&amp;#x200B;\n\nAlso, have any of you played with Amazon's DeepLens -----> camera !!! ? [https://aws.amazon.com/deeplens/](https://aws.amazon.com/deeplens/)\n\nThe idea of running the model directly on the hardware sounds really cool - but not clear how flexible the interface with AWS is.", "link": "https://www.reddit.com/r/MachineLearning/comments/9j33if/d_hardware_cameras_for_ml_video_applications/"}, {"autor": "Frixoe", "date": "2018-09-25 11:40:46", "content": "Please answer the following questions using the simplest words as I am new to Deep Learning:\n\n1. What is latent modeling? what are latent variables? why are they called latent variables?\n2. What are \"priors\" and \"complex priors?\"\n3. What are distributed latents?\n4. What is good epoch range to train GANs for -----> image !!!  generation and how does it vary when using a simple MNIST dataset and when using a more complex dataset like cifar or -----> image !!! net?", "link": "https://www.reddit.com/r/MachineLearning/comments/9irdoz/many_questions_regarding_vaes_and_gans/"}, {"autor": "svantana", "date": "2018-11-09 09:12:52", "content": "I'm trying to draw an -----> image !!!  with lots of small textures in it, sort of like a particle renderer, and then backprop this to the -----> image !!!  coordinates and texture pixels. I've implemented this naively in both TF and pyTorch, i.e. looping over each sub-image and adding it to only the affected sub-tensor output, it works but with abysmal performance on both cpu and gpu. I suppose it needs to be parallelized to be efficient, but I can't figure out how to do that without updating each out pixel for each sub-image, which is also very inefficient.\n\nCurrently I'm looking at julia and even c++ to get decent performance, but it's frustrating because this is basically the raison d'etre for gpus (drawing small-ish things in big images), yet I can't find a decent way of doing it outside of the fixed pipeline, which can't be used for backprop AFAIK.\n\nI suppose this is a special case of a bigger problem in current tensor libraries, that they're not designed to do lots of ops on local patches of big tensors. Any advice is highly appreciated!", "link": "https://www.reddit.com/r/MachineLearning/comments/9vizv7/d_how_to_do_standard_graphics_with_backprop/"}, {"autor": "bkmnsk", "date": "2018-11-08 21:11:23", "content": "Hi there!\n\nOur team make a tool for developers, based on computer vision algorithms, that helps to extract useful information from video stream in real-time and integrate it into apps without expertise in machine learning.\n\nThere are three simple steps to get started. Install the program on your computer, connect the -----> camera !!!  and choose what you want to see. That's it. Live video stream from camera will be processed locally on your computer by AI models. Result will be time series data in JSON format, which developer can integrate into his application.\n\nAs for now the tool is on the development stage. You can see a demonstration of the tool on the [website](https://heyml.com/vision).\nWe need your opinion and recommendations for improvement. We are open to discussion :)", "link": "https://www.reddit.com/r/MachineLearning/comments/9vdzrj/p_cv_tool_for_turning_video_stream_from_camera/"}, {"autor": "teclics", "date": "2018-10-25 11:59:07", "content": "Given an -----> image !!!  classification task with alot (1000000+) of unlabelled -----> image !!! s and only a few labelled examples (10000), would it be possible to apply a transfer learning approach combined with a GAN?\n\nTypically, GANs are considered to be unstable to train. Using a pretrained network (as a discriminator) such as the inception nets by Google would be troublesome because it would instantly detect an image to be fake, right? This would make it even more difficult te train?\n\nHad any work been done in this area of research?\n\n", "link": "https://www.reddit.com/r/MachineLearning/comments/9r9m6u/discussion_transfer_learning_and_gans/"}, {"autor": "Davoc8", "date": "2018-10-24 15:54:07", "content": "The goal of the project is a raspberri pi that can detect certain defects in a product that is moving past the Raspberri Pi. If a defect is detected, a signal is sent to a PLC.\nI want to get to the point where the Pi sends a signal whenever a defective product is detected.\nFollowing are my somehow unordered thoughts as I never did something like this before.\n\n\n**Task 1:**\n\nTake a -----> picture !!!  whenever a product passes a certain lightbarrier -&gt; this image will then be evaluated. If a product is detected, check if the product is \"good\" or has a defective.\nFor this I have to first train a model that detects a product in an image. I'd use a trigger signal to take a picture whenever a product passes some sensor. Images will always be from the same angle and camera position.\nOnce I have enough images that contain a product, I'd take some pictures from the same angle and position, just without any product.\nWith this set of images I'd train a model which should then be able to say whether an image contains a prodcut or not.\nPictures should be taken with a webcam connected to the Raspberry Pi and be sent to my computer through WiFi.\nTraining the model would then happen on my computer.\n\n\n**Task 2:**\n\n If a product is detected, check for abnormalities compared to a \"good\" product.\nFor this I am somewhat uncertain. Should I do this similar to task 1, but just manually take pictures of defective products? Or is it possible to detect defects, just based on the image data from good products?\n\nThere is one minor detail that should be considered: I know very little about computer vision and just started studying Data Science as a second Masters Degree.\nI know a little Python and can work to wrap my head around new technologies.\n\nWhat are your thoughts on this project? Is this in general a doable approach? What am I missing? Do you need more background information?\nI want to get this up and running as fast as possible, so I'm thinking about just buying a Raspberry Pi and downloading Tensorflow on my computer . From there I'd go through some TensorFlow tutorials and then start working on Task 1.\n\nSo what do you think? What things are important? What should I work on first?\n", "link": "https://www.reddit.com/r/MachineLearning/comments/9r0vxe/p_project_idea_for_defect_detection_in_a_product/"}, {"autor": "KJ-16", "date": "2018-10-23 20:10:51", "content": "Hi, I am an research assistant at the engineering department of my university, with a strong focus on factory planning, production management and quality management.\n\nReading different use cases of ML and its benefits, I would like to start with my own projects (unfortunately my collegues stick to known principles and methods), but I do not really know how begin. \n\nMain problem is the huge amount of software, books and papers about ML and the feeling of uncertainty without starting point.\n\nTherefore I would like to know:\n\nWhat book should I start with?\n- The Elements of Statistical Learning (Hastie et al)\n- Pattern Recognition and Machine Learning (Bishop)\n- others?\n\nWhich Software should I use (Windows)\n- TensorFlow\n- Anaconda with SciKit Learn\n- RapidMiner\n- others?\n\nAims : -----> Image !!!  recognition/classification and output prediction based on given inputs with artificial neural networks.\n\nI hope you can give me some information on how to start, recommandations about must-reads and common mistakes of beginners!", "link": "https://www.reddit.com/r/MachineLearning/comments/9qsmtc/d_get_started/"}, {"autor": "PLATINUMPETEDOG", "date": "2018-10-23 19:41:49", "content": "&lt;iframe src=\"\\\\\\[https://player.vimeo.com/video/296346787?byline=0&amp;portrait=0\\\\\\](https://player.vimeo.com/video/296346787?byline=0&amp;portrait=0)\" width=\"640\" height=\"360\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;    \n\n\n&lt;p&gt;&lt;a href=\"\\\\\\[https://vimeo.com/296346787\\\\\\](https://vimeo.com/296346787)\"&gt;Progress Bar&lt;/a&gt; from &lt;a href=\"\\\\\\[https://vimeo.com/petermccoubrey\\\\\\](https://vimeo.com/petermccoubrey)\"&gt;Peter McCoubrey&lt;/a&gt; on &lt;a href=\"\\\\\\[https://vimeo.com\\\\\\](https://vimeo.com)\"&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;    \n\n\n&lt;p&gt;In a near future, a woman navigates a virtual customer service call via her own avatar and digital assistant. &lt;br /&gt;    \n\n\n&lt;br /&gt;    \n\n\nThis -----> film !!!  was written and directed by a human, however, LEXI\\&amp;#039;s dialogue was generated by RivetAI, an artificial intelligence program.&lt;br /&gt;\n\n&lt;br /&gt;    \n\n\nFrom the producers of 2016\\&amp;#039;s \\&amp;quot;A.I. scripted\\&amp;quot; viral sensation, SUNSPRING:  [https://youtu.be/LY7x2Ihqjmc](https://youtu.be/LY7x2Ihqjmc)&lt;br /&gt;\n\n&lt;br /&gt;    \n\n\nPROGRESS BAR is an experiment that attempts to harness the inherent absurdity of dialogue\u200b generated by A.I. (as seen in SUNSPRING) by placing it within the confines of a more traditional (human-written)\u200b narrative film structure. &lt;br /&gt;\n\n&lt;br /&gt;    \n\n\nStarring Jennifer Kim (Search Party, Mozart in the Jungle) Lucy Walters (Get Shorty, Power) and Kevin Breznahan (The Deuce, Billions)&lt;br /&gt;\n\n&lt;br /&gt;    \n\n\n[www.rivetai.com](https://www.rivetai.com)&lt;br /&gt;\n\n[www.endcue.com](https://www.endcue.com)&lt;br /&gt;\n\n&lt;br /&gt;    \n\n\nWritten \\&amp;amp; Directed by Peter McCoubrey&lt;br /&gt;\n\nLEXI dialogue\u200b generated by RivetAI&lt;br /&gt;\n\nDirector of Photography Luke McCoubrey&lt;br /&gt;\n\nProduced by Andrew Kortschak, Walter Kortschak, Debajyoti Ray&lt;br /&gt;\n\nCo-Producer Sadaf Amouzegar&lt;br /&gt;\n\nProduced by Emily Wiedemann, Chazz Carfora, Jennifer Sharpe&lt;br /&gt;\n\nComposer Ron Patane&lt;br /&gt;\n\nProduction Designer Marta Castaing&lt;br /&gt;\n\nWardrobe Jackie McCoubrey&lt;br /&gt;\n\nProduction Company Greencard Pictures / End Cue&lt;br /&gt;\n\n&lt;br /&gt;    \n\n\n&lt;br /&gt;    \n\n\nRAE - Jennifer Kim&lt;br /&gt;\n\nLEXI - Lucy Walters&lt;br /&gt;\n\nREP/AVATAR - Kevin Breznahan&lt;/p&gt;", "link": "https://www.reddit.com/r/MachineLearning/comments/9qsdah/progress_bar_a_short_film_cowritten_by_rivetai/"}, {"autor": "CSGOvelocity", "date": "2018-10-23 19:23:50", "content": "I am planning to train the YOLO model on a custom dataset of images of 13 people of significance.\n\nWhat I want to know is:\n\n1. How many images it might need per person ? Currently, I think it needs about 400-450 per person seeing the PASCAL VOC dataset.\n2. Can some -----> image !!! s be redundant or does every -----> image !!!  need to be unique in some way ?\n3. Should I do some partial labelling of faces as in label only the top half / bottom half of the face in some images ? \n4. Should I keep drawings of that person in the dataset ?\n\nI already know how to label and format the labelling for images using the BBox Label tool.", "link": "https://www.reddit.com/r/MachineLearning/comments/9qs7ee/p_training_the_yolo_model_to_recognize_person_of/"}, {"autor": "arc144", "date": "2018-09-06 18:57:46", "content": "Hi folks, I just read the [PSPNet paper](https://arxiv.org/pdf/1612.01105.pdf) and there is something I don't understand; it would be nice if someone could help me here.\n\nFrom the paper, we have the attached -----> image !!! . What I don't understand is where this module is applied; is it on all CNN layers or just the last?\n\nFrom the paper it seems that it is applied only on the last layer of the CNN backbone, which is based on dilated convolutions. But from my understanding if it is based on dilated convs, the final feature maps would have the same spatial size as the input but in the paper it says it is 1/8th of the input's size. I'm quite lost here, any help is welcomed.\n\nFurthermore, if I use a U-net based architecture (encoder-decoder, using pooling instead of dilated convs), where would I add this PSP module? Just in the transition layer (transition from encoder to decoder)?\n\nThanks!\n\n&amp;#x200B;\n\n*Processing img 3cp4zjgt0ok11...*", "link": "https://www.reddit.com/r/MachineLearning/comments/9dlsxj/d_doubts_in_pspnet_paper/"}, {"autor": "Kristery", "date": "2018-09-06 17:58:08", "content": "We present a novel and unified deep learning framework which is capable of learning domain-invariant representation from data across multiple domains. Realized by adversarial training with additional ability to exploit domain-specific information, the proposed network is able to perform continuous cross-domain -----> image !!!  translation and manipulation, and produces desirable output -----> image !!! s accordingly. In addition, the resulting feature representation exhibits superior performance of unsupervised domain adaptation, which also verifies the effectiveness of the proposed model in learning disentangled features for describing cross-domain data.", "link": "https://www.reddit.com/r/MachineLearning/comments/9dl723/r_nips_2018_a_unified_feature_disentangler_for/"}, {"autor": "iliauk", "date": "2018-08-30 16:55:30", "content": "Considering cases such as empty spaces in parking lots, empty shelves in a store, empty parts of the road (not necessarily just background class)\n\nWould it make sense to try and do traditional object-detection with the free-space being the bounding-box to predict? Or would it be better to instead predict the main boundary around the objects, identify the objects and then subtract (e.g. this is a car-park in the -----> photo !!! , these are the cars so car-park - cars = empty parking spots).\n\nWith the latter it seems you can avoid having to label anything just by chopping a pre-trained two-stage detection model after RPN and just using the region proposals. However you have the issue that not all empty space is free space (e.g. if a car or motorbike can't fit here it's not empty car-park space)\n\nI was curious in general if there has been any work done on cases like detecting empty spaces on store-shelves.", "link": "https://www.reddit.com/r/MachineLearning/comments/9bldbz/d_freespace_object_detection/"}, {"autor": "akanimax", "date": "2018-08-30 12:49:32", "content": "In continuation to my previous post about MSG-GAN:\n\n\nI have been able to generate 1024 x 1024 resolution -----> image !!! s using my proposed MSG (Multi-Scale Gradients) - GAN architecture, refer the accompanying -----> image !!! . here -&gt; https://github.com/akanimax/MSG-GAN/blob/master/sourcecode/samples/Celeba/1/doing_%20Work/sample_sheet.jpeg \n\n\nCode used for this training has been modified to use the DataParallel feature for multiple GPUs (latest commits). find here -&gt; https://github.com/akanimax/MSG-GAN\n\n\nI have trained the network for 3 epochs till now; the training is still in progression. The trained models till this checkpoint have been made available here -&gt; https://drive.google.com/drive/folders/119n0CoMDGq2K1dnnGpOA3gOf4RwFAGFs\n\n\ncommand for reproduction:\n\n    $ python train.py --images_dir=[path to celeba directory] \\\n                               --sample_dir=[generated samples directory] \\\n                               --model_dir=[checkpoints dir] \\\n                               --depth=9 \\\n                               --latent_size=512 \\\n                               --batch_size=32 \\\n                               --g_lr=0.0003 \\\n                               --d_lr=0.0003 \\\n\n\nYou can train from scratch, or you can use the 3 epochs pretrained models as mentioned above.\n\n\nI perceive this experiment should validate the hypothesis that the network architecture can indeed be used to synthesise high resolution images.", "link": "https://www.reddit.com/r/MachineLearning/comments/9bjbzm/r_msg_multi_scale_gradients_gan_generates_1024_x/"}, {"autor": "philippbatura", "date": "2018-08-30 10:30:04", "content": "&amp;#x200B;\n\n[Example of how Chudo's technology works](https://i.redd.it/r365ha8qj7j11.gif)\n\n&amp;#x200B;\n\n[Another Example](https://i.redd.it/xf6etsovj7j11.gif)\n\n&amp;#x200B;\n\n[Another example](https://i.redd.it/lf7hqxezj7j11.gif)\n\nHey everyone! Our team has been working on the [app called Chudo](https://www.producthunt.com/posts/chudo), that uses machine learning to turn people into personalized characters. We've been building this for two years, collecting unique data, building proprietary machine learning algorithms for face recognition, emotions tracking. The characters are generated automatically by AI and look like animals (and even vegetables).\n\nMost importantly, the app works without the TrueDepth -----> camera !!!  and supports old devices like iPhone 5S and Android ones too. The app is cross-platform.\n\nWe have launched the app on Android (iOS Version is \"In Review\" status). We also started a [Product Hunt page](https://www.producthunt.com/posts/chudo).\n\nFeel free to check out the app and leave some fair feedback using the link: [https://www.producthunt.com/posts/chudo](https://www.producthunt.com/posts/chudo)", "link": "https://www.reddit.com/r/MachineLearning/comments/9bigbf/p_machine_learning_based_app_turns_users_into/"}, {"autor": "nottakumasato", "date": "2018-08-29 13:02:59", "content": "Hi,\n\nI just read this paper titled \"Focal Loss for Dense Object Detection\", found here: https://arxiv.org/abs/1708.02002\n\nThe authors show that this new method with the RetinaNet architecture can outperform the two-stage object detectors like Faster R-CNN in both accuracy and speed.\n\nI have some questions about the paper:\n\n1. Why was the hinge loss unstable? Is it because of the not differentiable region of the hinge loss function? Would Generalized Smooth Hinge loss have worked?\n\n2. How scalable is focal loss on the number of classes? RetineNet requires 9 filters for each class, so would the speed slow down inference drastically if the number of classes was very large?\n\n3. The paper talks about the \"hard example\", but I couldn't understand it completely. Could anyone give an -----> image !!!  region that is a hard example?\n\n4. Why cant the alpha-balanced cross entropy loss differentiate between easy and hard examples?\n\n5. Does this improvement in accuracy open up new applications for one-stage detectors?\n\nAny help is appreciated!", "link": "https://www.reddit.com/r/MachineLearning/comments/9b939u/d_focal_loss_for_dense_object_detection_retinanet/"}, {"autor": "zacharyYoon", "date": "2018-08-29 07:58:18", "content": "Hi there, \n\n&amp;#x200B;\n\nMy name is Zachary Yoon and I am a senior research engineer of Deepstudio Co Ltd. \n\nOur company mainly deals with deep learning theories and practice to generate **-----> photo !!! -realistic synthesized image and videos**. \n\nBelow is our company's recent research result where the left actor reenact the right person's expression and torso movement. \n\n![video](ji3pat10kzi11)\n\nRight now our company has a **open position** for a deep learning research engineer. \n\nOur company is located in Seoul, the capital city of South Korea.\n\nWe guarantee every employees being provided with the most advanced research and development environment such as unlimited AWS credits, free work-hours for given tasks and iMAC pro for every R&amp;D personnel. \n\n&amp;#x200B;\n\nQualification for this position is quite simple as following:\n\n1. At least 1 year of experience of dealing with deep learning framework- such as tensorflow or pytorch \n2. Clear understanding over the foundational theories of deep-learning  \n3. Github portfolio that can verify your competency over deep learning development process - such as data collection/pre-processing, data-feeding, network structuring and result-checking and feedback process upon acquired result through experiment. \n\nWe prefer a person who have an experience of reproducing a paper of which the corresponding codes is not provided. \n\n&amp;#x200B;\n\nHope you enjoy your day and if you have any question, just email me via [syyun@snu.ac.kr](mailto:syyun@snu.ac.kr) ! \n\n&amp;#x200B;", "link": "https://www.reddit.com/r/MachineLearning/comments/9b7a44/we_are_looking_for_a_deep_learning_engineer/"}, {"autor": "noofen", "date": "2018-05-03 16:25:00", "content": "I'm brand new to machine learning, but I'm trying to build something that recognizes objects in images. A lot of the algorithms I've researched like SSD and YOLO seem to be built for realtime (video).\n\nI'm curious if these algorithms sacrifice accuracy for speed (trying to hit that 30fps mark, for example). If I'm okay with 1s processing times to recognize all objects in an -----> image !!! , what is the best algorithm for that?", "link": "https://www.reddit.com/r/MachineLearning/comments/8grn22/d_object_detection_algorithm_where_speed_isnt/"}, {"autor": "filipequincas", "date": "2018-03-08 16:38:05", "content": "I've been studying GANs for some time now and I have seen many examples of it's applications on -----> Image !!!  Inpainting, -----> Image !!!  enhancing (Super resolution) problems etc., but I am yet to see a paper that leverages image inpainting/enhancing techniques using GANs to make \"better\" face recognition systems for example. Could someone recommend me a project/paper about that?", "link": "https://www.reddit.com/r/MachineLearning/comments/82yu9f/d_is_there_any_studypaper_that_uses_gans_to/"}, {"autor": "sleeppropagation", "date": "2018-03-02 08:04:06", "content": "I'm observing a weird behavior when training CNNs on CIFAR10/100 with Pytorch: https://imgur.com/a/cAFr0.\n\nThis happens with ResNets, DenseNets and even a vanilla VGG without batch-norm. I haven't experienced this while using data augmentation or decaying learning rates. The -----> image !!!  above is the training log of a VGG on CIFAR10, with 0.005 learning rate (SGD+Nesterov) and 0.0005 weight decay. The frequency of these \"cycles\" seem to be very dependent on the learning rate and weight decay, and they only happen at 100% training accuracy.\n\nAlso, I'm using the imagenet example from pytorch's repo (except I'm training on CIFAR instead). I have tried removing most of the code almost to the point of only having forward / zero_grad / backward / step, and this still happens. Tried other training scripts / repos, no luck. I'm guessing this is not a bug / pytorch-related issue, and probably a general issue on network optimization?\n\nHas anyone observed this before? This is a huge obstacle for a current project that I have, which involves collecting a lot of statistics regarding generalization and regularization to model network dynamics.", "link": "https://www.reddit.com/r/MachineLearning/comments/81cq0g/d_sudden_performance_drops_when_training_cnns/"}, {"autor": "Peng_Lu", "date": "2018-03-01 02:25:13", "content": "My codes can be found at [Deep------> Image !!! -Analogy-PyTorch](https://github.com/Ben-Louis/Deep------> Image !!! -Analogy-PyTorch). It achieved near outputs of [official code](https://github.com/msracver/Deep-Image-Analogy). \nBut there might be some detailed problems during attribute transfer. Welcome for any issue or pr!", "link": "https://www.reddit.com/r/MachineLearning/comments/811v57/p_pytorch_implementation_of_deepimageanalogy/"}, {"autor": "Cabrill", "date": "2018-04-13 17:45:27", "content": "#Intro:  \nHello, kind stranger.  I\u2019m a student at University of Helsinki in pursuit of my MSc in *Algorithms, Data Analysis and Machine Learning.*  For my thesis I\u2019ve created a unique story-based game that utilizes all three facets of my degree in novel ways.  The AI utilizes machine learning and the enemies have individual genetics as part of the gameplay, and to fully accomplish the \u2018Data Analysis\u2019 portion of my degree I need as many people to play the game as I can convince to give it a try.  The game is completely free and designed in such a way that anyone of any skill level can play it, because whether you win or lose a level the story will continue progressing on to the next level.   Skip to the bottom of my post to start downloading it from one of three indie game sites.\n  \n\n#Game:  \nThe game is a futuristic, story-based tower defense game that runs on the Windows and MacOS 64-bit platforms.  The premise is that it is the year 2238, and humans have left the destabilized Earth to terraform the nearest habitable planets.  You are Shepherd Cartalia, of planet B382, and have supervised the growth of your colony since its founding in 2148.  You have successfully increased your colony\u2019s population from meager beginnings to over a thousand citizens, but at the start of the game your sustenance and nutrient generator has begun to malfunction.  Rather than spawning livestock that is ready for slaughter, the animals now produced have become violent and are attacking your people.  It is now your responsibility to put up defenses against the attacks, while you evacuate your people from the outposts scattered across the land, and search for a lasting solution to the unexplained crisis.  \n  \n\n#Gameplay Overview:  \nThe game features ten levels, six tower types with three shared upgrades each, fifteen enemy types, a branching dialogue tree of over 2,500 interactions, and four different endings.  The primary gameplay mechanics are tower selection/placement/upgrading, and dialogue choice.  It will take an average of an hour to complete.  There\u2019s only a single, non-managed save slot but the game can be replayed with increasing difficulty as the machine learning model improves.  \n  \n\n#Development Overview:  \nI developed this game alone, starting in November of 2017.  The character art, attribute icons, and landscape objects were designed by Yuexin Du, a UI &amp; UX designer from Aalto University.  I used the Monogame framework, FlatRedBall game engine, Accord.Net for machine learning and genetic algorithm classes, Keen.IO for data collection, Sprite Pro for creating the animations, Articy Draft for creating the dialogue tree, Visual Studio 2017 IDE, Tiled for map design, GIMP for artwork creation, and Audacity for sound design.  Most of the music was created by Anttis Instrumentals (/u/Mrloop), a few of the tracks were created by my brother Brallit (/u/Brallit), and one was by Zack Darshon (/u/how_small_a_thought).  The basis for the sound effects were sourced from many CC0 sources on FreeSound.org.  The UI assets and tileset for levels were purchased from GameDevMarket.net.  The title screen images were found on Google -----> Image !!!  search using \u2018labeled for reuse with modification.\u2019  The base for the world map was purchased from the Unity marketplace, and lastly, the sun/moon calculations were performed using Vladimir Agafonkin\u2019s   \n  \n\n#Technical:  \nThe game utilizes a Deep Belief Network, with one hundred and fifty hidden nodes, and two layers (input/output).  It is trained at the completion of each wave with six hundred and fifty-five data points, comprised of enemy attributes (health, speed, resistances, etc), defense attributes (range, damage type, attack speed, etc.), pathing points, and water hazard size/locations.  Training is done with a mixture of unsupervised and supervised learning on the resulting score of a group of enemies, i.e. a wave.  Each wave is scored calculated by determining how close to the goal each enemy could get, with the total score being the average of all enemies individual scores.  The score is weighted such that if even one enemy reaches the goal that wave would outscore a wave in which every enemy got 99.9% of the way to the goal.    \n\nIn addition to machine learning, the game also implements genetic algorithms.  Each type of animal starts with a set of randomized genetics, consisting of eight chromosomes, that alter the animal\u2019s base attributes.  After every wave in the game the machine learning model is used to rank the fitness of the genetics, by hypothetically scoring an animal using those genetics in the immediate circumstances, and then a tournament ranking is applied to select the best specimens for the following generations with the ideal of progressively improving the genetics of an animal as the machine learning gains proficiency in evaluating fitness.    \n\nTo prevent these computationally intensive processes from impacting game performance, they are delegated to a low priority worker thread.  A new model training process only starts when the last one has completed, but each time the process is kicked off all the available data is used to train the models so the intermittent training won\u2019t impact model performance.  The game utilizes whichever model was most recently trained for all its decisions, and that model is replaced after each learning and evaluation process.  \n  \n#Data Collected:  \nFirst, I want to stress that none of the data is personally identifiable.  It is all anonymous and collected for the sole purpose of presenting my thesis.  None of it will be used commercially or for any use outside of writing my thesis.  Having said that, the following data is collected: machine learning model performance, genetic fitness improvement, dialogue choices, tower selection, ending type received, originating geolocation of the player, game launch count, and time played.  \n  \n#Objective:  \nI will be writing a 50+ page thesis on the usage of machine learning and genetic algorithms in games, and will be doing some data analysis on the anonymous statistics reported by the game.  Primarily I am looking at the effectiveness of the machine learning models and genetic algorithms.  I want to see at what sample size the machine learning model outperforms random guesses, and identify the point at which model performance stabilizes.  I am also going to perform a data analysis on the met data collected with no leading hypothesis, but simply make observations based on player generated data.  This may be what dialogue choices are favored by different parts of the world, what dialogue choices are most often chosen, and attempt to identify correlations between dialogue selection and tower selection.  \n  \n#Personal Note:  \nI started this project with the idea of applying machine learning and genetic algorithms to a game, and the theme that most strongly resonated with those concepts isn\u2019t one I\u2019m particularly fond of.  To be blunt, it\u2019s quite dark and a bit depressing.  I personally prefer to play, and make, games with happy, uplifting themes (shout out to Stardew Valley which is the inspiration for my other game, Let\u2019s Go Fish King!) and creating this game was entirely out of character for me.    \n  \n\nThere are still many features I wanted to add to this game, such as a technology tree and more play-reactive dialogue, but I don\u2019t believe I\u2019ll be returning to it this project after this thesis unless I get strong support for the concept.  Perhaps it\u2019s not a very good game, or perhaps my dislike for the theme I settled on has bled through, but regardless of the actual cause, I\u2019m just not a fan of how it turned out.  For that reason I\u2019d like to apologize to you ahead of time for asking you to play this quite depressing game for the sake of helping me with my thesis.  I can assure you my next game will be quite a bit brighter, cheerful and hopefully a lot more fun than this attempt.  Despite those misgivings, I truly hope you\u2019ll give it a play through in the interest of helping me with my thesis.  \n  \n[Small Album on Imgur](https://imgur.com/a/7XAOR)  \nYou can download The Abbattoir Intergrade for Mac/Windows at:  \n[Itch.IO](https://cabrill.itch.io/ai)  \n[GameJolt] (https://cabrill.gamejolt.io/abbattoirintergrade)  \n[Indiexpo]( https://www.indiexpo.net/en/games/the-abbattoir-intergrade)  ", "link": "https://www.reddit.com/r/MachineLearning/comments/8c13ir/p_game_utilizing_accordnet_framework_for_ai/"}, {"autor": "kiunthmo", "date": "2018-07-13 14:38:24", "content": "I'm having a problem finding an answer to how to create my desired CNN architecture. I'm still deciding on tensorflow or NN toolkit in matlab, but I can't find the answer I need to either. \n\nI basically want to create a network in which one layer will combine the outputs of two channels from two previous layers (4 inputs to the node). I'm first applying 2 convolutions to an -----> image !!! , then using sgn on both (though i'll be experimenting with different activation functions as well hopefully), then another layer which takes the outputs of both convolutions AND their respective sgns. The formula of this layer is as follows: (1+sgnC1).C1+(1+sgnC2).C2 where C1 and \n\nThe difficulty I'm having is finding examples which can create layers that use not just the previous layer but also the one before that. \n\nAny help would be great, thanks.   ", "link": "https://www.reddit.com/r/MachineLearning/comments/8yki4o/cnn_architecture_using_multiple_previous_layers/"}, {"autor": "alpha_quant", "date": "2018-06-21 14:08:00", "content": "Machine learning (ML) is one of the most promising areas of innovation that companies from all sectors are recently seeking to explore. Companies ranging from the manufacturing sector to the robotics and mechanical engineering sector are increasingly using Artificial Intelligence (AI) and ML.  \nBeyond that, the AI concept has extended its impact to the financial markets through machine learning. Over the last two decades, markets have become more dynamic and trading using ML algorithms is seemingly taking over from the traditional exchange-based trading. Hedge fund managers and traders alike are now focusing on developing programs that assist their daily trading business in an effort to increase returns. \n\n![img](qs69fxjq2d511)\n\n There is also a recent trend for banks and finance firms to ask for more data analysis skills and market knowledge. This is quite rational, since understanding the economics behind the data and the market is as important as developing complex technical solutions.  \nIt is also a fact that human beings cannot be excluded from trading. However, the size of data required for making a good trade are increasingly bigger. For this reason, it is inevitable that at some point in the near future machines will become increasingly prevalent over humans on this task. Machines have the ability to quickly analyze data, news and tweets, process earnings statements, scrape websites, and trade on all these instantaneously. This \u2018ability\u2019 will provide an invaluable tool for traders, fundamental analysts, equity long-short managers and macro investors.  \nNevertheless, humans still retain an advantage on seeing the big -----> picture !!! . Machines are still not very good at spotting market turning points and making forecasts involving human responses such as those of politicians and central bankers, or anticipating how markets are going to move. In addition to this, the data required for \u2018predicting\u2019 the markets are getting more and more complex.  \nThis is why data scientists are getting increasingly sought after nowadays. Before machine learning strategies can be implemented, data scientists and quantitative researchers need to acquire and analyze the data with the aim of deriving tradable positions and trends.  \nThis data analysis is extremely complex. Today\u2019s datasets are much bigger than yesterday\u2019s. They can include anything from data generated by individuals (social media posts, product reviews, announcements, search trends, etc.), to data generated by business processes (company data, commercial transactions, trades etc)  \nThese new forms of data need to be thoroughly analyzed before they can be used in a trading strategy. \n\n![img](hs1ct3lu2d511)\n\nMachine learning includes many concepts such as supervised learning, unsupervised learning and deep and reinforcement learning.  \nThe purpose of supervised learning is to establish a relationship between two datasets and to use one dataset to forecast the other.  \nThe purpose of unsupervised learning is to train an artificial intelligence algorithm using information that is neither classified nor labelled and allowing the algorithm to act on that information without guidance.  \nThe purpose of deep learning is to use multi-layered neural networks to analyze a trend, while reinforcement learning uses algorithms to explore and find the most profitable trading strategies.  \nAs a result, the skills required for a data scientist is actually the same as for any other quantitative researchers. Existing buy side and sell side quants with backgrounds in computer science, statistics, maths, financial engineering, econometrics and natural sciences are continuously moving into this new field of expertise.\n\nApart from Machine Learning skills, expertise in software development is also a useful asset. Most of the Machine Learning methods and libraries are already there (e.g. in Python): you just need to know how to apply the existing models and be confident to modify them if required.\n\nIn general, the use of ML-based trading systems has started to make trading easier and more profitable. While these systems are common among both large hedge funds and smaller startups, third party trading software developers have also provided a new way for individual traders to get in the game.  \nAll the above have changed the way traders are doing their every day jobs. We are living in an era where making money in the stock market is not only a question of how experienced you are, but also of how powerful your trading tool is.", "link": "https://www.reddit.com/r/MachineLearning/comments/8ss56l/machine_learning_in_finance/"}, {"autor": "Teen-Ninja-Turtle", "date": "2018-06-20 16:42:24", "content": "The possibilities are in -----> image !!!  processing or control engineering, or perhaps even something related to audio engineering seeing as I have an interest in linguistics as well (so maybe some natural language processing+audio circuits)\n\nMaybe there is something I'm missing.", "link": "https://www.reddit.com/r/MachineLearning/comments/8sjtit/d_what_ece_field_should_i_master_in_if_im_also/"}, {"autor": "itypewords", "date": "2018-06-20 05:02:43", "content": "I\u2019m not a software engineer and I only have a superficial understanding of how machine learning works. I\u2019m hoping one of you can help. I\u2019d like to install a thermal FLIR -----> camera !!!  above my bed, which is analyzed via machine learning to determine the position of 2 human bodies; statistically proving that one of them (my wife) takes up more bed surface real estate than the other (me) at any given time, with almost 100% certainty. Optional automatic nerf projectile triggered at subject when the agreed boundary of shared space is sufficiently violated. Thank you in advance, kind sirs.", "link": "https://www.reddit.com/r/MachineLearning/comments/8sfn3w/d_help_with_project_of_utmost_importance_for/"}, {"autor": "DeltruS", "date": "2018-06-20 02:18:08", "content": "Algorithm:\n\n1. Two or more randomly weighted neural networks receive the same input.\n\n2. If they disagree on the output, use backpropogation on the odd one[s] out until the output number is the same as the the other network[s]. \n\nThe goal is to have many different models agree with each-other. The output numbers being the same means the models agree on what it is seeing.  The outputs \"mirror\" each-other.  \n\nAfter many rounds of backprop, this generates thousands of \"numbers\" which do not behave as numbers, but rather symbols that represent models/experiences.  \n\nThe numbers could then be used as causal logic. Like 1.557 + 1.665 -&gt; 4.234. This would create a causal spaggetti code. These would represent something like dog + hungry expression -&gt; dog will seek food.  This is because the network experienced seeing a hungry dog, and then saw it eating food, so the logic center would make the causal connection.  \n\n\nIt could also see that dogs don't always have a hungry expression before they eat food. This would represent a weakened connection on some hypergraph. Or a \"!-&gt;\" relation.   Or even a statistical relation.  \n\n\nEventually, after the network is fully trained and has few disagreements on what it is experiencing, the numbers could be paired with words, representing learning language.  For example you could input a few pictures of animals running and the numbers which result could be paired with the word/tag \"running\".  \n\nAs the mirror networks get more and more trained similar \"objects/groups/archtypes\" from the outside world will group together in their numbers, creating Venn diagram like structures, which are each represented by symbols pairs.  For example, nouns, cats, frisbees.  Every group has the same causal behaviour.   \n\nIf trained networks disagree on what they are seeing, the causal logic center can step in. And the correct model is chosen.  \n\nThe best part of this is that there is no ego and no doer. Pure awareness(experiencer) and intelligence(thinker). No possibility for a robot uprising.  \n\nAt the start, the network might see two completely different things and experience two similar numbers, but the glorious thing is that those numbers don't represent anything yet. As the neural networks start to differentiate, the separate \"objects\" start to emerge. Similar output numbers would mean similar things.  For example numbers 1.002 to 1.009 could be different types of dogs. \n\nOnce you start labelling the output numbers, you can start labelling them with pictures that triggered the experiencer neurons. This completes the input-output loop. It allows for memory and lets something re-experience something by putting in the -----> image !!!  for analysis by the mirror networks.  \n\nThis explains why babies do not have memories. What neurons represent what experience are constantly in flux.   A picture that triggered one group of neurons early on would trigger another group of neurons later on.  \n\nBabies also do not learn basic concepts at the same time they learn words, they spend a very long time just trying to figure out what they are experiencing.  ", "link": "https://www.reddit.com/r/MachineLearning/comments/8senc9/d_mirror_networks_have_i_cracked_the_brain_code/"}, {"autor": "throwawayfarway2", "date": "2018-06-14 20:23:39", "content": "Given I can take a -----> picture !!!  of the rover at start time, then I want to track that guy, as I point the camera to him. I have read about OpenCV's KFC tracker (not available on my platform), but I was wondering what you guys can suggest ?", "link": "https://www.reddit.com/r/MachineLearning/comments/8r52mr/p_what_approach_you_would_suggest_to_track_a/"}, {"autor": "throwawayfarway2", "date": "2018-06-14 20:13:38", "content": "Given I can take a -----> picture !!!  of the rover at start time, then I want to track that guy, as I point the camera to him. I have read about OpenCV's KFC tracker (not available on my platform), but I was wondering what you guys can suggest ?", "link": "https://www.reddit.com/r/MachineLearning/comments/8r4zov/what_approach_you_would_suggest_to_track_a_small/"}, {"autor": "ashz8888", "date": "2018-06-05 11:25:03", "content": "One of the reasons, Capsule Net was invented was to overcome one of the problems with CNN, related to Pooling operation that throws away too much useful information. CapsNets solve this issue by introducing EM based routing, however they still employ convolutional layer as the primary layer, which naively scans the entire -----> image !!! . As an another alternative, I thought of using an attention based scanning with deep reinforcement learning and recurrent layers, which can give image recognition a human like vision. Later however, I found out that this idea has already been proposed by [V. Mnih](https://arxiv.org/abs/1406.6247) in 2014. Although a bit disappointed, I was thrilled to see the progress that have been made in this direction since then and how RL approaches are progressively making their way into mainstream machine learning tasks. The attention based scanning has been found suitable for fine\\-grained visual classification tasks with [RA\\-CNN](http://openaccess.thecvf.com/content_cvpr_2017/papers/Fu_Look_Closer_to_CVPR_2017_paper.pdf), [online tracking](http://openaccess.thecvf.com/content_ICCV_2017/papers/Supancic_Tracking_as_Online_ICCV_2017_paper.pdf), and also finding applications in tasks where parameters are non\\-differentiable and therefore cannot be learned directly via back\\-propagation, such as in \u201c[ReasoNet](https://arxiv.org/abs/1609.05284): Learning to Stop Reading\u201d. I believe with the increasing fusion of RL with DL, we will be able to tackle challenges ahead of deep learning, a lot sooner than expected. I would like to encourage others to share their perspective on the progress that's being made in this direction or about the challenges faced by these approaches.", "link": "https://www.reddit.com/r/MachineLearning/comments/8oq0q1/r_deep_reinforcement_learning_to_solve_mainstream/"}], "name": "Subreddit_MachineLearning_01_01_2018-30_12_2018"}
{"interestingcomments": [{"autor": "SamPusegaonkar", "date": "2020-03-13 12:55:16", "content": "Template matching uisng machine learning /!/ Hello,\n\nI'm trying to find associate some cropped -----> image !!! s with a bigger sized -----> image !!! . This is something similar to template matching however, the cropped images here are also rotated and streatched.\n\nSo far, I've tried QATM's approach, however, the accuracy is super low and  my 6GB VRAM always is full.\n\nHow do I proceed?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fhz7gj/template_matching_uisng_machine_learning/"}, {"autor": "Chroteus", "date": "2020-03-13 11:08:48", "content": "Calculating IoU for semantic segmentation /!/ Hello\n\nIn semantic segmentation, we calculate mIoU, or IoU averaged over classes. Now, I was wondering, let's say I have a cat class, but in one -----> image !!!  I don't have a cat. My model doesn't see a cat, and in labels there isn't a cat. So, intersectio/union would be 0/0.\n\nHow to handle this case?\n\nDo I just ignore this class altogether for this image?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fhxy92/calculating_iou_for_semantic_segmentation/"}, {"autor": "kittwo", "date": "2020-03-12 15:52:47", "content": "Need help with issuing actions or notifications after a model classifies an -----> image !!! . /!/ Hi!\n\nI am working on an anomaly detection and notifier system. In this project, I want to use my PC as a server, where a model would be classifying the CCTV footage inputs. I developed the model using keras and OpenCV.\n\nMy question is, is there any way to issue a notification to nearby android phones, assuming bluetooth connects them to the PC, if the model detects any anomaly? Can I do some integration with android studio, and develop some dedicated app?\n\nIt would be great to have some insight. It is a college project. And I am asked to make it useful.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fhig86/need_help_with_issuing_actions_or_notifications/"}, {"autor": "Character-Comb", "date": "2020-03-12 15:38:39", "content": "Yay! Google colab is working. In the run time menu, select run all ctrl f9. This is how I made a -----> picture !!!  of a sunflower, which is supposed to look like a cartoon, but kind of does not.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fhi81g/yay_google_colab_is_working_in_the_run_time_menu/"}, {"autor": "martin1285", "date": "2020-03-11 14:15:21", "content": "Cox Proportional Hazards Failing Schoenfeld Residuals? /!/ Hello,\n\n&amp;#x200B;\n\nI am currently working on a Survival Analysis problem utilizing Cox Proportional Hazards and I am facing some issues with violations of the proportional hazards assumption via Schoenfeld Residuals test (using lifelines in Python).\n\n&amp;#x200B;\n\nI do not have much experience in Survival Analysis, and have gone through the guide listed on lifelines in order to make edits to pass this test. I have set the threshold at 0.05.\n\n&amp;#x200B;\n\nFrom my understanding, we want p-values greater than this threshold to assure that there is no strong evidence for a relation between residuals and time. With your p values being greater than the threshold, it gives greater confidence that the model has correctly handled the relation between predictor and time so that there is nothing left for that relation for the residuals to spot. \n\n&amp;#x200B;\n\nNow I have tried almost all the methods in lifelines to address the violations of this assumption (binning -&gt; stratification, modifying the functionality form, etc), but there is always a variable that is violating the assumption, and the removal of that variable is causing the p values of other variables to decrease, which causes other variables to violate the assumption. I get the same message in the last -----> picture !!!  for all variables.\n\n&amp;#x200B;\n\nIn a scenario like this, how is the best way to proceed? Any help would be much appreciated!\n\nhttps://preview.redd.it/13md4lepx1m41.png?width=1916&amp;format=png&amp;auto=webp&amp;s=0a42bb0c5c29fa2d2b09aa1e0377adbdd226401a\n\nhttps://preview.redd.it/b815hlepx1m41.png?width=938&amp;format=png&amp;auto=webp&amp;s=394b5d42efbe448819f12c5b5d23ab0927414f40\n\nhttps://preview.redd.it/psin7lepx1m41.png?width=564&amp;format=png&amp;auto=webp&amp;s=5a55ac8549d362b35a4c5801f7d83353c8a93560\n\nhttps://preview.redd.it/5bp8jfepx1m41.png?width=1892&amp;format=png&amp;auto=webp&amp;s=5a62d09481eca51314400aa987fac9afafa49cf9", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fgxt5d/cox_proportional_hazards_failing_schoenfeld/"}, {"autor": "Yiz_ml", "date": "2020-03-11 02:20:10", "content": "How can I get the result of variance formulation in Gaussian process classification /!/  As we can see equation (3.23) in the book \"Gaussian processes for machine learning\" (please see the uploaded -----> image !!! ), why are there two terms in this equation? This equation is used in the Gaussian process classification with Laplace approximation.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fgptyt/how_can_i_get_the_result_of_variance_formulation/"}, {"autor": "ks3ni4", "date": "2020-03-11 00:05:18", "content": "GAN generator doesn't evolve past a certain number of epochs /!/ Hi everyone, I'm fairly new on the machine learning field and I'm working on a project where I have to develop and train a CGAN. I'm doing pretty well so far but I've come to a point where I don't seem to be able to get the generator to generate more realistic photos of the eye area. \n\n&amp;#x200B;\n\n[Generated -----> image !!! s - 710 epoch](https://preview.redd.it/0l1j9lvroxl41.png?width=605&amp;format=png&amp;auto=webp&amp;s=cf206ab21d34027e90115f83bbc4d8a674c7bd0f)\n\n&amp;#x200B;\n\nAs you can see the generator actually generates close photos to what I'm looking for , the problem is that the -----> image !!!  above is one of the best so far, and the generator doesn't get really much better than this.\n\n&amp;#x200B;\n\nFor example,\n\n&amp;#x200B;\n\n[Generated images - 950 epoch](https://preview.redd.it/4lhat0agpxl41.png?width=603&amp;format=png&amp;auto=webp&amp;s=b40b2377616f8cc58997b2e8b506444b00354d62)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nand,\n\n&amp;#x200B;\n\n[Generated images - 1040 epoch](https://preview.redd.it/ge2fg68opxl41.png?width=603&amp;format=png&amp;auto=webp&amp;s=e6033feb9c4bc022b26409f19b9e9ea9e134acda)\n\n&amp;#x200B;\n\nThe GAN has been training for 25 hours. My question is, Am I being too impatient and I should let it train for a lot longer or there might be a problem with the generator model since I don't really see more improvement so far? \n\nThank you in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fgnwea/gan_generator_doesnt_evolve_past_a_certain_number/"}, {"autor": "nickbild", "date": "2020-04-18 20:29:42", "content": "Train machines like it's 1979! Play Atari 2600 games with gestures. /!/ Vectron AI interfaces with the Vectron 64 breadboard computer (6502 CPU @ 1MHz, 32KB RAM, 32KB ROM) to provide gesture detecting artificial intelligence. The gesture detection is used to control an Atari 2600 emulator.  \n\n\nHow It Works\n\n\\----  \nImages are captured, downscaled, and converted to an integer vector by a Raspberry Pi 3 B+. The vector is then transferred, one byte at a time, to a shift register in Vectron AI. After each byte is loaded, an interrupt is sent to the Vectron 64 computer.  \n\n\nThe Vectron 64 retrieves each byte and stores it in RAM. When a full -----> image !!!  has been received, it runs a k-nearest neighbors algorithm to classify the current -----> image !!!  against 50 known -----> image !!! s that are stored in the ROM. The class of the best match (minimum sum of all pixel distances) determines the predicted class of the current image. The 6502 assembly can be found here.  \n\n\nThe Vectron 64 then puts an address on the address bus that Vectron AI interprets and in turn sends a signal to a GPIO pin on another Raspberry Pi 3 B+. This Raspberry Pi is running a script that converts the GPIO signal to a simulated keypress. The simulated keypress controls a Stella Atari 2600 emulator (I suspect it would be only a small task to feed the GPIO directly into the joystick port of a real Atari 2600, but I don't have one available at present).  \n\n\nWhen this is all put together, you can place your hand in front of the camera with any of the known gestures (e.g. \"up\", \"down\", \"left\", \"right\"), and the game will be controlled accordingly.  \n\n\nMore Info\n\n\\----\n\n[https://github.com/nickbild/vectron\\_ai](https://github.com/nickbild/vectron_ai)  \n\n\n[https://www.youtube.com/watch?v=HILEYKIFixw](https://www.youtube.com/watch?v=HILEYKIFixw)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g3ug6f/train_machines_like_its_1979_play_atari_2600/"}, {"autor": "nraynaud", "date": "2020-04-18 00:29:28", "content": "My SIFT keypoints don't seem well placed /!/ Hi all, \n\nI'm trying on learn [Scale-invariant feature transform](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf), and I started a toy implementation here: [https://github.com/nraynaud/nraybot](https://github.com/nraynaud/nraybot) ( runs live here: [https://nraynaud.github.io/nraybot/index.html](https://nraynaud.github.io/nraybot/index.html) )  \n\n\nI started by creating just a succession of gaussians of my -----> picture !!! , doing the differences, and try to detects keypoints on that (without downsampling the images for now).  \n\n\nbut in my tests, I don't think my keypoints are really clustering at the corners, I might have done something wrong, I'd like someone to give it a look please.\n\nI have not yet implemented the coordinate refinement (4.1 in the paper), my understanding is that it is linked to the resolution downsampling.  \n\n\nWhen I hold a black rectangular object in front of the camera, no keypoint seem to accumulate around the corners, I just see a bit of accumulation around the clear edges in the room, but the rest seems random.\n\nAny help would be welcome.\n\nThanks.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g3e258/my_sift_keypoints_dont_seem_well_placed/"}, {"autor": "random_638", "date": "2020-04-17 18:12:57", "content": "Convolutional Neural Network and Q learning /!/ Hi a few questions in one really. what actually goes on the hidden layer of the CNN? in terms of -----> image !!!  recognition and also what is sampling?\n\nHow does Q learning work? i want to start programming and playing around with it, the goal really is to create a q learning model which can learn to play league of legends (by the end of the year at least).\n\n&amp;#x200B;\n\ni have semi-intermediate python knowledge (i understand the syntax etc) \n\n&amp;#x200B;\n\nany helps with links, books, github examples would be amazing! \n\nhope everyone is staying safe &lt;3", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g37b7z/convolutional_neural_network_and_q_learning/"}, {"autor": "dzanardo", "date": "2020-04-17 17:45:06", "content": "[Question] How to train an alghoritm? /!/ There are many apartments ads selling.\n\nSome of them are the same (differents between the -----> image !!! s attached - just with different brightness/size of -----> image !!! ).\n\nSome of them are in the same building.\n\n&amp;#x200B;\n\nBased on it, I'm working in: given an apartament, the alghoritm returns the apartaments related.\n\n&amp;#x200B;\n\nAt now, I did:\n\n\\- Input an apartament\n\n\\- Get the apartaments in 1 miles (using LAT and LON)\n\n\\- Compare the images - Based on it I give the percentages to the apartament is the same or not.\n\n&amp;#x200B;\n\nMy question is: How can I transform it on something that is able to self train?\n\n&amp;#x200B;\n\nI can better the comparission images. Or I can develop more steps to check it.\n\n&amp;#x200B;\n\nBut how can I give a step to change the system to determine it? \n\nOr when I talk about machine learning in other terms I just need to create more steps/methods?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g36rif/question_how_to_train_an_alghoritm/"}, {"autor": "abluecrate", "date": "2020-04-17 16:09:55", "content": "Approaches to 2D (non------> image !!! ) Data Analysis? /!/ Are there any machine learning methods to handle 2D curve data as an input? In other words, to predict some output z*,* given a 2D array (2 \\* n) of x and y values. I can't seem to find any information that isn't image related. \n\nFor background, I have a large data set containing curves -  specifically stress-strain data - with associated parameters. The goal is to predict these parameters given only the 2D curve itself. It's essentially a form of regression, where I already know x and y, but instead need to predict an underlying constant.\n\nMy current approach involves an MLP neural network with 2 input branches, where I split the 2D data into two 1D arrays with an extra hidden layer and then combine them in a concatenation layer. This appears to work, at least when training against linear pseudo-data.\n\nAre there any other network structures and/or machine learning algorithms that could be used in this application? From what I've read a CNN could work, though I can't find any information that isn't image related. Or perhaps an RNN or LSTM, since the data is sequential - though again, I can't find any information on series data that isn't time related.\n\nAny help or links/resources would be greatly appreciated", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g34y8x/approaches_to_2d_nonimage_data_analysis/"}, {"autor": "IluvGuyincognito", "date": "2020-04-17 07:50:08", "content": "What computer specifications do I need to build, train and run a GAN (text to -----> image !!! )? /!/ I will be basically following every and any online course and tutorial to try and learn how to build a text to image GAN. However, that would be a waste of time without the appropriate computer set up. I\u2019ve been googling it for an hour and I\u2019m at my wits ends, the most helpful resource I\u2019ve found has been a Quora answer with a broken link in it... I currently only own Mac computers, so I\u2019m thinking I\u2019ll need something that runs Windows? Thank you so very much.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g2xily/what_computer_specifications_do_i_need_to_build/"}, {"autor": "ThiccShadyy", "date": "2020-04-17 02:59:13", "content": "How exactly does a GAN perform -----> image !!!  translation tasks such as -----> image !!!  colorization? /!/ Can someone explain the high level process to me, perhaps in the context of this image describing the flow of a GAN's working:\n\nhttps://developers.google.com/machine-learning/gan/images/gan_diagram.svg?dcb_=0.27897943710060225\n\nWhere are the greyscale images provided as input? Is that the input in place of a random input given to the generator? What exactly is the distribution that is learnt in this context?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g2tofh/how_exactly_does_a_gan_perform_image_translation/"}, {"autor": "random_638", "date": "2020-04-16 18:25:54", "content": "convolutional neural networks /!/ Hi what exactly is sampling ? is it exactly how it sounds? each layer takes a smaller sample of the -----> image !!!  and is able to properly and efficiently categorize the data it receives?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g2kiqy/convolutional_neural_networks/"}, {"autor": "PM_ME_GOOD_NEWS_", "date": "2020-02-05 20:43:26", "content": "Considerations when taking photos for deep learning -----> image !!!  recognition? /!/ Hey guys,\n\nI'm working with a biologist on a project to build image recognition software to identify individual fish for a group of around 100 (based on their unique scale patterning). We'll be collecting the data ourselves by taking lots of photos of each fish. Obviously we need lots of a photos of each but are there any considerations we should take in this initial data collection phase to help with model performance downstream (pun intended)?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ezg2th/considerations_when_taking_photos_for_deep/"}, {"autor": "KPeyanski", "date": "2020-02-05 20:42:33", "content": "How to Set Up Object Detection on Raspberry Pi with TensorFlow Lite (video tutorial) /!/  Hi,\n\nIn this video I will show you how you can use TensorFlow to perform real-time object detection using -----> image !!! s streamed from the Raspberry Pi Camera.\n\n[https://youtu.be/iKQC4oCvSXU](https://youtu.be/iKQC4oCvSXU)\n\n[ How to Set Up Object Detection on Raspberry Pi with TensorFlow Lite](https://preview.redd.it/vllk30vv26f41.jpg?width=360&amp;format=pjpg&amp;auto=webp&amp;s=bf71c9ddb097dd0fb54cfd3e9882bdbf83fabb6f)\n\nIf you prefer to read check the full article here - https://peyanski.com/how-to-set-up-object-detection-on-raspberry-pi-with-tensorflow-lite/\n\nDuring my last video [https://youtu.be/-7Yuo\\_VUBiw](https://youtu.be/-7Yuo_VUBiw) I asked you whether you are interested in -----> image !!!  recognition software and you said - \"Yes\" big time, so I listen and execute\n\nI really hope that you find this information useful and you now can do object detection on your Raspberry Pi using TensorFlow lite. \n\nThank you for reading or watching, stay safe and see you next time.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ezg2aw/how_to_set_up_object_detection_on_raspberry_pi/"}, {"autor": "cherouvim", "date": "2020-02-05 13:40:47", "content": "Does -----> image !!!  scaling algorithm play any role when querying into a trained model? /!/ Hello, total noob here. I have the following question.\n\nAssume I have created a model by training it with images of dogs and cats. The purpose is that I'll be later on feeding it with new (never before seen by the model) images of cats and dogs and the model will say whether it is a cat or a dog.\n\nTo speed up the training process I downsized all images to 256x256 using \"Nearest-neighbor interpolation\" scaling algorithm.\n\nFor technical reasons (different libraries used in the API) when querying the model for an answer I'll be first resizing the image to 256x256 using \"Bicubic interpolation\" and then feeding the downscaled image to the model.\n\nMy question is, does it matter that I'm scaling down using a different algorithm? The differences observed via the human eye between those 2 algorithms are very very minor. Will this change affect the quality of results returned from the model?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ez9pk1/does_image_scaling_algorithm_play_any_role_when/"}, {"autor": "insanelylogical", "date": "2020-02-04 08:29:25", "content": "When extracting -----> image !!!  features with a CNN, are we allowed to use the cnn training data features for the next algorithm? /!/ I have a cnn I am extracting image features 2048 image features from. I then use PCA to reduce to 40. I then feed those 40 into a random forests algorithm. I know you need a seperate test and validation set if you want to see how good your cnn is, but what if the cnn is not what you are testing?  \n\n\nMy question is, am I allowed to use examples that we used to train the neural network as part of the input to the RF?  \n\n\nI am  5-fold cross validation for the RF, but I don't know if this is valid or if I am only allowed to use data my neural net has not seen.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eyn1ye/when_extracting_image_features_with_a_cnn_are_we/"}, {"autor": "tomblock", "date": "2020-02-03 15:01:13", "content": "I've googled for you some content about ML /!/ I've spent hours looking for information on machine learning for another project. If I help someone save this time, it should make sense, right?\n\n[**https://allaboutml.io**](https://allaboutml.io)\n\nThe site contains two types of the content:\n\n* 100% curated:\n   * books (with prices, ratings, reviews, first/last edition years)\n   * podcasts (ratings, number of episodes and date of the last one)\n   * courses (prices, rating, duration, number of lessons)\n   * communities (local and online)\n* Semi-curated (aggregated content but manually reviewed): \n   * events (conferences and local events - US-only for now)\n   * articles\n\nAll the content can be sorted by difficulty level (assigning levels was the hardest job).\n\nI tried to get some feedback over the weekend before posting here, but didn't get any attention. Please let me know if you see any value in it. \n\nI'd love to add more content (like jobs or events outside the US) or tag everything to make searching easier (e.g. all content related to -----> image !!!  processing), but I have this feeling it could be waste of time.\n\np.s. is this post ok for r/machinelearning ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ey8cbi/ive_googled_for_you_some_content_about_ml/"}, {"autor": "lifespills", "date": "2020-02-03 03:22:12", "content": "Loss does not reduce after certain point. /!/ I am training a Unet for -----> image !!!  segmentation of a person. My neural network seems to improve till a certain point after that it just stays constant. \n\nDice coefficient improves till a certain point in my case its 0.34 and it is constant since more than 2 hours. \n\nI am using batch normalisation and dropout after each convolution layer. \n\nMy relu activation is using max value as 1, i.e max(0,1).\n\nPlease suggest me methods to improve it.\n\nIf any more details are needed please tell me.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ey0iw7/loss_does_not_reduce_after_certain_point/"}, {"autor": "User1377420", "date": "2020-05-25 11:23:52", "content": "Would deep q-learning be suitable for the board game \"The settlers of catan\"? /!/ I was thinking of giving the agent a reward for every victory point he scored in the game. The input to the neural network would then also be somewhat questionable, a -----> picture !!!  of the playing field would probably not be useful.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gq9bwi/would_deep_qlearning_be_suitable_for_the_board/"}, {"autor": "alkaway", "date": "2020-05-25 05:23:24", "content": "pytorch color jitter /!/ From the documentation: \"brightness\\_factor is chosen uniformly from \\[max(0, 1 - brightness), 1 + brightness\\]\"\n\nbrightness by default is set to 0. This means that the brightness factor is chosen uniformly from \\[1, 1\\] meaning that brightness factor=1. The other parameters (contrast, saturation, hue) also seem to be constant under the default arguments. Does this mean that if color jitter is applied to the same -----> image !!!  twice, the output will be the same?\n\nIf not, is there a way to perform the same color jitter twice on a pair of -----> image !!! s? Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gq53c0/pytorch_color_jitter/"}, {"autor": "Amazing_Aniket", "date": "2020-05-23 20:28:54", "content": "I developed a mask detection app please give me suggestions. /!/ Play store:[https://play.google.com/store/apps/details?id=com.aniket.maskdetector](https://play.google.com/store/apps/details?id=com.aniket.maskdetector)\n\nSource code:[https://github.com/AniketSindhu/mask\\_detector](https://github.com/AniketSindhu/mask_detector)\n\nHow does it work\n\nI collected many samples of selfies/photos of people with a face mask and without a face mask and created a neural -----> image !!!  classification model in tensor flow lite. That model helps the machine to recognize whether there is a mask in the image or not.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gpc0kd/i_developed_a_mask_detection_app_please_give_me/"}, {"autor": "Amazing_Aniket", "date": "2020-05-23 19:04:57", "content": "I developed a mask detection app please give me suggestions. /!/ Play store:[https://play.google.com/store/apps/details?id=com.aniket.maskdetector](https://play.google.com/store/apps/details?id=com.aniket.maskdetector)\n\nSource code:[https://github.com/AniketSindhu/mask\\_detector](https://github.com/AniketSindhu/mask_detector)\n\nHow does it work\n\nI collected many samples of selfies/photos of people with a face mask and without a face mask and created a neural -----> image !!!  classification model in tensor flow lite. That model helps the machine to recognize whether there is a mask in the image or not.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gpak5j/i_developed_a_mask_detection_app_please_give_me/"}, {"autor": "-Hanazuki-", "date": "2020-05-23 18:22:14", "content": "Help starting -----> image !!!  classifying /!/ Hello, I'm completely new to machine learning. All I know is basic python (enough to figure out how to make a program work) and what pop videos show about nueral networks. However I'd like to start a project and don't know where to start and how much I need to know. \n\nI'd like to be able to train a computer to recognize and find a given object or person in some arbitrary image. I figured maybe learning to recognize faces would help but was wondering if there was a more general approach. \n\nFurther down the line I'd like to track the object in some video. Like a falling rock. \n\nIf anyone could help me by letting me know what topics I should look into, what libraries, or how to start the project that would be helpful. I see that keras and tensor flow may be useful but without prior knowledge I don't know if that's overkill or not.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gp9t3x/help_starting_image_classifying/"}, {"autor": "WinterGene7", "date": "2020-05-23 18:10:52", "content": "Question about -----> image !!!  dataset normalization(using PyTorch) /!/ I have been learning how to do transfer learning using the PyTorch pretrained models(based on ImageNet) and applying them to CIFAR10.\n\nI was wondering if I understood correctly how to normalize the images to fit my model.\n\nIf my model weights are frozen and I am only training the fully connected layer, should my normalization of the CIFAR10 dataset be using values(mean/std) from ImageNet since that is what the model expects?\n\nOtherwise, if I am training my model from scratch, should my normalization of the CIFAR10 dataset be the values calculated from that same dataset?\n\nPlease let me know. Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gp9m1c/question_about_image_dataset_normalizationusing/"}, {"autor": "Rahul_Desai1999", "date": "2020-05-23 15:32:46", "content": "I was trying to create a dog breed classifying nn using pytorch, but even after 5 epochs, i only get 1% accuracy and the loss is very slightly going down every epoch /!/     MAKE_TRAINING_TESTING_DATA = False\n    IMG_SIZE = 50\n    train_data = []\n    if MAKE_TRAINING_TESTING_DATA:\n        \n        PATH = \"\"\n        df = pd.read_csv(\"labels.csv\")\n    \n        breed_classes = {}\n        index = 0\n    \n        for breed in df['breed'].unique():\n            breed_classes[breed] = index\n            index+=1\n        \n        TRAIN_FOLDER = os.path.join(PATH, \"train\")\n        TEST_FOLDER = os.path.join(PATH, \"test\")\n        for -----> image !!!  in tqdm(os.listdir(TRAIN_FOLDER)):\n            if(-----> image !!! .endswith(\".jpg\")):\n                -----> image !!! _name = -----> image !!! .replace(\".jpg\", \"\")\n                breed = (df[df['id'] == -----> image !!! _name]['breed'].values)[0]\n                img = cv2.imread(os.path.join(TRAIN_FOLDER, -----> image !!! ), 0)\n                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n                train_data.append([np.array(img), np.eye(120)[breed_classes[breed]]])\n        np.random.shuffle(train_data)       \n        np.save(\"train_data.npy\", train_data)\n\nIn the above code i made an array of the data and saved it in the \"train\\_data.npy\" file. Then i import it.\n\n    class Net(nn.Module):\n        def __init__(self):\n            super().__init__()       \n            self.conv1 = nn.Conv2d(1, 32, 5)\n            self.conv2 = nn.Conv2d(32, 64, 5)\n            self.conv3 = nn.Conv2d(64, 128, 5)\n            \n            self.fc1 = nn.Linear(128*2*2, 512)\n            self.fc2 = nn.Linear(512, 120)\n            \n        def forward(self, x):\n            x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) #23 23\n            x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2)) #9 9\n            x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2)) #2 2\n            \n            x = x.flatten(start_dim=1)\n            \n            x = F.relu(self.fc1(x))\n            x = self.fc2(x)\n            \n            return F.softmax(x, dim=1)\n        \n    net = Net()\n    \n    loss_function = nn.MSELoss()\n    optimizer = optim.Adam(net.parameters(), lr = 0.001)\n\nThis is my nn.\n\n&amp;#x200B;\n\n    train_data = np.load(\"train_data.npy\", allow_pickle=True)\n    \n    X = torch.tensor([i[0] for i in train_data]).view(-1, 50, 50)\n    X = X/255.0\n    y = torch.tensor([i[1] for i in train_data])\n    \n    TRAIN_PCT = 0.8\n    \n    data_size = len(train_data)\n    \n    train_X = X[:int(TRAIN_PCT*data_size)]\n    train_y = y[:int(TRAIN_PCT*data_size)]\n    test_X = X[int(TRAIN_PCT*data_size):]\n    test_y = y[int(TRAIN_PCT*data_size):]\n    \n    print(len(train_X))    #8177\n    print(len(test_X))    #2045\n\nHere i seperate the training and testing data.\n\n&amp;#x200B;\n\n    EPOCHS = 5\n    BATCH_SIZE = 32\n    \n    for epoch in range(EPOCHS):\n        for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n            batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50)\n            batch_y = train_y[i:i+BATCH_SIZE]\n            \n            net.zero_grad()\n            outputs = net(batch_X.float())\n            loss = loss_function(outputs.float(), batch_y.float())\n            loss.backward()\n            optimizer.step()\n        print(f\"{loss}\")\n    \n    #0.00826213974505663\n    \n    #0.00826143380254507\n    \n    #0.008261081762611866\n    \n    #0.008260885253548622\n    \n    008260761387646198\n    \n\nThese are the losses i get\n\n&amp;#x200B;\n\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for i in tqdm(range(len(test_X))):\n            prediction = net(test_X[i].view(-1, 1, 50, 50))\n            if torch.argmax(test_y[i]) == torch.argmax(prediction):\n                correct+=1\n            total+=1\n    print((correct/total)*100)    #1.17359413202934\n\nThis is my accuracy.\n\n&amp;#x200B;\n\nCan someone help please?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gp6w1s/i_was_trying_to_create_a_dog_breed_classifying_nn/"}, {"autor": "icybreath11", "date": "2020-06-30 17:00:50", "content": "Confused about gradient descent with multiple variables equation - andrew ng ML course /!/ I am trying to create the equation for gradient descent with multiple variables in python. -----> Picture !!!  of equation: https://www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables_files/Image%20[3].png\n\nThe final solution is:\n\ntheta = theta - alpha *((np.dot((np.dot(X,theta)-y), X))*(1/m)). \nI made an error by adding an extra \"np.sum\":\n\ntheta = theta - alpha *((`np.sum`(((np.dot((np.dot(X,theta)-y), X))*(1/m))))*(1/m))\n\n\nI am confused about why the solution does not require an np.sum?\n\nNote: There is a for loop i with a set iterations. Alpha = learning rate. X is a matrix of (n+1) features. y = vector of actual values. m = number of training sets,", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hirao7/confused_about_gradient_descent_with_multiple/"}, {"autor": "violethoax", "date": "2020-06-30 15:54:08", "content": "Score distribution of AADB? /!/ Is anyone familiar with the Aesthetics and Attributes DataBase (AADB)? The paper mentions that it contains -----> image !!!  ids, aesthetic score distribution and attributes distribution for each -----> image !!! . I've found the image ids and attribute distribution, however, I'm not able to find the file that contains the aesthetic score distribution. \n\nThe closest I've found is the attMat.mat file, but it contains aesthetic score as only a single value for each image. \n\nLink to dataset:[AADB dataset](https://drive.google.com/open?id=0BxeylfSgpk1MOVduWGxyVlJFUHM)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hipzkw/score_distribution_of_aadb/"}, {"autor": "samurzele", "date": "2020-06-30 13:48:55", "content": "-----> Image !!!  autoencoder /!/ I have implemented an image autoencoder that takes rgb 64x64. Since I need this for a larger model, I need it to generalize well, but also have pretty high quality. I've been playing with hyperparameters for the last few days, I got it to generalize a bit better, but it's still way worse than I need. I realized that if the number of neurons in the layers between the code layer and the output is greater than the amount of training images, it doesn't generalize at all. The problem is I have gone down to having 12 features as the code layer, but it still wouldn't generalize, so maybe something is wrong with my implementation : I have three 2dConv layers that bring the dimension up to 16, and then 3 Fully connected layers to scale the thing back up. Pls help how can I make a good general autoencoder for images ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hinond/image_autoencoder/"}, {"autor": "enlightenseeker95", "date": "2020-06-30 13:18:49", "content": "Sorry for this question if it sounds trivial... /!/ So, I'm learning PyTorch and deep learning generally. I have gotten an OK understanding of general stuff in regards to CNN. I do want to learn more about -----> image !!!  segmentation but the online guides I have found seem to be more blog post style or just say how to do it in an obscure way. I was wondering, does anyone know of any good starting points or tutorials that could aid me here?\n\n&amp;#x200B;\n\nThanks.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hin709/sorry_for_this_question_if_it_sounds_trivial/"}, {"autor": "SuccMyStrangerThings", "date": "2020-06-30 09:23:13", "content": "Query Regarding -----> Image !!!  Labels /!/ My team and I are working on Thoracic Disease Detection from X-Ray. There are overall 14 targets and almost 110k images. Some of the images have more than one labels. \n\n|Image Index|Finding\\_Label|\n|:-|:-|\n|Image\\_001|Pneumonia|Hernia|\n|Image\\_002|No Finding|\n|Image\\_003|Atelectasis|Pneumothorax|Cardiomegaly|\n\nMy approach is for every label create a copy of image and assign that label to the image. So it looks something like this: \n\n&amp;#x200B;\n\n|Image Index|Finding\\_Label|\n|:-|:-|\n|Image\\_001|Pneumonia|\n|Image\\_001|Hernia|\n|Image\\_002|No Finding|\n|Image\\_003|Atelectasis|\n|Image\\_003|Pneumothorax|\n|Image\\_003|Cardiomegaly|\n\nMy teammate asked since a single image is having more than one label assigned to it, how will then the algorithm know which class it actually belongs to? Suppose for image\\_001, how will the machine predict if it has Pneumonia or Hernia? What is the possible explanation for this? \n\nWould it be better to drop all the images that have more than one label? But that will significantly reduce the data set. \n\nHelp Please .", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hijypo/query_regarding_image_labels/"}, {"autor": "Bala_venkatesh", "date": "2020-06-30 03:39:27", "content": "How to auto rotate the -----> image !!!  using Deep learning /!/ I wrote one blog about how to auto rotate the human images using computer vision. this use case really useful for image editing applications.\n\nit's simple steps to implement using OpenCV and Caffe model.\n\nCheck it out:- [https://medium.com/@venkateshpnk22/how-to-auto-rotate-the-image-using-deep-learning-c34b2e0e157d](https://medium.com/@venkateshpnk22/how-to-auto-rotate-the-image-using-deep-learning-c34b2e0e157d)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hifiym/how_to_auto_rotate_the_image_using_deep_learning/"}, {"autor": "Bala_venkatesh", "date": "2020-06-30 03:38:57", "content": "How to auto rotate the -----> image !!!  using Deep learning /!/ I wrote one blog about how to auto rotate the human images using computer vision. this use case really useful for image editing applications.\n\nit's simple steps to implement using OpenCV and Caffe model.\n\nCheck it out:- [https://medium.com/@venkateshpnk22/how-to-auto-rotate-the-image-using-deep-learning-c34b2e0e157d](https://medium.com/@venkateshpnk22/how-to-auto-rotate-the-image-using-deep-learning-c34b2e0e157d)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hifio1/how_to_auto_rotate_the_image_using_deep_learning/"}, {"autor": "Bala_venkatesh", "date": "2020-06-30 03:38:35", "content": "How to auto rotate the -----> image !!!  using Deep learning /!/ I wrote one blog about how to auto rotate the human images using computer vision. this use case really useful for image editing applications.\n\nit's simple steps to implement using OpenCV and Caffe model.\n\nCheck it out:- [https://medium.com/@venkateshpnk22/how-to-auto-rotate-the-image-using-deep-learning-c34b2e0e157d](https://medium.com/@venkateshpnk22/how-to-auto-rotate-the-image-using-deep-learning-c34b2e0e157d)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hifihc/how_to_auto_rotate_the_image_using_deep_learning/"}, {"autor": "Bala_venkatesh", "date": "2020-06-30 03:38:03", "content": "How to auto rotate the -----> image !!!  using Deep learning /!/ I wrote one blog about how to auto rotate the human images using computer vision. this use case really useful for image editing applications.\n\nit's simple steps to implement using OpenCV and Caffe model.\n\nCheck it out:- [https://medium.com/@venkateshpnk22/how-to-auto-rotate-the-image-using-deep-learning-c34b2e0e157d](https://medium.com/@venkateshpnk22/how-to-auto-rotate-the-image-using-deep-learning-c34b2e0e157d)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hifi7b/how_to_auto_rotate_the_image_using_deep_learning/"}, {"autor": "Bala_venkatesh", "date": "2020-06-30 03:33:24", "content": "How to auto rotate the -----> image !!!  using Deep learning /!/ I wrote one blog about how to auto rotate the human images using computer vision. this use case really useful for image editing applications.\n\nit's simple steps to implement using OpenCV and Caffe model.\n\nCheck it out:- [https://medium.com/@venkateshpnk22/how-to-auto-rotate-the-image-using-deep-learning-c34b2e0e157d](https://medium.com/@venkateshpnk22/how-to-auto-rotate-the-image-using-deep-learning-c34b2e0e157d)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hiffmb/how_to_auto_rotate_the_image_using_deep_learning/"}, {"autor": "Bala_venkatesh", "date": "2020-06-30 03:28:50", "content": "How to auto rotate the -----> image !!!  using Deep learning /!/ I wrote one blog about how to auto rotate the human images using computer vision. this use case really useful for image editing applications.\n\nit's simple steps to implement using OpenCV and Caffe model.\n\nCheck it out:- [https://medium.com/@venkateshpnk22/how-to-auto-rotate-the-image-using-deep-learning-c34b2e0e157d](https://medium.com/@venkateshpnk22/how-to-auto-rotate-the-image-using-deep-learning-c34b2e0e157d)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hifd5c/how_to_auto_rotate_the_image_using_deep_learning/"}, {"autor": "nathan_sam", "date": "2020-06-30 00:13:18", "content": "I\u2019m looking to hire a consultant to help build a pipeline for the restoration of archival footage. /!/ Hi everyone! I have been lurking around various machine learning subreddits and I have started to see some really exciting results in regard to upscaling and de-artifacting that can be done using GAN as well as some other techniques. \n\nI am looking to hire someone that can help me build a comprehensive -----> film !!!  restoration pipeline using these tools.\n\nLet me know if you\u2019re interested or are able to point me in the right direction!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hic8rf/im_looking_to_hire_a_consultant_to_help_build_a/"}, {"autor": "romenotbuiltinday", "date": "2020-06-29 20:11:22", "content": "Basic implementation for -----> image !!!  segmentation in Python /!/ I have an image segmentation exercise where I have grayscale images and masks that classify pixels into one of 8 categories. I need to use a convolutional neural network architecture to build and train the model. It doesn\u2019t have to be good or accurate, just fast. Can anyone help me? Ive done image classification before with one hot encoding vectors but never image segmentation where pixels have a classification. I have no idea how to build an image segmentation CNN.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hi7ocz/basic_implementation_for_image_segmentation_in/"}, {"autor": "tim-hilt", "date": "2020-06-29 07:30:12", "content": "Implementing Conv-Layers - Coping with dimensionality /!/ So i recently [read this blogpost by Victor Zhou](https://victorzhou.com/blog/intro-to-cnns-part-1/), where he describes the inner workings of a Convolutional Neural Network (CNN). In the article he implements classes for a simple conv-layer, a pooling- and a softmax-layer. He states, that the conv-layer is only usable, if we don't chain multiple conv-layers together, but i didn't really understand, how i should adapt his class if i were to use it in a chain.\n\nWhat i understood from how conv-layers are used in `keras.sequential` or in [Erik Lindernorens implementation in the ML-From-Scratch-Project](https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/deep_learning/layers.py#L200), there is a distinction between if the conv-layer is used as an input-layer, or if it exists somewhere in the deep layers.\n\nUltimately my question is this:\n\nVictor inputs a grayscale -----> image !!!  directly in the conv layer (8 filters, kernel size is 3) and doesn't apply any padding. So the shape of the input- and output arrays are:\n\nInput: 28x28; Output: 26x26x8\n\nSo there is an added dimension, because of the volume size. Therefore, when i chain such layers directly, one dimension would be added for each consecutive layer.\n\nWhen implementing a conv-layer for chained use, do i work around the increase in dimensionality? If so, how do i do it?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hhvhbg/implementing_convlayers_coping_with_dimensionality/"}, {"autor": "julianp20", "date": "2020-06-28 23:21:05", "content": "Bike-sharing prediction - am I doing it right? /!/ Hi, thanks for taking the time to read.\n\nBasically I am utilizing 3 months of bike-sharing data (July, August, September 2019) from the Citi Bike NYC bike-sharing system and I'm aggregating the number of outgoing trips from a station at each hour (I call it 'hourlydemand' in my dataset). The data starts as raw trip data, then is aggregated, I scale the station id, and finally, weather data for each day is added.  See the attached -----> image !!!  for my processed dataset.\n\nMy goal is to predict, at a specified day, hour, and bicycle station, what will be the 'hourlydemand'? This is my target variable. \n\nHowever, my models seem to be struggling at predicting at high accuracy and I think it's probably a data problem.\n\nI am using Python, Pandas, Numpy, Jupyter notebook for data processing, and mostly ScikitLearn and Keras for modeling.\n\nI have a couple of questions I want to clear out:\n\n1. Is this a time series problem, or a supervised learning problem? At first, I thought that this was a time series problem but recently I have been told to add columns of lag and forecast observations regarding the target variable so I assume this is now a supervised learning problem.\n2. Should I index my data with a DateTime index, or leave it as default?\n3. How should I split my dataset? Currently, I am attempting to use July and August data as Training and September to predict. \n4. Should I scale or normalize some columns? Currently, the 'startstationid' column is being scaled using ScikitLearn RobustScaler.\n\nI have tried countless ways to try to improve my data, from setting different indexes to adding more lag.\n\nAny constructive criticism and feedback are genuinely appreciated since I am seriously struggling to find a proper solution.\n\nhttps://preview.redd.it/b6z2a1plfq751.png?width=1757&amp;format=png&amp;auto=webp&amp;s=2556631f350c601b4c6f671b05472506395efa0f", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hhoq9v/bikesharing_prediction_am_i_doing_it_right/"}, {"autor": "ArloP", "date": "2020-08-06 04:11:30", "content": "At wits end with machine learning for thesis /!/ Hello!\nSo i'm a psych major doing an honours. My thesis is on methamphetamine induced dopamine super sensitivity in genetically modified mice. I have a bunch of brain slices with stained neurons and i need to count them. I could load them into ImageJ and spend two weeks clicking the neurons in each -----> image !!! , but i want to utilise machine learning. I found a github repository with code for a convolutional neural network which was trained on the exact same neurons/staining/slices. However i've spent the past 3 days straight (12 hours per day) trying to get it to work in Anaconda/Spyder/Visual Studio/MATLAB. Although i have been trying hard, its unfeasible for me to learn python in a month while doing my thesis.\nDoes anyone have any advice for more user friendly ways of implementing machine learning, specifically for images?\nThe two ways i've found that seem feasible are cross correlation Template matching, and using that model/training my own model to count the cells; however every time i attempt to implement either all i get is errors. I don't need to do this but psych can be pretty boring and soft sciency so i want to spice it up in the hope that i get real good marks.\nAny help greatly appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i4kw72/at_wits_end_with_machine_learning_for_thesis/"}, {"autor": "Elbarro", "date": "2020-08-05 08:28:14", "content": "Displaying a logo on Dash navbar on a remote server /!/ Hello, not sure if this is the best place to post but here it goes. \n\nI am currently facing a problem on a dashboard that I building using Dash. \n\nSo I basically have a following code:\n\n&amp;#x200B;\n\n    \n    import dash\n    import dash_bootstrap_components as dbc\n    \n    app = dash.Dash(external_stylesheets=[dbc.themes.BOOTSTRAP])\n    \n    navbar = dbc.Navbar(\n        [\n            html.A(\n                dbc.Row(\n                    [\n                        dbc.Col(html.Img(src=app.get_asset_url(\"img/logo.png\"), height=\"30px\")),\n                        dbc.Col(dbc.NavbarBrand(\"Navbar\", className=\"ml-2\")),\n                    ],\n                    align=\"center\",\n                    no_gutters=True,\n                ),\n                href=\"https://plot.ly\",\n            ),\n            dbc.NavbarToggler(id=\"navbar-toggler\"),\n            dbc.Collapse(search_bar, id=\"navbar-collapse\", navbar=True),\n        ],\n        color=\"dark\",\n        dark=True,\n    )\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nWhen I run this code locally, I can see the logo -----> image !!!  on the navbar and everything else is fine. When I run the code on a remote server, and serve the page with gunicorn on a private network, I can see the entire page and the content of the dashboard, except the logo image. Its the exact same code, I just pull from a git repo on the remote server. \n\n&amp;#x200B;\n\nWhen I try to inspect element on the image, it seems to me that I do not have access to the image. How do I fix this? The remote server is Ubuntu 20 LTS. \n\n&amp;#x200B;\n\nThank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i41jr7/displaying_a_logo_on_dash_navbar_on_a_remote/"}, {"autor": "guylostintheuniverse", "date": "2020-08-04 16:57:21", "content": "Which kind of algorithm to use to predict where a person is looking? /!/ Hi, I'm very new to machine learning so please bear with me.\n\nI'm trying to build an AI to play my game. In my game you are given a -----> photo !!!  of one or more people who are looking at a ball being thrown into the air. In the photo the ball is removed and only the people remain. this is done as I have taken a photo of the background before I threw the ball, then I just overlay the parts over. \n\nYou play by selecting the co ordinates about where the ball is. The game then gives you a score depending on how close you are. \n\nI'd like to develop a ML learning algorithm that given this image, will predict where the ball is, based on characteristics of the photo. \n\nI have lots of training data such as pictures and co ordinates. I'm very skilled in Python but have little to no machine learning experience.\n\nCould someone please give me some tips to move forward? So far I've been told I should use keras and tensorflow as starting points.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i3n10v/which_kind_of_algorithm_to_use_to_predict_where_a/"}, {"autor": "Marvin000093", "date": "2020-09-11 12:53:36", "content": "Reinforcement Learning for classification of a CSV file /!/ I would like to try a Reinforcement Learning approach for a multi-label or binary classification of a CSV file. I know that Supervised Learning is probably easier and I have already tried a couple of approaches. I works pretty good, but this RL approach could be an interesting thing for a small chapter and the outlook of my Master thesis. So i just want to try it even if the results might be much worse.\n\nI found a couple of papers about -----> image !!!  classification using RL and it seems to be possible. So i don't see a reason why it shouldn't work for a numerical Dataset. I could use the difference of the prediction/action of the RL agent and the actual label to train the model.\n\nMy problem is that i am not that good in coding. I have never done Reinforcement Learning and don't realy know where to start. Has anyone tried to use RL for classification or knows a good publication with a rather simple code attatched that i could take a look at?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iqpvlr/reinforcement_learning_for_classification_of_a/"}, {"autor": "CertainName9", "date": "2020-09-10 20:59:22", "content": "Book recommendations for the different applications of machine learning models in business. /!/ Anyone know of any books that cover different business use cases for machine learning models across different fields (e.g. marketing, e-commerce, NLP engines, finance, etc.)? Ideally they're software use cases rather than insights.\n\nExamples of such use cases would be\n1. Ranking search results to maximize click through rate\n2. Identifying fraudulent transactions\n3. Object detection in an -----> image !!! ", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iqc71s/book_recommendations_for_the_different/"}, {"autor": "spmallick", "date": "2020-09-10 20:13:38", "content": "AI For Entrepreneurs Episode 2 : Kwabena Agyeman /!/ In 2015, they did a Kickstarter campaign for a smart -----> camera !!!  based on a microcontroller. The campaign was a huge success and they raised more than $100k.\u00a0\n\nAnd then came a disaster they were not prepared for. One of the components in the camera failed leaving 80% of the cameras unusable. But they fought back hard and delivered the cameras against all odds.\u00a0\n\nIn this episode, we will learn about OpenMV, microcontrollers, why they matter, and the challenges of porting computer vision algorithms to microcontrollers.\u00a0\n\nBut above all, we will learn about the amazing journey of a resilient entrepreneur Kwabena Aygeman who is one of the co-founders of OpenMV.\u00a0\n\n[https://youtu.be/EBiI627K8A8](https://youtu.be/EBiI627K8A8)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iqbaip/ai_for_entrepreneurs_episode_2_kwabena_agyeman/"}, {"autor": "luis_likes_math", "date": "2020-09-10 19:19:02", "content": "Singular value decomposition and -----> image !!!  compression tutorial with code /!/ In this video I explain how singular value decomposition works in a geometric way, where we express a linear transformation as a combination of rotations and scalings. Then we get into dimensionality reduction, and show how svd can be used to approximate a matrix by a much simpler matrix of lower rank. We then use this to compress an image. Python code included. Here is the link to the video.\n\n[https://www.youtube.com/watch?v=DG7YTlGnCEo](https://www.youtube.com/watch?v=DG7YTlGnCEo)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iqa6hd/singular_value_decomposition_and_image/"}, {"autor": "luis_likes_math", "date": "2020-09-10 19:17:59", "content": "Singular decomposition and -----> image !!!  compression tutorial with code /!/ In this video I explain how singular value decomposition works in a geometric way, where we express a linear transformation as a combination of rotations and scalings. Then we get into dimensionality reduction, and show how svd can be used to approximate a matrix by a much simpler matrix of lower rank. We then use this to compress an image. Python code included. Here is the link to the video.\n\n[https://www.youtube.com/watch?v=DG7YTlGnCEo](https://www.youtube.com/watch?v=DG7YTlGnCEo)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iqa5ol/singular_decomposition_and_image_compression/"}, {"autor": "GeorgeFree2018", "date": "2020-09-10 14:41:36", "content": "Data Generators - TF &amp; Keras /!/ I'm trying to implement a generator in Keras, but my system is still running out of memory (36GB available). Each set of 64 images and masks totals \\~100MB. I can't even get through one epoch without consuming &gt;30GB of memory.\n\nHere is my generator code:\n\n    import numpy as np\n    import tensorflow as tf\n    import keras\n    import glob\n    import math\n    \n    \n    def getFiles(stack_dir, mask_dir):\n        '''\n        ** find all stack -----> image !!! s in dir., along with matching mask -----> image !!!  files **\n        ~~~~~~~~~\n        INPUTS:\n                - stack_dir = directory containing all stack -----> image !!! s\n                - mask_dir = directory containing all mask -----> image !!! s\n        RETURNS:\n                - 2d array of stack and mask pairs [[stack_image_loc, mask_image_loc], ...]\n        '''\n        stacks = []\n        masks = []\n        for stack_image in glob.glob(stack_dir+\"/*.npy\"):\n            x, y, z = stack_image.split(\n                stack_dir+\"/stack_tile_\")[1].split(\".npy\")[0].split(\"_\")\n            mask_image = mask_dir+\"/mask_tile_{}_{}_{}.npy\".format(x, y, z)\n            stacks.append(stack_image)\n            masks.append(mask_image)\n        return stacks, masks\n    \n    \n    class dataset_gen(keras.utils.Sequence):\n        ' See https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence'\n    \n        def __init__(self, stacks, masks, batch_size):\n            self.stacks = stacks\n            self.masks = masks\n            self.batch_size = batch_size\n    \n        def __len__(self):\n            ' number of batches per epoch '\n            return math.ceil(len(self.stacks) / self.batch_size)\n    \n        def __getitem__(self, index):\n            print(\"\\n\\n\\n{}\\n\\n\\n\".format(index))\n            indices = [index*self.batch_size, index+1*self.batch_size]\n            i = indices[0]\n            temp = []\n            while i &lt; indices[1]:\n                temp.append(i)\n                i = i+1\n    \n            X, y = self.__data_generation(temp)\n    \n            return X, y\n    \n        def __data_generation(self, list_IDs_temp):\n            X = np.empty(\n                (self.batch_size, *(256, 256), 5))\n            Y = np.empty(\n                (self.batch_size, *(256, 256), 3))\n    \n            for i, ID in enumerate(list_IDs_temp):\n                X[i, ] = np.load(self.stacks[ID])\n                Y[i, ] = np.load(self.masks[ID])\n    \n            return X, Y\n    \n\nand a basic training setup:\n\n    import tensorflow as tf\n    \n    import sys\n    \n    from tensorflow.keras.backend import pow\n    from tensorflow.keras.models import Model\n    from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D, Concatenate, Add\n    from tensorflow.keras.losses import binary_crossentropy\n    \n    \n    from resunetBuild import *\n    from generator import *\n    \n    TRAIN_LENGTH=5000\n    EPOCHS=20\n    BATCH_SIZE=64\n    BUFFER_SIZE=100\n    STEPS_PER_EPOCH=TRAIN_LENGTH // BATCH_SIZE\n    \n    stacks,masks=getFiles('/home/george/Documents/Github/larger-dataset-test/Data/np_tiles/stack', '/home/george/Documents/Github/larger-dataset-test/Data/np_tiles/mask')\n    \n    train_dataset = dataset_gen(stacks, masks, BATCH_SIZE)\n    \n    \n    model = resuneta(256, 256, channels=5, outClasses=3)\n    adam=tf.keras.optimizers.Adam(lr=0.05, epsilon=0.1)\n    model.compile(optimizer=adam, loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n    \n    history=model.fit(train_dataset, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)\n\n&amp;#x200B;\n\nIs there a problem with my generator, or is this likely the result of the model that I'm training?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iq4lev/data_generators_tf_keras/"}, {"autor": "GeorgeFree2018", "date": "2020-09-09 21:42:16", "content": "Loading TFRecords into model - exceeds available memory (TF2 &amp; Keras) /!/ I'm new to training on large datasets. I've created several TFRecords files containing many -----> image !!!  pairs (picture and mask) - although distributed into \\~200mb files they total around 15Gb - but I cannot load them into my model without exceeding my computer's memory. I can only presume the entire dataset is being loaded into memory.\n\n    TRAIN_LENGTH=5000 # length of training dataset\n    EPOCHS=20\n    BATCH_SIZE=64\n    BUFFER_SIZE=100\n    VAL_SUBSPLITS=5\n    VALIDATION_STEPS=100//BATCH_SIZE//VAL_SUBSPLITS\n    STEPS_PER_EPOCH=TRAIN_LENGTH // BATCH_SIZE\n    \n    # (find_tfrecords finds all of the relevant .tfrecords files)\n    training_files=find_tfrecords(prefix=\"val\")\n    test_files=find_tfrecords(prefix=\"test\")\n    train = tf.data.TFRecordDataset(training_files) \n    test = tf.data.TFRecordDataset(test_files)\n    \n    train=train.map(parse_function)\n    test=test.map(parse_function)\n    \n    train_dataset=train.shuffle(BUFFER_SIZE)\n    train_dataset=train_dataset.batch(BATCH_SIZE)\n    train_dataset=train_dataset.repeat()\n    test_dataset=test.batch(BATCH_SIZE)\n    \n\nIs there a way to load TFRecords into my model without physically increasing my computer's memory. And/or is there a better approach than TFRecords?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ippv3o/loading_tfrecords_into_model_exceeds_available/"}, {"autor": "marcopaaah", "date": "2020-09-09 20:24:36", "content": "Semantic segmentation of each instance /!/ Say I have two objects of interest in an -----> image !!! : motorcycles and regular bicycles.\n\nMy current task is to detect each instance of all motorcycles and regular bicycles in the image. This can easily be done with Mask RCNN or any other variant.\n\nBut for this problem I also want to segment out each part of the motorcycle and bicycle. For an example, for each instance segment out the wheels, steering wheel etc. Some of the classes might be shared for the motorcycle and bicycle while other classes wont. I will also have to do keypoint detection of each instance, where the keypoints might vary between classes as well.\n\nIs anyone aware of what this task is referred to in the literature or have any resources to get me started?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ipocw0/semantic_segmentation_of_each_instance/"}, {"autor": "pennyplane", "date": "2020-09-09 20:07:20", "content": "GANs: How many parameters should D and G have based on -----> image !!!  size? /!/ I want to create images of the size 128x92 based on around 9000 images of the same resolution. I've been looking around for guidelines for how deep/large my networks should be / how many parameters they should have. I'm assuming the bigger the images, the bigger the networks have to be. In most implementations I've seen, the generator has significantly more parameters than the discriminator, but I've seen it the other way around as well. I'd be grateful for any rough guideline or any useful links. Thank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ipo08d/gans_how_many_parameters_should_d_and_g_have/"}, {"autor": "OnlyProggingForFun", "date": "2020-09-09 12:07:47", "content": "AI Generates Real Faces From Sketches! DeepFaceDrawing Overview | -----> Image !!! -to-image translation in 2020", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ipempj/ai_generates_real_faces_from_sketches/"}, {"autor": "harabayashi", "date": "2020-10-17 06:56:09", "content": "Loss of more than 1? /!/ I made a model to classify trash in an -----> image !!! . I am still in the testing stage so the model is just simple:\n\n    model = tf.keras.Sequential(\n        [\n            tf.keras.layers.Reshape(input_shape=(384*512,), target_shape=(384, 512, 1)),\n            \n            tf.keras.layers.Conv2D(kernel_size=3, filters=12, use_bias=False, padding='same'),\n            tf.keras.layers.BatchNormalization(center=True, scale=False),\n            tf.keras.layers.Activation('relu'),\n            \n            tf.keras.layers.Flatten(),\n            \n            tf.keras.layers.Dense(100, use_bias=False),\n            tf.keras.layers.BatchNormalization(center=True, scale=False),\n            tf.keras.layers.Activation('relu'),\n            \n            tf.keras.layers.Dense(6, activation='softmax')\n        ])\n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\nWhen I train this model, it shows a loss of more than 1 and accuracy of around 50% (though validation accuracy is only around 30%).\n\nBut what does it mean to have a loss of more than 1? Does it mean that every time it tried to guess it will be wrong?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jcqjtm/loss_of_more_than_1/"}, {"autor": "fbeilstein", "date": "2020-10-16 20:12:49", "content": "I simplified the SMO algorithm for SVM to make it more approachable for students. Feedback/Review is highly appreciated. /!/ The description of the SMO algorithm is quite complicated even considering its simplified versions (e.g. [http://cs229.stanford.edu/materials/smo.pdf](http://cs229.stanford.edu/materials/smo.pdf), [http://web.cs.iastate.edu/\\~honavar/smo-svm.pdf](http://web.cs.iastate.edu/~honavar/smo-svm.pdf)). Moreover, derivation of all the formulas needed is quite cumbersome and takes an enormous amount of time (e.g. [http://fourier.eng.hmc.edu/e176/lectures/ch9/node9.html](http://fourier.eng.hmc.edu/e176/lectures/ch9/node9.html)). \n\n\nSo I tried to create my own simplified version of the SMO algorithm basing on ideas from linear algebra. Now it takes just about 30 lines of code in Python (posted on GitHub [https://github.com/fbeilstein/simplest_smo_ever](https://github.com/fbeilstein/simplest_smo_ever)) and less than 2 pages of mathematical derivation (posted on ResearchGate [https://www.researchgate.net/publication/344460740_Yet_more_simple_SMO_algorithm](https://www.researchgate.net/publication/344460740_Yet_more_simple_SMO_algorithm)) to get the things done. Probably, it may be useful for studying SVM. I will highly appreciate any feedback regarding approachability and/or correctness of the method. -----> Image !!! -comparison to sklearn.svm.SVC is enclosed.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jchbzk/i_simplified_the_smo_algorithm_for_svm_to_make_it/"}, {"autor": "Marvin000093", "date": "2020-10-15 11:49:20", "content": "Classification of tabular data with a CNN - convolve the data /!/  Hello everyone,\n\nI am trying to build a CNN to classify tabular data.\n\nFor that purpose i use a base -----> image !!!  to convolve the feature vectors of my dataset to create a new -----> image !!!  based dataset. (There is a paper about that if anyone is interested and wants to know more: [https://www.researchgate.net/publication/341117286\\_A\\_novel\\_method\\_for\\_classification\\_of\\_tabular\\_data\\_using\\_convolutional\\_neural\\_networks](https://www.researchgate.net/publication/341117286_A_novel_method_for_classification_of_tabular_data_using_convolutional_neural_networks))\n\nThis is my Code to convolve the images. It seems to work so far, but i am a bit confused about the color code.\n\n    import pandas as pd\n    from sklearn.preprocessing import StandardScaler, PowerTransformer, MinMaxScaler\n    import numpy as np\n    from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img, save_img\n    \n    Dataset = pd.read_csv(\"....csv\", header=0)\n    feature_columns = ['...']\n    x = Dataset[feature_columns]\n    y = Dataset.Classifier\n    #sc = MinMaxScaler(feature_range=(0.1, 0.9))\n    sc = PowerTransformer()\n    x = sc.fit_transform(x)\n    \n    \n    x_new = x[0].reshape(5,5)\n    x_new = x_new.repeat(repeats=50, axis=0).repeat(repeats=50, axis=1)\n    x_new = (x_new.reshape(x_new.shape+(1,)))\n    \n    img = load_img(\".../base_image.png\", grayscale=True)\n    img_array = img_to_array(img)\n    \n    new_array_new = np.multiply(img_array, x_new)\n    \n    save_img('.../new_image.png', new_array_new)\n    img_new = load_img('.../new_image.png')\n    img_new.show()\n\nSo i wanted to scale the data first and thought i am going for the minmaxscaler so that i can define the boundaries and not get a 0 as a color code. That would mean that in case i have many 0 in my Dataset a lot of the image would be black. But then i realized it doesn't matter that much. I can even use other scalers with values below 0 and the image still looks fine. After that i tried to add or subtract a constant like 255 or even 5000 to the image array: \n\n    new_array_new = (np.multiply(img_array, x_new) + 255)\n\nThe picture got brighter, but it still wasn't totally white. So i am wondering if there is anything i am missing. Is there some kind of scaler included in the save\\_img function or something?\n\nBesides that, does anyone has some tips about how to improve my approach so far?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jbm1ie/classification_of_tabular_data_with_a_cnn/"}, {"autor": "earnnu0", "date": "2020-02-02 12:20:10", "content": "Extracting information from table within invoices /!/ I am trying to find specific information from supplier invoices using ML. Now, I have a lot of invoices I can use to create a dataset (+100k). I figure the first step is to label them, so I can train a model to extract the information I need.\n\nBelow is a sample invoice (I have anonymized it by removing the shipper address details, logo etc.).\n\n&amp;#x200B;\n\n[Example invoice](https://preview.redd.it/82luba2f6ie41.png?width=1576&amp;format=png&amp;auto=webp&amp;s=0bd1f90c03310a9ff67aadf86b72da069829842e)\n\nAs you can see in the above -----> image !!! , I am trying to extract three entities from the invoice:\n\n1. table\n2. amount (for each line in the table)\n3. tariff (for each line in the table)\n\nNow I actually don't need the table, but what seems to be common across all my supplier invoices is that the `amount` and `tariff` for line items, is always within a table. In the above example, the table only contains 1 line-item - it can contain multiple (spanning multiple pages as well)\n\nMy idea of a pipeline is:\n\n* Get all invoices (PDF files) and convert each page to a PNG image.\n* Label all the individual pages, using a label annotation tool, to label `table`, `amount` and `tariff` \\- including the **bbox** information for each label.\n* Train a model using the above dataset.\n* ???\n\nMy questions are:\n\n* Does this seem like a good approach to such a problem?\n* Do I have to split up the model into two? With this I mean, does it make more sense to *first* identify the `table` within an image with **one model**, then serve the table image to another model, and extract the `tariff` and `amount` with the 2nd model?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/exnnzb/extracting_information_from_table_within_invoices/"}, {"autor": "hacksdump", "date": "2020-02-02 08:45:19", "content": "Project Idea /!/ We are three juniors pursuing our bachelors in CSE, and we have to come up with an ML/DL project as part of our curriculum. All three of us happen to be beginners but can't afford to make something very simple like a neural style net. Could y'all suggest a localized field in -----> image !!!  processing that we could look into and possibly whip up a decent project? We have about 3 months to learn and build. Thank you", "link": "https://www.reddit.com/r/learnmachinelearning/comments/exlng4/project_idea/"}, {"autor": "oDOOMo", "date": "2020-02-02 03:10:53", "content": "Is procedural generation of realistic/desirable models or dialogue possible? /!/ So, sorry to disappoint, but this is a question post by a person with no real experience or understanding of computer science or how neural networks worm, I just thought this was the appropriate place to post. \n\nAs someone who loves a good story, there's nothing worse than buying a new game and having some aspect of it ruined by a bad UI, sort of off models, or bad voice acting/lines. But more than that, I feel like what contributes to the whole thing usually feels like an experience that I've already had, where most of everything is based on models that have worked in the past, like basic storylines, generic/unintentionally unrealistic characters, and, mostly the overwhelming feeling that the world is built around the player. So I came up with some ideas and wanted to see how realistic they are or if/when they might be able to come into fruition based on where we are today.\n\nThe first idea is procedurally generated objects in a game. The basis of idea is that the computer can come up with a platonic idea of some object, let's say a dog for now, based on images on the internet. Then, using computer magic, it would eventually be able to render that into a 3D model, that would be mostly realistic. I guess the -----> image !!!  I have in my mind is telling a computer \"dog,\" then it generates a dog model with a few AI elements based on keywords (like walking or fetching) for a few minutes/hours, then you can run around with a basic dog on a grey plane or whatever terrain has been generated. I guess it would also be able to learn what was and wasn't realistic based on user input, and maybe it would archive the better models for comparison with others to try to find common traits that were considered realistic throughout the whole final design (maybe like multiple users saying the licking was a realistic trait). I know it's far fetched, but I'm just going to throw it out there to see if it's an idea that has any merit.\n\nMy second main idea is along the same lines, but more or less focuses on finding realistic interactions. So maybe like building an algorithm that comes to understand that it's uncommon for something to crawl in a vent or that floating objects don't really happen that much. I guess speech lines might fall under this category, where it understands normal speech patterns (I assume it would work like google duplex) or maybe be able to develop speaking patterns for different people. In this scenario, it also falls under that same idea of cross referencing to try to find the common traits to attempt to eventually find the platonic idea of any specific interaction. \n\nThanks for reading that whole thing; sorry it sounds like the babbling of uneducated idiot, I'm just curious to see if it has any actual merit or if it would be hopeful dreaming. \n\n:)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/exhwt7/is_procedural_generation_of_realisticdesirable/"}, {"autor": "samagl94", "date": "2020-02-01 16:32:44", "content": "Need directions /!/ Hi, i am a web developer and looking forward to to dive into ml . Cheers to the community. \nI want to make an application which will recognise gestures ( from hand). Can you guys help me by telling how do i go about doing that? ( What libraries, accessing -----> camera !!!  , any relevant links would be a huge help) . Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ex8m3n/need_directions/"}, {"autor": "IamGroot_19", "date": "2020-02-01 13:35:05", "content": "Doubt regarding memory consumption for Deep CNN /!/ I was going through VGG architecture and it was mentioned that total memory consumption turns  out to be approximately 96 MB/-----> image !!!  (only for forward prop). What I don't understand is this:\n\nThe memory for total number of parameters is assigned to hold the weights &amp; bias of all layers in the network. Now every time we introduce a new image to the network, we pick it apart using the layers, apply filters (forward prop) and then update the weights through backprop. \n\nSo for every image we only need 96MB of holding memory and few updations to those bunch of 96MB parameters. If that's the case why do we run out of memory? Where am i going wrong?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ex6776/doubt_regarding_memory_consumption_for_deep_cnn/"}, {"autor": "niceguyofall", "date": "2020-02-01 09:10:58", "content": "Suggestion for a face recognition model /!/ I want to create a facial-attendance system for my school of about 2000 students. A -----> camera !!!  will be placed in the entrance of the school . Students will pass looking at the camera one by one. Every student will face the camera for about 1.5 seconds. The camera will be a 720p camera of 30fps. The camera will feed the input to the backend and the backend powered by python will analyze the input and mark the attendance  in a DB. As I am a beginner to ML. I don't know which technology/model to use to recognize the face in a prepared model of about 2000 students in a short time with high accuracy as I have to feedback to the student about the successful attendance. On every annual year a human will enter the details of each new comer with the video of about 5-15 seconds of each student into the model. The training time of model can vary from hours to days. Any help/suggestion is highly appreciated", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ex3jch/suggestion_for_a_face_recognition_model/"}, {"autor": "MLtinkerer", "date": "2020-02-01 06:06:42", "content": "ICYMI from Nvidia researchers: Produce a 3D object from a 2D -----> image !!!  (in less than 100 milliseconds!)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ex1ujo/icymi_from_nvidia_researchers_produce_a_3d_object/"}, {"autor": "Metalwrath22", "date": "2020-01-31 12:39:03", "content": "Understanding WGAN-GP Loss Functions and Labels /!/ Hello,  \nI'm a total noob when it comes to loss functions. I've been reading, searching and trying to understand for last few days the intuition behind this.\n  \nI have an implementation (not my implementation) which works fine. It has classic WGAN loss with Gradient Penalty.  \n\nHere I get a prediction from discriminator given a real -----> image !!! .  And then it is given to softplus activation.\n    real_predict = discriminator(real_image, latent , step,  alpha)  \n    real_predict = nn.functional.softplus(-real_predict).mean()\n    real_predict.backward(retain_graph=True)  \n  \nAnd for images generated by generator:  \n    fake_predict, fake_logits = discriminator(fake_image, latent_w1, step, alpha)\n    fake_predict = nn.functional.softplus(fake_predict).mean()\n    fake_predict.backward()  \n  \nHow are labels used here for real and fake images? My intuition was all fake images are labeled as 0 and real images as 1. But there are no labels here. How does this discriminator calculates loss if it does not know labels? I am wondering about this because I want to change the implementation a bit and make it use labels when calculating loss (I am trying to convert this model to something that generates images given text descriptions). I am a bit confused because there is nothing about labels in the code.\n\nAlso I have problem understanding why we are multiplying real_predict with a minus sign, what does it mean?  Any source of information is welcome.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ewnvuw/understanding_wgangp_loss_functions_and_labels/"}, {"autor": "zweza", "date": "2020-01-17 03:52:46", "content": "Quick question from somebody completely out of their element /!/ Hi there, I had a quick question that hopefully I'm in the right place for. I'm a photographer/videographer and I've always been excited by the prospect of things like stylegan where computers are trained to make short animations [like this](https://youtu.be/z3lsZcV76KE) and their artistic potentials.\n\nMy question: Can somebody point me in the right direction to how I would go about making something like the above video using my own images? I've checked out Artbreeder, but the process of animating seems a little cumbersome for what I'm doing considering it takes about an hour to upload an -----> image !!!  and certain features are gated behind a subscription.\n\nCurrently I have Runway installed and have had some luck with that in terms of making images I could style transfer on top of some videos, but not quite in the realm of what I'd like to do specifically. Any suggestions? Hopefully this makes sense. I'm mostly looking to do it on portraits, and I've looked around in this sub and on youtube and haven't had much luck.\n\nThank you so much! Any help is greatly appreciated!!!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/epuzmw/quick_question_from_somebody_completely_out_of/"}, {"autor": "Character-Comb", "date": "2020-01-16 02:12:10", "content": "I'm having trouble loading Matlab. Advice? /!/ I'm trying to use Google Colab and its neural style transfer lesson but I have not been able to get Matlab.plt to work. I'm also having trouble uploading files from Wikimedia Commons. The example uses a -----> picture !!!  of a turtle for the content of the generated image, and painting of a tidal wave for the style of the generated image.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/epckh6/im_having_trouble_loading_matlab_advice/"}, {"autor": "JsonPun", "date": "2020-01-15 17:17:46", "content": "Question: How do you review predictions and update models efficiently? /!/ Hello I currently have an -----> image !!!  classifier I made. I was wondering though how others efficiently review their predictions and update their model?\n\nCurrently I have run my classifier against a few thousand images. My process right now, is to save the prediction to my DB then based on the winning prediction having my server move the classified image to a folder with that label.  This way I end up with folders that should mostly be right.  I then have to go through the folders and clean them up. Is this what others do? I was also thinking about making a UI that would help do this as right now when dragging and dropping images to new folders I sometimes mess up, which takes time to fix.\n\nIs there something out there already that can help me review the predictions? \n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ep5523/question_how_do_you_review_predictions_and_update/"}, {"autor": "SagaciousRaven", "date": "2020-01-15 16:14:34", "content": "What is this called? /!/ I have a Fashoin MNist greyscale array/-----> picture !!!  such as:\n\n    [\n      [ [1], [0] ],\n      [ [0], [1] ]\n    ]\n\n(It's small for visualization's sake.) A 2x2x1 checker and I want to \"resize\" in 3x3x3 it like:\n\n    [\n      [ [1,  1,  1],    [.5, .5, .5],   [0,  0,  0]  ],\n      [ [.5, .5, .5],   [.5, .5, .5],   [.5, .5, .5] ],  # \"smoothed\" row from enlarging\n      [ [0,  0,  0],    [.5, .5, .5],   [1   1,  1]  ]\n    ]\n\nAs if I was resizing a picture, and converting the mono greyscale values into an RGB vector.\n\nMy Google-fu sults in numpy's reshape and resize methods, which do something different.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ep4a8f/what_is_this_called/"}, {"autor": "seriousgourmetshit", "date": "2020-01-15 04:37:11", "content": "Flattening a Matrix. /!/ A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b \u2217\u2217 c \u2217\u2217 d, a) is to use:\n\nX_flatten = X.reshape(X.shape[0], -1).T\n\nCan someone please help me understand exactly what this code is doing to flatten the -----> image !!! ? I understand what it is doing, but now how it is doing it. \n\nThis is in the context of flattening an image to a single vector.\n\nThank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eoxhwn/flattening_a_matrix/"}, {"autor": "dfd0226", "date": "2020-01-15 01:23:42", "content": "Are there workflow tools to streamline -----> image !!!  tagging and export to database? /!/ Hi learn machine learning!\n\nAre there **workflow tools you use to extract information from raw image data sources** (i.e., scanned PDFs)? \n\nFor example, I have a ton of PDF data from a research project where I want to be able to **tag things that students draw** around chemistry content.\n\nIdeally, I'd like to be able to:\n\n1. **Open a scan** of student work, \n2. **Draw boxes** around the individual molecules &amp; ions (Bonus if there are tools that do the object segmentation automatically), \n3. **Label the objects** with my categories (like molecule or ion), &amp; then\n4. **Export** the clipped image + labels to a database that I could load in Scikit-learn or similar.  \n\nAre there tools designed for this? Thanks for any guidance!\n\nhttps://preview.redd.it/dh2y0e59vta41.png?width=1152&amp;format=png&amp;auto=webp&amp;s=75a094dcb429cb84e6365255b699fdb946b112d3", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eov3b3/are_there_workflow_tools_to_streamline_image/"}, {"autor": "servuslucis", "date": "2020-01-14 16:36:00", "content": "terminology question /!/ &amp;#x200B;\n\nmy input is a single text file and my output is a folder with around 80 files of various types.\n\ni have around a thousand of these matched pairs\n\nwhat im wondering is (unfamiliar terminology is in quotations)\n\ncan  i \"-----> image !!! \" or \"compress\" the whole folder of 80 files. train the nn to  create the compressed file, and then uncompress it manually once the  model is trained? note that none of the files require registry keys or  anything like that. the filenames are important.\n\nive been looking into this on and off for a year now and have finally admitted defeat\n\ni dont expect the model to work perfectly but this step is what im stuck on and am hoping for a little momentum.\n\nso  really what id like to know is, is this possible to even do even if the  trained output is garbage and unusable? if it is possible what  terminology should i be using for this type of data  organizing/structure? is there something similar or helpful you can  point me to?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eonmh1/terminology_question/"}, {"autor": "ahmedbesbes", "date": "2020-01-14 14:37:40", "content": "Understanding deep Convolutional Neural Networks \ud83d\udc41 with a practical use-case in Tensorflow and Keras /!/ In this post, you'll learn about Convolutional Neural Networks and how to use them for classification problem:\n\nThe article is broken into 4 parts:\n\n1. **Present the dataset and the use-case and explain the complexity of the -----> image !!!  classification task**\n2. **Go over the details about Convolutional Neural Nets by going through their inner meachanisms and the reason why they are more suitable to image classification than ordinary neural netwoks**\n3. **Set up a deep learning dedicated environment on a powerful GPU-based EC2 instance from Amazon Web Services (AWS)**\n4. **Train two deep learning models: one from scratch in an end-to-end pipeline using Keras and Tensorflow, and another one by using a pre-trained network on a large dataset**\n\nHappy learning !\n\nPost \u27a1\ufe0f\u27a1\ufe0f\u27a1\ufe0f [https://www.ahmedbesbes.com/blog/introduction-to-cnns](https://www.ahmedbesbes.com/blog/introduction-to-cnns)\n\n[Convolutional Neural Network](https://preview.redd.it/m74f173k9ra41.png?width=913&amp;format=png&amp;auto=webp&amp;s=dc4b51e489c3878fe301b2656c84820e4528b755)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eom2nn/understanding_deep_convolutional_neural_networks/"}, {"autor": "Karboxyl", "date": "2020-01-14 07:17:43", "content": "[Question - TensorFlow/Python] Save result/classification of an object as a variable and use it in a filename /!/ I am new to Python and TensorFlow, and am doing some object recognition using the TensorFlow API. Once an object has been identified, I want to save the identification label as a variable and then use that variable in a filename when saving the classified -----> image !!!  using cv2.imwrite(). Is this possible?\n\nHere's the code I'm currently working with:\n\n    # Iterate through the files to be analysed in the directory\n    for filename in os.listdir(PATH_TO_DIR):\n        if filename.lower().endswith(\".jpg\"):\n            # Load image using OpenCV and\n            # expand image dimensions to have shape: [1, None, None, 3]\n            # i.e. a single-column array, where each item in the column has the pixel RGB value\n            image = cv2.imread(filename)\n            image = cv2.resize(image, (800,600))\n            image_expanded = np.expand_dims(image, axis=0)\n            # Perform the actual detection by running the model with the image as input\n            (boxes, scores, classes, num) = sess.run(\n                [detection_boxes, detection_scores, detection_classes, num_detections],\n                feed_dict={image_tensor: image_expanded})\n            # Draw the results of the detection (aka 'visulaize the results')\n            vis_util.visualize_boxes_and_labels_on_image_array(\n                image,\n                np.squeeze(boxes),\n                np.squeeze(classes).astype(np.int32),\n                np.squeeze(scores),\n                category_index,\n                use_normalized_coordinates=True,\n                line_thickness=4,\n                min_score_thresh=0.60)\n            # All the results have been drawn on image. Now display the image.\n            cv2.imshow('Object detector', image)\n            # Press any key to close the image\n            cv2.waitKey(0)\n            # Save the image\n            cv2.imwrite(os.path.join(NEW_DIR_PATH, 'ID_'+filename), image)\n            # Clean up\n            cv2.destroyAllWindows()\n\n I've been slightly modifying code from [https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/blob/master/Object\\_detection\\_image.py](https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/blob/master/Object_detection_image.py)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eohzm3/question_tensorflowpython_save/"}, {"autor": "lgmorgado", "date": "2020-04-16 14:10:35", "content": "[Career Help] From Engineering to Data Science /!/  Hello guys!  \nI'm a 24 yrs old Industrial Engineering student ( 9th semester out of 10 ) in Rio, Brazil.  \nThroughout all my academic life, I've studied MS excel, VBA, then Power BI with some programming.  \nDuring  the year of 2018, I took part in a business consulting project ( that  mainly focused on processes optimization, marketing, business identity,  etc ) where I stepped up because of  critical thinking and analytical  skills, which made me really proud of myself. I was able redesign lots  of processes and helped some other members develop these skills.  \nOnce I was done with the project I began researching for an internship program.  \nOn  May of 2019, I was very fortunate to start a Industrial Engineering  internship on Michelin, one of the most respected tires industries out  there. And once again I was recognized for good analytical skills,  agility with numbers and was learning stuff really quick.  \nHowever,  after a couple months I realized Industrial engineering was more about  being able to find physical solutions to processes based on analytical  data then analyzing data for the sole purpose of getting new insights.  And I've never been interested in tools, machines, manufacturing and  such, only data and analysis. Because of that, I dropped off to focus on  what interested me more: anything more related to analytics.\n\n**TL;DR:** *Just  left an internship program in Industrial Engineering because I feel  like I should give a shot to analytics, and would like to get to know  better about my opportunities to learn and develop skills*.\n\nAfter  I left the job, I strumbled upon some key words: Big data, Machine  learning, Cloud computing, Python, Ansible, Jenkins, and been into a  hard time trying to figure it all by myself. Even started a python and  data engineer courses.\n\nI woud like  some tips on what to figure out first, some good professionals/ content  creators I could follow on social media, and **most importantly**,  whats the big -----> picture !!!  of it all? How all these pieces fit together?  What's the ways from servers' infrastrutcture to data scientists?\n\nThank you so much for your time,  \nI appreciate it!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g2flsp/career_help_from_engineering_to_data_science/"}, {"autor": "m1thr", "date": "2020-04-16 00:55:40", "content": "Feature prepration for NN /!/ Hey, I am just learning how to implement and use NN. Most of the examples covers area of -----> image !!!  recognition which is pretty straightforward as -----> image !!!  can be represented as byte array.\n\nProject I would like to start with is something that will work as event prioritizer where most of the features would be text sentences like\nName\nCategory\nLocation\nParticipants\n\nI have read already about tokenization used for example in NLP but I am not quite sure if I understand it correctly. \nSimplified Tokenization of name \u201cbirthday of kate\u201d will result in matrix [1,2,3] while category \u201cbirthday\u201d will be just [1]\n\nIf I tokenize a matrix of features of a single training example from matrix [n,1] I will get matrix [n,m] where m is length of the kindest sentence in a feature set. \n\nThis input will be proper to be feature in NN?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g24vgu/feature_prepration_for_nn/"}, {"autor": "umdthrowaway141", "date": "2020-04-15 20:16:01", "content": "Which -----> image !!!  package is most general-use? /!/ I'm starting out in computer vision. Some of my classes use PIL, I like to use cv2 because it lets me work easily with numpy arrays, and I know some other researchers who use skimage. \n\nI know there is no \"best\" package, but could someone tell me the pros and cons of each?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g1zt4u/which_image_package_is_most_generaluse/"}, {"autor": "Theweekendstate", "date": "2020-04-15 19:46:16", "content": "Object localization using ML and traditional template matching /!/ I have a pretty strong -----> image !!!  processing background, where I've used techniques like template/patch matching for finding objects in -----> image !!! s and/or -----> image !!!  registration problems. Here though, the template image is typically fixed and pre-determined -- there is no concept of \"learning\" a general template.\n\nWhen looking through ML techniques, the existing architectures seem to be based on training a network with \"full\" images that have been pixel-wise labelled into classes, or, a with a list of object bounding boxes and their classes. But, I can't help but wonder if the template matching approach isn't amenable to ML techniques - that is, training a network with a bunch of images of (e.g.) cats, then, we use this learned cat template to do the localization?\n\nAdmittedly I don't really understand some of the more advanced stuff (region prediction, bounding box regression), so maybe this is in fact what's happening?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g1z8fp/object_localization_using_ml_and_traditional/"}, {"autor": "[deleted]", "date": "2020-04-15 13:29:06", "content": "Looking for courses cover -----> image !!!  recognition in depth /!/ [deleted]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g1rwxy/looking_for_courses_cover_image_recognition_in/"}, {"autor": "ai_yoda", "date": "2020-04-15 09:12:26", "content": "6 GAN architectures nicely explained /!/ Hi all, \n\nWe've put together an article explaining in detail 6 common GAN architectures:\n\n* CycleGAN\n* StyleGAN\n* pixelRNN\n* text-2------> image !!! \n* DiscoGAN\n* lsGAN\n\nYou can see how are they different and where to use them with links to papers and implementations.\n\nI think it can be quite useful.\n\nAny feedback welcome.\n\n&amp;#x200B;\n\n[Read the article](https://neptune.ai/blog/6-gan-architectures?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=blog-6-gan-architectures)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g1oi8u/6_gan_architectures_nicely_explained/"}, {"autor": "PsyRex2011", "date": "2020-04-14 22:07:54", "content": "Free Udemy Courses - 3 Courses from Hadelin de Ponteves &amp; SuperDataScience Team /!/ UPDATE : 2 NEW COURSES ADDED. Altogether 5 free courses, 1 for a limited time. \n\nHello everyone,\n\nJust as promised, I'm sharing my another 3 courses that I found to be free. Personally I haven't completed any of them and they appear to be relatively new. Feel free to leave a review if someone has competed these before...\n\n[A Complete Guide on TensorFlow 2.0 using Keras API](https://www.udemy.com/course/tensorflow-2/?couponCode=STAYATHOME_GIVEAWAY) - THIS WILL BE FOR A LIMITED TIME! \n\n[Logistic Regression Practical Case Study](https://www.udemy.com/course/logistic-regression-cancer-detection-case-study/)\n\n[Artificial Neural Network for Regression](https://www.udemy.com/course/linear-regression-with-artificial-neural-network/)\n\n[Natural Language Processing (NLP) with BERT](https://www.udemy.com/course/natural-language-processing-with-bert/)\n\n[Practical Deep Learning: Image Search engine](https://www.udemy.com/course/practical-deep-learning------> image !!! -search-engine/)\n\nAs before, I'm no way affiliated with the course owners. And I personally feel like they are using this \"free enrollment\" system just to grab the first few hundred students. But since it doesn't cost us anything and since these people have created some nice courses before, it might be a good chance to get free access. \n\nHope these will help you to stay productive at home and learn new skills!\n\nEDIT : Updated the list", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g1f07r/free_udemy_courses_3_courses_from_hadelin_de/"}, {"autor": "neuron_whisperer", "date": "2020-04-14 21:30:15", "content": "Pure NumPy implementation of convolutional neural network (CNN) /!/ tl;dr up front -\n\nI wrote a pure NumPy implementation of the prototypical convolutional neural network classes (ConvLayer, PoolLayers, FlatLayer, and FCLayer, with subclasses for softmax and such), and some sample code to classify the MNIST database using any of several architectures.\n\nAlong the way, I found that the typical ConvLayer example was absurdly inefficient, so I provided an equivalent solution that uses NumPy more efficiently and achieves a 1,000-fold performance improvement.\n\nHere's [the GitHub repository](https://www.github.com/neuron-whisperer/cnn-numpy), including a readme and a FAQ about the project and the new \"Stride Groups\" technique.\n\n---\n\nDuring my machine learning studies, I spent some time completing Dr. Andrew Ng's Deep Learning Coursera sequence, which is generally excellent. The main example, \"Building a Convolutional Network Step By Step,\" provides a NumPy-based implementation of a convolutional layer and max / average pooling layers and is a great learning exercise. However, looking back on the code, I was disappointed to find that it has some problems.\n\n1) The code is provided in a Jupyter notebook with a lot of intermediate exposition and unit tests. While it is a great learning exercise, it makes for a poor reference for the code of an actual CNN implementation.\n\n2) The code isn't very readable. Global functions, variables passed around in a clumsy dictionary called \"cache\" with string names for variables, cached variables that are never used again... it's quite messy.\n\n3) The example isn't complete. The ConvLayer function calculates dW/db, but doesn't update the weights. It doesn't have a flattening layer, or any fully-connected layers, or any loss function or training algorithm - all of those are provided in different exercises.\n\n4) Worst of all, it doesn't have any kind of application to a real problem. The next exercise in the course involves applying machine learning to classify the [MNIST handwritten digits data set](http://yann.lecun.com/exdb/mnist/)... but it doesn't use the NumPy code at all. Instead, it makes a hard transition into TensorFlow. \"Now that you know how CNNs work, forget all of that under-the-hood stuff and just use this platform!\" (Worse, the TensorFlow code is all 1.x, so it won't even run in today's TF 2.x environments without a bunch of backwards-compatibility edits.)\n\nGiven all of that - I set aside some time to take the example code, clean it up, and add the bits that are missing. The results are nice: every basic layer type as a Python class and a Network class to train them.\n\n[https://www.github.com/neuron-whisperer/cnn-numpy/cnn_numpy.py](https://www.github.com/neuron-whisperer/cnn-numpy/blob/master/cnn_numpy.py)\n\nThen I wrote some code to use it to classify the MNIST handwritten digits data set:\n\n[https://www.github.com/neuron-whisperer/cnn-numpy/cnn_numpy.py](https://www.github.com/neuron-whisperer/cnn-numpy/blob/master/mnist.py)\n\nThe problem was that the CNN class is so slow that it's unusable! It is educational... but non-functional.\n\nBoth the ConvLayer and PoolLayer classes feature a four-layer-deep iteration for both forward propagation and backpropagation:\n\n    for i in range(number_of_samples):\n        for h in range(output_height):\n            for w in range(output_width):\n                for f in range(number_of_filters):\n                    # do some stuff\n\nLet's say you want to apply a simple CNN to the MNIST database, which has 70,000 images. You choose a 95%/5% train/test split, so the training set has 65,500 inputs. Each -----> image !!!  is 28x28x1. Let's say you want to apply one convolutional layer with 32 filters of size 3x3, stride 1, padding 0. (A 3x3 filter of stride 1 is shifted 26x26 steps over each image.)\n\nBased on those hyperparameters, this iteration requires 65,500 * 26 * 26 * 32 = 14,168,960,000 iterations of this loop. Twelve *trillion* iterations for forward propagation over one training epoch. Backpropagation requires another Twelve trillion. Altogether, it requires about 36 hours *for one epoch* on a decently powered workstation (no GPU, because NumPy).\n\nNow, NumPy is really fast - if you use it right. But no matter how optimized it may be, 28 trillion calculations is going to take forever.\n\nSo I redesigned it to minimize iteration, and instead came up with a new computational technique that focuses on array slices to align the parts of the input tensor with the corresponding parts of the filter tensor.\n\nThe new implementation is 100% equivalent to the prototypical example - you can drop it right into place. And it runs *1,000 times faster*. You can actually train this updated CNN on the MNIST database to under 5% error in about five minutes. It's certainly nowhere near as efficient as TensorFlow or PyTorch, but it works reasonably well for simple problems like the MNIST data set.\n\n[https://www.github.com/neuron-whisperer/cnn-numpy/cnn_numpy.py](https://www.github.com/neuron-whisperer/cnn-numpy/blob/master/cnn_numpy_sg.py)\n\nI've provided a complete write-up of the original technique, my extension into a full implementation, and my optimization. A FAQ is provided for more information.\n\nI hope that other students of deep learning find this useful or interesting. Comments welcome.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g1ebem/pure_numpy_implementation_of_convolutional_neural/"}, {"autor": "[deleted]", "date": "2020-04-14 20:11:26", "content": "What are some cool -----> image !!!  classification use cases? /!/ [deleted]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g1cx6n/what_are_some_cool_image_classification_use_cases/"}, {"autor": "braincrowd", "date": "2020-04-14 14:51:31", "content": "I have labeled data now what? Gender and Age detection. /!/ Hello\n\nFor a Project that im doing I have a very large dataset (couple Millions of Images) of People. (Most of the time i have multible images per person)\n\nI have about 1 Million Images labeled with correct Age and Gender of the person.\n\nI would like to train a network on the labeled data i already have to help label the rest of the data.\n\nWhat i would like as a label is a agegroup (labeled data has age as number but a appromiate range is enough) and gender of the person (male/female)\n\nI would say i understand the basic principles and already used some finished tensor flow and python scripts but i dont really know where to start with this project. I would prefer a simple already made tool/script/libary/network that i can only feed my dataset to train.\n\n&amp;#x200B;\n\nI have all Images in Full (-----> Picture !!!  of upper body) and in crop of only face. What is better to train the algorythm on? Probably only the face i guess to minimize training on wrong features right?\n\n&amp;#x200B;\n\nThanks a lot for your help.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g16wsa/i_have_labeled_data_now_what_gender_and_age/"}, {"autor": "Bryan-Ferry", "date": "2020-04-14 13:29:42", "content": "Choosing a CNN Architecture for MNIST Style Data /!/ I am currently working on a project based on analysing simple images similar to those in the MNIST data set (though they are not actually hand written characters). I have built some CNN models for this data set using Tensorflow and I am looking to optimise their architecture for better results. \n\nThis kaggle article proposes a methodology for testing different architectures:\nhttps://www.kaggle.com/cdeotte/how-to-choose-cnn-architecture-mnist\n\nAre these pairs of convolution-subsambling layers still the preferable method for simple -----> image !!!  classification tasks? Is this method of selecting a CNN architecture still considered a good option? \n\nThanks in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g15jky/choosing_a_cnn_architecture_for_mnist_style_data/"}, {"autor": "aidang95", "date": "2020-03-10 14:31:13", "content": "What is the best api for this task? /!/ \nI want to create a python/flask app where the user can upload a -----> picture !!!  of an animal and the algorithm based on its training will be able to specify what species of animal it is.\n\nI know there\u2019s a fair few apis out there but I was wondering what is the best (and most simple) to use? Preferably with clear docs as I\u2019m still learning myself and this is for a side project of mine", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fgeovd/what_is_the_best_api_for_this_task/"}, {"autor": "edude10", "date": "2020-03-09 19:58:28", "content": "A graphic designer working on a book - Looking for someone to collaborate with /!/ Hi!\n\nI'm Elliott, a graphic designer from London working on a publication looking at CCTV surveillance and loitering.\n\nCurrently I'm working on a -----> photography !!!  series within the book using screengrabs of insecam (an open source CCTV website). I'd like to somehow (I'm a complete novice but keen to learn) work on some form of pose(?) recognition that could essentially 'watch' the video and try and capture 'loitering' (if say someone stands still for longer than three seconds) and then save that as an image. I'm not fussed on how accurate the ML is, infact it getting stuff wrong would be equally facinating!\n\nI was wondering if anyone would like to work with me on this/point me in the right direction as to how to even get about starting.\n\nPlease dm if interested obviously you'd be credited in the book as a collaborator, it's also going to be very pretty and well designed I promise :)))) and I'd send you a copy of the final printed book!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fg0qjd/a_graphic_designer_working_on_a_book_looking_for/"}, {"autor": "spunker540", "date": "2020-03-09 00:46:43", "content": "Is there a good CMS for managing machine learning -----> image !!!  datasets? /!/ I am doing a lot of machine learning with images and have been creating my own datasets as well as downloading existing well-known data sets. I have photos that I may want to classify or tag into different datasets (e.g both \"architecture\" and \"church\") and I also need to keep track of different sizes / crops from the originals. \n\nIs there any CMS that can help me manage all this, that maybe offers good filesystem or API access to point to when I train my models?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ffmc8v/is_there_a_good_cms_for_managing_machine_learning/"}, {"autor": "MatzeDaBoss", "date": "2020-03-08 18:31:30", "content": "Using a CNN to predict class and time simulateously /!/ Hi,\n\nI am relatively new to the concept of CNNs, and I would like to know if (and how) the following would be possible using a CNN.\n\nMy data consists of -----> image !!! s, where one type of -----> image !!!  is the \"wildtype\", and there are six variations, each representing another class. The classes originate from the wildtype and morph over time. For example, say the wildtype is a circle, and the two classes are a triangle and a square. I know that the circle takes ten hours to morph into one of the shapes, and I have data from every hour in between. Given a new picture, is it possible to predict which variation and which timeframe  the new picture belongs to (so my output would be for example \"square, six hours in\")? \n\nHow would I train such a model? Can I just give it the image and add an extra dimension for time? Or is there a smarter way to do this?\n\nThank you for your help :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ffgpmk/using_a_cnn_to_predict_class_and_time/"}, {"autor": "Milderf", "date": "2020-03-07 18:05:28", "content": "I need help on how to approach this problem! /!/ Hey guys, I have recently been working on a project that I am discovering is really difficult for me. Here is the problem:\n\nI am given 2 devices, a phone and a watch. Each device comes with data for both an accelerometer and a gyroscope for the tradition Cartesian coordinate directions (x, y, z). For each phone accelerometer, phone gyro, watch accelerometer, and watch gyro, there is also an activity label (A-S excluding N) that indicates walking, clapping, eating, sleeping, etc. There are 51 users/subjects studied. [Here is where the data comes from to get a better idea.](https://archive.ics.uci.edu/ml/datasets/WISDM+Smartphone+and+Smartwatch+Activity+and+Biometrics+Dataset+)\n\nMy task is to see if I can predict what activity any user is doing. However, I am getting extremely stuck. I discovered there is a whole world of signal processing for time series data out there that I don't quite understand because I am fairly new to this subject. I linked a -----> photo !!!  to this post to demonstrate how there is a bunch of noise that I don't know how to deal with.\n\nSo, can anyone give me some tips on how to deal with all of this noise? My only idea at the moment is to smooth it with a moving average, but that is resulting in some bad predictions. Also, since this is a classification problem, I plan on using SVM and random forests to see which one works out better. I am getting SUPER stressed over this problem lately and I just need some guidance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/feytrg/i_need_help_on_how_to_approach_this_problem/"}, {"autor": "Theweekendstate", "date": "2020-03-07 18:05:16", "content": "Many small files vs. single large file for training data storage? /!/ Currently, I'm generating synthetic datasets on the order of 50,000x64x128x1. With deep CNNs, I'm not able to keep this all in my GPU's memory so I'm having to use a flow generator approach (custom function for fit_generator in keras). \n\nMy question is just in regards to the best approach (speed-wise) for physically storing such a dataset: Am I better off storing 50,000 individual 64x128 -----> image !!!  files OR should I just store the entire thing as a single big file, and keep track of my position within it?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/feytn4/many_small_files_vs_single_large_file_for/"}, {"autor": "_Xyborg_", "date": "2020-02-22 20:33:14", "content": "Are there any good tutorials or papers on having a robot arm pickup a target object? /!/ Like I have read some papers that talk about using Reinforcement Learning to pickup a target object, but are there any lessons on using a Raspberry Pi and a -----> Camera !!!  or some sensors/interface to pick up a target object?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f7yclh/are_there_any_good_tutorials_or_papers_on_having/"}, {"autor": "Naveen-reptile", "date": "2020-02-22 14:14:09", "content": "Needed help in knowing how to implement our project /!/ Hello everyone, I am beginner to machine learning. And I am working on a project to identify a plant disease of regional crops. Me and my team decided on using KNN as we were introduced to it, we are comfortable with it and feel like it can be usefull for the project we are developing.\n\n Past 1 week we have collected datasets of images consisting of disease of various types on crops such as maze, corn etc.( And still trying to gather more from farmer association and such). Now that we have the datasets, we dunno how to take it as input. Do we have to apply -----> image !!!  processing techniques other than canny for edge detection and grayscaling.\n\n I would be happy to have any kind of suggestions. We have went through papers relating to the topic, but we need knowledge on how to implement it practically. \n\nI attended the workshop on ml and worked on the mini project of identifying our face , but we only used our images of a single person, so it got me confused on hot to use more than one category of images.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f7t0xa/needed_help_in_knowing_how_to_implement_our/"}, {"autor": "Tesla-Turing", "date": "2020-02-22 10:16:53", "content": "On how to grow a Random Forest /!/  \n\nHello everybody,\n\nI'm preparing a Machine Learning exam and after carefully reading how the CART algorithm works in Random Forests a question arose to my mind:\n\n![img](hmr9h42nagi41 \" (credits for the -----> picture !!! : Elements of Statistical Learning, by Hastie, Tibshirani and Friedman; Springer Series in Statistics)\")\n\n&amp;#x200B;\n\n \n\nIs the sampling of m variables from the total p variables done only once for each bootstrapped sample or if I keep randomly selecting m variables from the remaining p-n (where n is the number of splits already occurred) at each sub-split?\n\nCould one of you be so kind to clarify this to me?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f7qsxt/on_how_to_grow_a_random_forest/"}, {"autor": "PullThisFinger", "date": "2020-02-22 03:38:26", "content": "Scikit------> image !!!  tutorial nearly done. What's next? /!/ I've made a first pass through all of the material on Scikit-image.com &amp; will post the Jupyter notebooks on GitHub once they are cleaned up. \n\nThis is an extremely interesting topic. What should I study next in the field of machine vision? There's literally hundreds of subgenres available on [paperswithcode.com](https://paperswithcode.com) and the state of the art is changing daily. Also: I'm stuck, for now, with a GPU-less local machine and limited bandwidth at home - so spinning up AWS instances is not ideal.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f7myll/scikitimage_tutorial_nearly_done_whats_next/"}, {"autor": "GoBacksIn", "date": "2020-02-22 00:07:51", "content": "How can I predict the following value of time series using Tensorflow predict method? /!/ Thank you for reading. I'm not good at English.\n\nI am wondering how to predict and get future time series data after model training. \n\nI would like to get the values after N steps.\n\nI wonder if the time series data has been properly learned and predicted.\n\nHow i do this right get the following(next) value?\n\nI want to get the next value using like model.predict or etc\n\nI have `x_test` and `x_test[-1] == t` So, the meaning of the next value is `t+1, t+2, .... t+n, `\n\nin this example **I want to get `t+1, t+2 ... t+n`**\n\n\n\n\n\n#First#\nI tried using stock index data\n\n\n```\ninputs = total_data[len(total_data) - forecast - look_back:]\ninputs = scaler.transform(inputs)\nX_test = []\nfor i in range(look_back, inputs.shape[0]):\n    X_test.append(inputs[i - look_back:i])\nX_test = np.array(X_test)\npredicted = model.predict(X_test)\n```\n\nbut the result is like below\n\n[![enter -----> image !!!  description here][1]][1]\n\nThe results from `X_test[-20:]` and the following 20 **predictions looks like same.**\n\nI'm wondering if it's the correct train and predicted value.\n\nand I'm wondering if it was a right training and predict.\n\n[full source](https://gist.github.com/Lay4U/2e1759a0e435ff95b7a017e301db634f)\n\nThe method I tried first did not work correctly.\n\n\n\n\n#Seconds#\nI realized something is wrong, I tried using another official data\n\nSo, I used the time series in the **Tensorflow tutorial** to practice predicting the model.\n\n```\na = y_val[-look_back:] \nfor i in range(N-step prediction): #predict a new value n times.\n    tmp = model.predict(a.reshape(-1, look_back, num_feature)) #predicted value     \n    a = a[1:] #remove first     \n    a = np.append(a, tmp) #insert predicted value\n```\nThe results were predicted in a linear regression shape very differently from the real data.\n\n![2](https://imgur.com/7tenqRd.png)\n\nOutput a linear regression abnormal that is independent of the real data:\n\n[full source](https://gist.github.com/Lay4U/96e0ba8d8c251046e89eae4bc5d40510)  (After the 25th line is my code.)\n\n  [1]: https://i.stack.imgur.com/L6qL1.png\n\n\nI'm really very curious that **How can I predict the following value of time series using Tensorflow predict method**\n\nI'm not wondering if this works or not working with a theoretically. I'm just wondering how to **get the following n steps using the predict method.**\n\nThank you for reading the long question. I seek advice about your priceless opinion.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f7kbo7/how_can_i_predict_the_following_value_of_time/"}, {"autor": "cmillionaire9", "date": "2020-02-21 18:45:45", "content": "Experiment with the core of -----> image !!!  processing", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f7fl6t/experiment_with_the_core_of_image_processing/"}, {"autor": "riribeng", "date": "2020-02-21 09:29:47", "content": "QUESTION -----> IMAGE !!!  ANNOTATION /!/ **Question :**\n\n should i go through image annotation before training a localizing car damage algorithm  ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f78qol/question_image_annotation/"}, {"autor": "clorky123", "date": "2020-02-21 08:20:02", "content": "Bechelor's thesis on reinforcement learning? /!/ Hey guys, I'm a beginner in machine learning and a second year in CS, I've written a seminar paper on supervised learning and -----> image !!!  classification last semester.\n\nI've extended this work in Graphics algorithms class using imgaug and cutout, explaining how these algorithms work and why they are beneficial to the quality of the model.\n\nPretty much only very basic stuff. I believe I have a very basic understanding on how it all works under the surface. \n\nMy question is - is it possible for a beginner like me to write a bechelor's thesis on reinforcement learning f.e. in a game like Doom? \"Making\" the model complete the first level? I've seen some videos, like using reinforcement to complete the first level of Mario or to learn playing Snake much better than a human would.\n\nWhere should I start? Is it even possible? Any good courses or \"guides\" on this subject?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f785ag/bechelors_thesis_on_reinforcement_learning/"}, {"autor": "V3yhron", "date": "2020-02-21 05:29:36", "content": "Question regarding Deep SORT /!/ Correct my understanding if I have misinterpreted this but how does Deep SORT work when new people enter the frame of the -----> image !!! . My understanding is that you train it by annotating each object in the frame\u2019s bounding box so that when that object moves it uses both kalman predictions and attempting to classify the objects to deal objects being blocked at certain points. But say you\u2019ve done all this training and it was looking at a camera or a basketball game. How would it handle someone it hasn\u2019t seen before coming into frame. It doesn\u2019t have a classification for them. Is this valid or when I do my classifications should I make my classifications more broad like \u201cperson\u201d or \u201chome team\u201d, \u201caway team\u201d? If the latter is the case then how does it use classification to differentiate between people at all?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f76ilg/question_regarding_deep_sort/"}, {"autor": "ivannson", "date": "2020-02-20 16:04:32", "content": "What's a good methodology for semantic segmentation? /!/ I wrote a simple fully convolutional neural net that segments an -----> image !!!  into 2 classes, but my approach is pretty head on. I was wondering if someone could suggest (or link) a good methodology for semantic segmentation, such as data augmentation techniques?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f6v6bw/whats_a_good_methodology_for_semantic_segmentation/"}, {"autor": "AdvancedManufacture", "date": "2020-02-20 14:02:12", "content": "Training binary -----> image !!!  classifier on a single label only? (No negative sample) /!/ Here is the case: I have a computer vision algorithm which can spot trash on a filter made up of vertical steel bars. It simply looks for interruptions in vertical edges found via Hough transform. But it does under-perform when the camera image suffers from glare/mist and other quality issues. I want to try training a shallow CNN to do the same job in a more robust manner. \n\nIt would be a binary classifier with only two outputs- 'I see the filter bars' (they're clean) OR 'I don't see the filter bars' (they're obstructed, alert human user). Because we don't want to make any assumptions about what the trash will be (could be anything from grass to traffic cones) I am unsure how to create an obstructed filter images set. Two solutions come to mind: \n\n1. Create random crops from an unrelated image corpus and label all of them as trash, feed them into train/dev set. Train/dev set has labels for bars and trash.\n2. Train/dev set contains only clean filter samples. Image samples are diverse in terms of light conditions image quality etc. Training identifies features shared between all clean filter samples. Train/dev set only has label for bars. During testing the images with very low recognition score (human-set threshold) are marked as likely to contain trash.\n\nIs training the second type of model a valid approach? It seems like a type of unsupervised learning problem focused on similarity testing. What do I need to know to implement it? I have just finished some online courses and I'm starting to learn to implement my own models, preferably starting with PyTorch.\n\nAll tips are appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f6thnq/training_binary_image_classifier_on_a_single/"}, {"autor": "aloser", "date": "2020-11-23 01:25:20", "content": "Using Apple's CreateML to train your iPhone to recognize sharks in a live -----> camera !!!  feed without writing a single line of code", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jz80j8/using_apples_createml_to_train_your_iphone_to/"}, {"autor": "emdayr123", "date": "2020-11-22 18:44:53", "content": "How do I make a cnn that can be trained to produce 3D -----> image !!! s from a 2D -----> image !!! ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jz0wzu/how_do_i_make_a_cnn_that_can_be_trained_to/"}, {"autor": "spmallick", "date": "2020-11-22 06:09:05", "content": "Image Classification with OpenCV for Android /!/ Our last post was all about using [OpenCV with Java](https://click.convertkit-mail.com/38uvxqr24vskh04eddhp/l2heh6uo524gn7u6/aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL2ltYWdlLWNsYXNzaWZpY2F0aW9uLXdpdGgtb3BlbmN2LWphdmEv) ([https://www.learnopencv.com/image-classification-with-opencv-java/](https://www.learnopencv.com/image-classification-with-opencv-java/)). This week, we are sharing how to use OpenCV in an Android application using the same example of -----> image !!!  classification.\n\n[https://www.learnopencv.com/image-classification-with-opencv-for-android/](https://click.convertkit-mail.com/38uvxqr24vskh04eddhp/m2h7h6uok59rqpim/aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL2ltYWdlLWNsYXNzaWZpY2F0aW9uLXdpdGgtb3BlbmN2LWZvci1hbmRyb2lkLw==)\n\nand the code is at\n\n[https://github.com/spmallick/learnopencv/tree/master/DNN-OpenCV-Classification-Android](https://click.convertkit-mail.com/38uvxqr24vskh04eddhp/dphehmuqz6k3o9tm/aHR0cHM6Ly9naXRodWIuY29tL3NwbWFsbGljay9sZWFybm9wZW5jdi90cmVlL21hc3Rlci9ETk4tT3BlbkNWLUNsYXNzaWZpY2F0aW9uLUFuZHJvaWQ=)\n\nhttps://preview.redd.it/xfnlc50dgq061.png?width=600&amp;format=png&amp;auto=webp&amp;s=37a9d439e279dd9701b829eab2e910a5a65adedc", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jyqu40/image_classification_with_opencv_for_android/"}, {"autor": "comeculosgrandesymed", "date": "2020-11-21 23:39:03", "content": "Face detection on security -----> camera !!! ? /!/ As we know, there are several object detection models that have been trained to detect faces, specially frontal faces, they all have different accuracies.\n\nHowever, I've tried using most of those available in OpenCV, such as haar cascade models, DNNs, Dlib and some others. The thing is, those models are not intented to be succesful in detecting faces from something like a security camera footage (which is my goal).\n\nI want to either find, or train by myself, a model that allows me to detect FRONTAL and PROFILE faces, from a not-so-good quality and certain distance (let's say 7-12 foot) considering that's the footage that it will receive from the security camera. I'm using OpenCV.\n\nAny recommendations? (I'm new in the Computer Vision field so please be as detailing as you can) THANKS IN ADVANCE.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jyl5hl/face_detection_on_security_camera/"}, {"autor": "Fit_Faithlessness122", "date": "2020-11-21 19:22:08", "content": "How to create a 3D reconstruction from a 2D -----> image !!! ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jygraq/how_to_create_a_3d_reconstruction_from_a_2d_image/"}, {"autor": "vaseline555", "date": "2020-11-21 15:36:30", "content": "Can anybody help me to understand GAN's mechanism? /!/ Hi all!\n\nI am now trying to be familiar with GANs using PyTorch.\n\nAccording to this DCGAN tutorial ([https://pytorch.org/tutorials/beginner/dcgan\\_faces\\_tutorial.html](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)),\n\nthere exist following code lines:\n\n    . \n    . \n    . \n    # Generate fake -----> image !!!  batch with G\n    fake = netG(noise) \n    . \n    . \n    . \n    # Update D \n    optimizerD.step()\n      \n    ############################ \n    # (2) Update G network: maximize log(D(G(z)))\n    ########################### \n    netG.zero_grad() \n    label.fill_(real_label)  \n    \n    # fake labels are real for generator cost \n    # Since we just updated D, perform another forward pass of all-fake batch through D \n    output = netD(fake).view(-1) \n    \n    # Calculate G's loss based on this output\n    errG = criterion(output, label) \n    \n    # Calculate gradients for G \n    errG.backward() \n    D_G_z2 = output.mean().item() \n    \n    # Update G \n    optimizerG.step() \n\nAccording to original GAN paper, for updating parameters of a generator (which is denoted as 'netG' in the code), I need to calculate \\\\nabla\\_(\\\\theta\\_g){\\\\log{(D(G(z))}}, which is equivalent to 'criterion(output, label)' &amp; 'errG.backward()' part, right?\n\nWhat I am wondering now is that when calling \\`errG.backward()\\`, which one is correct?\n\n1. Parameters of netD is JUST used for calculating ouptut of netD(fake), and calling backward on generators' output which is internally coming from netD(fake) (== netD(netG(noise)) ), does NOT update parameters of netD; only update ones of netG.\n2. Parameters of netD is also changed as the computational graph on 'fake' ( == netG(noise)) has already been flowed through netD's parameter (i.e. netD(netG(noise)) ).\n\nIt confuses me... can anybody clarify this?\n\nThank you all in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jycool/can_anybody_help_me_to_understand_gans_mechanism/"}, {"autor": "Independent-Square32", "date": "2020-11-21 13:45:39", "content": "The latest AI algorithm to make the -----> image !!!  dance", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jyayyo/the_latest_ai_algorithm_to_make_the_image_dance/"}, {"autor": "tommyk1210", "date": "2020-12-29 22:59:39", "content": "On the fly data augmentation vs saving out augmented images - which is best? /!/ Hey, I\u2019m pretty new to machine learning, although I\u2019ve been a developer for more than a decade.\n\nI\u2019m currently writing a classifier using a CNN and Tensorflow. My classifier essentially classifies trading cards based on their -----> image !!! . I then want to be able to give an image of any given card to the CNN and get back the card type/variant.\n\nRight now I have just over 7000 cards, and for each I have a perfect image for each. I was reading online and saw information data augmentation using Keras. Now, generally these tutorials do on the fly augmentation, generating a bunch of variants from each image type. My problem is I have over 7000 classes, but only 1 image for each. I was thinking of pregenerating 500 augmented images for each and saving them out, and using that as my training/validation data.\n\nThe main problem I have is - the augmentations alone take about 4 days to compute (if anyone knows of a way to multiprocess the augmentation process that would be much appreciated!) so on the fly augment generation seems like a no-go. My machine isn\u2019t being pushed in the slightest (2% CPU and 1% GPU usage, no IO bottlenecks).\n\nIs pregenerating augments the right option here? \n\nThanks :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kmp59z/on_the_fly_data_augmentation_vs_saving_out/"}, {"autor": "hiphop1987", "date": "2020-12-29 14:54:48", "content": "Don\u2019t Make These Pandas Mistakes /!/ Have you ever planned you\u2019d need an hour to finish a short task, but then you spend a whole day working on it? If yes, welcome to my world!\n\nI originally published this in \"[Don\u2019t Make These 3 Pandas Mistakes](https://towardsdatascience.com/dont-make-these-3-pandas-mistakes-f22337a47e31?sk=f04765d3180176c4eb39ebe647c74b74)\", so take a look in case you'll like to avoid more pitfalls with pandas.\n\nOnly a fool learns from his own mistakes. The wise man learns from the mistakes of others.\n\n# How NOT to visualize a Weighted Average\n\nThe weighted average is similar to an ordinary average, except that some data points contribute more than others.\n\n[Weighted arithmetic mean from Wikipedia](https://preview.redd.it/s5ulxhqo25861.png?width=724&amp;format=png&amp;auto=webp&amp;s=442b477d4c30dc6899bd94e1d924bc479e9a55db)\n\nThis mistake with pandas occurred to me when I was working on an Exploratory Data Analysis \u2014 a typical day for a Data Scientist.\n\nI extracted the data from the database, loaded it with pandas and followed a piece of advice from an ancient philosopher:\n\n&gt;A -----> picture !!!  is worth a thousand words.\n\nIn other words, I\u2019ve visualized the data.\n\nSomething caught my eye on the plot and I wasn\u2019t sure what at first.\n\n**Mistake**\n\nLet\u2019s repeat the steps and see if you spot the mistake. First, let\u2019s define a sample DataFrame:\n\n    df = pd.DataFrame({\"bin\": [0, 1, 2, 4, 5], \"value\": [1, 2, 3, 2, 5]})\n\nhttps://preview.redd.it/rqs76vts25861.png?width=1394&amp;format=png&amp;auto=webp&amp;s=2e5d9ec01c4cd79d32af0162ef4714489b8ab07c\n\nCalculate the weighted average, which is 3.15.\n\n    weighted_average = (df.bin * df.value).sum() / df.value.sum()\n\n\u2026 and visualize the plot:\n\n    # visualize the plot\nax = df.plot.bar(x=\"bin\", y=\"value\", figsize=(14, 7))\n    \n    # plot the weighted average\nax.axvline(weighted_average, color=\"black\", linestyle=\"--\", label=\"Weighted average\")\n\nhttps://preview.redd.it/td8cfyfb35861.png?width=1400&amp;format=png&amp;auto=webp&amp;s=9c8f72c8eb0be16f0c88ee91b66a9eb0234618fb\n\nIt took me a while to figure out what was the mistake.\n\nThe weighted average is 3.15, but on the plot, it\u2019s shown at \\~4.20. What\u2019s happening here? A bug in matplotlib?\n\n**Solution**\n\nNot really! The problem is that bin 3 is not present in the DataFrame. Let\u2019s add it and set the value to 0 as it\u2019s not present.\n\n    df = df.append({\"bin\": 3, \"value\": 0}, ignore_index=True)\ndf = df.sort_values([\"bin\"]).reset_index(drop=True)\n\nhttps://preview.redd.it/tgu5p0mf35861.png?width=1376&amp;format=png&amp;auto=webp&amp;s=871e2443e2398482fee98b6d05a8cece8ddd165a\n\nWe don\u2019t need to recalculate the weighted average as it stays the same.\n\nLet\u2019s look at the plot now. Better, right?\n\n    # visualize the plot\nax = df.plot.bar(x=\"bin\", y=\"value\", figsize=(14, 7))\n    \n    # plot the weighted average\nax.axvline(weighted_average, color=\"black\", linestyle=\"--\", label=\"Weighted average\")\n\nhttps://preview.redd.it/g7a8zoji35861.png?width=1400&amp;format=png&amp;auto=webp&amp;s=020cba69e614af675d3d6759852f6e822cfcb1e7", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kmfvcb/dont_make_these_pandas_mistakes/"}, {"autor": "gbbb1982", "date": "2020-12-29 14:44:14", "content": "One way to repaint -----> image !!!  using two neural networks - short explanation in video", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kmfoxr/one_way_to_repaint_image_using_two_neural/"}, {"autor": "0111010101", "date": "2020-12-29 13:11:57", "content": "Is there a program or type of AI for doing reverse -----> image !!!  search on a Windows desktop? /!/ I think there are probably Github repos or Python libraries for all of the steps required--image segmentation, image classification, computer vision--but I've never found a package that does it all.  You'd think there would be Windows software that does this, it would be such a useful product.  Is there maybe a Google or other (e.g., IBM, Amazon) cloud service to setup your own image search?\n\nWhat I'd like to do is create something like a relational database for my digital comics collection, so I could search for panels containing a character, setting, prop, or type of composition.  Ideally I could provide a target image to find something visually similar that meets other specified criteria, like a panel that shows Dr. Doom in the pose of the character in my image, or a panel with three characters in roughly the composition of my image.  Ideally my target image could be a sketch quickly drawn in MS Paint.\n\nI know there are AI demos that guess what you're drawing.  I'm thinking however that's done, that could be a component of the total package.\n\nI'm not great at programming, but I can do script kiddy stuff if I have source code, so anything you could link would be helpful.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kme5qn/is_there_a_program_or_type_of_ai_for_doing/"}, {"autor": "gbbb1982", "date": "2020-12-29 12:51:06", "content": "How to create AI -----> image !!!  repainting - project explanation", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kmdu2r/how_to_create_ai_image_repainting_project/"}, {"autor": "sai-krishna-das", "date": "2020-12-29 10:01:42", "content": "[D] Does down-sampling -----> image !!! s cause reduction in -----> image !!!  information? /!/ I was trying to implement AlexNet. So, I came across its research paper and found that they down-sample the images to 256x256 by center cropping it. \nSome datas online have 1280x720 or more and center croping it cause lose in edge information? (Other than ImageNet dataset).\n\nIs 256x256 a default or is it changeable?\n\nIn that paper they down-sampled to 256x256 but still used 224x224 as input ! Any idea why?\n\nLink to the paper :\n\nhttps://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n\nI'm new to this! Hope someone helps me :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kmbl2v/d_does_downsampling_images_cause_reduction_in/"}, {"autor": "sbjr47", "date": "2020-12-29 09:39:17", "content": "Should I implement SOTA architecture from scratch and train them? /!/ Hi all, I am currently going through various sources to get more knowledge in the field of Deep Learning(mainly CNN for Computer Vision tasks). I am aspiring to become a researcher in the field of Deep Learning and Reinforcement Learning.\n\n# A small Background\nFor the past 1 year, I have been fighting Major Depressive Disorder(Clinical Depression). I have also been unemployed since then. Currently, whenever I get stuck at any place while going through any SOTA research paper, it takes me days to overcome it and move forward. I was thinking that after understanding various concepts like -----> Image !!!  classification, object detection, image tracking I would apply for jobs regarding this field and later pursue my Masters and Ph.D.\n\n# Help required For this\nBasically, I want to plan my learning concentrated on implementation enough to get a job but concentrated on concepts and maths and logic also enough that later I am fit to pursue academics and complete my Ph.D.\n\n\nSo I am not able to understand \n- whether am I wasting my time trying to implement various research papers and train them on some huge dataset(considering the \"Validation set\" of -----> Image !!!  net which is 6GB in size for training as it is not as huge as -----> Image !!! Net but not as small as other datasets either)\n\nOR\n\n- Should I just read the research papers and just implement the model without training them?(This way I know how to build the models, but wouldn't know if it works or not)\n\nOR\n\n- Should I just make notes while reading the research paper and later combine my knowledge of all the papers in some projects(using transfer learning mostly) rather than implementing each paper independently?(Here, I will be able to put projects in my *Resume* thus helping me to get jobs and colleges for Masters later, but I might miss on the deep level concepts that many people face while implementing models from scratch)\n\nSorry for the big post", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kmbb6u/should_i_implement_sota_architecture_from_scratch/"}, {"autor": "alexandervalkyrie", "date": "2020-12-29 00:30:36", "content": "Building effective neural network architecture? /!/ How do I know how to architect my neural network? I am currently working on an ML model to place a point on the desired object. The model accepts a -----> picture !!!  and the object of interest is the person's in the -----> picture !!!  nose. I'm not really sure how to create an architecture for this. I want to use a CNN but I'm a bit confused about how I can create the most effective model. I'm specifically confused about what the output layer of my neural network should look like. Any help or reference to guides on how to create this would be greatly appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/km2wgf/building_effective_neural_network_architecture/"}, {"autor": "jimmyracecar", "date": "2020-12-28 16:35:36", "content": "Face recognition with labeling and confidence in C++ /!/ Hi all,\n\nSo during the xmas break I've started working on a project that I've been thinking about for a long time. Essentially I want to build a software in C++ which uses deep learning to recognize my face and label it in my name (jimmyracecar) and also displays the confidence at which it recognizes me.\n\nSo far I've explored OpenCV, dLib but I've not managed to find the information necessary on how exactly do can I get the algorithms to label the input -----> image !!!  for me. I have no problem detecting faces.\n\nSo far I've flicked through most of the examples in dLib and also flicked through OpenCV docs. There also dont seem to be that much tutorials for this kind of stuff specifically in C++\n\nCan someone give me some pointers please on where to search? I've been trying to get this running for the last two or three days and I feel utterly defeated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kltpme/face_recognition_with_labeling_and_confidence_in_c/"}, {"autor": "Sahil8141", "date": "2020-12-27 15:13:19", "content": "Neural network and its variation back-end procedure !! /!/ I have two questions which are bugging me regarding Neural network and its variation.\n\n1) Suppose we have one CSV with 1000 rows and 50 columns (features). So in the neural network's input first layer, we will start with the 50 neurons as we have 50 columns. But if we increase these numbers with 100-150 neurons, still the neural network works fine (With Keras). So what the input are taking place in these extra neurons?\n\n2) In the regression problem or some -----> image !!!  stuff like in CNN, we are using mini-batch. In that, we are providing all batch data into training at the same time. So This data is passing in the matrix form so we can train our NN optimally. So how this all data fits in the NN at the same time? And if my above assumption is wrong then we are training the data(row) one by one right? Same for the CNN we are providing a bunch of images so how these images fit into the NN at the same time and what benefits we get by that.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kl56uf/neural_network_and_its_variation_backend_procedure/"}, {"autor": "tiwari_ai_harsh", "date": "2020-12-27 15:08:20", "content": "Can Someone Help Me? My Loss is freaking increasing(PyTorch -----> Image !!!  captioning system). /!/ So I am trying to create an Image captioning system.\n\nSpend almost an entire week and no results. Loss is for some odd reason not decreasing.\n\nI can find any bug in it and I don't anyone who can. If you can help me save my entire week's work, I would be really thankful.\n\nHere is the link to colab. [https://colab.research.google.com/drive/1m3MkrnIUQ7AEhA2MSadzLYePL-P0ppZq?usp=sharing](https://colab.research.google.com/drive/1m3MkrnIUQ7AEhA2MSadzLYePL-P0ppZq?usp=sharing)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kl53xy/can_someone_help_me_my_loss_is_freaking/"}, {"autor": "dxjustice", "date": "2020-03-30 09:19:08", "content": "Pseudo-classifying with an untrained CNN via localized activations of filters? /!/ Consider an untrained CNN. Take a simple -----> image !!! , a ball on a background of sand or water, fr example.\n\nGiven that a CNN uses filters, which become progressively more complex with layers, would it not be possible for the CNN to produce different activation mappings in an image that has a few distinct components?\n\nPerhaps by selecting your own filters.\n\nThe network naturally would not be able to \"classify\" as there is no loss, but could we compare localized activation mappings to produce a pseudo classification?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/frnj8k/pseudoclassifying_with_an_untrained_cnn_via/"}, {"autor": "bestpix", "date": "2020-03-30 07:27:06", "content": "Machine Learning Workshop /!/ Hi,\n\nI have some extra time because I have no commute and thought i would offer an online workshop.\n\nPM me if you are interested and if there is enough interest ( &gt; 20) we can get started next week!\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\nThis is a hands on workshop in machine learning. The goal for attendees is to become familiar with practical application of machine learning models to -----> image !!!  based applications. The focus will be on using the algorithms/models rather than the research/math behind them. \n\nPrereqs: Basic Python programming knowledge. High-school level linear algebra and probability and statistics. \n\nThe plan is to teach via zoom/google hangouts . \n\nTwo weeks duration. \nTotal 6 sessions. \nMonday Wednesday Friday 1hr/day\n(We can figure out a time that works for everyone, I am in California)\n\nNo equipment or software installation is needed, just a decent internet connection, browser and a gmail account.\n\nWe\u2019ll be using the online Google Colab (Jupyter-like python IDE) that can install everything (tensorflow, numpy, Pandas, seaborne, maplotlib) we will need.\n\nBefore each class I\u2019ll share a deck and the python notebooks that you can follow along while I teach.\n\nSome homework but, no large assignments.\n\nContent:\n1) Intro to ML, Neural Nets, Colab, data processing \n2) Regression, feed-forward NN, Classification\n3) Image data, AutoEncoders\n4) Denoising AutoEncoders, CNN\n5) Variational AutoEncoders, GANs\n6) End to End example, Wrap-up", "link": "https://www.reddit.com/r/learnmachinelearning/comments/frmc9d/machine_learning_workshop/"}, {"autor": "mr_haseeb", "date": "2020-03-29 15:18:07", "content": "How to split an -----> image !!!  dataset in X_train, y_train, X_test, y_test /!/ I have csv file which contain images path and labels (categories)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fr7few/how_to_split_an_image_dataset_in_x_train_y_train/"}, {"autor": "arduino04", "date": "2020-03-29 09:22:10", "content": "Is there a way, using the PoseNet/Pose estimation framework, so track an entire movement rather than just a pose? /!/ I am using this in a fitness application. I want to use the -----> camera !!!  and see if the users are doing specific exercises correctly. If they are, reps will be added to the repetition counter, and if not, I will give them feedback.\n\nMy only question is how to create a model for an entire exercise, not just a single pose. Answers in either python, javascript, or just descriptions of algorithms are much appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fr3739/is_there_a_way_using_the_posenetpose_estimation/"}, {"autor": "vaden_arth", "date": "2020-03-28 21:35:29", "content": "Currency Detection Advice, Basic currency value detection using an -----> image !!! . Would like to know in which direction should I look for tutorials in trying to achieve this aim /!/ Hey guys am a beginner in machine learning, and for my project I have chosen to try out how to denote the value of currency notes. I have a large database of images which have photos of currency notes held in hands.\n\nMy aim is to detect the values and determine them and to build a model to do so\n\nI would really appreciate if someone could point me in the direction I need to go and do research/ learn from tutorials for this. Anything anywhere you think that would help me in doing so , please do let me know . \n\n\nI guess numberplate detection would be a valid assets to work in this direction. Please do let me know if anything relevant pops up in your mind.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fqt44w/currency_detection_advice_basic_currency_value/"}, {"autor": "sgp75", "date": "2020-03-28 03:41:59", "content": "Partial -----> Image !!!  Matching /!/ Finding matching parts of two images\n\nI have two images of the same object. The second image the object is rotated and scaled up or down and perspective may be slightly different. Therefore I have a lot of commonality between images but they are not identical. \n\nNow I use cross correlation to figure out how much the image is rotated but this is slow since I have to change perspective and scale up and down and compare. \n\nI am looking for a DL approach. What type of deep learning approach would be good to solve this problem.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fqcmwz/partial_image_matching/"}, {"autor": "hopye", "date": "2020-03-27 17:55:16", "content": "Please take a look at my COVID Classifier iOS app. How come is validating no xray -----> image !!!  as covid , watch complete video. Model was trained using apple xcode create ml , public xrays dataset and some of kaggles xrays dataset . There were around 74 covid images , training and testing 80% / 20%.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fq1u10/please_take_a_look_at_my_covid_classifier_ios_app/"}, {"autor": "trekhleb", "date": "2020-05-05 15:31:04", "content": "\ud83e\udd16 Interactive Machine Learning Experiments /!/ ## TL;DR\n\nHey readers!\n\nI've open-sourced new [**\ud83e\udd16 Interactive Machine Learning Experiments**](https://github.com/trekhleb/machine-learning-experiments) project on GitHub. Each experiment consists of \ud83c\udfcb\ufe0f *Jupyter/Colab notebook* (to see how a model was trained) and \ud83c\udfa8 *demo page* (to see a model in action right in your browser).\n\nAlthough the models may be a little dumb (remember, these are just experiments, not a production ready code), they will try to do their best to:\n\n* \ud83d\udd8c Recognize digits or sketches you draw in your browser\n* \ud83d\udcf8 Detect and recognize the objects you'll show to your -----> camera !!! \n* \ud83c\udf05 Classify your uploaded image\n* \ud83d\udcdd Write a Shakespeare poem with you\n* \u270a\ud83d\udd90\u270c\ufe0f Play with you in Rock-Paper-Scissors game\n* etc.\n\nI've trained the models on *Python* using *TensorFlow 2* with *Keras* support  and then consumed them for a demo in a browser using *React* and *JavaScript* version of *Tensorflow*.\n\nhttps://preview.redd.it/3u5jdch1uyw41.png?width=1800&amp;format=png&amp;auto=webp&amp;s=946b73c9dc85f432abb025e285286f9209fb62ae\n\n## Models performance\n\n\u26a0\ufe0f First, let's set our expectations.\ufe0f The repository contains machine learning **experiments** and **not** a production ready, reusable, optimised and fine-tuned code and models. This is rather a sandbox or a playground for learning and trying different machine learning approaches, algorithms and data-sets. Models might not perform well and there is a place for overfitting/underfitting.\n\nTherefore, sometimes you might see things like this:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/j2q8whk2uyw41.png?width=1198&amp;format=png&amp;auto=webp&amp;s=a70ae224254ea2f19886c952cfb294d2b0d42317\n\nBut be patient, sometimes the model might get smarter \ud83e\udd13 and give you this:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/eyvhdtp3uyw41.png?width=1208&amp;format=png&amp;auto=webp&amp;s=60ade63facde47c0ea2c6d1737679ee35b463d6a\n\n## Background\n\nI'm a [software engineer](https://www.linkedin.com/in/trekhleb/) and for the last several years now I've been doing mostly frontend and backend programming. In my spare time, as a hobby, I decided to dig into machine learning topics to make it less *like magic* and *more like math* to myself.\n\n1. \ud83d\uddd3 Since **Python** might be a good choice to start experimenting with Machine Learning I decided to learn its basic syntax first. As a result a [\ud83d\udc0d Playground and Cheatsheet for Learning Python](https://github.com/trekhleb/learn-python) project came out. This was just to practice Python and at the same time to have a cheatsheet of basic syntax once I need it (for things like `dict_via_comprehension = {x: x**2 for x in (2, 4, 6)}` etc.).\n2. \ud83d\uddd3 After learning a bit of Python I wanted to dig into the basic **math** behind Machine Learning. So after passing an awesome [Machine Learning course by Andrew Ng](https://www.coursera.org/learn/machine-learning) on Coursera the [\ud83e\udd16 Homemade Machine Learning](https://github.com/trekhleb/homemade-machine-learning) project came out. This time it was about creating a cheatsheet for basic machine learning math algorithms like linear regression, logistic regression, k-means, multilayer perceptron etc.\n3. \ud83d\uddd3 The next attempt to play around with basic Machine Learning math was [\ud83e\udd16 NanoNeuron](https://github.com/trekhleb/nano-neuron). It was about 7 simple JavaScript functions that supposed to give you a feeling of how machines can actually \"learn\".\n4. \ud83d\uddd3 After finishing yet another awesome [Deep Learning Specialization by Andrew Ng](https://www.coursera.org/specializations/deep-learning) on Coursera I decided to practice a bit more with **multilayer perceptrons**, **convolutional** and **recurrent neural networks** (CNNs and RNNs). This time instead of implementing everything from scratch I decided to start using some machine learning framework. I ended up using [TensorFlow 2](https://www.tensorflow.org/) with [Keras](https://www.tensorflow.org/guide/keras/overview). I also didn't want to focus too much on math (letting the framework do it for me) and instead I wanted to come up with something more practical, applicable and something I could try to play with right in my browser. As a result new [\ud83e\udd16 Interactive Machine Learning Experiments](https://github.com/trekhleb/machine-learning-experiments) came out that I want to describe a bit more here.\n\n## Tech-stack\n\n## Models training\n\n* \ud83c\udfcb\ud83c\udffb\u200d I used [Keras](https://www.tensorflow.org/guide/keras/overview) inside [TensorFlow 2](https://www.tensorflow.org/) for modelling and training. Since I had zero experience with machine learning frameworks, I needed to start with something. One of the selling points in favor of TensorFlow was that it has both Python and [JavaScript flavor](https://www.tensorflow.org/js) of the library with similar API. So eventually I used Python version for training and JavaScript version for demos.\n* \ud83c\udfcb\ud83c\udffb\u200d I trained TensorFlow models on Python inside [Jupyter](https://jupyter.org/) notebooks locally and sometimes used [Colab](https://colab.research.google.com/) to make the training faster on GPU.\n* \ud83d\udcbb Most of the models were trained on good old MacBook's Pro CPU (2,9 GHz Dual-Core Intel Core i5).\n* \ud83d\udd22 Of course there is no way you could run away from [NumPy](https://numpy.org/) for matrix/tensors operations.\n\n## Models demo\n\n* \ud83c\udfcb\ud83c\udffb\u200d I used [TensorFlow.js](https://www.tensorflow.org/js) to do predictions with previously trained models.\n* \u267b\ufe0f To convert *Keras HDF5* models to *TensorFlow.js Layers* format I used [TensorFlow.js converter](https://github.com/tensorflow/tfjs/tree/master/tfjs-converter). This might be inefficient to transfer the whole model (megabytes of data) to the browser instead of making predictions through HTTP requests, but again, remember that these are just experiments and not production-ready code and architecture. I wanted to avoid having a dedicated back-end service to make architecture simpler.\n* \ud83d\udc68\ud83c\udffb\u200d\ud83c\udfa8 The [Demo application](http://trekhleb.github.io/machine-learning-experiments) was created on [React](https://reactjs.org/) using [create-react-app](https://github.com/facebook/create-react-app) starter with a default [Flow](https://flow.org/en/) flavour for type checking.\n* \ud83d\udc85\ud83c\udffb For styling, I used [Material UI](https://material-ui.com/). It was, as they say, \"to kill two birds\" at once and try out a new styling framework (sorry, [Bootstrap](https://getbootstrap.com/) \ud83e\udd37\ud83c\udffb\u200d).\n\n## Experiments\n\nSo, in short, you may access Demo page and Jupyter notebooks by these links:\n\n* \ud83c\udfa8 [**Launch ML experiments demo**](http://trekhleb.github.io/machine-learning-experiments)\n* \ud83c\udfcb\ufe0f [**Check ML experiments Jupyter notebooks**](https://github.com/trekhleb/machine-learning-experiments)\n\n## Experiments with Multilayer Perceptron (MLP)\n\n&gt;A [multilayer perceptron (MLP)](https://en.wikipedia.org/wiki/Multilayer_perceptron) is a class of feedforward artificial neural network (ANN). Multilayer perceptrons are sometimes referred to as \"vanilla\" neural networks (composed of multiple layers of perceptrons), especially when they have a single hidden layer.\n\n## Handwritten Digits Recognition\n\nYou draw a digit, and the model tries to recognize it.\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/DigitsRecognitionMLP)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_mlp/digits_recognition_mlp.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_mlp/digits_recognition_mlp.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/8rc55ws7uyw41.gif\n\n## Handwritten Sketch Recognition\n\nYou draw a sketch, and the model tries to recognize it.\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/SketchRecognitionMLP)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_mlp/sketch_recognition_mlp.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_mlp/sketch_recognition_mlp.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/uhavc6d9uyw41.gif\n\n## Experiments with Convolutional Neural Networks (CNN)\n\n&gt;A [convolutional neural network (CNN, or ConvNet)](https://en.wikipedia.org/wiki/Convolutional_neural_network) is a class of deep neural networks, most commonly applied to analyzing visual imagery (photos, videos). They are used for detecting and classifying objects on photos and videos, style transfer, face recognition, pose estimation etc.\n\n## Handwritten Digits Recognition (CNN)\n\nYou draw a digit, and the model tries to recognize it. This experiment is similar to the one from MLP section, but it uses CNN under the hood.\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/DigitsRecognitionCNN)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_cnn/digits_recognition_cnn.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/digits_recognition_cnn/digits_recognition_cnn.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/jakuk4lauyw41.gif\n\n## Handwritten Sketch Recognition (CNN)\n\nYou draw a sketch, and the model tries to recognize it. This experiment is similar to the one from MLP section, but it uses CNN under the hood.\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/SketchRecognitionCNN)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_cnn/sketch_recognition_cnn.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/sketch_recognition_cnn/sketch_recognition_cnn.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/dzc4aghbuyw41.gif\n\n## Rock Paper Scissors (CNN)\n\nYou play a Rock-Paper-Scissors game with the model. This experiment uses CNN that is trained from scratch.\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/RockPaperScissorsCNN)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_cnn/rock_paper_scissors_cnn.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_cnn/rock_paper_scissors_cnn.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/ewnsaudcuyw41.gif\n\n## Rock Paper Scissors (MobilenetV2)\n\nYou play a Rock-Paper-Scissors game with the model. This model uses transfer learning and is based on [MobilenetV2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2).\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/RockPaperScissorsMobilenetV2)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_mobilenet_v2/rock_paper_scissors_mobilenet_v2.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/rock_paper_scissors_mobilenet_v2/rock_paper_scissors_mobilenet_v2.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/70ruj86euyw41.gif\n\n## Objects Detection (MobileNetV2)\n\nYou show to the model your environment through your camera, and it will try to detect and recognize the objects. This model uses transfer learning and is based on [MobilenetV2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2).\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/ObjectsDetectionSSDLiteMobilenetV2)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/objects_detection_ssdlite_mobilenet_v2/objects_detection_ssdlite_mobilenet_v2.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/objects_detection_ssdlite_mobilenet_v2/objects_detection_ssdlite_mobilenet_v2.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/16xi3nthuyw41.gif\n\n## Image Classification (MobileNetV2)\n\nYou upload a picture, and the model tries to classify it depending on what it \"sees\" on the picture. This model uses transfer learning and is based on [MobilenetV2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2).\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/ImageClassificationMobilenetV2)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/image_classification_mobilenet_v2/image_classification_mobilenet_v2.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/image_classification_mobilenet_v2/image_classification_mobilenet_v2.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/pv8zkkuiuyw41.gif\n\n## Experiments with Recurrent Neural Networks (RNN)\n\n&gt;A [recurrent neural network (RNN)](https://en.wikipedia.org/wiki/Recurrent_neural_network) is a class of deep neural networks, most commonly applied to sequence-based data like speech, voice, text or music. They are used for machine translation, speech recognition, voice synthesis etc.\n\n## Numbers Summation\n\nYou type a summation expression (i.e. `17+38`), and the model predicts the result (i.e. `55`). The interesting part here is that the model treats the input as a *sequence*, meaning it learned that when you type a sequence `1` \u2192 `17` \u2192 `17+` \u2192 `17+3` \u2192 `17+38` it \"translates\" it to another sequence `55`. You may think about it as translating a Spanish `Hola` sequence to English `Hello`.\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/NumbersSummationRNN)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/numbers_summation_rnn/numbers_summation_rnn.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/numbers_summation_rnn/numbers_summation_rnn.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/rix288wjuyw41.gif\n\n## Shakespeare Text Generation\n\nYou start typing a poem like Shakespeare, and the model will continue it like Shakespeare. At least it will try to do so \ud83d\ude00.\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/TextGenerationShakespeareRNN)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_shakespeare_rnn/text_generation_shakespeare_rnn.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_shakespeare_rnn/text_generation_shakespeare_rnn.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/nbkm3askuyw41.gif\n\n## Wikipedia Text Generation\n\nYou start typing a Wiki article, and the model tries to continue it.\n\n* \ud83c\udfa8 [Demo](https://trekhleb.github.io/machine-learning-experiments/#/experiments/TextGenerationWikipediaRNN)\n* \ud83c\udfcb\ufe0f [Training in Jupyter](https://nbviewer.jupyter.org/v2/gh/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_wikipedia_rnn/text_generation_wikipedia_rnn.ipynb)\n* \ufe0f\ud83c\udfcb\ufe0f  [Training in Colab](https://colab.research.google.com/github/trekhleb/machine-learning-experiments/blob/master/experiments/text_generation_wikipedia_rnn/text_generation_wikipedia_rnn.ipynb)\n\n&amp;#x200B;\n\nhttps://i.redd.it/a0l2g1pluyw41.gif\n\n## Future plans\n\nAs I've mentioned above the main purpose of [the repository](https://github.com/trekhleb/machine-learning-experiments) is to be more like a playground for learning rather than for production-ready models. Therefore, the main plan is to **continue learning and experimenting** with deep-learning challenges and approaches. The next interesting challenges to play with might be:\n\n* Emotions detection\n* Style transfer\n* Language translation\n* Generating images (i.e. handwritten numbers)\n* etc.\n\nAnother interesting opportunity would be to **tune existing models to make them more performant**. I believe it might give a better understanding of how to overcome overfitting and underfitting and what to do with the model if it just stuck on `60%` accuracy level for both training and validation sets and doesn't want to improve anymore \ud83e\udd14.\n\nAnyways, I hope you might find some useful insights for models training from [the repository](https://github.com/trekhleb/machine-learning-experiments) or at least to have some fun playing around with the demos!\n\nHappy learning! \ud83e\udd16", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gdzv8h/interactive_machine_learning_experiments/"}, {"autor": "shwetashri", "date": "2020-05-05 10:37:29", "content": "[Need help] Hair segmentation using python and machine learning /!/ Hello,\n\nI am trying to segment hair and face from a -----> picture !!! . I am doing the dumb/simple way of doing this by extracting face and area till neck . I then tried subtracting the two arrays but its throwing error. Tried subtraction with opencv also but getting errors.\n\nAny ideas on other simple methods to try?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gdvf23/need_help_hair_segmentation_using_python_and/"}, {"autor": "Troy1729", "date": "2020-05-05 04:14:13", "content": "Multivariate regression using deep learning /!/ I'm working on a project in where I need to use regression method on MRI images of patient. There is a csv file containing certain parameters related to each patients MRI images. So I need to predict these parameters using regression with deep learning. I need help on how to approach. No. Of parameters to predict are 3 and each patient has T1, T2 and there segmentation -----> image !!!  files. Kindly help", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gdqxja/multivariate_regression_using_deep_learning/"}, {"autor": "Brilliant_Potato", "date": "2020-05-04 10:11:34", "content": "Which data Annotation tool for texture recognition? /!/ Hello :).\n\nFor my bachelor thesis i want to train two CNN's to get a texture descriptor from a given sub------> image !!! . \n\nOne CNN is going to be trained, the other one is an auto-encoder. I will compare how well they perform. \n\nI got some images from my prof. which i will be using, but they aren't labeled. So i need some software to label the data. \n\nWhen i researched this, i found that there are many solutions and i didn't know which would be most applicable to my usecase, which is:\n\n&amp;#x200B;\n\nLabel the data in a way, such that every sub-image (e.g. 32x32 square) is labeled with exactly one class (or none). \n\nIt would be ok if the image is labeled pixel-wise and the exact outlines of the object are labeled, if i can convert that to the desired structure. (should be quite simple ... e.g. more then 1/4 of pixel labeled as class -&gt; square labeled as class.)\n\n&amp;#x200B;\n\nI hope you guys can recommend me a tool which is suitable. Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gd8xpl/which_data_annotation_tool_for_texture_recognition/"}, {"autor": "andrewkhorkin", "date": "2020-05-23 05:15:17", "content": "Robot with -----> camera !!!  and neural network start", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gozby7/robot_with_camera_and_neural_network_start/"}, {"autor": "2020Corp", "date": "2020-05-22 17:56:29", "content": "Programming beginner interested in learning Python for AI/ML/Computer Vision /!/  Hi All\n\nHope you are well.\n\nI am a programming beginner and would like to look at learning Python for use working with AI/ML/Computer Vision.\n\nI appreciate these are highly technical fields, however if possible I would like to start with building some kind of basic level -----> image !!!  recognition/processing/detection algo using Python.\n\nCan anybody recommend a start point in terms of courses/resources for me to learn Python with a view to using it as described above?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/goo9w6/programming_beginner_interested_in_learning/"}, {"autor": "ErrorVT", "date": "2020-05-22 13:23:32", "content": "What type of Autoencoder should be used for Recommendation(Collaborative filtering) of Cellular network data? /!/ Hi folks\n\nI'm new to machine learning and want to build an autoencoder over recommendation system for cellular network data to compare the results after with or without autoencoder, I'm confused where should i start and what python libraries have builtin functionalities for such task, the most examples I found are for -----> Image !!!  and movie/product recommendation. any help would be greatly appreciated, thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/goj9v3/what_type_of_autoencoder_should_be_used_for/"}, {"autor": "Rahul_Desai1999", "date": "2020-05-22 09:49:33", "content": "I have two pytorch conv nn which to me seem the same, but are not. /!/     import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    \n    class Net(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 32, 5)\n            self.conv2 = nn.Conv2d(32, 64, 5)\n            self.conv3 = nn.Conv2d(64, 128, 5)\n            self.fc1 = nn.Linear(128*2*2, 512)\n            self.fc2 = nn.Linear(512, 2)\n            \n        def forward(self, x):\n            x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n            x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n            x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n            \n            x = x.flatten(start_dim=1)\n            \n            x = F.relu(self.fc1(x))\n            x = F.relu(self.fc2(x))\n            \n            return F.softmax(x, dim=1)\n            \n    net = Net()\n\nThis is a nn that i coded after watching sentdex's tutorials. The problem with this is that, at each epoch, the loss is constant at 0.25 and the accuracy is 48.99%\n\n&amp;#x200B;\n\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    \n    class Net(nn.Module):\n        def __init__(self):\n            super().__init__() # just run the init of parent class (nn.Module)\n            self.conv1 = nn.Conv2d(1, 32, 5) # input is 1 -----> image !!! , 32 output channels, 5x5 kernel / window\n            self.conv2 = nn.Conv2d(32, 64, 5) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 5x5 kernel / window\n            self.conv3 = nn.Conv2d(64, 128, 5)\n    \n            x = torch.randn(50,50).view(-1,1,50,50)\n            self._to_linear = None\n            self.convs(x)\n    \n            self.fc1 = nn.Linear(self._to_linear, 512) #flattening.\n            self.fc2 = nn.Linear(512, 2) # 512 in, 2 out bc we're doing 2 classes (dog vs cat).\n    \n        def convs(self, x):\n            # max pooling over 2x2\n            x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n            x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n            x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n    \n            if self._to_linear is None:\n                self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n            return x\n    \n        def forward(self, x):\n            x = self.convs(x)\n            x = x.view(-1, self._to_linear)  # .view is reshape ... this flattens X before \n            x = F.relu(self.fc1(x))\n            x = self.fc2(x) # bc this is our output layer. No activation here.\n            return F.softmax(x, dim=1)\n    \n    \n    net = Net()\n\nI wanted to create a nn such given above. This is copied from sentdex's website. When i change the nn in my code, i start getting improved losses. And the accuracy on it's first epoch is 60%. Can someone explain why is this happening and whats wrong with i have coded? \n\nNote that this is the part thats causing the difference in accuracies. I just have to change this in my code to see the differences.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/goge4n/i_have_two_pytorch_conv_nn_which_to_me_seem_the/"}, {"autor": "frlazzeri", "date": "2020-05-21 16:14:23", "content": "Fairlearn - A Python package to assess AI system's fairness /!/ In 2015, Claire Cain Miller wrote on [The New York Times](https://www.nytimes.com/2015/07/10/upshot/when-algorithms-discriminate.html) that there was a widespread belief that software and algorithms that rely on data were objective. Five years later, we know for sure that AI is not free of human influence. Data is created, stored, and processed by people, machine learning algorithms are written and maintained by people, and AI applications simply reflect people\u2019s attitudes and behavior.\u00a0\n\nData scientists know that no longer accuracy is the only concern when developing machine learning models, fairness must be considered as well. In order to make sure that machine learning solutions are fair and the value of their predictions easy to understand and explain, it is essential to build tools that developers and data scientists can use to assess their AI system\u2019s fairness and mitigate any observed unfairness issues.\n\nThis article will focus on **AI fairness**, by explaining the following aspects and tools:\n\n1. [**Fairlearn**](https://github.com/fairlearn/fairlearn?WT.mc_id=build2020_ca-blogpost-lazzeri): a tool to assess AI system\u2019s fairness and mitigate any observed unfairness issues\n2. How to use [**Fairlearn**](https://github.com/fairlearn/fairlearn?WT.mc_id=build2020_ca-blogpost-lazzeri) in [**Azure Machine Learning**](http://www.aka.ms/AzureMLDoc)\n3. What we mean by fairness\n4. [**Fairlearn algorithms**](https://github.com/fairlearn/fairlearn?WT.mc_id=docs-twitter-lazzeri#fairlearn-algorithms)\n5. [**Fairlearn dashboard**](https://github.com/fairlearn/fairlearn?WT.mc_id=docs-twitter-lazzeri#fairlearn-dashboard)\n6. Comparing multiple models\n7. Additional resources and [**how to contribute**](https://github.com/fairlearn/fairlearn?WT.mc_id=docs-twitter-lazzeri#contributing)\n\n## 1. Fairlearn: a tool to assess AI system\u2019s fairness and mitigate any observed unfairness issues\n\n[Fairlearn](https://github.com/fairlearn/fairlearn?WT.mc_id=build2020_ca-blogpost-lazzeri) is a Python package that empowers developers of artificial intelligence (AI) systems to assess their system\u2019s fairness and mitigate any observed unfairness issues. [Fairlearn](https://github.com/fairlearn/fairlearn?WT.mc_id=build2020_ca-blogpost-lazzeri) contains mitigation algorithms as well as a Jupyter widget for model assessment. The [Fairlearn](https://github.com/fairlearn/fairlearn?WT.mc_id=build2020_ca-blogpost-lazzeri) package has two components:\n\n* A\u00a0*dashboard*\u00a0for assessing which groups are negatively impacted by a model, and for comparing multiple models in terms of various fairness and accuracy metrics.\n* *Algorithms*\u00a0for mitigating unfairness in a variety of AI tasks and along a variety of fairness definitions.\n\nThere is also a collection of [Jupyter notebooks](https://github.com/fairlearn/fairlearn/tree/master/notebooks?WT.mc_id=build2020_ca-blogpost-lazzeri) and an a detailed [API guide](https://fairlearn.github.io/api_reference/index.html?WT.mc_id=build2020_ca-blogpost-lazzeri), that you can check to learn how to leverage [Fairlearn](https://github.com/fairlearn/fairlearn?WT.mc_id=build2020_ca-blogpost-lazzeri) for your own data science scenario.\n\n## 2. How to use Fairlearn in Azure Machine Learning\n\nThe [Fairlearn](https://github.com/fairlearn/fairlearn?WT.mc_id=build2020_ca-blogpost-lazzeri) package can be installed via:\n\n`pip install fairlearn` \n\nor optionally with a full feature set by adding extras, e.g. pip install fairlearn\\[customplots\\], or you can clone the repository locally via:\n\n`git clone git@github.com:fairlearn/fairlearn.git` \n\nIn Azure Machine Learning, there are a few options to use Jupyter notebooks for your experiments:\n\n### a) Get Fairlearn samples on your notebook server\n\nIf you\u2019d like to bring your own notebook server for local development, follow these steps:\n\n1. Use the instructions at\u00a0[**Azure Machine Learning SDK**](https://docs.microsoft.com/python/api/overview/azure/ml/install?view=azure-ml-py?WT.mc_id=build2020_ca-blogpost-lazzeri)\u00a0to install the Azure Machine Learning SDK for Python\n2. Create an\u00a0[**Azure Machine Learning workspace**](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?WT.mc_id=build2020_ca-blogpost-lazzeri).\n3. Write a\u00a0[**configuration file**](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment#workspace?WT.mc_id=build2020_ca-blogpost-lazzeri) \n4. Clone\u00a0[**the GitHub repository**](https://github.com/fairlearn/fairlearn?WT.mc_id=build2020_ca-blogpost-lazzeri).\n\n`git clone git@github.com:fairlearn/fairlearn.git` \n\n5. Start the notebook server from your cloned directory.\n\n`jupyter notebook` \n\nFor more information, see [Install the Azure Machine Learning SDK for Python](https://docs.microsoft.com/python/api/overview/azure/ml/install?WT.mc_id=build2020_ca-blogpost-lazzeri).\n\nb) Get Fairlearn samples on DSVM\n\nThe Data Science Virtual Machine (DSVM) is a customized VM -----> image !!!  built specifically for doing data science. If you\u00a0[create a DSVM](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment?WT.mc_id=build2020_ca-blogpost-lazzeri#dsvm), the SDK and notebook server are installed and configured for you. However, you\u2019ll still need to create a workspace and clone the sample repository.\n\n1. [Create an Azure Machine Learning workspace](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?WT.mc_id=build2020_ca-blogpost-lazzeri).\n2. Clone\u00a0[the GitHub repository](https://github.com/fairlearn/fairlearn?WT.mc_id=build2020_ca-blogpost-lazzeri).\n\n`git clone git@github.com:fairlearn/fairlearn.git`\n\n3. Add a workspace configuration file to the cloned directory using either of these methods:\n\n* In the\u00a0[Azure portal](https://ms.portal.azure.com/?WT.mc_id=build2020_ca-blogpost-lazzeri), select\u00a0Download config.json\u00a0from the\u00a0Overview section of your workspace.\n* Create a new workspace using code in the\u00a0[configuration.ipynb](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb?WT.mc_id=build2020_ca-blogpost-lazzeri) notebook in your cloned directory\n\n4. Start the notebook server from your cloned directory:\n\n`jupyter notebook`\n\n## 3. What we mean by\u00a0fairness\n\nFighting against unfairness and discrimination has a long history in philosophy and psychology, and recently in machine learning. However, in order to be able to achieve fairness, we should first define the notion of it. An AI system can behave unfairly for a variety of reasons and many different fairness explanations have been used in literature, making this definition even more challenging. In general, fairness definitions fall under three different categories as follows:\n\n* *Individual Fairness* \u2013 Give similar predictions to similar individuals.\u00a0\n* *Group Fairness* \u2013 Treat different groups equally.\n* *Subgroup Fairness* \u2013 Subgroup fairness intends to obtain the best properties of the group and individual notions of fairness.\u00a0\n\nIn [Fairlearn](https://github.com/fairlearn/fairlearn?WT.mc_id=build2020_ca-blogpost-lazzeri), we define whether an AI system is behaving unfairly in terms of its impact on people \u2013 i.e., in terms of harms. We focus on two kinds of harms:\n\n* *Allocation harms*. These harms can occur when AI systems extend or withhold opportunities, resources, or information. Some of the key applications are in hiring, school admissions, and lending.\n* *Quality-of-service harms*.\u00a0Quality of service refers to whether a system works as well for one person as it does for another, even if no opportunities, resources, or information are extended or withheld.\n\nWe follow the approach known as group fairness, which asks: Which groups of individuals are at risk of experiencing harm? The relevant groups need to be specified by the data scientist and are application-specific. Group fairness is formalized by a set of constraints, which require that some aspect (or aspects) of the AI system\u2019s behavior be comparable across the groups. The [Fairlearn](https://github.com/fairlearn/fairlearn?WT.mc_id=build2020_ca-blogpost-lazzeri) package enables the assessment and mitigation of unfairness under several common definitions.\n\n## 4. Fairlearn algorithms\n\n[Fairlearn](https://github.com/fairlearn/fairlearn?WT.mc_id=build2020_ca-blogpost-lazzeri) contains the following algorithms for mitigating unfairness in binary classification and regression:\n\nhttps://preview.redd.it/2inmvd6g75051.png?width=899&amp;format=png&amp;auto=webp&amp;s=3386410974a9e3640ef8ef8a409a2f19f989330a\n\n## 5. Fairlearn dashboard\n\n[Fairlearn](https://github.com/fairlearn/fairlearn?WT.mc_id=build2020_ca-blogpost-lazzeri) dashboard is a Jupyter notebook widget for assessing how a model\u2019s predictions impact different groups (e.g., different ethnicities), and also for comparing multiple models along different fairness and accuracy metrics.\n\nTo assess a single model\u2019s fairness and accuracy, the dashboard widget can be launched within a Jupyter notebook as follows:\n\n`from fairlearn.widget import FairlearnDashboard`\n\n`# A_test containts your sensitive features (e.g., age, binary gender)` \n\n`# sensitive_feature_names containts your sensitive feature names` \n\n`# y_true contains ground truth labels` \n\n`# y_pred contains prediction labels` \n\n`FairlearnDashboard(sensitive_features=A_test,` \n\n`sensitive_feature_names=['BinaryGender', 'Age'],`  \n\n`y_true=Y_test.tolist(),`  \n\n`y_pred=[y_pred.tolist()])`\n\nAfter the launch, the widget walks the user through the assessment set-up, where the user is asked to select:\n\n1. the sensitive feature of interest (e.g., binary gender or age)\n2. the accuracy metric (e.g., model precision) along which to evaluate the overall model performance as well as any disparities across groups.\u00a0\n\nThese selections are then used to obtain the visualization of the model\u2019s impact on the subgroups (e.g., model precision for females and model precision for males). The following figures illustrate the set-up steps, where binary gender is selected as a sensitive feature and the accuracy rate is selected as the accuracy metric:\n\nAfter the set-up, the dashboard presents the model assessment in two panels, as summarized in the table, and visualized in the screenshot below:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/enskhh7i75051.png?width=900&amp;format=png&amp;auto=webp&amp;s=db98cb058029655757df1946e42bca4831170451\n\n## 6. Comparing multiple models\n\nAn additional feature that this dashboard offers is the comparison of multiple models, such as the models produced by different learning algorithms and different mitigation approaches, including:\n\n* `fairlearn.reductions.GridSearch`\n* `fairlearn.reductions.ExponentiatedGradient`\n* `fairlearn.postprocessing.ThresholdOptimizer`\n\nAs before, the user is first asked to select the sensitive feature and the accuracy metric. The\u00a0model comparison\u00a0view then depicts the accuracy and disparity of all the provided models in a scatter plot. This allows the user to examine trade-offs between algorithm accuracy and fairness. Moreover, each of the dots can be clicked to open the assessment of the corresponding model.\u00a0\n\nThe figure below shows the model comparison view with binary gender selected as a sensitive feature and\u00a0accuracy rate\u00a0selected as the accuracy metric.\n\n# 7. Additional resources and how to contribute\n\nFor references and additional resources, please refer to:\n\n* Fairlearn GitHub repo: [**www.aka.ms/FairlearnAI**](http://www.aka.ms/FairlearnAI) \n* Azure Machine Learning: [**www.aka.ms/AzureMLDoc**](http://www.aka.ms/AzureMLDoc)\n* Responsible ML at Microsoft Build: [**www.aka.ms/Build2020ResponsibleML**](http://www.aka.ms/Build2020ResponsibleML) \n* Responsible ML on Azure: [**www.aka.ms/AzureResponsibleML**](http://www.aka.ms/AzureResponsibleML)\n* Responsible ML documentation: [**www.aka.ms/ResponsibleMLDoc**](http://www.aka.ms/ResponsibleMLDoc)\n* Discrimination-aware Data Mining: [**http://pages.di.unipi.it/ruggieri/Papers/kdd2008.pdf**\u00a0](https://dl.acm.org/doi/10.1145/1401890.1401959)\n* A Survey on Bias and Fairness in Machine Learning: [**https://arxiv.org/pdf/1908.09635.pdf**](https://arxiv.org/pdf/1908.09635.pdf)\n* Can an Algorithm Hire Better Than a Human? [**https://www.nytimes.com/2015/06/26/upshot/can-an-algorithm-hire-better-than-a-human.html**](https://www.nytimes.com/2015/06/26/upshot/can-an-algorithm-hire-better-than-a-human.html)\n\nTo contribute please check this\u00a0[contributing guide](https://fairlearn.github.io/?WT.mc_id=build2020_ca-blogpost-lazzeri#contribute).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gnzrs3/fairlearn_a_python_package_to_assess_ai_systems/"}, {"autor": "selling_crap_bike", "date": "2020-05-21 14:23:16", "content": "Best strategy to train -----> image !!!  segmentation for one class? /!/ You all have seen image segmentation examples on street images, where cars (and in turn people, etc.) are masked. Also, all cars seem to be contained in one class - so it doesn't matter if it's a Volkswagen, Mercedes or Volvo - they all have the same 'car' mask.\n\nHow do people train this? If one trains image segmentation model on 1000 images of Volkswagen, then on 1000 images of Mercedes, then the model will not be able to generalize to detect Volvos (or will it?).\n\nOr should one train 10 images of Volkswagen, 10 of Mercedes, 10 of Volvo, 10 of Tesla, etc. in the hopes of making the model generalize better by exposing it to smaller, but more different sets of training data?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gnxr3r/best_strategy_to_train_image_segmentation_for_one/"}, {"autor": "kae9248", "date": "2020-05-21 07:59:00", "content": "Time Series Shapes Classification /!/ I am pretty new to the machine learning and the thing that I am trying to do is the time series shapes classification...Basically the problem is as following:\n\nLet's say I have some time series and my goal is to have an algorithm that \"finds\" the similar shapes in the data, that occur throughout the dataset and labels it accordingly (a, b, c...etc.)...The -----> image !!!  might be worth than 1000 words, so here is the workflow that I have in mind: \n\n![img](zmzayqo0r2051 \"Classification workflow\")\n\nIt would be great if anyone directs me to the algorithms/papers/codes that deal with this kind of problems", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gnsns7/time_series_shapes_classification/"}, {"autor": "Pawan315", "date": "2020-05-21 07:11:36", "content": "Cnn for -----> image !!!  input and -----> image !!!  as output? /!/ hi everyone I want to know if there is any way if we can create a convolution neural network which has image input and output of image. I would love if you tell me with an example of Kerala as I am good at keras.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gns55e/cnn_for_image_input_and_image_as_output/"}, {"autor": "Pawan315", "date": "2020-05-21 07:10:37", "content": "Cnn for -----> image !!!  input and -----> image !!!  as output? /!/ hi everyone I want to know if there is any way if we can create a convolution neural network which has image input and output of image. I would love if you tell me with an example of Kerala as I am good at keras.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gns4rb/cnn_for_image_input_and_image_as_output/"}, {"autor": "abdeljalil73", "date": "2020-06-11 16:01:50", "content": "Different images in different channels of CNN input /!/ I have a dataset with pairs of grayscale images which I want to use as an input to CNN network. Images in -----> image !!!  pairs have different meaning, but they both must be used as an input to the model which will use them to make a single prediction (classification) (each pair contains scaleograms of both vertical and horizontal vibrations, sometimes the first scaleograms contains the needed information to make a prediction, sometimes the second, sometimes both). The use of channels I came across is with RGB images where each channel is different but basically they all come from a single image. Would using those different images as channels work? Or should I consider a different architecture?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h12315/different_images_in_different_channels_of_cnn/"}, {"autor": "miladink", "date": "2020-06-11 02:12:49", "content": "Any Map of AI? /!/ AI nowadays is covering lots of topics and consists of numerous methods(Statistical Learning, Deep Learning, Graphical Models, Planning, A-star algorithm, etc).\n\nI am wondering whether there exists a nice overview of all these fields in a short text/video/-----> photo !!! . I am looking for something like this which is about the whole mathematics:\n\n[the map of mathematics](https://www.youtube.com/watch?v=OmJ-4B-mS-Y)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h0pz6g/any_map_of_ai/"}, {"autor": "TrackLabs", "date": "2020-06-10 22:01:09", "content": "How can I make pyplot show the RGB -----> image !!! /reshape it? /!/ I want to display images from a GAN to then save it with pyplot, with this code\n\n    #\u00a0SAVING\u00a0IMAGES\u00a0DURING\u00a0TRAINING\n    plt.imshow(generated_images)\n    plt.savefig(f'pokemon_images/{img_counter}.png',\u00a0dpi=500)\n    plt.clf()\n\ngenerated\\_images has a shape of (100, 28, 28, 3), and it doesnt exist it because of the 3 channels. I found no way how to make pyplot accept this. Every article on the internet uses some cv2 or other stuff to convert it to RGB in 1 dimension, but pyplot doesnt accept any of that. How can I make pyplot accept this image shape/reshape it and show the image?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h0ld46/how_can_i_make_pyplot_show_the_rgb_imagereshape_it/"}, {"autor": "seek_it", "date": "2020-06-10 08:50:44", "content": "Umpire Detection and Umpire Pose Detection in cricket videos /!/ I need to detect umpire in cricket videos followed by pose detection of the umpire to detect whether the shot is Six, Four etc.\n\nI'm pretty confuse how to approach this problem.\n\nI've tried to train an -----> image !!!  classification of umpire vs non-umpire using transfer learning but that doesn't seem to work correctly.\n\nHow should I approach this problem?\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h06zym/umpire_detection_and_umpire_pose_detection_in/"}, {"autor": "wstcpyt1988", "date": "2020-06-10 04:58:26", "content": "How to evaluate your -----> image !!!  classification model", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h044pb/how_to_evaluate_your_image_classification_model/"}, {"autor": "techsavvynerd91", "date": "2020-06-10 00:20:47", "content": "What's the best food/meal classification library available on GitHub? /!/ I'm working on a mini Android app for fun that classifies what food you're eating based on the -----> picture !!!  you take. I thought it would be a good idea to clone a GitHub project that already does this very well so I was wondering does anyone know the best GitHub project that does?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gzzx6y/whats_the_best_foodmeal_classification_library/"}, {"autor": "hidajua", "date": "2020-07-16 14:37:28", "content": "Has anyone taken this Udemy course \"Machine Learning in GIS: Land Use/Land Cover -----> Image !!!  Analysis\" by Kate Alison? /!/  [https://www.udemy.com/course/advanced-land-useland-cover-mapping-with-machine-learning/](https://www.udemy.com/course/advanced-land-useland-cover-mapping-with-machine-learning/)  \n\nIs it worth it?\n\n I'm about to take my first course on ArcGIS, I'm comfortable with ML concepts and would like to learn how to apply them to Geospatial Data. thanks for your help!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hsaqzx/has_anyone_taken_this_udemy_course_machine/"}, {"autor": "enlightenseeker95", "date": "2020-07-16 11:08:32", "content": "-----> image !!!  segmentation noob /!/ Hi there,\n\nSorry  to bother you, I was hoping you could help me out. I understand what  unet and such  do, I understand what image segmentation is but I don't  get how images  can be passed into the network and get segmented and  placed back on the  original image. What are these 'masks' and 'ground  truths'?\n\nI have followed a simple  guide previously, it used a fcn network and  then passed the image to an  encoder and decoder for the segmentation. I  tried to the add a unet to  it and use the the segmentation technique but  it didn't work, I then  saw that the original guide asks for manual mean  and standard deviation  but the unet for pytorch guide is automatically  done. But I also cant  find any information explaining the steps to  achieve this. I only need  one guide so I can grasp how it works and then  I can go from there to  learning it. If that makes sense?\n\nI apologize if I annoy anyone with my simple question.\n\nThank you for your time!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hs7oe6/image_segmentation_noob/"}, {"autor": "forktothevocalcords", "date": "2020-07-16 09:07:39", "content": "Blood group analysis using -----> image !!!  processing /!/ Hey guys! I've been working on a project that analyses blood type using image processing. I've planned to take a data set of around 360 blood sample pictures. However I don't really know where to start with this. How do I execute the code, how do I make use of machine learning in this. What are the necessary steps to be taken for making a project like this. Need some help, thanks.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hs6c6x/blood_group_analysis_using_image_processing/"}, {"autor": "Potato_Knishes", "date": "2020-07-15 22:04:37", "content": "Is it possible to not only classify that an -----> image !!!  contains the desired item, but be able to also identify the location or (x,y) of the item in the -----> image !!! ? /!/ The title is pretty much it, I just wanted to know before I delve into it if it is possible which is definitely sounds like it should be. Any tips or suggestions on what type of DL or NN to start with would be greatly appreciated, thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hrx6oc/is_it_possible_to_not_only_classify_that_an_image/"}, {"autor": "qyler_", "date": "2020-07-15 18:27:47", "content": "Error in code taken from Deep Learning with PyTorch /!/ I am working through the book Deep Learning with PyTorch, and I'm just starting in chapter 2. I have copied some code from the book to run an -----> image !!!  through a pre-trained network, but I am getting an error while running the preprocess function. \n\nIt seems as though the error is occurring because it expects the mean and std arrays to have 4 elements, but from the text and from the PyTorch documentation it seems to me like the ToTensor() should transform the PIL image into a 3 element array.\n\nI have tried simply extending the std and mean by an element, but then I get an error further down in the program. Any help would be appreciated; I will include the code below\n\n    from matplotlib import pyplot as plt\n    \n    import numpy as np\n    \n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    \n    from torchvision import models\n    from torchvision import transforms\n    \n    from PIL import Image\n    \n    torch.set_printoptions(edgeitems=2)\n    torch.manual_seed(123)\n    \n    resnet = models.resnet101(pretrained=True)\n    \n    preprocess = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n    \n    img = Image.open(\"data/ch2/octane.jpg\")\n    img_t = preprocess(img)\n    \n    batch_t = torch.unsqueeze(img_t, 0)\n    \n    resnet.eval()\n    \n    out = resnet(batch_t)    \n\nThe error I get when running the code as given from the text is...\n\n    Traceback (most recent call last):\n      File \"pretrained.py\", line 28, in &lt;module&gt;\n        img_t = preprocess(img)\n      File \"C:\\Users\\kkren\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 61, in __call__\n        img = t(img)\n      File \"C:\\Users\\kkren\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 166, in __call__\n        return F.normalize(tensor, self.mean, self.std, self.inplace)\n      File \"C:\\Users\\kkren\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\torchvision\\transforms\\functional.py\", line 208, in normalize\n        tensor.sub_(mean).div_(std)\n    RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0\n\nand the error I get when adding a fourth element to the mean and std arrays is...\n    \n    Traceback (most recent call last):\n      File \"pretrained.py\", line 33, in &lt;module&gt;\n        out = resnet(batch_t)\n      File \"C:\\Users\\kkren\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 550, in __call__\n        result = self.forward(*input, **kwargs)\n      File \"C:\\Users\\kkren\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\torchvision\\models\\resnet.py\", line 220, in forward\n        return self._forward_impl(x)\n      File \"C:\\Users\\kkren\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\torchvision\\models\\resnet.py\", line 203, in _forward_impl\n        x = self.conv1(x)\n      File \"C:\\Users\\kkren\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 550, in __call__\n        result = self.forward(*input, **kwargs)\n      File \"C:\\Users\\kkren\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 353, in forward\n        return self._conv_forward(input, self.weight)\n      File \"C:\\Users\\kkren\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 350, in _conv_forward\n        self.padding, self.dilation, self.groups)\n    RuntimeError: Given groups=1, weight of size [64, 3, 7, 7], expected input[1, 4, 224, 224] to have 3 channels, but got 4 channels instead", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hrt1g2/error_in_code_taken_from_deep_learning_with/"}, {"autor": "singularitai", "date": "2020-08-23 01:43:22", "content": "I created an AI powered bot that compliments people using -----> image !!!  embedding sequences and transformers! This video covers its creation, analysis and other aspects.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ieuue6/i_created_an_ai_powered_bot_that_compliments/"}, {"autor": "aagoro", "date": "2020-08-21 18:46:13", "content": "Need help using Kinect V1(Xbox 360) for -----> image !!!  processing and segmentation with Python. /!/ Hello everyone. Just wondering if anyone has any experience working with Python and the Kinect V1 (the one that came out for the Xbox 360). I dug mine up yesterday and started looking for frameworks to get data from it, but they are all really poorly documented and tutorials are really sparse and not of very good quality.\n\nI'm not a complete beginner, but I'm also not an expert with ML and Computer Vision, and I would like to use my Kinect as a way of learning more about how to use depth sensor data and the camera that it has.\n\nAny help is appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ie2o2d/need_help_using_kinect_v1xbox_360_for_image/"}, {"autor": "magnusderrote", "date": "2020-08-21 15:42:44", "content": "My take on Named Entity Recognition and Disambiguation (NERD) /!/ Please have a look at \\[Simple NERD\\]([http://142.93.230.57/](http://142.93.230.57/))\n\n&amp;#x200B;\n\nHere are a couple of examples in case you haven\u2019t heard of NERD.\n\n&amp;#x200B;\n\n\\* black cat: A short horror story by Edgar Poe || a manga by Kentaro Yabuki || a dark feline\n\n\\* harry potter: A book series || a -----> film !!!  series || the character || a journalist ([https://en.wikipedia.org/wiki/Harry\\_Potter\\_(journalist)](https://en.wikipedia.org/wiki/Harry_Potter_(journalist)))\n\n\\* prince: A son of a king || A musician\n\n&amp;#x200B;\n\nA lot of this is based on the context around the word, which I am aiming to solve for a very short uncased text, since it is more common for search engine, and the lack of context is also a challenge\n\n&amp;#x200B;\n\nStay awhile and have fun with the page. I\u2019d like to see it as an early test for the system, please throw all of comments / bug / design idea in the thread. Thank you\n\n&amp;#x200B;\n\nTech stuff\n\n&amp;#x200B;\n\n\\* Software:\n\n&amp;#x200B;\n\n\\&gt; - FLAIR for POS\n\n\\- Wikidata for knowledge base\n\n\\- Docker compose for deploying\n\n\\- ReactJS for front end\n\n&amp;#x200B;\n\n\\* Hardware \n\n&amp;#x200B;\n\n\\&gt; - 2GB Ram", "link": "https://www.reddit.com/r/learnmachinelearning/comments/idz3ec/my_take_on_named_entity_recognition_and/"}, {"autor": "thejeran", "date": "2020-08-21 13:26:59", "content": "How do I prepare -----> image !!!  segmentation training set? Is there a way to use pre-segmented shapefiles over satellite data? /!/ I'm working on creating a crop field identifier and I have huge amounts of GIS data of fields and crops and I want to try creating a model to segment a satellite image into specific cropped fields. \n\nI'm new to Tensorflow and can only do image classification and object detection, but since I already have the shapefiles I'd like to learn image segmentation. I found a couple tools to create masks, but I'm still unsure what file format and whatnot they should be in.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/idwozr/how_do_i_prepare_image_segmentation_training_set/"}, {"autor": "ralphieIsAlive", "date": "2020-08-21 12:06:26", "content": "Making a Detector from a Classifier in Pytorch /!/ Hi guys,\n\nI'm trying to create an object detector detector in Pytorch from the ground up, the primary purpose for which is to detect circular objects on a dark background. My plan was to start with a working classifier, that I have already created and verified that it works. I used this tutorial as a guide:  [https://pytorch.org/tutorials/beginner/blitz/cifar10\\_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) \n\n&amp;#x200B;\n\nWhat I want to do next is implement a \"sliding-window\" like network that takes in an -----> image !!!  tensor \\[3, Height, Width\\] where 3 is the rgb channels and outputs a tensor of sliced -----> image !!! s \\[n, 3, h, w\\]. My classifier is designed to run on 32x32 images and I just resize the last 2 dimensions before running an image through it. Does anyone have any suggestions or references on how I could implement this sliding window system with pytorch?\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/idvh6l/making_a_detector_from_a_classifier_in_pytorch/"}, {"autor": "Sidharth_r", "date": "2020-08-21 05:53:35", "content": "Pest Detection using -----> image !!!  processing /!/ I wanted to do a project on the detection of pests and the removal of weed using image [processing. ](https://processing.How)  How could I connect this neural network to the motors or external devices to actuate the process?. I wanted to make a realtime pesticide spraying robot using an image processing.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/idr6d0/pest_detection_using_image_processing/"}, {"autor": "UAS_Data_Analyst", "date": "2020-08-21 01:38:02", "content": "Looking for how-to books on CNNs (with Python examples) /!/ Hello everybody, I am looking for some good books/textbooks covering Convolutional neural network networks (CNNs) for -----> image !!!  classification. I\u2019m new to this area, so I\u2019m looking for some with good how-to tutorials and examples of Python code. Does anybody have any suggestions?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/idnin5/looking_for_howto_books_on_cnns_with_python/"}, {"autor": "Competitive_Mongoose", "date": "2020-08-20 17:44:30", "content": "What kind of network will generate an -----> image !!!  from just a single number? /!/ I want to start a project where the end result will be a network that you can put a number into and it will give an output image (2D matrix). I am trying to do this in Tensorflow/Python. I am planning on training this by giving the network the number and then an image in a 2D numpy array (eg 1 -&gt; Mona Lisa, 2 -&gt; Starry Night, 3 -&gt; a world map ...). I want to do this and see what the network generates when I put something like 1.5 or 2.31.\n\nI was wondering if anyone wiser than me could recommend a way forward to me.\n\nI have tried three ways so far. The first one is having a single number as an input and somehow outputting a 2D array (just black and white). I am a beginner and got completely stuck with the shapes of everything.\n\nThe second way is creating an array filled with the single number that is the same shape as the output array. Again here I am also getting a bit stuck with Tensorflow's dimension errors.\n\nThe third is to flatten the image and have an array of the number that is the same length as the image.  From what I have read online, this one is my best bet.\n\nI'm a beginner and trying to do my first independent project and I am not sure of things like what layers to use, what kind of network is best and how I can fix errors about shapes that I'm getting (I'm pretty sure I'm getting them because I'm barking up the wrong tree).\n\nCould anyone advise?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/idf0df/what_kind_of_network_will_generate_an_image_from/"}, {"autor": "pranavsnr", "date": "2020-08-20 14:25:51", "content": "Confused about the path to pursue. /!/ Hi,\n\nI am an RPA (Robotics Process Automation) engineer who primarily works on tools like Uipath, Blueprism. It mostly involves rule based office automation. The job is becoming pretty boring for me, as all the use cases are very much similar in nature. And since it is rule based automation, there is no brainstorming involved, it's just developing code based on certain business rules.\n\nNow, I am planning to progress my career and move a step ahead and I want to venture into the territory of 'intelligent automation'. I have been doing some reaserch and I was comming across a lot of terms like Machine Learning, AI, Data science etc., But now I am confused on choosing the right path for me. My ambition is to work in a product company or start my own and build highly automated and intelligent self learning systems that can solve complex problems. It can be developing deployable recommendation engines, or complex -----> image !!!  recognition and processing system. \n\nCurrently I have some basic knowledge of Python, Probability and statistics. I know I need to learn much more. Reading so much has made me confused, so could you please let know which path I should pursue and which courses should I start of with so that I can progress my career in the right direction\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/idb904/confused_about_the_path_to_pursue/"}, {"autor": "nobgamer", "date": "2020-09-09 01:19:54", "content": "give me datasets for noobs /!/ TL;DR : i need some datasets to apply what i learned in andrew NG deep learning course that is not very hard\n\ni just finished 2nd andrew ng specialization course out of the 5. i thought why not make my own NN model to test what i understood and all that. i got some data set about flower type : 5 category , 3670 total -----> image !!!  , each -----> image !!!  is of different resolution, random named and they are taken anywhere ( some even are taken with as selfie )\n\ni did resize all -----> image !!! s to 256x256 ( for no reason i just thought its a good number) and had to rename everything  (thanks to little code i wrote :D ) then i divided them  about 80/10/10 .\n\nbuild the model and all but it seem to be way under fitting , like way way under fitting (25% train accuracy, and somehow 31% on test)\n\ni am really unsure is it the model or the data. right now im keep making the model bigger ( im training the 500 and 300  neurons on the deep layers) and i have no idea what are good numbers for the neurons \n\nif you read my ranting, give me advice if you can too XD", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ip6h5a/give_me_datasets_for_noobs/"}, {"autor": "JohnnyJoePLG", "date": "2020-09-07 03:37:45", "content": "Train Multi-Class Classifier for Multi-Label Classifier (on -----> Image !!! ) /!/ Hello, I\u2019ve stepped into the world of deep learning for months. I have finished the simple DNN for binary classification on image, or multi-class classification with CNN. \n\nCurrently, I want to try another project, but I encountered some problem. \n\nI want to implement \u201cface detection\u201d with deep learning. After searching, I found that YOLO will be a good choice because I can train neural network for face or object detection. After reading the YOLO1 Paper, I thought I can do some easier first...\n(Hope someone can give me some advice, I think detection = localization + classification. But I discovered that \u201clocalization\u201d is very difficult!!!)\n\nHence, I want to simply implement \u201cmulti-label\u201d classification. That is, an image including three people/faces is the input of model, model will output the labels representing three people. \n\nMy question is that I train my model for multi-class classification (1000 images for person A, 1000 images for person B and 1000 images for person C), and testing my model for multi-label classification (input an image including three people, model should output three class all with high value). But the test result is bad.\n\nI\u2019m so confused about this result. I think if the model is trained on multi-class classification, it means that the model can \u201cclassify\u201d these three people. However, why the testing result is still bad?\n\nThank you for your reading and comment.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/io069c/train_multiclass_classifier_for_multilabel/"}, {"autor": "MattPat1981", "date": "2020-09-06 20:08:01", "content": "Convolutional Neural Network to detect exoplanets? /!/ [I created a Recurrent Neural Network to act in concert with a Random Forest Classifier and a Logistic Regression Model to predict the existence of around six hundred exoplanets](https://github.com/MattPat1981/exoplanet_alpha_models)\n\nI want to build a Convolutional Neural Network using timeseries data to reinforce my findings, and would also like to build a CNN on -----> image !!!  data in the form of .fits files. Does anyone have experience in this area, or in the Astronomy domain using .fits files in python using astropy.io.fits, that might have input for my project? \n\nI would really like to use the .fits data to complete my CNN but the astropy.io.fits package seems to have lineage from Fortran and I am trying to figure out how to use it to put these images into Tensorflow.Keras.\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/insys7/convolutional_neural_network_to_detect_exoplanets/"}, {"autor": "nobgamer", "date": "2020-09-06 03:48:32", "content": "how does highly complex AI software take samples from games ? /!/ all videos on youtube show that the person makes the game from scratch so that he can access the data for AI to learn , such as: adding ray tracing for distance sensing , taking the value of orientation directly from the game code and so on \n\nmy question is: for games that is either too complicated to make just for an AI ( something AAA or even just a very hard to code game) , how to sample the game view? \n\njust taking a frame every X frames is the answer in my mind but then i ask wouldnt the resolution  be too high for efficiency and and if the game is 3D, wouldnt it be very hard to estimate ,say distance to enemy, just of the changing -----> camera !!!  instead of the fixed 2D screen?\n\nim wondering all this cuz i saw OpenAI play DOTA 2", "link": "https://www.reddit.com/r/learnmachinelearning/comments/inf8rr/how_does_highly_complex_ai_software_take_samples/"}, {"autor": "exiledforce1", "date": "2020-09-05 21:49:56", "content": "Image Recognition Machine /!/ Hi, I am looking for an -----> image !!!  recognition machine that can recognize duplicates of trading cards. I can easily pull all the images from trading card website. Ideally the machine could go through 100 cards at a time. Would something like this be possible to make or is there something already on the market that could be used for this that anyone knows of?  Thank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/in9ruj/image_recognition_machine/"}, {"autor": "stickybobcat", "date": "2020-09-28 06:15:02", "content": "Trying to list points based on an -----> image !!!  /!/ So, lets start by describing what I am trying to do. I want to take a picture of a hand and place points at the tip each knuckle and one point on either side of the wrist. I have about 1k of unpointed and pointed hand images, which can act as my sample. \n\nAt this point I can give a numerical rating for the correctness of a randomly pointed image based on the provided pointed image. I'm not really sure to go from here... I'm really new to this type of stuff, but the problem doesn't really use linear classification, which is the only thing I have worked with until now.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j16uj1/trying_to_list_points_based_on_an_image/"}, {"autor": "OnlyProggingForFun", "date": "2020-09-27 19:07:10", "content": "Imagine having the old, folded, and even torn pictures of your grandmother when she was 18 years old in high definition with zero artifacts. This is called old -----> photo !!!  restoration and this paper just opened a whole new avenue to address this problem using deep learning approaches", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j0wtfg/imagine_having_the_old_folded_and_even_torn/"}, {"autor": "kaiser_grey", "date": "2020-09-27 09:17:42", "content": "Algorithms for unsupervised -----> image !!!  clustering? /!/ guys i\u2019m a beginner to deep learning and this is going to be my very first project involving a big database. it involves creating classes for images and grouping the similar ones together. i\u2019m thinking of doing principles component analysis on the images first and then apply a few algorithms (minimum five) to cluster the images. which algorithms do you think will be most helpful in my case?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j0o6by/algorithms_for_unsupervised_image_clustering/"}, {"autor": "SuccMyStrangerThings", "date": "2020-09-27 06:11:16", "content": "A few queries about Faster RCNN and Bounding Boxes /!/ 1) Faster RCNN which is built on top of VGG16 uses pretrained VGG16 Conv2D layers and omits Dense Layers right? How do i use the pretrained vgg16 Conv2d layers only? \n \n\n\n2) VGG16 takes images of size 224x224 as input. So would it affect my -----> Image !!!  annotations and Object Detection?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j0m32w/a_few_queries_about_faster_rcnn_and_bounding_boxes/"}, {"autor": "EpiStat", "date": "2020-09-27 05:39:09", "content": "Coursera enough time learn about -----> image !!!  process and using CNN? /!/ I\u2019m interested in replicating the projects below for my research thesis but for another anatomical body part in orthopaedics. \n\nI know the Coursera online course has been regarded and mentioned several times here. I am posting to see if that course would teach me the fundamentals of how to take large image datasets, process them accordingly, enter into a model and be able to test that model with another sample of images?\n\n\nhttps://pubmed.ncbi.nlm.nih.gov/31002938/\n\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC5789045/", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j0lors/coursera_enough_time_learn_about_image_process/"}, {"autor": "Danielo444", "date": "2020-09-26 15:29:03", "content": "Real -----> image !!!  or not neural network /!/ Hello,\nI'm trying to find a pretrained neural network that you can input an image into and get the probability it's a real image rather then a Synthetic one. The goal is to use it on mostly landscape images and not faces.\nIf anyone knows such a pretrained model or a network configuration with the database to train it on and can post a link it that would be very appreciated.\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j080fp/real_image_or_not_neural_network/"}, {"autor": "Pawan315", "date": "2020-09-26 11:01:26", "content": "Upscale any -----> image !!!  TUTORIAL", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j0446q/upscale_any_image_tutorial/"}, {"autor": "CitizenCinco", "date": "2020-09-26 02:17:14", "content": "Can ML be used to train for -----> image !!!  classification and distance estimation? /!/ Hello, I would like to start a project to implement a trained NN in an FPGA. I would like to mix it up adding a 3D stereo camera to estimate the distance of a face.\n\n&amp;#x200B;\n\nI could possibly see this being beneficial for a drone or some type of tactical/spy/security equipment in which bandwidth is limited/costly but information is vital at times. (let's say you are looking to identify a target in a mountainous region with a drone that performs periodic sweeps of the area and relays information once something is found. However, since surrounding region is a bunch of forest or sea or limited in landmarks, the exact location of the target is hard to identify just based on the drone's GPS.)\n\n&amp;#x200B;\n\nI want to use an FPGA because of the benefits of low power. I have a Cyclone V GX starter kit but I don't have to use this device target but ideally something I can get my hands on. (AWS would be good enough to train but if I wanted to implement this in a real device, ideally I transfer the model parameters to my device.) I am very familiar with RTL but not so much with ML and training and inference. \n\n&amp;#x200B;\n\nIs there a dataset I could use that trains both these features at the same time? Can I train a NN with two different datasets? At different training times? How would I begin to go about this project if I decided to try it? And what existing work is out there?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/izx9pu/can_ml_be_used_to_train_for_image_classification/"}, {"autor": "NuwandaCZE", "date": "2020-09-25 20:15:06", "content": "General questions regarding semantic segmentation and -----> Image !!!  labeler (Matlab) for upcoming thesis", "link": "https://www.reddit.com/r/learnmachinelearning/comments/izr2wr/general_questions_regarding_semantic_segmentation/"}, {"autor": "SuccMyStrangerThings", "date": "2020-09-25 16:25:56", "content": "Which is the best object detection for Medical Datasets? /!/ Title is pretty self explanatory. I'll be dealing with small details. I have approx 2.5k images in my data set along with bounding boxes co ordinates in a xaml file. Which algorithm would suit better?\n\nFollow up question:\n\nDo you guys suggest reading the coordinates of BBoxes in a list of list i.e for every -----> image !!!  in another array there will be a Bbox coordinates for that in another list?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/izmm2z/which_is_the_best_object_detection_for_medical/"}, {"autor": "awezmm", "date": "2020-08-03 02:03:54", "content": "Semantic Segmentation with FCN variable -----> image !!!  sizes /!/ Does anyone know of any resource that provides code for implementing semantic segmentation with FCN so that images with different sizes can be segmented?\n\n&amp;#x200B;\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i2ol3o/semantic_segmentation_with_fcn_variable_image/"}, {"autor": "etdeagle", "date": "2020-08-02 23:52:36", "content": "Style transfer with Inception v1 is overly saturated /!/ Hi Reddit,\n\nI am a hobbyist in style transfer and recently I have tried a method based on the Inception v1 network for style transfer: [https://distill.pub/2018/differentiable-parameterizations/#section-styletransfer](https://distill.pub/2018/differentiable-parameterizations/#section-styletransfer)\n\nThe code is at: [https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/style\\_transfer\\_2d.ipynb](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/style_transfer_2d.ipynb)\n\nThe code works pretty well on some style -----> image !!! s like the examples in the blog post and on some others it produces bad results:\n\n\\- -----> image !!!  is over saturated\n\n\\- noticeable white or black \"clouds\" cover the -----> image !!! \n\nHere is an example of when it does not work:\n\ncontent -----> image !!! : [http://13.57.25.13/static/whale.png](http://13.57.25.13/static/whale.png)style image: [http://13.57.25.13/static/watercolor1.png](http://13.57.25.13/static/watercolor1.png)content\\_weight = 50\n\n    content_obj = 50 * activation_difference(content_layers, difference_to=CONTENT_INDEX)\n\nresult (overly saturated): [http://13.57.25.13/static/styletransfer.png](http://13.57.25.13/static/styletransfer.png)\n\nNow I am not a computer vision expert, just a hobbyist, so I am not sure where to start to resolve this issue. What do you think? What sort of data or experiments should I pursue to troubleshoot the issue?\n\nThank you in advance for your advice.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i2miv8/style_transfer_with_inception_v1_is_overly/"}, {"autor": "EmptyEntertainment8", "date": "2020-08-02 21:50:11", "content": "What would it take to create a sign language translator? /!/ Hi I\u2019m a high school student, I was looking for creating a an algorithm to take a -----> picture !!!  of a hand like \ud83e\udd1eand recognize it as the letter R and print out R. Hopefully later it becoming more advanced to reach full words and then sentences, but that\u2019s super far off. Please don\u2019t steal my idea lol this is the only one I\u2019ve had. I haven\u2019t done anything in machine learning and I sort of find it boring with all the math. I do like programming though. Even if it\u2019s a little boring, I really want to do it. What do I need to learn to be able do this?\n\nIt\u2019s basically includes:\n\n- Hand recognition through a picture taken from the computer \n\n-taking the picture and connecting it to a database of hand signs that connect to an English word or letter  \n\n-determining which letter it is and printing it out to the screen", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i2kic4/what_would_it_take_to_create_a_sign_language/"}, {"autor": "NikolasTs", "date": "2020-08-02 09:22:24", "content": "Visualizing Classes in CNNs with gradient ascent - L2 norm /!/ Hello,\n\nI am studying about CNN visualization and while implementing the gradient ascent algorithm for generating an -----> image !!!  that would maximize the activation for a certain class I came across this piece of code: \n\n    img.data += learning_rate * img_grad / img_grad.norm() \n\nThe code is implemented with pytorch and the img is a tensor that has been generated randomly. The img pixels are updated on every iteration of gradient ascent so that the output for a certain class is maximized (so far so good). \n\nHowever, I don't understand why dividing by the img\\_grad.norm() (basically the L2 norm of the tensor) drastically improves the output! Could someone explain it?\n\nI have attached below two examples. The first image was generated using the norm() and the second one without using it. (the goal was to maximize the output for the tarantula class) \n\n&amp;#x200B;\n\n&amp;#x200B;\n\n[Using the .norm\\(\\)](https://preview.redd.it/95nd44z74ke51.png?width=217&amp;format=png&amp;auto=webp&amp;s=6fbe8cea1f8a86a79894514a7801a551cf3ccc0d)\n\n&amp;#x200B;\n\n[Without the .norm\\(\\)](https://preview.redd.it/3bdcvr3a4ke51.png?width=216&amp;format=png&amp;auto=webp&amp;s=7fd2f49a5f71b4fa7c81ee970ab1143c79adc6b3)\n\n&amp;#x200B;\n\nThanks in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i29bay/visualizing_classes_in_cnns_with_gradient_ascent/"}, {"autor": "OnlyProggingForFun", "date": "2020-08-01 13:54:12", "content": "This AI can generate the pixels of half of a -----> picture !!!  from no other information using a NLP model", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i1sk9v/this_ai_can_generate_the_pixels_of_half_of_a/"}, {"autor": "afreydoa", "date": "2020-08-01 11:12:12", "content": "How do I know, that this person does not exist? /!/ Given generated -----> image !!! s like [thispersondoesnotexist.com](https://thispersondoesnotexist.com), how do implementors of GANs check if the generated -----> image !!!  or a very similar one really was not in the training data? How does one ensure that the generator is not overfitting to the training data through the discriminator? \n\nLooking through the database by hand is probably infeasible. Building a distance measure from simple pixels is probably not going to work and building a more sophisticated distance measure is as hard as training that GAN itself, I'd suppose.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i1qk83/how_do_i_know_that_this_person_does_not_exist/"}, {"autor": "sneaky____beaver", "date": "2020-08-01 08:52:07", "content": "Attempting deepfake to make images talk using a video. /!/ DeepFakes have taken the internet by storm so I thought I'd try some myself. I had way too much fun creating these absurd videos and watching still images come to life. I took 6 still images of my face (first square) and tried to make them talk using a clip from a Donald Trump speech (second square). The output is displayed in the third square. Two things I learnt while playing with this notebook,\n\n&amp;#x200B;\n\n1. The model DOES NOT LIKE having teeth in the source -----> image !!! , it tries to keep it in frame always.\n2. The model NEEDS teeth in the source image, or else it creates buggy teeth to compensate for it.\n\n&amp;#x200B;\n\nThe output was better than expected and a lot of fun nonetheless. Would love to hear your opinion and suggestions on this. Enjoy some wacky videos for the weekend and try it out yourself!\n\n[Notebook Used](https://colab.research.google.com/github/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb)\n\n![video](x9he83ycuce51)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i1p4o7/attempting_deepfake_to_make_images_talk_using_a/"}, {"autor": "BigGuy5630", "date": "2020-08-01 02:07:28", "content": "transfer learning for writing style /!/ I apologize if this is a vague question, I don't know of a better way to ask this question.\n\nJust as you can replace a person's face with another person's -----> photo !!! , has there been any research work that can transfer writing style from one person to another and keep the meaning of their writing?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i1kcc5/transfer_learning_for_writing_style/"}, {"autor": "pablocael", "date": "2020-07-31 23:48:42", "content": "How to study and where to go /!/ \nIm a 15 years experienced software engineer with a CS and math Bachelor\u2019s and master in computer graphics. Ive done a few online courses about ML, and Im currently working as a Software Engineer in Japan (basic development stuff). Im willing to move to ML are as I find it interesting, but as we all know, its not trivial to move to another area even if its within CS.\nSo I decided to take a break and dedicate to studying ML and creating my own projects to build a portfolio. I will be taking 6 months to 1 years time off to dedicate to changing area.\nMy questions to you guys are:\n1) what seems to the hottest sub area (NLP, GAN, Convolutional for -----> Image !!!  recognition, ...) that will get more demand in near future?\n2) What is recommended approach for self studying my way into ML? Im thinking about being pro active and putting my self in kaggle and other challenges to force into learning. But also study the very foundations and math background as I like to understand things, although I know in daily work life what matters is knowing how to use thinks and not to know how all works.\n3) Recommended courses, blogs, youtube channels, books?\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i1iayg/how_to_study_and_where_to_go/"}, {"autor": "bkelleher15", "date": "2020-07-31 21:15:00", "content": "Looking for advice /!/ Looking for direction with a project I am working on. Using -----> camera !!!  feeds for motion detection and tracking. Thanks in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i1fqti/looking_for_advice/"}, {"autor": "DJ771997", "date": "2020-10-15 02:31:50", "content": "Is there an existing code where I can use my own images to train rather than use already trained classifiers like haarcascade? /!/  Is there an existing algorithm/code (that classifies human faces and gives true/false output based on whether there is an -----> image !!!  in the picture) where I can train the model by using my own -----> image !!! s? I know I can use haarcascade etc, but I want to use my own training data. sorry, I'm new", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jbf529/is_there_an_existing_code_where_i_can_use_my_own/"}, {"autor": "aagnone", "date": "2020-10-14 23:24:58", "content": "[D] Machine Learning Up-To-Date #18 /!/ Here's #18 from the ML UTD newsletter!\n\n[https://www.lifewithdata.org/newsletter/mlutd18](https://www.lifewithdata.org/newsletter/mlutd18)\n\n&amp;#x200B;\n\n[The layout of the transformer architecture applied to an -----> image !!!  recognition task \\[source\\]](https://preview.redd.it/cabfqf3095t51.png?width=1056&amp;format=png&amp;auto=webp&amp;s=98b62919b04a66828e2d40ccc7d4fe39a7d30567)\n\n## ML UTD 18\n\n### Applications \n\n* What\u2019s new in TensorFlow Lite for NLP\n* Dagster: The Data Orchestrator\n* Imaginaire: NVIDIA + PyTorch = GANs\n\n### Academia\n\n* Introducing Dynabench: Rethinking the way we benchmark AI\n* An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n* What Can We Learn from Collective Human Opinions on Natural Language Inference Data?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jbc3ee/d_machine_learning_uptodate_18/"}, {"autor": "iAmTheAlchemist", "date": "2020-10-14 11:08:42", "content": "Looking to sample diverse data in 3D space to train a RBF model /!/ Hi all !\n\nI am very new to machine learning and I got started with a project that I am working on at the moment.\n\nIt uses RBF Interpolation to reverse engineer color transormations made by legacy photographic -----> film !!!  scanners to convert negative images into positive final images. What it does is it samples the same pixels between an raw negative scanned image and a final image that has been inverted by a legacy but very good software, and the RBF is trained to find the relationship between the RGB values in the raw file and the target image. This RBF is then used to generate a 3D LUT that can be applied to other raw images aquired by the same sensor to apply the color transformation.\n\nIn practice, I have 3 RBFs running, one for each color channel in the final image, so one that converts original RGB to final R, one that does original RGB to final G, and same for blue.\n\nThis system works even when trained on little data points but it could be made more accurate by sampling the pixels to train it with in a smarter way. RBF is slow when it is fed a lot of very similar data and right now, i only sample regularly spaced pixels in the image, meaning some important colors could be left out and it could sample very close colors.\n\nHence my idea to cherry-pick pixels based on their position in 3D space, rather than choosing random ones, to maximize diversity in data to feed to the RBF algorithm.\n\nIs there any way to do that in an elegant way ? I have thought about organizing the points into smaller cubic bins (ie divide the space in x wide cubes that contain some points to sample from), but there might be a method based on density or something I am missing entirely ?\n\nLet me know if it makes sense to you, I'm not really great at explaining this stuff, and thank you very much in advance for your help !\n\n&amp;#x200B;\n\nExample pic : [https://imgur.com/a/vN0pJAV](https://imgur.com/a/vN0pJAV)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jayn8a/looking_to_sample_diverse_data_in_3d_space_to/"}, {"autor": "New_bie_25", "date": "2020-10-13 18:13:37", "content": "AI-Powered Video Conferencing with NVIDIA /!/ Can someone share the necessary research papers related to the Nvidia ai-powered video conferencing based on advanced deep learning techniques especially reducing background noise and enhancing the -----> image !!!  resolution.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jaj065/aipowered_video_conferencing_with_nvidia/"}, {"autor": "reddituser20-08", "date": "2020-10-13 07:17:50", "content": "Need help with collecting data for a healthcare Flutter app /!/  Hey everyone. A few of my college mates and I are working on a project that aims to make the quality of life for PD patients better. We are also planning on applying to hackathons to win funds to better support this project of ours. Our idea aims to provide a Parkinson\u2019s Disease symptom assessment tool on an Android app. This app would allow PD patients to self-assess their symptoms with the help of -----> image !!!  analysis by a machine learning model to analyze the severity of PD. The problem we have run into is the lack of data hence the accuracy of our model suffers. We are therefore requesting anyone who can here, to volunteer to collect more data for us by requesting as many PD patients as they can, to take this test on our behalf. This data will be submitted anonymously with the consent of the patient. Please find the google drive link as a comment which consists of a smaller app that we have developed to collect this data. Anyone who volunteers to help us with this needs to get the PD patients to take the tests on the app and send me the result images as a direct chat message without the personal details of the patient. The video there explains how that app is to be used. Please find a presentation file in the drive link for more details on this project. Thank you for taking your time out to read this, especially if you are volunteering to help us in this endeavor.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ja8ln1/need_help_with_collecting_data_for_a_healthcare/"}, {"autor": "deeplearningperson", "date": "2020-10-12 13:54:01", "content": "An -----> Image !!!  is Worth 16x16 Words:Transformers for -----> Image !!!  Recognition at Scale (Paper Explained)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j9rab3/an_image_is_worth_16x16_wordstransformers_for/"}, {"autor": "RSchaeffer", "date": "2020-01-23 18:14:26", "content": "Convert TensorBoard -----> Image !!!  Sequence to Video/GIF /!/ I don't know if this is a permissible question to ask here, but I posted the following question on StackOverflow ([https://stackoverflow.com/questions/59884565/convert-tensorboard-image-sequence-to-video-gif](https://stackoverflow.com/questions/59884565/convert-tensorboard-image-sequence-to-video-gif)):\n\n&amp;#x200B;\n\nI have a script that generates a sequence of matplotlib figures and writes them to disk using a TensorBoard SummaryWriter().  TensorBoard offers the ability to move a little slider to advance  forwards and backwards through the image sequence, but I'd like to  convert the image sequence to a video or animation. Is there a way to do this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eswyun/convert_tensorboard_image_sequence_to_videogif/"}, {"autor": "ashis_sinha", "date": "2020-01-23 10:07:53", "content": "How do I dig deeper into Machine Learning. /!/ I know the overview of how the model is made and how basic algorithms work and how to make a basic classifier model and similar stuff like that, lately I was able to build a -----> image !!!  classifier using transfer learning, i am that familiar with tools in this field.\n\nBut the thing is I need to know a lot of things which are being used and done in the industry, being a masters student i am finding it very difficult to get hired. How do I further this knowledge to more real world applications and gain more insight into this field.\n\nI am attaching my LinkedIn and GitHub profile along with this post, so any help in this direction would be appreciated.\n\n[LinkedIn](https://www.linkedin.com/in/ashishsinha5)\n[GitHub](https://www.github.com/AshishSinha5)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/esr2e0/how_do_i_dig_deeper_into_machine_learning/"}, {"autor": "rappatic", "date": "2020-01-23 01:57:41", "content": "Python library to train model with images and outlines /!/ I have a dataset that is comprised of -----> image !!! s linked together like this:\n\n*Processing img s907os71qfc41...*\n\nhttps://preview.redd.it/ib5f13rzofc41.png?width=5000&amp;format=png&amp;auto=webp&amp;s=a34618f2d3604067221bc86c311a7f59565f1690\n\nThe first is an ordinary -----> image !!! , but the second is the same -----> image !!!  with the structures I want to identify in white and the rest of the map in black.\n\nIs there a good ML library in Python that will allow me to easily pass the two images, along with many others similarly linked, to train a model on?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/esm7js/python_library_to_train_model_with_images_and/"}, {"autor": "ml_runway", "date": "2020-01-22 20:32:59", "content": "How many times to augment each -----> image !!!  in training set? /!/ I thought this would be an easy question, with an answer accepted independently of the particulars. So silly.\n\nMy particulars: I have a relatively simple object detection task, with about 1100 annotated images, only a single category. I do an 80/20 split, and even when I augment just 10x for each training image [h/v flips, small rotations, brightness changes, gaussian blur, motion blur], the network is learning fairly well given all my other parameters. I am using faster-rcnn with resnet-101 backbone.\n\nMy next step is to add more augmented images per training images (I had always though 100 was a good number but now that I think about it, this is sort of arbitrary and as I research it online I realize it is not really based on anything). \n\nAny advice, suggestions for good literature or sites or simulations to look at about this? I thought this would be easier I have a feeling I'm using the wrong search terms or something.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eshpz7/how_many_times_to_augment_each_image_in_training/"}, {"autor": "alinrauta", "date": "2020-01-22 15:59:26", "content": "My attempt at explaining what Artificial Intelligence is /!/ First, let\u2019s take each word of the term individually and try to explain it. Artificial refers to something made by humans (in contrast to nature made stuff) and usually is a copy of something natural. So, artificial is about humans trying to replicate Mother Nature. \n\nWhat about the meaning of intelligence? Well, that\u2019s when the fun begins.  We call ourselves \u201cHomo Sapiens\u201d, which in Latin means \u201cwise man\u201d and we like to take pride in being so intelligent in comparison to our fellow peers from the animal kingdom. But what does intelligent really mean? Some of us may think of IQ tests, while others may think of surviving. I propose the following definition: being able to learn and apply what was learned (of course, this is a simplified definition, not a thorough one). \n\nPutting these together we may say that artificial intelligence is \u201csomething made by humans that is able to learn and apply what was learned\u201d. \n\nThis is just a starting point because in real life things are far more complex. The textbook definition of AI is the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximise its chance of successfully achieving its goals. Truth be told, there is no quite an agreement about an exact definition even among AI researchers. So, I think the best way to understand the meaning of artificial intelligence is to discuss about its pursuits.\n\nEven though you may have heard about it recently, AI was founded as an academic discipline in 1956 and along the years it has experienced its share of ups and downs (the history of AI will be discussed in a future article). \n\nThe long-term of AI is to eventually carry on any task that a human being can do it. It\u2019s more like reverse engineer our brain and then create an artificial brain that functions the same as our human brain. Since we haven\u2019t yet (fully) discovered how our brain works that\u2019s why this goal is a long-term one. \n\nIn the meantime, let\u2019s discuss about more approachable pursuits of AI that the reader may have already heard of (there are more than those discussed here).\n\n**Machine Learning (ML)**\nTagging people and objects in photos, content recommendation, google search, what about them? They are all examples of machine learning application. The next video to watch showed to you on youtube is based on past data of your online behaviour on youtube. Same thing applies to facebook feed algorithm. The more I click on real estate links, the more facebook will show to me sponsored real estate pages. When you search on google let\u2019s say \u201cjava\u201d it can show you the first results about coffee or about the programming language depending on your search history. This is all machine learning. It\u2019s about using data to answer questions, finding and extrapolating patterns. \n\nLet\u2019s take a numerical example. We have a couple of values for two number and the task is to find the relationship between them (try to find it alone before reading the answer): \n\nx = 0, 1, 2, 3, 4, 5\ny = -1, 1, 3, 5, 7, 9\n\nUsing machine learning we can quickly learn that the relationship between the two numbers is the following: 2x - 1 = y. Basically, machine learning helped us finding a pattern in the numbers (data) we have. \n\n**Natural Language Processing (NLP)**\nSiri, Cortana, Alexa, these are prime examples of NLP at play. Natural Language Processing is about machines understanding what you mean when you are saying something, to get the context of the conversation. When you ask Siri to play some rock you are referring to play some rock songs, not to play with a (physical) rock. That\u2019s the challenge of NLP, to make a machine to be able to talk with a human being by understanding what is being discussed and to come up with its own opinions.\n\n**Robotics**\nSelf-driving cars are an example of robotics. A self-driving car takes data from its environment, process that data and makes a decision. Or take for example a smart vacuum cleaner robot. What it really does is to take cues (data) from the environment (your room), process it and decide which way to move. If is continuously bumping into your furniture then that wouldn\u2019t be too intelligent, right? Another example of AI in robotics could be drone delivery. The road from factory to destination is paved with obstacles, so the ability to make the delivery is a matter of understanding the environment and make decisions accordingly.\n\n**Computer Vision**\nCoffee mugs inspection on a production line is an example of computer vision. It \u201clooks\u201d at a coffee mug and tries to find any evidence of cracks, if none found then the mug is ready to be packed and shipped to customers. Basically, computer vision deals with making a machine to gain understanding from an -----> image !!!  or video. That\u2019s what happens when you use Face ID on your iPhone, the phone recognises you and unlocks itself. This is called facial recognition and is widely used in China where cameras are almost everywhere and can track your behaviour on the streets. It has the ability to catch you jaywalking, information which will be used to lower your social score (which is kind of a reputation score).\n\nAI is comprised of quite a few number of subfields making it a complex and a universal field. To be truly understood you have to see it through its numerous lenses. That\u2019s why is so hard to come up with an encompassing definition. \n\nHowever, I will end by giving a shot at defining AI in my own view. For me, Artificial Intelligence is the field that deals with replicating every form of human intelligence with the purpose of creating a machine capable of acting and thinking intelligently.\n\nI hope you find this article useful and it will get you intrigued about the AI field. If I manage to do that, then my goal is achieved. \n\nIf you liked this article and want to see more of these, then follow me on [twitter](https://twitter.com/RautaAlin)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/esdyvp/my_attempt_at_explaining_what_artificial/"}, {"autor": "niszoig", "date": "2020-01-22 09:07:46", "content": "[D] How come face attributes are linearly separable in the latent space of a GAN? /!/ How come attributes like smiling,glasses are linearly separable by a hyperplane in the latent space?.What confuses me is that the generator is a function that maps from input RANDOM noises z drawn from a Gaussian distribution P(Z) to -----> image !!!  space.\n\nI stress on Random because: Let us assume we are in R2 .We randomly sample 10 points from the Gaussian distribution and label them \"RED\"(Analogous to Non-smiling faces).We sample another 10 points and label them \"BLUE\"(Analogous to Smiling faces) Isn't it unlikely that the RED points and BLUE points are linearly separable? Is it the case that in higher dimensions,it is more likely that the points are linearly separably by a hyperplane?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/es9gw9/d_how_come_face_attributes_are_linearly_separable/"}, {"autor": "earnnu0", "date": "2020-01-30 12:27:23", "content": "Extraction information from PDF files (Invoice number, line items in table) /!/ Hi there\n\n  \nFirst of all, I am fairly new to the ML world. I have researched quite a lot on the different use cases that ML have, both in regards to working with text and images.\n\nI am trying to build a \"pipeline\", that can extract a few data points from various supplier **invoices**:\n\n1. Invoice number\n2. Line items (that resides in tables)\n   1. For each line item, I would need: **quantity** and **line amount**\n\n&amp;#x200B;\n\nMy first thought was to just use a classic OCR parsing tool such as DocParser (which is basically a template-based OCR parsing tool, where you can create parsing rules for each different type of invoice layout). However, I took a look at my suppliers, and I have a lot of different layouts (with new ones being added regularly).\n\n**I was thinking if ML can be used to accomplish this task?**   \n\n\nMy idea for a pipeline:\n\n1. All supplier invoices are converted from PDF to an -----> image !!!  file (.jpg) and then resized so all have the same width and height.\n2. Train a custom model to extract invoice number using **named entity recognization (NER)** \n3. Train a custom *computer vision* model to identify tables that contain line items (product information)\n   1. For each table found, extract that as an image and train another model to identify the entities that I need for each line: **quantity** and **line amount**\n\n  \nI am not sure if this is a good approach to this problem? Does it make sense to ultimately end off with **three** models to extract the information that I need? Is there an easier way to detect the **quantity** and **line amount,** than by first locating the table on the PDF file?  \n\n\nDoes anyone have any experience with a similar process?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ew4og5/extraction_information_from_pdf_files_invoice/"}, {"autor": "MLtinkerer", "date": "2020-01-30 03:16:59", "content": "State of the art in producing high-resolution -----> photo !!! -realistic images (using generative models)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/evyrux/state_of_the_art_in_producing_highresolution/"}, {"autor": "MarciOaks", "date": "2020-01-28 19:10:53", "content": "Help implementing MNIST model with RPi and Pi Camera /!/ Hi, I'm doing a highschool research project on ML and ANNs, and I want to show it by implementing a MNIST trained NN into a RPi using the -----> camera !!! . I've already made the model and saved it as a .h5, but I'm stuck with implementing new data to it. \n\nI've tried using OpenCV with a script I found on a tutorial on hackster.io, but it has given me more and more problems. Is there any way of taking a photo with the PiCamera, normalizing it and then feeding it to the model without using OpenCv?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ev9y5g/help_implementing_mnist_model_with_rpi_and_pi/"}, {"autor": "arsalyou", "date": "2020-01-28 12:18:09", "content": "How can I improve pix2pix model's output -----> image !!!  /!/  I have a dataset consisting of 216 images. I trained for 100 epochs but unfortunately the results are not good. Can anyone me how can I improve the results?\n\nIs dataset not enough for model to produce better output or is there something else which can be done to improve results?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ev4v0f/how_can_i_improve_pix2pix_models_output_image/"}, {"autor": "Guyot11", "date": "2020-01-28 06:25:40", "content": "Which model to run? /!/ I have a large dataset (\\~7k images) in which I have 3 channels as well as binary arrays for truth. I am trying to find the horizontal line that can be seen at the bottom of this -----> image !!! . Each image will have one horizontal line at a different height (near the bottom) and essentially demarcates where the signal drops off (below the line). I want to use some machine learning algorithm to train from the large dataset I have to best predict where this line should be. This example is clear-cut, however, I have other cases where the position of the line is much more difficult for a conventional algorithm to find.\n\n [https://imgur.com/a/vgxGhFz](https://imgur.com/a/vgxGhFz) \n\n&amp;#x200B;\n\nI've been looking into semantic segmentation, but am unsure if that is the best route to take and would appreciate any suggestions pointing in the right way!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ev1rlb/which_model_to_run/"}, {"autor": "xuximpea", "date": "2020-01-07 01:57:49", "content": "model.predict doesn't return expected result /!/ from keras.models import load\\_model  \nfrom keras.preprocessing import -----> image !!!   \nimport numpy as np\n\n  \nimg\\_width, img\\_height = 200, 200  \n\\# load the model we saved  \nnmodel = load\\_model('vgg16.h5')  \nmodel.compile(loss='binary\\_crossentropy',  \noptimizer=optimizers.SGD(lr=1e-4, momentum=0.9),  \nmetrics=\\['accuracy'\\])  \ntest\\_image = image.load\\_img('/content/cat-pet-animal-domestic-104827.jpeg', target\\_size=(img\\_width, img\\_height))  \ntest\\_image = image.img\\_to\\_array(test\\_image)  \ntest\\_image = np.expand\\_dims(test\\_image, axis=0)  \ntest\\_image = test\\_image.reshape(1, img\\_width, img\\_height, 3)    # Ambiguity!  \npredict = nmodel.predict(test\\_image, batch\\_size=32, verbose=1)  \ncategory = np.where(predict &gt; threshold, 1,0)  \nprint(category)\n\n&amp;#x200B;\n\nthe code returns, instead of a 0 or 1 as expected, this:\n\n \\[\\[\\[\\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 0 0 0\\]\\]    \\[\\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 1 0 0\\]    \\[0 0 0 ... 0 0 0\\]\\]    \\[\\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 1 0 0\\]    \\[0 0 0 ... 0 0 0\\]\\]    \\[\\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 1 0 0\\]    \\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 1 0 0\\]    \\[0 0 0 ... 0 0 0\\]\\]    \\[\\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 0 0 0\\]\\]    \\[\\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 0 1 0\\]    \\[0 0 0 ... 0 0 0\\]    \\[0 0 0 ... 0 1 0\\]    \\[0 0 0 ... 0 1 0\\]    \\[0 0 0 ... 0 0 0\\]\\]\\]\\]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/el4cqi/modelpredict_doesnt_return_expected_result/"}, {"autor": "dreyfus34", "date": "2020-01-07 00:04:45", "content": "How to Increase Iterations in StyleGAN? /!/ I\u2019m a complete noob trying to learn/play with StyleGAN. I\u2019ve no formal qualifications in ML.\n\nI\u2019m using StyleGAN to train a model with an -----> image !!!  set that I built, but the output -----> image !!!  quality is poor. The training stops after 10 ticks. \n\nHow can I change the default parameters in the code to continue training past 10 ticks. Do I need to change the fid50k settings? Are the settings in this block of the train.py file? or the metrics file?\n\nPlease help, been breaking my head for many hours.\n\n\n    # Default options.\n    train.total_kimg = 25000\n    sched.lod_initial_resolution = 8\n    sched.G_lrate_dict = {128: 0.0015, 256: 0.002, 512: 0.003, 1024: 0.003}\n    sched.D_lrate_dict = EasyDict(sched.G_lrate_dict)\n\n\nTraining output log:\n\n\ntick 1     kimg 140.3    lod 4.00  minibatch 128  time 8m 38s       sec/tick 461.9   sec/kimg 3.29    maintenance 56.1   gpumem 3.0 \nnetwork-snapshot-000140        time 13m 11s      fid50k 353.7462  \ntick 2     kimg 280.6    lod 4.00  minibatch 128  time 29m 42s      sec/tick 453.7   sec/kimg 3.23    maintenance 809.9  gpumem 3.4 \ntick 3     kimg 420.9    lod 4.00  minibatch 128  time 37m 16s      sec/tick 453.8   sec/kimg 3.23    maintenance 0.9    gpumem 3.4 \ntick 4     kimg 561.2    lod 4.00  minibatch 128  time 44m 52s      sec/tick 454.4   sec/kimg 3.24    maintenance 0.9    gpumem 3.4 \ntick 5     kimg 681.5    lod 3.87  minibatch 128  time 1h 01m 06s   sec/tick 973.4   sec/kimg 8.09    maintenance 0.9    gpumem 4.5 \ntick 6     kimg 801.8    lod 3.66  minibatch 128  time 1h 21m 43s   sec/tick 1235.8  sec/kimg 10.27   maintenance 1.6    gpumem 4.5 \ntick 7     kimg 922.1    lod 3.46  minibatch 128  time 1h 42m 20s   sec/tick 1235.3  sec/kimg 10.27   maintenance 1.2    gpumem 4.5 \ntick 8     kimg 1042.4   lod 3.26  minibatch 128  time 2h 02m 57s   sec/tick 1235.9  sec/kimg 10.27   maintenance 1.2    gpumem 4.5 \ntick 9     kimg 1162.8   lod 3.06  minibatch 128  time 2h 23m 34s   sec/tick 1236.4  sec/kimg 10.28   maintenance 1.2    gpumem 4.5 \ntick 10    kimg 1283.1   lod 3.00  minibatch 128  time 2h 43m 58s   sec/tick 1222.6  sec/kimg 10.16   maintenance 1.2    gpumem 4.5 \nnetwork-snapshot-001283        time 14m 20s      fid50k 349.5387", "link": "https://www.reddit.com/r/learnmachinelearning/comments/el2v3k/how_to_increase_iterations_in_stylegan/"}, {"autor": "tamay1", "date": "2020-01-06 21:29:12", "content": "Are there any good databases on ML performance over time? /!/ I've been thinking about doing some research into the rate of progress in ML, so I'm in need of data on performance on some standardised benchmarks over time. So my questions are:\n\n* Are there any good repositories of performance of ML models on different benchmarks, such as [Benchmarks.ai](https://benchmarks.ai/)?\n* Are there any longstanding benchmarks for any particular application (NLP, -----> image !!!  recognition, translation, etc.) with many submissions made over a period of at least a year or so, such as the [Gluebenchmark](https://gluebenchmark.com/leaderboard/)?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/el0nmd/are_there_any_good_databases_on_ml_performance/"}, {"autor": "_allo_", "date": "2020-01-06 19:07:03", "content": "Is there a good tutorial, that is neither too abstract nor too practical? /!/ Every time I try to learn how to build ML models, I have one of these these problems:\n\n- It is too abstract. I see a diagram of blocks and even when I understand the abstract aspects, I am missing what it actually is. A LSTM is a nice block, but what *is* the math formulation that describes a gate in it? It is often described like an algorithm, but inside the black box there is a piece of differential math formulation.\n- It is *only* math and I do not see how this is applied in a program.\n- It is a tutorial that just tells you which lines of python get the job done, but does not really teach why it works this way and even more important how you learn the important parts for coming up with own models.\n\nEspecially I often see tutorials like \"From zero to MNIST in half an hour\". It is nice when I can put something together in keras that allows me to recognize digits. But there are already a lot of codes for this, but when I have an own problem, I learned *nothing* about how I would design a ML model to solve my problem.  \n-----> Image !!!  problems have a nice fixed input/output format, but when I read a tutorial how to apply a network to a 28x28x256 tensor, I know nothing about e.g. how to process text, what is a type of input that has a variable length.\n\nThese tutorials usually pick the most basic case and then stop when you start wondering if you *design* a model for a given type of data, that is not already given as a tensor.\n\nI am looking for something like a \"Learn to design neural networks by example\" tutorial, that explains with practical examples how to solve some problems, but does not leave out the theory and math behind it and actually explains *why* the model is designed that way and what is the magic why this kind of model works for this kind of problem.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ekyla1/is_there_a_good_tutorial_that_is_neither_too/"}, {"autor": "adversarialexamples", "date": "2020-01-06 04:02:17", "content": "Questions about the paper 'Intriguing Properties of Neural Networks' /!/ I recently read the paper ['Intriguing Properties of Neural Networks'](https://arxiv.org/pdf/1312.6199.pdf), which I believe is the first paper to discuss the idea of adversarial examples in neural networks. However, I have a few questions about the setup of adversarial examples in section 4.1.\n\n1.  Why is D(x,f(x))=f(x)? My interpretation of the definition of D(x,l) from the sentence before is that it is equal to x+r where r is the minimum magnitude vector such that f(x+r)=l. It appears that D(x,f(x))=x. Am I misunderstanding something here? \n2.  Why are we looking for the minimum such c? My intuition is that the c|r| term of the problem serves to pull r towards the 0 vector, while the loss\\_f term serves to pull r towards the \"perfect\" input -----> image !!!  representing label l, which will likely be away from the 0 vector. If this intuition is true, then increasing c should pull the minimizer r towards the 0 vector and reduce its magnitude. So I would think we would want to find the maximum c for which the minimizer r of that expression satisfies f(x+r)=l, as this would lead to a smaller perturbation that still causes an adversarial example. What is wrong with this logic?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ekohnp/questions_about_the_paper_intriguing_properties/"}, {"autor": "yugiooooh", "date": "2020-01-05 08:31:50", "content": "Can someone explain model collapse to me? /!/ Especially for -----> image !!!  models, I am having a hard time understanding what the exact instigator for collapse is, and if there is some sort of threshold. \n\nI really don't know much about it, nor have I ever trained anything long enough to see it happen, but I am curious.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ekaeua/can_someone_explain_model_collapse_to_me/"}, {"autor": "johnnymo1", "date": "2020-01-05 04:21:06", "content": "Reproducing AI Portraits /!/ Back in July or so, the IBM Watson put up a website that let you upload a face -----> photo !!!  and it would generate a \"painted\" portrait, in varying styles with a face matching the -----> photo !!! . The site has been down for a while now, but the results really impressed me.\n\nMy question is: have any technical details of how this was implemented been released? When I google for it, all I get are news and blog articles about the site. I've been wanting to play around with writing a GAN for a while, and since the website is down now, I thought it might be fun to try to reproduce it. Since I am not the IBM Watson team, I'm sure the results would not be nearly as impressive, but all the same...\n\nUGATIT looks like it functions about how I would naively expect it to work, but the results in the paper don't look quite as impressive, and if those results aren't as impressive, then I'm sure my attempt to implement something naively would just be me flailing around at random. Does anyone know what AI Portraits did, or know of any implementations with similar results?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ek7xtp/reproducing_ai_portraits/"}, {"autor": "teb311", "date": "2020-01-03 19:55:02", "content": "Help: GAN Mode Collapse or Just Expected Behavior? /!/ Hey all,\n\nI just built a GAN from scratch largely following this wonderful tutorial: [https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/](https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/) as well as this advice from the same author: [https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/](https://machinelearningmastery.com/how-to-code-generative-adversarial-network-hacks/)\n\nMy GAN is working well enough for my purposes, but something that I thought was strange happened during training and I was hoping if someone could help me understand it. For the first roughly 120k samples it looked like mode collapse: the discriminator achieved 100% accuracy on both fake and real images, and the generator (using inverted labels) ALSO reported 100% accuracy. \n\nThat seems impossible to me \u2014 how could the discriminator achieve 100% accuracy on generated images with and *without* the labels inverted? If it's accurately classifying those images as \"fake\" 100% of the time, how could it ALSO classify them as \"real\" 100% of the time while I'm training the generator. (It's possible my reporting code is wrong... but I don't think it is!)\n\nFurthermore, I generated -----> image !!!  samples during training also made me think mode collapse. At first of course they were just noise:\n\n[0 samples](https://preview.redd.it/8o0w9fk89m841.png?width=361&amp;format=png&amp;auto=webp&amp;s=4b2ff38363938c75b57ce5f4f2449ae9211f6b50)\n\nAfter \\~50k samples though they were starting to look kind of like uniform grids:\n\n[50k samples](https://preview.redd.it/nhbh9xrb9m841.png?width=361&amp;format=png&amp;auto=webp&amp;s=33738f53332fdb06fa424697ee2ee71c9f933afc)\n\nAt the time of the second image during training, the accuracy for the discriminator on real and fake images was 100% AND the generator (with inverted labels) was still also 100%. But, incredibly, eventually the model broke out of that pattern of 100%'s around 150k samples:\n\n[150k samples](https://preview.redd.it/vyudmqaw9m841.png?width=356&amp;format=png&amp;auto=webp&amp;s=b87f6ccb9c15eced56bc2b3ffcb83ce706001bf0)\n\nAt that point accuracy for real/fake discriminator and the generator started to diverge with better, more expected values: Close to 50% / 50% for discriminator real / fake, and 35-45% for the generator (with inverted labels). Also, fakes resembling digits started to emerge:\n\n[250k samples](https://preview.redd.it/t92j5coxam841.png?width=340&amp;format=png&amp;auto=webp&amp;s=7fd8ffe5ccaebb9b4c52eeaa1ee3cbbc1339f31d)\n\nWhat I don't understand is how the system could have \"escaped\" from what looked like mode collapse. If the discriminator and generator were both always getting 100% accuracy and very low loss values, what would have pushed the system out of that loop? Is a period of really good discriminator performance expected at the start of training? Are there any well known ways to \"jump start\" the generator's performance, and get to the desired 50 / 50 ratio earlier?\n\nThank you for any advice you have to offer!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ejkq1w/help_gan_mode_collapse_or_just_expected_behavior/"}, {"autor": "aditya_mangalampalli", "date": "2020-01-03 09:18:56", "content": "Help with Dataset Mainpulation /!/ Hey all,\n\nThis may be a noob question for some but I'm kinda stuck right now on a project that I was working on. So I have a dataset of about 4000 fundus images. However, they are fully colored and I need to convert that dataset of 400 images from fully colors fundus images to images with the retinal vessel segmentation. An example of what I talk about can be seen here: [https://github.com/orobix/retina-unet](https://github.com/orobix/retina-unet). Basically the -----> image !!!  which shows the grayscale eyes along with their segmentation on the bottom. I need to convert a large number of images around 4000 to those images with veins. I was thinking about running the network which that link described in order to grab the retinal vessels from each image. What do you suggest I do to process such a large number of images like that?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ejd280/help_with_dataset_mainpulation/"}, {"autor": "KoronaSenpai", "date": "2020-04-07 00:36:15", "content": "Where can I learn how TensorFlow does -----> image !!!  classification? /!/ [deleted]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fwakpn/where_can_i_learn_how_tensorflow_does_image/"}, {"autor": "sickleRunner", "date": "2020-04-06 18:31:37", "content": "Keras OCR with CTC loss implementation error /!/ Hello, don't know if it's the righ place to post a question, but I am a bit desperate to solve it.\n\nSo, using [this](https://github.com/DeepSystems/supervisely-tutorials/blob/master/anpr_ocr/src/image_ocr.ipynb) example i wrote this code.\n\n    import tensorflow as tf\n    import numpy as np\n    import time\n    import cv2\n    import os\n    import random\n    from tensorflow.keras import backend as K\n    from tensorflow.keras.layers import Conv2D, MaxPooling2D, MaxPooling3D, ZeroPadding2D\n    from tensorflow.keras.layers import Input, Dense, Activation, Dropout, Reshape\n    from tensorflow.keras.layers import Reshape, Lambda, BatchNormalization\n    from tensorflow.keras.layers import add, concatenate\n    from tensorflow.keras.models import Model\n    from tensorflow.keras.optimizers import SGD\n    from os.path import join\n    import random\n    import itertools\n    import re\n    import datetime\n    import cairocffi as cairo\n    import editdistance\n    import numpy as np\n    from scipy import ndimage\n    import pylab\n    import matplotlib.pyplot as plt\n    import matplotlib.gridspec as gridspec\n    from tensorflow.keras import backend as K\n    from tensorflow.keras.layers import Conv2D, MaxPooling2D\n    from tensorflow.keras.layers import Input, Dense, Activation\n    from tensorflow.keras.layers import Reshape, Lambda\n    from tensorflow.keras.layers import add, concatenate\n    from tensorflow.keras.models import Model, load_model\n    from tensorflow.keras.layers import GRU\n    from tensorflow.keras.optimizers import SGD\n    from tensorflow.keras.utils import get_file\n    from tensorflow.keras.preprocessing import -----> image !!! \n    import tensorflow.keras.callbacks\n    import pandas as pd\n    \n    data_dir = '/home/vadim/Desktop/plate_and_nr_dataset/'\n    data = pd.read_csv('/home/vadim/Desktop/plate_and_nr_dataset/dataset.csv')\n    \n    img_links = list(data.iloc[:, 0].values)\n    plates = list(data.iloc[:, 1].values)\n    \n    max_plate_len = max(list(map(lambda x: len(x), plates)))\n    \n    alphabet = []\n    for plate in plates:\n        for c in plate:\n            alphabet.append(c)\n    \n    alphabet = list(set(alphabet))\n    alphabet.sort()\n    \n    sess = tf.Session()\n    K.set_session(sess)\n    \n    \n    def labels_to_text(labels):\n        return ''.join(list(map(lambda x: alphabet[int(x)], labels)))\n    \n    \n    def text_to_labels(text):\n        return list(map(lambda x: alphabet.index(x), text))\n    \n    \n    class TextImageGenerator:\n        def __init__(self, img_w, img_h,\n                     batch_size, downsample_factor):\n            self.samples = []\n            self.downsample_factor = downsample_factor\n            self.img_h = img_h\n            self.img_w = img_w\n            self.batch_size = batch_size\n            self.samples_len = len(plates)\n            self.indexes = list(range(self.samples_len))\n            self.cur_index = 0\n    \n        def build_data(self):\n            self.imgs = np.zeros((self.samples_len, self.img_h, self.img_w))\n            for i, l in enumerate(img_links):\n                img = cv2.imread(data_dir + l)\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n                img = cv2.resize(img, (self.img_w, self.img_h))\n                img = img.astype(np.float32)\n                img /= 255\n                # width and height are backwards from typical Keras convention\n                # because width is the time dimension when it gets fed into the RNN\n                self.imgs[i, :, :] = img\n    \n        def get_output_size(self):\n            return len(plates) + 1\n    \n        def next_sample(self):\n            self.cur_index += 1\n            if self.cur_index &gt;= self.samples_len:\n                self.cur_index = 0\n                random.shuffle(self.indexes)\n            return self.imgs[self.indexes[self.cur_index]], plates[self.indexes[self.cur_index]]\n    \n        def next_batch(self):\n            # width and height are backwards from typical Keras convention\n            # because width is the time dimension when it gets fed into the RNN\n            while True:\n                if K.image_data_format() == 'channel_first':\n                    X_data = np.ones([self.batch_size, 1, self.img_w, self.img_h])\n                else:\n                    X_data = np.ones([self.batch_size, self.img_w, self.img_h, 1])\n                Y_data = np.ones([self.batch_size, max_plate_len])\n    \n                input_length = np.ones((self.batch_size, 1)) * (self.img_w // self.downsample_factor - 2)\n                label_length = np.zeros((self.batch_size, 1))\n    \n                for i in range(self.batch_size):\n                    img, text = self.next_sample()\n                    img = img.T\n                    if K.image_data_format() == 'channel_first':\n                        img = np.expand_dims(img, 0)\n                    else:\n                        img = np.expand_dims(img, -1)\n                    X_data[i] = img\n                    Y_data[i] = text_to_labels(text) + [0] * (max_plate_len - len(text))\n                    label_length[i] = len(text)\n    \n                    inputs = {\n                        'the_input': X_data,\n                        'the_labels': Y_data,\n                        'input_length': input_length,\n                        'label_length': label_length\n                    }\n                    outputs = {'ctc': np.zeros([self.batch_size])}\n                    yield (inputs, outputs)\n    \n    \n    def small_basic_block(nr_filters):\n        def func(inp):\n            xx = Conv2D(filters=nr_filters // 4, kernel_size=1, strides=1, activation='relu')(inp)\n            xx = BatchNormalization()(xx)\n            xx = ZeroPadding2D(padding=(1, 0))(xx)\n            xx = Conv2D(filters=nr_filters // 4, kernel_size=(3, 1), strides=1, activation='relu')(xx)\n            xx = BatchNormalization()(xx)\n            xx = ZeroPadding2D(padding=(0, 1))(xx)\n            xx = Conv2D(filters=nr_filters // 4, kernel_size=(1, 3), strides=1, activation='relu')(xx)\n            xx = BatchNormalization()(xx)\n            xx = Conv2D(filters=nr_filters, kernel_size=1, strides=1, activation='relu')(xx)\n            return BatchNormalization()(x)\n    \n        return func\n    \n    \n    # LPRNet\n    class_number = len(alphabet)\n    img_width = 94  # cols\n    img_height = 24  # rows\n    inp = Input(shape=(img_height, img_width, 3))  # data_format is channels_last\n    x = Conv2D(filters=64, kernel_size=(3, 3), strides=1, activation='relu')(inp)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D(pool_size=(3, 3), strides=(1, 1))(x)\n    x = small_basic_block(nr_filters=128)(x)\n    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 1))(x)\n    x = small_basic_block(nr_filters=256)(x)\n    x = small_basic_block(nr_filters=256)(x)\n    \n    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 1))(x)\n    x = Dropout(rate=0.5)(x)\n    x = Conv2D(filters=256, kernel_size=(4, 1), strides=1)(x)\n    x = Dropout(rate=0.5)(x)\n    x = Conv2D(filters=class_number, kernel_size=(1, 13), strides=1)(x)\n    y_pred = BatchNormalization()(x)\n    y_pred = Reshape((y_pred.shape[2], y_pred.shape[3]))(y_pred)\n    \n    \n    def ctc_lambda_func(args):\n        y_pred, labels, input_length, label_length = args\n        # the 2 is critical here since the first couple outputs of the RNN\n        # tend to be garbage:\n        y_pred = y_pred[:, 2:, :]\n        return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n    \n    \n    labels = Input(name='the_labels', shape=[max_plate_len], dtype='float32')\n    input_len = Input(name='input_length', shape=[1], dtype='int64')\n    label_len = Input(name='label_length', shape=[1], dtype='int64')\n    \n    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_len, label_len])\n    sgd = SGD(learning_rate=0.02, decay=1e-6, momentum=0.9, nesterov=True)\n    \n    model = Model(inputs=[inp, labels, input_len, label_len], outputs=loss_out)\n    # the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n    \n    train_gen = TextImageGenerator(94, 24, 30, 4)\n    train_gen.build_data()\n    valid_gen = TextImageGenerator(94, 24, 30, 4)\n    valid_gen.build_data()\n    \n    test_func = K.function([inp], [y_pred])\n    model.fit_generator(generator=train_gen.next_batch(),\n                        steps_per_epoch=train_gen.samples_len,\n                        epochs=1,\n                        validation_data=valid_gen.next_batch(),\n                        validation_steps=valid_gen.samples_len)\n    \n\ni got the following error\n\n    Traceback (most recent call last):\n      File \"/home/vadim/anaconda3/envs/tensorflow_v_1/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 315, in standardize_input_data\n        for x in names\n      File \"/home/vadim/anaconda3/envs/tensorflow_v_1/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 315, in &lt;listcomp&gt;\n        for x in names\n    KeyError: 'input_1'\n    \n    During handling of the above exception, another exception occurred:\n    \n    Traceback (most recent call last):\n      File \"/home/vadim/PycharmProjects/keras_ocr/main.py\", line 218, in &lt;module&gt;\n        validation_steps=valid_gen.samples_len)\n      File \"/home/vadim/anaconda3/envs/tensorflow_v_1/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1433, in fit_generator\n        steps_name='steps_per_epoch')\n      File \"/home/vadim/anaconda3/envs/tensorflow_v_1/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\", line 264, in model_iteration\n        batch_outs = batch_function(*batch_data)\n      File \"/home/vadim/anaconda3/envs/tensorflow_v_1/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1153, in train_on_batch\n        extract_tensors_from_dataset=True)\n      File \"/home/vadim/anaconda3/envs/tensorflow_v_1/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 2651, in _standardize_user_data\n        exception_prefix='input')\n      File \"/home/vadim/anaconda3/envs/tensorflow_v_1/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 319, in standardize_input_data\n        'for each key in: ' + str(names))\n    ValueError: No data provided for \"input_1\". Need data for each key in: ['input_1', 'the_labels', 'input_length', 'label_length']\n\npython 3.7\n\ntf 1.14.0\n\nI have no idea what's wrong. If someone wants the dataset to launch it locally i can provide it. I tried to debug it in PyCharm but couldn't find even the call stack )", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fw44tg/keras_ocr_with_ctc_loss_implementation_error/"}, {"autor": "xartaetos", "date": "2020-04-06 13:40:17", "content": "Regression network with small dataset (Keras)? /!/ I will preface my question by saying that I have minimal experience in NN and this is the first concrete project I'm doing that is not a tutorial example. So apologies for the potentially very basic questions.\n\nMy goal is to build a simple NN that can predict the value of a parameter from an -----> image !!! . The parameter in question is normally computed analytically through a function that analyzes the image luminance distribution and is then used in an image processing algorithm that enhances the image. So the value I'm regressing on is strongly correlated with the average image luminance, which I imagine is something a network should be able to predict relatively easily. For the most part the analytical function works well, except at times it requires manual adjustment to get optimal results (in an aesthetic sense). I am starting simple though as I'm trying to learn what works and how, so all I'm trying to do is replicate what the analytical function does, but through a NN of some sort. So image in -&gt; continuous value out. \n\nMy current solution is based on the network architecture described in this tutorial [https://www.pyimagesearch.com/2019/01/28/keras-regression-and-cnns/](https://www.pyimagesearch.com/2019/01/28/keras-regression-and-cnns/) which seems relatively similar as a problem. I have about 150 image and parameter value pairs at the moment, which is I imagine not enough. The parameter values normally range between 1 and 1.5 but I have normalized them to be between 0 to 1. I'm using the Adam optimizer, with MSE as the loss, which seems to fall to around 0.1 after 30-40 epochs, but at that point the predictions seem to be all the same or nearly the same value (assuming the batch mean?). \n\nHow do I go about solving this? Any guidance would be much appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fvyx8x/regression_network_with_small_dataset_keras/"}, {"autor": "emrectn", "date": "2020-04-06 11:07:16", "content": "Neural Style Transfer - Can artificial intelligence draw pictures like Van Gogh? /!/ &amp;#x200B;\n\nhttps://i.redd.it/s6324yvtj6r41.gif\n\nHello my friend. If you are interested in deep learning, check out the link to learn about Neural Style Transfer and try it out. Artificial intelligence drew my dog's -----> picture !!!  like Van Gogh. You can see the change epoch by epoch. For more, visit my kernel. If you like it please upvote the kernel.\n\nIt is my first kernel and I am waiting for your suggestions and criticisms to develop better.\n\n [https://www.kaggle.com/emrecetin/neural-style-transfer](https://www.kaggle.com/emrecetin/neural-style-transfer)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fvwuoo/neural_style_transfer_can_artificial_intelligence/"}, {"autor": "PsydeliX_", "date": "2020-04-06 10:34:55", "content": "Help with my DL architecture that uses 5 ResNet50 models in parallel /!/ I'm working on a DL research project where I am classifying a -----> photo !!!  with people in it into 10 categories based on what kind of -----> photo !!!  it is (a -----> photo !!!  of friends, family (couple), (family (with children) etc.).\n\nI was suggested an architecture where i crop the body parts of the people in the photo and give each body part cropped photo to a ResNet50 model, concatenate the result, and put two FC layers with ReLU and Softmax activations respectively.\n\nI prepared a dataset of 400 training images and 50 validation images per class, i.e 4000 images for training and 500 validation images.\n\n \n\nBody Part 1 Region (150px, 150px, 3) -&gt; ResNet50 -&gt; Conv (1x1, 512 filters) -&gt;\n\nBody Part 2 Region (150px, 150px, 3) -&gt; ResNet50 -&gt; Conv (1x1, 512 filters) -&gt; \n\n... till Body Part 5\n\n&amp;#x200B;\n\nConcatenate the above ResNet outputs and connect to a FC layer with 512 units and finally a FC layer with 10 units.\n\n&amp;#x200B;\n\nI went with the defaults in Keras, chose Adam as the optimizer and categorical crossentropy as the loss function with 128 batch size.\n\nThe training accuracy shoots up to 90% after \\~3rd epoch but validation accuracy plateaus at 64%. In another 1-2 epochs the training accuracy reaches 96%+ but validation accuracy remains around 60%; the model is overfitting.\n\n&amp;#x200B;\n\nWhat I've tried so far,\n\nI tried adding Dropout after the 512 FC layer and the layer just before the concatenation step but the validation accuracy suffers.\n\nTried changing the optimizer to RMSProp and AdaGrad but again validation acc drops a few percents.\n\nTried adding L2 regularization to the Conv layers after the ResNet and the 512 layer FC but again same thing happens.\n\nWhatever I do the training accuracy does not fall one bit, so can anyone give some advice on what I can do next?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fvwgvn/help_with_my_dl_architecture_that_uses_5_resnet50/"}, {"autor": "KoronaSenpai", "date": "2020-04-06 10:03:53", "content": "TableBank: Benchmark for -----> Image !!! -based Table Detection and Recognition", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fvw3n8/tablebank_benchmark_for_imagebased_table/"}, {"autor": "glass_decay", "date": "2020-04-05 01:49:28", "content": "Scheduling jobs with MPI Operator. /!/ Wondering if anyone has any experience with scheduling a MPIJob to run on a cron schedule?\n\nHere's the job I'm working with.\n\n    apiVersion: kubeflow.org/v1alpha2\n    kind: MPIJob\n    metadata:\n      name: tf-topic-model\n    spec:\n      slotsPerWorker: 1\n      cleanPodPolicy: Running\n      mpiReplicaSpecs:\n        Launcher:\n          replicas: 1\n          template:\n            spec:\n              containers:\n                - -----> image !!! : gcr.io/...\n                  name: tf-topic-model\n                  command:\n                    - mpirun\n                    - --allow-run-as-root\n                    - -np\n                    - \"2\"\n                    - -bind-to\n                    - none\n                    - -map-by\n                    - slot\n                    - -x\n                    - NCCL_DEBUG=INFO\n                    - -x\n                    - LD_LIBRARY_PATH\n                    - -x\n                    - PATH\n                    - -mca\n                    - pml\n                    - ob1\n                    - -mca\n                    - btl\n                    - ^openib\n                    - python\n                    - /app/topic_model.py\n        Worker:\n          replicas: 2\n          template:\n            spec:\n              containers:\n                - image: gcr.io/...\n                  name: tf-topic-model\n                  resources:\n                    limits:\n                      nvidia.com/gpu: 1", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fv5q5q/scheduling_jobs_with_mpi_operator/"}, {"autor": "DeepStrategy", "date": "2020-04-04 13:13:12", "content": "Non-max suppression deciding on candidates for IOU comparison efficiently /!/ Hi fellows,\n\nI am currently trying to implement a realtime digit detector, for that I use a convolutional implementation of sliding windows. I trained on 28x28 and doing the inference on currently on those resolutions:\n\n    (1920,1080),(1792,1008),(1664,936),(1536,864),(1408,792),(1280,720),(1152,648),(1024,576),(896,504),(768,432),(640,360),(512,288),(384,216),(256,144),(128,72)\n\nI know that later I can only choose one or two resolutions, but for now I do all those resolutions and discard all results with threshold below 0.1.\n\nHere is also a summary of my network:\n\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #   \n    =================================================================\n    conv1 (Conv2D)               (None, 28, 28, 32)        832       \n    _________________________________________________________________\n    batch_normalization_1038 (Ba (None, 28, 28, 32)        128       \n    _________________________________________________________________\n    leaky_re_lu_1038 (LeakyReLU) (None, 28, 28, 32)        0         \n    _________________________________________________________________\n    max_pooling2d_496 (MaxPoolin (None, 14, 14, 32)        0         \n    _________________________________________________________________\n    conv2 (Conv2D)               (None, 12, 12, 64)        18496     \n    _________________________________________________________________\n    batch_normalization_1039 (Ba (None, 12, 12, 64)        256       \n    _________________________________________________________________\n    leaky_re_lu_1039 (LeakyReLU) (None, 12, 12, 64)        0         \n    _________________________________________________________________\n    max_pooling2d_497 (MaxPoolin (None, 6, 6, 64)          0         \n    _________________________________________________________________\n    conv3 (Conv2D)               (None, 5, 5, 128)         32896     \n    _________________________________________________________________\n    batch_normalization_1040 (Ba (None, 5, 5, 128)         512       \n    _________________________________________________________________\n    leaky_re_lu_1040 (LeakyReLU) (None, 5, 5, 128)         0         \n    _________________________________________________________________\n    max_pooling2d_498 (MaxPoolin (None, 3, 3, 128)         0         \n    _________________________________________________________________\n    conv4 (Conv2D)               (None, 1, 1, 1152)        1328256   \n    _________________________________________________________________\n    conv5 (Conv2D)               (None, 1, 1, 1152)        1328256   \n    _________________________________________________________________\n    conv6 (Conv2D)               (None, 1, 1, 10)          11530     \n    =================================================================\n    Total params: 2,721,162\n    Trainable params: 2,720,714\n    Non-trainable params: 448\n\nIt is a bit \"bottom heavy\" at the moment, I will also fix that later.\n\nBelow you can see my cuurent result, there are some flaws with the actual size of the boxes, they are a bit to small, will fix that later.\n\n[Result -----> image !!!  of all resolutions](https://preview.redd.it/aie99a79wsq41.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=afbe0aaa3c54ee78dcd6168cf377276bbc84e034)\n\n**Now I want to do some kind of non-max suppression and I already implemented IOU for two boxes, but I do not want to do it with all boxes. I only want check those who have the same class and are somewhat close to each other. Is there a low computational trick someone can share or at least give me a hint in the right direction?**\n\nThanks for your time and efforts.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/futi3n/nonmax_suppression_deciding_on_candidates_for_iou/"}, {"autor": "DeepStrategy", "date": "2020-04-04 13:07:24", "content": "Non-max supression decide IOU candidates for comparison in -----> image !!! ? /!/ Hi fellows,\n\nI am currently trying to implement a realtime digit detector, for that I use a convolutional implementation of sliding windows. I trained on 28x28 and doing the inference on currently on those resolutions:\n\n    (1920,1080),(1792,1008),(1664,936),(1536,864),(1408,792),(1280,720),(1152,648),(1024,576),(896,504),(768,432),(640,360),(512,288),(384,216),(256,144),(128,72)\n\nI know that later I can only choose one or two resolutions, but for now I do all those resolutions and discard all results with threshold below 0.1.\n\nHere is also a summary of my network:\n\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #   \n    =================================================================\n    conv1 (Conv2D)               (None, 28, 28, 32)        832       \n    _________________________________________________________________\n    batch_normalization_1038 (Ba (None, 28, 28, 32)        128       \n    _________________________________________________________________\n    leaky_re_lu_1038 (LeakyReLU) (None, 28, 28, 32)        0         \n    _________________________________________________________________\n    max_pooling2d_496 (MaxPoolin (None, 14, 14, 32)        0         \n    _________________________________________________________________\n    conv2 (Conv2D)               (None, 12, 12, 64)        18496     \n    _________________________________________________________________\n    batch_normalization_1039 (Ba (None, 12, 12, 64)        256       \n    _________________________________________________________________\n    leaky_re_lu_1039 (LeakyReLU) (None, 12, 12, 64)        0         \n    _________________________________________________________________\n    max_pooling2d_497 (MaxPoolin (None, 6, 6, 64)          0         \n    _________________________________________________________________\n    conv3 (Conv2D)               (None, 5, 5, 128)         32896     \n    _________________________________________________________________\n    batch_normalization_1040 (Ba (None, 5, 5, 128)         512       \n    _________________________________________________________________\n    leaky_re_lu_1040 (LeakyReLU) (None, 5, 5, 128)         0         \n    _________________________________________________________________\n    max_pooling2d_498 (MaxPoolin (None, 3, 3, 128)         0         \n    _________________________________________________________________\n    conv4 (Conv2D)               (None, 1, 1, 1152)        1328256   \n    _________________________________________________________________\n    conv5 (Conv2D)               (None, 1, 1, 1152)        1328256   \n    _________________________________________________________________\n    conv6 (Conv2D)               (None, 1, 1, 10)          11530     \n    =================================================================\n    Total params: 2,721,162\n    Trainable params: 2,720,714\n    Non-trainable params: 448\n\nIt is a bit bottom heavy at the moment, I will also fix that later.\n\nBelow you can see my cuurent result, there are some flaws with the actual size of the boxes, they are a bit to small, will fix that later.\n\n[Result of all the different resolutions, bounding boxes printed to result image ](https://preview.redd.it/o9jg8d6itsq41.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=f61f18a0120e4656bf4e08e8ce499680c36b49ee)\n\n**Now I want to do some kind of non-max supression and I already implemented IOU for two boxes, but I do not want to do it with all boxes. I only want check those who have the same class and are somewhat close to each other. Is there a low computational trick someone can share or at least give me a hint in the right direction?**\n\nThanks for your time and efforts.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/futf44/nonmax_supression_decide_iou_candidates_for/"}, {"autor": "H3t0N", "date": "2020-04-13 17:01:35", "content": "Help Needed! Processing Documents represented as 2D-Chargrids. /!/ Heay community,  \n\n\nim in a project to extract information out of delivery notes. I found following paper:\n\n[https://arxiv.org/pdf/1809.08799v1.pdf](https://arxiv.org/pdf/1809.08799v1.pdf)\n\n&amp;#x200B;\n\nI found that the chargrid representation is a good way to keep structural information on the document. \n\nThe first part, to rebuild the chargrid is already done. Now im close to the neural network part but im hung on the one hot encoding part before that. \n\n&amp;#x200B;\n\nI dont know if i misunderstood the paper or not. \n\n&amp;#x200B;\n\nIn specific:\n\nIn 3.1 Chargrid there is mentioned that before the chargrid representation is used as input, they applied 1 hot encoding to the chargrid. And now im a little bit out of tracks. \n\n&amp;#x200B;\n\nSo the encoding is there to represent the chargrid as a vector. Is it meant that i feed the generated chargrid -----> image !!!  to the network or just the one hot encoded information.\n\n&amp;#x200B;\n\nMaybe you guys can help me out a little bit. \n\nThanks in advance", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g0mxq2/help_needed_processing_documents_represented_as/"}, {"autor": "[deleted]", "date": "2020-04-13 12:59:30", "content": "Is it possible to predict -----> image !!! s using -----> image !!!  array? /!/ [deleted]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g0ih0u/is_it_possible_to_predict_images_using_image_array/"}, {"autor": "cmonc1", "date": "2020-04-13 10:54:44", "content": "[Research] -----> Image !!!  observation space and continuous action space /!/ Hello Reddit,\n\nI  have made a OpenAI gym env that returns a depth-map as the observation  space and uses 3 continuous values in the action space (for joint  positions). The env is going to be used to train a 2 finger gripper to  pickup objects.\n\nIs it necessary for me to discretise the action space? I've been trying to use [garage](https://github.com/rlworkgroup/garage) but it seems it's not a common thing to have pixel input and a continuous action space.\n\nIf anyone has done something similar can you please give me some tips or bless me with code.\n\nSimon", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g0gqaj/research_image_observation_space_and_continuous/"}, {"autor": "ajinkyajawale", "date": "2020-04-12 17:14:23", "content": "Guidance required for segmenting eyes and lips from a given -----> image !!!  /!/ Hi guys! hope you are doing great. I am stuck at the project and I need your valuable help regarding deep learning and image segmentation\n\n1. My task is to segment only lips and eyes from the images in [CelebAMask-HQ dataset](https://drive.google.com/open?id=1badu11NqxGf6qM3PTTooQDJvQbejgbTv)\n\n[input and output image](https://preview.redd.it/xuvexvfp6fs41.png?width=694&amp;format=png&amp;auto=webp&amp;s=c1837813fc9b6df5d1bc5d42214a298b1eb20323)\n\nPlease help me here if anyone know how to get this output. kindly share the code. Thanks ! stay safe :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g00mdt/guidance_required_for_segmenting_eyes_and_lips/"}, {"autor": "tensorhere", "date": "2020-04-12 16:25:01", "content": "Face detection API for comparing two images /!/ I want an API that can assign face ID to a particular user's face and if that user uploads an -----> image !!!  again then I can compare the two face IDs and check whether it's the same user or not. Please suggest some APIs that can do that and if it's free then cherry on cake.\n\nP.S: I don't have any ML experience.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fzzp93/face_detection_api_for_comparing_two_images/"}, {"autor": "TrackLabs", "date": "2020-04-12 13:35:36", "content": "What is some good ML beginner project I could use to ease myself into it? /!/ Frequently asked question, I know.\n\nBut I am looking for some project that I could use to ease myself into ML. I am currently learning the base math of it, but I want to create some actual projects at the same time too. I know Algebra, Linear Algebra, and at the moment I am beginning with Calculus.\n\n&amp;#x200B;\n\nI was thinking of some basic -----> Image !!!  classificator, but thats something everyone does...what are some good (and maybe not too complex) projects to create as ML beginner? It can be image classification, but other ideas would be cool too. If someone could also provide a tutorial on YT about that project that would be even more helpfull.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fzwvcz/what_is_some_good_ml_beginner_project_i_could_use/"}, {"autor": "R0llingOnwards", "date": "2020-04-12 11:10:11", "content": "New to machine learning and need to work with sensor data - could I get some guidance on how to approach this? /!/ For my background: I'm an electronics engineer. I've been learning python and machine learning for a few weeks now to help with a project.\n\nIt's kinda hard for me to go into specifics given the project, but basically I've got data coming in from a sensor, this is saved in a csv file and I've made line graphs to visualise this data. What I need to do is look at these line graphs or the data in the csv file and have model that can learn when there is an object being detected (more than one type of sensor is being used but I'm just focusing on one for now). This could be due to a signal reading changing for a certain period of time and different amplitudes or noise could give indication of the type of object.\n\nI've done tutorials on logistic regression, knn models and CNNs. I've been asked to look into CNN but it seems specific to -----> image !!!  or audio data analysis. The former isn't relevant but I thought going through tutorials on analysing audios might provide useful, but I'd like to know if there's another model or something I should look into. Any suggestions? Any resources I could look into relevant to my scenario? Would really appreciate some pointers if what I've provided makes sense. I'm really stuck which way to go.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fzuwpz/new_to_machine_learning_and_need_to_work_with/"}, {"autor": "martian_rover", "date": "2020-11-20 20:00:14", "content": "[Question] cross_val_score is returning nan list of scores in scikit learn /!/ I am trying to handle imbalanced `multi label dataset` using `cross validation` but scikit learn `cross_val_score` is returning `nan list of values` on running classifier.\n\nHere is the code:\n\n    import pandas as pd\n    import numpy as np\n    data = pd.DataFrame.from_dict(dict, orient = 'index') # save the given data below in dict variable to run this line\n    \n    from sklearn.model_selection import train_test_split\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.preprocessing import MultiLabelBinarizer\n    from sklearn.multiclass import OneVsRestClassifier\n    \n    multilabel = MultiLabelBinarizer()\n    y = multilabel.fit_transform(data['Tags']) \n    from nltk.corpus import stopwords \n    stop_words = set(stopwords.words('english')) \n    tfidf = TfidfVectorizer(stop_words = stop_words,max_features= 40000, ngram_range = (1,3))\n    X = tfidf.fit_transform(data['cleaned_title'])\n    \n    from skmultilearn.model_selection import IterativeStratification\n    k_fold = IterativeStratification(n_splits=10, order=1)\n    \n    \n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import jaccard_score\n    class_weight = {0:1,1:10}\n    lr = LogisticRegression(class_weight = class_weight, n_jobs = -1)\n    scores = cross_val_score(lr, X, y, cv=k_fold, scoring = 'f1_micro')\n    scores\n\nHere is the data (first 10 rows) using `data.head(10).to_dict()`\n\n    {0: {'Tags': ['python', 'list', 'loops', 'for-loop', 'indexing'],\n      'cleaned_title': 'for loop we use any local variable   what if we use any number present in a list  ',\n      'cleaned_text_of_ques': 'in the for   loop we use any local variable   what if we use any number in a list   what will be the output   a   [ 1 2 3 4 5 6 ] b   [ ] for a[ 1 ] in a   b append a[ 1 ]   print b  '},\n     1: {'Tags': ['python', 'loops', 'tkinter', 'algorithm-animation'],\n      'cleaned_title': 'contain a mainloop [ duplicate ]',\n      'cleaned_text_of_ques': 'my code be a bubble sort that i be try to visualise   but i be struggle to find a way to make a block of code only be use once   i also think that if i could only mainloop a section that would'},\n     2: {'Tags': ['android',\n       'android-lifecycle',\n       'activity-lifecycle',\n       'onsaveinstancestate'],\n      'cleaned_title': 'when onrestoreinstancestate be not call  ',\n      'cleaned_text_of_ques': 'docs describe when onrestoreinstancestate be call   this method be call after onstart     when the activity be be re   initialize from a previously save state   give here in savedinstancestate  '},\n     3: {'Tags': ['python', 'r', 'bash', 'conda', 'spyder'],\n      'cleaned_title': 'point conda r to already   instal version of r',\n      'cleaned_text_of_ques': 'my problem have to do with the fact that rstudio and conda be point to different version of r  my r and rstudio be instal independent of anaconda   and everything be work great   in my    '},\n     4: {'Tags': ['android',\n       'firebase',\n       'firebase-realtime-database',\n       'android-recyclerview'],\n      'cleaned_title': 'how to use a recycleview with several different layout   accord to the datum collect in firebase   [ close ]',\n      'cleaned_text_of_ques': 'i have a problem   there be day that i do research and test code   but nothing work   my application will have a window where i will post datum take in firebase   use a recycleview   with the'},\n     5: {'Tags': ['html', 'css', 'layout'],\n      'cleaned_title': 'how to create side by side layout of an -----> image !!!  and label  ',\n      'cleaned_text_of_ques': 'i have be try for a while now and can not seem to achive the bellow design    exploreitem   background   color     353258       rgba 31   31   31   1       border   1px solid   4152f1   color  '},\n     6: {'Tags': ['php', 'jquery', 'file'],\n      'cleaned_title': 'php jquery ajax   _ files[ file   ] undefined index error',\n      'cleaned_text_of_ques': 'i have a form that upload image file and it be not work   i have try submit and click event   the error appear when i have remove the if statement   thank in advance for your help  '},\n     7: {'Tags': ['python', 'pandas', 'dataframe'],\n      'cleaned_title': 'how to update value in pandas dataframe in a for loop  ',\n      'cleaned_text_of_ques': 'i be try to make a data frame that can store variable coeff value after each iteration   i be able to plot the graph after each iteration   but when i try to insert the value in the data frame'},\n     8: {'Tags': ['xpath', 'web-scraping', 'scrapy'],\n      'cleaned_title': 'scrapy   how can i handle a random number of element  ',\n      'cleaned_text_of_ques': 'i have a scrapy crawler that i can comfortably acquire the first desire paragraph   but sometimes there be a second or third paragraph   response xpath f string   h2[contains text           card   ] '},\n     9: {'Tags': ['bootstrap-4', 'tabs', 'collapse'],\n      'cleaned_title': 'collapse three column with bootstrap',\n      'cleaned_text_of_ques': 'i be try to make three tab with cross   reference with one tab visible at the time   i be use the bootstrap v4 collapse scheme with functionality support by jquery   here be the example   https  '}}\n\nThis is how I get the `cross_val_score` in `scores` variable \n\n`array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])`.\n\nIt should have each value in range `0-1`. However, this is happening with `every algorithm model`.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jxw9y1/question_cross_val_score_is_returning_nan_list_of/"}, {"autor": "Immediate-Magician23", "date": "2020-11-20 14:51:43", "content": "Algorithms for document clarification, -----> image !!!  binarization and where to start studying them? /!/ Not so long ago, my friend and I were entrusted with a program research project on ways to clarify documents.\n\nTwo main paths were identified - classical algorithms and neural networks, and a program was selected to experiment- Tesseract-OCR. Actually, the question is: where to start? Where can I find examples of algorithms and neural networks (as well as where can I find out about their features and implementation)? Where to start studying neural networks and algorithms that recognize images?\n\nAny material related to the topic is welcome.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jxqdd6/algorithms_for_document_clarification_image/"}, {"autor": "tobzulu", "date": "2020-11-20 13:11:31", "content": "Get two parameters from images /!/ I thought about using machine learning to achive the following task:\n\n&amp;#x200B;\n\nTake an -----> image !!!  of a rectangle and get the lenght of the sides a and b. \n\nLets assume the sides a,b of the rectangles can take values from 1 to 10. The orientation is always the same\n\nFor the supervised learning I have 100 images with different values for a and b. the images are labeled with those parameters. \n\nWhere do I start to make a Neuronal Network that I can feed other images of the rectangle that \"predicts\" the values for a and b? Any help would be welcome!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jxorcp/get_two_parameters_from_images/"}, {"autor": "Trash_Truck", "date": "2020-11-19 22:00:37", "content": "Visualizing a simple neural network to conceptually grasp it. /!/  Hi thanks for considering helping me.\n\nSo  say I define this simple neural net in PyTorch for a binary classification task, where the input is a tensor of length 27, of floats:\n\n    def __init__(self, input_size=27):     \n        super(MyNNModel, self).__init__()     \n        self.input_size = input_size     \n        self.hidden_size = input_size*2     \n        self.output_size = 1      \n    \n        self.fc1 = nn.Linear(self.input_size, self.hidden_size)     \n        self.fc2 = nn.Linear(int(self.hidden_size), self.output_size) \n\nWould a visualization of the network resemble this description, (A):\n\n* One input neuron that accepts a length 27 (input size) tensor\n* 54 neurons connected to the one input\n* One output neuron connected to the each of the 54 hidden layer neurons\n\nOr (B), would it:\n\n* have 27 input neurons\n* Each input neurons fully connected to the 54 hidden layer neurons\n* Each hidden layer neuron connected to the one output neuron\n\nCast any objections to the networks effectiveness aside for the moment and humor me I suppose.\n\nBy neuron I mean a circle in this style -----> image !!! .\n\nI  know a Linear Layer takes multiplies the input data by the transpose  weight matrix and adds the bias. So does the Linear Layer represent one  node taking in a tensor and applying its transformation to the total  tensor. Or, does it represent many nodes (amount being the input size)  fo9r each data point, that is multiplied by its respective piece of the  transposed matrix, adding the respective bias value? The images are  kinda screwing with my head at this point.\n\n&amp;#x200B;\n\n[Image from: https:\\/\\/natureofcode.com\\/book\\/chapter-10-neural-networks\\/](https://preview.redd.it/51rfw8a2r9061.png?width=862&amp;format=png&amp;auto=webp&amp;s=deca6e95683d86a0a567618d01a07d3b789ff9fa)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jxcb5v/visualizing_a_simple_neural_network_to/"}, {"autor": "qtpoison", "date": "2020-11-19 20:11:59", "content": "Why there are so few websites which actually use neural networks ? /!/ I was looking for machine learning examples online, like -----> image !!!  classification where we can upload an -----> image !!!  and classify it or upload a DNN model and test it, but I couldn't find any website which has this feature. AFAIK big companies use DNN's on their websites but why other websites don't use them? There are so many source codes and tutorials about machine learning but so few running websites which we can test a neural network. Is it really heavy on server side?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jxa8t3/why_there_are_so_few_websites_which_actually_use/"}, {"autor": "theo_035", "date": "2020-11-19 10:21:40", "content": "Unkown Face Categorisation (Apple Photos) /!/ In the iOS Photos app there is a section called 'Persons' with all the -----> photo !!! 's where a particular person is a part of categorised per person. This is done via face recognition but I've never given apple a label to one of the persons listed in this section.\n\nMy question is:\n\nHow does apple differentiate between different unknown faces in my photos? What is this technique called? And how can I use this technique? Does someone have an online example for me?\n\nThanks in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jx0b40/unkown_face_categorisation_apple_photos/"}, {"autor": "Laurence-Lin", "date": "2020-11-19 07:22:08", "content": "I would like to generate webpage -----> image !!!  data for web page defect prediction, could anyone give me some advice? /!/ I'm working on a task to detect the defect in webpage for UX improvement. In order to fit the needs, I need to have multiple positive data, which is normal webpage; and multiple negative data, which is webpage that contains defection.\n\n&amp;#x200B;\n\nI could screenshot a webpage and add some defect by hand, but generate data manually is inefficient. I'm considering two ways to generate my dataset:\n\n&amp;#x200B;\n\n1. Create my own website, and use auto-testing tools like Selenium to automatically adjust the CSS element in order to create multiple negative sample.\n2. Use data generation model like GAN to create negative sample and positive sample.\n\n&amp;#x200B;\n\nI'm not familiar with generative models, so I'm not sure how GAN would work in my case. Could anyone give me some advice about this, or is other method recommended?\n\n&amp;#x200B;\n\nThanks a lot!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jwybjo/i_would_like_to_generate_webpage_image_data_for/"}, {"autor": "eldiablocondor", "date": "2020-01-13 15:56:56", "content": "EEG + AI -----> Image !!!  generator /!/ Hi guys (sorry for my english) I want to know if it's possible to create an EEG (electroencephalogram) interpreter for generate an image with Machine Learning. for example: I'd like to create a system that is able to generate a digital image based to my moods during sleep. I know it's a very ambitious project but I think that with the right skills it's not even that difficult. for now I know that all EEG on the market detect 8 frequencies based on brain activities such as stress and other emotions. so, hypothetically, it would be enough to program an interpreter of these 8 signals and through machine learning  create images with colors and shapes inherent with one's moods.  Do you think it can work?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eo5xt9/eeg_ai_image_generator/"}, {"autor": "smlaccount", "date": "2020-01-13 09:16:59", "content": "Generative adversarial networks in satellite -----> image !!!  datasets augmentation", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eo1tt7/generative_adversarial_networks_in_satellite/"}, {"autor": "kelkka_7", "date": "2020-01-12 18:24:07", "content": "Intermediate weights activation in pre-trained networks /!/ Hi\n\nI'm trying to visualize the activations (for a specific -----> image !!! ) and/or weights (for the network without inputs). However, because I'm using a pre-trained ResNet50 V2 from TensorFlow 2.1 tf.keras framework, I cannot see each layer when I call tf.summary. I've put the question up on SO:  [https://stackoverflow.com/questions/59705507/how-to-access-and-visualize-the-weights-in-a-pre-trained-tensorflow-2-model](https://stackoverflow.com/questions/59705507/how-to-access-and-visualize-the-weights-in-a-pre-trained-tensorflow-2-model) .\n\n&amp;#x200B;\n\nAny suggestions?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/enr49x/intermediate_weights_activation_in_pretrained/"}, {"autor": "rapture_inc", "date": "2020-01-12 17:14:18", "content": "For medical -----> image !!!  segmentation, do I need a background class? /!/ If every image is expected to always have the same number of components and I just want to localize them, should my training masks have a separate channel for background pixels? Thanks.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/enq5eo/for_medical_image_segmentation_do_i_need_a/"}, {"autor": "thunderclap82", "date": "2020-01-12 17:11:17", "content": "TensorFlow -----> Image !!!  Enhancement scripts /!/ I found a website that does a really good job blowing up images from an old DV movie I did. I want to take it to the next level and do this as an image sequence and try and enhance the entire thing. The problem is the site limits you to the number of photos per month. It would take a long time to do 108k frames. I started researching Tensorflow and machine learning scripts on gibhub so I can set it up as a batch script and do it all on my own system.\n\nThe problem is I can get as far and installation of the scripts but after that I'm stuck; none of the scripts I've found indicate how to actually use the script to import the stills and export them once converted. Is there a walkthrough anyone knows of that can help me figure out this process? I have installed Ubuntu bash in Windows 10 (not a VM) and I've also tried using Anaconda.\n\nThanks for any suggestions.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/enq3t7/tensorflow_image_enhancement_scripts/"}, {"autor": "neelay_shah", "date": "2020-01-12 12:28:03", "content": "Loss function of a simple neural network /!/ I was browsing ths Jax Github repo and found an example of an -----> image !!!  classifier from scratch. However, I found their score predictions and the loss function to be a bit unusual. \n\nCan someone explain why we are subtracting 'logsumexp' from the raw score vector 'logits' and how the subsequent loss function works with this ?\n\ndef predict(params, input):\n\n  activations = input\n\n  for w, b in params\\[:-1\\]:\n\noutput = [np.dot](https://np.dot)(activations, w) + b\n\nactivations = np.tanh(output)\n\n  final\\_w, final\\_b = params\\[-1\\]\n\n  logits = [np.dot](https://np.dot)(activations, final\\_w) + final\\_b\n\n  return logits - logsumexp(logits, axis=1, keepdims=True)\n\n&amp;#x200B;\n\ndef loss(params, batch):\n\n  inputs, targets = batch\n\n  preds = predict(params, inputs)\n\n  return -np.mean(np.sum(preds \\* targets, axis=1))", "link": "https://www.reddit.com/r/learnmachinelearning/comments/enmw5z/loss_function_of_a_simple_neural_network/"}, {"autor": "Intro24", "date": "2020-01-12 06:59:05", "content": "Is there no deep learning GUI? What's the easiest way to automate these simple tasks? /!/ I'm interested in playing with machine learning and hopefully automate some tasks while I'm at it. I'm willing to learn some code but not particularly interested in it. I just want an environment where I can define a dataset (text in a spreadsheet or photos in a folder), pick out some of the \"correct\" examples, hit the go button, and then see how well it worked. For example I want to say:\n\n* This folder is full of unlabeled animal pictures.\n\n* This other folder is full of cat pictures, which are all also in the first folder.\n\n * Then I want deep learning to do it's dark magic and figure out whatever the cat photos have in common that differentiate them from the other animals.\n\n* So then I give it another animal -----> picture !!!  and it spits out the likeliness that it is a cat or not.\n\nSorry, I don't know the terminology. Even if it doesn't work that well I can think of about a dozen things in my life that I'd like to have ranked by an AI before I manually review them.\n\nSo in the cat photo example, I could give it all pictures from my phone, pick out the known pictures of my cat, and then have it rank the rest of my pictures from \"cat\" to \"not cat\". Then I would just review my pictures until it stopped being cats and ta-da, I've found all my cat pictures.\n\n-----\n\nIs there a crucial setup step that I'm missing here? Is there not a GUI to do what I'm describing or a very simple way to do it? Will Amazon Autogluon work for what I want?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/enkc83/is_there_no_deep_learning_gui_whats_the_easiest/"}, {"autor": "NoelDaviesD", "date": "2020-01-10 20:21:21", "content": "Data extraction and classification of game state from only a game stream /!/ I want to achieve a process/pipeline where given a media video stream, like direct video output from a capture card)  a select few number of scene types would trigger the extraction of various elements from the scene.  Some of these big text elements and others being detectable objects in the form of images.\n\nIn my current pipeline, an -----> image !!!  classifier detects what the current scene is, then using an array of (6) known fixed coordinates (bounding boxes essentially) - a collection of tasks is run  for each of those sections of that particular frame. These consist of:\n    \u2022\tAn image classifier to recognize a sprite in that fixed block\n    \u2022\t3x OCR workers focused on specific segments of that block:\n        o\tGeneral text for the name of the character\n        o\tA restricted character set ([0-9]+) to pull the level of the character\n       o\tA restricted character set ([0-9]{1,3} / [0-9]{1,3}) to pull the current max health points.\n\nI\u2019ve added what is essential a sanity check / denounce function that won\u2019t run the scene-specific model recognitions unless:\n    \u2022\tThe last 2/3 frames are detected as bring the same. With a similar confidence to avoid any mis-labelling  our strange outliers.\n    \u2022\tThere is a definitive and obvious change to between this and the last successful image.\n    \u2022\tThe scenes and it\u2019s contents en is readable and recognizable\n\nI'm wondering what better solutions are applicable as my exposure to machine learning is rather fresh.\n\nThis is my most indulgent real world application of machine learning and I would appreciate any advice  or suggestions you may have in the hope this helps others.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/emwkjh/data_extraction_and_classification_of_game_state/"}, {"autor": "ml_kid", "date": "2020-01-10 17:25:56", "content": "is it possible to do multi label text classification using pytorch? /!/ I am looking for examples, articles about multi label text classification using pytorch? Is that even possible? cos I see so many multi label -----> image !!!  classification examples, but none that deal with text", "link": "https://www.reddit.com/r/learnmachinelearning/comments/emu4kn/is_it_possible_to_do_multi_label_text/"}, {"autor": "gonzaloGarde", "date": "2020-01-10 10:07:58", "content": "[Help] - Regression to output coordinates /!/ Good moorning,\n\nI am working in a project where I am building a regressor that, given an -----> image !!!  and two additional numerical inputs, tries to infer a coordinate so the output of the regressor consist on two values, X and Y. The X and Y values are in the range \\[-1,1\\]. However, when I plot the regressor predicted values vs the ground truth, I obtain the following results:\n\n![img](crldqxvc8x941 \"Fig.1 Output of the regressor. The ground truth is marked as an \\\"x\\\". The infered values as a circle.\")\n\nFrom what I see, it seems like the regressor understands the distribution (the color order in colums and rows is correct and matches the order of the ground truth) but the range of the output is between \\[-0.4, 0.3\\] for both axis. I dont scale the predicted values neither apply any other kind of postprocessing to the predicted values, and I plot them directly against the ground truth value. The model that I am using for the regressor is this:\n\n*Resnet18,\u00a0\\_\u00a0=\u00a0Classifiers.get('resnet18')*  \n*input\\_image\u00a0=\u00a0keras.layers.Input(shape=(None,None,3))*\n\n*base\\_model\u00a0=\u00a0Resnet18(input\\_tensor=input\\_image,\u00a0weights='imagenet',\u00a0include\\_top=False)*  \n*x\u00a0=\u00a0base\\_model.output*  \n*x\u00a0=\u00a0keras.layers.GlobalAveragePooling2D()(x)*  \n*size =\u00a0keras.layers.Input(shape=(1))*  \n*distance =\u00a0keras.layers.Input(shape=(1))*  \n*concat\u00a0=\u00a0keras.layers.Concatenate(name='Concatenate\\_size\\_distance\\_and\\_img')(\\[x,\u00a0size,\u00a0distance\\])*  \n*x\u00a0=\u00a0keras.layers.Dense(64,\u00a0activation='relu',\u00a0name='Middle\\_Dense\\_1')(concat)*  \n*x\u00a0=\u00a0keras.layers.Dense(64,\u00a0activation='relu',\u00a0name='Middle\\_Dense\\_2')(x)*  \n*x\u00a0=\u00a0keras.layers.Dense(32,\u00a0activation='relu',\u00a0name='Middle\\_Dense\\_3')(x)*  \n*x\\_val\u00a0=\u00a0keras.layers.Dense(32,\u00a0activation='relu',\u00a0name='Dense\\_x\\_1')(x)*  \n*x\\_val\u00a0=\u00a0keras.layers.Dense(16,\u00a0activation='relu',\u00a0name='Dense\\_x\\_2')(x\\_val)*  \n*x\\_val\u00a0=\u00a0keras.layers.Dense(16,\u00a0activation='relu',\u00a0name='Dense\\_x\\_3')(x\\_val)*  \n*regression\\_x\u00a0=\u00a0keras.layers.Dense(1,\u00a0activation='linear',\u00a0name='regression\\_layer\\_x')(x\\_val)*  \n   \n*y\\_val\u00a0=\u00a0keras.layers.Dense(32,\u00a0activation='relu',\u00a0name='Dense\\_y\\_1')(x)*  \n*y\\_val\u00a0=\u00a0keras.layers.Dense(16,\u00a0activation='relu',\u00a0name='Dense\\_y\\_2')(y\\_val)*  \n*y\\_val\u00a0=\u00a0keras.layers.Dense(16,\u00a0activation='relu',\u00a0name='Dense\\_y\\_3')(y\\_val)*  \n*regression\\_y\u00a0=\u00a0keras.layers.Dense(1,\u00a0activation='linear',\u00a0name='regression\\_layer\\_y')(y\\_val)*  \n*regression\u00a0=\u00a0keras.layers.Concatenate(name='Concatenate\\_Regression')(\\[regression\\_x,\u00a0regression\\_y\\])*  \n*model\u00a0=\u00a0keras.models.Model(inputs=\\[base\\_model.input,\u00a0size,\u00a0distance\\],\u00a0outputs=\\[regression\\_x,\u00a0regression\\_y,\u00a0regression\\])*\n\nThe idea of the model is that I have a box that is in at an specific distance from camera (known and used as numerical input) and with some size (known and also used as input) and I want to infer the coordinates of a pattern that occurs in the box relative to the borders of the object. I can not add a real image as I am not propietary of the dataset, but it would be similar to the following sketch:\n\n&amp;#x200B;\n\n![img](k2o13asucx941 \"Fig 2. Example of the scenario. Three boxes, 2 of them with the same size but at different distances. One of the with twice the size but at half of the distance. The regressor outcome for all of them should be equal to (0,0) as the cross is in the middle (origin of coordinates) of the box.\")\n\nI have a hard time trying to understand what it is limiting the output range of the model to (-0.4, 0.3) while the distribution of the points seems correct and the final activation is linear.\n\nThank you for your time, have a good day!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/emozd1/help_regression_to_output_coordinates/"}, {"autor": "oryil", "date": "2020-01-10 09:33:09", "content": "Which GAN model suits best for generating images of beard and no_beard with paired dataset /!/ Im rather new to GANs and confused which GAN model would suite the best for this use case:\n\nI have a dataset which contains pair images of men that have NO\\_BEARD and BEARD.\n\nI want to train a GAN with those paired -----> image !!! s and in the end I want to feed the NN with an input -----> image !!!  and want a generated output -----> image !!! .\n\nI think it might be an Image-to-Image translation GAN or CycleGAN for that purpose.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/emoog4/which_gan_model_suits_best_for_generating_images/"}, {"autor": "ktony58", "date": "2020-02-29 13:53:00", "content": "[Help needed] Struggling with the KITTI 3D object dataset /!/ Probably a post that most will find silly, but here it goes.\n\nI am using the KITTI 3D Object dataset ([link](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d)) and trying to separately compute 3D bounding boxes of each object in the left -----> image !!!  and the right -----> image !!! . I have found various implementations that can predict this using the 2D coordinates and the orientation of the object([example](https://github.com/lzccccc/3d-bounding-box-estimation-for-autonomous-driving)).\n\nDoing this is quite straight forward for the left image because the left image labels are present in the dataset, these contain the 2D coordinates, orientation and other properties. However, I can't figure what would be the best way to go about making the 3D bounding box predictions for the right image since no label information is present. I was thinking of first running a 2D object detection algorithm like YOLOv3 on the right images to get the 2D coordinates for the objects, but that still doesn't allow me to make the 3D predictions because I don't have the orientation.\n\nAny help would be much appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fbcsex/help_needed_struggling_with_the_kitti_3d_object/"}, {"autor": "11igor", "date": "2020-02-29 13:21:22", "content": "[Question] Neural Style Transfer tutorial for a designer /!/ Hi! I am a designer interested in Neural Style Transfer. I want to train a model with my own style -----> image !!! . Which tutorial should I begin with? \n\nIf I browse github there are a lot of implementations which are based on different libraries, which one should I stick to if I want to write as little low-level code as possible?  Should I consider using Keras for it? \n\nI was wondering about it a couple of years ago. Has anything new emerged in this area since then?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fbcf1b/question_neural_style_transfer_tutorial_for_a/"}, {"autor": "pythonistaaaaaaa", "date": "2020-02-28 18:47:04", "content": "Beginner PyTorch - trying to plot a confusion matrix /!/ I'm trying to plot a confusion matrix and it doesn't work. I'm getting a weird result and I'm not sure how to interpret it (see below). I think my problem comes from just having the last confusion matrix and plotting it, but I'm not even sure because it should still plot something that looks like the 2nd -----> picture !!! , I think?\n\nIf someone can take a look at this and help that'd be amazing. \n\n[my current confusion matrix][1]\n\n\n[what I would like to have][2]\n\n\nHere's my code generating this:\n\n    model = torch.load('model-5-layers.pt')\n    \n    correct = 0\n    total = 0\n    \n    # Why don't we need gradients? What happens if we do include gradients?\n    with torch.no_grad():\n        \n        # Iterate over the test set\n        for data in test_loader:\n            images, labels = data\n    \n            images = images.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(images)\n            \n            # torch.max is an argmax operation\n            _, predicted = torch.max(outputs.data, 1)\n            \n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n\n    print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))\n\n**which prints an accuracy of 48%.**\n\nand my plotting function:\n\n\n    from sklearn.metrics import confusion_matrix\n    import matplotlib.pyplot as plt\n    \n    \n    cm = confusion_matrix(labels, predicted)\n    \n    import itertools\n    \n    \n    def plot_confusion_matrix(cm,\n                              classes,\n                              normalize=False,\n                              title='Confusion matrix',\n                              cmap=plt.cm.Blues):\n        \"\"\"\n        This function prints and plots the confusion matrix very prettily.\n        Normalization can be applied by setting `normalize=True`.\n        \"\"\"\n        if normalize:\n            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n            print(\"Normalized confusion matrix\")\n        else:\n            print('Confusion matrix, without normalization')\n    \n        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n        plt.title(title)\n    \n        # Specify the tick marks and axis text\n        tick_marks = np.arange(len(classes))\n        plt.xticks(tick_marks, classes, rotation=90)\n        plt.yticks(tick_marks, classes)\n    \n        # The data formatting\n        fmt = '.2f' if normalize else 'd'\n        thresh = cm.max() / 2.\n    \n        # Print the text of the matrix, adjusting text colour for display\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(j, i, format(cm[i, j], fmt),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] &gt; thresh else \"black\")\n    \n        plt.ylabel('True label')\n        plt.xlabel('Predicted label')\n        plt.tight_layout()\n        plt.show()\n    \n    plot_confusion_matrix(cm, classes)\n\n  [1]: https://i.stack.imgur.com/B4Ez3.png\n  [2]: https://i.stack.imgur.com/DVVSn.png", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fazhk6/beginner_pytorch_trying_to_plot_a_confusion_matrix/"}, {"autor": "django_free", "date": "2020-02-28 06:21:31", "content": "How do I build a customised object detection Algorithm? /!/ I have to create a object recognition for different foods on a tray. Some of the foods are on the same plate on the tray. \nThe t\nY_train data is just a list of foods that are present in that -----> picture !!! .\n\nI don't know where to start can somebody help me where to look?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/faqeh1/how_do_i_build_a_customised_object_detection/"}, {"autor": "ikomhoog", "date": "2020-02-27 15:18:05", "content": "How to convert .h5 to .weights file? /!/ I want to use yolov3, i trained an -----> image !!!  using:  \n[https://github.com/AntonMu/TrainYourOwnYOLO/](https://github.com/AntonMu/TrainYourOwnYOLO/)\n\nbut its output is in .h5 and i need a .cfg, .weights and .names file\n\nIs there a way i can get those from the .h5 file?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fadk7q/how_to_convert_h5_to_weights_file/"}, {"autor": "Iwannabefree10", "date": "2020-06-27 23:14:03", "content": "Why my CNN gives the same output when fed a lot of training data? /!/ Hello guys!   \n\n\nIm currently learning deep learning and I am trying my first CNN! What im doing is an -----> image !!!  colorizer. The model and my whole code works for one picture. It can colorize properly some image as long as you feed same pictures to it. So I believe it is learning but when I put a lot of training data for some epochs. The output is always the same regardless of image. It is most of the time black and white image. Please help. I got the model from the internet so I think It should work.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hh3rqs/why_my_cnn_gives_the_same_output_when_fed_a_lot/"}, {"autor": "takathur", "date": "2020-06-27 11:15:04", "content": "ML to convert text into a handwritten document like school homework. /!/ Hi,\n\nI have been through open-book handwritten exams and the last exam really tended to break my hand. I have been thinking about the idea of converting text to the handwritten documents. For example, I will input text with formatting and the output would be a formatted handwritten document in form of an -----> image !!!  that looks like scanned using a mobile app and have the option to configure metadata so that my professor doesn't catch me since he has written a Java code that reads metadata, there are some projects (I could find only two, one of them is super unnatural and the other is not completely open-source).\n\nI would like to know if anyone here has such a tool already and would like to share and if not could you guys please walk me through how can I create one of my own? Because my professor told the final exam would be of 8 hours and we won't be able to wipe the sweat off our foreheads lol.\n\nThanks.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hgrs48/ml_to_convert_text_into_a_handwritten_document/"}, {"autor": "WinterGene7", "date": "2020-06-27 11:14:44", "content": "Improving performance of PyTorch CNNs (comparing to Fastai) /!/ I have been doing machine learning for slightly over a year and I have become decently familiar with pytorch for -----> image !!!  data. However, as I try to create models for various datasets, I always get the feeling that I am missing something or underperforming. This is particularly obvious to me when I compare my results in straight pytorch to those achieved using the Fastai library, which are better by a decent margin. Are there any courses or tutorials that can help me improve and fine tune the performance of my models? How can I learn to reproduce the relatively high performance of Fastai models? Thanks.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hgrrz2/improving_performance_of_pytorch_cnns_comparing/"}, {"autor": "jonas__m", "date": "2020-03-07 01:14:57", "content": "Accurate -----> image !!!  classification in 3 lines of code with AutoGluon /!/ AutoGluon is an easy-to-use AutoML toolkit for deep learning that allows you to leverage state-of-the-art techniques.  Writing barely any code, we recently used AutoGluon to achieve around top 10% ranks in four Kaggle image classification competitions: \n\n&amp;#x200B;\n\n[https://medium.com/@zhanghang0704/image-classification-on-kaggle-using-autogluon-fc896e74d7e8](https://medium.com/@zhanghang0704/image-classification-on-kaggle-using-autogluon-fc896e74d7e8)\n\n&amp;#x200B;\n\nTo learn how to use AutoGluon for your own image classification problems, see our tutorial here:  [https://autogluon.mxnet.io/tutorials/image\\_classification/kaggle.html](https://autogluon.mxnet.io/tutorials/image_classification/kaggle.html) \n\n&amp;#x200B;\n\nBeyond image classification, AutoGluon also makes it easy to get started with object detection, as well as prediction tasks involving tabular/text data instead of images.  If you're already a deep learning practitioner, AutoGluon helps you automatically tune your own custom models.\n\n&amp;#x200B;\n\nAutoGluon is open-source and available on GitHub:  [https://github.com/awslabs/autogluon/](https://github.com/awslabs/autogluon/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fenqo1/accurate_image_classification_in_3_lines_of_code/"}, {"autor": "HallucinatesPenguins", "date": "2020-03-06 07:52:44", "content": "Could use some advice about detecting and editing specific part of an -----> image !!! . /!/ Hi, I'm still very new to machine learning but I would like to create an application that will take a photo and switch out whatever clothes you are wearing in an image for clothing from a select menu, essentially enabling you to 'try on' those outfits. Any advice on how I might set out to do this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fea9o8/could_use_some_advice_about_detecting_and_editing/"}, {"autor": "TheOGWaffle", "date": "2020-03-06 07:40:28", "content": "Image Annotation Tips? /!/ Are there any tips on how to annotate images for the best results in object detection? For example, if the object in the -----> image !!!  is obscured in half by another object, do you annotate both sides separately as the same thing or keep the general shape? I've googled this but I cannot find any tips (Using polygonal annotation btw)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fea5mg/image_annotation_tips/"}, {"autor": "hyvok", "date": "2020-03-06 06:34:55", "content": "How to estimate needed amount of training data? /!/ To preface my question I already know there is no simple answer to my question, but I am still hoping to extract some insight on the topic.\n\nI would like to develop a quite simple machine learning algorithm as a first project. The goal would be to detect an object of certain type from video. The object has a \"generally\" fixed shape (round) like a car wheel, but the details like which kind of rims, color etc. vary. The object will be visible \"well\" in the -----> image !!!  and the angle is fairly fixed so detection should be relatively easy.\n\nI do not have a ready made data set but I can generate training data fairly simply with OpenCV by taking pre-existing videos of the type of object, marking the object of interest for motion tracking and extracting the resulting images. So I can potentially easily get thousands of frames of data from ONE type of object. But the videos in question will be quite fixed and I doubt there is much information in the whole video compared to just a few frames of it (usually taken from a fixed position, object angle does not change etc.). Problem is that lets say if I'd need images of a thousand different types of the same object then downloading and marking the object in a thousand different videos would start to be unfeasible.\n\nRealistically how much training data would I need, as a rough ballpark? How many different type of objects and how many frames of each etc. How can I even get a rough guesstimate?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fe9ivh/how_to_estimate_needed_amount_of_training_data/"}, {"autor": "sh0rt_boy", "date": "2020-03-05 22:14:19", "content": "Best practice for working with a lot of data (files)? How to store and load data? /!/ I am currently working on an object detection pipeline with the **xView satellite -----> image !!!  data set** inside a **google colab notebook**. Therefor i do have a folder with all the images on my google drive folder which i mount into the notebook. Since there are **80k+ images in one single folder** it (obviously?) comes to **errors when loading data** from the folder.\n\n*How are you guys storing / loading big data sets?*\n\nI have heard you should not store more than like 1000 images in the same directory?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fe35q1/best_practice_for_working_with_a_lot_of_data/"}, {"autor": "alexx600", "date": "2020-03-05 17:46:11", "content": "offline skeleton extraction from the depth map /!/ Hello!\nI have the following problem. I have motion recordings recorded with the D435 intel realsense depth -----> camera !!! . The recording contains only depth channel, the files are *.bag.\n\nI'm looking for a tool that would allow to determine the skeleton either from a single frame or the whole recording.\n\nFor RGB images is openPose or do you know the equivalent for the depth image?\n\nI've had a few approaches to the subject I've tried \"cubemos\" but it gives poor results (and also requires RGB). At the moment I'm trying to get started with \"nuitrack\".\n\nI would appreciate your help.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fdyy8y/offline_skeleton_extraction_from_the_depth_map/"}, {"autor": "DeepStrategy", "date": "2020-03-05 17:21:14", "content": "Keras inference on big images learning on small /!/ Hi fellows,\n\nI made my own model to detect a sequence of numbers on an -----> image !!!  in realtime. I trained with MNIST, so pictures are 28x28. I trained my model without any dense or flatten layer.\n\nNow I want to do the inference on big images(1920x1080) to create an output of (x,y,classes) as outputs, x and y representing my sliding windows and class is a score. Output would be 119x67x10. \n\nI got some problems syntactical problems in keras, when I want use the same model for inference it does not let me do it:\n\n    ValueError: Error when checking input: expected conv1_input to have shape (28, 28, 1) but got array with shape (1080, 1920, 1)\n\nBut when I create a new model with different inputs and copy the weights of the old model, it gives me not reasonable outputs.\n\n**How am I supposed to approach that kind of scenario, any ideas? Is it even possible to do those things in keras or do I have to switch to native tensorflow?**\n\nGreetings, thanks for your help and time\u2026\n\n&amp;#x200B;\n\nFor Reference, my model\n\n    def create_model_new(input_shape, num_classes, training = True):\n    \n        model = Sequential()\n        # conv 1\n        model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', name='conv1', input_shape=input_shape))\n        model.add(BatchNormalization()) #(epsilon=1e-06, mode=0, momentum=0.9, weights=None)\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(MaxPooling2D((2, 2), padding='same'))\n        # conv 2\n        model.add(Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2'))\n        model.add(BatchNormalization())\n        model.add(LeakyReLU(alpha=0.1))\n        model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n        # conv 3\n        model.add(Conv2D(128, (3, 3), activation='relu', padding='same', name='conv3'))\n        model.add(BatchNormalization())\n        model.add(LeakyReLU(alpha=0.1))                  \n        model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n        # conv 4\n        model.add(Conv2D(64, (3, 3), activation='relu', padding='same', name='conv4'))\n        model.add(BatchNormalization())\n        model.add(LeakyReLU(alpha=0.1))                  \n        model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n        # conv 5\n        model.add(Conv2D(num_classes, (2, 2), activation='relu', padding='valid', name='conv5'))\n    \n    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n        \n        model.summary()\n            \n        return model\n\nDimensions for different input: training\n\n    Layer (type)                 Output Shape              Param #\n    =================================================================\n    conv1 (Conv2D)               (None, 28, 28, 32)        320\n    _________________________________________________________________\n    batch_normalization_5 (Batch (None, 28, 28, 32)        128\n    _________________________________________________________________\n    leaky_re_lu_5 (LeakyReLU)    (None, 28, 28, 32)        0\n    _________________________________________________________________\n    max_pooling2d_5 (MaxPooling2 (None, 14, 14, 32)        0\n    _________________________________________________________________\n    conv2 (Conv2D)               (None, 14, 14, 64)        18496\n    _________________________________________________________________\n    batch_normalization_6 (Batch (None, 14, 14, 64)        256\n    _________________________________________________________________\n    leaky_re_lu_6 (LeakyReLU)    (None, 14, 14, 64)        0\n    _________________________________________________________________\n    max_pooling2d_6 (MaxPooling2 (None, 7, 7, 64)          0\n    _________________________________________________________________\n    conv3 (Conv2D)               (None, 7, 7, 128)         73856\n    _________________________________________________________________\n    batch_normalization_7 (Batch (None, 7, 7, 128)         512\n    _________________________________________________________________\n    leaky_re_lu_7 (LeakyReLU)    (None, 7, 7, 128)         0\n    _________________________________________________________________\n    max_pooling2d_7 (MaxPooling2 (None, 4, 4, 128)         0\n    _________________________________________________________________\n    conv4 (Conv2D)               (None, 4, 4, 64)          73792\n    _________________________________________________________________\n    batch_normalization_8 (Batch (None, 4, 4, 64)          256\n    _________________________________________________________________\n    leaky_re_lu_8 (LeakyReLU)    (None, 4, 4, 64)          0\n    _________________________________________________________________\n    max_pooling2d_8 (MaxPooling2 (None, 2, 2, 64)          0\n    _________________________________________________________________\n    conv5 (Conv2D)               (None, 1, 1, 10)          2570\n    =================================================================\n    Total params: 170,186\n    Trainable params: 169,610\n    Non-trainable params: 576\n\nDimensions for different input: FullHD (inference)\n\n    Layer (type)                 Output Shape              Param #   \n    =================================================================\n    conv1 (Conv2D)               (None, 1080, 1920, 32)    320       \n    _________________________________________________________________\n    batch_normalization_11 (Batc (None, 1080, 1920, 32)    128       \n    _________________________________________________________________\n    leaky_re_lu_11 (LeakyReLU)   (None, 1080, 1920, 32)    0         \n    _________________________________________________________________\n    max_pooling2d_11 (MaxPooling (None, 540, 960, 32)      0         \n    _________________________________________________________________\n    conv2 (Conv2D)               (None, 540, 960, 64)      18496     \n    _________________________________________________________________\n    batch_normalization_12 (Batc (None, 540, 960, 64)      256       \n    _________________________________________________________________\n    leaky_re_lu_12 (LeakyReLU)   (None, 540, 960, 64)      0         \n    _________________________________________________________________\n    max_pooling2d_12 (MaxPooling (None, 270, 480, 64)      0         \n    _________________________________________________________________\n    conv3 (Conv2D)               (None, 270, 480, 128)     73856     \n    _________________________________________________________________\n    batch_normalization_13 (Batc (None, 270, 480, 128)     512       \n    _________________________________________________________________\n    leaky_re_lu_13 (LeakyReLU)   (None, 270, 480, 128)     0         \n    _________________________________________________________________\n    max_pooling2d_13 (MaxPooling (None, 135, 240, 128)     0         \n    _________________________________________________________________\n    conv4 (Conv2D)               (None, 135, 240, 64)      73792     \n    _________________________________________________________________\n    batch_normalization_14 (Batc (None, 135, 240, 64)      256       \n    _________________________________________________________________\n    leaky_re_lu_14 (LeakyReLU)   (None, 135, 240, 64)      0         \n    _________________________________________________________________\n    max_pooling2d_14 (MaxPooling (None, 68, 120, 64)       0         \n    _________________________________________________________________\n    conv5 (Conv2D)               (None, 67, 119, 10)       2570      \n    =================================================================\n    Total params: 170,186\n    Trainable params: 169,610\n    Non-trainable params: 576\n    _________________________________________________________________", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fdykh2/keras_inference_on_big_images_learning_on_small/"}, {"autor": "deadpool3727", "date": "2020-03-04 10:25:21", "content": "Any link or book or anything from where a beginner like me can learn Neural networks and -----> image !!!  processing in Python?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fdb34e/any_link_or_book_or_anything_from_where_a/"}, {"autor": "DeepStrategy", "date": "2020-03-04 09:32:31", "content": "Train network in keras consisting only of conv2d layers /!/ Hi fellows, \n\nI trained my own model in keras on mnist. I only got conv2d layers because I want to train the network on small images (mnist: 28x28 px) and later do the inference on large images 1920x1080. \n\nMy model (for training):\n\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #   \n    =================================================================\n    conv1 (Conv2D)               (None, 28, 28, 64)        640       \n    _________________________________________________________________\n    batch_normalization_117 (Bat (None, 28, 28, 64)        256       \n    _________________________________________________________________\n    leaky_re_lu_117 (LeakyReLU)  (None, 28, 28, 64)        0         \n    _________________________________________________________________\n    max_pooling2d_119 (MaxPoolin (None, 14, 14, 64)        0         \n    _________________________________________________________________\n    conv2 (Conv2D)               (None, 14, 14, 128)       73856     \n    _________________________________________________________________\n    batch_normalization_118 (Bat (None, 14, 14, 128)       512       \n    _________________________________________________________________\n    leaky_re_lu_118 (LeakyReLU)  (None, 14, 14, 128)       0         \n    _________________________________________________________________\n    max_pooling2d_120 (MaxPoolin (None, 7, 7, 128)         0         \n    _________________________________________________________________\n    conv3 (Conv2D)               (None, 7, 7, 256)         295168    \n    _________________________________________________________________\n    batch_normalization_119 (Bat (None, 7, 7, 256)         1024      \n    _________________________________________________________________\n    leaky_re_lu_119 (LeakyReLU)  (None, 7, 7, 256)         0         \n    _________________________________________________________________\n    max_pooling2d_121 (MaxPoolin (None, 4, 4, 256)         0         \n    _________________________________________________________________\n    conv4 (Conv2D)               (None, 4, 4, 128)         295040    \n    _________________________________________________________________\n    batch_normalization_120 (Bat (None, 4, 4, 128)         512       \n    _________________________________________________________________\n    leaky_re_lu_120 (LeakyReLU)  (None, 4, 4, 128)         0         \n    _________________________________________________________________\n    max_pooling2d_122 (MaxPoolin (None, 2, 2, 128)         0         \n    _________________________________________________________________\n    conv5 (Conv2D)               (None, 1, 1, 10)          5130      \n    =================================================================\n    Total params: 672,138\n    Trainable params: 670,986\n    Non-trainable params: 1,152\n\nFor inference:\n\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #   \n    =================================================================\n    conv1 (Conv2D)               (None, 1920, 1080, 64)    640       \n    _________________________________________________________________\n    batch_normalization_113 (Bat (None, 1920, 1080, 64)    256       \n    _________________________________________________________________\n    leaky_re_lu_113 (LeakyReLU)  (None, 1920, 1080, 64)    0         \n    _________________________________________________________________\n    max_pooling2d_115 (MaxPoolin (None, 960, 540, 64)      0         \n    _________________________________________________________________\n    conv2 (Conv2D)               (None, 960, 540, 128)     73856     \n    _________________________________________________________________\n    batch_normalization_114 (Bat (None, 960, 540, 128)     512       \n    _________________________________________________________________\n    leaky_re_lu_114 (LeakyReLU)  (None, 960, 540, 128)     0         \n    _________________________________________________________________\n    max_pooling2d_116 (MaxPoolin (None, 480, 270, 128)     0         \n    _________________________________________________________________\n    conv3 (Conv2D)               (None, 480, 270, 256)     295168    \n    _________________________________________________________________\n    batch_normalization_115 (Bat (None, 480, 270, 256)     1024      \n    _________________________________________________________________\n    leaky_re_lu_115 (LeakyReLU)  (None, 480, 270, 256)     0         \n    _________________________________________________________________\n    max_pooling2d_117 (MaxPoolin (None, 240, 135, 256)     0         \n    _________________________________________________________________\n    conv4 (Conv2D)               (None, 240, 135, 128)     295040    \n    _________________________________________________________________\n    batch_normalization_116 (Bat (None, 240, 135, 128)     512       \n    _________________________________________________________________\n    leaky_re_lu_116 (LeakyReLU)  (None, 240, 135, 128)     0         \n    _________________________________________________________________\n    max_pooling2d_118 (MaxPoolin (None, 120, 68, 128)      0         \n    _________________________________________________________________\n    conv5 (Conv2D)               (None, 119, 67, 10)       5130      \n    =================================================================\n    Total params: 672,138\n    Trainable params: 670,986\n    Non-trainable params: 1,152\n\nGoal here is to create a convolved -----> image !!!  with the dimensions of my output classes, which represent the sliding windows in my large -----> image !!!  for inference.\n\nBut keras will not let me train, because in the last layer it will reduce the shape of my training input:\n\n    ValueError: Error when checking target: expected conv5 to have 4 dimensions, but got array with shape (48000, 10)\n\nThe shape needs to be (48000, 1, 1, 10) !!! What can i do to prevent this? When I introduce flatten and dense, I can not use it later for inference on big images?\n\nThanks for your time and help.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fdamjs/train_network_in_keras_consisting_only_of_conv2d/"}, {"autor": "cantfindusernameomg", "date": "2020-03-03 23:47:41", "content": "Question about googleNet architecture /!/ I would like to make sure my understanding of CNN architecture is correct about GoogleNet. It's more of a general architecture question but I'm using googleNet as an example.\n\nThe first layer gets the Input -----> image !!!  which is 224 x 224 x 3. \n\nThe second layer is the convolution layer where the padding size is given by \\[3,3,3,3\\] which I believe means 3 rows of zeros added to the top, bottom, left and right of each image, thus making the size 230 x 230 x 3.  \nThen there are 64 7x7x3 filters that go over it with a stride of \\[2,2\\], to give an output of 112 x 112 x 3 x 64. The 112 x 112 x 3 gets summed to give 112 x 112 for a final output of 112 x 112 x 64.\n\n1) Do these 112 x 112 x 64 values go through the ReLU function to give a 112 x 112 x 64 output that is 0 for negative numbers and the same otherwise?\n\n2) This is followed by a MaxPool Layer of pool size \\[3,3\\] and stride \\[2,2\\] with \\[0,1,0,1\\] padding. Does this give an output of 56 x 56 x 64?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fd3xea/question_about_googlenet_architecture/"}, {"autor": "Uurii", "date": "2020-03-03 20:47:54", "content": "Noob tools to merge images of two people? /!/ Does anyone know if there is a relatively easy way to \"mix\" two people? I have an idea for an artsy party \u2013 to take a -----> picture !!!  of two people, preferably full height, and produce an image of their \"child\". Would be great to automate this in some or full extent.\n\nI don't have any practical experience with Machine Learning. Certainly won't be able to code something like this. I'd thought maybe tools like RunwayML could do that. But I'm not sure where to start. All models I've seen are more focused on transferring the \"style\", which is a bit different.\n\nThanks in advance to everyone who takes their time to consider this.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fd1808/noob_tools_to_merge_images_of_two_people/"}, {"autor": "insatiableone2", "date": "2020-02-12 21:26:20", "content": "How to to put/create visuals of architectures in research paper? /!/ Hello, \n\nI am in the process of writing a research paper which using deep learning for an -----> image !!!  classification problem. I am in the methods section of the paper and hit a road bump. I need to put a visual representation of the architecture used in code (I used Keras with Tensorflow backend). The ways I currently know of is to use model.summary() and the variation in the image I have below. The problem with this is that it simply cannot fit onto a single page without being shrunk to the point where you can't even see it well. How should I get around this? Preferrably I would also like to learn how to create block-like visual represenations or any other representations that are frequently used in research papers. What do you recommend to do here and how do people usually implement visuals of their architectures in a paper? Thanks \n\nhttps://preview.redd.it/f2eb700a8kg41.png?width=544&amp;format=png&amp;auto=webp&amp;s=ad1b09620650b3f1d90349bc59019c568fe44e5e", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f2yf9w/how_to_to_putcreate_visuals_of_architectures_in/"}, {"autor": "diditraki97", "date": "2020-02-12 20:47:47", "content": "How to start CV - for an EE Bachelor /!/  \n\nHello everyone.\n\nI have a bachelor degree in Electrical Engineering. During my studies, I have taken,among other courses, DSP , SSP, Image Processing and Numerical Analysis and loved all of them after learning the theory and having some basic practical labs.\n\nAfter some digging and basic experimentation I decided on getting into computer vision. Sadly I won't be able to find a job or internship for at least a year and a half, due to some unfortunate circumstances that are beyond my control.\n\nI do , however, want to seriously get into the field and I am willing to work hard and learn a lot. I would love to hear about your recommendations for resources - books, videos, courses, free or not - I want to learn the field theoretically ( I have no problem with the math) , and to get practical experience. What are the prerequisites I need (-----> Image !!!  processing, programming , statistics etc.) to be fluent? What are the practical tools and what is the equipment that I might need?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f2xta7/how_to_start_cv_for_an_ee_bachelor/"}, {"autor": "milk_runner", "date": "2020-02-11 12:09:15", "content": "Help me understand Training and Validation loss /!/  \n\nHi, Can anyone help me understand how to explain this graph?\n\nI'm trying to develop a classification model for fruit bunches. The fruits are classified into four different grades based on their ripeness. The model is supposed to predict the ripeness grade for a given -----> image !!!  of a fruit.\n\n&amp;#x200B;\n\nAdditionally, what would be the recommended model/framework for a case of fine-grain classification such as this?\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/n0n2o1oicag41.png?width=651&amp;format=png&amp;auto=webp&amp;s=3256330841ac9c3df2a05139f12534eb317e14a7", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f27g9j/help_me_understand_training_and_validation_loss/"}, {"autor": "MLtinkerer", "date": "2020-02-10 21:57:18", "content": "State of the art in -----> image !!!  inpainting!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f1xpeo/state_of_the_art_in_image_inpainting/"}, {"autor": "dmdmello", "date": "2020-02-10 21:21:36", "content": "How is vector arithmetic and interpolation possible in the latent space of GANs? /!/ In the DCGAN paper (Alec Radford et al), the authors were able to  perform vector arithmetic for semantic analogies by averaging the latent  vectors of generated images with the same class. They've also performed  interpolation, as it has been done in countless GAN papers. Both of  these require that similar examples be grouped by nearby areas in the  geometry of the latent space.\n\nI can perfectly understand how these operations could be performed in  the latent space of a VAE, for instance, since it's loss function  encourages an organized representation in the latent space by semantic  proximity. On the other hand, the GAN's Generator's latent space , as we  all know it, is sampled by a multivariate Gaussian (at least in the  DCGAN paper).There's nothing forcing a structure in it, nothing  preventing the latent representations of similar images to be extremely  far apart from each other, right? How can it preserve any geometric  structure? Or am I missing something here?\n\nEdit:\n\nTo be more specific, the -----> image !!!  bellow is what I'm referring to by vector arithmetic for semantic analogies:\n\nhttps://preview.redd.it/v6c51fi9z5g41.png?width=735&amp;format=png&amp;auto=webp&amp;s=3a42854880dd17a41cae3e2c7f7d81c17ab8f212", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f1x5qj/how_is_vector_arithmetic_and_interpolation/"}, {"autor": "pirwlan", "date": "2020-02-10 20:20:09", "content": "Where to train my models? /!/ Dear all, \n\nAfter doing some DL courses, I set out to do some projects on my own. I decided to start with some -----> image !!!  classification on the CIFAR10 dataset (50000 -----> image !!! s), as this seemed to be straight forward and good for setting up everything. \nI started doing some coding on my notebook, yet it takes quiet a while to train even the simplest model. However, when trying to reactivate my old PC, the script crashed right from the start due to limited memory. When reducing the input data, the script runs, but painfully slow. \nI had a look at cloud-based options, and found google colab, and tested it since it is free. Yet, this was even slower than my notebook. \n\nWhat would you think is my best budget option? Is anyone training models routinely in commercial clouds and would have a rough cost estimate? Would getting a better notebook be worth it, or should I rather invest in a GPU-heavy computer? \n\nBest wishes\npirwlan", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f1w7b9/where_to_train_my_models/"}, {"autor": "LawlessWalrus1", "date": "2020-02-19 19:34:33", "content": "Why am I getting this error? The xml file is in my file system. /!/ Here's the code:\n\n    import numpy as np\n    import cv2\n    import os\n    import requests\n    \n    \n    URLFACE = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_alt.xml\"\n    URLEYE = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_eye.xml\"\n    \n    cap = cv2.VideoCapture(0)\n    dr = os.getcwd()\n    \n    def downloadClassifier(URL, filename):\n        response = requests.get(URL)\n        with open(filename+'.xml', 'wb') as file:\n            file.write(response.content)\n    \n    downloadClassifier(URLFACE, 'face')\n    downloadClassifier(URLEYE, 'eye')\n        \n    faceClassifier = cv2.CascadeClassifier('face.xml')\n    eyeClassifier = cv2.CascadeClassifier('eye.xml')\n    \n    \n    def getImage(filename):\n        if(cv2.waitKey(1)):\n            # Capture frame-by-frame\n            ret, frame = cap.read()\n            \n    #         make grayscale\n    #         gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    \n            # Display the resulting frame\n            while(True):\n                cv2.imshow('frame',frame)\n                cv2.imwrite(filename + '.jpg', frame)\n                if(cv2.waitKey(0)):\n                    break\n    #Takes -----> picture !!!  and saves it into current file directory.\n    getImage('myFace')    \n    \n    pixels = cv2.imread('myFace.jpg')\n    cap.release()\n    \n    def drawBox(image):\n        #use detectMultiScale classifier method on face image\n        boundingBoxesFaces = faceClassifier.detectMultiScale(image)\n        #get coordinates and dimensions of the box drawn around face\n        for xCord, yCord, width, height in boundingBoxesFaces:\n            #find end of box coordinates \n            lastX = xCord+width\n            lastY = yCord+height\n            imagesWithRectFace = boundingBoxesFaces[yCord:lastY, xCord:lastX]\n            imageBBFace = cv2.rectangle(pixels, (xCord,yCord), (lastX, lastY), (0,0,255), 1)\n            boundingBoxesEyes = eyeClassifier.detectMultiScale(imagesWithRectFace)\n            for eyeXCord, eyeYCord, eyeWidth, eyeHeight in boundingBoxesEyes:\n                lastEyeX = eyeXCord+eyeWidth\n                lastEyeY = eyeYCord+eyeHeight\n                imagesWithRectEye = boundingBoxesEyes[eyeYCord: lastEyeY, eyeXCord:lastEyeX]\n        #create new image with rectangle drawn onto face\n        imageBBWithEyes = cv2.rectangle(imagesWithRectEye, (eyeXCord, eyeYCord), (lastEyeX, lastEyeY), (0,0,255), 1)\n        return imageBBWithEyes\n    imageFinal = drawBox(pixels)\n    #save image with rectangles to file system\n    cv2.imwrite('final.jpg', imageFinal)\n    #show image\n    while(True):\n        cv2.imshow('Final Image', imageFinal)\n        if(cv2.waitKey(0)):\n            break\n    # When everything done, turn off the camera\n    cap.release()\n    cv2.destroyAllWindows()\n\nHere's the error:\n\n    error: OpenCV(4.2.0) C:\\projects\\opencv-python\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1389: error: (-215:Assertion failed) scaleFactor &gt; 1 &amp;&amp; _image.depth() == CV_8U in function 'cv::CascadeClassifierImpl::detectMultiScale'", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f6g050/why_am_i_getting_this_error_the_xml_file_is_in_my/"}, {"autor": "rravenclaww", "date": "2020-02-19 00:33:44", "content": "Best Video Classification methods and their tutorials? /!/ I have done -----> image !!!  classification with CNN and some NLP with RNN, LSTM and GRU..so I do have an idea what to expect in a video classification task, basically a combination of CNN and RNN I believe due to images being the time series data here. I want to get straight to the point and not loiter around looking for methods and tutorials that are not upto the mark, so please guide me to the ones that are, lol\n\nAny resources, may it be blogs, books, courses..they will be appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f620i5/best_video_classification_methods_and_their/"}, {"autor": "prof-heisenberg", "date": "2020-02-18 13:20:12", "content": "Difference between U-net and ResNet /!/ In a -----> image !!!  segmentation task I came across these two architectures for -----> image !!!  segmentation. Now I am curious what are the differences between the two architectures? And for what kind of applications would you use one over the other?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f5rot3/difference_between_unet_and_resnet/"}, {"autor": "changerqu1", "date": "2020-02-17 19:02:57", "content": "How can I train any model if I know only excel and statistics ? Recommend some service please /!/ Hi! I new to python and data science. My background is mostly statistics and plain old excel. Maybe you guys, can suggest me some tools or service that helps to train model and predict data based on the model without coding ? I need some simple drag and drop tool with couple buttons.\n\nAfter training I have to deploy the model at companies infrastructure. What tool can help to make it easier ?\n\nThe developers at my company are such jerks and don't want to help me to deploy it. At least I have to prepare docker -----> image !!! . And it also require specific skills.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f5don7/how_can_i_train_any_model_if_i_know_only_excel/"}, {"autor": "spammyreddit", "date": "2020-11-03 11:50:47", "content": "Home CCTV object detection beginners project /!/ As a practical way to learn about ML I've created a real life use case that I want to implement.\n\nI currently have a DIY CCTV system consisting of rtsp streams being fed into a 4th gen i3 which does two things per stream: a) dump the raw rtsp as a video and b) dump each keyframe. As my -----> camera !!! s run at 7fps with an iframe every 7 frames, this results in a relatively efficient set up as I believe that there is no decoding involved (I get a new frame once per second per -----> camera !!! ).\n\nI now want to run object detection on these feeds and have a question about which model (?) to use. I see things about yolov5 and faster-rcnn as well as mobilenet running in coral accelerators or openvino and although I'm happy to invest the time in learning these things it seems optimal to pick the most appropriate one from the start. Here are some notes:\n\n1. For now I want to run on my existing HW. My CPU is an i3-4130t which comes with HD graphics 4400. I see that Intel encourage the use of IGDs for acceleration but not sure if that's only for specific technologies or models.\n\n2. My initial target is to run OD on the 1fps keyframe per camera. The idea is to save on the decoding of the rtsp stream. However I'm confused if this still counts as realtime detection or not (does realtime just mean \"faster than the inputs?). If I'm faced with performance issues with the hw I currently have, I would turn off OD for some cameras. I expect to at least be able to do 1 or 2 within the bounds of my processor (and hopefully igd).\n\n3. Depending on my results I'd be willing to expand my HW with an accelerator to scale the system (eg to the 7fps FPS video stream). The Google Coral with mobilenet appears to be the most accessible (not to mention cheap) way to add compute, but I'd consider a more expensive Intel compute stick (or something else) if it means a more flexible approach starting out.\n\nUltimately this is a learning experience too so I'm not afraid to get my hands dirty. But from what I've seen it's not easy to simply chop and change models, especially when extra HW comes into play, so I'd appreciate any insight this sub has.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jn8xnc/home_cctv_object_detection_beginners_project/"}, {"autor": "GV_9wj", "date": "2020-11-03 11:46:18", "content": "How to use Naive Bayes classifier for -----> image !!!  classification in Python /!/ I have a set of 260 images of blood samples of people with Leukaemia. A description of the data set would be as :-\n\n**The image files are named with the notation ImXXX\\_Y.jpg where XXX is a progressive 3-digit integer and Y is a boolean digit equal to 0 if the cell placed in the centre of the image is not a blast cell,  and equal to 1 if the cell placed in the centre of the image is a blast cell. Please note that all  images  labelled with Y=0 are from for healthy individuals, and all images labelled with Y=1 are from ALL patients.**\n\nI would like to use 200 images for training and 60 images for testing. The files are in the form of .tif files. I really want to know how I can use the Naive Bayes Classifier for this image data. I do not care as much about high accuracy as all I want to do is just compare it to a CNN I am using right now, Please help me out.\n\nSince I am new to the forum please let me know if I need to mention anything else.\n\nI am running on Python 3.8.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jn8vpa/how_to_use_naive_bayes_classifier_for_image/"}, {"autor": "zbqv", "date": "2020-11-03 10:31:37", "content": "[Q] Finding adversarial examples on a single model vs. an ensemble model /!/ I recently conducted some experiments on adversarial attacks. To be more specific, I was trying to attack several -----> image !!!  classification models on CIFAR-10. During the experiment, I encountered some intriguing phenomena as following.\n\nMy goal is to attack a model called `model_target`. (It does not matter what model `model_target` actually is, since I encountered same phenomena on almost all models I tried.) The attacking algorithm I tried was iterative FGSM.\n\nAnd I tried two ways to find the adversarial examples for `model_target`.\n\nFirst, I applied iterative FGSM on `model_target` alone to find the adversarial examples. I noted the found adversarial examples from this method as `X_adv_target`.\n\nSecond, I created an ensemble model which includes `model_target` as a submodel. For example, an ensemble model consists of n+1 models: `model_target`, `model_0`, `model_1`, ..., `model_n`. While applying FGSM attack (same as the one used in first method), the attacker got an average gradient over these n+1 models, which I think it would make the attack more general or having higher transferability. I noted the found adversarial examples from this method as `X_adv_ensemble`.\n\nThe strange thing I observed was that when evaluating adversarial examples on `model_target`, `X_adv_ensemble` always outperformed `X_adv_target`. In other words, the `model_target` gave lower accuracy for `X_adv_ensemble` than `X_adv_target`.\n\n**My question is that why** `X_adv_ensemble` **works better.** Since I only need to attack a single model (`model_target`), I don't think a set of adversarial examples with higher generalizability can help.\n\nPS. The models I used are *NiN*, *ResNet+BatchNorm*, *PyramidNet+BatchNorm*, *DenseNet-BC*, *RoR*, *RiR*, and *WRN*. And the phenomena I mentioned can be observed on all these models as the `model_target`.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jn81gq/q_finding_adversarial_examples_on_a_single_model/"}, {"autor": "DropsofAI", "date": "2020-11-03 06:07:01", "content": "Convolutional Denoising Autoencoders for -----> image !!!  noise reduction", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jn565t/convolutional_denoising_autoencoders_for_image/"}, {"autor": "Less-Holiday-719", "date": "2020-11-02 14:57:48", "content": "Using a value from my env and plotting it with matplotlib? /!/ My triangle drawing is appearing lopsided, how I make it proportional? I am rendering a custom env and using matplotlib to display it. I am using a target location from my env which I call x.\n\nI am simulating a seesaw with a circle that rolls on it, the triangle serves as the pivot of the seesaw. I am not strong in math so I am struggling to figure this out. What I want is a triangle with slightly longer sides but equal on the left and right. I want the triangle to point upwards like most do.\n\n    x = self.target_location     \n    points = np.array([[x, 10], [x/2, 10], [x+1/2, np.sqrt(5**2 - 2**2)]])     \n    pivot = plt.Polygon(points, closed = True)     \n    axes.add_patch(pivot)\n\nyou can find an -----> image !!!  here of how it appears now [https://stackoverflow.com/questions/64639554/matplotlib-draw-proportional-triangle](https://stackoverflow.com/questions/64639554/matplotlib-draw-proportional-triangle)\n\nself.target\\_location is an already defined variable, it generates a random number", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jmow5a/using_a_value_from_my_env_and_plotting_it_with/"}, {"autor": "madzthakz", "date": "2020-11-02 14:40:08", "content": "I'm a Senior Data Scientist at Disney and I'm hosting another free Data Science Q&amp;A session this Thursday @ 5:30 PM PST. This time with a guest presenter! /!/ \\[Disclaimer: These are completely free!\\]\n\nAs the title mentions, I'm a Senior Data Scientist at Disney and I'm going to host **another** Data Science Q&amp;A this Thursday at 5:30 PM PST. I know some of you may have already registered but I still wanted to post so other folks here have an opportunity to attend. Last week's session was an absolute blast with over 100 people attend from all over the world (Recording available below!).\n\nNow, things will be a bit different this time around. I've invited Keith Dowd, a Lead Data Scientist at Verizon, to join us and share his journey into Data Science. I'm really excited to bring a different perspective to our sessions.\n\nHope to see you there!\n\nMadhav\n\nRegistration Link:\n\n[https://disney.zoom.us/webinar/register/4016042551052/WN\\_aBBjIsvATCib8c2yHMiWVw](https://disney.zoom.us/webinar/register/4016042551052/WN_aBBjIsvATCib8c2yHMiWVw)\n\nMore Data Science Content:\n\n[https://www.madhavthaker.com/qaposts](https://www.madhavthaker.com/qaposts)\n\nLast week's Q&amp;A Session:\n\n[https://youtu.be/Sx\\_C5g3\\_CSc](https://youtu.be/Sx_C5g3_CSc)\n\nVerification:\n\n* My -----> photo !!! : [https://imgur.com/a/Wg3DMLV](https://imgur.com/a/Wg3DMLV)\n* My LinkedIn: [https://www.linkedin.com/in/madhavthaker/](https://www.linkedin.com/in/madhavthaker/) (Feel free to connect!)\n* Keith's LinkedIn: [https://www.linkedin.com/in/keithdowd/](https://www.linkedin.com/in/keithdowd/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jmokfi/im_a_senior_data_scientist_at_disney_and_im/"}, {"autor": "latentlatent", "date": "2020-11-01 12:37:36", "content": "\"Guided\" -----> image !!!  classification for small targets on -----> image !!! s? /!/ Let's state you'd like to classify images about dogs and cats, but the animals are small on the images. Is there any approach to \"guide\" the network without any additional labeling (like boxes), or just a small amount?\n\nWhat are the SOTA techniques/soltuions?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jm1k3m/guided_image_classification_for_small_targets_on/"}, {"autor": "dumb_programmer", "date": "2020-12-10 12:40:21", "content": "How can I classify -----> image !!! s based on data that is hidden inside them using -----> image !!!  steganography. Two types of hidden data: A or B. We've to classify these images also as either A or B. The idea is to protect hidden information such that classification is done without needing to see data explicitly.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kaelfd/how_can_i_classify_images_based_on_data_that_is/"}, {"autor": "dumb_programmer", "date": "2020-12-10 12:30:47", "content": "How can I classify -----> image !!! s based on data that is hidden inside them using -----> image !!!  steganography. Two types of hidden data: A or B. We've to classify these images also as either A or B. The idea is to protect hidden information such that classification is done without needing to see data explicitly.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kaeh4x/how_can_i_classify_images_based_on_data_that_is/"}, {"autor": "photonymous", "date": "2020-12-10 02:52:00", "content": "Simple MLP to generate random \"paintings\" /!/ Just for fun, here's a tiny little simple script to generate random \"paintings\" using an MLP with tanh activations. The code below runs fine in GNU Octave. Enjoy!\n\nhttps://preview.redd.it/qfppupdgx9461.png?width=911&amp;format=png&amp;auto=webp&amp;s=aa243f5bda2ef3f7140d80b4711e689805c1d091\n\n(suggestions for simple tweaks and improvements are welcome)\n\n    clear;\n    N_LAYERS     = 5; % number of layers in network\n    LAYER_WIDTH  = 20; % number of neurons per layer\n    N_OUTPUTS    = 3; % number of outputs in final linear readout layer\n    N_INPUTS     = 2; % number of inputs into the input layer\n    WEIGHT_RANGE = 2.0;\n    BIAS_RANGE   = 1;\n    SCALE_RANGE  = 1;\n    IMG_DIM_X    = 401;\n    IMG_DIM_Y    = 401;\n    N_PIX        = IMG_DIM_X*IMG_DIM_Y;\n    \n    % Weights:\n    W_i = WEIGHT_RANGE*randn(N_INPUTS, LAYER_WIDTH); % input weights\n    W_n = WEIGHT_RANGE*randn(LAYER_WIDTH, LAYER_WIDTH, N_LAYERS-1); % network weights\n    W_r = WEIGHT_RANGE*randn(LAYER_WIDTH, N_OUTPUTS); % linear readout weights\n    \n    % Biases &amp; Scales:\n    biases = BIAS_RANGE*randn(N_LAYERS, LAYER_WIDTH); % (no biases for readout layer)\n    scales = SCALE_RANGE*randn(N_LAYERS, LAYER_WIDTH); % (no scales for readout layer)\n    \n    % Loop over pixels to generate -----> image !!! :\n    the_-----> image !!!  = rand(IMG_DIM_Y, IMG_DIM_X, 3);\n    X = linspace(-1,1,IMG_DIM_X);\n    Y = linspace(-1,1,IMG_DIM_Y);\n    for iiX = 1:IMG_DIM_X\n        x = X(iiX);     \n        for iiY = 1:IMG_DIM_Y   \n            y = Y(iiY);\n            \n            layer_input = [x,y]; % use &lt;x,y&gt; as input to 1st layer\n            layer_sums = layer_input*W_i;\n            layer_activations = tanh(scales(1,:).*(layer_sums + biases(1,:)));\n            \n            layer_input = layer_activations;        \n            for ii = 1:N_LAYERS-1\n                layer_sums = layer_input*W_n(:,:,ii);\n                layer_activations = tanh(scales(ii+1,:).*(layer_sums + biases(ii+1,:)));\n                layer_input = layer_activations;\n            end        \n            layer_sums = layer_activations*W_r;\n            \n            the_image(iiY, iiX, :) = layer_sums;\n        end\n    end\n    \n    % Normalize and show the image:\n    normalized = the_image - min(the_image(:));\n    normalized = normalized/max(normalized(:));\n    image(normalized);", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ka6xrk/simple_mlp_to_generate_random_paintings/"}, {"autor": "Ailothaen", "date": "2020-12-09 17:17:33", "content": "A good introduction book (or course) for someone with programming experience? /!/ Hello everyone,\n\nI am someone with a quite good programming experience (lately I have been doing a lot of Python, but I also have a bit of experience with PHP and compiled languages) and I also have some math knowledge (usually the one of a high-schooler: I know things like derivatives, normal and Bernoulli laws, vectors... but never went into complicated things such as differential equations or partial derivatives).\n\nHowever, I know absolutely nothing about machine learning. Lately I have been wanting to learn about it, because I got interested into that, mostly for making simple classification systems at first (simple spam classifier, detection of the same -----> image !!!  with slight variations...)\n\nSo I am open for suggestions about resources for beginners in ML, but still oriented on programmers, and if possible also focused on this kind of systems. I feel like I would be more comfortable with books, but a web course or videos would also be nice.\nFor example I stumbled on [Machine Learning for Hackers](https://www.oreilly.com/library/view/machine-learning-for/9781449330514/) from O'Reilly, but looking into it I am a bit worried whether it may be already a bit outdated as now.\n\nThank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k9vzi5/a_good_introduction_book_or_course_for_someone/"}, {"autor": "stolzen", "date": "2020-12-09 16:07:19", "content": "Tutorial: deploying deep learning models using dockerized lambda functions /!/ Plan:\n\n* Create the needed resources in AWS\n* Convert the model from Keras to TF Lite\n* Extract all the pre-processing logic\n* Prepare the code for the lambda\n* Package everything into a Docker -----> image !!! \n* Push the -----> image !!!  to ECR\n* Create the lambda function\n* Create an API Gateway\n\nHere's the tutorial: [https://docs.google.com/document/d/e/2PACX-1vSXbZQYQEiFLC3Y6JW0xgcmBR13NMsXekXzMPzPzp7-6CXlraciOViP6tI9F2Hh7ACEu0HcmxvmKNqi/pub](https://docs.google.com/document/d/e/2PACX-1vSXbZQYQEiFLC3Y6JW0xgcmBR13NMsXekXzMPzPzp7-6CXlraciOViP6tI9F2Hh7ACEu0HcmxvmKNqi/pub)\n\n&amp;#x200B;\n\nPlease let me know what you think!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k9ukml/tutorial_deploying_deep_learning_models_using/"}, {"autor": "Laurence-Lin", "date": "2020-12-09 09:27:06", "content": "Input layers is zero for yolov4 model /!/ I'm training my own -----> image !!!  of size 945\\*465 with this tutorial: [https://mrhandbyhand.medium.com/hand-by-hand-train-your-yolov4-kaggle-dataset-ac1456e06604](https://mrhandbyhand.medium.com/hand-by-hand-train-your-yolov4-kaggle-dataset-ac1456e06604)\n\n&amp;#x200B;\n\nAnd this is my notebook:\n\n[https://github.com/laurence-lin/Defect\\_detection/blob/main/yolov4.ipynb](https://github.com/laurence-lin/Defect_detection/blob/main/yolov4.ipynb)\n\n&amp;#x200B;\n\nI'd adjust the .cfg file, and whenever I start training, the error shows:\n\n    131 route  130 128 The width and height of the input layers are different. \n     \t                           -&gt;    0 x   0 x   0 \n     132 Layer before convolutional layer must output image.: File exists\n    darknet: ./src/utils.c:331: error: Assertion `0' failed.\n\n&amp;#x200B;\n\nThe log seems like it gives zero size output at the layer, but I'm not sure why this occur. \n\n&amp;#x200B;\n\nDoes anyone have idea what's going on?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k9ouyl/input_layers_is_zero_for_yolov4_model/"}, {"autor": "dirtrockground", "date": "2020-12-08 23:01:09", "content": "RAM Problem When Using MNIST Dataset /!/ Hi all. I'm learning how to do classification on the MNIST dataset, and I have been retrieving it as follows:\n\n```python\nfrom sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784', version=1\n```\n\nSomehow in the process, my RAM usage has skyrocketed to 9.1GB out of 12GB. \n\n[Here](https://imgur.com/a/znSvKXo) is a -----> photo !!!  of my memory process. \n\nI noticed this problem because when I was trying to fit an SVM model to the data, I began getting a memory allocation error, so I would close VSCode and reopen it and run all of my Jupyter cells again, and then I began getting the memory allocation error on the first code cell where I retrieve the MNIST dataset. \n\nI'm confused and don't really know why this problem is occurring. How am I supposed to access the data and why is my RAM all of the sudden full? How do I clear it and get back to business? And how do I make sure this doesn't happen again?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k9f9i5/ram_problem_when_using_mnist_dataset/"}, {"autor": "jukitoyo", "date": "2020-12-08 22:18:41", "content": "CNN Angle Prediction /!/ I have about 100 biomedical images with labels (angle). It's a regression task and I want to be able to predict the angle of the object in the -----> image !!! . Can anyone recommend a CNN architecture for this task? Also, since I only have 100 images, should I augment the dataset and use the 100 real images as the validation? Or should I split into train/test first and then augment both the train and test.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k9egel/cnn_angle_prediction/"}, {"autor": "zimmer550king", "date": "2020-12-08 02:09:55", "content": "I am trying to find the rotation of an object from its 2D -----> image !!!  /!/ I have 6600 images and I am supposed to know the rotation of the object in each image. So, given an image, I want to regress to a single value.  \n\nMy attempt: I use Resnet-18 to extract a feature vector of length 1000 from an image. This is then passed to three fully-connected layers: fc(1000, 512) -&gt; fc(512, 64) -&gt; fc(64, 1) \n\nThe problem I am facing right now is that my training loss and validation loss immediately go down after the first 5 epochs and then they barely change. But my training and validation accuracy fluctuates wildly throughout.  \n\nI understand that I am experiencing over-fitting and I have done the following to deal with it: (1) data augmentation (Gaussian noise and color jittering) (2) L1 regularization (3) dropout  So far, nothing seems to be changing the results much. The next thing I haven't tried is reducing the size of my neural net. Will that help? If so, how should I reduce the size?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k8v58u/i_am_trying_to_find_the_rotation_of_an_object/"}, {"autor": "Programmingwithme", "date": "2020-12-27 07:23:48", "content": "Machine Learning: Handle missing data /!/ In today's post on machine learning, I will explain how to work in your CSV file if there is **no data/ missing data in a row**, which means some of your rows contain blank space. I will explain this to you.\n\nYou may know that the computer is not as developed as the brain of the human body. So, when System/Model/Computer find a blank row in your dataset, it is difficult for System/Model/Computer how to work with it. We have to give some instructions.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/0l8qgmwnjo761.png?width=318&amp;format=png&amp;auto=webp&amp;s=1621378133f93b6fb0f05ee87e074b8a5f028bbe\n\nAs you can see in the above -----> image !!!  2 blank rows are found in your dataset. It is best not to have this type of free space/blank row /missing data in your dataset. \n\n**Read more**:- [Handle missing Data](https://programmingwithharshida.blogspot.com/2020/12/machine-learning-handle-missing-data.html)\n\n**If you are interested to learn how to use GOOGLE COLAB** THEN CLICK HERE AS WELL \"[Machine Learning: Matrix of features and dependent variable](https://programmingwithharshida.blogspot.com/2020/12/Build%20and%20test%20your%20first%20machine%20learning%20model.html)\"", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kkzhxs/machine_learning_handle_missing_data/"}, {"autor": "pocomaco", "date": "2020-12-26 15:49:56", "content": "What happens to this? /!/ Let artificial intelligence learn a total of three -----> image !!! s, two -----> image !!! s and a composite -----> image !!!  of the two.  At that time, does the artificial intelligence recognize the synthesized image as \"a composite of two images\"?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kkkket/what_happens_to_this/"}, {"autor": "NamedTNT", "date": "2020-12-26 11:22:01", "content": "Identifying a pattern in images /!/ Hi, I've been studying machine learning from the very beginning for the past 4 months and still have a lot to learn so I was wondering maybe you guys could help me see if what I'm trying to do is possible by training a model or maybe I should be looking out for some other way of doing this.\n\nI have a set of images, [here is a sample (Imgur)](https://imgur.com/a/sSKeben). As you can see, the images are classified as 0 or 1. 1 means there are 3 consecutive green bars in the -----> image !!! , and each of them is equal or taller than the last one. 0 means this pattern isn't present in the image.\n\nWhat I want to do is automate the identification of this pattern. Would it be as simple as training a model with this kind of images? My question comes from the fact that the pattern that I want to find can be in the first 3 bars, the last 3, somewhere in the middle, etc. Also, the relative height of the bars can change a lot from image to image (sometimes the bar are so low they are barely visible as in the third image of the Imgur album). Also, how many images would you think I need? Classifying them manually will be slow. Any suggestions for the model are appreciated. I recently learned ResNet9 from a tutorial applied to the CIFAR10 dataset, so I was going to go with that.\n\nThanks in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kkh41j/identifying_a_pattern_in_images/"}, {"autor": "versus_7", "date": "2020-12-26 11:01:55", "content": "Prediction based on a tensorflow model /!/  I have the following [machine learning model](https://pastebin.ubuntu.com/p/hNTC5xYx4k/) and I am trying to make predictions based on it . I would like there to be an input and based on the model that has been trained make a prediction. I have written the following [python program](https://pastebin.ubuntu.com/p/Mm8qcsBK79/) to accomplish the task( the input would be the address where the -----> image !!!  is stored). But I am getting an error with the [following trace](https://pastebin.ubuntu.com/p/wMGV95ywqj/). Any help to overcome it would be seriously appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kkgvsg/prediction_based_on_a_tensorflow_model/"}, {"autor": "gauthamramesh3110", "date": "2020-12-25 15:08:50", "content": "Cannot figure out why final sigmoid layer doesn't output anything else other than 1 or 0. /!/ Sorry if I am missing something obvious, I am a beginner trying to perform transfer learning for bounding box regression over a cars dataset that has 616 images and corresponding bounding box coordinates.\n\n&amp;#x200B;\n\nHere is what I did:\n\n* I imported VGG19 model from keras, excluded the top layer, and added a couple of dense layers, one with relu activation, and other (top layer) with sigmoid activation with 4 nodes to get bounding box coordinates - (xmin, ymin, xmax, ymax). \n* I resized the train -----> image !!!  to (512, 512, 3) and coverted it to numpy array, and divided it by 255 to normalise it. \n* I tried using the preprocess\\_input function provided by the library (with and without normalization performed above).\n* I  added black padding to the images to avoid distorting the image when resizing.\n* As for the output, I transformed the coordinates to ratios with respect to the dimensions. \n* I made sure the transformations were right by converting it back to coordinates and plotting the bounding box. \n* I made sure the dtype of imput and output is float32\n* I used adam optimiser and mean\\_squared\\_error loss when training, with validation\\_split = 0.2 and shuffle=True\n\nThe issue I am facing:\n\nAfter training for 10 epochs, the validation and train accuracy plateaus at 85%, and the the output value is always some combination of 0 or 1, usually \\[0, 0, 1, 1\\] which basically selects the entire image.\n\nGoogle doesn't seem to help me, or at least I don't know how to google this up correctly. I would appreciate any help, I feel stuck with this problem.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kk0faz/cannot_figure_out_why_final_sigmoid_layer_doesnt/"}, {"autor": "Laurence-Lin", "date": "2020-12-25 03:58:48", "content": "In object detection model, why does the model detect a class that is not trained? /!/ I'm building an detection model by YOLOv4 darknet, and my configuration contains 7 different classes. However, I only train model with a single class, which I provided the same label for all training images. \n\n&amp;#x200B;\n\nAfter training, the detect -----> image !!!  shows all 7 classes, although other classes has low probability, but that confused me. Why would a model detect class that is not trained?\n\n&amp;#x200B;\n\nIf it's because I list all 7 classes in the configuration, then do I have to set only one class in case I only want to detect one class?\n\n&amp;#x200B;\n\nI'm new to object detection, thank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kjsnxv/in_object_detection_model_why_does_the_model/"}, {"autor": "the320x200", "date": "2020-12-24 23:35:44", "content": "Struggling to find any practical dataset management software /!/ I'm hoping this is just a case of not knowing the right terms to search for, but I've been really struggling to find any practical dataset management software/tools for -----> image !!!  classification.\n\nThere's a lot of software that is alright if you want to make a data set once and never touch it again but lots of common changes seem completely unhandled by the tooling out there.\n\nTwo common, practical use cases are adding additional images to an existing data set and adding additional classifications to an existing data set.\n\nA contrived example to illustrate what I mean, say you have a data set of (\"cat\", \"dog\") images and you've tagged them as such. Then as you work with your application you find you need to add another classification on the same image set covering (\"standing\", \"sitting\", \"lying down\") and also add 10,000 new images. The tooling needs to be able to track that all the images are in need of tagging for the new classification as well as that the latest 10,000 images are also in need of classification under the original cat/dog tags as well.\n\nHas anyone found tools which even begin to attempt this sort of dynamic/evolving/practical use case? I've been searching a lot without any luck...", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kjp4vw/struggling_to_find_any_practical_dataset/"}, {"autor": "kjee1", "date": "2020-03-20 14:42:30", "content": "Looking For Constructive Feedback on My Machine Learning / Data Science Focused YouTube Channel /!/ Hi Everyone - My name is Ken and I have worked as a data scientist in the sports analytics field for the last 4 years. In grad school, I was required to post one of my projects on youtube, and I realized that I had a tremendous passion for creating informative content on the platform. \n\nWhen I was starting out in data science, I felt like there was a lack of useful content about how to get started and how to get a job as a data scientist. As of now, I have focused my videos on how to learn data science/ ML effectively and how to navigate the interview process. \n\nThis year, I have decided to truly dedicate myself to improving my videos, and I think an important part of that is honest feedback. I would greatly appreciate it if you would take a look at my channel and let me know if there are ways that I could make it better.\n\nSpecifically I would like to know:\n\n* How is the content? (Would you like to see more day in the life stuff, tutorials, code reviews, projects?)\n* How is my delivery? (Could I improve how I communicate my ideas)\n* How is the production quality? (Do my editing and -----> camera !!!  angles need work?)\n* Video topic ideas? (I love taking user requests!) \n\nChannel Link:  [https://www.youtube.com/c/kenjee1](https://www.youtube.com/c/kenjee1) \n\nThank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/flw4kw/looking_for_constructive_feedback_on_my_machine/"}, {"autor": "___smurf", "date": "2020-03-20 01:14:35", "content": "[D] How do you segment 1-D vectors? /!/ I have a set of encoded images in different papers. I extracted all the images from papers and represented every paper as a 1-D vector. Every vector contains the encoded values of the images. \n\nExample: \n\nPaper 1 = [-----> image !!!  1, -----> image !!!  3, -----> image !!!  4]\n\nPaper 2 = [image 7, image 8, image 4, image 7]\n\n\nSo we encoded all images and the result is as follows\nPaper 1 = [1, 3, 4,0]\n\nPaper 2 = [7, 8, 4, 7]\n\nWe padded the vectors (they have same length m) . If I have 1 million papers  represented by 1-D. Which algorithm do you suggest using for this task?\n\nShould I use one-encoding in this case or should I go with the same representation?\n\nI can always use a cousin similarity or distance to measure similarity betweeen two vectors but how would you go for clustering them?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fllx8a/d_how_do_you_segment_1d_vectors/"}, {"autor": "PM_ME_GOOD_NEWS_", "date": "2020-03-19 21:41:58", "content": "How to use these pre-trained models? /!/ Hi everyone, \n\nI'm trying to build a tool to detect fish in an -----> image !!! . I found some pre-trained models that I think might work:\n\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models\n\nI'm still new to this stuff though so I don't know how to utilize these. Can someone give me a very rough guide on how to use the files included in each of these models (a \"graph proto\", a *frozen* graph proto, a checkpoint and a config file).\n\nI really appreciate any help you can give! Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/flig7g/how_to_use_these_pretrained_models/"}, {"autor": "CaptainAmyAmyAmy", "date": "2020-03-18 17:25:44", "content": "Need Course Advice: Image Recognition or Reinforcement Learning /!/ First time posting here!\n\nI'm a graduate student studying data science. For my elective next quarter, I'm deciding between Image Recognition and Reinforcement Learning. Since it's the last quarter before my full-time job search in September, it'd be fantastic if I could get some advice from you guys.\n\nMy goal is to land a data science position in a tech company, preferably e-commerce and ride sharing, but would also like to explore a bit while I'm in grad school.\n\nI think reinforcement learning is more linear to my goal since it could be applied to recommendation systems and route planning. However, I heard that this field is somewhat bloated with bubble--a great proportion of academic researches can't be replicated. A DS friend of mine said that RL didn't outperform ML models for her tasks at work.\n\n-----> Image !!!  recognition, in contrast, has wider application as of now and more academic researches to study from. Nevertheless, 2 counter points: 1) image recognition could be learned with lower efforts than RL so taking a course might not be worthwhile, 2) image recognition is more common on resumes than RL\n\nWhat do you guys think? Any input will be greatly appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fktby6/need_course_advice_image_recognition_or/"}, {"autor": "DvelDeveloper", "date": "2020-03-18 16:08:23", "content": "How to make predictions for a single -----> image !!!  in a classification model /!/ I followed a few colabs and am able to train a model and evaluate its accuracy. However, it doesn't show how to make predictions for a single input. I just want to test the model to predict a new example not a batch of examples.\n\nWhen I use `model.predict(image)` where the image is the NumPy array of an image I am getting this error\n\n `Error when checking input: expected keras_layer_input to have 4 dimensions, but got array with shape (2, 1)`\n\nHow do I predict a single image and find its class?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fkrzu1/how_to_make_predictions_for_a_single_image_in_a/"}, {"autor": "AyeshaHagan", "date": "2020-03-18 14:35:38", "content": "What We Need to Program Natural Language Processing (NLP) \u2013 Things Needed Below /!/ What We Need to Program Natural Language Processing (NLP) \u2013 Things Needed Below\n\nAnalogize your data to what a human would do, this is known as Embedding.\n\nProcess the data that the -----> camera !!!  picks up and send it to your computer. Most cameras have a simple Python interface, but for really hardcore power users, there are dedicated software packages.\n\nWrite functions that take in parameters and return appropriate output.\u00a0\n\nBegin with a language that allows you to describe the world.\n\nHave the basic knowledge of the basics of programming . You will also need a good programming IDE that is capable of compiling LALR(Natural Language Processing) code to run in your web browsers .\n\nDevelop a language \u00a0to communicate with the computer.\n\nTake a brief walk on the wild side to fully understand what goes into any given solution. Your screen will be a bit empty without hints, so lets take a look at what to look for\u00a0 before you get started.\n\nCreate a series of short NLP queries, generated by simple text files, that can then be fed into WordNet.\n\nLearn to think in sets of NLP rules. How you learn rules is\u00a0different\u00a0for different languages, but in Java, there are some popular tools for learning models, based on Gaussian processes. Each of them has its own issues and limitations. Below is a list of popular\u00a0tools. I have not tried all of them, and I don\u2019t even know if they are also practical or\u00a0useful\u00a0to anyone. Some of them seem\u00a0less\u00a0popular\u00a0than others.\n\nProgram the elements you want to extract data from, like \u201cpeople\u201d, \u201cservices\u201d, \u201ccategories\u201d, \u201ctasks\u201d, \u201cplaces\u201d, \u201cfoundations\u201d, and \u201ccultural references\u201d (i.e. documents, videos, programs, songs).\n\nUnderstand the essence of a Neural network.\u00a0Artificial Neural Networks are composed of a series of layers \u00a0of interconnected nodes \u00a0each corresponding to a specific function or function term of your model\n\nRead and write code in a spreadsheet, and the programs I use for Natural Language Processing are available on GitHub. A good place to get them is this python module . By using these modules, you can compile and run your own python scripts using your Python interpreter.\n\nKnow how to write code.\n\nFollow a structured sequence of tasks (usually to solve a single task) and a set of tools\u00a0(either Python or R).\n\nHave some basic training in NLP or Natural Language Processing.\n\nHave a good understanding of machine learning, statistics, C programming and computer vision.\n\nInform your users of certain basic things: how the algorithm works, what languages to test, the data that will be used, and some initial assignments.\n\nDevelop models and perform things like sentence production and parsing. \u00a0If you are not familiar with these types of things you should read this before continuing.\n\nUse an intelligent language. \u00a0In the long term it makes sense to not need to write the analysis code in a machine language, and I\u2019m not sure why our systems are only built using hand written programs. I have lots of personal experience as I\u2019ve contributed code to many of the systems. \u00a0\n\nCode a process that finds relationships between words. To do that, it is common to find a more or less intuitive feature array, but such an array will become a bottleneck when you are trying to run the algorithm fast.\n\nSource : [Deep Learning UK](http://deep-learning.co.uk/natural-language-processing-nlp-31-things-that-we-need-to-program-nlp/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fkqgsq/what_we_need_to_program_natural_language/"}, {"autor": "Transit-Strike", "date": "2020-03-18 14:34:45", "content": "Beefing up my network makes no changes to -----> image !!!  quality: WGAN /!/ I tried varying hyper parameters of my  network on a smaller network before increasing the filter count and I realised that the network doesn't really \"improve\" with time.\n\n&amp;#x200B;\n\nI tried working around with everything(learning rate, batch size, n\\_critic and the issue persists. Starting to think that the error runs deeper.) Any idea about what I should be doing?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fkqgbo/beefing_up_my_network_makes_no_changes_to_image/"}, {"autor": "_4lexander_", "date": "2020-03-18 08:21:30", "content": "What's a good paper to start with regarding zero-shot learning? /!/ Google scholar throws up a few options including some review papers. Not sure exactly where to start.\n\nRight now my specific purpose is in the context of CNN's for -----> image !!!  classification, and even more specifically (if you've been following it) understanding the top entries in Kaggle's BengaliAI competition.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fkm6w7/whats_a_good_paper_to_start_with_regarding/"}, {"autor": "khan166", "date": "2020-03-26 11:36:59", "content": "Using Drones to detect potential Covid-19 patients. /!/ Hello everyone i am trying to come up with a solution to detect potential people who may have covid-19. We are doing this by mounting a thermal -----> camera !!!  on top of a small drone. The drone will be as close as 5 feet to that person and measure the temperature of that individual, and then move on further to the next person. It will not only save law enforcement personnel and others from direct exposure but will also help in effectively and efficiently carrying out the task of screening more people in lesser time. \n\nNow the problem is that the thermal camera costs a lot, hence making this solution an expensive one. We are also considering thermal-sensors but that requires the drone to go dangerously close to the person and hence not a viable solution.\n\nCan anyone help me out by suggesting any alternative ways we can measure the temperature of many people as fast and as safely as possible? \n\nWe have drones so if they could be put to good use that would be great as they are more mobile covers more ground and decreases the chances of transmitting the bacteria to others.\n\nMy email is [trusttechdigital@gmail.com](mailto:trusttechdigital@gmail.com). Your help/suggestion will be of great value and can save save lives.  \n\n\nThank you,\n\nRegards, CEO of TrustTechDigital.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fpa0x9/using_drones_to_detect_potential_covid19_patients/"}, {"autor": "aagnone", "date": "2020-08-13 01:35:01", "content": "[D] Machine Learning Up-To-Date #9 /!/ ### Academia\n\n* [Truly Unsupervised -----> Image !!! -to------> Image !!!  Translation](https://www.lifewithdata.org/newsletter/mlutd9#unsup-image-translation)\n* [SIREN: Period Activations](https://www.lifewithdata.org/newsletter/mlutd9#periodic-activations)\n* [Gated Linear Networks](https://www.lifewithdata.org/newsletter/mlutd9#gated-linear-networks)\n\n### Applications\n\n* [Machine Learning on IoT Without the T](https://www.lifewithdata.org/newsletter/mlutd9#ml-iot-no-t)\n* [Build a Discord Bot on Google Cloud](https://www.lifewithdata.org/newsletter/mlutd9#discord-bot-gcp)\n* [Github Actions for MLOps](https://www.lifewithdata.org/newsletter/mlutd9#github-actions-ml-ops)\n\n\\---\n\nRead it here!\n\nLike it? [Join the email list](https://www.lifewithdata.org/) or [follow me](https://twitter.com/@anthonyagnone) on Twitter.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i8qtxl/d_machine_learning_uptodate_9/"}, {"autor": "bbellonl", "date": "2020-08-12 17:34:31", "content": "Ms Thesis ideas. Help! /!/ Hi everyone, I'm finishing my masters on ML and AI and I need to choose a topic and a dataset available on the Internet to do my master thesis. My problem is that I find very few things interesting. I thought in doing something with medical -----> image !!!  segmentation but I don't know if that would be very ambitious.  Ideas? Help? Thanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i8i59y/ms_thesis_ideas_help/"}, {"autor": "rat22s", "date": "2020-08-12 09:05:56", "content": "Decision tree: A short hypothesis that fits the data is unlikely to be a coincidence /!/  What does it really mean to have 'coincidence' in a **decision tree** hypothesis. Does it mean that If the tree is long than it may have the same subtree at different parts of the tree? Can't really get the -----> picture !!!  how it works.\n\nAlso if my assumption is right (same subtree), that means I can use the same attribute more than once in the same tree, like same attribute for different nodes? I thought once an attribute is used it cannot be tested for later nodes/descendants.\n\nTIA", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i8aftk/decision_tree_a_short_hypothesis_that_fits_the/"}, {"autor": "YouMakeMeSaaaaaad", "date": "2020-08-12 02:43:27", "content": "Keypoint detection problem - -----> Image !!!  tagging /!/ guys, please help me. I can't find any tool or software like \"LabelImg\" with which I can save keypoints data in pascal VOC format.\ni am at my wits end. I had been using website makesense.ai to get keypoints in csv file.\nplease kindly give me some solution/suggestion. i am at my wits end. i am at my breaking point. this is hard. i am in hell", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i85qry/keypoint_detection_problem_image_tagging/"}, {"autor": "candescentchameleon", "date": "2020-08-12 00:03:19", "content": "Anyone else interested in or currently using Ximilar for DIY -----> image !!!  recognition projects? /!/ I just created a Slack channel where we can discuss project ideas and exchange information. DM me if you'd like to be added! :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i8396d/anyone_else_interested_in_or_currently_using/"}, {"autor": "spaizadv", "date": "2020-08-11 10:55:40", "content": "Model or technic needed to teach the AI how it should change the people photos (particularly, changing the people body parts size) /!/  Hi. Let's assume I have many high quality photos of the people in a life-size. The quality is such that any AI can easily detect body/head parts and create mask or cut it).\n\nThen I take a -----> photo !!! shop, and do some manipulation on the head, like enlarging, to make it be like in 1.5-2 times bigger that it's in on the src -----> photo !!! .\n\nWhich AI technics can I use, so that I can teach the AI to understand the \"head size enlarged\" effect, and then it will be able to do it automatically on the photos with a normal people life-size photos?\n\nTnx!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i7poiz/model_or_technic_needed_to_teach_the_ai_how_it/"}, {"autor": "ThisVineGuy", "date": "2020-08-20 11:34:10", "content": "Given an -----> image !!!  of a person, they are able to create synthetic -----> image !!! s of the person in different poses or with different clothing!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/id8ln6/given_an_image_of_a_person_they_are_able_to/"}, {"autor": "CptanPanic", "date": "2020-08-20 10:34:21", "content": "Want to build something to be able to read code from images of coupon cards, where should I start? /!/ Not the actual card, but something like the link below, but -----> image !!!  can contain one card, or many, and may not show entire outline of card, and I would just like to be able to extract the code(s). Where would be a good place to start, or a tutorial to follow?  Thanks.\n\n&amp;#x200B;\n\n[https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2F52f4e29a8321344e30ae-0f55c9129972ac85d6b1f4e703468e6b.ssl.cf2.rackcdn.com%2Fproducts%2Fpictures%2F1199731.jpg&amp;f=1&amp;nofb=1](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2F52f4e29a8321344e30ae-0f55c9129972ac85d6b1f4e703468e6b.ssl.cf2.rackcdn.com%2Fproducts%2Fpictures%2F1199731.jpg&amp;f=1&amp;nofb=1)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/id7vel/want_to_build_something_to_be_able_to_read_code/"}, {"autor": "cloud_weather", "date": "2020-08-20 10:30:52", "content": "2D -----> image !!!  to 3D-like with AI learnt depth-awareness &amp; Inpainting", "link": "https://www.reddit.com/r/learnmachinelearning/comments/id7tyh/2d_image_to_3dlike_with_ai_learnt_depthawareness/"}, {"autor": "Slajni", "date": "2020-08-20 08:56:05", "content": "Marking differences between images /!/ I would like to make model that would take as input two -----> image !!! s, look at them with the aim to checking similarity and then outputing again, the -----> image !!! , that would be the heatmap marking regions that are different.\n\nThe important part here is that images are not to be de-facto identical, these two can be for example photos of the same person but took from a slightly different angle and the model would for example see that the person has new tattoo on them.\n\nI read a lot about siamese cnns being used for similar tasks, but mostly they would just output the similarity measure, where in my case the **location** of differences is of critical importance.\n\nWhat would you suggest as a potential solution for this problem. It doesn't even have to be my own model if there exists already trained one.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/id6quc/marking_differences_between_images/"}, {"autor": "portirfer", "date": "2020-08-19 20:51:52", "content": "About the structure of filters in a CNN /!/ From my understanding roughly speaking there are different filters that detect different patterns of the pixels in a -----> picture !!!  and then there could be a multiple of higher order filters that detects patterns of the activation of one filter and only one filter lower down. I imagine as an analogy a tree where the higher the order the more branches/filters. \n\nBut wouldn\u2019t it make sense if one higher order filter would detect the activation of combined and different filters lower down like a network instead of a branching tree? Like a filter that recognizes (in a picture) eyes and then another filter recognizing ears and then there is a filter higher up that detects the combination of eyes and ears. And as far I understand that\u2019s not how it works...?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/icw5mt/about_the_structure_of_filters_in_a_cnn/"}, {"autor": "OnlyProggingForFun", "date": "2020-08-19 14:08:08", "content": "Transfer clothes between photos using AI. From a single -----> image !!! !", "link": "https://www.reddit.com/r/learnmachinelearning/comments/icoali/transfer_clothes_between_photos_using_ai_from_a/"}, {"autor": "IdleKing", "date": "2020-08-19 12:59:24", "content": "Automatic cropping to feature from -----> image !!!  dataset /!/ I'm working on a scanner which has a base plate that is taken in and out to load objects on, and a camera mounted above that moves around during operation. The goal is to be able to supply an image from this camera like the one attached to a piece of software, and to then automatically crop it to the guidelines you can see surrounding the dots that make up the sample, to get a standardised image that can be processed further. These guidelines are fixed to the based of the scanner and are hence being used as a reference point.\n\nMy question is: any pointers of where to start? I'm thinking of maybe segmenting/labelling the guidelines on all the images in my dataset and then running that through a CV algorithm? In the past, I've used simple edge detection routines, but this is starting to be unreliable, and very behind current techniques to solve this kind of thing. Any advice or discussion appreciated! I'm new to all this kind of stuff (coming from an engineering and not CS background) and hence have used this sub and not the main one :)\n\nhttps://preview.redd.it/67ajjlbziyh51.jpg?width=1897&amp;format=pjpg&amp;auto=webp&amp;s=4fd2a5f546cdecb23fcaa6f6410695c4eb721021", "link": "https://www.reddit.com/r/learnmachinelearning/comments/icn5s4/automatic_cropping_to_feature_from_image_dataset/"}, {"autor": "IdleKing", "date": "2020-08-19 12:56:27", "content": "Automatic cropping to feature from -----> image !!!  data-set /!/ I'm working on a scanner which has a base plate that is taken in and out to load objects on, and a camera mounted above that moves around during operation. The goal is to be able to supply an image from this camera like the one attached to a piece of software, and to then automatically crop it to the guidelines you can see surrounding the dots that make up the sample, to get a standardised image that can be processed further. These guidelines are fixed to the based of the scanner and are hence being used as a reference point. \n\nMy question is: any pointers of where to start? I'm thinking of maybe segmenting/labelling the guidelines on all the images in my dataset and then running that through a CV algorithm? In the past, I've used simple edge detection routines, but this is starting to be unreliable, and very behind current techniques to solve this kind of thing. Any advice or discussion appreciated! I'm new to all this kind of stuff (coming from an engineering and not CS background) and hence have used this sub and not the main one :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/icn45b/automatic_cropping_to_feature_from_image_dataset/"}, {"autor": "zoeteaardappel", "date": "2020-08-19 10:36:18", "content": "Why is my test accuracy much worse than validation accuracy? And what can I do? /!/ I am training a binary -----> image !!!  classifier, with which I managed to get 93% validation accuracy, however, when I test the model on unseen data, I only get only 54% accuracy. Is this some kind of overfitting? What can I do to prevent this and make my model more generalized? \n\nThing I've tried\n1. Uncomplicated the model architecture by freezing more layers on resnet.\n\n2. Added regularization on every step.\n\n3. K-fold cross validation was of no use. \n2.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/icl7pb/why_is_my_test_accuracy_much_worse_than/"}, {"autor": "samurzele", "date": "2020-07-15 13:52:01", "content": "CycleGAN step by step /!/ I understand how CycleGAN works, but I was just wondering how training goes step by step. My guess was that first, an -----> image !!!  gets put into the generator, then the output of that goes through the discriminator, then the discriminator gets backpropagated, then the original -----> image !!!  gets put into the discriminator, then it gets backpropagated again, then the output of the generator gets fed into the second generator, and then the two generators get backpropagated with the loss from MSE + the loss from the discriminator. Is this true ? If not, how does it work ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hrnxfb/cyclegan_step_by_step/"}, {"autor": "spmallick", "date": "2020-07-14 13:09:14", "content": "Kickstarter Campaign for OpenCV AI Kit (OAK) /!/  The Kickstarter Campaign for OpenCV AI Kit (OAK) goes live on July 14, 9 AM Eastern Time.  \n\n\n[https://www.kickstarter.com/projects/opencv/opencv-ai-kit](https://www.kickstarter.com/projects/opencv/opencv-ai-kit)  \n\n\nWhat is OAK?  \n\n\nOpenCV AI Kit (OAK) is a smart -----> camera !!!  based on Intel\u00ae Myriad X\u2122. There are two variants of OAK.  \n\n\nOAK-1 is a single camera solution that can do neural inference (image classification, object detection, segmentation and a lot more) on the device.  \n\n\nOAK-D is our Spatial AI solution. It comes with a stereo camera in addition to the standard RGB camera.  \n\n\nWe have come up with super attractive pricing. The early bird prices are limited to 200 smart cameras of each kind.\u00a0  \n\n\nOAK-1 : $79 \\[Early Bird Price\\] and $99 \\[Kickstarter Price\\]  \nOAK-D : $129 \\[Early Bird Price\\] and $149 \\[Kickstarter Price\\]  \n\n\nFor the price of a webcam, you can buy a smart camera that can not only do neural inference on the device, it can also do depth estimation in real time.  \n\n\nIt is not only a good solution for companies wanting to build an industrial smart camera, it is also an excellent platform for students, programmers, engineers and hobbyists to get a taste of Spatial AI and Edge AI.  \n\n\nThe two cameras will come with excellent software support.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hr17f7/kickstarter_campaign_for_opencv_ai_kit_oak/"}, {"autor": "AdhokshajaPradeep", "date": "2020-07-14 10:06:45", "content": "Calculating errors of convolutional filters with differing number of channels in input and output /!/ I am trying to implement a convolutional neural network with the following architecture\n\n* Input -----> image !!!  is (1X28X28). This just has one channel\n*  3 Convolutional Filters, each with the dimensions (1X5X5) with a stride = 1. This is to convolve the image and to represent it in three channels. The resulting output will be (3X24X24)\n* Max Pool Layer with (2X2) dimension and stride =2. This results in output (3X12X12)\n* This is then following by a fully connected layer which has (12X12X3=432) nodes.\n* A fully connected layer with 80 nodes\n* An output layer with 10 nodes.\n\nI am able to understand how the errors of each fully connected layer and the errors of their weights are calculated. The errors in the first fully connected layer, are then projected back to the first convolutional output layer, in positions where max values were retrieved via max pooling. \n\nThis would correspond to the error of the convolutional output. This would have the dimension (3X24X24). To find the error of the convolutional filters, we need to do convolution between the input (1X28X28)(after rotating each channel by 180 degrees), and the error of the the convolutional output(3X24X24). Given that the channel dimensions do not match here, should we do zero padding for the input so that dimension of the input becomes (3X28X28), thus resulting in the convolutional output of (3X5X5)?\n\nIts 3X5X5 because (28-24)+1 = 5", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hqyul4/calculating_errors_of_convolutional_filters_with/"}, {"autor": "Amir-HZH", "date": "2020-07-14 00:32:55", "content": "Image Processing /!/ I've been struggling with this problem for a while. \n\nThe goal is to take 2-D images of objects and reshape them (for instance, a -----> picture !!!  of  a shirt laid on a surface and shape it as if it is worn...form of a body)\n\nAny idea or thoughts?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hqrecc/image_processing/"}, {"autor": "_ygoloiB", "date": "2020-07-13 18:40:53", "content": "DNN always predicts same class /!/ I'm working on training a neural network for a multi-class classification but the model always classifies all of the train images as the same class. The confusion matrix shows that all the images are predicted as the same class.\n\n&amp;#x200B;\n\nIn the full data set, the train accuracy maxes at 55% by always choosing class3 that makes up 55% of the total data set. In a smaller version of the data set where all classes make up 33% of the data set the model still picks one class the entire time but this time it picks class1.\n\nCode:\n\n    model = tf.keras.models.Sequential([\n        # Input shape is the desired size of the -----> image !!!  640x480 with 3 bytes color\n        # This is the first convolution\n        tf.keras.Input(shape=(640,480, 3)),\n        tf.keras.layers.Flatten(),            \n        tf.keras.layers.Dense(8, activation='relu'),\n        tf.keras.layers.Dense(3, activation='softmax')      # 3 class \n    ])\n    \n    model.summary()\n    model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n    history = model.fit(train_generator, epochs=15, verbose = 1)\n    \n    Y_pred = model.predict(train_generator)\n    y_pred = np.argmax(Y_pred, axis=1)\n    print('Confusion Matrix:')\n    print(confusion_matrix(train_generator.classes, y_pred))\n    print('Classification Report')\n    target_names = ['class1', 'class2', 'class3']\n    print(classification_report(train_generator.classes, y_pred, target_names=target_names))", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hql1p2/dnn_always_predicts_same_class/"}, {"autor": "SebastianGarcia96", "date": "2020-07-08 05:56:09", "content": "How can i get descriptors? Banknote &amp; Coin recognition using multiclass SVM /!/ Hi everyone, i need to classify banknotes and coins using SVM in Matlab, they can be in any position, rotation or even flipped. So far for the coins i have managed to get interest points with some -----> image !!!  processing and then using Harris\u2013Stephens Algorithm function to get corner points (but i think it is not doing good job). Now , how can i get descriptors, Im stuck at this point. \n\nAny suggestions?\n\nBtw im using Peruvian currency", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hnblno/how_can_i_get_descriptors_banknote_coin/"}, {"autor": "awezmm", "date": "2020-07-07 07:50:54", "content": "Help with separating curves /!/ How could I use machine learning to separate the 3 curves in the left -----> image !!!  below:\n\nhttps://preview.redd.it/tw6nyemp4e951.png?width=542&amp;format=png&amp;auto=webp&amp;s=9ccbeef54116c0b42d645290732962e78069c50a\n\nThe -----> image !!!  on the left would be the input -----> image !!!  and the -----> image !!!  on the right has labels for the three networks. Semantic segmentation wouldn't be enough since I need to separate curved lines from each other.\n\nI was trying to do object detection with YOLO but I'm having difficulty using roi rectangular bounding box labels to separate the lines since they all overlap with each other. I'm not sure if YOLO is the right framework to use.\n\nI understand separating the lines may be easier with classical algorithms but I am wondering how I could do it with machine learning.\n\nAny advice would be greatly appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hmq8qo/help_with_separating_curves/"}, {"autor": "Unchart3disOP", "date": "2020-06-09 11:15:29", "content": "How do you perform data augmentation on images using Python /!/ As the title says, I wanted to do data augmentation and in the same time generate labels for my augmented data, so that they are the same as the main -----> image !!!  before augmentation, so is there to automate this using Python?\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gzl4cc/how_do_you_perform_data_augmentation_on_images/"}, {"autor": "TrackLabs", "date": "2020-06-09 10:25:52", "content": "Input shape error from generator in a GAN network. Would someone be able to help out? /!/ I made a similiar post some time ago, but I rewrote the code and I am mostly done. Except I am getting complains from my generator network when feeding it a fake -----> image !!!  noise. \n\nThe fake image noise is being created with a shape of (batch\\_size, 256, 256), and thats exactly the input shape the generator is programmed to take in. Yet it complains about this:\n\n&gt;WARNING:tensorflow:Model was constructed with shape (None, 100, 256, 256) for input Tensor(\"dense\\_2\\_input:0\", shape=(None, 100, 256, 256), dtype=float32), but it was called on an input with incompatible shape (100, 256, 256).\n\n&amp;#x200B;\n\n The fake image noise is being created and parsed with this\n\n    fin\u00a0=\u00a0np.random.randn(batch_size,\u00a0256,\u00a0256).astype(\"float32\")\n    generated_images\u00a0=\u00a0generator(fin)\n\nAnd the generators input layer looks like this\n\n    model.add(tf.keras.layers.Dense(64,\u00a0input_shape=(batch_size,\u00a0256,\u00a0256)))\n\n&amp;#x200B;\n\nSince both are exactly the same size, I am unsure why it complains about a fourth layer with None missing? Also, the batch\\_size is 100, resulting in the (100, 256, 256) shape.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gzkgj1/input_shape_error_from_generator_in_a_gan_network/"}, {"autor": "pgmcr", "date": "2020-06-08 17:27:04", "content": "How do the generators in GANs generate data? /!/ I get that they can generate from a random number, but for example if I want a generator that takes a sketch and generates a photorealistic -----> image !!! , how does the -----> image !!!  look like the sketch?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gz3oeo/how_do_the_generators_in_gans_generate_data/"}, {"autor": "pseudopodia_", "date": "2020-06-08 16:12:14", "content": "How to take video input from webpage and pass it to a Deep Learning model? /!/ Deep Learning noob here. I am making a human pose detection project. I need the user to upload a video or if possible, take livestream feed from the user's webcam. I'm then going to divide the video in frames, select one frame after every 2sec and then pass these -----> image !!!  frames to my deep learning model. I'm using a Resnet model from FastAI for this. I know how to divide a standalone video into image frames using OpenCV. But I am not understanding how to integrate this with the webpage and the deep learning model. I've used Flask before on a small project which dealt only with images and not videos.\n\nCan somebody please guide me about how to proceed and what to use to get the results I want?\nThanks a lot :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gz28ti/how_to_take_video_input_from_webpage_and_pass_it/"}, {"autor": "AerosolHubris", "date": "2020-06-07 21:51:31", "content": "Intro video or series to motivate ML? /!/ I'd like to get my teenager into the basics, and 3Bl1Br's series are just great. But I'd prefer to start with general ML before getting into neural nets. I think conceptually it makes more sense to start there, to understand basic choice of features, random trees and nearest neighbor algorithms, etc. But it doesn't need to dive deeply into the theory nor how to build a ML algorithm from scratch. Just something a bit beyond finding a best fit curve using linear regression. \n\nI'm a professional mathematician and I use ML methods from time to time, so I feel ok filling in any blanks. We will definitely move onto 3Bl1Br's neural net series next. I'm really just looking for something that can motivate and spark interest in ML, and show some applications that aren't boring to a teenager but also aren't the usual -----> image !!!  classifier problems.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gylfxe/intro_video_or_series_to_motivate_ml/"}, {"autor": "superare09", "date": "2020-06-07 21:41:23", "content": "CNN with -----> image !!!  as a label /!/ I want to make a CNN where the input is an image and the label is also an image. For example, the input image can be a tree and the label is an outline of that tree. Can this be done in Pytorch? Are there any examples of pytorch code where this has been done? Thanks in advance", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gyl8og/cnn_with_image_as_a_label/"}, {"autor": "Forgot_the_semicolon", "date": "2020-06-07 20:06:12", "content": "Applying K-Means Clustering on pixels in an -----> image !!!  /!/ I've recently been reading about unsupervised learning and clustering algorithms and decided to make a small python project that runs k-means clustering on JPEGs. Feel free to play around with it!\n\nProject Repo:\n\n[https://github.com/BenjaminWSwenson/k-means-image-visualization](https://github.com/BenjaminWSwenson/k-means-image-visualization)\n\nLinux executable (beware: it's almost 50 MB):\n\n[https://drive.google.com/drive/folders/1bB1itr3FCQxwnjM1W4xLThI8DD8\\_8szC?usp=sharing](https://drive.google.com/drive/folders/1bB1itr3FCQxwnjM1W4xLThI8DD8_8szC?usp=sharing)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/08gzrnukoj351.png?width=1928&amp;format=png&amp;auto=webp&amp;s=6a3e831c7c9ededbc5d50be8b5970418259cbd5a", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gyjcxt/applying_kmeans_clustering_on_pixels_in_an_image/"}, {"autor": "DrCocoaBean", "date": "2020-06-07 18:14:29", "content": "I would like to have some help with clustering /!/ Hi all, I'm quite new to this sub, never posted here before so I hope my post is fine (The rules looks fine with my post)\n\nSo I have a data of group of graphs, and each group has 20\\~ graphs inside. I want to cluster the graphs by similarity, as you can see down below (2 groups: red and green, sorry if it's not clear in the -----> picture !!! ).\n\nI want similar graphs to be at the same group (2 groups, for start).\n\nIllustration of the data: (one group)\n\n\\[x11, x12, x13... x1n\\]  \n\\[x21, x22, x23... x2n\\]  \n\\[x31, x32, x33... x3n\\]  \n\\[x41, x42, x43... x4n\\]  \n\\[xn1, xn2, xn3...xnn\\]\n\n  \nthe groups are not related to each other, but I want the algorithm to cluster each group successfully enough.  \n\n\nI tried many clustering algorithms (from here: [https://scikit-learn.org/stable/modules/clustering.html](https://scikit-learn.org/stable/modules/clustering.html) )  \n, but I didn't get the results I want.\n\nI also normalized the data, which improved the clustering but it's still not good. Do you guys have any tips so I can cluster it properly? maybe it's a certain clustering algorithm I need? maybe another normalization?  \nThank you!  \n\n\n[After Clustering](https://preview.redd.it/svyfgmfy1j351.png?width=1284&amp;format=png&amp;auto=webp&amp;s=655351d04dac61cec19d5933f37434fa65338c45)\n\n[Before Clustering](https://preview.redd.it/ifya0kgy1j351.png?width=1284&amp;format=png&amp;auto=webp&amp;s=4d8609130906b8d55e6c5a3ae1244961ee357d12)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gyhat6/i_would_like_to_have_some_help_with_clustering/"}, {"autor": "pinter69", "date": "2020-06-07 16:39:49", "content": "Free zoom lecture about generative deep learning, fake and digital art for the reddit community /!/ Following the amazing turn in of [this community](https://www.reddit.com/r/learnmachinelearning/comments/gkr44a/free_zoom_lecture_about_advances_in_deep_learning/) for the previous lecture (700 people registered!). We are organizing the next event for you redditors :)\n\nIn this next lecture Dr. Eyal Gruss ( [u/eyaler](https://www.reddit.com/u/eyaler/), Linkedin: [https://www.linkedin.com/in/eyalgruss/](https://www.linkedin.com/in/eyalgruss/) ) will teach us about generative neural networks and digital art. This is a hands-on python lecture.\n\n**Lecture title:**  \n\n*Fake Anything: \"The Art of Deep Learning\"*  \n\n**Lecture abstract:**\n\nThe age of creative machines is afoot. I will review recent state of the  art applications of generative deep learning algorithms in -----> image !!!   processing, language modeling and media arts. I will also exhibit my  digital artwork and perform live demos of the new generation of deep  learning algorithms. I will also present my latest work, \"The Art of  Deep Learning\", which is a collection of short videos and slides that  demonstrate the power of deep learning. \\[text partly generated by a  neural network\\] \n\n**Bio:**  \nDr. Eyal Gruss is a machine learning researcher, consultant and  teacher, working mainly in image and language processing. Eyal has a  diverse industry background, including medical, financial, cyber,  sensors, ads, web, real-estate and creative. Eyal hold a PhD in physics,  and is also a new-media artist creating and using generative  algorithms. \n\n&amp;#x200B;\n\nTwo time slots are scheduled for the lecture (to make it easier for people from the east and west hemisphere to participate). Links to reddit events:\n\n[Fake Anything: \"The Art of Deep Learning\" - Dr. Eyal Gruss - East hemisphere](https://www.reddit.com/r/2D3DAI/comments/gy8yxh/fake_anything_the_art_of_deep_learning_dr_eyal/)\n\n[Fake Anything: \"The Art of Deep Learning\" - Dr. Eyal Gruss - West hemisphere](https://www.reddit.com/r/2D3DAI/comments/gy91ea/fake_anything_the_art_of_deep_learning_dr_eyal/)\n\n&amp;#x200B;\n\nAs usual, these lectures happen because of your amazing participation and enthusiasm.  Let's do this \ud83d\ude80\ud83d\ude80", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gyfm4a/free_zoom_lecture_about_generative_deep_learning/"}, {"autor": "Rahul_Desai1999", "date": "2020-06-07 15:38:41", "content": "I am training a nn using pytorch to recognize breeds of dogs. I am using resnet18 and I am using -----> image !!!  augmentation to increase the size of my dataset, but the loss isn't going down. /!/ I have run this exact project in jupyter notebooks locally, and also on google colab\n\nThe only difference is that I'm not using image augmentation in jupyter lab.\n\nProblem: When i run it in jupyter notebook, I can see a difference in losses in each epoch. \n\nBut while running in google colab, where i use image augmentation, I can't see much difference in the loss after each epoch.\n\n&amp;#x200B;\n\nCode:\n\n    from torchvision import models\n    \n    net = models.resnet18(pretrained=True)\n    net.fc = nn.Linear(512, 120)\n\nThis is because I have 120 breeds\n\n    MAKE_TRAINING_TESTING_DATA = True\n    IMG_SIZE = 80\n    train_data = []\n    if MAKE_TRAINING_TESTING_DATA:\n        \n        PATH = \"/content/drive/My Drive/Dog Breed\"\n        df = pd.read_csv(\"/content/drive/My Drive/Dog Breed/labels.csv\")\n    \n        breed_classes = {}\n        index = 0\n    \n        for breed in df['breed'].unique():\n            breed_classes[breed] = index\n            index+=1\n        \n        TRAIN_FOLDER = os.path.join(PATH, \"train\")\n        # TEST_FOLDER = os.path.join(PATH, \"test\")\n        for image in tqdm(os.listdir(TRAIN_FOLDER)):\n            if(image.endswith(\".jpg\")):\n              try:\n                image_name = image.replace(\".jpg\", \"\")\n                breed = (df[df['id'] == image_name]['breed'].values)[0]\n                img = cv2.imread(os.path.join(TRAIN_FOLDER, image))\n                img_original = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n                \n                img_r_left = cv2.rotate(img_original, cv2.ROTATE_90_COUNTERCLOCKWISE)\n                \n                img_r_right = cv2.rotate(img_original, cv2.ROTATE_90_CLOCKWISE)\n                \n                img_r_360 = cv2.rotate(img_original, cv2.ROTATE_180)\n                \n                img_flip = cv2.flip(img_original, 0)\n                \n                #cropping image\n                img_cropped = cv2.resize(img, (200, 200))\n                img_cropped = img_cropped[60:140, 60:140]\n                \n                #rotate by a particular angle?\n                img_l_45 = rotate_image(img_original, 45)\n                \n                #rotate by a particular angle?\n                img_r_45 = cv2.flip(rotate_image(cv2.flip(img_original, 0), 45), 0)\n                \n                train_data.append([np.transpose(np.array(img_original), (2,0,1)), np.eye(120)[breed_classes[breed]]])\n                \n                train_data.append([np.transpose(np.array(img_r_left), (2,0,1)), np.eye(120)[breed_classes[breed]]])\n                \n                train_data.append([np.transpose(np.array(img_r_right), (2,0,1)), np.eye(120)[breed_classes[breed]]])\n                \n                train_data.append([np.transpose(np.array(img_r_360), (2,0,1)), np.eye(120)[breed_classes[breed]]])\n                \n                train_data.append([np.transpose(np.array(img_flip), (2,0,1)), np.eye(120)[breed_classes[breed]]])\n                \n                train_data.append([np.transpose(np.array(img_cropped), (2,0,1)), np.eye(120)[breed_classes[breed]]])\n                \n                train_data.append([np.transpose(np.array(img_l_45), (2,0,1)), np.eye(120)[breed_classes[breed]]])\n                \n                train_data.append([np.transpose(np.array(img_r_45), (2,0,1)), np.eye(120)[breed_classes[breed]]])\n    \n              except:\n                pass\n                \n        np.random.shuffle(train_data)   \n\nThis is data preparation and augmentation. I store it all in \"train\\_data\"\n\n    TRAIN_PCT = 0.80\n    data_size = len(train_data)\n    EPOCHS = 5\n    BATCH_SIZE = 32\n    \n    for epoch in range(EPOCHS):\n        for i in tqdm(range(0, int(TRAIN_PCT*data_size), BATCH_SIZE)):\n    \n            batch = train_data[i:i+BATCH_SIZE]\n    \n            batch_X = torch.tensor([i[0] for i in batch])\n            batch_X = batch_X/255\n            batch_y = torch.tensor([i[1] for i in batch])\n            \n            optimizer.zero_grad()\n            outputs = net(batch_X.float())\n            outputs = outputs.squeeze()\n            loss = loss_function(outputs.float(), batch_y.view(-1,120).float())\n            loss.backward()\n            optimizer.step()\n        print(f\"{loss}\")\n\nRunning epochs, I'm dividing the train\\_data into batches and I'm passing them into the network. \n\nIf you're wondering about the .float() in my data, it's because the data in it's original form wasn't accepted.\n\n&amp;#x200B;\n\nThese are my losses in google colab:\n\nep 1: 008543343283236027 \n\nep 2:  008276751264929771 \n\nep 3:  008288090117275715 \n\n&amp;#x200B;\n\nBtw i ran 40 epochs in my jupyter file, got 1% accuracy in insample data. Whats wrong here, please help.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gyekf2/i_am_training_a_nn_using_pytorch_to_recognize/"}, {"autor": "satyayeeet", "date": "2020-06-07 12:58:12", "content": "What should I do to not receive an OOM error for Deep Learning Projects? /!/ I've been working on guided projects these days, using Tensorflow-GPU version. I tried a Neural Style Transfer project for two images. \n\nBut as soon as I enter the training loop part of the code, it throws out an OOM (Out Of Memory) error. I tried resizing the -----> image !!!  but it didn't work.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gybwdj/what_should_i_do_to_not_receive_an_oom_error_for/"}, {"autor": "CommunismDoesntWork", "date": "2020-06-01 15:47:38", "content": "Is there a metric for multiclass classification that takes confidence into consideration? /!/ For instance, out of 10 -----> image !!! s the only -----> image !!!  my classifier got wrong also was also the only prediction to have a very low confidence. That's useful information that I'd like to capture.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gumxyd/is_there_a_metric_for_multiclass_classification/"}, {"autor": "namenomatter85", "date": "2020-06-01 15:08:21", "content": "George Floyd Library - Brutality Detection Library/Framework /!/  \n\nWith everything going on I wanted to create something that would remove racism and brutality from tough situations.\n\nLibraries Created(on Github):\n\n[Privy](https://github.com/Deamoner/GDPR-privacy-filter------> photo !!! -scrubbing) \\- Photo Privacy Filter to remove faces and identifiable attributes like race from pictures.\n\n[George Floyd Library](https://github.com/Deamoner/George-Floyd-Library) \\- Library dedicated to detecting brutality in photos in real time.\n\nI would love any feedback on the current documented approaches/methodologies to improve these to make the world a better place.\n\nMy background is in Clinical Trial NLP and Privacy so the brutality metric will take some iteration.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/guma0k/george_floyd_library_brutality_detection/"}, {"autor": "bobhwantstoknow", "date": "2020-05-31 19:18:02", "content": "How do you format ground truth for a null bounding box? /!/ I'm trying to implement the simple object detector described in this video https://www.youtube.com/watch?v=LZRfHkTNQqo  I have a data set with ground truth bounds for multiple objects.  But i'm not sure how to format the bounding box training data for objects that are not in an -----> image !!! .  For example, the classifier training data might be:\n\n[0,1,0,0]\n\nand the location data:\n\n[[???],[2,1,4,5],[???],[???]]\n\nWhat should the \"???\" be?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gu51mq/how_do_you_format_ground_truth_for_a_null/"}, {"autor": "xXDarkCubeXx", "date": "2020-05-31 18:09:37", "content": "How to transform a caffe model to a tensorflow model? /!/ I am trying to implement Gatys et al, paper about -----> image !!!  style transfer. And there is this section about normalizing the network. He described how, and after the help of a few stack exchange posts I know how . But... I'm too lazy to implement it :)\n\nSo during my search I found the weights that the author came up with. The problem is: it's a caffe model (like, who tf uses caffe!?) And I use tensorflow 2.1.0. Is there a way to export the weights into a jason file or something and then into a keras model?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gu3tdg/how_to_transform_a_caffe_model_to_a_tensorflow/"}, {"autor": "ztay0001", "date": "2020-05-31 06:15:31", "content": "Need help in creating dataset /!/   \n\nI\u2019m a university student currently working on a school project focusing on training an Artificial Intelligence to distinguish between raw shrimp, cooked shrimp, raw chicken chunks and cooked chicken chunks. I am forced to switch into this project due to the current Covid-19 situation which prevents me from accessing the university laboratory, leaving me with quarter the time other students would have for their projects. I am required to provide the AI with at least 1000 images for each of the items mentioned above in order for it to learn about their distinct features. I tried sourcing different -----> image !!! s from google and other website such as instagram with the help of tools such as -----> image !!!  scrapper but repetition and irrelevant -----> image !!! s starts to pop up around the 100th -----> image !!! . To make things worse, each group of images have a specific criteria that needs to be followed. Hence I would like to seek your kind assistance in collecting enough number of images to train the artificial intelligence using Google form. It would be nicer if someone is willing to spread this post. It only takes 4000 people to complete this mission if everyone uploaded 1 image.  \n Please snap a photo of the items listed above if the criteria listed below is satisfied. Link for submission for each item is listed below, sample images is also shown in the link provided.\n\nCooked chicken\n\nCriteria: The image of cooked chicken chunks should not include cooked shrimp, raw shrimp and raw chicken chunks as it might cause confusion. It is also important that the natural color of the cooked chicken chunks is not covered by condiments.\n\n[https://docs.google.com/forms/d/e/1FAIpQLSetz655iyw5H10nnHGnYK-sDFIm-12ZeneX3fWrVM1UTNGmfA/viewform?usp=sf\\_link](https://docs.google.com/forms/d/e/1FAIpQLSetz655iyw5H10nnHGnYK-sDFIm-12ZeneX3fWrVM1UTNGmfA/viewform?usp=sf_link)\n\nCooked shrimp\n\nCriteria: The image of cooked shrimp should not include raw shrimp, raw chicken chunks and cooked chicken chunks as it might cause confusion. It is also important that the natural color of the cooked shrimp is not covered by condiments. The shrimp should also be headless. \n\n[https://docs.google.com/forms/d/e/1FAIpQLSee4tZRV3fxCdJevs16bfYcwUlr07XYfCvquoR-qAshbZUWSA/viewform?usp=sf\\_link](https://docs.google.com/forms/d/e/1FAIpQLSee4tZRV3fxCdJevs16bfYcwUlr07XYfCvquoR-qAshbZUWSA/viewform?usp=sf_link)\n\nRaw chicken chunks\n\nCriteria: The image of raw chicken chunks should not include cooked shrimp, raw shrimp and cooked chicken chunks as it might cause confusion.\n\n[https://docs.google.com/forms/d/e/1FAIpQLSecSzQk8ho3y0ZD6TPrPfrUa3KbZgTEXzjIObMPiW9pkyqwUw/viewform?usp=sf\\_link](https://docs.google.com/forms/d/e/1FAIpQLSecSzQk8ho3y0ZD6TPrPfrUa3KbZgTEXzjIObMPiW9pkyqwUw/viewform?usp=sf_link)\n\nRaw shrimp\n\nCriteria: The image of raw shrimp should not include cooked shrimp, raw chicken chunks and cooked chicken chunks as it might cause confusion. The shrimp should also be headless. \n\n[https://docs.google.com/forms/d/e/1FAIpQLScuiQSGMM9U3cbgHdpY0ye-EzOKtUojQh3RiVeQYl-BJrKtdg/viewform?usp=sf\\_link](https://docs.google.com/forms/d/e/1FAIpQLScuiQSGMM9U3cbgHdpY0ye-EzOKtUojQh3RiVeQYl-BJrKtdg/viewform?usp=sf_link)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gttuzh/need_help_in_creating_dataset/"}, {"autor": "xolotl96", "date": "2020-05-20 14:21:03", "content": "help with -----> image !!!  classification on cifar100 /!/ Hi, is there a good step by step guide on how to perform image classification with this dataset? I have to use resnet, but I am interested primarely on how to load and split the data", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gnc6za/help_with_image_classification_on_cifar100/"}, {"autor": "dhokna", "date": "2020-05-20 08:19:49", "content": "Best course for opencv tutorial and -----> image !!!  analysis? /!/ I found out a lot of courses but didn't understand which is the best. \n\nI am a got only a little knowledge about this subject.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gn7gw5/best_course_for_opencv_tutorial_and_image_analysis/"}, {"autor": "arkeidolon", "date": "2020-05-20 05:18:50", "content": "Conv2D: \"Dimension Collapse\" due to not having more than 32 filters? /!/ There's this StackOverflow question regarding determining the filter size of your Conv2D layer:\n\n[https://stackoverflow.com/questions/48243360/how-to-determine-the-filter-parameter-in-the-keras-conv2d-function](https://stackoverflow.com/questions/48243360/how-to-determine-the-filter-parameter-in-the-keras-conv2d-function)\n\nThe most upvoted answer has a list of rule of thumbs, number 1 being:\n\n1. **Avoid a dimension collapse in the first layer.** Let's assume that your input filter has a (n, n) spatial shape for RGB -----> image !!! . In this case, it is a good practice to set the filter numbers to be greater than n \\* n \\* 3 as this is the dimensionality of the input of a single filter. If you set smaller number - you could suffer from the fact that many useful pieces of information about the image are lost due to initialization which dropped informative dimensions. Of course - this is not a general rule - e.g. for a texture recognition, where image complexity is lower - a small number of filters might actually help.\n\nI can't seem to wrap my head around this. If you have an 64 x 64 image, you need to have 64 x 64 x 3 = 12,288 filters for your Conv2D to be effective? This is far from the examples you see scattered all-throughout the Internet, wherein their usual filter size is 32 or 64.\n\nI'm currently building an Autoencoder, and lo and behold, the images generated skew to the color red due to the training data being mostly of the color red, but it captures the shape of the images pretty well. I'm wondering if I should increase my filter size from a measly 64 into a whopping 3,072. What do you guys think?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gn5avr/conv2d_dimension_collapse_due_to_not_having_more/"}, {"autor": "connor-owen", "date": "2020-05-20 00:47:12", "content": "I hate English essays and creative writing and want to make an artificial learning program to do it for me. Is this possible, if so how hard would it be? Here is a -----> picture !!!  of my friends cat for you to enjoy.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gn19d3/i_hate_english_essays_and_creative_writing_and/"}, {"autor": "Rahul_Desai1999", "date": "2020-05-19 18:49:12", "content": "I am unable to understand how to use torch.flatten method to pass the data from a Conv2d layer to a dense layer /!/ I was watching sentdex's pytorch tutorials and i got confused how he, in his 6th tutorial, tried to flatten the data. There exists a flatten method now in the pytorch library now but I don't know how to use it. Can someone help please?\n\n&amp;#x200B;\n\n    class Net(nn.Module):\n        def __init__(self):\n            super().__init__() # just run the init of parent class (nn.Module)\n            self.conv1 = nn.Conv2d(1, 32, 5) # input is 1 -----> image !!! , 32 output channels, 5x5 kernel / window\n            self.conv2 = nn.Conv2d(32, 64, 5) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 5x5 kernel / window\n            self.conv3 = nn.Conv2d(64, 128, 5)\n    \n            x = torch.randn(50,50).view(-1,1,50,50)\n            self._to_linear = None\n            self.convs(x)\n    \n        def convs(self, x):\n            # max pooling over 2x2\n            x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n            x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n            x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n    \n            if self._to_linear is None:\n                self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n            return x\n\nAlso, what exactly does F.max\\_pool2d do? And what are its arguments?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gmumv7/i_am_unable_to_understand_how_to_use_torchflatten/"}, {"autor": "TrPhantom8", "date": "2020-05-19 13:25:07", "content": "CNN for regression task /!/ Hello, I would like to ask some suggestions on **how to structure a CNN for function regression**.\n\nI need to create a neural network which takes a 2D black and white -----> image !!!  and outputs and array of values. This output is basically the function `f(x)` mapped over a fixed array `x=np.linspace(1,10,100)`. \nThe connection between f(x) and the 2D map is not trivial, and it is not possible to determine f(x) from the 2D map analytically. \n\nFor this task, for several reasons, I can't use transfer learning and I need to be able to define the structure of the network using Keras, thus I can't try different pre-trained models to see what works best. \n\nWhat are some successful neural network architectures which can be implemented? I've read many things about classification problems, but not much about regression problems.\n\nI've implemented a CNN structured in the following way:\n- 3 blocks made of Conv2D + MaxPooling + BatchNormalization\n- Dropout layer (0.2)\n- 3 FC layers \n\nAs a loss function I've used the MSE.\n\nIt was able to achieve a relatively good performance, but it tends to overfit a little bit and the mean absolute percentage error on the lower x (from 1 to 5) is relatively high (40%) while on the higher x (6 to 10) it is around 1% (really good).\n\nDo you have any advice or articles I can read? Thank you very much and I apologize for my broken English", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gmoi3p/cnn_for_regression_task/"}, {"autor": "badamtus", "date": "2020-05-19 08:44:35", "content": "Hyperparameter Tuning Experiments /!/ Hi there!\n\nI am currently trying to work with different automatic hyperparameter optimization techniques such as the [BOHB](http://proceedings.mlr.press/v80/falkner18a.html) and [Hyperband](https://arxiv.org/pdf/1603.06560.pdf). I have added slight modifications to the BOHB and I want to benchmark my tuning algorithm to BOHB and Hyperband. So, I am  trying to optimize different deep / machine learning models' hyperparameter and to monitor 2 aspects:\n\n1: Given a fixed amount of compute resource, which algorithm would return the configuration resulting in highest validation accuracy.\n\n2: Given sufficient resource, which algorithm would converge to the best possible validation accuracy.\n\nMost of the papers seem to use MNIST as a benchmark. I did run on MNIST and the new algorithm does look promising. However, I want to try it out with a couple of more datasets and models so as to get an accurate -----> picture !!!  of the optimization algorithm. So, I wanted to ask if anyone from this community has a model they would like to tune for any of the dataset (not only limited to recognition) or if anyone could point me to an example of models that I could use for hyperparameter tuning? \n\nThanks! :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gmkskm/hyperparameter_tuning_experiments/"}, {"autor": "QuasiEvil", "date": "2020-05-12 14:55:07", "content": "Why does my softmax only work with a linear layer? /!/ I'm working on a problem where I'm trying to classify the 64 columns of an -----> image !!!  into one of 3 categories. The final few layers of my network look like this (everything before the Flatten layer is a standard CNN head):\n\n   \n    X = Flatten()(X)\n    X = Dense(64*3, activation='relu')(X)\n    X = Dense(64*3, activation='linear')(X)\n    X = Reshape( (64,3) )(X)\n    X = Softmax(axis=2)(X)\n\nIf I remove that Dense-linear layer, or replace the linear with relu activation, I get substantially worse classification results (train and test). Basically, my network only works if I have that linear layer ahead of the final softmax layer.\n\nI've never seen this in any tutorials/examples so I'm confused about why I apparently need it!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gicpvt/why_does_my_softmax_only_work_with_a_linear_layer/"}, {"autor": "minbh7373", "date": "2020-05-12 01:32:33", "content": "Picking up an object(Please help!) /!/ Hi I am a college freshman and our team is working on picking up this object when it is piled like the -----> photo !!!  below.\n\nhttps://preview.redd.it/85g7aeq5m8y41.jpg?width=720&amp;format=pjpg&amp;auto=webp&amp;s=49001e340f6bbccebe8726f0e2e4d5b4e5193c7d\n\nhttps://preview.redd.it/8wpa66w2m8y41.png?width=802&amp;format=png&amp;auto=webp&amp;s=19d98c6daf310dc9ab5cf2aa25847a71c3182a44\n\nWhen the photo or the video is taken above, we want the program to find out where the thing is and detect certain 3 points(like the orange circles added in the photo) constantly for picking up. [https://docs.opencv.org/master/d1/de0/tutorial\\_py\\_feature\\_homography.html](https://docs.opencv.org/master/d1/de0/tutorial_py_feature_homography.html)\n\nWe are first trying to use this tutorial as the reference which 1. Extracts features 2.Recognizes pattern using homography.\n\nThe question is,\n\n1. is this tutorial the most appropriate method for what we are trying to do? As I read through the threads I feel like the most opt method varies a lot by the assignment, but I do not have the eye for making a decision which one is for ours yet, so I will be grateful for the explanation.\n2. If this tutorial is appropriate for what we are trying to do, it seems like there are many algorithms for feature extraction like **Harris Corner Detection, SIFT, FAST, BRIEF, ORB.** Do you have any opinions on what to use for when?\n3. What kind of pattern recognization algorithms is out there for this kind of problem, and what is appropriate to use?\n\nI did ask 3 questions but I will be happy even if only question 1 is answered. As you can see I am a newbie in this field, and if you any other advice about studying this field, I will be grateful. Thanks for reading!\n\n(Also what should this kind of problem be called exactly? An object landmark detection? Object detection?)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gi19j7/picking_up_an_objectplease_help/"}, {"autor": "DynamicDumbA55", "date": "2020-05-11 21:31:41", "content": "Image Recognition / Tracking - Help with finding useful examples/information on tracking and recognizing a specific -----> image !!!  /!/ Hey all, \n\nI want to start a project but am not quite sure how to begin. I'd like to train a model that can recognize a specific pattern/image. Basically it's like finding waldo only significantly easier in this particular application. However, googling 'image recognition' doesn't yield desirable results as I do not want to develop an image classifier \\[the most common search result I come across\\].\n\nI essentially want to feed data into the model, compare it for this 'waldo', document where waldo is on the screen, and then use information to keep the waldo in frame. Any tips or tricks for this would be MUCH appreciated. I feel kind of overwhelmed from all the material I've searched/looked at so far.\n\nThank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ghwx6k/image_recognition_tracking_help_with_finding/"}, {"autor": "levelupdigital", "date": "2020-05-11 19:44:19", "content": "How to make a web tool out of my trained model? /!/ Machine learning model to web tool?\n\nHi,\nthis might seem like a strange and rather broad question but I can\u2019t seem to find anything on this topic that really helps me out.\n\nMy problem:\nI have a trained model (python) and I\u2019d like to make a web tool out of that model. (Example: classify -----> image !!! s by uploading an -----> image !!! , ...)\n\nI know how to make the model and how to train it and I can also make a web application (with simple backend - auth, database, etc)\n\nBut I don\u2019t seem to find any recourses on how to \u2018combine\u2019 that and make a ml tool. I don\u2019t even know where to start...\n\nCan somebody give sort of a roadmap or something how I can do that or learn that?\n\nThank you very much.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ghux1g/how_to_make_a_web_tool_out_of_my_trained_model/"}, {"autor": "Luca715", "date": "2020-05-11 15:47:44", "content": "Good course to take after andrew ng machine learning? Specifically the computer vision aspect. /!/ Hello, Is there a good course that explains how to do video recognition? Machine pearning by Ng explained -----> image !!!  recognition with neural networks quite well so I'm interested in seeing how it's done for videos. Is there a good course that covers this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ghq1jc/good_course_to_take_after_andrew_ng_machine/"}, {"autor": "pirwlan", "date": "2020-05-11 15:36:09", "content": "How to feed synthetic data into my CNN? /!/ Dear all, \n\nAfter taking several courses in ML/AI, I started to participate in some Kaggle competitions, and it is great. \n\nCurrently I am working on a multiclassification problem with an imbalanced dataset. One of the classes is severely under sampled. I wanted to try to create some synthetic images (horizontal/vertical/diagonal fusions of the class).\n\nBut now, I do not know how to add them in my CNN. As the dataset is too big to work with it, I am using the tf.data generator class to feed my input data into the network. This class takes a dataframe table as an input, gets the labels and paths to the -----> image !!! s, and the whole -----> image !!!  preprocessing. \n\nI divided preprocessing into two parts, first, the \"normal\" dataset undergoes part 1, then I wanted to fuse/concatenate the normal and synthetic dataset to do preprocessing part 2. After part1, both datasets have the following format:\n\n&lt;ParallelMapDataset shapes: ((400, 400, 3), (4,)), types: (tf.float32, tf.int64)&gt;\n\nBoth tf.concat and tf.stack drop the following error message:\n\nValueError: Attempt to convert a value (&lt;ParallelMapDataset shapes: ((400, 400, 3), (4,)), types: (tf.float32, tf.int64)&gt;) with an unsupported type (&lt;class 'tensorflow.python.data.ops.dataset_ops.ParallelMapDataset'&gt;) to a Tensor.\n\nHas anyone experience with tf.data and know how to add additional data? Or is there a data generator which support online data addition?\n\nBest wishes\n\npirwlan", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ghptr3/how_to_feed_synthetic_data_into_my_cnn/"}, {"autor": "MrMegaGamerz", "date": "2020-05-04 01:38:29", "content": "List index out of range.. work fine on google collab but not on local machine /!/ I'm trying to recreate [this project](https://towardsdatascience.com/tutorial-using-deep-learning-and-cnns-to-make-a-hand-gesture-recognition-model-371770b63a51) on my local machine. It's designed to run on Google Colab and I've recreated it there, and it works just fine. I want to try running it on my local machine now, so I installed all the required packages, anaconda, Juypter Notebook etc.\n\nWhen I come to the part where I process the -----> image !!! s:\n\n    # Processing label in -----> image !!!  path    category = path.split(\"/\")[3]    label = int(category.split(\"_\")[0][1])    y.append(label)   \n\nIt throws the following error:\n\n    IndexError: list index out of range   \n\nThe code has not been changed, for the most part, and the dataset is the same. The only difference is I'm running locally vs google colab. I searched online and someone said do len(path) to verify that (in my case) it goes up to \\[3\\], which it does (its size 33).\n\nCode has changed here:\n\nI did not use this line, as I'm not using google colab:\n\n    from google.colab import files  \n\nThe \"files\" is used in this part of code:\n\n    # We need to get all the paths for the images to later load them imagepaths = []  # Go through all the files and subdirectories inside a folder and save path to images inside list for root, dirs, files in os.walk(\".\", topdown=False):    for name in files:     path = os.path.join(root, name)     if path.endswith(\"png\"): # We want only the images       imagepaths.append(path)  print(len(imagepaths)) # If &gt; 0, then a PNG image was loaded  \n\nOn my local machine, I removed the \"from google.colab...\" line, and ran everything else normally. The keyword files is used in the code snippet above, however when running it I was thrown no errors, and the print(len(imagepaths)) output the correct amount (same as it did in google collab, so I assumed its fine).\n\nDoes anyone have any idea what the issue could be? I don't think it came from removing that one line of code. If it did, what do you suggest I do to fix it? Thanks!\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gd2j49/list_index_out_of_range_work_fine_on_google/"}, {"autor": "titian101", "date": "2020-05-03 05:27:29", "content": "Training a model to tranform people's age? /!/ So, FaceApp has been pretty popular of late. How do I train a model, given an -----> image !!!  outputs the older/younger version of it? I have a dataset of images of people when they were young and now when they are old. What kind of DL technique do I use ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gcl9eu/training_a_model_to_tranform_peoples_age/"}, {"autor": "fnunogomes", "date": "2020-05-02 19:12:52", "content": "Deploy custom object detector from Tensorflow Object Detection API /!/ Hi all!\n\nI am a newbie in machine learning and tensorflow and I could use your guidance in my project.\n\nI am currently working on my master's degree final thesis in which I have to develop a custom object detector and build a REST API (with Django) to be the interface between the object detector and, for proof-of-concept, an Android app.The idea of this proof-of-concept is for the user to take a -----> picture !!!  in the Android app, POST it to the API, which in turn returns the category of the detected object(s).\n\nI have already trained a custom Faster-RCNN object detector using the Tensorflow Object Detection API and exported the frozen inference graph and I have tested the model using a python script. Now, I would like to know your opinion on the best way to deploy it and to make the API \"talk\" to it. Since I have this frozen graph, can I simply include it in my project and use it like I did in the test script? I have searched about this, and didn't find anything specific to models from the Object Detection API, but I have seen it generally is done using a Docker container and Tensorflow serving. However, I know almost nothing about these tools, and couldn't really understand their purpose/usefulness in this case.\n\nAnother question i have is that right now, I can post an image to the API and call the testing script locally to obtain the detections, but I have to run the entire code for each image posted (start tensorflow, \"activate\" the gpu...) which consumes a lot of time, especially since Faster-RCNN already isn't the fastest of models. Is there a way I can make \"the service keep running\" and just make it wait to receive new images when no new images arrive?\n\nI am sorry if I was not as clear as I should, but English isn't my first language. If you think you can help me but didn't understand my problems/questions, please tell me and I will try to clarify as best as i can.\n\nThanks in advance!\n\nEdit: formatting", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gcc10y/deploy_custom_object_detector_from_tensorflow/"}, {"autor": "euqroto", "date": "2020-05-02 12:37:19", "content": "Preprocessing for CNN /!/ I am training a CNN and want to know what kinds of preprocessing does ConvNet allow me to skip? For example I know that I don't need to centre my -----> image !!!  during training to get a better loss as the filters don't care if the -----> image !!!  is in the centre or not. Just like this I skipped upscaling some photos in training(128) and got a great result validation result for 480p. Why did this happen? And what other preprocessing can be skipped because of this great convolutions which are so helpful.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gc5r8l/preprocessing_for_cnn/"}, {"autor": "Legnaru", "date": "2020-05-01 23:09:22", "content": "Looking for learning about GAN /!/ As the title says, I would like to learn about GAN (and whatever structure that derives from it) in order to do projects like improving -----> image !!!  resolution. Any type of resource is welcome (books, papers, online courses, youtube videos, etc) and I hope this thread will help other people too!\n\nThank you in advance!\n\nPS: I have started by the section about GANs in \u201cDeep Learning with Python\u201d by Francois Chollet", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gbslee/looking_for_learning_about_gan/"}, {"autor": "madzthakz", "date": "2020-10-05 00:31:55", "content": "[X-Post] IAMA Senior Data Scientist at Disney and I\u2019m setting up free Q&amp;A Zoom sessions to help people who are looking to enter/transition into data science/machine learning /!/ DISCLAIMER: This is completely free and not sponsored in any way. I really just enjoy helping folks get into Data Science\n\nSome of you may have already seen this on the data science subreddit but I wanted to post regardless for those who haven\u2019t. I\u2019m a Senior Data Scientist at Disney and I\u2019ve had a bit of an unorthodox path into this field and learned a few things along the way. I\u2019ve been trying to help aspiring data scientists (and MLEs) get started by answering their questions via Zoom Q&amp;As and our first few sessions have gone really well! \n\nWe\u2019re planning to continue hosting these sessions every month and we\u2019ll be posting Q&amp;A content (recordings, articles, etc) on our blog and will continue to do so regularly. They can be found here:\n\n1. [https://www.madhavthaker.com/qaposts](https://www.madhavthaker.com/qaposts)\n\nTo make sure you\u2019re included in any future sessions, please sign up using the following [form](https://forms.gle/Qs333FLRahxY6vSu7). The extra questions on there help us create content for future sessions.\n\nHope to see you all soon!\n\nVerification:\n\n* My -----> photo !!! : [https://imgur.com/a/Wg3DMLV](https://imgur.com/a/Wg3DMLV)\n* My LinkedIn: and the [https://www.linkedin.com/in/madhavthaker/](https://www.linkedin.com/in/madhavthaker/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j59udv/xpost_iama_senior_data_scientist_at_disney_and_im/"}, {"autor": "Avi_TheCodex", "date": "2020-10-04 21:58:31", "content": "A List of 100+ Random Python/DataScience/MachineLearning Project Ideas /!/ Hey guys!\n\nI know it's currently quarantine for most people, recruiting season for students/graduates, but also just a good time to keep up with coding and learning new things. I love projects because I think they're the best way to apply what you've learned and also create something relevant and functional to you-- plus they're great resume/portfolio builders. We know that sometimes it's hard to get come up with ideas or it's just better to start small. Check out this list of more than a 100 Python projects that I compiled on topics such as web development, AI/ML, data science etc. to get inspired and start building!\n\n[https://docs.google.com/spreadsheets/d/1X3\\_hWF4oPKqwi\\_sDrryTmt2s23TbE1yXbesmjB-IrLg/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1X3_hWF4oPKqwi_sDrryTmt2s23TbE1yXbesmjB-IrLg/edit?usp=sharing)\n\nA preview of some of the DS and AI/ML ideas:\n\n1. Movie/TV Show/Music/Book Recommenders with K-Means Clustering\n2. Face Detection using Optical Character Recognition\n3. Sentiment Analysis of Customer Feedback/Reviews (or Reddit Threads)\n4. -----> Image !!!  Caption Generator using CNN\n5. Product Prices Estimates with ML\n6.  Rainfall Prediction with Logistic Regression\n7. Twitter News Detection with Na\u00efve Bayes Classifier \n8.  Explore Datasets with [\\#numpy](https://twitter.com/hashtag/numpy?src=hashtag_click), [\\#scikitlearn](https://twitter.com/hashtag/scikitlearn?src=hashtag_click),  [\\#tensorflow](https://twitter.com/hashtag/tensorflow?src=hashtag_click) and more \n9. Baby Name Generator using [\\#DeepLearning](https://twitter.com/hashtag/DeepLearning?src=hashtag_click) \n10.  Recipe Recommendation using K-Means Algorithm \n\nThanks,\n\nAvi", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j57gxa/a_list_of_100_random/"}, {"autor": "lil_kriszxdxd", "date": "2020-10-04 16:39:12", "content": "Adversarial sticker proposal /!/ Hi,\n\nIm generally interested if there is already an algorithm/model that can propose adversarial stickers(where to put them on an -----> image !!! , with what size)  based on supervised data aka already generated adversarial sticker samples. I have a few idea on wherw to go, but would be intrested if any of you have stumbled across the same problem, and can give me some sources to study on the topic.\n\nCheers", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j51xhs/adversarial_sticker_proposal/"}, {"autor": "Pawan315", "date": "2020-10-03 14:20:45", "content": "-----> Image !!!  Upscaling in less than a second full step by step tutorial proSR", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j4ft8p/image_upscaling_in_less_than_a_second_full_step/"}, {"autor": "CitationNotNeeded", "date": "2020-10-02 21:21:52", "content": "Backpropagation over a volume in convolutional neural network layers? /!/ If I have a colour -----> image !!!  (input 1) with three channels and I input it into a convolutional layer (layer 1) that has two filters, the two filters would have to have their own unique three channels to be convolved with the three channels of the -----> image !!! . \n\n\nFilter 1 channel 1 is convolved with input 1 channel 1. This gives filter 1 output 1.\n\nFilter 1 channel 2 is convolved with input 1 channel 2. This gives filter 1 output 2.\n\n\nFilter 1 channel 3 is convolved with input 1 channel 3. This gives filter 1 output 3.\n\nFilter 2 channel 1 is convolved with input 1 channel 1. This gives filter 2 output 1.\n\nFilter 2 channel 2 is convolved with input 1 channel 2. This gives filter 2 output 2.\n\nFilter 2 channel 3 is convolved with input 1 channel 3. This gives filter 2 output 3.\n\n\n\nThe filter outputs are summed together and passed on to the inputs of convolutional layer 2. Since the image was \"input 1\"; let's simply refer to layer 2's input as \"input 2\"\n\nFilter 1's outputs 1, 2 and 3 are summed together. This becomes input 2 channel 1.\n\nFilter 2's outputs 1, 2 and 3 are summed together. This becomes input 2 channel 2.\n\n\n\n\nSuppose layer 2 also has two filters:\n\n\n\n\n\n\nFilter 1 channel 1 is convolved with input 2 channel 1. This gives filter 1 output 1.\n\nFilter 1 channel 2 is convolved with input 2 channel 2. This gives filter 1 output 2.\n\nFilter 2 channel 1 is convolved with input 2 channel 1. This gives filter 2 output 1.\n\nFilter 2 channel 2 is convolved with input 2 channel 2. This gives filter 2 output 2.\n\n\n\nThe outputs are summed together again and sent to the next layer:\n\nFilter 1's outputs 1 and 2 are summed together. This becomes input 3 channel 1.\n\nFilter 2's outputs 1 and 2 are summed together. This becomes input 3 channel 2.\n\n\n\n\nDuring backpropagation, layer 2 will receive gradient matrices from the backpropagation of later stages of the neural network (pooling, relu, softmax, etc). Since layer 2 gave two outputs (input 3 channel 1 and input 3 channel 2), it will receive two gradient matrices. One for each filter. Each filter's channel has weights that need to be updated by using the derivative of the filter weights w.r.t the corresponding input (multiplied by the gradient that the filter received). This derivative is simply the convolution of the received gradient with the respective input channel. Let's call these the \"results\". \n\n\n\n\nLayer 2 Gradient 1 is convolved with input 2 channel 1 to update the weights of filter 1 channel 1. This is result 1.\n\nLayer 2 Gradient 1 is convolved with input 2 channel 2 to update the weights of filter 1 channel 2. This is result 2.\n\nLayer 2 Gradient 2 is convolved with input 2 channel 1 to update the weights of filter 2 channel 1. This is result 3.\n\nLayer 2 Gradient 2 is convolved with input 2 channel 2 to update the weights of filter 2 channel 2. This is result 4.\n\n\n\nWe have four results and need to do backpropagation for layer 1. Layer 1 only has two filters that each had three channels. Would result 1 and 3 be added together and sent as layer 1's gradient 1? With result 2 and 4 then added together and sent as layer 1's gradient 2? \n\n**tldr:**\nDuring back propagation: Layer 2 with its 2x 2-channel filter volumes will receive a gradient matrix for each filter. This backprop convolution outputs four results. How does this get back propagated to layer 1's 2x 3-channel filters to update the weights?\n\nIf this is the wrong place to ask, my apologies. Please point me in the right direction. Stack overflow thinks my text is spam.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j43ail/backpropagation_over_a_volume_in_convolutional/"}, {"autor": "spmallick", "date": "2020-10-01 06:05:28", "content": "Bag of Tricks for Image Classification /!/ What do you do when your -----> image !!!  classification model fails to produce the results you want?  \n\n\nToday, we are going to learn a bag of tricks you can use to improve the accuracy of your network. Here is what you will learn.  \n\n\nTrick #1: Large Batch Training  \nTrick #2: LR Warm-up  \nTrick #3: Mixed Precision operations  \nTrick #4: Cosine LR Decay  \nTrick #5: Label Smoothing  \nTrick #6: Knowledge Distillation  \nTrick #7: Mix-up Augmentation \n\n[https://www.learnopencv.com/bag-of-tricks-for-image-classification/](https://www.learnopencv.com/bag-of-tricks-for-image-classification/)\n\nhttps://preview.redd.it/ry0h67nbcfq51.jpg?width=600&amp;format=pjpg&amp;auto=webp&amp;s=fa5d5e9b203cb7fd82e476c81b79a16a90d697b9", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j3311d/bag_of_tricks_for_image_classification/"}, {"autor": "khaledskhaled", "date": "2020-10-01 05:16:47", "content": "Need help with my ML assignment /!/ Hey everyone. I'm taking a machine learning graduate elective course in my Architecture degree. And boy let me tell you I'm having a hard time! It's an interesting class, but unfortunately the professor (nice guy honestly) is not the best when it comes to teaching and explaining, and I am spending a lot (like a lot) of time watching YouTube videos just to get a glimpse of what the hell is going on. \n\nFirst assignment went with mercy, and now the second one is painful. Here's an -----> image !!!  of the [Assignment](https://imgur.com/rjgDpgp) and here's the [link](http://archive.ics.uci.edu/ml/datasets/credit+approval) for the dataset we are using. Also here is the [example](https://github.com/Amirosimani/Machine_Learning_Pratt/blob/master/labs/lab_03.ipynb) he gave us to understand. The features in the credit dataset are anonymous. \n\nThe things I have no problem with is loading dataset, cleaning + replacing missing values, anything related to basic exploring.\n\nMy problem is how can I find relations between features so I can move on and build a training module, and then test it? I watched several videos but it seems I don't know which technique to use or method.\n\nI promised myself not to use reddit, but this mofo left me no choice but to resort to reddit. It's due next Tuesday and I'm losing hope.\n\nAny guidance or help is appreciated", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j32fe4/need_help_with_my_ml_assignment/"}, {"autor": "Bitches5", "date": "2020-09-05 06:36:43", "content": "Solve this /!/ You are given a -----> image !!!  of 127x127x3. You add a convolutional layer with 32 kernels of size 7x7 with stride 4 and padding 0. What will be the size of output?\n\nA) 30X30X32\n\nB) 31X31x32\n\nC) 32x32x30\n\nD) 30X31X32", "link": "https://www.reddit.com/r/learnmachinelearning/comments/imwh4b/solve_this/"}, {"autor": "lifesaboxofchoco", "date": "2020-09-05 04:41:30", "content": "Finding the root for a linear regression model with RMSE as cost function /!/ I am aware that for a more complicated model finding the root algebraically is not feasible when you have many samples and that finding the global minima might not be best because it could lead to overfitting, which is why we use gradient descent.  \n\n\nBut I just want to ask the following question about linear regression with RMSE to make sure my maths understanding is correct:  \nI am currently takes a maths course in coursera and it says that the partial derivative for a linear regression with RMSE as cost function would look like the -----> image !!!  below. Since it just looks like a 1st order polynomial to me, the root should be quite easy to find algebraically right?\n\nhttps://preview.redd.it/lq69c87gc9l51.png?width=1566&amp;format=png&amp;auto=webp&amp;s=325861e53673640599ab317cf81ff9353353bf43", "link": "https://www.reddit.com/r/learnmachinelearning/comments/imv4fh/finding_the_root_for_a_linear_regression_model/"}, {"autor": "Deepit_Shah", "date": "2020-09-04 12:00:50", "content": "TFLITE APP Help /!/ Hello,\nI have created an -----> image !!!  classification app and successfully built the project. Everytime I run the app, it doesnt display the labels n probability of the images to be classified. I have change the labels.txt file wherever required. Along with this, if I wish to publish my tflite project on Google play store then is it okay? Since it's a github code for the app template.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/imemv6/tflite_app_help/"}, {"autor": "tankado95", "date": "2020-09-03 13:48:28", "content": "Siamese Network: how to make predictions /!/ Hey There, I'm trying to understand how the Siamese Network works. I understood the architecture of this model but I can't get how to make predictions.  \nSuppose that I have trained the model and then I want to use it to make predictions, I could have an -----> image !!!  and 50 possible classes.   \nDo I have to input in the network the image paired with an image for each of the classes and produce 50 outputs?   \nI guess that this testing method could be a problem because I should do a lot of comparisons but I still can't understand how to do the predictions.  \n\n\nThank you  \n\n\nP.S. Can you explain some possible real use case for this network?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iltpxo/siamese_network_how_to_make_predictions/"}, {"autor": "py_ml", "date": "2020-09-03 13:04:35", "content": "How do you deal with situations where you need to predict small objects in noisy environment? /!/ I am looking at detecting cracks in concrete. It's relatively easy to do it with some datasets where images are just concrete with or without cracks. However, in real situation there is a lot of \"noise\", additional objects like shadows, wires, etc and in -----> picture !!!  there is only some part of concrete and the other parts are vegetation, sky or any other object. What are your thoughts how to start solving such problem:\n\n1. concentrate on your custom dataset gathering;\n2. decide on method type (classification, object detection, segmentation), what type would you recommend?\n3. any other thoughts?\n\nLet's discuss other situations where you've been solving similar ML problem.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ilszt3/how_do_you_deal_with_situations_where_you_need_to/"}, {"autor": "DarthChicken89", "date": "2020-10-11 11:56:02", "content": "Hand gesture recognition /!/ The final -----> picture !!!  I have in my mind is a leap motion like controller, the input to the model would be through a webcam and I want the model to classify the hand gestures in the video input. \n\nI did some googling and came across many examples of problems similar to this, and it was a bit overwhelming. I want a simplified overview of the type of models used for this type of application.\n\nI saw some examples that classify static hand poses like rock paper scissors but I want one that classifies things like swipes, pinch to zoom, etc.\n\nI also came across the jester dataset but I've no idea how training on short video clips translates to a real time application.\n\nThank you", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j9453h/hand_gesture_recognition/"}, {"autor": "Angry_Triceratops", "date": "2020-10-11 10:57:44", "content": "Using deep learning for capture/recapture studies of salamanders /!/ Hi all!  I am taking my first steps in the universe of machine learning, watching youtube videos and experimenting in R (using Keras). My ultimate goal would be to script a neural network that could be used in capture/recapture studies in fire salamanders (Salamandra salamandra), a species of which every individual has a unique dorsal pattern. The goal would be: you take a -----> picture !!!  of a salamander and you get an ID for the animal which is then stored together with the coordinates of the -----> picture !!! , adding a new record to the database. \n\nNow, what I am struggling with is constructing an initial database. It is near impossible to draft a training dataset, in other words, to curate every salamander picture by eye and give each one an ID (a population can contain hundreds of indivuals). Is there a way to circumvent this? To group the salamander pictures based on their pattern? I hope I am making my question clear.\n\nHave a nice day!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j93gg2/using_deep_learning_for_capturerecapture_studies/"}, {"autor": "aeonax", "date": "2020-10-11 08:09:33", "content": "Is it possible for a system to imagine something? Given certain inputs? /!/ I'm trying out Unity ML-agents. I am a noob in ML.\n\nI created an environment that consists of the following:\n\n* A Box containing a cube, light and a -----> camera !!! .\n* Camera is at certain distance from the center, always looking at the center but can have any rotation.\n* Cube and light can be placed anywhere in the box\n\nInput to the learning system is the following\n\n* Position and rotation of the cube\n* Position of the light\n* Rotation of the camera\n\nExpected output\n\n* monochrome image as seen by the camera\n\nWould it be possible for the machine to learn what I am trying to make it learn?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j91ko4/is_it_possible_for_a_system_to_imagine_something/"}, {"autor": "Tyron_Slothrop", "date": "2020-10-11 03:46:24", "content": "CNN -----> Image !!!  Features Inquiry /!/ From what I understand, an image is fed into a CNN; from there, features are extracted from the pooling and convolutional layers--simplifying the image. What I'm  confused on is whether the entire image is featurized (not a real word); in other words, is the entire image broken into separate features that continue through the CNN layers, or are only a select few features extracted? Is this process completed for every single image as input, or is the entire corpus of input images stored in each subsequent layer?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j8yf10/cnn_image_features_inquiry/"}, {"autor": "robrandom", "date": "2020-10-09 20:05:35", "content": "Advice on a Project that summaries text /!/ Hello everyone,\n\nI am undertaking my final year undergraduate project. For my project I hope to create a system that's summaries a paragraph that a user takes a -----> picture !!!  of. \n\nHere are my 2 plans of action so far\n\nMy first idea is for an app where a user takes a photo of a paragraph and use an API such as Google's cloud vision API for OCR and then feed that String of text to a neural network where I score each sentence and output whatever ones pass a threshold.\n\nMy second idea is for the user to highlight sections they find important and then input an image to a neural network and output the highlighted words. I'm thinking of edge detection to detect where the highlighted words are but I feel that it would be to prone to errors\n\nIf you have any recommendations or know of any resources it would be extremely helpful.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j868vs/advice_on_a_project_that_summaries_text/"}, {"autor": "iim7_V6_IM7_vim7", "date": "2020-10-09 19:21:22", "content": "In Keras, when using flow_from_dataframe, can I keep train and test -----> image !!!  files in same directory? /!/ I'm building a CNN with Keras and have one directory with my image dataset. I plan on using flow_from_dataframe to pull them into ImageDataGenerator and I was hoping if I randomly assign items in the dataframe as belonging to either the train or test set, I don't have to keep the images in separate train and test folders.\n\nDoes that question make sense? Does anyone know if it'll work this way? \n\nThanks so much!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j85fff/in_keras_when_using_flow_from_dataframe_can_i/"}, {"autor": "user_--", "date": "2020-10-09 15:44:21", "content": "Pre-trained networks/transfer learning and their limitations? /!/ I want to get an idea of what's possible with pre-training and fine-tuning networks.\n\nLet's say I have a set of -----> image !!! s of 3 woodland animals (deer, coyotes, squirrels), all with similar backgrounds (woods), and I train a network to take in the -----> image !!!  and return the probabilities that the -----> image !!!  contains each of the three animals. Then, I obtain a new set of images of foxes in the woods, and I want to have the network return the probabilities for each of the four animals.\n\nIs there a way to take the network pre-trained on three animals and train it further to include the fourth, without retraining the whole thing? If so, how? And what is this method called? Furthermore, would I require more/fewer/the same number of fox images as the number of images for the other animals?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j81c9f/pretrained_networkstransfer_learning_and_their/"}, {"autor": "MichaelBaranov", "date": "2020-10-09 14:09:00", "content": "No-code AI platform? /!/ Hey there,\n\nA group of like-minded people and I are working on a no-code AI platform. Our goal is to make machine learning technologies accessible to people without any specific knowledge. \n\nThe workflow would look like the following:\n\n1. Select a task and data types.\n2. Upload data (with annotation or annotate it in our platform).\n3. Wait for a model to be trained automatically (up to 30-40 minutes).\n4. Use the created model (through API, through integration with other no-/low-code platforms, or as a docker -----> image !!! ).\n\nWhat do you think about our idea? Would anybody use it?  \nI'd really appreciate it if you could share your thoughts.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j7zm0y/nocode_ai_platform/"}, {"autor": "LZDM", "date": "2020-10-09 04:50:24", "content": "Help understanding NLP input /!/ Can anyone help me understand how language models handle variable input lengths? In computer vision, there\u2019s generally a set input size (e.g. -----> image !!!  dimensions and channels are the same for any input sample), but for NLP models I would assume this is not the case because a sentence could be anywhere from 2 words to N words. Do NLP models always narrow down input to a set number of grams? I have a decent amount of computer vision experience but no NLP experience aside from random reading.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j7sucg/help_understanding_nlp_input/"}, {"autor": "kimtaengsshi9", "date": "2020-10-08 20:58:51", "content": "Is this the correct way to implement k-fold cross-validation in neural network training? /!/ I'm trying to implement a visual attention model for -----> image !!!  captioning, following an example I found on Kaggle, but I want to implement k-fold cross validation to improve the training performance. Is this pseudo-code the correct approach?\n\n[https://pastebin.com/0ev7c2w8](https://pastebin.com/0ev7c2w8)\n\ntldr: \n\n1. For each fold, generate train-validation data.\n2. Train model\n3. Append this fold's loss and weights to this epoch's array variables\n4. Reinitialise weights for next fold, repeat from step 1\n5. After all folds completed, identify the iteration with the best validation loss\n6. Load that iteration's weights for the next epoch\n7. If this epoch's best loss is better than the all-time best, save these models to file\n8. Increment curr\\_epoch count, check for stop condition\n9. If don't stop training, repeat from step 1\n10. If stop training, load last best model from file and evaluate against test data\n11. Deploy this model for production", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j7ldva/is_this_the_correct_way_to_implement_kfold/"}, {"autor": "Bademeister95", "date": "2020-10-08 19:06:37", "content": "Need help in finding the right Approach to solving Object Detection in Greyscale Images. /!/ Hello, i am a student currently writing my masters thesis. The subject of my thesis will be about locating standart euro pallets in a 2D greyscale -----> image !!!  obtained by using a Lidar Sensor. The Images will look something like the one attached to this post. In search for a solution i came across different methods e.g. Yolo or Faster RCNN.   \n\n\nHowever it feels like these Networks are somewhat of an overkill for this Problem.   \nI would love to talk to someone who is familiar in the area of object detection and has some time to answer one or two questions.   \n\n\nThanks in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j7j9h2/need_help_in_finding_the_right_approach_to/"}, {"autor": "NitinChella", "date": "2020-01-27 10:05:37", "content": "Help understanding Keras Code. /!/ Hi guys, the following code is from the Tensorflow in Practice specialization on Coursera. I am running the code on colab. I'm not able to understand some of the code.\n\nCODE:\n\nimport numpy as np\nfrom google.colab import files\nfrom keras.preprocessing import -----> image !!! \n\nuploaded = files.upload()\n\nfor fn in uploaded.keys():\n \n  # predicting -----> image !!! s\n  path = '/content/' + fn\n  img = -----> image !!! .load_img(path, target_size=(300, 300))\n  x = image.img_to_array(img)\n  x = np.expand_dims(x, axis=0)\n\n  images = np.vstack([x])\n  classes = model.predict(images, batch_size=10)\n  print(classes[0])\n  if classes[0]&gt;0.5:\n    print(fn + \" is a human\")\n  else:\n    print(fn + \" is a horse\")\n\nEND CODE\n\nMy doubts are:\n1. What is the use of np.expand_dims when when img_to array automatically adds a dimension for the channel?\n2. What is the use of np.vstack?\n\nI'm really confused. Any help appreciated. Thanks in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eulzlm/help_understanding_keras_code/"}, {"autor": "uqw269f3j0q9o9", "date": "2020-01-26 17:30:03", "content": "What are recommended ways of hosting a simple ML based web application? /!/ I've written an -----> image !!!  generator in Pytorch that I interact with using a web browser. The serving part is handled by Flask and it's all run in a docker container.  \n\nSince I'm using a Mac without a proper GPU, I would like to upload the code/model to some server and do the serving and processing there, but I'm pretty clueless about which service to choose and from which company, so any help regarding this would be appreciated.  \nAlso, would it then be possible to somehow share the hosted page with someone else other than me?\n\nThank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eu9p8y/what_are_recommended_ways_of_hosting_a_simple_ml/"}, {"autor": "hellopaperspace", "date": "2020-01-26 15:55:40", "content": "Tutorial: -----> Image !!!  Captioning with Keras /!/  [https://blog.paperspace.com/image-captioning-with-ai/](https://blog.paperspace.com/image-captioning-with-ai/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eu8dtf/tutorial_image_captioning_with_keras/"}, {"autor": "insanelylogical", "date": "2020-01-25 01:35:05", "content": "Theories on why transfer learning sometimes gives worse results than training from scratch. /!/ A lot of people I have talked to seem to think that you should always use transfer learning if a decent pretrained model is available. They say there is no harm doing it so might as well.\n\nIn my field (medical imaging) CNNs like those trained by Google on ImageNet are used ubiquitously. While it makes sense in the case where there is a small amount of -----> image !!!  data, I am not sure it's always a good idea when you have lots of data.\n\nI've seen a handful of cases, including my own, where using a model pretrained on ImageNet actually gives worse results than training from randomly initialized weights. Even if all they layers are trainable.  \nI admit, I am not too sure why this happens. Images such as tissue biopsy slides are very different from the images in ImageNet, but one would think that some of the very early layers would still be useful. Like those that detect simple lines and edges.\n\nThis leaves me to conclude that even when a state of the art model that is pretrained on a gold standard dataset like ImageNet still might not be beneficial if the dataset is too different, and can even be harmful. Possibly because the function shape is to difficult to reshape. Thoughts?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/etk6l1/theories_on_why_transfer_learning_sometimes_gives/"}, {"autor": "Hollowax", "date": "2020-01-24 12:25:12", "content": "CNN training extremely slow with 8gb RAM /!/ I usually have 16 gb of RAM, but while making a dual boot system i saw that one RAM stick was broken so I had to take it out. After I set up my system I wanted to train again yn -----> image !!!  recognition with pytorch but it was veeeery slow (in comparison with 16gb). Im just training 350, 475px images and chaining the batch size has almost no influence to speed it up. So my question:\n\nHas the ram such an influence to the speed?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/et9qa0/cnn_training_extremely_slow_with_8gb_ram/"}, {"autor": "terribleprogrammer12", "date": "2020-07-31 18:24:02", "content": "What is the correct way to clean -----> image !!!  datasets? /!/ Sorry if this sounds like a mini- ranting, but I am frustrated.\n\nI have take the Google, Kaggle, and Fastai course and I have had no problem using example datasets. Give me the data and I can build a model from scratch that converges, or bottom out the loss on a pretrained model.\n\nHowever it seems like every tutorial doesn't address how to use real world data where everything isn't need a neat box. Even if I use datasets like LFW, the results are disappointing.\n\nI recently created a model with 3 classes and 4000 images. Regardless of the optimizer, weights, model, ect I would not get validation loss lower than a coin flip. My typical thought process is to write my own program, have it fail, and go back to the tutorials thinking I did something wrong. However, this isn't something that anybody addresses. It is sort of glossed over.\n\nWhen I am cleaning dataset I either underfit with a lot of data, or weed so many images that my model won't generalization. I would appreciate some pointers.\n\nThanks.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i1cogn/what_is_the_correct_way_to_clean_image_datasets/"}, {"autor": "19Summer", "date": "2020-07-31 11:58:36", "content": "How do you usually treat features that are vectors themselves? /!/ Greetings.  \nImagine there is a task related to automatic vehicle plate recognition of cars, which break traffic rules.    \n\n\nDataset contains features such as 'id of a -----> camera !!! ', 'timestamp' and so on.   \nLabel, that is to be predicted, is a binary value, displaying whether a camera recognized a vehicle plate number correctly or human interaction is required.   Cameras sometimes recognize 'Z' as 2, '0' as o, '5' as s and etc.   \n\n\nAmong features there are two, which significantly differ from others, because they are actually vectors.  \nOne feature is a list of symbols, which constitute the plate number. For instance, if a car's plate number is GYI2RZB then this record has \\['G', 'Y', 'I', '2', 'R', 'Z', 'B'\\] in this feature. Another feature shows how confident(probability) algorithm is in each symbol, so for the aforementioned plate number the probabilities are next \\[0.99, 0.99, 0.97, 0.8, 0.99, 0.7, 0.99\\].   \n\n\nHow do you usually treat such features? \n\nThanks in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i169jd/how_do_you_usually_treat_features_that_are/"}, {"autor": "aagnone", "date": "2020-07-30 19:29:07", "content": "[D] Life With Data ML UTD #7 /!/ I took way too long to get this next one out the door, but here we go -- it's #7 of the Machine Learning Up-To-Date series!\n\n## Software\n\n* [Beating Atari Pong on a Raspberry Pi Without Backpropagation](https://www.lifewithdata.org/newsletter/mlutd7#beating-backprop)\n* [Disappearing People Project](https://www.lifewithdata.org/newsletter/mlutd7#disappearing-people)\n* [Make a Renaissance Photo of Yourself](https://www.lifewithdata.org/newsletter/mlutd7#renaissance------> photo !!! )\n* [Overview of tinyML](https://www.lifewithdata.org/newsletter/mlutd7#tinyml-overview)\n\n## Academia\n\n* [Generating Music in the Waveform Domain](https://www.lifewithdata.org/newsletter/mlutd7#generate-music-waveform-domain)\n* [A Visual Guide to Evolution Strategies](https://www.lifewithdata.org/newsletter/mlutd7#visual-evo-strategies)\n* [Connected Papers](https://www.lifewithdata.org/newsletter/mlutd7#connected-papers)\n\n## Industry\n\n* [Why We Need DevOps for ML Data](https://www.lifewithdata.org/newsletter/mlutd7#why-devops-ml)\n* [How to Hire Machine Learning Engineers](https://www.lifewithdata.org/newsletter/mlutd7#hiring-ml-eng)\n* [Monitoring Machine Learning Models in Production](https://www.lifewithdata.org/newsletter/mlutd7#monitoring-prod-ml)\n\nCheck it out [here](https://www.lifewithdata.org/newsletter/mlutd7).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i0sgi3/d_life_with_data_ml_utd_7/"}, {"autor": "canarysplit", "date": "2020-07-29 12:04:25", "content": "If the intercept is 0, and slope of the curve is -5.7, why does the equation looks like this when plotted? /!/  \n\nI'm following the tutorial that is explaining Gradient Descent and now I came to the part where the derivation of the sum of the squared residuals is being explained. I've plotted the quadratic function which represents all the calculations of the sum of the squared residuals for different intercepts.\n\nHowever, when I'm trying to plot the same line as them that is a linear function I'm not able to do it?\n\nI'm plotting the function f(x) = -5.7\\*x + 0 as these values are implied from the -----> image !!! . When I plot this function I get something different. How can I get this line plot that is this image?\n\nhttps://preview.redd.it/c72cmv80esd51.png?width=1604&amp;format=png&amp;auto=webp&amp;s=03239ab0c08bbbd27778a95df0a19a3e127514eb\n\nSource (9:50 - [https://www.youtube.com/watch?v=sDv4f4s2SB8](https://www.youtube.com/watch?v=sDv4f4s2SB8))", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hzziow/if_the_intercept_is_0_and_slope_of_the_curve_is/"}, {"autor": "ThisVineGuy", "date": "2020-07-29 11:47:29", "content": "DeepFaceDrawing Generates Real Faces From Sketches. -----> Image !!! -to-image translation in 2020+, is it biased, could it be used in a real world application? Paper explained", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hzzaps/deepfacedrawing_generates_real_faces_from/"}, {"autor": "spmallick", "date": "2020-09-25 15:23:05", "content": "Simple Background Estimation in Videos using OpenCV (C++/Python) /!/ In today's post, we will go over a real-life application of OpenCV that is especially helpful when the processing power at your disposal is low or you have a static -----> camera !!!  and moving object.  \n\n\n![video](k7pumnpbabp51)\n\nFind details of the blog - Simple Background Estimation in Videos using OpenCV - with code at [https://www.learnopencv.com/simple-background-estimation-in-videos-using-opencv-c-python/](https://www.learnopencv.com/simple-background-estimation-in-videos-using-opencv-c-python/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/izlfjs/simple_background_estimation_in_videos_using/"}, {"autor": "abrasaldo", "date": "2020-09-23 16:52:44", "content": "Tensorflow Object Detection api get object position and send it to Arduino /!/  \n\nHello I'm new to Tensorflow Object Detection api, and I'm sorry for the noob question, so my question is how can I get the position or the x and y of the object the has been detected in the -----> camera !!! , so I am planning to create a software that will follow or center the object detected by the -----> camera !!!  that has been mounted to the 2 servos and it will be controlled by arduino, so basically the x &amp; y position of the object detected will be send to the arduino via serial communication, and it will control the 2 servos to center the object by using the x &amp; y that been sent by the software\n\n    with detection_graph.as_default():\n            with tf.Session(graph=detection_graph) as sess:\n                while True:\n                    ret, image_np = cap.read()\n                    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n                    image_np_expanded = np.expand_dims(image_np, axis=0)\n                    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n                    # Each box represents a part of the image where a particular object was detected.\n                    boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n                    # Each score represent how level of confidence for each of the objects.\n                    # Score is shown on the result image, together with the class label.\n                    scores = detection_graph.get_tensor_by_name('detection_scores:0')\n                    classes = detection_graph.get_tensor_by_name('detection_classes:0')\n                    num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n                    # Actual detection.\n                    (boxes, scores, classes, num_detections) = sess.run(\n                        [boxes, scores, classes, num_detections],\n                        feed_dict={image_tensor: image_np_expanded})\n                    # Visualization of the results of a detection.\n                    vis_util.visualize_boxes_and_labels_on_image_array(\n                        image_np,\n                        np.squeeze(boxes),\n                        np.squeeze(classes).astype(np.int32),\n                        np.squeeze(scores),\n                        category_index,\n                        use_normalized_coordinates=True,\n                        line_thickness=5)\n        \n                    cv2.imshow('object detection', cv2.resize(image_np, (600, 400)))\n                    if cv2.waitKey(25) &amp; 0xFF == ord('q'):\n                        cv2.destroyAllWindows()\n                        break", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iydy5m/tensorflow_object_detection_api_get_object/"}, {"autor": "Gameatro", "date": "2020-09-23 14:34:09", "content": "Need help understanding a Auto -----> Image !!!  Colorization research paper /!/ I am having trouble understanding implementation of a research paper. [Paper](https://drive.google.com/drive/folders/1fDtg_camiWpe6R-0A7jEk3SV32pEZ7vL)\n\nI need assistance in the **3.6 Final Classification Model** on page 3. How exactly should the pixels be discretized into 50 bins? Also, how will the model behave as classification model with 50 classes? what will be the shape of the output? Can someone please help me understand the model?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iyb9ow/need_help_understanding_a_auto_image_colorization/"}, {"autor": "learn2m", "date": "2020-09-23 13:02:11", "content": "[LONG] Seeking Advice in Applying ML to a real life problem /!/ Hey guys,\n\nI've posted this question in /r/machinelearning's Simple Questions Thread, but didn't get an answer there, so I am reposting it here (with some extra info).\n\nI have a process at my work which could GREATLY benefit from application of Machine Learning. However, I've been racking my brain on how to actually do it, and whether or not I am understanding the entire flow of applying ML to a problem correctly. Anyhow, here is what is happening now:\n\n* We intake -----> image !!!  data that has a lot of noise\n* We de-noise the -----> image !!!  (first set of parameters that need optimization here)\n* Pass de-noised -----> image !!!  to another process that does -----> image !!!  recognition (second set of parameters for optimization), and it produces text data\n\nThose two sets of params are completely independent. De-noising is done in Python, and recognition is in another language.\n\nThis sounds like a simple setup for a two-layer network, with each process representing a layer. Here is where I am not sure if I am correct. From reading some other examples/projects I see that folks add layers into a single \"process\", so perhaps I am misunderstanding the concept here, please feel free to correct me!\n\nI am using (or rather, attempting to use) PyTorch, and was able to run this example (https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/). I assume, that in this definition of the forward step:\n\ndef forward(self, X):\n    X = self.hidden1(X)\n    # De-noising\n    X = self.act1(X)\n\n    X = self.hidden2(X)\n    # Recognition\n    X = self.act2(X)\n\n    return X\n\n\nThe reason why we need this type of automatic optimization is due to the other set of parameters, which are physical, and we are iterating through them slowly, but we need a quick way of seeing whether they are better than previous ones or not. Basically, we set physical params, run through a bunch of tests (this is the bit I am trying to build now with help of PyTorch), see if we can get a good accuracy, and then move on to the next set. And at the end of trials we will pick the best set of physical parameters and whatever PyTorch came up with in terms of software parameters. I hope this makes sense. I can elaborate more in private, but I am keeping this vague on purpose.\n\nDoes this approach sound correct? This is my first attempt to use something like this, so I am quite unsure about this.\n\nIf you have any materials that are pertinent -- I'd be glad to read up. If I am not clear in anything here -- please feel free to ask!\n\nThank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iy9nd6/long_seeking_advice_in_applying_ml_to_a_real_life/"}, {"autor": "cloud_weather", "date": "2020-08-29 14:24:40", "content": "-----> Image !!!  Decomposition AI - Edit Highlights and Textures Easily", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iit7pq/image_decomposition_ai_edit_highlights_and/"}, {"autor": "Debate_Apprehensive", "date": "2020-08-29 07:32:41", "content": "Why does balancing classes not ruin data expectations? A mind-blowing question /!/ When calculating the loss function, you use an expectation over your data distribution of the model's log likelihood in order to get a more accurate -----> picture !!!  with less variance. \n\nHowever, we also frequently balance classes so that we sample them evenly. The problem is that this changes the distribution you are taking the expectation over, since you are no longer drawing samples from your data distribution but a different one.\n\nIf your dataset has 90% 0s and 10% 1s, but then you sample 50% 0s and 50% 1s when training, then wouldn't this invalidate the expectation over your data distribution?\n\n**Now, I know people will say that class imbalance is due to minority / majority classes in the data which do not represent the dataset evenly.** This leads me to the question: why does assuming a uniformly distributed model work? If I say 50% 0s and 50% 1s, how do I know that represents the actual data distribution when I do not have access to it? For all I know, the ratio could actually be 0.9:0.1\n\nYet, we still balance data. Can anyone explain this? Am I crazy?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iiogfs/why_does_balancing_classes_not_ruin_data/"}, {"autor": "CkmCpvis", "date": "2020-08-29 03:27:51", "content": "Synthetic -----> image !!!  morph /!/ I have images of satellite rain data that is taken at intervals of 5mins however, I would like to generate a synthetic interpolation between frames so that I can approximate a minute by minute change. \n\nCan anyone point me in the right direction for this project?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iilifc/synthetic_image_morph/"}, {"autor": "csmath773", "date": "2020-08-28 13:54:37", "content": "Computing elementary logic function with perceptron? Confused by given example /!/ If you go to [this](http://neuralnetworksanddeeplearning.com/chap1.html) online textbook and find on page \" I've described perceptrons as a method for weighing evidence to make decisions.\" you will see an example of using a perceptron to compute elementary logic functions. I've now studied logic functions and understand how the inputs and outputs work for most of them, but I am not quite understanding this example with the perceptron. I thought I understood inputs and outputs but I am still feeling lost. For example:\n\n&gt;I've described perceptrons as a method for weighing evidence to make decisions. Another way perceptrons can be used is to compute the elementary logical functions we usually think of as underlying computation, functions such as AND, OR, and NAND. For example, suppose we have a perceptron with two inputs, each with weight \u22122\u22122, and an overall bias of 33. Here's our perceptron:  \n(-----> image !!!  of peceptron - scroll down to example [here](http://neuralnetworksanddeeplearning.com/chap1.html))  \n***I don't understand this example:*** Then we see that input 00 produces **output 11** (?), since (\u22122)\u22170+(\u22122)\u22170+3=3(\u22122)\u22170+(\u22122)\u22170+3=3 is positive. Here, I've introduced the \u2217\u2217 symbol to make the multiplications explicit. Similar calculations show that the inputs 0101 and 1010 produce output 11. But the input 1111 produces output 00, since (\u22122)\u22171+(\u22122)\u22171+3=\u22121(\u22122)\u22171+(\u22122)\u22171+3=\u22121 is negative. And so our perceptron implements a NAND gate!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ii7fl3/computing_elementary_logic_function_with/"}, {"autor": "mtl2dubai", "date": "2020-08-27 13:36:20", "content": "In Google AutoML vision, is it possible to generate a localization map of the important regions in the -----> image !!!  (heatmap)? /!/ I\u2019m new to machine learning. I am working with a public dataset of medical images and Google\u2019s AutoML vision. My goal is to create a multi-class classifier using single-label classification to diagnose different diseases based on labeled photos.\n\nI was wondering if it was possible to obtain heat maps of regions in the photos that the learning algorithm relied on to make its prediction. This would make the algorithm more clinician-friendly and more understandable. In CNNs, I think this concept is called gradient-weighted class activation mapping (grad-cam).\n\nPlease let me know if this is possible with Google AutoML. Thanks a lot", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ihl5ai/in_google_automl_vision_is_it_possible_to/"}, {"autor": "Real_MichaelCera", "date": "2020-04-26 03:30:35", "content": "Deploy into outside game /!/ Hey guys, sorry if this seems like a very noob-like question but I just never learned how to deploy a neural network model into a public game like Minecraft or rocket league.\n\nI\u2019ve made some neural nets for -----> image !!!  identification, and I\u2019ve even made some that run in my own, self-programmed games, but never a public game like Minecraft. \n\nCan someone please point me in the right direction? A tutorial or something that explains it well would be preferred. Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g87ac4/deploy_into_outside_game/"}, {"autor": "dhokna", "date": "2020-04-25 08:00:11", "content": "Any software where I can input custom feature detectors/filters on an -----> image !!!  and see the results? /!/ I am a noob and I want to experiment with them. \n\nI downloaded Gimp, but it has got preloaded feature detectors. I cannot create custom ones.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g7pun9/any_software_where_i_can_input_custom_feature/"}, {"autor": "PetterGriffinFriend", "date": "2020-04-25 03:09:54", "content": "Transfer learning - model gets very high accuracy on the first epoch... /!/ I have extracted features from Resnet50 which amount to 100352 features per -----> image !!! , then when I divide the dataset into training, validation, and testing, I feed them to my own constructed network.\n\nI have 3 classes to classify. The strange thing is that my model starts with around 70% accuracy, and 90% validation accuracy, and increases little by little until like 95% accuracy.\n\nThe testing results are similar, the model predicts most of the tests correctly. \n\nMy question is that, is it normal for a model to have a high accuracy from the first epoch ? is that normal ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g7me57/transfer_learning_model_gets_very_high_accuracy/"}, {"autor": "Charlie_M_", "date": "2020-04-25 02:38:46", "content": "Loss Functions - Linear Classification - Logistic Regression /!/ During linear regression the algorithm jumbles with the coefficients to create a linear model that closely fits real life data points. How good/crappy models are is tested by measuring the residuals, squaring them and taking the sum. If you graph the sum of squared residuals for a series of linear models (each of them getting progressively better at predicting the real life data points)  it will look like a quadratic function. So ultimately the best model produces the minimized value of the quadratic loss function. \n\nI also understand that in order to test how good/crappy a linear classification model is, you can measure how many misclassifications there are and assign one point for every misclassification and zero points for every correct classification (0-1 loss function - it looks like a step model, check -----> picture !!!  below). Therefore the best linear classification model is selected by determining which model minimizes the number of misclassifications. I have recently learned that the loss function for logistic regression appears to be exponential in shape (look at picture). I have questions regarding this loss function. \n\n**First Question:** The connection between linear regression and the quadratic loss function is very clear to me. I can  graph  the sum of squared residuals for progressively better  linear models and it will produce a quadratic function (a parabola). However how is logistic regression models and the exponential loss function connected?? Can you prove/tell me how this is possible? I read  that the traditional 0-1 loss function is altered for logistic regression models and  a heavier penalty (not just one point) is applied for misclassification - however I'm still lost in seeing the connection between logistic regression and its corresponding loss function.  \n\n**Second Question:** If the loss function for logistic regression is an exponential function, how do we look to minimize this loss function? If I look at the quadratic loss function i can clearly see the minimum value, moreover I can even take the derivative of the quadratic loss function to find when the slope is zero. If I'm looking at the logistic loss function how would I minimize the function? \n\n&amp;#x200B;\n\nhttps://preview.redd.it/2u3scbigmvu41.png?width=699&amp;format=png&amp;auto=webp&amp;s=e5e25bcca5c09ff3ed1fc479963512226ba391d8\n\n&amp;#x200B;\n\nhttps://preview.redd.it/zb6h8r6imvu41.png?width=683&amp;format=png&amp;auto=webp&amp;s=a6f082d4d834f093259c393f8e82f0d590ff6c92", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g7lyj8/loss_functions_linear_classification_logistic/"}, {"autor": "cmillionaire9", "date": "2020-04-24 10:28:31", "content": "Hand gesture recognition without an optical -----> camera !!!  | Radar + AI", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g76224/hand_gesture_recognition_without_an_optical/"}, {"autor": "crepuscopoli", "date": "2020-11-18 08:38:53", "content": "Create a different text comment, based on each different -----> picture !!!  /!/ Hi guys!!!!!!!! \n\nMe and Friends are creating an AI Assistant, that needs to take a picture, and print out a text comment based on that picture. How hard it would be to differentiate the result for each picture? I mean having a different comment (with a sense), based on different each picture?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jwc9gp/create_a_different_text_comment_based_on_each/"}, {"autor": "Fit_Faithlessness122", "date": "2020-11-17 23:07:47", "content": "Am I allowed to make a website where users can upload an -----> image !!!  of something (not gonna go into detail) and turn the -----> image !!!  into 3D? There\u2019s already softwares for that.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jw3v8x/am_i_allowed_to_make_a_website_where_users_can/"}, {"autor": "[deleted]", "date": "2020-11-17 23:06:30", "content": "Am I allowed to make a website where users can upload an -----> image !!!  of something (not gonna go into detail) and turn the -----> image !!!  into 3D? There\u2019s already softwares for that, but can I still /!/ [deleted]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jw3ufr/am_i_allowed_to_make_a_website_where_users_can/"}, {"autor": "SaferSephy", "date": "2020-11-17 21:17:35", "content": "Creating receipt/invoice recognition with Detectron2 and Tesseract and running into a question in terms of training method /!/ So i'm creating an api that can recognize receipts in an -----> image !!!  and extract the text . For this i'm doing two things:\n\n1. I'm training a RNN model with Detectron2 that will be able to recognize segments in the receipt (header, rows and footer)\n2. After that i throw each segment through Tesseract OCR \n\nThe result are pre-cut blocks of text that can easily be used in other places.\n\nThe question i'm having; \n\n* i can train the model to recognize the rows as a block (see this example: [example](https://i.imgur.com/BWCprJ6.png) ), \n* or i can train the model to recognize each individual row as a block. \n\nWhich route should i take?\n\nThe latter option is considerable more work to collect training/test data, and at this point i'm not really sure if it will be very beneficial. The only real benefits i can see is:\n\n* I won't have to worry about linebreaks (but honestly this is more of a minor thing to me)\n* Each individual row might have less \"noise\" and will result in better OCR results \n\nEspecially the latter benefit is very hard for me to predict, and to find it out, it will take me a LOONNGG time to  A/B test.\n\nDoes anyone have any experience with this and can share some insight that help me to make a choice? \n\nThanks!\n\nPS. Please be gentle, i'm a beginner in this field, haven't even finished Andrew Ng's course yet so if i'm talking rubbish, please let me know :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jw1pz6/creating_receiptinvoice_recognition_with/"}, {"autor": "hellopaperspace", "date": "2020-11-17 18:12:45", "content": "[Tutorial] Object Detection Using Mask R-CNN with TensorFlow 1.14 and Keras /!/ Mask R-CNN is state-of-the-art when it comes to object instance segmentation. This tutorial covers how to train Mask R-CNN on a custom dataset using TensorFlow 1.14 and Keras, and how to perform inference.\n\nSpecifically, the topics covered include:\n\n* Overview of the Mask\\_RCNN project\n* Preparing the model configuration parameters\n* Building the Mask R-CNN model architecture\n* Loading the model weights\n* Reading an input -----> image !!! \n* Detecting objects\n* Visualizing the results\n* Complete code for prediction\n* Preparing the training dataset\n* Preparing model configuration\n* Training Mask R-CNN with TensorFlow 1.14 and Keras\n\nLink to the article: [https://blog.paperspace.com/mask-r-cnn-in-tensorflow-2-0/](https://blog.paperspace.com/mask-r-cnn-in-tensorflow-2-0/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jvy1gp/tutorial_object_detection_using_mask_rcnn_with/"}, {"autor": "DEATH_STAR_EXTRACTOR", "date": "2020-11-17 15:07:37", "content": "My alternative to Backprop? /!/  So I finally connected my AGI path to the mainstream path. I mean I found where we differed at.  \n\n\nMarkov Chains &gt; Hidden Markov Models &gt; RNN &gt; LSTM &gt; Transformers  \n\n\nMy AGI design is like the Markovs, but NOT like RNN and thereafter. That's where I stop. If I go ahead to RNN then all my AI makes no sense, we get complex maths like Backpropagation etc.  \n\n\nI learned though the only difference between my (markov) and modern (RNN and thereafter) AI is THIS !:  \n\n\n[https://www.quora.com/Why-are-hidden-Markov-models-replaced-by-RNN-nowadays-in-many-applications-What-is-the-strength-of-RNN-over-HMM#:\\~:text=Hidden%20Markov%20Models%20and%20Recurrent,timestep%20and%20HMMs%20don't](https://www.quora.com/Why-are-hidden-Markov-models-replaced-by-RNN-nowadays-in-many-applications-What-is-the-strength-of-RNN-over-HMM#:~:text=Hidden%20Markov%20Models%20and%20Recurrent,timestep%20and%20HMMs%20don't).  \n\n\nNon-linear. That's it. My AI is modern but just lacks the ability to solve non-linear problems good. Or does it? Maybe my design solves those problems?  \n\n\nAnd that's the topic here. I know non-linear problems are not like \"he got 15$ for working 1 hour, someone got 60$ - how long did they work?\". They're like x\\^2 or 2\\^x (try putting 1, 2, 3, or 4 where the x is), this gives you an ex. exponential curve, not a linear.  \n\n\nBUT that doesn't make sense. Even linear problems are not exact matches, 15$&gt;1 hour, 60$&gt;? You have to do tricks to find the pattern that gives you all answers fast. 60/15=answer. Same for the x\\^2. Input is ex. 1 output is 1, input is 2 output is 4, 3: 9, if input is 5 answer is ?, it's 25 because 5 \\* itself. If 5\\^3 we get 125 because 5\\*itself\\*original\\_input(5).  \n\n\nMy design \"can\" discover these rules by natural \"thinking\" by brainstorming intelligently, but one wouldn't hardcode them all either, and Backprop is unnatural as well.  \n\n\nIf I look for non-linear problems in vision or text, what are they? I don't mean math problems like Joey earns 45$....no, rather problems like \"I was walking down the ?\" prediction, or predict the rest of an -----> image !!!  with only a tail and bowl shown. And I don't see any non-linear problems here right away. What could they be though? What if a cat was observed to be 1 inch high 1 time, 3 inches high 5 times, and 8 inches high 873 times, is this an exponential recognition task that requires me (so to recognize A or predict A&gt;? (B) \"prediction\") to find the pattern that governs cat height growth and therefore allow we to determine that cats will more likely be super high and rarely short? What about luminosity? Maybe grapes are rarely fully transparent in a certain lighting? Or some man made engine or a natural hurricane? And how often are these a problem?\n\n&amp;#x200B;\n\nBackpropagation is unnatural and so complex that no one can explain it clearly to me even after reading about it and talking to others. I believe my alternative can naturally learn functions behind observations in a simple way a lot more generally too. They mention non-linear problems need Backprop to solve them but really these are all not linear/nonlinear they are just patterns/problems, Backprop may find a function behind observed data but my alternative can too learn those rules that generate the observed data and tell you not only how it found the function but also why the function generates observations and unobserved points.\n\n&amp;#x200B;\n\nTo me all problems are non-linear because you may see a exact match like in text \"I was walking down the\" when prompted to predict, or you may see a unseen but similar cat face that is different or brighter simply, and your prediction may be favoring now brighter ex. tail because of the bright face even though you only knew a darker tail comes with the cat face, and perhaps cats grow in height exponentially or linearly, these are all problems and require some robustness. There's nowhere where half are non-linear and need a cheat to get learned, they all can be learnt by known ideas or rules. To learn the answer to the 8-queens problem you may discover in your brain you should use the algorithm (manually do it by hand) called min-conflict which tries to move the problem piece around until removes conflicts instead of starting all over completely or partially if fail.\n\n&amp;#x200B;\n\nSo,  why don't we use an alternative to Backpropagation (which is complex and unnatural)? Doesn't the brain learn functions by thinking about known ideas? If I don't know the answers to x\\^2 or 2\\^x (put 1, 2, 3, or 4 where the x is) and only have some of the answers, i can realize the algorithm that generates the other answers, by thinking \"hmm, let me try 2\\*2, it =4, nope, not matching observations, maybe it times itself? Repeat\".", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jvujtf/my_alternative_to_backprop/"}, {"autor": "mwitiderrick", "date": "2020-11-16 13:55:48", "content": "[D] Object Detection in Android Applications with Fritz AI /!/ In this piece, we shall focus on detecting objects in an -----> image !!!  using Fritz AI . Fritz AI pre-trained models allow you to add object detection to your Android and iOS applications with just a few lines of code.\n\n[https://heartbeat.fritz.ai/object-detection-in-android-with-fritz-ai-fd6e0eb7adfa](https://heartbeat.fritz.ai/object-detection-in-android-with-fritz-ai-fd6e0eb7adfa)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jv749j/d_object_detection_in_android_applications_with/"}, {"autor": "tylersuard", "date": "2020-02-26 11:35:56", "content": "Can I train Stylegan on rectangular images? /!/ I'd like to retrain Stylegan on 500x3000 images.  It only seems to accept one parameter for -----> image !!!  dimension.  Is there any way to change this so it accepts rectangular images rather than just square ones?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f9rwx4/can_i_train_stylegan_on_rectangular_images/"}, {"autor": "RealisticWear1", "date": "2020-02-26 00:41:05", "content": "Classifying a book as a \"textbook\" /!/  \n\nI have a list of book ISBNs with their title, author, link to cover -----> image !!!  (which I can download to my desktop), and number of pages\n\nHow would I accurately determine which book is a textbook, and which is a regular book\n\nSome hints might be\n\n1. textbooks often have word such as \"edition\" or \"3e\" or \"4ed\"\n2. textbooks often have a lot of pages\n3. textbooks often cost over $40\n4. oreilly/packt books are considered textbooks\n5. textbooks titles often contains words ending in ology or atry eg. psychology, immunology, sychiatry\n6. I can provide a list of 'known' textbooks\n\nHow do I start tackling this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f9kiqq/classifying_a_book_as_a_textbook/"}, {"autor": "SHKmi", "date": "2020-02-25 08:50:33", "content": "Need an advice on research topic /!/ I am about to choose ML research topic for my master thesis, but i am at a dead end.  The problem is, that while reading research papers, i find solutions, but not an open problems.\n\nFor now, a came up with such ideas:\n\n1. Research neural network quantization algorithms - i started to research this topic 3 months ago and it seems to me, that current papers reveals high accucary methods. I have not idea what can i to do in this field\n2. Implement online video-processing system using quantized neural networks. It sounds also good, but it could be done with few lines of Tensorflow Lite code.\n3. Intelligent Image Database - -----> image !!!  database, that has -----> image !!!  classifiers and detectors for automatic annotation. It seems to be a good idea, but i need to find here a research part. For me this tasks sounds like pure engineering task\n4. Make some signal processing research. It could be connected with first idea as optimal hardware implementation. But i don't know where to search for suitfull specific task for ML, because a lot of dsp problems are solved with classical analytical mathematical methods.\n\nAs you can see, i always see some disadvatages in ideas. It  stopping me from starting to work. Can you give me an advice how to develop one of this ideas to make it suitable both for engineering and research? Generally, how to search for research topics?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f97fbk/need_an_advice_on_research_topic/"}, {"autor": "legoloco45", "date": "2020-02-25 08:46:06", "content": "Looking for suggestions - Clothing classifier from images /!/ Greetings! I'm new to machine learning and to reddit so feel free to correct me if I'm wrong.\n\nFor the past days I've been looking into building a system that could serve as a feature extractor from an -----> image !!!  - One of the main goals I have is to have human clothing identifier - color, type, if it has any prints on it e.t.c.\n\nSo far I've found only the MNIST fashion dataset but I don't think it would be in any way usable in a real-life situation.\n\nThe absolute goal would be something like [https://cloudsight.ai/](https://cloudsight.ai/) API.\n\nMy question is - what model should I choose for training and are there datasets that cover this topic; Maybe I don't need to use network at all. Or are there any videos that could help my cause. Anyhow, Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f97dt1/looking_for_suggestions_clothing_classifier_from/"}, {"autor": "shaikhman", "date": "2020-02-25 07:43:49", "content": "GAN Trouble! need help! /!/ I am trying to train a W-GAN for Denoising of Low-dose CT scan images. I am training on 6800 -----> image !!!  patches of size 64x64 randomly sampled from 1000 CT scan -----> image !!! s and observing a negative loss value for the discriminator. Is this behavior normal? How do I interpret the loss values of the network? The generator doesn't seem to be learning, It produces completely black images.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f96sro/gan_trouble_need_help/"}, {"autor": "compet_layer", "date": "2020-02-24 16:19:16", "content": "When you zero mean a set of images /!/ Do you subtract the mean of the individual -----> image !!!  from the corresponding -----> image !!!  or subtract the mean of all -----> image !!! s?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f8tknp/when_you_zero_mean_a_set_of_images/"}, {"autor": "DeepStrategy", "date": "2020-02-24 13:21:43", "content": "CNN for MNIST with efficient sliding windows, to detect in realtime /!/ I want to train a CNN that can do inference in Realtime on big resolution images. The CNN must read handwritten digits consisting out of 5 digits.\n\nSo far I trained LeNet-5, Overfeat and Yolo on MNIST.\n\nLeNet-5 gave me a good accuracy, but with a naive sliding windows approach, I ended up with 4000 windows on a FullHD -----> image !!!  (with a big stride though), which is not fast enough. (10ms for one window on CPU-&gt;40sek per image)\n\nSo I looked into some more efficient ways for a sliding window and came across Overfeat. It convolves the whole picture and create a Class-dimensional output array \\[windowsX, windowsY, Classes\\]. Right now I am trying to train and evaluate the network with the help of slim.\n\nSo finally, to my question:\n\n**Is there a better approach or CNN for this problem?**\n\nThank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f8r4kr/cnn_for_mnist_with_efficient_sliding_windows_to/"}, {"autor": "phobrain", "date": "2020-02-24 10:24:11", "content": "Fine-tuning e.g. VGG with multi-label images (keras) /!/ I have 'keyword vectors' of 0/1 for -----> photo !!! s, vector size ~1200, and -----> photo !!! s labeled with 1-15 keywords, with an average of ~5 bits set per -----> photo !!! 's vector. Logically this is equivalent to an unordered bag of words, or the bitwise sum of one-hot encodings of the words. The Imagenet fine-tuning examples I'm finding have one class per photo - are there any with multi-class training photos?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f8pba0/finetuning_eg_vgg_with_multilabel_images_keras/"}, {"autor": "Bruno_Br", "date": "2020-02-23 16:29:33", "content": "Library to create Heatmaps /!/ Hello, could anyone recommend a good library for creating heatmaps? I am trying to display the most common positions reached by a simulated robot in a 2D control problem. I still don't know how much granularity I should use, but basically I want to display the heatmaps on top of an -----> image !!!  of the field. \n\nAny tutorials are very welcome too.\n\nThank you so much.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f8bp65/library_to_create_heatmaps/"}, {"autor": "George_math", "date": "2020-04-09 12:45:45", "content": "Deep learning guided -----> image !!! -based droplet sorting for on-demand selection and analysis of single cells and 3D cell cultures", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fxrs72/deep_learning_guided_imagebased_droplet_sorting/"}, {"autor": "kapoor13", "date": "2020-04-09 11:03:20", "content": "Densecap paper providing dense -----> image !!!  captioning /!/ I have been implementing a project based on this paper and have run into a doubt at this point.\nThis paper generates a set of captions based on ROIs extracted from the image and generates a set of scores per image. It would be very helpful if someone could help me understand how these scores are generated and how to discard the irrelevant captions based on these scores.\n\n[paper](http://cs.stanford.edu/people/karpathy/densecap/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fxqhef/densecap_paper_providing_dense_image_captioning/"}, {"autor": "nightschool", "date": "2020-04-09 10:24:50", "content": "Questions re GAN -----> image !!!  analysis process. /!/ I'm currently working on an artistic project involving the use of images produced by a WGAN trained on multiple thousands of sample images of the same genre of visual art. \n\nCan someone possibly point me towards some resources that give an explanation of the actual analysis method used during the training process?\n\nAs a newcomer to Python (this project is PyTorch/Keras based) I understand the general outline of the code but I still can't wrap my head around what usable info is being pulled from these images and how are they then interpreted as tensors/vectors.\n\nApologies if this question doesn't even conceptually make sense as I'm not sure i'm coming at this from the right angle.\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fxq1ns/questions_re_gan_image_analysis_process/"}, {"autor": "JonesOnCorners", "date": "2020-04-09 07:20:57", "content": "Learning Machine Learning As A Backend Developer /!/ ***Background:***\n\nMy current workplace keeps handing out random work associated with different skills, a month back I was developing a webapp for automation of some tasks, now I have been asked to analyse a DB table and help categorize the data. As much as I love new challenges it's all a bit too random but considering the job market and limited new opportunities I got to stick to this jobs.\n\n***My Problem:***\n\nI have zero knowledge of data science or Machine learning and the task assigned i.e. categorizing data  from a DB table can be achieved via Machine learning. Now, from a bigger -----> picture !!!  perspective can I start directly learning Machine learning and work on the issue at hand or do I have to do the whole data science learning bit as well as I do not intend to be a Data Science guy even in the long term as I love application development.\n\nMany thanks to the community in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fxnxm7/learning_machine_learning_as_a_backend_developer/"}, {"autor": "kallaben", "date": "2020-04-08 09:39:16", "content": "Why does batch size increase memory usage? (Keras) /!/ My memory usage increases radically when increasing the batch size for training my conv-net in Keras. It seems like Keras is keeping all activations for each -----> image !!!  in memory, so if the conv-net has a total of 200,000 activations, then the total activations saved is (200,000 * batch_size).\n\nI don't understand why the backpropogation algorithm can't compute the gradient one input at a time, since it ends up being a sum of gradients anyways. Here is a simple example showcasing that the gradient of the mean squared error is a sum over the training data: https://i.imgur.com/xDhIKjD.gif\n\nSo why is it not possible to compute the gradients one input at a time? Instead of keeping every activation in memory.\n\nThank you:)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fx3jnz/why_does_batch_size_increase_memory_usage_keras/"}, {"autor": "ML_Boy", "date": "2020-04-08 06:38:26", "content": "What are the requiremens for MOST companies for an entry-level Machine learning engineer role? /!/ There was a post on this thing a little while ago, but there were many INEXPERIENCED people answering.\n\nImportant: Only the fellows with an experience of work should answer (as only in this way can the inexperienced fellows get the right -----> picture !!! ).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fx1di5/what_are_the_requiremens_for_most_companies_for/"}, {"autor": "esenthil", "date": "2020-04-07 23:32:25", "content": "Any suggestions for an ML Project. /!/ Guys,\n\n&amp;#x200B;\n\nAny suggestion for an ML project.\n\n1. The project duration is 12 weeks.\n\nI am trying to do it using Tensorflow or Scikit Learn libraries\n\nIn the following areas\n\n* -----> Image !!!  classification/recognition\n* NLP\n* Linear Regression \n\nIt will be great if you can point to the dataset and base code in GitHub or any articles.\n\nPlease provide me suggestions.\n\nThanks in advance for the help :-)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fwv9ms/any_suggestions_for_an_ml_project/"}, {"autor": "hirenk15", "date": "2020-04-04 07:16:49", "content": "Best practices for machine learning deployment /!/ It would be great if anyone can help to address my following queries.\n \n1) What are the best practices to deploy machine learning models for production in real world environment?\n2) I'm currently interested in AWS machine learning services. Can anyone has any tutorials or resources to deploy fully custom ML models with AWS SageMaker from scratch. I have no idea how to do it from writing docker -----> image !!! , defining directory structure, etc. \n\nThanks in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fupafb/best_practices_for_machine_learning_deployment/"}, {"autor": "Mattx98C", "date": "2020-04-03 13:29:44", "content": "Eureqa (AI) /!/ I was wondering if anyone has ever heard of eureqa, it's an AI that basically does some super non linear regressions between arrays of data by using just basic mathematical operations from \u00d7+-\u00f7 to simple moving average, weighted moving average, sin, cos, log and so on based on your preferences of complexity as you choose from a list. It ends up with a formula that explains the dependent variable incredibly well, but does anyone know what kind of technique does it use to actually build the regression? I'm gonna immediatly post a -----> picture !!!  of it where the dependent variable is the closing price of the S&amp;P 500 and the independent variable is the same S&amp;P 500 delayed by one day. The AI is able to autonomously build the model ignoring the first days of the independent variable and consider it as it's more delayed, if it's worth to. So what I'm actually saying by uploading the array delayed just by one day is \"explain me the closing price of today, by using however you want and delaying as much as you want, all the values from t0 to yesterday, using the mathematical operations I chose\".\nCool fact is that the guy who gave life to this AI has managed to solve the physical dilemma of the double pendant frequency equation, unsolved till that moment.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fu8g2k/eureqa_ai/"}, {"autor": "closet_coder", "date": "2020-04-03 12:23:16", "content": "How to I train this neural network? /!/  I have a 512\\*512 pixel -----> image !!! . I'm working on a project wherein I need my deep learning model to predict the value of a pixel of an image, by giving 4 neighbouring pixel values as input. I cannot take the entire image as input, I can only take 4 neighbouring pixel values as input(2 to the left of the required pixel, 2 to the right of the pixel). I have to do this over the entire image to get the predicted values of the middle pixels. In this case, how do i train my model to predict the output? Since this is not a standard data set, I don't know how i can split it for training and testing. How can i achieve this? I thought i would convert the image to a pandas dataframe and do it. However, I would like to know if there is a better way. Any help in this would be appreciated, since I need to complete this project soon. \n\n&amp;#x200B;\n\nP.S. I have already built the deep learning model, i would just like to know how to take the input and train it.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fu7gvy/how_to_i_train_this_neural_network/"}, {"autor": "agju", "date": "2020-04-03 12:09:56", "content": "UNET - Multiple classes output /!/ I'm working with a customized UNET model in order to get a segmentation of different objects inside an -----> image !!! . I've been reading a lot, and looking for as much information as I can, but I have not yet arrived to a good conclusion. \n\nMy question is this: in order to get an output with multiple possible classes (let's say bird, dog, cat), is it better to\n\n\\- Have 1 output channel, with pixel values 0, 1, 2, 3 (background, bird, dog, cat)\n\n\\- Have 3 output channels, with pixel values 0, 1 (background, bird/dog/cat depending on channel)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fu7a86/unet_multiple_classes_output/"}, {"autor": "versaceblues", "date": "2020-04-02 18:07:52", "content": "Implementing a tweet generator advice? /!/ I dont have much experience with NLP and am curious what you guys think would be the best way to implement something like this.   \n\n\nBasically I want to take a few twitter accounts, scrape all their tweets, and train a model on them. I want this model to be able to then generate a tweet in the style of the tweets that I trained it on.    \n\n\nBasically something like this which I guess use a GPT-2 model, although im not quite sure if that is the best approach [https://talktotransformer.com/](https://talktotransformer.com/).   \n\n\nQuestions:   \n\n\n1.) How many minimum tweets would I need for this to work reasonably well. I mostly doing it for laughs and to learn some NLP so it doesn't need to be to fool anyway, but it should be somewhat comprehensible.   \n\n\n2.) Back when I played with -----> image !!!  detection using AlexNet/VGGNet there was a technique where you could take a pertained network, then retrain it using your specific dataset, to better fit it to your purpose.   \n\n\nIs there a similar approach to take with generative NLP models like this. Or do I need to train from scratch.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ftr90x/implementing_a_tweet_generator_advice/"}, {"autor": "Oxbowerce", "date": "2020-04-02 17:59:30", "content": "DCGAN unable to create faces /!/ For a personal project I am trying to use a GAN to create images of faces of footballers.\nMy dataset consists of approximately 14000 images of footballer faces such as [this one](https://i.imgur.com/QdBF59W.png).\nFor this I am using a DCGAN architecture in pytorch based on [this pytorch example](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html).\nThe generator makes use of transposed convolutional layers to upsample the -----> image !!!  with ReLU activation functions, the discriminator uses convolutional layers with a leaky ReLU activation.\nBoth the generator and the discriminator also use batchnorm layer. See the model definitions below.\n\n    class Generator(nn.Module):\n        def __init__(self, latent_dim, image_size):\n            super(Generator, self).__init__()\n\n            self.latent_dim = latent_dim\n            self.conv_blocks = nn.Sequential(\n                nn.ConvTranspose2d(self.latent_dim, 1024, kernel_size=(4,3), stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(1024),\n                nn.ReLU(),\n                nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),\n                nn.BatchNorm2d(512),\n                nn.ReLU(),\n                nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n                nn.BatchNorm2d(256),\n                nn.ReLU(),\n                nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n                nn.BatchNorm2d(128),\n                nn.ReLU(),\n                nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n                nn.BatchNorm2d(64),\n                nn.ReLU(),\n                nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n                nn.Tanh()\n            )\n\n        def forward(self, x):\n            img = self.conv_blocks(x)\n            return img\n\n\n    class Discriminator(nn.Module):\n        def __init__(self, image_size):\n            super(Discriminator, self).__init__()\n\n            self.image_heigth, self.image_width = image_size\n            self.conv_block = nn.Sequential(\n                nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n                nn.LeakyReLU(0.2),\n                nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n                nn.BatchNorm2d(128),\n                nn.LeakyReLU(0.2),\n                nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n                nn.BatchNorm2d(256),\n                nn.LeakyReLU(0.2),\n                nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n                nn.BatchNorm2d(512),\n                nn.LeakyReLU(0.2),\n                nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1, bias=False),\n                nn.BatchNorm2d(1024),\n                nn.LeakyReLU(0.2),\n                nn.Conv2d(1024, 1, kernel_size=(4,3), stride=1, padding=0, bias=False),\n                nn.Sigmoid()\n            )\n\n        def forward(self, img):\n            out = self.conv_block(img)\n            return out\n\nThe following hyperparameters are used:\n- 10 epochs\n- adam optimizer with learning rates of 0.0002 and beta1 of 0.5 \n- binary cross entropy loss\n- batch size of 16\n- random weight initialization using a mean of 0 and standard deviation of 0.02\n- label smoothing for both real and fake labels\n\nI am also using label smoothing for both real and fake labels.\nThe following code to train the model, which also includes the hyperparameters I am using.\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Create models\n    generator = Generator(128, (128, 96)).to(device)\n    discriminator = Discriminator((128, 96)).to(device)\n\n    # Weight initialization\n    generator.apply(weights_init)\n    discriminator.apply(weights_init)\n\n    # Initialize optimizers\n    optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999)) # lr of 0.0002\n    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\n    # Initialize loss function\n    loss_fn = nn.BCELoss().to(device)\n\n    seed = torch.randn((16, 128, 1, 1), device=device)\n\n    # Create dataset\n    dataset = ImageDataset('data/images.h5')\n    dataloader = DataLoader(dataset, batch_size=16)\n\n    n_epochs = 10\n    for epoch in range(n_epochs):\n        print(f\"\\nEpoch {epoch+1}/{n_epochs}\\n===================================\")\n        for i, imgs in enumerate(dataloader, 1):\n            if i % 100 == 0:\n                print(f\"Epoch {epoch+1}: batch {i}/{len(dataloader)}\")\n\n            # Ground truths with label smoothing\n            real = torch.rand(imgs.shape[0], device=device)*(1.2-0.7) + 0.7\n            fake = torch.rand(imgs.shape[0], device=device)*(0.3-0) + 0\n\n            # Train generator\n            z = torch.randn((imgs.shape[0], 128, 1, 1), device=device) # batch_size * latent_dim\n            imgs_fake = generator(z)\n            predicted = discriminator(imgs_fake).detach().cpu()\n            loss_gen = loss_fn(discriminator(imgs_fake).squeeze(), real) # batch_size\n\n            optimizer_g.zero_grad()\n            loss_gen.backward()\n            optimizer_g.step()\n            \n            # Train discriminator\n            loss_real = loss_fn(discriminator(imgs.float().to(device)).squeeze(), real)\n            loss_fake = loss_fn(discriminator(imgs_fake.detach()).squeeze(), fake)\n            loss_desc = (loss_real + loss_fake) / 2\n\n            optimizer_d.zero_grad()\n            loss_desc.backward()\n            optimizer_d.step()\n\n\nThis gives the following losses for the generator and discriminator: https://i.imgur.com/XJ2ftBh.png\nA GAN constructed from the generator and discriminator above is able to construct the general shape of the images alright, the main issue I have is related to the definition within the face.\n[This is an example of the images created by the GAN.](https://i.imgur.com/uGumgyA.png)\nAny ideas or tips that could help the performance of this GAN are very welcome, thanks in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ftr3ep/dcgan_unable_to_create_faces/"}, {"autor": "jbuddy_13", "date": "2020-01-02 21:25:18", "content": "-----> image !!! -text embeddings using encoder-decoder architecture /!/ I'm interested in learning more about image-text embeddings, particularly for image based search and recommender systems. In context of a clothing recommendation service, if I have an image of a vest, textual tags such as \"vest\", \"top\", and \"menswear\" might be retrieved. Likewise, other products might be retrieved based on latent features learned as well as visible features. \n\nLooking back to my grad program, I likely need a CNN and RNN composite network using some sort of encoder/decoder architecture. This architecture might allow for image inputs and textual outputs (tags mentioned above.)\n\nIs anyone doing this? Or is this similar enough to something that has been done? Any references, articles, tutorials, etc would be welcomed.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ej4kf6/imagetext_embeddings_using_encoderdecoder/"}, {"autor": "balloonboom", "date": "2020-01-02 14:47:27", "content": "Intro to comp vision/-----> image !!!  proc /!/  [https://towardsdatascience.com/how-i-sort-of-learned-computer-vision-in-a-month-c3faec83b3d6](https://towardsdatascience.com/how-i-sort-of-learned-computer-vision-in-a-month-c3faec83b3d6)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eiz38l/intro_to_comp_visionimage_proc/"}, {"autor": "bromideforge", "date": "2020-01-02 02:14:30", "content": "Why is my model only predicting 2 out of 4 total classes? /!/ The code for my kernel, written on Kaggle can be found here: https://www.kaggle.com/jefffro/science-research-jeff-cheng, apologies for messed up formatting on it.\n\nThis is part of the [diabetic retinopathy competition](https://www.kaggle.com/c/aptos2019-blindness-detection) on Kaggle, where I wanted to use what I learned from the resources here to do something with the fastai library, and out of interest for the topic. For some reason, even though the classes go from 0-4, as per [the data](https://www.kaggle.com/c/aptos2019-blindness-detection/data), it seems to only be using 0 and 2. It shouldn\u2019t be an issue with the accuracy, as to me it was satisfactory, ending up with ~75% accuracy.\n\nI'm a rather novice coder, and I can't really see how we are excluding 1, 3, and 4 from this, which confuses me a bit. I don't think it's a bad model as the error rate has proven to be pretty decent, at around ~0.24-25, seen in this [-----> image !!! ] (https://imgur.com/a/ez6rfTD) + the kernel.\n\nSo, what am I doing wrong here? \n\nThank you so much!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eirot4/why_is_my_model_only_predicting_2_out_of_4_total/"}, {"autor": "AHopefulLoser", "date": "2020-01-01 22:47:15", "content": "Why does this Kaggle kernel only have 2 out of 4 classes in their submission file? /!/ I've been exploring some of the resources at fast.ai, and wanted to see if what they used from their -----> image !!!  recognition lesson could be used in [this](https://www.kaggle.com/c/aptos2019-blindness-detection) competition, which I found very interesting and personally relevant to me. I am aware that it ended, but I liked the topic.\n\nI wanted to see if anyone used the fast.ai libraries with it, and came across this kernel here: https://www.kaggle.com/nathanh12/aptos-2019-fastai-starter/output\n\nI noticed that there are 4 classes, 0-4, but in the output they seem to only use 0's and 2's. I tried looking through the code to the best of my knowledge(I'm not super experienced), and I couldn't really see why. If anybody would be so kind to take a look, as it isn't much code, could they possibly let me know as to why? I may be interested in doing something myself with fewer classes, as I'm not super experienced with programming and those libraries.\n\nThanks so much!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eip2k4/why_does_this_kaggle_kernel_only_have_2_out_of_4/"}, {"autor": "freelance3d", "date": "2020-01-01 21:02:56", "content": "Coding an art generator with Machine Learning/AI - where to start? /!/ As the title says I want to learn to make AI/ML generative art, probably two kinds:\n\n\n- Input an -----> image !!!  and a style to generate the -----> image !!!  in that chosen style (ie. a portrait like Starry Night by Van Gogh as done [here](https://deepart.io/).\n\n\n- Inputting abstract art of a chosen theme and it generates new ones in that style. ie. input images of butterflies and it learns to generate new butterflies.\n\n\nCan anyone give me a good starting point for this? I've done some intermediate python and javascript work and am pretty savvy with code once I've started. Thoughts?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/einqq1/coding_an_art_generator_with_machine_learningai/"}, {"autor": "Misaria", "date": "2020-01-01 18:47:07", "content": "Questions regarding -----> image !!!  learning apps. /!/ I have some questions around machine learning in general but at the moment it's for the app DeepFaceLab and/or Faceswap which are related.  \nThis is only for fun, nothing more than that, on a low budget.  \n\nI'm asking here because I see a lot of unanswered posts in related subs and you might be able to answer the questions I have, so I hope it's okay.\n\nI see the recommendation is NVIDIA (CUDA), but does it affect the end result or the process? Besides speed.  \nAs in, does the model collapse (not sure exactly why that happens) earlier or is the training unstable on an AMD-card?  \nAMD offers more VRAM for a lower price, so if it's just slower then it seems the trade-off is worth it for the extra VRAM.   \nE.g. one could get two RX 580 8GB for the price of one 1660 Ti 6GB / 1660 Super 6GB, or one RX 5500 XT 8GB.   \nI understand the VRAM isn't added up to 16GB with two cards but the price difference is huge.  \n\nIf you use a CPU with integrated graphics, it uses regular RAM as VRAM?  \nIf so, even if it's slower, could you just use the integrated GPU and buy 64GB of RAM? It's cheaper than a standalone GPU. \n\nWhat's the recommended setting for Windows' Advanced System Settings; prioritize Programs or Background Services?\n\nAny other tips?  \nE.g. hardware.  \nI was thinking of getting a fast M.2 SSD but I don't know if it speeds up the process beyond extracting and sorting images.\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eilx15/questions_regarding_image_learning_apps/"}, {"autor": "snicksn", "date": "2020-01-01 12:41:27", "content": "State of the art for coil-100, -----> image !!!  classification? /!/ I should write something about state of the art for image classification of the coil-100 dataset, and preferably  have a good reference.\n\nAny tips on finding that?  \nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eihyp2/state_of_the_art_for_coil100_image_classification/"}, {"autor": "Armin71", "date": "2020-09-16 17:42:15", "content": "I want to find patterns in a model with multiple variables But it's a bit complicated /!/  to clarify the question I want to give you a -----> picture !!! :\n\n[https://imgur.com/a/suv4aZW](https://imgur.com/a/suv4aZW)\n\nThis is a instance of x = \\[a ,b , c\\] x: \\[5 , 1000, 500\\]\n\nYou can see the chart go up in x1, x9, x17 and go down in x5, x13\n\nI assume exists a pattern in the chart But I do not know which X is related to which X or does many X have a relationship with another? what happens when the chart goes up and goes down. Is there any pattern in the chart or not? If there is, where? which variable? I want the machine to consider any possible in the chart. of course, I can provide the car with a lot of charts for training.\n\nNow, I'm working on an algorithm and I had some progress but I wish to know your opinion.\n\nOf course, The real problem I want to solve actually very more complex than this problem. I'm not looking for a specific algorithm or magic formula because I know it's a complex problem. I just want you to give me an overview and tell me which field of math I should follow.\n\nLet me know if you have any questions about the problem because maybe I couldn't explain it clearly.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iu0g28/i_want_to_find_patterns_in_a_model_with_multiple/"}, {"autor": "OnlyProggingForFun", "date": "2020-09-16 10:50:41", "content": "PiFuHD: A new method for high-fidelity 3d reconstruction. It only needs a single -----> image !!!  of you to generate a 3D avatar that looks just like you, even from the back!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/itt530/pifuhd_a_new_method_for_highfidelity_3d/"}, {"autor": "FPL_dissappointment", "date": "2020-09-15 16:48:13", "content": "What's the state of the art in reading text in a -----> photo !!! ? (models and datasets) /!/ Hi all! I'm interested in trying to automatically read the text on a book spine. Does anyone have familiarity of what datasets exists for training and what the state of the art is for models in this domain?\n\nMany thanks in advance", "link": "https://www.reddit.com/r/learnmachinelearning/comments/itcm3v/whats_the_state_of_the_art_in_reading_text_in_a/"}, {"autor": "wutheringsouls", "date": "2020-09-15 15:04:55", "content": "Getting an error in svi step due to to a multiclass distribution in sample using pyro and pytorch /!/ Hello,\n\nI'm working on a causal variational autoencoder which works with class segmentation masks, class labels and causality(0 or 1) as the inputs.\n\nI'm getting an error when working with batch sizes more than 1 due to the svi step. I'm using a Bernoulli distribution because I want it to learn the probability distribution for multiple classes in an -----> image !!! . I think that the Categorical distribution also fits the bill here, but I get the same error with it too.\n\nWhen I tried narrowing down the code lines which create the problem, I think it's in the model function:\n\n    one_vec2 = torch.ones([batch_size, self.lbl_shape[0]], **options) \n    class_labels = pyro.sample('class_labels', dist.Bernoulli(one_vec2*0.5), obs = lbls) \n\nThe error:\n\n    ValueError                                Traceback (most recent call last)\n    &lt;ipython-input-19-8cbc046dd2c1&gt; in &lt;module&gt;()\n          6 vae = Vae_Model1(lbl_sz, ch, img_sz).to(device)\n          7 svi = SVI(vae.model, vae.guide, optimizer, loss = Trace_ELBO())\n    ----&gt; 8 train(svi, train_loader, USE_CUDA)\n    \n    6 frames\n    /usr/local/lib/python3.6/dist-packages/pyro/util.py in check_site_shape(site, max_plate_nesting)\n        320                 '- enclose the batched tensor in a with plate(...): context',\n        321                 '- .to_event(...) the distribution being sampled',\n    --&gt; 322                 '- .permute() data dimensions']))\n        323 \n        324     # Check parallel dimensions on the left of max_plate_nesting.\n    \n    ValueError: at site \"class_labels\", invalid log_prob shape\n      Expected [-1], actual [32, 21]\n      Try one of the following fixes:\n      - enclose the batched tensor in a with plate(...): context\n      - .to_event(...) the distribution being sampled\n      - .permute() data dimensions\n\nCurrently, the batch size is 32 and the lbl\\_shape\\[0\\] is 21 (VOC Dataset (background and other labels))\n\nCould someone help me with this? It'll be very much appreciated. Thank you", "link": "https://www.reddit.com/r/learnmachinelearning/comments/itajgl/getting_an_error_in_svi_step_due_to_to_a/"}, {"autor": "_i_v_a_n_", "date": "2020-07-24 15:29:50", "content": "\"Clients/server\" AI advice needed /!/ Dear,\n\nI'm quite new to AI and need some advice from you.\n\nI have some clients that need to recognize some objects inside images.  \nIf an unknown object is present on an -----> image !!!  it will be classified (human) and then the AI must be sure to recognize it after sometime (after a training based on other -----> image !!! s)\n\nWhat I need is: \n\n* recognize done by client without server/internet connection\n* every since all -----> image !!! s captured by client N will be sent to server\n* server reinforce the AI learning with all clients' -----> image !!! s\n* each client downloads the \"learning final results\" locally in order to have an up to date ability.\n\nMy questions:\n\n* is it viable as a solution for quite offline clients (think about low/no connection working areas)\n* how (which technology) can I use to implement this \"learning final results\" transfer server-&gt;clients (in terms of AI, not FTP or something else;) )\n\nThanks a lot in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hx3tcb/clientsserver_ai_advice_needed/"}, {"autor": "Boringman_", "date": "2020-07-24 12:59:37", "content": "Help Find the Fastest People Detector in Images /!/ Hi guys, I am developing a program one of the tasks of which is to recognize people in the -----> image !!!  from a CCTV cameras. The problem is that there are a lot of cameras that the program processes (more than 30) and the program must process all images in real time. I have a limited amount of computer time, so I am looking for the most efficient neural network for recognition.\n\nI tried to use Facebook Detector2, it works correctly and even supports segmentation and keypoints detection, but it works very slowly.\n\nI also tried to use the yolov4 neural network, it works about twice as fast as detectron, but does not support segmentation and keypoints detection and slightly less accurate than the detectron.\n\nI am programming in python, and I know that there are several built-in fast human detectors (like  cv2.HOGDescriptor\\_getDefaultPeopleDetector()), but they work very badly (as in the image below).\n\n&amp;#x200B;\n\n[cv2.HOGDescriptor\\_getDefaultPeopleDetector\\(\\)](https://preview.redd.it/qiwoixi8xsc51.png?width=999&amp;format=png&amp;auto=webp&amp;s=27c52078e7cbc16885e03f095c2ad034ff23dbde)\n\nWhat fast and accurate people detectors do you know? Do you know if there are person detection algorithms specifically for video streams?\n\nThanks in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hx1cat/help_find_the_fastest_people_detector_in_images/"}, {"autor": "spmallick", "date": "2020-07-24 09:32:22", "content": "CNN Receptive Field Computation Using Backprop with TensorFlow /!/ Consider an -----> image !!!  classification problem that we typically solve using a Convolutional Neural Network (CNN).  \n\n\nMost tutorials on image classification treat CNN as a black box.  \n\n\nHow do we know which part of the image the CNN saw to make the classification decision?  \n\n\nTo understand which part of the image our model is looking at, we need to calculate the receptive field by backpropagating the response at the output layer.  \n\n\nWe wrote a post about this a few months back and shared code in PyTorch.  \n\n\nWe got several requests for a Tensorflow version of the code. So, here it is  \n\n\n[https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/](https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/)\n\nhttps://preview.redd.it/k0rqdna8yrc51.jpg?width=600&amp;format=pjpg&amp;auto=webp&amp;s=1367deffb5f601322aba994ec648eafe3e5538d0", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hwysaq/cnn_receptive_field_computation_using_backprop/"}, {"autor": "an_aillen", "date": "2020-07-23 19:01:47", "content": "Creating an Environment /!/ I have been doing all of my coding on google colabs, but now that I've begun doing large amounts of -----> image !!!  recognition and it is taking a very long time, I wanted to transition to doing it on my computer as I have a graphics card (1080 ti).  I have a Windows, how should I set everything up?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hwlqtv/creating_an_environment/"}, {"autor": "joojoo1020", "date": "2020-07-23 05:02:54", "content": "Need suggestions for Chess Game /!/ So basically I am trying to develop an AI to play chess, as my first ML project, (but I am using python +12yrs :) )  \n\u00a0I have more than 3.5 M\u00a0 games by humans\u2026  \nI am wondering which ML method you recommend, so I could heavily leverage the old games, also my model can play VS itself to improve.\n\nHow do you recommend me to start? Should I use RNN?  \nShould I even bother using chess lib to generate the move as an -----> image !!! , or just try to learn using the moves ex: B.A4, W.B6...\n\nAll recommendations are welcome ..", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hw9ouk/need_suggestions_for_chess_game/"}, {"autor": "JacksonSteel", "date": "2020-07-23 01:20:53", "content": "In this deep learning assignment, why need \u201cscipy.misc.imresize(-----> image !!! , size=(num_px,num_px))\u201d instead of just \u201c-----> image !!! \u201d? /!/ &amp;#x200B;\n\n[ ](https://preview.redd.it/hjk01dyndic51.png?width=1730&amp;format=png&amp;auto=webp&amp;s=e5e822c9f47cee6e2835a6a1f1fb918fbcf75d96)\n\nmy\\_image = scipy.misc.imresize(image, size=(num\\_px,num\\_px)).reshape((1, num\\_px\\*num\\_px\\*3)).T", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hw6izj/in_this_deep_learning_assignment_why_need/"}, {"autor": "sroberts89", "date": "2020-07-22 21:11:02", "content": "SKImage - Appending/Updating collections /!/ I am importing an -----> image !!!  sequence using imread_collection. Each image has a lot of white space I want to remove, so I am (successfully) using a homebrew function that crops each image to remove most of the extra space. But I am not able to update my image collection with the new data. \nWith the code below, I get the error \"'ImageCollection' object does not support item assignment\"\n\n    ImSeq = imread_collection('path/to/file/*.png')\n    for i in range (len(ImSeq)):\n        temp = ImSeq[i]\n        temp = rgb2gray(temp)\n        temp = Crop_Image(temp)\n        ImSeq[i] = temp\n\nAs a workaround, I am saving the images in a temp folder, and reimporting the new image sequence to a collection, but I am sure there's a faster way. My workaround code is below. Is there a way to create a new image collection that doesn't require saving temp?\n\n    ImSeq = imread_collection('path/to/file/*.png')\n    for i in range (len(ImSeq)):\n        temp = ImSeq[i]\n        temp = rgb2gray(temp)\n        temp = Crop_Image(temp)\n        temp = img_as_ubyte(temp)\n        io.imsave(\"path/to/temp%d.png\"%(i),temp);\n    ImSeq = imread_collection(\"path/to/temp/*.png')", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hw29mp/skimage_appendingupdating_collections/"}, {"autor": "OnlyProggingForFun", "date": "2020-07-22 14:33:38", "content": "DeepFaceDrawing Generates Real Faces From Sketches. -----> Image !!! -to-image translation in 2020+, is it biased, could it be used in a real world application? Paper explained", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hvuu1e/deepfacedrawing_generates_real_faces_from/"}, {"autor": "meeep08", "date": "2020-07-22 09:55:16", "content": "Deconvolution with a known kernel, or more generally how to encode suplimentary data with the input to a cNN? /!/ Hi, this might be a poorly formed question so any help is appreciated.\n\nI'm running a physics simulation which requires some very costly de-convolution steps. We have itterative solvers for this that we can run with any accuracy but if we could generate close initial guesses for the solvers they would run a lot faster overall. I want to use a NN to generate these guesses. We always have the full result of the convolution and one of the convolution pairs, we are trying to solve for the other. (There are also some slightly complicated constraints on what we are trying to find so we can't solve directly)\n\nTo me this seems very like -----> image !!!  de bluring and I have read some papers around it but, from reading, it is apprent that knowing the convolution kernel which was used to do the bluring is not normal at all (and would basically be cheating for the task). I should also mention that it is a different but still known kernel every time. I would like to pass the network the result of the convolution and the kernel and it spit out an answer. \n\nDoes anyone have any experiance of encoding this sort of extra information? The only CNN's I have made so far just have a single image input, it's not really clear to me how extra information should be passed in? Any help would be appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hvr0f8/deconvolution_with_a_known_kernel_or_more/"}, {"autor": "helpamonkpls", "date": "2020-07-22 08:51:32", "content": "Difference between -----> image !!!  segmenting and CNN's? /!/ In healthcare setting, we have images such as MRI images where as far as I can read, you can choose to manipulate this data numerically either by manually segmenting parts of the image using LifeX or Freesurfer for example, which spits out numbers. Or I could use a CNN to read the image.\n\nWhat is the difference between these two methods? Pros? Cons?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hvqas6/difference_between_image_segmenting_and_cnns/"}, {"autor": "jamestuckk", "date": "2020-01-20 17:26:40", "content": "What's the secret sauce in finding pixel-wise masks? /!/ What is the mechanism that is responsible for generating pixel-wise masks in networks that do -----> image !!!  segmentation? I've seen that Mask R-CNN can do this. Is this the first model introduced that could do this? Specifically I'm interested in knowing beyond stacked layers of convolution, what pieces are necessary to track pixel-object instance correspondence. Paper and blog post links are welcomed!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ergcca/whats_the_secret_sauce_in_finding_pixelwise_masks/"}, {"autor": "QuietCow5", "date": "2020-01-20 13:45:10", "content": "Ask me about my project /!/ I'm currently doing my bachelor's degree and I'm working on automated yield estimation of oranges. I've created a basic idea document for you guys to see what the project is actually about. I would love to hear your thoughts and questions that come in your mind because I have my project defense coming up in a few days. So throw your questions at me.  \n\n\n**Automated yield estimation of oranges:** \n\nPakistan is among the top ten producers of oranges. Due to this, most of the target export markets of oranges are those of developing countries. This leads to the increasing demand for oranges during winters which puts a significant burden on the agriculture sector. Modern technologies are not being adopted by farmers in Pakistan, which propels them to manually count the fruit for yield estimation. This increases manual labor and decrease in overall profit. With the use of automated yield estimation, current production can be maximized through automation, -----> image !!!  processing, computer vision, artificial intelligence, and machine learning. The estimated yield of Oranges on trees will be given as a result. In the long run, automated yield estimation can reduce labor force requirement, decrease costs, increase efficiency and boost agricultural production.\n\nDrones will be used to capture 360\u00b0 images of trees. With the help of image processing and AI/ML techniques images will be segmented and data of the oranges will be extracted from images. Global coordinates will then be allocated to record various detections of the same orange to reduce over counting.\n\n**The main objective** of this project is to estimate the yield of oranges on trees with minimum error as well as improved accuracy to enhance the quality of oranges, which, in turn, will reduce operating costs. \n\n**In conclusion**, Pakistan needs automated yield estimation to increase its agricultural output. Not only will it speed up oranges harvesting, but it will also prevent oranges from rotting before reaching its destination. Automated yield estimation is needed that may enable the local farmers to make their livelihood under harsh situations. In summary, the use of automated yield estimation could mean more income for farmers, more taxes and foreign reserves for the government.\n\n&amp;#x200B;\n\nPlease let me know what you think of this and if you have any questions.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/erddjv/ask_me_about_my_project/"}, {"autor": "Unturned3", "date": "2020-01-20 08:43:52", "content": "Age estimation accuracy plateaus at 60% /!/ Hello,\n\nI've been trying to train a CNN to take a face -----> image !!!  as input and assign an age label to it. The labels are integers from 0 to 5, which corresponds to the following age groups: 0-6, 6-18, 18-25, 25-35, 35-60, 60+. I'm using roughly 23k cropped face images from the UTKFace dataset with their age labeled.\n\nSo far I've tried to train the standard ResNet and MobileNet v2 models on the dataset, and their validation accuracy all stops improving after hitting 60%. I have tried many things, such as decreasing the learning rate over time (using a pytorch scheduler), increasing the batch size, adding random horizontal flips to augment the dataset, etc. but nothing seems to substantially improve the model's performance. The highest I've ever got is just merely 63%.\n\nDoes anyone know what should I do in this case? What could the problem possibly be (model capacity, overfitting, etc.)?\n\nAny help is appreciated! I've been stuck on this problem for a week now...", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eranop/age_estimation_accuracy_plateaus_at_60/"}, {"autor": "imp3order", "date": "2020-01-19 23:38:18", "content": "How long does it take? /!/ For a school project, I want to develop a machine learning algorithm that processes images (of a specific \"thing\") and spits out some quality factor or rating. I sort of understand how the process goes. But I am concerned that, since I'm a nooby software developer/hacker, I won't be able to learn and develop all that's necessary in time to submit the project.\n\nI guess my question is, how long does a first timer take to develop an -----> image !!! -processing algorithm, given a pre-existing data set? more so, taking into account the fact that I have very little software experience?\n\nThanks in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/er4fdx/how_long_does_it_take/"}, {"autor": "kb10241024", "date": "2020-01-19 21:01:53", "content": "Getting started with Deep Learning, -----> Image !!!  classification. /!/ Hola guys,\n\nThis is a beginner-friendly Github repo for getting started with Deep-Learning for Image classification tasks with a large variety of examples and self-explanatory google-colab notebooks, taken from Tensorflow.ai.  \nPlease have a look :)  \n[Image Classifier models](https://github.com/kb10241024/CLASSIFIER_MODELS)\n\nHappy Learning.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/er2879/getting_started_with_deep_learning_image/"}, {"autor": "BkkReady", "date": "2020-01-18 16:00:22", "content": "Learning ML for -----> photo !!!  retouching: where to start? /!/ I\u2019m an experienced retoucher with minimal programming knowledge. Is there a good online course that will teach some of the fundamentals related to this kind of image workflow?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eqi2sa/learning_ml_for_photo_retouching_where_to_start/"}, {"autor": "cmillionaire9", "date": "2020-01-18 12:33:38", "content": "Model only needs one face -----> image !!!  for each person", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eqfvk5/model_only_needs_one_face_image_for_each_person/"}, {"autor": "strongEverDay", "date": "2020-01-18 12:28:14", "content": "Can't use styleGAN encoder pretrained model /!/ I am using the repository for  [styleGAN encoder](https://github.com/Puzer/stylegan-encoder), but can't use the file encode\\_-----> image !!! s.py to encode my -----> image !!! . The error is \n\n`Traceback (most recent call last):   File \"encode_images.py\", line 80, in &lt;module&gt;     main()   File \"encode_images.py\", line 50, in main     with dnnlib.util.open_url(URL_FFHQ, cache_dir=config.cache_dir) as f:   File \"/content/stylegan/dnnlib/util.py\", line 378, in open_url     raise IOError(\"Google Drive quota exceeded\") OSError: Google Drive quota exceeded`\n\nI tried copying it in my drive, but now I am getting \n\n`Traceback (most recent call last):   File \"encode_images.py\", line 80, in &lt;module&gt;     main()   File \"encode_images.py\", line 51, in main     generator_network, discriminator_network, Gs_network = pickle.load(f) _pickle.UnpicklingError: invalid load key, '\\x0a'.`\n\nThis [Stackoverflow Post](https://stackoverflow.com/questions/33049688/what-causes-the-error-pickle-unpicklingerror-invalid-load-key) says that the pickle might be corrupted during copying. I am confused what to do, I can't train that much huge model on my own. Btw, I am using Google colab itself.\n\n&amp;#x200B;\n\nHere is the Google drive link for the pickle file.\n\n[karras2019stylegan-ffhq-1024x1024.pkl\\~300MB](https://drive.google.com/uc?id=1MEGjdvVpUsu1jB4zrXZN7Y4kBBOzizDQ)\n\n&amp;#x200B;\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eqftsq/cant_use_stylegan_encoder_pretrained_model/"}, {"autor": "cipher89", "date": "2020-01-18 11:21:55", "content": "Tensorflow - How to predict -----> image !!!  using .pb file in Tensorflow.JS? /!/ Is there any way how to predict .pb file that generated by my python script ([https://mnurdin.com/easy-image-classification-with-tensorflow](https://mnurdin.com/easy-image-classification-with-tensorflow)) using Tensorflow.JS? I would like to use my .pb model deploy in my ionic app (attendance system) and predict the faces. But what I heard I need to convert the .pb file into .tflite file first then deploy.\n\nAny idea so far? Please advise.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eqf9ue/tensorflow_how_to_predict_image_using_pb_file_in/"}, {"autor": "sh0rt_boy", "date": "2020-01-18 11:06:54", "content": "[Tensorflow OD API] - SSD_mobilenet model does not learn - too big images? /!/ I am trying to train a model for airplane object detection on (big: 3000x3000px) satellite images. \n\nTherefore i am using the TFODAPI and the model \"ssd\\_mobilenet\\_v1\\_coco\" from the TF modelzoo.\n\nNow i do have several questions:  \n1. the standard input size of the ssd network is 300x300px. Thats way to small for my big satellite images which display 1 square kilometer. Airplanes are quite small but easily to spot with human eyes. \n\nCan i just change the input size of the network to 3000x3000px? how does that affect the performance / does it even work?\n\n2. As it seems, it does not work so far for me. As i am training on the big -----> image !!! s, the model can't predict any airplane, as you can see in the -----> image !!!  (left: predictions, right: gound truth). Furthermore Tensorboard does not display the correct label\\_name. Maybe there is some issue with loading the right labels? It only displays: \"N/A\".\n\nMy label\\_map looks like this: \n\n`item {`\n\n`id: 11`\n\n`name: 'Fixed-wing Aircraft'`\n\n`}`\n\n`item {`\n\n`id: 12`\n\n`name: 'Small Aircraft'`\n\n`}`\n\n`item {`\n\n`id: 13`\n\n`name: 'Cargo Plane'`\n\n`}`\n\nAfter 50 epochs the loss plateaus out and there is still no true prediction.\n\nAny ideas?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/eqf5fm/tensorflow_od_api_ssd_mobilenet_model_does_not/"}, {"autor": "SergiosKar", "date": "2020-04-11 12:53:53", "content": "Deep learning in medical imaging - 3D medical -----> image !!!  segmentation with PyTorch", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fz4mb9/deep_learning_in_medical_imaging_3d_medical_image/"}, {"autor": "baptofar", "date": "2020-04-11 10:51:22", "content": "Used Keras to visualize the filters most activated by a -----> picture !!!  in a CNN", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fz2hk3/used_keras_to_visualize_the_filters_most/"}, {"autor": "baptofar", "date": "2020-04-11 10:48:40", "content": "Used Keras to visualize the layers most activated by a -----> picture !!!  in a CNN", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fz2g06/used_keras_to_visualize_the_layers_most_activated/"}, {"autor": "james_marsden", "date": "2020-04-10 21:56:09", "content": "Reshaping 91,500 rows of 2D arrays (pixel data) into 91,500 rows of 1D arrays /!/ # Brief problem\nI have a numpy array with shape (91500,). Each row contains a 2D array that is shaped (60, 80). I'm trying to reshape each row to be 1D with length 4800 so that my resultant numpy array would be (91500, 4800).\n\n# Longer version with some context\nI have 91,500 unlabeled grey scale images of 60 x 80 pixels. My actual goal is to run the images through kmeans so I can start manually labeling the images.  \n\nI couldn't shove my raw data into kmeans, and thought it would work better if i had one row per -----> image !!!  and each column was one of the 4800 pixels as the features.  \n\nThere might be a much smarter way to do this.\n\n# Things I've tried\nIn pandas I can use `df[col].apply()` to run a function that will join all pixel values, but this eats up crazy ram on my computer. I'm assuming that since my data is in numpy already, there's a faster and more efficient way than using pandas.  \n\nI looked into `np.vectorize()` as a numpy equivalent for `pd.Series.apply()` but I could not get it iterate through each row. I am probably just doing it wrong.  \n\n# Thanks!\nAny help and advice is appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fypss8/reshaping_91500_rows_of_2d_arrays_pixel_data_into/"}, {"autor": "10macattack", "date": "2020-04-10 21:45:47", "content": "Reusing input in neural network /!/ Ok so I've never done this before, I'm planning on teaching a neural network how to check if an -----> image !!!  is a dog or cat, I want it to be about 95% effective or so, just trying to learn how to make one. Would it still work if I download \\~2000 images of dogs and cats, and reuse the image when showing the neural network? For example, iterate randomly through the images, then once all have been shown, iterate through the list again but in a different order, reshow inputs.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fyple6/reusing_input_in_neural_network/"}, {"autor": "shapka-pa-gisht", "date": "2020-04-10 11:51:24", "content": "Line-based Image Segmentation using Deep Learning /!/ For a project, I have a dataset of some medical scans from different patients. The dataset consists of 510 grayscale 4000x2000 images. These -----> image !!! s contain a lot of useless information that I do not need, meaning that I can segment the -----> image !!!  so that I only get the useful part of the data.\n\nMy task is to do this using a line-based approach, meaning that I should be able to input a 4000x2000 image and get a vector of dimension 1x2000 as an output. [Here is a rough example of what the input and the output should look like.] (https://imgur.com/a/UJq2vMq) What type of architecture or what type of method should I use to achieve my desired result? Note that it should be a Deep Learning approach, as I will have to compare it with conventional image processing later.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fyejlv/linebased_image_segmentation_using_deep_learning/"}, {"autor": "gungunthegun", "date": "2020-11-11 03:44:46", "content": "Map Generation Project Help /!/ **Project Description**\n\nThe idea for this project comes from the desire to create world maps for fantasy games or imaginary worlds as part of worldbuilding projects and the like. I've seen many world map generators where you are able to set certain parameters (% of water, amount of detail, etc.), but I haven't seen one where you might be able to specify a generic outline of your world and the rest is generated. While in most cases, where somebody might generate a map just to get started, this wouldn't really be necessary, if you had an idea for how the world might look and you just need more detail, this project could be perfect for you. In this project, I would like for the user to be able to draw a map of coastlines and the edges of continents or islands. The network would then fill in the rest of the area with mountains, valleys and similar land structures. The network would only be responsible for generating the heightmap and not any moisture or biome data.\n\n**Background**\n\nWhile I have taken a class on deep learning, it didn't really provide the necessary experience for implementing something like this. It mostly dealt with the theoretical aspects of machine learning, which were very dense, and there wasn't much hands-on learning. On the other hand, I am in my final year as a software engineering student, so I feel I have a decent idea of common practices outside of machine learning.\n\n**The Problem**\n\nOne of my main inspirations for this project was NVIDIA's GauGAN, and this project seemed like a good use case for their semantic -----> image !!!  synthesis and a GAN. My first question has to do with whether or not this type of technology would be too complicated for this project. Would it be possible to implement this easier with an autoencoder or some other kind of network or algorithm entirely?\n\nNext, I'd like to ask about the data representation. To keep it simple, I'm thinking of starting with a 500x500 grid of values. I'll generate the heightmaps using some type of noise function and use edge detection or something similar to extract the coastlines. In short, I'm attempting to generate my own dataset for this. I'm thinking that these would be the inputs and labels for the neural network. So far, I'm struggling finding the best way to represent this data. I've started by storing the heightmaps in a CSV file where each row is a flattened heightmap. This leads to a huge file size and I'm concerned about loading this file into memory for training. This brings me to my next question.\n\nI'm familiar with python being a language commonly used with TensorFlow for machine learning tasks, but I'm also aware that languages like R are good for data science and similar things. I'm not familiar with how to load in files like those that might be generated earlier into a TensorFlow program or similar. Would this be a use case where a database might be used? Is python even a good choice for this kind of project? TensorFlow?\n\nI realize that this is a lot of questions for a single post, but I really am new to this type of programming and designing. Any type of feedback would be greatly appreciated. Thank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/js0o4y/map_generation_project_help/"}, {"autor": "SQL_beginner", "date": "2020-11-10 17:42:41", "content": "What kind of data is this? Panel Data? Longitudinal Data? Cross Section Data? Time Series Data? /!/ I created some fake data that represents the problem I am working with :\n\nhttps://imgur.com/a/jWsmulO\n\nIn short: Patients arrive at the hospital and have their \"health measurements\" taken. It is also recorded whether they were immediately discharged from the hospital, or if they were required to stay. Given a new patient that enters the hospital and their health measurements are taken, I need to predict the response_variable for this patient - will they be immediately discharged from the hospital, or will they have to stay?\n\nI have data from the last 50 years. It is obvious, that society, people's health and medicine have changed over the last 50 years (e.g. life expectancy has increased in many parts of the world). Perhaps in 1971, an elderly man who smoked his entire life would have had to stay for a prolonged period in the hospital - but in 2019, he might have been given a new drug that was developed in the last 5 years and then immediately discharged. The big question is : do these macro and micro time dependent trends within the variables need to be taken into account when creating statistical models?\n\nSo far, I created a decision tree (C5 algorithm) which completely ignores the \"time\" variable. That is, it is assumed that an elderly man from 1971 who smokes is identical to an elderly man that smokes in 2019. By completely ignoring the \"time\" variable, I was able to create a standard decision tree model that produces good results. I tested this two different ways:\n\n1st way: I trained the decision tree using a random 70% sample of the data, and recorded its performance on the remaining 30% of the data.\n\n2nd way: I kept data from 2019 separate. I then trained a decision tree using using data from previous years. Then I recorded the performance of the decision tree on 2019's data.\n\nIn both tests, the performance of the decision tree was good enough for my requirements.\n\nBut is just makes me curious: (looking at the imgur -----> picture !!! ) what kind of data do I have? In statistics, is this considered Panel Data? Cross Section Data? Longitudinal Data? Time Series Data?\n\nI have a feeling that is not longitudinal data - as far as I understand, longitudinal data repeatedly records measurements on the same patient (e.g. once every year). As far as I am concerned, all patients in my data are unique. Perhaps I am dealing with multivariate time series data?\n\nCan someone please help me identify what kind of data this is? And what models are usually used for classifying this kind of data ?\n\nMy model results are good enough as is, but I am still curious.\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jrpitu/what_kind_of_data_is_this_panel_data_longitudinal/"}, {"autor": "Wrap_Speed", "date": "2020-11-10 07:52:10", "content": "Help in trying to create a mathematical expression identifier /!/ I am trying to get a general idea of how to create a mathematical  expression identifier. Here the expression will be fed as an -----> image !!!  and i  am able to break it into component. I am also able to recognize various  digits and operators what i am having problem with is how to evaluate  fractions and exponents in this Since the fraction can be very recursive  in nature(One fraction inside another fraction) If someone can point me  in right direction or give me some resource which i can refer to  complete this task will be very helpful", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jrgytm/help_in_trying_to_create_a_mathematical/"}, {"autor": "Responsible_Skill820", "date": "2020-11-10 07:13:51", "content": "Function of colorization loss /!/ I started to paint and I got this popular paper, \"Coloring Paint 1,\" but I don't really understand that the function of this paper is disappearing because the math is not so good. I tested some applications on the PapersWick code site, but it didn't help much. If anyone can clarify this article or share some good resources for a better understanding of this article and other articles in the -----> image !!!  collage, I would appreciate it. Thanks in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jrgitz/function_of_colorization_loss/"}, {"autor": "Vendredi46", "date": "2020-11-10 07:08:48", "content": "Regarding datasets for machine learning /!/ Hey all, I'm just getting started learning ML and I'm interested in using random forests particularly for detecting street signs.\n\nNow, I have downloaded a data-set of street signs in a number of varieties, such as graphics of the street signs such as drawing of street signs or vectors of them, small pictures of actual street signs, street sign symbols without the signboard. \n\nMy ultimate goal is to be able to detect these signs from a still -----> picture !!! ; how do I go about deciding which of these to use? How do I know if one is better than the other.\n\nFinally, assuming I made the right choice in that step, is it safe to start using them as-is? Like I have 6000 signs downloaded, should I start dividing them 80-20 for the training and test process?\n\nThank you for your insight!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jrggq5/regarding_datasets_for_machine_learning/"}, {"autor": "Haztec2750", "date": "2020-11-09 21:56:29", "content": "How do I decode the data in an LSTM? /!/ If I have an LSTM model where I use the keras tokenizer to convert the words to unique integers and then put this into a model like the one depicted by the -----> image !!! , how would I then turn the output at the decoder which would be numbers between 0 and 1 (because I used softmax) back into words?\n\n[https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639](https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639)\n\n\\^ This article says that I'm looking for \"reversed vocabulary\".", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jr7dra/how_do_i_decode_the_data_in_an_lstm/"}, {"autor": "tzaddiq", "date": "2020-11-09 20:23:19", "content": "Knowledge base for the black magic of deep learning /!/ Is  there a central resource where one can aggregate the voodoo learned in  the field about what works and what doesn't in deep learning?\n\nOne  way to figure it out is to learn by experience, but that's a lot of  effort per bit. Smarter is to learn from other's experience, which to me  means digesting numerous papers or GitHub repos. Even this is a lot of work; one paper's approach is but one sample in a distribution, when you just want the *mode*  (the 'best practice'). Secondly, papers often just report what worked,  not what didn't, and provide scarce justification for their recipe.  Finally, the selection bias means a lot of experience gets shredded  because papers of failed models don't typically get accepted in  journals.\n\nThere are so many loss  functions, activation functions, optimizer parameters, architectures,  regularization tricks, that these form a hyper-parameter space too large  for individuals to explore.\n\nAnd  while the highest level of best practices exists, usually in books, they  don't (to my knowledge) give  the granular info you need to know when  implementing a real system.\n\nHere are the *kind*  of best practices it would be nice to learn (note: these are just for  the purposes of clarifying intention, not necessarily accurate):\n\n1. *Use a 5x5 kernel size on the first layer of an -----> image !!!  CNN, and 3x3 in deeper layers*\n2. *Representations of a signal with X amount of entropy will need at least a depth of Y layers and embedding size Z*\n3. *To increase orthogonality in filter maps, add* this *loss term*\n4. *To prevent mode collapse in ABC-GAN, normalize* this *layer, add noise here, add this loss term, etc*\n5. *Use a denormalization layer when your multiple real outputs have distinct distribution params (mean, variance)*  \\- [https://youtu.be/JQxAGhhflDc?t=1036](https://youtu.be/JQxAGhhflDc?t=1036)\n6. *For NLP tasks use GLU activations (ref: GPT)*\n7. etc", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jr5huc/knowledge_base_for_the_black_magic_of_deep/"}, {"autor": "dark_--knight", "date": "2020-11-09 13:53:26", "content": "How should I approach my object detection problem? /!/ Hi, I am kinda new in this field.\n\nI am tasked with  object(vehicles) detection problem. The data set is composed of vehicle -----> image !!! s, where an -----> image !!!  contains a vehicle of one or more of 21 different classes of vehicle. \n\nHow should I approach this problem? \n\nLearn and try various object detection algorithms and find out which works best ?\n\nOr should I choose a popular algorithm and try to improve my model ( if so suggest me one)? \n\nI need better  accuracy. \n\nThanks in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jqxyws/how_should_i_approach_my_object_detection_problem/"}, {"autor": "ThickDoctor007", "date": "2020-11-09 13:51:42", "content": "Understanding consecutive convolutional layers /!/ Hi,\n\nWhile trying to re-implement Darknet-53 architecture (and later on YOLOv3), I would like to understand the numbers in the Darknet-53 representation table:\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n[Darknet-53](https://preview.redd.it/shgd4iy6w7y51.png?width=550&amp;format=png&amp;auto=webp&amp;s=9403fee2556f3f609eedd34d88d67382e40e1057)\n\nFirst, I noticed in other blog posts, that the input size is 416x416 (for example, [in this blog post](https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b)) .Therefore, for the first layer (32 filters, 3x3 kernel size), I don't know how output can be 256x256. \n\nAlso, according to the blog post:\n\n&gt;**YOLO v3 makes prediction at three scales, which are precisely given by downsampling the dimensions of the input -----> image !!!  by 32, 16 and 8 respectively.**  \n&gt;  \n&gt;The first detection is made by the 82nd layer. For the first 81 layers, the image is down sampled by the network, such that the 81st layer has a stride of 32. If we have an image of 416 x 416, the resultant feature map would be of size 13 x 13. One detection is made here using the 1 x 1 detection kernel, giving us a detection feature map of 13 x 13 x 255.  \n&gt;  \n&gt;Then, the feature map from layer 79 is subjected to a few convolutional layers before being up sampled by 2x to dimensions of 26 x 26. This feature map is then depth concatenated with the feature map from layer 61. Then the combined feature maps is again subjected a few 1 x 1 convolutional layers to fuse the features from the earlier layer (61). Then, the second detection is made by the 94th layer, yielding a detection feature map of 26 x 26 x 255.  \n&gt;  \n&gt;A similar procedure is followed again, where the feature map from layer 91 is subjected to few convolutional layers before being depth concatenated with a feature map from layer 36. Like before, a few 1 x 1 convolutional layers follow to fuse the information from the previous layer (36). We make the final of the 3 at 106th layer, yielding feature map of size 52 x 52 x 255.\n\nBy reading the original article and blog posts, I would like to understand how the stride at the arbitrary layer is calculated and how to calculate the size of feature maps.\n\nAgain, by observing the above table, I don't understand how the original image of size 416 x 416 was transformed to the output of size 256 x 256 after the first convolutional layer. Also, the output size numbers, I don't how they represent the stride instead of feature map size instead and what is the rationale behind calculating feature map size by dividing the original image by the number of strides.\n\nI would appreciate it if anyone helped out clarify the above numbers.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jqxxy8/understanding_consecutive_convolutional_layers/"}, {"autor": "untitled20", "date": "2020-11-09 13:36:46", "content": "Can I skip some geometry topics (congruence, similarity, etc) while learning maths for ML? /!/ What the title says.\n\nI'm really bad at visual problem solving. I love dealing with numbers, but when I'm shown a triangle and asked to guess what it would look like if rotated by 180 degrees, I find that really hard to do because I'm bad at visualizing things. I also feel like its vague and 'subjective'.\n\n&amp;#x200B;\n\nAre topics like these, where you're asked to look at a -----> photo !!!  and answer questions about it / make 'guesses', useful for ML? Specifically congruence, similarity, etc as it applies to geometric shapes?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jqxpqo/can_i_skip_some_geometry_topics_congruence/"}, {"autor": "Curious_Log_84", "date": "2020-11-09 12:45:58", "content": "Shapelet Task Classification /!/ Hi all, I am trying to achieve Task Classification of multivariate time-series data (see pseudo data in -----> image !!! ). The classifier should be able to detect multiple classes within one time series and if possible the time stamps at which theses occur. \n\n I have researched various types of TSC from DTW with KNN to Shapelet Transforms, however I am not sure as to how methods other than Shapelet Transforms can Classify multiple classes within one time series. \n\nThe main issue could be that the amount of data I can produce is limited to say 50 time series at most. Thus, deep learning methods such as conversion to recurrence plots are off the table. \n\nCould you point me in the right direction, or even sample code for this problem?\n\nGreatly Appreciate it!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jqwywh/shapelet_task_classification/"}, {"autor": "alfredborden00", "date": "2020-01-09 01:00:56", "content": "cross-entropy equation and notation /!/ I'm working through the intro to deep learning with pytorch course on Udacity, and I'm a bit stuck on understanding the cross-entropy equations that have been presented.\n\n1. The equations for binary classification and multi-class classification are given as shown in the attached -----> image !!!  (top). Why does an example with y = 0 need to have its CE calculated for binary classification, but not for multi-class classification? \n2. I've also attached an image of a lecture slide (bottom). The example being used is that there are 3 doors, behind which there is either a duck, beaver or walrus - the probability for each animal (class) being behind the door is given for each door. The lecturer explained that p\\_11 is the probability of finding a duck behind door 1, p\\_12 is the probability of finding a duck behind door 2 etc. In the multi-class CE equation, what are the notations (the lecturer did not fully explain)? I was thinking that:\n\n* i = class\n* j = a training example\n* m = training examples\n* n = number of classes\n\nBut I saw somewhere that n = the number of features, and the lecturer said that m = number of classes, which I really do not understand. Would anyone be able to help?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/5sebg98ijn941.png?width=1360&amp;format=png&amp;auto=webp&amp;s=e3404746ecb00be24137756862f54f21ab2da3b8\n\nThank you very much", "link": "https://www.reddit.com/r/learnmachinelearning/comments/em23iq/crossentropy_equation_and_notation/"}, {"autor": "joshua_y", "date": "2020-01-08 17:04:55", "content": "Predicting future order dates and order amounts for customers with online retail data /!/ I originally posted this question [here](https://datascience.stackexchange.com/questions/65989/predicting-future-order-dates-and-order-amounts-for-customers-with-online-retail), but it hasn't really gained any traction.  I just found this community on reddit and thought you guys might have some interesting insights!  I'm still trying to ramp up with self-education in the ML space, and most of my work so far has been in the -----> image !!!  classification/robotics space so this is a fairly new domain.\n\n&gt;Using the online retail II dataset ([https://archive.ics.uci.edu/ml/datasets/Online+Retail+II](https://archive.ics.uci.edu/ml/datasets/Online+Retail+II)), I'm trying to predict when each customer will place subsequent orders, and if possible, the monetary value of those orders. This will be a proof of concept for a real online retail store, in which I'll have access to even more data/features.  \n&gt;  \n&gt;This current dataset contains the following attributes:  \n&gt;  \n&gt;**InvoiceNo**: Invoice number. Nominal. A 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'c', it indicates a cancellation.  \n&gt;  \n&gt;**StockCode**: Product (item) code. Nominal. A 5-digit integral number uniquely assigned to each distinct product.  \n&gt;  \n&gt;**Description**: Product (item) name. Nominal.  \n&gt;  \n&gt;**Quantity**: The quantities of each product (item) per transaction. Numeric.  \n&gt;  \n&gt;**InvoiceDate**: Invoice date and time. Numeric. The day and time when a transaction was generated.  \n&gt;  \n&gt;**UnitPrice**: Unit price. Numeric. Product price per unit in sterling.  \n&gt;  \n&gt;**CustomerID**: Customer number. Nominal. A 5-digit integral number uniquely assigned to each customer.  \n&gt;  \n&gt;**Country**: Country name. Nominal. The name of the country where a customer resides.  \n&gt;  \n&gt;My current intuition is to model the data as a *daily time series* that would output which customers are likely to order for the day and the value of those orders (if possible?). The output would either be a sparse, one-hot encoded matrix of shape num\\_customers X num\\_customers, or a more dense vector of size num\\_customers(which could have thousands of customers). I'm not quite sure how, or even if the model would be able to also output the amount of the order. This model will likely be fed into an LSTM network or similar. I suppose this would be considered a *multivariate time series forecast* (still learning here)?  \n&gt;  \n&gt;From this, I'm hoping to be able to run the prediction for each day in the following year to forecast the next order date, customer value and churn risk  \n&gt;  \n&gt;Since this would be an attempt to model human behavior as a time series, without a direct correlation from one day to the next (in contrast to predicting daily temperature or a stock price), and with so many variables, I'm also worried it would just come off as unpredictable noise.  \n&gt;  \n&gt;1. Anyway, am I on the right track? (any insights would be greatly appreciated)  \n&gt;  \n&gt;2. Should this be broken into multiple models, and if so, how many?  \n&gt;  \n&gt;3. Does anyone have any tips, or know good articles or tutorials that could point me in the right direction?  \n&gt;  \n&gt;4. What are the merits of the one-hot encoded matrix, vs the multi-hot encoded vector (not sure if that's the proper name)?  \n&gt;  \n&gt;  \n&gt;  \n&gt;Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/elvh6n/predicting_future_order_dates_and_order_amounts/"}, {"autor": "bhatta90", "date": "2020-01-08 16:53:49", "content": "Google cloud /!/ Guys, I am new to GC platform. I am doing a deep learning based project for segmentation. \nI have downloaded the -----> image !!!  data and the segmented output. \n\nHow can I import the data in GC? And what should I code so that my main python file will import these images(training data) from the GC?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/elvbj1/google_cloud/"}, {"autor": "scott-gleeson", "date": "2020-01-08 06:11:48", "content": "To buy a computer vision &amp; OpenCV course as a Group. /!/ Hello Everyone,\n\nHope you guys having an wonderful day. I'm so interested in learning computer vision &amp; OpenCV and build something amazing. But resources to learn ML properly is so less. Past month I was looking for a CV &amp; OCV course to buy, and I found an amazing course by a instructor called  **Adrian Rosebrock.**   He is a Ph.D and entrepreneur that has spent his entire life studying computer vision, machine learning, and -----> image !!!  search engines. Over the past 2 years alone. but the course price is a little bit expensive for a student like me who struggling a lot with financial problems daily basis. the course is around  $995. So thought there is a lot of people who also interested in learning computer vision &amp; OpenCV like me, so I decided to find people who interested in buying this course as a group. We have 7 people in the group already bringing price down to $142.14 for each individual person. but for us it's still a little bit expensive. so...\n\n&amp;#x200B;\n\n**Please send me a message if you interested in buying this course as a group.**\n\n&amp;#x200B;\n\nHere is the course link: [https://www.pyimagesearch.com/pyimagesearch-gurus/](https://www.pyimagesearch.com/pyimagesearch-gurus/)\n\nThank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/elosd2/to_buy_a_computer_vision_opencv_course_as_a_group/"}, {"autor": "badrobit", "date": "2020-01-07 13:30:49", "content": "Rotation of images being input into pose detection (OpenPose) /!/ I am using the OpenPose models to attempt to determine the pose of a person in a webcam feed. The -----> camera !!!  is attached to a mini robot and doesn't always capture the images from the same angle causes the people in the images to not always appear to be vertical. This causes issues with the pose detection. \n\nI was wondering if there is a well known method to correct the orientation in these photos. I have thought about checking for lines and trying to ensure they are mostly vertical but this doesn't always work. This is not on a live feed so taking time to pre-process the images is not an issue. \n\nThanks in advance for your help!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/elbkq3/rotation_of_images_being_input_into_pose/"}, {"autor": "tylersuard", "date": "2020-12-17 15:51:16", "content": "Actual, real-life machine learning projects clients have asked me to complete /!/ Hello.  It can be hard to know what skills are useful and what skills are just for fun, or what projects to do to fill out your resume.  Here is a list of actual projects that clients have contacted me to do in the past few months:\n\n&amp;#x200B;\n\n* An RNN to parse street addresses into their unique parts\n* Developing online courses\n* Generative art application\n* Tutoring and help with exams\n* Create music using facial expressions\n* Duplicate someone else's AI app/website ([emastered.com](https://emastered.com) is a common one)\n* Input a -----> photo !!!  of a person and output a 3d model\n* Using AI to predict an analyze human interactions or facial expressions\n* Convert Python2 code to Python3\n* Guess the winner of a horse race\n* Using AI to debug code\n* Creating tests and quizzes for classes\n* OCR scanning and parsing text to a database\n* Voice recognition, extracting audio from video,\n* cropping out the background music\n* Translating documents from one language to another\n* Creating games like tic-tac-toe using reinforcement learning\n* Write a social media platform\n* Help with raspberry pi\n* Generate text in a certain style using GPT-2\n* Generate 3d environments for VR\n* Applying NLP to political science for a research project\n* Machine learning app for predicting disease based on symptoms\n* Help with an ML model that is not performing well\n* Create an app to recommend makeup based on skin type\n* Predicting insurance claims outcomes\n* Startup co-founder offers\n* Educational apps\n* Trade bots\n* Differentiating between people based on data from movement sensors\n* Factory scheduling software\n* Computer vision for farming\n* LDR to HDR photos\n* AI personal assistant like Siri\n* Distributed machine learning software like Leela, the chess program\n* Guessing the price of a product based on a photo of it\n* Pattern prediction\n* Mastering audio automatically\n* Intruder detection using video\n* Web scraping using Beautifulsoup and Selenium\n* Stock price prediction\n* Crypto recommendation app\n* Eye gaze detection in photos\n* Prediction for betting\n* Setting up machine learning computers with Linux\n* Summarize text\n* Predict real estate prices", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kezvtr/actual_reallife_machine_learning_projects_clients/"}, {"autor": "Meeesh-", "date": "2020-12-16 08:15:28", "content": "What is the name of the problem of figuring out where you are in an environment? /!/ Like for an AR based application let\u2019s say you put a flag on the ground in a specific spot on your dining table. As you move around the table to the other side, the flag should stay where it is relative to the table and environment at all times.  \n  \nIs there a name for the problem of knowing a given environment and how it changes between different frames so that you can keep that AR object at the same point in the real world even when moving a -----> camera !!!  around?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ke5ay7/what_is_the_name_of_the_problem_of_figuring_out/"}, {"autor": "derzemel", "date": "2020-12-16 06:14:50", "content": "How would you do fast reliable One-Shot face recognition? /!/ Hello, \n\nI am working on a personal project (learning purposes) and I have been stumped by the face recognition part. \n\nI am starting from a single -----> image !!!  of the face (front, single face in -----> image !!! ). From this single image, I have to recognize the same person in the future. All of this to be ran on a machine with a Intel i5-7600 (4 core 4 threads processor) with no dedicated graphics card.\n\nI am using the [OpenCV's DNN](https://docs.opencv.org/4.5.0/d2/d58/tutorial_table_of_content_dnn.html) Module based on this [article](https://towardsdatascience.com/face-detection-models-which-to-use-and-why-d263e82c302c) and on this [article](https://www.pyimagesearch.com/2018/09/24/opencv-face-recognition/). \n\nNow, because I start from a single image (as a dataset), I am forced to do data augmentation on that image. I am doing [image flipping](https://pillow.readthedocs.io/en/stable/reference/ImageOps.html#PIL.ImageOps.mirror) from left to right, [CLAHE](https://www.geeksforgeeks.org/clahe-histogram-eqalization-opencv/), [autocontrast](https://pillow.readthedocs.io/en/stable/reference/ImageOps.html#PIL.ImageOps.autocontrast), [blurring](https://docs.opencv.org/4.5.0/d4/d13/tutorial_py_filtering.html), [solarize](https://pillow.readthedocs.io/en/stable/reference/ImageOps.html#PIL.ImageOps.solarize), [color invert](https://pillow.readthedocs.io/en/stable/reference/ImageOps.html#PIL.ImageOps.invert), grayscale, adding [noise](https://scikit-image.org/docs/dev/api/skimage.util.html#skimage.util.random_noise), clipping parts of the face (chin or forhead or both), a few [perspective transformations](https://www.pyimagesearch.com/2014/08/25/4-point-opencv-getperspective-transform-example/). \n\nTo this I am adding 50 faces labeled \"unknown\" and another 50 labeled \"face mask\"\n\nEach of the above generate a new image that I use for training and it all works fairly fast (depending on the number of faces to be recognized and combinations), but no mater what combination I do, the accuracy is low. \n\nHow would you approach this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ke3sy1/how_would_you_do_fast_reliable_oneshot_face/"}, {"autor": "wymco", "date": "2020-12-15 16:07:54", "content": "How to handle this -----> image !!!  classification task? /!/ I have been working on an image classification problem for a while, and I am looking for some input. I realized that classifying numbers or a full face is one type of problem, but classifying a small feature on a human face is another. In my case, it is classifying a tiny scars on the face. I have done a lot of preprocessing, but most of them are not useful in my case. Because of that, I am either getting over-fitting or no learning....\n\nHowever, I would like to try training on the scars only, and see if that can work when a full profile is presented. Has any of your worked on such a problem before? Any tips to help me?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kdoh5q/how_to_handle_this_image_classification_task/"}, {"autor": "ShlomiRex", "date": "2020-12-15 12:26:48", "content": "How to detemine what / how many layers for CNN? /!/ I am totally lost here. All the examples in google and youtube magically write and tell you what layers to use. Dense, Conv2D, Pool and more.\n\nI did not find any material or papers on how to actually CHOOSE the layers.\n\nI only know the input and output layers are supposed to be -----> image !!!  size and classifier number of classes.\n\nBut how to BUILD hidden layers?\n\nHow does one build CNN simple and start adding more layers?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kdktsc/how_to_detemine_what_how_many_layers_for_cnn/"}, {"autor": "sapnupuasop", "date": "2020-12-15 07:59:12", "content": "[Question] Classifying different gas flows in videos via infrared -----> camera !!!  /!/ Hey guys,\n\nI have a question. I am doing an internship in CV at a large company and I am the only one in my department who knows about DL. The task is to predict the amount of gas (volume flows) running through a leakage from infrared images/videos. How would you approach this task? I have collected training videos of different leakages and have trained a neural net which classifies the amount of gas on images into 4 different clusters (from nothing - .... -  40 ml/min and above) with an accuracy of about 88%. \n\nIn the end we want to take videos of a huge component and tell locations where workers need to touch up leakages. I am not sure which architecture to pick. In the video several classes of leakages can occur. I do not know the length of the video at the moment but i would guess from 3 to 6 minutes. My training videos are 30 seconds and only one class occurs. Training shows that just predicting each frame with a simple convnet is not feasible, because different perspectives can easily change the predicted class. \n\nSo I would do transfer learning with my pretrained model and make it sequential to take several previous frames into account and still predict each frame. If I would pick a lstm+convnet for example, how do I determine the sequence length in training? Should I just pick, for example, 30  frames? And in testing on the longer videos only process 30 frames at once? As I understand, sequential models are not flexible and do not work properly on sequence lengths unseen in training. \n\nIt would be so kind if some of you could give a little insight :)\n\n (Obligatory sorry for my english, not a native speaker, if something is unclear, please let me know)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kdhl3t/question_classifying_different_gas_flows_in/"}, {"autor": "mind-blown-creative", "date": "2020-12-15 05:56:08", "content": "Please help advising me in the right Machine Learning System for my project /!/ Hi there, I'm a digital creative director trying to produce some tech using machine learning and am now turning to you brilliant folks at reddit for help in the planning stage. Given my specific intentions, I'm still having difficulty selecting the best neural networks for the job. I'm hoping this high-level scope outline may help tailor your feedback recommendations to guide me in the direction I need to be taking for the planning phase. \n\n**Here's my intention:**  \n**1.** User uploads/captures -----> photo !!!  of themselves on webpage (front-end)  \n**2.** Using GAN, CNN... DCGAN (or other recommendations) the AI produces a like-like 3D Generated   \nphoto from the sample data set. (I'll leverage TensorFlow libraries and source the right GPU for   \nprocessing, manage my weights and balances and use Back Propagation for quality control etc.)  \n**3.** I then want to generate a 3D model (with life-like accuracy) to the photo's author.   \nI'll then overlay the Normal, Defuse, Specular map textures onto the 3D generated model.  \n**4.** Model is then exported in a usable web format (.fbx .gltf etc.) to be loaded into the front-end of the   \nwebsite using WebGL and three.js engine  \n**5.** Basically, user inputs a photo and (in some undetermined amount of time) a life-like 3D model of   \ntheir head is produced. I can then overlay products like sunglasses to offer a personalized   \nshopping experience. This is the intention with the MVP at least.\n\nTo achieve this end-rproduct, does anyone have any recommendations on the machine learning system(s) I need to adopt in order to help me focus my planning?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kdg12k/please_help_advising_me_in_the_right_machine/"}, {"autor": "mwitiderrick", "date": "2020-12-07 14:50:14", "content": "[D] Style Transfer in Android Applications with Fritz AI /!/ Neural style transfer is a technique used to generate -----> image !!! s in the style of another -----> image !!! . A deeper dive into the theory of style transfer can found here. In this piece, we shall focus on applying style transfer in Android applications using Fritz AI.\n\n[https://heartbeat.fritz.ai/style-transfer-in-android-applications-with-fritz-ai-9c8985b0915](https://heartbeat.fritz.ai/style-transfer-in-android-applications-with-fritz-ai-9c8985b0915)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k8hy1s/d_style_transfer_in_android_applications_with/"}, {"autor": "madzthakz", "date": "2020-12-07 14:35:17", "content": "I'm a Senior Data Scientist at Disney and I'm hosting another Data Science Q&amp;A session this Thursday @ 5:30 PM PST /!/ \\[Disclaimer: I know there's a bit of self-promo here but since these sessions have been getting great feedback from you all, I feel like it's alright. I\u2019m going to make an effort to cut down on these posts to avoid cluttering your feed. I\u2019ll be posting every 4-6 weeks from now on\\]\n\nAs the title mentions, I'm a Senior Data Scientist at Disney and I'm going to host **another** Data Science Q&amp;A this Thursday at 5:30 PM PST. I know some of you may have already registered but I still wanted to post so other folks here have an opportunity to attend. Last month\u2019s sessions were an absolute blast with over 250 people who attended from all over the world.\n\nHope to see you there!\n\nMadhav\n\nWe are going to have two sessions this month and I\u2019m including both registration links here. If you'd like to attend both, you're more than welcome!\n\n(12/10) Registration Link:\n\n[https://disney.zoom.us/webinar/register/WN\\_Lk2WFSJmTL-214XoyC9WWQ](https://disney.zoom.us/webinar/register/WN_Lk2WFSJmTL-214XoyC9WWQ)\n\n(12/17) Registration Link (With Guest!):\n\n[https://disney.zoom.us/webinar/register/WN\\_8ytQyDd1TBWric95VpD3Bw](https://disney.zoom.us/webinar/register/WN_8ytQyDd1TBWric95VpD3Bw)\n\nVerification:\n\n* My -----> photo !!! : [https://imgur.com/a/Wg3DMLV](https://imgur.com/a/Wg3DMLV)\n* My LinkedIn: [https://www.linkedin.com/in/madhavthaker/](https://www.linkedin.com/in/madhavthaker/) (Feel free to connect!)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k8hok0/im_a_senior_data_scientist_at_disney_and_im/"}, {"autor": "Minkkowski", "date": "2020-12-07 12:20:22", "content": "[Project] Implementations of a total of 18 state-of-the-art GANs using PyTorch /!/ &amp;#x200B;\n\nhttps://preview.redd.it/1uaitg4fbr361.png?width=1754&amp;format=png&amp;auto=webp&amp;s=de602f89d7076344f7d06c8ff66fb59bb7807750\n\nHello!\n\nI would like to introduce a project I created while writing the paper \"ContraGAN: Contrastive Learning for Conditional Image Generation (Neurips, 2020)\". The name of the project is StudioGAN :)\n\nPaper: [https://papers.nips.cc/paper/2020/hash/f490c742cd8318b8ee6dca10af2a163f-Abstract.html](https://papers.nips.cc/paper/2020/hash/f490c742cd8318b8ee6dca10af2a163f-Abstract.html)\n\nStudioGAN Github: [https://github.com/POSTECH-CVLab/PyTorch-StudioGAN](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN)\n\n**StudioGAN** is a Pytorch library providing implementations of representative Generative Adversarial Networks (GANs) for conditional/unconditional -----> image !!!  generation. StudioGAN aims to offer an identical playground for modern GANs so that machine learning researchers can readily compare and analyze the new idea.\n\n\\[Features\\]\n\n* Extensive GAN implementations using PyTorch\n* Comprehensive benchmark of GANs using CIFAR10, Tiny ImageNet, and ImageNet datasets (will be updated)\n* Better performance and lower memory consumption than original implementations\n* Providing pre-trained models that is fully compatible with up-to-date PyTorch environment (will be updated)\n* Support Multi-GPU(both DP and DDP), Mixed precision, Synchronized Batch Normalization, and Tensorboard Visualization\n\nThank you:)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k8fnmi/project_implementations_of_a_total_of_18/"}, {"autor": "StrangeArugala", "date": "2020-12-07 03:35:25", "content": "Need help programming an optimization problem on Python /!/ -----> Picture !!!  the following data:\n\nDate - Test Factory - Test Units - Control Factory - Control Units\nW1 - Test Factory 1 - 30 - Control Factory 1 - 27\nW1 - Test Factory 1 - 30 - Control Factory 2 - 45\nW1 - Test Factory 1 - 30 - Control Factory 3 - 36\nW2 - Test Factory 1 - 47 - Control Factory 1 - 65\nW2 - Test Factory 1 - 47 - Control Factory 2 - 54\nW2 - Test Factory 1 - 47 - Control Factory 3 - 27\nW3 - Test Factory 1 - 25 - Control Factory 1 - 26\nW3 - Test Factory 1 - 25 - Control Factory 2 - 33\nW3 - Test Factory 1 - 25 - Control Factory 3 - 31\n\nI'm trying to pick a combination of control factories to best match the output of the test factory for all dates.\n\nI'm doing this by assigning a weight to each control factory.\n\nControl Factory - Weight\nControl Factory 1 - 1\nControl Factory 2 - 1\nControl Factory 3 - 1\n\nObjective Function:\n\nI want to minimize the sum of the absolute difference between the test units and the sum of the weighted control units for all dates. Same thing expressed mathematically:\n\nMINIMIZE [sum for all dates(abs(test unit - sum for all control factories(weight_i*control units_i)))] where i is each individual control factory.\n\nObjective function with the above data included:\n\nMINIMIZE [abs((30 - 1*27+1*45+1*36) + (47 - 1*65+1*54+1*27) + (25 - 1*26+1*33+1*31))].\n\nConstraints:\n\nThe weights need to be between 0 and 1.\n\nAlso the following ratio needs to be between 0.9 and 1.1:\n\nsum for all control factories(weight_i*control units_i) / test unit\n\nThe above ratio will be represented as a constraint 3 times since there are 3 dates.\n\nExample of the above ratio with the data: \n\n0.9 &lt;= 1*27+1*45+1*36 / 30 &lt;= 1.1 for W1\n0.9 &lt;= 1*65+1*54+1*27 / 47 &lt;= 1.1 for W2\n0.9 &lt;= 1*26+1*33+1*31 / 25 &lt;= 1.1 for W3\n\nI want to get the best combination of weights for each control factory.\n\nI'm having a hard time programming this on Python. I've been manipulating the tables above using Pandas.\n\nAny help would be appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k88s4g/need_help_programming_an_optimization_problem_on/"}, {"autor": "samu_l1", "date": "2020-12-06 14:13:12", "content": "Text detection in documents /!/ I am trying a to detect text in documents, magazines, newspapers, etc. I have been searching for a good model for detecting text in these situations but I am able to find only papers for text detection in the wild.\n\nCompared to text in wild task, document text detection has *easily identifiable text* but the *number of words in the -----> image !!!  are a magnitude higher*. So I'm a bit hesitant to use the methods for text in wild.\n\nCan you suggest me a good model for this task?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k7ufpo/text_detection_in_documents/"}, {"autor": "trustfulvoice94", "date": "2020-11-30 00:30:41", "content": "Shortest path problem - Student data /!/ Hi there!\n\nI have a dataset of students who are completing an online degree. They are able to select the \"path\" or order of subjects that they do.\n\nDataset looks something like this: https://imgur.com/a/4MMPb5S\n\nIn the -----> image !!! , I have the order of completion of each of the subjects and the duration to complete the entire programme (some don't finish the degree). Hence, each student takes their own unique path.\n\nAre there any concepts or algorithms I could use to gain insight into the \"shortest\" or \"fastest\" to complete the programme for the entire student body (I have &gt;1000 student data points)?  \n\nThank you for any advice!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k3kz2a/shortest_path_problem_student_data/"}, {"autor": "Ruffybeo", "date": "2020-11-29 23:23:53", "content": "Data augmentation for time series data? /!/ Hi everyone!\n\nI'm working on an EEG classification problem and want to solve this with an LSTM. (I want to write an algorithm that can classify if somebody has a low or high case of trait anxiety) My problem is that I have a really small dataset (67 subjects with around 700 Features)  and so my attempts of models overfit tremendously. To reduce this effect, I would perform data augmentation with Keras when this would be an -----> image !!!  classification problem, But now I have time-series data and I was wondering if there is a way to use data augmentation in this use case as well?  Or maybe anyone has other ideas to reduce the overfitting? \n\nI would be grateful for every input :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k3ju8c/data_augmentation_for_time_series_data/"}, {"autor": "Vendredi46", "date": "2020-11-29 15:14:09", "content": "About images used for training... /!/ So data-sets i've looked into for training tend to come in like 6K images of symbols. However, the tutorials i've been following like MNIST seem to come in .CSV format rather than an -----> image !!!  like what I can find online.\n\nSo, am I supposed to convert these images into a csv format if I want to follow these kinds of guides? (I want to run a symbol classifier w/sci-kit), like, how do they do this. Is this a general process that is usually performed? \n\nAlso, about the csv of the MNIST data-set, I'm having trouble understanding it. row headers are labels, 0-9 which are the data classes i'm sure. The column headers however are 1x2, 1x3 etc. a majority of them having a 0 value with some cells have 253, what do these mean, or how am I supposed to read it?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k3axqi/about_images_used_for_training/"}, {"autor": "yyuummE", "date": "2020-11-29 01:14:35", "content": "created a pix2pix- how to turn it into an app? /!/ Hi, I've created a pix2pix, sketches to objects and have successfully run it in google colab. I would like to present it online and would appreciate any advice/link to tutorial on a work flow for doing this. Would like for the user to be able to draw their sketch and have the website return a generated -----> image !!! . thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k2zzrn/created_a_pix2pix_how_to_turn_it_into_an_app/"}, {"autor": "domarom", "date": "2020-11-28 15:03:31", "content": "how to building training data /!/ Hello everyone \n\nI'm pretty new to ML but i am trying to find a way to create my own training data in python. I have 1000s of images of one sea sponge that doesn't change he location \n\nI'm trying to label the background as my negative and the sponge as my positive so I can track how it changes size over time. all the pre-written code required the -----> image !!!  negative to not include the positive \n\nany suggestions on how to build this training set? I'm willing to manually draw two ROIs on 100s of images i just don't know how....it could also just be pixel classification sponge / not sponge or semantic  segmentation even \n\nthank you in advance!!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k2p35x/how_to_building_training_data/"}, {"autor": "LividPie10", "date": "2020-11-28 10:55:00", "content": "PARTIAL DIFFERENTIAL EQUATION /!/  \n\nHello, I am learning about -----> image !!!  denoising right now, and saw a paper where -----> image !!!  denoising is achieved using Non-linear diffusion. Going through the paper, I ended up in PDE.\n\nCan someone please explain to me how PDE works in a practical and real-life way?.  \nI guess I've just mugged up on how to solve those problems back in school.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k2lry7/partial_differential_equation/"}, {"autor": "umairs433", "date": "2020-10-31 07:30:41", "content": "Does Watermarked images effect model accuracy? /!/ I  want to do -----> image !!!  classification, but the datasets available are  watermarked. Does watermarked images effect model accuracy? How much  does it effect the train model? For one dataset, the watermark is at  bottom left, and for one dataset, the watermark is big and covers the  whole background. The classes are around 300 and images are 10,000 to  20,000.\n\nWhat is the workaround? Is there a way to remove or crop watermark?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jleb43/does_watermarked_images_effect_model_accuracy/"}, {"autor": "skymen75", "date": "2020-10-30 20:30:53", "content": "Deep fake with a single -----> image !!!  in Javascript /!/ Hey, I've had a silly idea for a machine learning project. I want to make a website that when opened fetches an image from \"thatpersondoesnotexist\" and does a deep fake of it with another image to force it to smile.\n\nI have looked around for information and there is very little information on where to start if I wanna write a deepfake algorithm, and wether I'd need to write WASM for it or if regular JS can work.\n\nAnyone could point me to the right direction?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jl4luv/deep_fake_with_a_single_image_in_javascript/"}, {"autor": "headwar", "date": "2020-10-30 19:30:48", "content": "ML -----> image !!!  recognition library tags / \u201cobject list\u201d? /!/ Is it possible to get a list of all the objects a certain image detection library can detect? Like, all the possible results?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jl3hcg/ml_image_recognition_library_tags_object_list/"}, {"autor": "hellopaperspace", "date": "2020-10-30 16:38:00", "content": "[Tutorial] A Guide to Optimizers in Deep Learning /!/ In this tutorial, we take a look at the different optimizers in deep learning. We\u2019ll start with a look at gradient descent with and without momentum, and implement both from scratch in Python. We also visualize our gradient updates on Ackley's function as movement along the contour plots. We\u2019ll then discuss and benchmark several optimizers for an -----> image !!!  classification task using the CIFAR10 dataset, and train a ResNet18 for this purpose.\n\nOptimizers covered include:\n\n* Nesterov momentum\n* Adam \n* AdaGrad\n* AdaMax\n* AdamW\n* AdaDelta\n\nFinally, there is a brief discussion of less-popular optimizers such as QHAdam, YellowFin, and Demon.\n\nTutorial link: [https://blog.paperspace.com/optimization-in-deep-learning/](https://blog.paperspace.com/optimization-in-deep-learning/)\n\nNotebook with all the code (totally free to run): [https://ml-showcase.paperspace.com/projects/optimizers-in-deep-learning](https://ml-showcase.paperspace.com/projects/optimizers-in-deep-learning)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jl06z5/tutorial_a_guide_to_optimizers_in_deep_learning/"}, {"autor": "PeltedVenom", "date": "2020-10-30 14:59:11", "content": "Some guidance: -----> Image !!!  classification based on arc shape in image. /!/ I'm exploring the idea of creating a model that would classify images based on a prevailing arc made from elements in the image. For example, lets say you had 10 wooden children's blocks on the kitchen floor photographed top-down, the blocks were laid end-to-end forming various arcs. I want to classify the the images based on a set of arc examples (A, B, C). \n\n[Example of bocks in image and the arc classifications](https://preview.redd.it/wqbrc6i2x8w51.jpg?width=512&amp;format=pjpg&amp;auto=webp&amp;s=01afb972c0bd6e45481be48a42a321629d0e4842)\n\nI've played around with basic image tagging mostly from sample data in various tutorials. This is a bit different as it's looking at a prevailing shape made from other shapes in the images. Another example would be cars on the road from up high, or people standing in line, or dominos on a table. Not every item need be the same shape (cars, people) but they will always be in some line forming an arc.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jkybmk/some_guidance_image_classification_based_on_arc/"}, {"autor": "nachifur", "date": "2020-10-30 07:22:09", "content": "Mulimg viewer easily enlarge multiple pictures in parallel mode /!/ I will recommend this project - [Mulimg\\_viewer](https://github.com/nachifur/Mulimg_viewer/wiki) to you. Mulimg viewer can easily enlarge multiple pictures in parallel mode. I hope this project of mine can help you in the comparison -----> picture !!!  of your paper in the future. github: [https://github.com/nachifur/Mulimg\\_viewer/wiki](https://github.com/nachifur/Mulimg_viewer/wiki)\n\nSupport win10 and ubuntu18.04. [https://github.com/nachifur/Mulimg\\_viewer/releases](https://github.com/nachifur/Mulimg_viewer/releases/tag/v3.0)\n\nhttps://i.redd.it/8d75d7j4o6w51.gif", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jksa4q/mulimg_viewer_easily_enlarge_multiple_pictures_in/"}, {"autor": "kk_ai", "date": "2020-10-30 07:15:53", "content": "Beginner Friendly Intro to the ML Methods + Explanation Through Examples /!/ The field of Machine Learning is very diverse, in the sense that every problem is different and requires an appropriate approach.\n\nThe problem can vary from being a stock price prediction, classifying an -----> image !!! , detecting objects in an -----> image !!! , grouping the same type of data, and about a million other things.\n\n**But Machine Learning methods can be split into three types of learning:**\n\n1. Supervised Learning (Regression, Classification, Ensemble Learning),\n2. Unsupervised Learning (Clustering, Dimensionality Reduction),\n3. Reinforcement Learning - agent based learning where an agent learns to behave in an environment by performing the actions to get the maximum rewards.\n\n**Let's have a closer look at classification**\n\nIn Classification, we assign a class or a label to a given object (represented by some feature vector). For example, we can train a classification model to predict whether an email is a spam or not.\n\n![Binary classification](https://i2.wp.com/neptune.ai/wp-content/uploads/download.jpeg?w=302&amp;ssl=1)\n\nIn classification problems, we are not limited to just two-class classification (known as binary classification). We can also have multiple classes to be classified with our model.\nFor example, our task could be to classify the digits from 0 to 9. It means we have 9 classes to classify (choose from). Problems like this one are called Multi-class classification problems.\n\n![Multi-class classification](https://i0.wp.com/neptune.ai/wp-content/uploads/download-1-1.png?resize=300%2C243&amp;ssl=1)\n\nHope his helps you grasp the general idea behind classification task.\n\nIn the article we describe **Regression, Classification, Ensemble Learning, Clustering, Dimensionality Reduction and Reinforcement Learning**. All in the beginner-friendly and vidsual way -&gt; [Beginner Friendly Intro to the ML Methods](https://neptune.ai/blog/machine-learning-methods?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=blog-machine-learning-methods&amp;utm_content=learnmachinelearning).\n\nFeedback is appreciated so if you think there is anything missing or have any questions, see you in the comments!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jks7ss/beginner_friendly_intro_to_the_ml_methods/"}, {"autor": "_YouCanDoIt", "date": "2020-10-30 06:16:43", "content": "Beginner looking for model recommendations / resources for recognizing text on book covers (OCR task) /!/ Hi! I have a toy project that involves detecting text on different book covers, classifying them as title, subtitle, authors, etc. and extracting that text like this -----> image !!!  below:  \n\nhttps://preview.redd.it/dzacf74vb6w51.png?width=1014&amp;format=png&amp;auto=webp&amp;s=17a3fc6b0e2d5b97623b042dbce0b7641736d841\n\nI'm more or less a beginner in ML. I've read a couple articles on OCR and it looks like they all mention Convolutional Recurrent Neural networks (CRNN) where the CNN extracts feature vectors, feeds it into a bidirectional LSTM, and then into a transcription layer to generate the sequence of text. \n\nThe questions that I have are:\n\n* Will a CRNN allow me to achieve what I want for this toy project? If not, what models should I look into?\n* Is this correctly classified as an OCR task? \n\n&amp;#x200B;\n\nI'm open to papers, articles, github repos!! I'm at the beginning phase where there's just so much information that it's kind of overwhelming, and I don't know the best place to start.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jkrkm3/beginner_looking_for_model_recommendations/"}, {"autor": "black0017", "date": "2020-10-29 20:29:48", "content": "[D] - Deep learning in MRI beyond segmentation: Medical -----> image !!!  reconstruction, registration, and synthesis /!/ If you believe that medical imaging and deep learning is just about  segmentation, this article is here to prove you wrong. We will cover a  few basic applications of deep neural networks in **Magnetic Resonance Imaging** (**MRI**). Specifically:  \n\n\n* **image reconstruction**\n* **image registration**\n* **super-resolution**\n* **image synthesis**\n\n[https://theaisummer.com/mri-beyond-segmentation/](https://theaisummer.com/mri-beyond-segmentation/?fbclid=IwAR1Gv4cY-EpN3lqIxosPz589706-LPPfbHlGcdojmpQZTFfcftUzTxods_U)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jki11l/d_deep_learning_in_mri_beyond_segmentation/"}, {"autor": "bass1012dash", "date": "2020-10-29 19:25:21", "content": "-----> Image !!!  sets for classification ml5 /!/ I\u2019m looking into ml5:\n\nMy problem - use three images (front,back,side) to predict a label.\n\nImageClassifier seems to be able to handle 1 image, and classify() seems to deal just with normalized vectors. \n\nIs there an easy way (with ml5.js or a different library) to input three related images and output a prediction?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jkgrw9/image_sets_for_classification_ml5/"}, {"autor": "Williamcorrea", "date": "2020-10-29 01:37:13", "content": "Drone path planning /!/ Hey guys, I would like to design a system so I can make a drone navigate through a bridge in order to detect Cracks on it. It would have a rgb -----> camera !!!  and a gps device. What kind of methods could I use to program the behaviour of the drone ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jk135s/drone_path_planning/"}, {"autor": "JimGFM", "date": "2020-10-24 20:13:51", "content": "Music generation /!/ I\u2019m new to ML... very new! I make electronic music and am interested in the idea of using my audio sketches/textures, found sound and even full finished tracks as an input, in the hope that the outcome can be high res audio files of mashed up textures, noises and rhythm. Kind of like a sonic palette of sounds and rhythms based on the inputs. At minimum this might create some interesting audio artefacts which I can chop up and use in my music and potentially at best, maybe it would create interesting enough standalone finished experimental audio tracks.\n\nLike I said, I\u2019m new to this. I\u2019ve dabbled in some GAN -----> image !!!  recognition stuff via apps with GUI, no coding. So I have no idea if this is something that is realistic or doable at all. Perhaps something exists already? Any thoughts or advice would be appreciated. Thanks.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jhftmb/music_generation/"}, {"autor": "dominic_l", "date": "2020-10-22 16:20:53", "content": "How do I create an .csv file from an -----> image !!! ? /!/ I'd like to extract some data from a paper file so that I can add it to a database", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jg2spt/how_do_i_create_an_csv_file_from_an_image/"}, {"autor": "untitled20", "date": "2020-10-22 02:29:54", "content": "Can I learn ML if I'm bad at geometry / visual problem solving? /!/ I'm a software developer, trying to learn the Maths background for machine / deep learning.\n\nThings like Algebra, set theory, writing code, etc are quite easy for me. But things involving geometry, where you have to look at a -----> picture !!!  to figure out details, are really hard.\n\nAs an example, if you spell out a problem for me, I can do it easily. But if you show me a diagram and ask me to find a problem, its really hard for me (trying to focus on it is makes me want to throw my computer out the window). I have ADHD, and its really difficult for me to focus on doing visual problem solving where things are vague and feel 'subjective'.\n\nI'm also directionally challenged in real life, I easily get lost, etc. I had to do an IQ test once which had a verbal and non-verbal portion - I was below average on non-verbal (i.e visual) and above average on verbal (i.e where a problem is spelled out in words).\n\nCan I still do machine learning if I skip geometry (beyond basic Pythagorean / trig stuff) and just do algebra, stats, and calculus? Or do I need to push through and finish all geometry units as well?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jfr6av/can_i_learn_ml_if_im_bad_at_geometry_visual/"}, {"autor": "cereal_final", "date": "2020-10-22 01:54:15", "content": "Help understanding VGG19 layers /!/ &amp;#x200B;\n\nhttps://preview.redd.it/6v9rf5l0yju51.png?width=600&amp;format=png&amp;auto=webp&amp;s=634169c63cf3c356eead2ac109044ba82219fc7c\n\nI was looking at this -----> image !!!  of the VGG19 architecture, and I was confused as to why each layer(not sure what the correct term is) has multiple convolution layers(conv1\\_1 and conv1\\_2). How do the 2 work together? is conv1\\_2 performed on conv1\\_1? are they both applied to the input image? if so, what's the difference between two conv layers with 64 filters and one conv layer with 128 filters?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jfqm9k/help_understanding_vgg19_layers/"}, {"autor": "Pencilcaseman12", "date": "2020-10-21 21:40:48", "content": "How to program a GAN from first principles /!/ I am trying to make a (simple) GAN for a project at school, but I have been unable to do so. I am trying to make the entire thing using my own code and first principles (i.e. no numpy, tensorflow, pytorch, keras, etc.).\n\nI know that the speed of matrix operations is key, so I made a farily primitive Python matrix library (using C for the number crunching with optimised and  parallelized algorithms) and an exceedingly simple neural network library here: [libpymath](https://github.com/Pencilcaseman/LibPyMath)\n\nI have read many articles around GANs and how they work, however I have been unable to find anything programmed from first principles, as everything is either a huge equation or makes use of complex and advanced libraries like those mentioned above. I am doing my GCSEs this year so, while I understand the matrix math, I am not sure how to approach things like this: \n\n[An equation I found that \\(supposedly\\) represents a GAN](https://preview.redd.it/y0g9f6b7liu51.png?width=1165&amp;format=png&amp;auto=webp&amp;s=6dbd45d6488e1b3b14d1adbff071c0aff1c10a7a)\n\nI have tried to create a simple GAN that generates data points that fit to a circle with some deviation, like the -----> image !!!  shown below:\n\n&amp;#x200B;\n\n[Data I am trying to generate](https://preview.redd.it/4qhjrri5miu51.png?width=636&amp;format=png&amp;auto=webp&amp;s=8865083342f3ea3602201717f84e023e0d347282)\n\n&amp;#x200B;\n\nI made a generator and a discriminator network, where the generator's input was a 5 dimensional vector, and its output was a 200 dimensional vector (i.e. 100 x coordinates and 100 y coordinates), and the discriminator had a 200 dimensional vector input (for the same reasons) and a single output representing a probability that the inputted data was from the generated dataset.\n\nTo train the discriminator, I first applied the standard backpropagation algorithm to it, using real data as the input and 1 as the output, representing genuine data. Then I generated some random numbers and fed those into the generator network, applying the same backpropagation function with that data and an output of 0, representing generated data.\n\n**Here is where I am stuck**\n\nTo train the generator, I tried to effectively stick the discriminator onto the end of it, feed forward over the entire thing and then run an adapted backpropagation algorithm over the entire thing, where the desired output is 1. The backpropagation algorithm I created for this would only update the weights and biases of the generator, using the discriminator purely for the error calculation and gradients. My thinking here was that, by doing this, I am only adjusting the weights of the generator, and am shifting them in such a way that it is trying to fool the discriminator into producing an outcome with a higher probability, however this did not work.\n\nI am not entirely sure if I am just doing something fundamentally wrong, or if there is simply a step I am missing to make this process work correctly. I found that the discriminator would reach nearly zero loss almost immeadiately, while the generator would slowly converge to a loss of 1. During this process there would be many spikes and dips, but the general trend was very clear. I have been adjusting the learning rate, activation functions and the size of both networks all day, but have not gotten any closer to making something that produces valid data.\n\nIf it is of any relevence, it appeared that all of the points it was generating were being pushed into the corners of the grid, which I think may be due to some sort of exploding/vanishing gradient issue, but I am not entirely sure. I understand that explaining what I have done wrong is exceedingly difficult, though maybe someone could help me understand where in my process I am going wrong and therefore getting the strange results I got.\n\n&amp;#x200B;\n\nAny help would be greatly apprecieated, and please remember that I am only doing my GCSEs this year, so will most likely not understand any complex math beyond the matrix stuff requried for the networks themselves.\n\nThanks!\n\n&amp;#x200B;\n\nP.S -- If it helps, I was using the `tanh` activation function on all of my layers as it gave an output between -1 and 1, which was useful as my data ranged from -1 to 1", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jfmc8l/how_to_program_a_gan_from_first_principles/"}, {"autor": "elrd5150", "date": "2020-02-17 03:05:03", "content": "What's the intuition behind skip connections? /!/ So here is a -----> picture !!!  of resnet residual block:\n\n[https://www.researchgate.net/publication/333407474/figure/fig1/AS:763343056941057@1559006575644/The-architecture-of-the-a-residual-block-and-b-ResNet.png](https://www.researchgate.net/publication/333407474/figure/fig1/AS:763343056941057@1559006575644/The-architecture-of-the-a-residual-block-and-b-ResNet.png)\n\nAs I understand it, we just take the current features, pass them through several conv layers to find more complex features. And then just sum the original features and new more complex features??? Shouldn't they be different enough so that a simple 1 + 1 addition will make everything messy? Why does this even work?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f5243d/whats_the_intuition_behind_skip_connections/"}, {"autor": "abbaahmad", "date": "2020-02-15 20:55:43", "content": "[PyTorch] Output and target tensors dimensions /!/ I need help figuring out what the target dimension should be for a 4-class model. I'm trying to retrain Alexnet for bounding box prediction, the dataloader class is shown below:\n\ndef \\_\\_getitem\\_\\_(self,idx):\n\n\t\t\\#load -----> image !!! \n\n\t\timg\\_path = os.path.join(self.root,'-----> image !!! s',self.-----> image !!! s\\[idx\\])\n\n\t\tlabel\\_path = os.path.join(self.root,'labels',self.labels\\[idx\\])\n\n\t\timg\\_array = [Image.open](https://Image.open)(img\\_path).convert('RGB')\n\n\t\tpreprocess = transforms.Compose(\\[\n\ntransforms.Resize(256),\n\ntransforms.CenterCrop(224),\n\ntransforms.ToTensor(),\n\ntransforms.Normalize(mean=\\[0.485,0.456,0.406\\],std=\\[0.229,0.224,0.225\\]),\n\n\t\t\\])\n\n\t\timg = preprocess(img\\_array)\n\n\t\t\n\n\t\t\\#label\n\n\t\twith open(label\\_path,'r') as f:\n\n\t\t\tlabel = [f.read](https://f.read)().split()\n\n\t\t\tlabel = \\[float(x) for x in label if x != '0'\\]\n\n\t\t\n\n\t\tlabel\\_tensor = torch.FloatTensor(label).flatten()\n\n\t\treturn img, label\\_tensor \n\nI keep getting this error. The other suggestion i got was to use \".squeeze(1)\" and it still throws up the same error. Any help is appreciated.\n\nhttps://preview.redd.it/fyortpxai5h41.png?width=793&amp;format=png&amp;auto=webp&amp;s=a800d100d29aac344f3a1b230280b2d800144647", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f4fkgg/pytorch_output_and_target_tensors_dimensions/"}, {"autor": "karan991136", "date": "2020-02-15 10:38:02", "content": "23 Amazing Deep Learning Project Ideas with Source Code that you must know!! /!/ We know that machine learning is the rage these days. But the machine learning technique that shines the most brightly is deep learning. Deep learning is all about how a computer program can learn through observation and make decisions based on its experience. Deep learning methods are useful for computer vision, natural language processing, speech recognition and processing, and so much more.\n\nThe best way to learn something is with a hands-on approach and, therefore, we bring these amazing project ideas for you to practice and improve your deep learning knowledge and skills. These project ideas are divided according to there difficulty level so that you can easily find a project that interests you and is within your skill level. So let\u2019s not waste any more time and jump right into it.\n\nDataFlair is providing you an amazing series of top project ideas. This is the 6th project of our series.\n\n* [Python Project Ideas](https://data-flair.training/blogs/python-project-ideas/)\n* [Python Django (Web Development) Project Ideas](https://data-flair.training/blogs/django-project-ideas/)\n* Python Game Development Project Ideas\n* Python Artificial Intelligence Project Ideas\n* [Python Machine Learning Project Ideas](https://data-flair.training/blogs/machine-learning-project-ideas/)\n* [Python Data Science Project Ideas](https://data-flair.training/blogs/data-science-project-ideas/)\n* Python Deep Learning Project Ideas\n* [Python Computer Vision Project Ideas](https://data-flair.training/blogs/computer-vision-project-ideas/)\n* [Python Internet of Things Project Ideas](https://data-flair.training/blogs/iot-project-ideas/)\n\n## Deep Learning Project Ideas for Beginners\n\n### 1. Predict Next Sequence\n\n**Deep Learning Project Idea \u2013** To start with deep learning, the very basic project that you can build is to predict the next digit in a sequence. Create a sequence like a list of odd numbers and then build a model and train it to predict the next digit in the sequence. A simple neural network with 2 layers would be sufficient to build the model.\n\n### 2. Cats vs Dogs\n\n**Deep Learning Project Idea \u2013** The cats vs dogs is a good project to start as a beginner in deep learning. You can build a model that takes an -----> image !!!  as input and determines whether the -----> image !!!  contains a picture of a dog or a cat.\n\n**Dataset:** [Cat vs Dogs Dataset](https://www.kaggle.com/c/dogs-vs-cats)\n\n### 3. Build your Own Neural Net from Scratch\ud83d\udcf7\n\n**Deep Learning Project Idea \u2013** Mostly you will be doing the neural network works using a deep learning library or framework. You should definitely build your own neural network library in order to understand how everything works. This project will enhance your skills and understanding of the subject. [Read More](https://data-flair.training/blogs/deep-learning-project-ideas/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f47sso/23_amazing_deep_learning_project_ideas_with/"}, {"autor": "Biddls", "date": "2020-02-15 01:21:00", "content": "YOLO why split up the -----> image !!!  into chunks /!/ I\u2019m in the process of implementing YOLO by hand (to learn about it). I\u2019m wondering if there are any other reasons as to why the image is split up?\n\nI guess it\u2019s because that way the model can Jsut learn the object not the position it could be\n\nAre there any other reasons and what kind of chunk size am I going for\n\nAnd if the input will always be a certain size do I have to split it up?\n\nAlso if I had an image with a bunch of cars ho would I change the YOLO algorithm to only output the 1 bounding box with the highest % confidence?\n\nAlso is there a better obj detection algorithm for finding the bounding box of the closest in the scene and highest % chance of it being that?\n\nManny thanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f428um/yolo_why_split_up_the_image_into_chunks/"}, {"autor": "Glog97", "date": "2020-02-13 12:03:13", "content": "Help with choice of architecture /!/  Hi everyone,\n\nI've decided to learn pytorch and ML, and I have completed andrew ng's course on coursera and after that I read the book about pytorch which they have on their website. After that I tried a couple of tutorials for -----> image !!!  recognition/classification.\n\nNow, I would like to make a nn that can \"restore\" old images, so i figured I could get a set of RGB images, turn them to greyscale, add some noise or whatever to make them look like old photos and use that as my train set.\n\nMy question is, can I make a NN with 3x more outputs than inputs (rgb having 3 channels vs greyscale having 1), do any of you have any experience with that, and can someone provide me with guidance as to what types of layers/activation functions etc should be used in this case?\n\nThank you all in advance. Big love\n\nMilorad", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f38yq9/help_with_choice_of_architecture/"}, {"autor": "Tradingtophat", "date": "2020-02-09 19:40:04", "content": "Finding state of the art papers /!/ With the large amount of ml papers released, how do I find the state of the art papers for a particular task (i.e. -----> image !!!  compression or instance segmentation)?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f1dslb/finding_state_of_the_art_papers/"}, {"autor": "LucidCoding", "date": "2020-02-08 17:24:49", "content": "An app to recognize simple handwritten mathematical expressions and turn into TeX markup /!/ This is a toy app that I've made a while back. A user draws a simple mathematical expression like 4 + 8 \\* 3 - 3 / 5 on the canvas and the program outputs a TeX markup.\n\n[The Github repository of the app](https://github.com/X-rayLaser/handwriting-2-TeX)\n\nOriginally I set an ambitious goal to recognize an arbitrary mathematical expression with symbols, sums, trigonometric functions, integrals, etc. But because I was quite amateurish back then, this turned out to be a little more complicated than I thought! So I decided to solve a much smaller problem: recognizing expressions consisting of numbers and basic arithmetical operations.\n\nAnd this is what I've have got. The app can recognize quite complex expressions consisting of sums, products, fractions, numbers, powers, nested fractions, etc. There is also additional UI that can be used to fine tune the classifier on your own hand-writings/drawings.\n\nIn case you are interested, below is a brief explanation of how it works:\n\n&gt;!In the nutshell, the algorithm localizes all the objects in the canvas and classifies them using a convolutional neural network. Then it put them together and outputs a TeX markup.!&lt;\n\n&gt;!For localization, the algorithm constructs a grid graph from the -----> image !!!  pixels and runs a graph processing algorithm to find all connected components (digits, + or - signs, etc.). Once those connected components are identified, it becomes possible to find their midpoints.!&lt;\n\n&gt;!Reducing recognized objects into a final string of TeX markup is more involved. The basic approach is to use something like a \"divide and conquer\" strategy. That is you create a function that reduces the collection of objects occupying some region to TeX markup. The function finds an object corresponding to some mathematical operator (say, division). Then, it identifies relevant sub-regions (say above and below the division line) and recursively reduces them into TeX, combines their TeX markup and returns it.!&lt;\n\nI am aware that this is not the best way of solving the problem. And the algorithm is very slow. But still, I got something that works in the end. Plus, it was a great learning experience.\n\nIn any case, let me know what you think. Also, If you had to implement something like this, how would you do it?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f0ulp5/an_app_to_recognize_simple_handwritten/"}, {"autor": "hollammi", "date": "2020-02-08 14:17:04", "content": "Crazy Feature Augmentation Ideas for Handwritten Character Classification? /!/ Heya,\n\n\n\nI'm about to tackle this [Bengali Character Classification](https://www.kaggle.com/c/bengaliai-cv19) problem on Kaggle. The task is two classify images of hand written characters, similar to MNIST.\n\n\n\nI've seen some interesting ways to augment -----> image !!!  training data to improve classifier performance. For example, Google's pix2pix architecture flips images horizontally / vertically, and performs a 'random jiggle'.\n\n\n\nSince these are characters, I feel like image preprocessing steps like edge detection could potentially be helpful. I'll also have a try at inverting the colours.\n\n\n\nDoes anyone have some suggestions for similar methods I could try out? \n\n\n\nThanks a lot \ud83d\ude42", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f0s480/crazy_feature_augmentation_ideas_for_handwritten/"}, {"autor": "MLtinkerer", "date": "2020-02-07 03:26:25", "content": "State of the art in -----> image !!!  to -----> image !!!  translation (guided)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f04h1f/state_of_the_art_in_image_to_image_translation/"}, {"autor": "JonathanSum777", "date": "2020-02-06 16:36:01", "content": "Do you use -----> image !!!  nets mean and std and its pre-trained for training Japanese Animation?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ezuv9n/do_you_use_image_nets_mean_and_std_and_its/"}, {"autor": "ydgqq07", "date": "2020-03-03 14:05:15", "content": "random forest \uff0ctime prediction /!/ hello, I would like to predict 'diffsecond'(the last column) by data of timestamp and other strings.i don't know why my timestamp can't be the training data.\n\nsafe links of my -----> picture !!! \ud83d\udc47\ndata:\nhttps://tlgur.com/d/gvqzMLQG\ncode:\nhttps://tlgur.com/d/8nojBe1G\nerror:\nhttps://tlgur.com/d/Gozay208\n\nI'm an English learner and sorry for the grammar error. thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fcv0rm/random_forest_time_prediction/"}, {"autor": "beatricejensen", "date": "2020-03-03 03:10:06", "content": "Has the current state-of-the-art in -----> image !!!  captioning fixed this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fco7vw/has_the_current_stateoftheart_in_image_captioning/"}, {"autor": "ahmedbesbes", "date": "2020-03-02 22:38:48", "content": "Has anyone tried triplet loss combined with cross-entropy to get good embeddings? /!/ Hello everyone,\n\nSo my problem is the following, I hope you guys can give me your feedbacks\n\n\\- I wang to build a similarity engine on an -----> image !!!  dataset\n\n\\- My dataset consists of -----> image !!! s, categorized by classes, where the number of -----> image !!! s varies between classes (highly imbalanced)\n\nWhat I tried so far is training a triplet loss with hard negative sampling. The results are promising but I'm hoping for more.\n\nMy question is the following: does it make sense to combine a triplet loss with a cross-entropy in the same network. The architecture would allow, at the same time, to learn similarity embedding via the triplet loss and learn to separate the images between the classes via the cross-entropy?\n\nHas anyone tried this approach before?\n\nThank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fckg71/has_anyone_tried_triplet_loss_combined_with/"}, {"autor": "letsthrowawaylove", "date": "2020-03-02 07:43:39", "content": "Reading ingredients from a -----> picture !!!  /!/ I want to be able to read ingredients from pictures of labels. If i simply feed pictures into an OCR api I don't get great results unless the image is well zoomed and cropped. My question is what are the best tools I can use to learn how to crop and rotate images and perhaps play with their lighting etc so that I can receive good results for OCR? \n\nThe solution I would like to implement is to find the x,y position of the word ingredient and wrap a box around it to contain the ingredients list. Is there a library that could help me achieve this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fc8oyv/reading_ingredients_from_a_picture/"}, {"autor": "65-76-69-88", "date": "2020-03-01 18:09:28", "content": "Basic result-oriented machine learning steps? /!/ Hi,\n\nI'm familiar with a decent breadth of compsci topics, however not at all regarding ML topics. I might be aware of the very very basics of what ML is fundamentally, know some of the statistical methods behind it, as well as very basic algorithms like naive bayes or kmeans -- but that's pretty much it.\n\nHowever: I recently had a project idea I would enjoy working on, and am certain that I could really use some ML in there. Therefore my question; how would you describe the basic steps you do when training a new ML model for an arbitrary purpose, in a result-oriented way, ie skipping the, let's call it mathematically interesting stuff? (While ML is super interesting, right now I am more interested in how I can actually use it without needing to learn too much first)\n\nFor context, here's a simplified example of what I am more or less trying to achieve:\n\nLet's say I have a huge collection of images depicting roads with cars on them, with many containing one or more yellow taxis. I also have the exact same images with the taxis photoshopped out, so that only the road is visible. I want to use this data as a training set for an ML model, such that whatever -----> image !!!  of a road I feed into it, it removes all yellow taxis and fakes the background behind them.\n\nThis, by all means, seems like an insanely complex task -- but also one where an ML algo would seem like a fitting way of solving it. Now, is there some kind of \"recipe\" I can follow, without getting into the mathematical details of ML, to create and run a model capable of solving this task somewhat nicely? What would those steps be?\n\n&amp;#x200B;\n\nThanks a lot!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fbxxrk/basic_resultoriented_machine_learning_steps/"}, {"autor": "Silencio04", "date": "2020-03-01 11:32:17", "content": "Is this a beginner\u2019s mistake? /!/ So, my approach to learn: \nWrite most algorithms from scratch, to have a better and a clearer -----> picture !!!  of how they work.\nThe first problem with that is that some of algorithms are complicated and take so much time to implement. The second thing is that I won\u2019t see results immediately, which might be a real pain sometimes.\n\nI\u2019m realizing that what I did and been doing is not that productive, I\u2019ve been focusing on the theory too much that I\u2019m not super comfortable with Python and its ml libraries. \n\nI\u2019m a cs undergraduate, I think I know some linear algebra, probabilities, and calculus. And I\u2019m always learning something! \n\nNow, I don\u2019t think I\u2019ll be doing research in the future. Don\u2019t get me wrong, this field is super interesting, but they are many awesome fields out there too. \n\nIn short, I need some assurance, please. \nI think maybe start afresh, learn a framework such as Pytorch or TensorFlow, stick to it, keep doing projects and playing with data, and maybe eventually I\u2019ll be good enough to work or teach the topic.\n\nI\u2019d really appreciate any replies. Thank you for reading.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fbsusu/is_this_a_beginners_mistake/"}, {"autor": "pakhira55", "date": "2020-03-01 04:11:39", "content": "How can i train -----> image !!!  captioning model? /!/ Can we train image caption without pre trained model like keras model vgg16 or resnet. Because i read many blogs in that they have used pre trained model. So is it  necessary to use that model because it takes lot of time to load this model when it is deployed.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fbop5c/how_can_i_train_image_captioning_model/"}, {"autor": "aditya_mangalampalli", "date": "2020-03-01 00:28:55", "content": "Code Hanging for GradCAM /!/ Hello all,\n\nI was working on a Tensorflow Implementation of Grad-CAM on my own model which I ended up training. However, when trying to run the following the code, it just hangs and does not throw any error even when I go into the stack trace. Does anyone know how I could fix such an issue?\n\nBelow is the code:\n\n    import sys\n    import os\n    import numpy as np\n    import cv2\n    from matplotlib import pyplot as plt\n    from tensorflow.keras import backend as K\n    from tensorflow.keras.preprocessing import -----> image !!! \n    from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n    \n    import tensorflow as tf\n    from tensorflow.python.framework import ops\n    \n    # Define model here ---------------------------------------------------\n    def build_model():\n        \"\"\"Function returning keras model instance.\n        \n        Model can be\n         - Trained here\n         - Loaded with load_model\n         - Loaded from keras.applications\n        \"\"\"\n        tf.keras.models.load_model('/Users/User/Desktop/modelFile/Logs/Logs_0/save.h5')\n        #return VGG16(include_top=True, weights='imagenet')\n    \n    H, W = 299, 299 # Input shape, defined by the model (model.input_shape)\n    # ---------------------------------------------------------------------\n    \n    def load_image(path, preprocess=True):\n        \"\"\"Load and preprocess image.\"\"\"\n        x = image.load_img(path, target_size=(H, W))\n        if preprocess:\n            x = image.img_to_array(x)\n            x = np.expand_dims(x, axis=0)\n            x = preprocess_input(x)\n        return x\n    \n    \n    def deprocess_image(x):\n        \"\"\"Same normalization as in:\n        https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n        \"\"\"\n        # normalize tensor: center on 0., ensure std is 0.25\n        x = x.copy()\n        x -= x.mean()\n        x /= (x.std() + K.epsilon())\n        x *= 0.25\n    \n        # clip to [0, 1]\n        x += 0.5\n        x = np.clip(x, 0, 1)\n    \n        # convert to RGB array\n        x *= 255\n        if K.image_data_format() == 'channels_first':\n            x = x.transpose((1, 2, 0))\n        x = np.clip(x, 0, 255).astype('uint8')\n        return x\n    \n    \n    def normalize(x):\n        \"\"\"Utility function to normalize a tensor by its L2 norm\"\"\"\n        return (x + 1e-10) / (K.sqrt(K.mean(K.square(x))) + 1e-10)\n    \n    \n    def build_guided_model():\n        \"\"\"Function returning modified model.\n        \n        Changes gradient function for all ReLu activations\n        according to Guided Backpropagation.\n        \"\"\"\n        if \"GuidedBackProp\" not in ops._gradient_registry._registry:\n            @ops.RegisterGradient(\"GuidedBackProp\")\n            def _GuidedBackProp(op, grad):\n                dtype = op.inputs[0].dtype\n                return grad * tf.cast(grad &gt; 0., dtype) * \\\n                       tf.cast(op.inputs[0] &gt; 0., dtype)\n    \n        g = tf.get_default_graph()\n        with g.gradient_override_map({'Relu': 'GuidedBackProp'}):\n            new_model = build_model()\n        return new_model\n    \n    \n    def guided_backprop(input_model, images, layer_name):\n        \"\"\"Guided Backpropagation method for visualizing input saliency.\"\"\"\n        input_imgs = input_model.input\n        layer_output = input_model.get_layer(layer_name).output\n        grads = K.gradients(layer_output, input_imgs)[0]\n        backprop_fn = K.function([input_imgs, K.learning_phase()], [grads])\n        grads_val = backprop_fn([images, 0])[0]\n        return grads_val\n    \n    \n    def grad_cam(input_model, image, cls, layer_name):\n        \"\"\"GradCAM method for visualizing input saliency.\"\"\"\n        y_c = input_model.output[0, cls]\n        conv_output = input_model.get_layer(layer_name).output\n        grads = K.gradients(y_c, conv_output)[0]\n        # Normalize if necessary\n        # grads = normalize(grads)\n        gradient_function = K.function([input_model.input], [conv_output, grads])\n    \n        output, grads_val = gradient_function([image])\n        output, grads_val = output[0, :], grads_val[0, :, :, :]\n    \n        weights = np.mean(grads_val, axis=(0, 1))\n        cam = np.dot(output, weights)\n    \n        # Process CAM\n        cam = cv2.resize(cam, (W, H), cv2.INTER_LINEAR)\n        cam = np.maximum(cam, 0)\n        cam_max = cam.max() \n        if cam_max != 0: \n            cam = cam / cam_max\n        return cam\n        \n    def grad_cam_batch(input_model, images, classes, layer_name):\n        \"\"\"GradCAM method for visualizing input saliency.\n        Same as grad_cam but processes multiple images in one run.\"\"\"\n        loss = tf.gather_nd(input_model.output, np.dstack([range(images.shape[0]), classes])[0])\n        layer_output = input_model.get_layer(layer_name).output\n        grads = K.gradients(loss, layer_output)[0]\n        gradient_fn = K.function([input_model.input, K.learning_phase()], [layer_output, grads])\n    \n        conv_output, grads_val = gradient_fn([images, 0])    \n        weights = np.mean(grads_val, axis=(1, 2))\n        cams = np.einsum('ijkl,il-&gt;ijk', conv_output, weights)\n        \n        # Process CAMs\n        new_cams = np.empty((images.shape[0], W, H))\n        for i in range(new_cams.shape[0]):\n            cam_i = cams[i] - cams[i].mean()\n            cam_i = (cam_i + 1e-10) / (np.linalg.norm(cam_i, 2) + 1e-10)\n            new_cams[i] = cv2.resize(cam_i, (H, W), cv2.INTER_LINEAR)\n            new_cams[i] = np.maximum(new_cams[i], 0)\n            new_cams[i] = new_cams[i] / new_cams[i].max()\n        \n        return new_cams\n    \n    \n    def compute_saliency(model, guided_model, img_path, layer_name='block5_conv3', cls=-1, visualize=True, save=True):\n        \"\"\"Compute saliency using all three approaches.\n            -layer_name: layer to compute gradients;\n            -cls: class number to localize (-1 for most probable class).\n        \"\"\"\n        preprocessed_input = load_image(img_path)\n    \n        predictions = model.predict(preprocessed_input)\n        top_n = 5\n        top = decode_predictions(predictions, top=top_n)[0]\n        classes = np.argsort(predictions[0])[-top_n:][::-1]\n        print('Model prediction:')\n        for c, p in zip(classes, top):\n            print('\\t{:15s}\\t({})\\twith probability {:.3f}'.format(p[1], c, p[2]))\n        if cls == -1:\n            cls = np.argmax(predictions)\n        class_name = decode_predictions(np.eye(1, 1000, cls))[0][0][1]\n        print(\"Explanation for '{}'\".format(class_name))\n        \n        gradcam = grad_cam(model, preprocessed_input, cls, layer_name)\n        gb = guided_backprop(guided_model, preprocessed_input, layer_name)\n        guided_gradcam = gb * gradcam[..., np.newaxis]\n    \n        if save:\n            jetcam = cv2.applyColorMap(np.uint8(255 * gradcam), cv2.COLORMAP_JET)\n            jetcam = (np.float32(jetcam) + load_image(img_path, preprocess=False)) / 2\n            cv2.imwrite('gradcam.jpg', np.uint8(jetcam))\n            cv2.imwrite('guided_backprop.jpg', deprocess_image(gb[0]))\n            cv2.imwrite('guided_gradcam.jpg', deprocess_image(guided_gradcam[0]))\n        \n        if visualize:\n            plt.figure(figsize=(15, 10))\n            plt.subplot(131)\n            plt.title('GradCAM')\n            plt.axis('off')\n            plt.imshow(load_image(img_path, preprocess=False))\n            plt.imshow(gradcam, cmap='jet', alpha=0.5)\n    \n            plt.subplot(132)\n            plt.title('Guided Backprop')\n            plt.axis('off')\n            plt.imshow(np.flip(deprocess_image(gb[0]), -1))\n            \n            plt.subplot(133)\n            plt.title('Guided GradCAM')\n            plt.axis('off')\n            plt.imshow(np.flip(deprocess_image(guided_gradcam[0]), -1))\n            plt.show()\n            \n        return gradcam, gb, guided_gradcam\n    \n    if __name__ == '__main__':\n        model = build_model()\n        guided_model = build_guided_model()\n        gradcam, gb, guided_gradcam = compute_saliency(model, guided_model, layer_name='conv2d_187',\n                                                 img_path='/Users/User/Desktop/download.jpg', cls=-1, visualize=True, save=True)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fblw6y/code_hanging_for_gradcam/"}, {"autor": "Gruhit13", "date": "2020-06-26 02:26:40", "content": "YOLO algorithm gives multiple anchor boxes, how to get a single one among them ? /!/ In YOLO algorithm for object detection the entire -----> image !!!  is divided in SxS grids with each grid having its own anchor box. So how to create one such last layer that takes all those SxS anchor boxes and gives only one anchor box as output. Because keras does not allow to pass the ground truth value in any intermediate layer so than how to accomplish desired task ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hfzb5j/yolo_algorithm_gives_multiple_anchor_boxes_how_to/"}, {"autor": "GlassSculpture", "date": "2020-06-26 01:11:39", "content": "What are your favourite / the most in-demand subfields of ML? /!/ I recently had [this 80,000 hours podcast](http://80000hours.org/podcast/episodes/olsson-and-zie\u2026) on fast track routes to becoming an ML engineer recommended to me. A strong recommendation that both of the guests shared was to pick a subfield of ML - say a particular class of tasks as viewed at [https://paperswithcode.com/sota](https://paperswithcode.com/sota) , or a type of method - and to read + reimplement a handful of the key papers and algorithms in that area. Effectively directly show that you're capable of implementing the state of the art in the area that you want to work in. \n\nThis got me thinking about the kinds of subfields that I and others might find interesting. So far my ML education has been pretty generalist as I imagine it is for most people when starting out.\n\nHaving thought about it my own personal favourite would probably be the set of Computer Vision tasks applied to the Medical area - eg medical -----> image !!!  segmentation. But there's a bunch of others I find interesting too (time series forecasting, adversarial attacks and defence, deep RL). \n\nAnyway, curious to hear what those of you who've given any thought to this think are the most interesting, or most in-demand (in a career sense), subfields of ML?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hfy7ry/what_are_your_favourite_the_most_indemand/"}, {"autor": "chubbs2402", "date": "2020-06-25 21:50:00", "content": "Soft SVM on MNIST data /!/ How to implement the soft-SVM learning algorithm, implemented using stochastic gradient descent on [MNIST data](http://yann.lecun.com/exdb/mnist/).The MNIST data has more than two classes. Instead of directly performing multi-class classification,  I want  to perform binary classification for each digit.This means, for a digit D, I will run a binary classification to label each -----> image !!!  as D or not-D.I also want to implement both linear and ref kernel without using any predefined libraries in [python.Is](https://python.Is) there any reference code or please tell me how to go about it?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hfuypm/soft_svm_on_mnist_data/"}, {"autor": "Transit-Strike", "date": "2020-06-25 15:30:21", "content": "How are the downsampling pipelines employed in a CNN different from a Gaussian Image Pyramid? /!/ I have been reading about -----> image !!!  preprocessing trough Image Pyramids and there are mentions about how the filter + downsample prcoess is similar to what is seen in CNNs and I can't understand what makes them different.\n\n&amp;#x200B;\n\nThe only thing I can think of is that gaussian WMF has a fixed structure used to clean up images while a CNN does something different( tries to learn filters/shapes).\n\n&amp;#x200B;\n\nI have also seen some papers that seem to try and combing them to create CNPs(Convolutional Neural Pyramids)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hfo3cx/how_are_the_downsampling_pipelines_employed_in_a/"}, {"autor": "treask14", "date": "2020-06-25 12:33:33", "content": "Learning to Build a Model for Sketch-to-Color Image Generation using Conditional GANs /!/ I started a series about **Generative Adversarial Networks** on the Towards Data Science Publication at [https://towardsdatascience.com/tagged/gans-series](https://towardsdatascience.com/tagged/gans-series) and recently have posted a new article that covers a walkthrough tutorial of building a Conditional GAN model from scratch which **predicts a colored -----> image !!!  of the given black &amp; white sketch input of an Anime**. You can also find the link to it's GitHub repository in the article if you want to download it onto your systems and try it for yourself.\n\n&amp;#x200B;\n\nDo give it a read and let me know if you find it interesting, informative, or for any feedback on how it could've been better.\n\n&amp;#x200B;\n\n**Link to the recent article:** [https://towardsdatascience.com/generative-adversarial-networks-gans-89ef35a60b69?source=friends\\_link&amp;sk=635c457c652ecc8eb92cc6f202565b49](https://towardsdatascience.com/generative-adversarial-networks-gans-89ef35a60b69?source=friends_link&amp;sk=635c457c652ecc8eb92cc6f202565b49)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hflalf/learning_to_build_a_model_for_sketchtocolor_image/"}, {"autor": "wymco", "date": "2020-06-25 11:58:03", "content": "How to detect specific words in an -----> image !!! ? /!/ I am working on a scanned document with description of items in the list. I want to train a model that will put specific words in a box, at each occurrence. I am reading the documentation of Opencv and Tesseract, but can't seem to find a solution yet.\n\nDo you know any resource that can help me", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hfktlc/how_to_detect_specific_words_in_an_image/"}, {"autor": "botechga", "date": "2020-06-24 22:39:35", "content": "Microscope slides shape detection questions/feedback. YOLO? RCNN? /!/ Hey everyone,\n\nSo I am a grad student and I often record microscope images for QC.  I would like to automate some -----> image !!!  analysis. I have previously played around with some python scripts in with moderate [results](https://imgur.com/a/ycYyUzv). These are based on simple canny edge detection and contour algorithms. \n\nI would like to implement a ML algorithm but I come from a biochemistry background so I am far from an ML expert. So I would appreciate any feedback or suggestions :)\n\nMy goal is to count and locate instances of 2 different classes of triangles in a slide like [this](https://imgur.com/l2KrLe1).\n\nThe goal is to detect, count and then pull statistics on the number of regular [triangle](https://imgur.com/wD1gN7T) (class 1 you can see in the magnified image) or [labeled triangles](https://imgur.com/3FIWNTc) with the little bright spots (class 2). These are example low quality images I pulled from the figure above. Normally these are much better images and more clearly labeled but I think you guys get the idea.\n\nSo I currently have a inspiron laptop with with a fresh linux mint ubuntu os and miniconda installed. There's  duel core intel i7s and no GPU so the hardware is a little limited.  \n\nFrom my reading so far it seems like R-CNN or YOLO would be my best options for detection. Ive used opencv before and I figure keras and tensorflow are a good option for implementing networks. But also the YOLOv5 based on pytorch seems interesting to me. However I'm not sure how feasible these are with my hardware...\n\nI probably have about 50 slides with maybe 100 structures each for training purposes. But going through them to annotate would be a hustle for sure. \n\nAre there any pre-trained weights for simple triangles that would work on these slides? Does this seem like a pipedream project to you guys? Or does this seem like something thats feasible to complete as a hobbiest? I'm I even looking to take this in the right direction in yall's opinions??\n\nAny feedback is welcome Thanks!!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hfb551/microscope_slides_shape_detection/"}, {"autor": "ConVit", "date": "2020-06-24 18:50:13", "content": "Where to train model online? Please help. /!/ I tried to fine tune Deep Lab 3 model on my local machine. Unfortunatly, it\u2019s a large model and cannot be trained using my 4GB GPU. Out of memory is thrown even when batch size is 1.\n\nI am thinking about renting a cloud server or use a cloud service to train 200k images. My budget is low since this is my personal -----> image !!!  segmentation project.\n\nWhat service do you recommend? Can you estimate the cost and the time needed for training? Thanks in advange.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hf6v56/where_to_train_model_online_please_help/"}, {"autor": "NoDipThisIsMyAlt", "date": "2020-06-24 16:03:35", "content": "Help with a project on NVIDIA Jetson Nano /!/ Hello, I am a graduating high school senior that was just recently hired at a tech company as a summer intern. They set me up with a NVIDIA Nano and had me go through the tutorial.\n\nI worked through it just fine and understood most of the concepts. However, after I finished the course my employer asked me to set up the board differently so instead of using the -----> camera !!!  to capture a training model we upload pictures as well. I had a little trouble with that and got stuck.\n\nCurrently where I am at is using several datasets as well as the terminal to predict some pictures. However, everywhere I looked I found no place to make my own database. That is one place I wish to start. \n\nSome background: I have VERY little experience in CS. I have taken AP Computer science in high school, but that really only teaches the basics of JS. I have needed to teach myself python as that is what it seems much of Machine Learning is about. However, if anyone has any pointers there I would be happy to accept help as well. I've seen several examples on GitHub, but would have no idea where to even start on that website.\n\nBasically all I am trying to ask is for some help in machine learning in general. I was kinda rushed into this without any background. It is what I want to study in college, but I thought this job would give me a little background before I got there.\n\n&amp;#x200B;\n\nTL;DR: How to create your own datasets. Any other help is welcome", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hf3nc9/help_with_a_project_on_nvidia_jetson_nano/"}, {"autor": "grid_world", "date": "2020-06-24 11:22:21", "content": "TensorFlow 2.0 Data Augmentation: tf.keras.preprocessing.image.ImageDataGenerator flow() method /!/ I am trying to perform data augmentation using TensorFlow 2.2.0 and Python 3.7 for LeNet-300-100 Dense neural network for MNIST dataset. The code I have is as follows:\n\n&amp;#x200B;\n\n`batch_size = 60`\n\n`num_classes = 10`\n\n`num_epochs = 100`\n\n&amp;#x200B;\n\n`# Data preprocessing and cleadning:`\n\n`# input -----> image !!!  dimensions`\n\n`img_rows, img_cols = 28, 28`\n\n&amp;#x200B;\n\n`# Load MNIST dataset-`\n\n`(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()`\n\n&amp;#x200B;\n\n`if tf.keras.backend.-----> image !!! _data_format() == 'channels_first':`\n\n`X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)`\n\n`X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)`\n\n`input_shape = (1, img_rows, img_cols)`\n\n`else:`\n\n`X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)`\n\n`X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)`\n\n`input_shape = (img_rows, img_cols, 1)`\n\n&amp;#x200B;\n\n`print(\"\\n'input_shape' which will be used = {0}\\n\".format(input_shape))`\n\n`# 'input_shape' which will be used = (28, 28, 1)`\n\n&amp;#x200B;\n\n`# Convert datasets to floating point types-`\n\n`X_train = X_train.astype('float32')`\n\n`X_test = X_test.astype('float32')`\n\n&amp;#x200B;\n\n`# Normalize the training and testing datasets-`\n\n`X_train /= 255.0`\n\n`X_test /= 255.0`\n\n&amp;#x200B;\n\n`# convert class vectors/target to binary class matrices or one-hot encoded values-`\n\n`y_train = tf.keras.utils.to_categorical(y_train, num_classes)`\n\n`y_test = tf.keras.utils.to_categorical(y_test, num_classes)`\n\n&amp;#x200B;\n\n`X_train.shape, y_train.shape`\n\n`# ((60000, 28, 28, 1), (60000, 10))`\n\n&amp;#x200B;\n\n`X_test.shape, y_test.shape`\n\n`# ((10000, 28, 28, 1), (10000, 10))`\n\n&amp;#x200B;\n\n`# Example of using 'tf.keras.preprocessing.-----> image !!! .ImageDataGenerator class's - flow(x, y)':`\n\n`datagen = ImageDataGenerator(`\n\n`# featurewise_center=True,`\n\n`# featurewise_std_normalization=True,`\n\n`rotation_range = 20,`\n\n`width_shift_range = 0.2,`\n\n`height_shift_range = 0.2,`\n\n`horizontal_flip = True`\n\n`)`\n\n&amp;#x200B;\n\nNow, when I see the number of batches produced by 'datagen.flow()' with the code:\n\n&amp;#x200B;\n\n`# Sanity check-`\n\n`i = 0`\n\n&amp;#x200B;\n\n`for x, y in datagen.flow(X_train, y_train, batch_size = batch_size, shuffle = True):`\n\n`# print(\"\\ntype(x) = {0}, type(y) = {1}\".format(type(x), type(y)))`\n\n`# print(\"x.shape = {0}, y.shape = {1}\\n\".format(x.shape, y.shape))`\n\n`print(i, end = ', ')`\n\n`i += 1`\n\n&amp;#x200B;\n\nThe value of \\*i\\* keeps increasing without terminating. Of course something is going wrong. According to what I know, the number of batches = number of training examples / batch size.\n\nTherefore, in this example, the number of batches = 60000 / 60 = 1000.\n\n&amp;#x200B;\n\nThen why is it producing so many batches of augmented data? And how can I stop it? What's going wrong?\n\n&amp;#x200B;\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hezam8/tensorflow_20_data_augmentation/"}, {"autor": "adam0ling", "date": "2020-06-24 09:12:01", "content": "Make your first -----> image !!!  classifier using tensorflow.js and node.js", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hextzu/make_your_first_image_classifier_using/"}, {"autor": "Arabum97", "date": "2020-06-18 22:14:22", "content": "-----> Image !!!  similarity /!/ I'm building a network that chooses between 2 images the most similar to a third one. I'm a bit confused on how to proceed, I've looked to triple networks and I've built one that takes the 3 images as input. The 3 images are passed to a pre-trained net (VGG16) that extracts the features (the network and its weights are shared). After that, there are 2 dense layers and the network calculates the euclidean distances between the anchor image and the other 2. Finally, the 2 distances are subtracted and the result is passed into a sigmoid.\n\nIs this the right approach to the problem? Do I have understand wrong triplet networks? I'm getting very low accuracy (around 50%) so I guess that the approach is wrong...", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hbo8y0/image_similarity/"}, {"autor": "fw_Flicker", "date": "2020-06-18 17:56:10", "content": "I made an ML powered app, using IMDB data for training, that predicts Box Office Revenue of Fantasy Films - roll a random -----> film !!! , or manually input your own, and the model predicts success of your -----> film !!! !", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hbjll1/i_made_an_ml_powered_app_using_imdb_data_for/"}, {"autor": "Unchart3disOP", "date": "2020-06-18 14:51:52", "content": "How did you deal with a very small -----> image !!!  dataset /!/ Hey, I currently am working with a very small image dataset of around 300 images, I have used feature extraction extracting around 100 features and passing them to classifiers such as XGB and Linear Classifiers to achieve only an accuracy on 0.5. What are other ways should I be looking into inorder to increase this accuracy?\n\nSome options in my head are:\n- Transfer Learning on something like a resnet\n- Building a CNN and trying to see if it manages to achieve anything\n- More Features\n\nAlso, I have noticed I get a balanced accuracy of 0.31, is that the actual accuracy I should be looking at, since my dataset is unbalanced? Thanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hbg2gq/how_did_you_deal_with_a_very_small_image_dataset/"}, {"autor": "ykilcher", "date": "2020-06-18 13:40:42", "content": "[D] Paper Explained - -----> Image !!!  GPT: Generative Pretraining from Pixels (Full Video Analysis)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hbesav/d_paper_explained_image_gpt_generative/"}, {"autor": "crystal_cthulhu", "date": "2020-06-18 08:38:52", "content": "Cross Datasets Openly Available /!/ Hello everyone. I am an undergraduate student new to ML.\n\nI  am working on a fine-tuning method which is meant to generalize the model (I guess I should call it de-tune) to do better on a wider range of datasets and with varying distributions (such as different lighting or -----> camera !!!  distance).\n\nI  am new in this area of trying to generalize the model. Are there openly available datasets having images of the same classes? I tried searching  for dog-cat datasets as I thought there would be multiple such openly available datasets but I can only find the Kaggle dataset online. What I basically mean is that suppose I choose ImageNet as a dataset, then I need another dataset with the same classes as ImageNet to compare my model's performance without fine-tuning on the second dataset. Of course, the datasets need not be only of images, NLP tasks are ok with me as well. \n\nFurther, if anyone can cite any relevant work done previously on this it would be great!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hbatys/cross_datasets_openly_available/"}, {"autor": "AskSid_AI", "date": "2020-06-18 07:08:23", "content": "Is AI and Machine Learning the same? /!/ We have all heard or read about how AI is the next big thing. It seems like every 3rd article or blog is about AI and how companies are leveraging this shiny new tech to solve some of their biggest problems. And the same words are seen together. AI vs Machine learning, Machine learning vs Deep Learning, Neural Networks \u2026.. and so on.\n\nEvery so often I get asked (by engineers and non-engineers both) \u201cAre AI and machine learning the same?\u201d So let\u2019s answer this.\n\nWhat is AI? What are its goals?\n\nI don\u2019t want to give a textbook answer here on the AI meaning. I am sick of them myself. So here it goes\u2026\n\nAI\u2019s primary goal is to create an artificial intelligence. This may or may not include a robotic body as such. That\u2019s only for movies, not in real world always. An artificial intelligence needs to be capable of most, if not all, the things that humans do.\n\nIt should be able to learn by observation or actions.\n\nIt should be able to decide on a goal\n\nIt should be able to create a plan to achieve this goal in an optimal way.\n\nIt should be able to sense or feel and have an awareness of its surroundings\n\nIt should be able to reason and draw conclusions and so on..\n\nAs you can see this is a pretty lofty goal. It is quite abstract as to how this can be achieved. And that\u2019s the main point. AI is abstract in general. It has goals but needs ways of achieving them.\n\nHow does machine learning assist AI goals? Is machine learning the same as AI?\n\nMachine learning is one of many tools that is used by an AI to achieve its goals. Machine learning unlike AI is very clear about its goals. Machine learning wants to create function approximations or models for some input and output combinations (we call this \u201ctraining data\u201d). Using these AI models, it can later generate outputs (we call this \u201cpredictions\u201d) on unseen data. To create these models, it uses various AI algorithms which have hyper-parameters to help tune them.A very easy example is,\n\nInput (X)         Output (Y)\n\n1                      3\n\n2                      6\n\n3                      9\n\n4                      12\n\nIt\u2019s clear from these combinations that the output is 3 times the input. So the function would be,\n\nY = 3X\n\nNow this becomes our model. If we have unseen X values, we apply the model to predict the Y values. But of course, the AI algorithm is never this simple in real life. Here X could refer to a vector generated by text, -----> image !!! , audio, etc and Y could be any useful prediction.\n\nThese types of predictive models are used by AI to reach its goals.\n\nWhere exactly is Machine Learning used?\n\nSo we know that machine learning is used to create predictive models. But where exactly are these models used? What can you do with them? Let\u2019s look at some relevant examples for businesses.\n\nPersonalized recommendations \u2013 When netflix recommends movies based on your previous actions, when amazon recommends products for you to checkout, Machine learning is used to predict them\n\nSearch engines \u2013 Google uses machine learning to provide you with search results for your queries\n\nMaps \u2013 Google uses machine learning to recommend the best route for your journey based to driving time calculations taking into consideration the live traffic and road closures data\n\nVirtual Assistants and Customer Service Chatbot \u2013 [Natural language processing](https://www.asksid.ai/resources/what-is-natural-language-processing/) techniques of machine learning are used to provide a conversational AI experience to the user. This can be used in a wide variety of fields like customer service and customer engagement in ecommerce.\n\nVideo Surveillance uses machine learning to reduce human effort by predicting any unusual behavior in video feeds\n\nFace Recognition \u2013 Many smartphone providers use machine learning to help you unlock your phone with your face\n\nSpam filtering \u2013 Email and SMS providers use machine learning to help filter out unwanted messages\n\nAI vs Machine learning vs Deep learning\n\nThis is another familiar question: AI vs machine learning \u2013 what is the difference? If I had a nickel\u2026\n\nMachine learning\u2019s goal is to create predictive models. Deep learning wants to do the same. Really. They\u2019re like twins.\n\nWhile machine learning likes to use traditional models like SVM, Decision trees, etc Deep learning uses fancier (read as \u201cmore complex\u201d) neural network AI algorithms which work like the human brain (in theory). And that\u2019s it. Everything else is pretty much the same.\n\nI usually don\u2019t refer to Deep Learning as a separate term. To me they are the same. Deep Learning is Machine Learning with updated AI Algorithms.\n\nConclusion \u2013 AI vs Machine learning\n\n**To summarise,**\n\nAI vs Machine learning \u2013 AI is an abstract concept with many goals. Machine Learning is a specific tool used by AI to realise them.\n\nDeep Learning is Machine Learning with updated algorithms.\n\nFor more content on AI and how businesses can leverage it, please check this article from Dinesh Sharma  \u201cCan AI make your business far more intelligent\u201d\n\nCurious Much? Drop us a note at [contact@asksid.ai](mailto:contact@asksid.ai) or visit our website [www.asksid.ai](https://www.asksid.ai) and let\u2019s start a conversation!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hb9rki/is_ai_and_machine_learning_the_same/"}, {"autor": "David_hack", "date": "2020-06-18 03:34:47", "content": "text from -----> image !!!  python | how to convert -----> image !!!  to editable text", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hb6u8v/text_from_image_python_how_to_convert_image_to/"}, {"autor": "yesgodyes", "date": "2020-06-18 02:29:55", "content": "Preprocessing DICOM for MRI-to-CT -----> Image !!!  translation. /!/ I am trying to do image translation in two different modalities of medical images. I am thinking of using recently published U-GAT-IT (ICLR-2020) architecture. I have datasets of MRI and CT images in DICOM format. If I had to do simple horse to zebra translation, the task is simple. You can just feed the images as it is without any kind of preprocessing. But, I am unsure of how to go about preprocessing the DICOM images (3D image). My initial thought is that one might not want to just use the DICOM-TO-PNG converter as it is 3D image.\n\nI found some people converting DICOM to NIfTI format before any kind of preprocessing. Can someone help me with how to preprocess these kinds of 3D images before you can feed it for image translation? Any related resources would be great.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hb5vid/preprocessing_dicom_for_mritoct_image_translation/"}, {"autor": "GeNiaaz", "date": "2020-06-18 01:26:10", "content": "Formatting dataset for Detectron 2 /!/ I have a 10000 -----> image !!!  dataset of jpg files, labelled as \"01\\_01. jpg, 01\\_02.jpg\" and so on. I also have a labels\\_coco.json file with data on the images such as bounding boxes, label of the object in the bounding boxes etc. Is there anything else I have to prepare before I can run this in Detechtron 2?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hb4uh0/formatting_dataset_for_detectron_2/"}, {"autor": "OutOfApplesauce", "date": "2020-06-17 18:54:11", "content": "Reasonably experienced with ML, but not vision tasks. Is there a model that can take a general shape or -----> image !!!  and verify that the shape/-----> image !!!  is inside of another -----> image !!!  /!/ For example, I want to verify that there is a sword icon inside of the of the frame of a video.  Or possibly a tell if there is a *red* shield on the screen.\n\nThe shape/color of the objects will pretty much always be the same, however will appear and disappear randomly and appear in different parts of the image.  Just looking for a binary \"is it there\" do not need bouding boxes.\n\nI was writing a script to do this, but figured if I could make use of a GPU/CNN for speed it would greatly save me some time and effort.\n\nLet me know if my description is not detailed enough.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/haxljp/reasonably_experienced_with_ml_but_not_vision/"}, {"autor": "ai_yoda", "date": "2020-03-25 09:19:22", "content": "-----> Image !!!  segmentation in 2020: Architectures, Losses, Datasets, and Frameworks. What are you using? /!/ Hi, all!\n\nWe figured that since image segmentation is used so heavily in so many areas it would be interesting to take a look at what is used in 2020 in terms of:  \n\n\n* architectures\n* losses\n* datasets\n* frameworks\n\nand put it all in a blog post.\n\nI hope you'll find this is useful.  \nAny important things we missed?\n\n[Read the article](https://neptune.ai/blog/image-segmentation-in-2020?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=blog-image-segmentation-2020)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fonb7p/image_segmentation_in_2020_architectures_losses/"}, {"autor": "BigPP6964", "date": "2020-03-24 20:21:43", "content": "Please help /!/ Hi I am new in OpenCV and trying to run this code but I keep getting this error\n\n&amp;#x200B;\n\nimport cv2\n\nimport numpy as np\n\nfrom PIL import -----> Image !!! \n\nimport os,json\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\ncam =cv2.VideoCapture(0)\n\nface\\_detector = cv2.CascadeClassifier('scratch.xml')\n\n&amp;#x200B;\n\nglobal user\n\nuser = input(\"Kullanici adi :\")\n\nprint(\"\\\\n Kameraya bakin ve bekleyin\")\n\nsay = 0\n\nos.mkdir(\"dataset/\"+ user)\n\n&amp;#x200B;\n\nwhile (True):\n\nret,[cerceve=cam.read](https://cerceve=cam.read)()\n\ncerceve=cv2.flip(cerceve,1)\n\ngri =cv2.cvtColor(cerceve, cv2.COLOR\\_BGR2GRAY)\n\ngri = np.array(gri, dtype='uint8')\n\nfaces = face\\_detector.detectMultiScale(gri,1,5, 5)\n\nfor (x, y, w, h) in faces:\n\ncv2.rectangle(cerceve, (x, y), (x + w, y + h), (255,0, 0), 2)\n\nsay+=1\n\npath =\"dataset/\"+user+\"/\"\n\ncv2.imwrite(path.str(say)+\".jpg\",gri\\[y:y+ h,x:x +w\\])\n\ncv2.imshow('DATA',cerceve)\n\nk = cv2.waitKey(100) &amp; 0xff\n\nif k == 27:\n\nbreak\n\nelif say &gt;=50:\n\nbreak\n\ncam.release()\n\ncv2.destroyAllWindows()\n\n&amp;#x200B;\n\nThe error is\n\nfaces = face\\_detector.detectMultiScale(gri,1,5, 5)\n\ncv2.error: OpenCV(4.2.0) /Users/travis/build/skvark/opencv-python/opencv/modules/objdetect/src/cascadedetect.cpp:1389: error: (-215:Assertion failed) scaleFactor &gt; 1 &amp;&amp; \\_image.depth() == CV\\_8U in function 'detectMultiScale'\n\n&amp;#x200B;\n\nCan somebody please help? Thanks so much in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/focdif/please_help/"}, {"autor": "RememberMyFart", "date": "2020-03-23 19:48:26", "content": "Looking to improve messy programming for advanced/ bigger projects /!/ I have some fair experience in ML and working with tensorflow and have already implemented some projects (e.g. -----> Image !!!  Captioning using MSCOCO). I am usually trying to implement as much as I can on my own (training steps, not using keras sequential models. My code looks quite similiar to this: https://www.tensorflow.org/tutorials/quickstart/advanced)\n Since my background primarily is not CS I lack skills in proper software-engineering/ coding, meaning, that my code often times gets quite confusing and messy especially if the project scales up. Could you recommend me any Github Repo of a rather big (bigger than a simple mnist implementation) project which I can use as somewhat of a blueprint for my programming? Do you alternatively know of any tutorials/ guides on that topic?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fnqka3/looking_to_improve_messy_programming_for_advanced/"}, {"autor": "SuccMyStrangerThings", "date": "2020-08-10 14:44:35", "content": "GAN loss /!/ Sorry for bothering. You'll be seeing me here frequently for asking my silly doubts. \n\nAccording to minimax, the discriminator's object is log(D(x)) + log(1-D(G(z))) where D(x) is to be maximized and D(G(z)) is minimized. Logically it makes sense but thinking about it mathematically, I'm confused. I know the discriminator should not be able to tell the difference between the real and fake -----> image !!!  but if we are minimizing D(G(z)), then log(1-D(G(z)) will be maximum right? Because log(1-small value) will be large", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i7643e/gan_loss/"}, {"autor": "Is_verydeep69_dawg", "date": "2020-08-10 04:53:09", "content": "-----> Image !!!  segmentation", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i6yj91/image_segmentation/"}, {"autor": "debaser32", "date": "2020-08-09 21:59:36", "content": "Help needed defining model for Music Recommendation System. /!/ Hi, I'm currently working on a music recommendation system. I'm using a PyTorch neural network to try and recognize genres by analyzing an MEL spectrogram of a given song and returned the genre the model thinks the song is. Each -----> image !!!  has a genre associated with it there are 8 different genres in the data set.\n\nCurrently I have an extremely poor accuracy.  I'm not certain how to define my model so it is better at analyzing the images. Or what type of loss optimizer I should be using.\n\nI've attached a sample image from my data set. This is what my model will be analyzing. \n\nThis is what my model looks like now as well as the optimizer and loss function I'm using.\n\n    Net(\n      (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n      (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n      (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n      (conv4): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1))\n      (fc1): Linear(in_features=4096, out_features=512, bias=True)\n      (fc2): Linear(in_features=512, out_features=8, bias=True)\n    )\n    \n    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n    loss_function = nn.MSELoss()\n\nAny type of help which might improve my model would be appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i6s45v/help_needed_defining_model_for_music/"}, {"autor": "spmallick", "date": "2020-08-09 18:44:20", "content": "Otsu\u2019s Thresholding with OpenCV /!/ One of the most common pre-processing techniques used in traditional computer vision is called -----> image !!!  thresholding. It simplifies the image for easy analysis. For example, you may use it in medical image processing to reveal tumor in a mammogram or localize a natural disaster in satellite images.  \n\n\nhttps://preview.redd.it/8axl5qgev0g51.png?width=576&amp;format=png&amp;auto=webp&amp;s=61b9f122b3ca3c691d9e257e84afea0f3742996b\n\nA problem with simple thresholding is that you have to manually specify the threshold value. That requires a lot of trial and error. A threshold that works with one image may not work with another image. So, we need a way to automatically determine the threshold.  \n\n\nThis is where Nobuyuki Otsu's creation aptly named as Otsu's technique helps us with auto thresholding. Let's look at the post for more details.  \n\n\n[https://www.learnopencv.com/otsu-thresholding-with-opencv/](https://el2.convertkit-mail.com/c/38uvxqr24vskhvex6zip/n2hohquo4p04eq/aHR0cHM6Ly93d3cubGVhcm5vcGVuY3YuY29tL290c3UtdGhyZXNob2xkaW5nLXdpdGgtb3BlbmN2Lw==)  \n\n\nand the code is at the link below.  \n[https://github.com/spmallick/learnopencv/tree/master/otsu-method](https://github.com/spmallick/learnopencv/tree/master/otsu-method?ck_subscriber_id=371373457).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i6olso/otsus_thresholding_with_opencv/"}, {"autor": "jelly141", "date": "2020-08-09 13:12:05", "content": "need help with satellite data processing /!/ So i have hyper spectral satellite -----> image !!! s of the same area, each -----> image !!!  representing a single band (242 bands = 242 -----> image !!! s). I have to use classification to classify land, water etc. and since I have just one image of the area. I'm guessing to apply a classification algorithm I need to slice this image into multiple images in order to form my train/test set. I've imported rasterio but cant figure out how to slice this image and save it as separate images and then apply an algorithm.\n\nAny input is appreciated. PLEASE HELP\n\n&amp;#x200B;\n\nhttps://preview.redd.it/bk26w1o28zf51.png?width=422&amp;format=png&amp;auto=webp&amp;s=72faf5cdbc2a48ef9de9bd2cc0ec7be2b039db9d", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i6j3pd/need_help_with_satellite_data_processing/"}, {"autor": "OnlyProggingForFun", "date": "2020-08-09 11:44:52", "content": "This AI can cartoonize any -----> picture !!!  or video you feed it! Tune in the video in caption at 3:08 to see more awesome examples using it, they passed in on The Avengers movie and the results are impressive!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i6i0bm/this_ai_can_cartoonize_any_picture_or_video_you/"}, {"autor": "AerosolHubris", "date": "2020-08-09 11:27:10", "content": "[Question] How are deep learning methods used to alter images? /!/ I understand the basics of ML, neural nets, and feature extraction. So I get how deep learning is used to classify -----> image !!! s (\"-----> image !!!  contains a banana\" for example). What I don't get is how it's used to convert one image to another. For example, what's going on under the hood when a portrait is aged, or given makeup? It can't be as basic as identifying lips and then coloring them. There's something else I'm missing. \n\nWhat are the features and the output in these cases? They seem so different from the usual classification/regression problems that output a numerical or categorical value.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i6htcw/question_how_are_deep_learning_methods_used_to/"}, {"autor": "luxowl", "date": "2020-08-08 20:45:36", "content": "Advices for building a classification model for succulent plant /!/ Hello friends,\n\nI  want to create a model classifying succulent plants, however there  doesn't seem to be a good dataset and there will be a lot of class.\n\nWhat  would be your approaches to scrap/create/clean the dataset and to  choose the best model for this problem? I'm not necessarily asking for a  whole solution but more of a lead.\n\nFor context, I took Andrew Ng's course and built some small models of -----> image !!!  classification.\n\nI  know this problem is not simple at all but I think this is a project  that will allow me to learn a lot and to get out of this \"framed\"  environment where you are already provided with everything.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i66gzv/advices_for_building_a_classification_model_for/"}, {"autor": "OnlyProggingForFun", "date": "2020-08-08 13:59:15", "content": "This AI can cartoonize any -----> picture !!!  or video you feed it! Paper Introduction &amp; Results examples", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i5zkpn/this_ai_can_cartoonize_any_picture_or_video_you/"}, {"autor": "thedeepreader", "date": "2020-08-07 15:20:16", "content": "[D] (A Brief Paper Review) Contrastive Learning for Unpaired -----> Image !!! -to------> Image !!!  Translation /!/ Video [https://youtu.be/PPQ-7HPkwBE](https://youtu.be/PPQ-7HPkwBE)\n\nPaper  [https://arxiv.org/abs/2007.15651](https://arxiv.org/abs/2007.15651) \n\nCode  [https://github.com/taesungp/contrastive-unpaired-translation](https://github.com/taesungp/contrastive-unpaired-translation) \n\n**Abstract**\n\nIn image-to-image translation, each patch in the output should reflect the content of the corresponding patch in the input, independent of domain. We propose a straightforward method for doing so -- maximizing mutual information between the two, using a framework based on contrastive learning. The method encourages two elements (corresponding patches) to map to a similar point in a learned feature space, relative to other elements (other patches) in the dataset, referred to as negatives. We explore several critical design choices for making contrastive learning effective in the image synthesis setting. Notably, we use a multilayer, patch-based approach, rather than operate on entire images. Furthermore, we draw negatives from within the input image itself, rather than from the rest of the dataset. We demonstrate that our framework enables one-sided translation in the unpaired image-to-image translation setting, while improving quality and reducing training time. In addition, our method can even be extended to the training setting where each \"domain\" is only a single image.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i5fppk/d_a_brief_paper_review_contrastive_learning_for/"}, {"autor": "vicpylon", "date": "2020-05-27 23:33:50", "content": "Best Approach to Categorizing D20 Dice /!/ I am in need of some guidance regarding sorting dice rolls. I have a data set of 5000 labeled D20 dice roll images, so I have 20 categories to work with. What is the best approach to this problem? I have tried AWS' \"Rekognition\" offering with custom labels and got that up to 97% accurate, but that system provides no feedback on what algorithm it uses to do the categorization. And it is expensive to operate. \n\nI am approaching this as an -----> image !!!  categorization problem. Should I be using text recognition instead?\n\nI am still learning ML and the subtleties of the different models/approaches eludes me. Any thoughts are appreciated.\n\n&amp;#x200B;\n\nThank you,\n\n&amp;#x200B;\n\nVic", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gruxpo/best_approach_to_categorizing_d20_dice/"}, {"autor": "SuccessfulLeadership", "date": "2020-05-27 22:07:16", "content": "How to deal with images with text noise? /!/ I have a dataset of images collected from google and bing images (scraped). basically I want to classify these images into binary classes (positive, negative). Images that contain a text originally from the -----> image !!!  (a photo of contract) should be classified positive (there are some other cases where the -----> image !!!  also should be positive but my problem is with the textual -----> image !!! s). I'm facing a problem that many negative images have text and caption that's add on the photo like a website address or logo (not original from the image). I'm afraid these images could hurt the model performance, in this case, what should I do? It doesn't make sense to through away useful images because of tiny text added on them. And could that actually hurt the model or the model would be able to capture the actual pattern from the dataset? \n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/grth1u/how_to_deal_with_images_with_text_noise/"}, {"autor": "Rahul_Desai1999", "date": "2020-05-27 17:00:27", "content": "I am trying to build a dog breed classifying nn using pytorch, but I am getting 0.00 loss while training. I have, in detail, explained my code. /!/ ABOUT THE DATASET:\n\nI'm using a kaggle dataset with 10,222 -----> image !!! s and 120 breeds, it has a .csv file which contains all the -----> image !!!  name with breed of the dog on it.\n\n&amp;#x200B;\n\nCODE:\n\n(I'm leaving the entire code but in sections and I'll explain what am i trying to do in each section\n\n    MAKE_TRAINING_TESTING_DATA = False\n    \n    if MAKE_TRAINING_TESTING_DATA == True:\n        IMG_SIZE = 100\n        train_data = []\n    \n        PATH = \"../Dog Breed/\"\n        TRAIN_PATH = os.path.join(PATH, \"train\")\n        df = pd.read_csv(\"../Dog Breed/labels.csv\")\n    \n        breed_dict = {}\n    \n        for i, breed in enumerate(df['breed'].unique()):\n            breed_dict[breed] = i\n    \n        for image in tqdm(os.listdir(TRAIN_PATH)):\n            if(image.endswith(\".jpg\")):\n                img = cv2.imread(os.path.join(TRAIN_PATH, image))\n                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n                img_name = image.replace(\".jpg\", \"\")\n                breed_name = (df[df['id'] == img_name]['breed'].values)[0]\n                breed_id = breed_dict[breed_name]\n    \n                train_data.append([np.array(img.transpose(2, 1, 0)), breed_id])\n    \n        np.random.shuffle(train_data)\n        np.save(\"train_data.npy\", train_data)\n    \n    train_data = np.load(\"train_data.npy\", allow_pickle=True)\n\nI'm preparing the data here, for me to use. \n\nI have created a BREED\\_DICT which contains all the breed names, and an index number corresponding to it.\n\nThen i create a list, which contains a list of two elements. 1st element is the image in its np.array form. 2nd is the index number of the breed\n\nCan it be a problem that I'm using integers as outputs and not one-hot vectors?\n\nAnd then i save it as .npy file for later use.\n\n&amp;#x200B;\n\n    TRAIN_PCT = 0.9\n    \n    X = torch.tensor([i[0] for i in train_data])\n    y = torch.tensor([i[1] for i in train_data])\n    train_X = X[0:int(TRAIN_PCT * len(train_data))]\n    train_y = y[0:int(TRAIN_PCT * len(train_data))]\n    \n    test_X = X[int(TRAIN_PCT * len(train_data)):]\n    test_y = y[int(TRAIN_PCT * len(train_data)):]\n    \n    print(len(train_X))            #9199\n    print(len(train_y))            #9199\n    print(len(test_X))            #1023\n    print(len(test_y))            #1023\n\nNow here I'm parting the training data and the test data.\n\n    class Net(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv1 = nn.Conv2d(3, 32, 5)\n            self.conv2 = nn.Conv2d(32, 64, 5)\n            self.conv3 = nn.Conv2d(64, 128, 5)\n            self.fc1 = nn.Linear(128*2*2, 512)\n            self.fc2 = nn.Linear(512, 120)\n            \n        def forward(self, x):\n            x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n            x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n            x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n            \n            x = x.flatten(start_dim=1)\n            \n            x = F.relu(self.fc1(x))\n            x = self.fc2(x)\n            \n            return F.softmax(x, dim=1)\n            \n    net = Net()\n\nHere, I have defined my nn\n\nMy first conv layer takes in 3 inputs as i am using rgb images and not grayscaling them\n\nMy last linear layer outputs 120 layers as there are 120 classes\n\nAnd i return softmax of the output.\n\n    EPOCHS = 3\n    BATCH_SIZE = 100\n    \n    optimizer = optim.Adam(net.parameters(), lr=0.001)\n    \n    for epoch in range(EPOCHS):\n        for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n    \n            batch_X = train_X[i:i+BATCH_SIZE].view(-1, 3, 100, 100)\n            batch_y = train_y[i:i+BATCH_SIZE]\n            net.zero_grad()\n            outputs = net(batch_X.float())\n            loss = F.nll_loss(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n        print(f\"{epoch}: {loss}\")\n\nOutput:\n\n 0: 0.0 \n\n 1: 0.0  \n\n 2: 0.0  \n\nHere, i pass in data for training.\n\nPlease check the form of my input data and my output data as i pass it in to the loss function. My output will be a tensor of 120 elements and I'm passing in batch\\_y as a tensor with single element.\n\nloss = F.nll\\_loss(outputs, batch\\_y)\n\noutput ----&gt; a tensor with 120 elements of softmax probabilities of each class\n\nbatch\\_y ----&gt; a tensor with single value of integer between 1 and 120.\n\nNote that I'm using nll\\_loss and Adam optimizer, can that be a problem?\n\n&amp;#x200B;\n\nPlease help if possible.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/grnjpi/i_am_trying_to_build_a_dog_breed_classifying_nn/"}, {"autor": "Rahul_Desai1999", "date": "2020-05-27 15:43:59", "content": "I can't figure out what the size of the input of the first Linear layer should be after going through some conv layers and some maxpooling. Can someone please explain? (Pytorch) /!/     class Net(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv1 = nn.Conv2d(3, 16, 5)\n            self.conv2 = nn.Conv2d(16, 64, 5)\n            self.conv3 = nn.Conv2d(64, 256, 5)\n            self.conv4 = nn.Conv2d(256, 1024, 5)\n            \n            self.fc1 = nn.Linear(1024*2*2, 512)            !!!!!!!!!!!!!!!!\n            self.fc2 = nn.Linear(512, 128)\n            self.fc3 = nn.Linear(128, 120)\n            \n        def forward(self, x):    #100 100\n            x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) #48 48\n            x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2)) #22 22\n            x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2)) #9 9  \n            x = F.max_pool2d(F.relu(self.conv4(x)), (2, 2)) #2 2\n            \n            x = x.flatten(start_dim=1)\n            \n            x = F.relu(self.fc1(x))\n            x = F.relu(self.fc1(x))\n            x = self.fc1(x)\n            \n            return F.softmax(x, dim=1)\n\nI am confused about how the parameters have to be passed from a conv layer to a linear layer in pytorch. This is my nn. And it gives me an error while trying to predict something random of a dimension that I want to predict.\n\n    op = net(torch.randn(3, 100, 100).view(-1, 3, 100, 100))\n    print(op)\n    \n    ERROR: size mismatch, m1: [1 x 512], m2: [4096 x 512]\n\nI want to pass in images with 3 color channels. \n\nWhat logic I'm using to decide it will be \"1024\\*2\\*2\" inputs for the first linear layer:\n\nIts a way i found online, which worked when i used a single color channel ( grayscaled images).\n\nWhenever you pass an -----> image !!!  of (X,X) dimensions through a conv layer, you have to decrease the dimensions by 4. And when you pass it through a maxpool layer with a filter size of (2, 2), you have to half the dimensions. \n\nBy that logic, I pass in images of 100, 100 dimensions. \n\nBy the end of the first conv, the image will be 96, 96 and after maxpool, it will be 48, 48. And this new dimensions will be the input to the next layer. And so on for the next layers.\n\nAt the end of it, here i have 2, 2 dimensions left, so to flatten it, I put the input size as 1024\\*2\\*2. \n\n&amp;#x200B;\n\nCan someone explain the correct way to do this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/grm2am/i_cant_figure_out_what_the_size_of_the_input_of/"}, {"autor": "sp7412", "date": "2020-05-27 15:08:14", "content": "How do you find accessible tutorial papers? /!/ This paper was written by non-CS researchers explaining things to other non-CS people. It makes their writing much more accessible to newbies. How do I find more papers like it?\n\n[Improving plankton -----> image !!!  classi\ufb01cation using context metadata](https://aslopubs.onlinelibrary.wiley.com/doi/epdf/10.1002/lom3.10324)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/grlf42/how_do_you_find_accessible_tutorial_papers/"}, {"autor": "StrasJam", "date": "2020-05-27 07:37:25", "content": "Is it possible to accurately classify these two small and overlapping classes? /!/ Disclaimer: I am a bit limited in my machine learning/stats knowledge. I come from more of a coding background. \n\nI have a small dataset that I am trying to classify into two groups. Each member of each group represents a tree branch that has a certain percentage of damage from pests (this is the Y variable). Overall, the two groups come from a larger dataset where the mean value of damage does show a greater difference between the groups, but we only have a few images of the branches so we are stuck with this small subset.\n\n  \nGroup 1: Y variable: \\[ 26.7,  33.3,  40.0,  32.0,  42.9,  16.0,  6.3,  37.8,  28.6,  29.4,  59.1\\]\n\nGroup 2: Y variable: \\[ 43.8,  15.4,  72.2,  16.0,  85.7,  0.0\\]\n\nI am trying to use a RF to classify them using a number of spectral features from the -----> image !!!  (about 13 features). Since the dataset is so small I just use a leave-on-out cross validation at the moment to test the performance.\n\nTo me, I feel that given the two groups don't really have much seperation in terms of their percentage of damage, that no classification method I try is going to achieve a very high accuracy (so far I achieve 65% accuracy). \n\nDoes this seem reasonable? Or do you think that I could manage to get better performance on this subset, though a different classifier perhaps (I have read that SVM can work better with low amounts of data)?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/grf6qi/is_it_possible_to_accurately_classify_these_two/"}, {"autor": "vladseremet", "date": "2020-05-27 07:33:29", "content": "Can you use an autoencoder with the input different from the output? /!/ For example, my input is a 128x128x2 -----> image !!!  where 1 layer is the depth map and the other contains semantic labels. I'm trying to map this input to a 32x32x32 3D semantically labeled occupancy grid. Both the input and the output are obtained in simulation and represent the same scene. Basically I want to train an autoencoder type model to extract a latent representation from the input image such that it would contain enough info to reconstruct the 3D occupancy grid. All explanations of autoencoders that I came across say that the dimensionality of the input and output should be the same, but I don't see any reason why it shouldn't work with different dimensionalities that relate to the same ground truth... Any ideas?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/grf4x5/can_you_use_an_autoencoder_with_the_input/"}, {"autor": "Hokus_Bogus", "date": "2020-05-26 18:15:29", "content": "What is the most optimized framework for deep learning with CUDA? /!/ I own a measly 1060 6GB and I want to know your opinion on which library is more efficient to train cnn -----> image !!!  classification networks.\n\n[View Poll](https://www.reddit.com/poll/gr2ck6)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gr2ck6/what_is_the_most_optimized_framework_for_deep/"}, {"autor": "Rahul_Desai1999", "date": "2020-05-26 17:43:19", "content": "I want to save a nn that i have trained using pytorch. How do i load this in another python script that i am working in? /!/  I have built a nn that predicts if the given -----> image !!!  is a dog or a cat.  Now i plan to give this an interface using tkinter. For that I'm working  in another .py file. How do i load the model in? And how would i use  the object to pass in images to it?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gr1qdg/i_want_to_save_a_nn_that_i_have_trained_using/"}, {"autor": "noobfish996", "date": "2020-05-26 16:58:22", "content": "Data Science/Machine Learning masters in Germany /!/ I am an BSc., international student in Finland. I'm aiming to apply for a Master in Machine Learning/Data Science field in Germany in 2021. Here is the list that I have researched so far:\n\n1. LMU: MSc Data Science\n2. TUM: Data Engineering and Analytics\n3. TU Berlin: EIT program (quite expensive: 13k per year)\n4. Saarland University: MSc Data Science and Artificial Intelligence\n5. T\u00fcbingen University: MSc Machine Learning\n6. Freie Universit\u00e4t Berlin: MSc Data Science\n\nMy current GPA is about 4.29 (1.5 in German scale) and I have done courses in Machine learning, Computer vision and -----> Image !!!  processing at my university, so I want to focus on Computer vision in Masters. I'm also planning to get B2 German at the end of this year, but I would prefer to study in English. \n\nI heard that Saarland and T\u00fcbingen have good labs and research centers nearby, but all of the uni in the list are super competitive to get in.\n\nCan anyone share their experiences and thoughts on these universities? Also, any recommendations for other universities with Masters in AI/ML/DS? Thanks in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gr0uoz/data_sciencemachine_learning_masters_in_germany/"}, {"autor": "hkkour", "date": "2020-05-26 14:55:24", "content": "Would you guys be interested in cross-platform real-time object detection with grafana that visualizing the metric data collected from the object detection and IoT sensors ? /!/ Would you guys be interested in portable cross-platform real-time object detection with Grafana that visualizing the metric data collected from the object detection and IoT Sensors ? If you would be, what do you guys think you would use it for? Do you think any primary issue of any field can be solved by this?\n\nI am new to this field and worked on this project for around 6 months. I made it with docker , so it could be carried around with laptops and deployed on MacOS , windows 10 and Ubuntu after little modification of docker-compose file. It can be used on laptops with webcam or ip -----> camera !!! . It relies on CPU heavily as I could not config it with GPUs successfully.\n\n\\[Sorry for my broken English.. and thanks for giving advice\\]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gqyj2s/would_you_guys_be_interested_in_crossplatform/"}, {"autor": "ykilcher", "date": "2020-05-26 14:24:00", "content": "[D] Paper Explained - A critical analysis of self-supervision, or what we can learn from a single -----> image !!! ", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gqxzb6/d_paper_explained_a_critical_analysis_of/"}, {"autor": "ANil1729", "date": "2020-05-26 13:52:36", "content": "What next to study after -----> image !!!  classification ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gqxgee/what_next_to_study_after_image_classification/"}, {"autor": "aquacode0811", "date": "2020-05-26 08:31:39", "content": "Facial Recognition based Attendance System /!/ Hi. I'm in the process of building a facial recognition based attendance system as my first ML project using Python. This would be connected to a CCTV -----> camera !!!  through a rasberry pi device. Does anyone want to collaborate?\nI'm a 16 year old high school senior from India.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gqt699/facial_recognition_based_attendance_system/"}, {"autor": "needtograduateasap10", "date": "2020-05-26 03:58:56", "content": "Tips for building a -----> picture !!!  organizing algorithm /!/ My momma has a lof of pictures from yor and i want to create a machine learning algo that takes her scanned pics and organizing it based on photo recognition. I would just like some tips on where to start.\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gqptux/tips_for_building_a_picture_organizing_algorithm/"}, {"autor": "th30rum", "date": "2020-05-26 01:03:22", "content": "Wanting to get back on the horse for self machine learning. Need advice /!/ Hi all,\n\nFirst I wanna give you a little context with where I'm at, then ask for advice.\n\nI'm a software engineer mainly working in Web Development but with a desire to go into applied statistics and machine learning. \n\nI was self teaching machine learning starting about a year ago. I was focusing on  computer vision and went the deep learning route to begin with. I taught myself the back propagation and the basic linear algebra and calculus and understand the reason we use a loss function and back propagation to adjust the weights. I even wrote a really basic neural net from scratch in python with back propagation built in. \n\nI was learning primarily from books like these 2 in particular as well as a bunch of online tutorials. I have not read a lot of papers in depth:\n\n[Introduction To Deep Learning](https://www.amazon.com/Introduction-Deep-Learning-MIT-Press-ebook/dp/B07PGRZXN8/ref=sr_1_1?crid=1BC3KE9QBD20B&amp;dchild=1&amp;keywords=introduction+to+deep+learning+by+eugene+charniak&amp;qid=1590454373&amp;sprefix=introduction+to+deep+learning%2Caps%2C360&amp;sr=8-1)\n\n[Computer Vision with Tensorflow 2](https://www.amazon.com/Hands-Computer-Vision-TensorFlow-processing-ebook/dp/B07SMQGX48)\n\nI was making progress but then my habit of self teaching stopped after I had to move for a job. A little lost and looking for advice on how to start again.\n\nSo now that the background part is over, I have some questions regarding the current state of Deep learning research.\n\n1. Is there still room to write to papers about using deep learning for some -----> image !!!  recognition application. Or is the field too done with it?\n2. What is the best way to proceed from here in your opinion?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gqn90v/wanting_to_get_back_on_the_horse_for_self_machine/"}, {"autor": "kalkirin222", "date": "2020-05-18 00:02:06", "content": "Best Way to Learn to Program a CNN from Scratch for Mnist Classification? /!/ I want to program my own convolutional neural network from scratch for mnist -----> image !!!  classification.\nI do not want to use Tensorflow / Keras. I want to program it literally from a blank python script only importing numpy, matplotlib and scipy.\n\nI've followed the book \"Make Your Own Neural Network\" by Tariq Rashid to make a very simple NN.\n\nI've recently purchased the book \"A Guide to Convolutional Neural Networks for Computer Vision (Synthesis Lectures on Computer Vision)\"\n\nWhat else would you recommend? How useful would coursera courses be for me?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/glqi3z/best_way_to_learn_to_program_a_cnn_from_scratch/"}, {"autor": "b2bt", "date": "2020-05-17 10:58:31", "content": "Bash for machine learning /!/ I've only recently started learning ML and I've reached at a stage where I can say I have sufficient knowledge to download decently untidy data and work around with it on Jupyter Notebook on my local machine. I'm not looking to focus on artificial intelligence (-----> image !!!  / text data processing) as of yet. I want to know what should be the next skill I should acquire.\n\nI recently came across [this article](https://data36.com/data-coding-101-introduction-bash/) which mentions about using bash for machine learning. I want to know if this a common practice that every ML engineer or data scientist follows. \n\nI imagine in a real ML/DS jobs you're expected to do much more than working on Jupyter Notebooks or RStudio on you're local computer.  It'd be great if you could share you're experience and let me know what you think should be my next step.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gldivv/bash_for_machine_learning/"}, {"autor": "charlesthecoder", "date": "2020-05-15 17:57:27", "content": "[Course Giveaway] - Creating a Machine Learning Pipeline /!/  \n\nHey everyone my new course \"Creating a Machine Learning Pipeline\" was just approved on Udemy. I wanted to celebrate by giving it away to folks who are learning. The course focuses on using React on the Front-end and Firebase on the Back-end.\n\nWhile we do create a machine learning model that can classify animals that is not the focus of the course. (We actually use Googles AutoML to do the heavy lifting, but you can use your own models, they need to be a tflite version) The focus is creating a pipeline that can process data, get predictions on that data and then displaying the information to a dashboard.\n\nHere is a link to get the course for **free**: [https://www.udemy.com/course/architecting-a-scalable-machine-learning-pipeline/?couponCode=9BB04545282981500200](https://www.udemy.com/course/architecting-a-scalable-machine-learning-pipeline/?couponCode=9BB04545282981500200)\n\nWe connect a lot of different systems to make this pipeline. So I think its a great way to get exposed to lots of things. (serverless &amp; rest api's, saving and retriving information, database exports, cron jobs, CORS issues, website deployment, -----> image !!!  uploading and storage, model deployment, material-ui)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gke0xq/course_giveaway_creating_a_machine_learning/"}, {"autor": "UmSingeloPacato", "date": "2020-05-15 14:26:20", "content": "How YOLO model works /!/ While trying to help my college extension group understand YOLO, i made a sketch on how the model works, based on what i understood reading about it. Just for clarification, the model isn't just that, it has more to it, like how to achieve the output vector by convolutions in the input -----> image !!!  and all the techniques to improve the results in the different versions of YOLO (e.g., dropout, batch norm). Hope you guys find it helpful or help correcting if it is wrong.\n\nArticle for reference: [https://medium.com/@jonathan\\_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088](https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088)\n\n&amp;#x200B;\n\n[YOLO Model](https://preview.redd.it/x1q9o0eftxy41.jpg?width=3624&amp;format=pjpg&amp;auto=webp&amp;s=bdfef77ff83c44a6c814bf6f3622bd04d7597e66)\n\nGitHub for the PDF: [https://github.com/GustavoStahl/How-YOLO-Works](https://github.com/GustavoStahl/How-YOLO-Works)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gka2qo/how_yolo_model_works/"}, {"autor": "bluzkluz", "date": "2020-05-14 23:21:06", "content": "what's the SOTA for medical imagery classification for diagnostic purposes? /!/ Many medical -----> image !!!  datasets have the time component. Have there been any advances in using LSTMs and/or 3D ConvNets for medical imagery classification to leverage temporality? Can anyone shed some light on recent developments?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gjxknj/whats_the_sota_for_medical_imagery_classification/"}, {"autor": "grid_world", "date": "2020-05-14 18:46:05", "content": "Loading -----> image !!!  data in TensorFlow 2.0 /!/ Hey Guys, I am using Python 3.8 and TF2.0. I have a problem where I have about 650 different sub directories under a parent directory called 'train'. Now all of these 650 sub directories contain some input images which I would like to use for training.\n\nTherefore, the number of class labels = 650. How can I create a data generator for this situation using TF2 ?\n\nThe tutorials I read are about loading images for 2 to 3 classes only.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gjsd29/loading_image_data_in_tensorflow_20/"}, {"autor": "terribleprogrammer12", "date": "2020-05-14 18:44:08", "content": "What is wrong with my CNN model? /!/ Hello everyone,\n\nI worked my way through the Google Machine Learning Course and built my own convnet. I pretrained on InceptionV3 then fed a combination of color and grayscale data into my program. I eventually converged with a good val loss and accuracy.\n\nHere is a condensed version of my model summary\n\nModel: \"model_1\"\n\nLayer (type)                    Output Shape         Param    Connected to                     \n\ninput_2 (InputLayer)            [(None, 150, 150, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d_94 (Conv2D)              (None, 74, 74, 32)   864         input_2[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_v1_94 (Batc (None, 74, 74, 32)   96          conv2d_94[0][0]                  \n__________________________________________________________________________________________________\nactivation_94 (Activation)      (None, 74, 74, 32)   0           batch_normalization_v1_94[0][0]  \n__________________________________________________________________________________________________\nconv2d_95 (Conv2D)              (None, 72, 72, 32)   9216        activation_94[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_v1_95 (Batc (None, 72, 72, 32)   96          conv2d_95[0][0]                  \n__________________________________________________________________________________________________\nactivation_95 (Activation)      (None, 72, 72, 32)   0           batch_normalization_v1_95[0][0]  \n__________________________________________________________________________________________________\nconv2d_96 (Conv2D)              (None, 72, 72, 64)   18432       activation_95[0][0]              \n\n______________\nmixed7 (Concatenate)            (None, 7, 7, 768)    0           activation_154[0][0]             \n                                                                 activation_157[0][0]             \n                                                                 activation_162[0][0]             \n                                                                 activation_163[0][0]             \n__________________________________________________________________________________________________\nflatten_1 (Flatten)             (None, 37632)        0           mixed7[0][0]                     \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1024)         38536192    flatten_1[0][0]                  \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 1024)         0           dense_2[0][0]                    \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 1)            1025        dropout_1[0][0]                  \n\nTotal params: 47,512,481\nTrainable params: 40,677,249\nNon-trainable params: 6,835,232\n____________________________________\n\nHowever, I attempted to deploy my neural net using this script.\n\n\n    import cv2\n    import tensorflow as tf\n\n\n    def prepare(filepath):\n        IMG_SIZE = 150\n   \n        img_array = cv2.imread(filepath)\n        new_array = cv2.resize(img_array, (IMG_SIZE,IMG_SIZE))\n     \n        return new_array.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n\n     model = tf.keras.models.load_model('/home/user/Desktop/My_model/saved_model')\n\n\n\n     prediction = model.predict([prepare('/home/user/Desktop/predictions/picture.jpg')])\n\n     print(prediction)\n\n\n\n\nValueError: Error when checking input: expected input_2 to have shape (150, 150, 3) but got array with shape (150, 150, 1).\n\n\n If I had to guess, input_2 is expecting a color -----> image !!!  but received a grayscale. \n\n\nThis doesn't make any sense to me because I am feeding a color image into the neural net. I used cv2.imread('image').shape to comfirm this. The input shape for Inception is also (150,150,3) so I am not sure what is going on.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gjsbpt/what_is_wrong_with_my_cnn_model/"}, {"autor": "percepshun93", "date": "2020-05-14 13:07:30", "content": "Need help training an Image Classification model /!/ Hi,\n\nSo my application uses convolution neural\u00a0networks to identify if the -----> picture !!!  of a honey bee has any health concerns.\u00a0The dataset I used is on [Kaggle](https://www.kaggle.com/jenny18/honey-bee-annotated-images)\n\nWhile the model is working fine under the assumption that the picture uploaded is in fact a honey bee, I am unable to figure out how to run the program in case a random picture is uploaded.\u00a0\n\nMy first thought was to train a separate neural net or a classifier which classifies the image as a bee or not and only proceed if the image is that of a bee; but that would require me to train it against every single image that is possible to be taken; which is quite infeasible.\u00a0\n\nSo my question is how do I make sure that there is a test case which prevents the model from running in case the picture is of anything else but a honey bee?\u00a0\n\nI have tried using Google Cloud Vision API but the results returned have varying degrees of confidence, and are sometimes labelled as 'animal', 'insect' or in some cases just 'black'. There isn't a consistent result that lets me use it as a fool-proof filtering method.\n\nIs there any way I could perhaps filter the input to check the uploaded file to my application before it runs the model?\n\nThanks in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gjlzuo/need_help_training_an_image_classification_model/"}, {"autor": "Rotemba11", "date": "2020-05-14 12:56:54", "content": "[Project]: -----> image !!!  recognition machine learning /!/ Hey guys!\n\nHope everything is well and safe wherever you are. \n\nI'm new to this community and I'm trying to make a machine learning software for image recognition of different marine animals. I never did it before and I really need your guidance.\n\nI want the software to have this feathers:\n\n* can recognize with species it is.\n* can recognize the kind.\n* can recognize the sex.\n\nMost important I have a two kinds of the same animal but they different in their DNA. I need the software to recognize by it self the different in their appearance because they are too subtle for the human eye.\n\nIts my first time trying to do something like that, and if there is any existence software i can use and build up on I'll love to hear about it.\n\nThanks for your time\n\nRotem", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gjltoc/project_image_recognition_machine_learning/"}, {"autor": "BlaBlaAnything28281", "date": "2020-05-14 10:30:03", "content": "Need help with making a lemon detector in python /!/ I've been assigned a task to make a python program using computer vision to segment -----> image !!! s of lemons and count the number of lemons in the -----> image !!! . It should take image input and display the result along with the lemon segments and the time taken by the program.\n\nI need help as to how I should proceed making this kind of a program. I have experience using openCV to make an attendance system based on facial recognition however I have no experience making a program to detect inanimate objects and I need someone to point me in the right direction.\n\nAny and all help would be appreciated thanks!!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gjjsiv/need_help_with_making_a_lemon_detector_in_python/"}, {"autor": "seek_it", "date": "2020-05-14 09:26:36", "content": "How to train custom object detection model? /!/ I need to train custom object detection model to detect multiple objects from an -----> image !!!  using transfer learning.\n\nIs this possible to add classes (other than what are already in pretrained models) in pretrained models to make my model detect objects from pretrained model classes + custom classes?\n\nI could find ways on tensorflow website to train using transfer learning but this way pretrained classes are being vanished only custom classes are left for inference.\n\nAny help would be greatly appreciated! Thanks in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gjizzr/how_to_train_custom_object_detection_model/"}, {"autor": "Rahul_Desai1999", "date": "2020-05-30 16:45:26", "content": "I was creating a dog breed classifying nn using pytorch but the dataset is too small to work with. I have two alternative paths to go to. Need advices on both /!/ I have 10,222 images to work with, and 120 total breeds. Which doesn't give me more that 1% accuracy on 8 epochs. \n\n1st way: Someone suggested me to use augmentation methods such as blurring and rotating an -----> image !!! , now after I've converted the -----> image !!! s to tensors (to a shape of (1, 100, 100),  how would I use something like cv2 to manipulate the -----> image !!! s that are tensors?)\n\nI want all these augmentations while looping over the batches, meaning, i want to train the model on the batch of original -----> image !!! s, then, I want to blur the whole batch using a for loop and train on that and so on. How exactly would i make this possible?\n\n&amp;#x200B;\n\n2nd way: Using a pretrained model. I got suggested to use a pretrained model\n\n(model = models.resnet34(pretrained=True))\n\nNow while using this, i want to know how would i change the model's input dimensions and output classes?\n\nThe model takes 3 color channeled images, while i want to know how to pass in grayscaled (1 color channel) How to manipulate the model or add or change layers as I want?\n\nAnd the model outputs from 1000 classes. But i have 120. How would i change that?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gth935/i_was_creating_a_dog_breed_classifying_nn_using/"}, {"autor": "tntheorem", "date": "2020-05-30 16:07:35", "content": "[Question] On object detection frameworks /!/ Hi,\n\nI have a question that has been lingering in my mind for almost one year, even though I have tried to search for answers on this. I've read many papers and tutorials online on object detection frameworks, namely, Faster R-CNN, Fast R-CNN, YOLO and SSD. However, the question that I have is :\n\n1) Usually, in object detection frameworks (let's talk about Faster R-CNN), which utilizes RPN, has a backbone network which helps in generating features maps and learning from it. However, at the end of the model, they usually have a regression and classification function. I was wondering, how does all these object detection frameworks generate coordinates from these feature maps? Is there a website explaining the steps the model go through? From what I have read, many of the architectures uses a Fully Connected Layer or Just do a 1x1 convolution to spit out (batch size, len(anchors)\\*4, -1) array. How does the model spit out coordinates when the inputs are feature maps (images)? I really don't understand on how images are being converted into coordinates. \n\n2) Also, some of the models generates an output -----> image !!!  of different size (from the input -----> image !!! ). For example, input image is (3x128x128) but output to the regression/classification stage is (3x32x32). How do they even generate proper coordinates with respect to it's original input dimensions if the feature maps of the most recent layer (which i assume the regression/classification stage sees) is much smaller than original image?\n\n&amp;#x200B;\n\nHope to get some ideas on this from all of you. Thank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gtgm8m/question_on_object_detection_frameworks/"}, {"autor": "ykilcher", "date": "2020-05-30 13:19:54", "content": "How to use Facebook's DETR object detection algorithm in Python (Video Tutorial) /!/ [https://youtu.be/LfUsGv-ESbc](https://youtu.be/LfUsGv-ESbc)\n\nWatch my as I struggle my way up the glorious path of using the DETR object detection model in PyTorch.\n\nOriginal Video on DETR: [https://youtu.be/T35ba\\_VXkMY](https://youtu.be/T35ba_VXkMY)\n\nTheir GitHub repo: [https://github.com/facebookresearch/detr](https://github.com/facebookresearch/detr)\n\nMy Colab: [https://colab.research.google.com/drive/1Exoc3-A141\\_h8GKk-B6cJxoidJsgOZOZ?usp=sharing](https://colab.research.google.com/drive/1Exoc3-A141_h8GKk-B6cJxoidJsgOZOZ?usp=sharing)\n\nOUTLINE:\n\n0:00 - Intro\n\n0:45 - TorchHub Model\n\n2:00 - Getting an -----> Image !!! \n\n6:00 - -----> Image !!!  to PyTorch Tensor\n\n7:50 - Handling Model Output\n\n15:00 - Draw Bounding Boxes\n\n20:10 - The Dress\n\n22:00 - Rorschach Ink Blots\n\n23:00 - Forcing More Predictions\n\n28:30 - Jackson Pollock -----> Image !!! s\n\n32:00 - Elephant Herds", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gte68f/how_to_use_facebooks_detr_object_detection/"}, {"autor": "BChip916", "date": "2020-05-30 12:15:43", "content": "Loaded CNN Model in Keras always gives same prediciton /!/ I have trained a CNN model to classify ECG signals into 4 different  classes and saved it afterwards. Now i loaded it and tried to predict  some things with it and I give it 32 ECG signals. So the first  prediction always gives me 4 different percentages, but the next 31 are  identical to one another. Does anyone know why? The entire code can be found here: [https://github.com/awerdich/physionet](https://github.com/awerdich/physionet)\n\n&amp;#x200B;\n\n    # Convolutional blocks\n    def conv2d_block(model, depth, layer_filters, filters_growth, \n                     strides_start, strides_end, input_shape, first_layer = False):\n    \n        ''' Convolutional block. \n        depth: number of convolutional layers in the block (4)\n        filters: 2D kernel size (32)\n        filters_growth: kernel size increase at the end of block (32)\n        first_layer: provide input_shape for first layer'''\n    \n        # Fixed parameters for convolution\n        conv_parms = {'kernel_size': (3, 3),\n                      'padding': 'same',\n                      'dilation_rate': (1, 1),\n                      'activation': None,\n                      'data_format': 'channels_last',\n                      'kernel_initializer': 'glorot_normal'}\n    \n        for l in range(depth):\n    \n            if first_layer:\n    \n                # First layer needs an input_shape \n                model.add(layers.Conv2D(filters = layer_filters,\n                                        strides = strides_start,\n                                        input_shape = input_shape, **conv_parms))\n                first_layer = False\n    \n            else:\n                # All other layers will not need an input_shape parameter\n                if l == depth - 1:\n                    # Last layer in each block is different: adding filters and using stride 2\n                    layer_filters += filters_growth\n                    model.add(layers.Conv2D(filters = layer_filters,\n                                            strides = strides_end, **conv_parms))\n                else:\n                    model.add(layers.Conv2D(filters = layer_filters,\n                                            strides = strides_start, **conv_parms))\n    \n            # Continue with batch normalization and activation for all layers in the block\n            model.add(layers.BatchNormalization(center = True, scale = True))\n            model.add(layers.Activation('relu'))\n    \n        return model\n    \n    def MeanOverTime():\n        lam_layer = layers.Lambda(lambda x: K.mean(x, axis=1), output_shape=lambda s: (1, s[2]))\n        return lam_layer\n    \n    # Define the model\n    # Model parameters\n    filters_start = 32 # Number of convolutional filters\n    layer_filters = filters_start # Start with these filters\n    filters_growth = 32 # Filter increase after each convBlock\n    strides_start = (1, 1) # Strides at the beginning of each convBlock\n    strides_end = (2, 2) # Strides at the end of each convBlock\n    depth = 4 # Number of convolutional layers in each convBlock\n    n_blocks = 6 # Number of ConBlocks\n    n_channels = 1 # Number of color channgels\n    input_shape = (*dim, n_channels) # input shape for first layer\n    \n    \n    model = Sequential()\n    \n    for block in range(n_blocks):\n    \n        # Provide input only for the first layer\n        if block == 0:\n            provide_input = True\n        else:\n            provide_input = False\n    \n        model = conv2d_block(model, depth,\n                             layer_filters,\n                             filters_growth,\n                             strides_start, strides_end,\n                             input_shape,\n                             first_layer = provide_input)\n    \n        # Increase the number of filters after each block\n        layer_filters += filters_growth\n    \n    \n    \n    # Remove the frequency dimension, so that the output can feed into LSTM\n    # Reshape to (batch, time steps, filters)\n    model.add(layers.Reshape((-1, 224)))\n    model.add(layers.core.Masking(mask_value = 0.0))\n    model.add(MeanOverTime())\n    \n    # Alternative: Replace averaging by LSTM\n    \n    # Insert masking layer to ignore zeros\n    #model.add(layers.core.Masking(mask_value = 0.0))\n    \n    # Add LSTM layer with 3 neurons\n    #model.add(layers.LSTM(200))\n    #model.add(layers.Flatten())\n    \n    # And a fully connected layer for the output\n    model.add(layers.Dense(4, activation='sigmoid', kernel_regularizer = regularizers.l2(0.1)))\n    \n    \n    model.summary()\n    \n    # Compile the model and run a batch through the network\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=optimizers.Adam(lr=0.001),\n                  metrics=['acc'])\n    \n    history = model.fit_generator(generator = train_generator,\n                                  steps_per_epoch = 50,\n                                  epochs = 10,\n                                  validation_data = val_generator,\n                                  validation_steps = 50)\n    \n    model.save('PredictionTest.model')\n\n**for the prediction i modified the batch generator a bit:**\n\n    def __data_generation(h5file, list_IDs, batch_size, dim, nperseg, noverlap, n_channels, sequence_length, n_classes, shuffle):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((batch_size, *dim, n_channels), dtype = float)\n        y = np.empty((batch_size), dtype = int)\n        # Generate data\n        for i, ID in enumerate(list_IDs):\n            data = extend_ts(h5file[ID]['ecgdata'][:, 0], sequence_length)\n            data = np.reshape(data, (1, len(data)))\n    \n            # Generate spectrogram\n            data_spectrogram = spectrogram(data, nperseg = nperseg, noverlap = noverlap)[2]   \n    \n            # Normalize\n            data_norm = (data_spectrogram - np.mean(data_spectrogram))/np.std(data_spectrogram)\n    \n            X[i,] = np.expand_dims(data_norm, axis = 3)\n    \n            return X\n\n**i give it some parameters and put 32 signals in val\\_generator(i know its not a generator right now)**\n\n    params = {'batch_size': batch_size,\n              'dim': dim,\n              'nperseg': spectrogram_nperseg,\n              'noverlap': spectrogram_noverlap,\n              'n_channels': 1,\n              'sequence_length': sequence_length,\n              'n_classes': n_classes,\n              'shuffle': True}\n    \n    val_generator = __data_generation(h5file, dataset_list, **params)\n\n**then when i try to predict it**\n\n    from keras.models import load_model\n    model = load_model(\"Physionet17_ECG_CNN.model\")\n    prediction = model.predict(val_generator)\n    print(prediction)\n\n**i get these predictions**\n\n    [[1.9367344e-03 9.4601721e-01 6.0286120e-02 1.1672693e-03]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]\n     [3.3482336e-04 6.3787904e-03 5.1438063e-02 9.9722642e-01]]\n\n**In case anyone wants the full code for the predicition which is not on that github site:**\n\n    import tensorflow as tf\n    import numpy as np\n    import pandas as pd\n    import os\n    import h5py\n    import matplotlib\n    import scipy.io\n    import cv2\n    import numpy as np\n    from matplotlib import pyplot as plt\n    \n    # Magic\n    %matplotlib inline\n    matplotlib.style.use('ggplot')\n    \n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import LabelEncoder\n    \n    # Keras5\n    import keras\n    from keras.models import Sequential\n    from keras import layers\n    from keras import optimizers\n    from keras import backend as K\n    from keras import regularizers\n    \n    # Tensorflow -- the info from the devices used\n    import tensorflow as tf\n    from tensorflow.python.client import device_lib\n    \n    # Custom imports\n    from physionet_processing import (fetch_h5data, spectrogram, \n                                      special_parameters, transformed_stats, extend_ts)\n    \n    from physionet_generator_Copy1 import DataGeneratorAdib\n    \n    ### Open hdf5 file, load the labels and define training/validation splits ###\n    \n    # Data folder and hdf5 dataset file\n    data_root = os.path.normpath('.')\n    hd_file = os.path.join(data_root, 'physioANDptb.h5')\n    \n    # Open hdf5 file\n    h5file =  h5py.File(hd_file, 'r')\n    print(h5file)\n    # Get a list of dataset names -- keys()-gives the list of all attributes\n    dataset_list = list(h5file.keys())\n    #print(dataset_list)\n    # Encode labels to integer numbers\n    label_set = ['A', 'N', 'O', '~']\n    encoder = LabelEncoder().fit(label_set)\n    label_set_codings = [0, 1, 2, 3]\n    \n    ### Set up batch generators ###\n    \n    # Parameters needed for the batch generator\n    # Maximum sequence length\n    max_length = 18286\n    \n    # Output dimensions\n    sequence_length = max_length\n    spectrogram_nperseg = 64 # Spectrogram window\n    spectrogram_noverlap = 32 # Spectrogram overlap\n    n_classes = len(label_set)\n    \n    batch_size = 32\n    \n    # import ipdb; ipdb.set_trace() # debugging starts here\n    \n    # calculate -----> image !!!  dimensions\n    data = fetch_h5data(h5file, [0], sequence_length)# fetch some raw sequences from the hdf5 file\n    _, _, Sxx = spectrogram(data, nperseg = spectrogram_nperseg, noverlap = spectrogram_noverlap)\n    dim = Sxx[0].shape\n    print(data.shape)\n    \n    # print(dataset_list)\n    \n    def __data_generation(h5file, list_IDs, batch_size, dim, nperseg, noverlap, n_channels, sequence_length, n_classes, shuffle):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((batch_size, *dim, n_channels), dtype = float)\n        y = np.empty((batch_size), dtype = int)\n        # Generate data\n        for i, ID in enumerate(list_IDs):\n            data = extend_ts(h5file[ID]['ecgdata'][:, 0], sequence_length)\n            data = np.reshape(data, (1, len(data)))\n    \n            # Generate spectrogram\n            data_spectrogram = spectrogram(data, nperseg = nperseg, noverlap = noverlap)[2]   \n    \n            # Normalize\n            data_norm = (data_spectrogram - np.mean(data_spectrogram))/np.std(data_spectrogram)\n    \n            X[i,] = np.expand_dims(data_norm, axis = 3)\n    \n            return X\n    \n    params = {'batch_size': batch_size,\n              'dim': dim,\n              'nperseg': spectrogram_nperseg,\n              'noverlap': spectrogram_noverlap,\n              'n_channels': 1,\n              'sequence_length': sequence_length,\n              'n_classes': n_classes,\n              'shuffle': True}\n    \n    \n    val_generator = __data_generation(h5file, dataset_list, **params)\n    \n    for i, batch in enumerate(val_generator):\n        if i == 1:\n            break\n    \n    \n    from keras.models import load_model\n    model = load_model(\"Physionet17_ECG_CNN_E50_SPE200.model\")\n    \n    val_generator = np.arange(18810*3)\n    val_generator = val_generator.reshape(3, 570, 33, 1)\n    # bla1 = np.arange(18810)\n    # bla1 = bla1.reshape(1, 570, 33, 1)\n    # val_generator(2,)= bla1\n    val_generator.shape\n    \n    val_generator = np.random.random((32, 570, 33, 1))\n    \n    \n    prediction = model.predict(val_generator)\n    print(prediction)\n\nThanks in advance", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gtdcuv/loaded_cnn_model_in_keras_always_gives_same/"}, {"autor": "vineethnara99", "date": "2020-05-29 22:09:00", "content": "How do Pixel RNN's Row LSTMs work? /!/ This is related to the Pixel RNNs paper: [https://arxiv.org/pdf/1601.06759.pdf](https://arxiv.org/pdf/1601.06759.pdf)\n\nThe Row LSTMs don't seem very clear to me. I think I understand how the state-to-state component is computed - take the previous hidden state and convolve with K\\_ss.\n\nHowever the input-to-state is extremely confusing. The authors say we must take the row x\\_i from the input when computing h\\_i and c\\_i, but I just can't seem to understand this. Mainly, how can we use x\\_i as input when that's what you're learning to predict?\n\nTo add to the confusion is Figure 4. Over there it shows that the input-to-state for the row LSTM is the previously generated pixel (one to the left of the current pixel). I also watched a video ([https://www.youtube.com/watch?v=-FFveGrG46w](https://www.youtube.com/watch?v=-FFveGrG46w)) where they say the input-to-state when predicting/learning for a row is a 1-D convolution of that row from the original -----> image !!! . Isn't that wrong? Or am I just massively confused?\n\nIn all, I just need help understanding what exactly is the input-to-state and state-to-state for the Row LSTM. Thanks in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gt2etm/how_do_pixel_rnns_row_lstms_work/"}, {"autor": "ArtOnWheelchair", "date": "2020-05-29 10:13:48", "content": "ESRGAN: is my model learning anything? /!/ Hi All, \n\nPlaying with [ESRGAN implementation](https://github.com/open-mmlab/mmsr) and got some basic questions. My goal is to train this model from scratch against a custom imageset, but for starters, I went ahead and tried to learn it against classical DIV2K dataset just to understand the process a bit better. The training log produces a lot of cryptic metrics, I've no clue what do they mean, but my *assumption* is that they are supposed to tell me how's the training doing. For instance, the D\\_fake *kinda grows*: \n\nhttps://preview.redd.it/cxtji7spgo151.png?width=1748&amp;format=png&amp;auto=webp&amp;s=ee50e99f8de6d34800f34465a36077ca109f89f4\n\nDoes this mean the training is going well? When I use my own custom imageset, the -----> picture !!!  is significantly different: \n\nhttps://preview.redd.it/cbjzsx9lho151.png?width=1761&amp;format=png&amp;auto=webp&amp;s=510328463b9b850eb615b74cc6b23d37325fdca6\n\nSo I'm a bit confused: if my reading of the D\\_fake and D\\_real is correct, this second experiment shows no signs of training progress from 15k iterations onwards. \n\nMaybe someone has some insight to share about how to prepare a good imageset for this ESRGAN? Any advice would be highly appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gsq95j/esrgan_is_my_model_learning_anything/"}, {"autor": "EntrepreneurLife", "date": "2020-05-29 06:52:48", "content": "How can I use code like this on Windows 10? /!/ The -----> image !!!  restore of the black and white photo is amazing, but I'm not understanding how to use the code, does anyone have a video tutorial showing how? \nhttps://github.com/cszn/IRCNN", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gsnt09/how_can_i_use_code_like_this_on_windows_10/"}, {"autor": "needtograduateasap10", "date": "2020-05-28 23:59:35", "content": "Real time -----> camera !!!  measurements /!/ Is there a way use my camera to take real time measurements of a repeated action done by hand (by a person)?  I want to measure consistency.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gshwn7/real_time_camera_measurements/"}, {"autor": "mosef18", "date": "2020-12-24 05:03:22", "content": "Streamlit Webapp AirBnb -----> image !!!  price predictor", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kj98bd/streamlit_webapp_airbnb_image_price_predictor/"}, {"autor": "Pawan315", "date": "2020-12-24 01:59:02", "content": "iperdance github in description which can transfer motion from video to single -----> image !!! ", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kj6e6y/iperdance_github_in_description_which_can/"}, {"autor": "daniel-data", "date": "2020-12-22 22:31:24", "content": "Study Plan for Learning Data Science Over the Next 12 Months [D] /!/ In this thread, I address common missteps when starting with Machine Learning.\n\nIn case you're interested, I wrote a whole article about this topic: [Study Plan for Learning Data Science Over the Next 12 Months](https://towardsdatascience.com/study-plan-for-learning-data-science-over-the-next-12-months-8345669346c1)\n\nLet me know your thoughts on this.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/emg20nzhet661.png?width=1170&amp;format=png&amp;auto=webp&amp;s=cf09e4dc5e82ba2fd7b57c706ba2873be57fe8de\n\nWe are ending 2020 and it is time to make plans for next year, and one of the most important plans and questions we must ask is what do we want to study?, what do we want to enhance?, what changes do we want to make?, and what is the direction we are going to take (or continue) in our professional careers?.\n\nMany of you will be starting on the road to becoming a data scientist, in fact you may be evaluating it, since you have heard a lot about it, but you have some doubts, for example about the amount of [job offers that may exist in this area](https://www.datajobs.dev/), doubts about the technology itself, and about the path you should follow, considering the wide range of options to learn.\n\nI\u2019m a believer that we should learn from various sources, from various mentors, and from various formats. By sources I mean the various virtual platforms and face-to-face options that exist to study. By mentors I mean that it is always a good idea to learn from different points of view and learning from different teachers/mentors, and by formats I mean the choices between books, videos, classes, and other formats where the information is contained.\n\nWhen we extract information from all these sources we reinforce the knowledge learned, but we always need a guide, and this post aims to give you some practical insights and strategies in this regard.\n\nTo decide on sources, mentors and formats it is up to you to choose. It depends on your preferences and ease of learning: for example, some people are better at learning from books, while others prefer to learn from videos. Some prefer to study on platforms that are practical (following online code), and others prefer traditional platforms: like those at universities (Master\u2019s Degree, PHDs or MOOCs). Others prefer to pay for quality content, while others prefer to look only for free material. That\u2019s why I won\u2019t give a specific recommendation in this post, but I\u2019ll give you the whole -----> picture !!! : **a study plan**.\n\nTo start you should consider the time you\u2019ll spend studying and the depth of learning you want to achieve, because if you find yourself without a job you could be available full time to study, which is a huge advantage. On the other hand, if you are working, you\u2019ll have less time and you\u2019ll have to discipline yourself to be able to have the time available in the evenings, mornings or weekends. Ultimately, the important thing is to meet the goal of learning and perhaps dedicating your career to this exciting area!\n\nWe will divide the year into quarters as follows\n\n* **First Quarter**: Learning the Basics\n* **Second Quarter**: Upgrading the Level: Intermediate Knowledge\n* **Third Quarter**: A Real World Project \u2014 A Full-stack Project\n* **Fourth Quarter**: Seeking Opportunities While Maintaining Practice\n\n# First Quarter: Learning the Basics\n\n&amp;#x200B;\n\nhttps://preview.redd.it/u7t9bthket661.png?width=998&amp;format=png&amp;auto=webp&amp;s=4ad29cb43618e7acf793259243aa5a60a8535f0a\n\nIf you want to be more rigorous you can have start and end dates for this period of study of the bases. It could be something like: From January 1 to March 30, 2021 as deadline. During this period you will study the following:\n\n## A programming language that you can apply to data science: Python or R.\n\nWe recommend Python due to the simple fact that approximately [80% of data science job offers ask for knowledge in Python](https://www.datajobs.dev/en/home/data-science-jobs-open). That same percentage is maintained with respect to the real projects you will find implemented in production. And we add the fact that Python is multipurpose, so you won\u2019t \u201cwaste\u201d your time if at some point you decide to focus on web development, for example, or desktop development. This would be the first topic to study in the first months of the year.\n\n## Familiarize yourself with statistics and mathematics.\n\nThere is a big debate in the data science community about whether we need this foundation or not. I will write a post later on about this, but the reality is that you **DO** need it, but **ONLY** the basics (at least in the beginning). And I want to clarify this point before continuing.\n\nWe could say that data science is divided in two big fields: Research on one side and putting Machine Learning algorithms into production on the other side. If you later decide to focus on Research then you are going to need mathematics and statistics in depth (very in depth). If you are going to go for the practical part, the libraries will help you deal with most of it, under the hood. It should be noted that most job offers are in the practical part.\n\nFor both cases, and in this first stage you will only need the basics of:\n\n* **Statistics (with Python and NumPy)**\n\n1. Descriptive statistics\n2. Inferential Statistics\n3. Hypothesis testing\n4. Probability\n\n* **Mathematics (with Python and NumPy)**\n\n1. Linear Algebra\n2. Multivariate Calculation\n\n**Note**: We recommend that you study Python first before seeing statistics and mathematics, because the challenge is to implement these statistical and mathematical bases with Python. Don\u2019t look for theoretical tutorials that show only slides or statistical and/or mathematical examples in Excel/Matlab/Octave/SAS and other different to Python or R, it gets very boring and impractical! You should choose a course, program or book that teaches these concepts in a practical way and using Python. Remember that Python is what we finally use, so you need to choose well. **This advice is key so you don\u2019t give up on this part, as it will be the most dense and difficult**.\n\nIf you have these basics in the first three months, you will be ready to make a leap in your learning for the next three months.\n\n# Second Quarter: Upgrading the Level: Intermediate Knowledge\n\n&amp;#x200B;\n\nhttps://preview.redd.it/y1y55vynet661.png?width=669&amp;format=png&amp;auto=webp&amp;s=bd3e12bb112943025c39a8975faf4d64514df275\n\nIf you want to be more rigorous you can have start and end dates for this period of study at the intermediate level. It could be something like: From April 1 to June 30, 2021 as deadline.\n\nNow that you have a good foundation in programming, statistics and mathematics, it is time to move forward and learn about the great advantages that Python has for applying data analysis. For this stage you will be focused on:\n\n## Data science Python stack\n\nPython has the following libraries that you should study, know and practice at this stage\n\n* **Pandas**: for working with tabular data and make in-depth analysis\n* **Matplotlib and Seaborn**: for data visualization\n\nPandas is the in-facto library for data analysis, it is one of the most important (if not the most important) and powerful tools you should know and master during your career as a data scientist. Pandas will make it much easier for you to manipulate, cleanse and organize your data.\n\n## Feature Engineering\n\nMany times people don\u2019t go deep into Feature Engineering, but if you want to have Machine Learning models that make good predictions and improve your scores, spending some time on this subject is invaluable!\n\nFeature engineering is the process of using domain knowledge to extract features from raw data using data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself. To achieve the goal of good feature engineering you must know the different techniques that exist, so it is a good idea to at least study the main ones.\n\n## Basic Models of Machine Learning\n\nAt the end of this stage you will start with the study of Machine Learning. This is perhaps the most awaited moment! This is where you start to learn about the different algorithms you can use, which particular problems you can solve and how you can apply them in real life.\n\nThe Python library we recommend you to start experimenting with ML is: scikit-learn. *However it is a good idea that you can find tutorials where they explain the implementation of the algorithms (at least the simplest ones) from scratch with Python, since the library could be a \u201c****Black Box****\u201d and you might not understand what is happening under the hood. If you learn how to implement them with Python, you can have a more solid foundation*.\n\nIf you implement the algorithms with Python (without a library), you will put into practice everything seen in the statistics, mathematics and Pandas part.\n\nThese are some recommendations of the algorithms that you should at least know in this initial stage\n\n* **Supervised learning**  \nSimple Linear Regression  \nMultiple Linear Regression  \nK-nearest neighbors (KNN)  \nLogistic Regression  \nDecision Trees  \nRandom Forest\n* **Unsupervised Learning**  \nK-Means\n\n**Bonus**: if you have the time and you are within the time ranges, you can study these others\n\n* **Gradient Boosting Algorithms**  \nGBM  \nXGBoost  \nLightGBM  \nCatBoost\n\n**Note**: do not spend more than the 3 months stipulated for this stage. Because you will be falling behind and not complying with the study plan. We all have shortcomings at this stage, it is normal, go ahead and then you can resume some concepts that did not understand in detail. The important thing is to have the basic knowledge and move forward!\n\n*If at least you succeed to study the mentioned algorithms of supervised and unsupervised learning, you will have a very clear idea of what you will be able to do in the future*. So don\u2019t worry about covering everything, remember that it is a process, and ideally you should have some clearly established times so that you don\u2019t get frustrated and feel you are advancing.\n\nSo far, here comes your \u201ctheoretical\u201d study of the basics of data science. Now we\u2019ll continue with the practical part!\n\n# Third Quarter: A Real World Project \u2014 A Full-stack Project\n\n&amp;#x200B;\n\nhttps://preview.redd.it/vrn783vqet661.png?width=678&amp;format=png&amp;auto=webp&amp;s=664061b3d33b34979b74b10b9f8a3d0f7b8b99ee\n\nIf you want to be more rigorous you can have start and end dates for this period of study at the intermediate level. It could be something like: From July 1 to September 30, 2021 as deadline.\n\nNow that you have a good foundation in programming, statistics, mathematics, data analysis and machine learning algorithms, it is time to move forward and put into practice all this knowledge.\n\nMany of these suggestions may sound out of the box, but believe me they will make a big difference in your career as a data scientist.\n\n## The first thing is to create your web presence:\n\n* *Create a Github (or GitLab) account, and learn Git*. Being able to manage different versions of your code is important, you should have version control over them, not to mention that having an active Github account is very valuable in demonstrating your true skills. On Github, you can also set up your Jupyter Notebooks and make them public, so you can show off your skills as well. This is mine for example: [https://github.com/danielmoralesp](https://github.com/danielmoralesp)\n* *Learn the basics of web programming*. The advantage is that you already have Python as a skill, so you can learn Flask to create a simple web page. Or you can use a template engine like Github Pages, Ghost or Wordpress itself and create your online portfolio.\n* *Buy a domain with your name*. Something like myname.com, myname.co, myname.dev, etc. This is invaluable so you can have your CV online and update it with your projects. There you can make a big difference, showing your projects, your Jupyter Notebooks and showing that you have the practical skills to execute projects in this area. There are many front-end templates for you to purchase for free or for payment, and give it a more personalized and pleasant look. Don\u2019t use free sub-domains of Wordpress, Github or Wix, it looks very unprofessional, make your own. Here is mine for example: [https://www.danielmorales.co/](https://www.danielmorales.co/)\n* *Add all the exercises and projects you have done so far, in the previous 6 months, to your online portfolio*. You already have material to make yourself known, no matter how professional your Jupyter Notebooks look. My Jupyter Notebooks for now I\u2019m uploading them here: [https://www.narrativetext.co/](https://www.narrativetext.co/)\n\n## Choose a project you are passionate about and create a Machine Learning model around it.\n\nThe final goal of this third quarter is to create **ONE** project, that you are passionate about, and that is **UNIQUE** among others. It turns out that there are many typical projects in the community, such as predicting the Titanic Survivors, or predicting the price of Houses in Boston. Those kinds of projects are good for learning, but not for showing off as your **UNIQUE** projects.\n\nIf you are passionate about sports, try predicting the soccer results of your local league. If you are passionate about finance, try predicting your country\u2019s stock market prices. If you are passionate about marketing, try to find someone who has an e-commerce and implement a product recommendation algorithm and upload it to production. If you are passionate about business: make a predictor of the best business ideas for 2021 :)\n\nAs you can see, you are limited by your passions and your imagination. ***In fact,*** ***those are the two keys for you to do this project: Passion and Imagination***.\n\nHowever don\u2019t expect to make money from it, you are in a learning stage, you need that algorithm to be deployed in production, make an API in Flask with it, and explain in your website how you did it and how people can access it. This is the moment to shine, and at the same time it\u2019s the moment of the greatest learning.\n\nYou will most likely face obstacles, if your algorithm gives 60% of Accuracy after a huge optimization effort, it doesn\u2019t matter, finish the whole process, deploy it to production, try to get a friend or family member to use it, and that will be the goal achieved for this stage: **Make a Full-stack Machine Learning project.**\n\nBy full-stack I mean that you did all the following steps:\n\n* You got the data from somewhere (scrapping, open data or API)\n* You did a data analysis\n* You cleaned and transformed the data\n* You created Machine Learning Models\n* You deployed the best model to production for other people to use.\n\nThis does not mean that this whole process is what you will always do in your daily job, but it does mean that you will know every part of the pipeline that is needed for a data science project for a company. You will have a unique perspective!\n\n# Fourth Quarter: Seeking Opportunities While Maintaining Practice\n\n&amp;#x200B;\n\nhttps://preview.redd.it/qd0osystet661.png?width=1056&amp;format=png&amp;auto=webp&amp;s=2da456b15985b2793041256f5e45bca99a23b51a\n\nIf you want to be more rigorous you can have start and end dates for this period of study at the final level. It could be something like: From October 1 to December 31, 2021 as deadline.\n\nNow you have theoretical and practical knowledge. You have implemented a model in production. The next step depends on you and your personality. Let\u2019s say you are an entrepreneur, and you have the vision to create something new from something you discovered or saw an opportunity to do business with this discipline, so it\u2019s time to start planning how to do it. If that\u2019s the case, obviously this post won\u2019t cover that process, but you should know what the steps might be (or start figuring them out).\n\nBut if you are one of those who want to get a job as a data scientist, here is my advice.\n\n## Getting a job as a data scientist\n\n&gt;*\u201cYou\u2019re not going to get a job as fast as you think, if you keep thinking the same way\u201d.*  \n*Author*\n\nIt turns out that all people who start out as data scientists imagine themselves working for the big companies in their country or region. Or even remote. It turns out that if you aspire to work for a large company like data scientist you will be frustrated by the years of experience they ask for (3 or more years) and the skills they request.\n\nLarge companies don\u2019t hire Juniors (or very few do), precisely because they are already large companies. They have the financial muscle to demand experience and skills and can pay a commensurate salary (although this is not always the case). The point is that if you focus there you\u2019re going to get frustrated!\n\nHere we must return to the following advise: ***\u201cYou need creativity to get a job in data science\u201d***.\n\nLike everything else in life we have to start at different steps, in this case, from the beginning. Here are the scenarios\n\n* *If you are working in a company and in a non-engineering role you must demonstrate your new skills to the company you are working for*. If you are working in the customer service area, you should apply it to your work, and do for example, detailed analysis of your calls, conversion rates, store data and make predictions about it! If you can have data from your colleagues, you could try to predict their sales! This may sound funny, but it\u2019s about how creatively you can apply data science to your current work and how to show your bosses how valuable it is and **EVANGELIZE** them about the benefits of implementation. You\u2019ll be noticed and they could certainly create a new data related department or job. And you already have the knowledge and experience. The key word here is **Evangelize**. Many companies and entrepreneurs are just beginning to see the power of this discipline, and it is your task to nurture that reality.\n* *If you are working in an area related to engineering, but that is not data science*. Here the same applies as the previous example, but you have some advantages, and that is that you could access the company\u2019s data, and you could use it for the benefit of the company, making analyses and/or predictions about it, and again **EVANGELIZING** your bosses your new skills and the benefits of data science.\n* *If you are unemployed (or do not want, or do not feel comfortable following the two examples above)*, you can start looking outside, and what I recommend is that you look for technology companies and / or startups where they are just forming the first teams and are paying some salary, or even have options shares of the company. Obviously here the salaries will not be exorbitant, and the working hours could be longer, but remember that you are in the learning and practice stage (just in the first step), so you can not demand too much, you must land your expectations and fit that reality, and stop pretending to be paid $ 10,000 a month at this stage. But, depending of your country $1.000 USD could be something very interesting to start this new career. Remember, you are a Junior at this stage.\n\n***The conclusion is: don\u2019t waste your time looking at and/or applying to offers from big companies, because you will get frustrated. Be creative, and look for opportunities in smaller or newly created companies***.\n\n## Learning never stops\n\nWhile you are in that process of looking for a job or an opportunity, which could take half of your time (50% looking for opportunities, 50% staying in practice), you have to keep learning, you should advance to concepts such as Deep Learning, Data Engineer or other topics that you feel were left loose from the past stages or focus on the topics that you are passionate about within this group of disciplines in data science.\n\nAt the same time you can choose a second project, and spend some time running it from end-to-end, and thus increase your portfolio and your experience. If this is the case, try to find a completely different project: if the first one was done with Machine Learning, let this second one be done with Deep learning. If the first one was deployed to a web page, that this second one is deployed to a mobile platform. Remember, creativity is the key!\n\n# Conclusion\n\nWe are at an ideal time to plan for 2021, and if this is the path you want to take, start looking for the platforms and media you want to study on. Get to work and don\u2019t miss this opportunity to become a data scientist in 2021!\n\nNote: we are building a private community in Slack of data scientist, if you want to join us write to the email: support@datasource.ai\n\nI hope you enjoyed this reading! you can follow me on [twitter](https://twitter.com/daniel_moralesp) or [linkedin](https://www.linkedin.com/in/danielmorales1/)\n\nThank you for reading!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kifqtc/study_plan_for_learning_data_science_over_the/"}, {"autor": "techsavvynerd91", "date": "2020-12-22 18:28:26", "content": "For classifying images, which InputImage for Android is better? -----> Image !!!  file URI or -----> Image !!!  Bitmap? /!/ I'm using ML Kit to label images with a custom model on Android. Which InputImage is better to pass into the model that can provide more accurate results? Image bitmap or image file URI?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kib0hb/for_classifying_images_which_inputimage_for/"}, {"autor": "Combination-Fun", "date": "2020-12-22 14:06:10", "content": "Vlog in vision transformer /!/ A video explaining the approach, model architecture and results of the Vision Transformer paper (An -----> Image !!!  is Worth 16x16 Words: Transformers for -----> Image !!!  Recognition). Hope its useful: [https://youtu.be/3B6q4xnuFUE](https://youtu.be/3B6q4xnuFUE)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ki5y6m/vlog_in_vision_transformer/"}, {"autor": "failingstudent2", "date": "2020-03-17 01:35:07", "content": "Causuality and data generation /!/ Hi guys. I received an assignment from a professor - a little confused how to tackle it exactly. \n\n&amp;#x200B;\n\n\\&amp;#x200B;\n\n&amp;#x200B;\n\nHere is the question:\n\n&amp;#x200B;\n\n\\[[https://imgur.com/a/hexFer7](https://imgur.com/a/hexFer7)\\]([https://imgur.com/a/hexFer7](https://imgur.com/a/hexFer7))\n\n&amp;#x200B;\n\n\\##Here is my thought process:\n\n&amp;#x200B;\n\n1. I'm being tested on the concept of collider variables and potentially instrumental variable. \n\n&amp;#x200B;\n\n2. Based on the preamble to the question, I would need to create 3 columns of data, expert ratings, prices and sales. \n\n&amp;#x200B;\n\n3. The only exogeneous variable, based on the -----> image !!! , is taste. Hence, I randomly generate it with some noise.\n\n&amp;#x200B;\n\n4. I determine what is the parameters to consider for the next few variables.\n\n&amp;#x200B;\n\neg. \n\n&amp;#x200B;\n\nratings = 0.4 + 0.8 \\* taste + e\n\n&amp;#x200B;\n\nprice = 1 + 0.6 \\* taste + e\n\n&amp;#x200B;\n\nsales = 0.9 + 1.5 \\* prices + 3.1\\*taste + e\n\n&amp;#x200B;\n\n\\## Question\n\n&amp;#x200B;\n\nI am confused if my thought process is correct. I think that I should explain how i set the above parameters using the following framework:\n\n&amp;#x200B;\n\n\\- Should it be less than or more than 0?\n\n\\- Should it be less than or more than 1?\n\n\\- In comparison to the \\*\\*other\\*\\* paramters, less or more?\n\n&amp;#x200B;\n\n\\### Final question - how does the 'constant' affect the regression line and should I consider it as well?\n\n&amp;#x200B;\n\nThank you guys :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fjwo7y/causuality_and_data_generation/"}, {"autor": "carolflyjs", "date": "2020-03-16 14:29:33", "content": "Does Augmentation perform happen on each epoch when using ImageDataGenerator? /!/ Using ImageDataGenerator with augmentation such as flipping and rotating, does Augmentation get performed on each epoch? \n\nMy understanding is that the generator gets -----> image !!! s streamed from the directory without \u201csaving\u201d, and the random augmentation on each -----> image !!!  essentially increases the data size by multi folds. Is my understanding correct?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fjlthz/does_augmentation_perform_happen_on_each_epoch/"}, {"autor": "qalis", "date": "2020-03-16 12:04:22", "content": "Medical datasets for disease prediction and/or detection /!/ For my bachelor\u2019s final project I\u2019m doing a medical app, where doctors will be able to input the measurements of the patient (in the specific format that I can require) and get predictions on disease prediction (is the patient at risk?, will he have to be hospitalized in the near future?) and/or disease detection (based on measurements and questionnaires, can my patient have given disease?, what disease might measurements indicate?). I\u2019m looking for datasets concerning this fields, from which I can train my models. I\u2019m NOT looking for -----> image !!!  data - my project concerns mostly \u201cclassical\u201d machine learning for tables of numerical data, not -----> image !!! s. I\u2019ve found a couple datasets, but the more, the better. Please provides any links and/or directions.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fjjvjg/medical_datasets_for_disease_prediction_andor/"}, {"autor": "janissary2016", "date": "2020-03-15 01:00:07", "content": "Colab can\u2019t find chromedriver path /!/ I\u2019m trying to create datasets on Google Colab using the google\\_images\\_download  \n library. Despite installing the chrome driver, it\u2019s not able to find it.\n\nHere are my code cells:\n\n&amp;#x200B;\n\n    %reload_ext autoreload\n    %autoreload 2\n    %matplotlib inline\n    \n    !pip install google_images_download\n    \n    !wget https://chromedriver.storage.googleapis.com/2.42/chromedriver_linux64.zip  &amp;&amp; unzip chromedriver_linux64\n    \n    import os\n    #Mount the drive from Google to save the dataset\n    from google.colab import drive # this will be our driver\n    drive.mount('/gdrive')\n    root = '/gdrive/My Drive/'     # if you want to operate on your Google Drive\n    \n    colab_path = '/gdrive/../content/'\n    \n    chromedriver_path = '/gdrive/../content/chromedriver'\n    \n    from google_-----> image !!! s_download import google_-----> image !!! s_download   #importing the library\n    \n    keyws   = \"jaguar\"\n    limit   = 1000\n    chromedriver = chromedriver_path\n    offset  = None  # how many links to skip\n    color_type  = None# color type you want to apply to the -----> image !!! s.[full-color, black-and-white, transparent]\n    size    = None  #relative size of the -----> image !!!  to be downloaded. [large, medium, icon, &gt;400*300, &gt;640*480, &gt;800*600, &gt;1024*768, &gt;2MP, &gt;4MP, &gt;6MP, &gt;8MP, &gt;10MP, &gt;12MP, &gt;15MP, &gt;20MP, &gt;40MP, &gt;70MP]\n    usage_rights    = 'labeled-for-reuse' #Very important! Check the doc\n    \n    arguments = {\n            \"keywords\" : keyws,\n            \"limit\" :limit,\n            \"chromedriver\":chromedriver,\n            \"offset\" : offset,\n            \"color_type\" : color_type,\n            \"size\" : size,\n            \"usage_rights\" : usage_rights\n            }   #creating list of arguments\n    response  = google_images_download.googleimagesdownload()   #class instantiation\n    response.download(arguments)  \n\nAnd this is the error:\n\n&amp;#x200B;\n\n    Item no.: 1 --&gt; Item name = jaguar\n    Evaluating...\n    Looks like we cannot locate the path the 'chromedriver' (use the '--chromedriver' argument to specify the path to the executable.) or google chrome browser is not installed on your machine (exception: Message: Service /gdrive/../content/chromedriver unexpectedly exited. Status code was: -6\n    )\n    An exception has occurred, use %tb to see the full traceback.\n    \n    SystemExit\n    /usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n      warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n\nThis is the file structure (Sorry about the transparency. My screenfetch does that for some reason):\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/cbszj5wjjqm41.png?width=319&amp;format=png&amp;auto=webp&amp;s=262cbca525442541b584ba10d8e97f331bb70d26", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fit2my/colab_cant_find_chromedriver_path/"}, {"autor": "dragonite1422", "date": "2020-08-17 02:22:47", "content": "Help for a Project (What Sonic Character Do You Look Like) /!/ Hi, right now I have an idea to make a project where the user turns on the -----> camera !!!  and based on that live feed, it will tell you what sonic character you look like (sonic, tails, knuckles, etc). I am pretty new to data science so I was wondering how I should execute this. Right now, I plan on using tensorflow, but the thing I'm worried about is that the input (the persons face) and the potential outputs (the sonic characters) will obviously look a lot different, so the object detection will probably have a low percentage match. Please let me know if you all have any ideas on how I can make this project. Thanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ib5n33/help_for_a_project_what_sonic_character_do_you/"}, {"autor": "cloud_weather", "date": "2020-08-16 15:27:41", "content": "-----> Image !!!  Restoration AI - Upscale and Restore Faces with DFDNet", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iau8jv/image_restoration_ai_upscale_and_restore_faces/"}, {"autor": "rich_kang", "date": "2020-08-16 06:33:43", "content": "Which model has better accuracy for object detection with a small training dataset - YOLO, SSD or Faster RCNN? /!/ We tried out first cut of object detection with these 3 models trained using default parameters but none giving good accuracy.\n\nBefore we dive into parameter tuning, which model has a better chance of improving accuracy with small (&lt;100) -----> image !!!  set? \n\nWe are using mobilenet for both YOLO and SSD, and resnet50 for Faster RCNN. Should we try darknet for YOLO instead?\n\nAlso, I assumed dataset format VOC or COCO has not impact to the accuracy of the model?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ianqh8/which_model_has_better_accuracy_for_object/"}, {"autor": "HV250", "date": "2020-07-12 19:51:54", "content": "Can I compress a sequence of number through an autoencoder? /!/ Specifically: I would like compress a set of coordinates, which map to the locations of 1's in a binary -----> image !!! , and then decode back to the original set. For instance, for a 16x16 image, my input might be something like the following:  \n\n  \n`[5, 4], [12, 5], [8, 7],....`\n\n I am not looking to recognize any spatial patterns or anything, but just a compressed representation of those numbers. The trained autoencoder should be able to handle any array of arbitrary \"coordinates\".  Is this doable? What would be a good loss function to use between the input and the uncompressed output?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hq0ru8/can_i_compress_a_sequence_of_number_through_an/"}, {"autor": "iim7_V6_IM7_vim7", "date": "2020-07-11 17:52:45", "content": "Anyone know how to output contents of bounding box when using TF Object Detection API?? /!/ I'm using the TF Object Detection API which returns my -----> image !!!  with a bounding box around given objects.\n\nHow can I output just the contents of the box(es) to feed into a new layer of my pipeline??\n\nWould really appreciate any help!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hpedmv/anyone_know_how_to_output_contents_of_bounding/"}, {"autor": "z_shit", "date": "2020-07-11 08:01:04", "content": "Need help with a project that uses Gait to recognise people. /!/ Hey guys, hope you all are doing well. I am in the middle of a small project. My project is basically simple implementation of facial recognition and gait recognition using the -----> camera !!!  quality that a typical(or high end) CCTV uses. \n\nI'm stuck on the gait recognition part as I don't know how to build my dataset(I didn't find any dataset that's open sourced online) and how to train my model. It's a whole new thing for me as I'm a novice at ML. Please leave some advice as to how I can achieve my goal. I'd appreciate any tech blogs that show a simple version of gait recognition through some code for the purpose of learning. Thanks in advance, stay home and stay safe.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hp6bs0/need_help_with_a_project_that_uses_gait_to/"}, {"autor": "Bala_venkatesh", "date": "2020-07-06 01:07:18", "content": "Computer vision /!/ Three methods for feature extraction from -----> image !!!  data.\n\n1) Grayscale Pixel Values as Features\n\n2) Mean Pixel Value of Channels\n\n3) Extracting Edge Features\n\n&amp;#x200B;\n\nAnimation video:- [https://youtu.be/AAs2yCa7G14](https://youtu.be/AAs2yCa7G14)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hly4hd/computer_vision/"}, {"autor": "jl303", "date": "2020-07-05 21:56:00", "content": "Classification Live Images From Camera Feed? /!/ I can successfully run -----> image !!!  classification if I feed examples from Google -----> image !!!  through mobilenet model.\n\nHowever, if I feed live images from camera in my livingroom, the result is pretty bad.\n\nI assume it's because live image from camera has bunch of objects?\n\nI tried feed live images through mobilenet_ssd for object detection, and the result is pretty good. However, it only recognizes 80 classes, and I don't need object location , so it's overkill.\n\nIs there a way to make mobilenet to pick out an object from an image with multiple objects and classify?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hlv2ho/classification_live_images_from_camera_feed/"}, {"autor": "dhruvmk", "date": "2020-07-05 06:17:35", "content": "How much theory do you need to know before moving on to implementing neural networks using Tensorflow.keras or PyTorch? /!/ Basically I'm almost done with Andrew Ng's course of basic Deep Learning and Neural Networks. I know about the inner workings of a neural network and know how to calculate forward and back propagation. To test my knowledge, I created a few neural network models from scratch using just NumPy, that can do basic regression and classification. Is this knowledge enough to move on to implementing neural networks using a high level API like Torch or Keras? Or should I learn about regularisation, hyper parameter tuning and optimization in the next course of this series? Thanks in advance, sorry if this is a stupid question.\n\nContext: I have the required programming language and know about advanced data structures, etc. I also know how to use Sci-Kit Learn to build models like -----> image !!!  classifiers and NLPs.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hlhwrp/how_much_theory_do_you_need_to_know_before_moving/"}, {"autor": "leonardoblanger", "date": "2020-07-04 17:22:10", "content": "A Tensorflow implementation of the DETR architecture (End-to-End Object Detection with Transformers) [Project] /!/ A Tensorflow port of the recent DETR Object Dection architecture, recently proposed by Facebook AI in the paper *End-to-End Object Detection with Transformers*.\n\nA radical shift in how to perform Deep Learning based Object Detection. It works by casting the detection problem as an -----> image !!!  to set mapping. The authors used a traditional CNN feature extractor + a Transformer architecture that converts a fixed set of object queries to detections, and trained it with a bipartite matching loss that uniquely assigns detections to ground truth objects to inforce unique detections. For details see the original [paper](https://arxiv.org/abs/2005.12872) and [blog post](https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers), and the official [Pytorch implementation](https://github.com/facebookresearch/detr).\n\nStill a work in progress, but already allows loading the converted Pytorch weights and matches the official code results on the COCO val2017 detection.\n\nLink to my Tensorflow implementation: [https://github.com/Leonardo-Blanger/detr\\_tensorflow](https://github.com/Leonardo-Blanger/detr_tensorflow)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hl6zm2/a_tensorflow_implementation_of_the_detr/"}, {"autor": "Chukypedro", "date": "2020-07-04 13:43:55", "content": "Where can I find geotiff drone images for agricultural purpose /!/ Please I want to know where I can download drone images in geotiff format for research purposes.\n\n-----> Image !!!  that displays crop row field, weeds, livestock etc\n\nIs there any other website where i can get such images. thanks in advance", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hl3iwp/where_can_i_find_geotiff_drone_images_for/"}, {"autor": "enlguy", "date": "2020-07-04 11:34:33", "content": "Data Storage /!/ I'm somewhat new to all this, so hope these aren't annoyingly simple matters, but hoping to gather some helpful thoughts...\n\nI'm working on some facial recognition stuff. I have some simple code I've tested with files on my hard drive, and things are working well. The next steps to scale this involve scraping and storing large amounts of -----> image !!!  files, and properly grouping them. Any thoughts on how to effectively do this for quick retrieval without turning my computer into nothing more than a giant image storage drive? Is there a way for python to do I/O from the cloud? I have read a bit about using JSON for this, but I'm not as well versed in js. Is reading analyzing files from URLs as simple as from my hard drive, or are there added concerns? And can a scraping script direct files to be stored in a cloud somewhere? Any help better understanding how this works in the cloud is appreciated, or maybe I just need to establish a server and get a huge hard drive...\n\nThank you", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hl1xd7/data_storage/"}, {"autor": "IHDN2012", "date": "2020-07-04 04:04:56", "content": "How can I create and train a custom CNN model architecture for object detection in Pytorch, for photos around 512x512? /!/ All the tutorials I have seen online either:\n\na. Focus on re-training a pretrained model\n\nb. Use MNIST or some other dataset with small -----> image !!!  dimensions\n\nI want to investigate the effects of maxpooling vs meanpooling/minpooling on object detection for human faces.  Are there any tutorials online that can help me do that?\n\nThank you for your help.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hkx2ej/how_can_i_create_and_train_a_custom_cnn_model/"}, {"autor": "DrMudTurtle", "date": "2020-07-03 15:53:06", "content": "Training -----> image !!!  classifier for a very specific task /!/ Greetings. I'm a biologist trying to get an image classifier to learn a very specific photo comparison, but the training photos I've tried are not getting the job done. I use timelapse photography to study snake activity, and the vast majority of my photos have no snakes in them. I want my image classifier to find the few (1-2%) that do contain snakes.\n\nMy student programmer and I tried training the machine using small (N=215) image sets thinking that the photos without animals are so nearly identical that the presence of any animal, should be obvious by comparison. That approach was unsuccessful. We then tried much larger training image sets - several hundred images downloaded from google with snakes vs. several hundred of various walls/windows without snakes. This also failed. The results were no better than if the AI was guessing randomly.\n\nMy student programmer has taken and modified the \\[Keras \"Image classification from scratch\"\\]([https://keras.io/examples/vision/image\\_classification\\_from\\_scratch/](https://keras.io/examples/vision/image_classification_from_scratch/)). Using the example Kaggle Cats vs. Dogs, the program runs with 85%+ accuracy, but our approaches with a much smaller sample size of photos of snakes have been unsuccessful.\n\nFor an example of images we're working with, \\[here\\]([https://imgur.com/a/JdncJKe](https://imgur.com/a/JdncJKe)) are a few of our images, a couple with snakes in view, and a couple without. We have tens of thousands of images of the window with no snake, and hundreds with snakes visible, about 1% of the images.\n\nSo what sort of image training set would be effective at this task? How many images? How diverse? Beyond the training images, do you have other suggestions to help the AI spot snakes when they appear?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hklh4u/training_image_classifier_for_a_very_specific_task/"}, {"autor": "redditorsurfinreddit", "date": "2020-07-03 15:03:17", "content": "Multi Image to Single Image Association /!/ Hello!\n\nI have a dataset which consists of drone captured images of a solar plant. I'm looking to build an orthomosic of that. In other terms, I want to combine all these images &amp; stitch them together. Keep in mind that these images are NOT geotagged. \n\nSo far I've tried to use a Non-ML approach i.e SIFT but the results are not that good. Images get easily warped, feature matching fails (&amp; non adaptive) &amp; manually tweaking the number of feature points is laborious.\n\nIs there a model/architecture type that I could use to associate multiple -----> image !!! s to a single -----> image !!! ?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hkklgf/multi_image_to_single_image_association/"}, {"autor": "NeverURealName", "date": "2020-07-03 08:33:54", "content": "I still don't know why we use the word \"semantic\" /!/ Hi, I am doing some translation on this word semantic classification. I know it is using UNet to do some kind of downsampling to upsampling. But semantic means grammar in many translations. Does it mean the UNet understands the -----> image !!! 's grammar to reconstruct the -----> image !!! ? And it is very confusing to translate it into grammar because I am talking about the image, not NLP.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hkfgi8/i_still_dont_know_why_we_use_the_word_semantic/"}, {"autor": "RonEng909", "date": "2020-07-03 03:22:10", "content": "Computer Vision Tutorial | -----> Image !!!  Processing | Convolution Neural Network", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hkbpw5/computer_vision_tutorial_image_processing/"}, {"autor": "Milo55545", "date": "2020-06-04 18:25:44", "content": "Easy software to train a neural network? /!/ Does anyone know of any user-friendly software that I could use to train an input-output -----> image !!!  neural network without putting too much time into learning something like TensorFlow? I am only making one neural network tool for one purpose, and I don't think that it would be worth it to learn how to create complicated neural networks at the moment. I would prefer just to provide a dataset with pairs of image files and train it, then I could give it an input and it would return an output image based on what it had learned. Any help would be appreciated. Thank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gwn2ck/easy_software_to_train_a_neural_network/"}, {"autor": "Edesak", "date": "2020-06-04 17:01:37", "content": "Mask R-CNN Coco /!/ Hi, i have done some models in CNN and i told my self let's try something else. So i looked up for multiple object detection and found a few guides/tutorials. But now i'm in dead end where i could not find any solution for my problem on internet. I'm stuck like 2-3 days on one specific problem. So if any good soul have some knowledge about coco model and mask-RCNN a would be glad to get in touch with you. If someone is interested please add me on Discord Edesak#5182 or comment section would be ok also.\n\nThe current problem i have is with loading mask function. Where i trying to load -----> image !!!  1080x1920 (FullHD desktop).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gwlepc/mask_rcnn_coco/"}, {"autor": "TrackLabs", "date": "2020-06-04 15:52:25", "content": "Anyone know whats causing this dimension of filter error? (GAN) /!/ Im trying to make a GAN to create pokemon -----> image !!! s, but its causing problems on the discriminator, whenever Im passing a -----> image !!!  to it. The images are RGB, and the dataset has a shape of (819, 256, 256, 3), 819 images, 256x256 pixels with 3 channels (RGB).\n\nBut whenever I pass each image to the discriminator, which is this code \n\n    #\u00a0DISCRIMINATOR\ndef make_discriminator_model():\n\u00a0\u00a0\u00a0\u00a0model\u00a0=\u00a0tf.keras.Sequential()\n\u00a0\u00a0\u00a0\u00a0model.add(tf.keras.layers.Conv2D(7,(3,3),padding=\"same\",input_shape=(train_images.shape[1],\u00a0train_images.shape[2],\u00a0train_images.shape[3])))\n\u00a0\u00a0\u00a0\u00a0model.add(tf.keras.layers.Flatten())\n\u00a0\u00a0\u00a0\u00a0model.add(tf.keras.layers.LeakyReLU())\n\u00a0\u00a0\u00a0\u00a0model.add(tf.keras.layers.Dense(50,activation=\"relu\"))\n\u00a0\u00a0\u00a0\u00a0model.add(tf.keras.layers.Dense(1))\n return\u00a0model\n\nIt throws this error: \n\n&gt;ValueError: number of input channels does not match corresponding dimension of filter, 1 != 3\n\nWould anyone know whats causes this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gwk2jo/anyone_know_whats_causing_this_dimension_of/"}, {"autor": "Mjjjokes", "date": "2020-06-04 01:17:28", "content": "Is it possible to use deep learning to extract a payload from a stego -----> image !!! ? /!/ I've done a bit of research. Current (public) steganalysis methods merely detect steganography. Some research papers mention \"locating steganalysis,\" which is basically finding where exactly the hidden message is in the image/stego item. After it's found, the problem becomes a cryptanalysis problem. Can deep learning be used for locating steganalysis as well as cryptanalysis? And could this be used for any stego image? Or will it be rendered obsolete as steganography software evolves?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gw78fg/is_it_possible_to_use_deep_learning_to_extract_a/"}, {"autor": "spmallick", "date": "2020-06-03 18:53:08", "content": "Graph Convolutional Networks: Model Relations In Data /!/ Graph Convolutional Network: An Introduction  \n\n\nIn Artificial Intelligence, data is the king!  \nThat said, people squeeze different amounts of information from their data depending on how clever they are.  \nConsider the problem of -----> image !!!  tagging which is an -----> image !!!  classification problem with a slight twist - every -----> image !!!  can have one or more labels/tags associated with it.  \nHowever, some of the tags are not independent. For example, if there is a SKY label for an image, the probability of seeing the CLOUD or SUNSET labels for the same picture are high.  \nResearchers are trying to use prior knowledge about connections between labels to get better results on image tagging problem.  \n\n\nIn today's post, we go over the idea of Graph Convolutional Network that solves this problem.  \n[https://www.learnopencv.com/graph-convolutional-networks-model-relations-in-data/](https://www.learnopencv.com/graph-convolutional-networks-model-relations-in-data/?fbclid=IwAR2e3UzRL9vsMNlMO4GnSdkCgFYWrh3A_EI_SFgm5_QDfQDQwlKbRpsePqs)\n\n&amp;#x200B;\n\nLink to code:  [https://github.com/spmallick/learnopencv/tree/master/Graph-Convolutional-Networks-Model-Relations-In-Data](https://el2.convertkit-mail.com/c/lmu58r7d56imhrdnmdfg/z4igh7udwqlvzp/aHR0cHM6Ly9naXRodWIuY29tL3NwbWFsbGljay9sZWFybm9wZW5jdi90cmVlL21hc3Rlci9HcmFwaC1Db252b2x1dGlvbmFsLU5ldHdvcmtzLU1vZGVsLVJlbGF0aW9ucy1Jbi1EYXRh?fbclid=IwAR0Is9g36ZPDEA9HJcDL841v2SP2_96yBAT6BpiDXjcrjehLmQ1L1XB5OAY) )  \n\nhttps://preview.redd.it/pkyw7erurq251.jpg?width=500&amp;format=pjpg&amp;auto=webp&amp;s=de3ff34c4439e791d96a0831087b798beee56866", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gw00gb/graph_convolutional_networks_model_relations_in/"}, {"autor": "dliteful23", "date": "2020-06-03 16:19:51", "content": "[P] whisk - an open-source ML project framework that makes collaboration, reproducibility, and deployment \u201cjust work\u201d /!/ Hey folks,\n\nI used to dive into a data science project with just a Jupyter Notebook and lots of impatience. However, that boundless, explore-at-all-cost drive soon backfired. In short time, I\u2019d have an unorganized notebook, duplicate code, plenty of ugly hacks, and a project that\u2019s difficult to share, reproduce, and deploy as an ML model.\n\nAbstracted from my own pain, I\u2019m happy to introduce [**whisk**](http://docs.whisk-ml.org/)**,** an open-source data science project framework that makes collaboration, reproducibility, and deployment \u201cjust work\u201d. It combines a [data science-flavored Python project structure](https://docs.whisk-ml.org/en/latest/project_structure.html) with a suite of lightweight tools. It adds *just enough* structure to make a project easy to share.\n\nIf you're looking for another magical, highly-abstracted ML tool, this isn't it. Instead, whisk leverages the ability to do special things when your project resembles the [standard Python project structure](https://docs.python-guide.org/writing/structure/). From collaboration to distribution, it's all easier when this foundation is used.\n\nEnough fluff - here's some sample projects using whisk. You should be able to clone these from GitHub, run locally, and even deploy to Heroku with just a couple of commands:\n\n* [Bike -----> Image !!!  Classifier](https://github.com/whisk-ml/bike_image_classifier_tensorflow) \\- A Tensorflow-backed Classifier that determines if an image is of a Mountain bike or a Road bike.\n* [Real or Not? NLP with disaster tweets](https://github.com/whisk-ml/disaster_tweets) \\- A Tensorflow-backed Keras model that predicts which tweets are about real disasters and which ones are not. [DVC](https://dvc.org) is used to version control the model training pipeline.\n\nWant to setup your own project? Much like `django-admin startproject` creates the structure for a Django web app, whisk sets up a project directory structure and environment for you. Open a terminal and run:\n\n    pip3 install whisk\n    whisk create &lt;project_name&gt;\n\nReplacing &lt;project\\_name&gt; with your unique name. See the [quick tour](https://docs.whisk-ml.org/en/latest/tour_of_whisk.html) in the docs for more on setting up a project.\n\n[Screen recording of \\`whisk create\\` at https:\\/\\/asciinema.org\\/a\\/uXW9s6FtkmbxNQQIdtDRLFYII](https://preview.redd.it/7kkpxops1q251.png?width=808&amp;format=png&amp;auto=webp&amp;s=215310c69d868b189d1b057771a7857068583067)\n\nAnyway, give whisk a shot. Would be great to hear your feedback.\n\n## whisk resources\n\n* [Documentation](https://docs.whisk-ml.org)\n* [whisk on GitHub](https://github.com/whisk-ml/whisk)\n* [Announcement post](https://dlite.cc/2020/06/02/whisk-data-science-project-framework.html) \\- has more details and screen recordings of the whisk CLI commands.\n* Example projects\n   * [Disaster Tweets](https://github.com/whisk-ml/disaster_tweets)\n   * [Bike Image Classifier](https://github.com/whisk-ml/bike_image_classifier_tensorflow)\n\n## PS - what about Cookiecutter Data Science?\n\nMy first attempts at structuring an ML project used [Cookiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/), and life was way better. whisk takes this well-regarded structure and sprinkles on some magic: more graceful upgrades, environment configuration, packaging, [DVC](https://dvc.org), and deployment. I go into more details on the differences between whisk and Cookie DS on my blog in my [announcement post](https://dlite.cc/2020/06/02/whisk-data-science-project-framework.html).\n\nHonestly though: if you are deciding between Cookiecutter DS and whisk, we\u2019re already winning the war against sloppy data science.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gvwyqh/p_whisk_an_opensource_ml_project_framework_that/"}, {"autor": "MightyParserer", "date": "2020-06-03 15:41:33", "content": "-----> Image !!!  captioning [P]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gvw9a4/image_captioning_p/"}, {"autor": "0_marauders_0", "date": "2020-06-02 21:31:04", "content": "OpenAI \u2013 Learning Dexterity End-to-End - Experiment Report /!/ Today OpenAI published a Weights &amp; Biases Report ([here](https://app.wandb.ai/openai/published-work/Learning-Dexterity-End-to-End--VmlldzoxMTUyMDQ)) on some recent work done by the Robotics team at OpenAI where they trained a policy to manipulate objects with a robotic hand in an end-to-end manner. Specifically, they solved the block reorientation task from our 2018 release \"[Learning Dexterity](https://openai.com/blog/learning-dexterity/)\" using a policy with -----> image !!!  inputs rather than training separate vision and policy models (as in the original release).\n\nIn the report they describe their experimental process in general and then detail the findings of this specific work. In particular, they contrast the use of Behavioral Cloning and Reinforcement Learning for this task, and ablate several aspects of our setup including model architecture, batch size, etc.\n\nAlex and I happy to discuss this and answer any questions about it.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gvgbsj/openai_learning_dexterity_endtoend_experiment/"}, {"autor": "deliberatelymistaken", "date": "2020-06-07 09:18:42", "content": "What is the best way of combining audio and visual data to make predictions? /!/ I am trying to predict the probability of a disease by using audio and images, the audio and the images do not come from the same source. I am thinking of combining the outputs (maybe average them) of two models one utilizing -----> image !!!  data to calculate the probability the other using audio data. I am feeling unsure of this approach as I will not be able to verify the final combined output. Will the outcomes be reliable, or should I use a different approach?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gy92hz/what_is_the_best_way_of_combining_audio_and/"}, {"autor": "MLtinkerer", "date": "2020-06-06 23:16:57", "content": "Latest from Samsung researchers: State of the art in -----> photo !!!  editing (Harmonization)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gy16ei/latest_from_samsung_researchers_state_of_the_art/"}, {"autor": "OnlyProggingForFun", "date": "2020-06-06 13:46:04", "content": "AI Generates Real Faces From Sketches! DeepFaceDrawing Overview | -----> Image !!! -to-image translation in 2020", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gxr9kc/ai_generates_real_faces_from_sketches/"}, {"autor": "MrMegaGamerz", "date": "2020-05-08 20:10:36", "content": "Looking for advice on how to set up parameters for CNN (sequential) /!/ Is there any guide I can follow to help set up a CNN, using a sequential method? I have a dataset of over 100,000 images from Kaggle and am looking to categorize them into 25 different groups.\n\nI understand I need to add Conv2D, MaxPooling2D, Flatten, and Dense layers. But I'm unsure of how many to put and what parameters to set them at. I understand that I will have to do trial and error until I get the accuracy high but right now I am getting an accuracy of 0%. I found the following code snippet online which works for a 10 category classification and I'm trying to see how this would be changed to go to 25 groups.\n\n    model = Sequential()\n    model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(100, 100, 1))) \n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu')) \n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(10, activation='softmax'))\n\nI tried to change the input scale to input\\_shape=(200, 200, 1) to match my input -----> image !!!  size, and I also changed the Final Dense layer from 10 to 25, however, I am still getting 0% accuracy. Any advice is appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gg0m3x/looking_for_advice_on_how_to_set_up_parameters/"}, {"autor": "lekorotkov", "date": "2020-05-08 14:32:40", "content": "ASK: Maybe anyone knows how to implement a search by an -----> image !!!  algorithm? /!/ You can google similar images by uploading an image, I want to replicate this technology for my own project. Maybe somebody knows where to find a course or an article on how to do that?  \nThanks in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gfuaik/ask_maybe_anyone_knows_how_to_implement_a_search/"}, {"autor": "Void-Nut", "date": "2020-05-08 06:30:54", "content": "What to use to run a regression problem in python? /!/ I have a large set of data in a csv file, roughly 1700 rows by 35 columns, that gets added to every day and my goal is to take that data and use it to predict one number per row. The problem that I am running into is that I don't know where go from here. All of the machine learning software I have looked at is focused on -----> image !!!  recognition and not text data problems. If I do find a guide for one it's super basic and uses downloaded data that I can't easily replace with a csv import. So I was wondering if you guys have any recommendations for software I could use to solve this problem and if you know any tutorials that go along with them.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gfnujz/what_to_use_to_run_a_regression_problem_in_python/"}, {"autor": "ConVit", "date": "2020-05-07 19:36:51", "content": "How to use machine learning to change clothes for people? /!/ Imagine you have a -----> picture !!!  of a dress and a -----> picture !!!  of a person. You want to combine them into the picture of that person wearing that dress.\n\nI want to ask if it is possible with deep learning? If it is possible, where can I start researching it? Thanks in advance.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gfdi9x/how_to_use_machine_learning_to_change_clothes_for/"}, {"autor": "HTKasd", "date": "2020-05-11 14:44:32", "content": "CycleGAN vs DiscoGAN vs DualGAN vs XGAN /!/ Out of the mentioned -----> Image !!!  to -----> Image !!!  models, which one is better than others in what kind of task(s)?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ghouqx/cyclegan_vs_discogan_vs_dualgan_vs_xgan/"}, {"autor": "Competitive_Mongoose", "date": "2020-05-11 14:06:56", "content": "How to create a data generator in Tensorflow 2.0 /!/ I want to load my data into Tensorflow but since it is too big I'm trying to make a data generator (for the first time) to load the data one by one into memory. The data I have is a list of lists and the target is a list of 1s and 0s. My attempt so far is to do this\n\n    data = tf.data.Dataset.from_tensor_slices(X)\n\nbut this only has the data and not the targets and when I put [model.fit](https://model.fit)(data, Y) I get an error saying that I should only have one argument if I want to use a data generator. I have tried to put (X,Y) as the argument in the from\\_tensor\\_slices but this gives me a shape error.\n\n    ValueError: Error when checking input: expected input_1 to have shape (441000,) but got array with shape (1,)\n\nThe closest example I have found to what I want is from a Cifar10 tutorial but I cannot understand how I can translate this from code for -----> image !!!  processing to code for list classification.\n\n    batch_size = 32\n    data_generator = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n    train_generator = data_generator.flow(x_train, y_train, batch_size)\n    steps_per_epoch = x_train.shape[0] // batch_size\n\nCould anyone point me in the right direction? Does anyone know any reading material for this (other than TF documentation)?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gho64d/how_to_create_a_data_generator_in_tensorflow_20/"}, {"autor": "rahulm44", "date": "2020-05-11 12:16:59", "content": "How to install nvidia transfer learning toolkit in google colab? /!/ I tried with solutions available online but none worked. TLT is using docker -----> image !!!  so is it possible in colab?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ghmgc6/how_to_install_nvidia_transfer_learning_toolkit/"}, {"autor": "FatherCannotYell", "date": "2020-05-11 03:42:03", "content": "Need help defining custom Tensorflow recurrent graph with intermediate time step dependencies with dynamic_rnn /!/ I have a question regarding the tensorflow graph definitions using dynamic_rnn (still using TF 1).\n\nI would like to define a TF graph as described in this -----> image !!!  as [Tensorflow Parallel/Sequential Custom RNNs](https://imgur.com/a/bZIEKWP)\n\nI have multiple LSTMs, and a custom RNN that are used for processing time-steps.\n\nBetween each time step, each LSTM takes as input the outputs of other LSTMs.\n\nSo for example, at timestep = 2, LSTM 1 takes as input the state/output of LSTM 2 too. I am not sure how to go about defining the tensorflow graph in code.\n\nI can define LSTM1 and CustomRNN1 as a MultiRNNCell, but not too sure how to go about defining it across LSTM1 and LSTM2 (i.e. parallel operations.\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ghg30d/need_help_defining_custom_tensorflow_recurrent/"}, {"autor": "_notdivyanshuuuu", "date": "2020-05-10 07:36:06", "content": "Need help with deeper understanding of what's going on in neural networks. /!/ I completed the first assignment in deeplearning.ai and got 100/100.But i still dont understand how numbers equate -----> image !!!  values and it all seems like magic to me.Is it normal or there's a better explanation or am i doing something wrong?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ggx4fu/need_help_with_deeper_understanding_of_whats/"}, {"autor": "hedgehogist", "date": "2020-05-10 07:03:14", "content": "[D] Anyone interested in music recommendation? /!/ I'm really fascinated by apps like Spotify and Shazam, and would like to pursue a career related to music information retrieval (MIR) and recommender systems. It seems like an esoteric field, especially since deep learning these days seems to primarily be focused on -----> image !!!  and text data.\n\nAny of you interested in (or working in) similar areas? Maybe we can connect and form a study group! Maybe we can share interesting papers (e.g. from ISMIR and ACM RecSys conferences), work on projects together, and share study materials.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ggwqus/d_anyone_interested_in_music_recommendation/"}, {"autor": "fromnighttilldawn", "date": "2020-05-10 06:33:20", "content": "Does fast gradient sign attack also work for non-neural network models? /!/ Fast gradient sign attack is popularized with the panda-to-gibbon -----> picture !!!  on GoogLeNet\n\nMy question is simple: does it also work for other types of model with well-defined loss functions, such as SVM, logistic regression, etc?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ggwe6r/does_fast_gradient_sign_attack_also_work_for/"}, {"autor": "diffu5e", "date": "2020-05-10 01:09:29", "content": "Tool to quickly label single or multiclass -----> image !!!  data /!/ I made a tool to quickly associate classification labels with images, without your hands leaving the keyboard.  Improvement suggestions/contributions welcome!\n\nIf anyone is interested [here it is](https://github.com/diffuse/jabber)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ggrvzl/tool_to_quickly_label_single_or_multiclass_image/"}, {"autor": "Piollinas", "date": "2020-05-09 19:19:05", "content": "About the equivalence of convolutions and transposed convolutions /!/ Hi everyone,\nI'm relatively new to machine learning, and interested to -----> image !!! -to------> image !!!  translation.\nI've recently read StarGAN v2 ([https://arxiv.org/abs/1912.01865](https://arxiv.org/abs/1912.01865)) and noticed in their generator architecture (Section E. Table 5.) that they use residual convolutional blocks for both the encoder and the decoder.\n\nFrom this, several questions arise: \n\n1. Can convolutions and transposed convolutions be equivalent?\nHere's a small example:\n```py\nimport torch\nfrom torch.nn import Conv2d, ConvTranspose2d\n\nn = 1\nh, w = 128, 128\ni, o = 16, 32\nk, p = 3, 1\n\na = Conv2d(in_channels=i, out_channels=o, kernel_size=k, padding=p)\nb = ConvTranspose2d(in_channels=i, out_channels=o, kernel_size=k, padding=p)\n\nx = torch.ones(n, i, h, w)\nprint(x.shape)\n\ny = a(x)\nz = b(x)\n\nprint(y.shape)\nprint(z.shape)\n\n```\nGiven the same input, can both layers produce the same output? If yes, is there a relation between their weights (and bias)?\n\n2. If I got it right, a downsampling operation of a given stride is equivalent to a constant channel-wise convolution, and respectively an upsampling layer is equivalent to a constant channel-wise transposed convolution, is that true? If so, then why not learning transposed convolutions instead of upsampling?\n\nThanks in advance,\n\nPiollinas", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gglpxr/about_the_equivalence_of_convolutions_and/"}, {"autor": "khalilmeftah", "date": "2020-05-09 14:58:44", "content": "Gender Prediction from Offline Handwriting Using Convolutional Neural Networks /!/  \n\nStarting from the fact that handwritten documents style are  gender-dependent (male and female have different writing styles), I'm  trying to predict writer's gender from its handwritten scripts using  Convolutional Neural Network (CNN). I choose the IAM and KHATT datasets  for English and Arabic respectively.  After I read research articles  related to this problem, I realized that few of them used deep learning  (with handwritten word and/or sentence as input to CNN) and the majority  used classic methods like (LBP, HOG, GLCM, SFTA), the state of the art  accuracy is 80.79% and 85% (for CNN and classic methods respectively), I  also read this [Article](https://www.etsmtl.ca/ETS/media/ImagesETS/Labo/LIVIA/Publications/2012/Hanusiak_IJDAR_2012.pdf)  about identifying/verifying writer from its handwritten scripts which  uses texture blocks from the written documents that give good results.  As data preprocessing I used the method of texture blocks, after line  and word segmentation, i constructed texture -----> image !!!  from handwriting  words, and segment the texture -----> image !!!  to texture blocks of size  100\\*100px.\n\nI used 60 writers per gender for training (which give 42,000 texture  blocks), and 7 writers per gender for validation (which give 4,000  texture blocks) and 7 writers per gender for testing (which give 4,000  texture blocks).\n\nI'm using TensorFlow and Keras as framework, I started with simple  (LeNet-like architectures) for base line, I got 60% test accuracy. As a  second approach, I used different state of the art architecture in image  classification like (VGG16, VGG19, RESNET34, RESNET50) and I got 64%  test accuracy.\n\nSo, my questions are:\n\n&amp;#x200B;\n\n1. How to reduce overfitting though I tried regularization methods (Dropout, L1, L2, Batch Norm) to reduce overfitting ?\n2. I observed high variance in accuracy, if I re-shuffle the data (e.g from 64% to 50%) ?\n3. How to modify CNN architectures to (multi-input CNN) to make the decision on multiple texture blocks, instead of one?\n\nTexture blocks examples : \n\nhttps://preview.redd.it/lp3ekbfa7rx41.png?width=558&amp;format=png&amp;auto=webp&amp;s=f981dc32dd388318ddd7c6f6d7637addf20c22de", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gggtve/gender_prediction_from_offline_handwriting_using/"}, {"autor": "murphinate", "date": "2020-04-29 02:40:12", "content": "Does concatenation in math mean use contiguous tensor in code? /!/ See -----> picture !!!  for context\n\nhttps://preview.redd.it/m0gk5ebf6ov41.png?width=1715&amp;format=png&amp;auto=webp&amp;s=b1c9846d099123d7b3e93d3155b49edf21ec88b3", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ga19tq/does_concatenation_in_math_mean_use_contiguous/"}, {"autor": "Stewie977", "date": "2020-04-28 15:03:11", "content": "Approaches for solving Connect Four /!/ How would you go about solving the classic game Connect Four?\n\nThere is no hidden information, so it would take in the board as input every round right? But I can't just flatten it out because I would lose important row, column and diagonal information? I was thinking maybe process it as a matrix similar to -----> image !!!  object recognition. Any thoughts, is that overkill is there a better approach?\n\nIdeally I would like to take advantage of deep reinforcement learning.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g9ot20/approaches_for_solving_connect_four/"}, {"autor": "redditpolldancer", "date": "2020-04-28 13:54:44", "content": "Building a \"Self Steering\" Car Based on Nvidia's Paper; Would Love Feedback on How to Improve the Videos /!/ Hey everyone, I'm working on building a car that can self steer on main roads (basically lane keep assist) like in [this](https://devblogs.nvidia.com/deep-learning-self-driving-cars/) Nvidia paper from 2016.\n\nI'm planning to take it on a 2500 mile road trip this summer to collect data that I'll open source. It should be around 70 hours of steering angle + -----> image !!!  + speed data.\n\nI've been documenting this process and have released a few YouTube videos. I would love to get some feedback on how \"in depth\" I should go with the building / explaining of the model and system.\n\nI know the videos aren't great right now, but hopefully with some suggestions I can make the next ones more watchable.\n\nSelf Steering Car Playlist:\n\n[https://www.youtube.com/watch?v=gL4-Od9mui4&amp;list=PLv9Rb3wOwHfte5ynMZVJA98VIM6TZKg9p](https://www.youtube.com/watch?v=gL4-Od9mui4&amp;list=PLv9Rb3wOwHfte5ynMZVJA98VIM6TZKg9p)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g9nlzb/building_a_self_steering_car_based_on_nvidias/"}, {"autor": "kiryangol", "date": "2020-04-28 07:14:52", "content": "Mathematics behind conditional GANs /!/ In conditional GANs ( if I understood correctly from the paper ), at some point we create shared representation of -----> image !!!  and the label by merging them together both in generator and discriminator at the deeper hidden layers. While I understand that by this we learn a joint (?) distribution of image and a label, I don't quite get how it works from the mathematical view point. Does this idea leverages something like a+b = c + b for probability distributions - adding a label distribution to an image distribution both for generator and discriminator ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g9ijjm/mathematics_behind_conditional_gans/"}, {"autor": "PixelPixell", "date": "2020-04-28 06:56:57", "content": "How to get into neural network /!/ First time poster &amp; not native to English, I apologize for any mistakes.\n\nI'm about to achieve a university degree in computer science. I took classes in ML and -----> image !!!  processing and I found it fascinating. I would like to start a career in the field of image processing and neural network.\nI guess my questions are: How do I get really good at it? In school they always give us the architecture for the network and I believe the biggest challenge is to build that architecture. Should I just look at as many projects as possible? Should I experiment with my own? Where would I get datasets? Can I create meaningful work on my humble laptop?\nThanks in advance and any other tips would be greatly appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g9ic31/how_to_get_into_neural_network/"}, {"autor": "-john--doe-", "date": "2020-04-28 06:21:10", "content": "Two fresh Kaggle competitions on Image Analysis /!/ Two new intersting datasets and challenges from Kaggle to learn how to work with images and associated metadata.\n\n[Alaska2 -----> Image !!!  StegAnalysis](https://www.kaggle.com/c/alaska2-image-steganalysis/overview) \n\n[Photo Slideshow Optimization](https://www.kaggle.com/c/hashcode-photo-slideshow/overview)\n\nHave fun!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g9hx8a/two_fresh_kaggle_competitions_on_image_analysis/"}, {"autor": "absurd234", "date": "2020-04-28 05:44:01", "content": "Can you suggest me some topics on -----> image !!!  recognition /!/ I was doing some research on image recognition so that I can use it for my project. Now I am quite confused which algorithm to follow, should I proceed with open cv or tensor flow and to use which algorithm like SFIT Etc. Can you suggest me something", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g9hhbe/can_you_suggest_me_some_topics_on_image/"}, {"autor": "cmillionaire9", "date": "2020-04-27 14:21:56", "content": "Diverse -----> Image !!!  Synthesis for Multiple Domains", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g91kyl/diverse_image_synthesis_for_multiple_domains/"}, {"autor": "grid_world", "date": "2020-04-27 11:53:43", "content": "Kernel Density Estimation for bimodal distribution with Python /!/ I have a bimodal distribution for the range \\[-0.1, 0.1\\] which can be viewed here:\n\n \\[!\\[enter -----> image !!!  description here\\]\\[1\\]\\]\\[1\\]\n\n&amp;#x200B;\n\nI want to train/fit a Kernel Density Estimation (KDE) on the bimodal distribution as shown in the picture and then, given any other distribution say a uniform distribution such as:\n\n&amp;#x200B;\n\n\\# a uniform distribution between the same range \\[-0.1, 0.1\\]-\n\nu\\_data = np.random.uniform(low = -0.1, high = 0.1, size = (1782,)) \n\n&amp;#x200B;\n\nI want to be able to use the trained KDE to 'predict' how many of the data points from the given data distribution (say, 'u\\_data') belong to the target bimodal distribution.\n\n&amp;#x200B;\n\nI tried the following code but it doesn't work out:\n\n&amp;#x200B;\n\n\\# Here 'a' is the numpy array containing target bimodal distribution.\n\n&amp;#x200B;\n\n\\# Generate random samples-\n\nkde\\_samples = {}\n\n\n\nfor kernel in \\['tophat', 'gaussian'\\]:\n\n\\# Train a kernel on bimodal data distribution 'a'-\n\nkde = KernelDensity(kernel=kernel, bandwidth=0.2).fit(a.reshape(-1, 1))\n\n&amp;#x200B;\n\n\\# Try and generate 300 random samples from trained model-\n\nkde\\_samples\\[kernel\\] = np.exp(kde.sample(300))\n\n\n\n\n\n\n\n\\# Visualize data distribution using histograms-\n\nplt.hist(a, bins=20, label = 'original distribution')\n\n\\# sns.distplot(a, kde = True, bins = 20, label = 'original distribution')\n\nplt.hist(kde\\_samples\\['gaussian'\\], bins = 20, label = 'KDE: Gaussian')\n\nplt.hist(kde\\_samples\\['tophat'\\], bins = 20, label = 'KDE: tophat')\n\n\n\nplt.title(\"KDE: Data distribution\")\n\nplt.xlabel(\"weights\")\n\nplt.ylabel(\"frequency\")\n\nplt.legend(loc = 'best')\n\n[plt.show](https://plt.show)()\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nThis gives the following visualization:\n\n&amp;#x200B;\n\n\\[!\\[KDE different kernels\\]\\[2\\]\\]\\[2\\]\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nTwo things are wrong:\n\n&amp;#x200B;\n\n 1. The range of the generated samples are wrong!\n\n 2. The distribution of generated data is NOT bimodal\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nHow can I therefore: train/fit a Kernel Density Estimation (KDE) on the bimodal distribution and then, given any other distribution (say a uniform or normal distribution) be able to use the trained KDE to 'predict' how many of the data points from the given data distribution belong to the target bimodal distribution.\n\n&amp;#x200B;\n\nI am using Python 3.8 and sklearn 0.22.\n\n&amp;#x200B;\n\nThanks!\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n  \\[1\\]: [https://i.stack.imgur.com/DMcXH.png](https://i.stack.imgur.com/DMcXH.png)\n\n  \\[2\\]: [https://i.stack.imgur.com/Sbl4x.png](https://i.stack.imgur.com/Sbl4x.png)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g8zawm/kernel_density_estimation_for_bimodal/"}, {"autor": "trexd___", "date": "2020-05-01 13:22:08", "content": "Why do the paper's authors suggest using symmetric matrices in DCGAN? /!/ Recently I have been reading a paper ([Generative Modeling for Protein Structures](https://papers.nips.cc/paper/7978-generative-modeling-for-protein-structures.pdf)) that I have also been working on implementing to teach myself more about GANs. One questions I have is; why do they specify in the [supplement](https://papers.nips.cc/paper/7978-generative-modeling-for-protein-structures) that the matrices created by the generator have to be made symmetric and clamp above zero when passing from the generator to the discriminator?\n  \nIn this case the model being used is DCGAN but making the generator output symmetric usually doesn't apply to DCGANs from what I've read. When I implement clamping and symmetry, the generator doesn't learn at all why the discriminator converges almost immediately to zero. The model seems to learn better without the use of symmetry and clamping (although it seems to converge and overfit quickly).\n\nIf you want to have a look at my current implementation you can find my current training loop below:\n```python\n# Training Loop\n# Lists to keep track of progress\nimg_list = []\nG_losses = []\nD_losses = []\niters = 0\n\nprint(\"Starting Training Loop...\")\n# For each epoch\nfor epoch in range(num_epochs):\n    # For each batch in the dataloader\n    for i, data in enumerate(dataloader, 0):\n        ############################\n        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n        ###########################\n        ## Train with all-real batch\n        netD.zero_grad()\n        # Format batch\n        # Unsqueezed dim one to convert [128, 64, 64] to [128, 1, 64, 64] to conform to D architecture \n        real_cpu = (data.unsqueeze(dim=1).type(torch.FloatTensor)).to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label, device=device)\n        # Forward pass real batch through D\n        output = netD(real_cpu).view(-1)\n        # Calculate loss on all-real batch\n        errD_real = criterion(output, label)\n        # Calculate gradients for D in backward pass\n        errD_real.backward()\n        D_x = output.mean().item()\n\n        ## Train with all-fake batch\n        # Generate batch of latent vectors\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        # Generate fake -----> image !!!  batch with G\n        fake = netG(noise)\n        label.fill_(fake_label)\n        # Make Symmetric\n        sym_fake = (fake.detach().clamp(min=0) + fake.detach().clamp(min=0).permute(0, 1, 3, 2)) / 2\n        # Classify all fake batch with D\n        output = netD(sym_fake).view(-1)\n        # Calculate D's loss on the all-fake batch\n        errD_fake = criterion(output, label)\n        # Calculate the gradients for this batch\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        # Add the gradients from the all-real and all-fake batches\n        errD = errD_real + errD_fake\n        # Update D\n        optimizerD.step()\n        #adjust_optim(optimizerD, iters)\n        ############################\n        # (2) Update G network: maximize log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        label.fill_(real_label)  # fake labels are real for generator cost\n        # Since we just updated D, perform another forward pass of all-fake batch through D\n        output = netD(fake.detach()).view(-1)\n        # Calculate G's loss based on this output\n        errG = criterion(output, label)\n        # Calculate gradients for G\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        # Update G\n        optimizerG.step()\n        adjust_optim(optimizerG, iters)\n\n        # Output training stats\n        if i % 50 == 0:\n            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n                  % (epoch, num_epochs, i, len(dataloader),\n                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n        # Save Losses for plotting later\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n\n        # Check how the generator is doing by saving G's output on fixed_noise\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n\n        iters += 1\n```", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gbhzvq/why_do_the_papers_authors_suggest_using_symmetric/"}, {"autor": "cmillionaire9", "date": "2020-04-30 18:13:52", "content": "AI creates high-quality 3D avatars from a single -----> image !!! .", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gb104e/ai_creates_highquality_3d_avatars_from_a_single/"}, {"autor": "rushik_subba", "date": "2020-04-30 15:23:47", "content": "Effiecient deep q-learning model /!/ Hello guys I am training a deep qlearning model to play atari games. In that i have to pass an -----> image !!!  of size 84x84x4 as input to the network. Each frame (84x84) of the game is part of 4 different images. I have to store approximately 1 million of these images on my memory removing the oldest one (like a queue with max length 1 mil). Also the images are produced while the game is running. How do i store these frames effieciently such that each frmae is stored only once? I only have a limited amount of ram of size 8 GB. Any help would be greatly appreciated.\n\nI am doing this in python.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gaxoxu/effiecient_deep_qlearning_model/"}, {"autor": "bodytexture", "date": "2020-04-30 03:52:15", "content": "GANs on Runway /!/ hello, wish to make a \"merged syntetized artificial\" -----> picture !!!  out of a dataset of -----> picture !!! s of faces I have personally collected, possibly using stylegan2 on runway. Wich model should I use?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gaog6z/gans_on_runway/"}, {"autor": "MLtinkerer", "date": "2020-04-30 02:21:47", "content": "From CVPR: Reconstruct photorealistic 3D faces from a single \"in-the-wild\" -----> image !!!  with an increasing level of detail", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gan4a3/from_cvpr_reconstruct_photorealistic_3d_faces/"}, {"autor": "NYGooner17", "date": "2020-04-30 02:07:42", "content": "Is there a way to pass an -----> image !!!  caption and -----> image !!!  to a NN in Tensorflow? /!/ I have a CNN that can classify images from Reddit but I want to pass the image caption as well because I want to use this to help predict the upvotes on a post. I'm using a post's upvote count after a day but I'm not sure how to connect my image classifier with a caption to output a number (upvotes).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gamwr1/is_there_a_way_to_pass_an_image_caption_and_image/"}, {"autor": "MrCactus19", "date": "2020-04-29 18:42:33", "content": "Best way to convert a paper-drawn UI wireframe into HTML/CSS output? /!/ I would like to be able to input an -----> image !!!  file with a hand-drawn UI-wireframe with multiple predefined components (button, title, paragraph) but also more complex elements like a list and a progress bar. \n\nThe output would be a HTML file with CSS styling. Of course I already know what HTML elements to expect and the CSS classes that apply. \n\nWhat would be the best way to tackle this problem? \n\n&amp;#x200B;\n\nI am quite new to Python/ML, but somewhat experienced with developing in general. All help is welcome, thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gaeydq/best_way_to_convert_a_paperdrawn_ui_wireframe/"}, {"autor": "codegeass30", "date": "2020-04-29 14:09:34", "content": "-----> Image !!!  Compression using different ML Techniques /!/ https://heartbeat.fritz.ai/image-compression-using-different-machine-learning-techniques-5787c88515f8", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gaa52a/image_compression_using_different_ml_techniques/"}, {"autor": "C_morling", "date": "2020-04-29 10:53:51", "content": "Is there a online data set for hadnwritten algebreic equations? /!/ Hi, I'm doing a project where I'm going to input a -----> image !!!  of algebreic equations into the neural network. The ouput is obviously the network trying to figure out what the equation is. I'm new to machine learning, and I am wondering if there is already test data for this kind of exercise or if I will have to draw all of the test data myself.   \n\n\nI'd like the equations to include addition, subtraction, multiplication, division, radicals, roots, brackets, radicals, logarithmics and 2 variable system of equations.   \n\n\nAny help would be appreciated to find a data set.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ga7aky/is_there_a_online_data_set_for_hadnwritten/"}, {"autor": "MigorRortis96", "date": "2020-09-30 14:58:48", "content": "For my first python ML project I would like to create an algo that determines whether or not what I'm wearing looks nice /!/ I realize this is quite a large project, but this is keeping up with tradition for me.  The only way I learn things is to start super complex projects and painfully work my way upwards from there (not sure exactly why).\n\nI figured this algo would use color theory to determine if the colors match first, it would then be trained to distinguish nice clothes from not nice clothes (remember this is all subjective so I will feed it images that I personally like and dislike). In the end it would be connected to a -----> camera !!!  in my room and it will either tell me to go change or if I can leave the house.\n\nHow would you go about doing this personally? Im not sure how realistic this algo is or how complex it would be. Let me know if I am being crazy.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j2npl9/for_my_first_python_ml_project_i_would_like_to/"}, {"autor": "QuasiEvil", "date": "2020-09-30 02:12:45", "content": "Running your training data through your model to find worst-case examples? /!/ Often times after I train a model, I like to run my entire dataset through it (train+test+validation) in order to find a few worst-case examples, which I find often provides insights into issues with the labels and data quality - \"no wonder it did so bad on this -----> image !!! , it was full of 0s/NaNs/poorly labelled/etc\"\n\nAnyway, is there a common name for this inspection process?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j2dbni/running_your_training_data_through_your_model_to/"}, {"autor": "TrackLabs", "date": "2020-09-29 13:24:14", "content": "Keras Sequential model expecting shape, getting exactly that shape, but complaining about shape? (Deep Q Learning) /!/ I am trying to make a Snake AI, deep Q learning, but I am running into shape errors.  The model expects a shape of (700, 700), which is 700x700 pixels of the game running in pygame.  I am extracting that -----> image !!!  with pygames function \n\n    pygame.surfarray.array2d\n\nWhen printing that shape it is (700, 700). Yet, when I run it I get this  \n\n    ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [None, 700, 700]\n\nSo..what could I do wrong?\n\nAlso I am using keras as library, and this is the model thats supposed to play the game. \n\n \n\n    def create_model(self):\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 model\u00a0=\u00a0Sequential()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0model.add(Conv2D(245,\u00a0(3,3),\u00a0input_shape=(1,\u00a01,\u00a0SCREEN_SIZE,\u00a0SCREEN_SIZE)))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0model.add(Activation(\"relu\"))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0model.add(MaxPooling2D(2,\u00a02))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0model.add(Dropout(0.2))\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0model.add(Conv2D(245,\u00a0(3,3)))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0model.add(Activation(\"relu\"))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0model.add(MaxPooling2D(2,\u00a02))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0model.add(Dropout(0.2))\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0model.add(Flatten())\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0model.add(Dense(32))\n        #\u00a04\u00a0because\u00a0we\u00a0have\u00a04\u00a0actions\u00a0that\u00a0are\u00a0possible\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0model.add(Dense(4,\u00a0activation=\"linear\"))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0model.summary()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0model.compile(loss=\"mse\",\u00a0optimizer=Adam(lr=0.001),\u00a0metrics=[\"accuracy\"])\n     return\u00a0model", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j1z4iy/keras_sequential_model_expecting_shape_getting/"}, {"autor": "desmap", "date": "2020-09-29 08:39:42", "content": "Ok, NER was easy but how do I get quotes and attributions extracted? /!/ Tinkered an hour around and found spaCy's Docker -----> image !!!  which is great and fast for simple named entity extraction. \n\nHow do I now get quotes and attributions extracted? So no just entities but quotes and who said them? I know that Standford's CoreNLP can do this but it's slower and doesn't have an official Docker distribution (the biggest one is 5 years old).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j1vezo/ok_ner_was_easy_but_how_do_i_get_quotes_and/"}, {"autor": "043270", "date": "2020-09-29 05:25:09", "content": "Help needed getting started with a neural network. /!/ Hi all,\n\nI am interesting in building a simple neural network that can be used to predict values based on a large data set.\n\nThe first thought that I had was to look into regression methods, but I would really like to try and use neural networks so that I get a better understanding.\n\nThe -----> image !!!  shows a sample set of some of the data I am looking at. I would like to be able to predict values where x &lt; -5.\n\nIs anyone able to give me a good starting place to begin, or some resources that deal with this kind of problem? I have found an abundance of material, but I am having trouble narrowing it down to data problems like this.\n\nAny help would be great.\n\nhttps://preview.redd.it/tpebyjfwt0q51.png?width=529&amp;format=png&amp;auto=webp&amp;s=27220b84540b23dc8c2ef4189844cc3a8b6fff8d", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j1t461/help_needed_getting_started_with_a_neural_network/"}, {"autor": "spmallick", "date": "2020-09-28 19:45:28", "content": "OpenCV Threshold ( Python, C++ ) /!/ Sometimes the easiest solution is the most suitable one. This is especially true in the Computer Vision domain. Thresholding is one of the most commonly used -----> image !!!  processing techniques and despite its ease of use, it has proven to be highly effective even in advanced computer vision problems.  \n\n\nIn this blog, we will explore different thresholding techniques and implement them using OpenCV in Python and C++.  \n\n\n[https://www.learnopencv.com/opencv-threshold-python-cpp/](https://www.learnopencv.com/opencv-threshold-python-cpp/) \n\nhttps://preview.redd.it/8s50mzuvzxp51.jpg?width=1024&amp;format=pjpg&amp;auto=webp&amp;s=a8870f89afdfadf6628e175297047de41c6147b7", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j1jai7/opencv_threshold_python_c/"}, {"autor": "shivamsingha", "date": "2020-09-28 15:23:51", "content": "Am I doing Linear filtering right? /!/ I'm required to implement linear filter and convolution for assignment. My deadline is close, I'm not getting right answers (MCQ, so my output s don't match) and instructors are not being helpful. Please help me out. \n\nFor some of the questions I'm supposed to find the indices of min/max values in the results. There are multiple of these questions and for none of them am I getting answers.\n\nI can't find anything wrong in my code. So I have no idea what to do.\n\nI'm supposed to write only the for loop part. The rest was already there. I also have some instructions that I believe I've followed correctly.\n\n`result shape will be of size --&gt; (((W1\u2212F1+2P) / S) + 1) x (((W2\u2212F2+2P) / S) + 1), where 'P' is padding length S is stride length, if you don't know about them, don't worry, you will learn in upcoming lectures.For now we will use simplest setting P=0,S=1. See the next line.`\n\n```python\ndef linear_filter(-----> image !!! , filter_):\n    -----> image !!!  = np.array(-----> image !!! .convert('L')) \n    image_height, image_width = image.shape[0], image.shape[1]\n\n    filter_ = np.array(filter_.convert('L'))\n\n    filter_height, filter_width = filter_.shape[0], filter_.shape[1]\n    \n    result_height, result_width = (image_height - filter_height) + 1, (image_width - filter_width) + 1\n    result = np.array([[0 for j in range(result_width)] for i in range(result_height)])\n\n    for y in range(result_width):\n        for x in range(result_height):\n            result[x, y] = np.multiply(filter_, image[x: x + filter_height, y: y + filter_width]).sum()\n    \n    \n    return result\n\ndef convolution2D(image, kernel):\n    image = np.array(image.convert('L'))\n    image_height, image_width = image.shape[0], image.shape[1]\n\n    kernel = np.array(kernel.convert('L'))\n\n    kernel = np.flipud(np.fliplr(kernel))\n    kernelheight, kernelwidth = kernel.shape[0], kernel.shape[1]\n\n    result_height, result_width = (image_height - kernelheight) + 1, (image_width - kernelwidth) + 1\n    result = np.array([[0 for j in range(result_width)] for i in range(result_height)])\n\n    for y in range(result_width):\n        for x in range(result_height):\n            result[x, y] = np.multiply(kernel, image[x: x + kernelheight, y: y + kernelwidth]).sum()\n            \n    \n    return result\n```", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j1e0v7/am_i_doing_linear_filtering_right/"}, {"autor": "MichaelBaranov", "date": "2020-09-03 09:17:10", "content": "What do you think about no-code AI development /!/ A team of like-minded people and I are working on a no-code AI development platform. Our goal is to make deep learning technologies accessible for everyone, not just for machine learning engineers and big companies. \n\nWe've decided to start with computer vision tasks. \n\nThe workflow would look like the following:  \n1. Select task (-----> image !!!  classification, object detection, -----> image !!!  tagging).  \n2. Upload data (with annotation or annotate it in our platform).  \n3. Wait for a model to be trained automatically (about 10 minutes).  \n4. Integrate the created model into your product (through API, docker image, or through integration with other no-/low-code platforms).\n\nWhat do you think is there any future for no-code AI development platforms?  \nI'd really appreciate it if you could share your thoughts.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ilq3p7/what_do_you_think_about_nocode_ai_development/"}, {"autor": "torquemada90", "date": "2020-09-03 02:04:55", "content": "What's the difference between MS Azure Machine Learning Studio and other MS AI tools? /!/ I've been looking around the different machine learning tools from Microsoft and I'm a bit confused about all the products. I have used Microsoft LUIS for language processing and creating text-based prediction models. Overall pretty straight forward. I have also tried Machine Learning Studio which does what LUIS does and more but it's also beyond my comprehension.\n\nI have noticed that there are other tools like MS Computer Vision for -----> image !!!  processing. However. Machine Learning Studio also does image processing.\n\nWhat's the difference between MLS and the other separate tools? Would there be a reason to use one over the other?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ilkmzu/whats_the_difference_between_ms_azure_machine/"}, {"autor": "finite-difference", "date": "2020-09-02 15:29:39", "content": "Problem replicating ResNet results on ImageNet /!/ I am working on a project regarding robustness of neural networks. I use ResNet50 as a baseline both in a version with bottlenecks and without.\n\nSo far I have managed to get meaningful results while training on the CIFAR-10 dataset. To gather further data I decided to move to training on ImageNet, but I hit a problem.\n\nI am using [the keras implementation](https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/applications/resnet.py) of ResNet with TF1.15. To load the data I am using code from [this](https://github.com/jkjung-avt/keras_imagenet/) handy repository which utilizes tf.data.dataset for efficient handling of the dataset.\n\nI was unable to get the accuracy as presented in the original paper. I can only manage to get to \\~65-67% top-1 validation accuracy.  I obtain similar results when using either the augmentation methods as coded in the dataset loading repository (various color/saturation along with crop/resize augmentation) and the augmentation mentioned in the original ResNet paper (normalize pixel values + horizontal flip + resize smallest size randomly within the range 256-480 and crop a 224 x 224 window).  I am not using the PCA color augmentation as described in the AlexNet paper, but this kind of augmentation should not account for such a huge difference (\\~67% vs \\~77% in the paper).\n\nTo train the model I am using the hyperparameters from the [second ResNet paper](https://arxiv.org/abs/1603.05027) e.g. 90 epoch SGD with batch size 256, momentum 0.9 and weight decay set at 0.0001, the learning rate starts from 0.1 and is divided by 10 at 30 and 60 epochs. I also tried training for additional 10 epochs with learning rate further decreased, but this does not help. For validation I use the method from the paper which resizes the -----> image !!!  so that its smallest size is 256 px and then takes the central 224x224 crop.\n\nLooking online I have noticed that the TF implementations of ResNets have worse results than PyTorch ones. Sadly, I have to use TF since I need to calculate some Jacobians within the computational graph and I found no easy way to do this in PyTorch.\n\nI would welcome any suggestions or tips regarding this as my project is now stuck at what seems to be a reproduction issue. My own idea relies on regularization so I need to compare it against a baseline, but if my baseline results are way worse than what they should be any kind of comparison against it could be rightfully dismissed.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/il8xv0/problem_replicating_resnet_results_on_imagenet/"}, {"autor": "gr3atm4n", "date": "2020-09-02 00:03:55", "content": "Purpose of Convolution in CNN /!/ I don't understand why why need to filter the images in a convolutional neural network. Won't the pooling layer alone do a good enough job in extracting the features of the -----> image !!! . Or is the filtering done to diversify.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ikw3j5/purpose_of_convolution_in_cnn/"}, {"autor": "CoyoteSimple", "date": "2020-09-01 17:39:23", "content": "Do you pick the size of a CNN filters/kernels depending on the actual size of the objects you intend to detect/analyse? /!/ Hello, \n\nI was reading about some famous CNN architectures such as AlexNet and I was wondering if the size of kernels was usually related to the size of faces, birds, cars, etc.. on a typical -----> image !!! ?\n\nI understand that for middle layers this might be different...Anyways... \n\nWhen designing a new CNN is there a general rule of thumb for picking the \"first guess\" size of a kernel?\n\nI hope my question makes sense.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ikowj9/do_you_pick_the_size_of_a_cnn_filterskernels/"}, {"autor": "sassysalmnder", "date": "2020-09-01 07:49:16", "content": "Implementation of Neural Style Transfer Algorithm with ResNet Model /!/ Hi! I am trying to implement NST using my custom ResNet50 architecture model which I have trained for a week using the Cifar 100 dataset. \n\nIn the process of implementing NST, we usually pick layers from our pre trained model to get the content and style representation of the -----> image !!! . But  I am not sure which shallow and deep layers to pick from my model which will help my cause.\n\nAny help would be appreciated !", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ikg58d/implementation_of_neural_style_transfer_algorithm/"}, {"autor": "CkmCpvis", "date": "2020-08-31 18:41:15", "content": "Rain Detection /!/ I am trying to make program that can determine if it has been raining from images alone. I have played with -----> image !!!  pre-processing like the photo below where the left side includes all moving objects (rain/cars) and the right side is the same -----> image !!!  with the large clusters (cars/other) removed:\n\nhttps://preview.redd.it/lgb39pgvsdk51.png?width=3782&amp;format=png&amp;auto=webp&amp;s=4fa817e9c187e1530250f32cb8b7494e94134d58\n\nThis worked well, unfortunately it turns out the cameras I will be using are potatoes (See below):\n\nhttps://preview.redd.it/7k1i6vicqdk51.png?width=1374&amp;format=png&amp;auto=webp&amp;s=94ada1a28227a45c2808e2dd8efa70656f676bf9\n\nhttps://preview.redd.it/02efod2eqdk51.png?width=1425&amp;format=png&amp;auto=webp&amp;s=d3245197bb26c7f758f9a950152c36cc98f7410c\n\nhttps://preview.redd.it/56dkfj8gqdk51.png?width=1433&amp;format=png&amp;auto=webp&amp;s=0b18d9ff91d6be78cb8ddf45ca7bedee0c8f39ad\n\nhow should I generalize?\n\nlabel the sky types (95% show the sky)?\n\nlabel head lights and determine if they are on or off (most headlights are automatic and turn on even it over cast but we could do like percent of headlights on, this could only be used as supporting evidence)?\n\nroad reflectance (because the road gets shinny lol)\n\ntime of day vs brightness?\n\n&amp;#x200B;\n\nor maybe something to do with noticing how the image gets hazy when it rains?\n\n&amp;#x200B;\n\na decision tree that combines any of the above?\n\n&amp;#x200B;\n\nwhat do you think?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ik3ttf/rain_detection/"}, {"autor": "guoyande", "date": "2020-10-08 09:45:58", "content": "[P]Open Source Vearch Vector Database V3.2.0 is officially released /!/ # Open Source Vearch Vector Database V3.2.0 is officially released\n\n&gt; Introduction:  [vearch]( https://github.com/vearch/vearch) is a distributed vector search system, which can be used to store and calculate massive feature vectors and provide basic system support and guarantee for vector retrieval in the AI field. The system can be widely used in -----> image !!! , audio, video, natural language processing and other machine learning fields.\n&gt;\n&gt; This article focuses on the Vearch V3.2.0 update content. If you are new to Vearch, you can see more documentation on Vearch here [wiki](https://github.com/vearch/vearch/wiki). In addition, Vearch one-click installation script is provided, such as [Baidu network disk](https://pan.baidu.com/s/1LyFNtRuUSrH9TmMUY91oTw ), extraction code: 6u4b, [Google cloud disk](https://drive.google.com/drive/folders/1w8KzXdj612rWOxIbZduo5gxNoBAzba7I?usp=sharing). Refer to [Binaryinstall.md](https://github.com/vearch/vearch/blob/master/docs/BinaryInstallation.md) for instructions on how to use the script.\n\n\n\n**V3.2.0 is a major update to the original version. The new version has richer API, better performance, and more stable services.**\n\n- **Richer API**\n\n  Added a batch query and delete interface based on multiple ID;\n\n  Add an interface to get the specified field information for the specified ID;\n\n  Add an interface to query similar vectors by ID;\n\n  Add gRPC interface.\n\n- **Better performance**\n\n  Optimized the data transmission protocol between Router and PS;\n\n  Optimized the data format received by raft log;\n\n  Fragmented storage;\n\n  Optimized the memory footprint of vector and string fields.\n\n- **More stable service**\n\n  Support for independent etcd;\n\n  PS node supports automatic recovery and manual recovery;\n\n  Support a variety of storage methods to ensure data security.\n\n- **Convenient index customization**\n\n  Gamma engine index interfaces become more abstract and simpler. Users can more easily customize the index to the Gamma engine, which allows them to better customize the development.\n\n\n\n## **Richer API**\n\n### Added a batch query and delete interface based on multiple ID;\n\nBefore V3.2.0 only supports single record query and delete the results. If multiple specified ID need to be deleted, loop traversal must be used. V3.2.0 starts to support batch query and delete the results, the specific usage is as follows:\n\n```shell\n# Get all field information of a single ID\ncurl -XGET 127.0.0.1:9001/your-db-name/your-space-name/id1\n\n# Get all field information of multiple ID\ncurl -XGET 127.0.0.1:9001/your-db-name/your-space-name /id1,id2,id3\n\n# Delete a single ID\ncurl -XGET 127.0.0.1:9001/your-db-name/your-space-name/id1\n\n# Delete multiple ID\ncurl -XGET 127.0.0.1:9001/your-db-name/your-space-name/id1,id2,id3\n```\n\n### Gets the specified field information for the specified ID\n\n```shell\ncurl -H \"content-type: application/json\" -XPOST -d'\n{\n\t\"query\": {\n\t\t\"ids\": [\"id1\", \"id2\"],\n\t\t\"fields\": [\"your-field1\", \"your-field2\"]\n\t}\n}\n' 127.0.0.1:9001/your-db-name/your-space-name/_query_byids\n```\n\n### Query similar vectors by ID\n\nBased on user feedback, it takes two steps for the older version to query a similar vector for that ID based on the ID. (1) Search the original vector by ID; (2) Search similar vectors with the original vector. In order to make it easier for users to use, we added an interface to directly conduct similar search through ID, which is used in the following ways:\n\n```shell\ncurl -H \"content-type: application/json\" -XPOST -d'\n{\n\t\"size\": 50,\n\t\"query\": {\n\t\t\"ids\": [\"id1\"],\n\t\t\"sum\": [{\n\t\t\t\"field\": \"vector\",\n\t\t\t\"min_score\": 0.7\n\t\t}]\n\t}\n}\n' 127.0.0.1:9001/your-db-name/your-space-name/_query_byids_feature\n```\n\n### Add gRPC interface\n\nGRPC is a high-performance, general-purpose open source RPC framework developed by Google for mobile applications and designed based on the HTTP/2 protocol standard. It is developed based on the ProtoBuf(Protocol Buffers) serialization Protocol and supports a number of development languages. V3.2.0 has  new gRPC interface that allows multiple languages to call Vearch over TCP. TCP calls not only reduce the overhead of establishing a connection but also reduce the cost of JSON serialization and deserialization in the HTTP interface.\nThe use cases are [grpc_client_test.go](https://github.com/vearch/vearch/blob/master/test/grpc_client_test.go). Clients for GO, Java, C, and Python will be provided later.\n\n### Modifications to the build table interface\n\n**max_size removed:**  Before v3.2.0, you need to set `max_size` to apply for the memory required by the system at one time. This feature is removed after v3.2.0, and the memory will increase as the number of inserted records increases. **Caution**: Users should pay attention to the consumption of memory when using it. When the memory is insufficient, the system may crash!\n\n**metric_type removed:**  Before v3.2.0, you need to set the `metric_type` to `InnerProduct` or `L2` when building the table. In the v3.2.0 version, this parameter is specified when moving to search. Specific reference [search document by query and vector for GPU](https://github.com/vearch/vearch/blob/master/docs/APILowLevel.md#search-document-by-query-and-vector-for-ivfpq).\n\n**store_type modifications:**  The value of `store_type` of v3.2.0 version has a new `MemoryOnly` type on top of the original `MMap` and `RocksDB`.\n\n\n\n## Better performance\n\n### Optimized the data transmission protocol between Router and PS;\n\nBefore v3.2.0, router and ps used JSON data protocol for interaction. This greatly affects the speed of interaction between router and ps and results in low service throughput. Therefore, in v3.2.0 version, we use protobuf instead of JSON, which greatly improves the throughput of ps and router, and increases the speed of insertion and search.\n\n### Optimized the data format received by raft log\n\nBefore v3.2.0, raft received a JSON serialized byte array. This has two disadvantages:\n\n1. The byte sequence after JSON serialization is relatively large and takes up space;\n2. After ps receives the logs submitted by raft, it is time-consuming to deserialize sequentially;\n\nTherefore, in v3.2.0, we process and serialize the request before raft. Reduce the operation of data in the period after the raft submits the log and before entering the Gamma engine. In this way, the throughput between PS and Gamma engines can be improved. Greatly improve the performance of data insertion and search.\n\n### Fragmented storage\n\nVector and forward fields use a segmented storage scheme. This avoids fixing the maximum capacity (max size) during initialization, resulting in subsequent failure to dynamically expand. Usually, the data volume of the business grows slowly. It may be difficult for users to estimate the maximum capacity. At the same time, if a large amount of memory is applied for initialization, most of the memory cannot be used for a long period of time. This is a waste of memory space.\n\nThe solution of segmented continuous storage is mainly to segment the memory space. Each segment stores a fixed number of original vectors or forward fields. There is still a contiguous memory in the segment. During initialization, only one segment 0 is created. When segment 0 is full, segment 1 is allocated, and segments are sequentially added as needed. In this way, a solution to allocate memory on demand is realized.\n\n### Optimized memory usage for vector and string fields\n\nUse zfp to compress the original vector. Use zstd to compress variable-length forward fields. This can greatly reduce the memory and disk usage. Especially for the deployment of docker that uses limited memory and disk resources in the k8s cluster is more friendly.\n\n### Performance improvement\n\nAfter internal testing, the v3.2.0 version has a greater performance improvement compared to the v3.1.0 version. The speed of inserting data has increased by 35%. Search speed has also improved. Through flexible configuration and use of retrieval models such as ivfpq or hnsw, this version can meet the performance requirements of most business scenarios.\n\n\n\n## **More stable service**\n\n### Support for independent etcd\n\nIn the previous version, etcd and master are bound and then together, the disadvantages of doing so are\n\n1. Some users may have their own etcd cluster but there is no way to use and manage it uniformly.\n2. The master is restricted by etcd and cannot be deployed with a domain name.\n\n3. The master IP is permanently written in the configuration file. Subsequent increase or decrease of nodes requires a restart of the cluster to take effect. The master also does not use the failure recovery function and the mechanism of dynamically adding nodes.\n\n### Automatic recovery and manual recovery of PS node failure.\n\n**Automatic recovery**\n\n\u200b    Just add `auto_recover_ps = true` in config.toml. For details, please refer to [config.toml](https://github.com/vearch/vearch/blob/master/config/config.toml.example#L17).\n\n**Manual recovery**\n\n```shell\n# Query the IP and nodeid of the failed node\ncurl http://127.0.0.1:8817/schedule/fail_server/list\n# Delete the failed node nodeid1\ncurl -XPOST -H \"content-type:application/json\" -d '{\n\t\"fail_node_id\":1\n}' http://127.0.0.1:8817/meta/remove_server\n# Restart a PS and replace the failed node with that node\ncurl -XPOST -H \"content-type:application/json\" -d '{\n\t\"fail_node_addr\":\u201d192.168.1.1\",\n\t\"new_node_addr\":\u201d192.168.1.2\"\n}'http://127.0.0.1:8817/schedule/recover_server\n```\n\n### Support a variety of storage methods to ensure data security\n\nVearch uses the persistence of the raft log and gamma engine to ensure data security. Historical data will be persisted to disk by calling Gamma's dump interface. Real-time data will be continuously recorded through the log of raft. When Vearch restarts, it will first load the historical data persisted to disk. Then, through raft, the non persistent data is extracted from the log and loaded into the engine again. This avoids data loss caused by no persistence.\n\n\n\n## Convenient index customization\n\nThe Gamma engine index interface has become more abstract, simple and clear. Users can easily integrate custom indexes into the Gamma engine. Can better customize development.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j7a7ho/popen_source_vearch_vector_database_v320_is/"}, {"autor": "BoldSlogan", "date": "2020-10-07 16:23:59", "content": "I built a tool using machine learning and the RFID in phones to validate your passport /!/ **Feel free to use an invalid passport and/or use the app on airplane mode so that you feel comfortable.**\n\nI used the phone's RFID reader and machine learning to verify the personal data on passports using the secure chip embedded inside.\n\nThe app works by first using the -----> camera !!!  to scan the passport\u2019s data page, a step that is needed because the key used for reading the embedded chip is constructed out of the visible printed passport data. Then it will read and verify the embedded chip and display the extracted information.\n\nHere is a 30 second demo: https://www.youtube.com/watch?v=VTdpOcG1NSw\n\nI am looking for users/testers with different passports to try it out. I wanted to see if my weekend hackathon project actually works or if it is only my passport and my friends that works.\n\nWould love to hear your thoughts! \ud83d\ude0a\n\nThe app is available for both iOS and Android and requires an NFC-enabled device (most modern Android devices and all iPhones starting with the iPhone 7 are NFC-enabled). You will also need a biometric passport (sometimes called electronic passport) that you can read. Most modern passports issued today are biometric passports, and you can verify this by looking for a microchip icon usually printed on the passport cover. \n\nhttps://apps.apple.com/us/app/biometric-passport-reader/id1510585886\nhttps://play.google.com/store/apps/details?id=app.iris\"", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j6tyd7/i_built_a_tool_using_machine_learning_and_the/"}, {"autor": "HunterDude54", "date": "2020-10-07 05:21:02", "content": "Higher detail and resolution -----> photo !!!  extracted from a movie? /!/ I want to capture a better image from a movie than simply capturing single frames.  My question is whether there is a tool to improve the single movie frame that you capture in the first place.  For example, if I were to select a frame to capture, then the AI would use adjacent frames, on each side (an image stack?), to improve the resolution and accuracy of the selected one.  Providing a much higher quality jpg/image using the movie itself as the template. I'm not sure this has been done, and google is not my friend in this.  Is this called deconvolution?  Is there an AI or ML app or tool for this?\n\nI am well aware that you can now take a movie still or frame and upscale, denoise or whatever you want to do with AI or ML.  (Or take a movie and improve detail).  My question is sort of the reverse direction, using the additional data in the movie itself.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j6l6fp/higher_detail_and_resolution_photo_extracted_from/"}, {"autor": "74throwaway", "date": "2020-10-07 01:19:17", "content": "Does GPU provide speedup for NN inference in C++? /!/ Lets say I have a CNN for semantic segmentation on images that is trained in Keras or Tensorflow in Python. If I then use it for predictions on test -----> image !!! s, it takes roughly 2-5 sec to produce a prediction per test -----> image !!! \n\nLet's say I have a separate preexisting C++ code and I want to include that pretrained model in C++ and feed it test -----> image !!! s. I want it to predict a test image ideally in less than a second \n\nWould the inference of the pretrained CNN in C++ run much faster using a GPU instead of CPU? If so, would it run noticeably faster if that GPU had more cores/Tensor cores? For example, if it used one of the new GPUs such as the NVIDIA 3080 GPU instead of an older one like a 1080Ti?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j6hshk/does_gpu_provide_speedup_for_nn_inference_in_c/"}, {"autor": "lwilson747", "date": "2020-10-06 21:47:06", "content": "Deep Learning with TensorFlow 2.0 Certification Training /!/ Machine Learning colleagues, become certified as a Deep Learning Engineer. This Deep Learning with TensorFlow 2.0 Certification Training is curated with the help of experienced industry professionals per the latest requirements &amp; demands. You will master popular algorithms like CNN, RCNN, RNN, LSTM, RBM using TensorFlow 2.0 in Python. Work on real-time projects like Emotion and Gender Detection, Auto Image Captioning using CNN, LSTM, and many more. Skill-based training modules address: 1) Introduction to Deep Learning, 2) Getting Started with TensorFlow 2, 3) Convolutional Neural Network, 4) Regional CNN, 5) Boltzmann Machine &amp; Autoencoder, 6) Generative Adversarial Network(GAN), 7) Emotion and Gender Detection, 8) Introduction RNN and GRU, 9 ) LSTM,  and 10) Auto -----> Image !!!  Captioning Using CNN LSTM.\n\nRegister today. Visit: https://fxo.co/9PYj \n\nMuch career success, Lawrence E. Wilson -  Artificial Intelligence Academy (AIA)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j6ecs8/deep_learning_with_tensorflow_20_certification/"}, {"autor": "thijser2", "date": "2020-10-06 12:12:57", "content": "Anyone got any good tutorials for how to train an existing pytorch implementation onto multiple azure cloud vm's? /!/ So I have a pytorch implementation of a heavily modified pix2pix cyclegann (similar to [this](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)). However due to circumstances this version has to be both very accurate and has a very large amount of data available to it(100 000+ -----> image !!!  pairs). As such the training time is exceedingly large and I'm looking for a way to speed up this training, so the logical solution is of course \"use more machines\". So I'm now looking for advice on how to best approach this problem. I have access to a lot of azure related stuff (data exists in a azure blob etc.) so that was the direction I was leaning to, anyone got any tips/tutorials that might help?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j63xer/anyone_got_any_good_tutorials_for_how_to_train_an/"}, {"autor": "pterry0404", "date": "2020-10-06 12:05:57", "content": "-----> Image !!!  Class Issues /!/ Fairly new to ML.\n\nI am making a single class categorical image classifier for wildlife cameras. Most of my images sort fine except for \"false trigger\" photos, i.e. photos with no animals in them. For some reason, my model selects to put them with Deer photos.\n\nObviously, my model is picking up on some similarity between the two photo groups and learning \"bad\" patterns. To correct this issue I think I should:\n1. Add more \"false trigger\" data to the training set\n2. Double the deer training set with augmented data, such as shifts, flips, etc. so the model hopefully picks up different features to recognize deer.\n\nDo you all think that would be an adequate approach/place to start?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j63u1g/image_class_issues/"}, {"autor": "ali-nawaz14", "date": "2020-10-06 09:55:07", "content": "-----> Image !!!  Preprocessing /!/ Is image processing process is necessary for deep learning projects?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j62b1y/image_preprocessing/"}, {"autor": "renscy", "date": "2020-10-06 09:31:19", "content": "Ways to improve upon the accuracy of a pre-trained classification model trained on an imbalanced dataset? /!/ For reference, here's what I dug up so far, mostly coming from the first website.\n\n* [https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)\n* [https://machinelearningmastery.com/imbalanced-classification-with-python/](https://machinelearningmastery.com/imbalanced-classification-with-python/)\n* [https://www.reddit.com/r/MachineLearning/comments/12evgi/classification\\_when\\_80\\_of\\_my\\_training\\_set\\_is\\_of/](https://www.reddit.com/r/MachineLearning/comments/12evgi/classification_when_80_of_my_training_set_is_of/)\n* [https://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set](https://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set)\n\nHowever, the article itself is dated way back in 2015, and there might be advances in approaching this problem since then. Also, maybe there might be a solution unique to this situation.\n\nAlso for context, the classification model used is inception-v3, and the dataset is *really* small, with a *lot* of classes (\\~100) altogether. The class with the largest dataset is around only 100 images, while the smallest only amounts to 10.\n\nMy first idea without googling would be to even out the differences in size: get a number n between the highest and the lowest number of -----> image !!! s, then augment the existing data with some -----> image !!!  operations (flip x, flip y, rotate) applied randomly on random -----> image !!! s when training. Then, randomly pick images from the larger dataset to be used in training, maybe. The number is just arbitrary, say, 50, and datasets and classes that would require &gt;50% data augmentation to reach this number will be removed instead, until more data comes. This is just in theory though, still not sure how it would work in the context of Tensorflow or if it would work at all lol just a random shot-in-the-dark thought at most.\n\nWould love to hear your thoughts regarding this.\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j622gk/ways_to_improve_upon_the_accuracy_of_a_pretrained/"}, {"autor": "andrew_BD", "date": "2020-10-05 20:50:48", "content": "Deep Learning Final Project /!/ Hello there, let me start but saying that I'm sorry for my English.\nI have a project for my final year of my CS uni. I wanna implement a deep learning project and I prefer computer vision problems. I search for some CV project and found those projects write below, but any suggestions will help a lot ( wanna say, I have from now exactly 8 month's to finish the project and a documentation of 50-60 pages for it, enough time but I'm looking for intermediar level difficulty). \n\nIf someone have past experience with those project, let me know how difficult overall can be the specific project and what kind of difficulties did you have.\n\nThank you so much for you time to response or reading the post and have a wonderful day! ( don't forget to smile :) )\n\nSome ideas:\n\n* old -----> photo !!!  restaoration\n\n* image colorization [example](https://richzhang.github.io/colorization/resources/colorful_eccv2016.pdf) (old / b&amp;w photos to colored one)\n\n* semantic Image synthesis with spatially-adaptive normalization[example link ](https://arxiv.org/abs/1903.07291) ( draw the target classes with a brush and convert the classes to photorealistic photos)\n\n* 3D Photograhy[3D Photography](https://arxiv.org/abs/2004.04727) (2D mono photo to 3D moving photo)\n\n* data augmentation generator\n\n* text to image (convert text to images)\n\n* super resolution ( low res to high res photos)\n\n* night vision[night vision](https://cchen156.github.io/paper/18CVPR_SID.pdf) (denoising night photo / like night mode)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j5rlyf/deep_learning_final_project/"}, {"autor": "hedgerowbro", "date": "2020-07-28 16:15:13", "content": "Anyone familiar with the various methods of -----> image !!!  detection/classification? /!/ I am trying to develop a program that can identify objects within an image and match them against a database of said objects.  These objects are uniform and will only vary from their database counterparts based on the quality/angle/distance/lighting of the image being analyzed.\n\nThe image will only contain the objects in question, so no noise.  The number of objects in the database, however, will be huge.  So brute-force iterating over each object in the database to find a match in the image would take quite a while.\n\nI'm thinking that template matching might be the right move here, but I would like to know if there is already a method or an open-source/paid model available that can tackle this kind of problem?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hzhint/anyone_familiar_with_the_various_methods_of_image/"}, {"autor": "dathoangnd", "date": "2020-07-28 15:50:22", "content": "Can someone explain the bx, by, bw, bw in YOLOv3? /!/ What confused me is whether they are counted on grid cell unit (e.g. grid size is 13x13 and bw=9) or on pixel unit (e.g. input -----> image !!!  size is 416x416px and bw=300px).\n\n&amp;#x200B;\n\nhttps://preview.redd.it/kpqhtktfdmd51.png?width=613&amp;format=png&amp;auto=webp&amp;s=c5c046b08d79118d5ee59e68df79f635198606c5", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hzh2c7/can_someone_explain_the_bx_by_bw_bw_in_yolov3/"}, {"autor": "k4rth33k", "date": "2020-07-28 06:05:50", "content": "Solo dance videos dataset /!/ I am looking for a dataset with videos of people dancing (only one person in the frame). The only constraint is that the -----> camera !!!  should be fixed (tripod). Is there any such dataset out there that can help me with this or should I make my own dataset?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hz9ad8/solo_dance_videos_dataset/"}, {"autor": "Shensmobile", "date": "2020-07-27 20:30:44", "content": "NLP Multi-instance classification? /!/ Hi everyone,\n\nTrying to mentally solve a problem that I haven't found any definitive answers for with my Google-fu yet.  I haven't even started to write code for this yet because I'm trying to formulate the best approach.  I'm trying to extract information from text that can occur more than once.  Specifically, out of a work report, I'm trying to extract the likelihood of a procedure being done multiple times.  \n\nAt first, it was enough to classify the document to figure out which procedure the report is generally talking about, and then use a pre-trained NER model to pull context out and use logic to determine the number of times a procedure was done.  This is OK but I've found that NER is generally better at detecting nouns than it is at detecting verbs (with the various tenses) that describe the same thing.\n\nI was thinking about classification for -----> image !!! s, and how if I wanted to classify multiple instances in an -----> image !!! , I would use bounding boxes, and count boxes.  Is there a similar approach that I could use for NLP?  Is there something I can do with the attention modules in a transformer that can tell me what are the most relevant parts of the text I'm classifying?  I'm a relative newbie at this, I have done the tutorials on the PyTorch site for image recognition as well as used the HuggingFace transformers library and FastAI so far for NLP.  I don't think I have the chops to tackle anything too SOTA (since I rely so much on Reddit/Stack Overflow for guidance), but was wondering what are some keywords I could Google, or principals/topics I can read up on.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hz07wf/nlp_multiinstance_classification/"}, {"autor": "ank_itsharma", "date": "2020-07-27 05:55:41", "content": "[D] - Person detection using Inception model /!/ Hi, I am using the Inception model for person class detection. So far for a single -----> image !!! , I am getting my detection outcome and bounding box surrounding that person correctly. But, when I pass multiple images at the same time, i.e. in parallel I am getting a segmentation fault and detection error as it isn't  recognising the person and my programs exits.\n\nIs there any specific reason this happens for parallel requests?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hymmgd/d_person_detection_using_inception_model/"}, {"autor": "SuccMyStrangerThings", "date": "2020-09-21 17:15:52", "content": "Val and train loss exactly the same and don't change /!/ Trying to implement X ray -----> image !!!  classification (2 classes) wit VGG16. \n\n`from\u00a0tensorflow.keras.optimizers\u00a0import\u00a0SGD`  \n`opt\u00a0=\u00a0SGD(lr\u00a0=\u00a00.001)`  \n`inputs\u00a0=\u00a0tf.keras.layers.Input((224,\u00a0224,\u00a03))`  \n`c1\u00a0=\u00a0tf.keras.layers.Conv2D(64,\u00a0(3,\u00a03),\u00a0activation\u00a0=\u00a0'relu',\u00a0padding\u00a0=\u00a0'same')(inputs)`  \n`c2\u00a0=\u00a0tf.keras.layers.Conv2D(64,\u00a0(3,\u00a03),\u00a0activation\u00a0=\u00a0'relu',\u00a0padding\u00a0=\u00a0'same')(c1)`  \n`m1\u00a0=\u00a0tf.keras.layers.MaxPooling2D(pool_size\u00a0=\u00a0(2,2),\u00a0strides\u00a0=\u00a0(2,2))(c2)`  \n`c3\u00a0=\u00a0tf.keras.layers.Conv2D(128,\u00a0(3,\u00a03),\u00a0activation\u00a0=\u00a0'relu',\u00a0padding\u00a0=\u00a0'same')(m1)`  \n`c4\u00a0=\u00a0tf.keras.layers.Conv2D(128,\u00a0(3,\u00a03),\u00a0activation\u00a0=\u00a0'relu',\u00a0padding\u00a0=\u00a0'same')(c3)`  \n`m2\u00a0=\u00a0tf.keras.layers.MaxPooling2D(pool_size\u00a0=\u00a0(2,2),\u00a0strides\u00a0=\u00a0(2,2))(c4)`  \n`c5\u00a0=\u00a0tf.keras.layers.Conv2D(256,\u00a0(3,\u00a03),\u00a0activation\u00a0=\u00a0'relu',\u00a0padding\u00a0=\u00a0'same')(m2)`  \n`c6\u00a0=\u00a0tf.keras.layers.Conv2D(256,\u00a0(3,\u00a03),\u00a0activation\u00a0=\u00a0'relu',\u00a0padding\u00a0=\u00a0'same')(c5)`  \n`m3\u00a0=\u00a0tf.keras.layers.MaxPooling2D(pool_size\u00a0=\u00a0(2,2),\u00a0strides\u00a0=\u00a0(2,2))(c6)`  \n`c7\u00a0=\u00a0tf.keras.layers.Conv2D(512,\u00a0(3,\u00a03),\u00a0activation\u00a0=\u00a0'relu',\u00a0padding\u00a0=\u00a0'same')(m3)`  \n`c8\u00a0=\u00a0tf.keras.layers.Conv2D(512,\u00a0(3,\u00a03),\u00a0activation\u00a0=\u00a0'relu',\u00a0padding\u00a0=\u00a0'same')(c7)`  \n`m4\u00a0=\u00a0tf.keras.layers.MaxPooling2D(pool_size\u00a0=\u00a0(2,2),\u00a0strides\u00a0=\u00a0(2,2))(c8)`  \n`c9\u00a0=\u00a0tf.keras.layers.Conv2D(512,\u00a0(3,\u00a03),\u00a0\u00a0activation\u00a0=\u00a0'relu',\u00a0padding\u00a0=\u00a0'same')(m2)`  \n`c10\u00a0=\u00a0tf.keras.layers.Conv2D(512,\u00a0(3,\u00a03),\u00a0activation\u00a0=\u00a0'relu',\u00a0padding\u00a0=\u00a0'same')(c9)`  \n`m5\u00a0=\u00a0tf.keras.layers.MaxPooling2D(pool_size\u00a0=\u00a0(2,2),\u00a0strides\u00a0=\u00a0(2,2))(c10)`  \n`f1\u00a0=\u00a0tf.keras.layers.Flatten()(m5)`  \n`d1\u00a0=\u00a0tf.keras.layers.Dense(units\u00a0=\u00a04096,\u00a0activation\u00a0=\u00a0'relu')(f1)`  \n`d2\u00a0=\u00a0tf.keras.layers.Dense(units\u00a0=\u00a04096,\u00a0activation\u00a0=\u00a0'relu')(d1)`  \n`d3\u00a0=\u00a0tf.keras.layers.Dense(units\u00a0=\u00a01000,\u00a0activation\u00a0=\u00a0'relu')(d2)`  \n`output\u00a0=\u00a0tf.keras.layers.Dense(units\u00a0=\u00a02,\u00a0activation\u00a0=\u00a0'softmax')(d3)`  \n`vgg16\u00a0=\u00a0tf.keras.Model(inputs=[inputs],\u00a0outputs=[output])`  \n`vgg16.summary()`\n\n`vgg16.compile(optimizer\u00a0=\u00a0opt,\u00a0loss='binary_crossentropy',\u00a0metrics=['accuracy'])`  \n`history\u00a0=`\u00a0[`vgg16.fit`](https://vgg16.fit)`(x,\u00a0y,\u00a0validation_split=0.1,\u00a0batch_size=16,\u00a0epochs=25)`\n\nI'm using 551 images for class 1 and 551 images for class 2. So in all 551 + 551 images each of shape (224, 224, 3)\n\nOutput: \n\n**array(\\[\\[0.50063896, 0.49936107\\]\\], dtype=float32) . I GET THE SAME PROBABILITIES FOR EVER SINGLE IMAGE. LITERALLY NO DIFFERENCE. I TRIED USING ADAM and SGD WITH DIFFERENT rates.** \n\n&amp;#x200B;\n\n Epoch 1/25  2/31 \\[&gt;.............................\\] - ETA: 1s - loss: 0.6932 - accuracy: 0.8750WARNING:tensorflow:Callbacks method \\`on\\_train\\_batch\\_end\\` is slow compared to the batch time (batch time: 0.0240s vs \\`on\\_train\\_batch\\_end\\` time: 0.0490s). Check your callbacks. 31/31 \\[==============================\\] - 2s 79ms/step - loss: 0.6932 - accuracy: 0.8869 - val\\_loss: 0.6932 - val\\_accuracy: 0.9464\n\n Epoch 2/25 31/31 \\[==============================\\] - 2s 74ms/step - loss: 0.6932 - accuracy: 0.8848 - val\\_loss: 0.6932 - val\\_accuracy: 0.9464 \n\nEpoch 3/25 31/31 \\[==============================\\] - 2s 74ms/step - loss: 0.6932 - accuracy: 0.8828 - val\\_loss: 0.6932 - val\\_accuracy: 0.9464 \n\nEpoch 4/25 31/31 \\[==============================\\] - 2s 74ms/step - loss: 0.6932 - accuracy: 0.8707 - val\\_loss: 0.6932 - val\\_accuracy: 0.9464 \n\nEpoch 5/25 31/31 \\[==============================\\] - 2s 74ms/step - loss: 0.6932 - accuracy: 0.8606 - val\\_loss: 0.6932 - val\\_accuracy: 0.9464 \n\nand so on", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ix4p7o/val_and_train_loss_exactly_the_same_and_dont/"}, {"autor": "fnordstar", "date": "2020-09-21 13:03:44", "content": "-----> Image !!! -to-image translation: Advantage of using a GAN vs. UNet only (e.g. Pix2Pix)? /!/ I was wondering why pix2pix uses a GAN together with a UNet to perform image to image translation in a supervised setting (e.g. we have input/output image pairs). Wouldn't the natural choice here to use ONLY a Conv-Deconv model such as UNet and train it directly on input/output pairs?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ix0075/imagetoimage_translation_advantage_of_using_a_gan/"}, {"autor": "EdvardDashD", "date": "2020-09-20 19:44:08", "content": "New to deep learning, trying to understand how to approach a network I have in mind /!/ Hey all! I've been fascinated with machine/deep learning for years now, and am finally taking my first steps into this world. I want to take a stab at creating a stock trading AI, and came across [this fantastic article](https://towardsdatascience.com/aifortrading-2edd6fac689d) that outlines one approach. The goal of the network described in the article is to predict the price on a day-to-day basis, which seems like an obvious starting point. It uses a LSTM network as the generator in a GAN. The first -----> image !!!  in the article outlines how the approach is structured.\n\nThe thing is, I'm not very interested in predicting the stock price. My ideal system would instead output a \"conviction\" value for a variety of financial instruments. These would include holding cash, stocks, and options (both calls and puts, likely with a variety of strike prices and time horizons). A higher value would represent a stronger conviction that holding that financial instrument would be more profitable than not. The network would not be optimizing for raw numerical accuracy, but profitability. There would be non-machine learning based logic that translates the conviction values into actions (buy/sell/hold), with the outcome of those actions determining profitability.\n\nAn example of the conviction values it might return for a given day are as follows:\n\n* Cash: 20%\n* Stock: 50%\n* Calls: 25%\n* Puts: 10%\n\nIf I look at this, it tells me that the network thinks the stock is more likely to go up than not (stock and calls having higher percentages than cash and puts), but that it thinks there's enough of a chance it'll go down that buying some puts to hedge is worth their loss in value if it goes up instead. What to do with this information will depend on each person individually, but let's assume the action logic is pretty basic and allocates the funds proportionally based on the percentages. One note: the percentages don't have to add up to 100%. If it is 100% convinced the stock will go up, the conviction for both stock and calls would be 100%, while cash and puts would be 0%. In that case, with this super naive logic, it would split funds 50/50 between stocks and calls.\n\nThat leads me to my question: what would you use as the baseline-truth that the LSTM generator output gets compared to in the discriminator? With stock price it's obviously just the real stock price, but when we're talking about profitability across several financial instruments it's less so. My first thought is to use a 0/1 value based on whether or not holding that instrument through the next day was actually profitable, but it's important to me that the conviction value isn't just a binary YES/NO. I'm not familiar enough with GANs to know if it's possible to have it optimize towards an answer that doesn't necessarily match the baseline-truth it's being discriminated against. My gut reaction based on the little I know tells me it wouldn't be possible. I'm also not familiar enough with deep learning generally to know if another training methodology would be more appropriate in my situation.\n\nHow would you approach this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iwkp2l/new_to_deep_learning_trying_to_understand_how_to/"}, {"autor": "semprotanbayigonTM", "date": "2020-09-19 22:03:05", "content": "Libraries &amp; frameworks for -----> image !!!  recognition? /!/ There are TensorFlow, Fastai, Keras, PyTorch, etc but I don't know the differences and I don't know which ones I should pick. \n\nWhich tools/libraries/frameworks are the best for image recognition &amp; computer vision right now? I guess for the training it would rely on DL &amp; ANN and also do some PCA/LDA analysis.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iw1pdp/libraries_frameworks_for_image_recognition/"}, {"autor": "LeopardMiserable", "date": "2020-09-19 05:00:25", "content": "Any -----> image !!!  based(non lung/CT Scan) datasets for COVID-19? /!/ I am looking for datasets with chemical structures and the likes.\n\nTIA", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ivm73x/any_image_basednon_lungct_scan_datasets_for/"}, {"autor": "_try_again_later_", "date": "2020-08-27 09:43:54", "content": "Masks for segmentation /!/ Hi everyone.\n\nI've recently started trying to implement a model to perform -----> image !!!  segmentation for a specific task. I found that some similar tasks to the one I want use \"trimasks\" as the ground truth. One example is the The Oxford-IIIT Pet Dataset .\n\nMy question is the following:\n\nIs there any performance advantage in using these format instead of binary masks? Is the intention to create an ambiguous area, which may or may not be part of the targe? Or is its function to separate different instances of the target?\n\nThank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ihi3wi/masks_for_segmentation/"}, {"autor": "nobgamer", "date": "2020-08-26 18:57:02", "content": "Padding or resizing /!/ hi , im new to machine learning and i was doing some logistic regression by myself to make sure the material i took sticks \n\ni got dataset of flower types online , it consist of about 600 images of 5 type of flowers. however the images are all of different sizes. i searched a bit and found out about padding the -----> image !!!  with 0s or 1s so i coded a bit to find the max size and found it is 442x1024. while 442 is not that big , 1024 is way too big especially since  the average size is 271x375\n\nso the other choice  is resizing, i tried it and found that python stretch or compress the image and doesnt cut it\n\nso i was wonder what would happen if say i resize the whole dataset to say 250x250 or 271x375 , would the algorithm find it harder to learn since things are stretched ? or will it have no effect? or is padding is just the way to go and i need to find a way to optimize the data  ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ih4y8o/padding_or_resizing/"}, {"autor": "Lockonon2", "date": "2020-08-26 16:21:42", "content": "Deepfake fad going around /!/ There is this meme going around that basically deepfakes footage or pictures of people to a japanese singer:  [https://www.youtube.com/watch?v=SSJvgqKTBM8](https://www.youtube.com/watch?v=SSJvgqKTBM8) . It's kind of  interesting (and a bit terrifying), but I was wondering how these deepfakes (or deepfakes in general) could be made? I'm guessing they take one -----> photo !!!  of, for example, Rick Astley, and then take the footage of the japanese singer with all of its frames sliced, and I'm guessing the network applies itself with each frame and that -----> photo !!! ? Is there a way to do this with python?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ih1y80/deepfake_fad_going_around/"}, {"autor": "iHusk", "date": "2020-08-26 00:06:41", "content": "Struggling with Deep Learning with Python exercises /!/ Hello,\n\nI am going through PyTorch's book Deep Learning with Python and am stuck trying to adapt their simple thermometer model into the more complex wine model. [Here is a -----> picture !!!  of the question](https://imgur.com/a/lrvROJF). \n\n&amp;#x200B;\n\nMy understanding of the question is I need to manipulate the model parameters and raw data from the wine dataset. The original data set was very simple. It was a tensor size \\[11, 1\\] (My understanding this is 11 \"batches\" with 1 data point per batch) and we were looking for one output. With the wine dataset, there are 11 different inputs that are recorded. My tensor size is \\[2, 1, 11\\] (I am working with a stripped down dataset until I can get rid of this error). My understanding is that this is 2 batches, 1 channel, 11 different data points per batch. \n\n&amp;#x200B;\n\nHere is the sequential model for the thermometer examples:\n\n    seq_model = nn.Sequential(OrderedDict([\n        ('hidden_linear', nn.Linear(1, 8)),\n        ('hidden_activation', nn.Tanh()),\n        ('output_linear', nn.Linear(8, 1))\n    ]))\n\nAnd the model I have for the wine dataset:\n\n    seq_model = nn.Sequential(OrderedDict([\n        ('hidden_linear', nn.Linear(11, 15)),\n        ('hidden_activation', nn.Tanh()),\n        ('output_linear', nn.Linear(15, 1))\n    ]))\n\nI think this is where my error is but I could be wrong. I feel like I have tried every combination of parameters on the nn.Linear functions but I get a size mismatch error no matter what I do. With the code how it sits currently the error reads: \"**RuntimeError**: size mismatch, m1: \\[2 x 1\\], m2: \\[11 x 15\\] at ..\\\\aten\\\\src\\\\TH/generic/THTensorMath.cpp:41\"\n\nAs far as I understand, m1 is referring to my \"answers\" or the rating of the wine. Which is a tensor just containing two numbers on a scale 1-10. \n\nIf I change the parameters of 'output\\_linear' to nn.Linear(15, 2). I get a different error: \"**RuntimeError**: The size of tensor a (2) must match the size of tensor b (11) at non-singleton dimension 2\"\n\nI thought this had something to do with the tensor holding my data having too many dimensions. I tried doing a flatten and making \\[2, 1, 11\\] into \\[2, 11\\] but no luck. I am seriously lost and don't want to continue with the book until I really understand whats going on, but it has been like 3 days of off and on working on it and I am losing my drive.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/igo9rm/struggling_with_deep_learning_with_python/"}, {"autor": "aaqi2", "date": "2020-08-25 19:41:46", "content": "AlexNet Input Size: 256 or 224? /!/ Section 2 says -----> image !!! s were downsampled to 256\u00d7256\nSubsection 3.5 says the input -----> image !!!  is 224\u00d7224\n\nAm I missing something?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/igjg6t/alexnet_input_size_256_or_224/"}, {"autor": "Thomasedv", "date": "2020-08-25 18:07:58", "content": "Loading -----> image !!!  to Tensor using cv2 /!/ Hello, \n\nI'm trying to use some code, but for whatever reason, the code is broken because converting images to tensors creates just a jumble of colors. I've dealt with it before, but it seems like i deleted that code.... I however got so tired of trying to fix it, i replaced it with a simple `Image.open()` from pillow, and it worked out just fine. However in the interest of learning, can anyone tell me how to fix it. Only the last code \n\n    img_LR = cv2.imread(self.LR_env, LR_path) # png BGR image, lets ignore that we probably want it to be RGB at this moment, the original code does that.\n    \n    # This is the breaking line.\n    img_LR = torch.from_numpy(np.ascontiguousarray(np.transpose(img_LR, (2, 0, 1)))).float()\n\nI am quite sure i already solved this once before, but while doing this transpose action to go from (H, W, 3) to (3, H, W) in shape, it changes the entire image to a garbled mess. Perhaps i called .transpose() twice last time, or perhaps it was using something like .permute on the tensor, either way, i can't wrap my head around it.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ighke1/loading_image_to_tensor_using_cv2/"}, {"autor": "Theweekendstate", "date": "2020-08-25 15:19:43", "content": "Tracking fish! /!/ I'm working on a kind of fun project where I'm using ML to analyse fish passing in front of a -----> camera !!! . The fish pass through a squeeze so, while there can be (and usually are) multiple fish within a frame, they stream passed sequentially.\n\nI'm using standard ML image segmentation to crop out each fish, then pass each fish crop on for further image processing (to determine size of certain features). This is actually working quite well, EXCEPT for the fact that the same fish will get detected multiple times as it crosses the field-of-view - I obviously want to only analyze each individual fish, once. So, I need some way of saying \"this fish detection is the same as that fish detection\".\n\nI realize this isn't necessarily an ML specific problem, but I'm sure its something other ML folk working in computer vision have had to deal with, so any pointers would be much appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ige6hu/tracking_fish/"}, {"autor": "Pawan315", "date": "2020-08-25 13:16:35", "content": "Portable computer for opencv /!/ I want to run simple face detection and sum more operation on -----> image !!!  in realtime. so could you suggest me the pi should i use for this purpose like how about orangepi , raspberry pi or any other will they able to do so.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/igbzk2/portable_computer_for_opencv/"}, {"autor": "IHDN2012", "date": "2020-08-25 00:58:19", "content": "How can I get more consistent results for my computer vision model in various lighting? /!/ Hello.  I used my computer vision model on my Raspberry Pi with Pi -----> camera !!!  a few days ago, and it worked just fine.  I went to somebody else's house, and tried the same thing, and it could not pick out the same objects anymore.  I think it might be due to the lighting.  Both were fairly well-lit areas.  How can I get around this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ig214z/how_can_i_get_more_consistent_results_for_my/"}, {"autor": "umitkilic", "date": "2020-04-21 12:50:23", "content": "Article Recommendation About CBIR w/DL /!/ Hi everyone, I am PhD student dealing with ML especially Deep Learning. Can anyone suggest some article to read fundamental of content based -----> image !!!  retrieval which is also called reverse -----> image !!!  searching using deep learning techniques. Maybe I missed some important article. Especially article published last 2-3 years would be great. (I am still searching on that article, I just want to know if there are any good ones that i haven\u2019t realized yet)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g5emm3/article_recommendation_about_cbir_wdl/"}, {"autor": "boriswinner", "date": "2020-04-21 04:32:11", "content": "VAE Music Generation Network learns to generate just ONE melody /!/ Hello! \n\nI'm a 4th year AMI student. I'm currently working on study project: Variational Autoencoder for music generation. I'm inspired by this project : [https://github.com/HackerPoet/Composer](https://github.com/HackerPoet/Composer) (explanation: [https://youtu.be/UWxfnNXlVy8](https://youtu.be/UWxfnNXlVy8) ). I'm trying to write a similar network as a study project, but with the ability for multitrack music generation.\n\nI'm using Keras with TensorFlow backend. My piano rolls have 3rd dimension which represents MIDI instrument.\n\nThe layer architecture and loss function are almost the same as in your network (in VAE mode). I strip empty note heights (below the lowest note and upon the hightst notes) and MIDI instruments.\n\n&amp;#x200B;\n\nHowever, I've faced some problems. I'm asking for your help.\n\n&amp;#x200B;\n\nMy network tends to learn to generate just one melody. In the first epochs (1..20) the output is somewhat musical and different, but when I try to launch more epochs (for example, 100) it just learns to generate one song. This song is rhytmic, but has only about 4 repeating notes. After about 20 epochs loss and val\\_loss stop becoming smaller. When I try to plot the first two dimensions of the encoder prediction (latent space), with low number of epochs it looks like on -----> picture !!!  1, and with higher number of epochs it is simply a dot.\n\n&amp;#x200B;\n\nI tried to simplify the network architecture so it has only one hidden layer. In this case the result of first epochs is also somewhat musical, but with more epochs it learns to generate pure mess (a lot of notes at the same time).\n\n&amp;#x200B;\n\nThe question is - what can I be doing wrong?  \n\n&amp;#x200B;\n\nThe code can be found here:\n\n[https://github.com/boriswinner/BDB-VAE](https://github.com/boriswinner/BDB-VAE)\n\n&amp;#x200B;\n\nThe VAE code is in [VAE.py](https://VAE.py)\n\n[20 epochs plot of first two vatiables of latent dimension on test data](https://preview.redd.it/wqw0hr9dm3u41.png?width=600&amp;format=png&amp;auto=webp&amp;s=59ec8b3c298b9ea261245930b8820c66973290a0)\n\n&amp;#x200B;\n\n[Same, but 100 epochs](https://preview.redd.it/6odzqkjhm3u41.png?width=600&amp;format=png&amp;auto=webp&amp;s=0ca9a4c269141a4b61ec1905e9d4024d8e7cd649)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n[Network architecture](https://preview.redd.it/2t5a2gjjm3u41.png?width=545&amp;format=png&amp;auto=webp&amp;s=3d1bad9f5957609091c011221fe3f0349ea585a6)\n\n&amp;#x200B;\n\n[Encoder](https://preview.redd.it/8oamt321n3u41.png?width=709&amp;format=png&amp;auto=webp&amp;s=ab12a78620ebfbe9687bdec56a37db80780b769d)\n\n&amp;#x200B;\n\n[Decoder](https://preview.redd.it/bue2z282n3u41.png?width=656&amp;format=png&amp;auto=webp&amp;s=52f4a879c745a398adcc9afcc683c15ce584de9d)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g58nyq/vae_music_generation_network_learns_to_generate/"}, {"autor": "thewristenthusiast", "date": "2020-04-20 11:40:06", "content": "Very basic bayesian machine learning, is my pseudocode correct? /!/ Hey all,\n\nI'm trying to use a bayesian approach in a phylogeny reconstruction project.\n\nBasically, there are 3 different algorithms to rearrange a tree (NNI, SPR, TBR), and they each perform better at different stages of the search (better defined as generating a shorter tree). I'm trying to implement a penalty based system that uses a probability array, such that when an algorithm is picked, and if it is successful, a number will be added to the probability array, increasing its probability to be picked next round.\n\nEssentially, a machine learning approach that has a prior/posterior based on succes/failure. What kind of approach is this? Am i missing anything? Any help will be greatly appreciated. Here's an -----> image !!!  of my [pseudocode](https://i.imgur.com/r2jZc5s.png)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g4r8qs/very_basic_bayesian_machine_learning_is_my/"}, {"autor": "Cubandisimo", "date": "2020-04-19 22:04:43", "content": "ModuleNotFoundError: No module named 'sklearn.tree._classes' /!/ I'm working on a flask web application with a scikit-learn DecisionTreeClassifier as the back-end. Last week I exported the model as a pickle file from a Jupyter Notebook and attempted to import it into PyCharm and I received the error: ModuleNotFoundError: No module named 'sklearn.tree.\\_classes'. I posted that error here and I got a reply that said to use joblib rather than pickle, so I did, and it worked fine.\n\nHowever, today I attempted to import the model into my flask web application and that same error has come back to haunt me. When I import the model into PyCharm and use it in PyCharm it works perfectly. But when I run the flask application in my browser I see the error as shown in the provided -----> picture !!! . Any help solving this problem would be greatly appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g4gfjb/modulenotfounderror_no_module_named_sklearntree/"}, {"autor": "statsandecon", "date": "2020-04-19 20:37:33", "content": "How can I practice -----> image !!!  classification without using local storage? /!/ I would like to practice image classification using public data sets from Kaggle. The problem is the datasets can get pretty large. Is there a way to practice image classification without downloading these datasets onto my personal machine?\n\nI looked into Docker but from what I can tell all the files in the image are stored using the storage on my local machine.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g4ey8p/how_can_i_practice_image_classification_without/"}, {"autor": "_ygoloiB", "date": "2020-04-19 19:13:33", "content": "Testing a CNN that has been trained and validated /!/ I have a CNN for color classification of images and I have trained and validated it and gotten decent results (90%+ training/validation accuracy) but I am wondering how I can test it. Ideally I'm looking for a way I can now feed it an -----> image !!!  or -----> image !!! s and have it return what color it classified the -----> image !!!  as.\n\nProgram is written in Python and keras was used for the CNN.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g4dgx9/testing_a_cnn_that_has_been_trained_and_validated/"}, {"autor": "brahimkrm", "date": "2020-04-19 17:06:20", "content": "Problem with Eigenfaces for face recognition in python /!/ I'm trying to implement eigenfaces algorithm for face recognition in python using numpy and scikit learn for PCA then calculating the euclidean distance between the unrolled matrices produced by PCA.\n\nThe problem is that I don't get close distances between faces of the same person and large distances of different people:\n\n&amp;#x200B;\n\n    import numpy as np\n    from sklearn.decomposition import PCA\n    from PIL import -----> Image !!! \n    \n    person1_1 = -----> Image !!! .open('person1/1.jpg').convert('LA')\n    person1_2 = -----> Image !!! .open('person1/2.jpg').convert('LA')\n    \n    person2_1 = -----> Image !!! .open('person2/1.jpg').convert('LA')\n    person2_2 = Image.open('person2/2.jpg').convert('LA')\n    \n    \n    person1_1 = np.asarray(person1_1)[:,:,0]\n    person1_2 = np.asarray(person1_2)[:,:,0]\n    \n    person2_1 = np.asarray(person2_1)[:,:,0]\n    person2_2 = np.asarray(person2_2)[:,:,0]\n    \n    \n    pca = PCA(n_components=100)\n    \n    person1_1_pca = pca.fit_transform(person1_1)\n    person1_2_pca = pca.fit_transform(person1_2)\n    \n    person2_1_pca = pca.fit_transform(person2_1)\n    person2_1_pca = pca.fit_transform(person2_2)\n    \n    print(\"Same person: {}\".format(np.linalg.norm(np.ravel(person1_1_pca)-np.ravel(person1_2_pca))))\n    print(\"Different person: {}\".format(np.linalg.norm(np.ravel(person1_1_pca)-np.ravel(person2_1_pca))))\n\nThe code produces:\n\n    Same person: 9293.360941013\n    Different person: 10218.849581657663\n\nAnd sometimes I get larger distances for same person than distance of different people.\n\nWhere is the problem exactly", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g4b7c1/problem_with_eigenfaces_for_face_recognition_in/"}, {"autor": "JackIsNotInTheBox", "date": "2020-04-23 20:08:01", "content": "In linear classification, what is theta_0? (ignore the question mark in the -----> picture !!! )", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g6tuqh/in_linear_classification_what_is_theta_0_ignore/"}, {"autor": "pantoffl", "date": "2020-04-23 19:37:20", "content": "Using DCGANs for generating time series /!/ Hey guys,\n\nI'm fairly new to machine learning in general and currently trying to apply a deep convolutional GAN model to a time series generation problem in python using Keras. I looked at different implementations of DCGANs for -----> image !!!  generation and I superficially understand how they work. However I'm struggling to desing my model in a way that it accepts time series data with one feature (I'm just using generated sine and cosine series right now). I especially struggle with the way I have to shape my data and how to work with the dimensions of the convolutional layers. Do you guys have any suggestions on that?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g6tai1/using_dcgans_for_generating_time_series/"}, {"autor": "bodytexture", "date": "2020-04-22 22:32:05", "content": "Generative Adversarial Network /!/  Hello,  I'm working on a project that merges the immage of 2 or more people into an \"average result\" that should be the closest \"mix\" of the 2 or more pictures of peoples used to generate the merged immage, \n\npicture a \"+\" picrure b  \"+\" .... \"+\"  -----> picture !!!  n = -----> picture !!!  z\n\n  I would also like the image genarated to be treated  so that it dose not appear to be generated by a computer, and than possibly also upscale it to the highest resolution possible.  Can you point me int he right direction?  i'm a  a python programmer working with a small team on this project.  can you point us to a any git hub repository that dose just that, or a few project to be merged into this final result? or a course that can lead a very beginner to achive such results?  we've read of a similar project (styleGAN2) and we are thinking on implementing on a Generative Adversarial Network, so please feel free to point us in the right direction to get closer to our resoult.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g6ayuo/generative_adversarial_network/"}, {"autor": "fiddlest", "date": "2020-04-22 20:11:37", "content": "I am looking for a tool to collect voice data for the voice classifier. /!/ I need to make a voice classifier for my side project. However, it needs a lot of data to create it. The only way to get some voice data is that download a youtube video that has a certain person's voice and cut and extracts only that person's voice. Is there any tool to help me to reduce the manual process?\n\nFor a simple -----> image !!!  classifier, I can use python google -----> image !!!  downloader to download a bunch of -----> image !!! s related to the keyword. However, I can only think of a manual way of collecting dataset for the voice.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g68f3i/i_am_looking_for_a_tool_to_collect_voice_data_for/"}, {"autor": "NadavDagon", "date": "2020-04-22 18:08:17", "content": "Neural Network vs. Key-Points for drone navigation /!/ I'm working on a project involving indoor drone navigation. I'm using two drones so that a 3d -----> image !!!  can be constructed. The drones will always fly through the same doorway (thus eliminating overfitting for the learning agent, I think). I've considered using a NN or a SIFT like algorithm for this task and I'm not quite sure what suits this task more. Thanks in adavance :-)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g664gk/neural_network_vs_keypoints_for_drone_navigation/"}, {"autor": "cmillionaire9", "date": "2020-04-22 14:34:59", "content": "Remove unwanted obstructions from a short sequence of images captured by a moving -----> camera !!! ", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g626zt/remove_unwanted_obstructions_from_a_short/"}, {"autor": "goatsinthegraveyard", "date": "2020-04-22 12:44:24", "content": "Question about Text Generation /!/ I'm writing an algorithm to generate simple poems, such as haikus. I plan to use a recurrent neural net operating on the word level, and besides the normal vector representing each word to be fed into the LM, I would like to incorporate extra features, like the number of syllables that the word has.\n\nThe thing is, I would like to expand the training data for the grammar, so that it can use more words than just those found in preexisting haikus, as well as ensuring enough data so that the writing is coherent and grammatically correct.\n\nThe haikus ideally would just train, with the additional feature of # syllables, the structure, 5 7 5.\n\nI know that with convolutional neural nets for -----> image !!!  classification, we can pretrain many layers on a large swathe of -----> image !!! s, and then with base, we can essentially swap in a classifier on the end that learns a specific thing, like a cat.\n\nHas anyone done anything similar with text, to learn a basic conception of grammar on a big dataset, and then specific structure on a smaller one? Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g60g1x/question_about_text_generation/"}, {"autor": "wilhelmberghammer", "date": "2020-11-13 11:56:15", "content": "PyTorch: ImageLoader - use one hot encoding as target_transform /!/ Hi guys,\n\nI'm loading my -----> image !!!  dataset (3 classes) with **datasets.ImageFolder(),** unfortunately the labels are then integers. So for the first class the label is 0, for the second it is 1 and for the last it is 2. I'd like the labels to be one hot encoded tho. Is there a way I can do this with the **target\\_transform** property when loading the data?\n\n&amp;#x200B;\n\nThank you", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jtfqc7/pytorch_imageloader_use_one_hot_encoding_as/"}, {"autor": "MachineVision", "date": "2020-11-13 04:53:10", "content": "Convolution with RGB images - what values does a RGB filter hold? /!/ Convolution for a grayscale -----> image !!!  is straightforward. You have a filter of shape `nxnx1` and convolve the input image to extract whatever features you desire.\n\nI also understand how convolution would work for a RGB image. The filter would have a shape of `nxnx3`. However, would all 3 'layers' in the filter hold the same kernel? For example, if the 0th layer has the values:\n\n[0 1 0]\n\n[1 0 1]\n\n[0 0 1]\n\nwould layer 1 and 2 also hold the exact values? I am asking in regards to Convolution Neural Networks and not conventional image processing. I understand the weights of each filter are learned and are randomized initially, am I correct in thinking that each layer would have different randomized values?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jtaxru/convolution_with_rgb_images_what_values_does_a/"}, {"autor": "musclecard54", "date": "2020-11-13 02:14:47", "content": "Resources for learning GANs? /!/ Does anyone have any resources they\u2019d recommend as an intro to GANs? Bonus points if it relates to SRGANs for single -----> image !!!  super resolution. \n\nI\u2019m working on my final project for my Machine Learning class, and decided to try and implement single image super resolution.\n\nI\u2019ll be using R with Keras since that\u2019s what we\u2019ve been using while studying neural networks.\n\nWe have just gone over ANNs and haven\u2019t gone into any detail on CNNs or deep learning, I\u2019ll be taking deep learning next semester. \n\nAlso, I\u2019ve read that GANs are notoriously difficult to train, is that true? If so, does anyone have any tips for optimization?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jt8l6h/resources_for_learning_gans/"}, {"autor": "tesar-tech", "date": "2020-11-12 21:11:10", "content": "There is a great jump-in machine learning interactive course in matlab academy. Even if you don't plan to use Matlab, it's still good. /!/ [https://matlabacademy.mathworks.com/R2020a/portal.html?course=machinelearning](https://matlabacademy.mathworks.com/R2020a/portal.html?course=machinelearning) \n\nThere is also deep learning and -----> image !!!  processing course. This linked one is about classifying hand written letters.\n\n(you need mathworks account for that.)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jt3bsb/there_is_a_great_jumpin_machine_learning/"}, {"autor": "Jirne_VR", "date": "2020-11-12 20:31:01", "content": "openCV object detection in Java /!/ I just started with openCV in Java. I want to detect objects in an -----> image !!! , surround it with a boundingbox and get the coordinates of that object. I already have some experience with ML in other ML platforms, but those don't support object detection (yet).  When I look on the internet for openCV object detection in Java, I can't find the tutorial where i'm looking for. Does someone has a suggestion for me where I can find some nice tutorials or free courses about openCV in Java?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jt2jx7/opencv_object_detection_in_java/"}, {"autor": "Shaashwat05", "date": "2020-11-12 13:34:42", "content": "Virtual Background for video conferencing using OpenCV and TensorFlow(DeepLab v3) /!/ Aware of the concept of a virtual background? Get insight on -----> image !!!  segmentation, -----> image !!!  masking, and implementation of virtual background in python through my lastest medium article.\n\n[https://towardsdatascience.com/virtual-background-for-video-conferencing-using-machine-learning-dfba17d90aa9](https://towardsdatascience.com/virtual-background-for-video-conferencing-using-machine-learning-dfba17d90aa9)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jsuntn/virtual_background_for_video_conferencing_using/"}, {"autor": "vladiliescu", "date": "2020-11-12 12:21:33", "content": "Hey there Reddit! I'm writing a blog post on Azure Machine Learning, and I need your feedback! /!/ Hi girls and guys! I've written an article on getting started with Azure Automated ML to post on my [website](https://vladiliescu.net), but then my insecurities started kicking in and I decided to come and ask for feedback from you wonderful people. What do you think? Does it make sense to write about stuff like this, or it's not needed at all since everything si too obvious?\n\nThanks for taking the time to look at this!\n\nBombs away!\n\n\\---\n\nI\u2019m gonna start with an inconvenient truth: Machine learning is hard. It used to be harder though, and I feel like ML is getting more and more accessible each day. But acquiring the right background needed to understand what\u2019s going on under the hood of PyTorch or scikit-learn or whatever library you happen to be using is, well, still hard. It requires a lot of work, as the brilliant [A Super Harsh Guide to Machine Learning](https://www.reddit.com/r/MachineLearning/comments/5z8110/d_a_super_harsh_guide_to_machine_learning/) likes to remind us:\n\n*- First, read f\\\\***\\\\***\\\\\\*ing Hastie, Tibshirani, and whoever. Chapters 1-4 and 7-8. If you don\u2019t understand it, keep reading it until you do.*\n\n*- You can read the rest of the book if you want. You probably should, but I\u2019ll assume you know all of it.*\n\n*- Take Andrew Ng\u2019s Coursera. Do all the exercises in Python and R. Make sure you get the same answers with all of them.*\n\n*- Now forget all of that and read the deep learning book. Put TensorFlow and PyTorch on a Linux box and run examples until you get it. Do stuff with CNNs and RNNs and just feed forward NNs.*\n\n*- Once you do all of that, go on arXiv and read the most recent useful papers. The literature changes every few months, so keep up.*\n\nNo wonder machine learning intimidates people: you need to invest significant effort just to understand what\u2019s going on, what you can and cannot do. And then, once things start to make sense, you need to work twice as hard just to keep up with all the research being published all the time. [Red Queen\u2019s race](https://en.wikipedia.org/wiki/Red_Queen%27s_race) anyone?\n\n\u201cBut what if\u201d \u2014 you\u2019ll say \u2014 \u201cwhat if we could outsource this whole machine learning thing, at least partially? What if it were somebody else\u2019s problem?\u201d That would be nice, wouldn\u2019t it?\n\nJust imagine, handing over to someone whatever data you managed to gather, going to bed with a grateful heart, and waking up the next day to a shiny new trained-and-tested model, ready to be deployed and integrated and whatnot. Gee, that would be absolutely splendid \ud83d\ude31 wouldn\u2019t it?\n\nWell, apparently, other people \u2014 engineers, no doubt about it thought the same thing, and decided to solve this problem once and for all. You know how they like to automate this and that, so it was only a matter of time before they automated machine learning too \u2014 from [Wikipedia](https://en.wikipedia.org/wiki/Automated_machine_learning):\n\n**Automated machine learning** *(***AutoML***) is the process of* [*automating*](https://en.wikipedia.org/wiki/Automation) *the process of applying* [*machine learning*](https://en.wikipedia.org/wiki/Machine_learning) *to real-world problems. AutoML covers the complete pipeline from the raw dataset to the deployable machine learning model. AutoML was proposed as an* [*artificial intelligence*](https://en.wikipedia.org/wiki/Artificial_intelligence) *-based solution to the ever-growing challenge of applying machine learning. The high degree of automation in AutoML allows non-experts to make use of machine learning models and techniques without requiring becoming an expert in the field first.*\n\nIt goes on.\n\n*Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models.*\n\nWow. Models that often outperform hand-designed models?!? Well, sign me up with my main email! This is what we\u2019ve been looking for isn\u2019t it?\n\nThe truth is, there are several tools &amp; libraries &amp; online services that promise to help in this regard: [H2O.ai](https://www.h2o.ai/), [Microsoft\u2019s Automated Machine Learning](https://azure.microsoft.com/en-us/services/machine-learning/automatedml/), [Google\u2019s AutoML](https://cloud.google.com/automl), [auto-sklearn](https://automl.github.io/auto-sklearn/master/), [TPOT](https://github.com/EpistasisLab/tpot), and [Auto-PyTorch](https://github.com/automl/Auto-PyTorch) to name just a few. Each one with its own strengths and weaknesses, depending on your particular needs and background. Comparing them is somewhat outside the scope of this article, but I strongly suggest you give a try to at least a few of them and see how they stack up against each other.\n\nFor me, my current personal favorite is [Automated ML](https://azure.microsoft.com/en-us/services/machine-learning/automatedml/) \u2014 a cloud service-slash-library that acts like a recommender engine, looking at your data, checking its quirks and stats and whether it can work around them or not by, say, imputing or normalizing fields. It then uses those inputs to recommend a series of ML algorithms, selecting the best-performing one in the process. Let me show you how to use it.\n\n**Using Automated Machine Learning**\n\nAt a high level, all automated ml needs is some labeled data and a computer to run on, and this can be either your local computer or some machine in the cloud.\n\n&amp;#x200B;\n\n[The map is not the territory but it sure helps](https://preview.redd.it/ccua17rdwsy51.png?width=638&amp;format=png&amp;auto=webp&amp;s=75a591ac8ee1bf44f58eccce53f3de53117f7fdc)\n\nOnce started, Automated ML will use the compute you hand it over to run multiple experiments on your data, trying out various combinations of algorithms &amp; hyper-parameters, until it trains a good-enough model, which you can then use and integrate in whatever app you might be building.\n\nFirst things first though. As we all know, machine learning doesn\u2019t exist in a vacuum - we need to have a higher purpose for this whole \u201cget the data train the model\u201d thing. For example, let\u2019s say we\u2019re building a startup focused on real-estate investments, and a crucial functionality is the ability to forecast house prices for different cities, blocks, etc. This sounds like a great opportunity to use machine learning, maybe even add a bit of blockchain for good measure \ud83d\ude0b. This should be good enough to secure an initial round of funding, and then we\u2019re off to the races. \n\nIncidentally, this vision helps us start with the most difficult part of this process - finding and assembling the data. In order to automagically train a model that can forecast house prices, we need a dataset with house prices. Luckily, there are such open datasets, including the one from Kaggle\u2019s famous [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition. We could use that, but since we\u2019re not interested competing we can go directly for its source, the [entire Ames dataset](http://jse.amstat.org/v19n3/decock.pdf). From the description:\n\n*This paper presents a data set describing the sale of individual residential property in Ames, Iowa from 2006 to 2010. The data set contains 2930 observations and a large number of explanatory variables (23 nominal, 23 ordinal, 14 discrete, and 20 continuous) involved in assessing home values.*\n\nLooks good, let\u2019s use it! Using it instead of the one provided by Kaggle should give our model twice the data to train on, which in turn should yield better results. One thing you should not do though, is train a model using the entire Ames dataset, and use it to compete in the House Prices competition. That just spoils the fun for everyone.\n\nAnyways, now that we\u2019ve found a training dataset, we\u2019re ready to start using Automated ML. You will need an [Azure subscription](https://azure.microsoft.com/en-us/free/), along with an [Azure Machine Learning Workspace](https://azure.microsoft.com/en-us/services/machine-learning/) before you continue.\n\n**Registering a dataset**\n\nWhat we\u2019re gonna do first is load our dataset in Azure Machine Learning to be able to reference it later. We\u2019ll do that by going to [Azure Machine Learning Studio](https://ml.azure.com/) and choosing to Create a dataset from web files from the Datasets menu.\n\n&amp;#x200B;\n\n[Creating a new dataset](https://preview.redd.it/jxr8je4fwsy51.png?width=1680&amp;format=png&amp;auto=webp&amp;s=656ecb347a086990b4f5b72fb79a73ec8fc51f9b)\n\nThis is the simplest option really, you could also have uploaded the dataset from your computer, or referenced a dataset already uploaded on our datastore (we\u2019ll get to what this means in a moment). All you need to do now is enter the dataset\u2019s address (in our case it\u2019s http://jse.amstat.org/v19n3/decock/AmesHousing.txt), pick a good name (NotAmesHousing is always a winner in my book), and make sure the type of the dataset is Tabular and not some other option like File .\n\nNow that we\u2019ve told AML where the data is, we need to also describe it a little. It should deduce itself that this is a tab-delimited file, encoded as UTF-8, but it most likely won\u2019t know to read the first row as headers. Because who puts the headers in the first row, right? Anyways, make sure to set the Column headers as Use headers from the first file, which works even though in our case the first file is the only file. You\u2019ll also get the chance to review the data, and make sure it\u2019s parsed correctly (ignore the Id field column, that\u2019s just there to tell you the row number, it won\u2019t get added to the dataset).\n\nThe next step is a bit more challenging.\n\n&amp;#x200B;\n\n[Selecting features](https://preview.redd.it/c3eq14viwsy51.png?width=2810&amp;format=png&amp;auto=webp&amp;s=0554db7aac9ec9b48cf5c99a69be63313874959e)\n\nYou\u2019ll probably be wondering what\u2019s with the Path column, since this wasn\u2019t mentioned anywhere so far? Actually that would be useful if we had imported several files, each with its own path, but with just one file it\u2019s rather useless. You can leave it unchecked, while all other columns should be left checked. Except maybe PID, that looks useless - it\u2019s the Parcel Identification Number, as per the [Data Documentation](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt). Let\u2019s leave it in though, and see if Automated ML can figure that out by itself \ud83d\ude08.\n\nIn the end, your settings should look like this:\n\n&amp;#x200B;\n\n[Dataset creation summary](https://preview.redd.it/0z3lelbkwsy51.png?width=2086&amp;format=png&amp;auto=webp&amp;s=bb375638fbe2a3b49b1d0bcfe7b28eb0d04ad8ca)\n\nWe won\u2019t look at profiling datasets today, but you should know that checking this will calculate statistics such as mean, standard deviation, etc. for your entire dataset, as opposed to just getting them for a smaller subset. This won\u2019t be needed for now, and we\u2019ll be able to generate profiling data later on anyway.\n\nAnd that\u2019s it \ud83c\udf89! We now have a dataset, ready to be parsed and processed and used for training. The only thing standing between us and a bathtub full of VC money is finding out a way to run this automated ml thing - and we\u2019ll do just that by configuring a compute resource.\n\n&amp;#x200B;\n\n[Data is done, now check the rest](https://preview.redd.it/jaj76mimwsy51.png?width=638&amp;format=png&amp;auto=webp&amp;s=70f08f6f2fad85aa0ec0875094258398b9cf137c)\n\n**Configuring a compute resource**\n\nConfiguring a new compute in Azure ML is quite easy actually - what we need here is a Compute cluster. As you can see below, in the Compute menu you can create several types of compute, such as compute instances (useful for running notebooks in the cloud), inference clusters (useful for running your trained models and making predictions with them), and also attached compute (a bring-your-own-compute deal, in which you can attach existing HDInsight or Databricks clusters even virtual machines, and use them as compute targets). We\u2019ll stick to an Azure-managed Compute cluster though.\n\n&amp;#x200B;\n\n[Creating a new compute cluster](https://preview.redd.it/8nmgduiowsy51.png?width=1942&amp;format=png&amp;auto=webp&amp;s=0c4ac36e0d666ca6d946ec98721da494d6be8449)\n\nWe\u2019ll need to pay some attention to the next step here, since the compute size can and will significantly influence the time (and implicitly money) Automated ML needs to spend in order to train a good model.\n\nLooking at our data, at 2930 rows and 82 columns this won\u2019t take up that much RAM to load and process, so we can ignore RAM and focus on getting the best CPU we can afford. In our case this means something from the compute-optimized [Fsv2-series](https://azure.microsoft.com/en-us/pricing/details/virtual-machines/windows/#compute-optimized-tab-content) of machines, I went for Standard\\_F4s\\_v2 myself. Looking at the specs, it offers the same CPU performance as the default, general purpose Standard\\_DS3\\_v2 machine, but at a **30%** discount (you can check the pricing [here](https://azure.microsoft.com/en-us/pricing/details/machine-learning/), too). Not too shabby, if I do say so myself. \n\nYou could also go for an even faster machine, but that will fill up your core quota and you won\u2019t be able to start as many VMs in parallel. And generally you want to start as many VMs in parallel as possible, in order to allow Automated ML to explore as many options, as fast as possible. \n\n&amp;#x200B;\n\n[Configuring a compute cluster's size](https://preview.redd.it/hh93d6spwsy51.png?width=2738&amp;format=png&amp;auto=webp&amp;s=b60e82af8ccfcaf8e7b695cbc12f062bf430f116)\n\nOnce you select the best machine money (and quota) can buy, you only need to give it a good name (I named mine Spock \ud83e\udd13) and select a minimum and a maximum number of nodes. Considering our Automated Machine Learning scenario, I\u2019d set the minimum to 0 (we don\u2019t want to pre-allocate and implicitly pay for VMs if we\u2019re not gonna use them), and the maximum to whatever our quota allows (for Standard\\_F4s\\_v2 and my quota of 24, that means 6 nodes). As I said before, the more nodes we can allocate the faster Automated ML will train a suitable model.\n\n&amp;#x200B;\n\n[Two down, one to go](https://preview.redd.it/45id0iduwsy51.png?width=638&amp;format=png&amp;auto=webp&amp;s=d0f1f03be0efb3df3927f02cede52c9ea56082e7)\n\n**Might and Magic: Automated Machine Learning**\n\nThis is the fun part - we\u2019ll start off by going to the aptly named Automated ML menu, and choosing to create a new Automated ML run. We\u2019ll select our dataset, and then configure a few options: \n\n&amp;#x200B;\n\n[Creating a new Automated ML run](https://preview.redd.it/gejqwwgvwsy51.png?width=2886&amp;format=png&amp;auto=webp&amp;s=9ff0a6f1eb3a0d0e79222b4f668a6871511736d7)\n\nDon\u2019t worry too much about having to create a new Experiment, that\u2019s just the entity Azure ML uses to group Runs (and don\u2019t worry too much about what a Run is either, we\u2019ll talk about that some other time \ud83d\ude05). We\u2019ll need to tell Automated ML which column to predict - SalePrice, and on which compute resource to run - Spock. \n\nNext up, we\u2019ll need to tell it what kind of problem we\u2019re facing - do we need to predict a category (Classification), a continuous numeric value (Regression), or something based on time (Time series)? Now, we know that we want to predict house prices, which is to say numeric continuous values, so we\u2019re gonna go for Regression.\n\n&amp;#x200B;\n\n[Selecting the task type](https://preview.redd.it/n3vz72txwsy51.png?width=2838&amp;format=png&amp;auto=webp&amp;s=f89f3af0fe7343f731075571a950a05f7755585f)\n\nWe also have the change to fine-tune Automated ML\u2019s configuration settings, which control how it approaches the whole find-a-good-model-and-then-stop process, and its featurization settings, which control how it transforms the data. We\u2019ll only look at the configuration settings for now.\n\n&amp;#x200B;\n\n[Additional configuration settings](https://preview.redd.it/i5pmynxywsy51.png?width=1264&amp;format=png&amp;auto=webp&amp;s=51631ed0ff8d96fd0c41557304f0e73ec4ab076b)\n\nIn my case, I\u2019ve chosen to evaluate the automatically trained models using Normalized root mean squared error, since I want to be able to know how far off my predictions are. I don\u2019t want Automated ML to take more than one hour to find a suitable model, because I want my costs to be **really** predictable. I want to evaluate the models using a 5-fold cross validation to make sure it\u2019s not overfitting my data, and finally, I want to evaluate as many models as possible so I chose to have 6 concurrent iterations (remember, that\u2019s the maximum for my compute quota), effectively enabling it to try out 6 potential models in parallel.\n\n&amp;#x200B;\n\n[Run 1](https://preview.redd.it/464rw1a0xsy51.png?width=2130&amp;format=png&amp;auto=webp&amp;s=814b2520e026a3bdcc0c2be707d86b3907596d93)\n\nOnce started, you\u2019ll notice a new experiment has been created which has a run, well, running. This run, lovingly called Run 1, contains all there is to know about our automl run - most importantly any and all data issues it detected and their fixes in the Data guardrails tab, and also a growing list of potential models currently evaluated in the Child runs tab.\n\n&amp;#x200B;\n\n[Data guardrails](https://preview.redd.it/qf25ru41xsy51.png?width=3428&amp;format=png&amp;auto=webp&amp;s=0c63ea4e2270eed6bc5ddb919325a5927c001a8f)\n\n&amp;#x200B;\n\n[Child runs](https://preview.redd.it/wd2eti52xsy51.png?width=3210&amp;format=png&amp;auto=webp&amp;s=09188e79c6f4efd59b3b32cb8d32e4ad1381a373)\n\nYou\u2019ll notice a bit of a delay when you first start an AutoML run \u2014 this is because of mainly two reasons:\n\n&amp;#x200B;\n\n1. On the one hand, before it starts, Azure ML needs to configure a Docker -----> image !!!  with all the Python packages needed to run. This takes a few minutes, but it ensures further reproducibility (plus, it\u2019s really cool)\n2. On the other hand, our compute nodes each need to be allocated and then they need to pull those automl-enabled Docker images created earlier, taking a few minutes more \u2014 you can check the status of the compute allocation at any time in the Compute menu, but I suggest brewing a coffee instead and only take a look afterwards to make sure there\u2019s something to see.\n\n&amp;#x200B;\n\n[Compute clusters being allocated](https://preview.redd.it/20c7e924xsy51.png?width=2894&amp;format=png&amp;auto=webp&amp;s=6714a7ad5cb64ea369122d19fbb84babd929cf29)\n\nAfter what I can only hope were no more than three espressos and a latte, we\u2019ll start seeing some interesting results. For my setup, in less than 40 minutes Automated ML explored 65 possible models, including stacked and voting ensembles of the best performing models, and identified a Voting Ensemble as being the best of the best.\n\n&amp;#x200B;\n\n[Run 1 completed](https://preview.redd.it/ux2zocw6xsy51.png?width=1536&amp;format=png&amp;auto=webp&amp;s=d8832f8ea14b34e0fff080e5f6bd6b87a3faad16)\n\nYou can take a look at its details of course, there\u2019s a lot of interesting stuff here including but not limited to the generated explanations and metrics - you\u2019ll notice AutoML calculated a lot of metrics apart from our primary one - Normalized root mean squared error. These include Explained variance, Mean absolute error, R2 score, Spearman correlation, even Root mean squared error. The special thing about the primary metric is that it\u2019s used to identify the best model - had you chosen a different metric, you may have gotten a different model.\n\nOnce you decide which model to use, there\u2019s also a simple way for you to  Deploy your model directly in Azure ML, or even Download it and deploy it to the cloud of your liking.\n\n&amp;#x200B;\n\n[Best model details](https://preview.redd.it/j6ducc9ixsy51.png?width=1202&amp;format=png&amp;auto=webp&amp;s=3997863fea45a1b05d6db5abf3cde6088ce48c5b)\n\nAnd, that\u2019s it for a quick intro to Azure Automated Machine Learning. Let\u2019s review the steps here: you\u2019ve defined a problem, found meaningful data that can be used to solve said problem, configured the necessary Azure resources that enabled you to automatically train a model on the data, and now you\u2019re ready to make use of this model, be it for further evaluation and improvement, or for deploying it as a REST endpoint and integrating it in your AI-powered, blockchain-enabled startup.\n\nThanks for reading so far!\n\n&amp;#x200B;\n\n[The End](https://preview.redd.it/6f89tf1kxsy51.png?width=958&amp;format=png&amp;auto=webp&amp;s=16273d0f3a691ab8a411634a7a4bb2f2081f8759)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jstoh8/hey_there_reddit_im_writing_a_blog_post_on_azure/"}, {"autor": "hyperdx", "date": "2020-11-12 02:59:22", "content": "About gradients of a layer activations with regrad to a input -----> image !!! . (Guided backpropagation) /!/ I read [guided backpropagation article](https://arxiv.org/abs/1412.6806), brought some codes for the article implementation.\n\nI thought that the shape of the result of calculating gradients of a specific layer activation(relu after conv layer, \\[7,7,2048\\] shape) with regard to input image(\\[224, 224, 3\\] shape) would be \\[224, 224, 3, 7, 7, 2048\\].  \n\n\nBut it's shape was just \\[224, 224, 3\\].\n\nI think that the result has gradients input image per every single unit of the activation layer. What did i miss?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jsms41/about_gradients_of_a_layer_activations_with/"}, {"autor": "Fit_Faithlessness122", "date": "2020-11-11 23:10:20", "content": "Does anyone have any project ideas for -----> image !!!  segmentation? Preferably something having to do with science and something that hasn\u2019t been done before. /!/ I would greatly appreciate it. I\u2019m not good with making up ideas for projects.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jsiubl/does_anyone_have_any_project_ideas_for_image/"}, {"autor": "comeculosgrandesymed", "date": "2020-11-11 21:44:10", "content": "Best approach to train an object detection model? /!/ I am getting started in the computer vision field, I have been reading about different ways to train models for object detection (in this case I'm trying to detect face masks in people's faces, or if they're not wearing a mask at all or using them wrong e.g below the nose). I am currently using IBM Watson to train an object detection model for this.\n\nI am not sure if I should label a front face wearing a mask with the same label that I put to profile faces wearing a mask? Because while they're the same thing, they don't look exactly the same, same thing to  people not wearing masks.\n\nAnother question that I have is wether I should train the model with pictures where it is clear to identify such situations (big, clear, close to the -----> camera !!!  faces) or I should use pictures that are not very clear and might have some distance as well. I ask this because I expect my system to detect this situations from a range of 7-12 ft distance at least, but I'm not sure if using pictures that are not very clear will do bad to the training.\n\nMy last question is wether it is wrong to leave instances of the object not labeled in the training? For example, in a single picture there are 15 people wearing masks, but I only label 10 of them and leave the other unlabeled. Is that a bad practice? (IBM Watson free service only lets me label 10 objects per picture)\n\nI know these questions might be dumb, but I am new in this and I really want to know and learn from other people's experiences.\n\nTHANKS IN ADVANCE", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jsh5tt/best_approach_to_train_an_object_detection_model/"}, {"autor": "ElectronicBeat1386", "date": "2020-11-11 21:40:15", "content": "Ideas for a machine learning project in high school? /!/ Hi, I'm currently a junior in high school. I'm trying to come up with a science fair project idea, and I want to do something related to machine learning. I do have great programming experience, but no machine learning experience. I was thinking something that would involve -----> image !!!  processing, but I need ideas that are not too hard but also not very basic.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jsh308/ideas_for_a_machine_learning_project_in_high/"}, {"autor": "The_Danieel", "date": "2020-11-11 17:25:24", "content": "Need help creating math learning assistant bot /!/ I am trying to create learning assistant bot, I want it to be able to help with math and later maybe make it more universal, as a framework or system so that bot can help learn other stuff too.\n\nI don't care about speech recognition and speech synthesis for now, if it works correctly in chat mode it's just another feature to add later. The features I want to have are:\n\n\\-asking bot for help with certain category (i.e. 'I have a problem with solving inequalities in algebra') will trigger bot to give simple summary of stuff from that category and show mathematical formulas.\n\n\\-sending a -----> picture !!!  of math problem will show solution with steps and explanation.\n\n\\-asked for a random problem from certain category bot will give it to user and after getting answer it will check if it's correct.\n\nI don't want to have just basic language understanding with lots of 'ifs' and bot sending answers that were previously manually entered, same goes for generating math problems.\n\nDoes anyone have experience creating anything similar or can at least recommend technology, ways to make prototype of a bot like that?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jscd4g/need_help_creating_math_learning_assistant_bot/"}, {"autor": "yisevrethingtaken", "date": "2020-11-15 18:49:04", "content": "Is creating facial expressions on an uploaded face -----> image !!!  a feasable idea for a begginer? /!/ I am trying to create my own project using AI, and one thing that I thought would be interesting would be to try and take a face uploaded by a user and manipulate it in such a way that it would display emotions such as sad, happy, angry, ect. I have used a GAN before to create faces, so my original idea would have one neural network start with the image of a face rather than some random noise, and then have the other network be trained to tell if something is a face with a certain emotion. However, I realized that the GAN that I built and usd made a face of very low resolution and still took quite a while. My question is, is this a good idea for a project for someone who has just learned about these concepts, and if so, it would be great if any of you could point me in the right direction.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jur0uh/is_creating_facial_expressions_on_an_uploaded/"}, {"autor": "emdayr123", "date": "2020-11-15 12:27:23", "content": "Can I train a model to recognize each item in an -----> image !!!  with just that one item before I train it to recognize each item in a picture of mixed items? /!/ If I wanted to train a model to recognize different animals. Should I train the model to recognize the individual animals first through pictures of just that animal. And then train the model with the same process for other animals. And then having pictures of all the animals mixed? Or would that not help?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jul51i/can_i_train_a_model_to_recognize_each_item_in_an/"}, {"autor": "shyamcody", "date": "2020-11-15 10:14:53", "content": "spacy learning curve shared /!/ Last 2 months, I have been learning about spaCy and have written about the learning thoroughly in my blog. I am sharing these here so that anyone interested in spaCy can go through these and try using it as a resource.   \n(1) [spacy introduction](https://shyambhu20.blogspot.com/2020/09/introduction-to-spacy-basic-nlp-usage.html)  \n(2) [dependency tree creation using spacy](https://shyambhu20.blogspot.com/2020/09/dependency-parsing-using-spacy-spacy.html)   \n(3) [word similarity using spacy](https://shyambhu20.blogspot.com/2020/10/calculate-word-similarity-spacy-nlp.html)  \n(4) [updating or creating a ](https://shyambhu20.blogspot.com/2020/10/spacy-nlp-neural-net-model-training.html)[neural network model using spacy](https://shyambhu20.blogspot.com/2020/10/spacy-nlp-neural-net-model-training.html)  \n(5) [how to download and use spacy models](https://shyambhu20.blogspot.com/2020/11/spacy-model-pipelines-english-usage-download.html)  \n(6) [Understanding of pytextrank: a spacy based 3rd party module for summarization](https://shyambhu20.blogspot.com/2020/11/pytextrank-spacy-phrase-extraction-summarization.html)\n\nI ought to mention that I show ads on the above posts and stand to get some monetary help on viewing. Also, I have not mentioned it as a tutorial as I am still an amateur in spacy and therefore will not call it a tutorial.\n\nExpectation is that people don't have to spend the 100 around hours behind spacy as I did to get a full -----> picture !!!  of the framework. If you get helped please let me know. If you think some major concept is left/not discussed in details/ wrongly discussed; please let me know so that I can improve this list.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jujs9y/spacy_learning_curve_shared/"}, {"autor": "happyfriedchicken", "date": "2020-02-23 11:53:30", "content": "Where to start with ML sorting algorithm for security -----> camera !!!  images /!/ Hello,\n\nI would like to start my own ML project (as a Noob) and would like some advice on how to proceed.\n\nI have a very cheap security camera installed at the front door of my family's house that takes pictures whenever it senses movement. Unfortunately most of them are false alarms, triggered by rain, wind, or a bird. All I am interested in are those with people.\n\nIt has been running for about 18 months now and has amassed 200.000 images worth 20 GB. \n\nLong term dream:\n\nA very cool final stage would be a system that gives a daily summary the number of people detected, and maybe poinst out unusual activity during normally inactive hours (we've had some strangers snooping around in the past)\n\nCurrent plan:\n\nI assume a ML based system could be used top assign tags. depending on how sure it is, spam images(about 90% ) could be deleted, unsure ones could be seperated for manual review and the quality images can be enjoyed.\n\nWhat would be the most efficient way to randomly select some teaching data and assign it with the appropiate tags?\n\nHow would I proceed from then?\n\nI do not expect a detailled instruction, some friendly pointers to appropiate ressources would also help me alot:)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/f88j2n/where_to_start_with_ml_sorting_algorithm_for/"}, {"autor": "Transit-Strike", "date": "2020-04-01 18:44:46", "content": "Optimising Learning Rate for a WGAN? /!/ After a lot of trial and error, I got a GAN to train and produce some output.\n\n&amp;#x200B;\n\nThe issue: Generated output is super noisy and not usable.\n\n&amp;#x200B;\n\nI messed around with learning rates and found:\n\n&amp;#x200B;\n\nLr of 0.0005 oscillates a lot and what not\n\nLr of 0.00005 oscillates a lot less, but then tops out with a noisy -----> image !!!  and doesn't get better beyond that. IN fact it gets worse.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nCurrently, after discussing with members from my project team:\n\n&amp;#x200B;\n\nWe have a few solutions we think will help:  \n\n\nSpectral Normalisation, Adding noise to input of a discriminator to help train better.\n\n&amp;#x200B;\n\nMess around with learning rates for longer.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nThe issue is, right now, every epoch for me takes a 100 seconds as colab doesn't give me a GPU.\n\n&amp;#x200B;\n\nMy Graphics card on my laptop is a GeFOrce 940MX, for which CUDA 5 is my only option, that messes up everything. \n\n&amp;#x200B;\n\nAnd even my model is so much smaller than the others' in my group. My guess is the loss and RMSprop used by a WGAN really slow it down.\n\n&amp;#x200B;\n\nBoth models have 5 convolutional layers(64,64,32,32). \n\n&amp;#x200B;\n\n&amp;#x200B;\n\nIt could just be that the model is too shallow/narrow, but I am in no position to really commit", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ft5z7f/optimising_learning_rate_for_a_wgan/"}, {"autor": "AggressiveHospital2", "date": "2020-04-01 05:19:47", "content": "I am looking for someone to teach me (med student) OpenCV using Python. /!/ Hello!\n\n&amp;#x200B;\n\nSo as you can see from the title, it's quite an odd combination to be in med school and want to learn OpenCV. I know it's a bit too much to ask, but I haven't been able to find passionate + nice teachers in this -----> image !!!  recognition stuff which I eventually aim to build a software with for a local hospital (I'm not getting paid for it- I volunteered to do it for a doctor because the medical problems being fixed are very close and mean a lot to me on a personal level), and it will save a lot of children from medical errors during surgery. If you love teaching, please teach me lol!! In turn, I'll teach you everything I can about my medical lectures if you'd like. I generally love teaching and have volunteered to teach people my medical lectures online before, which I found super satisfying, so I thought I might hit the jackpot with this post? Maybe? Hmmm maybe not?? I already know some Python btw, and have used it to build stuff before! It's just OpenCV I'm struggling with!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fstcqk/i_am_looking_for_someone_to_teach_me_med_student/"}, {"autor": "sopld", "date": "2020-04-01 03:54:35", "content": "-----> Image !!!  Forged Detection via Pixels /!/ Hello,\n\nI don't know where else to post this, but I have a question.\n\nI have tried to research this, but I am having trouble finding what I need and I don't know if I'm just not understanding what I'm looking for or if it just doesn't exist.\n\nI have an image of a face. I want to compare the pixels in a bounding box and the pixels in another part of the face to determine whether that image is real or it has been forged. I have tried search for papers on image forensics, but I am having a lot of trouble finding what I'm looking for. \n\nThank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fss4kg/image_forged_detection_via_pixels/"}, {"autor": "9qspkj24majy440otx2r", "date": "2020-03-31 23:44:07", "content": "Feature Space Dimensionality and Integrity /!/ I am taking an ML course this term and my professor did not know how to answer it, and I couldn't find anything on SO. Maybe I am just bad with keywords. Hopefully I will have luck here. \n\nIf I have multiple features with different dimensions, how can I analyze each feature space while keeping the integrity of each feature?\n\nFor example, if I take an -----> image !!!  of a flower I may be able to determine the type by the, length of the petals, histogram of oriented gradients and the color histogram of the raw -----> image !!! . Here, the dimensions of the data are different. Should I run PCA on HOG and color to reduce dimensionallity to 1 number representative of each? Should I run multiple decision trees? A CNN to produce the probability of a feature resulting a classification as the feature instead? Do I  increase the feature size of the length of the petals? Do I flatten the dataset to one long list? I am unsure how to go about this and why one option may be better than the other.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fso6ur/feature_space_dimensionality_and_integrity/"}, {"autor": "Yajirobe404", "date": "2020-03-31 20:56:03", "content": "Object segmentation and -----> image !!!  resolution question /!/ Hello. I noticed that if I train a neural network to segment objects (produce masks) given, say, 256x256 images, it performs poorly on 512x512 images. The masks aren't uniform and holes start appearing inside of them.\n\nDoes the opposite work though? Can I expect a network trained on 512x512 images to perform well on 256x256 images as well?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fsl4tk/object_segmentation_and_image_resolution_question/"}, {"autor": "GetStuffTogether", "date": "2020-07-22 02:54:33", "content": "I am confused on where to start for this Kaggle competition. Can someone help me with big -----> picture !!! ? /!/ https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks?taskId=570\n\nIts the Kaggle Cover-19 Challenge about Diagnostics and Surveillance. I looked at other's notebooks, but what is the big picture? Is this even a supervised learning? So many questions.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hvlrly/i_am_confused_on_where_to_start_for_this_kaggle/"}, {"autor": "Vendredi46", "date": "2020-07-21 11:15:26", "content": "For the detection of music symbols, is ML a good fit or openCV? /!/ Basically the title, not sure what would be the best route to go for. \n\nIf I wanted an app to take a -----> picture !!!  of a g-clef, and say with certainty \"that's a g-clef\", should I be using ML?\n\nSorry for the very basic question!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hv5k4v/for_the_detection_of_music_symbols_is_ml_a_good/"}, {"autor": "newappeal", "date": "2020-07-21 08:48:31", "content": "Input normalization in Keras (audio data) /!/ Basic question: If my input to a neural network built with Keras in Python is audio data, what sort of normalization should I be applying, both to the training data and validation/evaluation data?\n\nA bit more background: I've been learning the basics of machine learning over the past few months, and up to now I've pretty much only worked with -----> image !!!  data, and with my own models written from scratch or with the scikit-learn Python library. I'm now trying out a different task - speech segmentation - with a more complicated approach using Keras, and I'm not sure how I should be normalizing my input data. For example, when I wrote a Gaussian Mixture class, I got the best results when I first normalized the training data (images in the Lab color space) to its own mean and standard deviation, then stored those statistics in the model and normalized any evaluation data to them.\n\nHow would I do something similar for audio data? The first question is whether I should be normalizing the amplitude-domain audio data or the frequency-domain data - since the model input at each point in time will be a series of filter band spectra. Second, what sort of processing should be applied to future data after the model is trained, and how can this best be achieved in Keras? Since the model should be able to produce output for individual input datapoints (e.g. a single sound file), it's not like I can normalize incoming data to itself (unless I normalize the time or frequency slices of a given file).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hv3xti/input_normalization_in_keras_audio_data/"}, {"autor": "lilsegmentationfault", "date": "2020-07-21 06:39:03", "content": "How is imagenet so compact (sizewise)? /!/ Imagenet is 150gb for 1 million images. When I tried building my dataset, I got 25gb for only 30k (approx. 0.8mb each file). The mean of my images resolution is 700 on 1200 stored in png format. Also, which archive compression format is best for -----> image !!!  datasets.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hv2ix4/how_is_imagenet_so_compact_sizewise/"}, {"autor": "WatIsThisSorcery", "date": "2020-07-21 05:55:54", "content": "Need to pass multiple images to classifier.(Explained in detail below) /!/ Here's what I'm doing -\n\n- Trained an object detection model and passing the -----> image !!!  to it.\n- In the image, there are multiple objects detected. \n- Cropping ROI of object and passing to a Classifier.\n- But the classifier takes a single object which makes the overall process slow.\n\n Instead of passing single image ROI to classifier , is it possible to pass all ROI's of previous image to classifier?\n\nGive suggestions, even if I need to make or train something form scratch.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hv1zn8/need_to_pass_multiple_images_to/"}, {"autor": "yudhiesh", "date": "2020-07-21 05:00:33", "content": "Preprocessing videos /!/ I'm working on a project with a dataset of videos of people. I've converted them to individual frames and am kinda stuck now at whether I should convert the frames to greyscale then apply haar cascade to crop out the faces or to just apply haar cascade and crop the faces out.\n\nThe final model will be dealing with video from a phones front facing -----> camera !!!  so I'm thinking the latter would be better but not a 100% sure.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hv19mz/preprocessing_videos/"}, {"autor": "dhokna", "date": "2020-07-20 20:46:35", "content": "The dataset in an -----> image !!!  classification problem is too small and contains almost the same -----> image !!! s. What to do? /!/ I was doing this problen on Kaggle about Naruto Hand Sign recognition. The size if the dataset was small. So I used image augmentation but then when I tested it myself using a webcam, I found its not working at all. Its only detecting correctly the signs if the image almost matches with the training set. Also it appears to detect the face more than the hands to detect certain signs. I mean it is definitely overfitting. But I am a noob in this field and really want to make a nice model. \n\nSo I thought if I would make a model to detect hands in an image. Then that will give the coordinates of the hands on the inage of this dataset and we can crop out the rest and try to use it in our model. But I cannot find any dataset to detect hands. There are hand gesture datasets but nothing to detect only hands. \n\nSo do you have any ideas what to do? I tried image augmentation and its not helping. Also some people have actually done this well. But I am not being able to do because I am a noob. \n\nSo please help.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hute7y/the_dataset_in_an_image_classification_problem_is/"}, {"autor": "sfscsdsf", "date": "2020-07-20 20:40:42", "content": "Does anyone know any AutoML frameworks for -----> image !!!  segmentation?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hutab7/does_anyone_know_any_automl_frameworks_for_image/"}, {"autor": "Intro24", "date": "2020-07-20 17:11:38", "content": "Looking to clear up this -----> image !!! . Are there any aerial-photography-specific AI enhancement projects? Anything that might work better than Let's Enhance?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hup67c/looking_to_clear_up_this_image_are_there_any/"}, {"autor": "BoringLie7", "date": "2020-07-20 16:58:49", "content": "Unable to extract facts from -----> image !!!  text, but works for a normal string? /!/ So I'm working on a program with OCR (optical character recognition) that identifies text from an image (from this youtube video [https://www.youtube.com/watch?v=fswR5cbmq-c&amp;t=3s](https://www.youtube.com/watch?v=fswR5cbmq-c&amp;t=3s)), then use spacy and textacy to extract facts from the image. My problem is that when I convert the image-text to a string and print the extracted facts, nothing shows up. But when i use a normal declaration of a string and pass it into the doc variable, it works. Why?\n\nI posted the question on stack overflow too, I'm stuck lol. Thanks to anyone for the help\n\n[https://stackoverflow.com/questions/62986948/unable-to-extract-facts-from-image-text-but-works-for-a-normal-string](https://stackoverflow.com/questions/62986948/unable-to-extract-facts-from-image-text-but-works-for-a-normal-string)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/huowx9/unable_to_extract_facts_from_image_text_but_works/"}, {"autor": "HeeebsInc", "date": "2020-07-20 16:47:49", "content": "Image Classification: Using AI to Detect Pneumonia /!/ Hey everyone!\n\nI am happy to share my new blog post where I show you how to create a convolutional neural network that can detect Pneumonia given a lung X-ray. I am really excited about this project as it is my first time applying a neural network for -----> Image !!!  classification. I would really love some back and forth to see if there is anything I should have done differently. I am a self taught programmer so any advice would be very much appreciated.\n\nMy blog was able to get published in Learn Programming, Machine Learning, and Artificial Intelligence publications on Medium. (crossing my fingers for Towards Data Science)\n\n[Blog](https://medium.com/@HeeebsInc/using-ai-to-detect-pneumonia-3ec4601acd07?source=friends_link&amp;sk=16cd37228d262f5b74eb31bf0a5439a8)\n\n[Github Repo](https://github.com/HeeebsInc/NN_Pneumonia_Detection)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/2bevksx9k1c51.png?width=1143&amp;format=png&amp;auto=webp&amp;s=03c903d6defe9ea124480425bd516377c9eeddb6", "link": "https://www.reddit.com/r/learnmachinelearning/comments/huop7b/image_classification_using_ai_to_detect_pneumonia/"}, {"autor": "HV250", "date": "2020-07-20 07:42:44", "content": "Autoencoder works but VAE does not - KL divergence loss quickly drops to zero /!/ I have an -----> image !!!  encoding problem, to which I have successfully applied an autoencoder. But when I try to convert this into a VAE framework by introducing KL divergence loss and sampling from a unit Gaussian, it fails catastrophically. \n\nIn the early phases of training, the KLD loss drops really quickly, and within a couple of epochs, the encoder essentially collapses, always returning exactly a unit Gaussian. If I 'turn off' the KL-divergence loss, network works well again (as it's equivalent to the normal autoencoder again). How do I diagnose the cause of this issue and fix it?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/huh8f3/autoencoder_works_but_vae_does_not_kl_divergence/"}, {"autor": "moribaba10", "date": "2020-04-09 18:36:35", "content": "Can someone help me with this problem statement? /!/ let's say the keyword \"Superman\" has a search volume of 250k per month. Other related search terms can be \"Superman Logo\" (55k), \"Batman vs Superman\" (60k), \"Superman returns\" (50k) etc. using which users search for a certain topic in Google in a country (say US). However, there may be thousands of keywords with the word \"Superman\" in it, and we don't always have exact data for many of these keywords with low volume (less than 1000). In order to find the volume of these topics we use predictions based on the data that we already have for high volume keywords. For example, we may say that the term \"Future Superman\" will not be searched by a lot of people and its volume can be medium (close to 300). Similarly, terms \"Superman -----> image !!! \" and \"Superman jacket\" will have medium volume and terms that people rarely search will have low volume like \"Long Sleeve Superman Shirt\" (100) and then there are terms like \"cheap superman shirt\" or \"Superman college\" which will have very low volume. Can you think of an approach on how you can classify keywords into \"Medium\", \"Low\" and \"Very Low\" by using some logic when we only know the high volume keywords. Your answer should be descriptive and backed by reason. (Hint- your approach may be to classify certain words which if present will make the keyword \"low\" volume or \"very low\" volume. Or your approach can be based on number of words in a keyword, spellings, where exactly a term appears in a phrase and other common human psychology that works during a search) *", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fxxwtl/can_someone_help_me_with_this_problem_statement/"}, {"autor": "ArgusMocny", "date": "2020-04-09 18:26:23", "content": "Some good approaches/articles on incremental learning /!/ Hi, \n\nI'm dealing with -----> image !!!  classification, the difference with standard -----> image !!!  classification is that number of classes can grow in time and classes have subclasses and only subclasses will be predicted - for example, class dresses with subclasses \\[color, length, cleavage\\].\n\nThe first idea was that (using TensorFlow btw) I will use transfer learning on ResNet50 model with non-trainable weights and put on output layer next dense layer with no\\_subclasses neurons, that approach gave good results, but:\n\n* having a model for each class isn't good since they share almost all variables\n* training only output layer and attaching it on ResNet model was somehow good, but then comes TensorFlow Serving and that guy doesn't know how to custom-load model\n\nSo I came with an idea that maybe incremental learning will be a solution, I read this paper: [https://www.microsoft.com/en-us/research/wp-content/uploads/2014/11/Error-Driven-Incremental-Learning-in-Deep-Convolutional-Neural-Network-for-Large-Scale-Image-Classification.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2014/11/Error-Driven-Incremental-Learning-in-Deep-Convolutional-Neural-Network-for-Large-Scale-Image-Classification.pdf)\n\nand I liked it very much (especially superclass concept) but in my case, I know \"superclasses\" and prediction for one class should be done only for that class - so I don't know if one model to rule them all is a good solution.\n\nDo you know some incremental solutions to this problem? Or maybe this is not a good way to go?\n\nAnd if any of you know some great articles about IL, I would be grateful for sharing them :)\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fxxpz1/some_good_approachesarticles_on_incremental/"}, {"autor": "student103322", "date": "2020-04-09 18:08:39", "content": "How to get this annotated -----> image !!! ? /!/ Hello guys, I have been using a dataset to segment the food items on a tray, in the dataset, there were images that were original food tray images and the annotated images, here is the link to both these images :\n\noriginal image:\n\n[https://imgur.com/2in81qx](https://imgur.com/2in81qx)\n\nannotated image:\n\n[https://imgur.com/mYQ25wk](https://imgur.com/mYQ25wk)\n\nhow to obtain such an annotated image for any image?, this annotated image has 41 classes(need some help understanding that too) and how to give label to those classes?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fxxdju/how_to_get_this_annotated_image/"}, {"autor": "shapka-pa-gisht", "date": "2020-04-09 16:27:27", "content": "Tips and Tricks for Line-based Image Segmentation using Deep Learning? /!/ For a project, I have a dataset of some medical scans from different patients. The dataset consists of 510 grayscale 4000x2000 images. These -----> image !!! s contain a lot of useless information that I do not need, meaning that I can segment the -----> image !!!  so that I only get the useful part of the data. \n\nMy task is to do this using a line-based approach, meaning that I should be able to input a 4000x2000 image and get a vector of dimension 1x2000 as an output. [Here is a rough example of what the input and the output should look like.] (https://imgur.com/a/UJq2vMq) What type of architecture or what type of method should I use to achieve my desired result and what are some general tips you can give me? Note that it should be a Deep Learning approach, as I will have to compare it with conventional image processing later.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fxvhy0/tips_and_tricks_for_linebased_image_segmentation/"}, {"autor": "lil_kriszxdxd", "date": "2020-04-09 16:00:22", "content": "school project, need source /!/ Hi guys,\n\nI am currently having a school project on covid19 related (chest)  -----> image !!!  classification. My main question would be if you know any kind of neural network architecture other than covidnet ? That would be a huge help.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fxuz91/school_project_need_source/"}, {"autor": "BanyanTreeTechLabs", "date": "2020-10-20 17:13:40", "content": "How to use ML to modify specfic features in an -----> image !!!  (Like Fatify or Snap's Baby Face filter)? /!/  I have a decent understanding of CNNs, FCNs for semantic segmentation and RNNs. I am very interested in building a ML algorithm that can do what Fitify ([https://play.google.com/store/apps/details?id=ly.appt.fatify&amp;hl=en\\_US&amp;gl=US](https://play.google.com/store/apps/details?id=ly.appt.fatify&amp;hl=en_US&amp;gl=US)) or Snap's baby face filter ([https://static.boredpanda.com/blog/wp-content/uploads/2019/05/office-characters-snapchat-child-filter-fb37-png\\_\\_700.jpg](https://static.boredpanda.com/blog/wp-content/uploads/2019/05/office-characters-snapchat-child-filter-fb37-png__700.jpg)) can. How does one pick specific features to change in another image? The only thing I can think of is to get 1000s of before-after photos, but I don't think its possible for Fatify to get so many before-after photos. Can you point me to some relevant papers or articles?\n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jetxfz/how_to_use_ml_to_modify_specfic_features_in_an/"}, {"autor": "Morica_", "date": "2020-10-19 10:13:50", "content": "Python keras model with multiple outputs throws error \"ValueError: Graph disconnected:\" /!/ Hey everyone\n\nso I'm pretty new to machine learning and all, and I have sucessfully build my first -----> image !!!  classification model. Now I want to build a model that not only tells me what object is in the Image but also where it is via a bounding box. Therefore, I read a few tutorials about how to build a model with multiple outputs and now I try to build my own model. \n\nI'm currently trying to make a model with 1 input and 2 outputs. The input is a image that get passed through multiple Conv2D layers with MaxPooling and based on this result,\n\noutput 1 should guess the class\n\n&amp;#x200B;\n\nand output 2 should guess the position of the guess object based on the result of output 1 as well as the results from the input Conv layers.\n\nThis is the code for the model I have right now:\n\n&amp;#x200B;\n\n  \n\n\\`\\`\\`def build\\_bbox\\_v2\\_model(NUM\\_CLASSES):  \n\n\n\u00a0\u00a0\u00a0\u00a0inp\u00a0=\u00a0keras.layers.Input(shape=(200,\u00a0200,\u00a03))  \n\u00a0\u00a0\u00a0\u00a0inputs\u00a0=\u00a0Conv2D(32,\u00a0(3,3),\u00a0activation=tf.nn.relu,\u00a0padding='same')(inp)  \n\u00a0\u00a0\u00a0\u00a0inputs\u00a0=\u00a0MaxPooling2D(pool\\_size=(2,\u00a02),\u00a0strides=(2,\u00a02))(inputs)  \n\u00a0\u00a0\u00a0\u00a0inputs\u00a0=\u00a0BatchNormalization()(inputs)  \n\u00a0\u00a0\u00a0\u00a0inputs\u00a0=\u00a0Conv2D(64,\u00a0(3,3),\u00a0activation=tf.nn.relu,\u00a0padding='same')(inputs)  \n\u00a0\u00a0\u00a0\u00a0inputs\u00a0=\u00a0MaxPooling2D(pool\\_size=(2,\u00a02),\u00a0strides=(2,\u00a02))(inputs)  \n\u00a0\u00a0\u00a0\u00a0inputs\u00a0=\u00a0BatchNormalization()(inputs)  \n\u00a0\u00a0\u00a0\u00a0inputs\u00a0=\u00a0Conv2D(128,\u00a0(3,3),\u00a0activation=tf.nn.relu,\u00a0padding='same')(inputs)  \n\u00a0\u00a0\u00a0\u00a0inputs\u00a0=\u00a0MaxPooling2D(pool\\_size=(2,\u00a02),\u00a0strides=(2,\u00a02))(inputs)  \n\u00a0\u00a0\u00a0\u00a0inputs\u00a0=\u00a0BatchNormalization()(inputs)  \n\u00a0\u00a0\u00a0\u00a0inputs\u00a0=\u00a0Conv2D(256,\u00a0(3,3),\u00a0activation=tf.nn.relu,\u00a0padding='same')(inputs)  \n\u00a0\u00a0\u00a0\u00a0inputs\u00a0=\u00a0MaxPooling2D(pool\\_size=(2,\u00a02),\u00a0strides=(2,\u00a02))(inputs)  \n\u00a0\u00a0\u00a0\u00a0inputs\u00a0=\u00a0BatchNormalization()(inputs)  \n\u00a0\u00a0\u00a0\u00a0inputs\u00a0=\u00a0Flatten()(inputs)  \n\u00a0\u00a0\u00a0\u00a0label\u00a0=\u00a0Dense(128,\u00a0activation='relu')(inputs)  \n\u00a0\u00a0\u00a0\u00a0label\u00a0=\u00a0Dropout(0.3)(label)  \n\u00a0\u00a0\u00a0\u00a0label\u00a0=\u00a0BatchNormalization()(label)  \n\u00a0\u00a0\u00a0\u00a0label\u00a0=\u00a0Dense(128,\u00a0activation='relu')(label)  \n\u00a0\u00a0\u00a0\u00a0label\u00a0=\u00a0BatchNormalization()(label)  \n\u00a0\u00a0\u00a0\u00a0label\u00a0=\u00a0Dense(NUM\\_CLASSES)(label)  \n\u00a0\u00a0\u00a0\u00a0label\u00a0=\u00a0Activation('softmax',\u00a0name='label')(label)  \n\n\n\u00a0\u00a0\u00a0\u00a0bbox\u00a0=\u00a0Concatenate(axis=1)(\\[inputs,\u00a0label\\])  \n\u00a0\u00a0\u00a0\u00a0bbox\u00a0=\u00a0Dense(128,\u00a0activation='relu')(bbox)  \n\u00a0\u00a0\u00a0\u00a0bbox\u00a0=\u00a0Dense(64,\u00a0activation='relu')(bbox)  \n\u00a0\u00a0\u00a0\u00a0bbox\u00a0=\u00a0Dense(32,\u00a0activation='relu')(bbox)  \n\u00a0\u00a0\u00a0\u00a0bbox\u00a0=\u00a0Dense(4)(bbox)  \n\u00a0\u00a0\u00a0\u00a0bbox\u00a0=\u00a0Activation('sigmoid',\u00a0name='bbox')(bbox)  \n\u00a0\u00a0\u00a0\u00a0model\u00a0=\u00a0keras.models.Model(inputs=inputs,\u00a0outputs=\\[label,\u00a0bbox\\])  \n\n\n\u00a0\u00a0\u00a0\u00a0losses\u00a0=\u00a0{  \n 'label':\u00a0'sparse\\_categorical\\_crossentropy',  \n 'bbox':\u00a0'mse'  \n\u00a0\u00a0\u00a0\u00a0}  \n\u00a0\u00a0\u00a0\u00a0lossWeights\u00a0=\u00a0{  \n 'label':\u00a01.0,  \n 'bbox':\u00a01.0  \n\u00a0\u00a0\u00a0\u00a0}  \n\u00a0\u00a0\u00a0\u00a0adam\u00a0=\u00a0keras.optimizers.Adam(learning\\_rate=LEARNING\\_RATE,\u00a0decay=LEARNING\\_RATE\u00a0/\u00a0EPOCHS)  \n\u00a0\u00a0\u00a0\u00a0model.compile(  \n optimizer=adam,  \n loss=losses,  \n loss\\_weights\u00a0=\u00a0lossWeights,  \n metrics=\\['accuracy'\\]  \n\u00a0\u00a0\u00a0\u00a0)  \n return\u00a0model\\`\\`\\`\n\n&amp;#x200B;\n\nHowever, if I try to start training, it throws this error: \"ValueError: Graph disconnected: cannot obtain value for tensor Tensor(\"input\\_1:0\", shape=(?, 200, 200, 3), dtype=float32) at layer \"input\\_1\". The following previous layers were accessed without issue: \\[\\]\"\n\n&amp;#x200B;\n\nAny help would be greatly appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jdz16b/python_keras_model_with_multiple_outputs_throws/"}, {"autor": "debbydai", "date": "2020-10-28 10:20:10", "content": "-----> Image !!!  Annotation Tools Comparison /!/ I am a newbie to annotation tools. After trying a few FREE annotation tools, I figured why not share my experience with others. I have tried annotation 3 tools, which are Labelbox, dLabel and supervised.\n\nMy comparison will be in sections. \n\n**1 Uploading**\n\nMy images sample were 50 frames. \n\n    dLabel ------easy to upload but a little confused to see\n\n&amp;#x200B;\n\nhttps://preview.redd.it/sxrpueww6tv51.png?width=1124&amp;format=png&amp;auto=webp&amp;s=477a06f8386408d463a10ee815aaf68ee0ec870f\n\nfinding where to upload your own database is easy to spot and upload. However, after uploading the selected images, it doesn\u2019t show how many images have been uploaded. And it doesn't have a complication button to finish uploading which is a bit confusing. This could be a problem if the data is massive.\n\n    Labelbox -----easy and clear to upload \n\n&amp;#x200B;\n\nhttps://preview.redd.it/7zft40ay6tv51.png?width=872&amp;format=png&amp;auto=webp&amp;s=26f284cb67385c851a57988a5e8bb0703058261d\n\nOnce I sign in, I can start to upload your own data and it shows how many images have been added and easily to complete uploading. Like dLabel you can see your items after uploading the dataset, set annotation tool etc. \n\n    Spervisely \u00a0-------Hard to upload for first time users\n\n&amp;#x200B;\n\nhttps://preview.redd.it/6hrkbel07tv51.png?width=615&amp;format=png&amp;auto=webp&amp;s=259381407541edb8b68ba6019dd973a04b2629eb\n\nThough it has a small demonstration Gif, it still takes more time to import the images as it has too many unnecessary steps such as asking about the uploading way and you can only add the project name after uploading your dataset. \n\n**2 adding annotation objects**\n\n    dLabel\u00a0   \n\nI can add annotation objects at both after uploading your dataset and whenever you want. And it has this \u201cattribute\u201d function\u201d where you can define, add more information, classify the objects.  But I can not edit the added annotation objects. \n\n    Labelbox\n\nLike dLabel, you can create annotation objects whenever you want. However, it is less convenient than dLabel as you need to go to the editor\u2019s page and edit or add more annotation objects. There is \u201cclassification\u201d function where you can classify the objects. I personally like the \u201cattribute \u201c feature from dLabel more and it can contain more information about the objects. \n\n    Spervisely \n\nAdding and modifying  the annotation objects are easy and clear. There is no feature like or similar to \u201cattribute\u201d from dLabel or \u201cclassification\u201d form Labelbox. One good part is that I can personalize my own hotkeys for each object, which can adapt an individual's annotation habit.\n\n**3 Annotation speed for 30 frames.**\n\nThe data set is 30images. Using bonding boxes. 2 annotation objects (hand and drink)\n\n&amp;#x200B;\n\n||uploading time time|labeling time|Clicks per frame|\n|:-|:-|:-|:-|\n|dLabel|2 min|9 min|7 clicks|\n|Labelbox|2 min|12min 10 sec|7 clicks|\n|Supervisly|4 min|12min 30 sec|9 clicks|\n\n&amp;#x200B;\n\n**4 layout of the annotation page** \n\n    dLabel \n    \n\n&amp;#x200B;\n\nhttps://preview.redd.it/et1osbo68tv51.png?width=1367&amp;format=png&amp;auto=webp&amp;s=77419947a904a3667c88e50289f1188f515ca7a7\n\nYou have the features at the left, the statues in the right and the image in the centre. \n\nOne the left side, there is one \u201cgroup\u201d feature which I found is very useful. It adds interaction between objects. Like you set the \u201ctomato\u201d and the \u201chand\u201d into one group which indicates they have interactions. The hand is picking the tomato. \n\nThere is a history column on the right where you can also trace the annotation history on the right. \n\nOn the bottom of the centre, there\u2019s one QA bar which can come in hand when doing quality control. \n\n**Labelbox** \n\n&amp;#x200B;\n\nhttps://preview.redd.it/rbk5mpq88tv51.png?width=1374&amp;format=png&amp;auto=webp&amp;s=fae5a6bef8937d97bc3a2bf2799d3f1737ac9ab5\n\nI personally find the layout too simple. There are not many features. So There is nothing much to say. \n\n**Spervisely** \n\n&amp;#x200B;\n\nhttps://preview.redd.it/37mewkw98tv51.png?width=1425&amp;format=png&amp;auto=webp&amp;s=3e45d7332340a1531b288b72f5650b299aac6865\n\nthe right side features are bit confusing...\n\nI didn't quite understand what are\"image properties\" and \"objects properties\". \n\nIt shows the annotation history which Labelbox doesn't have hotkeys easily  .", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jjl6ao/image_annotation_tools_comparison/"}, {"autor": "butchland", "date": "2020-10-28 10:15:13", "content": "Build (and Run!) Your Own -----> Image !!!  Classifier! - a quick video tutorial for Deep Learning beginners", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jjl49r/build_and_run_your_own_image_classifier_a_quick/"}, {"autor": "always-stressed", "date": "2020-10-28 05:32:10", "content": "Dataset Creation: Image Classification Software /!/ Hi Is there an -----> image !!!  classification software out that I can load a bunch of -----> image !!! s in from a specific folder and then I can just go through them and select each class and it outputs them in the Pytorch ImageFolder structure?\n\n```\nroot\\\n    class1\\\n         pic1.jpg\n         pic2.jpg \n    class2\\ \n         pic5.jpg\n         whocares.jpg\n```\n\nI would like to find something like this for Image classification, NOT for object detection. I know of labelImg but it looks like you have create bounding boxes which is not what I would like. \n\nThanks in advance for your help!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jji3ug/dataset_creation_image_classification_software/"}, {"autor": "emdayr123", "date": "2020-10-27 20:12:50", "content": "Does anyone know any good tutorials for using machine learning to identify multiple components in an -----> image !!! ? Read description. /!/ So if I were to have an image with limes and different types of shapes. Any good tutorials for identifying those an image with multiple components that may look alike?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jj8vhz/does_anyone_know_any_good_tutorials_for_using/"}, {"autor": "GubbyGubbler", "date": "2020-10-27 19:18:24", "content": "Nonlinearities in CNN? /!/ Why can't I just apply a filter to the input -----> image !!! ? For example, if I convolve a vertical edge detector feature with an image, it detects a vertical edge. So what's the problem? And, more importantly, how do nonlinearities relate to pixel values in an image and what will the feature map look like after I apply one?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jj7s5b/nonlinearities_in_cnn/"}, {"autor": "blessedrng", "date": "2020-10-27 15:50:31", "content": "College student: What math courses would be best for me to be taking as foundation for NLP/Machine Learning? /!/ I'm currently a sophmore dual majoring in CS and Math (statistics emphasis) whose ideal is to become well-versed in the world of NLP and machine learning in general. I'm wondering what an \"optimal\" path might look like, that is, if 10 years from now I was looking back at the courses I was choosing (with strong knowledge of what is important in the fields of NLP/ML), what courses would that future me advise current me to take?\n\nAt risk of spoiling what school I go to (not like it matters, u of utah), here is a -----> picture !!!  of the courses offered/needed for a math + stats emphasis major:\n\n[https://imgur.com/DWNbdUU](https://imgur.com/DWNbdUU)\n\nApologies if this comes off as a \"do my homework for me\" post; I would just really feel eased having a guideline on what math electives would be best for my area of interest. For example, maybe stochastic processes is unbeknownst to me rather relevant to NLP and would be quite a good idea to take.\n\nAnyone working in this area or has knowledge of what math this area entails, which of those electives (bottom-right list) would you consider to be \"def a good idea to take that one\"? Huge thanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jj3oac/college_student_what_math_courses_would_be_best/"}, {"autor": "SalemCruz", "date": "2020-10-27 14:37:02", "content": "Simplest way to implement a search engine /!/ I was using Teachable Machine and I got to train some classes based on samples from an -----> image !!!  bank that I have on my computer with about 30k -----> image !!! s. This website allows you to export these models. My question is: what would be the simplest way for me to implement a search system based on these classes that I trained to be used in my image bank? More specifically, what I want is something like a filter that selects from among these 30k images only those that correspond to a chosen class.\n\n&amp;#x200B;\n\nIs there any kind of framework that could make this task easier, or would I have to create from scratch programming in some language, like Python, to apply these exported models in some way?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jj2aew/simplest_way_to_implement_a_search_engine/"}, {"autor": "IronMan3197", "date": "2020-10-27 08:21:28", "content": "Guide to analyse a human face /!/ Hi Guys!\n\n I have a project where I need to analyse the human face in completion, as in, I want all the details of the face from the corresponding -----> picture !!!  of the face. The details that I need include the distances and lengths from(/of) the sense organs to other sense organs, complexion etc. How do I go about the same? Any help or study material will be appreciated. \n\nThanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jix9c1/guide_to_analyse_a_human_face/"}, {"autor": "SnoStrawberries5", "date": "2020-10-27 02:29:47", "content": "[D]Converting biology data to -----> image !!!  for CNN input? /!/ \nI have 640 samples of RNA seq data with 10k genes. I can reduce the genes to 700 if I only account for causal implication of cancer.\n\n I understand gene expression feature vectors do not have local structure. What are some tools I can use to provide better gene expression input into the cnn. There are some that \u201cconvert to an image\u201d (https://www.nature.com/articles/s41598-019-47765-6) (https://www.pnas.org/content/116/52/27151) tools. How do you feel about them?\n\nAny suggestions or tips are greatly appreciated. I am creating a 2D regression CNN that incorporates 3D drug structures and gene expression.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jiswzw/dconverting_biology_data_to_image_for_cnn_input/"}, {"autor": "msamericana1", "date": "2020-10-27 01:35:38", "content": "If someone has an -----> image !!!  that has its edges detected? What\u2019s the next step for identifying the comments of the image?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jis2ay/if_someone_has_an_image_that_has_its_edges/"}, {"autor": "headwar", "date": "2020-10-26 21:13:21", "content": "-----> Image !!!  recognition app with custom libraries? /!/ Dear reddit\nNot a coder myself but understand the basics. I was wondering: Is there an image recognition smartphone app like Taptapsee and Aipoly that allows to add my own libraries? Aipoly already offers several modes (Food, currencies,...), but they\u2019re preset. Wouldn\u2019t it be cool to have an app that allows more customization depending on my context? Finding ml libraries is not so hard..\n\nThank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jinff2/image_recognition_app_with_custom_libraries/"}, {"autor": "madzthakz", "date": "2020-10-26 15:07:04", "content": "I'm a Senior Data Scientist at Disney and I'm hosting another free Data Science Q&amp;A session this Thursday @ 5:30 PM PST /!/ \\[Disclaimer: These are completely free!\\]\n\nAs the title mentions, I'm a Senior Data Scientist at Disney and I'm going to host another Data Science Q&amp;A this Thursday at 5:30 PM PST. Some of you may have already registered but I still wanted to post so other folks here have an opportunity to attend. All of the sessions in the past have been a blast and we've tackled questions ranging from interview prep to how to build a churn model. \n\nHope to see you there! \n\nRegistration Link:\n\n[https://disney.zoom.us/webinar/register/WN\\_odHPvMGbS6GXHPoYDDL9OA](https://disney.zoom.us/webinar/register/WN_odHPvMGbS6GXHPoYDDL9OA)\n\nMore Data Science Content:\n\n[https://www.madhavthaker.com/qaposts](https://www.madhavthaker.com/qaposts)\n\nVerification:\n\n* My -----> photo !!! : [https://imgur.com/a/Wg3DMLV](https://imgur.com/a/Wg3DMLV)\n* My LinkedIn: [https://www.linkedin.com/in/madhavthaker/](https://www.linkedin.com/in/madhavthaker/) (Feel free to connect!)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jig7jw/im_a_senior_data_scientist_at_disney_and_im/"}, {"autor": "hellopaperspace", "date": "2020-11-27 16:11:42", "content": "[Tutorial] How To Fine-Tune Shallow Networks with Keras /!/ This tutorial explores how to fine-tune shallow networks for relatively computationally inexpensive -----> image !!!  classification. We\u2019ll compare models with only one or two hidden layers, various different numbers of neurons, differing activation functions, etc. The shallow model is able to achieve state-of-the-art performance.\n\nThis study shows that building deeper neural networks is not always necessary; instead, it can be more important to focus on the correct number of neurons in each layer. The aim of this study is to consider an optimal number of neurons in relatively shallow networks to effectively complete various tasks, thus eradicating our necessity for heavyweight models, while reducing expensive hardware requirements and time complexity.\n\nArticle link: [https://blog.paperspace.com/fine-tuning-shallow-networks-keras/](https://blog.paperspace.com/fine-tuning-shallow-networks-keras/)\n\nRun the code on a free GPU with Gradient Community Notebooks: [https://ml-showcase.paperspace.com/projects/fine-tuning-shallow-neural-networks-with-keras](https://ml-showcase.paperspace.com/projects/fine-tuning-shallow-neural-networks-with-keras)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k23yzv/tutorial_how_to_finetune_shallow_networks_with/"}, {"autor": "Snoo_85410", "date": "2020-11-27 15:10:51", "content": "[Research] Castle in the Sky: Dynamic Sky Replacement and Harmonization in Videos /!/ [Check out the paper presentation by 3-minute papers!](https://crossminds.ai/video/5fbeaf82550096d304510009/?playlist_id=5f07c51e2de531fe96279ccb)\n\n**Abstract:**\n\nWe propose a vision-based method for video sky replacement and harmonization, which can automatically generate realistic and dramatic sky backgrounds in videos with controllable styles. Different from previous sky editing methods that either focus on static photos or require inertial measurement units integrated in smartphones on shooting videos, our method is purely vision-based, without any requirements on the capturing devices, and can be well applied to either online or offline processing scenarios. Our method runs in real-time and is free of user interactions. We decompose this artistic creation process into a couple of proxy tasks including sky matting, motion estimation, and -----> image !!!  blending. Experiments are conducted on videos diversely captured in the wild by handheld smartphones and dash cameras, and show high fidelity and good generalization of our method in both visual quality and lighting/motion dynamics.\n\n**Video Sky Augmentation:**\n\nOur method produces vivid blending results with a high degree of realism and visual dynamics. With a single NVIDIA Titan XP GPU card, our method reaches a real-time processing speed (24 fps) at the output resolution of 640 x 320 and a near real-time processing speed (15 fps) at 854 x 480. The following gives several groups of our blending results on outdoor videos (floating castle, fire cloud, super moon, and galaxy night).\n\n**Weather/Lighting Translation:**\n\nAs a by-product, our method can be also used for image weather and lighting translation. A potential application of our method is data augmentation. Domain gap between datasets with limited samples and the complex real-world poses great challenges for data-driven computer vision methods. For example, domain sensitive visual perception models in self-driving may face problems at night or rainy days due to the limited examples in training data. We believe our method has great potential for improving the generalization ability of deep learning models in a variety of computer vision tasks such as detection, segmentation, tracking, etc. This is one of our future work.\n\nAuthor: Zhengxia Zou\n\n[Project Link](https://jiupinjia.github.io/skyar/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k22twm/research_castle_in_the_sky_dynamic_sky/"}, {"autor": "drollerfoot7", "date": "2020-11-27 11:49:14", "content": "Finding wally/waldo with AI /!/ Hi,\n\n&amp;#x200B;\n\nI'm looking for a simple project where I can detect wally in images. All the things I've tried only work when the -----> image !!!  is already trained but new -----> image !!! s won't work. I tried to find projects on Github but they all seem to have the same problem or have extremely complex code (I'm a beginner). \n\nIf anyone can help me out by linking a project or get me on track by explaining the best method that would be appreciated.\n\n&amp;#x200B;\n\nThanks for reading!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k1zrf7/finding_wallywaldo_with_ai/"}, {"autor": "kk_ai", "date": "2020-11-27 07:04:40", "content": "Beginners guide to data augmentation for deep learning /!/ The article covers basics of data augmentation and presents some nice libraries to do this task easier. It's useful for beginners and DL specialists who need quick refresher of common techniques.\n\nSpecifically we cover:\n- What is Data Augmentation \u2013 definition, the purpose of use, and techniques,\n- Built-in augmentation methods in DL frameworks \u2013 TensorFlow, Keras, PyTorch, MxNet,\n- -----> Image !!!  DA libraries \u2013 Augmentor, Albumentations, ImgAug, AutoAugment, Transforms,\n- Speed comparison of these libraries,\n- Best practices, tips, and tricks.\n\n[Beginners guide to data augmentation](https://neptune.ai/blog/data-augmentation-in-python?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=blog-data-augmentation-in-python&amp;utm_content=learnmachinelearning)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k1wdxc/beginners_guide_to_data_augmentation_for_deep/"}, {"autor": "gabegabe6", "date": "2020-11-26 15:22:34", "content": "Lightweight/SLE GAN reproduction with (hopefully) clean and understandable code /!/ - [**My Code - SLE-GAN**](https://github.com/gaborvecsei/SLE-GAN)\n- [**Paper**](https://openreview.net/forum?id=1Fqg133qRaI) submitted to ICLR 2021\n\nI tried to reproduce the discussed techniques in the paper and try it out on the *Oxford 17 flowers* dataset with a small batch size and without any additional methods.\n\nBased on my initial results the model suffers from mode collapse and you can overcome by applying differential augmentations.\n\nLater on I will attach (more) results and FID scores so I can investigate the effectiveness of the solution.\n\nBtw. my problem with the paper is that it does not describe the following which makes the implementation and experimentation harder:\n- architecture in details and how they modified for the different -----> image !!!  resolutions\n- training schedule\n- hyperparameters and differences with big and small (few shot) dataset training\n- they do not provide training loss graphs with which we could compare our trainings", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k1h09e/lightweightsle_gan_reproduction_with_hopefully/"}, {"autor": "jdaviddx", "date": "2020-11-26 14:12:18", "content": "Need an idea to recreate text from text spotting recognition algorithm /!/ Hello I need some help, with the following problem, if you know something about it. I use an algorithm to spot text in images (text scene recognition). The algorithm I found works fine, but I dont find anything usefull, or some research of how to arrange those word instances returned by the algorithm, in order to form a text so that I can feed it in a text2speech algorithm.  \nMy first idea was to iterate the bounding box returned, left right, up down but it doenst work in unstructured text (multiple columns by example you can see the -----> image !!! ).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k1fvgf/need_an_idea_to_recreate_text_from_text_spotting/"}, {"autor": "ahmedbesbes", "date": "2020-11-26 11:32:19", "content": "Data science portfolio: tutorials, projects and code /!/ Hey everyone!\n\nI've revamped my portfolio and gathered in one place different projects and tutorials (with code) on different topics:\n\n* deploying machine learning apps to production\n* computer vision (medical -----> image !!!  for example)\n* natural language processing\n* deep learning\n\nHope this helps!\n\nFeel free to share ideas of tutorials you'd like to see covered!   \n\nhttps://www.ahmedbesbes.com/", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k1dpmo/data_science_portfolio_tutorials_projects_and_code/"}, {"autor": "rupam268", "date": "2020-11-26 10:37:41", "content": "Understqnding -----> Image !!!  Processing Using Artificial Neural Networks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k1d1k2/understqnding_image_processing_using_artificial/"}, {"autor": "123trinitroxypropane", "date": "2020-11-26 05:54:41", "content": "Has anyone found the Google Landmark Images dataset with the name labels? /!/ Google provides its Landmark recognition and retrieval dataset with -----> image !!!  links online, however, the labels in the train set only contain -----> image !!!  id (i.e. 51255) as labels and not the actual names of the locations (i.e. Statue of Liberty, etc).\n\nHas anyone found a dataset that contains these named labels?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k19ke2/has_anyone_found_the_google_landmark_images/"}, {"autor": "tokyostormdrain", "date": "2020-11-25 21:08:48", "content": "-----> Image !!!  comparison /!/  \n\nHi,\n\nI want to find a way with machine learning to find self similar imagss amongst a large set of images.\n\nSo  I might have a wooden plank and want to get a score from the other  images of how close any of them are to my wooden plank image. I'm not  well versed in ML, but it seems I might need some sort of CNN trained on  my image set that I can then test the images in that set against?\n\nI'm not sure if there are any easily trainable models already out there that i can apply to this?\n\nThanks for any pointers", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k10xyi/image_comparison/"}, {"autor": "SuccMyStrangerThings", "date": "2020-11-25 19:22:53", "content": "A few general queries about neural nets /!/ 1. How is the input given to the neural net input layer? I mean when an -----> image !!!  is given to the network, is the same -----> image !!!  given to every unit in the input layer or the parts of the -----> image !!!  are distributed across the input layer? \n\n2. Shouldn't the Mnist digit classification CNN have 11 layers in the ouput layer? 10 neurons for 10 classes and 1 extra neuron for an image with a class that doesn't belong to the predefined 10 classes.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k0yvt7/a_few_general_queries_about_neural_nets/"}, {"autor": "msamericana1", "date": "2020-11-25 17:54:41", "content": "Genetic algorithm idea? Anyone want to check it out to see if it\u2019s possible? /!/ Hello, I randomly thought of an idea to use a genetic algorithm. Although, I\u2019m not sure if it is completely possible...\n\n\u2014- Here\u2019s how it would work. The computer would lay a random 3D face on top of a -----> photo !!! . The progress would be split up by pieces of the face. Let\u2019s say we do the nose first. The genetic algorithm would create new generations until the random 3D nose matches the nose from the photo. The fitness function could be the dimensions and such of how the 3D nose compares to the photo nose. This continues until a final 3D model of the photo is created.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k0x1wh/genetic_algorithm_idea_anyone_want_to_check_it/"}, {"autor": "Rayryu", "date": "2020-11-25 17:53:17", "content": "Given only an -----> image !!! s dataset of 10 classes, how to build a model to recognize if an -----> image !!!  is in the 10 classes of this dataset or not?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k0x0uo/given_only_an_images_dataset_of_10_classes_how_to/"}, {"autor": "BoldSlogan", "date": "2020-11-25 16:51:51", "content": "Authenticate your passport with your phone and machine learning /!/  I created an app that can read and verify the personal data on passports using the secure chip embedded inside.\n\nThe app works by first using the -----> camera !!!  to scan the passport\u2019s data page, a step that is needed because the key used for reading the embedded chip is constructed out of the visible printed passport data. Then it will read and verify the embedded chip and display the extracted information.\n\nHere is a 30 second demo: https://www.youtube.com/watch?v=VTdpOcG1NSw\n\nI am looking for users/testers with different passports to try it out. I wanted to see if my weekend hackathon project actually works or if it is only my passport and my friends that works. I made some speed improvements/bug updates.\n\nThe app is available for both iOS and Android and requires an NFC-enabled device (most modern Android devices and all iPhones starting with the iPhone 7 are NFC-enabled). You will also need a biometric passport (sometimes called electronic passport) that you can read. Most modern passports issued today are biometric passports, and you can verify this by looking for a microchip icon usually printed on the passport cover. \n\nhttps://apps.apple.com/us/app/biometric-passport-reader/id1510585886\nhttps://play.google.com/store/apps/details?id=app.iris\n\n**Feel free to use an invalid passport, the app does not send any personal data, and it has been manually reviewed by Apple and Google**", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k0vr7o/authenticate_your_passport_with_your_phone_and/"}, {"autor": "eliteqing", "date": "2020-11-25 16:18:30", "content": "[D] Learn ML: build a PC or buy M1 mac? /!/ **Background**: I currently do research in data and -----> image !!!  processing. I plan to learn Machine Learning related packages both as interest and also ready for possible future career development. I mostly use python, so most ML I can think of such as Tensorflow, PyTorch, Horvod for multi-core parallel computing. I may also need to use C++/C# for algorithm development in OpenCV.\n\n**Question**:  would my situation better to buy the 16Gb M1 Mac book pro/mini  or  build a PC at similar cost?  (PC build \\~:  AMD r9 3900x, RTX 2060s, 32Gb ddr4 3600Mhz, 1Tb NVMe ssd, ...)\n\n**Pros &amp; Cons**: Macbook pro is better for mobility, but would have less computing power? some software compatibility issue, but should be fine over a year or two? \n\nIf the M1 Mac can reach 80-90% performance of a AMD+GPU desktop PC of same price,  then I will be happy to obtain a M1, for its advantage on mobility and  versatility. \n\nany comments is appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k0v3iy/d_learn_ml_build_a_pc_or_buy_m1_mac/"}, {"autor": "s8R2rXjbNU", "date": "2020-12-05 14:52:46", "content": "Predict age, gender and race without using CNN /!/ As the title suggests, I have a task of predicting age, gender and race from an -----> image !!! , without using CNNs. I've looked into this and all the popular solutions that I've found use CNNs. What other type of model can be good for this?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k78s03/predict_age_gender_and_race_without_using_cnn/"}, {"autor": "deepclearliquidsnow", "date": "2020-12-04 17:57:06", "content": "Any ideas for how to put together my ML project idea? I want to do something in Tensorflow that can scan photos to recognize missing information on forms /!/  Hi folks, I am pretty new to ML but I've been teaching myself Python and Tensorflow via Coursera/some books and I really enjoy computer vision.\n\nI need help figuring out the architecture of this idea I have...anyone willing to spare some thoughts please? I work at a bureaucratic organization and one of my functions is to check paperwork for missing fields or errors. The applications I check are usually 3-4 pages but I want to try for the first 2 pages only.\n\nMy idea: create a model that could have a -----> photo !!!  uploaded to recognize which info is missing (blank spaces) or incorrect (for example, a signature that's too big which needs to be in a certain bounded box, incomprehensible writing, etc)\n\nI'd then like to add on a graphical part to the model which would box in red what info is missing or needs attention..\n\nI don't know how I would do this though - I figure a CNN would be involved but I'm struggling to figure out how to structure this\n\nIf you were me, what would you do? I thought I'd start by putting together dozens of fake applications myself with varying levels of info (some filled out entirely, some half filled out, some missing just 2 boxes of info) and using those images to train.\n\nAny pointers, please??\n\nI am soooo lost tbh. I am a newbie so any guidance would be super appreciated\n\n(I'm basically trying to figure out how to automate myself out of a job - while I can scan these applications,, I miss things here and there, and my job would be easier if I could have the vital info to review somehow highlighted first rather than reading the entire thing). I only know a bit of Python so I'd be looking to do it in this language.\n\nThank you!!!!!!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k6pwsm/any_ideas_for_how_to_put_together_my_ml_project/"}, {"autor": "black0017", "date": "2020-12-03 15:40:02", "content": "[D] - How to use Docker containers and Docker Compose for Deep Learning applications /!/ Containers have become the standard way to develop and deploy  applications these days with Docker and Kubernetes dominating the field.  This is especially true in machine learning and for a very good reason!  Containers provide flexibility to experiment with different frameworks,  versions, GPU\u2019s with minimal overhead. Besides, they eliminate  discrepancies between the development and the production environment,  they are lightweight compared to virtual machines, and they can easily  be scaled up or down.\n\nIn this article, we will containerize our Deep Learning application using Docker. Our application consists of a [Tensorflow model that performs -----> image !!!  segmentation](https://theaisummer.com/deep-learning-production/), [Flask](https://theaisummer.com/deploy-flask-tensorflow/), [uWSGI for serving purposes, and Nginx for load balancing](https://theaisummer.com/uwsgi-nginx/). For those who haven\u2019t followed this article series, I suggest having a look at the previous articles.  \n\n\n[https://theaisummer.com/docker/](https://theaisummer.com/docker/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k5zhsh/d_how_to_use_docker_containers_and_docker_compose/"}, {"autor": "Laurence-Lin", "date": "2020-12-03 01:44:58", "content": "I have large size -----> image !!!  for defect detection task, should I apply dimension reduction? /!/ I'm currently working on images for defect detection task of size 1900\\*920, which is larger than most of the pre-trained models in public. For example, YOLO receives input image of size 448\\*448\\*3, which is much smaller than my case.\n\n&amp;#x200B;\n\nAlthough some models in keras could receive larger image for the pre-trained models, it may took more time to calculate and the performance may not be ideal. \n\n&amp;#x200B;\n\nShould I deploy dimension reduction technique to match the size of these models, like YOLO or faster-RCNN? \n\n&amp;#x200B;\n\nI've some considerations of shortcomings: Since I'm doing detection for regional defect, during the dimension reduction, the small defect part might be distorted or vanished.\n\n&amp;#x200B;\n\nAny advice is appreciated! Thank you", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k5nmq6/i_have_large_size_image_for_defect_detection_task/"}, {"autor": "s_basu", "date": "2020-12-14 14:06:52", "content": "Help understanding a particular code /!/ Hi,\n\nI was going through \"Deep Learning for Coders with Fastai and Pytorch\" by Jeremy Howard and Sylvan Gugger. \n\nIn the Resnet section, there was a particular code that I would like to talk about.\n\n    def block(ni, nf): return ConvLayer(ni, nf, stride=2)\n    def get_model():\n    return nn.Sequential(\n    block(3, 16),\n    block(16, 32),\n    block(32, 64),\n    block(64, 128),\n    block(128, 256),\n    nn.AdaptiveAvgPool2d(1),\n    Flatten(),\n    nn.Linear(256, dls.c))\n\nHere, each block is a convolution layer with stride 2, padding 1 and kernel size (3, 3) (defined in a previous chapter).\n\n If that is the case, and assuming we are working with Imagenet's 128\\*128\\*3 sized -----> image !!! s, the first block generates 16 channels of 64\\*64 -----> image !!!  grids (worked out using the known formula for convolution operation). The second block generates 32, 32\\*32 image grids. The third 64, 16\\*16 grids and so on.\n\nUntil at the final block, we have 256, 4\\*4 grids.\n\nNow, is the AdaptiveAvgPool2d(1)  simply taking an average of each 4\\*4 grids and producing a single scalar (or 1\\*1 grid) 256 times, so we end up with a (256, 1) tensor which we then flatten?\n\nI just need confirmation that this is correct. Thanks.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kcy8qm/help_understanding_a_particular_code/"}, {"autor": "kmeanskeal", "date": "2020-12-14 01:32:13", "content": "I want to detect/track my cats, where do I start? /!/ I want to track them with a -----> camera !!!  from a fixed position in the kitchen to see when they are on the counters. What is my most efficient path for getting there? Does anyone know of examples of this or similar?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kcnn61/i_want_to_detecttrack_my_cats_where_do_i_start/"}, {"autor": "cloud_weather", "date": "2020-12-13 20:21:30", "content": "Liquid Warping GAN - \"Deepfake\" Movements with 1 -----> image !!!  ONLY", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kchxcj/liquid_warping_gan_deepfake_movements_with_1/"}, {"autor": "dhiraj8899", "date": "2020-12-13 08:26:09", "content": "Do you think this -----> image !!!  helps you understand Gradient Descent Algorithm Better?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kc75bk/do_you_think_this_image_helps_you_understand/"}, {"autor": "Rayryu", "date": "2020-11-08 21:51:29", "content": "Given only CIFAR-10 dataset, how to build a model to recognize if an -----> image !!!  is in the 10 classes of CIFAR-10 or not?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jqkeyv/given_only_cifar10_dataset_how_to_build_a_model/"}, {"autor": "IvanIvanicIvanovski", "date": "2020-11-08 15:02:18", "content": "Detection of one type of object in a -----> picture !!! . Where to start? /!/ Hi everyone, \n\n&amp;#x200B;\n\nIn order to survive the quarantine, I started a little project to keep myself busy. I love watching birds in my free time and decided to build on that. I mounted a Raspberry Pi and camera to my window, and placed a feeder on the outside. With the [pi-timolo](https://github.com/pageauc/pi-timolo) package it takes a picture when motion is detected. Now I want to analyse the pictures to determine if birds are captured. Unfortunately, the script also detects passing cars and I'm not interested in that.\n\n&amp;#x200B;\n\nI've had some experience with machine learning (ANN, k-means, market basket) but this was restricted to numerical data. I honestly don't know where to start with pictures. Picking something for my specific situation is somewhat difficult for me. **What are some good techniques or methods to do this?** I'm not (yet) interested in determining the species. For now, knowing if there is a bird or not will suffice. A true/false or yes/no situation. I also have some experience with Python and R when it comes to programming. I should also mention that my hardware isn't state of the art", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jqd2wh/detection_of_one_type_of_object_in_a_picture/"}, {"autor": "ermagawsh", "date": "2020-11-08 12:46:05", "content": "Is it possible to use an ANN to score handwriting based on its legibility? /!/ For simplicity just with numbers, and would be trained on MNIST. If the user inputted a -----> picture !!!  of a number would the network be able to take a large sample size and if it gets many wrong then the user has a lower legibility score.  Eg if it gets one wrong but 9 right it would be 90%, and if the score is above 80% the handwriting is deemed legible.\n\nOf course, that is possible to do, but does it make sense? Does that really count as how illegible your handwriting is or is it based on the neural networks training? Because I was thinking of doing this as a project which deems if your handwriting is generally neat or not, however some doubts have come into my mind that an ANN isn\u2019t looking at an image like a human would for it to say it\u2019s legible or not. \nPerhaps a CNN would work for this purpose? But maybe even that wouldn\u2019t be fit for the purpose, and this whole project won\u2019t really work for neural networks. \n\nI was just looking for some advice on this, thank you for your time reading this", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jqb5tg/is_it_possible_to_use_an_ann_to_score_handwriting/"}, {"autor": "serDavosOfSeaworth", "date": "2020-11-08 10:22:11", "content": "How to extract tables from images? /!/ I want to extract tables from a large set of images of same format ( [example](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTvE1PBfKbCO_9nkxfryFP9cGcUtLb89TOVpw&amp;usqp=CAU)) after getting initial location of table's starting with user input. \nFor example in the -----> image !!!  in the link, I will ask the user to annotate the table heading and first row by drawing a box. This will serve as a template and then I will know where the table starts but I don't know how to:\n1. Find out where the table ends\n2. Handle the cases where one of row values is multi line\n3. Format the data, i.e. store the row values for respective column values. Example: \nRow 1 =\n{\n  description = 'Project',\n  rate = '$5000',\n  Qty = 1,\n  LineTotal = '$5000'\n},\nRow 2 = \n... and so on\n\nCurrently I've tried direct tesseract to parse the entire table but problem is the number values are not recognised sometimes and I don't know how to match row and column values.\nI've also tried openCV bounded box method but a lot of the times the table columns aren't recognised properly (columns get merged)\nI've also tried [TableNet](https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;url=https://arxiv.org/abs/2001.01469&amp;ved=2ahUKEwjn4NDS2_LsAhVRmuYKHXlrBLgQFjAAegQIARAB&amp;usg=AOvVaw2Ca72sUOpaJcPSmqwTs3Pd&amp;cshid=1604830623423) but not having good datasets has proved an issue for non-research paper documents.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jq9j7i/how_to_extract_tables_from_images/"}, {"autor": "ml-agent", "date": "2020-11-07 16:19:28", "content": "How do you like this new GUI for tensorflow? /!/ [Perceptilabs](https://www.perceptilabs.com) has been working on building an easy to use GUI for machine learning frameworks, especially Tensorflow to make modeling faster, simpler and intuitive for ml enthusiasts from different fields. This approach can lower the barrier of entry for beginners while providing advanced users with code-level access to their models. What do you think about it? I myself have been practicing machine learning for past 6 years and started working at Perceptilabs about a year ago. I would appreciate what you like about this platform, what you don't like and what kind of features do you think are missing :) \n\nHere's a tutorial on building a simple -----> image !!!  classification model using Perceptilabs platform.   \n[https://www.youtube.com/watch?v=-w4kOh3C\\_po](https://www.youtube.com/watch?v=-w4kOh3C_po)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jptkn9/how_do_you_like_this_new_gui_for_tensorflow/"}, {"autor": "Numerous-Statement91", "date": "2020-11-04 05:42:43", "content": "Hi, python newbie and machine learning enthusiast here, I wanted to ask that is it possible to apply spectral clustering on an adjacency matrix with directed edges? /!/ I am creating a real-time analytics system with Java, and for clustering the data, I am using Python. Although my work is 80% complete, I am stuck in spectral clustering as the adjacency matrix of my data is based on directed edges between nodes. I am following this fantastic [tutorial](https://towardsdatascience.com/spectral-clustering-aba2640c0d5b) here but the problem is that this example pertains to undirected edges. I have read articles that paint a -----> picture !!!  that applying spectral clustering on directed edges is complex. Is there any way that I can use the shared link tutorial and change the degree matrix according to directed edges to obtain spectral clusters of adjacency matrix?\n\nDo the libraries in R or Python support this endeavor? or are there any links to tutorials even of basic nature that can help me in this task?  \nThanks in advance", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jnr521/hi_python_newbie_and_machine_learning_enthusiast/"}, {"autor": "thanhngtr", "date": "2020-11-04 03:02:11", "content": "Build a model object detection from scratch without using pre-train model /!/ I have a dataset containing more than 500 images which contain car, bike. Along with the -----> image !!! s, i have a text file  associated with each -----> image !!!  which contain:\n\n\\- bounding box of object in the -----> image !!!  and class of object\n\n I need to train a model which takes an -----> image !!!  as an input and outputs 4 integer values which are the coordinates for the bounding box ( vertices of the bounding box ) and 1 float value which is score of object.\n\nCan anyone give me an advise? Thank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jnowqo/build_a_model_object_detection_from_scratch/"}, {"autor": "muaz65", "date": "2020-09-15 00:13:07", "content": "[D] Suggestions regarding deep learning solution deployment /!/ I have to deploy a solution where I need to process 135 -----> camera !!!  streams in parallel. All streams are 16 hours long and should be processed within 24 hours. A single instance of my pipeline takes around 1.75 GB to process one stream with 2 deep learning models. All streams are independent and the output isn't related. I can process four streams in real-time on 2080 ti (11 GB). After four, the next instance starts lagging. That doesn't let me process more streams given the remaining memory (\\~4GB) of the GPU.\n\nI am looking out for suggestions regarding how can this be done in the most efficient way. Keeping the cost and efficiency factor in mind. Would making a cluster benefit me in the current situation?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/isxkah/d_suggestions_regarding_deep_learning_solution/"}, {"autor": "Longtursk", "date": "2020-12-11 17:44:00", "content": "How to transform time series weather data into non-stationary data to fit ARIMA model /!/ I have a sample of a dataset of weather data from a sensor that took temperature readings every hour for about 100 hours that I need to run a time series analysis on. The data is non-stationary as it contains trends and seasons which can be seen in the first -----> photo !!! . I need to transform this data into non-stationary data to fit an ARIMA model but my efforts have proven futile. How may I fit an ARIMA model to this data since none of the transformations I tried worked?\n\n&amp;#x200B;\n\n1. This is a plot of the weather data with trends and seasons\n\n2. ACF and PCF\n\n3. First order differencing\n\n4. Second order differencing\n\n5. log transform and first order differencing\n\n6. sqrt transform and first order differencing\n\n&amp;#x200B;\n\n\\[!\\[This is a plot of the weather data\\]\\[1\\]\\]\\[1\\]\n\n&amp;#x200B;\n\n\\[!\\[ACF and PCF\\]\\[2\\]\\]\\[2\\]\n\n&amp;#x200B;\n\n\\[!\\[First order differencing\\]\\[3\\]\\]\\[3\\]\n\n&amp;#x200B;\n\n\\[!\\[Second order differencing\\]\\[4\\]\\]\\[4\\]\n\n&amp;#x200B;\n\n\\[!\\[log transform and first order differencing\\]\\[5\\]\\]\\[5\\]\n\n&amp;#x200B;\n\n\\[!\\[sqrt transform and first order differencing\\]\\[6\\]\\]\\[6\\]\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n  \\[1\\]: [https://i.stack.imgur.com/fgcYW.png](https://i.stack.imgur.com/fgcYW.png)\n\n  \\[2\\]: [https://i.stack.imgur.com/gSTqa.png](https://i.stack.imgur.com/gSTqa.png)\n\n  \\[3\\]: [https://i.stack.imgur.com/yozsU.png](https://i.stack.imgur.com/yozsU.png)\n\n  \\[4\\]: [https://i.stack.imgur.com/QNdkw.png](https://i.stack.imgur.com/QNdkw.png)\n\n  \\[5\\]: [https://i.stack.imgur.com/XkxhF.png](https://i.stack.imgur.com/XkxhF.png)\n\n  \\[6\\]: [https://i.stack.imgur.com/DcfTy.png](https://i.stack.imgur.com/DcfTy.png)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kb7fim/how_to_transform_time_series_weather_data_into/"}, {"autor": "Kostozard", "date": "2020-12-11 14:41:07", "content": "Scaling of Residual Convolutions and Activations /!/ Hi everyone,\n\nI'm interested in -----> image !!!  generation using GANs.\nI am currently looking at the implementation of [StarGAN v2](https://github.com/clovaai/stargan-v2) and [StyleGAN 2](https://github.com/rosinality/stylegan2-pytorch).\n\nIn both architectures, they use residual convolution layers, where they multiply the output by ```1.0 / sqrt(2.0)```.\nAnd I am wondering where that value is coming from?\n\nI have also noticed in StyleGAN2 that their LeakyReLU is multiplied by ```sqrt(2.0)```.\nAnd again, I am wondering where that value is coming from?\n\nThanks in advance,\n\nKostozard", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kb3zxx/scaling_of_residual_convolutions_and_activations/"}, {"autor": "mrhodesit", "date": "2020-02-29 17:29:48", "content": "I'm looking for a library that can look at 100s of different -----> picture !!! s that only show a small portion of a face, and recreate the entire face in one -----> picture !!! . /!/ Where should I start? Something like this has to already exist.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fbfuu9/im_looking_for_a_library_that_can_look_at_100s_of/"}, {"autor": "GangLiu", "date": "2020-06-21 12:33:32", "content": "Perfect artificial neuron, maybe perfect most ANNs. It may be time to perfect the neuron of artificial neural network /!/   \n\n[https://www.techrxiv.org/articles/It\\_may\\_be\\_time\\_to\\_perfect\\_the\\_neuron\\_of\\_artificial\\_neural\\_network/12477266](https://www.techrxiv.org/articles/It_may_be_time_to_perfect_the_neuron_of_artificial_neural_network/12477266)\n\nAs we know, artificial neural networks are inspired by biological neural networks. Seventy years ago, people designed artificial neurons by imitating the knowledge about biological neurons at that time. Today, due to the development of biology, we have a relatively good understanding of how the work of neurons, especially dendrites.\n\nI found, at the time of design, the traditional artificial neurons ignored a fact that dendrites participate in pre-calculation in a biological neuron or biological neural network.\n\nMore specifically, biological dendrites play a role in the brain pre-processing to the interaction information of input data. This can be illustrated briefly by two tasks in life. For understanding a -----> picture !!!  task, biological dendrites play a role in extracting the relationship across parts of an input------> picture !!! . For understanding an article or a speech task, biological dendrites play a role in extracting the relationship across parts in an input-word.\n\nTraditional artificial neurons are insufficient in extracting the interaction information of input data. Thus we have designed a lot of convolutional layers. Gang neurons maybe reduce the number of layers in an existing network for the same task.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hd67mz/perfect_artificial_neuron_maybe_perfect_most_anns/"}, {"autor": "BatmantoshReturns", "date": "2020-06-21 06:36:06", "content": "Model created from other model layers do not contain all weights from compnent layers. But model.summary() / plot_model shows those weights as part of graph /!/ I created a model which takes two layers from an existing model, and creates a model from those two layers. However, the resulting model does not contain all the weights/layers from those component layers. Here's the code I used to figure this out. \n\n(edit: Here's a colab notebook to tinker with the code directly https://colab.research.google.com/drive/1tbel6PueW3fgFsCd2u8V8eVwLfFk0SEi?usp=sharing )\n\n\n    !pip install transformers --q\n    %tensorflow_version 2.x\n    \n    from transformers import TFBertModel, AutoModel, TFRobertaModel, AutoTokenizer\n    import tensorflow as tf\n    import tensorflow_addons as tfa\n    \n    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n    \n    from tensorflow import keras\n    from tensorflow.keras import layers\n    from copy import deepcopy\n    \n    logger = tf.get_logger()\n    logger.info(tf.__version__)\n    \n    \n    def get_mini_models():\n        tempModel = TFRobertaModel.from_pretrained('bert-base-uncased', from_pt=True)\n    \n        layer9 = deepcopy(tempModel.layers[0].encoder.layer[8])\n        layer10 = deepcopy(tempModel.layers[0].encoder.layer[9])\n    \n        inputHiddenVals = tf.keras.Input(shape=[None, None], dtype=tf.float32, name='input_Q',\n                                        batch_size=None) \n    \n        hidden1 = layer9((inputHiddenVals, None, None))\n        hidden2 = layer10((hidden1[0], None, None))\n        modelNew = tf.keras.Model(inputs=inputHiddenVals, outputs=hidden2)\n    \n        del tempModel\n    \n        return modelNew\n    \n    @tf.function\n    def loss_fn(_, probs):\n        bs = tf.shape(probs)[0]\n        labels = tf.eye(bs, bs)\n        return tf.losses.categorical_crossentropy(labels,\n                                                  probs,\n                                                  from_logits=True)\n    \n    model = get_mini_models()\n    model.compile(loss=loss_fn,\n                    optimizer=tfa.optimizers.AdamW(weight_decay=1e-4, learning_rate=1e-5, \n                                                    epsilon=1e-06))\n    \n    # Get model and layers directly to compare\n    tempModel = TFRobertaModel.from_pretrained('bert-base-uncased', from_pt=True)\n    layer9 = deepcopy(tempModel.layers[0].encoder.layer[8])\n    layer10 = deepcopy(tempModel.layers[0].encoder.layer[9])\n\nWhen I print out the trainable weights, only the keys, query, and values are printed, but each layer also has some dense layers and layer_norm layers. Also, the keys, queries, and values from one layer are printed, but there are two. \n\n    # Only one layer, and that layer also has missing weights. \n    for i, var in enumerate(model.weights):\n        print(model.weights[i].name)\n\n&gt; tf_roberta_model_6/roberta/encoder/layer_._8/attention/self/query/kernel:0\n&gt; tf_roberta_model_6/roberta/encoder/layer_._8/attention/self/query/bias:0 tf_roberta_model_6/roberta/encoder/layer_._8/attention/self/key/kernel:0 tf_roberta_model_6/roberta/encoder/layer_._8/attention/self/key/bias:0\n&gt; tf_roberta_model_6/roberta/encoder/layer_._8/attention/self/value/kernel:0\n&gt; tf_roberta_model_6/roberta/encoder/layer_._8/attention/self/value/bias:0\n\nHere it is for a full single layer\n\n    # Full weights for only one layer \n    for i, var in enumerate(layer9.weights):\n        print(layer9.weights[i].name)\n\nThe output is \n\n&gt; tf_roberta_model_7/roberta/encoder/layer_._8/attention/self/query/kernel:0\n&gt; tf_roberta_model_7/roberta/encoder/layer_._8/attention/self/query/bias:0 tf_roberta_model_7/roberta/encoder/layer_._8/attention/self/key/kernel:0 tf_roberta_model_7/roberta/encoder/layer_._8/attention/self/key/bias:0\n&gt; tf_roberta_model_7/roberta/encoder/layer_._8/attention/self/value/kernel:0\n&gt; tf_roberta_model_7/roberta/encoder/layer_._8/attention/self/value/bias:0 tf_roberta_model_7/roberta/encoder/layer_._8/attention/output/dense/kernel:0\n&gt; tf_roberta_model_7/roberta/encoder/layer_._8/attention/output/dense/bias:0\n&gt; tf_roberta_model_7/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0\n&gt; tf_roberta_model_7/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0\n&gt; tf_roberta_model_7/roberta/encoder/layer_._8/intermediate/dense/kernel:0 tf_roberta_model_7/roberta/encoder/layer_._8/intermediate/dense/bias:0\n&gt; tf_roberta_model_7/roberta/encoder/layer_._8/output/dense/kernel:0\n&gt; tf_roberta_model_7/roberta/encoder/layer_._8/output/dense/bias:0\n&gt; tf_roberta_model_7/roberta/encoder/layer_._8/output/LayerNorm/gamma:0\n&gt; tf_roberta_model_7/roberta/encoder/layer_._8/output/LayerNorm/beta:0\n\nBut all the missing layers/ weights are represented in the model summary\n\n    model.summary()\n\nOutput (EDIT: The output breaks Stackoverflow's character limit so I only pasted the partial output, but the full output can be seen in this colab notebook https://colab.research.google.com/drive/1n3_XNhdgH6Qo7GT-M570lIKWAoU3TML5?usp=sharing )\n\n\nAnd those weights are definitely connected, and going through the forward pass. This can be seen if you execute\n\n    tf.keras.utils.plot_model(\n        model, to_file='model.png', show_shapes=False, show_layer_names=True,\n        rankdir='TB', expand_nested=False, dpi=96\n    )\n\nThe -----> image !!!  is too large to display, but for convenience this colab notebook contains all the code that can be run. The output image will be at the bottom even without running anything\n\nhttps://colab.research.google.com/drive/1tbel6PueW3fgFsCd2u8V8eVwLfFk0SEi?usp=sharing\n\nFinally, I tested the output of the keras model, and running the layers directly, they are not the same.\n\n# Test what correct output should be \n\n    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n    inputt = tokenizer.encode('This is a sentence', return_tensors='tf')\n    outt = tempModel(inputt)[0]\n    hidden1 = layer9((outt, None, None))\n    layer10((hidden1[0], None, None))\n\nvs\n\n    model(outt)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hd26ou/model_created_from_other_model_layers_do_not/"}, {"autor": "OnlyProggingForFun", "date": "2020-06-20 14:05:19", "content": "This AI makes blurry faces look 60 times sharper! Introduction to PULSE: -----> photo !!!  upsampling", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hcmt6s/this_ai_makes_blurry_faces_look_60_times_sharper/"}, {"autor": "cmillionaire9", "date": "2020-06-20 10:19:40", "content": "-----> Image !!!  segmentation | Automate image labeling", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hcjv81/image_segmentation_automate_image_labeling/"}, {"autor": "segments-bert", "date": "2020-06-19 15:24:29", "content": "We're building a labeling platform for -----> image !!!  segmentation. Looking for feedback!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hc2mvv/were_building_a_labeling_platform_for_image/"}, {"autor": "python166", "date": "2020-06-19 03:57:22", "content": "I am making a program on python that detects if the user is wearing a face mask? /!/ The code runs through without any errors, but it doesn't start the webcam. What should I do so that it actually starts the webcam and tells me if I am wearing a mask or not? This is coded in python on idle on a MacBook Pro, late 2015 model. Here is the code:\n\n    # USAGE\n    # python detect_mask_video.py\n    \n    # import the necessary packages\n    from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n    from tensorflow.keras.preprocessing.image import img_to_array\n    from tensorflow.keras.models import load_model\n    from imutils.video import VideoStream\n    import numpy as np\n    import argparse\n    import imutils\n    import time\n    import cv2\n    import os\n    \n    def detect_and_predict_mask(frame, faceNet, maskNet):\n        # grab the dimensions of the frame and then construct a blob\n        # from it\n        (h, w) = frame.shape[:2]\n        blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n            (104.0, 177.0, 123.0))\n    \n        # pass the blob through the network and obtain the face detections\n        faceNet.setInput(blob)\n        detections = faceNet.forward()\n    \n        # initialize our list of faces, their corresponding locations,\n        # and the list of predictions from our face mask network\n        faces = []\n        locs = []\n        preds = []\n    \n        # loop over the detections\n        for i in range(0, detections.shape[2]):\n            # extract the confidence (i.e., probability) associated with\n            # the detection\n            confidence = detections[0, 0, i, 2]\n    \n            # filter out weak detections by ensuring the confidence is\n            # greater than the minimum confidence\n            if confidence &gt; args[\"confidence\"]:\n                # compute the (x, y)-coordinates of the bounding box for\n                # the object\n                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n                (startX, startY, endX, endY) = box.astype(\"int\")\n    \n                # ensure the bounding boxes fall within the dimensions of\n                # the frame\n                (startX, startY) = (max(0, startX), max(0, startY))\n                (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n    \n                # extract the face ROI, convert it from BGR to RGB channel\n                # ordering, resize it to 224x224, and preprocess it\n                face = frame[startY:endY, startX:endX]\n                face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n                face = cv2.resize(face, (224, 224))\n                face = img_to_array(face)\n                face = preprocess_input(face)\n                face = np.expand_dims(face, axis=0)\n    \n                # add the face and bounding boxes to their respective\n                # lists\n                faces.append(face)\n                locs.append((startX, startY, endX, endY))\n    \n        # only make a predictions if at least one face was detected\n        if len(faces) &gt; 0:\n            # for faster inference we'll make batch predictions on *all*\n            # faces at the same time rather than one-by-one predictions\n            # in the above `for` loop\n            preds = maskNet.predict(faces)\n    \n        # return a 2-tuple of the face locations and their corresponding\n        # locations\n        return (locs, preds)\n    \n    # construct the argument parser and parse the arguments\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"-f\", \"--face\", type=str,\n        default=\"face_detector\",\n        help=\"path to face detector model directory\")\n    ap.add_argument(\"-m\", \"--model\", type=str,\n        default=\"mask_detector.model\",\n        help=\"path to trained face mask detector model\")\n    ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n        help=\"minimum probability to filter weak detections\")\n    args = vars(ap.parse_args())\n    \n    # load our serialized face detector model from disk\n    print(\"[INFO] loading face detector model...\")\n    prototxtPath = os.path.sep.join([args[\"face\"], \"deploy.prototxt\"])\n    weightsPath = os.path.sep.join([args[\"face\"],\n        \"res10_300x300_ssd_iter_140000.caffemodel\"])\n    faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n    \n    # load the face mask detector model from disk\n    print(\"[INFO] loading face mask detector model...\")\n    maskNet = load_model(args[\"model\"])\n    \n    # initialize the video stream and allow the -----> camera !!!  sensor to warm up\n    print(\"[INFO] starting video stream...\")\n    vs = VideoStream(src=0).start()\n    time.sleep(2.0)\n    \n    # loop over the frames from the video stream\n    while True:\n        # grab the frame from the threaded video stream and resize it\n        # to have a maximum width of 400 pixels\n        frame = vs.read()\n        frame = imutils.resize(frame, width=400)\n    \n        # detect faces in the frame and determine if they are wearing a\n        # face mask or not\n        (locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n    \n        # loop over the detected face locations and their corresponding\n        # locations\n        for (box, pred) in zip(locs, preds):\n            # unpack the bounding box and predictions\n            (startX, startY, endX, endY) = box\n            (mask, withoutMask) = pred\n    \n            # determine the class label and color we'll use to draw\n            # the bounding box and text\n            label = \"Mask\" if mask &gt; withoutMask else \"No Mask\"\n            color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n    \n            # include the probability in the label\n            label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n    \n            # display the label and bounding box rectangle on the output\n            # frame\n            cv2.putText(frame, label, (startX, startY - 10),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n            cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n    \n        # show the output frame\n        cv2.imshow(\"Frame\", frame)\n        key = cv2.waitKey(1) &amp; 0xFF\n    \n        # if the `q` key was pressed, break from the loop\n        if key == ord(\"q\"):\n            break\n    \n    # do a bit of cleanup\n    cv2.destroyAllWindows()\n    vs.stop()\n\nThank you guys so much in advance. My team and I are really grateful for your help.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hbtmql/i_am_making_a_program_on_python_that_detects_if/"}, {"autor": "sassysalmnder", "date": "2020-06-23 13:42:26", "content": "How can I split data into X and Y for a multi class classifier? /!/ I have training and testing images divided in folders named 'Buildings' , 'Forest', 'Glacier', 'Mountains', 'Sea', 'Street'.\nSo I have 6 classes here and I want to train my model to identify if a -----> picture !!!  belongs to any of these 5 classes.\n\nHow can I split the data into X and Y ? Where X is the image and Y is the corresponding label ? \n\nPlease note that I am quite new to this so if possible explain me like I am 5. Thanks !", "link": "https://www.reddit.com/r/learnmachinelearning/comments/heety2/how_can_i_split_data_into_x_and_y_for_a_multi/"}, {"autor": "spmallick", "date": "2020-06-23 07:40:06", "content": "Efficient -----> image !!!  loading /!/ When it comes to writing optimized code, image loading can become a bottleneck in many tasks and it can often be the culprit behind bad performance.  \nToday, we are sharing a post on the speed of loading images using several decoders  \n1. OpenCV  \n2. Pillow  \n3. Pillow-SIMD  \n4. TurboJpeg  \nand two databases  \n1. LMDB  \n2. TFRecords  \n\n\n [https://www.learnopencv.com/efficient-image-loading/](https://www.learnopencv.com/efficient-image-loading/) \n\nYou will be able to reproduce the test using the code we have shared.\n\nhttps://preview.redd.it/oqrxdal16m651.jpg?width=600&amp;format=pjpg&amp;auto=webp&amp;s=45283197a33ccea128c7016400d91d80ce31a097", "link": "https://www.reddit.com/r/learnmachinelearning/comments/heaabs/efficient_image_loading/"}, {"autor": "Rit2Strong", "date": "2020-06-23 00:32:16", "content": "What are some tutorials where I can learn the basics of AI within a week? /!/ So I want to create a basic -----> image !!!  classifier project which recognizes hand-written numbers. Now while I do want to delve deeper into AI, I simply want to learn enough to be able to build something like this. I know this question gets asked all of the time, but what is a good resource to learn AI on a basic level. There are some great courses that I've seen, but all of them require lots of time and dedication. While I'm not saying I don't want to put time and dedication, I simply want to dive in and build this program and from there learn further. Some examples of resources I'm looking for are like Paul's Online Notes for calculus ([https://tutorial.math.lamar.edu/](https://tutorial.math.lamar.edu/) ) or MIT's OCW on linear algebra. I have a really bad habit of trying to learn everything before I actually build something, and then my motivation is lost halfway because I haven't done anything yet but learn the theory, so I'm trying to change that this time by just trying to start the project as soon as I can. Thanks in advanced!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/he4dl1/what_are_some_tutorials_where_i_can_learn_the/"}, {"autor": "istevewang", "date": "2020-06-22 06:04:47", "content": "\"Ask Me Anything\" with Hongxia Yang, a Judge of KDD Cup 2020 Challenges from Alibaba DAMO Academy /!/ KDD  Cup 2020 Challenges for Modern E-Commerce Platform, sponsored by  Alibaba, Alibaba  DAMO Academy, Duke University, Tsinghua University and  UIUC and hosted  by Alibaba Cloud Tianchi Platform, have announced the  final result (See top 20 lists: [Debiasing Track](https://tianchi.aliyun.com/forum/postDetail?&amp;postId=112530), [Multimodalities Recall Track](https://tianchi.aliyun.com/forum/postDetail?&amp;postId=112601)).   The two tasks of the competition, recall for multi-modal entities and   debiasing, are designed to help tackle some fundamental challenges and   to further nourish the development for the search and recommender   systems of e-commerce and retail companies.\n\nDuring  the competition, Dr. Hongxia Yang, a judge of the competition  from  Alibaba DAMO Academy was invited to have an \"Ask Me Anything\"  session  to answer some questions from participants and give some  insights about  the background and value of the competition.\n\nDr.  Hongxia Yang is a Senior Staff Data Scientist of Data Analytics  and  Intelligence Lab at Alibaba DAMO Academy. She received her PhD in   Statistics from Duke University in 2010. Her interests span the areas of   Bayesian statistics, time series analysis, spatial-temporal modeling,   survival analysis, machine learning, data mining (and their  applications  to problems in business analytics), and big data. She used  to work as  the Principal Data Scientist at Yahoo! Inc. and Research  Staff Member at  IBM T.J. Watson Research Center. She has published over  40 top  conference and journal papers and is serving as the associate  editor for  Applied Stochastic Models in Business and Industry. She has  been  selected as an Elected Member of the International Statistical  Institute  (ISI) in 2017.\n\nBelow  are selected Q&amp;A from the AMA with Dr. Yang. You can also  view the  full version of the AMA by clicking the links below:\n\n* Debiasing Track: [Click Here](https://tianchi.aliyun.com/forum/postDetail?postId=110939)\n* Multimodalities Recall Track: [Click Here](https://tianchi.aliyun.com/forum/postDetail?postId=111016)\n\n**Q: What is your main research focus at DAMO Academy?**\n\nA:  We are focusing on the cognitive intelligence that is applicable  to  the recommender systems. Cognitive intelligence mainly includes three   parts, cross-domain knowledge graph, graph neural network (GNN) based   inductive reasoning platform, and user-interacted content   (nlp/videl/-----> image !!! ) understanding; with their applications to the modern   recommender systems.\n\n**Q: The track of Debiasing is focusing on the fairness of exposure. How does exposure bias happen on a recommender system?**\n\nA:  It is more related to the nature of the big data algorithm, which  pays  more attention to those regions that are abundant with data so the   algorithms are more confident to issue the results. There is currently a   very popular trending in machine learning to solve this problem,   fairness in ML.\n\n**Q: What problem will exposure bias cause to e-commerce platforms?**\n\nA: Exploration for long-tailed/cold-start items will be not sufficient.\n\n**Q:  Why is NDCG@50 used to measure the result of the  competition? Is there  any other method being used to measure the  fairness of exposure in  practice?**\n\nA: Indeed,  ndcg@50 is an offline measure and recommender systems are  an online  changing environment. In practice, we will also pay more  attention to  the total number of items that are exposed in the long  term, for  example, one month.\n\n**Q: Does DAMO Academy have any progress on reducing the exposure bias during the past few years?**\n\nA:  Yes, we have one paper in submission for the NeurIPS, through   combining a contrastive learning framework with the recommender system   to explore more efficiently for those long-tailed items.\n\n**Q:  Will the short-term goals like ctr, cvr, or gmv (as  mentioned in the  competition introduction) be affected when reducing the  exposure bias  with algorithms? How can we balance them?**\n\nA:  Yes, indeed. We also consider long-term goals, e.g., the total  number  of items that are exposed in a long period (one month for  example) and  the distribution of exposed items in different categories.  We should  also pay attention to the users' duration time.\n\n**Q: What's your expectation on the participants' results through this challenge?**\n\nA:  Current modern recommender systems are far more complex and  attractive  compared to the traditional ones. We need to recommend not  only items,  but videos, texts, and topics, among several other formats.  We would  like to nourish the community to access the ongoing challenges  and make  progress in the interdisciplinary, which we believe will have   breakthroughs both in academia and the industry.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hdmka0/ask_me_anything_with_hongxia_yang_a_judge_of_kdd/"}, {"autor": "smol_smooth_brain", "date": "2020-06-13 16:23:03", "content": "[TensorFlow] Basic getting-started questions for a binary -----> image !!!  classifier /!/ Hi, I'm doing a school project in Deep Learning where I have to make use of a binary classifier to differentiate between gray scale pictures of images.\n\nI don't know much about TensorFlow or Keras so I'm really confused about how I should go about accomplishing this. \n\n* I have folders of pictures which are divided into train, validation and test sets (which are divided into the two classes). Is there any way I can conveniently convert these into a dataset which a Keras model can understand? Right now I'm using the ImageDataGenerator flow\\_from\\_directory method, but it seems to augment my data which is not desirable for my validation and test sets! :(  \n\n* What model should I pick for my task? I.e what machine learning model would be the most suitable for this kind of problem? I tried using Conv2 and it seems to be effective, but I don't quite understand why or if there are better alternatives available. The information I can find seems to be research papers and other literature aimed at PHD students, so it's hard for me to utilize.   \n\n* What is the most straight forward way to test the accuracy of my trained network? There seems to be a plethora of methods for testing this, but none of them can simply give me the loss and the accuracy of my network?  \n\n\nThanks in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h8ajqa/tensorflow_basic_gettingstarted_questions_for_a/"}, {"autor": "tylersuard", "date": "2020-06-13 15:25:56", "content": "From what I understand, it is possible to tell if an -----> image !!!  was used to train a neural network. If that is the case, couldn't I reverse-engineer the entire training set by just generating random pixels and then seeing if each image was used to train the model? /!/ Also, would this work for simpler data, like vectors with a few features?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h89jqu/from_what_i_understand_it_is_possible_to_tell_if/"}, {"autor": "seegedp", "date": "2020-06-12 00:35:34", "content": "Using HTML to Visualize Results /!/ Hey Everyone,\n\nI have used automatically generated static HTML files as a way of visualizing data. Someone recently asked me to explain how this worked, so I decided to write a really short post on it.\n\n&amp;#x200B;\n\nThe short story is that since HTML files are ASCII, you can pretty much generate them from any language really easily, and then just view them in a web browser.  I have used this trick for years for lots of different data, but especially on -----> image !!!  data.  In particular I use an HTML table to format pictures into rows and columns. I have done this in the past for before-and-after comparisons, algorithm comparisons, or even error analysis where I will include columns for errors from each  of different classes. I have also done even more complicated things that involved multiple HTML files and hyperlinks. It is actually surprisingly easy and quick to generate some really nice data displays.\n\n&amp;#x200B;\n\nHere is a link to the post that also includes an example from Python. There is also link to a simply Python notebook in GitHub that demonstrates the idea.\n\n[https://medium.com/@cjdellaporta/visualizing-data-with-static-html-pages-be09cc8119b8](https://medium.com/@cjdellaporta/visualizing-data-with-static-html-pages-be09cc8119b8)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h7adgb/using_html_to_visualize_results/"}, {"autor": "GeNiaaz", "date": "2020-06-16 06:15:40", "content": "How do I extract out subimages from larger images? /!/  \n\nMy dataset is basically a series of 512x512 jpg images. For context, there are several different objects in each -----> image !!! , and in a separate JSON file, I have coordinates of the bounding boxes for each type of object. Can anyone give me advice as to how to instruct pytorch to train on the specific objects in my images? Thanks so much in advance.\n\nLet me make it clearer through use of an example: Let's say I'm training a model to detect different animals, and in a one of the pictures I have a dog and a cat. Now I have already formed a bounding box around the dog and cat and labelled them according to their type respectively in a separate JSON file. However, how do I teach this to pytorch?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h9y3mf/how_do_i_extract_out_subimages_from_larger_images/"}, {"autor": "mrnerdy59", "date": "2020-06-15 18:08:49", "content": "Need help with developing a -----> image !!!  segmentation tool /!/ Hi Guys\n\nI'm planning to work on image segmentation tool, that can identify different objects in a image using an unsupervised manner. To show an example, looking to work on something like this:  [https://segments.ai/](https://segments.ai/) \n\nIf anyone has worked on something similar, please guide. I'm looking for \"basics\" as I want to do this without any pre-trained model, where should I start, what should I look at, any videos or links, please drop here.\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h9ligf/need_help_with_developing_a_image_segmentation/"}, {"autor": "TrackLabs", "date": "2020-06-15 17:54:58", "content": "This is a 256x256 -----> Image !!!  output of my GAN during training. It ends up making even big squares. Is this a common problem or common way to fix?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h9l8ph/this_is_a_256x256_image_output_of_my_gan_during/"}, {"autor": "Navid_A_I", "date": "2020-06-15 15:28:58", "content": "What is the best Machine Learning algorithm to convert a gridded data to a deformed one with the same size?! /!/ In the images below, each graph represents an elevation map where altitude of each point is represented by color gradient. As you can see the Input is a gridded data -----> image !!!  (size of m\\*m) and the target is the same gridded data -----> image !!!  (size of m\\*m) with some deformation/change (change of altitude) applied on.\n\nYou can also see the deformation applied on each graph as Transform graphs.(deformation = Output data \u2013 Input data)\n\nI am trying to train a Neural Network which takes Input images as Input of the CNN and the Output images as ground truth/target data and learn to guess the output image for a given input image. (Or just simply guess the transform image needed to convert the input images to deformed models)\n\nUnfortunately currently I don't have access to a big data-set of these graphs but in a few weeks time I will have a few hundred of input images and their corresponding output ones.\n\nI was wondering what could be the best machine learning algorithm for such problem? Would a encoder/decoder CNN work to train this type of data?\n\nhttps://preview.redd.it/fhzu9cqde3551.png?width=960&amp;format=png&amp;auto=webp&amp;s=5fb65c99223962abac82780295eeee95b2e4b75c", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h9iiud/what_is_the_best_machine_learning_algorithm_to/"}, {"autor": "cmillionaire9", "date": "2020-06-15 11:48:29", "content": "AI transforms a Low-Resolution -----> Image !!!  to a High-Res", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h9erj9/ai_transforms_a_lowresolution_image_to_a_highres/"}, {"autor": "BananaCharmer", "date": "2020-06-15 10:13:14", "content": "Object Detection - classify different text elements on a given product page /!/ I'm working on an object detection problem which isn't covered by the plethora of tutorials and projects out there. I'm hoping someone could steer me int he right direction/offer some advice.\n\n&amp;#x200B;\n\nThe object detection projects I've looked at involve object *location* \\- i.e. detecting regions of interest and calculating a boundary box /region - then *classifying* the objects within those regions (using a pre-trained classification model). These projects are usually looking at photos or videos and picking out complex objects that overlap (e.g. a busy city street).\n\n&amp;#x200B;\n\nMy project seems very stripped back compared to the typical implementation - I want to take a snapshot of a product (web)page and select the **product title** and **price**. My objects will all be text (with different styling - a title might be bold for example) and they will never overlap. The snapshots will all be the same dimensions, so **title** and **price** objects should also be situated in the same place for each snapshot, which if I can incorporate somehow, should make the classification more accurate.\n\n&amp;#x200B;\n\nI know the boundary box parameters and can discern the object classes ahead of time. But how (and what) model do I use to train? Is there a model which can incorporate location as a feature for classification? E.g. title is always in the top left.\n\n&amp;#x200B;\n\nMy thoughts on implementing this right now are:\n\n1. Generate the snapshots as -----> image !!!  files (these will all be exactly the same dimensions)\n2. Generate annotation files (Pascal VOC) which will define the boundaries for the objects I'm interested in\n3. **Train a suitable model**. This is where things become a bit hazy. I have the images and boundaries for each of the two objects.\n   1. How to I train a model / what algorithm do I use to **locate** the title and price elements? Since title is usually on the left and price on the right, in my head if makes sense to use each objects position relative to the entire snapshot.\n   2. What's the best classifier to use in this instance? The two object classes will consist of text, will be of variable length.\n\n&amp;#x200B;\n\nAny advice/re-directs/etc. are welcome - I want to have it clear in my head before I start coding. If there are similar projects out there/something I should google (but lack the terminology), please let me know.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h9dgid/object_detection_classify_different_text_elements/"}, {"autor": "usednamechecksout", "date": "2020-06-15 02:23:41", "content": "Recommend scripts/repos for input file format (COCO, PascalVOC, YOLO, etc.) for -----> image !!!  interpretation models /!/ One idea that could help machine learning beginners is conversion of input image file formats as required by popular models. I see that the efforts are often scattered. I intend to consolidate efforts and make it easy for beginners. Please share these if you have come across them.\nYou can share the links to efforts here or contribute directly to https://github.com/kshitizkhanal7/ml-image-format-conversion", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h974np/recommend_scriptsrepos_for_input_file_format_coco/"}, {"autor": "cmillionaire9", "date": "2020-03-21 13:44:30", "content": "AI learned to realistically change the time of day in the -----> photo !!! ", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fmf651/ai_learned_to_realistically_change_the_time_of/"}, {"autor": "a_beautiful_soul_", "date": "2020-03-21 11:16:17", "content": "Need some advice regarding a human pose estimation project /!/ Hello! I am new to Deep Learning and Computer Vision and have tried only basic classification and regression problems till now by following various online tutorials. Now I want to develop a model which will detect whether a human pose is suspicious or not. I want to use OpenPose which will give me the location of keypoints(joints of the body) when I feed it the -----> image !!! . After getting the keypoint coordinates, I would be feeding them into my own neural network and the output would be the probability of the pose being suspicious.\nThis would be my first project where I'm not following any tutorial and trying out the same things which the instructor says. I have a lot of confusion about how to proceed right from whether my idea is feasible to whether I will be able to train my model on Google Colab because I don't have a good GPU. I also don't know how to design the architecture for my neural network. How many layers should I have? How many data samples will I need?\n\nAny guidance regarding where I should go now or what I should read/try to clear my doubts would be greatly appreciated!\nThank you so much :)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fmdh75/need_some_advice_regarding_a_human_pose/"}, {"autor": "Randy-Brandy", "date": "2020-03-21 10:43:46", "content": "What are the steps to develop an app that recognizes objects in a still -----> image !!!  using Python (along with other software(or libraries) , and what are the software that I'll need )? /!/ Please be as detailed as possible, as I searched for that on Google, YouTube, etc, and the answers were use OpenCV or TensorFlow; yet I don't know where to start, what to learn, or how to use such tools.\n\nThe idea of the app:\n\n1)  Browse through a phone or pc\n\n2) Select a photo\n\n3) Upload the photo, then the object in the photo will be recognized\n\nThat's it\n\n(I completed a Python course on SoloLearn)\n\nYour help will be much appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fmd5se/what_are_the_steps_to_develop_an_app_that/"}, {"autor": "salinger_vignesh", "date": "2020-03-21 06:09:20", "content": "WGAN-GP implementation difficulties /!/ Hi everyone, Im trying to implement this paper https://arxiv.org/pdf/1705.02438.pdf and here is the authors code in tensorflow https://github.com/MandyZChen/srez/tree/925d72e62eed829a7f123a58fae3f64b202ec13d\n\nIm trying to reimplement this in pytorch. This is paper is about implementing a super resolution face generator using WGAN-GP { converting a 16\\*16 -----> image !!!  to 64\\*64 -----> image !!! } . Im having difficulty in understanding the generator architecture written in tensorflow about upscaling a 16\\*16\\*3 to 64\\*64\\*3 image done with and without RESNET encoder decoder model. can you pls help me or share a good blog for resnet encoder decoder model for upscaling.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fmail6/wgangp_implementation_difficulties/"}, {"autor": "Roalkege", "date": "2020-03-20 23:59:54", "content": "Good Projects for beginner /!/ Hello,\n\nI search good projects for me, a beginner.\n\nI used a few Projects from kaggle.com , most time -----> Image !!!  classification and I have to say that this works very good. I'm also using Pytorch.\n\nI hope you can give me good Projects or Datasets or Ideas what I can to do next. \n\nThank you", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fm5lfg/good_projects_for_beginner/"}, {"autor": "Edenio1", "date": "2020-03-20 19:28:27", "content": "Mini-Project Help: Counting Colored Lego Blocks Challenge /!/ A few years ago I got asked this as a question in an interview:\n\n\"Given an -----> image !!!  of Lego blocks, write an algorithm that counts the number of blocks of each color.\"\n\n[Am i the only one who finds it hard to keep count of each color manually?](https://preview.redd.it/ko0vi5glpvn41.png?width=1044&amp;format=png&amp;auto=webp&amp;s=eaef63bbecee2578317b24ced9956e79ebd7c64f)\n\nSimple question, difficult challenge.  When I first attempted this I ended up completely botching the question with some weirdly over complicated solution involving mirrored convolution neural networks.\n\nThis is my latest attempt that came to me, quite literally, in a dream: [https://colab.research.google.com/drive/1cPRc-KAoMmT8krMBWQrgn4GYnOgnZHSn](https://colab.research.google.com/drive/1cPRc-KAoMmT8krMBWQrgn4GYnOgnZHSn)\n\nIt tries to cluster the pixels into colors, then cluster the pixels of each color using DBSCAN. The hope was that the number of clusters would roughly equal the number blocks. As you can see that's far from the truth. \n\nDo you have any suggestion son how I could improve this attempt? All input welcome!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fm12r9/miniproject_help_counting_colored_lego_blocks/"}, {"autor": "Slajni", "date": "2020-08-07 13:05:55", "content": "How do I design NN that it takes as arguments inputs of different nature. /!/ Suppose I train a NN that processes some video frames of view from the car.\n\nI want to feed to my NN both the -----> image !!!  and the speed that car had while the photo was taken.\n\nWhat is the right way to do it?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i5dg8k/how_do_i_design_nn_that_it_takes_as_arguments/"}, {"autor": "canarysplit", "date": "2020-08-06 21:38:53", "content": "After training a model to 99.7% accuracy on the MNIST dataset, when I feed the model with other data he often confuses number 7 for number 2. How to solve this? /!/  Does anyone have experience with this issue? I'm feeding him now with numbers from the Sudoku game after I crop them from the -----> image !!! .", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i50uu8/after_training_a_model_to_997_accuracy_on_the/"}, {"autor": "zeknife", "date": "2020-08-06 16:46:46", "content": "What is the purpose of all the outputs in wavenet? /!/ Hi.  I'm working on a forecasting problem and have tried a lot of different  things. Next, I want to try wavenet, since I read it can do forecasting  quite well. This is the implementation I'm using:  [https://gist.github.com/sinusgamma/6ca55ea2d7c8fdd0c1104a2266f31b47#file-wavenet\\_core-py](https://gist.github.com/sinusgamma/6ca55ea2d7c8fdd0c1104a2266f31b47#file-wavenet_core-py)\n\nI  get the impression the input and output having the same dimensions is a  core feature of Wavenet, but I don't understand \\*why\\* this should be  desirable.\n\nLooking at diagrams of  dilated convolutions in Wavenet, it seems there is one neuron in the  output layer for each neuron in the input layer, and only the final  output neuron is affected by the final input neuron. Assuming the final  input neuron has the single highest influence on the next time step  (which I would like to forecast), why should any of the other output  neurons be of use to me? Are they only indirectly useful, used for  training the network but not for actually getting new, meaningful  output?\n\nI attached an -----> image !!!  that illustrates my confusion. Why are any of the crossed out neurons useful  if they don't relate to the final timestep of my input?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i4va63/what_is_the_purpose_of_all_the_outputs_in_wavenet/"}, {"autor": "ANil1729", "date": "2020-08-06 16:33:41", "content": "A 2020 guide to Semantic Segmentation /!/  Semantic segmentation is the task of classifying images on a pixel level \ud83d\udcf7.\n\nIt has multiple use-cases in self-driving cars \ud83d\ude93, medicine \ud83c\udfe5, makeup tools and -----> photography !!!  etc.\n\nIn this article, I cover various techniques which can be used to implement semantic segmentation for images, videos, point clouds and also cover loss functions, metrics, datasets and annotation tools involved\n\n[https://nanonets.com/blog/semantic-image-segmentation-2020/](https://nanonets.com/blog/semantic-image-segmentation-2020/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i4v1af/a_2020_guide_to_semantic_segmentation/"}, {"autor": "ANil1729", "date": "2020-08-06 16:23:26", "content": "A 2020 guide to Semantic Segmentation /!/  Semantic segmentation is the task of classifying images on a pixel level \ud83d\udcf7.\n\nIt has multiple use-cases in self-driving cars \ud83d\ude93, medicine \ud83c\udfe5, makeup tools and -----> photography !!!  etc.\n\nIn this article, I cover various techniques which can be used to implement semantic segmentation for images, videos, point clouds and also cover loss functions, metrics, datasets and annotation tools involved\n\n[https://nanonets.com/blog/semantic-image-segmentation-2020/](https://nanonets.com/blog/semantic-image-segmentation-2020/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i4uu8a/a_2020_guide_to_semantic_segmentation/"}, {"autor": "ANil1729", "date": "2020-08-06 16:15:20", "content": "A 2020 guide to Semantic Segmentation /!/  Semantic segmentation is the task of classifying images on a pixel level \ud83d\udcf7.\n\nIt has multiple use-cases in self-driving cars \ud83d\ude93, medicine \ud83c\udfe5, makeup tools and -----> photography !!!  etc.\n\nIn this article, I cover various techniques which can be used to implement semantic segmentation for images, videos, point clouds and also cover loss functions, metrics, datasets and annotation tools involved\n\n[https://nanonets.com/blog/semantic-image-segmentation-2020/](https://nanonets.com/blog/semantic-image-segmentation-2020/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i4uox8/a_2020_guide_to_semantic_segmentation/"}, {"autor": "ANil1729", "date": "2020-08-06 16:03:48", "content": "A 2020 guide to Semantic Segmentation /!/  Semantic segmentation is the task of classifying images on a pixel level \ud83d\udcf7.\n\nIt has multiple use-cases in self-driving cars \ud83d\ude93, medicine \ud83c\udfe5, makeup tools and -----> photography !!!  etc.\n\nIn this article, I cover various techniques which can be used to implement semantic segmentation for images, videos, point clouds and also cover loss functions, metrics, datasets and annotation tools involved\n\n[https://nanonets.com/blog/semantic-image-segmentation-2020/](https://nanonets.com/blog/semantic-image-segmentation-2020/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i4uh4z/a_2020_guide_to_semantic_segmentation/"}, {"autor": "canarysplit", "date": "2020-08-06 15:00:14", "content": "How to avoid crease of the paper when detecting numbers from the Sudoku game? /!/ I followed this tutorial and tried to recreate it so I can detect numbers extracted from the Sudoku game. \n\n[https://medium.com/@aakashjhawar/sudoku-solver-using-opencv-and-dl-part-1-490f08701179](https://medium.com/@aakashjhawar/sudoku-solver-using-opencv-and-dl-part-1-490f08701179)\n\n[https://github.com/aakashjhawar/SolveSudoku](https://github.com/aakashjhawar/SolveSudoku)\n\nHowever, when I enter my -----> image !!!  as the Original -----> image !!! :\n\n [https://i.stack.imgur.com/p2TO3.jpg](https://i.stack.imgur.com/p2TO3.jpg)\n\n I get this Modified -----> image !!!  as the output of the extract\\_digit function:\n\n[https://i.stack.imgur.com/sZxHi.jpg](https://i.stack.imgur.com/sZxHi.jpg)\n\nAs you can see in the 8th row, crease of the paper got picked up as something and I would want to prevent this from happening.\n\nAny ideas?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i4tc1b/how_to_avoid_crease_of_the_paper_when_detecting/"}, {"autor": "buptkang", "date": "2020-06-02 19:15:39", "content": "Nuwa-ai /!/ Hello community,\n\n* Are you currently learning or working on machine learning?\n* Have you working on cool stuff by using deep learning models to generate any media contents (text, audio, -----> image !!!  or videos)\n* Do you want to find one community to share your AI generated content in one community, learn from others.\n\nIf so, please visit [https://nuwa-ai.com/](https://nuwa-ai.com/) by reaching out [support@nuwa-ai.com](mailto:support@nuwa-ai.com). (In beta mode, invitation mode only)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gvdpdn/nuwaai/"}, {"autor": "Kal217", "date": "2020-06-02 16:31:29", "content": "I heard we're showing off results, so check out -----> image !!!  translation with my implementation of StarGAN 2!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gvajkl/i_heard_were_showing_off_results_so_check_out/"}, {"autor": "Mostafa_A", "date": "2020-06-02 12:46:49", "content": "Enhanceing images with ML /!/ Hello everyone\nI have model to enhance -----> image !!! s so when i test my model on Google colab using (cuda gpu) to send it an -----> image !!!  and get the enhanced -----> image !!!  (it works fine). \n\nSo when i test it in my pc to make an api using flask using (cpu) the enhanced image come with (white) around the image so i want to know is the cpu is responsible for that and i have to run it in the gpu to avoid the white around the image\n\nNote that : i was trying to run it in cuda on my pc and i downloaded every thing from Nvidia but when i test the cuda it tells false no cuda\n\nSo if someone know where the problem is please tell me or a good host using gpu to host the api on it to test", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gv6oqg/enhanceing_images_with_ml/"}, {"autor": "Ikcike", "date": "2020-07-02 08:46:06", "content": "Deep learning in gaming. /!/ Hey guys , I recently started learning about machine learning, and I saw OpenAI win against pro players in DOTA.\nOf course DOTA provided every information they needed to do this and im 100% sure they could speed up the game and play multiple ones in order to teach it faster. \nI was wondering about this and I pretty much have a dream about making a deep learning program that is capable of being good in multiple games with the same script. \nMy question is , is it possible to make a program that can learn multiple games with the same code and doesnt rely that much on importing the games -----> image !!! s and telling it what to do in that particular -----> image !!! ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hjtg5z/deep_learning_in_gaming/"}, {"autor": "dataskml", "date": "2020-07-02 04:41:14", "content": "Live zoom lecture about the evolution of machine learning and state of the art -----> image !!!  processing implementations - free for redditors", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hjqhee/live_zoom_lecture_about_the_evolution_of_machine/"}, {"autor": "Sloathe", "date": "2020-07-01 20:19:09", "content": "How could I zoom in on a recognized object? /!/ So this is my first machine learning project. I'm going to have a -----> camera !!!  constantly searching for an object, and then save a single zoomed in picture of the object itself. Any advice on how I could accomplish this? Thank you.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hji63d/how_could_i_zoom_in_on_a_recognized_object/"}, {"autor": "monkeyknowstyping", "date": "2020-07-01 17:19:26", "content": "Semi-Restricted Boltzmann Machine implementation /!/ Does anybody has an open-sourced implementation of a semi-restricted Boltzmann Machine as described in [Modeling -----> image !!!  patches with a directed hierarchy of Markov random fields (Simon Osindero and Geoffrey Hinton 2007)](https://papers.nips.cc/paper/3279-modeling------> image !!! -patches-with-a-directed-hierarchy-of-markov-random-fields) or any lead on where to find it?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hjekxs/semirestricted_boltzmann_machine_implementation/"}, {"autor": "Unchart3disOP", "date": "2020-07-01 13:05:50", "content": "Deep Learning for segmentation /!/ Hi guys,\nCurrently I have this problem that I want to extract a ROI from an -----> image !!!  and I'd like to use ML for this specifically deep neural networks. However I am not sure what the outputs of the image should be I already have a dataset with the coordinates of the center of the ROI and the radius. But how do I think of that as a ML problem, I have read some literature and found the YOLO algorithm tends to be used, but what do you guys I'd be quite grateful if anyone could give me a small brief on how I should be thinking?\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hj9shb/deep_learning_for_segmentation/"}, {"autor": "Transit-Strike", "date": "2020-07-01 11:25:43", "content": "How does facial recognition work and how is it different from a classifier? /!/ I've been mulling over this for a while now, I understand how classifiers work . They take a dataset of a number of classes to learn a good representation, then they find which class label works best for an arbitrary -----> image !!! .\n\n&amp;#x200B;\n\nI understand how a face filter on Snapchat looks and goes : \"That's a human face\" and then does some form of filtering to make a change.\n\n&amp;#x200B;\n\nEven gender changing Face App with Star-GAN makes sense as it is not trying to identify me as me, but just as a human male and then swapping my gender.\n\n&amp;#x200B;\n\nBut I really fail to see why Face ID should work. The way I see it, it's a classifier that classifies into 2 classes \"owner\" and \"not owner\". But given that it doesn't have a whole training set of my face, how can it accurately say that I am the owner of my phone and not someone else? Also, are there any pretrained algorithms that are trained to work on a low-res camera?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hj8e9w/how_does_facial_recognition_work_and_how_is_it/"}, {"autor": "Samurai9954", "date": "2020-07-09 09:26:59", "content": "[PROJECT] Collaboration On An Project /!/  \n\nHi, I wanted to collab on an AI portal is like AI Simplified Usually to make an -----> image !!!  classifier it takes a lot of time and energy Why spend so much time and energy when you can make it in 3 lines of python\n\nFor eg:- import AIPortal\n\nAIPortal.ImageClassifier('DATA',500)\n\nOf course, AI has a lot of parameters like activation function and learning rate and etc. So the user can also input that if he wants to, else it will reset to default\n\nI'm sorry this is my first post on Reddit", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ho03mi/project_collaboration_on_an_project/"}, {"autor": "altaML-cats", "date": "2020-07-09 04:55:28", "content": "[P] Introducing @pawcatso, cat artist to the stars /!/ A twitter bot that takes the images of cats and put hats and glasses on them.\n\n&gt;I, Pawblo Pawcatso, live for cat portraiture. Mention me [@pawcatso](https://twitter.com/pawcatso)with a headshot of your darling kitty and [\\#hatmycat](https://twitter.com/hashtag/hatmycat?src=hashtag_click), and I will create a masterpiece for you!\n\nTracks for mentions every 5 minutes and if there is a tweet with mention and a -----> image !!!  of a cat is attached, it DMs you the modified -----> image !!! .\n\nLink - [https://twitter.com/pawcatso](https://twitter.com/pawcatso)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hnx15m/p_introducing_pawcatso_cat_artist_to_the_stars/"}, {"autor": "ShellTitan", "date": "2020-07-08 21:27:18", "content": "Help needed for ML project /!/ Hello,\n\nI am a physics undergrad student and I am working on a summer project where I need to automate the measurement of mouse pupil size. (It isn't like the human pupil which is constricted with how much it can change so I gave up on using non ML solution after checking how they estimate it in humans) I used facebook's detectron2 model to segment the eye but the inference time is too slow as the application needs to be real-time ( around 30 fps)  and I still need to somehow estimate the pupil's radius after that so ideally, the inference should take less time than 0.03 seconds. I know the problem is a bit vague but I have two -----> camera !!!  recordings that I use as data at the moment it is a specific setup where the mouse doesn't move and the -----> camera !!!  is focused on the eye. I don't know if there is a better way to estimate the pupil size better than segmenting it and then trying to fit an ellipse on it somehow. I would be very thankful for any help. ( I checked the real-time segmentation on  [https://paperswithcode.com/task/real-time-semantic-segmentation](https://paperswithcode.com/task/real-time-semantic-segmentation) but I am not sure which algorithm would work best from there as I have limited data set although I can ask for more footage)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hnpyxu/help_needed_for_ml_project/"}, {"autor": "civdex", "date": "2020-07-08 19:09:13", "content": "General CNN question - what would be the dimensions of on output matrix for the convolution of a 6x6x3 input -----> image !!!  and a 3x3x3 kernel? /!/ (Using a stride of zero and padding of zero.)\n\nFor an input image of size 6x6 with 3 channels (therefore 6x6x3) and a kernel of size 3x3 with 3 channels, would the dimesnsions be 4x4x3 or 4x4x1? \n\nAny help would be appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hnnafq/general_cnn_question_what_would_be_the_dimensions/"}, {"autor": "Unchart3disOP", "date": "2020-07-08 15:24:56", "content": "I get an extra dimension in my images when I am using tensorflow /!/ Hello there,\n\nI have this error that I have been getting which is  `ValueError: Cannot feed value of shape (1, 500, 500, 3, 1) for Tensor 'sigmoid_target_2:0', which has shape '(?, ?, ?, ?)'`  \n I am not sure why I getting that last dimesion, just for reference, the first 3 are batch size, -----> image !!!  width and -----> image !!!  height. Does anyone know how do I fix that?\n\nThanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hnj0n3/i_get_an_extra_dimension_in_my_images_when_i_am/"}, {"autor": "namenomatter85", "date": "2020-07-08 14:43:33", "content": "Hard. Problems: Eliminating Racism in Machine Learning /!/ Hey Guys, \n\nWe've been working away on a framework to eliminate or decrease racism in machine learning. Today we have a demo setup and first version release of the library. Would love if anyone has any vision problems classifying people to check out our library to balance out the dataset and keep it private. Any feedback on ways to improve is greatly appreciated. \n\nDemo Site: [https://privyfilter.herokuapp.com/](https://privyfilter.herokuapp.com/)\n\nRepo: [https://github.com/Deamoner/privyfilter](https://github.com/Deamoner/privyfilter)\n\nArchitecture and Methodology Article: [https://medium.com/@mdavis\\_71283/hard-problems-racial-bias-in-machine-learning-bf631c9d680c](https://medium.com/@mdavis_71283/hard-problems-racial-bias-in-machine-learning-bf631c9d680c)\n\n&amp;#x200B;\n\nThings to learn in this project: \n\n\\- Creating a pipeline for new model creation - first create pipeline to generate dataset and then create model. \n\n\\- \n\nCurrent Features:\n\n\\- Face Detection \n\n\\- Demographic Information Extraction \n\n\\- Synthetic Face Generation \n\n\\- Face Swapping \n\n&amp;#x200B;\n\nFuture Features: \n\n\\- Skin Detection \n\n\\- Skin Hue Manipulation\n\n\\- Full Pipeline Process for the entire directory \n\n\\- Multi-person -----> photo !!!  support  \n\n\\- Remove Pipeline and Train pix2pix model \n\n&amp;#x200B;\n\nThis is an open source initiative, and open to any constructive feedback or even better actually getting your hands dirty by helping code. Looking for any feedback on ways we can improve it for any of your specific use cases.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hnia2i/hard_problems_eliminating_racism_in_machine/"}, {"autor": "andybernand", "date": "2020-07-08 13:29:41", "content": "An agenda for storing relevant ML resources /!/ I am looking for an agenda program that indexes the blog posts, tutorial links/ books links, etc. For instance, if I am interested in -----> image !!!  processing techniques, I will visit the Image Processing category and choose Adaptive Histogram Equalization subcategory. There I can see any resources links, code snippets, before and after images, etc,  that I found helpful in other ML projects.  The categories will be defined by me and I will add resources every time when a new category is added. This would be a scenario of what I am looking for.\n\nThe aim is to keep an order among ML resources which proved to be efficient and to avoid to google them each time when I need. In this way, the agenda can be also shared to others which look for something particular regarding ML.\n\nAlso any suggestions to be more organized in ML challenges would be appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hnh2s8/an_agenda_for_storing_relevant_ml_resources/"}, {"autor": "johncaling40", "date": "2020-07-08 10:08:39", "content": "AttributeError: module 'keras.layers.advanced_activations' has no attribute 'Softmax' /!/ I get this error when running the following code:\n\n    AttributeError: module 'keras.layers.advanced_activations' has no attribute 'Softmax' \n\nCode:\n\n    output_labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n    # For the first argument, use the filename of the newest .h5 file in the notebook folder.\n    coreml_mnist = coremltools.converters.keras.convert(\n        'best_model.09-0.03.h5', input_names=['image'], output_names=['output'], \n        class_labels=output_labels, image_input_names='image')\n\nHere is the full trace back:\n\n    AttributeError                            Traceback (most recent call last)\n    &lt;ipython-input-16-2b738bf652cd&gt; in &lt;module&gt;\n          3 coreml_mnist = coremltools.converters.keras.convert(\n          4     'best_model.09-0.03.h5', input_names=['image'], output_names=['output'],\n    ----&gt; 5     class_labels=output_labels, image_input_names='image')\n    /Volumes/Common/opt/anaconda3/lib/python3.7/site-packages/coremltools/converters/keras/_keras_converter.py in convert(model, input_names, output_names, image_input_names, input_name_shape_dict, is_bgr, red_bias, green_bias, blue_bias, gray_bias, image_scale, class_labels, predicted_feature_name, model_precision, predicted_probabilities_output, add_custom_layers, custom_conversion_functions, input_shapes, output_shapes, respect_trainable, use_float_arraytype)\n        801                          output_shapes=output_shapes,\n        802                          respect_trainable=respect_trainable,\n    --&gt; 803                          use_float_arraytype=use_float_arraytype)\n        804 \n        805     return _MLModel(spec)\n    \n    /Volumes/Common/opt/anaconda3/lib/python3.7/site-packages/coremltools/converters/keras/_keras_converter.py in convertToSpec(model, input_names, output_names, image_input_names, input_name_shape_dict, is_bgr, red_bias, green_bias, blue_bias, gray_bias, image_scale, class_labels, predicted_feature_name, model_precision, predicted_probabilities_output, add_custom_layers, custom_conversion_functions, custom_objects, input_shapes, output_shapes, respect_trainable, use_float_arraytype)\n        561                          respect_trainable=respect_trainable)\n        562     elif _HAS_KERAS2_TF:\n    --&gt; 563         from . import _keras2_converter\n        564         spec = _keras2_converter._convert(model=model,\n        565                                           input_names=input_names,\n    \n    /Volumes/Common/opt/anaconda3/lib/python3.7/site-packages/coremltools/converters/keras/_keras2_converter.py in &lt;module&gt;\n         30         _keras.layers.advanced_activations.ELU: _layers2.convert_activation,\n         31         _keras.layers.advanced_activations.ThresholdedReLU: _layers2.convert_activation,\n    ---&gt; 32         _keras.layers.advanced_activations.Softmax: _layers2.convert_activation,\n         33 \n         34         _keras.layers.convolutional.Conv2D: _layers2.convert_convolution,\n    \n    AttributeError: module 'keras.layers.advanced_activations' has no attribute 'Softmax'\n\nAnd screenshot if trackback in case that formatting was not right:\n\n&amp;#x200B;\n\nList of all my packages:\n\n    Package                            Version             ---------------------------------- -------------------                               \nabsl-py                            0.9.0              \n    alabaster                          0.7.12             \n    anaconda-client                    1.7.2              \n    anaconda-navigator                 1.9.12             \n    anaconda-project                   0.8.3              \n    applaunchservices                  0.2.1              \n    appnope                            0.1.0              \n    appscript                          1.0.1              \n    argh                               0.26.2             \n    asn1crypto                         1.3.0              \n    astor                              0.8.1              \n    astroid                            2.3.3              \n    astropy                            4.0                \n    atomicwrites                       1.3.0              \n    attrs                              19.3.0             \n    autopep8                           1.4.4              \n    Babel                              2.8.0              \n    backcall                           0.1.0              \n    backports.functools-lru-cache      1.6.1              \n    backports.shutil-get-terminal-size 1.0.0              \n    backports.tempfile                 1.0                \n    backports.weakref                  1.0.post1          \n    beautifulsoup4                     4.8.2              \n    bitarray                           1.2.1              \n    bkcharts                           0.2                \n    bleach                             3.1.0              \n    bokeh                              1.4.0              \n    boto                               2.49.0             \n    Bottleneck                         1.3.2              \n    certifi                            2019.11.28         \n    cffi                               1.14.0             \n    chardet                            3.0.4              \n    Click                              7.0                \n    cloudpickle                        1.3.0              \n    clyent                             1.2.2              \n    colorama                           0.4.3              \n    conda                              4.8.3              \n    conda-build                        3.18.11            \n    conda-package-handling             1.6.0              \n    conda-verify                       3.4.2              \n    contextlib2                        0.6.0.post1        \n    coremltools                        3.4                \n    cryptography                       2.8                \n    cycler                             0.10.0             \n    Cython                             0.29.15            \n    cytoolz                            0.10.1             \n    dask                               2.11.0             \n    decorator                          4.4.1              \n    defusedxml                         0.6.0              \n    diff-match-patch                   20181111           \n    distributed                        2.11.0             \n    docutils                           0.16               \n    entrypoints                        0.3                \n    et-xmlfile                         1.0.1              \n    fastcache                          1.1.0              \n    filelock                           3.0.12             \n    flake8                             3.7.9              \n    Flask                              1.1.1              \n    fsspec                             0.6.2              \n    future                             0.18.2             \n    gast                               0.3.3              \n    gevent                             1.4.0              \n    glob2                              0.7                \n    gmpy2                              2.0.8              \n    google-pasta                       0.2.0              \n    greenlet                           0.4.15             \n    grpcio                             1.30.0             \n    h5py                               2.10.0             \n    HeapDict                           1.0.1              \n    html5lib                           1.0.1              \n    hypothesis                         5.5.4              \n    idna                               2.8                \n    imageio                            2.6.1              \n    imagesize                          1.2.0              \n    importlib-metadata                 1.5.0              \n    inflection                         0.4.0              \n    intervaltree                       3.0.2              \n    ipykernel                          5.1.4              \n    ipython                            7.12.0             \n    ipython-genutils                   0.2.0              \n    ipywidgets                         7.5.1              \n    isort                              4.3.21             \n    itsdangerous                       1.1.0              \n    jdcal                              1.4.1              \n    jedi                               0.14.1             \n    Jinja2                             2.11.1             \n    joblib                             0.14.1             \n    json5                              0.9.1              \n    jsonschema                         3.2.0              \n    jupyter                            1.0.0              \n    jupyter-client                     5.3.4              \n    jupyter-console                    6.1.0              \n    jupyter-core                       4.6.1              \n    jupyterlab                         1.2.6              \n    jupyterlab-server                  1.0.6              \n    Keras                              2.0.6              \n    Keras-Applications                 1.0.8              \n    Keras-Preprocessing                1.1.2              \n    keyring                            21.1.0             \n    kiwisolver                         1.1.0              \n    lazy-object-proxy                  1.4.3              \n    libarchive-c                       2.8                \n    lief                               0.9.0              \n    llvmlite                           0.31.0             \n    locket                             0.2.0              \n    lxml                               4.5.0              \n    Markdown                           3.2.2              \n    MarkupSafe                         1.1.1              \n    matplotlib                         3.1.3              \n    mccabe                             0.6.1              \n    mistune                            0.8.4              \n    mkl-fft                            1.0.15             \n    mkl-random                         1.1.0              \n    mkl-service                        2.3.0              \n    mock                               4.0.1              \n    more-itertools                     8.2.0              \n    mpmath                             1.1.0              \n    msgpack                            0.6.1              \n    multipledispatch                   0.6.0              \n    navigator-updater                  0.2.1              \n    nbconvert                          5.6.1              \n    nbformat                           5.0.4              \n    networkx                           2.4                \n    nltk                               3.4.5              \n    nose                               1.3.7              \n    notebook                           6.0.3              \n    numba                              0.48.0             \n    numexpr                            2.7.1              \n    numpy                              1.18.1             \n    numpydoc                           0.9.2              \n    olefile                            0.46               \n    openpyxl                           3.0.3              \n    packaging                          20.1               \n    pandas                             1.0.1              \n    pandocfilters                      1.4.2              \n    parso                              0.5.2              \n    partd                              1.1.0              \n    path                               13.1.0             \n    pathlib2                           2.3.5              \n    pathtools                          0.1.2              \n    patsy                              0.5.1              \n    pep8                               1.7.1              \n    pexpect                            4.8.0              \n    pickleshare                        0.7.5              \n    Pillow                             7.0.0              \n    pip                                20.0.2             \n    pkginfo                            1.5.0.1            \n    pluggy                             0.13.1             \n    ply                                3.11               \n    prometheus-client                  0.7.1              \n    prompt-toolkit                     3.0.3              \n    protobuf                           3.12.2             \n    psutil                             5.6.7              \n    ptyprocess                         0.6.0              \n    py                                 1.8.1              \n    pycodestyle                        2.5.0              \n    pycosat                            0.6.3              \n    pycparser                          2.19               \n    pycrypto                           2.6.1              \n    pycurl                             7.43.0.5           \n    pydocstyle                         4.0.1              \n    pyflakes                           2.1.1              \n    Pygments                           2.5.2              \n    pylint                             2.4.4              \n    pyodbc                             4.0.0-unsupported  \n    pyOpenSSL                          19.1.0             \n    pyparsing                          2.4.6              \n    pyrsistent                         0.15.7             \n    PySocks                            1.7.1              \n    pytest                             5.3.5              \n    pytest-arraydiff                   0.3                \n    pytest-astropy                     0.8.0              \n    pytest-astropy-header              0.1.2              \n    pytest-doctestplus                 0.5.0              \n    pytest-openfiles                   0.4.0              \n    pytest-remotedata                  0.3.2              \n    python-dateutil                    2.8.1              \n    python-jsonrpc-server              0.3.4              \n    python-language-server             0.31.7             \n    pytz                               2019.3             \n    PyWavelets                         1.1.1              \n    PyYAML                             5.3                \n    pyzmq                              18.1.1             \n    QDarkStyle                         2.8                \n    QtAwesome                          0.6.1              \n    qtconsole                          4.6.0              \n    QtPy                               1.9.0              \n    Quandl                             3.5.0              \n    requests                           2.22.0             \n    rope                               0.16.0             \n    Rtree                              0.9.3              \n    ruamel-yaml                        0.15.87            \n    scikit------> image !!!                        0.16.2             \n    scikit-learn                       0.22.1             \n    scipy                              1.4.1              \n    seaborn                            0.10.0             \n    Send2Trash                         1.5.0              \n    setuptools                         46.0.0.post20200309\n    simplegeneric                      0.8.1              \n    singledispatch                     3.4.0.3            \n    six                                1.14.0             \n    snowballstemmer                    2.0.0              \n    sortedcollections                  1.1.2              \n    sortedcontainers                   2.1.0              \n    soupsieve                          1.9.5              \n    Sphinx                             2.4.0              \n    sphinxcontrib-applehelp            1.0.1              \n    sphinxcontrib-devhelp              1.0.1              \n    sphinxcontrib-htmlhelp             1.0.2              \n    sphinxcontrib-jsmath               1.0.1              \n    sphinxcontrib-qthelp               1.0.2              \n    sphinxcontrib-serializinghtml      1.1.3              \n    sphinxcontrib-websupport           1.2.0              \n    spyder                             4.0.1              \n    spyder-kernels                     1.8.1              \n    SQLAlchemy                         1.3.13             \n    statsmodels                        0.11.0             \n    sympy                              1.5.1              \n    tables                             3.6.1              \n    tblib                              1.6.0              \n    tensorboard                        1.14.0             \n    tensorflow                         1.14.0             \n    tensorflow-estimator               1.14.0             \n    termcolor                          1.1.0              \n    terminado                          0.8.3              \n    testpath                           0.4.4              \n    Theano                             1.0.4              \n    toolz                              0.10.0             \n    tornado                            6.0.3              \n    tqdm                               4.42.1             \n    traitlets                          4.3.3              \n    ujson                              1.35               \n    unicodecsv                         0.14.1             \n    urllib3                            1.25.8             \n    watchdog                           0.10.2             \n    wcwidth                            0.1.8              \n    webencodings                       0.5.1              \n    Werkzeug                           1.0.0              \n    wheel                              0.34.2             \n    widgetsnbextension                 3.5.1              \n    wrapt                              1.11.2             \n    wurlitzer                          2.0.0              \n    xlrd                               1.2.0              \n    XlsxWriter                         1.2.7              \n    xlwings                            0.17.1             \n    xlwt                               1.3.0              \n    xmltodict                          0.12.0             \n    yapf                               0.28.0             \n    zict                               1.0.0              \n    zipp                               2.2.0              \n    \n\nPlease help. Thanks", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hnefbp/attributeerror_module_keraslayersadvanced/"}, {"autor": "Zeevac", "date": "2020-05-14 04:10:01", "content": "Object detection way to follow /!/ Hello guys. I have to do a -----> image !!!  processing project  The project is to implement a MATLAB program that detects and identifies playing cards (standard 52-card deck) in a given -----> image !!! .  The pragram should:\n\nThe program should highlight card boundaries and indicate card type-number.\n\n\u2022 The program should work in the following conditions:\n\n\u2713 Images of a single card on a variety of backgrounds\n\n\u2713 Images where the card(s) are not centered and are rotated\n\n\u2713 Images containing multiple non-overlapping cards\n\nMy question is that what should I look for? I have no experience about this things. Should I look for like YOLO, RCNN vs.?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gjf0lo/object_detection_way_to_follow/"}, {"autor": "vineel_writes", "date": "2020-05-14 02:44:58", "content": "Loss function for -----> image !!!  denoising /!/ I am working on a image denoising project. As per my knowledge, I would like to use MSE/L2 loss. Can anyone please do suggest are there any better losses to perform this task in a better way.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gjdqss/loss_function_for_image_denoising/"}, {"autor": "insanelylogical", "date": "2020-05-28 14:59:46", "content": "Is there a term to refer to the class of popular neural network models like Inception, VGG? As opposed to customized architectures? /!/ From what I understand these models were developed with the general goal of -----> image !!!  recognition, (well technically developed just to perform well on ImageNet). But is there a term to refer to these in general, as opposed to some customized CNN developed for a specific task like detecting medical image issues?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gs7sds/is_there_a_term_to_refer_to_the_class_of_popular/"}, {"autor": "Sygmus1897", "date": "2020-05-28 06:44:21", "content": "How do I train Tesseract to recognize strikethrough characters? /!/ I need to read the strikethrough characters from the -----> image !!! . How do I achieve that?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gs14hb/how_do_i_train_tesseract_to_recognize/"}, {"autor": "Vetches1", "date": "2020-03-14 13:01:18", "content": "Getting error about not cloning KerasClassifier object when using sklearn's Randomized Search, how can I fix it? /!/ Hi! I'm currently doing a project to make an -----> image !!!  classifier using Keras and the Fashion MNIST dataset. Currently, I'm trying to use sklearn's `RandomizedSearchCV` to optimize the classifier. The code below is the build function, creates the wrapper from Keras, then runs the optimization. However, after completing all iterations, it errors out with the following error:\n\n\"Cannot clone object &lt;tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x7fb380dce9d0&gt;, as the constructor either does not set or modifies parameter n_neurons\"\n\nHere is the code for the build function:\n\n```\ndef build_model(n_hidden=1, n_neurons=30, input_shape=[187,]):\n    model = keras.models.Sequential()\n    model.add(keras.layers.Flatten(input_shape=input_shape))\n    for layer in range(n_hidden):\n        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n    return model\n```\n\nHere is the code for creating the wrapper:\n\n```\nmlp_optimized_reduced_model = keras.wrappers.scikit_learn.KerasClassifier(build_fn=build_model)\n```\n\nHere is the code to run the randomized search:\n\n```\nfrom scipy.stats import reciprocal\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_distribs = {\n    \"n_hidden\": [0, 1, 2, 3],\n    \"n_neurons\": np.arange(1, 100),\n}\n\nrnd_search_cv = RandomizedSearchCV(mlp_optimized_reduced_model, param_distribs, n_iter=1, cv=3, verbose=2)\nrnd_search_cv.fit(X_train_reduced, y_train, epochs=1,\n                  validation_data=(X_validation_reduced, y_validation),\n                  callbacks=[mlp_optimized_reduced_model_checkpoint_cb],\n                  verbose=2)\n```\n\nI've tried reading through some GitHub posts, but I've found nothing concrete besides reverting my current sklearn version, which I'm slightly hesitant to do. If that is what must be done, so be it, but I'm just wondering if there's any other approaches I can use. Any help would be greatly appreciated. Thanks so much!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fii03t/getting_error_about_not_cloning_kerasclassifier/"}, {"autor": "aholen", "date": "2020-03-14 08:36:17", "content": "Object Detection: Training on cropped, not labelled images /!/ Hi all, \n\nI'm basically using this [guide](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html) \\- except that the -----> image !!! s that I have are already cropped around the objects I'm training to recognize (only one label/type of object), so I made the annotation-files to use the entire -----> image !!!  as an object.  \n\nI have now trained around 5000 \"steps\", with a loss of 0.3433 - and when I'm testing with webcam, the whole videframe is detected as my one label.\n\nAnything I'm doing wrong?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/fifdex/object_detection_training_on_cropped_not_labelled/"}, {"autor": "Dawny33", "date": "2020-12-21 13:07:28", "content": "How do I optimize nparray to list for large -----> image !!!  matrices? /!/ I am trying to convert image matrices to list, in order to send them as a request to an inference API. \n\n&amp;#x200B;\n\nHowever, as most of my files are 100+ pages(I do pdf2img for getting images), the \\`.tolist()\\` operation takes a lot of time (Implementation below:)\n\n&amp;#x200B;\n\npage\\_images = \\[page.image.tolist() for page in pages\\]\n\n&amp;#x200B;\n\nThis single line takes forever to run for 100+ page files.\n\n&amp;#x200B;\n\n\\*\\*Is there something that can be done to improve the speed?\\*\\*  \n\n\nSO link to the same question:   \n[https://stackoverflow.com/questions/65393149/how-do-i-optimize-nparray-to-list-for-large-image-matrices](https://stackoverflow.com/questions/65393149/how-do-i-optimize-nparray-to-list-for-large-image-matrices)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/khh2fg/how_do_i_optimize_nparray_to_list_for_large_image/"}, {"autor": "Rishit-dagli", "date": "2020-12-21 12:33:45", "content": "Low light -----> image !!!  enhancement TFJS /!/ I am delighted to share the TensorFlow JS variants for the MIRNet model, capable of enhancing low-light images to really great extents.\n\nThe Project repo - [https://github.com/Rishit-dagli/MIRNet-TFJS](https://github.com/Rishit-dagli/MIRNet-TFJS)\n\nPlease consider giving it a star if you like it. More details in [this tweet](https://twitter.com/rishit_dagli/status/1340984448343367680).\n\n&amp;#x200B;\n\n[Project results](https://preview.redd.it/mdrgbvofbj661.jpg?width=1048&amp;format=pjpg&amp;auto=webp&amp;s=65b9a6f4cc9cdb3902633d7f8666411fb3e68204)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/khgjqm/low_light_image_enhancement_tfjs/"}, {"autor": "aaaafireball", "date": "2020-12-21 07:14:44", "content": "Images of planets for training data /!/ I am making an AI to detect what planets (I want to do this on my own, so please dont link other ais). For training data, should I send images of just the planet without a background, or should I send the ai pictures of the planet with the back ground (so rater a circle inscribed in a box aka the planet inscribed into the square -----> photo !!! , or just the planet somewhere in the -----> photo !!! ).", "link": "https://www.reddit.com/r/learnmachinelearning/comments/khcdo6/images_of_planets_for_training_data/"}, {"autor": "spmallick", "date": "2020-12-21 06:13:36", "content": "Classification with Localization: Convert any Keras Classifier to a Detector /!/ When it comes to common applications of Deep Learning in Computer Vision, the two answers that pop up in anyone's minds are -----> image !!!  classification and object detection. Unfortunately, writing the code for your own object detector in PyTorch or Keras is a difficult task.  \n\n\nIn this blog, we will introduce a method for carrying out classification with localization using a simple image classifier in TensorFlow.  \n\n\n[https://www.learnopencv.com/classification-with-localization/](https://www.learnopencv.com/classification-with-localization/)\n\nhttps://i.redd.it/7lydpsamfh661.gif", "link": "https://www.reddit.com/r/learnmachinelearning/comments/khbkyq/classification_with_localization_convert_any/"}, {"autor": "patdata", "date": "2020-12-20 16:08:55", "content": "Use ML/DL to extract a certain portion of an -----> image !!! ? /!/ I'm fairly new to ML/DeepLearning i'm trying to do two tasks using ML/DL on supermarket product brochures (there are around 2000 of them)\n\nTask 1 : Extract product sub images (with product image, name and price in one sub image)\n\nTask 2 : Extract price, product and product name separately in different sub-images\n\n&amp;#x200B;\n\nFor Task 2 i have thought about an approach where i use pretrained YOLOX and fine tune it on annotated dataset. I do annotation(using labelImg) by drawing bounding boxes on three types of classes ( price, product and product name ). \n\n&amp;#x200B;\n\nI have tried with opencv based approaches as described in [here](https://stackoverflow.com/questions/49049318/cropping-rectangular-photos-from-scans-in-opencv-with-python/49070934), [here](https://circuitdigest.com/tutorial/image-segmentation-using-opencv), [here](https://stackoverflow.com/q/4608255/5305401),[here](https://stackoverflow.com/a/31013215/5305401),[here](https://stackoverflow.com/questions/7667643/opencv-locate-and-process-store-shelf-labels-in-photo/7668366#7668366) and [here](https://towardsdatascience.com/edges-and-contours-basics-with-opencv-66d3263fd6d1) but with limited success as im not able to generalize it well.\n\nPlease let me know your opinions on how to best to approach both Task 1 and Task 2", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kgwlm2/use_mldl_to_extract_a_certain_portion_of_an_image/"}, {"autor": "smallfriedpotato", "date": "2020-12-20 13:58:23", "content": "Coarse grained simulations of molecules using machine learning techniques [Resources Help] /!/ Hello machine learners!\n\nI am somewhat new to the field of machine learning. I have taken the Ng class about 1,5 year ago but have not really used the knowledge ever since. Just used tensorflow for -----> image !!!  segmentation, -----> image !!!  recognition (mostly blindly following tutorials). \n\nI am a chemical engineer and I would like to use machine learning for the simulation of molecules as my thesis.\n(Am I over my head...? :) )\n\nDo you have any resources (more like a \"Coarse grained simulations of molecules using machine learning for dummies\" ) that you would suggest?\n\nThank you in advance!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kguezg/coarse_grained_simulations_of_molecules_using/"}, {"autor": "Independent-Square32", "date": "2020-08-14 14:30:01", "content": "80+ Jupyter Notebook tutorials on -----> image !!!  classification, object detection and -----> image !!!  segmentation", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i9mzax/80_jupyter_notebook_tutorials_on_image/"}, {"autor": "Aioli-Pleasant", "date": "2020-08-14 14:09:36", "content": "Accurate 3D Human Pose and Mesh Estimation from a Single RGB -----> Image !!! ", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i9mmu2/accurate_3d_human_pose_and_mesh_estimation_from_a/"}, {"autor": "piyush2003m", "date": "2020-08-13 13:26:13", "content": "Need some help for research paper /!/ I am writing a research paper on techniques used for handling imbalanced multi class -----> image !!!  datasets using CNN. Currently I have chosen random over sampling and random under sampling to compare. Do you guys have any suggestions for changing or adding something? Any help would be greatly appreciated!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/i8zvdn/need_some_help_for_research_paper/"}, {"autor": "brgreen25", "date": "2020-05-06 19:41:13", "content": "Deep Convolutional GAN for CT images /!/  Sorry if my question is too generic, but I'm new to deep learning. I'm creating a GAN network to generate CT images.\n\nThe train dataset is of 848x848 -----> image !!! , and the generated ones should have to be the same size, but I get CUDA out of memory error.\n\nI tried putting the batch size to 1 and still get the same error.\n\nIs there any way for me to solve this issue since I can't decrease the batch size more?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/geqy18/deep_convolutional_gan_for_ct_images/"}, {"autor": "Abdalrahman12", "date": "2020-05-09 05:30:13", "content": "Is it natural that an -----> image !!!  captioning model becomes spammy and a lot slower to train when adding attention?!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gg9lqd/is_it_natural_that_an_image_captioning_model/"}, {"autor": "iAliboy", "date": "2020-05-09 03:49:31", "content": "Latent space representation /!/ Hello everyone I am very new to ml and I'm playing around with Gans. I have made a basic DC Gan and instead of a random noise vector I want to convert my own -----> image !!!  into latent space. However I can't seem to find out how to do it online. I have seen Auto encoders but I don't want the entire model I just want an encoder. Are there any resources or examples that you guys know of that will help me covert single images to latent space. Thank you", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gg87kh/latent_space_representation/"}, {"autor": "MrMegaGamerz", "date": "2020-05-09 00:00:06", "content": "Losses of NAN, Accuracy of 0, I tried everything! (CNN) /!/ Hello! I'm trying to create a NN which can recognize the letters of the alphabet (26 classes). I apologize for the lengthy post, but I've included all my relevant code to be as clear as possible. In the end I've explained the issue.\n\nThe following block is where I name the paths correctly, and standardize/normalize the -----> image !!!  and get it ready for training.\n\n    X = [] # Image data y = [] # Labels  datagen = ImageDataGenerator(samplewise_center=True)  for path in imagepaths:   img = cv2.imread(path)   img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)    img = cv2.resize(img, (200, 200))   img = image.img_to_array(img)   img = datagen.standardize(img)   X.append(img)    # Processing label in image path   category = path.split(\"\\\\\")[1]   #print(category)   split = (category.split(\"_\"))        if int(split[0]) == 0:     label = int(split[1])   else:     label = int(split[0])   y.append(label)  # Turn X and y into np.array to speed up train_test_split X = np.array(X, dtype=\"uint8\") X = X.reshape(len(imagepaths), 200, 200, 1)  y = np.array(y) \n\nCreating the test set.\n\n    ts = 0.3  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ts, random_state=42) \n\nCreating a model. Dense of 26, one output for each letter, and size 200,200,1 to match input:\n\n    model = Sequential() model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(200, 200, 1)))  model.add(MaxPooling2D((2, 2))) model.add(BatchNormalization()) model.add(Dropout(0.5)) model.add(Conv2D(64, (3, 3), activation='relu'))  model.add(MaxPooling2D((2, 2))) model.add(BatchNormalization()) model.add(Dropout(0.5)) model.add(Conv2D(64, (3, 3), activation='relu')) model.add(MaxPooling2D((2, 2))) model.add(BatchNormalization()) model.add(Dropout(0.5)) model.add(Flatten()) model.add(Dense(128, activation='relu')) model.add(Dense(26, activation='softmax')) \n\nModel compiler and fit:\n\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  model.fit(X_train, y_train, epochs=1, batch_size=64, verbose=1, validation_data=(X_test, y_test)) \n\nThe issue comes here where the output of my [model.fit](https://model.fit/) is:\n\n    Train on 54600 samples, validate on 23400 samples Epoch 1/1 54600/54600 [==============================] - 79s 1ms/step - loss: nan - accuracy: 1.8315e-05 - val_loss: nan - val_accuracy: 0.0000e+00 \n\nI understand that it may not be high accuracy or anything from the get-go, but why are the losses nan? I posted elsewhere and I was first told to normalize my data (which I fixed for this post). Then I was told that it was possible that my dataset is corrupt or leaking - this is not the case because when I don't do 26 letters at once, it works perfectly fine. (Meaning I tested the code, using letters A-J, dense = 10, etc) and got a high accuracy of about 95%.\n\nAny help is appreciated, I've been scratching my had at this for hours!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gg4qs7/losses_of_nan_accuracy_of_0_i_tried_everything_cnn/"}, {"autor": "MrMegaGamerz", "date": "2020-05-08 22:57:37", "content": "Trying to standardize data to go into CNN, but having issues /!/ I have the following snippet of code and I'm trying to standardize the data before training my CNN. \n\n    X = [] # -----> Image !!!  data\n    y = [] # Labels\n    \n    datagen = ImageDataGenerator(samplewise_center=True)\n    \n    Loops through imagepaths to load images and labels into arrays\n    for path in imagepaths:\n      img = cv2.imread(path) # Reads image and returns np.array\n      img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n      img = cv2.resize(img, (200, 200)) \n      img = datagen.standardize(img) #ERROR POINTING HERE\n      X.append(img)\n    \n    ....\n\nHowever, when running this I get the following error prompt pointing to the line I've commented above:\n\n    UFuncTypeError: Cannot cast ufunc 'subtract' output from dtype('float64') to dtype('uint8') with casting rule 'same_kind' \n\nAny idea on where I'm going wrong in standardizing? Or is there any easier way for me to standardize? I saw some solutions where people divide by 255, but I'm not sure exactly how to implement that. Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gg3oh0/trying_to_standardize_data_to_go_into_cnn_but/"}, {"autor": "LucidCoding", "date": "2020-04-19 14:13:21", "content": "Built a toolkit for training line-level Handwritten Text Recognition system: model, data preparation, training, metrics and more. /!/ The toolkit is built on top of TensorFlow/Keras. It is shipped with a ready-to-train [CNN-1DRNN-CTC](http://www.jpuigcerver.net/pubs/jpuigcerver_icdar2017.pdf) model and all the surrounding code enabling training, performance evaluation, and prediction.\n\nIn a nutshell, you only have to tell the toolkit how to obtain the raw handwriting examples of a form **line -----> image !!!  -&gt; text**. The rest will be taken care of automatically including things like data preprocessing, normalization, generating batches of training data, training, etc.\n\nYou can train the model on the IAM Handwriting dataset as well as your own. Also, the code should work for arbitrary written language, not just English (at least in theory).\n\nI should mention that I was developing and testing the code on Ubuntu OS. So it might not work under Windows or Mac OS.\n\nIn any case, if anybody is interested here is a link to the repository:\n\n[A link to repository](https://github.com/X-rayLaser/Keras-HTR)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g48bg3/built_a_toolkit_for_training_linelevel/"}, {"autor": "matigekunst", "date": "2020-04-19 06:56:31", "content": "Looking for pretrained perceptual loss network with larger input than 224x224 /!/ Perceptual losses work great. I'm able to project real faces into the StyleGAN2 latent space that are (near) indistinguishable from the real -----> image !!!  at a resolution of 256x256. Images of 1k resolution, however, have some noticeable artifacts. I'm looking for a pretrained network with a larger input size other than the popular 224x224 (AlexNet, VGG, Inception). I know this might be a long shot because the very reason why I'm looking for a pretrained model (it would take ages to train) is probably why it doesn't exist yet, but asking has never hurt anyone! Another option I'm considering is adding a few layers with a larger input size at the bottom and training the model that way. Normally transfer learning is done at the end of the network so I have no idea whether this could or would work. Any papers or material on this idea are also welcome!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g438pi/looking_for_pretrained_perceptual_loss_network/"}, {"autor": "Carb2750", "date": "2020-08-24 22:49:57", "content": "Just some segmentation! Before and during quarantine /!/ This model use a u-net architecture to segment the hair, then I proceed to blend the -----> photo !!!  (only the segmented part) with the selected color.\n\n*Processing video rj8kp48g21j51...*", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ifzuko/just_some_segmentation_before_and_during/"}, {"autor": "HTKasd", "date": "2020-08-24 12:18:51", "content": "Latest papers on CycleGANs? /!/ I am trying to develop a project which involves -----> image !!!  to -----> image !!!  (Colored) translation. Does anybody know any papers which uses latest principles applied over CycleGANs (or similar architectures) which are more preferred to be used for a project as mentioned above?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ifnvin/latest_papers_on_cyclegans/"}, {"autor": "always-stressed", "date": "2020-08-24 03:08:04", "content": "Some questions about a ML paper I'm trying to understand /!/ Hello All,\n\nI've been instructed to read this paper: [AdaS: Adaptive Scheduling of stochastic gradients](https://arxiv.org/pdf/2006.06587.pdf) and I had some questions:\n\n**Defn 1**:\n\n\\- This is my first experience with tensor decomposition, and was confused about what 'low-rank singular values' means?\n\n\\- why is the tensor only in 4 dimensions? if its the weights of the conv layer then it should have as many dimensions as there are kernels/filters right?\n\n\\- what do they mean by 'knowledge gain across *a particular channel'*? Is this referring to the depth of an -----> image !!! ?  It's the particular channel (dth dim) of the tensor representing the weights of the conv so kinda confusing - especially since the tensor should only have 4 dimensions?\n\n**Defn 2:**\n\n\\- what does \" low-rank singular values of a single-channel convolutional weight\" mean (same q in 1)\n\n**Semi-related qs:**\n\n\\- what is a 'superblock'? How does it defer from a block?\n\nThank you for your help!\n\n&amp;#x200B;\n\nE: a few more questions", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ifh6r9/some_questions_about_a_ml_paper_im_trying_to/"}, {"autor": "OnlyProggingForFun", "date": "2020-08-24 00:12:27", "content": "Here's a new paper announced in the ECCV2020 where they proposed a new technique for 3D Human Pose and Mesh Estimation from a single RGB -----> image !!! ! (with code available)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ifeh1x/heres_a_new_paper_announced_in_the_eccv2020_where/"}, {"autor": "al_mc_y", "date": "2020-08-23 22:39:40", "content": "Neural Network Categorization - any sense or way to have an \"unrecognized\" category? /!/ Hi all.\nI've gone through Sentdex's Neural Networks from Scratch book online (bought the early access).\nI've been thinking about supervised categorization - and I'm wondering if there's any sense in having an \"unrecognized\" category in a model.\nIt seems that in a categorization model you force the  model to return one of the items as the highest probability match - however if your input is garbage, then your model will \"successfully\" (and incorrectly) categorize the garbage as one of the true categories.\nMy question is, are NNs made with a \"none of these\" category? Eg if you build and train a model of the Fashion MNIST dataset and then input a -----> picture !!!  of say a cat which has otherwise been properly preprocessed (correct scale, colorspace, etc) - can you get the cat/yacht/other non fashion image to be categorized in this \"other\" bin, rather than as a t-shirt or a shoe?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ifcz09/neural_network_categorization_any_sense_or_way_to/"}, {"autor": "Simaniac", "date": "2020-08-23 18:51:37", "content": "Why is my StyleGAN2 model trying to recreate specific images in my dataset? /!/ I've been training a dataset of just over 1000 images over the last 2 weeks or so. For some reason, when I generate images, it seems to only be creating images that are clearly attempts at recreating specific images in the dataset, rather than trying to meld together attributes from all the images in the dataset. What could be causing this? \n\nNot only that, but those -----> image !!!  recreations pop up constantly with little variation. So even though I've provided a dataset of over a thousand different images, the AI seems to just want to focus on a selection of maybe 30 or so images from the dataset and keep attempting to recreate them. \n\nHow do I get the machine to actually generate images that are actually unique and not just attempted carbon copies of very specific images in the dataset?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/if8t21/why_is_my_stylegan2_model_trying_to_recreate/"}, {"autor": "aboughtcusto", "date": "2020-07-26 23:14:06", "content": "I used KNN to show differences in color spaces when selecting a color palette from an -----> image !!! ", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hygy6r/i_used_knn_to_show_differences_in_color_spaces/"}, {"autor": "focusedchi", "date": "2020-10-05 17:59:34", "content": "Performing OCR w/ Tesseract4Android /!/ I am currently using this to extract text from a pdf with images and then logging it. I make use of the meanConfidence() in the API and my confidence is really low \\~49 . \n\nHow would I increase the accuracy and go about ensuring the extracted text matches a lot close to the text in the -----> image !!! ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j5ocmk/performing_ocr_w_tesseract4android/"}, {"autor": "jstar77", "date": "2020-10-05 14:27:55", "content": "Object Recognition For The Hobbyist /!/ Without using services from Clarifai, Amazon, Microsoft etc....  What is the best option for recognizing objects in images  with a very limited scope?  My current use case is binary {does this -----> image !!!  contain a vehicle $true or $false}.  The API's from the commercial services above do a really good job at this but I would like to be able to do this on device.  Are there any higher level image recognition packages aimed at the hobbyist?  A PowerShell module or something that integrates with PowerShell would be great but I'm open to any option.    I'm a bit of a newbie in this domain and so far what I have found are low level options like Tensor Flow and high level options from commercial providers.  I feel like maybe I am missing a search term for this specific aspect of machine learning. Any advice that puts me on the right path would be appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j5kd6l/object_recognition_for_the_hobbyist/"}, {"autor": "madzthakz", "date": "2020-10-05 13:29:38", "content": "[X-Post] IAMA Senior Data Scientist at Disney and I\u2019m setting up free Q&amp;A Zoom sessions to help people who are looking to enter/transition into data science /!/ DISCLAIMER: This is completely free and not sponsored in any way. I really just enjoy helping folks get into Data Science\n\nSome of you may have already seen this on the data science subreddit but I still want to post for those who haven\u2019t. I\u2019m a Senior Data Scientist at Disney and I\u2019ve had a bit of an unorthodox path into this field and learned a few things along the way. I\u2019ve been trying to help aspiring data scientists and MLEs get started by answering their questions via ZOOM Q&amp;As and our first few sessions have gone really well! \n\nWe\u2019re planning to host these sessions monthly and we\u2019ll be posting Q&amp;A content (recordings, articles, etc) on our blog weekly. They can be found [here](https://www.madhavthaker.com/qaposts).\n\nTo make sure you\u2019re included in any future sessions, please sign up using the following [form](https://forms.gle/Qs333FLRahxY6vSu7). The extra questions on there also help us create content for future sessions.\n\nHope to see you all soon!\n\nVerification:\n\n* My -----> photo !!! : [https://imgur.com/a/Wg3DMLV](https://imgur.com/a/Wg3DMLV)\n* My LinkedIn: [https://www.linkedin.com/in/madhavthaker/](https://www.linkedin.com/in/madhavthaker/)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/j5jeog/xpost_iama_senior_data_scientist_at_disney_and_im/"}, {"autor": "cloud_weather", "date": "2020-08-30 13:42:12", "content": "-----> Image !!!  Decomposition AI - Edit Highlights and Textures Easily", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ijcuy8/image_decomposition_ai_edit_highlights_and/"}, {"autor": "Liberal__af", "date": "2020-05-25 17:12:43", "content": "NLP vs Vision! I'm totally confused /!/ I  know I love DS but can't decide what should my destination be! I'm already involved in stuff related to Temporal Data (Business problems mostly), but, I want to get my hands dirty with NLP or Vision. I'm more  inclined towards NLP because it requires less computational resources  and I haven't seen someone asking for C++ knowledge with regards to NLP.  On the contrary, vision is fancy and imagine classification has already  reached a level where it's literally a technology unlike NLP which is  still evolving. What are my career prospects if I choose to put all my  efforts into NLP? Is it a good bet to assume that it would evolve like -----> Image !!!  classification did? Or should I perhaps, learn opencv and ditch NLP as I find the applications of NLP with the mastery achieved in the field right now to be limited! I'm totally confused.\n\nPS: I'm  interested in both vision and NLP, it's only that I believe I can make a good career by mastering only one of the two and as vision is intensive in terms of computational resources, coding knowledge(I can only code in Python, did C programming during my Bachelor's, never even looked into c++) compared to NLP and I have a non CS  bachelor's. Am I making reasonably rational assumptions?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/gqewl7/nlp_vs_vision_im_totally_confused/"}, {"autor": "sidneyy9", "date": "2020-04-21 19:07:25", "content": "covn2D input_shape -----> image !!!  classification /!/ Hi everyone,\n\nI am working on about image classification  project with VGG16.\n\n`base_model=VGG16(weights='imagenet',include_top=False,input_shape=(224,224,3))`\n\n`X_train = base_model.predict(X_train)`\n\n`X_valid = base_model.predict(X_valid)`\n\nwhen i run predict function i took that shape for X\\_train and X\\_valid\n\n`X_train.shape, X_valid.shape -&gt;`  \n Out\\[13\\]: ((3741, 7, 7, 512), (936, 7, 7, 512))\n\ni need to give input\\_shape for first layer the model but they do not match both.\n\n`model.add(Conv2D(32,kernel_size=(3, 3),activation='relu',padding='same',input_shape=(224,224,3),data_format=\"channels_last\"))`\n\ni tried to use reshape function like in the below code .  it gave to me valueError.\n\n`X_train = X_train.reshape(3741,224,224,3)`\n\n`X_valid = X_valid.reshape(936,224,224,3)`\n\nValueError: cannot reshape array of size 93854208 into shape (3741,224,224,3)\n\nhow can i fix that problem , someone can give me advice? thanks all.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g5lfk0/covn2d_input_shape_image_classification/"}, {"autor": "Shensmobile", "date": "2020-04-21 18:32:28", "content": "How to utilize Quarantine to its fullest /!/ Hi everyone,\n\nI've been itching to get better at Machine Learning and I've been taking it step by step when I've had time, but I've been given a golden opportunity and want to capitalize.  I have a major project coming to an end soon and I'll be \"without work\" probably until quarantine ends.  This is the perfect time for me to really dive in and give it some serious dedicated time.\n\nTo date, I've done:\n\n* Andrew Ng Machine Learning course on Coursera\n\n* Deep Learning with PyTorch on Udacity\n\n* Fast.ai part 1\n\nFast.ai was so easy to set up, I used it for a few NLP and -----> image !!!  classification tasks but I'd like to switch to using PyTorch natively moving forward.  I understand that Fast.ai streamlines a lot of the process but I feel like I'm not quite understanding the underlying principles to machine learning when I keep using their data loaders and learners.  I feel like I'm cheating when I use fast.ai, if that makes sense.  My plan is to tackle some of the medical Kaggle challenges (I work in the Medical space mostly) like the annual RSNA competitions.  I feel like being able to be competitive in those competitions would be a good goal/metric to measure myself by.\n\nWhat would be some good resources to help me along the way?  I have a decent background in coding (it's what I currently do for work) but I anticipate my largest challenges will be data preparation/processing and statistics.  Those are areas I'd like to improve at, especially  since I feel like the fast.ai library abstracts away the data prep/metrics/statistics in its attempt to streamline things.  If there are classes/courses or textbooks you guys would recommend, I'd appreciate it so much!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g5kraf/how_to_utilize_quarantine_to_its_fullest/"}, {"autor": "Chillo4747", "date": "2020-04-21 17:07:00", "content": "Creating Object Detection and Tracking Tool /!/ Hey Guys,\n\nI am pretty new into Object Detection and Tracking.\n\nI want to create a Tool, which can do following things:\n\n1. Detect Objects (not only one)\n2. Track them\n3. Create Trajectories from the Trackings\n4. Convert the Trajectories from -----> image !!!  coordinates to real world coordinates\n5. Give me a .csv or something similar with the trajectories for each object\n\nWhat would you say is the best way to create such a tool?\n\nWhere do i start? Are there some good tutorial to begin with?\n\n&amp;#x200B;\n\nThank you!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/g5j4wy/creating_object_detection_and_tracking_tool/"}, {"autor": "OnlyProggingForFun", "date": "2020-11-25 12:54:49", "content": "This AI Can Generate the Other Half of a -----> Picture !!!  Using a GPT Model", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k0ro7l/this_ai_can_generate_the_other_half_of_a_picture/"}, {"autor": "ISeeThings404", "date": "2020-11-23 19:25:14", "content": "Inception V1 Module and GoogLeNet (yes that's how you spell it) explained. /!/ Hey all. I'm a growing content creator primarily in Machine Learning. Someone on my Twitter asked me to help them understand Inception V1 Module so I made this video. \n\nLink: [https://www.youtube.com/watch?v=bcYkvnfrRI0&amp;feature=youtu.be](https://www.youtube.com/watch?v=bcYkvnfrRI0&amp;feature=youtu.be)\n\nIf you want to know more: \n\n This video explains in the Inception V1 module and GoogLeNet Neural Network. These are legendary neural networks both in the -----> image !!!  classification and detection phase and in the creation of Neural Networks in general. These showed that completely connected networks were inefficient for the cost and it was much cheaper to use sparse but deep networks in this case (technically another paper showed the last bit mathematically, but details). This is definitely a super-duper cool idea, and I thank my Twitter followers for this. I had a lot of fun learning about this. If there are any topics you want to be covered, let me know. \n\n Paper Details: \n\n Paper: Going Deeper with Convolutions Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich  \n\nLink: [https://arxiv.org/abs/1409.4842](https://www.youtube.com/redirect?q=https%3A%2F%2Farxiv.org%2Fabs%2F1409.4842&amp;v=bcYkvnfrRI0&amp;redir_token=QUFFLUhqbFRxMXduVGgzRWNValJZT2syVUNsQjhTQmRnZ3xBQ3Jtc0trTzZDbDRSdjdheU5zQjJtbjZiNkFvZUI5VGVtS1g3ZW1NdDY2VFFlcTZUS00wQXExQjNyLWF0Y1lYSUFLNEVvQVRtS1FvUXhuWllBVkxkTjVPZVN2RUhhU3dSUVZhbnBVbndfYS1rVTN5SDR6TU9TZw%3D%3D&amp;event=video_description) \n\nAbstract: We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jzog0h/inception_v1_module_and_googlenet_yes_thats_how/"}, {"autor": "mwitiderrick", "date": "2020-11-23 15:01:23", "content": "[D] Image Segmentation in Android with Fritz AI /!/ Add -----> image !!!  segmentation capabilities to your mobile apps using Fritz AI\n\n[https://heartbeat.fritz.ai/-----> image !!! -segmentation-in-android-with-fritz-ai-111b258802a3](https://heartbeat.fritz.ai/-----> image !!! -segmentation-in-android-with-fritz-ai-111b258802a3)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jzj4xg/d_image_segmentation_in_android_with_fritz_ai/"}, {"autor": "OnlyProggingForFun", "date": "2020-11-23 14:19:21", "content": "-----> IMAGE !!! -TO-PAINTING TRANSLATION WITH STYLE TRANSFER. This Image-to-Painting Translation method simulates a real painter on multiple styles using a novel approach that does not involve any GAN architecture, unlike all the current state-of-the-art approaches!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jziem0/imagetopainting_translation_with_style_transfer/"}, {"autor": "hiphop1987", "date": "2020-11-11 13:46:54", "content": "Don\u2019t Repeat These Mistakes with SQL /!/ SQL and Machine Learning have a few things in common. It\u2019s easy to start with one as it doesn\u2019t require a lot of coding. Also, code rarely crashes.\n\nI would argue that the fact that the SQL queries don\u2019t crash makes the Data Analysis even harder. How many datasets I\u2019ve extracted from the database, that turned out to have wrong or missing data? Many!\n\n# 1. Not knowing in what order queries execute\n\n[SQL query execution order](https://preview.redd.it/ncty0v0e7my51.png?width=1400&amp;format=png&amp;auto=webp&amp;s=7cfa8f55df0828f3ef61534b78d8668bfc7615a3)\n\nSQL has a low barrier to entry. You start writing queries \u2014 use a JOIN here and there, do some grouping and you\u2019re already an expert (at least some people think so).\n\n**But does the so-called expert even know in what order do SQL queries execute?**\n\nSQL queries don\u2019t start with SELECT \u2014 they do in the editor when we write them, but the database doesn\u2019t start with SELECT.\n\nThe database starts executing queries with FROM and JOIN. That\u2019s why we can use fields from JOINed tables in WHERE.\n\nWhy can\u2019t we filter the result of GROUP BY in WHERE? Because GROUP BY executes after WHERE. Hence, the reason for HAVING.\n\nAt last, we come to SELECT. It selects which columns to include and defines which aggregations to calculate. Also, Window Functions execute here.\n\nThis explains why we get an error when we try to filter with the output of a Window Function in WHERE.\n\n# 2. What do Window Functions actually do?\n\n[Example of a transformation with a SUM Window Function](https://preview.redd.it/jxz0203i7my51.png?width=1400&amp;format=png&amp;auto=webp&amp;s=b04a2a8be6a2e73493a00137cddae123fae43ee6)\n\nWindow Functions seemed cryptic to me when I first encountered them. Why use Window Functions as GROUP BY can aggregate the data?\n\nWell, a Window Function (WF) simplifies many operations when designing queries:\n\n* WF allows access to the records right before and after the current record. See Lead and Lag functions.\n* WF can perform an additional aggregation on already aggregated data with GROUP BY. See the example in the -----> image !!!  above, where I calculate sales all with a WF.\n* ROW\\_NUMBER WF enumerates the rows. We can also use it to remove duplicate records with it. Or to take a random sample.\n* As the name suggests WF can calculate statistics on a given window:\n\n&amp;#8203;\n\n    sum(sales) OVER (PARTITION BY CustomerID BY ts ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as cumulative_sum\n\nThe WF above would calculate the cumulative sum from the first record to the current record.\n\n**Where did I do a mistake with Window Functions?**\n\nI didn\u2019t take the time for a tutorial that would explain the basics and the power of Window Functions. Consequently, I avoided them and the queries became overcomplicated. Then bugs creep in.\n\nSee [Don\u2019t Make These 5 Mistakes with SQL](https://towardsdatascience.com/dont-repeat-these-5-mistakes-with-sql-9f61d6f5324f?sk=a688d76b0fa3fc4c737552aedcbe5aad) for more. \n\nThanks for reading. Feedback is welcome.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/js8e8e/dont_repeat_these_mistakes_with_sql/"}, {"autor": "tim-hilt", "date": "2020-11-11 07:17:09", "content": "Keras: Extract Features at Multiple Image-Scales /!/ Hi there! I try to replicate the results of [this paper](https://link.springer.com/article/10.1007/s11263-015-0872-3). They state, that they used VGG16- and VGG19-models pretrained on imagenet and used the output of the last convolutional layer (without relu and max-pooling) as feature vectors.\n\nMy configuration to configure the model accordingly is the following:\n\n    from tensorflow.keras.models import Model\n    from tensorflow.keras.applications import VGG16\n    \n    base_model = VGG16(include_top=False)  # Cut off the fully-connected-layers\n    model = Model(inputs=base_model.input, outputs=base_model.get_layer('block5_conv3').output)  # Discard the last max-pooling-layer\n    model.layers[-1].activation = None  # Change activation-function of last layer from Relu to Linear\n\nand i get a feature-tensor of shape (1, 14, 14, 512) for the default input-shape of (224, 224, 3). So far, this looks the way i would want it to be. However, the authors state:\n\n&gt;The VGG-M \\[equivalent to VGG16 in the context of the paper\\] convolutional features are extracted as the output of the last  convolutional layer, directly from the linear filters excluding ReLU  and max pooling, which **yields a field of 512-dimensional descriptor  vectors**\n\nNow, i have stated above, that the number 512 is part of my output-shape. However i thought, that it means that i get back 512 individual -----> image !!! -patches of size 14x14! The only way i could think of, that would get me 512-dimensional descriptor vectors would be something like this:\n\n    features = model.predict(img)\n    \n    feature_vectors = []\n    for i in range(features.shape[1]):\n        for j in range(features.shape[2]):\n            feature_vectors.append(features[0, i, j, :])\n    feature_vectors = np.array(feature_vectors)\n\nBut then i would slice through all of the existing image-patches! \n\n**Question 1:**\n\nDid the authors mean to do just that? Is this a common practice anyone her has used before? All the tutorials or blogposts i found online just `flatten()` the output-tensor and add it to the database of existing feature-vectors.\n\n**Question 2:**\n\nThe authors also state, that:\n\n&gt;...local descriptors are extracted at multiple scales, obtained by rescaling the image by factors 2\\^*s*,*s*=\u22123,\u22122.5,\u2026,1.5 (but, for efficiency, discarding scales that would make the image larger than 1024\\^2 pixels).\n\nI can totally extract images at different scales, by specifying the `input_shape`, when instantiating the VGG16-model. My method of getting 512-dimensional feature-vectors stated above would also work in that case, even though the output-tensor could be much larger (i.e. a shape of (1, 64, 64, 512)) in case of a bigger image.\n\nIs this the way to do feature-extraction at multiple scales?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/js3n9c/keras_extract_features_at_multiple_imagescales/"}, {"autor": "LeCollegeAbandon", "date": "2020-07-20 03:44:18", "content": "GAN Question /!/ Hi Friends,\n\nGAN Newb here. I am hoping someone could tell me whether the following is possible:  \n\n\nI know GAN can generate random human faces. But can it generate more than 1 -----> photo !!!  of the randomly generated human face? I.e can it do a photo of them with their neck rotated slightly, another with the same person looking forward, etc. Small changes like that. Or is it all completely random and no real way to generate the same person over again but in a different pose or look on their face.\n\n&amp;#x200B;\n\nThank You!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hueb95/gan_question/"}, {"autor": "astrohighh", "date": "2020-07-19 23:19:53", "content": "Extract text from an -----> image !!!  - Deep Learning /!/ Could someone lead me in the right direction about what the architecture would be or how it could be done?\n\nAlso, a dataset recommendation for this would be helpful", "link": "https://www.reddit.com/r/learnmachinelearning/comments/huae2e/extract_text_from_an_image_deep_learning/"}, {"autor": "thisispatts", "date": "2020-07-19 18:36:51", "content": "Seeking help for a -----> image !!!  classification problem /!/ I have some images which has various levels of intensities of spark. I need to classify the sparks based on high, medium, low levels. Tried transfer learning with inception model but validation accuracy is close to 50%.How can i achieve around 70 % accuracy.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hu5hvy/seeking_help_for_a_image_classification_problem/"}, {"autor": "0_marauders_0", "date": "2020-07-19 18:14:25", "content": "Can we design a neural network to find a full-sized color -----> image !!!  hidden inside another -----> image !!! ?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hu5343/can_we_design_a_neural_network_to_find_a/"}, {"autor": "[deleted]", "date": "2020-07-18 22:16:49", "content": "Please roast my -----> image !!!  classifier code as it's horrible at predicting food. /!/ [deleted]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/htpch2/please_roast_my_image_classifier_code_as_its/"}, {"autor": "[deleted]", "date": "2020-07-18 22:12:09", "content": "Please roast my -----> image !!!  classifier code as it's horrible at predicting food. /!/ [deleted]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/htp9o2/please_roast_my_image_classifier_code_as_its/"}, {"autor": "gordicaleksa", "date": "2020-07-18 10:39:54", "content": "[P] Open-sourcing Deep Dream repo in PyTorch (MIT license) /!/ Hi folks, I just open-sourced Deep Dream project written purely in Python + PyTorch:\n\n[Deep Dream - static -----> image !!! , video, ouroboros](https://github.com/gordicaleksa/pytorch-deepdream)\n\nI think it's an awesome way to get started with deep learning. I've additionally included a [playground.py](https://playground.py) file that will help you better understand some concepts.\n\nI hope somebody finds this useful!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/htelmp/p_opensourcing_deep_dream_repo_in_pytorch_mit/"}, {"autor": "Stutoucan12", "date": "2020-07-18 07:03:12", "content": "Seeking Help with Generative Modeling /!/ I want to create something similar to that of this video: [https://www.youtube.com/watch?v=OqLVi\\_Jd1FI&amp;t=46s](https://www.youtube.com/watch?v=OqLVi_Jd1FI&amp;t=46s)\n\nThe author says he \"started with oil paintings, then photographed them to build a dataset, then trained a StyleGAN using those photographs\", in which he was able to \"generate some 3D meshes that suggest unfamiliar floating architectural forms.\"\n\nNow, I trained my own StyleGAN v2 on an -----> image !!!  dataset of canvas pictures, in which I can now generate an inifinite amount of weird paintings/videos of them.\n\nSo my question is: How can I take these images/videos and create something HALF as immaculate as in the video? How do I generate my own 3D meshes? I have been looking all over the internet for days. Any help is GREATLY appreciated. Thank you so much.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/htcex1/seeking_help_with_generative_modeling/"}, {"autor": "Bad_memory_Gimli", "date": "2020-10-25 07:41:42", "content": "Beginner having problem with getting custom weights to work [YOLOv3] /!/ I have been fiddling around with deep learning the past few weeks, and I wanted to try to make my own dataset and weights. Well, they don't work. \n\nI am running YOLOv3 through python, and with the default coco weights I have been able to get everything up and running, being able to detect all the objects in the coco name file. \n\nThen I got cocky and wanted to create my own dataset. The object I had at hand was a Lion bar, so that is the single object I want to detect. Here is my progress so far:\n\n- Took 68 images of the Lion bar with a top-down perspective\nThe bar has different rotations, and some of the images are from its backside, most are from the front. The images are 1920x1080, the background is my kitchen bench.\n\n- Uploaded them to supervisely, rotated each -----> image !!!  90 degrees, then 180, then 270, totaling 272 -----> image !!! s.\n\n- Split the dataset into 80% train and 20% validation.\n\n- Based on the coco weights in the supervisely model library, I ran the training with the following settings:\n   - Learning rate: 0.0001\n   - Epochs: 100\n   - Batch size: train: 8\n   - Input size: 416 x 416\n   - Batch normalization momentum: 0.01\n   - Data workers: val: 1, train: 2\n   - Subdivisions: val: 1, train: 1\n   - Checkpoint every: 1\n   - Print every: 5\n   - Recompute anchors: false\n   - Weights init type: Transfer learning\n   - Enable augmentation: false\n\nNow the only thing I have changed here is the epoch, from 5 to 100. To be honest, a lot of the settings are the default from supervisely and I don't know the ramifications of changing them.\n\nResult: Out of 20 test images, with similar perspective and background, it only detects one lion bar (however it has 0.99 confidence in that detection), and nothing else.\n\nWhat am I doing wrong?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jhpl1k/beginner_having_problem_with_getting_custom/"}, {"autor": "rene7vick", "date": "2020-10-24 22:08:40", "content": "Good topic for a -----> image !!!  classification demo for teenagers? /!/ Hi guys, for my project work, I have to create a image classification demo application (based on EfficientNet) and I am curious, if someone has a good idea what dataset/images to use, so its both exciting and makes a little bit fun, as it should attract young students (14-18yo) into machine learning and get a first look into it", "link": "https://www.reddit.com/r/learnmachinelearning/comments/jhht38/good_topic_for_a_image_classification_demo_for/"}, {"autor": "TomerHorowitz", "date": "2020-06-21 20:10:13", "content": "How do you combine an -----> image !!!  and meta data as input? /!/ Well my question is really about text, but image is easier to understand.\n\nLet\u2019s say I have some images and some meta data about the images and I want to train a model with the metadata and the image combined, how would I do it?\n\nWould I combine the meta data and pixels as one array and pass that as input? Would my model learn properly this way?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hddldm/how_do_you_combine_an_image_and_meta_data_as_input/"}, {"autor": "the_dawmbreaker", "date": "2020-06-21 19:20:52", "content": "Pytorch DataLoader not instantiating /!/ I'm trying to build a CNN based classifier for which the dataset has the training and test images stored in a separate folder, with the -----> picture !!!  names and respective classes stored in a .csv file. Below is my custom dataset implementation: \n\nclass CustomDataset(Dataset):\n\ndef \\_\\_init\\_\\_(self, csv\\_file, root, transform = None):\n\nself.annotations = pd.read\\_csv(csv\\_file)\n\nself.root = root\n\nself.transform = transform  \n\ndef \\_\\_length\\_\\_(self):\n\nreturn len(self.annotations)    \n\ndef \\_\\_getitem\\_\\_(self, index):\n\nif torch.is\\_tensor(index):\n\nindex = index.tolist()         \n\nimg\\_path = os.path.join(self.root, self.annotations.iloc\\[index, 0\\])\n\nimg = io.imread(img\\_path)\n\nonehot = preprocessing.OneHotEncoder(handle\\_unknown='ignore')\n\ny\\_label\\_onehot = onehot.fit\\_transform(self.annotations\\[\\['target'\\]\\]).toarray()\n\nif self.transforms:\n\nimg = self.transform(img)          \n\nreturn (img, y\\_label\\_onehot)  \n\n\\------------------------------------------------------------------------------------------------------------------------------------------------- \n\ndata\\_set = CustomDataset(csv\\_file = 'train.csv', root = '/train/')    \n\nData\\_Loader = DataLoader(data\\_set, batch\\_size = 10, shuffle = True\n\n\\------------------------------------------------------------------------------------------------------------------------------------------------- \n\nOn running this, it throws an error: \n\nTraceback (most recent call last):\n\n&amp;#x200B;\n\n  File \"&lt;ipython-input-227-e32ed8ba2148&gt;\", line 2, in &lt;module&gt;\n\nData\\_Loader = DataLoader(data\\_set, batch\\_size = 10, shuffle = True, sampler = None)\n\n&amp;#x200B;\n\n  File \"C:\\\\Users\\\\monse\\\\Anaconda3\\\\envs\\\\practice\\_env\\\\lib\\\\site-packages\\\\torch\\\\utils\\\\data\\\\[dataloader.py](https://dataloader.py)\", line 213, in \\_\\_init\\_\\_\n\nsampler = RandomSampler(dataset)\n\n&amp;#x200B;\n\n  File \"C:\\\\Users\\\\monse\\\\Anaconda3\\\\envs\\\\practice\\_env\\\\lib\\\\site-packages\\\\torch\\\\utils\\\\data\\\\[sampler.py](https://sampler.py)\", line 92, in \\_\\_init\\_\\_\n\nif not isinstance(self.num\\_samples, int) or self.num\\_samples &lt;= 0:\n\n&amp;#x200B;\n\n  File \"C:\\\\Users\\\\monse\\\\Anaconda3\\\\envs\\\\practice\\_env\\\\lib\\\\site-packages\\\\torch\\\\utils\\\\data\\\\[sampler.py](https://sampler.py)\", line 100, in num\\_samples\n\nreturn len(self.data\\_source)\n\n&amp;#x200B;\n\nTypeError: object of type 'CustomDataset' has no len()\n\n\\------------------------------------------------------------------------------------------------------------------------------------------------- \n\nAnyone know any fix for this, or some kind of workaround? Any help would be greatly appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hdcqye/pytorch_dataloader_not_instantiating/"}, {"autor": "mukito6", "date": "2020-06-21 17:17:55", "content": "Beginner programs for predictive models /!/ Hi,  I am new to this world and am attempting to create predictive models just using tabular data(No -----> image !!!  detection or anything like that).\n\nI recently stumbled upon rapid minder go, it seems great for beginners as it requires no coding. I can just upload my data and it will make a few different models for me to choose from,  deep learning, svm, random forest etc. That I can then use to make predictions on new datasets.\n\nAre there any other beginner resources like this or better than this that don\u2019t require coding knowledge?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/hdaluz/beginner_programs_for_predictive_models/"}, {"autor": "soundgardener666", "date": "2020-12-10 13:22:45", "content": "Machine learning algorithm to work with body keypoints from -----> image !!!  /!/ I'm working on a project that will predict whether a person approaches and intends to interact.\n\nThere will be a camera and a pose estimation model that will analyze live frames and save the body parts keypoints of the people in the frame. Now I'm thinking which ML algorithm will learn from our sample videos and know if the person is willing and going to interact with what's under the camera.\n\nEvery 3-5 seconds with 2 frames each second will be the input of the model, which means 6-10 lists of keypoints, which are x and y coordinates of body parts in a frame. The idea is to process these lists to something meaningful that the model can learn from: The ankles are unnecessary for example, but the ratio between the shoulders in the first and last frames will be (meaning that the person got closer). Also the eyes are valuable.\n\nI am expecting some output between 0 to 1 that represents the level of willingness to interact, so that's is also how I'm going to label the sample training. My question is which ML model is suitable for this input and output and you think that will handle this kind of data well and give good predictions?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kaf6f5/machine_learning_algorithm_to_work_with_body/"}, {"autor": "Sasha1296", "date": "2020-09-12 00:27:48", "content": "What is more advisable, keeping 2080Ti vs upgrading to RTX 3080 or 3090 /!/ I bought the 2080 Ti recently and was wondering if it would be worth it to upgrade to the RTX 3080 or the 3090 given that I can get the value I paid for it back right now. I'm mostly going to be doing -----> image !!!  and convolutional networks/lstm and time series stuff. From what I understand the specs for each are:\n\nGTX 2080 Ti:\n\n4352 Cuda cores     544 Tensor cores    11 Gb memory\n\nRTX 3080 :\n\n8704 Cuda cores     272 Tensor cores    10 Gb memory\n\nRTX 3090:\n\n10496 Cuda cores   328 Tensor cores    24 Gb memory\n\nThe GTX 2080 Ti gives one extra gigabite of memory but has about 2x fewer cores... Is this offset by having about double cuda cores? Is there a way to say if one is better than the other?\n\nThe RTX 3090 has a lot more memory and a lot more cuda cores, however would having about 200 fewer tensor cores be an issue?\n\nThe other potential option is to combine two RTX 3080 cards, has anyone done this with other nvidia graphics cards and how did it go/how hard was it to get ml models to run? Would two RTX 3080 graphics cards be better overall than one 3090 graphics card?(aside from having 4 gb less memory)... This is the option I am currently thinking of going with \n\nposted also on r/MachineLearning", "link": "https://www.reddit.com/r/learnmachinelearning/comments/ir2o2g/what_is_more_advisable_keeping_2080ti_vs/"}, {"autor": "Contrail16", "date": "2020-09-11 20:56:46", "content": "Class Activation Mapping (CAM) on Regressing Neural Networks /!/ Hi, I'm currently working on a building a heat-map for a TensorFlow estimator model, the input is not really an -----> image !!!  but rather a signal distance function (which can be plotted to reveal an -----> image !!! ), and a few parameters. I know CAM works for class-activation in CNNs to highlight where the neural network is \"looking\". Can it be applied to a model that performs a regression, or estimation of parameters? Does anyone have any papers/sources of this sort of application. Thanks!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/iqz4rk/class_activation_mapping_cam_on_regressing_neural/"}, {"autor": "pgpgm", "date": "2020-12-13 02:50:48", "content": "[Q][H]Question regarding YOLO /!/  \n\nhave never learnt about machine learning and related knowledge before, I am just a mechanical student who study more in material and design related field.\n\nStarted to know about YOLO few days ago. After searching all the related topic and tutorial and research, I still do not understand what I should do in order to make my own custom object data and auto detect it. I am just too confuse about all the stuffs and all the coding techniques required.\n\nHere are some questions, hope some of you may help me out.\n\n1. What exactly YOLO is? I know it is an algorithm to detect -----> image !!! , but why we need to use darknet? Some use darknet to implement YOLO, some use tensorflow, some use pytorch. What exactly are them?\n2. Darknet can train YOLO dataset, then why someone still use tensorflow and pytorch? Besides, why we need to manipulate the code in Darknet? I thought Darknet can help training the dataset automatically ?\n3. Finally, I come out to this \"Conclusion\". I am not sure is it correct or not so I am asking here. YOLO is an concept or method to detect image. In order to reach this concept, we need to code inside darknet/pytorch/tensorflow so that they can train the dataset by using the algorithm. Is this correct?\n\nI really hope someone can help me clearify this as simple as possible. It is also very hard to find a source that I can follow step by step to achieve custom data training in YOLO.....", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kc2qn0/qhquestion_regarding_yolo/"}, {"autor": "SPAMinaCanCan", "date": "2020-12-02 20:56:45", "content": "[D] What do you think is the best object classification model? /!/ Hey all\n\nI'm working for a company considering automated object detection for maintenance.\n\nBasically the object is to automatically detect faults in the surface of infrastructure.\n\nWe are considering using Facebook's detection2 implementation.\n\nhttps://github.com/facebookresearch/detectron2\n\nHowever I would like to consider other options that might be more effective.\n\nWhat do you think is the best fully segmented -----> image !!!  classifier?\n\nBonus points if you include tutorials for how to implement and train the algorithms using python\n\nThank you all for your help", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k5i803/d_what_do_you_think_is_the_best_object/"}, {"autor": "samsung_analyst", "date": "2020-12-02 10:28:22", "content": "MATLAB automatic -----> image !!!  labelling /!/ Hi all,\n\nI have 500 images of cgi pictures of cups in different shapes and positions. There's nothing else in the image other than the cup. I want to label these all as 'cup' before putting them into a CNN, is there a way to do this automatically? I've had a look into the Create New Algorithm section of the image labeller but I don't really understand what to do once I'm in there. Any help would be greatly appreciated.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k56vcy/matlab_automatic_image_labelling/"}, {"autor": "alowlkz", "date": "2020-12-01 23:26:14", "content": "How to make use of inputs of different types? (For example, images plus categorical data) /!/ Suppose my input information is -----> image !!! s and categorical information related to the -----> image !!! . For example, suppose I have images of cars and want to predict what kind of car it is. Suppose I also have data about the color of the car, what year it was made in, etc. I understand how I can do classification using just the categorical information or just the image. I'm confused about how to combine them together. I was thinking maybe train one model for the images (CNN), and a different model for the categorical information, then somehow combine the predictions? For example suppose the CNN on images tells me that my car is a Honda Civic and my categorical information tells me my car is a Toyota Camry. Then what? Not sure how to combine the results of different models together.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k4wwhb/how_to_make_use_of_inputs_of_different_types_for/"}, {"autor": "OnlyProggingForFun", "date": "2020-12-01 22:58:13", "content": "I explain MODNet, a new human matting technique, and review the best techniques used over the years to remove the background of an -----> image !!!  (paper linked in comments)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k4wdc7/i_explain_modnet_a_new_human_matting_technique/"}, {"autor": "realsharaf", "date": "2020-12-01 18:36:32", "content": "How to change figure size in matplotlib.pyplot.hexbin? /!/ I know that gridsize changes the the number of hexagons in the figure but I want to change the pixel size. The quality of output -----> image !!!  is too low to be used anywhere.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k4qyd2/how_to_change_figure_size_in/"}, {"autor": "vijish_madhavan", "date": "2020-12-01 13:23:22", "content": "[P] ArtLine - Generate Amazing Line Art Portraits. /!/ Hello all! Please have a look at **ArtLine's** public repo, Artline is project to generate line art from portrait photos. Hope you guys like it.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://preview.redd.it/7909wu18sk261.png?width=500&amp;format=png&amp;auto=webp&amp;s=6550d87f76a362b9073c05e252380900da85f0b9\n\n**Few Examples.**\n\n&amp;#x200B;\n\nhttps://preview.redd.it/3cjwbx12sk261.jpg?width=672&amp;format=pjpg&amp;auto=webp&amp;s=e0ced4022b53d91912b5bb712be16fbc64aecea2\n\n&amp;#x200B;\n\nhttps://preview.redd.it/09bbwl23sk261.jpg?width=748&amp;format=pjpg&amp;auto=webp&amp;s=69269c5035294833796d236c05aed746fadef1a1\n\n&amp;#x200B;\n\nhttps://preview.redd.it/t2ow4xy4sk261.jpg?width=717&amp;format=pjpg&amp;auto=webp&amp;s=e99b3c65c300df8ed7604163ee4c89c31cd519cc\n\n&amp;#x200B;\n\n**Gist of the project.**\n\n \n\n## Technical Details\n\n* **Self-Attention Generative Adversarial Network** ([https://arxiv.org/abs/1805.08318](https://arxiv.org/abs/1805.08318)). Generator is pretrained UNET with spectral normalization and self-attention. Something that I got from Jason Antic's DeOldify([https://github.com/jantic/DeOldify](https://github.com/jantic/DeOldify)), this made a huge difference, all of a sudden I started getting proper details around the facial features.\n* **Progressive Growing of GANs** ([https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)). Progressive GANS takes this idea of gradually increasing the -----> image !!!  size, In this project the -----> image !!!  size were gradually increased and learning rates were adjusted. Thanks to fast.ai for intrdoucing me to Progressive GANS, this helped in generating high quality output.\n* **Generator Loss** : Perceptual Loss/Feature Loss based on VGG16. ([https://arxiv.org/pdf/1603.08155.pdf](https://arxiv.org/pdf/1603.08155.pdf)).\n\n**Surprise!! No critic,No GAN. GAN did not make much of a difference so I was happy with No GAN.**\n\n[**https://github.com/vijishmadhavan/ArtLine**](https://github.com/vijishmadhavan/ArtLine)", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k4kq84/p_artline_generate_amazing_line_art_portraits/"}, {"autor": "botechga", "date": "2020-11-30 21:25:43", "content": "Does the validation set effect training for the YOLOv5?? /!/ So I have some data that I am training the YOLOv5 network on.  I want to get some metrics of how my trained model is performing.  Say I have 20 annotated images and I am using 18 for training and 2 for validation. \n\nI have a set of separate 20 images which are not annotated and I have been using these as a test set independent of the first 20 images.\n\nEach -----> image !!!  may have anywhere from 100-200 structures in it. I am getting tired of counting the predictions made on the test set to determine the performance of the network. \n\nTherefore I was wondering does the validation set have any effect on the training of the weights? Could I annotate the images in my test set, then use those as my validation set? Then applying all of the already annotated structures to my training set?\n\nDoes this even make sense in general I would appreciate any feedback ! Thank you !", "link": "https://www.reddit.com/r/learnmachinelearning/comments/k45nfp/does_the_validation_set_effect_training_for_the/"}, {"autor": "kropotkiner", "date": "2020-12-19 10:42:10", "content": "Toy datasets for computer vision? /!/ Are there any toy data sets I can use for -----> image !!!  classification, which takes less than 2-3 minutes to train. This is for finding bugs in my code. As I am a beginner in coding stuff, I deal with bugs like 80% of the time.\nI don't want to spend hours training on MNIST or Imagenet before I figure out I didn't set the \"model.zero_grads()\" at the correct line.\n\nThanks.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kg5o73/toy_datasets_for_computer_vision/"}, {"autor": "hwpcspr", "date": "2020-12-19 07:58:06", "content": "Is there any program/module a simple -----> image !!!  editor that can be run within cmd on windows? /!/  \n\n1 customized size of output\n\n2 able to change size and position of  imported images and see it in real time\n\n3 can save template", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kg3u1c/is_there_any_programmodule_a_simple_image_editor/"}, {"autor": "dbs98", "date": "2020-12-19 07:13:06", "content": "Ran into an interesting issue training an MNIST classifier and not sure why it happened? /!/ I was training a rather simple fully-connected model using PyTorch to solve the MNIST digit classification and was experiencing an interesting issue -- my model would never predict certain digits even though it had \\~90% accuracy on the rest. See these results on my test set: [https://i.imgur.com/sUOnJTC.png](https://i.imgur.com/sUOnJTC.png)\n\n&amp;#x200B;\n\nI discovered that the cause of this error was because I added a ReLU activation on the output layer of my network, and simply removing it solved the issue. But why did this happen? How can it be so accurate on certain digits and get 0% on other just because of the ReLU? Any hints would be appreciated!\n\n&amp;#x200B;\n\n \\`\\`\\`\n\n\\# PyTorch model\n\nclass Net(nn.Module):  \ndef \\_\\_init\\_\\_(self):  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0super(Net,\u00a0self).\\_\\_init\\_\\_()  \n\\#\u00a0linear\u00a0layer\u00a0(784\u00a0-&gt;\u00a01\u00a0hidden\u00a0node)  \nself.fc1\u00a0=\u00a0nn.Linear(28\u00a0\\*\u00a028,\u00a0512)  \nself.fc2\u00a0=\u00a0nn.Linear(512,\u00a0512)  \nself.fc3\u00a0=\u00a0nn.Linear(512,\u00a0128)  \nself.classifier\u00a0=\u00a0nn.Linear(128,\u00a010)  \n def forward(self,\u00a0x):  \n\\#\u00a0flatten\u00a0-----> image !!! \u00a0input  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0x\u00a0=\u00a0x.view(-1,\u00a028\u00a0\\*\u00a028)  \n\\#\u00a0add\u00a0hidden\u00a0layer,\u00a0with\u00a0relu\u00a0activation\u00a0function  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0x\u00a0=\u00a0F.relu(self.fc1(x))  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0x\u00a0=\u00a0F.relu(self.fc2(x))  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0x\u00a0=\u00a0F.relu(self.fc3(x))  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0x\u00a0=\u00a0F.relu(self.classifier(x))  # the problem was this ReLU  \nreturn\u00a0x  \n\\`\\`\\`", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kg3ceq/ran_into_an_interesting_issue_training_an_mnist/"}, {"autor": "IHDN2012", "date": "2020-12-19 05:25:04", "content": "Advice for creating a machine learning mobile app? /!/ Hello.  I'm looking to create a mobile app.  It would take a -----> photo !!!  from the user, send that -----> photo !!!  to AWS sagemaker, the -----> photo !!!  would be transformed and then sent back to the app, and the transformed image would be displayed to the user.\n\nI can do the AWS and the machine learning part, but I don't know how to build the app around it.  I'd rather not learn JS, is there a no-code approach to building the interface?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kg1zck/advice_for_creating_a_machine_learning_mobile_app/"}, {"autor": "-TheBoyWhoLived", "date": "2020-12-19 05:11:14", "content": "So I am trying to generate text using LSTM but when I try to run loop which is in -----> image !!! , it show very long time to complete like over 100 days and I am sure that\u2019s not correct approach. So what should I do? My training text is very long.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kg1shv/so_i_am_trying_to_generate_text_using_lstm_but/"}, {"autor": "[deleted]", "date": "2020-12-19 05:09:17", "content": "So I am trying to generate text using LSTM but when I try to run loop which is in -----> image !!! , it show very long time to complete like over 100 days and I am sure that\u2019s not correct approach. So what should I do? I am training text is very long. /!/ [deleted]", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kg1rjd/so_i_am_trying_to_generate_text_using_lstm_but/"}, {"autor": "post_hazanko", "date": "2020-12-18 18:30:33", "content": "Could I use ML for automatically finding random objects based on histograms(1D/2D) and contours? /!/ I'm working on a personal ground-based navigation vision project. It's basic, not a lot of compute eg. running on a RaspberryPi but it's not running in real time eg. frame by frame.\n\nCurrently I'm working on figuring out some kind of algorithm that based on a -----> photo !!! 's histogram(1d for light intensity for v), histogram(2d for h and s ranges) ; I can use these values to generate the HSV masks to apply to an image. Then find the largest contour areas.\n\nWhile I have worked it out roughly how it works(from the plot outputs) and then it can be turned into math eventually by ranges... I'm wondering if this process could train something, is it worth it?\n\nI would have to manually take a lot of pictures(how many) and apply the expected masks and I don't know how exact that would be, or at least I could do the desired output which is the bounding squares.\n\nThis is not your cliche \"find the red cup\" thing. That's the hard part is the dynamic objects(clothes/plastic bags/irregular shapes) and random color/lighting.\n\nBut manually based on those histograms I can isolate almost everything in an image eventually.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kfqpo7/could_i_use_ml_for_automatically_finding_random/"}, {"autor": "AerosolHubris", "date": "2020-12-18 14:37:16", "content": "What's going on under the hood with face filters? /!/ I asked [this](https://www.reddit.com/r/learnmachinelearning/comments/i6htcw/question_how_are_deep_learning_methods_used_to/) question a few months back, but I'm hoping I can rephrase it to get some more clarification.\n\nI'm comfortable with the ideas behind regression and classification, both supervised and unsupervised. I follow the processes behind things like face recognition and other -----> image !!!  classification problems. What I'm trying to work out is how face filters and other image manipulation processes work. I'm guessing it's a regression problem, but with hundreds of thousands of outputs (one for each pixel). Is this all it is? Say we have a filter that attempts to age a person's face. A neural network is trained with inputs set {pixels from pictures of young person} and output set {pixels from pictures of same person but older}. Then an image of a younger person is run through the network and the output is a predicted set of pixels attempting to age them?\n\nI don't need an ELI5, but I could use an ELI'm a relative beginner who has done some ML.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kfm8k6/whats_going_on_under_the_hood_with_face_filters/"}, {"autor": "KT9000", "date": "2020-12-18 13:05:40", "content": "first StyleGAN2 training on fashion -----> image !!!  dataset on Runway - noob q: How to improve quality?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kfks3y/first_stylegan2_training_on_fashion_image_dataset/"}, {"autor": "LGariv", "date": "2020-12-18 12:25:01", "content": "Keras Custom Multi-Class Object Detection CNN with Custom Dataset /!/ Hey guys!\n\nI\u2019m very new to ML, and I\u2019m working a college project to detect allow entry to places with automatic doors (I.E. supermarkets, hospitals) only if the person is wearing a mask using a Raspberry Pi 4. I was able to train a model based on a pre-trained MobileNetV2 with TensorFlow 1.13.1, but the code is mostly written on the tensorflow/models GitHub repo &amp; it\u2019s a very ugly and un-intuitive code to rewrite by myself &amp; it is based off another pre-trained model - all of which are things I\u2019m trying to avoid.\n\nI started to learn how to do something similar with TensorFlow 2 and Keras. I\u2019ve watched the I/O 2019 ML Zero to Hero &amp; the 4 part mini-series to try and get the basic idea of convolutional networks and machine learning in general; However, I can\u2019t find a single example of using Keras for creating a convolutional network for object detection with multi-class classification in a single model - I DID find a version where the code used a model to predict bounding boxes for faces first, extract them, and use -----> image !!!  classification on it similar to the Rock/Paper/Scissors model in ML Zero To Hero - but that\u2019s kind of a compromise instead of using a single model that detects masks directly, and can also slow things down considering it will be running on a Raspberry Pi, that also does some more calculations based on the results of the model (temperature check, and audio instructions to the person requesting access if they are wearing the mask incorrectly).\n\nHere\u2019s my current implementation (dataset is available with tfrecord, xml, and csv annotations):\n\n[Google Colab Notebook](https://github.com/lgariv/mask-dataset/blob/main/Untitled2.ipynb)\n\nIt currently relies on the MobileNetV2 model, and for some reason While training the loss and accuracy are staying relatively the same - they\u2019re not improving over time as they should. If there is a better way or a more efficient way of implementing what I have been trying to do, I\u2019d be happy to learn about it.\n\n\n\nTo summarize,\nWhat I need help with (in relation to the notebook linked above):\n\n\u2022 Need to figure out if the way I implemented it is the ideal / code efficient way to do such task (probably not?)\n\n\u2022 re-implement it as a completely custom network (as in the ML Zero to Hero, but object detection instead of image classification), without relying on MobileNetV2 or any other pre-trained model.\n\n\u2022 Figure out why the model does not improve it\u2019s accuracy when training.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kfk5xb/keras_custom_multiclass_object_detection_cnn_with/"}, {"autor": "noragami_yatogami", "date": "2020-12-18 10:04:13", "content": "-----> Image !!!  classifier based on quality of image /!/ So i wanted to build an image classifier which will classify images based on category of images , all of the images belong to same category (Dolphin's). I have previously performed task of classification where i would classify images of different animals(eg cat vs dog ). However for my current project i have to classify images based on quality of images.\n\nI am working on image classification of dolphins , so basically all images will have dolphins or part of dolphin present and i have to classify them into 4 categories\n\n-&gt;Grade 1 would be a photo that you can clearly see the dorsal fin at a perpendicular angle, with sufficient size and clarity and proper lighting (i.e. not shaded or glared)\n\n-&gt;Grade 2 would more or less be similar but not achieving something for grade 1 but with but picture quality is not as good as Grade-1\n\n-&gt;Grade Health would be images which pretty much don't belong to the either grade -1 or grade-2.\n\n-&gt;Finally the last category is images without picture of dolphin.\n\nIs there any way i can leverage the information that if the picture is not in grade-1 and grade-2 then automatically assign it to Grade Health.\n\nI am fairly new to the topic of classification using machine learning , so i have not tackled any problems like this earlier.", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kfig4k/image_classifier_based_on_quality_of_image/"}, {"autor": "diabolikyt", "date": "2020-12-17 20:08:04", "content": "Good book on -----> image !!!  recognition through convolutional network? /!/ I recently did a course on machine learning, and convolutional networks in particular sparked my interests. I know the basics of it now and I would like to learn more. Are there any recommended books on this topic?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kf543m/good_book_on_image_recognition_through/"}, {"autor": "aaaafireball", "date": "2020-12-17 17:13:23", "content": "Im trying to create a machine learning algorithm/ AI to associate an -----> image !!!  (even if the -----> image !!!  is resized) to a name and know the position of it on screen /!/ I need help with knowing where to start on this task. I want to create an AI that can do this example:  \n\n\nI gave the AI a picture of say a a picture of the chrome icon, then told it that it was called \"chrome\".  \n\n\nThen when I told it to track the object called \"chrome\" it would look on my screen for what it thinks chrome looks like (again even if the picture was resized to be bigger, so cv2.matchtemplate wont work.  \n\n\nOnce it finds it, I can have it track its position. I would ideally like to have it know how far off screen it was dragged too, but that's something just regular code can do if the AI knows what the object was and its last position.  \n\n\nany tutorials/ tips/ ideas on how to do this would be greatly appreciated. I don't need someone to spell it out for me but to send me on the right direction!", "link": "https://www.reddit.com/r/learnmachinelearning/comments/kf1j6r/im_trying_to_create_a_machine_learning_algorithm/"}, {"autor": "sandeep_25", "date": "2020-06-11 18:11:37", "content": "Hyperspectral -----> image !!!  classification using 1D CNN /!/ Dear friends,I have a small doubt.\nI am working on Hyperspectral images.\nIf I have an Hyperspectral image having dimensions 145x145x200.\nI converted them to 2D and new dimension is 21025x200 where 200 columns are my features.\nNow I want to classify using 1D CNN.\nWhat will be the suitable input shape and what will be the suitable dimension to prepare my data for 1D CNN.\nShould  I reshape my data with (21025,200,1) or (21025,1,200)?", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h14lyd/hyperspectral_image_classification_using_1d_cnn/"}, {"autor": "toothfairy222", "date": "2020-06-14 14:43:58", "content": "Beginner here , looking for an online tool to create a dataset of (-----> image !!! , label ) /!/  Hi ! I hope someone can help me out because at this point I dont even know what Im doing .\n\nI am working on a small Edge detection project ( I have a background in GIS ) and this is my first time working with deep learning . I am not looking to get better at it or anything I just need to get this done for school . My goal is to compare results ( canny+segmentation VS semantic segmentation ).I will be using Unet, and my understanding is that I need to train this network using an already segmented dataset , containing 512x512 tiles of (image, label) .\n\nSo far, I have a georeferenced satellite image (3bands) with its matching vector file (where I drew the edges ) , I did rasterize the vector file to get a mask.\n\nNow Im completely lost at to how to cut my image and mask to tiles and how to label them. I found some solutions online like geo-label maker that seems appropriate for me but I would like to avoid using code (because I have to learn programming from the beginning and I dont have a lot of time to invest in programming) .\n\nSo I guess my question is , is there any online tool that will help me achieve this ? or maybe a software I can download ?\n\nPlease feel free to correct anything I just said and to give any information you judge useful . Thank you for reading .", "link": "https://www.reddit.com/r/learnmachinelearning/comments/h8v1kh/beginner_here_looking_for_an_online_tool_to/"}], "name": "Subreddit_learnmachinelearning_01_01_2020-30_12_2020"}
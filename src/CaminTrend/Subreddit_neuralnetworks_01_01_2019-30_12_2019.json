{"interestingcomments": [{"autor": "JHogg11", "date": "2019-04-12 00:46:23", "content": "Neural networks that decide on the number of hidden layers/nodes? /!/ I'm trying to learn about machine learning, but from what I've learned about neural networks, the limitation seems to be that the force guiding the \"understanding\" of the input data is not a product of the data itself but a product of an external correcting force, whereas humans can (seemingly) understand concepts, recognize objects, etc. with or without external feedback.\n\nFor example, you might not know what this is, but you know it's something:\n\n&amp;#x200B;\n\n[A widget?](https://i.redd.it/g7auxszhcqr21.jpg)\n\nI don't know what it is either - it was a result when I googled \"Ikea hardware.\"  The point is that my brain is able to organize the sensory data of the -----> image !!!  to form an idea of a thing, even if I'm not able to draw conclusions about exactly what it's used for.\n\nIn terms of neural networks, maybe an approach to imitate this functionality of our minds is to have a network that is able to intelligently determine how many hidden layers/nodes it needs.  This could be done with a brute force approach where multiple networks are created and trained and the best network is chosen (or maybe the most efficient if the marginal value of additional nodes diminishes after a certain point), but I'm wondering if there's anything out there that provides a better solution than that.", "link": "https://www.reddit.com/r/neuralnetworks/comments/bc7h3z/neural_networks_that_decide_on_the_number_of/"}, {"autor": "bladerunner2054", "date": "2019-04-06 22:14:32", "content": "Question on Pre-processing for CNNs /!/ I was curious if anyone could enlighten me as to why in this example of a binary CNN they don't pre-process their -----> image !!! s consistently between training and running classifications on a test -----> image !!! .\n\nWhen loading the training and test data they divide all the RGB values by 255 (in order to normalize them to decimals between 0-1) they then train it with this data. However when they load a single 'test\\_image' at the end to classify it, they don't divide by 255, and the values in the tensor are fed to the classifier as RGB values 0-255.\n\nI'd have assumed they should be pre-processing the input data the same as what they trained it with, or am I missing something?\n\n[https://becominghuman.ai/building-an-image-classifier-using-deep-learning-in-python-totally-from-a-beginners-perspective-be8dbaf22dd8](https://becominghuman.ai/building-an-image-classifier-using-deep-learning-in-python-totally-from-a-beginners-perspective-be8dbaf22dd8)", "link": "https://www.reddit.com/r/neuralnetworks/comments/ba9mah/question_on_preprocessing_for_cnns/"}, {"autor": "shsshs1", "date": "2019-04-02 19:08:51", "content": "convNets and -----> image !!!  resizing /!/ Hi Reddit!\n\n&amp;#x200B;\n\nhow do you deal with convNets and changing activiations due to image resizing? \n\n(My current solution is to save an image at different (square) resolution and use all of them as training input. During inference i average above them as well.\n\n&amp;#x200B;\n\nIs this only an issue for me or can others confirm?", "link": "https://www.reddit.com/r/neuralnetworks/comments/b8nhb0/convnets_and_image_resizing/"}, {"autor": "diff_laptop", "date": "2019-03-18 10:46:54", "content": "How do I go about labelling my images for my CNN? /!/ Hello!\n\nI'm sorry if this question seems really stupid. I recently just read about Convolutional Neural Network and there are datasets namely: Training, Validation and Testing. My questions is how do you label the images? If example, I have 10,000 images for training, how do I tell the machine if the images is of a building or a tree? Should I crop each -----> image !!!  to show just a building instead of showing the whole area? How do I even label each part of an image?\n\n&amp;#x200B;\n\nFor a classification sample, let's say for a dog and cat, would I need to convert the image into an array of RGB and have the last item as their classification? 0 - for cat and 1 - for dog?\n\n&amp;#x200B;\n\nI apologize again for this. I'm really new at this and the training blogs usually just give a dataset of images. I'm not entirely sure what it contains.", "link": "https://www.reddit.com/r/neuralnetworks/comments/b2h6l8/how_do_i_go_about_labelling_my_images_for_my_cnn/"}, {"autor": "keghn", "date": "2019-02-04 15:18:12", "content": "Front Door -----> Camera !!!  Sends Automatic Alerts By Text", "link": "https://www.reddit.com/r/neuralnetworks/comments/an2m34/front_door_camera_sends_automatic_alerts_by_text/"}, {"autor": "Altwhite", "date": "2019-02-03 18:10:44", "content": "Suitable Neural Network Camera - Item identification, Depth, Speed /!/ This is industrial size, and i'm the mechanical engineer responsible for the system and hardware.\n\nIf there is other research papers or information I can learn more about on the types of vision systems often used then please let me know (I have done some research, but I'm not yet confident in my position).\n\nI'm designing a conveyor belt system which has 2 H-Bot robots, and 1 Delta - items of ranging sizes come on the conveyor and the robots should act accordingly from the neural network's item/depth identifier. I need a -----> camera !!!  that has depth sense and reliability to use with my Neural network. Should I have two cameras per robot?\n\n&amp;#x200B;\n\nI looked at Intel D435 RealSense and Orbbec camereas.\n\nIf there is other equally or better cameras that would be helpful to note.", "link": "https://www.reddit.com/r/neuralnetworks/comments/ams4nj/suitable_neural_network_camera_item/"}, {"autor": "shamoons", "date": "2019-01-30 11:50:03", "content": "How can I use Keras to train an -----> image !!!  transformation? /!/ I have some images of MRI scans (with and without the skull) and I want to train a network to learn to remove the skull from an image. Is this possible using Keras? ", "link": "https://www.reddit.com/r/neuralnetworks/comments/albxhr/how_can_i_use_keras_to_train_an_image/"}, {"autor": "keghn", "date": "2019-01-26 19:40:15", "content": "What Makes a Good -----> Image !!!  Generator AI?", "link": "https://www.reddit.com/r/neuralnetworks/comments/ak47w6/what_makes_a_good_image_generator_ai/"}, {"autor": "xyakik", "date": "2019-01-13 11:59:26", "content": "Comuter Vision /!/ Can somebody brief me with some cool ideas on Computer vision using -----> image !!!  processing.Asap", "link": "https://www.reddit.com/r/neuralnetworks/comments/afiqdh/comuter_vision/"}, {"autor": "gwen0927", "date": "2019-01-03 19:58:23", "content": "InstaGAN Excels in Instance-Aware -----> Image !!! -To------> Image !!!  Translation", "link": "https://www.reddit.com/r/neuralnetworks/comments/ac9hot/instagan_excels_in_instanceaware_imagetoimage/"}, {"autor": "TomBarz", "date": "2019-03-13 22:30:00", "content": "A question about neural networks /!/ Hi guys, i\u2019m studying convolutionnal neural network for a project for my master\u2019s degree, and there\u2019s something i don\u2019t understand. I study through several sources, and depending on which it is, one say that  a filter ( for exemple size 5x5x3 on an -----> image !!!  32x32x3) convolves around the -----> image !!!  to give in output an activation map, and that filter is a neuron. Other sources say that as the filter is convolving, every possible location on the image passing by the filter is a neuron, and that location is the \u00ab\u00a0receptive field\u00a0\u00bb. I don\u2019t really understand what is a neuron then ? Btw, what\u2019s called a \u00ab\u00a0channel\u00a0\u00bb ?", "link": "https://www.reddit.com/r/neuralnetworks/comments/b0scms/a_question_about_neural_networks/"}, {"autor": "chrike", "date": "2019-02-19 10:19:38", "content": "Free credits for our computer vision platform /!/ Hi, I\u2019m a co-founder at the Belgian Computer vision startup, [Overture](https://www.overture.ai/). we have built a platform that offers an end-to-end fully-customizable pipeline for computer vision model training and deployment.\n\nWe are very eager to share our work and to find out what developers think of the platform. Of course, we would also love to hear which features you are still missing on it.Hence, we are offering 50 euros of free credits, so that you can use the platform for your own use cases and tell us what you like and dislike about it.\n\nATM, you can train and deploy -----> image !!!  classifiers, object detectors and face recognition systems. Many other algorithms like segmentation, OCR and object tracking are in development.\n\nClaim your free credits: [https://app.overture.ai/pages/register](https://app.overture.ai/pages/register) and use promo code: **reddit50** to redeem your 50 euros of free credits.\n\n**PLATFORM DEMO:**[https://www.youtube.com/watch?v=hQLrGjQAWWU](https://www.youtube.com/watch?v=hQLrGjQAWWU)\n\nIf you have any questions, feel free to contact me at [christiaan@overture.ai](mailto:christiaan@overture.ai). We are looking forward to seeing you on the platform.\n\nKind regards,\n\nChristiaan\n\nCo-Founder and AI engineer at [Overture](https://www.overture.ai/)", "link": "https://www.reddit.com/r/neuralnetworks/comments/as8x9z/free_credits_for_our_computer_vision_platform/"}, {"autor": "suyash93", "date": "2019-05-25 16:12:25", "content": "Stylizer: -----> Image !!!  to -----> Image !!!  transformation experiments", "link": "https://www.reddit.com/r/neuralnetworks/comments/bsw832/stylizer_image_to_image_transformation_experiments/"}, {"autor": "keghn", "date": "2019-05-19 20:01:04", "content": "-----> Camera !!!  Sees Electromagnetic Interference Using an SDR and Machine Vision", "link": "https://www.reddit.com/r/neuralnetworks/comments/bqlbkj/camera_sees_electromagnetic_interference_using_an/"}, {"autor": "_human404_", "date": "2019-05-05 12:41:25", "content": "Loss function help /!/  I have used resnet50 to solve a multi-class classification problem. The model outputs probabilities for each class. Which loss function should I choose for my model. I can't choose binary cross-entropy as it is used for binary classification. I can't choose categorical cross-entropy as it is not exactly telling which class an -----> image !!!  belongs to (1 or 0 for each) and is instead giving probabilities for each class.", "link": "https://www.reddit.com/r/neuralnetworks/comments/bkx9zl/loss_function_help/"}, {"autor": "Yuqing7", "date": "2019-05-01 15:25:10", "content": "Google Researchers Add Attention to Augment Convolutional Neural Networks /!/ A group of Google researchers led by Quoc Le\u200a\u2014\u200athe AI expert behind Google Neural Machine Translation and AutoML\u200a\u2014\u200ahave published a paper proposing attention augmentation. In experiment results, the novel two-dimensional relative self-attention mechanismfor -----> image !!!  classification delivers \u201cconsistent improvements in -----> image !!!  classification.\u201d\n\nFor more information [https://medium.com/syncedreview/google-researchers-add-attention-to-augment-convolutional-neural-networks-1490e9c245e1](https://medium.com/syncedreview/google-researchers-add-attention-to-augment-convolutional-neural-networks-1490e9c245e1)", "link": "https://www.reddit.com/r/neuralnetworks/comments/bjidw6/google_researchers_add_attention_to_augment/"}, {"autor": "joeblessyou", "date": "2019-06-25 17:05:09", "content": "What is the best approach for a NN that takes a drawing of a diagram, consisting of arrows, shapes, and text, and outputs the vector data... a set of linear equations to draw the shapes, and text glyphs, such that it can be rendered? Searching for this is very hard. /!/ Input would be a desaturated -----> image !!!  of hand drawn diagrams with the usual shapes, arrows and text, and output ideally would be vector graphics information in some format. \n\nSearch results give linear equations of the neural networks themselves, or diagrams of the architectures. The only things that come close have the output as the render itself and not the vector information. Any resources or ideas are appreciated.", "link": "https://www.reddit.com/r/neuralnetworks/comments/c5bn5w/what_is_the_best_approach_for_a_nn_that_takes_a/"}, {"autor": "deepdigitalfrog", "date": "2019-08-04 16:03:24", "content": "Neural Networks Architecture Helping Science /!/ Researchers have developed a deep neural network architecture that can identify manipulated -----> image !!! s at the pixel level with high precision by studying the boundaries of objects in the -----> image !!! ", "link": "https://www.reddit.com/r/neuralnetworks/comments/clxxtb/neural_networks_architecture_helping_science/"}, {"autor": "not_alreadytakenuser", "date": "2019-08-01 19:24:12", "content": "I'm seeking answers i couldn't find on internet /!/ I'm trying to make a CNN from scratch and there are few things i don't really understand.\nFirst, Convolution: when i look at different architectures of CNNs i see that the -----> image !!!  goes through 2 \"rows\" of filters. But does the output of each filter of the first \"row\" goes through every single filter from the second \"row\"? There would be too much inputs even with max pooling.\nHow do you choose filters? Are they random?is there any rule?\nSecondly, batch_size and other parameters:\nI heard learning rate should be 1e-3,drop out=0.2 however i don't know how to choose batch size, indeed, many websites recommend 128. But i feel that the bigger it is, the less the NN is effective because the gradient descent will minimize the average cost but not the cost for every single training example.(i'm actually doing an handwritten digits recognition and with a batch size too big, the output converges toward 0.1 for every output). How should i choos the batch size? Thank you in advance.", "link": "https://www.reddit.com/r/neuralnetworks/comments/ckszz3/im_seeking_answers_i_couldnt_find_on_internet/"}, {"autor": "chinmay_shah", "date": "2019-07-19 14:26:50", "content": "Pre-processing for CNN /!/ If we want to automate -----> image !!! -preprocessing for CNN starting with directly with dataset in .zip files, what should be the desired functions that would be grouped together?\n\nQuestion 2: Also, sometimes we use TF pre-processing for data-augmentation but data-generation is random; thus is it desired that data that was created randomly be shared in-case of when we want reproducible research?", "link": "https://www.reddit.com/r/neuralnetworks/comments/cf87ip/preprocessing_for_cnn/"}, {"autor": "MasterSama", "date": "2019-07-17 18:03:09", "content": "What is this? is this the so called \"Model Collapse\" phenomenon in a Gan or something completely different? /!/ This is a follow up question which I asked [here](https://www.reddit.com/r/neuralnetworks/comments/ce1k3o/why_are_the_images_generated_by_my_gan_get_darker/). I tried to see if my GAN memorized something or not. \n\nI'm 100% sure that in my earlier experiments I noticed couple of images that were identical to the ones in the dataset, though they had a bit of distortions but no mistaking it that they were indeed the images from dataset. \n\nI changed the architecture to the one that I shown in that previous question, and then tried to create a morphing effect on the first -----> image !!!  and see how it changes to another -----> image !!! . \n\nI cant figure out what is happening here so here is the image sequence it self plus the function I wrote that creates those morphing effect.  \n\nhere is the function : \n\n&amp;#x200B;\n\n    # morphing test\n    def morph_sample(G, z_vec_numpy, count=10, row =2, col=10 , \n                     device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n        \n        fig = plt.figure(figsize=(col,row))\n        plt.subplots_adjust(wspace=0,hspace=0)\n        for i in range(count):\n            \n            z = z_vec_numpy + 0.5 * (i*0.2)\n            z = torch.from_numpy(z).float().to(device)\n            imgs = G(z)\n            imgs = imgs.detach().to('cpu').numpy().squeeze().transpose(1,2,0)\n            # unnormalize the image using :  ((imgs - min)*255 // (max-min))\n            imgs = ((imgs +1)*255 /2 ).astype(np.uint8)\n            ax = fig.add_subplot(row, col, i+1,xticks=[], yticks=[])\n            ax.imshow(imgs)\n            \n            #plt.show()\n    z = np.random.uniform(-1,1, size=(1,100))\n    morph_sample(G, z, count=50, row=5, col=10)\n\nand here are several examples generated using this function : \n\n&amp;#x200B;\n\nhttps://i.redd.it/0zlf3vb9jwa31.png\n\n&amp;#x200B;\n\nhttps://i.redd.it/3npx4rnbjwa31.png\n\n&amp;#x200B;\n\nhttps://i.redd.it/zq8u2ryejwa31.png\n\nhttps://i.redd.it/91z4jfrjjwa31.png\n\nAs you you have just seen, they start from different images, then gradually fade into the same garbage image! if I continue this for example for 200, the image gets weirder and as a random blob!\n\nhere is an example with count of 100 : \n\n&amp;#x200B;\n\nhttps://i.redd.it/dle71mm4kwa31.png\n\nif I play with the latent vector values (e.g. add/subtract it from 0.1 instead of 0.5 and stuff like this) I get different results, but ultimately they all fade into  some image and wont morph for ever!  : \n\n&amp;#x200B;\n\nhttps://i.redd.it/i7zrnev1lwa31.png\n\nhttps://i.redd.it/44w7iqw6lwa31.png\n\nhttps://i.redd.it/tzzjuz5alwa31.png", "link": "https://www.reddit.com/r/neuralnetworks/comments/cegevk/what_is_this_is_this_the_so_called_model_collapse/"}, {"autor": "MasterSama", "date": "2019-07-16 19:12:37", "content": "Why are the images generated by my GAN get darker as the network trains more? /!/ I created a simple DCGAN (convolutional GAN) with 5 layers and trained it on CelebA dataset (the small one). \n\nI noticed my networks images are dimmed looking and as the network trains further, the bright color fades into dim ones! \n\nhere are some example, the number shows  the epoch number : \n\n&amp;#x200B;\n\nhttps://i.redd.it/v3wzs3vnppa31.png\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nhttps://i.redd.it/qp1vbpquppa31.png\n\n&amp;#x200B;\n\nhttps://i.redd.it/2tzbg6xxppa31.png\n\n&amp;#x200B;\n\nhttps://i.redd.it/z3jrk1p3qpa31.png\n\nWhat is the cause for this phenomenon? \n\nI didnt use batchnorm for the first layer of Discriminator, I also didnt use Batchnorm for the last layer of the Generator, \n\nI used LeakyRelu(0.2) in Discriminator, and ReLU for Generator. I have no idea why the images are this dim/dark! \n\nAny help is greatly appreciated. \n\n&amp;#x200B;\n\n    \n    def conv_batch(in_dim, out_dim, kernel_size, stride, padding, batch_norm=True):\n        layers = nn.ModuleList()\n    \n        conv = nn.Conv2d(in_dim, out_dim, kernel_size, stride, padding, bias=False)\n        layers.append(conv)\n     if batch_norm:\n            layers.append(nn.BatchNorm2d(out_dim))\n     return nn.Sequential(*layers)\n    \n    class Discriminator(nn.Module):\n    \tdef __init__(self, conv_dim=32, act = nn.ReLU(), mode=0):\n    \t\tsuper().__init__()\n     \n    \t\tself.mode = mode\n    \t\tself.conv_dim = conv_dim \n    \t\tself.act = act\n    \t\tself.conv1 = conv_batch(3, conv_dim, 4, 2, 1, False)\n    \t\tself.conv2 = conv_batch(conv_dim, conv_dim*2, 4, 2, 1)\n    \t\tself.conv3 = conv_batch(conv_dim*2, conv_dim*4, 4, 2, 1)\n    \t\tself.conv4 = conv_batch(conv_dim*4, conv_dim*8, 4, 1, 1)\n    \t\tself.conv5 = conv_batch(conv_dim*8, conv_dim*10, 4, 2, 1)\n    \t\tself.conv6 = conv_batch(conv_dim*10, conv_dim*10, 3, 1, 1)\n    \n    \t\tself.fc1 = nn.Linear(64*64*3, 64*10)\n    \t\tself.fc2 = nn.Linear(64*10, 64*10)\n    \t\tself.fc3 = nn.Linear(64*10,1)\n    \t\t# self.conv5 = conv_batch(conv_dim*4, conv_dim*5, 3, 2, 1)\n    \t\t# self.conv6 = conv_batch(conv_dim*5, conv_dim*6, 3, 2, 1)\n    \t\tself.drp = nn.Dropout(0.5)\n    \t\tself.fc = nn.Linear(conv_dim*10*3*3, 1) # it seems, larger fmaps provide better results?!\n    \n    \tdef forward(self, input):\n    \t\tif self.mode == 0:\n    \t\t\tbatch = input.size(0)\n    \t\t\toutput = self.act(self.conv1(input))\n    \t\t\toutput = self.act(self.conv2(output))\n    \t\t\toutput = self.act(self.conv3(output))#16\n    \t\t\toutput = self.act(self.conv4(output))#16\n    \t\t\toutput = self.act(self.conv5(output))#16\n    \t\t\toutput = self.act(self.conv6(output))#16\n    \n    \t\t\toutput = output.view(batch, self.fc.in_features)\n    \t\t\toutput = self.fc(output)\n    \t\t\toutput = self.drp(output)\n    \t\telse:\n    \t\t\toutput = input.view(-1, 64*64*3)\n    \t\t\toutput = self.drp(self.fc1(output))\n    \t\t\toutput = self.drp(self.fc2(output))\n    \t\t\toutput = self.drp(self.fc3(output))\n    \t\treturn output\n    \n    def deconv_convtranspose(in_dim, out_dim, kernel_size, stride, padding, batchnorm=True):\n        layers = []\n        deconv = nn.ConvTranspose2d(in_dim, out_dim, kernel_size = kernel_size, stride=stride, padding=padding)\n        layers.append(deconv)\n    \tif batchnorm:\n            layers.append(nn.BatchNorm2d(out_dim))\n    \treturn nn.Sequential(*layers)\n    \n    class Generator(nn.Module):\n    \tdef __init__(self, z_size=100, conv_dim=32, mode=0): \n    \t\tsuper().__init__()\n    \t\t self.conv_dim = conv_dim\n    \t\t # make the 1d input into a 3d output of shape (conv_dim*4, 4, 4 )\n    \t\t self.fc = nn.Linear(z_size, conv_dim*4*4*4)#4x4\n    \t\t # conv and deconv layer work on 3d volumes, so we now only need to pass the number of fmaps and the\n    \t\t # input volume size (its h,w which is 4x4!)\n    \t\t self.mode = mode\n    \t\t self.drp = nn.Dropout(0.5)\n    \t\tif mode == 0: \n    \t\t\tself.deconv1 = deconv_convtranspose(conv_dim*4, conv_dim*3, kernel_size=3, stride=2, padding=1)\n    \t\t\tself.deconv2 = deconv_convtranspose(conv_dim*3, conv_dim*2, kernel_size=3, stride=2, padding=1)\n    \t\t\tself.deconv3 = deconv_convtranspose(conv_dim*2, conv_dim, kernel_size=3, stride=2, padding=1)\n    \t\t\tself.deconv4 = deconv_convtranspose(conv_dim, conv_dim, kernel_size=4, stride=1, padding=0)\n    \t\t\tself.deconv5 = deconv_convtranspose(conv_dim, 3, kernel_size=5, stride=1, padding=0,batchnorm=False)\n    \t\telif mode ==1:\n    \t\t\tself.deconv1 = deconv_convtranspose(conv_dim*4, conv_dim*3, kernel_size =4, stride=2, padding=1)\n    \t\t\tself.deconv2 = deconv_convtranspose(conv_dim*3, conv_dim*2, kernel_size =4, stride=2, padding=1)\n    \t\t\tself.deconv3 = deconv_convtranspose(conv_dim*2, conv_dim, kernel_size =4, stride=2, padding=1)\n    \t\t\tself.deconv4 = deconv_convtranspose(conv_dim, conv_dim, kernel_size =3, stride=2, padding=1)\n    \t\t\tself.deconv5 = deconv_convtranspose(conv_dim, 3, kernel_size =4, stride=1, padding=1, batchnorm=False)\n    \t\telif mode==2:\n    \t\t\tself.deconv1 = deconv_convtranspose(conv_dim*4, conv_dim*2, kernel_size =4, stride=2, padding=1)\n    \t\t\tself.deconv2 = deconv_convtranspose(conv_dim*2, conv_dim, kernel_size =4, stride=2, padding=1)\n    \t\t\t# self.deconv3 = deconv_convtranspose(conv_dim*2, conv_dim, kernel_size =4, stride=1, padding=1)\n    \t\t\tself.deconv4 = deconv_convtranspose(conv_dim, conv_dim, kernel_size =4, stride=2, padding=1)\n    \t\t\tself.deconv5 = deconv_convtranspose(conv_dim, 3, kernel_size =4, stride=2, padding=1, batchnorm=False)\n    \n    \tdef forward(self, input):\n    \t\toutput = self.fc(input)\n    \t\toutput = self.drp(output)\n    \t\toutput = output.view(-1, self.conv_dim*4, 4, 4)\n    \t\tif self.mode != 2: \n    \t\t\toutput = F.relu(self.deconv1(output))\n    \t\t\toutput = F.relu(self.deconv2(output))\n    \t\t\toutput = F.relu(self.deconv3(output))\n    \t\t\toutput = F.relu(self.deconv4(output))\n    \t\t\t# we create the -----> image !!!  using tanh!\n    \t\t\toutput = F.tanh(self.deconv5(output))\n    \t\telse:\n    \t\t\toutput = F.relu(self.deconv1(output))\n    \t\t\toutput = F.relu(self.deconv2(output))\n    \t\t\toutput = F.relu(self.deconv4(output))\n    \t\t\t# we create the image using tanh!\n    \t\t\toutput = F.tanh(self.deconv5(output))\n    \t \n    \t\treturn output\n    \n    \n    dd = Discriminator(mode=0)\n    zd = np.random.rand(2,3,64,64)\n    zd = torch.from_numpy(zd).float()\n    # print(dd)\n    print(dd(zd).shape)\n    \n    gg = Generator(mode=1)\n    z = np.random.uniform(-1,1,size=(2,100))\n    z = torch.from_numpy(z).float()\n    print(gg(z).shape)", "link": "https://www.reddit.com/r/neuralnetworks/comments/ce1k3o/why_are_the_images_generated_by_my_gan_get_darker/"}, {"autor": "omdano", "date": "2019-07-10 21:14:44", "content": "GANs /!/ So.. I had this thought for a long time...\n\nWhy use GANs to generate rather than a Fully Connected Architecture that maps an (random number + category) to an (-----> image !!! ) ? rather than doing that with a GAN?  Is it due to the overfitting that will occur on the category variable?", "link": "https://www.reddit.com/r/neuralnetworks/comments/cbmkfk/gans/"}, {"autor": "dumneaiow", "date": "2019-07-10 11:39:28", "content": "CNN question /!/ Hello, I am currently working on a neural network that is supposed to return a yes/no answer for an -----> image !!!  after it's been trained. After looking on the internet I have came across ResNet50 and a way to change the output size of the network. My question is: it it okay to use ResNet50 as a base for something the returns a binary value? Is there something better that I could use?", "link": "https://www.reddit.com/r/neuralnetworks/comments/cbfn18/cnn_question/"}, {"autor": "dadiaar", "date": "2019-07-04 01:01:31", "content": "Looking for a D-noise alternative (clean pictures' noise) /!/ [D-noise](https://remingtongraphics.net/tools/d-noise/) is great, but I would like to get alternatives to it (paid or free), online or installed.\n\nI'm not looking for something that just diffuses the -----> picture !!!  to remove the noise.\n\nThanks in advance.", "link": "https://www.reddit.com/r/neuralnetworks/comments/c8w7c9/looking_for_a_dnoise_alternative_clean_pictures/"}, {"autor": "Password_0301", "date": "2019-09-08 20:57:15", "content": "Help Please /!/ Hi Does anyone know how I can open an -----> image !!!  and get pixel values in python, because I'm new to -----> image !!!  recognition and all this so help would be great.", "link": "https://www.reddit.com/r/neuralnetworks/comments/d1gtwp/help_please/"}, {"autor": "sk0lopandre", "date": "2019-08-28 14:51:02", "content": "Help on personnal project. Neural network not working as intended /!/ Hi!\n\nAt first , sorry for my english. I'm not a native and I just try my best.\n\n&amp;#x200B;\n\nI'm currently working on a personnal project of neural network using Excel. This is only because my data set to train the IA needs Excel.\n\nAfter finishing to setup all the formula's and trying to run the program, i came across a problem I can't solve. Even when my input change, my output won't change for a specific set of weights and bias. i put below the -----> picture !!!  of 2 different input producing the exact same output. If I use a different set of weights and bias, this will produce another output...but still locked for this specific set of weights.\n\nThere are 19 neurons in the input layer, for 2 possible output. Between them are 2 hidden layer with 10 neurons each. Weights and bias are generated to be anything between -1 and 1.\n\n&amp;#x200B;\n\nDo you have any idea of what could have gotten wrong?\n\n&amp;#x200B;\n\n![img](gxtxyb0vb7j31)\n\n![img](s0hq01atb7j31)", "link": "https://www.reddit.com/r/neuralnetworks/comments/cwluo3/help_on_personnal_project_neural_network_not/"}, {"autor": "JaapOosterbroek", "date": "2019-08-26 21:46:20", "content": "Mask_RCNN convoluted /!/ I just started working with mask RCNN. I use this implementation:  [https://github.com/matterport/Mask\\_RCNN](https://github.com/matterport/Mask_RCNN) \n\nFirst of all Thank you matterport! I could not build anythink like this, it is a fantastic work of art. The whole works like a charm!\n\nThat said the code is a frikking mess.\n\n\\- Serialization and configuration is incompatible with current keras libraries. Reading and writing a model is tricky and inconsistent.\n\n\\- File IO: Holy shit thats,over a 2000 lines of that do hardly anything but read an -----> image !!!  and maybe flip its axis.\n\n\\- Training code and data augmentation is very simplistic and convoluted. I have converted it to standart keras data augmentations, that boosts results by.... like a lot, especially on smaller data sets.\n\n\\- Configuration is a mess. Half the config fields are no longer used, a quater of them are redundant. It does not split cofiguration between construction and inference.  Layers get complicated non-json config objects that they keep as members, that pevent serialisation.\n\n\\- Visualisation: (no complaits here), clean cool code way better that what I cooked up for other projects.\n\n\\- Datasets: There is a bunch of work there to work with different datasets but non of it is really adaptable. \n\nIf I get enough replies/upvotes I would love to invest some time to strip it of useless parts.clean it up and post a nice repro on the gitstabuckets. But im a bit scared that the field will has moved on, have reinvented the wheel. \n\nWhat say you? is Mask RCNN still relevant 2 months from now?\n\nP.S. sorry about the rant", "link": "https://www.reddit.com/r/neuralnetworks/comments/cvuo71/mask_rcnn_convoluted/"}, {"autor": "Rogerjak", "date": "2019-10-09 11:13:33", "content": "Master Thesis Questionnaire /!/ Hey guys, I'm developing a terrain generator that allows authoring using a neural network.  \nI've  got some basic results and I am comparing them to a run of the mill  algorithm just to gauge people's perception regarding the realism of the  structures created.\n\nThe text is  in Portuguese but I'll translate and since the tests is visual, there's  no need to read all the questions ( they are all the same).\n\n*\"Compara\u00e7\u00e3o de realismo entre resultados de m\u00e9todos generativos\"* : Realism comparison between results of generative methods.  \n*\"Este  question\u00e1rio tem como objetivo perceber qual a perce\u00e7\u00e3o dos  utilizadores relativamente ao realismo dos mapas gerados entre estes   dois m\u00e9todos. N\u00e3o existem respostas correctas.\"* : This  questionnaire tries to understand the perception of the user regarding  the realism of the maps generated by these two algorithms. There are no  correct answers.  \n*\"Entre a -----> image !!! m A e B, qual delas representa um terreno mais realista?\"*  : Between -----> image !!!  A and B, which one represents a more realistic terrain?\n\nThere's only two options and the test is visual so it doesn't take long: [https://forms.gle/QDbNbfGUZBvmXd4X6](https://forms.gle/QDbNbfGUZBvmXd4X6)\n\nThanks in advance guys!", "link": "https://www.reddit.com/r/neuralnetworks/comments/dffizj/master_thesis_questionnaire/"}, {"autor": "stejac8636", "date": "2019-09-30 15:20:57", "content": "Could a neural network recreate a certain persons face with limited input data? /!/ I need a high resolution -----> picture !!!  of my grandfather for a portrait painting I\u2019m making. Unfortunately,  he died a few years ago and all of the pictures of him are low resolution and/or not ideal for a portrait.\n\nCould I use a neural network to upscale an existing picture of him or generate an entirely new one based on ideal lighting circumstances?", "link": "https://www.reddit.com/r/neuralnetworks/comments/dbd45b/could_a_neural_network_recreate_a_certain_persons/"}, {"autor": "gabrieldomene", "date": "2019-09-30 01:53:31", "content": "Help in CNN for aerial images /!/ Hey guys, last year i took a class in computer vision field and started to like it, nowdays i'm trying to get some personal project with drones + computer vision and I got by recommendation the CNN approach, I have some questions about the \"how to\" of preparing the dataset stage. Searching for datasets I got the  Cars Overhead With Context ([https://gdo152.llnl.gov/cowc/](https://gdo152.llnl.gov/cowc/)) and spend a day in looking through the READMEs provided.  \n\n\nMy question is how do i prepare these for my CNN? In the folders i have everything separated and labeled by category, for example:   \n\n\nUtah\\_AGRC/train/neg.Utah\\_AGRC-HRO\\_15.0cm\\_12TVL220180-CROP.05573.03863.030.png\n\n&amp;#x200B;\n\n(1) \\`neg\\` means this is a negative sample. It would say \\`car\\` otherwise.\n\n(2) 12TVL220180-CROP is the original -----> image !!!  name (see file\\_name\\_translate.txt)\n\n(3) 05573.03863.030 means this -----> image !!!  was taken centered from the pixel offset 05573,03863 in the -----> image !!! . The 030 means this patch was rotated 30 degrees from its original.  \n\n\nOk, so far so good, I understand how the label was made but do i need to modificate anything more? The train images are all 256x256, they are ready to be passed to my network? What are crucial for this first step? In the dataset they provide the network used (made in caffe framework) but since i'm trying to learn tensorflow i'll try to replicate in there.  I don't have much experience in networks (practical) but i have some knowledge of the theorical CNN's, MLP and some AI.  \n\n\nHere is some example of one train image: [https://imgur.com/qkLnwqY](https://imgur.com/qkLnwqY)  \n\n\nThanks in advance, and if anyone is trigged by some possible \"school work\" relax, i don't wan't any code, i just wan't some advices on how to procced after getting the dataset etc.", "link": "https://www.reddit.com/r/neuralnetworks/comments/db4p5j/help_in_cnn_for_aerial_images/"}, {"autor": "muliku", "date": "2019-09-25 12:05:23", "content": "How to do transfer learning on Darknet YOLOv3 /!/ I've started getting into object detection in -----> image !!! . I have YOLOv3 neural network with Darknet framework. The network is pre-trained from COCO data set. Now I need to do some transfer learning in order to try to make the results better. What I have so far:\n\nI've created my custom data set and did transfer learning following this guide: https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning\n\nAfter few hours it spit out some .pt file. From what I gathered .pt is PyTorch format but my program uses .weights which is Darknet format. First I've tried to find how to convert .pt to .weights and I stumbled upon this: https://github.com/marvis/pytorch-caffe-darknet-convert but apparently this works only with YOLOv2.\n\nThen I tried to look for something on how to transfer learn using Darknet but with no luck.\n\nSince then I'm stuck and can't move forward, can any of you give me some tips and pointers on how to achieve transfer learning on YOLOLv3 + Darknet (or how to convert .pt to .weights since I already know how to create .pt)? Thanks!", "link": "https://www.reddit.com/r/neuralnetworks/comments/d92bjv/how_to_do_transfer_learning_on_darknet_yolov3/"}, {"autor": "Oatilis", "date": "2019-09-12 13:10:59", "content": "Why would you need a class such as N/A, Background, etc? /!/ I noticed in some datasets and trained networks that images can be classified as \"background\", \"N/A\" and so on. I'm still learning about NN, but my current understanding is that you want your output neurons to actually represent classes of objects. \n\nWhy would you even need it? For instance, in FashionMNIST there isn't an N/A class, but if you see all results are below some threshold, you can determine that no object was found in the -----> image !!! .", "link": "https://www.reddit.com/r/neuralnetworks/comments/d37ov7/why_would_you_need_a_class_such_as_na_background/"}, {"autor": "Yingrjimsch", "date": "2019-11-19 04:29:07", "content": "Initial Help Story Creation /!/ Hi everyone\n\nI'm experimenting in the last few month with all kinds of neural networks. I was mostly playing around with -----> image !!!  recognition and tensorflow.\n\nNow I'd love to make something else. I would like to make a story telling neural network, which can create different stories out of its training data and in the future maybe change on user input. Unforunately I really don't know how thisntechnically should look like. \nI'm searching help?! Hopefully someone could help me understand the network I try to create.", "link": "https://www.reddit.com/r/neuralnetworks/comments/dyfg9k/initial_help_story_creation/"}, {"autor": "Capn_Crusty", "date": "2019-11-11 21:36:15", "content": "(Totally computer generated) Here's what you get when a neural network posts a comment. /!/ The Neural Networks Project: An introduction to Deep Learning for Computer Vision: Deep Neural Networks (CNNs) have been instrumental in making computers smarter for years. Now we are exploring the potential for them to help you look better too! Here are some more resources to get you started:\n \nA Quick Primer on Deep Learning:\n \nDeep Learning for Computer Vision\n \nThe Neural Networks Project is an open research project at the University of California, San Diego. We have designed and built deep neural networks to help us make computer vision systems smarter and more powerful.\n \nDeep Neural Networks (CNNs) have been instrumental in making computers smarter for years. The -----> image !!!  above is the one we get from a neural network that \"tweets\" a bunch of random tweets, and the -----> image !!!  on the right is the same tweet that the neural network posted. In this case, we know that the neural network is not just doing a simple word-matching task: It's actually trying to come up with a new image based on the data that was provided to it. The neural network's goal is to predict what image it will create, and it's just doing that. But how is this accomplished?\n \nIn the second case, we can see how this works. We want the neural network to predict the location of a photo (using the word \"london\"), which it is already able to do. We also don't know what's in that photo, so we need to provide some additional information to the neural network.\n  \nGoogle has just published an AI-driven image recognition system called WaveNet. It's a pretty big deal \u2014 the technology is capable of \"understanding human language\" according to a release.\n \nTo be clear, the WaveNet system doesn't understand words like \"fish\" or \"man\" or \"dog.\" It's more like a machine that can understand sentences, and when it's given an image like the one above, it can tell it's a cat \u2014 because it recognizes that it's a face. (See a video below.)\n \nA WaveNet neural network will be able to post a photo like the one above, where it's a cat with a human. (Image: Google Brain)\n \nThe neural network in the photo above was trained to recognize a cat, and when asked to recognize \"cat,\" it's able to do so. It doesn't know it's a cat...", "link": "https://www.reddit.com/r/neuralnetworks/comments/duzh8m/totally_computer_generated_heres_what_you_get/"}, {"autor": "kotesh_nitrkl", "date": "2019-11-06 08:43:10", "content": "How to read text/watermark from -----> image !!! . /!/ Is there any python package to read text/watermark from set of images", "link": "https://www.reddit.com/r/neuralnetworks/comments/dsdkuz/how_to_read_textwatermark_from_image/"}, {"autor": "kotesh_nitrkl", "date": "2019-11-03 15:13:39", "content": "Duplicate or similar -----> image !!!  detection using vgg16 /!/ I have a folder having images. Now how can i find similar or duplicate images from the folder or database", "link": "https://www.reddit.com/r/neuralnetworks/comments/dr1olu/duplicate_or_similar_image_detection_using_vgg16/"}, {"autor": "CultureImaginary", "date": "2019-10-24 10:51:01", "content": "Where can I find images of drones? /!/ I'm building a system which involves detecting drones using a neural network. I'm unable to find large sources of -----> image !!!  files of drones in an environment.\n\nCan someone help out?", "link": "https://www.reddit.com/r/neuralnetworks/comments/dmenyf/where_can_i_find_images_of_drones/"}, {"autor": "kennnnnnnnnnnnnnn", "date": "2019-12-09 20:26:25", "content": "Having problems performing a Dot product on 2 vectors /!/ I get the first error when trying to Dot the self.wih vector (100 hidden nodes, 1024 input nodes) with my input vector (3x32x32). I figured this was because the number of columns weren't matching, So I tried reshaping, by doing 'train\\_data\\[ 'X' \\]\\[ : , : , : , i \\].reshape(3,1024)', which results in the error in the second -----> image !!!  in red. I think the overflow error is because it is generating a number out of bounds, but I'm not sure how to go around this.\n\nPlease can someone give me some hints on how to fix this?\n\n&amp;#x200B;\n\nhttps://preview.redd.it/8mxy742h2o341.png?width=992&amp;format=png&amp;auto=webp&amp;s=5acabe4445c9ac98749e290461ce72c4e02e4116", "link": "https://www.reddit.com/r/neuralnetworks/comments/e8fhbm/having_problems_performing_a_dot_product_on_2/"}, {"autor": "laseluuu", "date": "2019-12-02 11:40:42", "content": "is there an -----> image !!!  likeness/matching/sorting programs that are for windows? /!/ I dont know if this is the right forum, but if not does anyone know where is?\n\n&amp;#x200B;\n\nI am a competent end user of software but zero coding skills, i'm  code-phobic for some reason, but massively productive and creative with software. \n\n&amp;#x200B;\n\nI really would love a simple program that uses an image 'A' and sorts through a folder and finds images 'B' with similar likeness, based on large shapes and/or colour. \n\n&amp;#x200B;\n\n&amp;#x200B;\n\nI have heard that this is basically what neural networks are good at, and wondered if there is a program that someone on windows could use?  (even if fairly slow and using single x64 cpu)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nI have thousands of artworks and would like to make an animation with them, by using an animation (image A) and finding matches from a folder that could be used in that frames place", "link": "https://www.reddit.com/r/neuralnetworks/comments/e4xwrs/is_there_an_image_likenessmatchingsorting/"}, {"autor": "hamhub7", "date": "2019-12-01 03:43:19", "content": "Neural Network to draw an -----> image !!! ? /!/ Hey guys, I have seen some Twitter posts of neural networks that can take a bunch of movie scripts and generate a script of their own. Would it be possible to do something similar with a picture? I think it would be funny to see the results if we fed it a bunch of meme formats or something and tried to make its own, or a bunch of video game characters and it made its own protagonist and villain. Any ideas where I might want to start with something like this? It would be my first NN project.", "link": "https://www.reddit.com/r/neuralnetworks/comments/e49pa2/neural_network_to_draw_an_image/"}], "name": "Subreddit_neuralnetworks_01_01_2019-30_12_2019"}
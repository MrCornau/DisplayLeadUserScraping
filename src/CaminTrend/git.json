{
  "name": "github",
  "interestingcomments": [
    {
      "autor": "face_recognition",
      "date": "NaN",
      "content": "Face Recognition\nYou can also read a translated version of this file in Chinese \u7b80\u4f53\u4e2d\u6587\u7248 or in Korean \ud55c\uad6d\uc5b4 or in Japanese \u65e5\u672c\u8a9e.\nRecognize and manipulate faces from Python or from the command line with the world's simplest face recognition library.\nBuilt using dlib's state-of-the-art face recognition built with deep learning. The model has an accuracy of 99.38% on the Labeled Faces in the Wild benchmark.\nThis also provides a simple face_recognition command line tool that lets you do face recognition on a folder of images from the command line!\nFeatures\nFind faces in pictures\nFind all the faces that appear in a picture:\nimport face_recognition\nimage = face_recognition.load_image_file(\"your_file.jpg\")\nface_locations = face_recognition.face_locations(image)\nFind and manipulate facial features in pictures\nGet the locations and outlines of each person's eyes, nose, mouth and chin.\nimport face_recognition\nimage = face_recognition.load_image_file(\"your_file.jpg\")\nface_landmarks_list = face_recognition.face_landmarks(image)\nFinding facial features is super useful for lots of important stuff. But you can also use it for really stupid stuff like applying digital make-up (think 'Meitu'):\nIdentify faces in pictures\nRecognize who appears in each -----> photo !!! .\nimport face_recognition\nknown_image = face_recognition.load_image_file(\"biden.jpg\")\nunknown_image = face_recognition.load_image_file(\"unknown.jpg\")\nbiden_encoding = face_recognition.face_encodings(known_image)[0]\nunknown_encoding = face_recognition.face_encodings(unknown_image)[0]\nresults = face_recognition.compare_faces([biden_encoding], unknown_encoding)\nYou can even use this library with other Python libraries to do real-time face recognition:\nSee this example for the code.\nOnline Demos\nUser-contributed shared Jupyter notebook demo (not officially supported):\nInstallation\nRequirements\nPython 3.3+ or Python 2.7\nmacOS or Linux (Windows not officially supported, but might work)\nInstallation Options:\nInstalling on Mac or Linux\nFirst, make sure you have dlib already installed with Python bindings:\nHow to install dlib from source on macOS or Ubuntu\nThen, make sure you have cmake installed:\nbrew install cmake\nFinally, install this module from pypi using pip3 (or pip2 for Python 2):\npip3 install face_recognition\nAlternatively, you can try this library with Docker, see this section.\nIf you are having trouble with installation, you can also try out a pre-configured VM.\nInstalling on an Nvidia Jetson Nano board\nJetson Nano installation instructions\nPlease follow the instructions in the article carefully. There is current a bug in the CUDA libraries on the Jetson Nano that will cause this library to fail silently if you don't follow the instructions in the article to comment out a line in dlib and recompile it.\nInstalling on Raspberry Pi 2+\nRaspberry Pi 2+ installation instructions\nInstalling on FreeBSD\npkg install graphics/py-face_recognition\nInstalling on Windows\nWhile Windows isn't officially supported, helpful users have posted instructions on how to install this library:\n@masoudr's Windows 10 installation guide (dlib + face_recognition)\nInstalling a pre-configured Virtual Machine image\nDownload the pre-configured VM image (for VMware Player or VirtualBox).\nUsage\nCommand-Line Interface\nWhen you install face_recognition, you get two simple command-line programs:\nface_recognition - Recognize faces in a photograph or folder full for photographs.\nface_detection - Find faces in a photograph or folder full for photographs.\nface_recognition command line tool\nThe face_recognition command lets you recognize faces in a photograph or folder full for photographs.\nFirst, you need to provide a folder with one picture of each person you already know. There should be one image file for each person with the files named according to who is in the picture:\nNext, you need a second folder with the files you want to identify:\nThen in you simply run the command face_recognition, passing in the folder of known people and the folder (or single image) with unknown people and it tells you who is in each image:\n$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/\n/unknown_pictures/unknown.jpg,Barack Obama\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person\nThere's one line in the output for each face. The data is comma-separated with the filename and the name of the person found.\nAn unknown_person is a face in the image that didn't match anyone in your folder of known people.\nface_detection command line tool\nThe face_detection command lets you find the location (pixel coordinatates) of any faces in an image.\nJust run the command face_detection, passing in a folder of images to check (or a single image):\n$ face_detection ./folder_with_pictures/\nexamples/image1.jpg,65,215,169,112\nexamples/image2.jpg,62,394,211,244\nexamples/image2.jpg,95,941,244,792\nIt prints one line for each face that was detected. The coordinates reported are the top, right, bottom and left coordinates of the face (in pixels).\nAdjusting Tolerance / Sensitivity\nIf you are getting multiple matches for the same person, it might be that the people in your photos look very similar and a lower tolerance value is needed to make face comparisons more strict.\nYou can do that with the --tolerance parameter. The default tolerance value is 0.6 and lower numbers make face comparisons more strict:\n$ face_recognition --tolerance 0.54 ./pictures_of_people_i_know/ ./unknown_pictures/\n/unknown_pictures/unknown.jpg,Barack Obama\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person\nIf you want to see the face distance calculated for each match in order to adjust the tolerance setting, you can use --show-distance true:\n$ face_recognition --show-distance true ./pictures_of_people_i_know/ ./unknown_pictures/\n/unknown_pictures/unknown.jpg,Barack Obama,0.378542298956785\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person,None\nMore Examples\nIf you simply want to know the names of the people in each photograph but don't care about file names, you could do this:\n$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/ | cut -d ',' -f2\nBarack Obama\nunknown_person\nSpeeding up Face Recognition\nFace recognition can be done in parallel if you have a computer with multiple CPU cores. For example, if your system has 4 CPU cores, you can process about 4 times as many images in the same amount of time by using all your CPU cores in parallel.\nIf you are using Python 3.4 or newer, pass in a --cpus <number_of_cpu_cores_to_use> parameter:\n$ face_recognition --cpus 4 ./pictures_of_people_i_know/ ./unknown_pictures/\nYou can also pass in --cpus -1 to use all CPU cores in your system.\nPython Module\nYou can import the face_recognition module and then easily manipulate faces with just a couple of lines of code. It's super easy!\nAPI Docs: https://face-recognition.readthedocs.io.\nAutomatically find all the faces in an image\nimport face_recognition\nimage = face_recognition.load_image_file(\"my_picture.jpg\")\nface_locations = face_recognition.face_locations(image)\n# face_locations is now an array listing the co-ordinates of each face!\nSee this example to try it out.\nYou can also opt-in to a somewhat more accurate deep-learning-based face detection model.\nNote: GPU acceleration (via NVidia's CUDA library) is required for good performance with this model. You'll also want to enable CUDA support when compliling dlib.\nimport face_recognition\nimage = face_recognition.load_image_file(\"my_picture.jpg\")\nface_locations = face_recognition.face_locations(image, model=\"cnn\")\n# face_locations is now an array listing the co-ordinates of each face!\nSee this example to try it out.\nIf you have a lot of images and a GPU, you can also find faces in batches.\nAutomatically locate the facial features of a person in an image\nimport face_recognition\nimage = face_recognition.load_image_file(\"my_picture.jpg\")\nface_landmarks_list = face_recognition.face_landmarks(image)\n# face_landmarks_list is now an array with the locations of each facial feature in each face.\n# face_landmarks_list[0]['left_eye'] would be the location and outline of the first person's left eye.\nSee this example to try it out.\nRecognize faces in images and identify who they are\nimport face_recognition\npicture_of_me = face_recognition.load_image_file(\"me.jpg\")\nmy_face_encoding = face_recognition.face_encodings(picture_of_me)[0]\n# my_face_encoding now contains a universal 'encoding' of my facial features that can be compared to any other picture of a face!\nunknown_picture = face_recognition.load_image_file(\"unknown.jpg\")\nunknown_face_encoding = face_recognition.face_encodings(unknown_picture)[0]\n# Now we can see the two face encodings are of the same person with `compare_faces`!\nresults = face_recognition.compare_faces([my_face_encoding], unknown_face_encoding)\nif results[0] == True:\nprint(\"It's a picture of me!\")\nelse:\nprint(\"It's not a picture of me!\")\nSee this example to try it out.\nPython Code Examples\nAll the examples are available here.\nFace Detection\nFind faces in a photograph\nFind faces in a photograph (using deep learning)\nFind faces in batches of images w/ GPU (using deep learning)\nBlur all the faces in a live video using your webcam (Requires OpenCV to be installed)\nFacial Features\nIdentify specific facial features in a photograph\nApply (horribly ugly) digital make-up\nFacial Recognition\nFind and recognize unknown faces in a photograph based on photographs of known people\nIdentify and draw boxes around each person in a photo\nCompare faces by numeric face distance instead of only True/False matches\nRecognize faces in live video using your webcam - Simple / Slower Version (Requires OpenCV to be installed)\nRecognize faces in live video using your webcam - Faster Version (Requires OpenCV to be installed)\nRecognize faces in a video file and write out new video file (Requires OpenCV to be installed)\nRecognize faces on a Raspberry Pi w/ camera\nRun a web service to recognize faces via HTTP (Requires Flask to be installed)\nRecognize faces with a K-nearest neighbors classifier\nTrain multiple images per person then recognize faces using a SVM\nCreating a Standalone Executable\nIf you want to create a standalone executable that can run without the need to install python or face_recognition, you can use PyInstaller. However, it requires some custom configuration to work with this library. See this issue for how to do it.\nArticles and Guides that cover face_recognition\nMy article on how Face Recognition works: Modern Face Recognition with Deep Learning\nCovers the algorithms and how they generally work\nFace recognition with OpenCV, Python, and deep learning by Adrian Rosebrock\nCovers how to use face recognition in practice\nRaspberry Pi Face Recognition by Adrian Rosebrock\nCovers how to use this on a Raspberry Pi\nFace clustering with Python by Adrian Rosebrock\nCovers how to automatically cluster photos based on who appears in each photo using unsupervised learning\nHow Face Recognition Works\nIf you want to learn how face location and recognition work instead of depending on a black box library, read my article.\nCaveats\nThe face recognition model is trained on adults and does not work very well on children. It tends to mix up children quite easy using the default comparison threshold of 0.6.\nAccuracy may vary between ethnic groups. Please see this wiki page for more details.\nDeployment to Cloud Hosts (Heroku, AWS, etc)\nSince face_recognition depends on dlib which is written in C++, it can be tricky to deploy an app using it to a cloud hosting provider like Heroku or AWS.\nTo make things easier, there's an example Dockerfile in this repo that shows how to run an app built with face_recognition in a Docker container. With that, you should be able to deploy to any service that supports Docker images.\nYou can try the Docker image locally by running: docker-compose up --build\nThere are also several prebuilt Docker images.\nLinux users with a GPU (drivers >= 384.81) and Nvidia-Docker installed can run the example on the GPU: Open the docker-compose.yml file and uncomment the dockerfile: Dockerfile.gpu and runtime: nvidia lines.\nHaving problems?\nIf you run into problems, please read the Common Errors section of the wiki before filing a github issue.\nThanks\nMany, many thanks to Davis King (@nulhom) for creating dlib and for providing the trained facial feature detection and face encoding models used in this library. For more information on the ResNet that powers the face encodings, check out his blog post.\nThanks to everyone who works on all the awesome Python data science libraries like numpy, scipy, scikit-image, pillow, etc, etc that makes this kind of stuff so easy and fun in Python.\nThanks to Cookiecutter and the audreyr/cookiecutter-pypackage project template for making Python project packaging way more tolerable.",
      "link": "https://github.com/ageitgey/face_recognition"
    },
    {
      "autor": "openpose",
      "date": "NaN",
      "content": "Build Type Linux MacOS Windows\nBuild Status\nOpenPose has represented the first real-time multi-person system to jointly detect human body, hand, facial, and foot keypoints (in total 135 keypoints) on single images.\nIt is authored by Gin\u00e9s Hidalgo, Zhe Cao, Tomas Simon, Shih-En Wei, Yaadhav Raaj, Hanbyul Joo, and Yaser Sheikh. It is maintained by Gin\u00e9s Hidalgo and Yaadhav Raaj. OpenPose would not be possible without the CMU Panoptic Studio dataset. We would also like to thank all the people who has helped OpenPose in any way.\nAuthors Gin\u00e9s Hidalgo (left) and Hanbyul Joo (right) in front of the CMU Panoptic Studio\nContents\nResults\nFeatures\nRelated Work\nInstallation\nQuick Start Overview\nSend Us Feedback!\nCitation\nLicense\nResults\nWhole-body (Body, Foot, Face, and Hands) 2D Pose Estimation\nTesting OpenPose: (Left) Crazy Uptown Funk flashmob in Sydney video sequence. (Center and right) Authors Gin\u00e9s Hidalgo and Tomas Simon testing face and hands\nWhole-body 3D Pose Reconstruction and Estimation\nTianyi Zhao testing the OpenPose 3D Module\nUnity Plugin\nTianyi Zhao and Gin\u00e9s Hidalgo testing the OpenPose Unity Plugin\nRuntime Analysis\nWe show an inference time comparison between the 3 available pose estimation libraries (same hardware and conditions): OpenPose, Alpha-Pose (fast Pytorch version), and Mask R-CNN. The OpenPose runtime is constant, while the runtime of Alpha-Pose and Mask R-CNN grow linearly with the number of people. More details here.\nFeatures\nMain Functionality:\n2D real-time multi-person keypoint detection:\n15, 18 or 25-keypoint body/foot keypoint estimation, including 6 foot keypoints. Runtime invariant to number of detected people.\n2x21-keypoint hand keypoint estimation. Runtime depends on number of detected people. See OpenPose Training for a runtime invariant alternative.\n70-keypoint face keypoint estimation. Runtime depends on number of detected people. See OpenPose Training for a runtime invariant alternative.\n3D real-time single-person keypoint detection:\n3D triangulation from multiple single views.\nSynchronization of Flir cameras handled.\nCompatible with Flir/Point Grey cameras.\nCalibration toolbox: Estimation of distortion, intrinsic, and extrinsic -----> camera !!!  parameters.\nSingle-person tracking for further speedup or visual smoothing.\nInput: Image, video, webcam, Flir/Point Grey, IP camera, and support to add your own custom input source (e.g., depth camera).\nOutput: Basic image + keypoint display/saving (PNG, JPG, AVI, ...), keypoint saving (JSON, XML, YML, ...), keypoints as array class, and support to add your own custom output code (e.g., some fancy UI).\nOS: Ubuntu (20, 18, 16, 14), Windows (10, 8), Mac OSX, Nvidia TX2.\nHardware compatibility: CUDA (Nvidia GPU), OpenCL (AMD GPU), and non-GPU (CPU-only) versions.\nUsage Alternatives:\nCommand-line demo for built-in functionality.\nC++ API and Python API for custom functionality. E.g., adding your custom inputs, pre-processing, post-posprocessing, and output steps.\nFor further details, check the major released features and release notes docs.\nRelated Work\nOpenPose training code\nOpenPose foot dataset\nOpenPose Unity Plugin\nOpenPose papers published in IEEE TPAMI and CVPR. Cite them in your publications if OpenPose helps your research! (Links and more details in the Citation section below).\nInstallation\nIf you want to use OpenPose without installing or writing any code, simply download and use the latest Windows portable version of OpenPose!\nOtherwise, you could build OpenPose from source. See the installation doc for all the alternatives.\nQuick Start Overview\nSimply use the OpenPose Demo from your favorite command-line tool (e.g., Windows PowerShell or Ubuntu Terminal). E.g., this example runs OpenPose on your webcam and displays the body keypoints:\n# Ubuntu\n./build/examples/openpose/openpose.bin\n:: Windows - Portable Demo\nbin\\OpenPoseDemo.exe --video examples\\media\\video.avi\nYou can also add any of the available flags in any order. E.g., the following example runs on a video (--video {PATH}), enables face (--face) and hands (--hand), and saves the output keypoints on JSON files on disk (--write_json {PATH}).\n# Ubuntu\n./build/examples/openpose/openpose.bin --video examples/media/video.avi --face --hand --write_json output_json_folder/\n:: Windows - Portable Demo\nbin\\OpenPoseDemo.exe --video examples\\media\\video.avi --face --hand --write_json output_json_folder/\nOptionally, you can also extend OpenPose's functionality from its Python and C++ APIs. After installing OpenPose, check its official doc for a quick overview of all the alternatives and tutorials.\nSend Us Feedback!\nOur library is open source for research purposes, and we want to improve it! So let us know (create a new GitHub issue or pull request, email us, etc.) if you...\nFind/fix any bug (in functionality or speed) or know how to speed up or improve any part of OpenPose.\nWant to add/show some cool functionality/demo/project made on top of OpenPose. We can add your project link to our Community-based Projects section or even integrate it with OpenPose!\nCitation\nPlease cite these papers in your publications if OpenPose helps your research. All of OpenPose is based on OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields, while the hand and face detectors also use Hand Keypoint Detection in Single Images using Multiview Bootstrapping (the face detector was trained using the same procedure than the hand detector).\n@article{8765346,\nauthor = {Z. {Cao} and G. {Hidalgo Martinez} and T. {Simon} and S. {Wei} and Y. A. {Sheikh}},\njournal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\ntitle = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},\nyear = {2019}\n}\n@inproceedings{simon2017hand,\nauthor = {Tomas Simon and Hanbyul Joo and Iain Matthews and Yaser Sheikh},\nbooktitle = {CVPR},\ntitle = {Hand Keypoint Detection in Single Images using Multiview Bootstrapping},\nyear = {2017}\n}\n@inproceedings{cao2017realtime,\nauthor = {Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh},\nbooktitle = {CVPR},\ntitle = {Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},\nyear = {2017}\n}\n@inproceedings{wei2016cpm,\nauthor = {Shih-En Wei and Varun Ramakrishna and Takeo Kanade and Yaser Sheikh},\nbooktitle = {CVPR},\ntitle = {Convolutional pose machines},\nyear = {2016}\n}\nPaper links:\nOpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields:\nIEEE TPAMI\nArXiv\nHand Keypoint Detection in Single Images using Multiview Bootstrapping\nRealtime Multi-Person 2D Pose Estimation using Part Affinity Fields\nConvolutional Pose Machines\nLicense\nOpenPose is freely available for free non-commercial use, and may be redistributed under these conditions. Please, see the license for further details. Interested in a commercial license? Check this FlintBox link. For commercial queries, use the Contact section from the FlintBox link and also send a copy of that message to Yaser Sheikh.",
      "link": "https://github.com/CMU-Perceptual-Computing-Lab/openpose"
    },
    {
      "autor": "awesome-deep-learning",
      "date": "NaN",
      "content": "Awesome Deep Learning\nTable of Contents\nBooks\nCourses\nVideos and Lectures\nPapers\nTutorials\nResearchers\nWebsites\nDatasets\nConferences\nFrameworks\nTools\nMiscellaneous\nContributing\nBooks\nDeep Learning by Yoshua Bengio, Ian Goodfellow and Aaron Courville (05/07/2015)\nNeural Networks and Deep Learning by Michael Nielsen (Dec 2014)\nDeep Learning by Microsoft Research (2013)\nDeep Learning Tutorial by LISA lab, University of Montreal (Jan 6 2015)\nneuraltalk by Andrej Karpathy : numpy-based RNN/LSTM implementation\nAn introduction to genetic algorithms\nArtificial Intelligence: A Modern Approach\nDeep Learning in Neural Networks: An Overview\nArtificial intelligence and machine learning: Topic wise explanation 10.Grokking Deep Learning for Computer Vision\nDive into Deep Learning - numpy based interactive Deep Learning book\nPractical Deep Learning for Cloud, Mobile, and Edge - A book for optimization techniques during production.\nMath and Architectures of Deep Learning - by Krishnendu Chaudhury\nTensorFlow 2.0 in Action - by Thushan Ganegedara\nCourses\nMachine Learning - Stanford by Andrew Ng in Coursera (2010-2014)\nMachine Learning - Caltech by Yaser Abu-Mostafa (2012-2014)\nMachine Learning - Carnegie Mellon by Tom Mitchell (Spring 2011)\nNeural Networks for Machine Learning by Geoffrey Hinton in Coursera (2012)\nNeural networks class by Hugo Larochelle from Universit\u00e9 de Sherbrooke (2013)\nDeep Learning Course by CILVR lab @ NYU (2014)\nA.I - Berkeley by Dan Klein and Pieter Abbeel (2013)\nA.I - MIT by Patrick Henry Winston (2010)\nVision and learning - computers and brains by Shimon Ullman, Tomaso Poggio, Ethan Meyers @ MIT (2013)\nConvolutional Neural Networks for Visual Recognition - Stanford by Fei-Fei Li, Andrej Karpathy (2017)\nDeep Learning for Natural Language Processing - Stanford\nNeural Networks - usherbrooke\nMachine Learning - Oxford (2014-2015)\nDeep Learning - Nvidia (2015)\nGraduate Summer School: Deep Learning, Feature Learning by Geoffrey Hinton, Yoshua Bengio, Yann LeCun, Andrew Ng, Nando de Freitas and several others @ IPAM, UCLA (2012)\nDeep Learning - Udacity/Google by Vincent Vanhoucke and Arpan Chakraborty (2016)\nDeep Learning - UWaterloo by Prof. Ali Ghodsi at University of Waterloo (2015)\nStatistical Machine Learning - CMU by Prof. Larry Wasserman\nDeep Learning Course by Yann LeCun (2016)\nDesigning, Visualizing and Understanding Deep Neural Networks-UC Berkeley\nUVA Deep Learning Course MSc in Artificial Intelligence for the University of Amsterdam.\nMIT 6.S094: Deep Learning for Self-Driving Cars\nMIT 6.S191: Introduction to Deep Learning\nBerkeley CS 294: Deep Reinforcement Learning\nKeras in Motion video course\nPractical Deep Learning For Coders by Jeremy Howard - Fast.ai\nIntroduction to Deep Learning by Prof. Bhiksha Raj (2017)\nAI for Everyone by Andrew Ng (2019)\nMIT Intro to Deep Learning 7 day bootcamp - A seven day bootcamp designed in MIT to introduce deep learning methods and applications (2019)\nDeep Blueberry: Deep Learning - A free five-weekend plan to self-learners to learn the basics of deep-learning architectures like CNNs, LSTMs, RNNs, VAEs, GANs, DQN, A3C and more (2019)\nSpinning Up in Deep Reinforcement Learning - A free deep reinforcement learning course by OpenAI (2019)\nDeep Learning Specialization - Coursera - Breaking into AI with the best course from Andrew NG.\nDeep Learning - UC Berkeley | STAT-157 by Alex Smola and Mu Li (2019)\nMachine Learning for Mere Mortals video course by Nick Chase\nMachine Learning Crash Course with TensorFlow APIs -Google AI\nDeep Learning from the Foundations Jeremy Howard - Fast.ai\nDeep Reinforcement Learning (nanodegree) - Udacity a 3-6 month Udacity nanodegree, spanning multiple courses (2018)\nGrokking Deep Learning in Motion by Beau Carnes (2018)\nFace Detection with Computer Vision and Deep Learning by Hakan Cebeci\nDeep Learning Online Course list at Classpert List of Deep Learning online courses (some are free) from Classpert Online Course Search\nAWS Machine Learning Machine Learning and Deep Learning Courses from Amazon's Machine Learning unviersity\nIntro to Deep Learning with PyTorch - A great introductory course on Deep Learning by Udacity and Facebook AI\nDeep Learning by Kaggle - Kaggle's free course on Deep Learning\nVideos and Lectures\nHow To Create A Mind By Ray Kurzweil\nDeep Learning, Self-Taught Learning and Unsupervised Feature Learning By Andrew Ng\nRecent Developments in Deep Learning By Geoff Hinton\nThe Unreasonable Effectiveness of Deep Learning by Yann LeCun\nDeep Learning of Representations by Yoshua bengio\nPrinciples of Hierarchical Temporal Memory by Jeff Hawkins\nMachine Learning Discussion Group - Deep Learning w/ Stanford AI Lab by Adam Coates\nMaking Sense of the World with Deep Learning By Adam Coates\nDemystifying Unsupervised Feature Learning By Adam Coates\nVisual Perception with Deep Learning By Yann LeCun\nThe Next Generation of Neural Networks By Geoffrey Hinton at GoogleTechTalks\nThe wonderful and terrifying implications of computers that can learn By Jeremy Howard at TEDxBrussels\nUnsupervised Deep Learning - Stanford by Andrew Ng in Stanford (2011)\nNatural Language Processing By Chris Manning in Stanford\nA beginners Guide to Deep Neural Networks By Natalie Hammel and Lorraine Yurshansky\nDeep Learning: Intelligence from Big Data by Steve Jurvetson (and panel) at VLAB in Stanford.\nIntroduction to Artificial Neural Networks and Deep Learning by Leo Isikdogan at Motorola Mobility HQ\nNIPS 2016 lecture and workshop videos - NIPS 2016\nDeep Learning Crash Course: a series of mini-lectures by Leo Isikdogan on YouTube (2018)\nDeep Learning Crash Course By Oliver Zeigermann\nDeep Learning with R in Motion: a live video course that teaches how to apply deep learning to text and images using the powerful Keras library and its R language interface.\nMedical Imaging with Deep Learning Tutorial: This tutorial is styled as a graduate lecture about medical imaging with deep learning. This will cover the background of popular medical image domains (chest X-ray and histology) as well as methods to tackle multi-modality/view, segmentation, and counting tasks.\nDeepmind x UCL Deeplearning: 2020 version\nDeepmind x UCL Reinforcement Learning: Deep Reinforcement Learning\nCMU 11-785 Intro to Deep learning Spring 2020 Course: 11-785, Intro to Deep Learning by Bhiksha Raj\nMachine Learning CS 229 : End part focuses on deep learning By Andrew Ng\nPapers\nYou can also find the most cited deep learning papers from here\nImageNet Classification with Deep Convolutional Neural Networks\nUsing Very Deep Autoencoders for Content Based Image Retrieval\nLearning Deep Architectures for AI\nCMU\u2019s list of papers\nNeural Networks for Named Entity Recognition zip\nTraining tricks by YB\nGeoff Hinton's reading list (all papers)\nSupervised Sequence Labelling with Recurrent Neural Networks\nStatistical Language Models based on Neural Networks\nTraining Recurrent Neural Networks\nRecursive Deep Learning for Natural Language Processing and Computer Vision\nBi-directional RNN\nLSTM\nGRU - Gated Recurrent Unit\nGFRNN . .\nLSTM: A Search Space Odyssey\nA Critical Review of Recurrent Neural Networks for Sequence Learning\nVisualizing and Understanding Recurrent Networks\nWojciech Zaremba, Ilya Sutskever, An Empirical Exploration of Recurrent Network Architectures\nRecurrent Neural Network based Language Model\nExtensions of Recurrent Neural Network Language Model\nRecurrent Neural Network based Language Modeling in Meeting Recognition\nDeep Neural Networks for Acoustic Modeling in Speech Recognition\nSpeech Recognition with Deep Recurrent Neural Networks\nReinforcement Learning Neural Turing Machines\nLearning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\nGoogle - Sequence to Sequence Learning with Neural Networks\nMemory Networks\nPolicy Learning with Continuous Memory States for Partially Observed Robotic Control\nMicrosoft - Jointly Modeling Embedding and Translation to Bridge Video and Language\nNeural Turing Machines\nAsk Me Anything: Dynamic Memory Networks for Natural Language Processing\nMastering the Game of Go with Deep Neural Networks and Tree Search\nBatch Normalization\nResidual Learning\nImage-to-Image Translation with Conditional Adversarial Networks\nBerkeley AI Research (BAIR) Laboratory\nMobileNets by Google\nCross Audio-Visual Recognition in the Wild Using Deep Learning\nDynamic Routing Between Capsules\nMatrix Capsules With Em Routing\nEfficient BackProp\nGenerative Adversarial Nets\nFast R-CNN\nFaceNet: A Unified Embedding for Face Recognition and Clustering\nSiamese Neural Networks for One-shot Image Recognition\nUnsupervised Translation of Programming Languages\nMatching Networks for One Shot Learning\nTutorials\nUFLDL Tutorial 1\nUFLDL Tutorial 2\nDeep Learning for NLP (without Magic)\nA Deep Learning Tutorial: From Perceptrons to Deep Networks\nDeep Learning from the Bottom up\nTheano Tutorial\nNeural Networks for Matlab\nUsing convolutional neural nets to detect facial keypoints tutorial\nTorch7 Tutorials\nThe Best Machine Learning Tutorials On The Web\nVGG Convolutional Neural Networks Practical\nTensorFlow tutorials\nMore TensorFlow tutorials\nTensorFlow Python Notebooks\nKeras and Lasagne Deep Learning Tutorials\nClassification on raw time series in TensorFlow with a LSTM RNN\nUsing convolutional neural nets to detect facial keypoints tutorial\nTensorFlow-World\nDeep Learning with Python\nGrokking Deep Learning\nDeep Learning for Search\nKeras Tutorial: Content Based Image Retrieval Using a Convolutional Denoising Autoencoder\nPytorch Tutorial by Yunjey Choi\nUnderstanding deep Convolutional Neural Networks with a practical use-case in Tensorflow and Keras\nOverview and benchmark of traditional and deep learning models in text classification\nHardware for AI: Understanding computer hardware & build your own computer\nProgramming Community Curated Resources\nThe Illustrated Self-Supervised Learning\nVisual Paper Summary: ALBERT (A Lite BERT)\nSemi-Supervised Deep Learning with GANs for Melanoma Detection\nNamed Entity Recognition using Reformers\nDeep N-Gram Models on Shakespeare\u2019s works\nWide Residual Networks\nFashion MNIST using Flax\nFake News Classification (with streamlit deployment)\nRegression Analysis for Primary Biliary Cirrhosis\nCross Matching Methods for Astronomical Catalogs\nNamed Entity Recognition using BiDirectional LSTMs\nImage Recognition App using Tflite and Flutter\nResearchers\nAaron Courville\nAbdel-rahman Mohamed\nAdam Coates\nAlex Acero\nAlex Krizhevsky\nAlexander Ilin\nAmos Storkey\nAndrej Karpathy\nAndrew M. Saxe\nAndrew Ng\nAndrew W. Senior\nAndriy Mnih\nAyse Naz Erkan\nBenjamin Schrauwen\nBernardete Ribeiro\nBo David Chen\nBoureau Y-Lan\nBrian Kingsbury\nChristopher Manning\nClement Farabet\nDan Claudiu Cire\u0219an\nDavid Reichert\nDerek Rose\nDong Yu\nDrausin Wulsin\nErik M. Schmidt\nEugenio Culurciello\nFrank Seide\nGalen Andrew\nGeoffrey Hinton\nGeorge Dahl\nGraham Taylor\nGr\u00e9goire Montavon\nGuido Francisco Mont\u00fafar\nGuillaume Desjardins\nHannes Schulz\nH\u00e9l\u00e8ne Paugam-Moisy\nHonglak Lee\nHugo Larochelle\nIlya Sutskever\nItamar Arel\nJames Martens\nJason Morton\nJason Weston\nJeff Dean\nJiquan Mgiam\nJoseph Turian\nJoshua Matthew Susskind\nJ\u00fcrgen Schmidhuber\nJustin A. Blanco\nKoray Kavukcuoglu\nKyungHyun Cho\nLi Deng\nLucas Theis\nLudovic Arnold\nMarc'Aurelio Ranzato\nMartin L\u00e4ngkvist\nMisha Denil\nMohammad Norouzi\nNando de Freitas\nNavdeep Jaitly\nNicolas Le Roux\nNitish Srivastava\nNoel Lopes\nOriol Vinyals\nPascal Vincent\nPatrick Nguyen\nPedro Domingos\nPeggy Series\nPierre Sermanet\nPiotr Mirowski\nQuoc V. Le\nReinhold Scherer\nRichard Socher\nRob Fergus\nRobert Coop\nRobert Gens\nRoger Grosse\nRonan Collobert\nRuslan Salakhutdinov\nSebastian Gerwinn\nSt\u00e9phane Mallat\nSven Behnke\nTapani Raiko\nTara Sainath\nTijmen Tieleman\nTom Karnowski\nTom\u00e1\u0161 Mikolov\nUeli Meier\nVincent Vanhoucke\nVolodymyr Mnih\nYann LeCun\nYichuan Tang\nYoshua Bengio\nYotaro Kubo\nYouzhi (Will) Zou\nFei-Fei Li\nIan Goodfellow\nRobert Lagani\u00e8re\nMerve Ayy\u00fcce K\u0131zrak\nWebsites\ndeeplearning.net\ndeeplearning.stanford.edu\nnlp.stanford.edu\nai-junkie.com\ncs.brown.edu/research/ai\neecs.umich.edu/ai\ncs.utexas.edu/users/ai-lab\ncs.washington.edu/research/ai\naiai.ed.ac.uk\nwww-aig.jpl.nasa.gov\ncsail.mit.edu\ncgi.cse.unsw.edu.au/~aishare\ncs.rochester.edu/research/ai\nai.sri.com\nisi.edu/AI/isd.htm\nnrl.navy.mil/itd/aic\nhips.seas.harvard.edu\nAI Weekly\nstat.ucla.edu\ndeeplearning.cs.toronto.edu\njeffdonahue.com/lrcn/\nvisualqa.org\nwww.mpi-inf.mpg.de/departments/computer-vision...\nDeep Learning News\nMachine Learning is Fun! Adam Geitgey's Blog\nGuide to Machine Learning\nDeep Learning for Beginners\nMachine Learning Mastery blog\nML Compiled\nProgramming Community Curated Resources\nA Beginner's Guide To Understanding Convolutional Neural Networks\nahmedbesbes.com\namitness.com\nAI Summer\nAI Hub - supported by AAAI, NeurIPS\nCatalyzeX: Machine Learning Hub for Builders and Makers\nThe Epic Code\nDatasets\nMNIST Handwritten digits\nGoogle House Numbers from street view\nCIFAR-10 and CIFAR-100\nIMAGENET\nTiny Images 80 Million tiny images6.\nFlickr Data 100 Million Yahoo dataset\nBerkeley Segmentation Dataset 500\nUC Irvine Machine Learning Repository\nFlickr 8k\nFlickr 30k\nMicrosoft COCO\nVQA\nImage QA\nAT&T Laboratories Cambridge face database\nAVHRR Pathfinder\nAir Freight - The Air Freight data set is a ray-traced image sequence along with ground truth segmentation based on textural characteristics. (455 images + GT, each 160x120 pixels). (Formats: PNG)\nAmsterdam Library of Object Images - ALOI is a color image collection of one-thousand small objects, recorded for scientific purposes. In order to capture the sensory variation in object recordings, we systematically varied viewing angle, illumination angle, and illumination color for each object, and additionally captured wide-baseline stereo images. We recorded over a hundred images of each object, yielding a total of 110,250 images for the collection. (Formats: png)\nAnnotated face, hand, cardiac & meat images - Most images & annotations are supplemented by various ASM/AAM analyses using the AAM-API. (Formats: bmp,asf)\nImage Analysis and Computer Graphics\nBrown University Stimuli - A variety of datasets including geons, objects, and \"greebles\". Good for testing recognition algorithms. (Formats: pict)\nCAVIAR video sequences of mall and public space behavior - 90K video frames in 90 sequences of various human activities, with XML ground truth of detection and behavior classification (Formats: MPEG2 & JPEG)\nMachine Vision Unit\nCCITT Fax standard images - 8 images (Formats: gif)\nCMU CIL's Stereo Data with Ground Truth - 3 sets of 11 images, including color tiff images with spectroradiometry (Formats: gif, tiff)\nCMU PIE Database - A database of 41,368 face images of 68 people captured under 13 poses, 43 illuminations conditions, and with 4 different expressions.\nCMU VASC Image Database - Images, sequences, stereo pairs (thousands of images) (Formats: Sun Rasterimage)\nCaltech Image Database - about 20 images - mostly top-down views of small objects and toys. (Formats: GIF)\nColumbia-Utrecht Reflectance and Texture Database - Texture and reflectance measurements for over 60 samples of 3D texture, observed with over 200 different combinations of viewing and illumination directions. (Formats: bmp)\nComputational Colour Constancy Data - A dataset oriented towards computational color constancy, but useful for computer vision in general. It includes synthetic data, -----> camera !!!  sensor data, and over 700 images. (Formats: tiff)\nComputational Vision Lab\nContent-based image retrieval database - 11 sets of color images for testing algorithms for content-based retrieval. Most sets have a description file with names of objects in each image. (Formats: jpg)\nEfficient Content-based Retrieval Group\nDensely Sampled View Spheres - Densely sampled view spheres - upper half of the view sphere of two toy objects with 2500 images each. (Formats: tiff)\nComputer Science VII (Graphical Systems)\nDigital Embryos - Digital embryos are novel objects which may be used to develop and test object recognition systems. They have an organic appearance. (Formats: various formats are available on request)\nUniverity of Minnesota Vision Lab\nEl Salvador Atlas of Gastrointestinal VideoEndoscopy - Images and Videos of his-res of studies taken from Gastrointestinal Video endoscopy. (Formats: jpg, mpg, gif)\nFG-NET Facial Aging Database - Database contains 1002 face images showing subjects at different ages. (Formats: jpg)\nFVC2000 Fingerprint Databases - FVC2000 is the First International Competition for Fingerprint Verification Algorithms. Four fingerprint databases constitute the FVC2000 benchmark (3520 fingerprints in all).\nBiometric Systems Lab - University of Bologna\nFace and Gesture images and image sequences - Several image datasets of faces and gestures that are ground truth annotated for benchmarking\nGerman Fingerspelling Database - The database contains 35 gestures and consists of 1400 image sequences that contain gestures of 20 different persons recorded under non-uniform daylight lighting conditions. (Formats: mpg,jpg)\nLanguage Processing and Pattern Recognition\nGroningen Natural Image Database - 4000+ 1536x1024 (16 bit) calibrated outdoor images (Formats: homebrew)\nICG Testhouse sequence - 2 turntable sequences from ifferent viewing heights, 36 images each, resolution 1000x750, color (Formats: PPM)\nInstitute of Computer Graphics and Vision\nIEN Image Library - 1000+ images, mostly outdoor sequences (Formats: raw, ppm)\nINRIA's Syntim images database - 15 color image of simple objects (Formats: gif)\nINRIA\nINRIA's Syntim stereo databases - 34 calibrated color stereo pairs (Formats: gif)\nImage Analysis Laboratory - Images obtained from a variety of imaging modalities -- raw CFA images, range images and a host of \"medical images\". (Formats: homebrew)\nImage Analysis Laboratory\nImage Database - An image database including some textures\nJAFFE Facial Expression Image Database - The JAFFE database consists of 213 images of Japanese female subjects posing 6 basic facial expressions as well as a neutral pose. Ratings on emotion adjectives are also available, free of charge, for research purposes. (Formats: TIFF Grayscale images.)\nATR Research, Kyoto, Japan\nJISCT Stereo Evaluation - 44 image pairs. These data have been used in an evaluation of stereo analysis, as described in the April 1993 ARPA Image Understanding Workshop paper ``The JISCT Stereo Evaluation'' by R.C.Bolles, H.H.Baker, and M.J.Hannah, 263--274 (Formats: SSI)\nMIT Vision Texture - Image archive (100+ images) (Formats: ppm)\nMIT face images and more - hundreds of images (Formats: homebrew)\nMachine Vision - Images from the textbook by Jain, Kasturi, Schunck (20+ images) (Formats: GIF TIFF)\nMammography Image Databases - 100 or more images of mammograms with ground truth. Additional images available by request, and links to several other mammography databases are provided. (Formats: homebrew)\nftp://ftp.cps.msu.edu/pub/prip - many images (Formats: unknown)\nMiddlebury Stereo Data Sets with Ground Truth - Six multi-frame stereo data sets of scenes containing planar regions. Each data set contains 9 color images and subpixel-accuracy ground-truth data. (Formats: ppm)\nMiddlebury Stereo Vision Research Page - Middlebury College\nModis Airborne simulator, Gallery and data set - High Altitude Imagery from around the world for environmental modeling in support of NASA EOS program (Formats: JPG and HDF)\nNIST Fingerprint and handwriting - datasets - thousands of images (Formats: unknown)\nNIST Fingerprint data - compressed multipart uuencoded tar file\nNLM HyperDoc Visible Human Project - Color, CAT and MRI image samples - over 30 images (Formats: jpeg)\nNational Design Repository - Over 55,000 3D CAD and solid models of (mostly) mechanical/machined engineering designs. (Formats: gif,vrml,wrl,stp,sat)\nGeometric & Intelligent Computing Laboratory\nOSU (MSU) 3D Object Model Database - several sets of 3D object models collected over several years to use in object recognition research (Formats: homebrew, vrml)\nOSU (MSU/WSU) Range Image Database - Hundreds of real and synthetic images (Formats: gif, homebrew)\nOSU/SAMPL Database: Range Images, 3D Models, Stills, Motion Sequences - Over 1000 range images, 3D object models, still images and motion sequences (Formats: gif, ppm, vrml, homebrew)\nSignal Analysis and Machine Perception Laboratory\nOtago Optical Flow Evaluation Sequences - Synthetic and real sequences with machine-readable ground truth optical flow fields, plus tools to generate ground truth for new sequences. (Formats: ppm,tif,homebrew)\nVision Research Group\nftp://ftp.limsi.fr/pub/quenot/opflow/testdata/piv/ - Real and synthetic image sequences used for testing a Particle Image Velocimetry application. These images may be used for the test of optical flow and image matching algorithms. (Formats: pgm (raw))\nLIMSI-CNRS/CHM/IMM/vision\nLIMSI-CNRS\nPhotometric 3D Surface Texture Database - This is the first 3D texture database which provides both full real surface rotations and registered photometric stereo data (30 textures, 1680 images). (Formats: TIFF)\nSEQUENCES FOR OPTICAL FLOW ANALYSIS (SOFA) - 9 synthetic sequences designed for testing motion analysis applications, including full ground truth of motion and camera parameters. (Formats: gif)\nComputer Vision Group\nSequences for Flow Based Reconstruction - synthetic sequence for testing structure from motion algorithms (Formats: pgm)\nStereo Images with Ground Truth Disparity and Occlusion - a small set of synthetic images of a hallway with varying amounts of noise added. Use these images to benchmark your stereo algorithm. (Formats: raw, viff (khoros), or tiff)\nStuttgart Range Image Database - A collection of synthetic range images taken from high-resolution polygonal models available on the web (Formats: homebrew)\nDepartment Image Understanding\nThe AR Face Database - Contains over 4,000 color images corresponding to 126 people's faces (70 men and 56 women). Frontal views with variations in facial expressions, illumination, and occlusions. (Formats: RAW (RGB 24-bit))\nPurdue Robot Vision Lab\nThe MIT-CSAIL Database of Objects and Scenes - Database for testing multiclass object detection and scene recognition algorithms. Over 72,000 images with 2873 annotated frames. More than 50 annotated object classes. (Formats: jpg)\nThe RVL SPEC-DB (SPECularity DataBase) - A collection of over 300 real images of 100 objects taken under three different illuminaiton conditions (Diffuse/Ambient/Directed). -- Use these images to test algorithms for detecting and compensating specular highlights in color images. (Formats: TIFF )\nRobot Vision Laboratory\nThe Xm2vts database - The XM2VTSDB contains four digital recordings of 295 people taken over a period of four months. This database contains both image and video data of faces.\nCentre for Vision, Speech and Signal Processing\nTraffic Image Sequences and 'Marbled Block' Sequence - thousands of frames of digitized traffic image sequences as well as the 'Marbled Block' sequence (grayscale images) (Formats: GIF)\nIAKS/KOGS\nU Bern Face images - hundreds of images (Formats: Sun rasterfile)\nU Michigan textures (Formats: compressed raw)\nU Oulu wood and knots database - Includes classifications - 1000+ color images (Formats: ppm)\nUCID - an Uncompressed Colour Image Database - a benchmark database for image retrieval with predefined ground truth. (Formats: tiff)\nUMass Vision Image Archive - Large image database with aerial, space, stereo, medical images and more. (Formats: homebrew)\nUNC's 3D image database - many images (Formats: GIF)\nUSF Range Image Data with Segmentation Ground Truth - 80 image sets (Formats: Sun rasterimage)\nUniversity of Oulu Physics-based Face Database - contains color images of faces under different illuminants and camera calibration conditions as well as skin spectral reflectance measurements of each person.\nMachine Vision and Media Processing Unit\nUniversity of Oulu Texture Database - Database of 320 surface textures, each captured under three illuminants, six spatial resolutions and nine rotation angles. A set of test suites is also provided so that texture segmentation, classification, and retrieval algorithms can be tested in a standard manner. (Formats: bmp, ras, xv)\nMachine Vision Group\nUsenix face database - Thousands of face images from many different sites (circa 994)\nView Sphere Database - Images of 8 objects seen from many different view points. The view sphere is sampled using a geodesic with 172 images/sphere. Two sets for training and testing are available. (Formats: ppm)\nPRIMA, GRAVIR\nVision-list Imagery Archive - Many images, many formats\nWiry Object Recognition Database - Thousands of images of a cart, ladder, stool, bicycle, chairs, and cluttered scenes with ground truth labelings of edges and regions. (Formats: jpg)\n3D Vision Group\nYale Face Database - 165 images (15 individuals) with different lighting, expression, and occlusion configurations.\nYale Face Database B - 5760 single light source images of 10 subjects each seen under 576 viewing conditions (9 poses x 64 illumination conditions). (Formats: PGM)\nCenter for Computational Vision and Control\nDeepMind QA Corpus - Textual QA corpus from CNN and DailyMail. More than 300K documents in total. Paper for reference.\nYouTube-8M Dataset - YouTube-8M is a large-scale labeled video dataset that consists of 8 million YouTube video IDs and associated labels from a diverse vocabulary of 4800 visual entities.\nOpen Images dataset - Open Images is a dataset of ~9 million URLs to images that have been annotated with labels spanning over 6000 categories.\nVisual Object Classes Challenge 2012 (VOC2012) - VOC2012 dataset containing 12k images with 20 annotated classes for object detection and segmentation.\nFashion-MNIST - MNIST like fashion product dataset consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\nLarge-scale Fashion (DeepFashion) Database - Contains over 800,000 diverse fashion images. Each image in this dataset is labeled with 50 categories, 1,000 descriptive attributes, bounding box and clothing landmarks\nFakeNewsCorpus - Contains about 10 million news articles classified using opensources.co types\nConferences\nCVPR - IEEE Conference on Computer Vision and Pattern Recognition\nAAMAS - International Joint Conference on Autonomous Agents and Multiagent Systems\nIJCAI - International Joint Conference on Artificial Intelligence\nICML - International Conference on Machine Learning\nECML - European Conference on Machine Learning\nKDD - Knowledge Discovery and Data Mining\nNIPS - Neural Information Processing Systems\nO'Reilly AI Conference - O'Reilly Artificial Intelligence Conference\nICDM - International Conference on Data Mining\nICCV - International Conference on Computer Vision\nAAAI - Association for the Advancement of Artificial Intelligence\nMAIS - Montreal AI Symposium\nFrameworks\nCaffe\nTorch7\nTheano\ncuda-convnet\nconvetjs\nCcv\nNuPIC\nDeepLearning4J\nBrain\nDeepLearnToolbox\nDeepnet\nDeeppy\nJavaNN\nhebel\nMocha.jl\nOpenDL\ncuDNN\nMGL\nKnet.jl\nNvidia DIGITS - a web app based on Caffe\nNeon - Python based Deep Learning Framework\nKeras - Theano based Deep Learning Library\nChainer - A flexible framework of neural networks for deep learning\nRNNLM Toolkit\nRNNLIB - A recurrent neural network library\nchar-rnn\nMatConvNet: CNNs for MATLAB\nMinerva - a fast and flexible tool for deep learning on multi-GPU\nBrainstorm - Fast, flexible and fun neural networks.\nTensorflow - Open source software library for numerical computation using data flow graphs\nDMTK - Microsoft Distributed Machine Learning Tookit\nScikit Flow - Simplified interface for TensorFlow (mimicking Scikit Learn)\nMXnet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning framework\nVeles - Samsung Distributed machine learning platform\nMarvin - A Minimalist GPU-only N-Dimensional ConvNets Framework\nApache SINGA - A General Distributed Deep Learning Platform\nDSSTNE - Amazon's library for building Deep Learning models\nSyntaxNet - Google's syntactic parser - A TensorFlow dependency library\nmlpack - A scalable Machine Learning library\nTorchnet - Torch based Deep Learning Library\nPaddle - PArallel Distributed Deep LEarning by Baidu\nNeuPy - Theano based Python library for ANN and Deep Learning\nLasagne - a lightweight library to build and train neural networks in Theano\nnolearn - wrappers and abstractions around existing neural network libraries, most notably Lasagne\nSonnet - a library for constructing neural networks by Google's DeepMind\nPyTorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration\nCNTK - Microsoft Cognitive Toolkit\nSerpent.AI - Game agent framework: Use any video game as a deep learning sandbox\nCaffe2 - A New Lightweight, Modular, and Scalable Deep Learning Framework\ndeeplearn.js - Hardware-accelerated deep learning and linear algebra (NumPy) library for the web\nTVM - End to End Deep Learning Compiler Stack for CPUs, GPUs and specialized accelerators\nCoach - Reinforcement Learning Coach by Intel\u00ae AI Lab\nalbumentations - A fast and framework agnostic image augmentation library\nNeuraxle - A general-purpose ML pipelining framework\nCatalyst: High-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code/ideas reusing\ngarage - A toolkit for reproducible reinforcement learning research\nDetecto - Train and run object detection models with 5-10 lines of code\nKarate Club - An unsupervised machine learning library for graph structured data\nSynapses - A lightweight library for neural networks that runs anywhere\nTensorForce - A TensorFlow library for applied reinforcement learning\nHopsworks - A Feature Store for ML and Data-Intensive AI\nFeast - A Feature Store for ML for GCP by Gojek/Google\nPyTorch Geometric Temporal - Representation learning on dynamic graphs\nlightly - A computer vision framework for self-supervised learning\nTrax \u2014 Deep Learning with Clear Code and Speed\nFlax - a neural network ecosystem for JAX that is designed for flexibility\nQuickVision\nTools\nNetron - Visualizer for deep learning and machine learning models\nJupyter Notebook - Web-based notebook environment for interactive computing\nTensorBoard - TensorFlow's Visualization Toolkit\nVisual Studio Tools for AI - Develop, debug and deploy deep learning and AI solutions\nTensorWatch - Debugging and visualization for deep learning\nML Workspace - All-in-one web-based IDE for machine learning and data science.\ndowel - A little logger for machine learning research. Log any object to the console, CSVs, TensorBoard, text log files, and more with just one call to logger.log()\nNeptune - Lightweight tool for experiment tracking and results visualization.\nCatalyzeX - Browser extension (Chrome and Firefox) that automatically finds and links to code implementations for ML papers anywhere online: Google, Twitter, Arxiv, Scholar, etc.\nDetermined - Deep learning training platform with integrated support for distributed training, hyperparameter tuning, smart GPU scheduling, experiment tracking, and a model registry.\nDAGsHub - Community platform for Open Source ML \u2013 Manage experiments, data & models and create collaborative ML projects easily.\nMiscellaneous\nGoogle Plus - Deep Learning Community\nCaffe Webinar\n100 Best Github Resources in Github for DL\nWord2Vec\nCaffe DockerFile\nTorontoDeepLEarning convnet\ngfx.js\nTorch7 Cheat sheet\nMisc from MIT's 'Advanced Natural Language Processing' course\nMisc from MIT's 'Machine Learning' course\nMisc from MIT's 'Networks for Learning: Regression and Classification' course\nMisc from MIT's 'Neural Coding and Perception of Sound' course\nImplementing a Distributed Deep Learning Network over Spark\nA chess AI that learns to play chess using deep learning.\nReproducing the results of \"Playing Atari with Deep Reinforcement Learning\" by DeepMind\nWiki2Vec. Getting Word2vec vectors for entities and word from Wikipedia Dumps\nThe original code from the DeepMind article + tweaks\nGoogle deepdream - Neural Network art\nAn efficient, batched LSTM.\nA recurrent neural network designed to generate classical music.\nMemory Networks Implementations - Facebook\nFace recognition with Google's FaceNet deep neural network.\nBasic digit recognition neural network\nEmotion Recognition API Demo - Microsoft\nProof of concept for loading Caffe models in TensorFlow\nYOLO: Real-Time Object Detection\nYOLO: Practical Implementation using Python\nAlphaGo - A replication of DeepMind's 2016 Nature publication, \"Mastering the game of Go with deep neural networks and tree search\"\nMachine Learning for Software Engineers\nMachine Learning is Fun!\nSiraj Raval's Deep Learning tutorials\nDockerface - Easy to install and use deep learning Faster R-CNN face detection for images and video in a docker container.\nAwesome Deep Learning Music - Curated list of articles related to deep learning scientific research applied to music\nAwesome Graph Embedding - Curated list of articles related to deep learning scientific research on graph structured data at the graph level.\nAwesome Network Embedding - Curated list of articles related to deep learning scientific research on graph structured data at the node level.\nMicrosoft Recommenders contains examples, utilities and best practices for building recommendation systems. Implementations of several state-of-the-art algorithms are provided for self-study and customization in your own applications.\nThe Unreasonable Effectiveness of Recurrent Neural Networks - Andrej Karpathy blog post about using RNN for generating text.\nLadder Network - Keras Implementation of Ladder Network for Semi-Supervised Learning\ntoolbox: Curated list of ML libraries\nCNN Explainer\nAI Expert Roadmap - Roadmap to becoming an Artificial Intelligence Expert\nContributing\nHave anything in mind that you think is awesome and would fit in this list? Feel free to send a pull request.\nLicense\nTo the extent possible under law, Christos Christofidis has waived all copyright and related or neighboring rights to this work.",
      "link": "https://github.com/ChristosChristofidis/awesome-deep-learning"
    },
    {
      "autor": "pwc",
      "date": "NaN",
      "content": "2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2008 Suggestions\nThis work is in continuous progress and update. We are adding new PWC everyday! Tweet me @fvzaur\nUse this thread to request us your favorite conference to be added to our watchlist and to PWC list.\nWeekly updated pushed!\n2018\nTitle Conf Code Stars\nVideo-to-Video Synthesis NIPS code 5578\nDeep Image Prior CVPR code 3736\nStarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation CVPR code 3405\nJoint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network ECCV code 2434\nLearning to See in the Dark CVPR code 2326\nGlow: Generative Flow with Invertible 1x1 Convolutions NIPS code 2088\nSqueeze-and-Excitation Networks CVPR code 1477\nEfficient Neural Architecture Search via Parameters Sharing ICML code 1382\nMultimodal Unsupervised Image-to-image Translation ECCV code 1296\nNon-Local Neural Networks CVPR code 992\nCan Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet? CVPR code 924\nSingle-Shot Refinement Neural Network for Object Detection CVPR code 875\nImage Generation From Scene Graphs CVPR code 851\nGANimation: Anatomically-aware Facial Animation from a Single Image ECCV code 772\nSimple Baselines for Human Pose Estimation and Tracking ECCV code 752\nVisualizing the Loss Landscape of Neural Nets NIPS code 724\nDetect-and-Track: Efficient Pose Estimation in Videos CVPR code 650\nRelation Networks for Object Detection CVPR code 635\nGenerative Image Inpainting With Contextual Attention CVPR code 609\nPointCNN NIPS code 607\nLook at Boundary: A Boundary-Aware Face Alignment Algorithm CVPR code 575\nPelee: A Real-Time Object Detection System on Mobile Devices NIPS code 548\nDistractor-aware Siamese Networks for Visual Object Tracking ECCV code 545\nObfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples ICML code 535\nWhich Training Methods for GANs do actually Converge? ICML code 520\nEnd-to-End Recovery of Human Shape and Pose CVPR code 502\nTaskonomy: Disentangling Task Transfer Learning CVPR code 502\nCascaded Pyramid Network for Multi-Person Pose Estimation CVPR code 497\nNeural 3D Mesh Renderer CVPR code 489\nZero-Shot Recognition via Semantic Embeddings and Knowledge Graphs CVPR code 489\nIn-Place Activated BatchNorm for Memory-Optimized Training of DNNs CVPR code 485\nThe Unreasonable Effectiveness of Deep Features as a Perceptual Metric CVPR code 447\nFrustum PointNets for 3D Object Detection From RGB-D Data CVPR code 434\nThe Lov\u00e1sz-Softmax Loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks CVPR code 416\nICNet for Real-Time Semantic Segmentation on High-Resolution Images ECCV code 415\nPWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume CVPR code 398\nEfficient Interactive Annotation of Segmentation Datasets With Polygon-RNN++ CVPR code 397\nGibson Env: Real-World Perception for Embodied Agents CVPR code 385\nAcquisition of Localization Confidence for Accurate Object Detection ECCV code 384\nNoise2Noise: Learning Image Restoration without Clean Data ICML code 370\nGeoNet: Geometric Neural Network for Joint Depth and Surface Normal Estimation CVPR code 359\nGeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose CVPR code 359\nA Style-Aware Content Loss for Real-time HD Style Transfer ECCV code 349\nSoccer on Your Tabletop CVPR code 338\nPyramid Stereo Matching Network CVPR code 335\nNeural Baby Talk CVPR code 332\nLicense Plate Detection and Recognition in Unconstrained Scenarios ECCV code 326\nSupervision-by-Registration: An Unsupervised Approach to Improve the Precision of Facial Landmark Detectors CVPR code 326\nPixel2Mesh: Generating 3D Mesh Models from Single RGB Images ECCV code 323\nTransparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning CVPR code 317\nFast End-to-End Trainable Guided Filter CVPR code 312\nDeep Clustering for Unsupervised Learning of Visual Features ECCV code 302\nDeep Photo Enhancer: Unpaired Learning for Image Enhancement From Photographs With GANs CVPR code 294\nNeural Relational Inference for Interacting Systems ICML code 289\nAdversarially Regularized Autoencoders ICML code 282\nLearning to Adapt Structured Output Space for Semantic Segmentation CVPR code 280\nConvolutional Neural Networks With Alternately Updated Clique CVPR code 272\nLearning to Segment Every Thing CVPR code 269\nSupervising Unsupervised Learning NIPS code 262\nLiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation CVPR code 261\nBilinear Attention Networks NIPS code 258\nESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation ECCV code 254\nAn intriguing failing of convolutional neural networks and the CoordConv solution NIPS code 249\nEnd-to-End Learning of Motion Representation for Video Understanding CVPR code 238\nImage Super-Resolution Using Very Deep Residual Channel Attention Networks ECCV code 234\nIterative Visual Reasoning Beyond Convolutions CVPR code 228\nSemi-Parametric Image Synthesis CVPR code 226\nCompressed Video Action Recognition CVPR code 225\nStyle Aggregated Network for Facial Landmark Detection CVPR code 223\nPose-Robust Face Recognition via Deep Residual Equivariant Mapping CVPR code 220\nMulti-Content GAN for Few-Shot Font Style Transfer CVPR code 218\nGraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models ICML code 214\nReferring Relationships CVPR code 210\nMoCoGAN: Decomposing Motion and Content for Video Generation CVPR code 205\nLatent Alignment and Variational Attention NIPS code 204\nLayoutNet: Reconstructing the 3D Room Layout From a Single RGB Image CVPR code 202\nLarge-Scale Point Cloud Semantic Segmentation With Superpoint Graphs CVPR code 197\nAn End-to-End TextSpotter With Explicit Alignment and Attention CVPR code 195\nDeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks CVPR code 189\nSPLATNet: Sparse Lattice Networks for Point Cloud Processing CVPR code 188\nAttentive Generative Adversarial Network for Raindrop Removal From a Single Image CVPR code 186\nSingle View Stereo Matching CVPR code 182\nMegaDepth: Learning Single-View Depth Prediction From Internet Photos CVPR code 181\nECO: Efficient Convolutional Network for Online Video Understanding ECCV code 180\nUnsupervised Feature Learning via Non-Parametric Instance Discrimination CVPR code 180\nST-GAN: Spatial Transformer Generative Adversarial Networks for Image Compositing CVPR code 179\nVideo Based Reconstruction of 3D People Models CVPR code 179\nSocial GAN: Socially Acceptable Trajectories With Generative Adversarial Networks CVPR code 178\nLearning Category-Specific Mesh Reconstruction from Image Collections ECCV code 176\nRealistic Evaluation of Deep Semi-Supervised Learning Algorithms NIPS code 175\nBSN: Boundary Sensitive Network for Temporal Action Proposal Generation ECCV code 175\nGroup Normalization ECCV code 175\nReal-Time Seamless Single Shot 6D Object Pose Prediction CVPR code 174\nMVSNet: Depth Inference for Unstructured Multi-view Stereo ECCV code 174\nNeural Motifs: Scene Graph Parsing With Global Context CVPR code 171\nLearning a Single Convolutional Super-Resolution Network for Multiple Degradations CVPR code 169\nOptimizing Video Object Detection via a Scale-Time Lattice CVPR code 168\nMultiPoseNet: Fast Multi-Person Pose Estimation using Pose Residual Network ECCV code 167\nUnsupervised Cross-Dataset Person Re-Identification by Transfer Learning of Spatial-Temporal Patterns CVPR code 166\nWeakly Supervised Instance Segmentation Using Class Peak Response CVPR code 166\nPlaneNet: Piece-Wise Planar Reconstruction From a Single RGB Image CVPR code 164\nResidual Dense Network for Image Super-Resolution CVPR code 163\nEmbodied Question Answering CVPR code 162\nEvolved Policy Gradients NIPS code 160\nCamera Style Adaptation for Person Re-Identification CVPR code 159\nWeakly and Semi Supervised Human Body Part Parsing via Pose-Guided Knowledge Transfer CVPR code 159\nScale-Recurrent Network for Deep Image Deblurring CVPR code 159\nUnsupervised Learning of Monocular Depth Estimation and Visual Odometry With Deep Feature Reconstruction CVPR code 158\nRelational recurrent neural networks NIPS code 157\nDensely Connected Pyramid Dehazing Network CVPR code 155\nImage Inpainting for Irregular Holes Using Partial Convolutions ECCV code 153\nSO-Net: Self-Organizing Network for Point Cloud Analysis CVPR code 152\nPix3D: Dataset and Methods for Single-Image 3D Shape Modeling CVPR code 152\nShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices CVPR code 152\nDenseASPP for Semantic Segmentation in Street Scenes CVPR code 151\nFacelet-Bank for Fast Portrait Manipulation CVPR code 150\nSelf-Imitation Learning ICML code 145\nGraph R-CNN for Scene Graph Generation ECCV code 144\nA Closer Look at Spatiotemporal Convolutions for Action Recognition CVPR code 143\nCross-Domain Weakly-Supervised Object Detection Through Progressive Domain Adaptation CVPR code 143\nQuantized Densely Connected U-Nets for Efficient Landmark Localization ECCV code 143\nRecurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining ECCV code 142\nTwo-Stream Convolutional Networks for Dynamic Texture Synthesis CVPR code 141\nIntegral Human Pose Regression ECCV code 141\nAdaptive Affinity Fields for Semantic Segmentation ECCV code 141\nLSTM Pose Machines CVPR code 141\nStructure Inference Net: Object Detection Using Scene-Level Context and Instance-Level Relationships CVPR code 140\nRecovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform CVPR code 139\nImage-Image Domain Adaptation With Preserved Self-Similarity and Domain-Dissimilarity for Person Re-Identification CVPR code 137\nLearning to Compare: Relation Network for Few-Shot Learning CVPR code 135\nCosFace: Large Margin Cosine Loss for Deep Face Recognition CVPR code 135\nDeep Depth Completion of a Single RGB-D Image CVPR code 134\nDeep Back-Projection Networks for Super-Resolution CVPR code 132\nContext Embedding Networks CVPR code 131\nMulti-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics CVPR code 131\nPerturbative Neural Networks CVPR code 130\nStyle Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis ICML code 129\nFast and Accurate Online Video Object Segmentation via Tracking Parts CVPR code 129\nNonlinear 3D Face Morphable Model CVPR code 128\nBodyNet: Volumetric Inference of 3D Human Body Shapes ECCV code 126\n3D-CODED: 3D Correspondences by Deep Deformation ECCV code 125\nDeepMVS: Learning Multi-View Stereopsis CVPR code 125\nHierarchical Imitation and Reinforcement Learning ICML code 124\nDomain Adaptive Faster R-CNN for Object Detection in the Wild CVPR code 123\nL4: Practical loss-based stepsize adaptation for deep learning NIPS code 123\nA Generative Adversarial Approach for Zero-Shot Learning From Noisy Texts CVPR code 122\nRecurrent Relational Networks NIPS code 121\nGated Path Planning Networks ICML code 121\nPSANet: Point-wise Spatial Attention Network for Scene Parsing ECCV code 121\nRethinking Feature Distribution for Loss Functions in Image Classification CVPR code 120\nDensity-Aware Single Image De-Raining Using a Multi-Stream Dense Network CVPR code 118\nFOTS: Fast Oriented Text Spotting With a Unified Network CVPR code 118\nELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes ECCV code 117\nPU-Net: Point Cloud Upsampling Network CVPR code 117\nPackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning CVPR code 117\nLong-term Tracking in the Wild: a Benchmark ECCV code 116\nFactoring Shape, Pose, and Layout From the 2D Image of a 3D Scene CVPR code 114\nRepulsion Loss: Detecting Pedestrians in a Crowd CVPR code 113\nUnsupervised Attention-guided Image-to-Image Translation NIPS code 110\nAttention-based Deep Multiple Instance Learning ICML code 109\nLearning Blind Video Temporal Consistency ECCV code 109\nNoisy Natural Gradient as Variational Inference ICML code 108\nEnd-to-End Weakly-Supervised Semantic Alignment CVPR code 106\nDecoupled Networks CVPR code 105\nLiDAR-Video Driving Dataset: Learning Driving Policies Effectively CVPR code 104\nMAttNet: Modular Attention Network for Referring Expression Comprehension CVPR code 104\nLQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks ECCV code 103\nFSRNet: End-to-End Learning Face Super-Resolution With Facial Priors CVPR code 100\nDeep Mutual Learning CVPR code 100\nMacro-Micro Adversarial Network for Human Parsing ECCV code 98\nScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans CVPR code 97\nLearning Depth From Monocular Videos Using Direct Methods CVPR code 97\nVITON: An Image-Based Virtual Try-On Network CVPR code 95\nCascade R-CNN: Delving Into High Quality Object Detection CVPR code 93\nLearning Human-Object Interactions by Graph Parsing Neural Networks ECCV code 93\nFuture Frame Prediction for Anomaly Detection \u2013 A New Baseline CVPR code 92\nMulti-view to Novel view: Synthesizing novel views with Self-Learned Confidence ECCV code 92\nTell Me Where to Look: Guided Attention Inference Network CVPR code 91\nNeural Kinematic Networks for Unsupervised Motion Retargetting CVPR code 90\nLearning SO(3) Equivariant Representations with Spherical CNNs ECCV code 89\nOne-Shot Unsupervised Cross Domain Translation NIPS code 89\nSynthesizing Images of Humans in Unseen Poses CVPR code 88\nDepth-aware CNN for RGB-D Segmentation ECCV code 88\nPiggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights ECCV code 88\nKnowledge Aided Consistency for Weakly Supervised Phrase Grounding CVPR code 87\nCSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes CVPR code 87\nNeural Arithmetic Logic Units NIPS code 87\nA PID Controller Approach for Stochastic Optimization of Deep Networks CVPR code 87\nVITAL: VIsual Tracking via Adversarial Learning CVPR code 86\nLearning Spatial-Temporal Regularized Correlation Filters for Visual Tracking CVPR code 86\nRecurrent Pixel Embedding for Instance Grouping CVPR code 85\nSGPN: Similarity Group Proposal Network for 3D Point Cloud Instance Segmentation CVPR code 84\nMulti-Scale Location-Aware Kernel Representation for Object Detection CVPR code 84\nRepeatability Is Not Enough: Learning Affine Regions via Discriminability ECCV code 84\n\u201cZero-Shot\u201d Super-Resolution Using Deep Internal Learning CVPR code 84\nDF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency ECCV code 82\nMulti-View Consistency as Supervisory Signal for Learning Shape and Pose Prediction CVPR code 80\nFactorizable Net: An Efficient Subgraph-based Framework for Scene Graph Generation ECCV code 78\nGeneralizing A Person Retrieval Model Hetero- and Homogeneously ECCV code 78\nCrafting a Toolchain for Image Restoration by Deep Reinforcement Learning CVPR code 77\nPairwise Confusion for Fine-Grained Visual Classification ECCV code 77\nLearning to Reweight Examples for Robust Deep Learning ICML code 76\nImproving Generalization via Scalable Neighborhood Component Analysis ECCV code 76\nSparseMAP: Differentiable Sparse Structured Inference ICML code 75\nPDE-Net: Learning PDEs from Data ICML code 75\nPose-Normalized Image Generation for Person Re-identification ECCV code 75\nDisentangled Person Image Generation CVPR code 75\nLearning to Navigate for Fine-grained Classification ECCV code 74\nSuperpixel Sampling Networks ECCV code 74\nShift-Net: Image Inpainting via Deep Feature Rearrangement ECCV code 74\n3DMV: Joint 3D-Multi-View Prediction for 3D Semantic Scene Segmentation ECCV code 74\nOrdinal Depth Supervision for 3D Human Pose Estimation CVPR code 74\nPath-Level Network Transformation for Efficient Architecture Search ICML code 73\nDiverse Image-to-Image Translation via Disentangled Representations ECCV code 72\nVisual Feature Attribution Using Wasserstein GANs CVPR code 72\nReal-World Anomaly Detection in Surveillance Videos CVPR code 72\nSelf-Supervised Adversarial Hashing Networks for Cross-Modal Retrieval CVPR code 72\nHolistic 3D Scene Parsing and Reconstruction from a Single RGB Image ECCV code 72\nLearning to Find Good Correspondences CVPR code 72\nLearning Less Is More - 6D Camera Localization via 3D Surface Regression CVPR code 72\nObject Level Visual Reasoning in Videos ECCV code 71\nWeakly-Supervised Semantic Segmentation Network With Deep Seeded Region Growing CVPR code 71\nAvatar-Net: Multi-Scale Zero-Shot Style Transfer by Feature Decoration CVPR code 71\nFast and Accurate Single Image Super-Resolution via Information Distillation Network CVPR code 71\nRegularizing RNNs for Caption Generation by Reconstructing the Past With the Present CVPR code 70\nMulti-Shot Pedestrian Re-Identification via Sequential Decision Making CVPR code 70\nPointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place Recognition CVPR code 69\nProgressive Neural Architecture Search ECCV code 68\nGenerative Neural Machine Translation NIPS code 68\nLearning Latent Super-Events to Detect Multiple Activities in Videos CVPR code 67\nGenerate to Adapt: Aligning Domains Using Generative Adversarial Networks CVPR code 67\nAdversarial Feature Augmentation for Unsupervised Domain Adaptation CVPR code 67\nLearning Attentions: Residual Attentional Siamese Network for High Performance Online Visual Tracking CVPR code 67\nPointwise Convolutional Neural Networks CVPR code 67\nOptimizing the Latent Space of Generative Networks ICML code 66\nPart-Aligned Bilinear Representations for Person Re-Identification ECCV code 64\nGeometry-Aware Learning of Maps for Camera Localization CVPR code 63\nFighting Fake News: Image Splice Detection via Learned Self-Consistency ECCV code 62\nIsolating Sources of Disentanglement in Variational Autoencoders NIPS code 62\nNeural Program Synthesis from Diverse Demonstration Videos ICML code 62\nLearning Rigidity in Dynamic Scenes with a Moving -----> Camera !!!  for 3D Motion Field Estimation ECCV code 61\nRotation-Sensitive Regression for Oriented Scene Text Detection CVPR code 61\nHuman Semantic Parsing for Person Re-Identification CVPR code 61\nUnsupervised Discovery of Object Landmarks as Structural Representations CVPR code 61\nIQA: Visual Question Answering in Interactive Environments CVPR code 60\nHierarchical Long-term Video Prediction without Supervision ICML code 60\nUnsupervised Domain Adaptation for 3D Keypoint Estimation via View Consistency ECCV code 60\nExploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning CVPR code 59\nNeural Style Transfer via Meta Networks CVPR code 59\nFrame-Recurrent Video Super-Resolution CVPR code 58\nPlaneMatch: Patch Coplanarity Prediction for Robust RGB-D Reconstruction ECCV code 57\nCBAM: Convolutional Block Attention Module ECCV code 57\nDecorrelated Batch Normalization CVPR code 57\nLearning Conditioned Graph Structures for Interpretable Visual Question Answering NIPS code 57\nHierarchical Bilinear Pooling for Fine-Grained Visual Recognition ECCV code 57\nLeveraging Unlabeled Data for Crowd Counting by Learning to Rank CVPR code 56\nDeep Marching Cubes: Learning Explicit Surface Representations CVPR code 56\nLearning From Synthetic Data: Addressing Domain Shift for Semantic Segmentation CVPR code 56\nLF-Net: Learning Local Features from Images NIPS code 55\nSemi-supervised Adversarial Learning to Generate Photorealistic Face Images of New Identities from 3D Morphable Model ECCV code 55\nDiscriminability Objective for Training Descriptive Captions CVPR code 54\nBlockDrop: Dynamic Inference Paths in Residual Networks CVPR code 54\nConditional Probability Models for Deep Image Compression CVPR code 54\nJointly Optimize Data Augmentation and Network Training: Adversarial Data Augmentation in Human Pose Estimation CVPR code 54\nLearning towards Minimum Hyperspherical Energy NIPS code 54\nDeepVS: A Deep Learning Based Video Saliency Prediction Approach ECCV code 53\nLearning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting ECCV code 52\nLearning Pixel-Level Semantic Affinity With Image-Level Supervision for Weakly Supervised Semantic Segmentation CVPR code 52\nWasserstein Introspective Neural Networks CVPR code 51\nSketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis CVPR code 51\nSelf-produced Guidance for Weakly-supervised Object Localization ECCV code 51\nMeasuring abstract reasoning in neural networks ICML code 51\nA Unified Feature Disentangler for Multi-Domain Image Translation and Manipulation NIPS code 51\nRayNet: Learning Volumetric 3D Reconstruction With Ray Potentials CVPR code 51\nColoring with Words: Guiding Image Colorization Through Text-based Palette Generation ECCV code 50\nEfficient end-to-end learning for quantizable representations ICML code 50\nVisual Question Generation as Dual Task of Visual Question Answering CVPR code 50\nFast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam ICML code 49\nSurface Networks CVPR code 48\nDeep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions ICML code 48\nStacked Cross Attention for Image-Text Matching ECCV code 48\nActor and Observer: Joint Modeling of First and Third-Person Videos CVPR code 48\nSuper SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation CVPR code 47\nLearning-based Video Motion Magnification ECCV code 47\nPose Partition Networks for Multi-Person Pose Estimation ECCV code 47\nNeural Autoregressive Flows ICML code 47\nWeakly- and Semi-Supervised Panoptic Segmentation ECCV code 46\nVideo Re-localization ECCV code 46\nReal-time 'Actor-Critic' Tracking ECCV code 46\nBlack-box Adversarial Attacks with Limited Queries and Information ICML code 46\nHyperbolic Entailment Cones for Learning Hierarchical Embeddings ICML code 46\nStructured Attention Guided Convolutional Neural Fields for Monocular Depth Estimation CVPR code 46\nDifferentiable Compositional Kernel Learning for Gaussian Processes ICML code 45\nVisualizing and Understanding Atari Agents ICML code 45\nImage Manipulation with Perceptual Discriminators ECCV code 45\nLearning Intrinsic Image Decomposition From Watching the World CVPR code 45\nOvercoming Catastrophic Forgetting with Hard Attention to the Task ICML code 44\nLearning Pose Specific Representations by Predicting Different Views CVPR code 44\nZero-Shot Object Detection ECCV code 43\nMean Field Multi-Agent Reinforcement Learning ICML code 43\nPartial Adversarial Domain Adaptation ECCV code 43\nMutual Learning to Adapt for Joint Human Parsing and Pose Estimation ECCV code 43\nRobust Classification With Convolutional Prototype Learning CVPR code 43\nSimplE Embedding for Link Prediction in Knowledge Graphs NIPS code 42\nPredRNN++: Towards A Resolution of the Deep-in-Time Dilemma in Spatiotemporal Predictive Learning ICML code 42\nLearning to Blend Photos ECCV code 42\nMask-Guided Contrastive Attention Model for Person Re-Identification CVPR code 41\nLink Prediction Based on Graph Neural Networks NIPS code 41\nGeneralisation in humans and deep neural networks NIPS code 41\nTowards Binary-Valued Gates for Robust LSTM Training ICML code 41\nMulti-scale Residual Network for Image Super-Resolution ECCV code 41\nFully Motion-Aware Network for Video Object Detection ECCV code 41\nInterpretable Convolutional Neural Networks CVPR code 40\nGenerative Adversarial Perturbations CVPR code 40\nThe Sound of Pixels ECCV code 40\nTowards Faster Training of Global Covariance Pooling Networks by Iterative Matrix Square Root Normalization CVPR code 40\nChoose Your Neuron: Incorporating Domain Knowledge through Neuron-Importance ECCV code 40\nMulti-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation NIPS code 40\nLearning Warped Guidance for Blind Face Restoration ECCV code 39\nAdversarial Complementary Learning for Weakly Supervised Object Localization CVPR code 39\nLearning Semantic Representations for Unsupervised Domain Adaptation ICML code 39\nNeural Architecture Search with Bayesian Optimisation and Optimal Transport NIPS code 39\nMutual Information Neural Estimation ICML code 39\nNetGAN: Generating Graphs via Random Walks ICML code 39\nLearning to Evaluate Image Captioning CVPR code 38\nHyperbolic Neural Networks NIPS code 37\nUnsupervised Geometry-Aware Representation for 3D Human Pose Estimation ECCV code 37\nAdversarially Learned One-Class Classifier for Novelty Detection CVPR code 37\nDisentangling by Factorising ICML code 37\nExtracting Automata from Recurrent Neural Networks Using Queries and Counterexamples ICML code 37\nTangent Convolutions for Dense Prediction in 3D CVPR code 37\nFew-Shot Image Recognition by Predicting Parameters From Activations CVPR code 37\nReal-Time Monocular Depth Estimation Using Synthetic Data With Domain Adaptation via Image Style Transfer CVPR code 37\nGeneralizing to Unseen Domains via Adversarial Data Augmentation NIPS code 36\nSeGAN: Segmenting and Generating the Invisible CVPR code 36\nGraphical Generative Adversarial Networks NIPS code 36\nPieAPP: Perceptual Image-Error Assessment Through Pairwise Preference CVPR code 36\nGated Fusion Network for Single Image Dehazing CVPR code 35\nNeural Code Comprehension: A Learnable Representation of Code Semantics NIPS code 35\nEye In-Painting With Exemplar Generative Adversarial Networks CVPR code 35\nDeep One-Class Classification ICML code 34\nDeep Regression Tracking with Shrinkage Loss ECCV code 34\nDeflecting Adversarial Attacks With Pixel Deflection CVPR code 34\nLearning Visual Question Answering by Bootstrapping Hard Attention ECCV code 33\nHuman-Centric Indoor Scene Synthesis Using Stochastic Grammar CVPR code 33\nImproved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering CVPR code 33\nCleanNet: Transfer Learning for Scalable Image Classifier Training With Label Noise CVPR code 33\nSpeaker-Follower Models for Vision-and-Language Navigation NIPS code 33\nImproving Shape Deformation in Unsupervised Image-to-Image Translation ECCV code 33\nLearning Single-View 3D Reconstruction with Limited Pose Supervision ECCV code 33\n3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data NIPS code 33\nAdversarial Logit Pairing NIPS code 32\nAttention in Convolutional LSTM for Gesture Recognition NIPS code 32\nGraph-Cut RANSAC CVPR code 32\nNeural Guided Constraint Logic Programming for Program Synthesis NIPS code 32\nLearning Dynamic Memory Networks for Object Tracking ECCV code 32\nGeoDesc: Learning Local Descriptors by Integrating Geometry Constraints ECCV code 32\nA Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks NIPS code 32\nFlow-Grounded Spatial-Temporal Video Prediction from Still Images ECCV code 32\nBidirectional Feature Pyramid Network with Recurrent Attention Residual Modules for Shadow Detection ECCV code 32\nOn the Robustness of Semantic Segmentation Models to Adversarial Attacks CVPR code 31\nLarge Scale Fine-Grained Categorization and Domain-Specific Transfer Learning CVPR code 31\nSketchyScene: Richly-Annotated Scene Sketches ECCV code 31\nDeep Randomized Ensembles for Metric Learning ECCV code 30\nDeep High Dynamic Range Imaging with Large Foreground Motions ECCV code 30\nRevisiting Video Saliency: A Large-Scale Benchmark and a New Model CVPR code 30\nBlazingly Fast Video Object Segmentation With Pixel-Wise Metric Learning CVPR code 30\nDeep Model-Based 6D Pose Refinement in RGB ECCV code 30\nTOM-Net: Learning Transparent Object Matting From a Single Image CVPR code 30\nQuaternion Convolutional Neural Networks ECCV code 30\nDensely Connected Attention Propagation for Reading Comprehension NIPS code 30\nA Trilateral Weighted Sparse Coding Scheme for Real-World Image Denoising ECCV code 30\nSelf-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings ICML code 29\nVideo Rain Streak Removal by Multiscale Convolutional Sparse Coding CVPR code 29\nRecurrent Scene Parsing With Perspective Understanding in the Loop CVPR code 29\nSingle Shot Scene Text Retrieval ECCV code 29\nToward Characteristic-Preserving Image-based Virtual Try-On Network ECCV code 29\nExplainable Neural Computation via Stack Neural Module Networks ECCV code 29\nExploring Disentangled Feature Representation Beyond Face Identification CVPR code 29\nControllable Video Generation With Sparse Trajectories CVPR code 28\nLayer-structured 3D Scene Inference via View Synthesis ECCV code 28\nEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation ECCV code 28\nPiCANet: Learning Pixel-Wise Contextual Attention for Saliency Detection CVPR code 28\nLearning Rich Features for Image Manipulation Detection CVPR code 27\nFast Video Object Segmentation by Reference-Guided Mask Propagation CVPR code 27\n3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration ECCV code 27\nWho Let the Dogs Out? Modeling Dog Behavior From Visual Data CVPR code 27\nEC-Net: an Edge-aware Point set Consolidation Network ECCV code 27\nInterpretable Intuitive Physics Model ECCV code 27\nLearning a Discriminative Feature Network for Semantic Segmentation CVPR code 26\nPartial Transfer Learning With Selective Adversarial Networks CVPR code 26\nCross-Modal Deep Variational Hand Pose Estimation CVPR code 26\nBetween-Class Learning for Image Classification CVPR code 26\nAON: Towards Arbitrarily-Oriented Text Recognition CVPR code 26\nConditional Image-to-Image Translation CVPR code 25\nLearning Convolutional Networks for Content-Weighted Image Compression CVPR code 25\nDiversity Regularized Spatiotemporal Attention for Video-Based Person Re-Identification CVPR code 25\nDynamic Multimodal Instance Segmentation Guided by Natural Language Queries ECCV code 25\nCBMV: A Coalesced Bidirectional Matching Volume for Disparity Estimation CVPR code 25\nDeep Texture Manifold for Ground Terrain Recognition CVPR code 25\nAudio-Visual Event Localization in Unconstrained Videos ECCV code 25\nFirst Order Generative Adversarial Networks ICML code 25\nVisual Coreference Resolution in Visual Dialog using Neural Module Networks ECCV code 25\nSYQ: Learning Symmetric Quantization for Efficient Deep Neural Networks CVPR code 24\nDeep Reinforcement Learning of Marked Temporal Point Processes NIPS code 24\nExplicit Inductive Bias for Transfer Learning with Convolutional Networks ICML code 24\nLEGO: Learning Edge With Geometry All at Once by Watching Videos CVPR code 24\nVerisimilar Image Synthesis for Accurate Detection and Recognition of Texts in Scenes ECCV code 24\nMulti-Agent Diverse Generative Adversarial Networks CVPR code 23\nFace Aging With Identity-Preserved Conditional Generative Adversarial Networks CVPR code 23\nLearning to Separate Object Sounds by Watching Unlabeled Video ECCV code 23\nExploiting the Potential of Standard Convolutional Autoencoders for Image Restoration by Evolutionary Search ICML code 23\nTo Trust Or Not To Trust A Classifier NIPS code 23\nIm2Flow: Motion Hallucination From Static Images for Action Recognition CVPR code 22\nISTA-Net: Interpretable Optimization-Inspired Deep Network for Image Compressive Sensing CVPR code 22\nHallucinated-IQA: No-Reference Image Quality Assessment via Adversarial Learning CVPR code 22\nAnonymous Walk Embeddings ICML code 22\nLearning to Multitask NIPS code 22\nCondenseNet: An Efficient DenseNet Using Learned Group Convolutions CVPR code 22\nHashGAN: Deep Learning to Hash With Pair Conditional Wasserstein GAN CVPR code 22\nHierarchical Relational Networks for Group Activity Recognition and Retrieval ECCV code 22\nCollaborative and Adversarial Network for Unsupervised Domain Adaptation CVPR code 22\nGeometry-Aware Scene Text Detection With Instance Transformation Network CVPR code 22\nLearning to Promote Saliency Detectors CVPR code 21\nCSGNet: Neural Shape Parser for Constructive Solid Geometry CVPR code 21\nLocal Spectral Graph Convolution for Point Set Feature Learning ECCV code 21\nHiDDeN: Hiding Data with Deep Networks ECCV code 21\nGraphBit: Bitwise Interaction Mining via Deep Reinforcement Learning CVPR code 20\nStacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal CVPR code 20\nFully-Convolutional Point Networks for Large-Scale Point Clouds ECCV code 20\nLearning Superpixels With Segmentation-Aware Affinity Loss CVPR code 20\nZero-Shot Visual Recognition Using Semantics-Preserving Adversarial Embedding Networks CVPR code 20\nCrowd Counting With Deep Negative Correlation Learning CVPR code 20\nDimensionality-Driven Learning with Noisy Labels ICML code 20\nObjects that Sound ECCV code 20\nDeep Expander Networks: Efficient Deep Networks from Graph Theory ECCV code 19\nLow-Shot Learning With Large-Scale Diffusion CVPR code 19\nLow-Shot Learning With Imprinted Weights CVPR code 19\nCross-Domain Self-Supervised Multi-Task Feature Learning Using Synthetic Imagery CVPR code 19\nLearning Descriptor Networks for 3D Shape Synthesis and Analysis CVPR code 19\nDisentangling Factors of Variation with Cycle-Consistent Variational Auto-Encoders ECCV code 19\nCTAP: Complementary Temporal Action Proposal Generation ECCV code 18\nDVAE#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors NIPS code 18\nConditional Image-Text Embedding Networks ECCV code 18\nEPINET: A Fully-Convolutional Neural Network Using Epipolar Geometry for Depth From Light Field Images CVPR code 18\nGlimpse Clouds: Human Activity Recognition From Unstructured Feature Points CVPR code 18\nBayesian Optimization of Combinatorial Structures ICML code 18\nFeaStNet: Feature-Steered Graph Convolutions for 3D Shape Analysis CVPR code 18\nLearning Type-Aware Embeddings for Fashion Compatibility ECCV code 17\nSliced Wasserstein Distance for Learning Gaussian Mixture Models CVPR code 17\nRevisiting Deep Intrinsic Image Decompositions CVPR code 17\nA Spectral Approach to Gradient Estimation for Implicit Distributions ICML code 17\nHierarchical Novelty Detection for Visual Object Recognition CVPR code 17\nTotal Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies CVPR code 17\nLearning Generative ConvNets via Multi-Grid Modeling and Sampling CVPR code 17\nLearning 3D Shape Completion From Laser Scan Data With Weak Supervision CVPR code 17\nTriplet Loss in Siamese Network for Object Tracking ECCV code 17\nAdversarial Attack on Graph Structured Data ICML code 17\nArbitrary Style Transfer With Deep Feature Reshuffle CVPR code 17\nVisual Question Reasoning on General Dependency Tree CVPR code 17\nPredicting Gaze in Egocentric Video by Learning Task-dependent Attention Transition ECCV code 16\nLipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks NIPS code 16\nCoded Sparse Matrix Multiplication ICML code 16\nWeakly-Supervised Action Segmentation With Iterative Soft Boundary Assignment CVPR code 16\nRecovering 3D Planes from a Single Image via Convolutional Neural Networks ECCV code 16\nSegStereo: Exploiting Semantic Information for Disparity Estimation ECCV code 16\nFunctional Gradient Boosting based on Residual Network Perception ICML code 16\nNAG: Network for Adversary Generation CVPR code 16\nGenerative Probabilistic Novelty Detection with Adversarial Autoencoders NIPS code 16\nHashing as Tie-Aware Learning to Rank CVPR code 15\nPose Proposal Networks ECCV code 15\nConvolutional Sequence to Sequence Model for Human Dynamics CVPR code 15\nJoint Pose and Expression Modeling for Facial Expression Recognition CVPR code 15\nGrounding Referring Expressions in Images by Variational Context CVPR code 15\nRethinking the Form of Latent States in Image Captioning ECCV code 15\nOpen Set Domain Adaptation by Backpropagation ECCV code 15\nNeural Sign Language Translation CVPR code 15\nSpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters ECCV code 15\nEfficient Neural Audio Synthesis ICML code 15\nDeep Learning Under Privileged Information Using Heteroscedastic Dropout CVPR code 14\nImage Transformer ICML code 14\nLearning to Understand Image Blur CVPR code 14\nLearning and Using the Arrow of Time CVPR code 14\nAction Sets: Weakly Supervised Action Segmentation Without Ordering Constraints CVPR code 14\nLearning to Forecast and Refine Residual Motion for Image-to-Video Generation ECCV code 14\nMulti-Scale Weighted Nuclear Norm Image Restoration CVPR code 14\nSynthesizing Robust Adversarial Examples ICML code 13\nFine-Grained Visual Categorization using Meta-Learning Optimization with Sample Selection of Auxiliary Data ECCV code 13\nAssessing Generative Models via Precision and Recall NIPS code 13\nDeep Diffeomorphic Transformer Networks CVPR code 13\nLearning by Asking Questions CVPR code 13\nTowards Human-Machine Cooperation: Self-Supervised Sample Mining for Object Detection CVPR code 13\nVariational Autoencoders for Deforming 3D Mesh Models CVPR code 13\nMin-Entropy Latent Model for Weakly Supervised Object Detection CVPR code 13\nBottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering CVPR code 13\nGradient-Based Meta-Learning with Learned Layerwise Metric and Subspace ICML code 13\nLearning a Discriminative Filter Bank Within a CNN for Fine-Grained Recognition CVPR code 13\nFinding Influential Training Samples for Gradient Boosted Decision Trees ICML code 13\nGesture Recognition: Focus on the Hands CVPR code 12\nCross-View Image Synthesis Using Conditional GANs CVPR code 12\nJoint Optimization Framework for Learning With Noisy Labels CVPR code 12\nFuture Person Localization in First-Person Videos CVPR code 12\nAutoLoc: Weakly-supervised Temporal Action Localization in Untrimmed Videos ECCV code 12\nLearning Transferable Architectures for Scalable Image Recognition CVPR code 12\nClipped Action Policy Gradient ICML code 12\nMix and Match Networks: Encoder-Decoder Alignment for Zero-Pair Image Translation CVPR code 12\nDecouple Learning for Parameterized Image Operators ECCV code 12\nGeneralized Earley Parser: Bridging Symbolic Grammars and Sequence Data for Future Prediction ICML code 12\nAdaptive Skip Intervals: Temporal Abstraction for Recurrent Dynamical Models NIPS code 12\nAMNet: Memorability Estimation With Attention CVPR code 12\nAdversarial Time-to-Event Modeling ICML code 12\nReversible Recurrent Neural Networks NIPS code 12\nHuman Pose Estimation With Parsing Induced Learner CVPR code 11\nShapeStacks: Learning Vision-Based Physical Intuition for Generalised Object Stacking ECCV code 11\nA Joint Sequence Fusion Model for Video Question Answering and Retrieval ECCV code 11\nLearning Face Age Progression: A Pyramid Architecture of GANs CVPR code 11\nRobust Physical-World Attacks on Deep Learning Visual Classification CVPR code 11\nHigh-Quality Prediction Intervals for Deep Learning: A Distribution-Free, Ensembled Approach ICML code 11\nMeta-Learning by Adjusting Priors Based on Extended PAC-Bayes Theory ICML code 11\nMultimodal Explanations: Justifying Decisions and Pointing to the Evidence CVPR code 11\nAccelerating Natural Gradient with Higher-Order Invariance ICML code 11\nHierarchical Multi-Label Classification Networks ICML code 11\nConvolutional Image Captioning CVPR code 11\nBoosting Domain Adaptation by Discovering Latent Domains CVPR code 11\nLogo Synthesis and Manipulation With Clustered Generative Adversarial Networks CVPR code 10\nPacGAN: The power of two samples in generative adversarial networks NIPS code 10\nAttention Clusters: Purely Attention Based Local Feature Integration for Video Classification CVPR code 10\nEnd-to-End Incremental Learning ECCV code 10\nMulti-Oriented Scene Text Detection via Corner Localization and Region Segmentation CVPR code 10\nOn GANs and GMMs NIPS code 10\nSalient Object Detection Driven by Fixation Prediction CVPR code 9\nSemantic Video Segmentation by Gated Recurrent Flow Propagation CVPR code 9\nConstraint-Aware Deep Neural Network Compression ECCV code 9\nStatistically-motivated Second-order Pooling ECCV code 9\nExcitation Backprop for RNNs CVPR code 9\nAnalyzing Uncertainty in Neural Machine Translation ICML code 9\nLearning Dynamics of Linear Denoising Autoencoders ICML code 9\nSaliency Detection in 360\u00b0 Videos ECCV code 9\nDensity Adaptive Point Set Registration CVPR code 9\nDecoupled Parallel Backpropagation with Convergence Guarantee ICML code 9\nClassification from Pairwise Similarity and Unlabeled Data ICML code 9\noi-VAE: Output Interpretable VAEs for Nonlinear Group Factor Analysis ICML code 9\nModeling Sparse Deviations for Compressed Sensing using Generative Models ICML code 9\nPixels, Voxels, and Views: A Study of Shape Representations for Single View 3D Object Shape Prediction CVPR code 9\nTowards Open-Set Identity Preserving Face Synthesis CVPR code 9\nFive-Point Fundamental Matrix Estimation for Uncalibrated Cameras CVPR code 8\nBourGAN: Generative Networks with Metric Embeddings NIPS code 8\nFast Information-theoretic Bayesian Optimisation ICML code 8\nDeep Variational Reinforcement Learning for POMDPs ICML code 8\nSpecular-to-Diffuse Translation for Multi-View Reconstruction ECCV code 8\nDynamic Conditional Networks for Few-Shot Learning ECCV code 8\nLearning Facial Action Units From Web Images With Scalable Weakly Supervised Clustering CVPR code 8\nHigh-Resolution Image Synthesis and Semantic Manipulation With Conditional GANs CVPR code 8\nDeep Defense: Training DNNs with Improved Adversarial Robustness NIPS code 8\nLearning K-way D-dimensional Discrete Codes for Compact Embedding Representations ICML code 8\nLight Structure from Pin Motion: Simple and Accurate Point Light Calibration for Physics-based Modeling ECCV code 7\nNon-metric Similarity Graphs for Maximum Inner Product Search NIPS code 7\nTowards Realistic Predictors ECCV code 7\nDeep Non-Blind Deconvolution via Generalized Low-Rank Approximation NIPS code 7\nDon\u2019t Just Assume Look and Answer: Overcoming Priors for Visual Question Answering CVPR code 7\nLearning Dual Convolutional Neural Networks for Low-Level Vision CVPR code 7\nThe Mirage of Action-Dependent Baselines in Reinforcement Learning ICML code 7\nDVQA: Understanding Data Visualizations via Question Answering CVPR code 7\nA Two-Step Disentanglement Method CVPR code 7\nDetecting and Correcting for Label Shift with Black Box Predictors ICML code 7\nConditional Prior Networks for Optical Flow ECCV code 7\nGenerative Adversarial Learning Towards Fast Weakly Supervised Detection CVPR code 7\nAdversarial Learning with Local Coordinate Coding ICML code 7\nStochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional Networks CVPR code 7\nAttnGAN: Fine-Grained Text to Image Generation With Attentional Generative Adversarial Networks CVPR code 7\nLearning to Explain: An Information-Theoretic Perspective on Model Interpretation ICML code 7\nBanach Wasserstein GAN NIPS code 7\nGradually Updated Neural Networks for Large-Scale Image Recognition ICML code 7\nLearning Steady-States of Iterative Algorithms over Graphs ICML code 7\nProgressive Attention Guided Recurrent Network for Salient Object Detection CVPR code 7\nZoom and Learn: Generalizing Deep Stereo Matching to Novel Domains CVPR code 6\nUnsupervised holistic image generation from key local patches ECCV code 6\nInner Space Preserving Generative Pose Machine ECCV code 6\nBilevel Programming for Hyperparameter Optimization and Meta-Learning ICML code 6\nOptical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition CVPR code 6\nBreaking the Activation Function Bottleneck through Adaptive Parameterization NIPS code 6\nUltra Large-Scale Feature Selection using Count-Sketches ICML code 6\nDynamic Scene Deblurring Using Spatially Variant Recurrent Neural Networks CVPR code 6\nOrthogonally Decoupled Variational Gaussian Processes NIPS code 6\nBatch Bayesian Optimization via Multi-objective Acquisition Ensemble for Automated Analog Circuit Design ICML code 6\nA Modulation Module for Multi-task Learning with Applications in Image Retrieval ECCV code 6\nA Memory Network Approach for Story-Based Temporal Summarization of 360\u00b0 Videos CVPR code 6\nTowards Effective Low-Bitwidth Convolutional Neural Networks CVPR code 5\nDisentangling Factors of Variation by Mixing Them CVPR code 5\nWeakly-supervised Video Summarization using Variational Encoder-Decoder and Web Prior ECCV code 5\nLearning Longer-term Dependencies in RNNs with Auxiliary Losses ICML code 5\nContour Knowledge Transfer for Salient Object Detection ECCV code 5\nHybridNet: Classification and Reconstruction Cooperation for Semi-Supervised Learning ECCV code 5\nSidekick Policy Learning for Active Visual Exploration ECCV code 5\nLearning to Localize Sound Source in Visual Scenes CVPR code 5\nNeural Architecture Optimization NIPS code 5\nCOLA: Decentralized Linear Learning NIPS code 5\nDiverse and Coherent Paragraph Generation from Images ECCV code 5\nDRACO: Byzantine-resilient Distributed Training via Redundant Gradients ICML code 5\nInter and Intra Topic Structure Learning with Word Embeddings ICML code 5\nEstimating the Success of Unsupervised Image to Image Translation ECCV code 5\nDynamic-Structured Semantic Propagation Network CVPR code 5\nThe Description Length of Deep Learning models NIPS code 5\nStereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving ECCV code 5\nBlind Justice: Fairness with Encrypted Sensitive Attributes ICML code 5\nTransfer Learning via Learning to Transfer ICML code 5\nDeepcode: Feedback Codes via Deep Learning NIPS code 4\nConfigurable Markov Decision Processes ICML code 4\nA Framework for Evaluating 6-DOF Object Trackers ECCV code 4\nDifferentially Private Database Release via Kernel Mean Embeddings ICML code 4\nRecognizing Human Actions as the Evolution of Pose Estimation Maps CVPR code 4\nConnecting Pixels to Privacy and Utility: Automatic Redaction of Private Information in Images CVPR code 4\nDeLS-3D: Deep Localization and Segmentation With a 3D Semantic Map CVPR code 4\nGeolocation Estimation of Photos using a Hierarchical Model and Scene Classification ECCV code 4\nTracking Emerges by Colorizing Videos ECCV code 4\nDiverse Conditional Image Generation by Stochastic Regression with Latent Drop-Out Codes ECCV code 4\nInference Suboptimality in Variational Autoencoders ICML code 4\nBlack Box FDR ICML code 4\nFeedback-Prop: Convolutional Neural Network Inference Under Partial Evidence CVPR code 4\nQuadrature-based features for kernel approximation NIPS code 4\nJoint Representation and Truncated Inference Learning for Correlation Filter based Tracking ECCV code 4\nTransferable Adversarial Perturbations ECCV code 4\nSingle Image Water Hazard Detection using FCN with Reflection Attention Units ECCV code 4\nMultimodal Generative Models for Scalable Weakly-Supervised Learning NIPS code 4\nImportance Weighted Transfer of Samples in Reinforcement Learning ICML code 3\nFeature Generating Networks for Zero-Shot Learning CVPR code 3\nDICOD: Distributed Convolutional Coordinate Descent for Convolutional Sparse Coding ICML code 3\nCapProNet: Deep Feature Learning via Orthogonal Projections onto Capsule Subspaces NIPS code 3\nBidirectional Retrieval Made Simple CVPR code 3\nMultilingual Anchoring: Interactive Topic Modeling and Alignment Across Languages NIPS code 3\nA Hybrid l1-l0 Layer Decomposition Model for Tone Mapping CVPR code 3\nSpatially-Adaptive Filter Units for Deep Neural Networks CVPR code 3\nLearning to Branch ICML code 3\nExplanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives NIPS code 3\nLifelong Learning via Progressive Distillation and Retrospection ECCV code 3\nCLEAR: Cumulative LEARning for One-Shot One-Class Image Recognition CVPR code 3\nNot to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care ICML code 3\nLearning Answer Embeddings for Visual Question Answering CVPR code 3\nInformation Constraints on Auto-Encoding Variational Bayes NIPS code 3\nParallel Bayesian Network Structure Learning ICML code 3\nRing Loss: Convex Feature Normalization for Face Recognition CVPR code 3\nTeaching Categories to Human Learners With Visual Explanations CVPR code 3\nStabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization ICML code 3\nDeep Burst Denoising ECCV code 3\nConvergent Tree Backup and Retrace with Function Approximation ICML code 3\nGaze Prediction in Dynamic 360\u00b0 Immersive Videos CVPR code 3\nStatistical Recurrent Models on Manifold valued Data NIPS code 3\nEnd-to-End Flow Correlation Tracking With Spatial-Temporal Attention CVPR code 3\n\u21a5 back to top\n2017\nTitle Conf Code Stars\nBridging the Gap Between Value and Policy Based Reinforcement Learning NIPS code 46593\nREBAR: Low-variance, unbiased gradient estimates for discrete latent variable models NIPS code 46593\nFocal Loss for Dense Object Detection ICCV code 18356\nMask R-CNN ICCV code 9493\nDeep Photo Style Transfer CVPR code 8655\nLightGBM: A Highly Efficient Gradient Boosting Decision Tree NIPS code 7536\nScalable trust-region method for deep reinforcement learning using Kronecker-factored approximation NIPS code 6449\nAttention is All you Need NIPS code 6288\nLarge Pose 3D Face Reconstruction From a Single Image via Direct Volumetric CNN Regression ICCV code 3354\nDensely Connected Convolutional Networks CVPR code 3130\nA Unified Approach to Interpreting Model Predictions NIPS code 3122\nDeformable Convolutional Networks ICCV code 2165\nELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games NIPS code 1823\nPointNet: Deep Learning on Point Sets for 3D Classification and Segmentation CVPR code 1523\nImproved Training of Wasserstein GANs NIPS code 1405\nFully Convolutional Instance-Aware Semantic Segmentation CVPR code 1395\nAggregated Residual Transformations for Deep Neural Networks CVPR code 1361\nPhoto-Realistic Single Image Super-Resolution Using a Generative Adversarial Network CVPR code 1301\nUnsupervised Image-to-Image Translation Networks NIPS code 1205\nPhotographic Image Synthesis With Cascaded Refinement Networks ICCV code 1142\nHigh-Resolution Image Inpainting Using Multi-Scale Neural Patch Synthesis CVPR code 1072\nSphereFace: Deep Hypersphere Embedding for Face Recognition CVPR code 1048\nDeep Feature Flow for Video Recognition CVPR code 966\nBayesian GAN NIPS code 942\nPyramid Scene Parsing Network CVPR code 934\nEfficient Modeling of Latent Information in Supervised Learning using Gaussian Processes NIPS code 906\nFinding Tiny Faces CVPR code 856\nToward Multimodal Image-to-Image Translation NIPS code 794\nLearning to Discover Cross-Domain Relations with Generative Adversarial Networks ICML code 784\nYOLO9000: Better, Faster, Stronger CVPR code 773\nPointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space NIPS code 772\nModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks ICML code 729\nFlowNet 2.0: Evolution of Optical Flow Estimation With Deep Networks CVPR code 720\nChannel Pruning for Accelerating Very Deep Neural Networks ICCV code 649\nDilated Residual Networks CVPR code 640\nInferring and Executing Programs for Visual Reasoning ICCV code 636\nDSOD: Learning Deeply Supervised Object Detectors From Scratch ICCV code 582\nArbitrary Style Transfer in Real-Time With Adaptive Instance Normalization ICCV code 572\nAccelerating Eulerian Fluid Simulation With Convolutional Networks ICML code 570\nLearning Disentangled Representations with Semi-Supervised Deep Generative Models NIPS code 556\nInductive Representation Learning on Large Graphs NIPS code 552\nRegressing Robust and Discriminative 3D Morphable Models With a Very Deep Neural Network CVPR code 537\nHow Far Are We From Solving the 2D & 3D Face Alignment Problem? (And a Dataset of 230,000 3D Facial Landmarks) ICCV code 526\nSSH: Single Stage Headless Face Detector ICCV code 515\nLearning From Simulated and Unsupervised Images Through Adversarial Training CVPR code 492\nPlug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space CVPR code 487\nVideo Frame Interpolation via Adaptive Convolution CVPR code 482\nVideo Frame Interpolation via Adaptive Separable Convolution ICCV code 482\nGMS: Grid-based Motion Statistics for Fast, Ultra-Robust Feature Correspondence CVPR code 460\nJoint Detection and Identification Feature Learning for Person Search CVPR code 459\nDual Path Networks NIPS code 451\nFlow-Guided Feature Aggregation for Video Object Detection ICCV code 436\nDeep Image Matting CVPR code 434\nRicher Convolutional Features for Edge Detection CVPR code 399\nAnnotating Object Instances With a Polygon-RNN CVPR code 397\nRecurrent Highway Networks ICML code 397\nDetect to Track and Track to Detect ICCV code 387\nRefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation CVPR code 379\nDetecting Oriented Text in Natural Images by Linking Segments CVPR code 364\nDeep Lattice Networks and Partial Monotonic Functions NIPS code 349\nMean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results NIPS code 347\nRON: Reverse Connection With Objectness Prior Networks for Object Detection CVPR code 345\nUniversal Style Transfer via Feature Transforms NIPS code 344\nResidual Attention Network for Image Classification CVPR code 329\nOne-Shot Video Object Segmentation CVPR code 316\nAccurate Single Stage Detector Using Recurrent Rolling Convolution CVPR code 314\nFeature Pyramid Networks for Object Detection CVPR code 310\nEfficient softmax approximation for GPUs ICML code 304\nOctNet: Learning Deep 3D Representations at High Resolutions CVPR code 302\nDeep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution CVPR code 301\nPixel Recursive Super Resolution ICCV code 301\nSelf-Critical Sequence Training for Image Captioning CVPR code 299\nAge Progression/Regression by Conditional Adversarial Autoencoder CVPR code 297\nStyle Transfer from Non-Parallel Text by Cross-Alignment NIPS code 296\nDilated Recurrent Neural Networks NIPS code 285\nLifting From the Deep: Convolutional 3D Pose Estimation From a Single Image CVPR code 280\nDeepBach: a Steerable Model for Bach Chorales Generation ICML code 276\nThe Predictron: End-To-End Learning and Planning ICML code 274\nConvolutional Sequence to Sequence Learning ICML code 258\nOptNet: Differentiable Optimization as a Layer in Neural Networks ICML code 245\nPrototypical Networks for Few-shot Learning NIPS code 244\nDeep Voice: Real-time Neural Text-to-Speech ICML code 242\nReinforcement Learning with Deep Energy-Based Policies ICML code 233\nLearning Deep CNN Denoiser Prior for Image Restoration CVPR code 231\nGANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium NIPS code 229\nA Point Set Generation Network for 3D Object Reconstruction From a Single Image CVPR code 228\nDeeply Supervised Salient Object Detection With Short Connections CVPR code 228\nBlitzNet: A Real-Time Deep Network for Scene Understanding ICCV code 227\nLanguage Modeling with Gated Convolutional Networks ICML code 221\nUnlabeled Samples Generated by GAN Improve the Person Re-Identification Baseline in Vitro ICCV code 215\nStacked Generative Adversarial Networks CVPR code 215\nRMPE: Regional Multi-Person Pose Estimation ICCV code 215\nKnowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning CVPR code 214\nGenerative Face Completion CVPR code 212\nVPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition ICCV code 210\nThe Reversible Residual Network: Backpropagation Without Storing Activations NIPS code 210\nRecurrent Scale Approximation for Object Detection in CNN ICCV code 209\nLearning From Synthetic Humans CVPR code 207\nSpatially Adaptive Computation Time for Residual Networks CVPR code 203\nBeyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis ICCV code 202\n3D Bounding Box Estimation Using Deep Learning and Geometry CVPR code 200\nMulti-View 3D Object Detection Network for Autonomous Driving CVPR code 199\nVisual Dialog CVPR code 199\nInterpretable Explanations of Black Boxes by Meaningful Perturbation ICCV code 192\nInverse Compositional Spatial Transformer Networks CVPR code 189\nFastMask: Segment Multi-Scale Object Candidates in One Shot CVPR code 189\nOnACID: Online Analysis of Calcium Imaging Data in Real Time NIPS code 189\nSemantic Scene Completion From a Single Depth Image CVPR code 188\nLearning Efficient Convolutional Networks Through Network Slimming ICCV code 186\nLearning Feature Pyramids for Human Pose Estimation ICCV code 185\nBe Your Own Prada: Fashion Synthesis With Structural Coherence ICCV code 183\nScene Graph Generation by Iterative Message Passing CVPR code 182\nFast Image Processing With Fully-Convolutional Networks ICCV code 180\nLearning Multiple Tasks with Multilinear Relationship Networks NIPS code 178\nLearning to Reason: End-To-End Module Networks for Visual Question Answering ICCV code 178\nSingle Shot Text Detector With Regional Attention ICCV code 176\nBinarized Convolutional Landmark Localizers for Human Pose Estimation and Face Alignment With Limited Resources ICCV code 175\nDeep Feature Interpolation for Image Content Changes CVPR code 170\nOn Human Motion Prediction Using Recurrent Neural Networks CVPR code 167\nImage Super-Resolution via Deep Recursive Residual Network CVPR code 163\nLearning Cross-Modal Embeddings for Cooking Recipes and Food Images CVPR code 160\nInput Convex Neural Networks ICML code 159\nSimple Does It: Weakly Supervised Instance and Semantic Segmentation CVPR code 159\nLow-Shot Visual Recognition by Shrinking and Hallucinating Features ICCV code 158\nOriented Response Networks CVPR code 157\nSoft Proposal Networks for Weakly Supervised Object Localization ICCV code 154\nAdversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks ICML code 147\nAxiomatic Attribution for Deep Networks ICML code 146\nGradient Episodic Memory for Continual Learning NIPS code 146\nDSAC - Differentiable RANSAC for Camera Localization CVPR code 144\nAttend to You: Personalized Image Captioning With Context Sequence Memory Networks CVPR code 143\nConditional Similarity Networks CVPR code 142\nLanguage Modeling with Recurrent Highway Hypernetworks NIPS code 141\nTriple Generative Adversarial Nets NIPS code 138\nInterpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning NIPS code 138\nOne-Sided Unsupervised Domain Mapping NIPS code 137\nDetecting Visual Relationships With Deep Relational Networks CVPR code 137\nAttentive Recurrent Comparators ICML code 136\nTowards 3D Human Pose Estimation in the Wild: A Weakly-Supervised Approach ICCV code 136\nLearning a Multi-View Stereo Machine NIPS code 135\nDeep Learning for Precipitation Nowcasting: A Benchmark and A New Model NIPS code 134\nMulti-Context Attention for Human Pose Estimation CVPR code 131\nControlling Perceptual Factors in Neural Style Transfer CVPR code 130\nBayesian Compression for Deep Learning NIPS code 130\nAdversarial Discriminative Domain Adaptation CVPR code 129\nWorking hard to know your neighbor's margins: Local descriptor learning loss NIPS code 128\nConcrete Dropout NIPS code 127\nSegFlow: Joint Learning for Video Object Segmentation and Optical Flow ICCV code 127\nSegmentation-Aware Convolutional Networks Using Local Attention Masks ICCV code 126\nDetail-Revealing Deep Video Super-Resolution ICCV code 126\nCREST: Convolutional Residual Learning for Visual Tracking ICCV code 126\nDiscriminative Correlation Filter With Channel and Spatial Reliability CVPR code 124\nSVDNet for Pedestrian Retrieval ICCV code 121\nSemantic Image Synthesis via Adversarial Learning ICCV code 121\nSpatiotemporal Multiplier Networks for Video Action Recognition CVPR code 121\nPoseTrack: Joint Multi-Person Pose Estimation and Tracking CVPR code 121\nHierarchical Attentive Recurrent Tracking NIPS code 121\nGood Semi-supervised Learning That Requires a Bad GAN NIPS code 120\nDeep Watershed Transform for Instance Segmentation CVPR code 120\nAssociative Domain Adaptation ICCV code 119\nLearning by Association -- A Versatile Semi-Supervised Training Method for Neural Networks CVPR code 119\nValue Prediction Network NIPS code 119\nUnrestricted Facial Geometry Reconstruction Using Image-To-Image Translation ICCV code 119\nMemNet: A Persistent Memory Network for Image Restoration ICCV code 119\nBayesian Optimization with Gradients NIPS code 117\nTernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning NIPS code 117\nCompressed Sensing using Generative Models ICML code 116\nSwitching Convolutional Neural Network for Crowd Counting CVPR code 116\nWILDCAT: Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation CVPR code 116\nShow, Adapt and Tell: Adversarial Training of Cross-Domain Image Captioner ICCV code 115\nVideo Frame Synthesis Using Deep Voxel Flow ICCV code 114\nMultiple Instance Detection Network With Online Instance Classifier Refinement CVPR code 113\nDeep Pyramidal Residual Networks CVPR code 112\nTrain longer, generalize better: closing the generalization gap in large batch training of neural networks NIPS code 112\nSplit-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction CVPR code 110\nUnite the People: Closing the Loop Between 3D and 2D Human Representations CVPR code 110\nLearning Combinatorial Optimization Algorithms over Graphs NIPS code 109\nFeUdal Networks for Hierarchical Reinforcement Learning ICML code 107\nThiNet: A Filter Level Pruning Method for Deep Neural Network Compression ICCV code 105\nLearning a Deep Embedding Model for Zero-Shot Learning CVPR code 104\nECO: Efficient Convolution Operators for Tracking CVPR code 103\nSCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning CVPR code 102\nMulti-View Supervision for Single-View Reconstruction via Differentiable Ray Consistency CVPR code 100\nTask-based End-to-end Model Learning in Stochastic Optimization NIPS code 100\nLearning to Compose Domain-Specific Transformations for Data Augmentation NIPS code 97\nGenetic CNN ICCV code 97\nHashNet: Deep Learning to Hash by Continuation ICCV code 97\nInterleaved Group Convolutions ICCV code 95\nDeeply-Learned Part-Aligned Representations for Person Re-Identification ICCV code 95\nBest of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model NIPS code 94\nMulti-Scale Continuous CRFs as Sequential Deep Networks for Monocular Depth Estimation CVPR code 93\nOctree Generating Networks: Efficient Convolutional Architectures for High-Resolution 3D Outputs ICCV code 92\nSemantic Autoencoder for Zero-Shot Learning CVPR code 92\nDeep Hyperspherical Learning NIPS code 92\nDecoupled Neural Interfaces using Synthetic Gradients ICML code 90\nGeometric Matrix Completion with Recurrent Multi-Graph Neural Networks NIPS code 90\nPractical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search NIPS code 90\nOptical Flow Estimation Using a Spatial Pyramid Network CVPR code 90\nAMC: Attention guided Multi-modal Correlation Learning for Image Search CVPR code 90\nDeep Video Deblurring for Hand-Held Cameras CVPR code 89\nUnsupervised Learning of Disentangled and Interpretable Representations from Sequential Data NIPS code 88\nCausal Effect Inference with Deep Latent-Variable Models NIPS code 87\nGANs for Biological Image Synthesis ICCV code 85\nMMD GAN: Towards Deeper Understanding of Moment Matching Network NIPS code 84\nRepresentation Learning by Learning to Count ICCV code 84\nOptical Flow in Mostly Rigid Scenes CVPR code 83\nFast-Slow Recurrent Neural Networks NIPS code 82\nUnsupervised Video Summarization With Adversarial LSTM Networks CVPR code 82\nConstrained Policy Optimization ICML code 81\nA-NICE-MC: Adversarial Training for MCMC NIPS code 80\nCoarse-To-Fine Volumetric Prediction for Single-Image 3D Human Pose CVPR code 80\nEnd-To-End Instance Segmentation With Recurrent Attention CVPR code 78\nDeLiGAN : Generative Adversarial Networks for Diverse and Limited Data CVPR code 78\nLearning Shape Abstractions by Assembling Volumetric Primitives CVPR code 77\nLocal Binary Convolutional Neural Networks CVPR code 77\nRaster-To-Vector: Revisiting Floorplan Transformation ICCV code 76\nPositive-Unlabeled Learning with Non-Negative Risk Estimator NIPS code 76\nHard-Aware Deeply Cascaded Embedding ICCV code 75\nDeep Image Harmonization CVPR code 73\nShape Completion Using 3D-Encoder-Predictor CNNs and Shape Synthesis CVPR code 73\nNot All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade CVPR code 73\nImproved Stereo Matching With Constant Highway Networks and Reflective Confidence Learning CVPR code 72\nQuery-Guided Regression Network With Context Policy for Phrase Grounding ICCV code 72\nTop-Down Visual Saliency Guided by Captions CVPR code 72\nFeedback Networks CVPR code 72\nWhat Actions Are Needed for Understanding Human Actions in Videos? ICCV code 71\nXception: Deep Learning With Depthwise Separable Convolutions CVPR code 71\nAction-Decision Networks for Visual Tracking With Deep Reinforcement Learning CVPR code 71\nVideo Propagation Networks CVPR code 70\nImage-To-Image Translation With Conditional Adversarial Networks CVPR code 70\nQuality Aware Network for Set to Set Recognition CVPR code 69\nSelf-Supervised Learning of Visual Features Through Embedding Images Into Text Topic Spaces CVPR code 69\nDeep Subspace Clustering Networks NIPS code 68\nEscape From Cells: Deep Kd-Networks for the Recognition of 3D Point Cloud Models ICCV code 68\nA Distributional Perspective on Reinforcement Learning ICML code 68\nPhysically-Based Rendering for Indoor Scene Understanding Using Convolutional Neural Networks CVPR code 67\nDeep Transfer Learning with Joint Adaptation Networks ICML code 67\nTraining Deep Networks without Learning Rates Through Coin Betting NIPS code 66\nFull Resolution Image Compression With Recurrent Neural Networks CVPR code 66\nSurfaceNet: An End-To-End 3D Neural Network for Multiview Stereopsis ICCV code 66\nDoubly Stochastic Variational Inference for Deep Gaussian Processes NIPS code 66\nTURN TAP: Temporal Unit Regression Network for Temporal Action Proposals ICCV code 66\nJointly Attentive Spatial-Temporal Pooling Networks for Video-Based Person Re-Identification ICCV code 65\nSynthesizing 3D Shapes via Modeling Multi-View Depth Maps and Silhouettes With Deep Generative Networks CVPR code 65\nDance Dance Convolution ICML code 65\nBorrowing Treasures From the Wealthy: Deep Transfer Learning Through Selective Joint Fine-Tuning CVPR code 64\nCurriculum Domain Adaptation for Semantic Segmentation of Urban Scenes ICCV code 64\nToward Controlled Generation of Text ICML code 63\nPerson Re-Identification in the Wild CVPR code 63\nALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching NIPS code 63\nDifferentiable Learning of Logical Rules for Knowledge Base Reasoning NIPS code 62\nPerson Search With Natural Language Description CVPR code 61\nMulti-Channel Weighted Nuclear Norm Minimization for Real Color Image Denoising ICCV code 61\nPlaying for Benchmarks ICCV code 61\nUnsupervised Learning by Predicting Noise ICML code 60\nLocalizing Moments in Video With Natural Language ICCV code 60\nEnd-To-End 3D Face Reconstruction With Deep Neural Networks CVPR code 60\nCoupleNet: Coupling Global Structure With Local Parts for Object Detection ICCV code 59\nAdaGAN: Boosting Generative Models NIPS code 59\nConvolutional Gaussian Processes NIPS code 57\nA Deep Regression Architecture With Two-Stage Re-Initialization for High Performance Facial Landmark Detection CVPR code 57\nModeling Relationships in Referential Expressions With Compositional Modular Networks CVPR code 57\nCuriosity-driven Exploration by Self-supervised Prediction ICML code 56\nWavelet-SRNet: A Wavelet-Based CNN for Multi-Scale Face Super Resolution ICCV code 56\nThe Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process NIPS code 56\nOnline and Linear-Time Attention by Enforcing Monotonic Alignments ICML code 56\nNeural Expectation Maximization NIPS code 56\nDense-Captioning Events in Videos ICCV code 55\nFactorized Bilinear Models for Image Recognition ICCV code 55\nNet-Trim: Convex Pruning of Deep Neural Networks with Performance Guarantee NIPS code 54\nOn-the-fly Operation Batching in Dynamic Computation Graphs NIPS code 54\nVisual Translation Embedding Network for Visual Relation Detection CVPR code 54\nLearning Blind Motion Deblurring ICCV code 54\nA Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning NIPS code 53\nTowards Diverse and Natural Image Descriptions via a Conditional GAN ICCV code 53\nCDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos CVPR code 53\nA Generic Deep Architecture for Single Image Reflection Removal and Image Smoothing ICCV code 52\nDeep IV: A Flexible Approach for Counterfactual Prediction ICML code 52\nTriangle Generative Adversarial Networks NIPS code 51\nEAST: An Efficient and Accurate Scene Text Detector CVPR code 51\nSST: Single-Stream Temporal Action Proposals CVPR code 51\nPredicting Deeper Into the Future of Semantic Segmentation ICCV code 51\nL2-Net: Deep Learning of Discriminative Patch Descriptor in Euclidean Space CVPR code 51\nTALL: Temporal Activity Localization via Language Query ICCV code 50\nHybrid Reward Architecture for Reinforcement Learning NIPS code 50\nFast Fourier Color Constancy CVPR code 49\nModulating early visual processing by language NIPS code 49\nAdversarial Examples for Semantic Segmentation and Object Detection ICCV code 49\nLearning Discrete Representations via Information Maximizing Self-Augmented Training ICML code 49\nEfficient Diffusion on Region Manifolds: Recovering Small Objects With Compact CNN Representations CVPR code 48\nReal Time Image Saliency for Black Box Classifiers NIPS code 48\nFC4: Fully Convolutional Color Constancy With Confidence-Weighted Pooling CVPR code 47\nMultiple People Tracking by Lifted Multicut and Person Re-Identification CVPR code 47\nLearned D-AMP: Principled Neural Network based Compressive Image Recovery NIPS code 47\nGP CaKe: Effective brain connectivity with causal kernels NIPS code 46\nPredicting Organic Reaction Outcomes with Weisfeiler-Lehman Network NIPS code 46\nSemantic Video CNNs Through Representation Warping ICCV code 46\nGrammar Variational Autoencoder ICML code 46\nEnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis ICCV code 46\nSafe Model-based Reinforcement Learning with Stability Guarantees NIPS code 45\nDeep Spectral Clustering Learning ICML code 45\nSemantic Compositional Networks for Visual Captioning CVPR code 45\nOn-Demand Learning for Deep Image Restoration ICCV code 45\nVideo Pixel Networks ICML code 45\nStabilizing Training of Generative Adversarial Networks through Regularization NIPS code 45\nStructured Bayesian Pruning via Log-Normal Multiplicative Noise NIPS code 44\nDeriving Neural Architectures from Sequence and Graph Kernels ICML code 44\nMasked Autoregressive Flow for Density Estimation NIPS code 44\nUnsupervised Adaptation for Deep Stereo ICCV code 44\nLearning Residual Images for Face Attribute Manipulation CVPR code 43\nLearning to Generate Long-term Future via Hierarchical Prediction ICML code 43\nAccurate Optical Flow via Direct Cost Volume Processing CVPR code 42\nGeneralized Orderless Pooling Performs Implicit Salient Matching ICCV code 42\nComparative Evaluation of Hand-Crafted and Learned Local Features CVPR code 42\nSchNet: A continuous-filter convolutional neural network for modeling quantum interactions NIPS code 41\nTemporal Generative Adversarial Nets With Singular Value Clipping ICCV code 41\nMultiplicative Normalizing Flows for Variational Bayesian Neural Networks ICML code 41\nNeural Scene De-Rendering CVPR code 40\nSemantic Image Inpainting With Deep Generative Models CVPR code 40\nA Linear-Time Kernel Goodness-of-Fit Test NIPS code 40\nLeast Squares Generative Adversarial Networks ICCV code 39\nDiversified Texture Synthesis With Feed-Forward Networks CVPR code 39\nNo Fuss Distance Metric Learning Using Proxies ICCV code 38\nTemplate Matching With Deformable Diversity Similarity CVPR code 38\nWhat's in a Question: Using Visual Questions as a Form of Supervision CVPR code 38\nFace Normals \"In-The-Wild\" Using Fully Convolutional Networks CVPR code 38\nConditional Image Synthesis with Auxiliary Classifier GANs ICML code 37\nNeural Episodic Control ICML code 37\n3D-PRNN: Generating Shape Primitives With Recurrent Neural Networks ICCV code 37\nStructured Embedding Models for Grouped Data NIPS code 36\nLearning Active Learning from Data NIPS code 36\nUnified Deep Supervised Domain Adaptation and Generalization ICCV code 35\nTransformation-Grounded Image Generation Network for Novel 3D View Synthesis CVPR code 35\nStructured Attentions for Visual Question Answering ICCV code 34\nGeometric Loss Functions for Camera Pose Regression With Deep Learning CVPR code 34\nVidLoc: A Deep Spatio-Temporal Model for 6-DoF Video-Clip Relocalization CVPR code 34\nQMDP-Net: Deep Learning for Planning under Partial Observability NIPS code 34\nUsing Ranking-CNN for Age Estimation CVPR code 33\nHierarchical Boundary-Aware Neural Encoder for Video Captioning CVPR code 33\nUnsupervised Learning of Disentangled Representations from Video NIPS code 32\nDeep Learning on Lie Groups for Skeleton-Based Action Recognition CVPR code 32\nDeep Variation-Structured Reinforcement Learning for Visual Relationship and Attribute Detection CVPR code 32\n3D Point Cloud Registration for Localization Using a Deep Neural Network Auto-Encoder CVPR code 32\nStyleNet: Generating Attractive Visual Captions With Styles CVPR code 32\nDynamic Word Embeddings ICML code 32\nLearning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon NIPS code 31\nContinual Learning Through Synaptic Intelligence ICML code 31\nFull-Resolution Residual Networks for Semantic Segmentation in Street Scenes CVPR code 31\nLearning Detection With Diverse Proposals CVPR code 31\nLCNN: Lookup-Based Convolutional Neural Network CVPR code 31\nTowards Accurate Multi-Person Pose Estimation in the Wild CVPR code 30\nReal-Time Neural Style Transfer for Videos CVPR code 30\nSpeaking the Same Language: Matching Machine to Human Captions by Adversarial Training ICCV code 30\nDeep Co-Occurrence Feature Learning for Visual Object Recognition CVPR code 29\nJoint distribution optimal transportation for domain adaptation NIPS code 29\nRealtime Multi-Person 2D Pose Estimation Using Part Affinity Fields CVPR code 29\nSplitNet: Learning to Semantically Split Deep Networks for Parameter Reduction and Model Parallelization ICML code 29\nThe Statistical Recurrent Unit ICML code 29\nA Unified Approach of Multi-Scale Deep and Hand-Crafted Features for Defocus Estimation CVPR code 28\nLearning Spread-Out Local Feature Descriptors ICCV code 28\nEvent-Based Visual Inertial Odometry CVPR code 27\nDropoutNet: Addressing Cold Start in Recommender Systems NIPS code 27\nPhrase Localization and Visual Relationship Detection With Comprehensive Image-Language Cues ICCV code 27\nHarvesting Multiple Views for Marker-Less 3D Human Pose Annotations CVPR code 27\nDeep 360 Pilot: Learning a Deep Agent for Piloting Through 360deg Sports Videos CVPR code 27\nNeural Message Passing for Quantum Chemistry ICML code 27\nState-Frequency Memory Recurrent Neural Networks ICML code 27\nDeepCD: Learning Deep Complementary Descriptors for Patch Representations ICCV code 26\nContrastive Learning for Image Captioning NIPS code 26\nStochastic Optimization with Variance Reduction for Infinite Datasets with Finite Sum Structure NIPS code 26\nLearning High Dynamic Range From Outdoor Panoramas ICCV code 26\nSpeed/Accuracy Trade-Offs for Modern Convolutional Object Detectors CVPR code 26\nLearning to Detect Salient Objects With Image-Level Supervision CVPR code 26\nImproved Variational Autoencoders for Text Modeling using Dilated Convolutions ICML code 26\nInterspecies Knowledge Transfer for Facial Keypoint Detection CVPR code 25\nYASS: Yet Another Spike Sorter NIPS code 25\nOpen Set Domain Adaptation ICCV code 25\nDomain-Adaptive Deep Network Compression ICCV code 24\nLong Short-Term Memory Kalman Filters: Recurrent Neural Estimators for Pose Regularization ICCV code 24\nTemporal Context Network for Activity Localization in Videos ICCV code 24\nIncremental Learning of Object Detectors Without Catastrophic Forgetting ICCV code 24\nDense Captioning With Joint Inference and Visual Context CVPR code 24\nUniversal Adversarial Perturbations CVPR code 24\nAsymmetric Tri-training for Unsupervised Domain Adaptation ICML code 24\nReducing Reparameterization Gradient Variance NIPS code 24\nExploiting Saliency for Object Segmentation From Image Level Labels CVPR code 24\nA Dirichlet Mixture Model of Hawkes Processes for Event Sequence Clustering NIPS code 24\nShading Annotations in the Wild CVPR code 24\nStraight to Shapes: Real-Time Detection of Encoded Shapes CVPR code 23\nDual Discriminator Generative Adversarial Nets NIPS code 23\nZero-Order Reverse Filtering ICCV code 23\nVariational Walkback: Learning a Transition Operator as a Stochastic Recurrent Net NIPS code 23\nLearning Spherical Convolution for Fast Features from 360\u00b0 Imagery NIPS code 22\nLearning to Detect Sepsis with a Multitask Gaussian Process RNN Classifier ICML code 22\nDeep Cross-Modal Hashing CVPR code 22\nWhen Unsupervised Domain Adaptation Meets Tensor Representations ICCV code 22\nImage Super-Resolution Using Dense Skip Connections ICCV code 22\nMultimodal Transfer: A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer CVPR code 22\nSTD2P: RGBD Semantic Segmentation Using Spatio-Temporal Data-Driven Pooling CVPR code 22\nLearning Continuous Semantic Representations of Symbolic Expressions ICML code 22\nDeep Growing Learning ICCV code 21\nCombined Group and Exclusive Sparsity for Deep Neural Networks ICML code 21\nHash Embeddings for Efficient Word Representations NIPS code 21\nAccuracy First: Selecting a Differential Privacy Level for Accuracy Constrained ERM NIPS code 21\nDisentangled Representation Learning GAN for Pose-Invariant Face Recognition CVPR code 21\nLearning to Pivot with Adversarial Networks NIPS code 21\nLearning Dynamic Siamese Network for Visual Object Tracking ICCV code 21\nPOSEidon: Face-From-Depth for Driver Pose Estimation CVPR code 20\nDeep Metric Learning via Facility Location CVPR code 20\nAutomatic Spatially-Aware Fashion Concept Discovery ICCV code 20\nThe Numerics of GANs NIPS code 20\nFrom Motion Blur to Motion Flow: A Deep Learning Solution for Removing Heterogeneous Motion Blur CVPR code 20\nUnpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks ICCV code 20\nZero-Inflated Exponential Family Embeddings ICML code 20\nInfoGAIL: Interpretable Imitation Learning from Visual Demonstrations NIPS code 20\nWeakly-Supervised Learning of Visual Relations ICCV code 20\nMulti-Label Image Recognition by Recurrently Discovering Attentional Regions ICCV code 20\nScene Parsing With Global Context Embedding ICCV code 20\nContext Selection for Embedding Models NIPS code 20\nDeep Mean-Shift Priors for Image Restoration NIPS code 20\nSkeleton Key: Image Captioning by Skeleton-Attribute Decomposition CVPR code 20\nFully-Adaptive Feature Sharing in Multi-Task Networks With Applications in Person Attribute Classification CVPR code 19\nLearning Compact Geometric Features ICCV code 19\nStructured Generative Adversarial Networks NIPS code 19\nJoint Gap Detection and Inpainting of Line Drawings CVPR code 19\nChained Multi-Stream Networks Exploiting Pose, Motion, and Appearance for Action Classification and Detection ICCV code 19\nAdversarial Feature Matching for Text Generation ICML code 18\nBIER - Boosting Independent Embeddings Robustly ICCV code 18\nPredictive-Corrective Networks for Action Detection CVPR code 18\nStochastic Generative Hashing ICML code 18\nA Bayesian Data Augmentation Approach for Learning Deep Models NIPS code 18\nAttentive Semantic Video Generation Using Captions ICCV code 18\nMDNet: A Semantically and Visually Interpretable Medical Image Diagnosis Network CVPR code 18\nDeep Unsupervised Similarity Learning Using Partially Ordered Sets CVPR code 17\nDualNet: Learn Complementary Features for Image Recognition ICCV code 17\nNeural system identification for large populations separating \u201cwhat\u201d and \u201cwhere\u201d NIPS code 17\nFALKON: An Optimal Large Scale Kernel Method NIPS code 17\nDeep Future Gaze: Gaze Anticipation on Egocentric Videos Using Adversarial Networks CVPR code 17\nDeep Learning with Topological Signatures NIPS code 17\nStreaming Sparse Gaussian Process Approximations NIPS code 17\nRPAN: An End-To-End Recurrent Pose-Attention Network for Action Recognition in Videos ICCV code 17\nAwesome Typography: Statistics-Based Text Effects Transfer CVPR code 17\nRoomNet: End-To-End Room Layout Estimation ICCV code 17\nDeep Spatial-Semantic Attention for Fine-Grained Sketch-Based Image Retrieval ICCV code 16\nDeep Supervised Discrete Hashing NIPS code 16\nFew-Shot Learning Through an Information Retrieval Lens NIPS code 16\nEstimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach NIPS code 16\nLearning to Push the Limits of Efficient FFT-Based Image Deconvolution ICCV code 16\nFederated Multi-Task Learning NIPS code 16\nLabel Distribution Learning Forests NIPS code 16\nDeep Multitask Architecture for Integrated 2D and 3D Human Sensing CVPR code 16\nEstimating Mutual Information for Discrete-Continuous Mixtures NIPS code 16\nSpatially-Varying Blur Detection Based on Multiscale Fused and Sorted Transform Coefficients of Gradient Magnitudes CVPR code 16\nStyleBank: An Explicit Representation for Neural Image Style Transfer CVPR code 16\nSurface Normals in the Wild ICCV code 15\nAutomatic Discovery of the Statistical Types of Variables in a Dataset ICML code 15\nLearning Diverse Image Colorization CVPR code 15\nLearning Proximal Operators: Using Denoising Networks for Regularizing Inverse Imaging Problems ICCV code 15\nNon-Local Deep Features for Salient Object Detection CVPR code 15\nStructure-Measure: A New Way to Evaluate Foreground Maps ICCV code 15\nShallow Updates for Deep Reinforcement Learning NIPS code 15\nWasserstein Generative Adversarial Networks ICML code 15\nRecurrent 3D Pose Sequence Machines CVPR code 15\nVariational Dropout Sparsifies Deep Neural Networks ICML code 15\nCaptioning Images With Diverse Objects CVPR code 15\nOff-policy evaluation for slate recommendation NIPS code 15\nAttributes2Classname: A Discriminative Model for Attribute-Based Unsupervised Zero-Shot Learning ICCV code 14\nBenchmarking Denoising Algorithms With Real Photographs CVPR code 14\nNeural Aggregation Network for Video Face Recognition CVPR code 14\nLearned Contextual Feature Reweighting for Image Geo-Localization CVPR code 14\nStreaming Weak Submodularity: Interpreting Neural Networks on the Fly NIPS code 14\nCVAE-GAN: Fine-Grained Image Generation Through Asymmetric Training ICCV code 14\nVQS: Linking Segmentations to Questions and Answers for Supervised Attention in VQA and Question-Focused Semantic Segmentation ICCV code 14\nSpherical convolutions and their application in molecular modelling NIPS code 14\nMulti-Information Source Optimization NIPS code 14\nConvolutional Neural Network Architecture for Geometric Matching CVPR code 14\nNeural Face Editing With Intrinsic Image Disentangling CVPR code 14\nRealistic Dynamic Facial Textures From a Single Image Using GANs ICCV code 14\nPredictive State Recurrent Neural Networks NIPS code 13\nDeep TextSpotter: An End-To-End Trainable Scene Text Localization and Recognition Framework ICCV code 13\nExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events NIPS code 13\nHunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs NIPS code 13\nConsensus Convolutional Sparse Coding ICCV code 13\nWeakly Supervised Affordance Detection CVPR code 13\nJoint Learning of Object and Action Detectors ICCV code 13\nLight Field Blind Motion Deblurring CVPR code 13\nAsynchronous Stochastic Gradient Descent with Delay Compensation ICML code 13\nUnrolled Memory Inner-Products: An Abstract GPU Operator for Efficient Vision-Related Computations ICCV code 12\nMaximizing Subset Accuracy with Recurrent Neural Networks in Multi-label Classification NIPS code 12\nSelf-Organized Text Detection With Minimal Post-Processing via Border Learning ICCV code 12\nCoordinated Multi-Agent Imitation Learning ICML code 12\nGradient descent GAN optimization is locally stable NIPS code 12\nRemoving Rain From Single Images via a Deep Detail Network CVPR code 12\nConvexified Convolutional Neural Networks ICML code 12\nMultigrid Neural Architectures CVPR code 12\nVegFru: A Domain-Specific Dataset for Fine-Grained Visual Categorization ICCV code 12\nAttend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin NIPS code 12\nDifferential Angular Imaging for Material Recognition CVPR code 12\nA Multilayer-Based Framework for Online Background Subtraction With Freely Moving Cameras ICCV code 11\nFormal Guarantees on the Robustness of a Classifier against Adversarial Manipulation NIPS code 11\nMax-value Entropy Search for Efficient Bayesian Optimization ICML code 11\nHigher-Order Integration of Hierarchical Convolutional Activations for Fine-Grained Visual Categorization ICCV code 11\nGeneralized Deep Image to Image Regression CVPR code 11\nAdversarial Image Perturbation for Privacy Protection -- A Game Theory Perspective ICCV code 11\nPredicting Human Activities Using Stochastic Grammar ICCV code 11\nDESIRE: Distant Future Prediction in Dynamic Scenes With Interacting Agents CVPR code 11\nFisher GAN NIPS code 11\nHigh-Order Attention Models for Visual Question Answering NIPS code 11\nIM2CAD CVPR code 11\nOn Fairness and Calibration NIPS code 11\nDeepPermNet: Visual Permutation Learning CVPR code 10\nf-GANs in an Information Geometric Nutshell NIPS code 10\nRevisiting IM2GPS in the Deep Learning Era ICCV code 10\nAttentional Correlation Filter Network for Adaptive Visual Tracking CVPR code 10\nLearning Cross-Modal Deep Representations for Robust Pedestrian Detection CVPR code 10\nConfident Multiple Choice Learning ICML code 10\nCurriculum Dropout ICCV code 9\nCognitive Mapping and Planning for Visual Navigation CVPR code 9\nOptimized Pre-Processing for Discrimination Prevention NIPS code 9\nLearning Motion Patterns in Videos CVPR code 9\nScalable Log Determinants for Gaussian Process Kernel Learning NIPS code 9\nA Hierarchical Approach for Generating Descriptive Image Paragraphs CVPR code 9\nDeep Crisp Boundaries CVPR code 9\nBreaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization NIPS code 9\nPractical Data-Dependent Metric Compression with Provable Guarantees NIPS code 9\nDo Deep Neural Networks Suffer from Crowding? NIPS code 9\nA Non-Convex Variational Approach to Photometric Stereo Under Inaccurate Lighting CVPR code 9\nEnd-To-End Learning of Geometry and Context for Deep Stereo Regression ICCV code 9\nFrom Bayesian Sparsity to Gated Recurrent Nets NIPS code 8\nRegret Minimization in MDPs with Options without Prior Knowledge NIPS code 8\nFollowing Gaze in Video ICCV code 8\nModel-Powered Conditional Independence Test NIPS code 8\nCost efficient gradient boosting NIPS code 8\nReflectance Adaptive Filtering Improves Intrinsic Image Estimation CVPR code 8\nDeepNav: Learning to Navigate Large Cities CVPR code 8\nLook, Listen and Learn ICCV code 8\nAttention-Aware Face Hallucination via Deep Reinforcement Learning CVPR code 8\nPlan, Attend, Generate: Planning for Sequence-to-Sequence Models NIPS code 8\nIntrospective Neural Networks for Generative Modeling ICCV code 8\nAffinity Clustering: Hierarchical Clustering at Scale NIPS code 8\nGaze Embeddings for Zero-Shot Image Classification CVPR code 8\nInput Switched Affine Networks: An RNN Architecture Designed for Interpretability ICML code 8\nOnline multiclass boosting NIPS code 8\nTowards a Visual Privacy Advisor: Understanding and Predicting Privacy Risks in Images ICCV code 8\nSubUNets: End-To-End Hand Shape and Continuous Sign Language Recognition ICCV code 7\nLearning Koopman Invariant Subspaces for Dynamic Mode Decomposition NIPS code 7\nUnsupervised Monocular Depth Estimation With Left-Right Consistency CVPR code 7\nPersonalized Image Aesthetics ICCV code 7\nReasoning About Fine-Grained Attribute Phrases Using Reference Games ICCV code 7\nLost Relatives of the Gumbel Trick ICML code 7\nWeakly Supervised Learning of Deep Metrics for Stereo Reconstruction ICCV code 7\nCentered Weight Normalization in Accelerating Training of Deep Neural Networks ICCV code 6\nScalable Planning with Tensorflow for Hybrid Nonlinear Domains NIPS code 6\nConvex Global 3D Registration With Lagrangian Duality CVPR code 6\nBuilding a Regular Decision Boundary With Deep Networks CVPR code 6\nLearning Spatial Regularization With Image-Level Supervisions for Multi-Label Image Classification CVPR code 6\nForecasting Human Dynamics From Static Images CVPR code 6\nAOD-Net: All-In-One Dehazing Network ICCV code 6\nK-Medoids For K-Means Seeding NIPS code 6\nDiverse Image Annotation CVPR code 6\nPractical Hash Functions for Similarity Estimation and Dimensionality Reduction NIPS code 6\nDeep Adaptive Image Clustering ICCV code 6\nRobust Adversarial Reinforcement Learning ICML code 6\nImproving Training of Deep Neural Networks via Singular Value Bounding CVPR code 6\nAnalyzing Hidden Representations in End-to-End Automatic Speech Recognition Systems NIPS code 6\nTensor Belief Propagation ICML code 6\nSparse convolutional coding for neuronal assembly detection NIPS code 6\nUnsupervised Pixel-Level Domain Adaptation With Generative Adversarial Networks CVPR code 6\nBayesian inference on random simple graphs with power law degree distributions ICML code 6\nTensor Biclustering NIPS code 6\nRiemannian approach to batch normalization NIPS code 6\nUnsupervised Learning of Object Landmarks by Factorized Spatial Embeddings ICCV code 6\nRolling-Shutter-Aware Differential SfM and Image Rectification ICCV code 5\nActive Decision Boundary Annotation With Deep Generative Models ICCV code 5\nObject Co-Skeletonization With Co-Segmentation CVPR code 5\nDiscover and Learn New Objects From Documentaries CVPR code 5\nUnderstanding Black-box Predictions via Influence Functions ICML code 5\nMaking Deep Neural Networks Robust to Label Noise: A Loss Correction Approach CVPR code 5\nDecoupling \"when to update\" from \"how to update\" NIPS code 5\nMarioQA: Answering Questions by Watching Gameplay Videos ICCV code 5\nDifferentially private Bayesian learning on distributed data NIPS code 5\nGrad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization ICCV code 5\nQuestion Asking as Program Generation NIPS code 5\nConic Scan-and-Cover algorithms for nonparametric topic modeling NIPS code 5\nLip Reading Sentences in the Wild CVPR code 5\nROAM: A Rich Object Appearance Model With Application to Rotoscoping CVPR code 5\nNeuralFDR: Learning Discovery Thresholds from Hypothesis Features NIPS code 5\nViraliency: Pooling Local Virality CVPR code 5\nLearning Algorithms for Active Learning ICML code 5\nPoint to Set Similarity Based Deep Feature Learning for Person Re-Identification CVPR code 5\nClick Here: Human-Localized Keypoints as Guidance for Viewpoint Estimation ICCV code 5\nThe World of Fast Moving Objects CVPR code 5\nCross-Modality Binary Code Learning via Fusion Similarity Hashing CVPR code 5\nTesting and Learning on Distributions with Symmetric Noise Invariance NIPS code 5\nSticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference NIPS code 5\nDiving into the shallows: a computational perspective on large-scale shallow learning NIPS code 5\nRotation Equivariant Vector Field Networks ICCV code 5\nRecursive Sampling for the Nystrom Method NIPS code 5\nLearning From Video and Text via Large-Scale Discriminative Clustering ICCV code 5\nGlobal optimization of Lipschitz functions ICML code 5\nDevice Placement Optimization with Reinforcement Learning ICML code 4\nAlternating Direction Graph Matching CVPR code 4\nMEC: Memory-efficient Convolution for Deep Neural Network ICML code 4\nExpert Gate: Lifelong Learning With a Network of Experts CVPR code 4\nA Simple yet Effective Baseline for 3D Human Pose Estimation ICCV code 4\nOn Structured Prediction Theory with Calibrated Convex Surrogate Losses NIPS code 4\nSub-sampled Cubic Regularization for Non-convex Optimization ICML code 4\nGeneralized Semantic Preserving Hashing for N-Label Cross-Modal Retrieval CVPR code 4\nBottleneck Conditional Density Estimation ICML code 4\nLearning Cooperative Visual Dialog Agents With Deep Reinforcement Learning ICCV code 4\nMulti-way Interacting Regression via Factorization Machines NIPS code 4\nJoint Discovery of Object States and Manipulation Actions ICCV code 4\nPredicting Salient Face in Multiple-Face Videos CVPR code 4\nFrom Red Wine to Red Tomato: Composition With Context CVPR code 4\nEncoder Based Lifelong Learning ICCV code 4\nDeep Recurrent Neural Network-Based Identification of Precursor microRNAs NIPS code 4\nGuarantees for Greedy Maximization of Non-submodular Functions with Applications ICML code 4\nPose-Aware Person Recognition CVPR code 4\nZero-Shot Recognition Using Dual Visual-Semantic Mapping Paths CVPR code 4\nAsynchronous Distributed Variational Gaussian Processes for Regression ICML code 3\nSaliency Pattern Detection by Ranking Structured Trees ICCV code 3\nToward Goal-Driven Neural Network Models for the Rodent Whisker-Trigeminal System NIPS code 3\nLearning Non-Maximum Suppression CVPR code 3\nDeep Latent Dirichlet Allocation with Topic-Layer-Adaptive Stochastic Gradient Riemannian MCMC ICML code 3\nDiscriminative Bimodal Networks for Visual Localization and Detection With Natural Language Queries CVPR code 3\nAdaNet: Adaptive Structural Learning of Artificial Neural Networks ICML code 3\nLarge Margin Object Tracking With Circulant Feature Maps CVPR code 3\nCompatible Reward Inverse Reinforcement Learning NIPS code 3\nAdversarial Surrogate Losses for Ordinal Regression NIPS code 3\nNon-monotone Continuous DR-submodular Maximization: Structure and Algorithms NIPS code 3\nUnifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning NIPS code 3\nA framework for Multi-A(rmed)/B(andit) Testing with Online FDR Control NIPS code 3\nCounting Everyday Objects in Everyday Scenes CVPR code 3\nLoss Max-Pooling for Semantic Image Segmentation CVPR code 3\nAesthetic Critiques Generation for Photos ICCV code 3\nExpectation Propagation with Stochastic Kinetic Model in Complex Interaction Systems NIPS code 3\nNear-Optimal Edge Evaluation in Explicit Generalized Binomial Graphs NIPS code 3\n\u21a5 back to top\n2016\nTitle Conf Code Stars\nR-FCN: Object Detection via Region-based Fully Convolutional Networks NIPS code 18356\nImage Style Transfer Using Convolutional Neural Networks CVPR code 16435\nDeep Residual Learning for Image Recognition CVPR code 4468\nConvolutional Pose Machines CVPR code 3260\nSynthetic Data for Text Localisation in Natural Images CVPR code 787\nCombining Markov Random Fields and Convolutional Neural Networks for Image Synthesis CVPR code 731\nInstance-Aware Semantic Segmentation via Multi-Task Network Cascades CVPR code 433\nLearning Multi-Domain Convolutional Neural Networks for Visual Tracking CVPR code 350\nConvolutional Two-Stream Network Fusion for Video Action Recognition CVPR code 342\nLearning Deep Features for Discriminative Localization CVPR code 323\nDeep Metric Learning via Lifted Structured Feature Embedding CVPR code 251\nLearning Deep Representations of Fine-Grained Visual Descriptions CVPR code 229\nEye Tracking for Everyone CVPR code 223\nNetVLAD: CNN Architecture for Weakly Supervised Place Recognition CVPR code 204\nStaple: Complementary Learners for Real-Time Tracking CVPR code 183\nJoint Unsupervised Learning of Deep Representations and Image Clusters CVPR code 182\nAccurate Image Super-Resolution Using Very Deep Convolutional Networks CVPR code 182\nTemporal Action Localization in Untrimmed Videos via Multi-Stage CNNs CVPR code 167\nLocNet: Improving Localization Accuracy for Object Detection CVPR code 155\nShallow and Deep Convolutional Networks for Saliency Prediction CVPR code 153\nCompact Bilinear Pooling CVPR code 148\nLearning Compact Binary Descriptors With Unsupervised Deep Neural Networks CVPR code 144\nDynamic Image Networks for Action Recognition CVPR code 133\nRethinking the Inception Architecture for Computer Vision CVPR code 130\nDeep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images CVPR code 126\nContext Encoders: Feature Learning by Inpainting CVPR code 124\nTI-Pooling: Transformation-Invariant Pooling for Feature Learning in Convolutional Neural Networks CVPR code 109\nWeakly Supervised Deep Detection Networks CVPR code 103\nNatural Language Object Retrieval CVPR code 100\nDeeply-Recursive Convolutional Network for Image Super-Resolution CVPR code 96\nReal-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network CVPR code 92\nImage Question Answering Using Convolutional Neural Network With Dynamic Parameter Prediction CVPR code 88\nRecurrent Convolutional Network for Video-Based Person Re-Identification CVPR code 82\nA Comparative Study for Single Image Blind Deblurring CVPR code 82\nNeural Module Networks CVPR code 81\nStacked Attention Networks for Image Question Answering CVPR code 78\nProgressive Prioritized Multi-View Stereo CVPR code 73\nMarr Revisited: 2D-3D Alignment via Surface Normal Prediction CVPR code 72\nA Hierarchical Deep Temporal Model for Group Activity Recognition CVPR code 71\nTowards Open Set Deep Networks CVPR code 71\nRobust 3D Hand Pose Estimation in Single Depth Images: From Single-View CNN to Multi-View CNNs CVPR code 70\nBilateral Space Video Segmentation CVPR code 63\nDeep Compositional Captioning: Describing Novel Object Categories Without Paired Training Data CVPR code 57\nEfficient 3D Room Shape Recovery From a Single Panorama CVPR code 55\nNon-Local Image Dehazing CVPR code 50\nVideo Segmentation via Object Flow CVPR code 50\nDeep Supervised Hashing for Fast Image Retrieval CVPR code 50\nDeep Region and Multi-Label Learning for Facial Action Unit Detection CVPR code 43\nCRAFT Objects From Images CVPR code 41\nSlicing Convolutional Neural Network for Crowd Video Understanding CVPR code 40\nSketch Me That Shoe CVPR code 39\nImage Captioning With Semantic Attention CVPR code 35\nDeep Saliency With Encoded Low Level Distance Map and High Level Features CVPR code 34\nA Benchmark Dataset and Evaluation Methodology for Video Object Segmentation CVPR code 33\nA Dual-Source Approach for 3D Pose Estimation From a Single Image CVPR code 32\nLearning Local Image Descriptors With Deep Siamese and Triplet Convolutional Networks by Minimising Global Loss Functions CVPR code 30\nOrdinal Regression With Multiple Output CNN for Age Estimation CVPR code 30\nStructured Feature Learning for Pose Estimation CVPR code 29\nUnsupervised Learning of Edges CVPR code 29\nPatchBatch: A Batch Augmented Loss for Optical Flow CVPR code 27\nDense Human Body Correspondences Using Convolutional Networks CVPR code 27\nActionness Estimation Using Hybrid Fully Convolutional Networks CVPR code 26\nYou Only Look Once: Unified, Real-Time Object Detection CVPR code 26\nFast Training of Triplet-Based Deep Binary Embedding Networks CVPR code 25\nRecurrent Attention Models for Depth-Based Person Identification CVPR code 24\nDetecting Vanishing Points Using Global Image Context in a Non-Manhattan World CVPR code 22\nFirst Person Action Recognition Using Deep Learned Descriptors CVPR code 21\nProposal Flow CVPR code 20\nScale-Aware Alignment of Hierarchical Image Segmentation CVPR code 20\nQuantized Convolutional Neural Networks for Mobile Devices CVPR code 20\nSemantic Segmentation With Boundary Neural Fields CVPR code 19\nSingle-Image Crowd Counting via Multi-Column Convolutional Neural Network CVPR code 19\nAccumulated Stability Voting: A Robust Descriptor From Descriptors of Multiple Scales CVPR code 19\nStructure From Motion With Objects CVPR code 17\nBottom-Up and Top-Down Reasoning With Hierarchical Rectified Gaussians CVPR code 16\nSemantic Filtering CVPR code 16\nOnline Detection and Classification of Dynamic Hand Gestures With Recurrent 3D Convolutional Neural Network CVPR code 16\nReconNet: Non-Iterative Reconstruction of Images From Compressively Sensed Measurements CVPR code 15\nInteractive Segmentation on RGBD Images via Cue Selection CVPR code 14\nObject Contour Detection With a Fully Convolutional Encoder-Decoder Network CVPR code 14\nAutomatic Content-Aware Color and Tone Stylization CVPR code 12\nSimilarity Learning With Spatial Constraints for Person Re-Identification CVPR code 11\nPersonalizing Human Video Pose Estimation CVPR code 10\nVisually Indicated Sounds CVPR code 9\nPatch-Based Convolutional Neural Network for Whole Slide Tissue Image Classification CVPR code 9\nRegion Ranking SVM for Image Classification CVPR code 8\nPairwise Matching Through Max-Weight Bipartite Belief Propagation CVPR code 8\nDeep Hand: How to Train a CNN on 1 Million Hand Images When Your Data Is Continuous and Weakly Labelled CVPR code 8\nCross-Stitch Networks for Multi-Task Learning CVPR code 8\nLearning a Discriminative Null Space for Person Re-Identification CVPR code 8\nEfficient Deep Learning for Stereo Matching CVPR code 7\nGlobally Optimal Manhattan Frame Estimation in Real-Time CVPR code 7\nWhere to Look: Focus Regions for Visual Question Answering CVPR code 7\nDetecting Migrating Birds at Night CVPR code 7\nUnsupervised Learning From Narrated Instruction Videos CVPR code 7\nEfficient and Robust Color Consistency for Community Photo Collections CVPR code 7\nRecurrent Attentional Networks for Saliency Detection CVPR code 7\n3D Shape Attributes CVPR code 6\nBeyond Local Search: Tracking Objects Everywhere With Instance-Specific Proposals CVPR code 5\nFunctional Faces: Groupwise Dense Correspondence Using Functional Maps CVPR code 5\nVisual Tracking Using Attention-Modulated Disintegration and Integration CVPR code 5\nImproving Human Action Recognition by Non-Action Classification CVPR code 4\nPrior-Less Compressible Structure From Motion CVPR code 4\nDenseCap: Fully Convolutional Localization Networks for Dense Captioning CVPR code 4\nTensor Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Tensors via Convex Optimization CVPR code 4\nForce From Motion: Decoding Physical Sensation in a First Person Video CVPR code 3\nContext-Aware Gaussian Fields for Non-Rigid Point Set Registration CVPR code 3\nUsing Spatial Order to Boost the Elimination of Incorrect Feature Matches CVPR code 3\nFast Algorithms for Convolutional Neural Networks CVPR code 3\n\u21a5 back to top\n2015\nTitle Conf Code Stars\nFaster R-CNN: Towards Real-Time Object Detectionwith Region Proposal Networks NIPS code 18356\nFast R-CNN ICCV code 18356\nConditional Random Fields as Recurrent Neural Networks ICCV code 1189\nFully Convolutional Networks for Semantic Segmentation CVPR code 911\nLearning to Track: Online Multi-Object Tracking by Decision Making ICCV code 308\nLearning to Compare Image Patches via Convolutional Neural Networks CVPR code 300\nLearning Deconvolution Network for Semantic Segmentation ICCV code 296\nSingle Image Super-Resolution From Transformed Self-Exemplars CVPR code 289\nSequence to Sequence - Video to Text ICCV code 239\nDeep Colorization ICCV code 198\nDeep Neural Decision Forests ICCV code 192\nHierarchical Convolutional Features for Visual Tracking ICCV code 179\nRender for CNN: Viewpoint Estimation in Images Using CNNs Trained With Rendered 3D Model Views ICCV code 176\nRealtime Edge-Based Visual Odometry for a Monocular Camera ICCV code 175\nUnderstanding Deep Image Representations by Inverting Them CVPR code 154\nContext-Aware CNNs for Person Head Detection ICCV code 153\nShow and Tell: A Neural Image Caption Generator CVPR code 141\nFace Alignment by Coarse-to-Fine Shape Searching CVPR code 140\nAn Improved Deep Learning Architecture for Person Re-Identification CVPR code 127\nFaceNet: A Unified Embedding for Face Recognition and Clustering CVPR code 124\nDepth-Based Hand Pose Estimation: Data, Methods, and Challenges ICCV code 121\nDynamicFusion: Reconstruction and Tracking of Non-Rigid Scenes in Real-Time CVPR code 118\nMassively Parallel Multiview Stereopsis by Surface Normal Diffusion ICCV code 105\nLearning to Propose Objects CVPR code 91\nLearning Spatially Regularized Correlation Filters for Visual Tracking ICCV code 86\nA Convolutional Neural Network Cascade for Face Detection CVPR code 85\nDiscriminative Learning of Deep Convolutional Feature Point Descriptors ICCV code 77\nUnsupervised Visual Representation Learning by Context Prediction ICCV code 73\nDeep Neural Networks Are Easily Fooled: High Confidence Predictions for Unrecognizable Images CVPR code 71\nDeep Filter Banks for Texture Recognition and Segmentation CVPR code 68\nSaliency Detection by Multi-Context Deep Learning CVPR code 66\nMulti-Objective Convolutional Learning for Face Labeling CVPR code 55\nFinding Action Tubes CVPR code 51\nCategory-Specific Object Reconstruction From a Single Image CVPR code 48\nConvolutional Color Constancy ICCV code 47\nFace Flow ICCV code 45\nP-CNN: Pose-Based CNN Features for Action Recognition ICCV code 45\nLearning From Massive Noisy Labeled Data for Image Classification CVPR code 45\nImage Specificity CVPR code 40\nPredicting Depth, Surface Normals and Semantic Labels With a Common Multi-Scale Convolutional Architecture ICCV code 35\nNeural Activation Constellations: Unsupervised Part Model Discovery With Convolutional Networks ICCV code 35\nVQA: Visual Question Answering ICCV code 35\nMid-Level Deep Pattern Mining CVPR code 34\nPoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization ICCV code 34\nParsimonious Labeling ICCV code 33\nCar That Knows Before You Do: Anticipating Maneuvers via Learning Temporal Driving Models ICCV code 33\nRecurrent Convolutional Neural Network for Object Recognition CVPR code 32\nTILDE: A Temporally Invariant Learned DEtector CVPR code 30\nIn Defense of Color-Based Model-Free Tracking CVPR code 30\nFast Bilateral-Space Stereo for Synthetic Defocus CVPR code 29\nPhase-Based Frame Interpolation for Video CVPR code 28\nUnderstanding Tools: Task-Oriented Object Modeling, Learning and Recognition CVPR code 27\nDeeply Learned Attributes for Crowded Scene Understanding CVPR code 27\nUnconstrained 3D Face Reconstruction CVPR code 26\nViewpoints and Keypoints CVPR code 25\nHolistically-Nested Edge Detection ICCV code 25\nGoing Deeper With Convolutions CVPR code 25\nReconstructing the World* in Six Days *(As Captured by the Yahoo 100 Million Image Dataset) CVPR code 25\nData-Driven 3D Voxel Patterns for Object Category Recognition CVPR code 24\nL0TV: A New Method for Image Restoration in the Presence of Impulse Noise CVPR code 22\nBeyond Frontal Faces: Improving Person Recognition Using Multiple Cues CVPR code 21\nUnderstanding Deep Features With Computer-Generated Imagery ICCV code 19\nHICO: A Benchmark for Recognizing Human-Object Interactions in Images ICCV code 18\nStructured Feature Selection ICCV code 17\nLearning Large-Scale Automatic Image Colorization ICCV code 17\nSemantic Component Analysis ICCV code 17\nSimultaneous Feature Learning and Hash Coding With Deep Neural Networks CVPR code 16\n3D Object Reconstruction From Hand-Object Interactions ICCV code 15\nLearning Temporal Embeddings for Complex Video Analysis ICCV code 14\nLearning to See by Moving ICCV code 14\nReflection Removal Using Ghosting Cues CVPR code 14\nWhere to Buy It: Matching Street Clothing Photos in Online Shops ICCV code 14\nOriented Edge Forests for Boundary Detection CVPR code 13\nA Large-Scale Car Dataset for Fine-Grained Categorization and Verification CVPR code 11\nAppearance-Based Gaze Estimation in the Wild CVPR code 10\nLearning a Descriptor-Specific 3D Keypoint Detector ICCV code 10\nRobust Image Filtering Using Joint Static and Dynamic Guidance CVPR code 10\nPartial Person Re-Identification ICCV code 9\nHigh Quality Structure From Small Motion for Rolling Shutter Cameras ICCV code 9\nBoosting Object Proposals: From Pascal to COCO ICCV code 8\nConvolutional Channel Features ICCV code 8\nLive Repetition Counting ICCV code 8\nUnsupervised Learning of Visual Representations Using Videos ICCV code 8\nSupervised Discrete Hashing CVPR code 7\nMulti-View Convolutional Neural Networks for 3D Shape Recognition ICCV code 7\nSimpler Non-Parametric Methods Provide as Good or Better Results to Multiple-Instance Learning ICCV code 7\nFinding Distractors In Images CVPR code 7\nPiecewise Flat Embedding for Image Segmentation ICCV code 7\nLong-Term Correlation Tracking CVPR code 6\nTowards Open World Recognition CVPR code 6\nPooled Motion Features for First-Person Videos CVPR code 6\nSimultaneous Deep Transfer Across Domains and Tasks ICCV code 6\nWhat Makes an Object Memorable? ICCV code 5\nMining Semantic Affordances of Visual Object Categories CVPR code 5\nDense Semantic Correspondence Where Every Pixel is a Classifier ICCV code 5\nSegment Graph Based Image Filtering: Fast Structure-Preserving Smoothing ICCV code 5\nFast Randomized Singular Value Thresholding for Nuclear Norm Minimization CVPR code 5\nUnsupervised Generation of a Viewpoint Annotated Car Dataset From Videos ICCV code 5\nMulti-Label Cross-Modal Retrieval ICCV code 4\nSuperdifferential Cuts for Binary Energies CVPR code 4\nPose Induction for Novel Object Categories ICCV code 4\nEfficient Minimal-Surface Regularization of Perspective Depth Maps in Variational Stereo CVPR code 4\nLow-Rank Matrix Factorization Under General Mixture Noise Distributions ICCV code 4\nRobust Saliency Detection via Regularized Random Walks Ranking CVPR code 3\nSimultaneous Video Defogging and Stereo Reconstruction CVPR code 3\nHyperspectral Super-Resolution by Coupled Spectral Unmixing ICCV code 3\nOriented Object Proposals ICCV code 3\nkNN Hashing With Factorized Neighborhood Representation ICCV code 3\nMinimum Barrier Salient Object Detection at 80 FPS ICCV code 3\n\u21a5 back to top\n2014\nTitle Conf Code Stars\nRich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation CVPR code 1681\nLocally Optimized Product Quantization for Approximate Nearest Neighbor Search CVPR code 437\nClothing Co-Parsing by Joint Image Segmentation and Labeling CVPR code 218\nMultiscale Combinatorial Grouping CVPR code 185\nFace Alignment at 3000 FPS via Regressing Local Binary Features CVPR code 164\nCross-Scale Cost Aggregation for Stereo Matching CVPR code 106\nTransfer Joint Matching for Unsupervised Domain Adaptation CVPR code 67\nDeep Learning Face Representation from Predicting 10,000 Classes CVPR code 62\nBING: Binarized Normed Gradients for Objectness Estimation at 300fps CVPR code 44\nOne Millisecond Face Alignment with an Ensemble of Regression Trees CVPR code 43\n3D Reconstruction from Accidental Motion CVPR code 42\nPredicting Matchability CVPR code 38\nDense Semantic Image Segmentation with Objects and Attributes CVPR code 28\nScene-Independent Group Profiling in Crowd CVPR code 28\nShrinkage Fields for Effective Image Restoration CVPR code 25\nAdaptive Color Attributes for Real-Time Visual Tracking CVPR code 25\nMinimal Scene Descriptions from Structure from Motion Models CVPR code 22\nParallax-tolerant Image Stitching CVPR code 20\nLearning Mid-level Filters for Person Re-identification CVPR code 20\nFast Edge-Preserving PatchMatch for Large Displacement Optical Flow CVPR code 18\nProduct Sparse Coding CVPR code 16\nConvolutional Neural Networks for No-Reference Image Quality Assessment CVPR code 16\nSeeing 3D Chairs: Exemplar Part-based 2D-3D Alignment using a Large Dataset of CAD Models CVPR code 15\nStoryGraphs: Visualizing Character Interactions as a Timeline CVPR code 14\nNonparametric Part Transfer for Fine-grained Recognition CVPR code 13\nScalable Multitask Representation Learning for Scene Classification CVPR code 11\nInvestigating Haze-relevant Features in A Learning Framework for Image Dehazing CVPR code 7\nReconstructing PASCAL VOC CVPR code 6\nCollaborative Hashing CVPR code 6\nTell Me What You See and I will Show You Where It Is CVPR code 6\nSalient Region Detection via High-Dimensional Color Transform CVPR code 6\n\u21a5 back to top\n2013\nTitle Conf Code Stars\nA generic decentralized trust management framework SPE code 6",
      "link": "https://github.com/zziz/pwc"
    },
    {
      "autor": "learnopencv",
      "date": "NaN",
      "content": "LearnOpenCV\nThis repo contains code for Computer Vision, Deep learning, and AI articles shared on our blog LearnOpenCV.com.\nWant to become an expert in AI? AI Courses by OpenCV is a great place to start.\nList of Blog Posts\nBlog Post\nWeChat QR Code Scanner in OpenCV Code\nAI behind the Diwali 2021 \u2018Not just a Cadbury ad\u2019\nModel Selection and Benchmarking with Modelplace.AI Model Zoo\nReal-time style transfer in a zoom meeting Code\nIntroduction to OpenVino Deep Learning Workbench Code\nRunning OpenVino Models on Intel Integrated GPU Code\nPost Training Quantization with OpenVino Toolkit Code\nIntroduction to Intel OpenVINO Toolkit\nHuman Action Recognition using Detectron2 and LSTM Code\nPix2Pix:Image-to-Image Translation in PyTorch & TensorFlow Code\nConditional GAN (cGAN) in PyTorch and TensorFlow Code\nDeep Convolutional GAN in PyTorch and TensorFlow Code\nIntroduction to Generative Adversarial Networks (GANs) Code\nHuman Pose Estimation using Keypoint RCNN in PyTorch Code\nNon Maximum Suppression: Theory and Implementation in PyTorch Code\nMRNet \u2013 The Multi-Task Approach Code\nGenerative and Discriminative Models\nPlaying Chrome's T-Rex Game with Facial Gestures Code\nVariational Autoencoder in TensorFlow Code\nAutoencoder in TensorFlow 2: Beginner\u2019s Guide Code\nDeep Learning with OpenCV DNN Module: A Definitive Guide Code\nDepth perception using stereo -----> camera !!!  (Python/C++) Code\nContour Detection using OpenCV (Python/C++) Code\nSuper Resolution in OpenCV Code\nImproving Illumination in Night Time Images Code\nVideo Classification and Human Activity Recognition Code\nHow to use OpenCV DNN Module with Nvidia GPU on Windows Code\nHow to use OpenCV DNN Module with NVIDIA GPUs Code\nCode OpenCV in Visual Studio\nInstall OpenCV on Windows \u2013 C++ / Python Code\nFace Recognition with ArcFace Code\nBackground Subtraction with OpenCV and BGS Libraries Code\nRAFT: Optical Flow estimation using Deep Learning Code\nMaking A Low-Cost Stereo Camera Using OpenCV Code\nOptical Flow in OpenCV (C++/Python) Code\nIntroduction to Epipolar Geometry and Stereo Vision Code\nClassification With Localization: Convert any keras Classifier to a Detector Code\nPhotoshop Filters in OpenCV Code\nTetris Game using OpenCV Python Code\nImage Classification with OpenCV for Android Code\nImage Classification with OpenCV Java Code\nPyTorch to Tensorflow Model Conversion Code\nSnake Game with OpenCV Python Code\nStanford MRNet Challenge: Classifying Knee MRIs Code\nExperiment Logging with TensorBoard and wandb Code\nUnderstanding Lens Distortion Code\nImage Matting with state-of-the-art Method \u201cF, B, Alpha Matting\u201d Code\nBag Of Tricks For Image Classification - Let's check if it is working or not Code\nGetting Started with OpenCV CUDA Module Code\nTraining a Custom Object Detector with DLIB & Making Gesture Controlled Applications Code\nHow To Run Inference Using TensorRT C++ API Code\nUsing Facial Landmarks for Overlaying Faces with Medical Masks Code\nTensorboard with PyTorch Lightning Code\nOtsu's Thresholding with OpenCV Code\nPyTorch-to-CoreML-model-conversion Code\nPlaying Rock, Paper, Scissors with AI Code\nCNN Receptive Field Computation Using Backprop with TensorFlow Code\nCNN Fully Convolutional Image Classification with TensorFlow Code\nHow to convert a model from PyTorch to TensorRT and speed up inference Code\nEfficient image loading Code\nGraph Convolutional Networks: Model Relations In Data Code\nGetting Started with Federated Learning with PyTorch and PySyft Code\nCreating a Virtual Pen & Eraser Code\nGetting Started with PyTorch Lightning Code\nMulti-Label Image Classification with PyTorch: Image Tagging Code\nFunny Mirrors Using OpenCV code\nt-SNE for ResNet feature visualization Code\nMulti-Label Image Classification with Pytorch Code\nCNN Receptive Field Computation Using Backprop Code\nCNN Receptive Field Computation Using Backprop with TensorFlow Code\nAugmented Reality using AruCo Markers in OpenCV(C++ and Python) Code\nFully Convolutional Image Classification on Arbitrary Sized Image Code\nCamera Calibration using OpenCV Code\nGeometry of Image Formation\nEnsuring Training Reproducibility in Pytorch\nGaze Tracking\nSimple Background Estimation in Videos Using OpenCV Code\nApplications of Foreground-Background separation with Semantic Segmentation Code\nEfficientNet: Theory + Code Code\nPyTorch for Beginners: Mask R-CNN Instance Segmentation with PyTorch Code\nPyTorch for Beginners: Faster R-CNN Object Detection with PyTorch Code\nPyTorch for Beginners: Semantic Segmentation using torchvision Code\nPyTorch for Beginners: Comparison of pre-trained models for Image Classification Code\nPyTorch for Beginners: Basics Code\nPyTorch Model Inference using ONNX and Caffe2 Code\nImage Classification Using Transfer Learning in PyTorch Code\nHangman: Creating games in OpenCV Code\nImage Inpainting with OpenCV (C++/Python) Code\nHough Transform with OpenCV (C++/Python) Code\nXeus-Cling: Run C++ code in Jupyter Notebook Code\nGender & Age Classification using OpenCV Deep Learning ( C++/Python ) Code\nInvisibility Cloak using Color Detection and Segmentation with OpenCV Code\nFast Image Downloader for Open Images V4 (Python) Code\nDeep Learning based Text Detection Using OpenCV (C++/Python) Code\nVideo Stabilization Using Point Feature Matching in OpenCV Code\nTraining YOLOv3 : Deep Learning based Custom Object Detector Code\nUsing OpenVINO with OpenCV Code\nDuplicate Search on Quora Dataset Code\nShape Matching using Hu Moments (C++/Python) Code\nInstall OpenCV 4 on CentOS (C++ and Python) Code\nInstall OpenCV 3.4.4 on CentOS (C++ and Python) Code\nInstall OpenCV 3.4.4 on Red Hat (C++ and Python) Code\nInstall OpenCV 4 on Red Hat (C++ and Python) Code\nInstall OpenCV 4 on macOS (C++ and Python) Code\nInstall OpenCV 3.4.4 on Raspberry Pi Code\nInstall OpenCV 3.4.4 on macOS (C++ and Python) Code\nOpenCV QR Code Scanner (C++ and Python) Code\nInstall OpenCV 3.4.4 on Windows (C++ and Python) Code\nInstall OpenCV 3.4.4 on Ubuntu 16.04 (C++ and Python) Code\nInstall OpenCV 3.4.4 on Ubuntu 18.04 (C++ and Python) Code\nUniversal Sentence Encoder Code\nInstall OpenCV 4 on Raspberry Pi Code\nInstall OpenCV 4 on Windows (C++ and Python) Code\nHand Keypoint Detection using Deep Learning and OpenCV Code\nDeep learning based Object Detection and Instance Segmentation using Mask R-CNN in OpenCV (Python / C++) Code\nInstall OpenCV 4 on Ubuntu 18.04 (C++ and Python) Code\nInstall OpenCV 4 on Ubuntu 16.04 (C++ and Python) Code\nMulti-Person Pose Estimation in OpenCV using OpenPose Code\nHeatmap for Logo Detection using OpenCV (Python) Code\nDeep Learning based Object Detection using YOLOv3 with OpenCV ( Python / C++ ) Code\nConvex Hull using OpenCV in Python and C++ Code\nMultiTracker : Multiple Object Tracking using OpenCV (C++/Python) Code\nConvolutional Neural Network based Image Colorization using OpenCV Code\nSVM using scikit-learn Code\nGOTURN: Deep Learning based Object Tracking Code\nFind the Center of a Blob (Centroid) using OpenCV (C++/Python) Code\nSupport Vector Machines (SVM) Code\nBatch Normalization in Deep Networks Code\nDeep Learning based Character Classification using Synthetic Dataset Code\nImage Quality Assessment : BRISQUE Code\nUnderstanding AlexNet\nDeep Learning based Text Recognition (OCR) using Tesseract and OpenCV Code\nDeep Learning based Human Pose Estimation using OpenCV ( C++ / Python ) Code\nNumber of Parameters and Tensor Sizes in a Convolutional Neural Network (CNN)\nHow to convert your OpenCV C++ code into a Python module Code\nCV4Faces : Best Project Award 2018\nFacemark : Facial Landmark Detection using OpenCV Code\nImage Alignment (Feature Based) using OpenCV (C++/Python) Code\nBarcode and QR code Scanner using ZBar and OpenCV Code\nKeras Tutorial : Fine-tuning using pre-trained models Code\nOpenCV Transparent API\nFace Reconstruction using EigenFaces (C++/Python) Code\nEigenface using OpenCV (C++/Python) Code\nPrincipal Component Analysis\nKeras Tutorial : Transfer Learning using pre-trained models Code\nKeras Tutorial : Using pre-trained Imagenet models Code\nTechnical Aspects of a Digital SLR\nUsing Harry Potter interactive wand with OpenCV to create magic\nInstall OpenCV 3 and Dlib on Windows ( Python only )\nImage Classification using Convolutional Neural Networks in Keras Code\nUnderstanding Autoencoders using Tensorflow (Python) Code\nBest Project Award : Computer Vision for Faces\nUnderstanding Activation Functions in Deep Learning\nImage Classification using Feedforward Neural Network in Keras Code\nExposure Fusion using OpenCV (C++/Python) Code\nUnderstanding Feedforward Neural Networks\nHigh Dynamic Range (HDR) Imaging using OpenCV (C++/Python) Code\nDeep learning using Keras \u2013 The Basics Code\nSelective Search for Object Detection (C++ / Python) Code\nInstalling Deep Learning Frameworks on Ubuntu with CUDA support\nParallel Pixel Access in OpenCV using forEach Code\ncvui: A GUI lib built on top of OpenCV drawing primitives Code\nInstall Dlib on Windows\nInstall Dlib on Ubuntu\nInstall OpenCV3 on Ubuntu\nRead, Write and Display a video using OpenCV ( C++/ Python ) Code\nInstall Dlib on MacOS\nInstall OpenCV 3 on MacOS\nInstall OpenCV 3 on Windows\nGet OpenCV Build Information ( getBuildInformation )\nColor spaces in OpenCV (C++ / Python) Code\nNeural Networks : A 30,000 Feet View for Beginners\nAlpha Blending using OpenCV (C++ / Python) Code\nUser stories : How readers of this blog are applying their knowledge to build applications\nHow to select a bounding box ( ROI ) in OpenCV (C++/Python) ?\nAutomatic Red Eye Remover using OpenCV (C++ / Python) Code\nBias-Variance Tradeoff in Machine Learning\nEmbedded Computer Vision: Which device should you choose?\nObject Tracking using OpenCV (C++/Python) Code\nHandwritten Digits Classification : An OpenCV ( C++ / Python ) Tutorial Code\nTraining a better Haar and LBP cascade based Eye Detector using OpenCV\nDeep Learning Book Gift Recipients\nMinified OpenCV Haar and LBP Cascades Code\nDeep Learning Book Gift\nHistogram of Oriented Gradients\nImage Recognition and Object Detection : Part 1\nHead Pose Estimation using OpenCV and Dlib Code\nLive CV : A Computer Vision Coding Application\nApproximate Focal Length for Webcams and Cell Phone Cameras\nConfiguring Qt for OpenCV on OSX Code\nRotation Matrix To Euler Angles Code\nSpeeding up Dlib\u2019s Facial Landmark Detector\nWarp one triangle to another using OpenCV ( C++ / Python ) Code\nAverage Face : OpenCV ( C++ / Python ) Tutorial Code\nFace Swap using OpenCV ( C++ / Python ) Code\nFace Morph Using OpenCV \u2014 C++ / Python Code\nDeep Learning Example using NVIDIA DIGITS 3 on EC2\nNVIDIA DIGITS 3 on EC2\nHomography Examples using OpenCV ( Python / C ++ ) Code\nFilling holes in an image using OpenCV ( Python / C++ ) Code\nHow to find frame rate or frames per second (fps) in OpenCV ( Python / C++ ) ? Code\nDelaunay Triangulation and Voronoi Diagram using OpenCV ( C++ / Python) Code\nOpenCV (C++ vs Python) vs MATLAB for Computer Vision\nFacial Landmark Detection\nWhy does OpenCV use BGR color format ?\nComputer Vision for Predicting Facial Attractiveness Code\napplyColorMap for pseudocoloring in OpenCV ( C++ / Python ) Code\nImage Alignment (ECC) in OpenCV ( C++ / Python ) Code\nHow to find OpenCV version in Python and C++ ?\nBaidu banned from ILSVRC 2015\nOpenCV Transparent API\nHow Computer Vision Solved the Greatest Soccer Mystery of All Time\nEmbedded Vision Summit 2015\nRead an Image in OpenCV ( Python, C++ ) Code\nNon-Photorealistic Rendering using OpenCV ( Python, C++ ) Code\nSeamless Cloning using OpenCV ( Python , C++ ) Code\nOpenCV Threshold ( Python , C++ ) Code\nBlob Detection Using OpenCV ( Python, C++ ) Code\nTurn your OpenCV Code into a Web API in under 10 minutes \u2014 Part 1\nHow to compile OpenCV sample Code ?\nInstall OpenCV 3 on Yosemite ( OSX 10.10.x )",
      "link": "https://github.com/spmallick/learnopencv"
    },
    {
      "autor": "mediapipe",
      "date": "NaN",
      "content": "layout title nav_order\ndefault\nHome\n1\nLive ML anywhere\nMediaPipe offers cross-platform, customizable ML solutions for live and streaming media.\nEnd-to-End acceleration: Built-in fast ML inference and processing accelerated even on common hardware Build once, deploy anywhere: Unified solution works across Android, iOS, desktop/cloud, web and IoT\nReady-to-use solutions: Cutting-edge ML solutions demonstrating full power of the framework Free and open source: Framework and solutions both under Apache 2.0, fully extensible and customizable\nML solutions in MediaPipe\nFace Detection Face Mesh Iris Hands Pose Holistic\nHair Segmentation Object Detection Box Tracking Instant Motion Tracking Objectron KNIFT\nAndroid iOS C++ Python JS Coral\nFace Detection \u2705 \u2705 \u2705 \u2705 \u2705 \u2705\nFace Mesh \u2705 \u2705 \u2705 \u2705 \u2705\nIris \u2705 \u2705 \u2705\nHands \u2705 \u2705 \u2705 \u2705 \u2705\nPose \u2705 \u2705 \u2705 \u2705 \u2705\nHolistic \u2705 \u2705 \u2705 \u2705 \u2705\nSelfie Segmentation \u2705 \u2705 \u2705 \u2705 \u2705\nHair Segmentation \u2705 \u2705\nObject Detection \u2705 \u2705 \u2705 \u2705\nBox Tracking \u2705 \u2705 \u2705\nInstant Motion Tracking \u2705\nObjectron \u2705 \u2705 \u2705 \u2705\nKNIFT \u2705\nAutoFlip \u2705\nMediaSequence \u2705\nYouTube 8M \u2705\nSee also MediaPipe Models and Model Cards for ML models released in MediaPipe.\nGetting started\nTo start using MediaPipe solutions with only a few lines code, see example code and demos in MediaPipe in Python and MediaPipe in JavaScript.\nTo use MediaPipe in C++, Android and iOS, which allow further customization of the solutions as well as building your own, learn how to install MediaPipe and start building example applications in C++, Android and iOS.\nThe source code is hosted in the MediaPipe Github repository, and you can run code search using Google Open Source Code Search.\nPublications\nBringing artworks to life with AR in Google Developers Blog\nProsthesis control via Mirru App using MediaPipe hand tracking in Google Developers Blog\nSignAll SDK: Sign language interface using MediaPipe is now available for developers in Google Developers Blog\nMediaPipe Holistic - Simultaneous Face, Hand and Pose Prediction, on Device in Google AI Blog\nBackground Features in Google Meet, Powered by Web ML in Google AI Blog\nMediaPipe 3D Face Transform in Google Developers Blog\nInstant Motion Tracking With MediaPipe in Google Developers Blog\nBlazePose - On-device Real-time Body Pose Tracking in Google AI Blog\nMediaPipe Iris: Real-time Eye Tracking and Depth Estimation in Google AI Blog\nMediaPipe KNIFT: Template-based feature matching in Google Developers Blog\nAlfred Camera: Smart -----> camera !!!  features using MediaPipe in Google Developers Blog\nReal-Time 3D Object Detection on Mobile Devices with MediaPipe in Google AI Blog\nAutoFlip: An Open Source Framework for Intelligent Video Reframing in Google AI Blog\nMediaPipe on the Web in Google Developers Blog\nObject Detection and Tracking using MediaPipe in Google Developers Blog\nOn-Device, Real-Time Hand Tracking with MediaPipe in Google AI Blog\nMediaPipe: A Framework for Building Perception Pipelines\nVideos\nYouTube Channel\nEvents\nMediaPipe Seattle Meetup, Google Building Waterside, 13 Feb 2020\nAI Nextcon 2020, 12-16 Feb 2020, Seattle\nMediaPipe Madrid Meetup, 16 Dec 2019\nMediaPipe London Meetup, Google 123 Building, 12 Dec 2019\nML Conference, Berlin, 11 Dec 2019\nMediaPipe Berlin Meetup, Google Berlin, 11 Dec 2019\nThe 3rd Workshop on YouTube-8M Large Scale Video Understanding Workshop, Seoul, Korea ICCV 2019\nAI DevWorld 2019, 10 Oct 2019, San Jose, CA\nGoogle Industry Workshop at ICIP 2019, 24 Sept 2019, Taipei, Taiwan (presentation)\nOpen sourced at CVPR 2019, 17~20 June, Long Beach, CA\nCommunity\nAwesome MediaPipe - A curated list of awesome MediaPipe related frameworks, libraries and software\nSlack community for MediaPipe users\nDiscuss - General community discussion around MediaPipe\nAlpha disclaimer\nMediaPipe is currently in alpha at v0.7. We may be still making breaking API changes and expect to get to stable APIs by v1.0.\nContributing\nWe welcome contributions. Please follow these guidelines.\nWe use GitHub issues for tracking requests and bugs. Please post questions to the MediaPipe Stack Overflow with a mediapipe tag.",
      "link": "https://github.com/google/mediapipe"
    },
    {
      "autor": "awesome-nlp",
      "date": "NaN",
      "content": "awesome-nlp\nA curated list of resources dedicated to Natural Language Processing\nRead this in English, Traditional Chinese\nPlease read the contribution guidelines before contributing. Please add your favourite NLP resource by raising a pull request\nContents\nResearch Summaries and Trends\nProminent NLP Research Labs\nTutorials\nReading Content\nVideos and Courses\nBooks\nLibraries\nNode.js\nPython\nC++\nJava\nKotlin\nScala\nR\nClojure\nRuby\nRust\nServices\nAnnotation Tools\nDatasets\nNLP in Korean\nNLP in Arabic\nNLP in Chinese\nNLP in German\nNLP in Polish\nNLP in Spanish\nNLP in Indic Languages\nNLP in Thai\nNLP in Danish\nNLP in Vietnamese\nNLP for Dutch\nNLP in Indonesian\nNLP in Urdu\nNLP in Persian\nOther Languages\nCredits\nResearch Summaries and Trends\nNLP-Overview is an up-to-date overview of deep learning techniques applied to NLP, including theory, implementations, applications, and state-of-the-art results. This is a great Deep NLP Introduction for researchers.\nNLP-Progress tracks the progress in Natural Language Processing, including the datasets and the current state-of-the-art for the most common NLP tasks\nNLP's ImageNet moment has arrived\nACL 2018 Highlights: Understanding Representation and Evaluation in More Challenging Settings\nFour deep learning trends from ACL 2017. Part One: Linguistic Structure and Word Embeddings\nFour deep learning trends from ACL 2017. Part Two: Interpretability and Attention\nHighlights of EMNLP 2017: Exciting Datasets, Return of the Clusters, and More!\nDeep Learning for Natural Language Processing (NLP): Advancements & Trends\nSurvey of the State of the Art in Natural Language Generation\nProminent NLP Research Labs\nBack to Top\nThe Berkeley NLP Group - Notable contributions include a tool to reconstruct long dead languages, referenced here and by taking corpora from 637 languages currently spoken in Asia and the Pacific and recreating their descendant.\nLanguage Technologies Institute, Carnegie Mellon University - Notable projects include Avenue Project, a syntax driven machine translation system for endangered languages like Quechua and Aymara and previously, Noah's Ark which created AQMAR to improve NLP tools for Arabic.\nNLP research group, Columbia University - Responsible for creating BOLT ( interactive error handling for speech translation systems) and an un-named project to characterize laughter in dialogue.\nThe Center or Language and Speech Processing, John Hopkins University - Recently in the news for developing speech recognition software to create a diagnostic test or Parkinson's Disease, here.\nComputational Linguistics and Information Processing Group, University of Maryland - Notable contributions include Human-Computer Cooperation or Word-by-Word Question Answering and modeling development of phonetic representations.\nPenn Natural Language Processing, University of Pennsylvania- Famous for creating the Penn Treebank.\nThe Stanford Nautral Language Processing Group- One of the top NLP research labs in the world, notable for creating Stanford CoreNLP and their coreference resolution system\nTutorials\nBack to Top\nReading Content\nGeneral Machine Learning\nMachine Learning 101 from Google's Senior Creative Engineer explains Machine Learning for engineer's and executives alike\nAI Playbook - a16z AI playbook is a great link to forward to your managers or content for your presentations\nRuder's Blog by Sebastian Ruder for commentary on the best of NLP Research\nHow To Label Data guide to managing larger linguistic annotation projects\nDepends on the Definition collection of blog posts covering a wide array of NLP topics with detailed implementation\nIntroductions and Guides to NLP\nUnderstand & Implement Natural Language Processing\nNLP in Python - Collection of Github notebooks\nNatural Language Processing: An Introduction - Oxford\nDeep Learning for NLP with Pytorch\nHands-On NLTK Tutorial - NLTK Tutorials, Jupyter notebooks\nTrain a new language model from scratch - Hugging Face \ud83e\udd17\nThe Super Duper NLP Repo (SDNLPR): Collection of Colab notebooks covering a wide array of NLP task implementations.\nBlogs and Newsletters\nDeep Learning, NLP, and Representations\nThe Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) and The Illustrated Transformer\nNatural Language Processing by Hal Daum\u00e9 III\narXiv: Natural Language Processing (Almost) from Scratch\nKarpathy's The Unreasonable Effectiveness of Recurrent Neural Networks\nMachine Learning Mastery: Deep Learning for Natural Language Processing\nVisual NLP Paper Summaries\nVideos and Online Courses\nBack to Top\nDeep Natural Language Processing - Lectures series from Oxford\nDeep Learning for Natural Language Processing (cs224-n) - Richard Socher and Christopher Manning's Stanford Course\nNeural Networks for NLP - Carnegie Mellon Language Technology Institute there\nDeep NLP Course by Yandex Data School, covering important ideas from text embedding to machine translation including sequence modeling, language models and so on.\nfast.ai Code-First Intro to Natural Language Processing - This covers a blend of traditional NLP topics (including regex, SVD, naive bayes, tokenization) and recent neural network approaches (including RNNs, seq2seq, GRUs, and the Transformer), as well as addressing urgent ethical issues, such as bias and disinformation. Find the Jupyter Notebooks here\nMachine Learning University - Accelerated Natural Language Processing - Lectures go from introduction to NLP and text processing to Recurrent Neural Networks and Transformers. Material can be found here.\nApplied Natural Language Processing- Lecture series from IIT Madras taking from the basics all the way to autoencoders and everything. The github notebooks for this course are also available here\nBooks\nSpeech and Language Processing - free, by Prof. Dan Jurafsy\nNatural Language Processing - free, NLP notes by Dr. Jacob Eisenstein at GeorgiaTech\nNLP with PyTorch - Brian & Delip Rao\nText Mining in R\nNatural Language Processing with Python\nPractical Natural Language Processing\nNatural Language Processing with Spark NLP\nDeep Learning for Natural Language Processing by Stephan Raaijmakers\nReal-World Natural Language Processing - by Masato Hagiwara\nNatural Language Processing in Action, Second Edition - by Hobson Lane and Maria Dyshel\nLibraries\nBack to Top\nNode.js and Javascript - Node.js Libaries for NLP | Back to Top\nTwitter-text - A JavaScript implementation of Twitter's text processing library\nKnwl.js - A Natural Language Processor in JS\nRetext - Extensible system for analyzing and manipulating natural language\nNLP Compromise - Natural Language processing in the browser\nNatural - general natural language facilities for node\nPoplar - A web-based annotation tool for natural language processing (NLP)\nNLP.js - An NLP library for building bots\nnode-question-answering - Fast and production-ready question answering w/ DistilBERT in Node.js\nPython - Python NLP Libraries | Back to Top\nTextAttack - Adversarial attacks, adversarial training, and data augmentation in NLP\nTextBlob - Providing a consistent API for diving into common natural language processing (NLP) tasks. Stands on the giant shoulders of Natural Language Toolkit (NLTK) and Pattern, and plays nicely with both \ud83d\udc4d\nspaCy - Industrial strength NLP with Python and Cython \ud83d\udc4d\ntextacy - Higher level NLP built on spaCy\ngensim - Python library to conduct unsupervised semantic modelling from plain text \ud83d\udc4d\nscattertext - Python library to produce d3 visualizations of how language differs between corpora\nGluonNLP - A deep learning toolkit for NLP, built on MXNet/Gluon, for research prototyping and industrial deployment of state-of-the-art models on a wide range of NLP tasks.\nAllenNLP - An NLP research library, built on PyTorch, for developing state-of-the-art deep learning models on a wide variety of linguistic tasks.\nPyTorch-NLP - NLP research toolkit designed to support rapid prototyping with better data loaders, word vector loaders, neural network layer representations, common NLP metrics such as BLEU\nRosetta - Text processing tools and wrappers (e.g. Vowpal Wabbit)\nPyNLPl - Python Natural Language Processing Library. General purpose NLP library for Python. Also contains some specific modules for parsing common NLP formats, most notably for FoLiA, but also ARPA language models, Moses phrasetables, GIZA++ alignments.\nPySS3 - Python package that implements a novel white-box machine learning model for text classification, called SS3. Since SS3 has the ability to visually explain its rationale, this package also comes with easy-to-use interactive visualizations tools (online demos).\njPTDP - A toolkit for joint part-of-speech (POS) tagging and dependency parsing. jPTDP provides pre-trained models for 40+ languages.\nBigARTM - a fast library for topic modelling\nSnips NLU - A production ready library for intent parsing\nChazutsu - A library for downloading&parsing standard NLP research datasets\nWord Forms - Word forms can accurately generate all possible forms of an English word\nMultilingual Latent Dirichlet Allocation (LDA) - A multilingual and extensible document clustering pipeline\nNLP Architect - A library for exploring the state-of-the-art deep learning topologies and techniques for NLP and NLU\nFlair - A very simple framework for state-of-the-art multilingual NLP built on PyTorch. Includes BERT, ELMo and Flair embeddings.\nKashgari - Simple, Keras-powered multilingual NLP framework, allows you to build your models in 5 minutes for named entity recognition (NER), part-of-speech tagging (PoS) and text classification tasks. Includes BERT and word2vec embedding.\nFARM - Fast & easy transfer learning for NLP. Harvesting language models for the industry. Focus on Question Answering.\nHaystack - End-to-end Python framework for building natural language search interfaces to data. Leverages Transformers and the State-of-the-Art of NLP. Supports DPR, Elasticsearch, HuggingFace\u2019s Modelhub, and much more!\nRita DSL - a DSL, loosely based on RUTA on Apache UIMA. Allows to define language patterns (rule-based NLP) which are then translated into spaCy, or if you prefer less features and lightweight - regex patterns.\nTransformers - Natural Language Processing for TensorFlow 2.0 and PyTorch.\nTokenizers - Tokenizers optimized for Research and Production.\nfairSeq Facebook AI Research implementations of SOTA seq2seq models in Pytorch.\ncorex_topic - Hierarchical Topic Modeling with Minimal Domain Knowledge\nSockeye - Neural Machine Translation (NMT) toolkit that powers Amazon Translate.\nDL Translate - A deep learning-based translation library for 50 languages, built on transformers and Facebook's mBART Large.\nC++ - C++ Libraries | Back to Top\nInsNet - A neural network library for building instance-dependent NLP models with padding-free dynamic batching.\nMIT Information Extraction Toolkit - C, C++, and Python tools for named entity recognition and relation extraction\nCRF++ - Open source implementation of Conditional Random Fields (CRFs) for segmenting/labeling sequential data & other Natural Language Processing tasks.\nCRFsuite - CRFsuite is an implementation of Conditional Random Fields (CRFs) for labeling sequential data.\nBLLIP Parser - BLLIP Natural Language Parser (also known as the Charniak-Johnson parser)\ncolibri-core - C++ library, command line tools, and Python binding for extracting and working with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.\nucto - Unicode-aware regular-expression based tokenizer for various languages. Tool and C++ library. Supports FoLiA format.\nlibfolia - C++ library for the FoLiA format\nfrog - Memory-based NLP suite developed for Dutch: PoS tagger, lemmatiser, dependency parser, NER, shallow parser, morphological analyzer.\nMeTA - MeTA : ModErn Text Analysis is a C++ Data Sciences Toolkit that facilitates mining big text data.\nMecab (Japanese)\nMoses\nStarSpace - a library from Facebook for creating embeddings of word-level, paragraph-level, document-level and for text classification\nJava - Java NLP Libraries | Back to Top\nStanford NLP\nOpenNLP\nNLP4J\nWord2vec in Java\nReVerb Web-Scale Open Information Extraction\nOpenRegex An efficient and flexible token-based regular expression language and engine.\nCogcompNLP - Core libraries developed in the U of Illinois' Cognitive Computation Group.\nMALLET - MAchine Learning for LanguagE Toolkit - package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.\nRDRPOSTagger - A robust POS tagging toolkit available (in both Java & Python) together with pre-trained models for 40+ languages.\nKotlin - Kotlin NLP Libraries | Back to Top\nLingua A language detection library for Kotlin and Java, suitable for long and short text alike\nKotidgy \u2014 an index-based text data generator written in Kotlin\nScala - Scala NLP Libraries | Back to Top\nSaul - Library for developing NLP systems, including built in modules like SRL, POS, etc.\nATR4S - Toolkit with state-of-the-art automatic term recognition methods.\ntm - Implementation of topic modeling based on regularized multilingual PLSA.\nword2vec-scala - Scala interface to word2vec model; includes operations on vectors like word-distance and word-analogy.\nEpic - Epic is a high performance statistical parser written in Scala, along with a framework for building complex structured prediction models.\nSpark NLP - Spark NLP is a natural language processing library built on top of Apache Spark ML that provides simple, performant & accurate NLP annotations for machine learning pipelines that scale easily in a distributed environment.\nR - R NLP Libraries | Back to Top\ntext2vec - Fast vectorization, topic modeling, distances and GloVe word embeddings in R.\nwordVectors - An R package for creating and exploring word2vec and other word embedding models\nRMallet - R package to interface with the Java machine learning tool MALLET\ndfr-browser - Creates d3 visualizations for browsing topic models of text in a web browser.\ndfrtopics - R package for exploring topic models of text.\nsentiment_classifier - Sentiment Classification using Word Sense Disambiguation and WordNet Reader\njProcessing - Japanese Natural Langauge Processing Libraries, with Japanese sentiment classification\nClojure | Back to Top\nClojure-openNLP - Natural Language Processing in Clojure (opennlp)\nInfections-clj - Rails-like inflection library for Clojure and ClojureScript\npostagga - A library to parse natural language in Clojure and ClojureScript\nRuby | Back to Top\nKevin Dias's A collection of Natural Language Processing (NLP) Ruby libraries, tools and software\nPractical Natural Language Processing done in Ruby\nRust\nwhatlang \u2014 Natural language recognition library based on trigrams\nsnips-nlu-rs - A production ready library for intent parsing\nrust-bert - Ready-to-use NLP pipelines and Transformer-based models\nNLP++ - NLP++ Langauge | Back to Top\nVSCode Language Extension - NLP++ Language Extension for VSCode\nnlp-engine - NLP++ engine to run NLP++ code on Linux including a full English parser\nVisualText - Homepage for the NLP++ Language\nNLP++ Wiki - Wiki entry for the NLP++ language\nServices\nNLP as API with higher level functionality such as NER, Topic tagging and so on | Back to Top\nWit-ai - Natural Language Interface for apps and devices\nIBM Watson's Natural Language Understanding - API and Github demo\nAmazon Comprehend - NLP and ML suite covers most common tasks like NER, tagging, and sentiment analysis\nGoogle Cloud Natural Language API - Syntax Analysis, NER, Sentiment Analysis, and Content tagging in atleast 9 languages include English and Chinese (Simplified and Traditional).\nParallelDots - High level Text Analysis API Service ranging from Sentiment Analysis to Intent Analysis\nMicrosoft Cognitive Service\nTextRazor\nRosette\nTextalytic - Natural Language Processing in the Browser with sentiment analysis, named entity extraction, POS tagging, word frequencies, topic modeling, word clouds, and more\nNLP Cloud - SpaCy NLP models (custom and pre-trained ones) served through a RESTful API for named entity recognition (NER), POS tagging, and more.\nCloudmersive - Unified and free NLP APIs that perform actions such as speech tagging, text rephrasing, language translation/detection, and sentence parsing\nAnnotation Tools\nGATE - General Architecture and Text Engineering is 15+ years old, free and open source\nAnafora is free and open source, web-based raw text annotation tool\nbrat - brat rapid annotation tool is an online environment for collaborative text annotation\ndoccano - doccano is free, open-source, and provides annotation features for text classification, sequence labeling and sequence to sequence\nINCEpTION - A semantic annotation platform offering intelligent assistance and knowledge management\ntagtog, team-first web tool to find, create, maintain, and share datasets - costs $\nprodigy is an annotation tool powered by active learning, costs $\nLightTag - Hosted and managed text annotation tool for teams, costs $\nrstWeb - open source local or online tool for discourse tree annotations\nGitDox - open source server annotation tool with GitHub version control and validation for XML data and collaborative spreadsheet grids\nLabel Studio - Hosted and managed text annotation tool for teams, freemium based, costs $\nDatasaur support various NLP tasks for individual or teams, freemium based\nKonfuzio - team-first hosted and on-prem text, image and PDF annotation tool powered by active learning, freemium based, costs $\nUBIAI - Easy-to-use text annotation tool for teams with most comprehensive auto-annotation features. Supports NER, relations and document classification as well as OCR annotation for invoice labeling, costs $\nTechniques\nText Embeddings\nWord Embeddings\nThumb Rule: fastText >> GloVe > word2vec\nword2vec - implementation - explainer blog\nglove - explainer blog\nfasttext - implementation - paper - explainer blog\nSentence and Language Model Based Word Embeddings\nBack to Top\nElMo - Deep Contextualized Word Representations - PyTorch implmentation - TF Implementation\nULMFiT - Universal Language Model Fine-tuning for Text Classification by Jeremy Howard and Sebastian Ruder\nInferSent - Supervised Learning of Universal Sentence Representations from Natural Language Inference Data by facebook\nCoVe - Learned in Translation: Contextualized Word Vectors\nPargraph vectors - from Distributed Representations of Sentences and Documents. See doc2vec tutorial at gensim\nsense2vec - on word sense disambiguation\nSkip Thought Vectors - word representation method\nAdaptive skip-gram - similar approach, with adaptive properties\nSequence to Sequence Learning - word vectors for machine translation\nQuestion Answering and Knowledge Extraction\nBack to Top\nDrQA - Open Domain Question Answering work by Facebook Research on Wikipedia data\nDocument-QA - Simple and Effective Multi-Paragraph Reading Comprehension by AllenAI\nTemplate-Based Information Extraction without the Templates\nPrivee: An Architecture for Automatically Analyzing Web Privacy Policies\nDatasets\nBack to Top\nnlp-datasets great collection of nlp datasets\ngensim-data - Data repository for pretrained NLP models and NLP corpora.\nMultilingual NLP Frameworks\nBack to Top\nUDPipe is a trainable pipeline for tokenizing, tagging, lemmatizing and parsing Universal Treebanks and other CoNLL-U files. Primarily written in C++, offers a fast and reliable solution for multilingual NLP processing.\nNLP-Cube : Natural Language Processing Pipeline - Sentence Splitting, Tokenization, Lemmatization, Part-of-speech Tagging and Dependency Parsing. New platform, written in Python with Dynet 2.0. Offers standalone (CLI/Python bindings) and server functionality (REST API).\nUralicNLP is an NLP library mostly for many endangered Uralic languages such as Sami languages, Mordvin languages, Mari languages, Komi languages and so on. Also some non-endangered languages are supported such as Finnish together with non-Uralic languages such as Swedish and Arabic. UralicNLP can do morphological analysis, generation, lemmatization and disambiguation.\nNLP in Korean\nBack to Top\nLibraries\nKoNLPy - Python package for Korean natural language processing.\nMecab (Korean) - C++ library for Korean NLP\nKoalaNLP - Scala library for Korean Natural Language Processing.\nKoNLP - R package for Korean Natural language processing\nBlogs and Tutorials\ndsindex's blog\nKangwon University's NLP course in Korean\nDatasets\nKAIST Corpus - A corpus from the Korea Advanced Institute of Science and Technology in Korean.\nNaver Sentiment Movie Corpus in Korean\nChosun Ilbo archive - dataset in Korean from one of the major newspapers in South Korea, the Chosun Ilbo.\nChat data - Chatbot data in Korean\nPetitions - Collect expired petition data from the Blue House National Petition Site.\nKorean Parallel corpora - Neural Machine Translation(NMT) Dataset for Korean to French & Korean to English\nKorQuAD - Korean SQuAD dataset with Wiki HTML source. Mentions both v1.0 and v2.1 at the time of adding to Awesome NLP\nNLP in Arabic\nBack to Top\nLibraries\ngoarabic - Go package for Arabic text processing\njsastem - Javascript for Arabic stemming\nPyArabic - Python libraries for Arabic\nRFTokenizer - trainable Python segmenter for Arabic, Hebrew and Coptic\nDatasets\nMultidomain Datasets - Largest Available Multi-Domain Resources for Arabic Sentiment Analysis\nLABR - LArge Arabic Book Reviews dataset\nArabic Stopwords - A list of Arabic stopwords from various resources\nNLP in Chinese\nBack to Top\nLibraries\njieba - Python package for Words Segmentation Utilities in Chinese\nSnowNLP - Python package for Chinese NLP\nFudanNLP - Java library for Chinese text processing\nAnthology\nfunNLP - Collection of NLP tools and resources mainly for Chinese\nNLP in German\nGerman-NLP - Curated list of open-access/open-source/off-the-shelf resources and tools developed with a particular focus on German\nNLP in Polish\nPolish-NLP - A curated list of resources dedicated to Natural Language Processing (NLP) in polish. Models, tools, datasets.\nNLP in Spanish\nBack to Top\nLibraries\nspanlp - Python library to detect, censor and clean profanity, vulgarities, hateful words, racism, xenophobia and bullying in texts written in Spanish. It contains data of 21 Spanish-speaking countries.\nData\nColumbian Political Speeches\nCopenhagen Treebank\nSpanish Billion words corpus with Word2Vec embeddings\nCompilation of Spanish Unannotated Corpora\nWord and Sentence Embeddings\nSpanish Word Embeddings Computed with Different Methods and from Different Corpora\nSpanish Word Embeddings Computed from Large Corpora and Different Sizes Using fastText\nSpanish Sentence Embeddings Computed from Large Corpora Using sent2vec\nBeto - BERT for Spanish\nNLP in Indic languages\nBack to Top\nData, Corpora and Treebanks\nHindi Dependency Treebank - A multi-representational multi-layered treebank for Hindi and Urdu\nUniversal Dependencies Treebank in Hindi\nParallel Universal Dependencies Treebank in Hindi - A smaller part of the above-mentioned treebank.\nISI FIRE Stopwords List (Hindi and Bangla)\nPeter Graham's Stopwords List\nNLTK Corpus 60k Words POS Tagged, Bangla, Hindi, Marathi, Telugu\nHindi Movie Reviews Dataset ~1k Samples, 3 polarity classes\nBBC News Hindi Dataset 4.3k Samples, 14 classes\nIIT Patna Hindi ABSA Dataset 5.4k Samples, 12 Domains, 4k aspect terms, aspect and sentence level polarity in 4 classes\nBangla ABSA 5.5k Samples, 2 Domains, 10 aspect terms\nIIT Patna Movie Review Sentiment Dataset 2k Samples, 3 polarity labels\nCorpora/Datasets that need a login/access can be gained via email\nSAIL 2015 Twitter and Facebook labelled sentiment samples in Hindi, Bengali, Tamil, Telugu.\nIIT Bombay NLP Resources Sentiwordnet, Movie and Tourism parallel labelled corpora, polarity labelled sense annotated corpus, Marathi polarity labelled corpus.\nTDIL-IC aggregates a lot of useful resources and provides access to otherwise gated datasets\nLanguage Models and Word Embeddings\nHindi2Vec and nlp-for-hindi ULMFIT style languge model\nIIT Patna Bilingual Word Embeddings Hi-En\nFasttext word embeddings in a whole bunch of languages, trained on Common Crawl\nHindi and Bengali Word2Vec\nHindi and Urdu Elmo Model\nSanskrit Albert Trained on Sanskrit Wikipedia and OSCAR corpus\nLibraries and Tooling\nMulti-Task Deep Morphological Analyzer Deep Network based Morphological Parser for Hindi and Urdu\nAnoop Kunchukuttan 18 Languages, whole host of features from tokenization to translation\nSivaReddy's Dependency Parser Dependency Parser and Pos Tagger for Kannada, Hindi and Telugu. Python3 Port\niNLTK - A Natural Language Toolkit for Indic Languages (Indian subcontinent languages) built on top of Pytorch/Fastai, which aims to provide out of the box support for common NLP tasks.\nNLP in Thai\nBack to Top\nLibraries\nPyThaiNLP - Thai NLP in Python Package\nJTCC - A character cluster library in Java\nCutKum - Word segmentation with deep learning in TensorFlow\nThai Language Toolkit - Based on a paper by Wirote Aroonmanakun in 2002 with included dataset\nSynThai - Word segmentation and POS tagging using deep learning in Python\nData\nInter-BEST - A text corpus with 5 million words with word segmentation\nPrime Minister 29 - Dataset containing speeches of the current Prime Minister of Thailand\nNLP in Danish\nNamed Entity Recognition for Danish\nDaNLP - NLP resources in Danish\nAwesome Danish - A curated list of awesome resources for Danish language technology\nNLP in Vietnamese\nLibraries\nunderthesea - Vietnamese NLP Toolkit\nvn.vitk - A Vietnamese Text Processing Toolkit\nVnCoreNLP - A Vietnamese natural language processing toolkit\nPhoBERT - Pre-trained language models for Vietnamese\npyvi - Python Vietnamese Core NLP Toolkit\nData\nVietnamese treebank - 10,000 sentences for the constituency parsing task\nBKTreeBank - a Vietnamese Dependency Treebank\nUD_Vietnamese - Vietnamese Universal Dependency Treebank\nVIVOS - a free Vietnamese speech corpus consisting of 15 hours of recording speech by AILab\nVNTQcorpus(big).txt - 1.75 million sentences in news\nViText2SQL - A dataset for Vietnamese Text-to-SQL semantic parsing (EMNLP-2020 Findings)\nEVB Corpus - 20,000,000 words (20 million) from 15 bilingual books, 100 parallel English-Vietnamese / Vietnamese-English texts, 250 parallel law and ordinance texts, 5,000 news articles, and 2,000 -----> film !!!  subtitles.\nNLP for Dutch\nBack to Top\npython-frog - Python binding to Frog, an NLP suite for Dutch. (pos tagging, lemmatisation, dependency parsing, NER)\nSimpleNLG_NL - Dutch surface realiser used for Natural Language Generation in Dutch, based on the SimpleNLG implementation for English and French.\nNLP in Indonesian\nDatasets\nKompas and Tempo collections at ILPS\nPANL10N for PoS tagging: 39K sentences and 900K word tokens\nIDN for PoS tagging: This corpus contains 10K sentences and 250K word tokens\nIndonesian Treebank and Universal Dependencies-Indonesian\nIndoSum for text summarization and classification both\nWordnet-Bahasa - large, free, semantic dictionary\nIndoBenchmark IndoNLU includes pre-trained language model (IndoBERT), FastText model, Indo4B corpus, and several NLU benchmark datasets\nLibraries & Embedding\nNatural language toolkit bahasa\nIndonesian Word Embedding\nPretrained Indonesian fastText Text Embedding trained on Wikipedia\nIndoBenchmark IndoNLU includes pretrained language model (IndoBERT), FastText model, Indo4B corpus, and several NLU benchmark datasets\nNLP in Urdu\nDatasets\nCollection of Urdu datasets for POS, NER and NLP tasks\nLibraries\nNatural Language Processing library for ( \ud83c\uddf5\ud83c\uddf0)Urdu language\nNLP in Persian\nBack to Top\nLibraries\nHazm: Python library for digesting Persian text.\nParsivar: A Language Processing Toolkit for Persian\nPerke: Perke is a Python keyphrase extraction package for Persian language. It provides an end-to-end keyphrase extraction pipeline in which each component can be easily modified or extended to develop new models.\nPerstem: Persian stemmer, morphological analyzer, transliterator, and partial part-of-speech tagger\nParsiAnalyzer: Persian Analyzer For Elasticsearch\nvirastar: Cleaning up Persian text!\nDatasets\nBijankhan Corpus: Bijankhan corpus is a tagged corpus that is suitable for natural language processing research on the Persian (Farsi) language. This collection is gathered form daily news and common texts. In this collection all documents are categorized into different subjects such as political, cultural and so on. Totally, there are 4300 different subjects. The Bijankhan collection contains about 2.6 millions manually tagged words with a tag set that contains 40 Persian POS tags.\nUppsala Persian Corpus (UPC): Uppsala Persian Corpus (UPC) is a large, freely available Persian corpus. The corpus is a modified version of the Bijankhan corpus with additional sentence segmentation and consistent tokenization containing 2,704,028 tokens and annotated with 31 part-of-speech tags. The part-of-speech tags are listed with explanations in this table.\nLarge-Scale Colloquial Persian: Large Scale Colloquial Persian Dataset (LSCP) is hierarchically organized in asemantic taxonomy that focuses on multi-task informal Persian language understanding as a comprehensive problem. LSCP includes 120M sentences from 27M casual Persian tweets with its dependency relations in syntactic annotation, Part-of-speech tags, sentiment polarity and automatic translation of original Persian sentences in English (EN), German (DE), Czech (CS), Italian (IT) and Hindi (HI) spoken languages. Learn more about this project at LSCP webpage.\nArmanPersoNERCorpus: The dataset includes 250,015 tokens and 7,682 Persian sentences in total. It is available in 3 folds to be used in turn as training and test sets. Each file contains one token, along with its manually annotated named-entity tag, per line. Each sentence is separated with a newline. The NER tags are in IOB format.\nFarsiYar PersianNER: The dataset includes about 25,000,000 tokens and about 1,000,000 Persian sentences in total based on Persian Wikipedia Corpus. The NER tags are in IOB format. More than 1000 volunteers contributed tag improvements to this dataset via web panel or android app. They release updated tags every two weeks.\nPERLEX: The first Persian dataset for relation extraction, which is an expert translated version of the \u201cSemeval-2010-Task-8\u201d dataset. Link to the relevant publication.\nPersian Syntactic Dependency Treebank: This treebank is supplied for free noncommercial use. For commercial uses feel free to contact us. The number of annotated sentences is 29,982 sentences including samples from almost all verbs of the Persian valency lexicon.\nUppsala Persian Dependency Treebank (UPDT): Dependency-based syntactically annotated corpus.\nHamshahri: Hamshahri collection is a standard reliable Persian text collection that was used at Cross Language Evaluation Forum (CLEF) during years 2008 and 2009 for evaluation of Persian information retrieval systems.\nOther Languages\nRussian: pymorphy2 - a good pos-tagger for Russian\nAsian Languages: Thai, Lao, Chinese, Japanese, and Korean ICU Tokenizer implementation in ElasticSearch\nAncient Languages: CLTK: The Classical Language Toolkit is a Python library and collection of texts for doing NLP in ancient languages\nHebrew: NLPH_Resources - A collection of papers, corpora and linguistic resources for NLP in Hebrew\nBack to Top\nCredits for initial curators and sources\nLicense\nLicense - CC0",
      "link": "https://github.com/keon/awesome-nlp"
    },
    {
      "autor": "the-gan-zoo",
      "date": "NaN",
      "content": "The GAN Zoo\nEvery week, new GAN papers are coming out and it's hard to keep track of them all, not to mention the incredibly creative ways in which researchers are naming these GANs! So, here's a list of what started as a fun activity compiling all named GANs!\nYou can also check out the same data in a tabular format with functionality to filter by year or do a quick search by title here.\nContributions are welcome. Add links through pull requests in gans.tsv file in the same format or create an issue to lemme know something I missed or to start a discussion.\nCheck out Deep Hunt - my weekly AI newsletter for this repo as blogpost and follow me on Twitter.\n3D-ED-GAN - Shape Inpainting using 3D Generative Adversarial Network and Recurrent Convolutional Networks\n3D-GAN - Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling (github)\n3D-IWGAN - Improved Adversarial Systems for 3D Object Generation and Reconstruction (github)\n3D-PhysNet - 3D-PhysNet: Learning the Intuitive Physics of Non-Rigid Object Deformations\n3D-RecGAN - 3D Object Reconstruction from a Single Depth View with Adversarial Learning (github)\nABC-GAN - ABC-GAN: Adaptive Blur and Control for improved training stability of Generative Adversarial Networks (github)\nABC-GAN - GANs for LIFE: Generative Adversarial Networks for Likelihood Free Inference\nAC-GAN - Conditional Image Synthesis With Auxiliary Classifier GANs\nacGAN - Face Aging With Conditional Generative Adversarial Networks\nACGAN - Coverless Information Hiding Based on Generative adversarial networks\nacGAN - On-line Adaptative Curriculum Learning for GANs\nACtuAL - ACtuAL: Actor-Critic Under Adversarial Learning\nAdaGAN - AdaGAN: Boosting Generative Models\nAdaptive GAN - Customizing an Adversarial Example Generator with Class-Conditional GANs\nAdvEntuRe - AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided Examples\nAdvGAN - Generating adversarial examples with adversarial networks\nAE-GAN - AE-GAN: adversarial eliminating with GAN\nAE-OT - Latent Space Optimal Transport for Generative Models\nAEGAN - Learning Inverse Mapping by Autoencoder based Generative Adversarial Nets\nAF-DCGAN - AF-DCGAN: Amplitude Feature Deep Convolutional GAN for Fingerprint Construction in Indoor Localization System\nAffGAN - Amortised MAP Inference for Image Super-resolution\nAIM - Generating Informative and Diverse Conversational Responses via Adversarial Information Maximization\nAL-CGAN - Learning to Generate Images of Outdoor Scenes from Attributes and Semantic Layouts\nALI - Adversarially Learned Inference (github)\nAlignGAN - AlignGAN: Learning to Align Cross-Domain Images with Conditional Generative Adversarial Networks\nAlphaGAN - AlphaGAN: Generative adversarial networks for natural image matting\nAM-GAN - Activation Maximization Generative Adversarial Nets\nAmbientGAN - AmbientGAN: Generative models from lossy measurements (github)\nAMC-GAN - Video Prediction with Appearance and Motion Conditions\nAnoGAN - Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery\nAPD - Adversarial Distillation of Bayesian Neural Network Posteriors\nAPE-GAN - APE-GAN: Adversarial Perturbation Elimination with GAN\nARAE - Adversarially Regularized Autoencoders for Generating Discrete Structures (github)\nARDA - Adversarial Representation Learning for Domain Adaptation\nARIGAN - ARIGAN: Synthetic Arabidopsis Plants using Generative Adversarial Network\nArtGAN - ArtGAN: Artwork Synthesis with Conditional Categorial GANs\nASDL-GAN - Automatic Steganographic Distortion Learning Using a Generative Adversarial Network\nATA-GAN - Attention-Aware Generative Adversarial Networks (ATA-GANs)\nAttention-GAN - Attention-GAN for Object Transfiguration in Wild Images\nAttGAN - Arbitrary Facial Attribute Editing: Only Change What You Want (github)\nAttnGAN - AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks (github)\nAVID - AVID: Adversarial Visual Irregularity Detection\nB-DCGAN - B-DCGAN:Evaluation of Binarized DCGAN for FPGA\nb-GAN - Generative Adversarial Nets from a Density Ratio Estimation Perspective\nBAGAN - BAGAN: Data Augmentation with Balancing GAN\nBayesian GAN - Deep and Hierarchical Implicit Models\nBayesian GAN - Bayesian GAN (github)\nBCGAN - Bayesian Conditional Generative Adverserial Networks\nBCGAN - Bidirectional Conditional Generative Adversarial networks\nBEAM - Boltzmann Encoded Adversarial Machines\nBEGAN - BEGAN: Boundary Equilibrium Generative Adversarial Networks\nBEGAN-CS - Escaping from Collapsing Modes in a Constrained Space\nBellman GAN - Distributional Multivariate Policy Evaluation and Exploration with the Bellman GAN\nBGAN - Binary Generative Adversarial Networks for Image Retrieval (github)\nBi-GAN - Autonomously and Simultaneously Refining Deep Neural Network Parameters by a Bi-Generative Adversarial Network Aided Genetic Algorithm\nBicycleGAN - Toward Multimodal Image-to-Image Translation (github)\nBiGAN - Adversarial Feature Learning\nBinGAN - BinGAN: Learning Compact Binary Descriptors with a Regularized GAN\nBourGAN - BourGAN: Generative Networks with Metric Embeddings\nBranchGAN - Branched Generative Adversarial Networks for Multi-Scale Image Manifold Learning\nBRE - Improving GAN Training via Binarized Representation Entropy (BRE) Regularization (github)\nBridgeGAN - Generative Adversarial Frontal View to Bird View Synthesis\nBS-GAN - Boundary-Seeking Generative Adversarial Networks\nBubGAN - BubGAN: Bubble Generative Adversarial Networks for Synthesizing Realistic Bubbly Flow Images\nBWGAN - Banach Wasserstein GAN\nC-GAN - Face Aging with Contextual Generative Adversarial Nets\nC-RNN-GAN - C-RNN-GAN: Continuous recurrent neural networks with adversarial training (github)\nCA-GAN - Composition-aided Sketch-realistic Portrait Generation\nCaloGAN - CaloGAN: Simulating 3D High Energy Particle Showers in Multi-Layer Electromagnetic Calorimeters with Generative Adversarial Networks (github)\nCAN - CAN: Creative Adversarial Networks, Generating Art by Learning About Styles and Deviating from Style Norms\nCapsGAN - CapsGAN: Using Dynamic Routing for Generative Adversarial Networks\nCapsuleGAN - CapsuleGAN: Generative Adversarial Capsule Network\nCatGAN - Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks\nCatGAN - CatGAN: Coupled Adversarial Transfer for Domain Generation\nCausalGAN - CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training\nCC-GAN - Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks (github)\ncd-GAN - Conditional Image-to-Image Translation\nCDcGAN - Simultaneously Color-Depth Super-Resolution with Conditional Generative Adversarial Network\nCE-GAN - Deep Learning for Imbalance Data Classification using Class Expert Generative Adversarial Network\nCFG-GAN - Composite Functional Gradient Learning of Generative Adversarial Models\nCGAN - Conditional Generative Adversarial Nets\nCGAN - Controllable Generative Adversarial Network\nChekhov GAN - An Online Learning Approach to Generative Adversarial Networks\nciGAN - Conditional Infilling GANs for Data Augmentation in Mammogram Classification\nCinCGAN - Unsupervised Image Super-Resolution using Cycle-in-Cycle Generative Adversarial Networks\nCipherGAN - Unsupervised Cipher Cracking Using Discrete GANs\nClusterGAN - ClusterGAN : Latent Space Clustering in Generative Adversarial Networks\nCM-GAN - CM-GANs: Cross-modal Generative Adversarial Networks for Common Representation Learning\nCoAtt-GAN - Are You Talking to Me? Reasoned Visual Dialog Generation through Adversarial Learning\nCoGAN - Coupled Generative Adversarial Networks\nComboGAN - ComboGAN: Unrestrained Scalability for Image Domain Translation (github)\nConceptGAN - Learning Compositional Visual Concepts with Mutual Consistency\nConditional cycleGAN - Conditional CycleGAN for Attribute Guided Face Image Generation\nconstrast-GAN - Generative Semantic Manipulation with Contrasting GAN\nContext-RNN-GAN - Contextual RNN-GANs for Abstract Reasoning Diagram Generation\nCorrGAN - Correlated discrete data generation using adversarial training\nCoulomb GAN - Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields\nCover-GAN - Generative Steganography with Kerckhoffs' Principle based on Generative Adversarial Networks\ncowboy - Defending Against Adversarial Attacks by Leveraging an Entire GAN\nCR-GAN - CR-GAN: Learning Complete Representations for Multi-view Generation\nCram\u00e8r GAN - The Cramer Distance as a Solution to Biased Wasserstein Gradients\nCross-GAN - Crossing Generative Adversarial Networks for Cross-View Person Re-identification\ncrVAE-GAN - Channel-Recurrent Variational Autoencoders\nCS-GAN - Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets\nCSG - Speech-Driven Expressive Talking Lips with Conditional Sequential Generative Adversarial Networks\nCT-GAN - CT-GAN: Conditional Transformation Generative Adversarial Network for Image Attribute Modification\nCVAE-GAN - CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training\nCycleGAN - Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (github)\nD-GAN - Differential Generative Adversarial Networks: Synthesizing Non-linear Facial Variations with Limited Number of Training Data\nD-WCGAN - I-vector Transformation Using Conditional Generative Adversarial Networks for Short Utterance Speaker Verification\nD2GAN - Dual Discriminator Generative Adversarial Nets\nD2IA-GAN - Tagging like Humans: Diverse and Distinct Image Annotation\nDA-GAN - DA-GAN: Instance-level Image Translation by Deep Attention Generative Adversarial Networks (with Supplementary Materials)\nDADA - DADA: Deep Adversarial Data Augmentation for Extremely Low Data Regime Classification\nDAGAN - Data Augmentation Generative Adversarial Networks\nDAN - Distributional Adversarial Networks\nDBLRGAN - Adversarial Spatio-Temporal Learning for Video Deblurring\nDCGAN - Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (github)\nDE-GAN - Generative Adversarial Networks with Decoder-Encoder Output Noise\nDeblurGAN - DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks (github)\nDeepFD - Learning to Detect Fake Face Images in the Wild\nDefense-GAN - Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models (github)\nDefo-Net - Defo-Net: Learning Body Deformation using Generative Adversarial Networks\nDeliGAN - DeLiGAN : Generative Adversarial Networks for Diverse and Limited Data (github)\nDF-GAN - Learning Disentangling and Fusing Networks for Face Completion Under Structured Occlusions\nDialogWAE - DialogWAE: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder\nDiscoGAN - Learning to Discover Cross-Domain Relations with Generative Adversarial Networks\nDistanceGAN - One-Sided Unsupervised Domain Mapping\nDM-GAN - Dual Motion GAN for Future-Flow Embedded Video Prediction\nDMGAN - Disconnected Manifold Learning for Generative Adversarial Networks\nDNA-GAN - DNA-GAN: Learning Disentangled Representations from Multi-Attribute Images\nDOPING - DOPING: Generative Data Augmentation for Unsupervised Anomaly Detection with GAN\ndp-GAN - Differentially Private Releasing via Deep Generative Model\nDP-GAN - DP-GAN: Diversity-Promoting Generative Adversarial Network for Generating Informative and Diversified Text\nDPGAN - Differentially Private Generative Adversarial Network\nDR-GAN - Representation Learning by Rotating Your Faces\nDRAGAN - How to Train Your DRAGAN (github)\nDropout-GAN - Dropout-GAN: Learning from a Dynamic Ensemble of Discriminators\nDRPAN - Discriminative Region Proposal Adversarial Networks for High-Quality Image-to-Image Translation\nDSH-GAN - Deep Semantic Hashing with Generative Adversarial Networks\nDSP-GAN - Depth Structure Preserving Scene Image Generation\nDTLC-GAN - Generative Adversarial Image Synthesis with Decision Tree Latent Controller\nDTN - Unsupervised Cross-Domain Image Generation\nDTR-GAN - DTR-GAN: Dilated Temporal Relational Adversarial Network for Video Summarization\nDualGAN - DualGAN: Unsupervised Dual Learning for Image-to-Image Translation\nDualing GAN - Dualing GANs\nDVGAN - Human Motion Modeling using DVGANs\nDynamics Transfer GAN - Dynamics Transfer GAN: Generating Video by Transferring Arbitrary Temporal Dynamics from a Source Video to a Single Target Image\nE-GAN - Evolutionary Generative Adversarial Networks\nEAR - Generative Model for Heterogeneous Inference\nEBGAN - Energy-based Generative Adversarial Network\necGAN - eCommerceGAN : A Generative Adversarial Network for E-commerce\nED//GAN - Stabilizing Training of Generative Adversarial Networks through Regularization\nEditable GAN - Editable Generative Adversarial Networks: Generating and Editing Faces Simultaneously\nEGAN - Enhanced Experience Replay Generation for Efficient Reinforcement Learning\nEL-GAN - EL-GAN: Embedding Loss Driven Generative Adversarial Networks for Lane Detection\nELEGANT - ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes\nEnergyWGAN - Energy-relaxed Wassertein GANs (EnergyWGAN): Towards More Stable and High Resolution Image Generation\nESRGAN - ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks\nExGAN - Eye In-Painting with Exemplar Generative Adversarial Networks\nExposureGAN - Exposure: A White-Box Photo Post-Processing Framework (github)\nExprGAN - ExprGAN: Facial Expression Editing with Controllable Expression Intensity\nf-CLSWGAN - Feature Generating Networks for Zero-Shot Learning\nf-GAN - f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization\nFairGAN - FairGAN: Fairness-aware Generative Adversarial Networks\nFairness GAN - Fairness GAN\nFakeGAN - Detecting Deceptive Reviews using Generative Adversarial Networks\nFBGAN - Feedback GAN (FBGAN) for DNA: a Novel Feedback-Loop Architecture for Optimizing Protein Functions\nFBGAN - Featurized Bidirectional GAN: Adversarial Defense via Adversarially Learned Semantic Inference\nFC-GAN - Fast-converging Conditional Generative Adversarial Networks for Image Synthesis\nFF-GAN - Towards Large-Pose Face Frontalization in the Wild\nFGGAN - Adversarial Learning for Fine-grained Image Search\nFictitious GAN - Fictitious GAN: Training GANs with Historical Models\nFIGAN - Frame Interpolation with Multi-Scale Deep Loss Functions and Generative Adversarial Networks\nFila-GAN - Synthesizing Filamentary Structured Images with GANs\nFirst Order GAN - First Order Generative Adversarial Networks (github)\nFisher GAN - Fisher GAN\nFlow-GAN - Flow-GAN: Bridging implicit and prescribed learning in generative models\nFrankenGAN - rankenGAN: Guided Detail Synthesis for Building Mass-Models Using Style-Synchonized GANs\nFSEGAN - Exploring Speech Enhancement with Generative Adversarial Networks for Robust Speech Recognition\nFTGAN - Hierarchical Video Generation from Orthogonal Information: Optical Flow and Texture\nFusedGAN - Semi-supervised FusedGAN for Conditional Image Generation\nFusionGAN - Learning to Fuse Music Genres with Generative Adversarial Dual Learning\nFusionGAN - Generating a Fusion Image: One's Identity and Another's Shape\nG2-GAN - Geometry Guided Adversarial Facial Expression Synthesis\nGAAN - Generative Adversarial Autoencoder Networks\nGAF - Generative Adversarial Forests for Better Conditioned Adversarial Learning\nGAGAN - GAGAN: Geometry-Aware Generative Adverserial Networks\nGAIA - Generative adversarial interpolative autoencoding: adversarial training on latent space interpolations encourage convex latent distributions\nGAIN - GAIN: Missing Data Imputation using Generative Adversarial Nets\nGAMN - Generative Adversarial Mapping Networks\nGAN - Generative Adversarial Networks (github)\nGAN Lab - GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation\nGAN Q-learning - GAN Q-learning\nGAN-AD - Anomaly Detection with Generative Adversarial Networks for Multivariate Time Series\nGAN-ATV - A Novel Approach to Artistic Textual Visualization via GAN\nGAN-CLS - Generative Adversarial Text to Image Synthesis (github)\nGAN-RS - Towards Qualitative Advancement of Underwater Machine Vision with Generative Adversarial Networks\nGAN-SD - Virtual-Taobao: Virtualizing Real-world Online Retail Environment for Reinforcement Learning\nGAN-sep - GANs for Biological Image Synthesis (github)\nGAN-VFS - Generative Adversarial Network-based Synthesis of Visible Faces from Polarimetric Thermal Faces\nGAN-Word2Vec - Adversarial Training of Word2Vec for Basket Completion\nGANAX - GANAX: A Unified MIMD-SIMD Acceleration for Generative Adversarial Networks\nGANCS - Deep Generative Adversarial Networks for Compressed Sensing Automates MRI\nGANDI - Guiding the search in continuous state-action spaces by learning an action sampling distribution from off-target samples\nGANG - GANGs: Generative Adversarial Network Games\nGANG - Beyond Local Nash Equilibria for Adversarial Networks\nGANosaic - GANosaic: Mosaic Creation with Generative Texture Manifolds\nGANVO - GANVO: Unsupervised Deep Monocular Visual Odometry and Depth Estimation with Generative Adversarial Networks\nGAP - Context-Aware Generative Adversarial Privacy\nGAP - Generative Adversarial Privacy\nGATS - Sample-Efficient Deep RL with Generative Adversarial Tree Search\nGAWWN - Learning What and Where to Draw (github)\nGC-GAN - Geometry-Contrastive Generative Adversarial Network for Facial Expression Synthesis\nGcGAN - Geometry-Consistent Adversarial Networks for One-Sided Unsupervised Domain Mapping\nGeneGAN - GeneGAN: Learning Object Transfiguration and Attribute Subspace from Unpaired Data (github)\nGeoGAN - Generating Instance Segmentation Annotation by Geometry-guided GAN\nGeometric GAN - Geometric GAN\nGIN - Generative Invertible Networks (GIN): Pathophysiology-Interpretable Feature Mapping and Virtual Patient Generation\nGLCA-GAN - Global and Local Consistent Age Generative Adversarial Networks\nGM-GAN - Gaussian Mixture Generative Adversarial Networks for Diverse Datasets, and the Unsupervised Clustering of Images\nGMAN - Generative Multi-Adversarial Networks\nGMM-GAN - Towards Understanding the Dynamics of Generative Adversarial Networks\nGoGAN - Gang of GANs: Generative Adversarial Networks with Maximum Margin Ranking\nGONet - GONet: A Semi-Supervised Deep Learning Approach For Traversability Estimation\nGP-GAN - GP-GAN: Towards Realistic High-Resolution Image Blending (github)\nGP-GAN - GP-GAN: Gender Preserving GAN for Synthesizing Faces from Landmarks\nGPU - A generative adversarial framework for positive-unlabeled classification\nGRAN - Generating images with recurrent adversarial networks (github)\nGraphical-GAN - Graphical Generative Adversarial Networks\nGraphSGAN - Semi-supervised Learning on Graphs with Generative Adversarial Nets\nGraspGAN - Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping\nGT-GAN - Deep Graph Translation\nHAN - Chinese Typeface Transformation with Hierarchical Adversarial Network\nHAN - Bidirectional Learning for Robust Neural Networks\nHiGAN - Exploiting Images for Video Recognition with Hierarchical Generative Adversarial Networks\nHP-GAN - HP-GAN: Probabilistic 3D human motion prediction via GAN\nHR-DCGAN - High-Resolution Deep Convolutional Generative Adversarial Networks\nhredGAN - Multi-turn Dialogue Response Generation in an Adversarial Learning framework\nIAN - Neural Photo Editing with Introspective Adversarial Networks (github)\nIcGAN - Invertible Conditional GANs for image editing (github)\nID-CGAN - Image De-raining Using a Conditional Generative Adversarial Network\nIdCycleGAN - Face Translation between Images and Videos using Identity-aware CycleGAN\nIFcVAEGAN - Conditional Autoencoders with Adversarial Information Factorization\niGAN - Generative Visual Manipulation on the Natural Image Manifold (github)\nIGMM-GAN - Coupled IGMM-GANs for deep multimodal anomaly detection in human mobility data\nImproved GAN - Improved Techniques for Training GANs (github)\nIn2I - In2I : Unsupervised Multi-Image-to-Image Translation Using Generative Adversarial Networks\nInfoGAN - InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets (github)\nIntroVAE - IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis\nIR2VI - IR2VI: Enhanced Night Environmental Perception by Unsupervised Thermal Image Translation\nIRGAN - IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval models\nIRGAN - Generative Adversarial Nets for Information Retrieval: Fundamentals and Advances\nISGAN - Invisible Steganography via Generative Adversarial Network\nISP-GPM - Inner Space Preserving Generative Pose Machine\nIterative-GAN - Two Birds with One Stone: Iteratively Learn Facial Attributes with GANs (github)\nIterGAN - IterGANs: Iterative GANs to Learn and Control 3D Object Transformation\nIVE-GAN - IVE-GAN: Invariant Encoding Generative Adversarial Networks\niVGAN - Towards an Understanding of Our World by GANing Videos in the Wild (github)\nIWGAN - On Unifying Deep Generative Models\nJointGAN - JointGAN: Multi-Domain Joint Distribution Learning with Generative Adversarial Nets\nJR-GAN - JR-GAN: Jacobian Regularization for Generative Adversarial Networks\nKBGAN - KBGAN: Adversarial Learning for Knowledge Graph Embeddings\nKGAN - KGAN: How to Break The Minimax Game in GAN\nl-GAN - Representation Learning and Adversarial Generation of 3D Point Clouds\nLAC-GAN - Grounded Language Understanding for Manipulation Instructions Using GAN-Based Classification\nLAGAN - Learning Particle Physics by Example: Location-Aware Generative Adversarial Networks for Physics Synthesis\nLAPGAN - Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks (github)\nLB-GAN - Load Balanced GANs for Multi-view Face Image Synthesis\nLBT - Learning Implicit Generative Models by Teaching Explicit Ones\nLCC-GAN - Adversarial Learning with Local Coordinate Coding\nLD-GAN - Linear Discriminant Generative Adversarial Networks\nLDAN - Label Denoising Adversarial Network (LDAN) for Inverse Lighting of Face Images\nLeakGAN - Long Text Generation via Adversarial Training with Leaked Information\nLeGAN - Likelihood Estimation for Generative Adversarial Networks\nLGAN - Global versus Localized Generative Adversarial Nets\nLipizzaner - Towards Distributed Coevolutionary GANs\nLR-GAN - LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation\nLS-GAN - Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities\nLSGAN - Least Squares Generative Adversarial Networks\nM-AAE - Mask-aware Photorealistic Face Attribute Manipulation\nMAD-GAN - Multi-Agent Diverse Generative Adversarial Networks\nMAGAN - MAGAN: Margin Adaptation for Generative Adversarial Networks\nMAGAN - MAGAN: Aligning Biological Manifolds\nMalGAN - Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN\nMaliGAN - Maximum-Likelihood Augmented Discrete Generative Adversarial Networks\nmanifold-WGAN - Manifold-valued Image Generation with Wasserstein Adversarial Networks\nMARTA-GAN - Deep Unsupervised Representation Learning for Remote Sensing Images\nMaskGAN - MaskGAN: Better Text Generation via Filling in the ______\nMC-GAN - Multi-Content GAN for Few-Shot Font Style Transfer (github)\nMC-GAN - MC-GAN: Multi-conditional Generative Adversarial Network for Image Synthesis\nMcGAN - McGan: Mean and Covariance Feature Matching GAN\nMD-GAN - Learning to Generate Time-Lapse Videos Using Multi-Stage Dynamic Generative Adversarial Networks\nMDGAN - Mode Regularized Generative Adversarial Networks\nMedGAN - Generating Multi-label Discrete Electronic Health Records using Generative Adversarial Networks\nMedGAN - MedGAN: Medical Image Translation using GANs\nMEGAN - MEGAN: Mixture of Experts of Generative Adversarial Networks for Multimodal Image Generation\nMelanoGAN - MelanoGANs: High Resolution Skin Lesion Synthesis with GANs\nmemoryGAN - Memorization Precedes Generation: Learning Unsupervised GANs with Memory Networks\nMeRGAN - Memory Replay GANs: learning to generate images from new categories without forgetting\nMGAN - Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks (github)\nMGGAN - Multi-Generator Generative Adversarial Nets\nMGGAN - MGGAN: Solving Mode Collapse using Manifold Guided Training\nMIL-GAN - Multimodal Storytelling via Generative Adversarial Imitation Learning\nMinLGAN - Anomaly Detection via Minimum Likelihood Generative Adversarial Networks\nMIX+GAN - Generalization and Equilibrium in Generative Adversarial Nets (GANs)\nMIXGAN - MIXGAN: Learning Concepts from Different Domains for Mixture Generation\nMLGAN - Metric Learning-based Generative Adversarial Network\nMMC-GAN - A Multimodal Classifier Generative Adversarial Network for Carry and Place Tasks from Ambiguous Language Instructions\nMMD-GAN - MMD GAN: Towards Deeper Understanding of Moment Matching Network (github)\nMMGAN - MMGAN: Manifold Matching Generative Adversarial Network for Generating Images\nMoCoGAN - MoCoGAN: Decomposing Motion and Content for Video Generation (github)\nModified GAN-CLS - Generate the corresponding Image from Text Description using Modified GAN-CLS Algorithm\nModularGAN - Modular Generative Adversarial Networks\nMolGAN - MolGAN: An implicit generative model for small molecular graphs\nMPM-GAN - Message Passing Multi-Agent GANs\nMS-GAN - Temporal Coherency based Criteria for Predicting Video Frames using Deep Multi-stage Generative Adversarial Networks\nMTGAN - MTGAN: Speaker Verification through Multitasking Triplet Generative Adversarial Networks\nMuseGAN - MuseGAN: Symbolic-domain Music Generation and Accompaniment with Multi-track Sequential Generative Adversarial Networks\nMV-BiGAN - Multi-view Generative Adversarial Networks\nN2RPP - N2RPP: An Adversarial Network to Rebuild Plantar Pressure for ACLD Patients\nNAN - Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning and A New Benchmark for Multi-Human Parsing\nNCE-GAN - Dihedral angle prediction using generative adversarial networks\nND-GAN - Novelty Detection with GAN\nNetGAN - NetGAN: Generating Graphs via Random Walks\nOCAN - One-Class Adversarial Nets for Fraud Detection\nOptionGAN - OptionGAN: Learning Joint Reward-Policy Options using Generative Adversarial Inverse Reinforcement Learning\nORGAN - Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models\nORGAN - 3D Reconstruction of Incomplete Archaeological Objects Using a Generative Adversary Network\nOT-GAN - Improving GANs Using Optimal Transport\nPacGAN - PacGAN: The power of two samples in generative adversarial networks\nPAN - Perceptual Adversarial Networks for Image-to-Image Transformation\nPassGAN - PassGAN: A Deep Learning Approach for Password Guessing\nPD-WGAN - Primal-Dual Wasserstein GAN\nPerceptual GAN - Perceptual Generative Adversarial Networks for Small Object Detection\nPGAN - Probabilistic Generative Adversarial Networks\nPGD-GAN - Solving Linear Inverse Problems Using GAN Priors: An Algorithm with Provable Guarantees\nPGGAN - Patch-Based Image Inpainting with Generative Adversarial Networks\nPIONEER - Pioneer Networks: Progressively Growing Generative Autoencoder\nPip-GAN - Pipeline Generative Adversarial Networks for Facial Images Generation with Multiple Attributes\npix2pix - Image-to-Image Translation with Conditional Adversarial Networks (github)\npix2pixHD - High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs (github)\nPixelGAN - PixelGAN Autoencoders\nPM-GAN - PM-GANs: Discriminative Representation Learning for Action Recognition Using Partial-modalities\nPN-GAN - Pose-Normalized Image Generation for Person Re-identification\nPOGAN - Perceptually Optimized Generative Adversarial Network for Single Image Dehazing\nPose-GAN - The Pose Knows: Video Forecasting by Generating Pose Futures\nPP-GAN - Privacy-Protective-GAN for Face De-identification\nPPAN - Privacy-Preserving Adversarial Networks\nPPGN - Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space\nPrGAN - 3D Shape Induction from 2D Views of Multiple Objects\nProGanSR - A Fully Progressive Approach to Single-Image Super-Resolution\nProgressive GAN - Progressive Growing of GANs for Improved Quality, Stability, and Variation (github)\nPS-GAN - Pedestrian-Synthesis-GAN: Generating Pedestrian Data in Real Scene and Beyond\nPSGAN - Learning Texture Manifolds with the Periodic Spatial GAN\nPSGAN - PSGAN: A Generative Adversarial Network for Remote Sensing Image Pan-Sharpening\nPS\u00b2-GAN - High-Quality Facial Photo-Sketch Synthesis Using Multi-Adversarial Networks\nRadialGAN - RadialGAN: Leveraging multiple datasets to improve target-specific predictive models using Generative Adversarial Networks\nRaGAN - The relativistic discriminator: a key element missing from standard GAN\nRAN - RAN4IQA: Restorative Adversarial Nets for No-Reference Image Quality Assessment (github)\nRankGAN - Adversarial Ranking for Language Generation\nRCGAN - Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs\nReConNN - Reconstruction of Simulation-Based Physical Field with Limited Samples by Reconstruction Neural Network\nRecycle-GAN - Recycle-GAN: Unsupervised Video Retargeting\nRefineGAN - Compressed Sensing MRI Reconstruction with Cyclic Loss in Generative Adversarial Networks\nReGAN - ReGAN: RE[LAX|BAR|INFORCE] based Sequence Generation using GANs (github)\nRegCGAN - Unpaired Multi-Domain Image Generation via Regularized Conditional GANs\nRenderGAN - RenderGAN: Generating Realistic Labeled Data\nResembled GAN - Resembled Generative Adversarial Networks: Two Domains with Similar Attributes\nResGAN - Generative Adversarial Network based on Resnet for Conditional Image Restoration\nRNN-WGAN - Language Generation with Recurrent Generative Adversarial Networks without Pre-training (github)\nRoCGAN - Robust Conditional Generative Adversarial Networks\nRPGAN - Stabilizing GAN Training with Multiple Random Projections (github)\nRTT-GAN - Recurrent Topic-Transition GAN for Visual Paragraph Generation\nRWGAN - Relaxed Wasserstein with Applications to GANs\nSAD-GAN - SAD-GAN: Synthetic Autonomous Driving using Generative Adversarial Networks\nSAGA - Generative Adversarial Learning for Spectrum Sensing\nSAGAN - Self-Attention Generative Adversarial Networks\nSalGAN - SalGAN: Visual Saliency Prediction with Generative Adversarial Networks (github)\nSAM - Sample-Efficient Imitation Learning via Generative Adversarial Nets\nsAOG - Deep Structured Generative Models\nSAR-GAN - Generating High Quality Visible Images from SAR Images Using CNNs\nSBADA-GAN - From source to target and back: symmetric bi-directional adaptive GAN\nScarGAN - ScarGAN: Chained Generative Adversarial Networks to Simulate Pathological Tissue on Cardiovascular MR Scans\nSCH-GAN - SCH-GAN: Semi-supervised Cross-modal Hashing by Generative Adversarial Network\nSD-GAN - Semantically Decomposing the Latent Spaces of Generative Adversarial Networks\nSdf-GAN - Sdf-GAN: Semi-supervised Depth Fusion with Multi-scale Adversarial Networks\nSEGAN - SEGAN: Speech Enhancement Generative Adversarial Network\nSeGAN - SeGAN: Segmenting and Generating the Invisible\nSegAN - SegAN: Adversarial Network with Multi-scale L1 Loss for Medical Image Segmentation\nSem-GAN - Sem-GAN: Semantically-Consistent Image-to-Image Translation\nSeqGAN - SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient (github)\nSeUDA - Semantic-Aware Generative Adversarial Nets for Unsupervised Domain Adaptation in Chest X-ray Segmentation\nSG-GAN - Semantic-aware Grad-GAN for Virtual-to-Real Urban Scene Adaption (github)\nSG-GAN - Sparsely Grouped Multi-task Generative Adversarial Networks for Facial Attribute Manipulation\nSGAN - Texture Synthesis with Spatial Generative Adversarial Networks\nSGAN - Stacked Generative Adversarial Networks (github)\nSGAN - Steganographic Generative Adversarial Networks\nSGAN - SGAN: An Alternative Training of Generative Adversarial Networks\nSGAN - CT Image Enhancement Using Stacked Generative Adversarial Networks and Transfer Learning for Lesion Segmentation Improvement\nsGAN - Generative Adversarial Training for MRA Image Synthesis Using Multi-Contrast MRI\nSiftingGAN - SiftingGAN: Generating and Sifting Labeled Samples to Improve the Remote Sensing Image Scene Classification Baseline in vitro\nSiGAN - SiGAN: Siamese Generative Adversarial Network for Identity-Preserving Face Hallucination\nSimGAN - Learning from Simulated and Unsupervised Images through Adversarial Training\nSisGAN - Semantic Image Synthesis via Adversarial Learning\nSketcher-Refiner GAN - Learning Myelin Content in Multiple Sclerosis from Multimodal MRI through Adversarial Training\nSketchGAN - Adversarial Training For Sketch Retrieval\nSketchyGAN - SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis\nSkip-Thought GAN - Generating Text through Adversarial Training using Skip-Thought Vectors\nSL-GAN - Semi-Latent GAN: Learning to generate and modify facial images from attributes\nSLSR - Sparse Label Smoothing for Semi-supervised Person Re-Identification\nSN-DCGAN - Generative Adversarial Networks for Unsupervised Object Co-localization\nSN-GAN - Spectral Normalization for Generative Adversarial Networks (github)\nSN-PatchGAN - Free-Form Image Inpainting with Gated Convolution\nSobolev GAN - Sobolev GAN\nSocial GAN - Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks\nSoftmax GAN - Softmax GAN\nSoPhie - SoPhie: An Attentive GAN for Predicting Paths Compliant to Social and Physical Constraints\nspeech-driven animation GAN - End-to-End Speech-Driven Facial Animation with Temporal GANs\nSpike-GAN - Synthesizing realistic neural population activity patterns using Generative Adversarial Networks\nSplitting GAN - Class-Splitting Generative Adversarial Networks\nSR-CNN-VAE-GAN - Semi-Recurrent CNN-based VAE-GAN for Sequential Data Generation (github)\nSRGAN - Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\nSRPGAN - SRPGAN: Perceptual Generative Adversarial Network for Single Image Super Resolution\nSS-GAN - Semi-supervised Conditional GANs\nss-InfoGAN - Guiding InfoGAN with Semi-Supervision\nSSGAN - SSGAN: Secure Steganography Based on Generative Adversarial Networks\nSSL-GAN - Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks\nST-CGAN - Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal\nST-GAN - Style Transfer Generative Adversarial Networks: Learning to Play Chess Differently\nST-GAN - ST-GAN: Spatial Transformer Generative Adversarial Networks for Image Compositing\nStackGAN - StackGAN: Text to -----> Photo !!! -realistic Image Synthesis with Stacked Generative Adversarial Networks (github)\nStainGAN - StainGAN: Stain Style Transfer for Digital Histological Images\nStarGAN - StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation (github)\nStarGAN-VC - StarGAN-VC: Non-parallel many-to-many voice conversion with star generative adversarial networks\nSteinGAN - Learning Deep Energy Models: Contrastive Divergence vs. Amortized MLE\nStepGAN - Improving Conditional Sequence Generative Adversarial Networks by Stepwise Evaluation\nSuper-FAN - Super-FAN: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANs\nSVSGAN - SVSGAN: Singing Voice Separation via Generative Adversarial Network\nSWGAN - Solving Approximate Wasserstein GANs to Stationarity\nSyncGAN - SyncGAN: Synchronize the Latent Space of Cross-modal Generative Adversarial Networks\nS^2GAN - Generative Image Modeling using Style and Structure Adversarial Networks\nT2Net - T2Net: Synthetic-to-Realistic Translation for Solving Single-Image Depth Estimation Tasks\ntable-GAN - Data Synthesis based on Generative Adversarial Networks\nTAC-GAN - TAC-GAN - Text Conditioned Auxiliary Classifier Generative Adversarial Network (github)\nTAN - Outline Colorization through Tandem Adversarial Networks\ntcGAN - Cross-modal Hallucination for Few-shot Fine-grained Recognition\nTD-GAN - Task Driven Generative Modeling for Unsupervised Domain Adaptation: Application to X-ray Image Segmentation\ntempCycleGAN - Improving Surgical Training Phantoms by Hyperrealism: Deep Unpaired Image-to-Image Translation from Real Surgeries\ntempoGAN - tempoGAN: A Temporally Coherent, Volumetric GAN for Super-resolution Fluid Flow\nTequilaGAN - TequilaGAN: How to easily identify GAN samples\nText2Shape - Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings\ntextGAN - Generating Text via Adversarial Training\nTextureGAN - TextureGAN: Controlling Deep Image Synthesis with Texture Patches\nTGAN - Temporal Generative Adversarial Nets\nTGAN - Tensorizing Generative Adversarial Nets\nTGAN - Tensor-Generative Adversarial Network with Two-dimensional Sparse Coding: Application to Real-time Indoor Localization\nTGANs-C - To Create What You Tell: Generating Videos from Captions\ntiny-GAN - Analysis of Nonautonomous Adversarial Systems\nTP-GAN - Beyond Face Rotation: Global and Local Perception GAN for Photorealistic and Identity Preserving Frontal View Synthesis\nTreeGAN - TreeGAN: Syntax-Aware Sequence Generation with Generative Adversarial Networks\nTriple-GAN - Triple Generative Adversarial Nets\ntripletGAN - TripletGAN: Training Generative Model with Triplet Loss\nTV-GAN - TV-GAN: Generative Adversarial Network Based Thermal to Visible Face Recognition\nTwin-GAN - Twin-GAN -- Unpaired Cross-Domain Image Translation with Weight-Sharing GANs\nUGACH - Unsupervised Generative Adversarial Cross-modal Hashing\nUGAN - Enhancing Underwater Imagery using Generative Adversarial Networks\nUnim2im - Unsupervised Image-to-Image Translation with Generative Adversarial Networks (github)\nUNIT - Unsupervised Image-to-image Translation Networks (github)\nUnrolled GAN - Unrolled Generative Adversarial Networks (github)\nUT-SCA-GAN - Spatial Image Steganography Based on Generative Adversarial Network\nUV-GAN - UV-GAN: Adversarial Facial UV Map Completion for Pose-invariant Face Recognition\nVA-GAN - Visual Feature Attribution using Wasserstein GANs\nVAC+GAN - Versatile Auxiliary Classifier with Generative Adversarial Network (VAC+GAN), Multi Class Scenarios\nVAE-GAN - Autoencoding beyond pixels using a learned similarity metric\nVariGAN - Multi-View Image Generation from a Single-View\nVAW-GAN - Voice Conversion from Unaligned Corpora using Variational Autoencoding Wasserstein Generative Adversarial Networks\nVEEGAN - VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning (github)\nVGAN - Generating Videos with Scene Dynamics (github)\nVGAN - Generative Adversarial Networks as Variational Training of Energy Based Models (github)\nVGAN - Text Generation Based on Generative Adversarial Nets with Latent Variable\nViGAN - Image Generation and Editing with Variational Info Generative Adversarial Networks\nVIGAN - VIGAN: Missing View Imputation with Generative Adversarial Networks\nVoiceGAN - Voice Impersonation using Generative Adversarial Networks\nVOS-GAN - VOS-GAN: Adversarial Learning of Visual-Temporal Dynamics for Unsupervised Dense Prediction in Videos\nVRAL - Variance Regularizing Adversarial Learning\nWaterGAN - WaterGAN: Unsupervised Generative Network to Enable Real-time Color Correction of Monocular Underwater Images\nWaveGAN - Synthesizing Audio with Generative Adversarial Networks\nWaveletGLCA-GAN - Global and Local Consistent Wavelet-domain Age Synthesis\nweGAN - Generative Adversarial Nets for Multiple Text Corpora\nWGAN - Wasserstein GAN (github)\nWGAN-CLS - Text to Image Synthesis Using Generative Adversarial Networks\nWGAN-GP - Improved Training of Wasserstein GANs (github)\nWGAN-L1 - Subsampled Turbulence Removal Network\nWS-GAN - Weakly Supervised Generative Adversarial Networks for 3D Reconstruction\nX-GANs - X-GANs: Image Reconstruction Made Easy for Extreme Cases\nXGAN - XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings\nZipNet-GAN - ZipNet-GAN: Inferring Fine-grained Mobile Traffic Patterns via a Generative Adversarial Neural Network\n\u03b1-GAN - Variational Approaches for Auto-Encoding Generative Adversarial Networks (github)\n\u03b2-GAN - Annealed Generative Adversarial Networks\n\u0394-GAN - Triangle Generative Adversarial Networks",
      "link": "https://github.com/hindupuravinash/the-gan-zoo"
    },
    {
      "autor": "awesome-production-machine-learning",
      "date": "NaN",
      "content": "Awesome production machine learning\nThis repository contains a curated list of awesome open source libraries that will help you deploy, monitor, version, scale, and secure your production machine learning.\nQuick links to sections in this page\n\ud83d\udd0d Explaining predictions & models \ud83d\udd0f Privacy preserving ML \ud83d\udcdc Model & data versioning\n\ud83c\udfc1 Model Training Orchestration \ud83d\udcaa Model Serving and Monitoring \ud83e\udd16 Neural Architecture Search\n\ud83d\udcd3 Reproducible Notebooks \ud83d\udcca Visualisation frameworks \ud83d\udd20 Industry-strength NLP\n\ud83e\uddf5 Data pipelines & ETL \ud83c\udff7\ufe0f Data Labelling \ud83d\udcc5 Metadata Management\n\ud83d\udce1 Functions as a service \ud83d\uddfa\ufe0f Computation distribution \ud83d\udce5 Model serialisation\n\ud83e\uddee Optimized computation frameworks \ud83d\udcb8 Data Stream Processing \ud83d\udd34 Outlier and Anomaly Detection\n\ud83c\udf00 Feature engineering \ud83c\udf81 Feature Stores \u2694 Adversarial Robustness\n\ud83d\udcb0 Commercial Platforms \ud83d\udcbe Data Storage Layer\n10 Min Video Overview\nThis 10 minute video provides an overview of the motivations for machine learning operations as well as a high level overview on some of the tools in this repo.\nWant to receive recurrent updates on this repo and other advancements?\nYou can join the Machine Learning Engineer newsletter. You will receive updates on open source frameworks, tutorials and articles curated by machine learning professionals.\nAlso check out the Awesome Artificial Intelligence Guidelines List, where we aim to map the landscape of \"Frameworks\", \"Codes of Ethics\", \"Guidelines\", \"Regulations\", etc related to Artificial Intelligence.\nMain Content\nExplaining Black Box Models and Datasets\nAequitas - An open-source bias audit toolkit for data scientists, machine learning researchers, and policymakers to audit machine learning models for discrimination and bias, and to make informed and equitable decisions around developing and deploying predictive risk-assessment tools.\nAlibi - Alibi is an open source Python library aimed at machine learning model inspection and interpretation. The initial focus on the library is on black-box, instance based model explanations.\nanchor - Code for the paper \"High precision model agnostic explanations\", a model-agnostic system that explains the behaviour of complex models with high-precision rules called anchors.\ncaptum - model interpretability and understanding library for PyTorch developed by Facebook. It contains general purpose implementations of integrated gradients, saliency maps, smoothgrad, vargrad and others for PyTorch models.\ncasme - Example of using classifier-agnostic saliency map extraction on ImageNet presented on the paper \"Classifier-agnostic saliency map extraction\".\nContrastiveExplanation (Foil Trees) - Python script for model agnostic contrastive/counterfactual explanations for machine learning. Accompanying code for the paper \"Contrastive Explanations with Local Foil Trees\".\nDeepLIFT - Codebase that contains the methods in the paper \"Learning important features through propagating activation differences\". Here is the slides and the video of the 15 minute talk given at ICML.\nDeepVis Toolbox - This is the code required to run the Deep Visualization Toolbox, as well as to generate the neuron-by-neuron visualizations using regularized optimization. The toolbox and methods are described casually here and more formally in this paper.\nELI5 - \"Explain Like I'm 5\" is a Python package which helps to debug machine learning classifiers and explain their predictions.\nFACETS - Facets contains two robust visualizations to aid in understanding and analyzing machine learning datasets. Get a sense of the shape of each feature of your dataset using Facets Overview, or explore individual observations using Facets Dive.\nFairlearn - Fairlearn is a python toolkit to assess and mitigate unfairness in machine learning models.\nFairML - FairML is a python toolbox auditing the machine learning models for bias.\nfairness - This repository is meant to facilitate the benchmarking of fairness aware machine learning algorithms based on this paper.\nGEBI - Global Explanations for Bias Identification - An attention-based summarized post-hoc explanations for detection and identification of bias in data. We propose a global explanation and introduce a step-by-step framework on how to detect and test bias. Python package for image data.\nIBM AI Explainability 360 - Interpretability and explainability of data and machine learning models including a comprehensive set of algorithms that cover different dimensions of explanations along with proxy explainability metrics.\nIBM AI Fairness 360 - A comprehensive set of fairness metrics for datasets and machine learning models, explanations for these metrics, and algorithms to mitigate bias in datasets and models.\niNNvestigate - An open-source library for analyzing Keras models visually by methods such as DeepTaylor-Decomposition, PatternNet, Saliency Maps, and Integrated Gradients.\nIntegrated-Gradients - This repository provides code for implementing integrated gradients for networks with image inputs.\nInterpretML - InterpretML is an open-source package for training interpretable models and explaining blackbox systems.\nkeras-vis - keras-vis is a high-level toolkit for visualizing and debugging your trained keras neural net models. Currently supported visualizations include: Activation maximization, Saliency maps, Class activation maps.\nL2X - Code for replicating the experiments in the paper \"Learning to Explain: An Information-Theoretic Perspective on Model Interpretation\" at ICML 2018\nLightly - A python framework for self-supervised learning on images. The learned representations can be used to analyze the distribution in unlabeled data and rebalance datasets.\nLightwood - A Pytorch based framework that breaks down machine learning problems into smaller blocks that can be glued together seamlessly with an objective to build predictive models with one line of code.\nLIME - Local Interpretable Model-agnostic Explanations for machine learning models.\nLOFO Importance - LOFO (Leave One Feature Out) Importance calculates the importances of a set of features based on a metric of choice, for a model of choice, by iteratively removing each feature from the set, and evaluating the performance of the model, with a validation scheme of choice, based on the chosen metric.\nMindsDB - MindsDB is an Explainable AutoML framework for developers. With MindsDB you can build, train and use state of the art ML models in as simple as one line of code.\nmljar-supervised - An Automated Machine Learning (AutoML) python package for tabular data. It can handle: Binary Classification, MultiClass Classification and Regression. It provides feature engineering, explanations and markdown reports.\nNETRON - Viewer for neural network, deep learning and machine learning models.\npyBreakDown - A model agnostic tool for decomposition of predictions from black boxes. Break Down Table shows contributions of every variable to a final prediction.\nrationale - Code to implement learning rationales behind predictions with code for paper \"Rationalizing Neural Predictions\"\nresponsibly - Toolkit for auditing and mitigating bias and fairness of machine learning systems\nSHAP - SHapley Additive exPlanations is a unified approach to explain the output of any machine learning model.\nSHAPash - Shapash is a Python library that provides several types of visualization that display explicit labels that everyone can understand.\nSkater - Skater is a unified framework to enable Model Interpretation for all forms of model to help one build an Interpretable machine learning system often needed for real world use-cases\nTensorboard's Tensorboard WhatIf - Tensorboard screen to analyse the interactions between inference results and data inputs.\nTensorflow's cleverhans - An adversarial example library for constructing attacks, building defenses, and benchmarking both. A python library to benchmark system's vulnerability to adversarial examples\ntensorflow's lucid - Lucid is a collection of infrastructure and tools for research in neural network interpretability.\ntensorflow's Model Analysis - TensorFlow Model Analysis (TFMA) is a library for evaluating TensorFlow models. It allows users to evaluate their models on large amounts of data in a distributed manner, using the same metrics defined in their trainer.\nthemis-ml - themis-ml is a Python library built on top of pandas and sklearn that implements fairness-aware machine learning algorithms.\nThemis - Themis is a testing-based approach for measuring discrimination in a software system.\nTreeInterpreter - Package for interpreting scikit-learn's decision tree and random forest predictions. Allows decomposing each prediction into bias and feature contribution components as described in http://blog.datadive.net/interpreting-random-forests/.\nwoe - Tools for WoE Transformation mostly used in ScoreCard Model for credit rating\nXAI - eXplainableAI - An eXplainability toolbox for machine learning.\nPrivacy Preserving Machine Learning\nFlower - Flower is a Federated Learning Framework with a unified approach. It enables the federation of any ML workload, with any ML framework, and any programming language.\nGoogle's Differential Privacy - This is a C++ library of \u03b5-differentially private algorithms, which can be used to produce aggregate statistics over numeric data sets containing private or sensitive information.\nIntel Homomorphic Encryption Backend - The Intel HE transformer for nGraph is a Homomorphic Encryption (HE) backend to the Intel nGraph Compiler, Intel's graph compiler for Artificial Neural Networks.\nMicrosoft SEAL - Microsoft SEAL is an easy-to-use open-source (MIT licensed) homomorphic encryption library developed by the Cryptography Research group at Microsoft.\nPySyft - A Python library for secure, private Deep Learning. PySyft decouples private data from model training, using Multi-Party Computation (MPC) within PyTorch.\nRosetta - A privacy-preserving framework based on TensorFlow with customized backend Operations using Multi-Party Computation (MPC). Rosetta reuses the APIs of TensorFlow and allows to transfer original TensorFlow codes into a privacy-preserving manner with minimal changes.\nSubstra - Substra is an open-source framework for privacy-preserving, traceable and collaborative Machine Learning.\nTensorflow Privacy - A Python library that includes implementations of TensorFlow optimizers for training machine learning models with differential privacy.\nTF Encrypted - A Framework for Confidential Machine Learning on Encrypted Data in TensorFlow.\nUber SQL Differencial Privacy - Uber's open source framework that enforces differential privacy for general-purpose SQL queries.\nModel and Data Versioning\nAim - A super-easy way to record, search and compare AI experiments.\nApache Marvin is a platform for model deployment and versioning that hides all complexity under the hood: data scientists just need to set up the server and write their code in an extended jupyter notebook.\nCatalyst - High-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code/ideas reusing.\nD6tflow - A python library that allows for building complex data science workflows on Python.\nData Version Control (DVC) - A git fork that allows for version management of models.\nDolt - Dolt is a SQL database that you can fork, clone, branch, merge, push and pull just like a git repository.\nFGLab - Machine learning dashboard, designed to make prototyping experiments easier.\nFlor - Easy to use logger and automatic version controller made for data scientists who write ML code\nGUILD AI - Open source toolkit that automates and optimizes machine learning experiments.\nHub - Store, access & manage datasets with version-control for PyTorch/TensorFlow locally or on any cloud with scalable data pipelines.\nHangar - Version control for tensor data, git-like semantics on numerical data with high speed and efficiency.\nKeepsake - Version control for machine learning.\nlakeFS - Repeatable, atomic and versioned data lake on top of object storage.\nMLflow - Open source platform to manage the ML lifecycle, including experimentation, reproducibility and deployment.\nMLWatcher - MLWatcher is a python agent that records a large variety of time-serie metrics of your running ML classification algorithm. It enables you to monitor in real time.\nModelChimp - Framework to track and compare all the results and parameters from machine learning models (Video)\nModelDB - An open-source system to version machine learning models including their ingredients code, data, config, and environment and to track ML metadata across the model lifecycle.\nModelStore - An open-source Python library that allows you to version, export, and save a machine learning model to your cloud storage provider.\nPachyderm - Open source distributed processing framework build on Kubernetes focused mainly on dynamic building of production machine learning pipelines - (Video)\nPolyaxon - A platform for reproducible and scalable machine learning and deep learning on kubernetes. - (Video)\nPredictionIO - An open source Machine Learning Server built on top of a state-of-the-art open source stack for developers and data scientists to create predictive engines for any machine learning task\nQuilt Data - Versioning, reproducibility and deployment of data and models.\nSacred - Tool to help you configure, organize, log and reproduce machine learning experiments.\nsteppy - Lightweight, Python3 library for fast and reproducible machine learning experimentation. Introduces simple interface that enables clean machine learning pipeline design.\nStudio.ML - Model management framework which minimizes the overhead involved with scheduling, running, monitoring and managing artifacts of your machine learning experiments.\nTerminusDB - A graph database management system that stores data like git.\nTRAINS - Auto-Magical Experiment Manager & Version Control for AI.\nModel Training Orchestration\nCML - Continuous Machine Learning (CML) is an open-source library for implementing continuous integration & delivery (CI/CD) in machine learning projects.\nDetermined - Deep learning training platform with integrated support for distributed training, hyperparameter tuning, and model management (supports Tensorflow and Pytorch).\nFlyte - Lyft\u2019s Cloud Native Machine Learning and Data Processing Platform. (Demo)\nHopsworks - Hopsworks is a data-intensive platform for the design and operation of machine learning pipelines that includes a Feature Store. (Video).\nKubeflow - A cloud native platform for machine learning based on Google\u2019s internal machine learning pipelines.\nMLeap - Standardisation of pipeline and model serialization for Spark, Tensorflow and sklearn\nNVIDIA TensorRT - TensorRT is a C++ library for high performance inference on NVIDIA GPUs and deep learning accelerators.\nOnepanel - Production scale vision AI platform, with fully integrated components for model building, automated labeling, data processing and model training pipelines.\nOpen Platform for AI - Platform that provides complete AI model training and resource management capabilities.\nPlanout - PlanOut is a multi-platform framework and programming language for online field experimentation. PlanOut was created to make it easy to run and iterate on sophisticated experiments, while satisfying the constraints of deployed Internet services with many users.\nPyCaret ) - low-code library for training and deploying models (scikit-learn, XGBoost, LightGBM, spaCy)\nRedis-ML - Module available from unstable branch that supports a subset of ML models as Redis data types. (Replaced by Redis AI)\nSkaffold - Skaffold is a command line tool that facilitates continuous development for Kubernetes applications. You can iterate on your application source code locally then deploy to local or remote Kubernetes clusters.\nTensorflow Extended (TFX) - Production oriented configuration framework for ML based on TensorFlow, incl. monitoring and model version management.\nTonY - TonY is a framework to natively run deep learning jobs on Apache Hadoop. It currently supports TensorFlow, PyTorch, MXNet and Horovod.\nZenML - ZenML is an extensible, open-source MLOps framework to create reproducible ML pipelines with a focus on automated metadata tracking, caching, and many integrations to other tools.\nModel Serving and Monitoring\nBackprop - Backprop makes it simple to use, finetune, and deploy state-of-the-art ML models.\nBentoML - BentoML is an open source framework for high performance ML model serving\nCortex - Cortex is an open source platform for deploying machine learning models\u2014trained with any framework\u2014as production web services. No DevOps required.\nDeepDetect - Machine Learning production server for TensorFlow, XGBoost and Cafe models written in C++ and maintained by Jolibrain\nEvidently - Evidently helps analyze machine learning models during development, validation, or production monitoring. The tool generates interactive reports from pandas DataFrame.\nForestFlow - Cloud-native machine learning model server.\nJina - Cloud native search framework that supports to use deep learning/state of the art AI models for search.\nKFServing - Serverless framework to deploy and monitor machine learning models in Kubernetes - (Video)\nm2cgen - A lightweight library which allows to transpile trained classic machine learning models into a native code of C, Java, Go, R, PHP, Dart, Haskell, Rust and many other programming languages.\nModel Server for Apache MXNet (MMS) - A model server for Apache MXNet from Amazon Web Services that is able to run MXNet models as well as Gluon models (Amazon's SageMaker runs a custom version of MMS under the hood)\nOpenScoring - REST web service for scoring PMML models built and maintained by OpenScoring.io\nRedis-AI - A Redis module for serving tensors and executing deep learning models. Expect changes in the API and internals.\nSeldon Core - Open source platform for deploying and monitoring machine learning models in kubernetes - (Video)\nTempo - Open source SDK that provides a unified interface to multiple MLOps projects that enable data scientists to deploy and productionise machine learning systems.\nTensorflow Serving - High-performant framework to serve Tensorflow models via grpc protocol able to handle 100k requests per second per core\nTorchServe - TorchServe is a flexible and easy to use tool for serving PyTorch models.\nTriton Inference Server - Triton is a high performance open source serving software to deploy AI models from any framework on GPU & CPU while maximizing utilization.\nWhyLogs - Lightweight solution for profiling and monitoring your ML data pipeline end-to-end\nAdversarial Robustness Libraries\nAdvBox - generate adversarial examples from the command line with 0 coding using PaddlePaddle, PyTorch, Caffe2, MxNet, Keras, and TensorFlow. Includes 10 attacks and also 6 defenses. Used to implement StealthTshirt at DEFCON!\nAdversarial DNN Playground - think TensorFlow Playground, but for Adversarial Examples! A visualization tool designed for learning and teaching - the attack library is limited in size, but it has a nice front-end to it with buttons you can press!\nAdverTorch - library for adversarial attacks / defenses specifically for PyTorch.\nAlibi Detect - alibi-detect is a Python package focused on outlier, adversarial and concept drift detection. The package aims to cover both online and offline detectors for tabular data, text, images and time series. The outlier detection methods should allow the user to identify global, contextual and collective outliers.\nArtificial Adversary AirBnB's library to generate text that reads the same to a human but passes adversarial classifiers.\nCleverHans - library for testing adversarial attacks / defenses maintained by some of the most important names in adversarial ML, namely Ian Goodfellow (ex-Google Brain, now Apple) and Nicolas Papernot (Google Brain). Comes with some nice tutorials!\nCounterfit - Counterfit is a command-line tool and generic automation layer for assessing the security of machine learning systems.\nDEEPSEC - another systematic tool for attacking and defending deep learning models.\nEvadeML - benchmarking and visualization tool for adversarial ML maintained by Weilin Xu, a PhD at University of Virginia, working with David Evans. Has a tutorial on re-implementation of one of the most important adversarial defense papers - feature squeezing (same team).\nFoolbox - second biggest adversarial library. Has an even longer list of attacks - but no defenses or evaluation metrics. Geared more towards computer vision. Code easier to understand / modify than ART - also better for exploring blackbox attacks on surrogate models.\nIBM Adversarial Robustness 360 Toolbox (ART) - at the time of writing this is the most complete off-the-shelf resource for testing adversarial attacks and defenses. It includes a library of 15 attacks, 10 empirical defenses, and some nice evaluation metrics. Neural networks only.\nMIA - A library for running membership inference attacks (MIA) against machine learning models.\nNicolas Carlini\u2019s Adversarial ML reading list - not a library, but a curated list of the most important adversarial papers by one of the leading minds in Adversarial ML, Nicholas Carlini. If you want to discover the 10 papers that matter the most - I would start here.\nRobust ML - another robustness resource maintained by some of the leading names in adversarial ML. They specifically focus on defenses, and ones that have published code available next to papers. Practical and useful.\nTextFool - plausible looking adversarial examples for text generation.\nTrickster - Library and experiments for attacking machine learning in discrete domains using graph search.\nNeural Architecture Search\nAutokeras - AutoML library for Keras based on \"Auto-Keras: Efficient Neural Architecture Search with Network Morphism\".\nENAS via Parameter Sharing - Efficient Neural Architecture Search via Parameter Sharing by authors of paper.\nENAS-PyTorch - Efficient Neural Architecture Search (ENAS) in PyTorch based on this paper.\nENAS-Tensorflow - Efficient Neural Architecture search via parameter sharing(ENAS) micro search Tensorflow code for windows user.\nKatib - A Kubernetes-based system for Hyperparameter Tuning and Neural Architecture Search.\nMaggy - Asynchronous, directed Hyperparameter search and parallel ablation studies on Apache Spark (Video).\nNeural Architecture Search with Controller RNN - Basic implementation of Controller RNN from Neural Architecture Search with Reinforcement Learning and Learning Transferable Architectures for Scalable Image Recognition.\nNeural Network Intelligence - NNI (Neural Network Intelligence) is a toolkit to help users run automated machine learning (AutoML) experiments.\nData Science Notebook Frameworks\nApache Zeppelin - Web-based notebook that enables data-driven, interactive data analytics and collaborative documents with SQL, Scala and more.\nBinder - Binder hosts notebooks in an executable environment (for free).\nH2O Flow - Jupyter notebook-like interface for H2O to create, save and re-use \"flows\"\nHydrogen - A plugin for ATOM that enables it to become a jupyter-notebook-like interface that prints the outputs directly in the editor.\nJupyter Notebooks - Web interface python sandbox environments for reproducible development\nML Workspace - All-in-one web IDE for machine learning and data science. Combines Jupyter, VS Code, Tensorflow, and many other tools/libraries into one Docker image.\n.NET Interactive - .NET Interactive takes the power of .NET and embeds it into your interactive experiences.\nPapermill - Papermill is a library for parameterizing notebooks and executing them like Python scripts.\nPloomber - Ploomber allows you to develop workflows in Jupyter and execute them in a distributed environment without code changes. It supports Kubernetes, AWS Batch, and Airflow.\nPolynote - Polynote is an experimental polyglot notebook environment. Currently, it supports Scala and Python (with or without Spark), SQL, and Vega.\nRMarkdown - The rmarkdown package is a next generation implementation of R Markdown based on Pandoc.\nStencila - Stencila is a platform for creating, collaborating on, and sharing data driven content. Content that is transparent and reproducible.\nVoil\u00e0 - Voil\u00e0 turns Jupyter notebooks into standalone web applications that can e.g. be used as dashboards.\nIndustrial Strength Visualisation libraries\nBokeh - Bokeh is an interactive visualization library for Python that enables beautiful and meaningful visual presentation of data in modern web browsers.\nGeoplotlib - geoplotlib is a python toolbox for visualizing geographical data and making maps\nggplot2 - An implementation of the grammar of graphics for R.\ngradio - Quickly create and share demos of models - by only writing Python. Debug models interactively in your browser, get feedback from collaborators, and generate public links without deploying anything.\nmatplotlib - A Python 2D plotting library which produces publication-quality figures in a variety of hardcopy formats and interactive environments across platforms.\nMissingno - missingno provides a small toolset of flexible and easy-to-use missing data visualizations and utilities that allows you to get a quick visual summary of the completeness (or lack thereof) of your dataset.\nPDPBox - This repository is inspired by ICEbox. The goal is to visualize the impact of certain features towards model prediction for any supervised learning algorithm. (now support all scikit-learn algorithms)\nPerspective Streaming pivot visualization via WebAssembly https://perspective.finos.org/\nPixiedust - PixieDust is a productivity tool for Python or Scala notebooks, which lets a developer encapsulate business logic into something easy for your customers to consume.\nPlotly Dash - Dash is a Python framework for building analytical web applications without the need to write javascript.\nPlotly.py - An interactive, open source, and browser-based graphing library for Python.\nPlotly.NET - Plotly.NET provides functions for generating and rendering plotly.js charts in .NET programming languages.\nPyCEbox - Python Individual Conditional Expectation Plot Toolbox\npygal - pygal is a dynamic SVG charting library written in python\nRedash - Redash is anopen source visualisation framework that is built to allow easy access to big datasets leveraging multiple backends.\nseaborn - Seaborn is a Python visualization library based on matplotlib. It provides a high-level interface for drawing attractive statistical graphics.\nStreamlit - Streamlit lets you create apps for your machine learning projects with deceptively simple Python scripts. It supports hot-reloading, so your app updates live as you edit and save your file\nSuperset - A modern, enterprise-ready business intelligence web application.\nXKCD-style plots - An XKCD theme for matblotlib visualisations\nyellowbrick - yellowbrick is a matplotlib-based model evaluation plots for scikit-learn and other machine learning libraries.\nIndustrial Strength NLP\nAdaptNLP - Built atop Zalando Research's Flair and Hugging Face's Transformers library, AdaptNLP provides Machine Learning Researchers and Scientists a modular and adaptive approach to a variety of NLP tasks with an Easy API for training, inference, and deploying NLP-based microservices.\nBlackstone - Blackstone is a spaCy model and library for processing long-form, unstructured legal text. Blackstone is an experimental research project from the Incorporated Council of Law Reporting for England and Wales' research lab, ICLR&D.\nCTRL - A Conditional Transformer Language Model for Controllable Generation released by SalesForce\nFacebook's XLM - PyTorch original implementation of Cross-lingual Language Model Pretraining which includes BERT, XLM, NMT, XNLI, PKM, etc.\nFlair - Simple framework for state-of-the-art NLP developed by Zalando which builds directly on PyTorch.\nGithub's Semantic - Github's text library for parsing, analyzing, and comparing source code across many languages .\nGluonNLP - GluonNLP is a toolkit that enables easy text preprocessing, datasets loading and neural models building to help you speed up your Natural Language Processing (NLP) research.\nGNES - Generic Neural Elastic Search is a cloud-native semantic search system based on deep neural networks.\nGrover - Grover is a model for Neural Fake News -- both generation and detection. However, it probably can also be used for other generation tasks.\nKashgari - Kashgari is a simple and powerful NLP Transfer learning framework, build a state-of-art model in 5 minutes for named entity recognition (NER), part-of-speech tagging (PoS), and text classification tasks.\nOpenAI GPT-2 - OpenAI's code from their paper \"Language Models are Unsupervised Multitask Learners\".\nsense2vec - A Pytorch library that allows for training and using sense2vec models, which are models that leverage the same approach than word2vec, but also leverage part-of-speech attributes for each token, which allows it to be \"meaning-aware\"\nSnorkel - Snorkel is a system for quickly generating training data with weak supervision https://snorkel.org.\nSpaCy - Industrial-strength natural language processing library built with python and cython by the explosion.ai team.\nStable Baselines - A fork of OpenAI Baselines, implementations of reinforcement learning algorithms http://stable-baselines.readthedocs.io/.\nTensorflow Lingvo - A framework for building neural networks in Tensorflow, particularly sequence models. Lingvo: A TensorFlow Framework for Sequence Modeling.\nTensorflow Text - TensorFlow Text provides a collection of text related classes and ops ready to use with TensorFlow 2.0.\nWav2Letter++ - A speech to text system developed by Facebook's FAIR teams.\nYouTokenToMe - YouTokenToMe is an unsupervised text tokenizer focused on computational efficiency. It currently implements fast Byte Pair Encoding (BPE) [Sennrich et al.].\nTransformers - Huggingface's library of state-of-the-art pretrained models for Natural Language Processing (NLP).\nData Pipeline ETL Frameworks\nApache Airflow - Data Pipeline framework built in Python, including scheduler, DAG definition and a UI for visualisation\nApache Nifi - Apache NiFi was made for dataflow. It supports highly configurable directed graphs of data routing, transformation, and system mediation logic.\nArgo Workflows - Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. Argo Workflows is implemented as a Kubernetes CRD (Custom Resource Definition).\nAzkaban - Azkaban is a batch workflow job scheduler created at LinkedIn to run Hadoop jobs. Azkaban resolves the ordering through job dependencies and provides an easy to use web user interface to maintain and track your workflows.\nBasin - Visual programming editor for building Spark and PySpark pipelines\nBonobo - ETL framework for Python 3.5+ with focus on simple atomic operations working concurrently on rows of data\nChronos - More of a job scheduler for Mesos than ETL pipeline. [OUTDATED]\nCouler - Unified interface for constructing and managing machine learning workflows on different workflow engines, such as Argo Workflows, Tekton Pipelines, and Apache Airflow.\nDagster - A data orchestrator for machine learning, analytics, and ETL.\nFlyte - Lyft\u2019s Cloud Native Machine Learning and Data Processing Platform. (Demo)\nGenie - Job orchestration engine to interface and trigger the execution of jobs from Hadoop-based systems\nGokart - Wrapper of the data pipeline Luigi\nKedro - Kedro is a workflow development tool that helps you build data pipelines that are robust, scalable, deployable, reproducible and versioned. Visualization of the kedro workflows can be done by kedro-viz\nLuigi - Luigi is a Python module that helps you build complex pipelines of batch jobs, handling dependency resolution, workflow management, visualisation, etc\nMetaflow - A framework for data scientists to easily build and manage real-life data science projects.\nNeuraxle - A framework for building neat pipelines, providing the right abstractions to chain your data transformation and prediction steps with data streaming, as well as doing hyperparameter searches (AutoML).\nOozie - Workflow scheduler for Hadoop jobs\nPipelineX - Based on Kedro and MLflow. Full comparison given at https://github.com/Minyus/Python_Packages_for_Pipeline_Workflow\nPrefect Core - Workflow management system that makes it easy to take your data pipelines and add semantics like retries, logging, dynamic mapping, caching, failure notifications, and more.\nSETL - A simple Spark-powered ETL framework that helps you structure your ETL projects, modularize your data transformation logic and speed up your development.\nSnakemake - Workflow management system for reproducible and scalable data analyses.\nData Labelling Tools and Frameworks\nbrat rapid annotation tool - Web-based text annotation tool for Named-Entity-Recogntion task.\nCOCO Annotator - Web-based image segmentation tool for object detection, localization and keypoints\nComputer Vision Annotation Tool (CVAT) - OpenCV's web-based annotation tool for both VIDEOS and images for computer algorithms.\nDoccano - Open source text annotation tools for humans, providing functionality for sentiment analysis, named entity recognition, and machine translation.\nImageTagger - Image labelling tool with support for collaboration, supporting bounding box, polygon, line, point labelling, label export, etc.\nImgLab - Image annotation tool for bounding boxes with auto-suggestion and extensibility for plugins.\nLabel Studio - Multi-domain data labeling and annotation tool with standardized output format\nLabelimg - Open source graphical image annotation tool writen in Python using QT for graphical interface focusing primarily on bounding boxes.\nmakesense.ai - Free to use online tool for labelling photos. Prepared labels can be downloaded in one of multiple supported formats.\nMedTagger - A collaborative framework for annotating medical datasets using crowdsourcing.\nOpenLabeling - Open source tool for labelling images with support for labels, edges, as well as image resizing and zooming in.\nPixelAnnotationTool - Image annotation tool with ability to \"colour\" on the images to select labels for segmentation. Process is semi-automated with the watershed marked algorithm of OpenCV\nRubrix - Open-source tool for tracking, exploring, and labeling data for AI projects.\nSemantic Segmentation Editor - Hitachi's Open source tool for labelling -----> camera !!!  and LIDAR data.\nSuperintendent - superintendent provides an ipywidget-based interactive labelling tool for your data.\nVGG Image Annotator (VIA) - A simple and standalone manual annotation software for image, audio and video. VIA runs in a web browser and does not require any installation or setup.\nVisual Object Tagging Tool (VOTT) - Microsoft's Open Source electron app for labelling videos and images for object detection models (with active learning functionality)\nMetadata Management\nAmundsen - Amundsen is a metadata driven application for improving the productivity of data analysts, data scientists and engineers when interacting with data.\nApache Atlas - Apache Atlas framework is an extensible set of core foundational governance services \u2013 enabling enterprises to effectively and efficiently meet their compliance requirements within Hadoop and allows integration with the whole enterprise data ecosystem.\nDataHub - DataHub is LinkedIn's generalized metadata search & discovery tool.\nMarquez - Marquez is an open source metadata service for the collection, aggregation, and visualization of a data ecosystem's metadata.\nMetacat - Metacat is a unified metadata exploration API service. Metacat focusses on solving these three problems: 1) Federate views of metadata systems. 2) Allow arbitrary metadata storage about data sets. 3) Metadata discovery.\nML Metadata - a library for recording and retrieving metadata associated with ML developer and data scientist workflows. Also TensorFlow ML Metadata.\nModel Card Toolkit - streamlines and automates generation of Model Cards\nData Storage Optimisation\nAlluxio - A virtual distributed storage system that bridges the gab between computation frameworks and storage systems.\nApache Arrow - In-memory columnar representation of data compatible with Pandas, Hadoop-based systems, etc\nApache Druid - A high performance real-time analytics database. https://druid.apache.org/. An introduction to Druid, your Interactive Analytics at (big) Scale.\nApache Ignite - A memory-centric distributed database, caching, and processing platform for transactional, analytical, and streaming workloads delivering in-memory speeds at petabyte scale. TensorFlow on Apache Ignite, Distributed ML in Apache Ignite\nApache Kafka - Distributed streaming platform framework\nApache Parquet - On-disk columnar representation of data compatible with Pandas, Hadoop-based systems, etc\nApache Pinot - A realtime distributed OLAP datastore https://pinot.apache.org. Comparison of the Open Source OLAP Systems for Big Data: ClickHouse, Druid, and Pinot.\nBayesDB - Database that allows for built-in non-parametric Bayesian model discovery and queryingi for data on a database-like interface - (Video)\nClickHouse - ClickHouse is an open source column oriented database management system supported by Yandex - [(Video)](https://\nDelta Lake - Delta Lake is a storage layer that brings scalable, ACID transactions to Apache Spark and other big-data engines.\nEdgeDB - NoSQL interface for Postgres that allows for object interaction to data stored\nHopsFS - HDFS-compatible file system with scale-out strongly consistent metadata.\nInfluxDB Scalable datastore for metrics, events, and real-time analytics.\nMilvus Milvus is a cloud-native, open-source vector database built to manage embedding vectors generated by machine learning models and neural networks.\nTimescaleDB An open-source time-series SQL database optimized for fast ingest and complex queries. Packaged as a PostgreSQL extension. Time-series ML in TimescaleDB www.youtube.com/watch?v=zbjub8BQPyE)\nWeaviate - A low-latency vector search engine (GraphQL, RESTful) with out-of-the-box support for different media types. Modules include Semantic Search, Q&A, Classification, Customizable Models (PyTorch/TensorFlow/Keras), and more.\nZarr - Python implementation of chunked, compressed, N-dimensional arrays designed for use in parallel computing.\nFunction as a Service Frameworks\nApache OpenWhisk - Open source, distributed serverless platform that executes functions in response to events at any scale.\nFission - (Early Alpha) Serverless functions as a service framework on Kubernetes\nHydrosphere Mist - Serverless proxy for Apache Spark clusters\nHydrosphere ML Lambda - Open source model management cluster for deploying, serving and monitoring machine learning models and ad-hoc algorithms with a FaaS architecture\nKNative Serving - Kubernetes based serverless microservices with \"scale-to-zero\" functionality.\nNuclio - A high-performance serverless framework focused on data, I/O, and compute intensive workloads. It is well integrated with popular data science tools, such as Jupyter and Kubeflow; supports a variety of data and streaming sources; and supports execution over CPUs and GPUs\nOpenFaaS - Serverless functions framework with RESTful API on Kubernetes\nComputation load distribution frameworks\nAnalytics Zoo - A unified Data Analytics and AI platform for distributed TensorFlow, Keras and PyTorch on Apache Spark/Flink & Ray\nApache Spark MLlib - Apache Spark's scalable machine learning library in Java, Scala, Python and R\nBagua - Bagua is a performant and flexible distributed training framework for PyTorch, providing a faster alternative to PyTorch DDP and Horovod. It supports advanced distributed training algorithms such as quantization and decentralization.\nBeam Apache Beam is a unified programming model for Batch and Streaming https://beam.apache.org/\nBigDL - Deep learning framework on top of Spark/Hadoop to distribute data and computations across a HDFS system\nDask - Distributed parallel processing framework for Pandas and NumPy computations - (Video)\nDEAP - A novel evolutionary computation framework for rapid prototyping and testing of ideas. It seeks to make algorithms explicit and data structures transparent. It works in perfect harmony with parallelisation mechanisms such as multiprocessing and SCOOP.\nDeepSpeed - A deep learning optimization library (lightweight PyTorch wrapper) that makes distributed training easy, efficient, and effective.\nFiber - Distributed computing library for modern computer clusters from Uber.\nHadoop Open Platform-as-a-service (HOPS) - A multi-tenancy open source framework with RESTful API for data science on Hadoop which enables for Spark, Tensorflow/Keras, it is Python-first, and provides a lot of features\nHivemind - Decentralized deep learning in PyTorch\nHorovod - Uber's distributed training framework for TensorFlow, Keras, and PyTorch\nNumPyWren - Scientific computing framework build on top of pywren to enable numpy-like distributed computations\nPyWren - Answer the question of the \"cloud button\" for python function execution. It's a framework that abstracts AWS Lambda to enable data scientists to execute any Python function - (Video)\nPyTorch Lightning - Lightweight PyTorch research framework that allows you to easily scale your models to GPUs and TPUs and use all the latest best practices, without the engineering boilerplate - (Video)\nRay - Ray is a flexible, high-performance distributed execution framework for machine learning (VIDEO)\nTensorFlowOnSpark - TensorFlowOnSpark brings TensorFlow programs to Apache Spark clusters.\nVespa Vespa is an engine for low-latency computation over large data sets. https://vespa.ai\nModel serialisation formats\nJava PMML API - Java libraries for consuming and producing PMML files containing models from different frameworks, including:\npyspark2pmml\nr2pmml\nsklearn2pmml\nsparklyr2pmml\nMMdnn - Cross-framework solution to convert, visualize and diagnose deep neural network models.\nNeural Network Exchange Format (NNEF) - A standard format to store models across Torch, Caffe, TensorFlow, Theano, Chainer, Caffe2, PyTorch, and MXNet\nONNX - Open Neural Network Exchange Format\nPFA - Created by the same organisation as PMML, the Predicted Format for Analytics is an emerging standard for statistical models and data transformation engines.\nPMML - The Predictive Model Markup Language standard in XML - (Video)_\nOptimized computation frameworks\nCuDF - Built based on the Apache Arrow columnar memory format, cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data.\nCuML - cuML is a suite of libraries that implement machine learning algorithms and mathematical primitives functions that share compatible APIs with other RAPIDS projects.\nCuPy - An implementation of NumPy-compatible multi-dimensional array on CUDA. CuPy consists of the core multi-dimensional array class, cupy.ndarray, and many functions on it.\nH2O-3 - Fast scalable Machine Learning platform for smarter applications: Deep Learning, Gradient Boosting & XGBoost, Random Forest, Generalized Linear Modeling (Logistic Regression, Elastic Net), K-Means, PCA, Stacked Ensembles, Automatic Machine Learning (AutoML), etc.\nJax - Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more\nModin - Speed up your Pandas workflows by changing a single line of code\nNumba - A compiler for Python array and numerical functions\nNumpyGroupies Optimised tools for group-indexing operations: aggregated sum and more\nOpenVINO\u2122 integration with TensorFlow - Highly optimized Neural Network inference with Tensorflow on Intel platforms by adding a single line of code\nVaex Vaex is a high performance Python library for lazy Out-of-Core DataFrames (similar to Pandas), to visualize and explore big tabular datasets. Vaex uses memory mapping, zero memory copy policy and lazy computations for best performance (no memory wasted).\nVulkan Kompute - Blazing fast, lightweight and mobile phone-enabled Vulkan compute framework optimized for advanced GPU data processing usecases.\nWeld High-performance runtime for data analytics applications, Interview with Weld\u2019s main contributor\nData Stream Processing\nApache Flink - Open source stream processing framework with powerful stream and batch processing capabilities.\nApache Samza - Distributed stream processing framework. It uses Apache Kafka for messaging, and Apache Hadoop YARN to provide fault tolerance, processor isolation, security, and resource management.\nBrooklin - Distributed stream processing framework. It uses Apache Kafka for messaging, and Apache Hadoop YARN to provide fault tolerance, processor isolation, security, and resource management.\nFaust - Streaming library built on top of Python's Asyncio library using the async kafka client inspired by the kafka streaming library.\nKafka Streams - Kafka client library for buliding applications and microservices where the input and output are stored in kafka clusters\nSpark Streaming - Micro-batch processing for streams using the apache spark framework as a backend supporting stateful exactly-once semantics\nOutlier and Anomaly Detection\nadtk - A Python toolkit for rule-based/unsupervised anomaly detection in time series.\nAlibi-Detect - Algorithms for outlier and adversarial instance detection, concept drift and metrics.\ndBoost - Outlier detection in heterogeneous datasets using automatic tuple expansion. Paper.\nDeequ - A library built on top of Apache Spark for defining \"unit tests for data\", which measure data quality in large datasets.\nDeep Anomaly Detection with Outlier Exposure - Outlier Exposure (OE) is a method for improving anomaly detection performance in deep learning models. Paper\nOpen Distro Anomaly Detection - The Open Distro (Amazon) for the Elasticsearch Anomaly Detection plugin.\nPyOD - A Python Toolbox for Scalable Outlier Detection (Anomaly Detection).\nSUOD (Scalable Unsupervised Outlier Detection) - An Acceleration System for Large-scale Outlier Detection (Anomaly Detection)\nTensorflow Data Validation (TFDV) - Library for exploring and validating machine learning data.\nFeature Engineering Automation\nauto-sklearn - Framework to automate algorithm and hyperparameter tuning for sklearn\nAutoGluon - Automated feature, model, and hyperparameter selection for tabular, image, and text data on top of popular machine learning libraries (Scikit-Learn, LightGBM, CatBoost, PyTorch, MXNet)\nAutoML-GS - Automatic feature and model search with code generation in Python, on top of common data science libraries (tensorflow, sklearn, etc)\nautoml - Automated feature engineering, feature/model selection, hyperparam. optimisation\nColombus - A scalable framework to perform exploratory feature selection implemented in R\nFeature Engine - Feature-engine is a Python library that contains several transformers to engineer features for use in machine learning models.\nFeaturetools - An open source framework for automated feature engineering\ngo-featureprocessing - A feature pre-processing framework in Go that matches functionality of sklearn\nkeras-tuner - Keras Tuner is an easy-to-use, distributable hyperparameter optimization framework that solves the pain points of performing a hyperparameter search. Keras Tuner makes it easy to define a search space and leverage included algorithms to find the best hyperparameter values.\nsklearn-deap Use evolutionary algorithms instead of gridsearch in scikit-learn.\nTPOT - Automation of sklearn pipeline creation (including feature selection, pre-processor, etc)\ntsfresh - Automatic extraction of relevant features from time series\nmljar-supervised - An Automated Machine Learning (AutoML) python package for tabular data. It can handle: Binary Classification, MultiClass Classification and Regression. It provides feature engineering, explanations and markdown reports.\nFeature Stores\nButterfree - A tool for building feature stores which allows you to transform your raw data into beautiful features.\nFeature Store for Machine Learning (FEAST) - Feast (Feature Store) is a tool for managing and serving machine learning features. Feast is the bridge between models and data.\nHopsworks Feature Store - Offline/Online Feature Store for ML (Video).\nIvory - ivory defines a specification for how to store feature data and provides a set of tools for querying it. It does not provide any tooling for producing feature data in the first place. All ivory commands run as MapReduce jobs so it assumed that feature data is maintained on HDFS.\nVeri - Veri is a Feature Label Store. Feature Label store allows storing features as keys and labels as values. Querying values is only possible with knn using features. Veri also supports creating sub sample spaces of data by default.\nCommercial Platforms\nAlgorithmia - Cloud platform to build, deploy and serve machine learning models (Video)\nallegro ai Enterprise - Automagical open-source ML & DL experiment manager and ML-Ops solution.\nAmazon SageMaker - End-to-end machine learning development and deployment interface where you are able to build notebooks that use EC2 instances as backend, and then can host models exposed on an API\nApheris - A platform for federated and privacy-preserving data science that lets you securely collaborate on AI with partners without sharing any data.\nArize AI - ML observability and automated model monitoring to help ML practitioners understand how their models perform in production, troubleshoot issues, and improve model performance. ML teams can upload offline (training or validation) baselines into an evaluation/inference store alongside online production data for model validation, drift detection, data quality checks, and model performance management.\nBackprop - Serverless hosting of ML models.\nbigml - E2E machine learning platform.\ncnvrg.io - An end-to-end platform to manage, build and automate machine learning\nComet.ml - Machine learning experiment management. Free for open source and students (Video)\nCubonacci - The Cubonacci platform manages deployment, versioning, infrastructure, monitoring and lineage for you, eliminating risk and minimizing time-to-market.\nD2iQ KUDO for Kubeflow - Enterprise machine learning platform that runs in the cloud, on premises (incl. air-gapped), in hybrid environments, or on the edge; based on Kubeflow and open-source Kubernetes Universal Declarative Operators (KUDO).\nDAGsHub - Community platform for Open Source ML \u2013 Manage experiments, data & models and create collaborative ML projects easily.\nDataiku - Collaborative data science platform powering both self-service analytics and the operationalization of machine learning models in production.\nDataRobot - Automated machine learning platform which enables users to build and deploy machine learning models.\nDatatron - Machine Learning Model Governance Platform for all your AI models in production for large Enterprises.\ndeepsense AIOps - Enhances multi-cloud & data center IT Operations via traffic analysis, risk analysis, anomaly detection, predictive maintenance, root cause analysis, service ticket analysis and event consolidation.\nDeep Cognition Deep Learning Studio - E2E platform for deep learning.\ndeepsense Safety - AI-driven solution to increase worksite safety via safety procedure check, thread detection and hazardous zones monitoring.\ndeepsense Quality - Automating laborious quality control tasks.\nDiffgram - Training Data First platform. Database & Training Data Pipelines for Supervised AI. Integrated with GCP, AWS, Azure and top Annotation Supervision UIs (or use built-in Diffgram UI, or build your own). Plus a growing list of integrated service providers! For Computer Vision, NLP, and Supervised Deep Learning / Machine Learning.\nGoogle Cloud Machine Learning Engine - Managed service that enables developers and data scientists to build and bring machine learning models to production.\nGraphsignal - Production model monitoring to ensure deployed machine learning model performance and reduce troubleshooting time.\nH2O Driverless AI - Automates key machine learning tasks, delivering automatic feature engineering, model validation, model tuning, model selection and deployment, machine learning interpretability, bring your own recipe, time-series and automatic pipeline generation for model scoring. (Video)\nHypervector - API-driven test data fixtures for machine learning and data science features, making it easy to build automated continuous integration steps for your data-driven components\nIBM Watson Machine Learning - Create, train, and deploy self-learning models using an automated, collaborative workflow.\nIguazio Data Science Platform - Bring your Data Science to life by automating MLOps with end-to-end machine learning pipelines, transforming AI projects into real-world business outcomes, and supporting real-time performance at enterprise scale.\nLabelbox - Image labelling service with support for semantic segmentation (brush & superpixels), bounding boxes and nested classifications.\nLogical Clocks Hopsworks - Enterprise version of Hopsworks with a Feature Store and scale-out ML pipeline design and operation.\nMCenter - MLOps platform automates the deployment, ongoing optimization, and governance of machine learning applications in production.\nMicrosoft Azure Machine Learning service - Build, train, and deploy models from the cloud to the edge.\nMLJAR - Platform for rapid prototyping, developing and deploying machine learning models.\nNeptune.ai - community-friendly platform supporting data scientists in creating and sharing machine learning models. Neptune facilitates teamwork, infrastructure management, models comparison and reproducibility.\nProdigy - Active learning-based data annotation. Allows to train a model and pick most 'uncertain' samples for labeling from an unlabeled pool.\nScribble Enrich - Customizable, auditable, privacy-aware feature store. It is designed to help mid-sized data teams gain trust in the data that they use for training and analysis, and support emerging needs such drift computation and bias assessment.\nSKIL - Software distribution designed to help enterprise IT teams manage, deploy, and retrain machine learning models at scale.\nSkytree 16.0 - End to end machine learning platform (Video)\nSpell - Flexible end-to-end MLOps / Machine Learning Platform. (Video)\nSuperAnnotate - A complete set of solutions for image and video annotation and an annotation service with integrated tooling, on-demand narrow expertise in various fields, and a custom neural network, automation, and training models powered by AI.\nSuperb AI - ML DataOps platform providing various tools to build, label, manage and iterate on training data.\nSyndicai - Easy-to-use cloud agnostic platform that deploys, manages, and scales any trained AI model in minutes with no configuration & infrastructure setup.\nTalend Studio\nValohai - Machine orchestration, version control and pipeline management for deep learning.\nWeights & Biases - Machine learning experiment tracking, dataset versioning, hyperparameter search, visualization, and collaboration",
      "link": "https://github.com/EthicalML/awesome-production-machine-learning"
    },
    {
      "autor": "awesome-aws",
      "date": "NaN",
      "content": "Awesome AWS\nA curated list of awesome AWS libraries, open source repos, guides, blogs, and other resources.\nInspired by the awesome list.\nThe Fiery Meter of AWSome\nRepo with 0100+ Stars: \ud83d\udd25\nRepo with 0200+ Stars: \ud83d\udd25\ud83d\udd25\nRepo with 0500+ Stars: \ud83d\udd25\ud83d\udd25\ud83d\udd25\nRepo with 1000+ Stars: \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nRepo with 2000+ Stars: \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nRepos not on The Fiery Meter of AWSome can still be awesome, see A Note on Repo AWSomeness.\nawesome-aws Python Module\nThe Python module awesome-aws regularly scans repos on Awesome AWS to maintain the accuracy of the Fiery Meter of AWSome.\nContributing\nContributions are welcome!\nReview the Contributing Guidelines.\nAlso check out the Watch List.\nIndex\nSDKs and Samples\nAndroid\nC++\nClojure\nGo\niOS\nIoT\nJava\nJavaScript\nHaskell\nPerl\nPHP\nPython\nRuby\nRust\nScala\nXamarin\nUnity\n.NET\nCommand Line Tools\nUniversal Command Line Interface\nWindows PowerShell\nIDE Toolkits\nEclipse Toolkit\nVisual Studio Toolkit\nOpen Source Repos\nAPI Gateway\nCLI\nCloudFormation\nCloudSearch\nCloudTrail\nCloudWatch\nCode Deploy\nCode Pipeline\nCognito\nData Pipeline\nDevice Farm\nDynamoDB\nElastic Beanstalk\nElastic Container Service\nElastic File System\nElastic MapReduce\nElastic Search\nElasticache\nGlacier\nKinesis\nLambda\nMachine Learning\nMobile Analytics\nOpsWorks\nRedshift\nRoute 53\nS3\nSNS\nSQS\nData\nDevOps\nSecurity\nAccompanying\nMiscellaneous\nGuides, Books, Documentation, and Training\nGetting Started Guides\nGeneral Guides\nBooks\nWhitepapers\nDocumentation\nTraining\nCase Studies: Powered by AWS\nSocial\nBlogs\nTwitter Influencers\nFacebook Pages\nYouTube Channels\nLinkedIn Groups\nSubreddits\nConferences\nLatest KPIs and Stats\nAppendix of Core Services\nServices in Plain English\nCompute\nNetworking\nEnterprise Applications\nAnalytics\nArtificial Intelligence\nManagement Tools\nSecurity and Identity\nInternet of Things\nMobile Services\nStorage and Content Delivery\nDatabases\nApplication Services\nDeveloper Tools\nMiscellaneous Services\nContributing\nCredits\nOther Awesome Lists\nContact Info\nLicense\nSDKs and Samples\nAWS and community SDKs with samples and docs, grouped by language.\nAndroid SDK\nRepo \ud83d\udd25\ud83d\udd25\ud83d\udd25\nRepo with Samples \ud83d\udd25\ud83d\udd25\ud83d\udd25\nInstall\nDocs\nLearn More\nC++ SDK\nRepo \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nBlog with Samples\nThe C++ SDK is a labs project with limited docs and/or samples.\nClojure SDK\nRepo \ud83d\udd25\ud83d\udd25\ud83d\udd25\nInstall\nDocs\nThe Clojure SDK is a community project with limited docs and/or samples.)\nGo SDK\nRepo \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nInstall\nDocs\nLearn More\nRelated Repos:\ngoamz/goamz \ud83d\udd25\ud83d\udd25\niOS SDK\nRepo \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nRepo with Samples \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nInstall\nDocs\nLearn More\nIoT SDK\nRepo for Arduino\nRepo for C \ud83d\udd25\ud83d\udd25\ud83d\udd25\nRepo for JavaScript \ud83d\udd25\ud83d\udd25\ud83d\udd25\nRepo for Arduino Yun \ud83d\udd25\nDocs\nThe IoT SDK is a labs project with limited docs and/or samples.\nJava SDK\nRepo \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nRepo with Samples \ud83d\udd25\ud83d\udd25\nInstall\nDocs\nLearn More\nJavaScript SDK\nRepo \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nRepo with Samples \ud83d\udd25\ud83d\udd25\nInstall\nDocs\nLearn More\nRelated Repos:\naws/aws-amplify \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nchilts/awssum \ud83d\udd25\ud83d\udd25\nmirkokiefer/aws-lib \ud83d\udd25\ud83d\udd25\ud83d\udd25\nSaltwaterC/aws2js \ud83d\udd25\ud83d\udd25\nHaskell SDK\nRepo \ud83d\udd25\ud83d\udd25\nDocs\nRelated Repos:\naristidb/aws \ud83d\udd25\ud83d\udd25\nThe Haskell SDK is a community project with limited docs and/or samples.\nPerl SDK\nRepo \ud83d\udd25\nRepo with Samples \ud83d\udd25\nInstall\nDocs\nLearn More\nThe Perl SDK is a community project.\nPHP SDK\nRepo \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nRepo with Samples\nInstall\nDocs\nLearn More\nRelated Repos:\naws-sdk-php-laravel \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\naws-sdk-php-silex\naws-sdk-php-zf2 \ud83d\udd25\nPython SDK\nRepo \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nRepo with Samples \ud83d\udd25\nInstall\nDocs\nLearn More\nRelated Repos:\nboto3 \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nbotocore \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nRuby SDK\nRepo \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nRepo with S3 Sample\nInstall\nDocs\nSamples \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nRelated Repos:\naws-sdk-rails \ud83d\udd25\ud83d\udd25\nappoxy/aws \ud83d\udd25\ud83d\udd25\nrightscale/right_aws \ud83d\udd25\ud83d\udd25\nRust SDK\nRepo \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nInstall\nDocs\nThe Rust SDK is a community project with limited docs and/or samples.\nScala SDK\nRepo\nRelated Repos:\natlassian/aws-scala\nseratch/AWScala \ud83d\udd25\ud83d\udd25\ud83d\udd25\nThe Scala SDK is a labs project with limited docs and/or samples.\nUnity SDK\nRepo \ud83d\udd25\nRepo with Samples \ud83d\udd25\nInstall\nDocs\nXamarin SDK\nRepo\nBlog with Samples\nThe Xamarin SDK is a labs project with limited docs and/or samples.\n.NET SDK\nRepo \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nRepo with Samples\nInstall\nDocs\nLearn More\nSamples \ud83d\udd25\nCommand Line Tools\nAWS and community command line tools with samples and docs.\nUniversal Command Line Interface\nRepo \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nInstall\nDocs\nLearn More\nRelated Repos:\nawslabs/aws-shell \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ndonnemartin/saws \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nWindows PowerShell\nInstall\nDocs\nLearn More\nIDE Toolkits\nOfficial IDE toolkits with samples and docs.\nEclipse Toolkit\nInstall\nDocs\nLearn More\nVisual Studio Toolkit\nInstall\nDocs\nLearn More\nOpen Source Repos\nAWS and community open source projects, grouped by service. See A Note on Repo AWSomeness for more details.\nAPI Gateway\nAWS Repos:\napi-gateway-secure-pet-store \ud83d\udd25\ud83d\udd25 - Cognito credentials through Lambda.\naws-apigateway-sdk-java - SDK for Java.\naws-apigateway-swagger-importer \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Tools to work with Swagger.\nCommunity Repos:\nContribute\nCLI\nAWS Repos:\nawscli-aliases \ud83d\udd25\ud83d\udd25 - Repository for AWS CLI aliases.\namazon-ecs-cli \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - ECS CLI using the same Docker Compose file format and familiar Compose commands.\naws-cli \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Universal Command Line Interface.\naws-shell \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nawscli-cookbook - Installs the CLI tools and provides a set of LWRPs for use within chef cookbooks.\nawsmobile-cli \ud83d\udd25 - CLI experience for Frontend developers in the JavaScript ecosystem.\nCommunity Repos:\nachiku/jungle \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Operations by EC2 and ELB cli should be simpler.\ndbcli/athenacli \ud83d\udd25 - a CLI tool for AWS Athena service that can do auto-completion and syntax highlighting.\ndonnemartin/saws \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - A Supercharged AWS Command Line Interface.\ntimkay/aws \ud83d\udd25\ud83d\udd25 - Easy command line access to Amazon EC2, S3, SQS, ELB, and SDB.\nwallix/awless \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - a Powerful CLI for EC2, IAM and S3 in Go.\n99designs/aws-vault \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - A tool for securely storing AWS credentials, written in Go.\nCloudFormation\nAWS Repos:\naws-cdk \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Framework for defining cloud infrastructure in code.\naws-cfn-custom-resource-examples - Custom resource examples.\naws-cfn-resource-bridge - Custom resource framework.\ncfn-python-lint \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - A tool for linting/validating CloudFormation.\ncfncluster-cookbook - Sample Cookbook.\ncfncluster \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Framework that deploys and maintains HPC clusters.\nCommunity Repos:\nAppliscale/perun - A CLI tool for linting/validation and managing CloudFormation templates and stacks.\nbazaarvoice/cloudformation-ruby-dsl \ud83d\udd25\ud83d\udd25 - Ruby DSL for creating templates.\nbeaknit/cform \ud83d\udd25 - SublimeText plugin.\ncloudreach/sceptre \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - A CLI tool for automating CloudFormation.\ncloudtools/troposphere \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Python library to create descriptions.\npeterkh/cumulus \ud83d\udd25\ud83d\udd25 - Manages stacks.\nenvato/stack_master \ud83d\udd25\ud83d\udd25 - A CLI tool to manage CloudFormation stacks.\nsparkleformation/sfn - CLI for stack management.\nsparkleformation/sparkle_formation \ud83d\udd25\ud83d\udd25 - Ruby DSL for template creation.\nStelligent/cfn_nag \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Linting tool for CloudFormation templates\nCloudSearch\nAWS Repos:\ncloudsearchable - An ActiveRecord-style ORM query interface.\nCommunity Repos:\nContribute\nCloudTrail\nAWS Repos:\naws-cloudtrail-processing-library - Easily consume and process log files.\nCommunity Repos:\nAppliedTrust/traildash \ud83d\udd25\ud83d\udd25 - Slick dashboard.\nGorillaStack/auto-tag \ud83d\udd25\ud83d\udd25 - Automatically tag AWS resources on creation, for cost assignment.\nCloudWatch\nAWS Repos:\ncloudwatch-logs-subscription-consumer \ud83d\udd25\ud83d\udd25 - Kinesis stream reader.\necs-cloudwatch-logs - Assets in the blog post on using Amazon ECS and Amazon CloudWatch logs.\nlogstash-output-cloudwatchlogs - A logstash plugin that sends logs to CloudWatch.\nopsworks-cloudwatch-logs-cookbooks - OpsWorks sample cookbook.\nCommunity Repos:\njorgebastida/awslogs \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Simple CLI for querying groups, streams and events.\nnewrelic-platform/newrelic_aws_cloudwatch_plugin \ud83d\udd25 - New Relic plugin.\nCode Deploy\nAWS Repos:\naws-codedeploy-agent \ud83d\udd25\ud83d\udd25 - Sample agent.\naws-codedeploy-plugin \ud83d\udd25 - Jenkins plugin.\naws-codedeploy-samples \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Samples and template scenarios.\nCommunity Repos:\nContribute\nCode Pipeline\nAWS Repos:\naws-codepipeline-custom-job-worker - Develop your own job worker when creating a custom action.\naws-codepipeline-jenkins-aws-codedeploy_linux - Four-stage pipeline for Linux.\naws-codepipeline-plugin-for-jenkins - Jenkins plugin.\naws-codepipeline-s3-aws-codedeploy_linux \ud83d\udd25 - Simple pipeline for Linux.\nAWSCodePipeline-Jenkins-AWSCodeDeploy_Windows - Four-stage pipeline for Windows.\nAWSCodePipeline-S3-AWSCodeDeploy_Windows - Simple pipeline for Windows.\nCommunity Repos:\nContribute\nCognito\nAWS Repos:\namazon-cognito-android - Sync SDK for Android.\namazon-cognito-developer-authentication-sample - Authentication sample.\namazon-cognito-dotnet - Sync SDK for .NET.\namazon-cognito-ios - Sync SDK for iOS.\namazon-cognito-js \ud83d\udd25\ud83d\udd25 - Sync SDK for JavaScript.\namazon-cognito-streams-sample - Consuming Streams sample.\ncognito-sample-nodejs \ud83d\udd25 - Sample App for Node.js.\nCommunity Repos:\ncapeless/warrant \ud83d\udd25\ud83d\udd25 - Python library for using Cognito.\nrahulpsd18/cognito-backup-restore \ud83d\udd25 - Tool for backing up and restoring Cognito user pools.\nData Pipeline\nAWS Repos:\ndata-pipeline-samples \ud83d\udd25\ud83d\udd25 - Sample pipelines.\nCommunity Repos:\nContribute\nDevice Farm\nAWS Repos:\naws-device-farm-appium-tests-for-sample-app - Appium TestNG Android tests.\naws-device-farm-calabash-tests-for-sample-app - Calabash Android tests.\naws-device-farm-gradle-plugin - Gradle plugin.\naws-device-farm-jenkins-plugin - Jenkins plugin.\naws-device-farm-sample-app-for-android \ud83d\udd25 - Sample Android app.\nCommunity Repos:\nContribute\nDynamoDB\nAWS Repos:\naws-dotnet-session-provider - A session state provider for ASP.NET apps.\naws-dotnet-trace-listener - A trace listener for System.Diagnostics that can be used to log events.\naws-dynamodb-encryption-java \ud83d\udd25 - Encryption Client for Java.\naws-dynamodb-examples \ud83d\udd25\ud83d\udd25 - Samples using the Java SDK.\naws-dynamodb-mars-json-demo - Stores and indexes NASA JPL Mars images.\naws-dynamodb-session-tomcat - Session store for Apache Tomcat.\naws-sessionstore-dynamodb-ruby - Handles sessions for Ruby web apps.\ndynamodb-cross-region-library \ud83d\udd25\ud83d\udd25 - Cross-region replication.\ndynamodb-geo \ud83d\udd25\ud83d\udd25 - Library to create and query geospatial data.\ndynamodb-import-export-tool - Import and export examples.\ndynamodb-online-index-violation-detector - Finds violations on an online GSI's hash key and range key.\ndynamodb-streams-kinesis-adapter - Kinesis interface to consume and process data from a DynamoDB stream.\ndynamodb-tictactoe-example-app - Lightweight python app.\ndynamodb-titan-storage-backend \ud83d\udd25\ud83d\udd25 - Storage Backend for Titan.\ndynamodb-transactions \ud83d\udd25\ud83d\udd25 - Performs atomic writes and isolated reads across multiple items and tables.\nlogstash-input-dynamodb \ud83d\udd25 - Logstash input plugin.\nCommunity Repos:\nchannl/dynamodb-lambda-autoscale \ud83d\udd25\ud83d\udd25 - Autoscale DynamoDB provisioned capacity using Lambda.\nlyft/confidant \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Stores secrets, encrypted at rest.\nsebdah/dynamic-dynamodb \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Provides auto-scaling.\nElastic Beanstalk\nAWS Repos:\naws-eb-glassfish-dockerfiles - GlassFish docker files.\naws-eb-python-dockerfiles - Python docker files.\neb-demo-php-simple-app \ud83d\udd25 - Simple PHP app.\neb-docker-multiple-ports - Simple Node.js and Tomcat apps using Docker images.\neb-docker-nginx-proxy \ud83d\udd25 - Simple PHP app using the PHP-FPM and Nginx Docker images.\neb-docker-virtual-hosting - Simple PHP, Tomcat, and Nginx applications using Docker images.\neb-node-express-sample \ud83d\udd25\ud83d\udd25 - Sample express app.\neb-node-express-signup - Express framework and Bootstrap Node.js sample app.\neb-node-express - Sample app referenced in the Developer Guide.\neb-py-flask-signup-worker - Python app that illustrates worker roles.\neb-py-flask-signup \ud83d\udd25\ud83d\udd25 - Python signup form app with Flask and Bootstrap.\neb-python-flask - Simple Python and Flask app.\neb-wif-sample - Sample login app with Web Identity Federation.\nCommunity Repos:\nalienfast/elastic-beanstalk \ud83d\udd25 - Gem with rake configuration and deployment for rails apps.\nThoughtWorksStudios/eb_deployer \ud83d\udd25\ud83d\udd25 - Blue-green deployment automation.\nElastic Compute Cloud\nAWS Repos:\nContribute\nCommunity Repos:\nalestic/ec2-consistent-snapshot \ud83d\udd25\ud83d\udd25 - Initiate consistent EBS snapshots in EC2.\nConradIrwin/aws-name-server \ud83d\udd25\ud83d\udd25\ud83d\udd25 - DNS server that lets you look up instances by name.\ncristim/autospotting \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Automatically rolling-replace on-demand EC2 instances in AutoScaling groups with compatible spot instances.\nevannuil/aws-snapshot-tool \ud83d\udd25\ud83d\udd25 - Automates EBS snapshots and rotation.\nkelseyhightower/kubernetes-the-hard-way \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Bootstrap Kubernetes the hard way on EC2. No scripts.\nmirakui/ec2ssh \ud83d\udd25\ud83d\udd25 - SSH config manager.\nopenebs/openebs \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Containerized block storage QoS SLAs, tiering and replica policies across AZs and environments, and predictable and scalable performance.\nskavanagh/EC2Box \ud83d\udd25\ud83d\udd25 - A web-based SSH console to manage multiple instances simultaneously.\nwbailey/claws \ud83d\udd25 - CLI-driven console with capistrano integration.\nElastic Container Service\nAWS Repos:\namazon-ecs-agent \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Agent that runs on and starts containers.\namazon-ecs-amazon-efs - Persists Data from containers.\namazon-ecs-init \ud83d\udd25 - RPM developed to support the Amazon ECS Container Agent.\nblox \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Open source tools for building custom schedulers on ECS.\necs-blue-green-deployment \ud83d\udd25\ud83d\udd25 - Blue-green deployment on ECS.\necs-cloudwatch-logs - Assets from the blog using Amazon ECS and Amazon CloudWatch logs.\necs-demo-php-simple-app \ud83d\udd25 - Simple PHP app.\necs-mesos-scheduler-driver \ud83d\udd25 - Integrates Apache Mesos.\necs-refarch-continuous-deployment \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Reference Architecture for continuous deployment to ECS using CodePipeline.\necs-task-kite - Simple ambassador container for inter-task communication.\nlambda-ecs-worker-pattern \ud83d\udd25\ud83d\udd25 - Extends Lambda using SQS and ECS.\npy-flask-signup-docker - Python sample app.\nservice-discovery-ecs-consul \ud83d\udd25 - Assets from the blog Service Discovery via Consul with Amazon ECS.\nCommunity Repos:\nLumoslabs/broadside - Command line tool for deploying revisions of containerized applications.\nStelligent/mu \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Command line tool to simplify ECS deployments via CodeBuild and CodePipeline.\nElastic File System\nAWS Repos:\namazon-ecs-amazon-efs - Persist data from ECS.\nCommunity Repos:\nContribute\nElastic MapReduce\nAWS Repos:\nemr-bootstrap-actions \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Sample bootstrap actions.\nemr-sample-apps - Sample apps.\nCommunity Repos:\nYelp/mrjob \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Run MapReduce jobs on Hadoop or EMR.\nElastic Search\nAWS Repos:\nlogstash-output-amazon_es \ud83d\udd25\ud83d\udd25 - Logstash output plugin to sign and export events.\nopsworks-elasticsearch-cookbook - OpsWorks Elasticsearch sample cookbook.\nCommunity Repos:\nelastic/elasticsearch-cloud-aws \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Plugin for Elasticsearch.\nElasticache\nAWS Repos:\naws-elasticache-cluster-client-libmemcached - Libmemcached library support.\naws-elasticache-cluster-client-memcached-for-java - Client for Java.\naws-elasticache-cluster-client-memcached-for-php - Enhanced PHP library connecting to ElastiCache.\nelasticache-cluster-config-net - Config object for Enyim's MemcachedClient to enable auto discovery.\nCommunity Repos:\nContribute\nGlacier\nCommunity Repos:\nvsespb/mt-aws-glacier \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Perl Multithreaded Multipart sync to Glacier.\nKinesis\nAWS Repos:\namazon-kinesis-aggregators \ud83d\udd25 - Provides a simple way to create real time aggregations.\namazon-kinesis-client-net - Client Library for .NET.\namazon-kinesis-client-nodejs \ud83d\udd25\ud83d\udd25 - Client Library for Node.js.\namazon-kinesis-client-python \ud83d\udd25\ud83d\udd25 - Client Library for Python.\namazon-kinesis-client-ruby \ud83d\udd25 - Client Library for Ruby.\namazon-kinesis-client \ud83d\udd25\ud83d\udd25\ud83d\udd25 Client library for Amazon Kinesis.\namazon-kinesis-connectors \ud83d\udd25\ud83d\udd25 - Libary to integrate with other AWS and non-AWS services.\namazon-kinesis-data-visualization-sample \ud83d\udd25 - Sample data visualization app.\namazon-kinesis-learning - Learning Kinesis Development.\namazon-kinesis-producer \ud83d\udd25\ud83d\udd25 - Producer Library.\namazon-kinesis-scaling-utils \ud83d\udd25\ud83d\udd25 - Provides the ability to scale streams.\naws-fluent-plugin-kinesis \ud83d\udd25\ud83d\udd25 - Fluent Plugin.\ndynamodb-streams-kinesis-adapter - DynamoDB Streams Adapter.\nkinesis-log4j-appender - Log4J Appender.\nkinesis-poster-worker - Simple multi-threaded Python Poster and Worker.\nkinesis-storm-spout \ud83d\udd25 - Spout for Storm.\nmqtt-kinesis-bridge - Simple MQTT bridge in Python.\nCommunity Repos:\nContribute\nLambda\nAWS Repos:\namazon-elasticsearch-lambda-samples \ud83d\udd25\ud83d\udd25 - Data ingestion for Elasticsearch from S3 and Kinesis.\nawslabs/aws-sam-local \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - CLI tool for local development and testing of Serverless applications.\naws-lambda-go \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Libraries, samples and tools to help Go developers develop Lambda functions.\naws-lambda-java-libs \ud83d\udd25\ud83d\udd25 - Official mirror for interface definitions and helper classes.\naws-lambda-redshift-loader \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Redshift loader.\nchalice \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Python Serverless Microframework.\ncreate-thumbnails-lambda - Uses the grunt-aws-lambda plugin to help you develop and test.\nlambda-ecs-worker-pattern \ud83d\udd25\ud83d\udd25 - Extends Lambda using SQS and ECS.\nlambda-refarch-fileprocessing \ud83d\udd25\ud83d\udd25 - Reference Architecture for Real-time File Processing.\nlambda-refarch-iotbackend \ud83d\udd25\ud83d\udd25 - Reference Architecture for creating an IoT Backend.\nlambda-refarch-mobilebackend \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Reference Architecture for creating a Mobile Backend.\nlambda-refarch-webapp \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Reference Architecture for creating a Web Application.\nCommunity Repos:\nalestic/lambdash \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Lambda shell - Run sh commands inside the Lambda environment.\nAlephbet/gimel \ud83d\udd25\ud83d\udd25 - Run your own A/B testing backend using Lambda.\napex/apex - Minimal AWS Lambda function manager with Go support.\nclaudiajs/claudia \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Deploy Node.js projects to Lambda and API Gateway easily.\ncloudnative/lambda-chat \ud83d\udd25\ud83d\udd25 - A chat application without servers.\ndanilop/LambdAuth \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Sample authentication service.\neawsy/aws-lambda-go \ud83d\udd25\ud83d\udd25\ud83d\udd25 - A fast and clean way to execute Go on Lambda.\ngarnaat/kappa \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Kappa is a CLI tool that makes it easier to deploy, update, and test functions for AWS Lambda.\ngoadapp/goad \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Lambda powered, highly distributed, load testing tool.\ngraphcool/chromeless \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Automate Chrome through Lambda.\ngrycap/scar \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Transparently execute containers out of Docker images in AWS Lambda.\njeremydaly/lambda-api \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Lightweight web framework for your serverless applications.\njimpick/lambda-comments \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Blog commenting system built with Lambda.\njorgebastida/gordon \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - \u03bb Gordon is a tool to create, wire and deploy AWS Lambdas using CloudFormation.\nks888/LambStatus \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - A status page system inspired by StatusPage.io, built on AWS Lambda.\nkubek2k/lambdoku \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Heroku-like experience when using Lambda.\nlambci/lambci \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - A continuous integration system built on Lambda.\nlittlstar/s3-lambda \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Lambda functions over S3 objects with concurrency control (each, map, reduce, filter).\nmentum/lambdaws \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Deploy, run and get results in a breeze.\nMiserlou/Zappa \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Serverless WSGI Python Web Apps with AWS Lambda + API Gateway.\nnficano/python-lambda \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - A toolkit for developing and deploying serverless Python code in Lambda.\nserverless/serverless \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 The Serverless Application Framework (formerly JAWS).\nTim-B/grunt-aws-lambda \ud83d\udd25\ud83d\udd25 - Grunt plugin.\ntrek10inc/aws-lambda-debugger \ud83d\udd25\ud83d\udd25 - Remote debugging tool for Lambda functions running on Node 6.10\nMachine Learning\nAWS Repos:\nmachine-learning-samples \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Sample apps.\nCommunity Repos:\nContribute\nMobile Analytics\nAWS Repos:\naws-sdk-mobile-analytics-js - JavaScript SDK.\nCommunity Repos:\nContribute\nOpsWorks\nAWS Repos:\nopsworks-attribute-customization - Attribute customization example.\nopsworks-capistrano - Capistrano with instances.\nopsworks-cloudwatch-logs-cookbooks - CloudWatch sample cookbook.\nopsworks-cookbooks \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Chef Cookbooks.\nopsworks-demo-php------> photo !!! -share-app - Simple PHP -----> photo !!!  share app.\nopsworks-demo-php-simple-app - Simple PHP app.\nopsworks-demo-rails-photo-share-app - A sample Rails app.\nopsworks-elasticsearch-cookbook - Elasticsearch sample cookbook.\nopsworks-example-cookbooks - Cookbooks used with the sample apps.\nopsworks-first-cookbook - Cookbook used to demonstrate simple recipes.\nopsworks-windows-demo- - A sample Node.JS app.\nopsworks-windows-demo-cookbooks - Cookbooks for Windows.\ntodo-sample-app-cookbooks - Custom cookbooks associated with the todo-sample-app.\nCommunity Repos:\nContribute\nRedshift\nAWS Repos:\naws-lambda-redshift-loader \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Lambda database loader.\namazon-redshift-utils \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Applies optimal Column Encoding to existing Tables.\nCommunity Repos:\nLumoslabs/aleph - A full featured web application for writing and running Redshift queries. Supports revision tracking of queries and has basic visualization support.\ngetredash/redash \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - A web application that allows to easily query an existing database, share the dataset and visualize it in different ways. Initially was developed to work with Redshift, and has great support for it.\neverythingMe/redshift_console - A simple tool to monitor and manage a Redshift cluster. The first release has basic tools to monitor running queries, WLM queue and your tables/schemas.\nRoute 53\nAWS Repos:\nroute53-infima \ud83d\udd25\ud83d\udd25 - Manages service-level fault isolation.\nCommunity Repos:\nbarnybug/cli53 \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - cli53 is a command line tool for Amazon Route 53 which provides import and export from BIND format and simple command line management of Route 53 domains.\nwinebarrel/roadworker \ud83d\udd25\ud83d\udd25 - Roadworker is a tool to manage Route53. It defines the state of Route53 using DSL, and updates Route53 according to DSL.\nS3\nCommunity Repos:\nanomalizer/ngx_aws_auth \ud83d\udd25\ud83d\udd25 - Implements proxying of authenticated requests.\nbloomreach/s4cmd \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - S3 command line tool, faster than S3cmd for large files.\nCulturalMe/meteor-slingshot \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Upload files in meteor.\ndanilop/yas3fs \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Yet Another S3-backed File System, inspired by s3fs.\ngrippy/node-s3 - Node.js app to manage buckets.\njubos/fake-s3 \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Lightweight S3 clone that simulates most commands.\nkahing/goofys \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - a Filey System for Amazon S3 written in Go.\nlittlstar/s3renity \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Batch functions with concurrency control (each, map, reduce, filter, join)\nmarcel/aws-s3 \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Ruby implementation of Amazon's S3 REST API.\nmardix/flask-cloudy \ud83d\udd25\ud83d\udd25 - Flask extension.\nMathieuLoutre/grunt-aws-s3 \ud83d\udd25\ud83d\udd25 - Grunt plugin.\nmickael-kerjean/filestash \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - A modern web client for S3.\nminio/mc \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Minio Client for filesystem and cloud storage.\nminio/minio \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Object storage server compatible with S3.\nmumrah/s3-multipart \ud83d\udd25 - Parallel upload/download to S3 via Python.\nncw/rclone \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Rsync for various cloud storage providers such as S3.\nowocki/s3_disk_util \ud83d\udd25 - S3 Disk usage (du) utility.\npeak/s5cmd \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Fast S3 and local filesystem execution tool with wildcard and batch command support.\npgherveou/gulp-awspublish \ud83d\udd25\ud83d\udd25 - Gulp plugin.\nrlmcpherson/s3gof3r \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Fast, concurrent, streaming access, includes a CLI.\ns3git/s3git \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - CLI tool that allows you to create a distributed, decentralized and versioned repository.\ns3fs-fuse/s3fs-fuse \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Allows Linux and Mac OS X to mount an S3 bucket via FUSE.\ns3tools/s3cmd \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - CLI for managing S3 and CloudFront.\nschickling/git-s3 \ud83d\udd25\ud83d\udd25 - Deploy your git repo to a bucket.\nsorentwo/carrierwave-aws \ud83d\udd25\ud83d\udd25 - Adapter for CarrierWave.\nspring-projects/aws-maven \ud83d\udd25\ud83d\udd25 - Maven Wagon for S3.\ntongwang/s3fs-c \ud83d\udd25 - Mounts buckets for use on a local file system.\nmishudark/s3-parallel-put \ud83d\udd25\ud83d\udd25 - CLI that supports parallel uploads.\nwaynehoover/s3_direct_upload \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Direct Upload to Amazon S3 With CORS\nweavejester/clj-aws-s3 \ud83d\udd25 - Client library for Clojure.\nSES\nCommunity Repos:\ndrewblas/aws-ses \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Provides an easy ruby DSL & interface.\nmicroapps/MoonMail \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Shoot billions of emails using SES and Lambda.\nSimple Workflow\nAWS Repos:\naws-flow-ruby \ud83d\udd25 - Creates background jobs and multistep workflows.\naws-flow-ruby-samples - AWS Flow Framework for Ruby samples.\naws-flow-ruby-opsworks-helloworld - Hello World sample.\nCommunity Repos:\nContribute\nSimpleDB\nCommunity Repos:\nrjrodger/simpledb \ud83d\udd25 - Node.js library.\nSNS\nAWS Repos:\naws-php-sns-message-validator \ud83d\udd25 - Message validation for PHP.\nCommunity Repos:\nContribute\nSQS\nAWS Repos:\namazon-sqs-java-messaging-lib \ud83d\udd25 - Holds the Java Message Service to communicate with SQS.\nCommunity Repos:\nphstc/shoryuken \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - A super efficient SQS thread based message processor for Ruby.\nData\nAWS Repos:\naws-data-wrangler \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Connects Pandas DataFrames and AWS data related services.\nCommunity Repos:\ndonnemartin/data-science-ipython-notebooks \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Big data/data science notebooks.\neverpeace/vagrant-mesos \ud83d\udd25\ud83d\udd25 - Spin up your Mesos Cluster with Vagrant.\njhorey/ferry \ud83d\udd25\ud83d\udd25 - Define, run, and deploy big data apps using Docker.\nnathanmarz/storm-deploy \ud83d\udd25\ud83d\udd25\ud83d\udd25 - One click deploy for Storm clusters.\nDevOps\nCommunity Repos:\ncloud-custodian/cloud-custodian \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Rules engine for management, DSL in yaml for query, filter, and actions on resources.\nchef-cookbooks/aws \ud83d\udd25\ud83d\udd25 - Development repository for aws Chef cookbook.\ncolinbjohnson/aws-missing-tools \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Tools for managing resources including EC2, EBS, RDS and Route53.\nk1LoW/awspec \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - RSpec tests your resources.\nmitchellh/vagrant-aws \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Use Vagrant to manage your EC2 and VPC instances.\nNixOS/nixops \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Use NixOS to provision EC2 instances, S3 buckets, and other resources.\nSecurity\nAWS Repos:\naws-sha256-agentcs - SHA256 Agent Compatibility Ccanner.\naws-tvm-anonymous - Token Vending Machine for Anonymous Registration.\naws-tvm-identity - Token Vending Machine for Identity Registration.\ns2n \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - An implementation of the TLS/SSL protocols.\nCommunity Repos:\nAdRoll/hologram \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Easy, painless credentials on developer laptops.\nalex/letsencrypt-aws \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Automatically provision and update certificates.\nbridgecrewio/checkov \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Terraform static analysis, verifies security best practices.\ncloudsploit/scans \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Detects security risks.\niSECPartners/Scout2 \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Security auditing tool.\njordanpotti/AWSBucketDump \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Security Tool to Look For Interesting Files in S3 Buckets.\nNetflix/bless \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - SSH Certificate Authority that runs as a Lambda function.\nNetflix/security_monkey \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Monitors policy changes and alerts on insecure configurations.\nRiotGames/cloud-inquisitor \ud83d\udd25\ud83d\udd25 - Tool to enforce ownership and data security.\nsalesforce/policy_sentry \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - IAM Least Privilege Policy Generator.\nsebsto/AWSVPN \ud83d\udd25 - Start a private VPN server in the cloud.\ntrailofbits/algo \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Set up a personal IPSEC VPN on EC2 and other cloud services.\nttlequals0/autovpn \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Create On Demand Disposable OpenVPN Endpoints.\nAccompanying Repos\nAWS Repos:\nRepos Accompanying Blogs, Training Events, and Conferences.\naws-arch-backoff-simulator \ud83d\udd25 - Jitter and backoff Simulator for AWS architecture blog.\naws-big-data-blog \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Samples from the AWS Big Data Blog.\naws-demo-php-simple-app - PHP apps from the AWS Blogs.\naws-mobile-sample-wif - Samples from the AWS Mobile SDK blog.\naws-mobile-self-paced-labs-samples - Android Snake Game from a self-paced lab.\naws-quickstart - Official repository for AWS Quick Start.\naws-spot-labs \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Best practices using AWS Spot Instances.\naws-training-demo \ud83d\udd25 - Demos from the Technical Trainers community.\njava-meme-generator-sample - Meme generation app from re:Invent 2012.\nrailsconf2013-tech-demo \ud83d\udd25 - Seahorse demo from RailsConf 2013.\nreinvent2013-js-blog-demo - Demo blogging app from re:Invent 2013.\nreinvent2013-mobile-photo-share - Mobile photo share app from re:Invent 2014.\nreinvent2014-scalable-site-management - Scalable site management sample from re:Invent 2014.\nreinvent2015-dev309 - Large Scale Metrics Analysis from re:Invent 2015.\ntimely-security-analytics - Security analytics sample from 2015 re:Invent 2015.\ntodo-sample-app - Simple \"Todo\" app from RailsConf 2014.\nCommunity Repos:\nstartup-class/setup \ud83d\udd25\ud83d\udd25 - EC2 setup files for Startup Engineering MOOC.\nMiscellaneous Repos\nAWS Repos:\namediamanager - Media manager.\naws-hal-client-java - Java client for the Hypertext Application Language.\naws-model-validators - Tools for validating the AWS service JSON model files.\naws-sdk-js-sample-video-transcoder - Sample cross-platform video transcoder app.\nsimplebeerservice \ud83d\udd25\ud83d\udd25 - Cloud-connected kegerator that streams live sensor data to AWS.\nCommunity Repos:\nbcoe/thumbd \ud83d\udd25\ud83d\udd25 - Node.js/ImageMagick-based image thumbnailing service.\ncdkpatterns/serverless \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Deployable serverless architecture patterns built in AWS CDK.\nComcast/cmb \ud83d\udd25\ud83d\udd25 - Highly available, horizontally scalable queuing and notification service.\nconvox/rack \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Open-source PaaS on AWS.\ndevops-israel/aws-inventory \ud83d\udd25\ud83d\udd25 - Display all your AWS resources on a single web page.\ndonnemartin/dev-setup \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Mac setup of various developer tools and AWS services.\ndtan4/terraforming \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Export existing resources to Terraform style (tf, tfstate).\nsegmentio/stack \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - A set of Terraform modules for configuring production infrastructure.\nj2labs/microarmy - Deploy micro instances to launch a coordinated siege.\njpillora/grunt-aws \ud83d\udd25 - Grunt interface into the Node.JS SDK.\njvehent/haproxy-aws \ud83d\udd25\ud83d\udd25 - Documentation on building a HTTPS stack with HAProxy.\nlocalstack/localstack \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - A fully functional local AWS cloud stack. Develop and test your cloud apps offline!\nmeducation/propono \ud83d\udd25\ud83d\udd25 - Easy-to-use pub/sub in Ruby.\nmozilla/awsbox \ud83d\udd25\ud83d\udd25\ud83d\udd25 - A featherweight PaaS on top of EC2 for deploying node apps.\nNetflix/aminator \ud83d\udd25\ud83d\udd25\ud83d\udd25 - A tool for creating EBS AMIs.\nNetflix/archaius \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Library for configuration management API.\nNetflix/asgard \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Web interface for application deployments and cloud management.\nNetflix/aws-autoscaling \ud83d\udd25\ud83d\udd25 - Tools for using auto scaling and documentation best practices.\nNetflix/chaosmonkey \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Resiliency tool that helps applications tolerate random instance failures.\nNetflix/eureka \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Service registry for resilient mid-tier load balancing and failover.\nNetflix/EVCache \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - A distributed in-memory data store.\nNetflix/Fenzo \ud83d\udd25\ud83d\udd25\ud83d\udd25 - Extensible Scheduler for Mesos Frameworks.\nNetflix/ice \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Usage and cost monitoring tool.\nNetflix/ribbon \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Remote procedure call library with built in software load balancers.\nNetflix/SimianArmy \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Tools to keep your cloud operating in top form.\nNetflix/zuul \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Edge service that provides dynamic routing, monitoring, resiliency, security, and more.\nniftylettuce/gulp-aws-splash \ud83d\udd25\ud83d\udd25 - Open-source LaunchRock alternative. Build beautiful splash pages.\npuppetlabs/puppetlabs-aws \ud83d\udd25 - Puppet module for managing resources to build out infrastructure.\nmhart/react-server-routing-example \ud83d\udd25\ud83d\udd25 - Sample universal client/server routing and data in React.\nSimilarweb/finala \ud83d\udd25\ud83d\udd25\ud83d\udd25 - A resource cloud scanner that analyzes and reports wasteful and unused resources to cut unwanted expenses.\nsnowplow/snowplow \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Enterprise-strength web, mobile and event analytics, powered by Hadoop, Kafka, Kinesis, Redshift and Elasticsearch.\nSpinnaker/spinnaker \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Successor to asgard supporting pipelines and more.\nspulec/moto \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25 - Allows your python tests to easily mock out the boto library.\nGuides, Books, Documentation, and Training\nHow-to's, training, whitepapers, docs, and case studies.\nGetting Started Guides\nAWS Guides:\nGetting Started with AWS\nGetting Started Tutorials\nRun a Virtual Server\nStore Files\nShare Digital Media\nDeploy a Website\nHost a Website (Linux)\nHost a Website (Windows)\nRun a Database\nAnalyze Your Data\nCommunity Guides:\nContribute\nGeneral Guides\nAWS Guides:\nAnalyzing Big Data\nWorking with the AWS Management Console\nDeploying a Web App Using Elastic Beanstalk\nHosting a Web App\nHosting a .NET Web App\nHosting a Static Website\nQuick Start Deployment Guides\nCommunity Guides:\nOpen Guide to AWS \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\nBooks\nAmazon Web Services in Action Manning or Amazon.com\nAWS Lambda in Action Manning or Amazon.com - Code Repo \ud83d\udd25\ud83d\udd25\nWhitepapers\nAWS Well-Architected Framework\nWhitepapers\nDocumentation\nDocumentation\nAWS Billing and Cost Management\nAWS Marketplace\nAWS Support\nAWS General Reference\nAWS Glossary\nTraining\nTraining and Certification\nWebinars\nCase Studies: Powered by AWS\nAdobe\nAdRoll\nAirbnb\nAutodesk\nCitrix\nComcast\nCoursera\nDocker\nDow Jones\nDropbox\nDropcam\nExpedia\nFoursquare\nIMDb\nInstrumental\nIntuit\nJohnson & Johnson\nLionsgate\nmlbam\nNASA\nNetflix\nNike\nNokia\nPBS\nPfizer\nPhilips\nReddit\nSamsung\nSiemens\nSlack\nSpotify\nSwiftkey\nThe Weather Company\nTicketmaster\nTime Inc\nTwilio\nU.S. Department of State\nUbisoft\nYelp\nZillow\nSocial\nBlogs, discussion groups, conferences, and social media.\nBlogs\nAWS Blogs:\nOfficial Blog\nBrasil\nChina\nGermany\nJapan\nKorea\nDevOps\nArchitecture\nBig Data\nCompute\nMobile\nMessaging\nJava\nPHP\nRuby\n.NET\nSecurity\nStartup\nPartner Network\nSAP\nCommunity Blogs:\nAll Things Distributed - Werner Vogels, AWS CTO.\nThings I Like... - Jeff Barr, AWS Chief Evangelist.\nNetflix Tech Blog\nA Curated List of Engineering Blogs\nAWS Geek\nTwitter Influencers\nAWS Tweeps:\n@awscloud - Official Twitter feed.\n@AWS_Partners\n@AWSIdentity\n@AWSMarketplace\n@AWSreInvent - Official Twitter account for re:Invent.\n@AWSStartups\n@ajassy - Andy Jassy: Senior Vice-President.\n@Ianmmmm - Ian Massingham - Technical Evangelist.\n@jeffbarr - Jeff Barr: Chief Evangelist.\n@mndoci - Deepak Singh: GM EC2.\n@mza - Matt Wood: Product Strategy.\n@Werner - Werner Vogels: CTO.\nCommunity heroes, Evangelists, etc\nCommunity Tweeps:\n@kennwhite\n@esh\n@garnaat\n@quinnypig\n@awsgeek\nFacebook Pages\nAWS Pages:\namazonwebservices - Official Facebook page.\nawsreinvent - Official Facebook page for re:Invent.\nCommunity Pages:\nContribute\nYouTube Channels\nAWS Channels:\nAmazonWebServices\nAWSDeutsch\nAWSJapan\nAWSKorea\nAWSLatinAmerica\nAWSTutorialSeries\nAWSWebinars\nCommunity Channels:\nBackspace Academy\nCloud Academy\nLinux Academy\nLinkedIn Groups\nAWS Page:\nAmazon Web Services\nCommunity Groups:\nAmazon AWS Architects\nAmazon AWS Architects, Engineers, Developers, Consultants, Entrepreneurs Experts\nAmazon Web Services (AWS) for Business\nAmazon Web Services Architects\nAmazon Web Services Community Network\nAmazon Web Services Enthusiasts\nAmazon Web Services Users\nSubreddits\n/r/aws/\n/r/AWS_cloud/\nConferences\nAWS Conferences:\nre:Invent - Annual user conference. The event features keynote announcements, training and certification opportunities, over 250 technical sessions, a partner expo, after hours activities, and more.\nSummits - Global one-day events that are designed to educate new customers about the AWS platform and offer existing customers deep technical content to be more successful with AWS.\nAWSome Day - Global one-day events are delivered by AWS Education's technical instructors and are ideal for IT pros, developers and technical managers who would like to learn about how to get started in the AWS Cloud.\nCommunity Conferences:\nContribute\nLatest KPIs and Stats\nLatest key performance indicators and other interesting stats.\nOver 1 million customers active in past 30 days.1\n$7B+ annual revenue run-rate business.1\n81% year over year revenue growth.1\nEC2 usage up 95% year over year.1\nS3 data transfer up 120% year over year.1\nS3 holds trillions of objects and regularly peaks at 1.5 million requests per second.2\nDatabase services usage up 127% year over year.1\n$1B annual revenue run-rate business.1\n2 million new EBS volumes created per day.4\nCustomers have launched more than 15 million Hadoop clusters.3\n102Tbps network capacity into a data center.2\n500+ major new features and services launched since 2014.1\nAll 14 other cloud providers combined have 1/5th the aggregate capacity of AWS.2\nEvery day, AWS adds enough new server capacity to support all of Amazon's global infrastructure when it was a $7B annual revenue enterprise (in 2004).2\nAppendix of Core Services\nAppendix of official services, grouped by service category.\nServices in Plain English\nAmazon Web Services in Plain English - Entertaining and educational, a community contribution.\nCompute Services\nAuto Scaling - Launches or terminates EC2 instances based on policies, schedules, and health checks.\nBatch - Run batch jobs at scale.\nBlox - Open source projects for building custom schedulers on ECS.\nEC2 Container Service (ECS) - Supports Docker containers on EC2 instances.\nEC2 Systems Manager - Easily configure and manage EC2 and on-premises systems.\nElastic Beanstalk - Provides quick deployment and management of applications in the cloud.\nElastic Compute Cloud (EC2) - Provides scalable virtual private servers using Xen.\nElastic GPUs - Attach low-cost GPUs to EC2 instances for graphics acceleration.\nElastic Load Balancing (ELB) - Automatically distributes incoming traffic across multiple EC2 instances.\nLambda - Runs code in response to events and automatically manages EC2 instances.\nLightsail - Launch and manage simple virtual private servers.\nVirtual Private Cloud (VPC) - Creates a logically isolated set of EC2 instances which can be connected to an existing network using a VPN connection.\nNetworking Services\nDirect Connect - Provides dedicated connections to AWS for faster and cheaper data throughput.\nElastic Load Balancing (ELB) - Automatically distributes incoming traffic across multiple EC2 instances.\nRoute 53 - Provides a highly available and scalable Domain Name System (DNS) web service.\nVirtual Private Cloud (VPC) - Creates a logically isolated set of EC2 instances which can be connected to an existing network using a VPN connection.\nEnterprise Applications\nWorkDocs - Provides a fully managed, secure enterprise storage and sharing service.\nWorkMail - Provides managed email and calendaring service.\nWorkSpaces - Provides a cloud-based desktop experience to end-users.\nWorkspaces Application Manager (WAM) - Simplifies deployment and management of WorkSpaces.\nAnalytics Services\nAthena - Query data on S3 instantly.\nData Pipeline - Provides workload management by processing and moving data between services.\nElastic MapReduce (EMR) - Hosts a Hadoop and Spark framework running on EC2 and S3.\nElasticsearch Service (ES) - Managed Elasticsearch, a popular open-source search and analytics engine.\nGlue - Prepare and load data to data stores.\nKinesis - Provides real-time data processing over large, distributed data streams.\nKinesis Analytics - Write standard SQL queries on streaming data without having to learn any new programming skills.\nKinesis Firehose - Captures and automatically loads streaming data into S3 and Redshift.\nQuicksight - Provides cloud-powered business intelligence for 1/10th the cost of traditional BI solutions.\nRedshift - Provides petabyte-scale data warehousing with columnar storage and multi-node compute.\nArtificial Intelligence\nLex - Build conversational interfaces through voice or text.\nMachine Learning - Provides managed machine learning technology.\nPolly - Turn text into lifelike speech.\nRekognition - Deep learning-based image analysis.\nManagement Tools\nCloudFormation - Provides a file-based interface for provisioning other resources.\nCloudTrail - Provides logs of all activity.\nCloudWatch - Provides monitoring for AWS cloud resources and applications, starting with EC2.\nCommand Line Interface (CLI) - Provides a CLI to manage all services.\nConfig - Provides a detailed view of all resources.\nManagement Console (AWS Console) - A web-based interface to manage all services.\nOpsWorks - Provides configuration of EC2 services using Chef.\nPersonal Health Dashboard - Your personalized view of service health.\nService Catalog - Service Catalog allows IT administrators to create, manage, and distribute portfolios of approved products to end users, who can then access the products they need in a personalized portal.\nSecurity and Identity Services\nCertificate Manager - Lets you easily provision, manage, and deploy SSL/TLS certificates for use with AWS services.\nCloudHSM - Helps meet corporate, contractual and regulatory compliance requirements for data security by using dedicated Hardware Security Module (HSM) appliances within the AWS cloud.\nDirectory Service - A managed service that allows you to connect your resources with an existing on-premises Microsoft Active Directory or to set up a new, stand-alone directory in the AWS Cloud.\nIdentity and Access Management (IAM) - An implicit service, the authentication infrastructure used to authenticate access to the various services.\nInspector - An automated security assessment service that helps improve the security and compliance of applications deployed on AWS.\nKey Management Service (KMS) - A managed service that makes it easy for you to create and control the encryption keys used to encrypt your data.\nShield - Managed DDoS Protection.\nWAF - A web application firewall service that monitors and manages CloudFront distributions.\nInternet of Things Service\nIoT - Enables secure, bi-directional communication between internet-connected things (such as sensors, actuators, embedded devices, or smart appliances) and the AWS cloud over MQTT and HTTP.\nMobile Services\nAPI Gateway - Service for publishing, maintaining and securing web service APIs.\nCognito - Provides user identity and data synchronization.\nDevice Farm - App testing service for iOS, Android and Fire OS apps on physical devices.\nMobile Analytics - Service for collecting, visualizing, and understanding app usage data.\nMobile Hub - Provides an integrated console that helps you build, test, and monitor your mobile apps.\nPinpoint - Targeted push notifications for mobile apps.\nSimple Notification Service (SNS) - Provides a hosted multi-protocol \"push\" messaging for applications.\nStorage and Content Delivery Services\nCloudFront - A content delivery network (CDN) for distributing objects to locations near the requester.\nElastic Block Store (EBS) - Provides persistent block-level storage volumes for EC2.\nElastic File System (EFS) - A file storage service for EC2 instances.\nGlacier - Provides a low-cost, long-term storage option, intended for archiving data.\nImport/Export - Accelerates moving large amounts of data into and out of AWS using portable storage devices for transport.\nSimple Storage Service (S3) - Provides Web Service based storage.\nStorage Gateway - An iSCSI block storage virtual appliance with cloud-based backup.\nDatabases\nAurora - MySQL and PostgreSQL compatible relational database with improved performance.\nDynamoDB - Provides a scalable, low-latency NoSQL online Database Service backed by SSDs.\nElastiCache - Provides in-memory caching for web apps (Memcached, Redis).\nRedshift - Provides petabyte-scale data warehousing with columnar storage and multi-node compute.\nRelational Database Service (RDS) - Provides a scalable database server with MySQL, Oracle, SQL Server, PostgreSQL, and MariaDB support.\nSchema Conversion Tool - App that helps you convert your database schema from an Oracle or Microsoft SQL Server database, to an RDS MySQL DB instance or an Aurora DB cluster.\nSimpleDB - Allows developers to run queries on structured data.\nApplication Services\nAPI Gateway - Service for publishing, maintaining and securing web service APIs.\nAppStream - Flexible, low-latency streaming service for apps and games.\nCloudSearch - Provides basic full-text search and indexing of textual content.\nDevPay - Provides billing and account management.\nElastic Transcoder (ETS) - Provides video transcoding of S3 hosted videos.\nFlexible Payments Service (FPS) - Provides an interface for micropayments.\nSimple Email Service (SES) - Provides bulk and transactional email sending.\nSimple Notification Service (SNS) - Provides a hosted multi-protocol \"push\" messaging for applications.\nSimple Queue Service (SQS) - Provides a hosted message queue for web applications.\nSimple Workflow (SWF) - A workflow service for building scalable, resilient applications.\nStep Functions - Coordinate components of distributed applications.\nDeveloper Tools\nCodeBuild - Build and test code.\nCodeCommit - Hosted Git version control service.\nCodeDeploy - Provides automated code deployment to EC2 instances.\nCodePipeline - Continuous delivery service.\nCommand Line Interface (CLI) - Provides a CLI to manage all services.\nX-Ray - Analyze and debug your applications.\nMiscellaneous Services\nFulfillment Web Service - Provides a programmatic web service for sellers to ship items to and from Amazon using Fulfillment by Amazon.\nMechanical Turk - Manages small units of work distributed among many persons.\nPartner Network (APN) - Provides partners with the technical information and sales and marketing support to increase business opportunities.\nProduct Advertising API - Provides access to product data and electronic commerce functionality.\nCredits\nCheck out the Credits page.\nOther Awesome Lists\nOther awesome lists can be found in awesome and awesome-awesomeness.\nContact Info\nFeel free to contact me to discuss any issues, questions, or comments.\nMy contact info can be found on my GitHub page.\nLicense\nI am providing code and resources in this repository to you under an open source license. Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).\nCopyright 2017 Donne Martin\nCreative Commons Attribution 4.0 International License (CC BY 4.0)\nhttp://creativecommons.org/licenses/by/4.0/",
      "link": "https://github.com/donnemartin/awesome-aws"
    },
    {
      "autor": "3D-Machine-Learning",
      "date": "NaN",
      "content": "3D Machine Learning\nIn recent years, tremendous amount of progress is being made in the field of 3D Machine Learning, which is an interdisciplinary field that fuses computer vision, computer graphics and machine learning. This repo is derived from my study notes and will be used as a place for triaging new research papers.\nI'll use the following icons to differentiate 3D representations:\n\ud83d\udcf7 Multi-view Images\n\ud83d\udc7e Volumetric\n\ud83c\udfb2 Point Cloud\n\ud83d\udc8e Polygonal Mesh\n\ud83d\udc8a Primitive-based\nTo find related papers and their relationships, check out Connected Papers, which provides a neat way to visualize the academic field in a graph representation.\nGet Involved\nTo contribute to this Repo, you may add content through pull requests or open an issue to let me know.\n\u2b50 \u2b50 \u2b50 \u2b50 \u2b50 \u2b50 \u2b50 \u2b50 \u2b50 \u2b50 \u2b50 \u2b50\nWe have also created a Slack workplace for people around the globe to ask questions, share knowledge and facilitate collaborations. Together, I'm sure we can advance this field as a collaborative effort. Join the community with this link.\n\u2b50 \u2b50 \u2b50 \u2b50 \u2b50 \u2b50 \u2b50 \u2b50 \u2b50 \u2b50 \u2b50 \u2b50\nTable of Contents\nCourses\nDatasets\n3D Models\n3D Scenes\n3D Pose Estimation\nSingle Object Classification\nMultiple Objects Detection\nScene/Object Semantic Segmentation\n3D Geometry Synthesis/Reconstruction\nParametric Morphable Model-based methods\nPart-based Template Learning methods\nDeep Learning Methods\nTexture/Material Analysis and Synthesis\nStyle Learning and Transfer\nScene Synthesis/Reconstruction\nScene Understanding\nAvailable Courses\nStanford CS231A: Computer Vision-From 3D Reconstruction to Recognition (Winter 2018)\nUCSD CSE291-I00: Machine Learning for 3D Data (Winter 2018)\nStanford CS468: Machine Learning for 3D Data (Spring 2017)\nMIT 6.838: Shape Analysis (Spring 2017)\nPrinceton COS 526: Advanced Computer Graphics (Fall 2010)\nPrinceton CS597: Geometric Modeling and Analysis (Fall 2003)\nGeometric Deep Learning\nPaper Collection for 3D Understanding\nCreativeAI: Deep Learning for Graphics\nDatasets\nTo see a survey of RGBD datasets, check out Michael Firman's collection as well as the associated paper, RGBD Datasets: Past, Present and Future. Point Cloud Library also has a good dataset catalogue.\n3D Models\nPrinceton Shape Benchmark (2003) [Link]\n1,814 models collected from the web in .OFF format. Used to evaluating shape-based retrieval and analysis algorithms.\nDataset for IKEA 3D models and aligned images (2013) [Link]\n759 images and 219 models including Sketchup (skp) and Wavefront (obj) files, good for pose estimation.\nOpen Surfaces: A Richly Annotated Catalog of Surface Appearance (SIGGRAPH 2013) [Link]\nOpenSurfaces is a large database of annotated surfaces created from real-world consumer photographs. Our annotation framework draws on crowdsourcing to segment surfaces from photos, and then annotate them with rich surface properties, including material, texture and contextual information.\nPASCAL3D+ (2014) [Link]\n12 categories, on average 3k+ objects per category, for 3D object detection and pose estimation.\nModelNet (2015) [Link]\n127915 3D CAD models from 662 categories\nModelNet10: 4899 models from 10 categories\nModelNet40: 12311 models from 40 categories, all are uniformly orientated\nShapeNet (2015) [Link]\n3Million+ models and 4K+ categories. A dataset that is large in scale, well organized and richly annotated.\nShapeNetCore [Link]: 51300 models for 55 categories.\nA Large Dataset of Object Scans (2016) [Link]\n10K scans in RGBD + reconstructed 3D models in .PLY format.\nObjectNet3D: A Large Scale Database for 3D Object Recognition (2016) [Link]\n100 categories, 90,127 images, 201,888 objects in these images and 44,147 3D shapes.\nTasks: region proposal generation, 2D object detection, joint 2D detection and 3D object pose estimation, and image-based 3D shape retrieval\nThingi10K: A Dataset of 10,000 3D-Printing Models (2016) [Link]\n10,000 models from featured \u201cthings\u201d on thingiverse.com, suitable for testing 3D printing techniques such as structural analysis , shape optimization, or solid geometry operations.\nABC: A Big CAD Model Dataset For Geometric Deep Learning [Link][Paper]\nThis work introduce a dataset for geometric deep learning consisting of over 1 million individual (and high quality) geometric models, each associated with accurate ground truth information on the decomposition into patches, explicit sharp feature annotations, and analytic differential properties.\n\ud83c\udfb2 ScanObjectNN: A New Benchmark Dataset and Classification Model on Real-World Data (ICCV 2019) [Link]\nThis work introduce ScanObjectNN, a new real-world point cloud object dataset based on scanned indoor scene data. The comprehensive benchmark in this work shows that this dataset poses great challenges to existing point cloud classification techniques as objects from real-world scans are often cluttered with background and/or are partial due to occlusions. Three key open problems for point cloud object classification are identified, and a new point cloud classification neural network that achieves state-of-the-art performance on classifying objects with cluttered background is proposed.\nVOCASET: Speech-4D Head Scan Dataset (2019( [Link][Paper]\nVOCASET, is a 4D face dataset with about 29 minutes of 4D scans captured at 60 fps and synchronized audio. The dataset has 12 subjects and 480 sequences of about 3-4 seconds each with sentences chosen from an array of standard protocols that maximize phonetic diversity.\n3D-FUTURE: 3D FUrniture shape with TextURE (2020( [Link]\nVOCASET, contains 20,000+ clean and realistic synthetic scenes in 5,000+ diverse rooms, which include 10,000+ unique high quality 3D instances of furniture with high resolution informative textures developed by professional designers.\nFusion 360 Gallery Dataset (2020) [Link][Paper]\nThe Fusion 360 Gallery Dataset contains rich 2D and 3D geometry data derived from parametric CAD models. The Reconstruction Dataset provides sequential construction sequence information from a subset of simple 'sketch and extrude' designs. The Segmentation Dataset provides a segmentation of 3D models based on the CAD modeling operation, including B-Rep format, mesh, and point cloud.\nMechanical Components Benchmark (2020)[Link][Paper]\nMCB is a large-scale dataset of 3D objects of mechanical components. It has a total number of 58,696 mechanical components with 68 classes.\nCombinatorial 3D Shape Dataset (2020) [Link][Paper]\nCombinatorial 3D Shape Dataset is composed of 406 instances of 14 classes. Each object in our dataset is considered equivalent to a sequence of primitive placement. Compared to other 3D object datasets, our proposed dataset contains an assembling sequence of unit primitives. It implies that we can quickly obtain a sequential generation process that is a human assembling mechanism. Furthermore, we can sample valid random sequences from a given combinatorial shape after validating the sampled sequences. To sum up, the characteristics of our combinatorial 3D shape dataset are (i) combinatorial, (ii) sequential, (iii) decomposable, and (iv) manipulable.\n3D Scenes\nNYU Depth Dataset V2 (2012) [Link]\n1449 densely labeled pairs of aligned RGB and depth images from Kinect video sequences for a variety of indoor scenes.\nSUNRGB-D 3D Object Detection Challenge [Link]\n19 object categories for predicting a 3D bounding box in real world dimension\nTraining set: 10,355 RGB-D scene images, Testing set: 2860 RGB-D images\nSceneNN (2016) [Link]\n100+ indoor scene meshes with per-vertex and per-pixel annotation.\nScanNet (2017) [Link]\nAn RGB-D video dataset containing 2.5 million views in more than 1500 scans, annotated with 3D -----> camera !!!  poses, surface reconstructions, and instance-level semantic segmentations.\nMatterport3D: Learning from RGB-D Data in Indoor Environments (2017) [Link]\n10,800 panoramic views (in both RGB and depth) from 194,400 RGB-D images of 90 building-scale scenes of private rooms. Instance-level semantic segmentations are provided for region (living room, kitchen) and object (sofa, TV) categories.\nSUNCG: A Large 3D Model Repository for Indoor Scenes (2017) [Link]\nThe dataset contains over 45K different scenes with manually created realistic room and furniture layouts. All of the scenes are semantically annotated at the object level.\nMINOS: Multimodal Indoor Simulator (2017) [Link]\nMINOS is a simulator designed to support the development of multisensory models for goal-directed navigation in complex indoor environments. MINOS leverages large datasets of complex 3D environments and supports flexible configuration of multimodal sensor suites. MINOS supports SUNCG and Matterport3D scenes.\nFacebook House3D: A Rich and Realistic 3D Environment (2017) [Link]\nHouse3D is a virtual 3D environment which consists of 45K indoor scenes equipped with a diverse set of scene types, layouts and objects sourced from the SUNCG dataset. All 3D objects are fully annotated with category labels. Agents in the environment have access to observations of multiple modalities, including RGB images, depth, segmentation masks and top-down 2D map views.\nHoME: a Household Multimodal Environment (2017) [Link]\nHoME integrates over 45,000 diverse 3D house layouts based on the SUNCG dataset, a scale which may facilitate learning, generalization, and transfer. HoME is an open-source, OpenAI Gym-compatible platform extensible to tasks in reinforcement learning, language grounding, sound-based navigation, robotics, multi-agent learning.\nAI2-THOR: Photorealistic Interactive Environments for AI Agents [Link]\nAI2-THOR is a photo-realistic interactable framework for AI agents. There are a total 120 scenes in version 1.0 of the THOR environment covering four different room categories: kitchens, living rooms, bedrooms, and bathrooms. Each room has a number of actionable objects.\nUnrealCV: Virtual Worlds for Computer Vision (2017) [Link][Paper]\nAn open source project to help computer vision researchers build virtual worlds using Unreal Engine 4.\nGibson Environment: Real-World Perception for Embodied Agents (2018 CVPR) [Link]\nThis platform provides RGB from 1000 point clouds, as well as multimodal sensor data: surface normal, depth, and for a fraction of the spaces, semantics object annotations. The environment is also RL ready with physics integrated. Using such datasets can further narrow down the discrepency between virtual environment and real world.\nInteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes Dataset [Link]\nSystem Overview: an end-to-end pipeline to render an RGB-D-inertial benchmark for large scale interior scene understanding and mapping. Our dataset contains 20M images created by pipeline: (A) We collect around 1 million CAD models provided by world-leading furniture manufacturers. These models have been used in the real-world production. (B) Based on those models, around 1,100 professional designers create around 22 million interior layouts. Most of such layouts have been used in real-world decorations. (C) For each layout, we generate a number of configurations to represent different random lightings and simulation of scene change over time in daily life. (D) We provide an interactive simulator (ViSim) to help for creating ground truth IMU, events, as well as monocular or stereo camera trajectories including hand-drawn, random walking and neural network based realistic trajectory. (E) All supported image sequences and ground truth.\nSemantic3D[Link]\nLarge-Scale Point Cloud Classification Benchmark, which provides a large labelled 3D point cloud data set of natural scenes with over 4 billion points in total, and also covers a range of diverse urban scenes.\nStructured3D: A Large Photo-realistic Dataset for Structured 3D Modeling [Link]\n3D-FRONT: 3D Furnished Rooms with layOuts and semaNTics [Link]\nContains 10,000 houses (or apartments) and ~70,000 rooms with layout information.\n3ThreeDWorld(TDW): A High-Fidelity, Multi-Modal Platform for Interactive Physical Simulation [Link]\nMINERVAS: Massive INterior EnviRonments VirtuAl Synthesis [Link]\n3D Pose Estimation\nCategory-Specific Object Reconstruction from a Single Image (2014) [Paper]\nViewpoints and Keypoints (2015) [Paper]\nRender for CNN: Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views (2015 ICCV) [Paper]\nPoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization (2015) [Paper]\nModeling Uncertainty in Deep Learning for Camera Relocalization (2016) [Paper]\nRobust camera pose estimation by viewpoint classification using deep learning (2016) [Paper]\nImage-based localization using lstms for structured feature correlation (2017 ICCV) [Paper]\nImage-Based Localization Using Hourglass Networks (2017 ICCV Workshops) [Paper]\nGeometric loss functions for camera pose regression with deep learning (2017 CVPR) [Paper]\nGeneric 3D Representation via Pose Estimation and Matching (2017) [Paper]\n3D Bounding Box Estimation Using Deep Learning and Geometry (2017) [Paper]\n6-DoF Object Pose from Semantic Keypoints (2017) [Paper]\nRelative Camera Pose Estimation Using Convolutional Neural Networks (2017) [Paper]\n3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions (2017) [Paper]\nSingle Image 3D Interpreter Network (2016) [Paper] [Code]\nMulti-view Consistency as Supervisory Signal for Learning Shape and Pose Prediction (2018 CVPR) [Paper]\nPoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes (2018) [Paper]\nFeature Mapping for Learning Fast and Accurate 3D Pose Inference from Synthetic Images (2018 CVPR) [Paper]\nPix3D: Dataset and Methods for Single-Image 3D Shape Modeling (2018 CVPR) [Paper]\n3D Pose Estimation and 3D Model Retrieval for Objects in the Wild (2018 CVPR) [Paper]\nDeep Object Pose Estimation for Semantic Robotic Grasping of Household Objects (2018) [Paper]\nMocapNET2: a real-time method that estimates the 3D human pose directly in the popular Bio Vision Hierarchy (BVH) format (2021) [Paper], [Code]\nSingle Object Classification\n\ud83d\udc7e 3D ShapeNets: A Deep Representation for Volumetric Shapes (2015) [Paper]\n\ud83d\udc7e VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition (2015) [Paper] [Code]\n\ud83d\udcf7 Multi-view Convolutional Neural Networks for 3D Shape Recognition (2015) [Paper]\n\ud83d\udcf7 DeepPano: Deep Panoramic Representation for 3-D Shape Recognition (2015) [Paper]\n\ud83d\udc7e\ud83d\udcf7 FusionNet: 3D Object Classification Using Multiple Data Representations (2016) [Paper]\n\ud83d\udc7e\ud83d\udcf7 Volumetric and Multi-View CNNs for Object Classification on 3D Data (2016) [Paper] [Code]\n\ud83d\udc7e Generative and Discriminative Voxel Modeling with Convolutional Neural Networks (2016) [Paper] [Code]\n\ud83d\udc8e Geometric deep learning on graphs and manifolds using mixture model CNNs (2016) [Link]\n\ud83d\udc7e 3D GAN: Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling (2016) [Paper] [Code]\n\ud83d\udc7e Generative and Discriminative Voxel Modeling with Convolutional Neural Networks (2017) [Paper]\n\ud83d\udc7e FPNN: Field Probing Neural Networks for 3D Data (2016) [Paper] [Code]\n\ud83d\udc7e OctNet: Learning Deep 3D Representations at High Resolutions (2017) [Paper] [Code]\n\ud83d\udc7e O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis (2017) [Paper] [Code]\n\ud83d\udc7e Orientation-boosted voxel nets for 3D object recognition (2017) [Paper] [Code]\n\ud83c\udfb2 PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation (2017) [Paper] [Code]\n\ud83c\udfb2 PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space (2017) [Paper] [Code]\n\ud83d\udcf7 Feedback Networks (2017) [Paper] [Code]\n\ud83c\udfb2 Escape from Cells: Deep Kd-Networks for The Recognition of 3D Point Cloud Models (2017) [Paper]\n\ud83c\udfb2 Dynamic Graph CNN for Learning on Point Clouds (2018) [Paper]\n\ud83c\udfb2 PointCNN (2018) [Paper]\n\ud83c\udfb2\ud83d\udcf7 A Network Architecture for Point Cloud Classification via Automatic Depth Images Generation (2018 CVPR) [Paper]\n\ud83c\udfb2\ud83d\udc7e PointGrid: A Deep Network for 3D Shape Understanding (CVPR 2018) [Paper] [Code]\n\ud83d\udc8e MeshNet: Mesh Neural Network for 3D Shape Representation (AAAI 2019) [Paper] [Code]\n\ud83c\udfb2 SpiderCNN (2018) [Paper][Code]\n\ud83c\udfb2 PointConv (2018) [Paper][Code]\n\ud83d\udc8e MeshCNN (SIGGRAPH 2019) [Paper][Code]\n\ud83c\udfb2 SampleNet: Differentiable Point Cloud Sampling (CVPR 2020) [Paper] [Code]\nMultiple Objects Detection\nSliding Shapes for 3D Object Detection in Depth Images (2014) [Paper]\nObject Detection in 3D Scenes Using CNNs in Multi-view Images (2016) [Paper]\nDeep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images (2016) [Paper] [Code]\nThree-Dimensional Object Detection and Layout Prediction using Clouds of Oriented Gradients (2016) [CVPR '16 Paper] [CVPR '18 Paper] [T-PAMI '19 Paper]\nDeepContext: Context-Encoding Neural Pathways for 3D Holistic Scene Understanding (2016) [Paper]\nSUN RGB-D: A RGB-D Scene Understanding Benchmark Suite (2017) [Paper]\nVoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection (2017) [Paper]\nFrustum PointNets for 3D Object Detection from RGB-D Data (CVPR2018) [Paper]\nA^2-Net: Molecular Structure Estimation from Cryo-EM Density Volumes (AAAI2019) [Paper]\nStereo R-CNN based 3D Object Detection for Autonomous Driving (CVPR2019) [Paper]\nDeep Hough Voting for 3D Object Detection in Point Clouds (ICCV2019) [Paper] [code]\nScene/Object Semantic Segmentation\nLearning 3D Mesh Segmentation and Labeling (2010) [Paper]\nUnsupervised Co-Segmentation of a Set of Shapes via Descriptor-Space Spectral Clustering (2011) [Paper]\nSingle-View Reconstruction via Joint Analysis of Image and Shape Collections (2015) [Paper] [Code]\n3D Shape Segmentation with Projective Convolutional Networks (2017) [Paper] [Code]\nLearning Hierarchical Shape Segmentation and Labeling from Online Repositories (2017) [Paper]\n\ud83d\udc7e ScanNet (2017) [Paper] [Code]\n\ud83c\udfb2 PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation (2017) [Paper] [Code]\n\ud83c\udfb2 PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space (2017) [Paper] [Code]\n\ud83c\udfb2 3D Graph Neural Networks for RGBD Semantic Segmentation (2017) [Paper]\n\ud83c\udfb2 3DCNN-DQN-RNN: A Deep Reinforcement Learning Framework for Semantic Parsing of Large-scale 3D Point Clouds (2017) [Paper]\n\ud83c\udfb2\ud83d\udc7e Semantic Segmentation of Indoor Point Clouds using Convolutional Neural Networks (2017) [Paper]\n\ud83c\udfb2\ud83d\udc7e SEGCloud: Semantic Segmentation of 3D Point Clouds (2017) [Paper]\n\ud83c\udfb2\ud83d\udc7e Large-Scale 3D Shape Reconstruction and Segmentation from ShapeNet Core55 (2017) [Paper]\n\ud83c\udfb2 Pointwise Convolutional Neural Networks (CVPR 2018) [Link]\nWe propose pointwise convolution that performs on-the-fly voxelization for learning local features of a point cloud.\n\ud83c\udfb2 Dynamic Graph CNN for Learning on Point Clouds (2018) [Paper]\n\ud83c\udfb2 PointCNN (2018) [Paper]\n\ud83d\udcf7\ud83d\udc7e 3DMV: Joint 3D-Multi-View Prediction for 3D Semantic Scene Segmentation (2018) [Paper]\n\ud83d\udc7e ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans (2018) [Paper]\n\ud83c\udfb2\ud83d\udcf7 SPLATNet: Sparse Lattice Networks for Point Cloud Processing (2018) [Paper]\n\ud83c\udfb2\ud83d\udc7e PointGrid: A Deep Network for 3D Shape Understanding (CVPR 2018) [Paper] [Code]\n\ud83c\udfb2 PointConv (2018) [Paper][Code]\n\ud83c\udfb2 SpiderCNN (2018) [Paper][Code]\n\ud83d\udc7e 3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans (CVPR 2019) [Paper][Code]\n\ud83c\udfb2 Real-time Progressive 3D Semantic Segmentation for Indoor Scenes (WACV 2019) [Link]\nWe propose an efficient yet robust technique for on-the-fly dense reconstruction and semantic segmentation of 3D indoor scenes. Our method is built atop an efficient super-voxel clustering method and a conditional random field with higher-order constraints from structural and object cues, enabling progressive dense semantic segmentation without any precomputation.\n\ud83c\udfb2 JSIS3D: Joint Semantic-Instance Segmentation of 3D Point Clouds (CVPR 2019) [Link]\nWe jointly address the problems of semantic and instance segmentation of 3D point clouds with a multi-task pointwise network that simultaneously performs two tasks: predicting the semantic classes of 3D points and embedding the points into high-dimensional vectors so that points of the same object instance are represented by similar embeddings. We then propose a multi-value conditional random field model to incorporate the semantic and instance labels and formulate the problem of semantic and instance segmentation as jointly optimising labels in the field model.\n\ud83c\udfb2 ShellNet: Efficient Point Cloud Convolutional Neural Networks using Concentric Shells Statistics (ICCV 2019) [Link]\nWe propose an efficient end-to-end permutation invariant convolution for point cloud deep learning. We use statistics from concentric spherical shells to define representative features and resolve the point order ambiguity, allowing traditional convolution to perform efficiently on such features.\n\ud83c\udfb2 Rotation Invariant Convolutions for 3D Point Clouds Deep Learning (3DV 2019) [Link]\nWe introduce a novel convolution operator for point clouds that achieves rotation invariance. Our core idea is to use low-level rotation invariant geometric features such as distances and angles to design a convolution operator for point cloud learning.\n3D Model Synthesis/Reconstruction\nParametric Morphable Model-based methods\nA Morphable Model For The Synthesis Of 3D Faces (1999) [Paper][Code]\nFLAME: Faces Learned with an Articulated Model and Expressions (2017) [Paper][Code (Chumpy)][Code (TF)] [Code (PyTorch)]\nFLAME is a lightweight and expressive generic head model learned from over 33,000 of accurately aligned 3D scans. The model combines a linear identity shape space (trained from 3800 scans of human heads) with an articulated neck, jaw, and eyeballs, pose-dependent corrective blendshapes, and additional global expression blendshapes. The code demonstrates how to 1) reconstruct textured 3D faces from images, 2) fit the model to 3D landmarks or registered 3D meshes, or 3) generate 3D face templates for speech-driven facial animation.\nThe Space of Human Body Shapes: Reconstruction and Parameterization from Range Scans (2003) [Paper]\nSMPL-X: Expressive Body Capture: 3D Hands, Face, and Body from a Single Image (2019) [Paper][Video][Code]\nPIFuHD: Multi-Level Pixel Aligned Implicit Function for High-Resolution 3D Human Digitization (CVPR 2020) [Paper][Video][Code]\nExPose: Monocular Expressive Body Regression through Body-Driven Attention (2020) [Paper][Video][Code]\nCategory-Specific Object Reconstruction from a Single Image (2014) [Paper]\n\ud83c\udfb2 DeformNet: Free-Form Deformation Network for 3D Shape Reconstruction from a Single Image (2017) [Paper]\n\ud83d\udc8e Mesh-based Autoencoders for Localized Deformation Component Analysis (2017) [Paper]\n\ud83d\udc8e Exploring Generative 3D Shapes Using Autoencoder Networks (Autodesk 2017) [Paper]\n\ud83d\udc8e Using Locally Corresponding CAD Models for Dense 3D Reconstructions from a Single Image (2017) [Paper]\n\ud83d\udc8e Compact Model Representation for 3D Reconstruction (2017) [Paper]\n\ud83d\udc8e Image2Mesh: A Learning Framework for Single Image 3D Reconstruction (2017) [Paper]\n\ud83d\udc8e Learning free-form deformations for 3D object reconstruction (2018) [Paper]\n\ud83d\udc8e Variational Autoencoders for Deforming 3D Mesh Models(2018 CVPR) [Paper]\n\ud83d\udc8e Lions and Tigers and Bears: Capturing Non-Rigid, 3D, Articulated Shape from Images (2018 CVPR) [Paper]\nPart-based Template Learning methods\nModeling by Example (2004) [Paper]\nModel Composition from Interchangeable Components (2007) [Paper]\nData-Driven Suggestions for Creativity Support in 3D Modeling (2010) [Paper]\nPhoto-Inspired Model-Driven 3D Object Modeling (2011) [Paper]\nProbabilistic Reasoning for Assembly-Based 3D Modeling (2011) [Paper]\nA Probabilistic Model for Component-Based Shape Synthesis (2012) [Paper]\nStructure Recovery by Part Assembly (2012) [Paper]\nFit and Diverse: Set Evolution for Inspiring 3D Shape Galleries (2012) [Paper]\nAttribIt: Content Creation with Semantic Attributes (2013) [Paper]\nLearning Part-based Templates from Large Collections of 3D Shapes (2013) [Paper]\nTopology-Varying 3D Shape Creation via Structural Blending (2014) [Paper]\nEstimating Image Depth using Shape Collections (2014) [Paper]\nSingle-View Reconstruction via Joint Analysis of Image and Shape Collections (2015) [Paper]\nInterchangeable Components for Hands-On Assembly Based Modeling (2016) [Paper]\nShape Completion from a Single RGBD Image (2016) [Paper]\nDeep Learning Methods\n\ud83d\udcf7 Learning to Generate Chairs, Tables and Cars with Convolutional Networks (2014) [Paper]\n\ud83d\udcf7 Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis (2015, NIPS) [Paper]\n\ud83c\udfb2 Analysis and synthesis of 3D shape families via deep-learned generative models of surfaces (2015) [Paper]\n\ud83d\udcf7 Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis (2015) [Paper] [Code]\n\ud83d\udcf7 Multi-view 3D Models from Single Images with a Convolutional Network (2016) [Paper] [Code]\n\ud83d\udcf7 View Synthesis by Appearance Flow (2016) [Paper] [Code]\n\ud83d\udc7e Voxlets: Structured Prediction of Unobserved Voxels From a Single Depth Image (2016) [Paper] [Code]\n\ud83d\udc7e 3D-R2N2: 3D Recurrent Reconstruction Neural Network (2016) [Paper] [Code]\n\ud83d\udc7e Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision (2016) [Paper]\n\ud83d\udc7e TL-Embedding Network: Learning a Predictable and Generative Vector Representation for Objects (2016) [Paper]\n\ud83d\udc7e 3D GAN: Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling (2016) [Paper]\n\ud83d\udc7e 3D Shape Induction from 2D Views of Multiple Objects (2016) [Paper]\n\ud83d\udcf7 Unsupervised Learning of 3D Structure from Images (2016) [Paper]\n\ud83d\udc7e Generative and Discriminative Voxel Modeling with Convolutional Neural Networks (2016) [Paper] [Code]\n\ud83d\udcf7 Multi-view Supervision for Single-view Reconstruction via Differentiable Ray Consistency (2017) [Paper]\n\ud83d\udcf7 Synthesizing 3D Shapes via Modeling Multi-View Depth Maps and Silhouettes with Deep Generative Networks (2017) [Paper] [Code]\n\ud83d\udc7e Shape Completion using 3D-Encoder-Predictor CNNs and Shape Synthesis (2017) [Paper] [Code]\n\ud83d\udc7e Octree Generating Networks: Efficient Convolutional Architectures for High-resolution 3D Outputs (2017) [Paper] [Code]\n\ud83d\udc7e Hierarchical Surface Prediction for 3D Object Reconstruction (2017) [Paper]\n\ud83d\udc7e OctNetFusion: Learning Depth Fusion from Data (2017) [Paper] [Code]\n\ud83c\udfb2 A Point Set Generation Network for 3D Object Reconstruction from a Single Image (2017) [Paper] [Code]\n\ud83c\udfb2 Learning Representations and Generative Models for 3D Point Clouds (2017) [Paper] [Code]\n\ud83c\udfb2 Shape Generation using Spatially Partitioned Point Clouds (2017) [Paper]\n\ud83c\udfb2 PCPNET Learning Local Shape Properties from Raw Point Clouds (2017) [Paper]\n\ud83d\udcf7 Transformation-Grounded Image Generation Network for Novel 3D View Synthesis (2017) [Paper] [Code]\n\ud83d\udcf7 Tag Disentangled Generative Adversarial Networks for Object Image Re-rendering (2017) [Paper]\n\ud83d\udcf7 3D Shape Reconstruction from Sketches via Multi-view Convolutional Networks (2017) [Paper] [Code]\n\ud83d\udc7e Interactive 3D Modeling with a Generative Adversarial Network (2017) [Paper]\n\ud83d\udcf7\ud83d\udc7e Weakly supervised 3D Reconstruction with Adversarial Constraint (2017) [Paper] [Code]\n\ud83d\udcf7 SurfNet: Generating 3D shape surfaces using deep residual networks (2017) [Paper]\n\ud83d\udcf7 Learning to Reconstruct Symmetric Shapes using Planar Parameterization of 3D Surface (2019) [Paper] [Code]\n\ud83d\udc8a GRASS: Generative Recursive Autoencoders for Shape Structures (SIGGRAPH 2017) [Paper] [Code] [code]\n\ud83d\udc8a 3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks (2017) [Paper][code]\n\ud83d\udc8e Neural 3D Mesh Renderer (2017) [Paper] [Code]\n\ud83c\udfb2\ud83d\udc7e Large-Scale 3D Shape Reconstruction and Segmentation from ShapeNet Core55 (2017) [Paper]\n\ud83d\udc7e Pix2vox: Sketch-Based 3D Exploration with Stacked Generative Adversarial Networks (2017) [Code]\n\ud83d\udcf7\ud83d\udc7e What You Sketch Is What You Get: 3D Sketching using Multi-View Deep Volumetric Prediction (2017) [Paper]\n\ud83d\udcf7\ud83d\udc7e MarrNet: 3D Shape Reconstruction via 2.5D Sketches (2017) [Paper]\n\ud83d\udcf7\ud83d\udc7e\ud83c\udfb2 Learning a Multi-View Stereo Machine (2017 NIPS) [Paper]\n\ud83d\udc7e 3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions (2017) [Paper]\n\ud83d\udc7e Scaling CNNs for High Resolution Volumetric Reconstruction from a Single Image (2017) [Paper]\n\ud83d\udc8a ComplementMe: Weakly-Supervised Component Suggestions for 3D Modeling (2017) [Paper]\n\ud83d\udc7e Learning Descriptor Networks for 3D Shape Synthesis and Analysis (2018 CVPR) [Project] [Paper] [Code]\nAn energy-based 3D shape descriptor network is a deep energy-based model for volumetric shape patterns. The maximum likelihood training of the model follows an \u201canalysis by synthesis\u201d scheme and can be interpreted as a mode seeking and mode shifting process. The model can synthesize 3D shape patterns by sampling from the probability distribution via MCMC such as Langevin dynamics. Experiments demonstrate that the proposed model can generate realistic 3D shape patterns and can be useful for 3D shape analysis.\n\ud83c\udfb2 PU-Net: Point Cloud Upsampling Network (2018) [Paper] [Code]\n\ud83d\udcf7\ud83d\udc7e Multi-view Consistency as Supervisory Signal for Learning Shape and Pose Prediction (2018 CVPR) [Paper]\n\ud83d\udcf7\ud83c\udfb2 Object-Centric Photometric Bundle Adjustment with Deep Shape Prior (2018) [Paper]\n\ud83d\udcf7\ud83c\udfb2 Learning Efficient Point Cloud Generation for Dense 3D Object Reconstruction (2018 AAAI) [Paper]\n\ud83d\udc8e Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images (2018) [Paper]\n\ud83d\udc8e AtlasNet: A Papier-M\u00e2ch\u00e9 Approach to Learning 3D Surface Generation (2018 CVPR) [Paper] [Code]\n\ud83d\udc7e\ud83d\udc8e Deep Marching Cubes: Learning Explicit Surface Representations (2018 CVPR) [Paper]\n\ud83d\udc7e Im2Avatar: Colorful 3D Reconstruction from a Single Image (2018) [Paper]\n\ud83d\udc8e Learning Category-Specific Mesh Reconstruction from Image Collections (2018) [Paper]\n\ud83d\udc8a CSGNet: Neural Shape Parser for Constructive Solid Geometry (2018) [Paper]\n\ud83d\udc7e Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings (2018) [Paper]\n\ud83d\udc7e\ud83d\udc8e\ud83d\udcf7 Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation (2018) [Paper] [Code]\n\ud83d\udc7e\ud83d\udc8e\ud83d\udcf7 Pixels, voxels, and views: A study of shape representations for single view 3D object shape prediction (2018 CVPR) [Paper]\n\ud83d\udcf7\ud83c\udfb2 Neural scene representation and rendering (2018) [Paper]\n\ud83d\udc8a Im2Struct: Recovering 3D Shape Structure from a Single RGB Image (2018 CVPR) [Paper]\n\ud83c\udfb2 FoldingNet: Point Cloud Auto-encoder via Deep Grid Deformation (2018 CVPR) [Paper]\n\ud83d\udcf7\ud83d\udc7e Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling (2018 CVPR) [Paper]\n\ud83d\udc8e 3D-RCNN: Instance-level 3D Object Reconstruction via Render-and-Compare (2018 CVPR) [Paper]\n\ud83d\udc7e Matryoshka Networks: Predicting 3D Geometry via Nested Shape Layers (2018 CVPR) [Paper]\n\ud83d\udc8e Deformable Shape Completion with Graph Convolutional Autoencoders (2018 CVPR) [Paper]\n\ud83d\udc7e Global-to-Local Generative Model for 3D Shapes (SIGGRAPH Asia 2018) [Paper][Code]\n\ud83d\udc8e\ud83c\udfb2\ud83d\udc7e ALIGNet: Partial-Shape Agnostic Alignment via Unsupervised Learning (TOG 2018) [Paper] [Code]\n\ud83c\udfb2\ud83d\udc7e PointGrid: A Deep Network for 3D Shape Understanding (CVPR 2018) [Paper] [Code]\n\ud83c\udfb2 GAL: Geometric Adversarial Loss for Single-View 3D-Object Reconstruction (2018) [Paper]\n\ud83c\udfb2 Visual Object Networks: Image Generation with Disentangled 3D Representation (2018) [Paper]\n\ud83d\udc7e Learning to Infer and Execute 3D Shape Programs (2019)) [Paper]\n\ud83d\udc7e Learning to Infer and Execute 3D Shape Programs (2019)) [Paper]\n\ud83d\udc8e Learning View Priors for Single-view 3D Reconstruction (CVPR 2019) [Paper]\n\ud83d\udc8e\ud83c\udfb2 Learning Embedding of 3D models with Quadric Loss (BMVC 2019) [Paper] [Code]\n\ud83c\udfb2 CompoNet: Learning to Generate the Unseen by Part Synthesis and Composition (ICCV 2019) [Paper][Code]\nCoMA: Convolutional Mesh Autoencoders (2018) [Paper][Code (TF)][Code (PyTorch)][Code (PyTorch)]\nCoMA is a versatile model that learns a non-linear representation of a face using spectral convolutions on a mesh surface. CoMA introduces mesh sampling operations that enable a hierarchical mesh representation that captures non-linear variations in shape and expression at multiple scales within the model.\nRingNet: 3D Face Reconstruction from Single Images (2019) [Paper][Code]\nVOCA: Voice Operated Character Animation (2019) [Paper][Video][Code]\nVOCA is a simple and generic speech-driven facial animation framework that works across a range of identities. The codebase demonstrates how to synthesize realistic character animations given an arbitrary speech signal and a static character mesh.\n\ud83d\udc8e Learning to Predict 3D Objects with an Interpolation-based Differentiable Renderer [Paper][Site][Code]\n\ud83d\udc8e Soft Rasterizer: A Differentiable Renderer for Image-based 3D Reasoning [Paper][Code]\nNeRF: Representing Scenes as Neural Radiance Fields for View Synthesis [Project][Paper][Code]\n\ud83d\udc8e\ud83c\udfb2 GAMesh: Guided and Augmented Meshing for Deep Point Networks (3DV 2020) [Project] [Paper] [Code]\n\ud83d\udc7e Generative VoxelNet: Learning Energy-Based Models for 3D Shape Synthesis and Analysis (2020 TPAMI) [Paper]\nThis paper proposes a deep 3D energy-based model to represent volumetric shapes. The maximum likelihood training of the model follows an \u201canalysis by synthesis\u201d scheme. Experiments demonstrate that the proposed model can generate high-quality 3D shape patterns and can be useful for a wide variety of 3D shape analysis.\n\ud83c\udfb2 Generative PointNet: Deep Energy-Based Learning on Unordered Point Sets for 3D Generation, Reconstruction and Classification (2021 CVPR) [Project] [Paper] [Code]\nGenerative PointNet is an energy-based model of unordered point clouds, where the energy function is parameterized by an input-permutation-invariant bottom-up neural network. The model can be trained by MCMC-based maximum likelihood learning, or a short-run MCMC toward the energy-based model as a flow-like generator for point cloud reconstruction and interpolation. The learned point cloud representation can be useful for point cloud classification.\n\ud83c\udfb2 \ud83d\udc8e Shape My Face: Registering 3D Face Scans by Surface-to-Surface Translation [Paper] [Code]\nShape My Face (SMF) is a point cloud to mesh auto-encoder for the registration of raw human face scans, and the generation of synthetic human faces. SMF leverages a modified PointNet encoder with a visual attention module and differentiable surface sampling to be independent of the original surface representation and reduce the need for pre-processing. Mesh convolution decoders are combined with a specialized PCA model of the mouth, and smoothly blended based on geodesic distances, to create a compact model that is highly robust to noise. SMF is applied to register and perform expression transfer on scans captured in-the-wild with an iPhone depth camera represented either as meshes or point clouds.\nTexture/Material Analysis and Synthesis\nTexture Synthesis Using Convolutional Neural Networks (2015) [Paper]\nTwo-Shot SVBRDF Capture for Stationary Materials (SIGGRAPH 2015) [Paper]\nReflectance Modeling by Neural Texture Synthesis (2016) [Paper]\nModeling Surface Appearance from a Single Photograph using Self-augmented Convolutional Neural Networks (2017) [Paper]\nHigh-Resolution Multi-Scale Neural Texture Synthesis (2017) [Paper]\nReflectance and Natural Illumination from Single Material Specular Objects Using Deep Learning (2017) [Paper]\nJoint Material and Illumination Estimation from Photo Sets in the Wild (2017) [Paper]\nJWhat Is Around The Camera? (2017) [Paper]\nTextureGAN: Controlling Deep Image Synthesis with Texture Patches (2018 CVPR) [Paper]\nGaussian Material Synthesis (2018 SIGGRAPH) [Paper]\nNon-stationary Texture Synthesis by Adversarial Expansion (2018 SIGGRAPH) [Paper]\nSynthesized Texture Quality Assessment via Multi-scale Spatial and Statistical Texture Attributes of Image and Gradient Magnitude Coefficients (2018 CVPR) [Paper]\nLIME: Live Intrinsic Material Estimation (2018 CVPR) [Paper]\nSingle-Image SVBRDF Capture with a Rendering-Aware Deep Network (2018) [Paper]\nPhotoShape: Photorealistic Materials for Large-Scale Shape Collections (2018) [Paper]\nLearning Material-Aware Local Descriptors for 3D Shapes (2018) [Paper]\nFrankenGAN: Guided Detail Synthesis for Building Mass Models using Style-Synchonized GANs (2018 SIGGRAPH Asia) [Paper]\nStyle Learning and Transfer\nStyle-Content Separation by Anisotropic Part Scales (2010) [Paper]\nDesign Preserving Garment Transfer (2012) [Paper]\nAnalogy-Driven 3D Style Transfer (2014) [Paper]\nElements of Style: Learning Perceptual Shape Style Similarity (2015) [Paper] [Code]\nFunctionality Preserving Shape Style Transfer (2016) [Paper] [Code]\nUnsupervised Texture Transfer from Images to Model Collections (2016) [Paper]\nLearning Detail Transfer based on Geometric Features (2017) [Paper]\nCo-Locating Style-Defining Elements on 3D Shapes (2017) [Paper]\nNeural 3D Mesh Renderer (2017) [Paper] [Code]\nAppearance Modeling via Proxy-to-Image Alignment (2018) [Paper]\n\ud83d\udc8e Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images (2018) [Paper]\nAutomatic Unpaired Shape Deformation Transfer (SIGGRAPH Asia 2018) [Paper]\n3DSNet: Unsupervised Shape-to-Shape 3D Style Transfer (2020) [Paper] [Code]\nScene Synthesis/Reconstruction\nMake It Home: Automatic Optimization of Furniture Arrangement (2011, SIGGRAPH) [Paper]\nInteractive Furniture Layout Using Interior Design Guidelines (2011) [Paper]\nSynthesizing Open Worlds with Constraints using Locally Annealed Reversible Jump MCMC (2012) [Paper]\nExample-based Synthesis of 3D Object Arrangements (2012 SIGGRAPH Asia) [Paper]\nSketch2Scene: Sketch-based Co-retrieval and Co-placement of 3D Models (2013) [Paper]\nAction-Driven 3D Indoor Scene Evolution (2016) [Paper]\nThe Clutterpalette: An Interactive Tool for Detailing Indoor Scenes (2015) [Paper]\nImage2Scene: Transforming Style of 3D Room (2015) [Paper]\nRelationship Templates for Creating Scene Variations (2016) [Paper]\nIM2CAD (2017) [Paper]\nPredicting Complete 3D Models of Indoor Scenes (2017) [Paper]\nComplete 3D Scene Parsing from Single RGBD Image (2017) [Paper]\nRaster-to-Vector: Revisiting Floorplan Transformation (2017, ICCV) [Paper] [Code]\nFully Convolutional Refined Auto-Encoding Generative Adversarial Networks for 3D Multi Object Scenes (2017) [Blog]\nAdaptive Synthesis of Indoor Scenes via Activity-Associated Object Relation Graphs (2017 SIGGRAPH Asia) [Paper]\nAutomated Interior Design Using a Genetic Algorithm (2017) [Paper]\nSceneSuggest: Context-driven 3D Scene Design (2017) [Paper]\nA fully end-to-end deep learning approach for real-time simultaneous 3D reconstruction and material recognition (2017) [Paper]\nHuman-centric Indoor Scene Synthesis Using Stochastic Grammar (2018, CVPR)[Paper] [Supplementary] [Code]\n\ud83d\udcf7\ud83c\udfb2 FloorNet: A Unified Framework for Floorplan Reconstruction from 3D Scans (2018) [Paper] [Code]\n\ud83d\udc7e ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans (2018) [Paper]\nDeep Convolutional Priors for Indoor Scene Synthesis (2018) [Paper]\n\ud83d\udcf7 Fast and Flexible Indoor scene synthesis via Deep Convolutional Generative Models (2018) [Paper] [Code]\nConfigurable 3D Scene Synthesis and 2D Image Rendering with Per-Pixel Ground Truth using Stochastic Grammars (2018) [Paper]\nHolistic 3D Scene Parsing and Reconstruction from a Single RGB Image (ECCV 2018) [Paper]\nLanguage-Driven Synthesis of 3D Scenes from Scene Databases (SIGGRAPH Asia 2018) [Paper]\nDeep Generative Modeling for Scene Synthesis via Hybrid Representations (2018) [Paper]\nGRAINS: Generative Recursive Autoencoders for INdoor Scenes (2018) [Paper]\nSEETHROUGH: Finding Objects in Heavily Occluded Indoor Scene Images (2018) [Paper]\n\ud83d\udc7e Scan2CAD: Learning CAD Model Alignment in RGB-D Scans (CVPR 2019) [Paper] [Code]\n\ud83d\udc8e Scan2Mesh: From Unstructured Range Scans to 3D Meshes (CVPR 2019) [Paper]\n\ud83d\udc7e 3D-SIC: 3D Semantic Instance Completion for RGB-D Scans (arXiv 2019) [Paper]\n\ud83d\udc7e End-to-End CAD Model Retrieval and 9DoF Alignment in 3D Scans (arXiv 2019) [Paper]\nA Survey of 3D Indoor Scene Synthesis (2020) [Paper]\n\ud83d\udc8a \ud83d\udcf7 PlanIT: Planning and Instantiating Indoor Scenes with Relation Graph and Spatial Prior Networks (2019) [Paper] [Code]\n\ud83d\udc7e Feature-metric Registration: A Fast Semi-Supervised Approach for Robust Point Cloud Registration without Correspondences (CVPR 2020) [Paper][Code]\n\ud83d\udc8a Human-centric metrics for indoor scene assessment and synthesis (2020) [Paper]\nSceneCAD: Predicting Object Alignments and Layouts in RGB-D Scans (2020) [Paper]\nScene Understanding (Another more detailed repository)\nRecovering the Spatial Layout of Cluttered Rooms (2009) [Paper]\nCharacterizing Structural Relationships in Scenes Using Graph Kernels (2011 SIGGRAPH) [Paper]\nUnderstanding Indoor Scenes Using 3D Geometric Phrases (2013) [Paper]\nOrganizing Heterogeneous Scene Collections through Contextual Focal Points (2014 SIGGRAPH) [Paper]\nSceneGrok: Inferring Action Maps in 3D Environments (2014, SIGGRAPH) [Paper]\nPanoContext: A Whole-room 3D Context Model for Panoramic Scene Understanding (2014) [Paper]\nLearning Informative Edge Maps for Indoor Scene Layout Prediction (2015) [Paper]\nRent3D: Floor-Plan Priors for Monocular Layout Estimation (2015) [Paper]\nA Coarse-to-Fine Indoor Layout Estimation (CFILE) Method (2016) [Paper]\nDeLay: Robust Spatial Layout Estimation for Cluttered Indoor Scenes (2016) [Paper]\n3D Semantic Parsing of Large-Scale Indoor Spaces (2016) [Paper] [Code]\nSingle Image 3D Interpreter Network (2016) [Paper] [Code]\nDeep Multi-Modal Image Correspondence Learning (2016) [Paper]\nPhysically-Based Rendering for Indoor Scene Understanding Using Convolutional Neural Networks (2017) [Paper] [Code] [Code] [Code] [Code]\nRoomNet: End-to-End Room Layout Estimation (2017) [Paper]\nSUN RGB-D: A RGB-D Scene Understanding Benchmark Suite (2017) [Paper]\nSemantic Scene Completion from a Single Depth Image (2017) [Paper] [Code]\nFactoring Shape, Pose, and Layout from the 2D Image of a 3D Scene (2018 CVPR) [Paper] [Code]\nLayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image (2018 CVPR) [Paper] [Code]\nPlaneNet: Piece-wise Planar Reconstruction from a Single RGB Image (2018 CVPR) [Paper] [Code]\nCross-Domain Self-supervised Multi-task Feature Learning using Synthetic Imagery (2018 CVPR) [Paper]\nPano2CAD: Room Layout From A Single Panorama Image (2018 CVPR) [Paper]\nAutomatic 3D Indoor Scene Modeling from Single Panorama (2018 CVPR) [Paper]\nSingle-Image Piece-wise Planar 3D Reconstruction via Associative Embedding (2019 CVPR) [Paper] [Code]\n3D-Aware Scene Manipulation via Inverse Graphics (NeurIPS 2018) [Paper] [Code]\n\ud83d\udc8e 3D Scene Reconstruction with Multi-layer Depth and Epipolar Transformers (ICCV 2019) [Paper]\nPerspectiveNet: 3D Object Detection from a Single RGB Image via Perspective Points (NIPS 2019) [Paper]\nHolistic++ Scene Understanding: Single-view 3D Holistic Scene Parsing and Human Pose Estimation with Human-Object Interaction and Physical Commonsense (ICCV 2019) [Paper & Code]",
      "link": "https://github.com/timzhang642/3D-Machine-Learning"
    },
    {
      "autor": "ImageAI",
      "date": "NaN",
      "content": "ImageAI (v2.1.6)\nAn open-source python library built to empower developers to build applications and systems with self-contained Deep Learning and Computer Vision capabilities using simple and few lines of code.\nIf you will like to back this project, kindly visit the Patreon page by clicking the badge below.\n---------------------------------------------------\nImageAI will switch to PyTorch backend starting from June, 2021.\n---------------------------------------------------\nAn DeepQuest AI (A Brand of AI Commons Global Limited) project deepquestai.com.\nDeveloped and Maintained by Moses Olafenwa and John Olafenwa, brothers, creators of TorchFusion, Authors of Introduction to Deep Computer Vision and creators of DeepStack AI Server.\nBuilt with simplicity in mind, ImageAI supports a list of state-of-the-art Machine Learning algorithms for image prediction, custom image prediction, object detection, video detection, video object tracking and image predictions trainings. ImageAI currently supports image prediction and training using 4 different Machine Learning algorithms trained on the ImageNet-1000 dataset. ImageAI also supports object detection, video detection and object tracking using RetinaNet, YOLOv3 and TinyYOLOv3 trained on COCO dataset. Finally, ImageAI allows you to train custom models for performing detection and recognition of new objects.\nEventually, ImageAI will provide support for a wider and more specialized aspects of Computer Vision including and not limited to image recognition in special environments and special fields.\nNew Release : ImageAI 2.1.6\nWhat's new:\nSupport Tensorflow 2.4.0\nSqueezeNet replaced with MobileNetV2\nAdded TF 2.x compatible pre-trained models for ResNet recognition and RetinaNet detection\nDeprecates '.predictImage()' function for '.classifyImage()'\nRenames Model types as below:\nResNet >> ResNet50\nDenseNet >> DenseNet121\nTABLE OF CONTENTS\n\ud83d\udd33 Dependencies\n\ud83d\udd33 Installation\n\ud83d\udd33 Image Prediction\n\ud83d\udd33 Object Detection\n\ud83d\udd33 Video Object Detection, Tracking & Analysis\n\ud83d\udd33 Custom Model Training\n\ud83d\udd33 Custom Image Prediction\n\ud83d\udd33 Custom Detection Model Training\n\ud83d\udd33 Custom Object Detection\n\ud83d\udd33 Custom Video Object Detection & Analysis\n\ud83d\udd33 Documentation\n\ud83d\udd33 Projects Built on ImageAI\n\ud83d\udd33 High Performance Implementation\n\ud83d\udd33 AI Practice Recommendations\n\ud83d\udd33 Contact Developers\n\ud83d\udd33 Citation\n\ud83d\udd33 Sponsors\n\ud83d\udd33 Contributors\n\ud83d\udd33 References\nDependencies\nTo use ImageAI in your application developments, you must have installed the following dependencies before you install ImageAI :\nPython 3.7.6\nTensorflow 2.4.0\nOpenCV\nKeras 2.4.3\nYou can install all the dependencies by running the commands below\nTensorflow\npip install tensorflow==2.4.0\nor Tensorflow GPU if you have NVIDIA GPU with CUDA and cuDNN installed.\npip install tensorflow-gpu==2.4.0\nOther Dependencies\npip install keras==2.4.3 numpy==1.19.3 pillow==7.0.0 scipy==1.4.1 h5py==2.10.0 matplotlib==3.3.2 opencv-python keras-resnet==0.2.0\nInstallation\nTo install ImageAI, run the python installation instruction below in the command line:\npip install imageai --upgrade\nImage Prediction\nconvertible : 52.459555864334106\nsports_car : 37.61284649372101\npickup : 3.1751200556755066\ncar_wheel : 1.817505806684494\nminivan : 1.7487050965428352\nImageAI provides 4 different algorithms and model types to perform image prediction, trained on the ImageNet-1000 dataset. The 4 algorithms provided for image prediction include MobileNetV2, ResNet50, InceptionV3 and DenseNet121.\nClick the link below to see the full sample codes, explanations and best practices guide.\n>>> Tutorial & Guide\nObject Detection\nperson : 91.946941614151\n--------------------------------\nperson : 73.61021637916565\n--------------------------------\nlaptop : 90.24320840835571\n--------------------------------\nlaptop : 73.6881673336029\n--------------------------------\nlaptop : 95.16398310661316\n--------------------------------\nperson : 87.10319399833679\n--------------------------------\nImageAI provides very convenient and powerful methods to perform object detection on images and extract each object from the image. The object detection class provides support for RetinaNet, YOLOv3 and TinyYOLOv3, with options to adjust for state of the art performance or real time processing.\nClick the link below to see the full sample codes, explanations and best practices guide.\n>>> Tutorial & Guide\nVideo Object Detection and Tracking\nVideo Object Detection & Analysis\nBelow is a snapshot of a video with objects detected.\nVideo Custom Object Detection (Object Tracking)\nBelow is a snapshot of a video with only person, bicycle and motorcyle detected.\nVideo Analysis Visualization\nBelow is a visualization of video analysis returned by ImageAI into a 'per_second' function.\nImageAI provides very convenient and powerful methods to perform object detection in videos and track specific object(s). The video object detection class provided only supports the current state-of-the-art RetinaNet, but with options to adjust for state of the art performance or real time processing. Click the link to see the full videos, sample codes, explanations and best practices guide.\n>>> Tutorial & Guide\nCustom Model Training\nA sample from the IdenProf Dataset used to train a Model for predicting professionals.\nImageAI provides classes and methods for you to train a new model that can be used to perform prediction on your own custom objects. You can train your custom models using SqueezeNet, ResNet50, InceptionV3 and DenseNet in 5 lines of code. Click the link below to see the guide to preparing training images, sample training codes, explanations and best practices.\n>>> Tutorials & Documentation\nCustom Image Prediction\nPrediction from a sample model trained on IdenProf, for predicting professionals\nmechanic : 76.82620286941528\nchef : 10.106072574853897\nwaiter : 4.036874696612358\npolice : 2.6663416996598244\npilot : 2.239348366856575\nImageAI provides classes and methods for you to run image prediction your own custom objects using your own model trained with ImageAI Model Training class. You can use your custom models trained with SqueezeNet, ResNet50, InceptionV3 and DenseNet and the JSON file containing the mapping of the custom object names. Click the link below to see the guide to sample training codes, explanations, and best practices guide.\n>>> Tutorials & Documentation\nCustom Detection Model Training\nTraining detection models to detect and recognize new objects.\nImageAI provides classes and methods for you to train new YOLOv3 object detection models on your custom dataset. This means you can train a model to detect literally any object of interest by providing the images, the annotations and training with ImageAI. Click the link below to see the guide to sample training codes, explanations, and best practices guide.\n>>> Tutorials & Documentation\nCustom Object Detection\nDetection result from a custom YOLOv3 model trained to detect the Hololens headset.\nhololens : 39.69653248786926 : [611, 74, 751, 154]\nhololens : 87.6643180847168 : [23, 46, 90, 79]\nhololens : 89.25175070762634 : [191, 66, 243, 95]\nhololens : 64.49641585350037 : [437, 81, 514, 133]\nhololens : 91.78624749183655 : [380, 113, 423, 138]\nImageAI now provides classes and methods for you detect and recognize your own custom objects in images using your own model trained with the DetectionModelTraining class. You can use your custom trained YOLOv3 mode and the detection_config.json file generated during the training. Click the link below to see the guide to sample training codes, explanations, and best practices guide.\n>>> Tutorials & Documentation\nCustom Video Object Detection & Analysis\nVideo Detection result from a custom YOLOv3 model trained to detect the Hololens headset in a video.\nNow you can use your custom trained YOLOv3 model to detect, recognize and analyze objects in videos. Click the link below to see the guide to sample training codes, explanations, and best practices guide.\n>>> Tutorials & Documentation\nDocumentation\nWe have provided full documentation for all ImageAI classes and functions in 2 major languages. Find links below:\nDocumentation - English Version https://imageai.readthedocs.io\nDocumentation - Chinese Version https://imageai-cn.readthedocs.io\nDocumentation - French Version https://imageai-fr.readthedocs.io\nReal-Time and High Performance Implementation\nImageAI provides abstracted and convenient implementations of state-of-the-art Computer Vision technologies. All of ImageAI implementations and code can work on any computer system with moderate CPU capacity. However, the speed of processing for operations like image prediction, object detection and others on CPU is slow and not suitable for real-time applications. To perform real-time Computer Vision operations with high performance, you need to use GPU enabled technologies.\nImageAI uses the Tensorflow backbone for it's Computer Vision operations. Tensorflow supports both CPUs and GPUs ( Specifically NVIDIA GPUs. You can get one for your PC or get a PC that has one) for machine learning and artificial intelligence algorithms' implementations. To use Tensorflow that supports the use of GPUs, follow the link below :\nFOR WINDOWS - install_windows\nFOR macOS - install_mac\nFOR UBUNTU - install_linux\nProjects Built on ImageAI\nBatBot : BatBot is an open source Intelligent Research Robot with image and speech recognition. It comes with an Android App which allows you to speak voice commands and instruct the robot to find objects using its -----> camera !!!  and an AI engine powered by ImageAI. It also allows you to re-train and improve the AI capabilities from new images captured by the robot. Learn more about the incredible capabilities and components of BatBot via the GitHub repository . It is developed and maintained by Lee Hounshell\nWe also welcome submissions of applications and systems built by you and powered by ImageAI for listings here. Should you want your ImageAI powered developments listed here, you can reach to us via our Contacts below.\nAI Practice Recommendations\nFor anyone interested in building AI systems and using them for business, economic, social and research purposes, it is critical that the person knows the likely positive, negative and unprecedented impacts the use of such technologies will have. They must also be aware of approaches and practices recommended by experienced industry experts to ensure every use of AI brings overall benefit to mankind. We therefore recommend to everyone that wishes to use ImageAI and other AI tools and resources to read Microsoft's January 2018 publication on AI titled \"The Future Computed : Artificial Intelligence and its role in society\". Kindly follow the link below to download the publication.\nhttps://blogs.microsoft.com/blog/2018/01/17/future-computed-artificial-intelligence-role-society\nContact Developer\nMoses Olafenwa\nEmail: guymodscientist@gmail.com\nWebsite: http://olafenwamoses.me\nTwitter: @OlafenwaMoses\nMedium: @guymodscientist\nFacebook: moses.olafenwa\nJohn Olafenwa\nEmail: johnolafenwa@gmail.com\nWebsite: https://john.aicommons.science\nTwitter: @johnolafenwa\nMedium: @johnolafenwa\nFacebook: olafenwajohn\nContributors\nWe are inviting anyone who wishes to contribute to the ImageAI project to reach to us. We primarily need contributions in translating the documentation of the project's code to major languages that includes but not limited to French, Spanish, Portuguese, Arabian and more. We want every developer and researcher around the world to benefit from this project irrespective of their native languages.\nWe give special thanks to Kang vcar for his incredible and excellent work in translating ImageAI's documentation to the Chinese language. Find below the contact details of those who have contributed immensely to the ImageAI project.\nKang vcar\nEmail: kangvcar@mail.com\nWebsite: http://www.kangvcar.com\nTwitter: @kangvcar\nCitation\nYou can cite ImageAI in your projects and research papers via the BibTeX entry below.\n@misc {ImageAI,\nauthor = \"Moses and John Olafenwa\",\ntitle = \"ImageAI, an open source python library built to empower developers to build applications and systems with self-contained Computer Vision capabilities\",\nurl = \"https://github.com/OlafenwaMoses/ImageAI\",\nmonth = \"mar\",\nyear = \"2018--\"\n}\nReferences\nSomshubra Majumdar, DenseNet Implementation of the paper, Densely Connected Convolutional Networks in Keras https://github.com/titu1994/DenseNet\nBroad Institute of MIT and Harvard, Keras package for deep residual networks https://github.com/broadinstitute/keras-resnet\nFizyr, Keras implementation of RetinaNet object detection https://github.com/fizyr/keras-retinanet\nFrancois Chollet, Keras code and weights files for popular deeplearning models https://github.com/fchollet/deep-learning-models\nForrest N. et al, SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size https://arxiv.org/abs/1602.07360\nKaiming H. et al, Deep Residual Learning for Image Recognition https://arxiv.org/abs/1512.03385\nSzegedy. et al, Rethinking the Inception Architecture for Computer Vision https://arxiv.org/abs/1512.00567\nGao. et al, Densely Connected Convolutional Networks https://arxiv.org/abs/1608.06993\nTsung-Yi. et al, Focal Loss for Dense Object Detection https://arxiv.org/abs/1708.02002\nO Russakovsky et al, ImageNet Large Scale Visual Recognition Challenge https://arxiv.org/abs/1409.0575\nTY Lin et al, Microsoft COCO: Common Objects in Context https://arxiv.org/abs/1405.0312\nMoses & John Olafenwa, A collection of images of identifiable professionals. https://github.com/OlafenwaMoses/IdenProf\nJoseph Redmon and Ali Farhadi, YOLOv3: An Incremental Improvement. https://arxiv.org/abs/1804.02767\nExperiencor, Training and Detecting Objects with YOLO3 https://github.com/experiencor/keras-yolo3\nMobileNetV2: Inverted Residuals and Linear Bottlenecks https://arxiv.org/abs/1801.04381",
      "link": "https://github.com/OlafenwaMoses/ImageAI"
    },
    {
      "autor": "awesome-project-ideas",
      "date": "NaN",
      "content": "Awesome Deep Learning Project Ideas\nA curated list of practical deep learning and machine learning project ideas\n30+ ideas\nRelevant to both the academia and industry\nRanges from beginner friendly to research projects\nContents\nText - With some topics about Natural language processing\nForecasting - Most of the topics in this section is about Time Series and similar forecasting challenges\nRecommendation Systems\nVision - With topics about image and video processing\nCovid19 - Multi or Single Domain ideas from the Covid19 theme\nMusic and Audio - These topics are about combining ideas from language and audio to understand music\nConclusion\nCovid19\nBing Coronavirus\nClassify Bing Queries as either specific (e.g. about a specific location) or generic. You might have to figure out a more exact definition of specific or generic though\nDataset: BingCoronavirusQuerySet\nCovid Clinical Data\nRank and sort high risk patients using clinical data. Pick an interpretable approach if you can.\nDataset: CovidClinicalData\nIf you haven't already, checkout Kaggle's Covid19 Section as well. It has datasets and ideas both.\nText\nAutonomous Tagging of StackOverflow Questions\nMake a multi-label classification system that automatically assigns tags for questions posted on a forum such as StackOverflow or Quora.\nDataset: StackLite or 10% sample\nKeyword/Concept identification\nIdentify keywords from millions of questions\nDataset: StackOverflow question samples by Facebook\nTopic identification\nMulti-label classification of printed media articles to topics\nDataset: Greek Media monitoring multi-label classification\nNatural Language Understanding\nAutomated essay grading\nThe purpose of this project is to implement and train machine learning algorithms to automatically assess and grade essay responses.\nDataset: Essays with human graded scores\nSentence to Sentence semantic similarity\nCan you identify question pairs that have the same intent or meaning?\nDataset: Quora question pairs with similar questions marked\nFight online abuse\nCan you confidently and accurately tell whether a particular comment is abusive?\nDataset: Toxic comments on Kaggle\nOpen Domain question answering\nCan you build a bot which answers questions according to the student's age or her curriculum?\nFacebook's FAIR is built in a similar way for Wikipedia.\nDataset: NCERT books for K-12/school students in India, NarrativeQA by Google DeepMind and SQuAD by Stanford\nSocial Chat/Conversational Bots\nCan you build a bot which talks to you just like people talk on social networking sites?\nReference: Chat-bot architecture\nDataset: Reddit Dataset\nAutomatic text summarization\nCan you create a summary with the major points of the original document?\nAbstractive (write your own summary) and Extractive (select pieces of text from original) are two popular approaches\nDataset: CNN and DailyMail News Pieces by Google DeepMind\nCopy-cat Bot\nGenerate plausible new text which looks like some other text\nObama Speeches? For instance, you can create a bot which writes some new speeches in Obama's style\nTrump Bot? Or a Twitter bot which mimics @realDonaldTrump\nNarendra Modi bot saying \"doston\"? Start by scrapping off his Hindi speeches from his personal website\nExample Dataset: English Transcript of Modi speeches\nCheck mlm/blog for some hints.\nSentiment Analysis\nDo Twitter Sentiment Analysis on tweets sorted by geography and timestamp.\nDataset: Tweets sentiment tagged by humans\nDe-anonymization\nCan you classify the text of an e-mail message to decide who sent it?\nDataset: 150,000 Enron emails\nForecasting\nUnivariate Time Series Forecasting\nHow much will it rain this year?\nDataset: 45 years of rainfall data\nMulti-variate Time Series Forecasting\nHow polluted will your town's air be? Pollution Level Forecasting\nDataset: Air Quality dataset\nDemand/load forecasting\nFind a short term forecast on electricity consumption of a single home\nDataset: Electricity consumption of a household\nPredict Blood Donation\nWe're interested in predicting if a blood donor will donate within a given time window.\nMore on the problem statement at Driven Data.\nDataset: UCI ML Datasets Repo\nRecommendation systems\nMovie Recommender\nCan you predict the rating a user will give on a movie?\nDo this using the movies that user has rated in the past, as well as the ratings similar users have given similar movies.\nDataset: Netflix Prize and MovieLens Datasets\nSearch + Recommendation System\nPredict which Xbox game a visitor will be most interested in based on their search query\nDataset: BestBuy\nCan you predict Influencers in the Social Network?\nHow can you predict social influencers?\nDataset: PeerIndex\nVision\nImage classification\nObject recognition or image classification task is how Deep Learning shot up to it's present-day resurgence\nDatasets:\nCIFAR-10\nImageNet\nMS COCO is the modern replacement to the ImageNet challenge\nMNIST Handwritten Digit Classification Challenge is the classic entry point\nCharacter recognition (digits) is the good old Optical Character Recognition problem\nBird Species Identification from an Image using the Caltech-UCSD Birds dataset dataset\nDiagnosing and Segmenting Brain Tumors and Phenotypes using MRI Scans\nDataset: MICCAI Machine Learning Challenge aka MLC 2014\nIdentify endangered right whales in aerial photographs\nDataset: MOAA Right Whale\nCan computer vision spot distracted drivers?\nDataset: State Farm Distracted Driver Detection on Kaggle\nBone X-Ray competition\nCan you identify if a hand is broken from a X-ray radiographs automatically with better than human performance?\nStanford's Bone XRay Deep Learning Competition with MURA Dataset\nImage Captioning\nCan you caption/explain the -----> photo !!!  a way human would?\nDataset: MS COCO\nImage Segmentation/Object Detection\nCan you extract an object of interest from an image?\nDataset: MS COCO, Carvana Image Masking Challenge on Kaggle\nLarge-Scale Video Understanding\nCan you produce the best video tag predictions?\nDataset: YouTube 8M\nVideo Summarization\nCan you select the semantically relevant/important parts from the video?\nExample: Fast-Forward Video Based on Semantic Extraction\nDataset: Unaware of any standard dataset or agreed upon metrics? I think YouTube 8M might be good starting point.\nStyle Transfer\nCan you recompose images in the style of other images?\nDataset: fzliu on GitHub shared target and source images with results\nChest XRay\nCan you detect if someone is sick from their chest XRay? Or guess their radiology report?\nDataset: MIMIC-CXR at Physionet\nFace Recognition\nCan you identify whose photo is this? Similar to Facebook's photo tagging or Apple's FaceId\nDataset: face-rec.org, or facedetection.com\nClinical Diagnostics: Image Identification, classification & segmentation\nCan you help build an open source software for lung cancer detection to help radiologists?\nLink: Concept to clinic challenge on DrivenData\nSatellite Imagery Processing for Socioeconomic Analysis\nCan you estimate the standard of living or energy consumption of a place from night time satellite imagery?\nReference for Project details: Stanford Poverty Estimation Project\nSatellite Imagery Processing for Automated Tagging\nCan you automatically tag satellite images with human features such as buildings, roads, waterways and so on?\nHelp free the manual effort in tagging satellite imagery: Kaggle Dataset by DSTL, UK\nMusic\nMusic/Audio Recommendation Systems\nCan you tell if two songs are similar using their sound or lyrics?\nDataset: Million Songs Dataset and it's 1% sample.\nExample: Anusha et al\nMusic Genre recognition using neural networks\nCan you identify the musical genre using their spectrograms or other sound information?\nDatasets: FMA or GTZAN on Keras\nGet started with Librosa for feature extraction\nFAQ\nCan I use the ideas here for my thesis? Yes, totally! I'd love to know how it went.\nDo you have any advice before I start my project? Advice for Short Term Machine Learning Projects by Tim R. is a pretty good starting point!\nWould you like to share my solution/code to a problem here? Sure - why not?\nGo to the GitHub issues tab in this repository and let me know there.\nHow can I add my ideas here? Just send a pull request and we'll discuss?\nHey, something is wrong here! Yikes, I am sorry. Please tell me by raising a GitHub issue.\nI'll fix it as soon as possible.\nAcknowledgements\nProblems are motivated by the ones shared at:\nCMU Machine Learning\nStanford CS229 Machine Learning Projects\nCredit\nBuilt with lots of keyboard smashing and copy-pasta love by NirantK. Find me on Twitter!\nReceive New & Exclusive Ideas right in your Inbox\nThese ideas have been seen by people in last few months!\nIf you are interested in seeing exclusive machine learning and deep learning project ideas, share your e-mail address here!\nLicense\nThis repository is licensed under the MIT License. Please see the LICENSE file for more details.",
      "link": "https://github.com/NirantK/awesome-project-ideas"
    },
    {
      "autor": "darkflow",
      "date": "NaN",
      "content": "Intro\nReal-time object detection and classification. Paper: version 1, version 2.\nRead more about YOLO (in darknet) and download weight files here. In case the weight file cannot be found, I uploaded some of mine here, which include yolo-full and yolo-tiny of v1.0, tiny-yolo-v1.1 of v1.1 and yolo, tiny-yolo-voc of v2.\nSee demo below or see on this imgur\nDependencies\nPython3, tensorflow 1.0, numpy, opencv 3.\nCitation\n@article{trieu2018darkflow,\ntitle={Darkflow},\nauthor={Trieu, Trinh Hoang},\njournal={GitHub Repository. Available online: https://github. com/thtrieu/darkflow (accessed on 14 February 2019)},\nyear={2018}\n}\nGetting started\nYou can choose one of the following three ways to get started with darkflow.\nJust build the Cython extensions in place. NOTE: If installing this way you will have to use ./flow in the cloned darkflow directory instead of flow as darkflow is not installed globally.\npython3 setup.py build_ext --inplace\nLet pip install darkflow globally in dev mode (still globally accessible, but changes to the code immediately take effect)\npip install -e .\nInstall with pip globally\npip install .\nUpdate\nAndroid demo on Tensorflow's here\nI am looking for help:\nhelp wanted labels in issue track\nParsing the annotations\nSkip this if you are not training or fine-tuning anything (you simply want to forward flow a trained net)\nFor example, if you want to work with only 3 classes tvmonitor, person, pottedplant; edit labels.txt as follows\ntvmonitor\nperson\npottedplant\nAnd that's it. darkflow will take care of the rest. You can also set darkflow to load from a custom labels file with the --labels flag (i.e. --labels myOtherLabelsFile.txt). This can be helpful when working with multiple models with different sets of output labels. When this flag is not set, darkflow will load from labels.txt by default (unless you are using one of the recognized .cfg files designed for the COCO or VOC dataset - then the labels file will be ignored and the COCO or VOC labels will be loaded).\nDesign the net\nSkip this if you are working with one of the original configurations since they are already there. Otherwise, see the following example:\n...\n[convolutional]\nbatch_normalize = 1\nsize = 3\nstride = 1\npad = 1\nactivation = leaky\n[maxpool]\n[connected]\noutput = 4096\nactivation = linear\n...\nFlowing the graph using flow\n# Have a look at its options\nflow --h\nFirst, let's take a closer look at one of a very useful option --load\n# 1. Load tiny-yolo.weights\nflow --model cfg/tiny-yolo.cfg --load bin/tiny-yolo.weights\n# 2. To completely initialize a model, leave the --load option\nflow --model cfg/yolo-new.cfg\n# 3. It is useful to reuse the first identical layers of tiny for `yolo-new`\nflow --model cfg/yolo-new.cfg --load bin/tiny-yolo.weights\n# this will print out which layers are reused, which are initialized\nAll input images from default folder sample_img/ are flowed through the net and predictions are put in sample_img/out/. We can always specify more parameters for such forward passes, such as detection threshold, batch size, images folder, etc.\n# Forward all images in sample_img/ using tiny yolo and 100% GPU usage\nflow --imgdir sample_img/ --model cfg/tiny-yolo.cfg --load bin/tiny-yolo.weights --gpu 1.0\njson output can be generated with descriptions of the pixel location of each bounding box and the pixel location. Each prediction is stored in the sample_img/out folder by default. An example json array is shown below.\n# Forward all images in sample_img/ using tiny yolo and JSON output.\nflow --imgdir sample_img/ --model cfg/tiny-yolo.cfg --load bin/tiny-yolo.weights --json\nJSON output:\n[{\"label\":\"person\", \"confidence\": 0.56, \"topleft\": {\"x\": 184, \"y\": 101}, \"bottomright\": {\"x\": 274, \"y\": 382}},\n{\"label\": \"dog\", \"confidence\": 0.32, \"topleft\": {\"x\": 71, \"y\": 263}, \"bottomright\": {\"x\": 193, \"y\": 353}},\n{\"label\": \"horse\", \"confidence\": 0.76, \"topleft\": {\"x\": 412, \"y\": 109}, \"bottomright\": {\"x\": 592,\"y\": 337}}]\nlabel: self explanatory\nconfidence: somewhere between 0 and 1 (how confident yolo is about that detection)\ntopleft: pixel coordinate of top left corner of box.\nbottomright: pixel coordinate of bottom right corner of box.\nTraining new model\nTraining is simple as you only have to add option --train. Training set and annotation will be parsed if this is the first time a new configuration is trained. To point to training set and annotations, use option --dataset and --annotation. A few examples:\n# Initialize yolo-new from yolo-tiny, then train the net on 100% GPU:\nflow --model cfg/yolo-new.cfg --load bin/tiny-yolo.weights --train --gpu 1.0\n# Completely initialize yolo-new and train it with ADAM optimizer\nflow --model cfg/yolo-new.cfg --train --trainer adam\nDuring training, the script will occasionally save intermediate results into Tensorflow checkpoints, stored in ckpt/. To resume to any checkpoint before performing training/testing, use --load [checkpoint_num] option, if checkpoint_num < 0, darkflow will load the most recent save by parsing ckpt/checkpoint.\n# Resume the most recent checkpoint for training\nflow --train --model cfg/yolo-new.cfg --load -1\n# Test with checkpoint at step 1500\nflow --model cfg/yolo-new.cfg --load 1500\n# Fine tuning yolo-tiny from the original one\nflow --train --model cfg/tiny-yolo.cfg --load bin/tiny-yolo.weights\nExample of training on Pascal VOC 2007:\n# Download the Pascal VOC dataset:\ncurl -O https://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\ntar xf VOCtest_06-Nov-2007.tar\n# An example of the Pascal VOC annotation format:\nvim VOCdevkit/VOC2007/Annotations/000001.xml\n# Train the net on the Pascal dataset:\nflow --model cfg/yolo-new.cfg --train --dataset \"~/VOCdevkit/VOC2007/JPEGImages\" --annotation \"~/VOCdevkit/VOC2007/Annotations\"\nTraining on your own dataset\nThe steps below assume we want to use tiny YOLO and our dataset has 3 classes\nCreate a copy of the configuration file tiny-yolo-voc.cfg and rename it according to your preference tiny-yolo-voc-3c.cfg (It is crucial that you leave the original tiny-yolo-voc.cfg file unchanged, see below for explanation).\nIn tiny-yolo-voc-3c.cfg, change classes in the [region] layer (the last layer) to the number of classes you are going to train for. In our case, classes are set to 3.\n...\n[region]\nanchors = 1.08,1.19, 3.42,4.41, 6.63,11.38, 9.42,5.11, 16.62,10.52\nbias_match=1\nclasses=3\ncoords=4\nnum=5\nsoftmax=1\n...\nIn tiny-yolo-voc-3c.cfg, change filters in the [convolutional] layer (the second to last layer) to num * (classes + 5). In our case, num is 5 and classes are 3 so 5 * (3 + 5) = 40 therefore filters are set to 40.\n...\n[convolutional]\nsize=1\nstride=1\npad=1\nfilters=40\nactivation=linear\n[region]\nanchors = 1.08,1.19, 3.42,4.41, 6.63,11.38, 9.42,5.11, 16.62,10.52\n...\nChange labels.txt to include the label(s) you want to train on (number of labels should be the same as the number of classes you set in tiny-yolo-voc-3c.cfg file). In our case, labels.txt will contain 3 labels.\nlabel1\nlabel2\nlabel3\nReference the tiny-yolo-voc-3c.cfg model when you train.\nflow --model cfg/tiny-yolo-voc-3c.cfg --load bin/tiny-yolo-voc.weights --train --annotation train/Annotations --dataset train/Images\nWhy should I leave the original tiny-yolo-voc.cfg file unchanged?\nWhen darkflow sees you are loading tiny-yolo-voc.weights it will look for tiny-yolo-voc.cfg in your cfg/ folder and compare that configuration file to the new one you have set with --model cfg/tiny-yolo-voc-3c.cfg. In this case, every layer will have the same exact number of weights except for the last two, so it will load the weights into all layers up to the last two because they now contain different number of weights.\n-----> Camera !!! /video file demo\nFor a demo that entirely runs on the CPU:\nflow --model cfg/yolo-new.cfg --load bin/yolo-new.weights --demo videofile.avi\nFor a demo that runs 100% on the GPU:\nflow --model cfg/yolo-new.cfg --load bin/yolo-new.weights --demo videofile.avi --gpu 1.0\nTo use your webcam/camera, simply replace videofile.avi with keyword camera.\nTo save a video with predicted bounding box, add --saveVideo option.\nUsing darkflow from another python application\nPlease note that return_predict(img) must take an numpy.ndarray. Your image must be loaded beforehand and passed to return_predict(img). Passing the file path won't work.\nResult from return_predict(img) will be a list of dictionaries representing each detected object's values in the same format as the JSON output listed above.\nfrom darkflow.net.build import TFNet\nimport cv2\noptions = {\"model\": \"cfg/yolo.cfg\", \"load\": \"bin/yolo.weights\", \"threshold\": 0.1}\ntfnet = TFNet(options)\nimgcv = cv2.imread(\"./sample_img/sample_dog.jpg\")\nresult = tfnet.return_predict(imgcv)\nprint(result)\nSave the built graph to a protobuf file (.pb)\n## Saving the lastest checkpoint to protobuf file\nflow --model cfg/yolo-new.cfg --load -1 --savepb\n## Saving graph and weights to protobuf file\nflow --model cfg/yolo.cfg --load bin/yolo.weights --savepb\nWhen saving the .pb file, a .meta file will also be generated alongside it. This .meta file is a JSON dump of everything in the meta dictionary that contains information nessecary for post-processing such as anchors and labels. This way, everything you need to make predictions from the graph and do post processing is contained in those two files - no need to have the .cfg or any labels file tagging along.\nThe created .pb file can be used to migrate the graph to mobile devices (JAVA / C++ / Objective-C++). The name of input tensor and output tensor are respectively 'input' and 'output'. For further usage of this protobuf file, please refer to the official documentation of Tensorflow on C++ API here. To run it on, say, iOS application, simply add the file to Bundle Resources and update the path to this file inside source code.\nAlso, darkflow supports loading from a .pb and .meta file for generating predictions (instead of loading from a .cfg and checkpoint or .weights).\n## Forward images in sample_img for predictions based on protobuf file\nflow --pbLoad built_graph/yolo.pb --metaLoad built_graph/yolo.meta --imgdir sample_img/\nIf you'd like to load a .pb and .meta file when using return_predict() you can set the \"pbLoad\" and \"metaLoad\" options in place of the \"model\" and \"load\" options you would normally set.\nThat's all.",
      "link": "https://github.com/thtrieu/darkflow"
    },
    {
      "autor": "kornia",
      "date": "NaN",
      "content": "English | \u7b80\u4f53\u4e2d\u6587\nWebsite \u2022 Docs \u2022 Try it Now \u2022 Tutorials \u2022 Examples \u2022 Blog \u2022 Community\nKornia is a differentiable computer vision library for PyTorch.\nIt consists of a set of routines and differentiable modules to solve generic computer vision problems. At its core, the package uses PyTorch as its main backend both for efficiency and to take advantage of the reverse-mode auto-differentiation to define and compute the gradient of complex functions.\nOverview\nInspired by existing packages, this library is composed by a subset of packages containing operators that can be inserted within neural networks to train models to perform image transformations, epipolar geometry, depth estimation, and low-level image processing such as filtering and edge detection that operate directly on tensors.\nAt a granular level, Kornia is a library that consists of the following components:\nComponent Description\nkornia a Differentiable Computer Vision library, with strong GPU support\nkornia.augmentation a module to perform data augmentation in the GPU\nkornia.color a set of routines to perform color space conversions\nkornia.contrib a compilation of user contrib and experimental operators\nkornia.enhance a module to perform normalization and intensity transformation\nkornia.feature a module to perform feature detection\nkornia.filters a module to perform image filtering and edge detection\nkornia.geometry a geometric computer vision library to perform image transformations, 3D linear algebra and conversions using different -----> camera !!!  models\nkornia.losses a stack of loss functions to solve different vision tasks\nkornia.morphology a module to perform morphological operations\nkornia.utils image to tensor utilities and metrics for vision problems\nInstallation\nFrom pip:\npip install kornia\npip install kornia[x] # to get the training API !\nOther installation options\nExamples\nRun our Jupyter notebooks tutorials to learn to use the library.\n\ud83d\udea9 Updates\n\u2705 Integrated to Huggingface Spaces with Gradio. See Gradio Web Demo.\nCite\nIf you are using kornia in your research-related documents, it is recommended that you cite the paper. See more in CITATION.\n@inproceedings{eriba2019kornia,\nauthor = {E. Riba, D. Mishkin, D. Ponsa, E. Rublee and G. Bradski},\ntitle = {Kornia: an Open Source Differentiable Computer Vision Library for PyTorch},\nbooktitle = {Winter Conference on Applications of Computer Vision},\nyear = {2020},\nurl = {https://arxiv.org/pdf/1910.02190.pdf}\n}\nContributing\nWe appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion. If you plan to contribute new features, utility functions or extensions, please first open an issue and discuss the feature with us. Please, consider reading the CONTRIBUTING notes. The participation in this open source project is subject to Code of Conduct.\nCommunity\nForums: discuss implementations, research, etc. GitHub Forums\nGitHub Issues: bug reports, feature requests, install issues, RFCs, thoughts, etc. OPEN\nSlack: Join our workspace to keep in touch with our core contributors and be part of our community. JOIN HERE\nFor general information, please visit our website at www.kornia.org",
      "link": "https://github.com/kornia/kornia"
    },
    {
      "autor": "TopDeepLearning",
      "date": "NaN",
      "content": "Top Deep Learning Projects\nA list of popular github projects related to deep learning (ranked by stars).\nLast Update: 2020.07.09\nProject Name Stars Description\ntensorflow 146k An Open Source Machine Learning Framework for Everyone\nkeras 48.9k Deep Learning for humans\nopencv 46.1k Open Source Computer Vision Library\npytorch 40k Tensors and Dynamic neural networks in Python with strong GPU acceleration\nTensorFlow-Examples 38.1k TensorFlow Tutorial and Examples for Beginners (support TF v1 & v2)\ntesseract 35.3k Tesseract Open Source OCR Engine (main repository)\nface_recognition 35.2k The world's simplest facial recognition api for Python and the command line\nfaceswap 31.4k Deepfakes Software For All\ntransformers 30.4k \ud83e\udd17Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.\n100-Days-Of-ML-Code 29.1k 100 Days of ML Coding\njulia 28.1k The Julia Language: A fresh approach to technical computing.\ngold-miner 26.6k \ud83e\udd47\u6398\u91d1\u7ffb\u8bd1\u8ba1\u5212\uff0c\u53ef\u80fd\u662f\u4e16\u754c\u6700\u5927\u6700\u597d\u7684\u82f1\u8bd1\u4e2d\u6280\u672f\u793e\u533a\uff0c\u6700\u61c2\u8bfb\u8005\u548c\u8bd1\u8005\u7684\u7ffb\u8bd1\u5e73\u53f0\uff1a\nawesome-scalability 26.6k The Patterns of Scalable, Reliable, and Performant Large-Scale Systems\nbasics 24.5k \ud83d\udcda Learn ML with clean code, simplified math and illustrative visuals.\nbert 23.9k TensorFlow code and pre-trained models for BERT\nfunNLP 22.1k (Machine Learning)NLP\u9762\u8bd5\u4e2d\u5e38\u8003\u5230\u7684\u77e5\u8bc6\u70b9\u548c\u4ee3\u7801\u5b9e\u73b0\u3001nlp4han:\u4e2d\u6587\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u96c6(\u65ad\u53e5/\u5206\u8bcd/\u8bcd\u6027\u6807\u6ce8/\u7ec4\u5757/\u53e5\u6cd5\u5206\u6790/\u8bed\u4e49\u5206\u6790/NER/N\u5143\u8bed\u6cd5/HMM/\u4ee3\u8bcd\u6d88\u89e3/\u60c5\u611f\u5206\u6790/\u62fc\u5199\u68c0\u67e5\u3001XLM\uff1aFace\u2026\nxgboost 19.4k Scalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or GBM) Library, for Python, R, Java, Scala, C++ and more. Runs on single machine, Hadoop, Spark, Flink and DataFlow\nReal-Time-Voice-Cloning 18.4k Clone a voice in 5 seconds to generate arbitrary speech in real-time\nd2l-zh 17.9k \u300a\u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60\u300b\uff1a\u9762\u5411\u4e2d\u6587\u8bfb\u8005\u3001\u80fd\u8fd0\u884c\u3001\u53ef\u8ba8\u8bba\u3002\u82f1\u6587\u7248\u5373\u4f2f\u514b\u5229\u201c\u6df1\u5ea6\u5b66\u4e60\u5bfc\u8bba\u201d\u6559\u6750\u3002\nopenpose 17.8k OpenPose: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation\nCoursera-ML-AndrewNg-Notes 17.7k \u5434\u6069\u8fbe\u8001\u5e08\u7684\u673a\u5668\u5b66\u4e60\u8bfe\u7a0b\u4e2a\u4eba\u7b14\u8bb0\nDeepFaceLab 17.3k DeepFaceLab is the leading software for creating deepfakes.\npytorch-tutorial 17.3k PyTorch Tutorial for Deep Learning Researchers\nMask_RCNN 17.2k Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow\nspaCy 16.8k \ud83d\udcab Industrial-strength Natural Language Processing (NLP) with Python and Cython\nNLP-progress 16.2k Repository to track the progress in Natural Language Processing (NLP), including the datasets and the current state-of-the-art for the most common NLP tasks.\n100-Days-Of-ML-Code 15.6k 100-Days-Of-ML-Code\u4e2d\u6587\u7248\ncs-video-courses 14.9k List of Computer Science courses with video lectures.\nWaveFunctionCollapse 14.7k Bitmap & tilemap generation from a single example with the help of ideas from quantum mechanics.\nlectures 14.7k Oxford Deep NLP 2017 course\nreinforcement-learning 14.7k Implementation of Reinforcement Learning Algorithms. Python, OpenAI Gym, Tensorflow. Exercises and Solutions to accom\u2026\npwc 14.7k Papers with code. Sorted by stars. Updated weekly.\nTensorFlow-Course 14.6k Simple and ready-to-use tutorials for TensorFlow\nDeepSpeech 14.4k A TensorFlow implementation of Baidu's DeepSpeech architecture\npumpkin-book 14k \u300a\u673a\u5668\u5b66\u4e60\u300b\uff08\u897f\u74dc\u4e66\uff09\u516c\u5f0f\u63a8\u5bfc\u89e3\u6790\uff0c\u5728\u7ebf\u9605\u8bfb\u5730\u5740\uff1ahttps://datawhalechina.github.io/pumpkin-book\ntfjs 13.5k A WebGL accelerated JavaScript library for training and deploying ML models.\nexamples 13.5k A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.\nopenface 13.5k Face recognition with deep neural networks.\nQix 13.3k Machine Learning\u3001Deep Learning\u3001PostgreSQL\u3001Distributed System\u3001Node.Js\u3001Golang\nspleeter 12.7k Deezer source separation library including pretrained models.\nVirgilio 12.7k Your new Mentor for Data Science E-Learning.\nnndl.github.io 12.7k \u300a\u795e\u7ecf\u7f51\u7edc\u4e0e\u6df1\u5ea6\u5b66\u4e60\u300b \u90b1\u9521\u9e4f\u8457 Neural Network and Deep Learning\nScreenshot-to-code 12.7k A neural network that transforms a design mock-up into a static website.\npytorch-CycleGAN-and-pix2pix 12.4k Image-to-Image Translation in PyTorch\npytorch-handbook 11.9k pytorch handbook\u662f\u4e00\u672c\u5f00\u6e90\u7684\u4e66\u7c4d\uff0c\u76ee\u6807\u662f\u5e2e\u52a9\u90a3\u4e9b\u5e0c\u671b\u548c\u4f7f\u7528PyTorch\u8fdb\u884c\u6df1\u5ea6\u5b66\u4e60\u5f00\u53d1\u548c\u7814\u7a76\u7684\u670b\u53cb\u5feb\u901f\u5165\u95e8\uff0c\u5176\u4e2d\u5305\u542b\u7684Pytorch\u6559\u7a0b\u5168\u90e8\u901a\u8fc7\u6d4b\u8bd5\u4fdd\u8bc1\u53ef\u4ee5\u6210\u529f\u8fd0\u884c\ngun 11.9k An open source cybersecurity protocol for syncing decentralized graph data.\nPaddle 11.8k PArallel Distributed Deep LEarning: Machine Learning Framework from Industrial Practice \uff08\u300e\u98de\u6868\u300f\u6838\u5fc3\u6846\u67b6\uff0c\u6df1\u5ea6\u5b66\u4e60&\u673a\u5668\u5b66\u4e60\u9ad8\u6027\u80fd\u5355\u673a\u3001\u5206\u5e03\u5f0f\u8bad\u2026\ntensorflow-zh 11.8k \u8c37\u6b4c\u5168\u65b0\u5f00\u6e90\u4eba\u5de5\u667a\u80fd\u7cfb\u7edfTensorFlow\u5b98\u65b9\u6587\u6863\u4e2d\u6587\u7248\ndarknet 11.4k YOLOv4 - Neural Networks for Object Detection (Windows and Linux version of Darknet )\nlearnopencv 11.4k Learn OpenCV : C++ and Python Examples\nneural-networks-and-deep-learning 11.3k Code samples for my book \"Neural Networks and Deep Learning\"\ngoogle-research 11.2k Google Research\nlabelImg 11.2k \ud83d\udd8d\ufe0f LabelImg is a graphical image annotation tool and label object bounding boxes in images\ngensim 11k Topic Modelling for Humans\npix2code 10.9k pix2code: Generating Code from a Graphical User Interface Screenshot\nfacenet 10.8k Face recognition using Tensorflow\nDeOldify 10.7k A Deep Learning based project for colorizing and restoring old images (and video!)\npython-machine-learning-book 10.7k The \"Python Machine Learning (1st edition)\" book code repository and info resource\nstanford-cs-229-machine-learning 10.6k VIP cheatsheets for Stanford's CS 229 Machine Learning\nmmdetection 10.5k OpenMMLab Detection Toolbox and Benchmark\nface-api.js 10.4k JavaScript API for face detection and face recognition in the browser and nodejs with tensorflow.js\nAwesome-pytorch-list 10.4k A comprehensive list of pytorch related content on github,such as different models,implementations,helper libraries,t\u2026\nnsfw_data_scraper 10.2k Collection of scripts to aggregate image data for the purposes of training an NSFW Image Classifier\nconvnetjs 10k Deep Learning in Javascript. Train Convolutional Neural Networks (or ordinary ones) in your browser.\nCycleGAN 9.8k Software that can generate photos from paintings, turn horses into zebras, perform style transfer, and more.\nstreamlit 9.8k Streamlit \u2014 The fastest way to build data apps in Python\nDeepCreamPy 9.7k Decensoring Hentai with Deep Neural Networks\nstylegan 9.7k StyleGAN - Official TensorFlow Implementation\nDive-into-DL-PyTorch 9.6k \u672c\u9879\u76ee\u5c06\u300a\u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60\u300b(Dive into Deep Learning)\u539f\u4e66\u4e2d\u7684MXNet\u5b9e\u73b0\u6539\u4e3aPyTorch\u5b9e\u73b0\u3002\nstanford-tensorflow-tutorials 9.6k This repository contains code examples for the Stanford's course: TensorFlow for Deep Learning Research.\nhorovod 9.6k Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.\nDeep-Learning-with-TensorFlow-book 9.4k \u6df1\u5ea6\u5b66\u4e60\u5165\u95e8\u5f00\u6e90\u4e66\uff0c\u57fa\u4e8eTensorFlow 2.0\u6848\u4f8b\u5b9e\u6218\u3002Open source Deep Learning book, based on TensorFlow 2.0 framework.\nneural-doodle 9.4k Turn your two-bit doodles into fine artworks with deep neural networks, generate seamless textures from photos, transfer style from one image to another, perform example-based upscaling, but wait... there's more! (An implementation of Semantic Style Transfer.)\ncaire 9.3k Content aware image resize library\nfast-style-transfer 9.2k TensorFlow CNN for fast style transfer \u26a1\ud83d\udda5\ud83c\udfa8\ud83d\uddbc\nncnn 9.2k ncnn is a high-performance neural network inference framework optimized for the mobile platform\nkubeflow 9.1k Machine Learning Toolkit for Kubernetes\nnltk 9k NLTK Source\nflair 9k A very simple framework for state-of-the-art Natural Language Processing (NLP)\nml-agents 9k Unity Machine Learning Agents Toolkit\nallennlp 8.8k An open-source NLP research library, built on PyTorch.\nbotpress 8.8k \ud83e\udd16 The Conversational Platform with built-in language understanding (NLU), beautiful graphical interface and Dialog Manager (DM). Easily create chatbots and AI-based virtual assistants.\nthe-gan-zoo 8.7k A list of all named GANs!\nEffectiveTensorflow 8.6k TensorFlow tutorials and best practices.\ntfjs-core 8.5k WebGL-accelerated ML // linear algebra // automatic differentiation for JavaScript.\nfairseq 8.4k Facebook AI Research Sequence-to-Sequence Toolkit written in Python.\nsonnet 8.4k TensorFlow-based neural network library\nmit-deep-learning-book-pdf 8.3k MIT Deep Learning Book in PDF format (complete and parts) by Ian Goodfellow, Yoshua Bengio and Aaron Courville\nTensorFlow-Tutorials 8.3k TensorFlow Tutorials with YouTube Videos\npytorch_geometric 8.2k Geometric Deep Learning Extension Library for PyTorch\ntutorials 8.2k \u673a\u5668\u5b66\u4e60\u76f8\u5173\u6559\u7a0b\nfashion-mnist 8k A MNIST-like fashion product database. Benchmark \ud83d\udc49\nbert-as-service 7.9k Mapping a variable-length sentence to a fixed-length vector using BERT model\npix2pix 7.8k Image-to-image translation with conditional adversarial nets\nmediapipe 7.7k MediaPipe is the simplest way for researchers and developers to build world-class ML solutions and applications for mobile, edge, cloud and the web.\nrecommenders 7.7k Best Practices on Recommendation Systems\nmit-deep-learning 7.7k Tutorials, assignments, and competitions for MIT Deep Learning related courses.\npytorch-book 7.6k PyTorch tutorials and fun projects including neural talk, neural style, poem writing, anime generation (\u300a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6PyTorch\uff1a\u5165\u95e8\u4e0e\u5b9e\u6218\u300b)\nWinds 7.6k A Beautiful Open Source RSS & Podcast App Powered by Getstream.io\nvid2vid 7.4k Pytorch implementation of our method for high-resolution (e.g. 2048x1024) photorealistic video-to-video translation.\nLearn_Machine_Learning_in_3_Months 7.3k This is the code for \"Learn Machine Learning in 3 Months\" by Siraj Raval on Youtube\ngolearn 7.3k Machine Learning for Go\nKeras-GAN 7.2k Keras implementations of Generative Adversarial Networks.\nmlcourse.ai 7k Open Machine Learning Course\nfaceai 7k \u4e00\u6b3e\u5165\u95e8\u7ea7\u7684\u4eba\u8138\u3001\u89c6\u9891\u3001\u6587\u5b57\u68c0\u6d4b\u4ee5\u53ca\u8bc6\u522b\u7684\u9879\u76ee.\npysc2 6.9k StarCraft II Learning Environment\npretrained-models.pytorch 6.9k Pretrained ConvNets for pytorch: NASNet, ResNeXt, ResNet, InceptionV4, InceptionResnetV2, Xception, DPN, etc.\nPyTorch-GAN 6.7k PyTorch implementations of Generative Adversarial Networks.\nvision 6.7k Datasets, Transforms and Models specific to Computer Vision\nnlp-tutorial 6.6k Natural Language Processing Tutorial for Deep Learning Researchers\nbullet3 6.6k Bullet Physics SDK: real-time collision detection and multi-physics simulation for VR, games, visual effects, robotics,\nDCGAN-tensorflow 6.6k A tensorflow implementation of \"Deep Convolutional Generative Adversarial Networks\"\ntfjs-models 6.5k Pretrained models for TensorFlow.js\nabu 6.5k \u963f\u5e03\u91cf\u5316\u4ea4\u6613\u7cfb\u7edf(\u80a1\u7968\uff0c\u671f\u6743\uff0c\u671f\u8d27\uff0c\u6bd4\u7279\u5e01\uff0c\u673a\u5668\u5b66\u4e60) \u57fa\u4e8epython\u7684\u5f00\u6e90\u91cf\u5316\u4ea4\u6613\uff0c\u91cf\u5316\u6295\u8d44\u67b6\u6784\npytorch-lightning 6.5k The lightweight PyTorch wrapper for ML researchers. Scale your models. Write less boilerplate\ntensorboardX 6.4k tensorboard for pytorch (and chainer, mxnet, numpy, ...)\nmachine-learning-course 6.4k \ud83d\udcac Machine Learning Course with Python:\nguess 6.3k \ud83d\udd2e Libraries & tools for enabling Machine Learning driven user-experiences on the web\npyro 6.3k Deep universal probabilistic programming with Python and PyTorch\nlab 6.2k A customisable 3D platform for agent-based AI research\nmml-book.github.io 6.2k Companion webpage to the book \"Mathematics For Machine Learning\"\nInterview 6.2k Interview = \u7b80\u5386\u6307\u5357 + LeetCode + Kaggle\ntensorlayer 6.2k Deep Learning and Reinforcement Learning Library for Scientists and Engineers \ud83d\udd25\ngenerative-models 6.1k Collection of generative models, e.g. GAN, VAE in Pytorch and Tensorflow.\nmachine-learning-yearning-cn 6.1k Machine Learning Yearning \u4e2d\u6587\u7248 - \u300a\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u79d8\u7c4d\u300b - Andrew Ng \u8457\nkeras-yolo3 6k A Keras implementation of YOLOv3 (Tensorflow backend)\nBossSensor 5.9k Hide screen when boss is approaching.\ntensorflow2_tutorials_chinese 5.9k tensorflow2\u4e2d\u6587\u6559\u7a0b\uff0c\u6301\u7eed\u66f4\u65b0(\u5f53\u524d\u7248\u672c:tensorflow2.0)\uff0ctag: tensorflow 2.0 tutorials\nTensorFlow-Tutorials 5.9k Simple tutorials using Google's TensorFlow Framework\nargo 5.9k Argo Workflows: Get stuff done with Kubernetes.\npython-machine-learning-book-2nd-edition 5.8k The \"Python Machine Learning (2nd edition)\" book code repository and info resource\ndvc 5.7k \ud83e\udd89Data Version Control\nEasyPR 5.7k An easy, flexible, and accurate plate recognition project for Chinese licenses in unconstrained situations.\nAdversarialNetsPapers 5.6k The classical paper list with code about generative adversarial nets\ntensorpack 5.6k A Neural Net Training Interface on TensorFlow, with focus on speed + flexibility\nphotoprism 5.6k Personal Photo Management powered by Go and Google TensorFlow\ntensorflow_cookbook 5.6k Code for Tensorflow Machine Learning Cookbook\nalbumentations 5.6k fast image augmentation library and easy to use wrapper around other libraries\nswift 5.6k Swift for TensorFlow\ndarkflow 5.6k Translate darknet to tensorflow. Load trained weights, retrain/fine-tune using tensorflow, export constant graph def to mobile devices\ntensorflow_tutorials 5.5k From the basics to slightly more interesting applications of Tensorflow\ndeep-learning-coursera 5.5k Deep Learning Specialization by Andrew Ng on Coursera.\ntransferlearning 5.5k Everything about Transfer Learning and Domain Adaptation--\u8fc1\u79fb\u5b66\u4e60\nML-NLP 5.5k \u6b64\u9879\u76ee\u662f\u673a\u5668\u5b66\u4e60(Machine Learning)\u3001\u6df1\u5ea6\u5b66\u4e60(Deep Learning)\u3001NLP\u9762\u8bd5\u4e2d\u5e38\u8003\u5230\u7684\u77e5\u8bc6\u70b9\u548c\u4ee3\u7801\u5b9e\u73b0\uff0c\u4e5f\u662f\u4f5c\u4e3a\u4e00\u4e2a\u7b97\u6cd5\u5de5\u7a0b\u5e08\u5fc5\u4f1a\u7684\u7406\u8bba\u57fa\u7840\u77e5\u8bc6\u3002\nnmt 5.5k TensorFlow Neural Machine Translation Tutorial\nfaster-rcnn.pytorch 5.5k A faster pytorch implementation of faster r-cnn\nUGATIT 5.4k Official Tensorflow implementation of U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Inst\u2026\npandas-profiling 5.4k Create HTML profiling reports from pandas DataFrame objects\ndeep-residual-networks 5.4k Deep Residual Learning for Image Recognition\nxlnet 5.3k XLNet: Generalized Autoregressive Pretraining for Language Understanding\nleeml-notes 5.2k \u674e\u5b8f\u6bc5\u300a\u673a\u5668\u5b66\u4e60\u300b\u7b14\u8bb0\uff0c\u5728\u7ebf\u9605\u8bfb\u5730\u5740\uff1ahttps://datawhalechina.github.io/leeml-notes\nwav2letter 5.2k Facebook AI Research's Automatic Speech Recognition Toolkit\nneural-style 5.2k Neural style in TensorFlow! \ud83c\udfa8\nCVPR2020-Paper-Code-Interpretation 5.2k cvpr2020/cvpr2019\uff0fcvpr2018/cvpr2017 papers\uff0c\u6781\u5e02\u56e2\u961f\u6574\u7406\nTensorFlow-2.x-Tutorials 5.2k TensorFlow 2.x version's Tutorials and Examples, including CNN, RNN, GAN, Auto-Encoders, FasterRCNN, GPT, BERT exampl\u2026\nyolov3 5.2k YOLOv3 in PyTorch > ONNX > CoreML > iOS\ncnn-text-classification-tf 5.2k Convolutional Neural Network for Text Classification in Tensorflow\nseq2seq 5.2k A general-purpose encoder-decoder framework for Tensorflow\nchineseocr_lite 5.1k \u8d85\u8f7b\u91cf\u7ea7\u4e2d\u6587ocr\uff0c\u652f\u6301\u7ad6\u6392\u6587\u5b57\u8bc6\u522b, \u652f\u6301ncnn\u63a8\u7406 , dbnet(1.7M) + crnn(6.3M) + anglenet(1.5M) \u603b\u6a21\u578b\u4ec510M\nfeaturetools 5k An open source python library for automated feature engineering\nlabelme 5k Image Polygonal Annotation with Python (polygon, rectangle, circle, line, point and image-level flag annotation).\nImageAI 5k A python library built to empower developers to build applications and systems with self-contained Computer Vision capabilities\nnlp-recipes 5k Natural Language Processing Best Practices & Examples\nhave-fun-with-machine-learning 4.9k An absolute beginner's guide to Machine Learning and Image Classification with Neural Networks\neat_tensorflow2_in_30_days 4.9k Tensorflow2.0 \ud83c\udf4e\ud83c\udf4a is delicious, just eat it! \ud83d\ude0b\ud83d\ude0b\ntensorflow-wavenet 4.9k A TensorFlow implementation of DeepMind's WaveNet paper\nPyTorch-Tutorial 4.9k Build your neural network easy and fast\nstylegan2 4.9k StyleGAN2 - Official TensorFlow Implementation\nh2o-3 4.9k Open Source Fast Scalable Machine Learning Platform For Smarter Applications: Deep Learning, Gradient Boosting & XGBo\u2026\nawesome-machine-learning-on-source-code 4.8k Cool links & research papers related to Machine Learning applied to source code (MLonCode)\nLearning-to-See-in-the-Dark 4.8k Learning to See in the Dark. CVPR 2018\nPyTorch-YOLOv3 4.8k Minimal PyTorch implementation of YOLOv3\nfirst-order-model 4.8k This repository contains the source code for the paper First Order Motion Model for Image Animation\nmodels 4.8k Pre-trained and Reproduced Deep Learning Models \uff08\u300e\u98de\u6868\u300f\u5b98\u65b9\u6a21\u578b\u5e93\uff0c\u5305\u542b\u591a\u79cd\u5b66\u672f\u524d\u6cbf\u548c\u5de5\u4e1a\u573a\u666f\u9a8c\u8bc1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff09\nsmile 4.8k Statistical Machine Intelligence & Learning Engine\nkeras-js 4.7k Run Keras models in the browser, with GPU support using WebGL\ncarla 4.7k Open-source simulator for autonomous driving research.\nkeras-rl 4.7k Deep Reinforcement Learning for Keras.\nuseful-java-links 4.7k A list of useful Java frameworks, libraries, software and hello worlds examples\nAwesome-CoreML-Models 4.7k Largest list of models for Core ML (for iOS 11+)\npython-small-examples 4.7k \u544a\u522b\u67af\u71e5\uff0c\u81f4\u529b\u4e8e\u6253\u9020 Python \u5bcc\u6709\u4f53\u7cfb\u4e14\u5b9e\u7528\u7684\u5c0f\u4f8b\u5b50\u3001\u5c0f\u6848\u4f8b\u3002\ngcn 4.7k Implementation of Graph Convolutional Networks in TensorFlow\nintroduction_to_ml_with_python 4.7k Notebooks and code for the book \"Introduction to Machine Learning with Python\"\nstargan 4.6k StarGAN - Official PyTorch Implementation (CVPR 2018)\npix2pixHD 4.6k Synthesizing and manipulating 2048x1024 images with conditional GANs\nData-Science-Wiki 4.6k A wiki of DataScience, Statistics, Maths, R,Python, AI, Machine Learning, Automation, Devops Tools, Bash, Linux Tutor\u2026\nMVision 4.6k \u673a\u5668\u4eba\u89c6\u89c9 \u79fb\u52a8\u673a\u5668\u4eba VS-SLAM ORB-SLAM2 \u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b yolov3 \u884c\u4e3a\u68c0\u6d4b opencv PCL \u673a\u5668\u5b66\u4e60 \u65e0\u4eba\u9a7e\u9a76\ncleverhans 4.6k An adversarial example library for constructing attacks, building defenses, and benchmarking both\nvaex 4.6k Out-of-Core DataFrames for Python, ML, visualize and explore big tabular data at a billion rows per second \ud83d\ude80\nGrokking-Deep-Learning 4.6k this repository accompanies the book \"Grokking Deep Learning\"\ntrax 4.5k Trax \u2014 Deep Learning with Clear Code and Speed\ngraph_nets 4.5k Build Graph Nets in Tensorflow\nedward 4.5k A probabilistic programming language in TensorFlow. Deep generative models, variational inference.\nTensorFlow-World 4.5k \ud83c\udf0e Simple and ready-to-use tutorials for TensorFlow\nimbalanced-learn 4.5k A Python Package to Tackle the Curse of Imbalanced Datasets in Machine Learning\nmachine-learning-mindmap 4.5k A mindmap summarising Machine Learning concepts, from Data Analysis to Deep Learning.\nseq2seq-couplet 4.5k Play couplet with seq2seq model. \u7528\u6df1\u5ea6\u5b66\u4e60\u5bf9\u5bf9\u8054\u3002\nEfficientNet-PyTorch 4.4k A PyTorch implementation of EfficientNet\nTensorFlow-Book 4.4k Accompanying source code for Machine Learning with TensorFlow. Refer to the book for step-by-step explanations.\nstanza 4.4k Official Stanford NLP Python Library for Many Human Languages\namazon-dsstne 4.4k Deep Scalable Sparse Tensor Network Engine (DSSTNE) is an Amazon developed library for building Deep Learning (DL) ma\u2026\ncnn-explainer 4.4k Learning Convolutional Neural Networks with Interactive Visualization.\nRealtime_Multi-Person_Pose_Estimation 4.4k Code repo for realtime multi-person pose estimation in CVPR'17 (Oral)\nstanford-cs-230-deep-learning 4.3k VIP cheatsheets for Stanford's CS 230 Deep Learning\nReal-Time-Person-Removal 4.3k Removing people from complex backgrounds in real time using TensorFlow.js in the web browser\nOpenNMT-py 4.3k Open Source Neural Machine Translation in PyTorch\ntensorflow_practice 4.3k tensorflow\u5b9e\u6218\u7ec3\u4e60\uff0c\u5305\u62ec\u5f3a\u5316\u5b66\u4e60\u3001\u63a8\u8350\u7cfb\u7edf\u3001nlp\u7b49\npytorch-cnn-visualizations 4.2k Pytorch implementation of convolutional neural network visualization techniques\ntensorspace 4.2k Neural network 3D visualization framework, build interactive and intuitive model in browsers, support pre-trained deep \u2026\nDeepLearningExamples 4.2k Deep Learning Examples\nsketch-code 4.2k Keras model to generate HTML code from hand-drawn website mockups. Implements an image captioning architecture to dra\u2026\ndeeplearning-papernotes 4.2k Summaries and notes on Deep Learning research papers\napex 4.2k A PyTorch Extension: Tools for easy mixed precision and distributed training in Pytorch\nAlphaPose 4.1k Real-Time and Accurate Multi-Person Pose Estimation&Tracking System\nattention-is-all-you-need-pytorch 4.1k A PyTorch implementation of the Transformer model in \"Attention is All You Need\".\nnmap 4.1k Nmap - the Network Mapper. Github mirror of official SVN repository.\nMachine-learning-learning-notes 4.1k \u5468\u5fd7\u534e\u300a\u673a\u5668\u5b66\u4e60\u300b\u53c8\u79f0\u897f\u74dc\u4e66\u662f\u4e00\u672c\u8f83\u4e3a\u5168\u9762\u7684\u4e66\u7c4d\uff0c\u4e66\u4e2d\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u673a\u5668\u5b66\u4e60\u9886\u57df\u4e0d\u540c\u7c7b\u578b\u7684\u7b97\u6cd5(\u4f8b\u5982\uff1a\u76d1\u7763\u5b66\u4e60\u3001\u65e0\u76d1\u7763\u5b66\u4e60\u3001\u534a\u76d1\u7763\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u96c6\u6210\u964d\u7ef4\u3001\u7279\u5f81\u9009\u62e9\u7b49)\uff0c\u8bb0\u5f55\u4e86\u672c\u4eba\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u7406\u89e3\u601d\u8def\u4e0e\u6269\u5c55\u77e5\u8bc6\u70b9\uff0c\u5e0c\u671b\u5bf9\u65b0\u4eba\u9605\u8bfb\u897f\u74dc\u4e66\u6709\u2026\nserenata-de-amor 4.1k \ud83d\udd75 Artificial Intelligence for social control of public administration\npractical-pytorch 4.1k DEPRECATED and not maintained - see official repo at https://github.com/pytorch/tutorials\npytorch-image-models 4.1k PyTorch image models, scripts, pretrained weights -- (SE)ResNet/ResNeXT, DPN, EfficientNet, MixNet, MobileNet-V3/V2, MNASNet, Single-Path NAS, FBNet, and more\nface-alignment 4k \ud83d\udd25 2D and 3D Face alignment library build using pytorch\nlearning-to-learn 4k Learning to Learn in TensorFlow\nmachine-learning-notes 4k My continuously updated Machine Learning, Probabilistic Models and Deep Learning notes and demos (1500+ slides) \u6211\u4e0d\u95f4\u65ad\u66f4\u2026\numap 4k Uniform Manifold Approximation and Projection\nDeepLearningZeroToAll 4k TensorFlow Basic Tutorial Labs\ngluon-cv 4k Gluon CV Toolkit\npipeline 4k PipelineAI Kubeflow Distribution\nsnorkel 4k A system for quickly generating training data with weak supervision\nDIGITS 4k Deep Learning GPU Training System\nDenseNet 4k Densely Connected Convolutional Networks, In CVPR 2017 (Best Paper Award).\nawesome-project-ideas 4k Curated list of Machine Learning, NLP, Vision, Recommender Systems Project Ideas\ntutorials 4k PyTorch tutorials.\nDeep-Learning-21-Examples 3.9k \u300a21\u4e2a\u9879\u76ee\u73a9\u8f6c\u6df1\u5ea6\u5b66\u4e60\u2014\u2014\u2014\u57fa\u4e8eTensorFlow\u7684\u5b9e\u8df5\u8be6\u89e3\u300b\u914d\u5957\u4ee3\u7801\nDeepLearningTutorials 3.9k Deep Learning Tutorial notes and code. See the wiki for more info.\ntextgenrnn 3.9k Easily train your own text-generating neural network of any size and complexity on any text dataset with a few lines \u2026\nlucid 3.8k A collection of infrastructure and tools for research in neural network interpretability.\nnsfwjs 3.8k NSFW detection on the client-side via TensorFlow.js\nssd.pytorch 3.8k A PyTorch Implementation of Single Shot MultiBox Detector\nMachineLearning 3.8k Basic Machine Learning and Deep Learning\nTensorflow-Tutorial 3.8k Tensorflow tutorial from basic to hard\nawesome-ml-for-cybersecurity 3.8k Machine Learning for Cyber Security\ndaily-paper-computer-vision 3.8k \u8bb0\u5f55\u6bcf\u5929\u6574\u7406\u7684\u8ba1\u7b97\u673a\u89c6\u89c9/\u6df1\u5ea6\u5b66\u4e60/\u673a\u5668\u5b66\u4e60\u76f8\u5173\u65b9\u5411\u7684\u8bba\u6587\nSSD-Tensorflow 3.8k Single Shot MultiBox Detector in TensorFlow\ncvat 3.8k Powerful and efficient Computer Vision Annotation Tool (CVAT)\ndeep-learning-roadmap 3.8k \ud83d\udce1 All You Need to Know About Deep Learning - A kick-starter\nsqlflow 3.8k Brings SQL and AI together.\nmmf 3.7k A modular framework for vision & language multimodal research from Facebook AI Research (FAIR)\ntensorflow-docs 3.7k TensorFlow \u6700\u65b0\u5b98\u65b9\u6587\u6863\u4e2d\u6587\u7248\niGAN 3.7k Interactive Image Generation via Generative Adversarial Networks\nCapsNet-Tensorflow 3.7k A Tensorflow implementation of CapsNet(Capsules Net) in paper Dynamic Routing Between Capsules\nYet-Another-EfficientDet-Pytorch 3.6k The pytorch re-implement of the official efficientdet with SOTA performance in real time and pretrained weights.\npytorch-examples 3.6k Simple examples to introduce PyTorch\nML_for_Hackers 3.6k Code accompanying the book \"Machine Learning for Hackers\"\ndocs 3.6k TensorFlow documentation\ntensorflow-generative-model-collections 3.6k Collection of generative models in Tensorflow\nDeepLearning.ai-Summary 3.6k This repository contains my personal notes and summaries on DeepLearning.ai specialization courses. I've enjoyed ever\u2026\nBERT-pytorch 3.6k Google AI 2018 BERT pytorch implementation\npwnagotchi 3.5k (\u2310\u25a0_\u25a0) - Deep Reinforcement Learning instrumenting bettercap for WiFi pwning.\nHyperLPR 3.5k \u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u9ad8\u6027\u80fd\u4e2d\u6587\u8f66\u724c\u8bc6\u522b High Performance Chinese License Plate Recognition Framework.\ndeep-learning 3.5k Repo for the Deep Learning Nanodegree Foundations program.\nTensorFlowOnSpark 3.5k TensorFlowOnSpark brings TensorFlow programs to Apache Spark clusters.\nBigDL 3.5k BigDL: Distributed Deep Learning Framework for Apache Spark\nAlgoWiki 3.5k Repository which contains links and resources on different topics of Computer Science.\nexamples 3.5k TensorFlow examples\ntf-faster-rcnn 3.4k Tensorflow Faster RCNN for Object Detection\ntf-pose-estimation 3.4k Deep Pose Estimation implemented using Tensorflow with Custom Architectures for fast inference.\nawesome-machine-learning-cn 3.4k \u673a\u5668\u5b66\u4e60\u8d44\u6e90\u5927\u5168\u4e2d\u6587\u7248\uff0c\u5305\u62ec\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u6846\u67b6\u3001\u5e93\u4ee5\u53ca\u8f6f\u4ef6\nmetaflow 3.4k Build and manage real-life data science projects with ease.\ndeep-reinforcement-learning 3.3k Repo for the Deep Reinforcement Learning Nanodegree program\nsemantic-segmentation-pytorch 3.3k Pytorch implementation for Semantic Segmentation/Scene Parsing on MIT ADE20K dataset\ngocv 3.3k Go package for computer vision using OpenCV 4 and beyond.\nd2l-pytorch 3.3k This project reproduces the book Dive Into Deep Learning (www.d2l.ai), adapting the code from MXNet into PyTorch.\nChinese-BERT-wwm 3.3k Pre-Training with Whole Word Masking for Chinese BERT\uff08\u4e2d\u6587BERT-wwm\u7cfb\u5217\u6a21\u578b\uff09\nSmartCropper 3.3k \ud83d\udd25 A library for cropping image in a smart way that can identify the border and correct the cropped image. \u667a\u80fd\u56fe\u7247\u88c1\u526a\u6846\u67b6\u3002\u81ea\u52a8\u8bc6\u522b\u8fb9\u6846\uff0c\u624b\u52a8\u8c03\u8282\u9009\u533a\uff0c\u4f7f\u7528\u900f\u89c6\u53d8\u6362\u88c1\u526a\u5e76\u77eb\u6b63\u9009\u533a\uff1b\u9002\u7528\u4e8e\u8eab\u4efd\u8bc1\uff0c\u540d\u7247\uff0c\u6587\u6863\u7b49\u7167\u7247\u7684\u88c1\u526a\u3002\nPyTorchZeroToAll 3.3k Simple PyTorch Tutorials Zero to ALL!\npyAudioAnalysis 3.3k Python Audio Analysis Library: Feature Extraction, Classification, Segmentation and Applications\nInterpretableMLBook 3.3k \u300a\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60--\u9ed1\u76d2\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7406\u89e3\u6307\u5357\u300b\uff0c\u8be5\u4e66\u4e3a\u300aInterpretable Machine Learning\u300b\u4e2d\u6587\u7248\nsnips-nlu 3.3k Snips Python library to extract meaning from text\npyod 3.3k A Python Toolbox for Scalable Outlier Detection (Anomaly Detection)\nDeepLearning 3.2k \u6df1\u5ea6\u5b66\u4e60\u5165\u95e8\u6559\u7a0b, \u4f18\u79c0\u6587\u7ae0, Deep Learning Tutorial\nvespa 3.2k Vespa is an engine for low-latency computation over large data sets.\ndeep-voice-conversion 3.2k Deep neural networks for voice conversion (voice style transfer) in Tensorflow\nlightfm 3.2k A Python implementation of LightFM, a hybrid recommendation algorithm.\nmachine-learning 3.2k Content for Udacity's Machine Learning curriculum\nskflow 3.2k Simplified interface for TensorFlow (mimicking Scikit Learn) for Deep Learning\nTensorflow-Project-Template 3.2k A best practice for tensorflow project template architecture.\nEasyOCR 3.2k Ready-to-use OCR with 40+ languages supported including Chinese, Japanese, Korean and Thai\ntext-classification-cnn-rnn 3.1k CNN-RNN\u4e2d\u6587\u6587\u672c\u5206\u7c7b\uff0c\u57fa\u4e8eTensorFlow\nMachineLearning_Python 3.1k \u673a\u5668\u5b66\u4e60\u7b97\u6cd5python\u5b9e\u73b0\nimagededup 3.1k \ud83d\ude0e Finding duplicate images made easy!\nMatchZoo 3.1k Facilitating the design, comparison and sharing of deep text matching models.\ntransformer 3.1k A TensorFlow Implementation of the Transformer: Attention Is All You Need\ntensorflow_poems 3.1k \u4e2d\u6587\u53e4\u8bd7\u81ea\u52a8\u4f5c\u8bd7\u673a\u5668\u4eba\uff0c\u5c4c\u70b8\u5929\uff0c\u57fa\u4e8etensorflow1.10 api\uff0c\u6b63\u5728\u79ef\u6781\u7ef4\u62a4\u5347\u7ea7\u4e2d\uff0c\u5febstar\uff0c\u4fdd\u6301\u66f4\u65b0\uff01\nDeep-Learning-Roadmap 3.1k \ud83d\udce1 Organized Resources for Deep Learning Researchers and Developers\nlabel-studio 3.1k Label Studio is a multi-type data labeling and annotation tool with standardized output format\nASRT_SpeechRecognition 3.1k A Deep-Learning-Based Chinese Speech Recognition System \u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e2d\u6587\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\nbenchmark_results 3.1k Visual Tracking Paper List\nMachine-Learning 3k \u26a1\u673a\u5668\u5b66\u4e60\u5b9e\u6218\uff08Python3\uff09\uff1akNN\u3001\u51b3\u7b56\u6811\u3001\u8d1d\u53f6\u65af\u3001\u903b\u8f91\u56de\u5f52\u3001SVM\u3001\u7ebf\u6027\u56de\u5f52\u3001\u6811\u56de\u5f52\nyolact 3k A simple, fully convolutional model for real-time instance segmentation.\ntrfl 3k TensorFlow Reinforcement Learning\npytorch-cifar 3k 95.16% on CIFAR10 with PyTorch\nsacred 3k Sacred is a tool to help you configure, organize, log and reproduce experiments developed at IDSIA.\nyolov5 3k YOLOv5 in PyTorch > ONNX > CoreML > iOS\nReinforcement-Learning 3k Learn Deep Reinforcement Learning in 60 days! Lectures & Code in Python. Reinforcement Learning + Deep Learning\ndistiller 3k Neural Network Distiller by Intel AI Lab: a Python package for neural network compression research. https://nervanasy\u2026\nFastMaskRCNN 3k Mask RCNN in TensorFlow\nprobability 3k Probabilistic reasoning and statistical analysis in TensorFlow\nDeepVideoAnalytics 2.9k A distributed visual search and visual data analytics platform.\ntensorwatch 2.9k Debugging, monitoring and visualization for Python Machine Learning and Data Science\ndarts 2.9k Differentiable architecture search for convolutional and recurrent networks\ncomputervision-recipes 2.9k Best Practices, code samples, and documentation for Computer Vision.\ntext-detection-ctpn 2.9k text detection mainly based on ctpn model in tensorflow, id card detect, connectionist text proposal network\ntensorflow-windows-wheel 2.9k Tensorflow prebuilt binary for Windows\ntensorflow-yolov3 2.9k \ud83d\udd25 Pure tensorflow Implement of YOLOv3 with support to train your own dataset\nzhihu 2.9k This repo contains the source code in my personal column (https://zhuanlan.zhihu.com/zhaoyeyu), implemented using Python 3.6. Including Natural Language Processing and Computer Vision projects, such as text generation, machine translation, deep convolution GAN and other actual combat code.\nBERT-BiLSTM-CRF-NER 2.9k Tensorflow solution of NER task Using BiLSTM-CRF model with Google BERT Fine-tuning And private Server services\nTensorFlowSharp 2.9k TensorFlow API for .NET languages\nignite 2.9k High-level library to help with training and evaluating neural networks in PyTorch flexibly and transparently.\ntensorflow-tutorial 2.9k Example TensorFlow codes and Caicloud TensorFlow as a Service dev environment.\n100-Days-of-ML-Code-Chinese-Version 2.9k Chinese Translation for Machine Learning Infographics\ndeep-learning-papers-translation 2.9k \u6df1\u5ea6\u5b66\u4e60\u8bba\u6587\u7ffb\u8bd1\uff0c\u5305\u62ec\u5206\u7c7b\u8bba\u6587\uff0c\u68c0\u6d4b\u8bba\u6587\u7b49\nDMTK 2.8k Microsoft Distributed Machine Learning Toolkit\ncaffe-tensorflow 2.8k Caffe models in TensorFlow\nlibpostal 2.8k A C library for parsing/normalizing street addresses around the world. Powered by statistical NLP and open geo data.\npigo 2.8k Pure Go face detection, pupil/eyes localization and facial landmark points detection library\nmindsdb 2.8k Machine Learning in one line of code\nTensorflow-Cookbook 2.8k Simple Tensorflow Cookbook for easy-to-use\neasy-tensorflow 2.8k Simple and comprehensive tutorials in TensorFlow\nDeepLearning 2.8k Deep Learning (Python, C, C++, Java, Scala, Go)\npytorch-yolo-v3 2.8k A PyTorch implementation of the YOLO v3 object detection algorithm\njukebox 2.8k Code for the paper \"Jukebox: A Generative Model for Music\"\nmakegirlsmoe_web 2.8k Create Anime Characters with MakeGirlsMoe\ndeep-learning-keras-tensorflow 2.8k Introduction to Deep Neural Networks with Keras and Tensorflow\nSiamMask 2.7k [CVPR2019] Fast Online Object Tracking and Segmentation: A Unifying Approach\ntencent-ml-images 2.7k Largest multi-label image database; ResNet-101 model; 80.73% top-1 acc on ImageNet\nDALI 2.7k A library containing both highly optimized building blocks and an execution engine for data pre-processing in deep le\u2026\nshogun 2.7k Sh\u014dgun\noptuna 2.7k A hyperparameter optimization framework\nAutomatic_Speech_Recognition 2.7k End-to-end Automatic Speech Recognition for Madarian and English in Tensorflow\npytorch-semseg 2.7k Semantic Segmentation Architectures Implemented in PyTorch\npygcn 2.7k Graph Convolutional Networks in PyTorch\ndeep-learning-book 2.7k Repository for \"Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in \u2026\npointnet 2.7k PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation\nCVPR2020-Code 2.7k CVPR 2020 \u8bba\u6587\u5f00\u6e90\u9879\u76ee\u5408\u96c6\nDetectron.pytorch 2.7k A pytorch implementation of Detectron. Both training from scratch and inferring directly from pretrained Detectron we\u2026\nLSTM-Human-Activity-Recognition 2.6k Human Activity Recognition example using TensorFlow on smartphone sensors dataset and an LSTM RNN. Classifying the type of movement amongst six activity categories - Guillaume Chevalier\nneural-style-tf 2.6k TensorFlow (Python API) implementation of Neural Style\nPytorch-UNet 2.6k PyTorch implementation of the U-Net for image semantic segmentation with high quality images\nkornia 2.6k Open Source Differentiable Computer Vision Library for PyTorch\nAd-papers 2.6k Papers on Computational Advertising\ndeep-high-resolution-net.pytorch 2.6k The project is an official implementation of our CVPR2019 paper \"Deep High-Resolution Representation Learning for Hum\u2026\nVoTT 2.6k Visual Object Tagging Tool: An electron app for building end to end Object Detection Models from Images and Videos.\nespnet 2.6k End-to-End Speech Processing Toolkit\nltp 2.6k Language Technology Platform\nLearn_Deep_Learning_in_6_Weeks 2.6k This is the Curriculum for \"Learn Deep Learning in 6 Weeks\" by Siraj Raval on Youtube\nkeras-vis 2.6k Neural network visualization toolkit for keras\nonnxruntime 2.6k ONNX Runtime: cross-platform, high performance ML inferencing and training accelerator\n3DDFA 2.6k The PyTorch improved version of TPAMI 2017 paper: Face Alignment in Full Pose Range: A 3D Total Solution.\nolivia 2.6k \ud83d\udc81\u200d\u2640\ufe0fYour new best friend powered by an artificial neural network\nalbert_zh 2.6k A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS, \u6d77\u91cf\u4e2d\u6587\u9884\u8bad\u7ec3ALBERT\u6a21\u578b\nai-deadlines 2.6k \u23f0 AI conference deadline countdowns\nopencvsharp 2.5k .NET Framework wrapper for OpenCV\ntelegram-list 2.5k List of telegram groups, channels & bots // \u0421\u043f\u0438\u0441\u043e\u043a \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u044b\u0445 \u0433\u0440\u0443\u043f\u043f, \u043a\u0430\u043d\u0430\u043b\u043e\u0432 \u0438 \u0431\u043e\u0442\u043e\u0432 \u0442\u0435\u043b\u0435\u0433\u0440\u0430\u043c\u0430 // \u0421\u043f\u0438\u0441\u043e\u043a \u0447\u0430\u0442\u043e\u0432 \u0434\u043b\u044f \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0441\u0442\u043e\u0432\neasy12306 2.5k \u4f7f\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5b8c\u6210\u5bf912306\u9a8c\u8bc1\u7801\u7684\u81ea\u52a8\u8bc6\u522b\nrust 2.5k Rust language bindings for TensorFlow\nmiles-deep 2.5k Deep Learning Porn Video Classifier/Editor with Caffe\nVisualDL 2.5k Deep Learning Visualization Toolkit\uff08\u300e\u98de\u6868\u300f\u6df1\u5ea6\u5b66\u4e60\u53ef\u89c6\u5316\u5de5\u5177 \uff09\nSinGAN 2.5k Official pytorch implementation of the paper: \"SinGAN: Learning a Generative Model from a Single Natural Image\"\nt81_558_deep_learning 2.5k Washington University (in St. Louis) Course T81-558: Applications of Deep Neural Networks\ntraining 2.5k \ud83d\udc1d Custom Object Detection and Classification Training\nPocketFlow 2.5k An Automatic Model Compression (AutoMC) framework for developing smaller and faster AI applications.\nautogluon 2.5k AutoGluon: AutoML Toolkit for Deep Learning\nsimple-faster-rcnn-pytorch 2.5k A simplified implemention of Faster R-CNN that replicate performance from origin paper\nMITIE 2.5k MITIE: library and tools for information extraction\nreinforcement-learning 2.5k Minimal and Clean Reinforcement Learning Examples\nISLR-python 2.4k An Introduction to Statistical Learning (James, Witten, Hastie, Tibshirani, 2013): Python code\nEAST 2.4k A tensorflow implementation of EAST text detector\nDeepNLP-models-Pytorch 2.4k Pytorch implementations of various Deep NLP models in cs-224n(Stanford Univ)\nMachine-Learning-with-Python 2.4k Python code for common Machine Learning Algorithms\nstanford_dl_ex 2.4k Programming exercises for the Stanford Unsupervised Feature Learning and Deep Learning Tutorial\ntensorflow-internals 2.4k It is open source ebook about TensorFlow kernel and implementation mechanism.\nDeepLearning 2.4k Python for\u300aDeep Learning\u300b\uff0c\u8be5\u4e66\u4e3a\u300a\u6df1\u5ea6\u5b66\u4e60\u300b(\u82b1\u4e66) \u6570\u5b66\u63a8\u5bfc\u3001\u539f\u7406\u5256\u6790\u4e0e\u6e90\u7801\u7ea7\u522b\u4ee3\u7801\u5b9e\u73b0\ntext 2.4k Data loaders and abstractions for text and NLP\nALAE 2.4k [CVPR2020] Adversarial Latent Autoencoders\npytorch-summary 2.4k Model summary in PyTorch similar to model.summary() in Keras\npytorch-doc-zh 2.4k Pytorch \u4e2d\u6587\u6587\u6863\nDeep_reinforcement_learning_Course 2.4k Implementations from the free course Deep Reinforcement Learning with Tensorflow\nML-Tutorial-Experiment 2.4k Coding the Machine Learning Tutorial for Learning to Learn\npytorch-Deep-Learning 2.4k Deep Learning (with PyTorch)\nmodels 2.4k A collection of pre-trained, state-of-the-art models in the ONNX format\nbook 2.4k Deep Learning 101 with PaddlePaddle \uff08\u300e\u98de\u6868\u300f\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5165\u95e8\u6559\u7a0b\uff09\nPyTorch_Tutorial 2.4k \u300aPytorch\u6a21\u578b\u8bad\u7ec3\u5b9e\u7528\u6559\u7a0b\u300b\u4e2d\u914d\u5957\u4ee3\u7801\nDive-into-DL-TensorFlow2.0 2.4k \u672c\u9879\u76ee\u5c06\u300a\u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60\u300b(Dive into Deep Learning)\u539f\u4e66\u4e2d\u7684MXNet\u5b9e\u73b0\u6539\u4e3aTensorFlow 2.0\u5b9e\u73b0\uff0c\u9879\u76ee\u5df2\u5f97\u5230\u674e\u6c90\u8001\u5e08\u7684\u540c\u610f\nArtificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials 2.4k A comprehensive list of Deep Learning / Artificial Intelligence and Machine Learning tutorials - rapidly expanding in\u2026\nsegmentation_models 2.4k Segmentation models with pretrained backbones. Keras and TensorFlow Keras.\nAwesome-PyTorch-Chinese 2.3k \u3010\u5e72\u8d27\u3011\u53f2\u4e0a\u6700\u5168\u7684PyTorch\u5b66\u4e60\u8d44\u6e90\u6c47\u603b\nFlux.jl 2.3k Relax! Flux is the ML library that doesn't make you tensor\nweld 2.3k High-performance runtime for data analytics applications\nPyTorch-BigGraph 2.3k Generate embeddings from large-scale graph-structured data.\nbyteps 2.3k A high performance and generic framework for distributed DNN training\nAI-Job-Notes 2.3k AI\u7b97\u6cd5\u5c97\u6c42\u804c\u653b\u7565\uff08\u6db5\u76d6\u51c6\u5907\u653b\u7565\u3001\u5237\u9898\u6307\u5357\u3001\u5185\u63a8\u548cAI\u516c\u53f8\u6e05\u5355\u7b49\u8d44\u6599\uff09\nluminoth 2.3k \u26a0\ufe0f UNMAINTAINED. Deep Learning toolkit for Computer Vision.\nAlink 2.3k Alink is the Machine Learning algorithm platform based on Flink, developed by the PAI team of Alibaba computing platf\u2026\nintrotodeeplearning 2.3k Lab Materials for MIT 6.S191: Introduction to Deep Learning\nTensorFlow-and-DeepLearning-Tutorial 2.3k A TensorFlow & Deep Learning online course I taught in 2016\nsrgan 2.3k Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\ncolorization 2.3k Automatic colorization using deep neural networks. \"Colorful Image Colorization.\" In ECCV, 2016.\nOpenNMT 2.3k Open Source Neural Machine Translation in Torch (deprecated)\nSuper-SloMo 2.3k PyTorch implementation of Super SloMo by Jiang et al.\norange3 2.3k \ud83c\udf4a \ud83d\udcca \ud83d\udca1 Orange: Interactive data analysis https://orange.biolab.si\nENAS-pytorch 2.3k PyTorch implementation of \"Efficient Neural Architecture Search via Parameters Sharing\"\n3D-ResNets-PyTorch 2.3k 3D ResNets for Action Recognition (CVPR 2018)\npomegranate 2.3k Fast, flexible and easy to use probabilistic modelling in Python.\nFaster-RCNN_TF 2.3k Faster-RCNN in Tensorflow\ndatascience 2.3k Curated list of Python resources for data science.\ndeep-learning-from-scratch 2.3k \u300e\u30bc\u30ed\u304b\u3089\u4f5c\u308b Deep Learning\u300f(O'Reilly Japan, 2016)\ncovid-chestxray-dataset 2.2k We are building an open database of COVID-19 cases with chest X-ray or CT images.\ndeepchem 2.2k Democratizing Deep-Learning for Drug Discovery, Quantum Chemistry, Materials Science and Biology\nawesome-learning-resources 2.2k \ud83d\udd25 Awesome list of resources on Web Development.\nomniscidb 2.2k OmniSciDB (formerly MapD Core)\ntablesaw 2.2k Java dataframe and visualization library\nSemantic-Segmentation-Suite 2.2k Semantic Segmentation Suite in TensorFlow. Implement, train, and test new Semantic Segmentation models easily!\nFCOS 2.2k FCOS: Fully Convolutional One-Stage Object Detection (ICCV'19)\nspotlight 2.2k Deep recommender models using PyTorch.\ndatasets 2.2k TFDS is a collection of datasets ready to use with TensorFlow, Jax, ...\nDeep-Learning-for-Recommendation-Systems 2.2k This repository contains Deep Learning based articles , paper and repositories for Recommender Systems\ngluon-nlp 2.1k NLP made easy\ndowhy 2.1k DoWhy is a Python library for causal inference that supports explicit modeling and testing of causal assumptions. DoWhy is based on a unified language for causal inference, combining causal graphical models and potential outcomes frameworks.\nkeras-attention-mechanism 2.1k Attention mechanism Implementation for Keras.\nMeta-Learning-Papers 2.1k Meta Learning / Learning to Learn / One Shot Learning / Few Shot Learning\ntacotron 2.1k A TensorFlow implementation of Google's Tacotron speech synthesis with pre-trained model (unofficial)\nmachinelearninginaction 2.1k Source Code for the book: Machine Learning in Action published by Manning\nBuildingMachineLearningSystemsWithPython 2.1k Source Code for the book Building Machine Learning Systems with Python\nCHINESE-OCR 2.1k [python3.6] \u8fd0\u7528tf\u5b9e\u73b0\u81ea\u7136\u573a\u666f\u6587\u5b57\u68c0\u6d4b,keras/pytorch\u5b9e\u73b0ctpn+crnn+ctc\u5b9e\u73b0\u4e0d\u5b9a\u957f\u573a\u666f\u6587\u5b57OCR\u8bc6\u522b\ndeepdetect 2.1k Deep Learning API and Server in C++11 support for Caffe, Caffe2, PyTorch,TensorRT, Dlib, NCNN, Tensorflow, XGBoost an\u2026\nXLM 2.1k PyTorch original implementation of Cross-lingual Language Model Pretraining.\ntensorflow-on-raspberry-pi 2.1k TensorFlow for Raspberry Pi\ndecaNLP 2.1k The Natural Language Decathlon: A Multitask Challenge for NLP\nAlphaZero_Gomoku 2.1k An implementation of the AlphaZero algorithm for Gomoku (also called Gobang or Five in a Row)\npytorch-beginner 2.1k pytorch tutorial for beginners\ntangent 2.1k Source-to-Source Debuggable Derivatives in Pure Python\nPerson_reID_baseline_pytorch 2.1k A tiny, friendly, strong pytorch implement of person re-identification baseline. Tutorial \ud83d\udc49https://github.com/layumi/\u2026\nmmlspark 2k Microsoft Machine Learning for Apache Spark\nFATE 2k An Industrial Level Federated Learning Framework\ntext-to-image 2k Text to image synthesis using thought vectors\npytorch-sentiment-analysis 2k Tutorials on getting started with PyTorch and TorchText for sentiment analysis.\nELF 2k An End-To-End, Lightweight and Flexible Platform for Game Research\ncatalyst 2k Accelerated DL R&D\nneuralcoref 2k \u2728Fast Coreference Resolution in spaCy with Neural Networks\nDeepRL-Agents 2k A set of Deep Reinforcement Learning Agents implemented in Tensorflow.\nRL-Adventure 2k Pytorch Implementation of DQN / DDQN / Prioritized replay/ noisy networks/ distributional values/ Rainbow/ hierarchic\u2026\nfe4ml-zh 2k \ud83d\udcd6 [\u8bd1] \u9762\u5411\u673a\u5668\u5b66\u4e60\u7684\u7279\u5f81\u5de5\u7a0b\nlingvo 2k Lingvo\npytorch-generative-model-collections 2k Collection of generative models in Pytorch version.\nResNeSt 2k ResNeSt: Split-Attention Networks\nTensorFlow-Tutorials 2k \ud150\uc11c\ud50c\ub85c\uc6b0\ub97c \uae30\ucd08\ubd80\ud130 \uc751\uc6a9\uae4c\uc9c0 \ub2e8\uacc4\ubcc4\ub85c \uc5f0\uc2b5\ud560 \uc218 \uc788\ub294 \uc18c\uc2a4 \ucf54\ub4dc\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4\nMUNIT 2k Multimodal Unsupervised Image-to-Image Translation\noneDNN 2k oneAPI Deep Neural Network Library (oneDNN)\ndeepvariant 2k DeepVariant is an analysis pipeline that uses a deep neural network to call genetic variants from next-generation DNA sequencing data.\ntexar 2k Toolkit for Machine Learning, Natural Language Processing, and Text Generation, in TensorFlow\nkaolin 2k A PyTorch Library for Accelerating 3D Deep Learning Research\nClairvoyant 2k Software designed to identify and monitor social/historical cues for short term stock movement\nimplicit 2k Fast Python Collaborative Filtering for Implicit Feedback Datasets\nDeepRL 2k Modularized Implementation of Deep RL Algorithms in PyTorch\nlgo 2k Interactive Go programming with Jupyter\nkcws 2k Deep Learning Chinese Word Segment\ntensorflow-build-archived 2k TensorFlow binaries supporting AVX, FMA, SSE\ndm_control 2k DeepMind's software stack for physics-based simulation and Reinforcement Learning environments, using MuJoCo.\ngpytorch 2k A highly efficient and modular implementation of Gaussian Processes in PyTorch\nNeural-Photo-Editor 1.9k A simple interface for editing natural photos with generative neural networks.\nalpha-zero-general 1.9k A clean implementation based on AlphaZero for any game in any framework + tutorial + Othello/Gobang/TicTacToe/Connect4\ntacotron2 1.9k Tacotron 2 - PyTorch implementation with faster-than-realtime inference\nsiamese-triplet 1.9k Siamese and triplet networks with online pair/triplet mining in PyTorch\nawesome-quant 1.9k \u4e2d\u56fd\u7684Quant\u76f8\u5173\u8d44\u6e90\u7d22\u5f15\nimage-super-resolution 1.9k \ud83d\udd0e Super-scale your images and run experiments with Residual Dense and Adversarial Networks.\ngenerative_inpainting 1.9k DeepFill v1/v2 with Contextual Attention and Gated Convolution, CVPR 2018, and ICCV 2019 Oral\ncode-of-learn-deep-learning-with-pytorch 1.9k This is code of book \"Learn Deep Learning with PyTorch\"\ngpt-2-simple 1.9k Python package to easily retrain OpenAI's GPT-2 text-generating model on new texts\nDeepInterests 1.9k \u6df1\u5ea6\u6709\u8da3\nsegmentation_models.pytorch 1.9k Segmentation models with pretrained backbones. PyTorch.\nhuman-pose-estimation.pytorch 1.9k The project is an official implement of our ECCV2018 paper \"Simple Baselines for Human Pose Estimation and Tracking(h\u2026\nBigGAN-PyTorch 1.9k The author's officially unofficial PyTorch BigGAN implementation.\npytorch-playground 1.9k Base pretrained models and datasets in pytorch (MNIST, SVHN, CIFAR10, CIFAR100, STL10, AlexNet, VGG16, VGG19, ResNet, Inception, SqueezeNet)\nbertviz 1.9k Tool for visualizing attention in the Transformer model (BERT, GPT-2, Albert, XLNet, RoBERTa, CTRL, etc.)\nface.evoLVe.PyTorch 1.9k \ud83d\udd25\ud83d\udd25High-Performance Face Recognition Library on PyTorch\ud83d\udd25\ud83d\udd25\nReco-papers 1.8k Classic papers and resources on recommendation\ncoach 1.8k Reinforcement Learning Coach by Intel AI Lab enables easy experimentation with state of the art Reinforcement Learning algorithms\nsling 1.8k SLING - A natural language frame semantics parser\npytorch-deeplab-xception 1.8k DeepLab v3+ model in PyTorch. Support different backbones.\nmmskeleton 1.8k A OpenMMLAB toolbox for human pose estimation, skeleton-based action recognition, and action synthesis.\nsru 1.8k Training RNNs as Fast as CNNs (https://arxiv.org/abs/1709.02755)\npytorch-seq2seq 1.8k Tutorials on implementing a few sequence-to-sequence (seq2seq) models with PyTorch and TorchText.\nDeep-Learning-Interview-Book 1.8k \u6df1\u5ea6\u5b66\u4e60\u9762\u8bd5\u5b9d\u5178\uff08\u542b\u6570\u5b66\u3001\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548cSLAM\u7b49\u65b9\u5411\uff09\npai 1.8k Resource scheduling and cluster management for AI\nAI-Blocks 1.8k A powerful and intuitive WYSIWYG interface that allows anyone to create Machine Learning models!\nscikit-optimize 1.8k Sequential model-based optimization with a scipy.optimize interface\nsequence_tagging 1.8k Named Entity Recognition (LSTM + CRF) - Tensorflow\nzh-NER-TF 1.8k A very simple BiLSTM-CRF model for Chinese Named Entity Recognition \u4e2d\u6587\u547d\u540d\u5b9e\u4f53\u8bc6\u522b (TensorFlow)\ndonkeycar 1.8k Open source hardware and software platform to build a small scale self driving car.\nedge-connect 1.8k EdgeConnect: Structure Guided Image Inpainting using Edge Prediction, ICCV 2019 https://arxiv.org/abs/1901.00212\nawd-lstm-lm 1.7k LSTM and QRNN Language Model Toolkit for PyTorch\npytorch-kaldi 1.7k pytorch-kaldi is a project for developing state-of-the-art DNN/RNN hybrid speech recognition systems. The DNN part is managed by pytorch, while feature extraction, label computation, and decoding are performed with the kaldi toolkit.\nBender 1.7k Easily craft fast Neural Networks on iOS! Use TensorFlow models. Metal under the hood.\nzi2zi 1.7k Learning Chinese Character style with conditional GAN\nautoml-gs 1.7k Provide an input CSV and a target field to predict, generate a model + code to run it.\nstats 1.7k A well tested and comprehensive Golang statistics library package with no dependencies.\nranking 1.7k Learning to Rank in TensorFlow\nmathAI 1.7k \u4e00\u4e2a\u62cd\u7167\u505a\u9898\u7a0b\u5e8f\u3002\u8f93\u5165\u4e00\u5f20\u5305\u542b\u6570\u5b66\u8ba1\u7b97\u9898\u7684\u56fe\u7247\uff0c\u8f93\u51fa\u8bc6\u522b\u51fa\u7684\u6570\u5b66\u8ba1\u7b97\u5f0f\u4ee5\u53ca\u8ba1\u7b97\u7ed3\u679c\u3002This is a mathematic expression recognition project.\nspark-ml-source-analysis 1.7k spark ml \u7b97\u6cd5\u539f\u7406\u5256\u6790\u4ee5\u53ca\u5177\u4f53\u7684\u6e90\u7801\u5b9e\u73b0\u5206\u6790\nvideo-object-removal 1.7k Just draw a bounding box and you can remove the object you want to remove.\ndatascience-pizza 1.7k \ud83c\udf55 Reposit\u00f3rio para juntar informa\u00e7\u00f5es sobre materiais de estudo em an\u00e1lise de dados e \u00e1reas afins, empresas que trabalham com dados e dicion\u00e1rio de conceitos\ndata-science-interviews 1.7k Data science interview questions and answers\nyolov3-tf2 1.7k YoloV3 Implemented in Tensorflow 2.0\nComputeLibrary 1.7k The ARM Computer Vision and Machine Learning library is a set of functions optimised for both ARM CPUs and GPUs using SIMD technologies.\ntacotron 1.7k A TensorFlow Implementation of Tacotron: A Fully End-to-End Text-To-Speech Synthesis Model\nDeepLearn 1.7k Implementation of research papers on Deep Learning+ NLP+ CV in Python using Keras, Tensorflow and Scikit Learn.\nanalytics-zoo 1.7k Distributed Tensorflow, Keras and PyTorch on Apache Spark/Flink & Ray\nPyTorch-NLP 1.7k Basic Utilities for PyTorch Natural Language Processing (NLP)\ncaptcha_break 1.7k \u9a8c\u8bc1\u7801\u8bc6\u522b\ncrnn 1.7k Convolutional Recurrent Neural Network (CRNN) for image-based sequence recognition.\nDeblurGAN 1.7k Image Deblurring using Generative Adversarial Networks\nrobosat 1.6k Semantic segmentation on aerial and satellite imagery. Extracts features such as: buildings, parking lots, roads, water, clouds\npointnet2 1.6k PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space\nAutonomousDrivingCookbook 1.6k Scenarios, tutorials and demos for Autonomous Driving\nimgclsmob 1.6k Sandbox for training convolutional networks for computer vision\ntf_unet 1.6k Generic U-Net Tensorflow implementation for image segmentation\ntorchsample 1.6k High-Level Training, Data Augmentation, and Utilities for Pytorch\nnlp 1.6k \ud83e\udd17nlp \u2013 Datasets and evaluation metrics for Natural Language Processing in NumPy, Pandas, PyTorch and TensorFlow\nhdbscan 1.6k A high performance implementation of HDBSCAN clustering.\nm2cgen 1.6k Transform ML models into a native code (Java, C, Python, Go, JavaScript, Visual Basic, C#, R, PowerShell, PHP, Dart, Haskell, Ruby) with zero dependencies\nfastNLP 1.6k fastNLP: A Modularized and Extensible NLP Framework. Currently still in incubation.\nkeras-yolo2 1.6k Easy training on custom dataset. Various backends (MobileNet and SqueezeNet) supported. A YOLO demo to detect raccoon run entirely in brower is accessible at https://git.io/vF7vI (not on Windows).\nAwesome-Chatbot 1.6k Awesome Chatbot Projects,Corpus,Papers,Tutorials.Chinese Chatbot =>:\nknockknock 1.6k \ud83d\udeaa\u270aKnock Knock: Get notified when your training ends with only two additional lines of code\nMTBook 1.6k \u300a\u673a\u5668\u7ffb\u8bd1\uff1a\u7edf\u8ba1\u5efa\u6a21\u4e0e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u300b\u8096\u6850 \u6731\u9756\u6ce2 \u8457 - Machine Translation: Statistical Modeling and Deep Learning Methods\ntrains 1.6k TRAINS - Auto-Magical Experiment Manager & Version Control for AI - NOW WITH AUTO-MAGICAL DEVOPS!\nself-driving-car 1.6k Udacity Self-Driving Car Engineer Nanodegree projects.\ncnn_captcha 1.6k use cnn recognize captcha by tensorflow. \u672c\u9879\u76ee\u9488\u5bf9\u5b57\u7b26\u578b\u56fe\u7247\u9a8c\u8bc1\u7801\uff0c\u4f7f\u7528tensorflow\u5b9e\u73b0\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u8fdb\u884c\u9a8c\u8bc1\u7801\u8bc6\u522b\u3002\nXLearning 1.6k AI on Hadoop\nTacotron-2 1.6k DeepMind's Tacotron-2 Tensorflow implementation\nfast-wavenet 1.6k Speedy Wavenet generation using dynamic programming \u26a1\nspacy-course 1.6k \ud83d\udc69\u200d\ud83c\udfeb Advanced NLP with spaCy: A free online course\ngandissect 1.6k Pytorch-based tools for visualizing and understanding the neurons of a GAN. https://gandissect.csail.mit.edu/\nNCRFpp 1.6k NCRF++, a Neural Sequence Labeling Toolkit. Easy use to any sequence labeling tasks (e.g. NER, POS, Segmentation). It includes character LSTM/CNN, word LSTM/CNN and softmax/CRF components.\nstargan-v2 1.6k StarGAN v2 - Official PyTorch Implementation (CVPR 2020)\ntinyflow 1.6k Tutorial code on how to build your own Deep Learning System in 2k Lines\nUNIT 1.6k Unsupervised Image-to-Image Translation\nssd_keras 1.6k A Keras port of Single Shot MultiBox Detector\ncosin 1.5k \ud83c\udf32 \u6625\u677e\u5ba2\u670d\uff0c\u591a\u6e20\u9053\u667a\u80fd\u5ba2\u670d\u7cfb\u7edf\uff0c\u5f00\u6e90\u5ba2\u670d\u7cfb\u7edf\nfoolbox 1.5k A Python toolbox to create adversarial examples that fool neural networks in PyTorch, TensorFlow, and JAX\ncapsule-networks 1.5k A PyTorch implementation of the NIPS 2017 paper \"Dynamic Routing Between Capsules\".\nflogo 1.5k Project Flogo is an open source ecosystem of opinionated event-driven capabilities to simplify building efficient & modern serverless functions, microservices & edge apps.\nlip-reading-deeplearning 1.5k \ud83d\udd13 Lip Reading - Cross Audio-Visual Recognition using 3D Architectures\nhummingbird 1.5k Hummingbird compiles trained ML models into tensor computation for faster inference.\ndeep-rl-tensorflow 1.5k TensorFlow implementation of Deep Reinforcement Learning papers\npractical-machine-learning-with-python 1.5k Master the essential skills needed to recognize and solve complex real-world problems with Machine Learning and Deep Learning by leveraging the highly popular Python Machine Learning Eco-system.\nNeuroNER 1.5k Named-entity recognition using neural networks. Easy-to-use and state-of-the-art results.\nwavenet_vocoder 1.5k WaveNet vocoder\nawesome-hand-pose-estimation 1.5k Awesome work on hand pose estimation/tracking\nmAP 1.5k mean Average Precision - This code evaluates the performance of your neural net for object recognition.\nagents 1.5k TF-Agents is a library for Reinforcement Learning in TensorFlow\nCADL 1.5k ARCHIVED: Contains historical course materials/Homework materials for the FREE MOOC course on \"Creative Applications of Deep Learning w/ Tensorflow\" #CADL\ntensorflow-DeepFM 1.5k Tensorflow implementation of DeepFM for CTR prediction.\ntensorflow-1.4-billion-password-analysis 1.5k Deep Learning model to analyze a large corpus of clear text passwords.\nDAT8 1.5k General Assembly's 2015 Data Science course in Washington, DC\nNeMo 1.5k NeMo: a toolkit for conversational AI\nMachine-Learning-Flappy-Bird 1.5k Machine Learning for Flappy Bird using Neural Network and Genetic Algorithm\nml-visuals 1.5k Visuals contains figures and templates which you can reuse and customize to improve your scientific writing.\nGANimation 1.5k GANimation: Anatomically-aware Facial Animation from a Single Image (ECCV'18 Oral) [PyTorch]\nEagleEye 1.5k Stalk your Friends. Find their Instagram, FB and Twitter Profiles using Image Recognition and Reverse Image Search.\nPyTorch-Encoding 1.5k A PyTorch CV Toolkit\nspark 1.5k .NET for Apache\u00ae Spark\u2122 makes Apache Spark\u2122 easily accessible to .NET developers.\nquiver 1.5k Interactive convnet features visualization for Keras\nMachineLearning 1.5k \u4e00\u4e9b\u5173\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5b66\u4e60\u8d44\u6599\u4e0e\u7814\u7a76\u4ecb\u7ecd\nGAT 1.5k Graph Attention Networks (https://arxiv.org/abs/1710.10903)\nmt-dnn 1.4k Multi-Task Deep Neural Networks for Natural Language Understanding\ndeep-neuroevolution 1.4k Deep Neuroevolution\na-PyTorch-Tutorial-to-Object-Detection 1.4k SSD: Single Shot MultiBox Detector\nlabelbox 1.4k Labelbox is the fastest way to annotate data to build and ship computer vision applications.\nopenvino 1.4k OpenVINO\u2122 Toolkit repository\nawesome-decision-tree-papers 1.4k A collection of research papers on decision, classification and regression trees with implementations.\nproject_alias 1.4k Alias is a teachable \u201cparasite\u201d that is designed to give users more control over their smart assistants, both when it comes to customisation and privacy. Through a simple app the user can train Alias to react on a custom wake-word/sound, and once trained, Alias can take control over your home assistant by activating it for you.\ndata-science-question-answer 1.4k A repo for data science related questions and answers\n-----> photo !!! 2cartoon 1.4k \u4eba\u50cf\u5361\u901a\u5316\u63a2\u7d22\u9879\u76ee (-----> photo !!! -to-cartoon translation project)\nvideo2x 1.4k A lossless video/GIF/image upscaler achieved with waifu2x, Anime4K, SRMD and RealSR. Started in Hack the Valley 2, 2018.\nTengine 1.4k Tengine is a lite, high performance, modular inference engine for embedded device\ncuml 1.4k cuML - RAPIDS Machine Learning Library\nBentoML 1.4k Model Serving Made Easy\nGANotebooks 1.4k wgan, wgan2(improved, gp), infogan, and dcgan implementation in lasagne, keras, pytorch\nMobileNet 1.4k MobileNet build with Tensorflow\nCRAFT-pytorch 1.4k Official implementation of Character Region Awareness for Text Detection (CRAFT)\nmlr 1.4k Machine Learning in R\nmonodepth2 1.4k Monocular depth estimation from a single image\nTensorKart 1.4k self-driving MarioKart with TensorFlow\nkeras-contrib 1.4k Keras community contributions\nstellargraph 1.4k StellarGraph - Machine Learning on Graphs\nGDLnotes 1.4k Google Deep Learning Notes\uff08TensorFlow\u6559\u7a0b\uff09\npydensecrf 1.4k Python wrapper to Philipp Kr\u00e4henb\u00fchl's dense (fully connected) CRFs with gaussian edge potentials.\nseldon-server 1.4k Machine Learning Platform and Recommendation Engine built on Kubernetes\nchainercv 1.4k ChainerCV: a Library for Deep Learning in Computer Vision\ntensorflow-nlp 1.4k Building blocks for NLP and Text Generation in TensorFlow 2.x / 1.x\niOS_ML 1.4k List of Machine Learning, AI, NLP solutions for iOS. The most recent version of this article can be found on my blog.\ntfgo 1.4k Tensorflow + Go, the gopher way\nbi-att-flow 1.4k Bi-directional Attention Flow (BiDAF) network is a multi-stage hierarchical process that represents context at different levels of granularity and uses a bi-directional attention flow mechanism to achieve a query-aware context representation without early summarization.\nCoreML-in-ARKit 1.4k Simple project to detect objects and display 3D labels above them in AR. This serves as a basic Template for an ARKit project to use CoreML.\nlightning 1.4k Large-scale linear classification, regression and ranking in Python\nDeepFace 1.4k Face analysis mainly based on Caffe. At this time, face analysis tasks like detection, alignment and recognition have been done.\ntorch2trt 1.4k An easy to use PyTorch to TensorRT converter\ndeepvoice3_pytorch 1.4k PyTorch implementation of convolutional neural networks-based text-to-speech synthesis models\nsphereface 1.4k Implementation for <SphereFace: Deep Hypersphere Embedding for Face Recognition> in CVPR'17.\njeelizFaceFilter 1.4k Javascript/WebGL lightweight face tracking library designed for augmented reality webcam filters. Features : multiple faces detection, rotation, mouth opening. Various integration examples are provided (Three.js, Babylon.js, FaceSwap, Canvas2D, CSS3D...).\nkaggle-web-traffic 1.4k 1st place solution\nminimalRL 1.4k Implementations of basic RL algorithms with minimal lines of codes! (pytorch based)\nMachine-Learning-Notes 1.4k \u5468\u5fd7\u534e\u300a\u673a\u5668\u5b66\u4e60\u300b\u624b\u63a8\u7b14\u8bb0\nGen.jl 1.4k A general-purpose probabilistic programming system with programmable inference\nfaster_rcnn_pytorch 1.4k Faster RCNN with PyTorch\nsod 1.4k An Embedded Computer Vision & Machine Learning Library (CPU Optimized & IoT Capable)\nkeras_to_tensorflow 1.4k General code to convert a trained keras model into an inference tensorflow model\nhandtracking 1.3k Building a Real-time Hand-Detector using Neural Networks (SSD) on Tensorflow\nword-rnn-tensorflow 1.3k Multi-layer Recurrent Neural Networks (LSTM, RNN) for word-level language models in Python using TensorFlow.\nTorchCraft 1.3k Connecting Torch to StarCraft\nAndroidTensorFlowMachineLearningExample 1.3k Android TensorFlow MachineLearning Example (Building TensorFlow for Android)\nmagnitude 1.3k A fast, efficient universal vector embedding utility package.\nhackermath 1.3k Introduction to Statistics and Basics of Mathematics for Data Science - The Hacker's Way\npytorch-grad-cam 1.3k PyTorch implementation of Grad-CAM\ngrenade 1.3k Deep Learning in Haskell\ncaptcha_trainer 1.3k [\u9a8c\u8bc1\u7801\u8bc6\u522b-\u8bad\u7ec3] This project is based on CNN/ResNet/DenseNet+GRU/LSTM+CTC/CrossEntropy to realize verification code identification. This project is only for training the model.\nanago 1.3k Bidirectional LSTM-CRF and ELMo for Named-Entity Recognition, Part-of-Speech Tagging and so on.\nmne-python 1.3k MNE : Magnetoencephalography (MEG) and Electroencephalography (EEG) in Python\neos 1.3k A lightweight 3D Morphable Face Model fitting library in modern C++14\nngraph 1.3k nGraph - open source C++ library, compiler and runtime for Deep Learning\nNSFWDetector 1.3k A NSFW (aka porn) detector with CoreML\nopen_nsfw_android 1.3k \ud83d\udd25\ud83d\udd25\ud83d\udd25\u8272\u60c5\u56fe\u7247\u79bb\u7ebf\u8bc6\u522b\uff0c\u57fa\u4e8eTensorFlow\u5b9e\u73b0\u3002\u8bc6\u522b\u53ea\u970020ms,\u53ef\u65ad\u7f51\u6d4b\u8bd5\uff0c\u6210\u529f\u738799%\uff0c\u8c03\u7528\u53ea\u8981\u4e00\u884c\u4ee3\u7801\uff0c\u4ece\u96c5\u864e\u7684\u5f00\u6e90\u9879\u76eeopen_nsfw\u79fb\u690d\uff0c\u8be5\u6a21\u578b\u6587\u4ef6\u53ef\u7528\u4e8eiOS\u3001java\u3001C++\u7b49\u5e73\u53f0\ncs230-code-examples 1.3k Code examples in pyTorch and Tensorflow for CS230\npytorch-generative-adversarial-networks 1.3k A very simple generative adversarial network (GAN) in PyTorch\nforecasting 1.3k Time Series Forecasting Best Practices & Examples\nVIBE 1.3k Official implementation of CVPR2020 paper \"VIBE: Video Inference for Human Body Pose and Shape Estimation\"\nbulbea 1.3k \ud83d\udc17 \ud83d\udc3b Deep Learning based Python Library for Stock Market Prediction and Modelling\nelectra 1.3k ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\nscattertext 1.3k Beautiful visualizations of how language differs among document types.\ntalos 1.3k Hyperparameter Optimization for TensorFlow, Keras and PyTorch\npracticalAI-cn 1.3k AI\u5b9e\u6218-practicalAI \u4e2d\u6587\u7248\nimpersonator 1.3k PyTorch implementation of our ICCV 2019 paper: Liquid Warping GAN: A Unified Framework for Human Motion Imitation, Appearance Transfer and Novel View Synthesis\ngluon-ts 1.3k Probabilistic time series modeling in Python\ndcgan-completion.tensorflow 1.3k Image Completion with Deep Learning in TensorFlow\nEfficientDet.Pytorch 1.3k Implementation EfficientDet: Scalable and Efficient Object Detection in PyTorch\nNeuronBlocks 1.3k NLP DNN Toolkit - Building Your NLP DNN Models Like Playing Lego\nyolo2-pytorch 1.3k YOLOv2 in PyTorch\nEmojiIntelligence 1.3k Neural Network built in Apple Playground using Swift\nefficientnet 1.3k Implementation of EfficientNet model. Keras and TensorFlow Keras.\nYOLOv3_TensorFlow 1.3k Complete YOLO v3 TensorFlow implementation. Support training on your own dataset.\nTensorFlow.NET 1.3k .NET Standard bindings for Google's TensorFlow for developing, training and deploying Machine Learning models in C#.\npytextrank 1.3k Python implementation of TextRank for phrase extraction and summarization of text documents\ninfer 1.3k Infer.NET is a framework for running Bayesian inference in graphical models\nuda 1.3k Unsupervised Data Augmentation (UDA)\nmmaction 1.3k An open-source toolbox for action understanding based on PyTorch\nspark-nlp 1.3k State of the Art Natural Language Processing\npytorch-semantic-segmentation 1.3k PyTorch for Semantic Segmentation\nDeep-Learning-Boot-Camp 1.2k A community run, 5-day PyTorch Deep Learning Bootcamp\npytorch-openai-transformer-lm 1.2k \ud83d\udc25A PyTorch implementation of OpenAI's finetuned transformer language model with a script to import the weights pre-trained by OpenAI\nhiddenlayer 1.2k Neural network graphs and training metrics for PyTorch, Tensorflow, and Keras.\nPaddleHub 1.2k Toolkit for Pre-trained Model Application of PaddlePaddle\uff08\u300e\u98de\u6868\u300f\u9884\u8bad\u7ec3\u6a21\u578b\u5e94\u7528\u5de5\u5177 \uff09\nPhotographicImageSynthesis 1.2k Photographic Image Synthesis with Cascaded Refinement Networks\nalpr-unconstrained 1.2k License Plate Detection and Recognition in Unconstrained Scenarios\nDeep-Image-Analogy 1.2k The source code of 'Visual Attribute Transfer through Deep Image Analogy'.\nspektral 1.2k Graph Neural Networks with Keras and Tensorflow 2.\nDeepAA 1.2k make ASCII Art by Deep Learning\nvvedenie-mashinnoe-obuchenie 1.2k \ud83d\udcdd \u041f\u043e\u0434\u0431\u043e\u0440\u043a\u0430 \u0440\u0435\u0441\u0443\u0440\u0441\u043e\u0432 \u043f\u043e \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u043c\u0443 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044e\nOpenSeq2Seq 1.2k Toolkit for efficient experimentation with Speech Recognition, Text2Speech and NLP\nForge 1.2k A neural network toolkit for Metal\nkeras-yolo3 1.2k Training and Detecting Objects with YOLO3\nNiftyNet 1.2k [unmaintained] An open-source convolutional neural networks platform for research in medical image analysis and image-guided therapy\nAwesome-TensorFlow-Chinese 1.2k Awesome-TensorFlow-Chinese\uff0cTensorFlow \u4e2d\u6587\u8d44\u6e90\u7cbe\u9009\uff0c\u5b98\u65b9\u7f51\u7ad9\uff0c\u5b89\u88c5\u6559\u7a0b\uff0c\u5165\u95e8\u6559\u7a0b\uff0c\u89c6\u9891\u6559\u7a0b\uff0c\u5b9e\u6218\u9879\u76ee\uff0c\u5b66\u4e60\u8def\u5f84\u3002QQ\u7fa4\uff1a167122861\uff0c\u516c\u4f17\u53f7\uff1a\u78d0\u521bAI\uff0c\u5fae\u4fe1\u7fa4\u4e8c\u7ef4\u7801\uff1ahttp://www.tensorflownews.com/\nStudyBook 1.2k Study E-Book(ComputerVision DeepLearning MachineLearning Math NLP Python ReinforcementLearning)\nawesome-semantic-segmentation-pytorch 1.2k Semantic Segmentation on PyTorch (include FCN, PSPNet, Deeplabv3, Deeplabv3+, DANet, DenseASPP, BiSeNet, EncNet, DUNet, ICNet, ENet, OCNet, CCNet, PSANet, CGNet, ESPNet, LEDNet, DFANet)\nRFBNet 1.2k Receptive Field Block Net for Accurate and Fast Object Detection, ECCV 2018\nGPflow 1.2k Gaussian processes in TensorFlow\ndlwpt-code 1.2k Code for the book Deep Learning with PyTorch by Eli Stevens, Luca Antiga, and Thomas Viehmann.\ndlcv_for_beginners 1.2k \u300a\u6df1\u5ea6\u5b66\u4e60\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u300b\u914d\u5957\u4ee3\u7801\nDeep-Learning-with-PyTorch-Tutorials 1.2k \u6df1\u5ea6\u5b66\u4e60\u4e0ePyTorch\u5165\u95e8\u5b9e\u6218\u89c6\u9891\u6559\u7a0b \u914d\u5957\u6e90\u4ee3\u7801\u548cPPT\nDLTK 1.2k Deep Learning Toolkit for Medical Image Analysis\nFCN.tensorflow 1.2k Tensorflow implementation of Fully Convolutional Networks for Semantic Segmentation (http://fcn.berkeleyvision.org)\nfacenet-pytorch 1.2k Pretrained Pytorch face detection (MTCNN) and recognition (InceptionResnet) models\nh2o-tutorials 1.2k Tutorials and training material for the H2O Machine Learning Platform\nnlp-journey 1.2k Documents, papers and codes related to Natural Language Processing, including Topic Model, Word Embedding, Named Entity Recognition, Text Classificatin, Text Generation, Text Similarity, Machine Translation)\uff0cetc. All codes are implemented intensorflow 2.0.\nobject_detector_app 1.2k Real-Time Object Recognition App with Tensorflow and OpenCV\ntnt 1.2k Simple tools for logging and visualizing, loading and training\ntensorflow-deeplab-resnet 1.2k DeepLab-ResNet rebuilt in TensorFlow\nreproducible-image-denoising-state-of-the-art 1.2k Collection of popular and reproducible image denoising works.\nsenet.pytorch 1.2k PyTorch implementation of SENet\npytorch-seq2seq 1.2k An open source framework for seq2seq models in PyTorch.\nefficient_densenet_pytorch 1.2k A memory-efficient implementation of DenseNets\npytorch-retinanet 1.2k Pytorch implementation of RetinaNet object detection.\ncakechat 1.2k CakeChat: Emotional Generative Dialog System\npytorch-fcn 1.2k PyTorch Implementation of Fully Convolutional Networks. (Training code to reproduce the original result is available.)\npython-machine-learning-book-3rd-edition 1.2k The \"Python Machine Learning (3rd edition)\" book code repository\ntf-quant-finance 1.2k High-performance TensorFlow library for quantitative finance.\ngnes 1.1k GNES is Generic Neural Elastic Search, a cloud-native semantic search system based on deep neural network.\nImage-OutPainting 1.1k \ud83c\udfd6 Keras Implementation of Painting outside the box\nFabrik 1.1k \ud83c\udfed Collaboratively build, visualize, and design neural nets in browser\nattention-transfer 1.1k Improving Convolutional Networks via Attention Transfer (ICLR 2017)\npytorch-cifar100 1.1k Practice on cifar100(ResNet, DenseNet, VGG, GoogleNet, InceptionV3, InceptionV4, Inception-ResNetv2, Xception, Resnet In Resnet, ResNext,ShuffleNet, ShuffleNetv2, MobileNet, MobileNetv2, SqueezeNet, NasNet, Residual Attention Network, SENet)\nMONAI 1.1k AI Toolkit for Healthcare Imaging\ntensorrec 1.1k A TensorFlow recommendation algorithm and framework in Python.\nmleap 1.1k MLeap: Deploy Spark Pipelines to Production\nnoreward-rl 1.1k [ICML 2017] TensorFlow code for Curiosity-driven Exploration for Deep Reinforcement Learning\nPyTorchNLPBook 1.1k Code and data accompanying Natural Language Processing with PyTorch published by O'Reilly Media https://nlproc.info\nmtcnn 1.1k MTCNN face detection implementation for TensorFlow, as a PIP package.\nWaveRNN 1.1k WaveRNN Vocoder + TTS\nUNet-family 1.1k Paper and implementation of UNet-related model.\nAwesome-pytorch-list-CNVersion 1.1k Awesome-pytorch-list \u7ffb\u8bd1\u5de5\u4f5c\u8fdb\u884c\u4e2d......\ntensorflow-fcn 1.1k An Implementation of Fully Convolutional Networks in Tensorflow.\nBicycleGAN 1.1k Toward Multimodal Image-to-Image Translation\nTensorFlow2.0-Examples 1.1k \ud83d\ude44 Difficult algorithm, simple code.\nfast-autoaugment 1.1k Official Implementation of 'Fast AutoAugment' in PyTorch.\nfastai_deeplearn_part1 1.1k Notes for Fastai Deep Learning Course\nHyperGAN 1.1k Composable GAN framework with api and user interface\nhome 1.1k ApacheCN \u5f00\u6e90\u7ec4\u7ec7\uff1a\u516c\u544a\u3001\u4ecb\u7ecd\u3001\u6210\u5458\u3001\u6d3b\u52a8\u3001\u4ea4\u6d41\u65b9\u5f0f\ntfx 1.1k TFX is an end-to-end platform for deploying production ML pipelines\nhandwriting-synthesis 1.1k Handwriting Synthesis with RNNs \u270f\ufe0f\nimage-quality-assessment 1.1k Convolutional Neural Networks to predict the aesthetic and technical quality of images.\nPerceptualSimilarity 1.1k Learned Perceptual Image Patch Similarity (LPIPS) metric. In CVPR, 2018.\nlanenet-lane-detection 1.1k Unofficial implemention of lanenet model for real time lane detection using deep neural network model https://maybeshewill-cv.github.io/lanenet-lane-detection/\nuTensor 1.1k TinyML AI inference library\ntorchgan 1.1k Research Framework for easy and efficient training of GANs based on Pytorch\nmerlin 1.1k This is now the official location of the Merlin project.\nCLUE 1k \u4e2d\u6587\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\u6d4b\u8bc4 Chinese Language Understanding Evaluation Benchmark: datasets, baselines, pre-trained models, corpus and leaderboard\ntfjs-node 1k TensorFlow powered JavaScript library for training and deploying ML models on Node.js.\nCycleGAN-TensorFlow 1k An implementation of CycleGan using TensorFlow\nEffectivePyTorch 1k PyTorch tutorials and best practices.\nhercules 1k Gaining advanced insights from Git repository history.\nAdvancedEAST 1k AdvancedEAST is an algorithm used for Scene image text detect, which is primarily based on EAST, and the significant improvement was also made, which make long text predictions more accurate.\none-pixel-attack-keras 1k Keras implementation of \"One pixel attack for fooling deep neural networks\" using differential evolution on Cifar10 and ImageNet\nCRNN_Chinese_Characters_Rec 1k (CRNN) Chinese Characters Recognition.\nhmtl 1k \ud83c\udf0aHMTL: Hierarchical Multi-Task Learning - A State-of-the-Art neural network model for several NLP tasks based on PyTorch and AllenNLP\nrethinking-network-pruning 1k Rethinking the Value of Network Pruning (Pytorch) (ICLR 2019)\npytorch-classification 1k Classification with PyTorch.\na-PyTorch-Tutorial-to-Image-Captioning 1k Show, Attend, and Tell\nreformer-pytorch 1k Reformer, the efficient Transformer, in Pytorch\npytorch-YOLOv4 1k PyTorch ,ONNX and TensorRT implementation of YOLOv4\nFaceMaskDetection 1k \u5f00\u6e90\u4eba\u8138\u53e3\u7f69\u68c0\u6d4b\u6a21\u578b\u548c\u6570\u636e Detect faces and determine whether people are wearing mask.\ngen-efficientnet-pytorch 1k Pretrained EfficientNet, EfficientNet-Lite, MixNet, MobileNetV3 / V2, MNASNet A1 and B1, FBNet, Single-Path NAS\nopen-reid 1k Open source person re-identification library in python\nwgan-gp 1k A pytorch implementation of Paper \"Improved Training of Wasserstein GANs\"",
      "link": "https://github.com/aymericdamien/TopDeepLearning"
    },
    {
      "autor": "machine-learning-roadmap",
      "date": "NaN",
      "content": "2020 Machine Learning Roadmap (still valid for 2021)\nA roadmap connecting many of the most important concepts in machine learning, how to learn them and what tools to use to perform them.\nNamely:\n\ud83e\udd14 Machine Learning Problems - what does a machine learning problem look like?\n\u267b\ufe0f Machine Learning Process - once you\u2019ve found a problem, what steps might you take to solve it?\n\ud83d\udee0 Machine Learning Tools - what should you use to build your solution?\n\ud83e\uddee Machine Learning Mathematics - what exactly is happening under the hood of all the machine learning code you're writing?\n\ud83d\udcda Machines Learning Resources - okay, this is cool, how can I learn all of this?\nSee the full interactive version.\nWatch a feature-length -----> film !!!  video walkthrough (yes, really, it's longer than most movies).\nMany of the materials in this roadmap were inspired by Daniel Formoso's machine learning mindmaps,so if you enjoyed this one, go and check out his. He also has a mindmap specifically for deep learning too.",
      "link": "https://github.com/mrdbourke/machine-learning-roadmap"
    },
    {
      "autor": "Awesome-CoreML-Models",
      "date": "NaN",
      "content": "Since iOS 11, Apple released Core ML framework to help developers integrate machine learning models into applications. The official documentation\nWe've put up the largest collection of machine learning models in Core ML format, to help iOS, macOS, tvOS, and watchOS developers experiment with machine learning techniques.\nIf you've converted a Core ML model, feel free to submit a pull request.\nRecently, we've included visualization tools. And here's one Netron.\nModels\nImage - Metadata/Text\nModels that take image data as input and output useful information about the image.\nTextDetection - Detecting text using Vision built-in model in real-time. Download | Demo | Reference\nPhotoAssessment - Photo Assessment using Core ML and Metal. Download | Demo | Reference\nPoseEstimation - Estimating human pose from a picture for mobile. Download | Demo | Reference\nMobileNet - Detects the dominant objects present in an image. Download | Demo | Reference\nPlaces CNN - Detects the scene of an image from 205 categories such as bedroom, forest, coast etc. Download | Demo | Reference\nInception v3 - Detects the dominant objects present in an image. Download | Demo | Reference\nResNet50 - Detects the dominant objects present in an image. Download | Demo | Reference\nVGG16 - Detects the dominant objects present in an image. Download | Demo | Reference\nCar Recognition - Predict the brand & model of a car. Download | Demo | Reference\nYOLO - Recognize what the objects are inside a given image and where they are in the image. Download | Demo | Reference\nAgeNet - Predict a person's age from one's portrait. Download | Demo | Reference\nGenderNet - Predict a person's gender from one's portrait. Download | Demo | Reference\nMNIST - Predict handwritten (drawn) digits from images. Download | Demo | Reference\nEmotionNet - Predict a person's emotion from one's portrait. Download | Demo | Reference\nSentimentVision - Predict positive or negative sentiments from images. Download | Demo | Reference\nFood101 - Predict the type of foods from images. Download | Demo | Reference\nOxford102 - Detect the type of flowers from images. Download | Demo | Reference\nFlickrStyle - Detect the artistic style of images. Download | Demo | Reference\nRN1015k500 - Predict the location where a picture was taken. Download | Demo | Reference\nNudity - Classifies an image either as NSFW (nude) or SFW (not nude) Download | Demo | Reference\nTextRecognition (ML Kit) - Recognizing text using ML Kit built-in model in real-time. Download | Demo | Reference\nImageSegmentation - Segment the pixels of a -----> camera !!!  frame or image into a predefined set of classes. Download | Demo | Reference\nDepthPrediction - Predict the depth from a single image. Download | Demo | Reference\nImage - Image\nModels that transform images.\nHED - Detect nested edges from a color image. Download | Demo | Reference\nAnimeScale2x - Process a bicubic-scaled anime-style artwork Download | Demo | Reference\nText - Metadata/Text\nModels that process text data\nSentiment Polarity - Predict positive or negative sentiments from sentences. Download | Demo | Reference\nDocumentClassification - Classify news articles into 1 of 5 categories. Download | Demo | Reference\niMessage Spam Detection - Detect whether a message is spam. Download | Demo | Reference\nNamesDT - Gender Classification using DecisionTreeClassifier Download | Demo | Reference\nPersonality Detection - Predict personality based on user documents (sentences). Download | Demo | Reference\nBERT for Question answering - Swift Core ML 3 implementation of BERT for Question answering Download | Demo | Reference\nGPT-2 - OpenAI GPT-2 Text generation (Core ML 3) Download | Demo | Reference\nMiscellaneous\nExermote - Predicts the exercise, when iPhone is worn on right upper arm. Download | Demo | Reference\nGestureAI - Recommend an artist based on given location and genre. Download | Demo | Reference\nArtists Recommendation - Recommend an artist based on given location and genre. Download | Demo | Reference\nVisualization Tools\nTools that help visualize CoreML Models\nNetron\nSupported formats\nList of model formats that could be converted to Core ML with examples\nCaffe\nKeras\nXGBoost\nScikit-learn\nMXNet\nLibSVM\nTorch7\nThe Gold\nCollections of machine learning models that could be converted to Core ML\nCaffe Model Zoo - Big list of models in Caffe format.\nTensorFlow Models - Models for TensorFlow.\nTensorFlow Slim Models - Another collection of TensorFlow Models.\nMXNet Model Zoo - Collection of MXNet models.\nIndividual machine learning models that could be converted to Core ML. We'll keep adjusting the list as they become converted.\nLaMem Score the memorability of pictures.\nILGnet The aesthetic evaluation of images.\nColorization Automatic colorization using deep neural networks.\nIllustration2Vec Estimating a set of tags and extracting semantic feature vectors from given illustrations.\nCTPN Detecting text in natural image.\nImage Analogy Find semantically-meaningful dense correspondences between two input images.\niLID Automatic spoken language identification.\nFashion Detection Cloth detection from images.\nSaliency The prediction of salient areas in images has been traditionally addressed with hand-crafted features.\nFace Detection Detect face from image.\nmtcnn Joint Face Detection and Alignment.\ndeephorizon Single image horizon line estimation.\nContributing and License\nSee the guide\nDistributed under the MIT license. See LICENSE for more information.",
      "link": "https://github.com/likedan/Awesome-CoreML-Models"
    },
    {
      "autor": "jetson-inference",
      "date": "NaN",
      "content": "Deploying Deep Learning\nWelcome to our instructional guide for inference and realtime DNN vision library for NVIDIA Jetson Nano/TX1/TX2/Xavier NX/AGX Xavier.\nThis repo uses NVIDIA TensorRT for efficiently deploying neural networks onto the embedded Jetson platform, improving performance and power efficiency using graph optimizations, kernel fusion, and FP16/INT8 precision.\nVision primitives, such as imageNet for image recognition, detectNet for object detection, segNet for semantic segmentation, and poseNet for pose estimation inherit from the shared tensorNet object. Examples are provided for streaming from live -----> camera !!!  feed and processing images. See the API Reference section for detailed reference documentation of the C++ and Python libraries.\nFollow the Hello AI World tutorial for running inference and transfer learning onboard your Jetson, including collecting your own datasets and training your own models. It covers image classification, object detection, semantic segmentation, pose estimation, and mono depth.\nTable of Contents\nHello AI World\nVideo Walkthroughs\nAPI Reference\nCode Examples\nPre-Trained Models\nSystem Requirements\nChange Log\n> JetPack 4.6 is now supported, along with updated containers.\n> Try the new Pose Estimation and Mono Depth tutorials!\n> See the Change Log for the latest updates and new features.\nHello AI World\nHello AI World can be run completely onboard your Jetson, including inferencing with TensorRT and transfer learning with PyTorch. The inference portion of Hello AI World - which includes coding your own image classification and object detection applications for Python or C++, and live camera demos - can be run on your Jetson in roughly two hours or less, while transfer learning is best left to leave running overnight.\nSystem Setup\nSetting up Jetson with JetPack\nRunning the Docker Container\nBuilding the Project from Source\nInference\nClassifying Images with ImageNet\nUsing the ImageNet Program on Jetson\nCoding Your Own Image Recognition Program (Python)\nCoding Your Own Image Recognition Program (C++)\nRunning the Live Camera Recognition Demo\nLocating Objects with DetectNet\nDetecting Objects from Images\nRunning the Live Camera Detection Demo\nCoding Your Own Object Detection Program\nSemantic Segmentation with SegNet\nSegmenting Images from the Command Line\nRunning the Live Camera Segmentation Demo\nPose Estimation with PoseNet\nMonocular Depth with DepthNet\nTraining\nTransfer Learning with PyTorch\nClassification/Recognition (ResNet-18)\nRe-training on the Cat/Dog Dataset\nRe-training on the PlantCLEF Dataset\nCollecting your own Classification Datasets\nObject Detection (SSD-Mobilenet)\nRe-training SSD-Mobilenet\nCollecting your own Detection Datasets\nAppendix\nCamera Streaming and Multimedia\nImage Manipulation with CUDA\nDeep Learning Nodes for ROS/ROS2\nVideo Walkthroughs\nBelow are screencasts of Hello AI World that were recorded for the Jetson AI Certification course:\nDescription Video\nHello AI World Setup\nDownload and run the Hello AI World container on Jetson Nano, test your camera feed, and see how to stream it over the network via RTP.\nImage Classification Inference\nCode your own Python program for image classification using Jetson Nano and deep learning, then experiment with realtime classification on a live camera stream.\nTraining Image Classification Models\nLearn how to train image classification models with PyTorch onboard Jetson Nano, and collect your own classification datasets to create custom models.\nObject Detection Inference\nCode your own Python program for object detection using Jetson Nano and deep learning, then experiment with realtime detection on a live camera stream.\nTraining Object Detection Models\nLearn how to train object detection models with PyTorch onboard Jetson Nano, and collect your own detection datasets to create custom models.\nSemantic Segmentation\nExperiment with fully-convolutional semantic segmentation networks on Jetson Nano, and run realtime segmentation on a live camera stream.\nAPI Reference\nBelow are links to reference documentation for the C++ and Python libraries from the repo:\njetson-inference\nC++ Python\nImage Recognition imageNet imageNet\nObject Detection detectNet detectNet\nSegmentation segNet segNet\nPose Estimation poseNet poseNet\nMonocular Depth depthNet depthNet\njetson-utils\nC++\nPython\nThese libraries are able to be used in external projects by linking to libjetson-inference and libjetson-utils.\nCode Examples\nIntroductory code walkthroughs of using the library are covered during these steps of the Hello AI World tutorial:\nCoding Your Own Image Recognition Program (Python)\nCoding Your Own Image Recognition Program (C++)\nAdditional C++ and Python samples for running the networks on static images and live camera streams can be found here:\nC++ Python\nImage Recognition imagenet.cpp imagenet.py\nObject Detection detectnet.cpp detectnet.py\nSegmentation segnet.cpp segnet.py\nPose Estimation posenet.cpp posenet.py\nMonocular Depth depthnet.cpp depthnet.py\nnote: for working with numpy arrays, see Converting to Numpy Arrays and Converting from Numpy Arrays\nThese examples will automatically be compiled while Building the Project from Source, and are able to run the pre-trained models listed below in addition to custom models provided by the user. Launch each example with --help for usage info.\nPre-Trained Models\nThe project comes with a number of pre-trained models that are available through the Model Downloader tool:\nImage Recognition\nNetwork CLI argument NetworkType enum\nAlexNet alexnet ALEXNET\nGoogleNet googlenet GOOGLENET\nGoogleNet-12 googlenet-12 GOOGLENET_12\nResNet-18 resnet-18 RESNET_18\nResNet-50 resnet-50 RESNET_50\nResNet-101 resnet-101 RESNET_101\nResNet-152 resnet-152 RESNET_152\nVGG-16 vgg-16 VGG-16\nVGG-19 vgg-19 VGG-19\nInception-v4 inception-v4 INCEPTION_V4\nObject Detection\nNetwork CLI argument NetworkType enum Object classes\nSSD-Mobilenet-v1 ssd-mobilenet-v1 SSD_MOBILENET_V1 91 (COCO classes)\nSSD-Mobilenet-v2 ssd-mobilenet-v2 SSD_MOBILENET_V2 91 (COCO classes)\nSSD-Inception-v2 ssd-inception-v2 SSD_INCEPTION_V2 91 (COCO classes)\nDetectNet-COCO-Dog coco-dog COCO_DOG dogs\nDetectNet-COCO-Bottle coco-bottle COCO_BOTTLE bottles\nDetectNet-COCO-Chair coco-chair COCO_CHAIR chairs\nDetectNet-COCO-Airplane coco-airplane COCO_AIRPLANE airplanes\nped-100 pednet PEDNET pedestrians\nmultiped-500 multiped PEDNET_MULTI pedestrians, luggage\nfacenet-120 facenet FACENET faces\nSemantic Segmentation\nDataset Resolution CLI Argument Accuracy Jetson Nano Jetson Xavier\nCityscapes 512x256 fcn-resnet18-cityscapes-512x256 83.3% 48 FPS 480 FPS\nCityscapes 1024x512 fcn-resnet18-cityscapes-1024x512 87.3% 12 FPS 175 FPS\nCityscapes 2048x1024 fcn-resnet18-cityscapes-2048x1024 89.6% 3 FPS 47 FPS\nDeepScene 576x320 fcn-resnet18-deepscene-576x320 96.4% 26 FPS 360 FPS\nDeepScene 864x480 fcn-resnet18-deepscene-864x480 96.9% 14 FPS 190 FPS\nMulti-Human 512x320 fcn-resnet18-mhp-512x320 86.5% 34 FPS 370 FPS\nMulti-Human 640x360 fcn-resnet18-mhp-512x320 87.1% 23 FPS 325 FPS\nPascal VOC 320x320 fcn-resnet18-voc-320x320 85.9% 45 FPS 508 FPS\nPascal VOC 512x320 fcn-resnet18-voc-512x320 88.5% 34 FPS 375 FPS\nSUN RGB-D 512x400 fcn-resnet18-sun-512x400 64.3% 28 FPS 340 FPS\nSUN RGB-D 640x512 fcn-resnet18-sun-640x512 65.1% 17 FPS 224 FPS\nIf the resolution is omitted from the CLI argument, the lowest resolution model is loaded\nAccuracy indicates the pixel classification accuracy across the model's validation dataset\nPerformance is measured for GPU FP16 mode with JetPack 4.2.1, nvpmodel 0 (MAX-N)\nLegacy Segmentation Models\nPose Estimation\nModel CLI argument NetworkType enum Keypoints\nPose-ResNet18-Body resnet18-body RESNET18_BODY 18\nPose-ResNet18-Hand resnet18-hand RESNET18_HAND 21\nPose-DenseNet121-Body densenet121-body DENSENET121_BODY 18\nRecommended System Requirements\nJetson Nano Developer Kit with JetPack 4.2 or newer (Ubuntu 18.04 aarch64).\nJetson Nano 2GB Developer Kit with JetPack 4.4.1 or newer (Ubuntu 18.04 aarch64).\nJetson Xavier NX Developer Kit with JetPack 4.4 or newer (Ubuntu 18.04 aarch64).\nJetson AGX Xavier Developer Kit with JetPack 4.0 or newer (Ubuntu 18.04 aarch64).\nJetson TX2 Developer Kit with JetPack 3.0 or newer (Ubuntu 16.04 aarch64).\nJetson TX1 Developer Kit with JetPack 2.3 or newer (Ubuntu 16.04 aarch64).\nThe Transfer Learning with PyTorch section of the tutorial speaks from the perspective of running PyTorch onboard Jetson for training DNNs, however the same PyTorch code can be used on a PC, server, or cloud instance with an NVIDIA discrete GPU for faster training.\nExtra Resources\nIn this area, links and resources for deep learning are listed:\nros_deep_learning - TensorRT inference ROS nodes\nNVIDIA AI IoT - NVIDIA Jetson GitHub repositories\nJetson eLinux Wiki - Jetson eLinux Wiki\nTwo Days to a Demo (DIGITS)\nnote: the DIGITS/Caffe tutorial from below is deprecated. It's recommended to follow the Transfer Learning with PyTorch tutorial from Hello AI World.\nExpand this section to see original DIGITS tutorial (deprecated)\nThe DIGITS tutorial includes training DNN's in the cloud or PC, and inference on the Jetson with TensorRT, and can take roughly two days or more depending on system setup, downloading the datasets, and the training speed of your GPU.\n\u00a9 2016-2019 NVIDIA | Table of Contents",
      "link": "https://github.com/dusty-nv/jetson-inference"
    },
    {
      "autor": "useful-java-links",
      "date": "NaN",
      "content": "Useful Java links\nThis is a fork of awesome link with new structure, additional license info and github's star info for every link, with a lot of new links (all non-mobile github projects with 390 or more star) and so on. The russian version is in this place.\nThe \"Hello Worlds examples\" project is in this place.\nUseful java links\nI. Development\n1. Common frameworks and libraries\n2. Web development\n3. GUI\n4. Business\n5. Game Development\n6. Useful libraries\nCollections\nDate and Time\nDependency Injection and AOP\nConsole and Command line\nFunctional Programming\nReactive Programming\nSecurity and Authentication\nHigh Performance\nSerialization and I/O\nLogging\nBean Mapping and Validation\n7. Imagery and Video\n8. Code generation and changing byte code\n9. Distributed Applications\n10. Science\n11. OSGI\nII. Databases, search engines, big data and machine learning\n1. Databases and storages - Distributed Databases\n2. Data structures\n3. Search engines\n4. Client and drivers for databases\n5. ORM\n6. Working with messy data\n7. Big data\n8. Machine Learning - Semantic Web and Linked Data\n- Constraint Satisfaction Problem Solver - Natural Language Processing (NLP) and Speech Recognition\nIII. Network and Integration\n1. Servers (Web Server and Application Server))\n2. Networking\n3. Message, message broker and message queue\n4. Http and ssh\n5. Rest Frameworks\n6. Integration frameworks\n7. Web Crawling and HTML parsering\n8. Json\n9. CSV\n10. Integratin with API\n11. Bitcoin\n12. Clouds\n13. Cluster Management\n14. Document Processing (XLS, DOC and PDF)\n15. Native\n16. XML and SOAP\n17. Geospatial Service Interation\n18. Reverse Proxy Servers\nIV. Testing\n1. Testing\n2. Code Coverage\n3. Continuous Integration\n4. Formal Verification\nV. Tools for developing\n1. IDE\n2. Deploy, config and build\nBuild\nConfiguration\nDistribution\n3. Perfomance tools\n4. Code Analysis\n5. Monitoring\n6. Redefinition of classes at runtime\n7. Documentation\n8. Other\nVI. Program languages and applications that were written with Java\n1. Program languages that were written with Java\n2. Other program languages tools that were written with Java\n3. Javascript\n4. Frameworks that help to create parsers, interpreters or compilers\n5. Opensource applications that were written with Java\n6. Opensource games that were written with Java\nVII. Other\n1. Source code examples\n2. Benchmark results\n3. Working with git and github\nVIII. Resources\n1. Communities\n2. Influential Books\n3. Websites\nI. Development\n1. Common frameworks and libraries\nUp\nSpring framework The Spring Framework provides a comprehensive programming and configuration model for modern Java-based enterprise applications -- on any kind of deployment platform. A key element of Spring is infrastructural support at the application level: Spring focuses on the \"plumbing\" of enterprise applications so that teams can focus on application-level business logic, without unnecessary ties to specific deployment environments. License: Apache 2 , , stackoverflow - more 100 000 questions.\nGoogle Guava The Guava project contains several of Google's core libraries that we rely on in our Java-based projects: collections, caching, primitives support, concurrency libraries, common annotations, string processing, I/O, and so forth.Requires JDK 1.6 or higher (as of 12.0). License: Apache 2 , .\nApache Commons - Apache Commons is an Apache project focused on all aspects of reusable Java components.Commons Proper is dedicated to one principal goal: creating and maintaining reusable Java components. License: Apache 2.\nSpring Boot \u2014 Spring Boot makes it easy to create Spring-powered, production-grade applications and services with absolute minimum fuss. It takes an opinionated view of the Spring platform so that new and existing users can quickly get to the bits they need. License: Apache 2 , .\nLombok Very spicy additions to the Java programming language. Project Lombok makes java a spicier language by adding 'handlers' that know how to build and compile simple, boilerplate-free, not-quite-java code. License: MIT , .\nPuniverse Quasar Fibers, Channels and Actors for the JVM. Hello World examples. License: Eclipse Public v1.0/GNU Lesser 3, .\nRootbeer GPU CompilerThe Rootbeer GPU Compiler lets you use GPUs from within Java. License: MIT , .\nOblac Jodd Jodd is set of open-source Java micro frameworks and tools; floppy size! Jodd = tools + ioc + mvc + db + aop + tx + json + html < 1.5 Mb. License: BSD 2, .\nGoogle Jimfs An in-memory file system for Java 7+. License: Apache 2 , .\nSpring batch Spring Batch is a framework for writing offline and batch applications using Spring and Java. License: Apache 2 , .\nMicroserver \u2014 A convenient Microservices plugin system for Spring & Spring Boot, with over 30 plugins and growing, that supports both micro-monolith and pure microservices styles. License: Apache 2 , .\nGreenrobot common General purpose utilities and hash functions for Android and Java (aka java-common). License: Apache 2 , .\nKilim Lightweight threads for Java, with message passing, nio, http and scheduling support. License: MIT , .\nApache Isis Apache Isis is a framework for rapidly developing domain-driven apps in Java. Write your business logic in entities, domain services and repositories, and the framework dynamically (at runtime) generates a representation of that domain model as a webapp or as a RESTful API. For prototyping or production. License: Apache 2.\nApache Zest Apache Zest is a community based effort exploring Composite Oriented Programming for domain centric application development. This includes evolved concepts from Aspect Oriented Programming, Dependency Injection and Domain Driven Design. License: Apache 2.\n2. Web development\nUp\nWeb Frameworks\nPlay Framework The Play Framework combines productivity and performance making it easy to build scalable web applications with Java and Scala. https://www.playframework.com/ . License: Apache 2 , .\nSpark Spark - a Sinatra inspired web framework. License: Apache 2 , .\nBlade a simple, elegant java web framework! License: Apache 2 , .\nAtmosphere - Realtime Client Server Framework for the JVM, supporting WebSockets with Cross-Browser Fallbacks. License: CDDL1 / Apache 2 , .\nGrails - Grails is a framework used to build web applications with the Groovy programming language. The core framework is very extensible and there are numerous plugins available that provide easy integration of add-on features. https://grails.org/ License: Apache 2 , .\nNinja Ninja is a full stack web framework for Java. Rock solid, fast and super productive. .http://www.ninjaframework.org. License: Apache 2 , .\nElectronicarts Orbit \u2014 Orbit - Virtual actor framework for building distributed systems. License: BSD 3, .\nRatpack Ratpack is a simple, capable, toolkit for creating high performance web applications. Ratpack is built on Java and the Netty event-driven networking engine. The API is optimized for Groovy and Java 8.. https://ratpack.io/ License: Apache 2 , .\nVaadin \u2014 Vaadin is a Java framework for building modern web applications that look great, perform well and make you and your users happy.https://vaadin.com/ License: Apache 2 , .\nZK framework ZK is a highly productive Java framework for building amazing enterprise web and mobile applications. License: GNU Lesser, .\nApache Tapestry - A component-oriented framework for creating highly scalable web applications in Java. License: Apache 2.\nApache Wicket - Open Source Java web framework that powers thousands of web applications and web sites for governments, stores, universities, cities, banks, email providers, and more. License: Apache 2.\nGoogle Web Toolkit - GWT is used by many products at Google, including Google AdWords and Google Wallet. It's open source, completely free, and used by thousands of enthusiastic developers around the world. License: Apache 2.\nPippo - It's an open source micro web framework in Java, with minimal dependencies and a quick learning curve.The goal of this project is to create a micro web framework in Java that should be easy to use and hack. Pippo can be used in small and medium applications and also in applications based on micro services architecture. License: Apache 2.\nSpring MVC - Spring MVC web application and RESTful web service framework. License: Apache 2.\nJavaServer Faces - JavaServer\u2122 Faces technology simplifies building user interfaces for JavaServer applications. License: GNU 2 or CDDL 1.0\nJavaServer Pages - This project provides a container independent implementation of JSP 2.1. License: GNU 2 or CDDL 1.0\nApache Cocoon Web development framework: separation of concerns, component-based. License: Apache 2.\nApache Struts Apache Struts is a free open-source framework for creating Java web applications. License: Apache 2.\nApache Sling Innovative Web framework based on JCR and OSGi. License: Apache 2.\nApache MyFaces The first free open source JavaServer(tm) Faces implementation. License: Apache 2.\nApache Pivot Apache Pivot is an open-source platform for building installable Internet applications (IIAs) [rich Internet application]. License: Apache 2.\nApache Turbine Turbine is a servlet based framework that allows Java developers to quickly build web applications. License: Apache 2.\nApache OODT Apache OODT software is component based, and offers a software architecture beyond simple science applications. License: Apache 2.\nVRaptor VRaptor 4 delivers high productivity to your Java Web applications on top of CDI. VRaptor is an opensource MVC framework with a large developers and users community, . http://www.vraptor.org . License: Apache 2.\nJooby Scalable, fast and modular micro web framework for Java , . http://jooby.org/ . License: Apache 2.\nCUBA Platform - High-level framework for developing enterprise applications with a rich web interface, based on Spring, EclipseLink and Vaadin. https://cuba-platform.com . License: Apache 2.\nJersey - Jersey RESTful Web Services framework is open source, production quality, framework for developing RESTful Web Services in Java that provides support for JAX-RS APIs and serves as a JAX-RS (JSR 311 & JSR 339) Reference Implementation.\nLibraries for web development\nTobie ua-parser A multi-language port of Browserscope's user agent parser. License: Apache 2/MIT/Perl, .\nGhost Driver Ghost Driver is an implementation of the Remote WebDriver Wire protocol, using PhantomJS as back-end. License: BSD 2, .\nB3log Solo - A blogging system written in Java, feel free to create your or your team own blog. License: Apache 2 , .\nNasher - A Spring Project with spring security support with angular admin panel. License: Apache 2 , .\nBaasbox BaasBox is an Open Source project that aims to provide a backend for mobile and web apps. License: ?, .\nKolorobot Spring MVC 4 Quickstart Maven Archetype \u2014 The project is a Maven archetype for Spring MVC 4 web application. License: ?, .\nGargl - Record web requests as they happen and turn them into reusable code in any programming language. Gargl - Generic API Recorder and Generator Lite. Pronounced \"Gargle.\" Automate any website. Record web requests as they happen and turn them into reusable code in any programming language. License: MIT , .\nLiferay plugins - The liferay-plugins repository is part of the Liferay Portal project. Liferay Portal is an open source enterprise web platform for building business solutions that deliver immediate results and long-term value. License: GNU Lesser 2.1, .\nNetflix Ribbon Ribbon is a Inter Process Communication (remote procedure calls) library with built in software load balancers. The primary usage model involves REST calls with various serialization scheme support. License: Apache 2 , .\nGwt bootstrap A GWT Library that provides the widgets of Bootstrap, from Twitter. License: Apache 2 , .\nNetflix Zuul Zuul is an edge service that provides dynamic routing, monitoring, resiliency, security, and more. License: Apache 2 , .\nPrimeFaces - PrimeUI is a pure CSS-JS library designed to work with any server side and client side technology featuring 50+ jQuery Widgets, Web Components, Premium Layouts-Themes and more. PrimeUI is an offspring project of the mighty PrimeFaces. License: Apache 2.\nSpring Boot - Spring Boot makes it easy to create Spring-powered, production-grade applications and services with absolute minimum fuss. It takes an opinionated view of the Spring platform so that new and existing users can quickly get to the bits they need. License: Apache 2.\nCSSEmbed A tool for embedding data URIs in CSS files. License: MIT , .\nAthou commafeed Google Reader inspired self-hosted RSS reader. License: Apache 2 , .\nApache Any23 Anything To Triples (Any23) is a library, a web service and a command line tool that extracts structured data in RDF format from a variety of Web. License: Apache 2.\nApache Forrest Apache Forrest software is a publishing framework that transforms input from various sources into a unified presentation in one or more output formats. License: Apache 2.\nTemplate Engine\nApache Velocity - Velocity is a project of the Apache Software Foundation, charged with the creation and maintenance of open-source software related to the Apache Velocity Engine (templating engine). License: Apache 2.\nApache FreeMarker - Apache FreeMarker is a template engine: a Java library to generate text output (HTML web pages, e-mails, configuration files, source code, etc.) based on templates and changing data. License: Apache 2.\nHandlebars.java - Logic-less and semantic templates with Java, . License: Apache 2.\nThymeleaf - It is a template engine capable of processing and generating HTML, XML, JavaScript, CSS and text, and can work both in web and non-web environments. It is better suited for serving the view layer of web applications, but it can process files in many formats, even in offline environments. License: Apache 2.\nMustache.java - Implementation of mustache.js (web application templating system) for Java. License: Apache 2 , .\nApache Tiles Apache Tiles is a templating framework built to simplify the development of web application user interfaces. License: Apache 2.\nCMS and content management\nApache Stanbol Software components for semantic content management. License: Apache 2.\nApache JSPWiki A feature-rich and extensible WikiWiki engine built around the standard J2EE components (Java, servlets, JSP). icense: Apache 2.\nApache Portals The Apache Portals project provides various software products, including Apache Jetspeed-2, Apache Pluto, and Apache Portals Applications. License: Apache 2.\nApache Roller Apache Roller is a full-featured, multi-user and group blog server suitable for blog sites large and small. License: Apache 2.\nApache Chemistry Apache Chemistry provides open source implementations of the Content Management Interoperability Services (CMIS) specification. License: Apache 2.\nJease Jease eases the development of content- & database-driven web-applications with Java. License: GPL v3.\n3. GUI\nJavaFX - JavaFX is the next step in the evolution of Java as a rich client platform. It is designed to provide a lightweight, hardware-accelerated Java UI platform for enterprise business applications. License: Oracle, proprietary.\nScene Builder - JavaFX Scene Builder is a visual layout tool that lets users quickly design JavaFX application user interfaces, without coding. Users can drag and drop UI components to a work area, modify their properties, apply style sheets, and the FXML code for the layout that they are creating is automatically generated in the background. The result is an FXML file that can then be combined with a Java project by binding the UI to the application\u2019s logic. License: Oracle, proprietary. , but see Gluon Scene Builder.\nGluon Scene Builder - Scene Builder is free and open source JavaFX Scene Builder, but is backed by Gluon. Commercial support offerings are available, including training and custom consultancy services. License: BSD.\nSWT - SWT is an open source widget toolkit for Java designed to provide efficient, portable access to the user-interface facilities of the operating systems on which it is implemented. License: Eclipse Public License v1.0.\nLanterna - Lanterna is a Java library allowing you to write easy semi-graphical user interfaces in a text-only environment, very similar to the C library curses but with more functionality. License: LGPL 2.1.\nGephi - Gephi is an award-winning open-source platform for visualizing and manipulating large graphs. It runs on Windows, Mac OS X and Linux. Localization is available in French, Spanish, Japanese, Russian, Brazilian Portuguese, Chinese and Czech. License: GNU 3 or CDDL 1.0.\nCodename One - Open-source cross-platform app development solution for writing native apps for desktop, web & mobile in Java/Kotlin. (GPL + Classpath Exception)\n4. Business\nUp\nCodecademy EventHub \u2014 An open source event analytics platform. License: MIT , .\nKillbill - Open-Source Subscription Billing & Payment Platform . License: Apache 2 , .\nActiviti Activiti is a light-weight workflow and Business Process Management (BPM) Platform targeted at business people, developers and system admins. Its core is a super-fast and rock-solid BPMN 2 process engine for Java. License: Apache 2 , .\nLiferay Portal Liferay Portal is an open source enterprise web platform for building business solutions that deliver immediate results and long-term value. Liferay Portal started out as a personal development project in 2000 and was open sourced in 2001. License: GNU Lesser 2.1, .\nBroadleaf Commerce - Broadleaf Commerce - Enterprise eCommerce framework based on Spring. License: Broadleaf Fair Use , .\nDroolsjbpm Drools Drools Expert is the rule engine and Drools Fusion does complex event processing (CEP). License: Apache 2 , .\nSpring Roo \u2014 Spring Roo is a next-generation rapid application development tool for Java developers. It focuses on higher productivity, stock-standard Java APIs, high usability, avoiding engineering trade-offs and facilitating easy Roo removal. License: Apache 2 , .\nApache OFBiz Apache OFBiz (The Apache Open For Business Project) is an open source enterprise automation software project. License: Apache 2.\nApache Portals The Apache Portals project provides various software products, including Apache Jetspeed-2, Apache Pluto, and Apache Portals Applications. License: Apache 2.\nApache ODE Apache ODE is a WS-BPEL implementation that supports web services orchestration using flexible process definitions. License: Apache 2.\n5. Game Development\nUp\nlibgdx Desktop/Android/HTML5/iOS Java game development framework. License: Apache 2 , .\nDisunity An experimental toolset for Unity asset and asset bundle files. License: unlicense.org, .\nLibGDX/LWJGL LibGDX/LWJGL tutorials and examples. License: ?, .\njMonkeyEngine A complete 3D game development suite written purely in Java.. https://jmonkeyengine.org/ License: BSD 3, .\nJetserver Jetserver is a high speed nio socket based multiplayer java game server written using Netty and Mike Rettig's Jetlang.It is specifically tuned for network based multiplayer games and supports TCP and UDP network protocols. License: MIT , .\nLWJGL 2.X LWJGL 2.X - The Lightweight Java Game Library. https://www.lwjgl.org/ License: BSD 3, .\nArdor3D - JogAmp\u2019s Ardor3D Continuation is a general-purpose, professionally oriented, open source, scenegraph based 3D Java engine for desktop and embedded environments, using JOGL for its hardware graphics acceleration. Fork of jMonkeyEngine 2.0. Cross-platform. License: zlib.\nDimensioneX Multiplayer Engine - Produces browser games with pseudo-3D views. Games can be turned into Facebook Apps. Intended for beginners. 2.5D. Cross-platform. Notable games: Underworld Online. License: GPL\nPlayN A Java game development framework that deploys to Windows, Linux, HTML5, Android and iOS. Notable games: Angry Birds Chrome. Cross-platform. License: Apache 2.0. .\nEnv3D - 3D game engine that creates an interface for dynamically adding EnvObjects. Built on jMonkeyEngine 2.0. Cross-platform License: GPL\nJake2 Java port of the Quake II game engine. 2D. Cross-platform License: GPL\nJogre - JOGRE (Java Online Gaming Real-time Engine) which is an open-source, online, real-time gaming engine and API programmed entirely in Java. License: GPL\nElflight Engine - Targeted for web based games. License: Proprietary\nFXGL - JavaFX Game Development Framework. License: MIT, .\n6. Useful libraries\nUp\nCollections\nGoogle Guava The Guava project contains several of Google's core libraries that we rely on in our Java-based projects: collections, caching, primitives support, concurrency libraries, common annotations, string processing, I/O, and so forth.Requires JDK 1.6 or higher (as of 12.0). Hello World examples License: Apache 2 , .\nApache Commons Collections - Commons-Collections seek to build upon the JDK classes by providing new interfaces, implementations and utilities. Hello World examples. License: Apache 2.\nGs collections A supplement or replacement for the Java Collections Framework. Hello World examples. License: Apache 2 , .\nEclipse Collections Eclipse Collections is a collections framework for Java. It has JDK-compatible List, Set and Map implementations with a rich API, additional types not found in the JDK like Bags, Multimaps and set of utility classes that work with any JDK compatible Collections, Arrays, Maps or Strings. The iteration protocol was inspired by the Smalltalk collection framework.Eclipse Collections started off as an open source project on GitHub called GS Collections. GS Collections has been presented at the JVM Language Summit in 2012 and JavaOne in 2014. Hello World examples. License: Eclipse Public 1.0.\njavatuples - javatuples is one of the simplest java libraries ever made. Its aim is to provide a set of java classes that allow you to work with tuples.A tuple is just a sequence of objects that do not necessarily relate to each other in any way. For example: [23, \"Saturn\", java.sql.Connection@li734s] can be considered a tuple of three elements (a triplet) containing an Integer, a String, and a JDBC Connection object. License: Apache 2.\nImmutables - Java annotation processors to generate simple, safe and consistent value objects. Do not repeat yourself, try Immutables, the most comprehensive tool in this field. License: Apache 2.\nOpenHFT Chronicle Queue Micro second messaging that stores everything to disk. License: GNU Lesser 3.0, .\nfastutil - fastutil extends the Java\u2122 Collections Framework by providing type-specific maps, sets, lists and queues with a small memory footprint and fast access and insertion; provides also big (64-bit) arrays, sets and lists, and fast, practical I/O classes for binary and text files. It requires Java 7 or newer. License: Apache 2.\nHPPC - HPPC provides template-generated implementations of typical collections, such as lists, sets and maps, for all Java primitive types. The primary driving force behind HPPC is optimization for highest performance and memory efficiency. License: Apache 2.\nKoloboke - Java Collections till the last breadcrumb of memory and performance . This library is a carefully designed and efficient extension of the Java Collections Framework with primitive specializations and more. Java 6+. License: Apache 2.\nTrove - The Trove library provides high speed regular and primitive collections for Java. License: GNU Lesser 2.1.\nunderscore-java - Underscore-java is a java port of Underscore.js. In addition to porting Underscore's functionality, Underscore-java includes matching unit tests. License: MIT , .\nDate and Time\nAlmanac Converter - An easy-to-use Java-based calendar converter - able to convert between various known calendars. License: Apache 2.\nJoda-Time - Joda-Time provides a quality replacement for the Java date and time classes. License: Apache 2, .\nThreeTenBP - Backport of functionality based on JSR-310 to Java SE 6 and 7. This is NOT an implementation of JSR-310. License: BSD 3.\nTime4J - Advanced date, time and interval library for Java. License: GNU Lesser 2.1.\nOcpsoft Prettytime Social Style Date and Time Formatting for Java.PrettyTime is an OpenSource time formatting library. Completely customizable, it creates human readable, relative timestamps like those seen on Digg, Twitter, and Facebook. Get started \u201cright now!\u201d and in over 30 languages! License: Apache 2 , .\nthreeten-extra Provides additional date-time classes that complement those in JDK 8. business-friendly BSD 3-clause license\nDependency Injection and AOP frameworks\nSpring Framework The Spring Framework provides a comprehensive programming and configuration model for modern Java-based enterprise applications -- on any kind of deployment platform. A key element of Spring is infrastructural support at the application level: Spring focuses on the \"plumbing\" of enterprise applications so that teams can focus on application-level business logic, without unnecessary ties to specific deployment environments. Hello World examples. License: Apache 2 , .\nDagger and Dagger2 A fast dependency injector for Android and Java. Hello World examples. License: Apache 2 , , stackoverflow - 800 questions.\nGoogle Guice - Guice (pronounced 'juice') is a lightweight dependency injection framework for Java 6 and above, brought to you by Google. Hello World examples. License: Apache 2, .\nApache DeltaSpike - DeltaSpike consists of a number of portable CDI extensions that provide useful features for Java application developers. We will also ensure true portability! We are testing DeltaSpike on different CDI implementations like Apache OpenWebBeans and JBoss Weld, and also on different Java EE servers like Apache Tomcat and TomEE, JBoss AS7, WildFly 8.x and 9.x, Oracle GlassFish 3.1+ and 4.x+, IBM WebSphere 8.x, Oracle WebLogic Server 12c, Jetty, and others. License: Apache 2\nHK2 - A light-weight and dynamic dependency injection framework. GNU 2/ CDDL 1.0.\nEclipse AspectJ - AspectJ is a seamless aspect-oriented extension to the Java programming language, Java platform compatible, easy to learn and use. License: Eclipse Public 1.0.\nConsole and Command line\njcommander Command line parsing framework for Java. http://jcommander.org/ License: Apache 2 , .\nJline JLine is a Java library for handling console input. It is similar in functionality to BSD editline and GNU readline. People familiar with the readline/editline capabilities for modern shells (such as bash and tcsh) will find most of the command editing features of JLine to be familiar. License: BSD 4, .\nargs4j - args4j is a small Java class library that makes it easy to parse command line options/arguments in your CUI application. License: MIT\nCRaSH - The shell for the Java Platform Open source and open minde. License: GNU Lesser 2.1\npicocli - Annotation based command line parser with strong typing for both options and positional args and support for git-like subcommands. Usage help with ANSI colors. Easily included as source to avoid external dependencies. License: Apache 2\nFunctional Programming\nLibraries that facilitate functional programming.\nRetrolambda Backport of Java 8's lambda expressions to Java 7, 6 and 5. License: Apache 2 , .\nJavaslang - Javaslang core is a functional library for Java 8+. It helps to reduce the amount of code and to increase the robustness. A first step towards functional programming is to start thinking in immutable values. Javaslang provides immutable collections and the necessary functions and control structures to operate on these values, . License: Apache 2.\njOO\u03bb - jOO\u03bb - The Missing Parts in Java 8 jOO\u03bb improves the JDK libraries in areas where the Expert Group's focus was elsewhere. It adds tuple support, function support, and a lot of additional functionality around sequential Streams, . License: Apache 2.\nFunctional Java - Functional Java is an open source library facilitating functional programming in Java. The library implements numerous basic and advanced programming abstractions that assist composition oriented development. Functional Java also serves as a platform for learning functional programming concepts by introducing these concepts using a familiar language. http://www.functionaljava.org License: BSD 3, .\nCyclops react - A comprehensive functional reactive platform for JDK8. Future & functional based programming via JDK compatible extensions for Java 8 and above, . License: MIT\nFugue -Java 8 has standardised some of the basic function interfaces, but does not include quite a few more tools that a functional programmer may expect to be available. This library attempts to fill in some of the gaps when using Java 8. In particular it provides Option and Either types as well as a Pair. There also additional helper classes for common Function, Supplier, and Iterable operations. License: Apache 2.\nTotallyLazy Another functional library for Java , License: Apache 2, .\nderive4j - Java 8 annotation processor and framework for deriving algebraic data types constructors, pattern-matching, morphisms, (near future: optics and typeclasses), . License: GNU 3.\nReactive Programming\nLibraries for developing reactive applications.\nReactiveX RxJava RxJava \u2013 Reactive Extensions for the JVM \u2013 a library for composing asynchronous and event-based programs using observable sequences for the Java VM. License: Apache 2 , , stackoverflow - 1814 questions.\nEclipse Vert.x Vert.x is a tool-kit for building reactive applications on the JVM . License: Eclipse Public 1 / Apache 2 , , stackoverflow - 728 questions.\nReactive Streams - The purpose of Reactive Streams is to provide a standard for asynchronous stream processing with non-blocking backpressure. License: Public Domain (CC0), .\nReactor - Reactor is a second-generation Reactive library for building non-blocking applications on the JVM based on the Reactive Streams Specification. License: Apache 2.\nRSocket - RSocket is a binary protocol for use on byte stream transports such as TCP, WebSockets, and Aeron. License: Apache 2,\nSecurity and Authentication\nLibraries that handle security, authentication, authorization or session management. Up\nOTP-Java - A small one-time password generator library according to RFC 4226 (HOTP) and RFC 6238 (TOTP). MIT\nScribe Java - Simple OAuth library for Java. License: MIT , .\nSpring security oauth - Support for adding OAuth1(a) and OAuth2 features (consumer and provider) for Spring web applications. License: Apache 2 , .\nJasig CAS (Central Authentication Service) - Apereo CAS - Single Sign On for the Web.The Central Authentication Service (CAS) is the standard mechanism by which web applications should authenticate users. License: Apache 2 , .\nSpring security \u2014 Spring Security provides security services for the Spring IO Platform. Spring Security 3.1 requires Spring 3.0.3 as a minimum and also requires Java 5. License: Apache 2 , .\nPlay Authenticate - An authentication plugin for Play Framework 2.x (Java). License: Apache 2 , .\nApache Shiro - Apache Shiro is a powerful and easy-to-use Java security framework that performs authentication, authorization, cryptography, and session management. With Shiro\u2019s easy-to-understand API, you can quickly and easily secure any application \u2013 from the smallest mobile applications to the largest web and enterprise applications. License: Apache 2.\nBouncy Castle - Legion of the Bouncy Castle Java cryptography APIs.A lightweight cryptography API. License: MIT.\nCryptomator - Free client-side encryption for your cloud files. Open source software: No backdoors, no registration. License: MIT.\nGoogle Keyczar - Easy-to-use crypto toolkit. License: Apache 2.\nKeycloak - Integrated SSO and IDM for browser apps and RESTful web services. Built on top of the OAuth 2.0, Open ID Connect, JSON Web Token (JWT) and SAML 2.0 specifications. Keycloak has tight integration with a variety of platforms and has a HTTP security proxy service where we don't have tight integration. Options are to deploy it with an existing app server, as a black-box appliance, or as an Openshift cloud service and/or cartridge. License: Apache 2.\nPicketLink - Simplified Security and Identity management for Java Applications PicketLink is an umbrella project for security and identity management for Java Applications. It is licensed under a friendly Apache v2 license. License: Apache 2.\nSantuario Library implementing XML Digital Signature Specification & XML Encryption Specification. License: Apache 2.\nApache Oltu ( has retired ) Apache Oltu is an OAuth protocol implementation in Java. License: Apache 2.\nApache Syncope Apache Syncope is an Open Source system for managing digital identities in enterprise environments. License: Apache 2.\nJObfuscator - JObfuscator is a source code obfuscator for the Java language to protect Java source code & algorithms from hacking, cracking, reverse engineering, decompilation & technology theft. License:\nHigh Performance\nEverything about high performance computation, from collections to specific libraries. Up\nAgrona - High Performance data structures and utility methods for Java and C++, License: Apache 2.\nDisruptor - Inter-thread messaging library. License: Apache 2.\nfastutil - fastutil extends the Java Collections Framework by providing type-specific maps, sets, lists and queues with a small memory footprint and fast access and insertion; provides also big (64-bit) arrays, sets and lists, and fast, practical I/O classes for binary and text files. License: Apache 2.\nEclipse Collections (older GS Collections) - Eclipse Collections is a collections framework for Java. It has JDK-compatible List, Set and Map implementations with a rich API and set of utility classes that work with any JDK compatible Collections, Arrays, Maps or Strings. The iteration protocol was inspired by the Smalltalk collection framework. The library modules in GS Collections are compatible with Java 5. License: Apache 2.\nHPPC -HPPC provides template-generated implementations of typical collections, such as lists, sets and maps, for all Java primitive types. The primary driving force behind HPPC is optimization for highest performance and memory efficiency. License: Apache 2.\nJavolution - ibrary for real-time and embedded systems. License: BSD 2.\nJCTools - Concurrency tools currently missing from the JDK. License: Apache 2.\nKoloboke - Hash sets and hash maps. License: Apache 2.\nTrove - Primitive collections. License: GNU Lesser 2.1.\nJUnion - Delivers struct types for Java programming language. 64 bit addressable struct arrays. https://tehleo.github.io/junion/ License: [License: BSD 3.\nSerialization and I/O\nLibraries that handle serialization with high efficiency. Up\nSquare Okio A modern I/O API for Java. License: Apache 2 , .\nSquare type A lightning fast, transactional, file-based FIFO for Android and Java. License: Apache 2 , .\nSimple Binary Encoding (SBE) Simple Binary Encoding (SBE) - High Performance Message Codec. License: Apache 2 , .\nMessagePack - MessagePack serializer implementation for Java. License: Apache 2 , .\nFlatBuffers - Memory efficient serialization library that can access serialized data without unpacking and parsing it. License: Apache 2.\nFST - JDK compatible high performance object graph serialization. License: Apache 2.\nKryo - Fast and efficient object graph serialization framework. License: BSD 3.\nLogging\nLibraries that log the behavior of an application. Up\nGraylog2 server Free and open source log management. License: GNU 3, .\nlogback The reliable, generic, fast and flexible logging framework for Java.. http://logback.qos.ch/. License: Eclipse Public 1.0/GNU Lesser 2.1, .\nslf4j Abstraction layer which is to be used with an implementation. http://www.slf4j.org/ License: MIT , .\nApache Log4j 2 - Apache Log4j 2 is an upgrade to Log4j that provides significant improvements over its predecessor. License: Apache 2.\ngraylog - Open-source aggregator suited for extended role and permission management. License: GNU 3.\nElastic Kibana - Analyzes and visualizes log files. Some features require payment. License: Apache 2.\nElastic Logstash - Tool for managing log files. License: Apache 2.\nMetrics - Expose metrics via JMX or HTTP and can send them to a database. License: Apache 2.\ntinylog - Lightweight logging framework with static logger class. License: Apache 2.\nBean Mapping and Validation\nFrameworks that ease bean mapping or bean validation. Up\nDozer - Dozer is a Java Bean to Java Bean mapper that recursively copies data from one object to another. Hello World examples. License: Apache 2, .\nMapStruct - An annotation processor for generating type-safe bean mappers. Hello World examples. License: Apache 2, .\nModelMapper -Simple, Intelligent, Object Mapping. Hello World examples. License: Apache 2, .\nOrika - Simpler, better and faster Java bean mapping framework. Hello World examples. License: Apache 2, .\nSelma - Stupid Simple Statically Linked Mapper. Hello World examples. Selma Java bean mapping that compiles. License: Apache 2, .\nApache BVal Apache BVal delivers an implementation of the Java Bean Validation (JSR303) Specification which is TCK compliant, works on Java SE 5 or later. License: Apache 2.\nOVal \u2013 is a pragmatic and extensible validation framework for any kind of Java objects. Not JSR303/JSR380 compliant but supports Bean Validation constraints. It's pretty old flexible and reliable. License: EPL v2.0.\n7. Imagery and Video\nLibraries that assist with the creation, evaluation or manipulation of graphical images or video. Up\nFacebook Rebound A Java library that models spring dynamics and adds real world physics to your app. License: BSD, .\nProcessing Source code for the Processing Development Environment (PDE) . License: GNU 2, .\nJavaCV JavaCV uses wrappers from the JavaCPP Presets of commonly used libraries by researchers in the field of computer vision (OpenCV, FFmpeg, libdc1394, PGR FlyCapture, OpenKinect, videoInput, ARToolKitPlus, and flandmark), and provides utility classes to make their functionality easier to use on the Java platform, including Android. License: Apache 2 , .\nimgscalr Simple Java image-scaling library implementing Chris Campbell's incremental scaling algorithm as well as Java2D's \"best-practices\" image-scaling techniques. License: Apache 2 , .\nwebcam-capture Project goal is to give users possibility to access build-in or connected via USB webcams or remote IP / network cameras directly from Java code. Using provided libraries user is able to read -----> camera !!!  images and detect motion. License: MIT , .\nThumbnailator - Thumbnailator's fluent interface can be used to perform fairly complicated thumbnail processing task in one simple step.\nzxing ZXing (\"zebra crossing\") is an open-source, multi-format 1D/2D barcode image processing library implemented in Java, with ports to other languages. License: Apache 2 , .\n8. Code generation and changing byte code\nLibraries to manipulate bytecode programmatically. Up\nGoogle Auto - A collection of source code generators for Java. License: Apache 2 , .\nSquare Javapoet - JavaPoet is a Java API for generating .java source files. License: Apache 2 , .\nByte buddy - Runtime code generation for the Java virtual machine. http://bytebuddy.net/ License: Apache 2 , .\nASM - ASM is an all purpose Java bytecode manipulation and analysis framework. It can be used to modify existing classes or dynamically generate classes, directly in binary form. Provided common transformations and analysis algorithms allow to easily assemble custom complex transformations and code analysis tools. License: BSD 3 / License: Apache 2.\nByteman - Byteman is a tool which makes it easy to trace, monitor and test the behaviour of Java application and JDK runtime code. It injects Java code into your application methods or into Java runtime methods without the need for you to recompile, repackage or even redeploy your application. License: LGPL 2.1.\nJavassist -Javassist (Java Programming Assistant) makes Java bytecode manipulation simple. It is a class library for editing bytecodes in Java; it enables Java programs to define a new class at runtime and to modify a class file when the JVM loads it. License: Apache 2 or LGPL or later 2.1 or Mozilla Public License 1.1..\nADT4J - This library implements Algebraic Data Types for Java. ADT4J provides annotation processor for @GenerateValueClassForVisitor annotation. ADT4J generates new class for each @GenerateValueClassForVisitor annotation. License: BSD 3.\nJHipster - Hipster stack for Java developers. Spring Boot + AngularJS in one handy generator. License: Apache 2.\ncglib - cglib - Byte Code Generation Library is high level API to generate and transform Java byte code. It is used by AOP, testing, data access frameworks to generate dynamic proxy objects and intercept field access. License: Apache 2.\n9. Distributed Applications\nLibraries and frameworks for writing distributed and fault-tolerant applications. Up\nAkka - Akka is a toolkit and runtime for building highly concurrent, distributed, and resilient message-driven applications on the JVM. License: Apache 2.\nApache Storm - Apache Storm is a free and open source distributed realtime computation system. Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Storm is simple, can be used with any programming language, and is a lot of fun to use! . License: Apache 2.\nTwitter Heron - Heron is realtime analytics platform developed by Twitter. It is the direct successor of Apache Storm, built to be backwards compatible with Storm's topology API but with a wide array of architectural improvements. . http://heronstreaming.io License: Apache 2.\nApache ZooKeeper - ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications. License: Apache 2.\nAlibaba Dubbo Dubbo is a distributed service framework empowers applications with service import/export capability with high performance RPC. License: Apache 2 , .\nNetflix Curator ZooKeeper client wrapper and rich ZooKeeper framework. License: Apache 2 , .\nAxon Framework - The axon framework is focussed on making life easier for developers that want to create a java application based on the CQRS principles. The framework is used in a lot of environments. License: Apache 2.\nHazelcast - The Leading Open Source In-Memory Data Grid: Distributed Computing, Simplified. License: Apache 2.\nNetflix Hystrix - Hystrix is a latency and fault tolerance library designed to isolate points of access to remote systems, services and 3rd party libraries, stop cascading failure and enable resilience in complex distributed systems where failure is inevitable, . License: Apache 2.\nJGroups - JGroups is a toolkit for reliable messaging. It can be used to create clusters whose nodes can send messages to each other. License: Apache 2.\nOrbit - Orbit is a modern framework for JVM languages that makes it easier to build and maintain distributed and scalable online services. It was developed by BioWare, a division of Electronic Arts. License: BSD 3.\nQuasar - Quasar is an open source JVM library that greatly simplifies the creation of highly concurrent software that is very easy to write and reason about, performant, and fault tolerant. License: Eclipse Public License v1.0 or LGPL 3.0.\nStorm contrib A collection of spouts, bolts, serializers, DSLs, and other goodies to use with Storm. License: Eclipse Public 1.0, .\nJADE - JADE (Java Agent DEvelopment Framework) is a software Framework fully implemented in the Java language. It simplifies the implementation of multi-agent systems through a middle-ware that complies with the FIPA specifications and through a set of graphical tools that support the debugging and deployment phases. License: GNU Lesser 2.0.\nApache River Apache River software provides a standards-compilani JINI service. License: Apache 2.\nApache Tuscany Apache Tuscany simplifies the task of developing SOA solutions by providing a comprehensive infrastructure for SOA development and management that is based on Service Component Architecture (SCA) standard. License: Apache 2.\nLagom - Reactive Microservices for the JVM. https://www.lightbend.com/lagom . License: Apache 2, .\n10. Science\nLibraries for scientific computing and analysis. Up\nDataMelt - DataMelt is a free mathematics software for scientists, engineers and students. It can be used for numeric computation, statistics, symbolic calculations, data analysis and data visualization. License: GNU 3.\nJGraphT - JGraphT is a free Java class library that provides mathematical graph-theory objects and algorithms. It runs on Java 2 Platform (requires JDK 1.7 or later). License: Eclipse Public License or LGPL 2.1.\nJScience - Provides a set of classes to work with scientific measurements and units. License: BSD 2.\nMichael Thomas Flanagan's Java Scientific Library General classes of a Java scientific and numerical library written by Michael Thomas Flanagan to support both his own research and his undergraduate programming courses and projects. License: only for non-commercial use.\n11. OSGI\nACE Apache ACE is a software distribution framework that allows you to centrally manage and distribute software components, configuration data and other artifacts to target systems. It is built using OSGi and can be deployed in different topologies. The target systems are usually also OSGi based, but don't have to be. License: Apache 2.\nAries The Aries project consists of a set of pluggable Java components enabling an enterprise OSGi application programming model. License: Apache 2.\nFelix OSGi framework implementation and related technologies. License: Apache 2.\nKaraf Apache Karaf is an OSGi distribution for server-side applications. License: Apache 2.\nEquinox OSGI framework implementation and related technologies. License: EPL\nEclipse Virgo OSGI based server that is designed to run enterprise Java applications and Spring-powered applications. License: EPL\nII. Databases, search engines, big data and machine learning\n1. Databases and storages\nEverything which simplifies interactions with the database. Up\nThinkaurelius Titan - Titan is a highly scalable graph database optimized for storing and querying large graphs with billions of vertices and edges distributed across a multi-machine cluster. Titan is a transactional database that can support thousands of concurrent users, complex traversals, and analytic graph queries. License: Apache 2 , .\nApache Cassandra - Cassandra is a partitioned row store. Rows are organized into tables with a required primary key. License: Apache 2 , .\nOrientdb OrientDB is the first Multi-Model DBMS with Document & Graph engine. OrientDB can run distributed (Multi-Master), supports SQL, ACID Transactions, Full-Text indexing, Reactive Queries and has a small memory footprint. OrientDB is licensed with Apache 2 license and the development is driven by OrientDB LTD and a worldwide Open Source community. License: Apache 2/CDDL 1/Eclipse Distribution 1.0, .\nNeo4j \u2014 Neo4j is the world\u2019s leading Graph Database. It is a high performance graph store with all the features expected of a mature and robust database, like a friendly query language and ACID transactions. License: GNU 3/ GNU AGPLv3, .\nMapdb MapDB provides concurrent Maps, Sets and Queues backed by disk storage or off-heap-memory. It is a fast and easy to use embedded Java database engine. http://www.mapdb.org/. License: Apache 2 , .\nVoldemort An open source clone of Amazon's Dynamo. Voldemort is a distributed key-value storage system. License: Apache 2 , .\nAlluxio (formerly Tachyon) Memory-Speed Virtual Distributed Storage System. License: Apache 2 , .\nOpentsdb A scalable, distributed Time Series Database. License: GNU 3, .\nHazelcast Open Source In-Memory Data Grid. License: Apache 2 , .\nTinkerpop Blueprints A Property Graph Model Interface. It provides implementations, test suites, and supporting extensions. Graph databases and frameworks that implement the Blueprints interfaces automatically support Blueprints-enabled applications. Likewise, Blueprints-enabled applications can plug-and-play different Blueprints-enabled graph backends. License: BSD 3, .\nApache Lucene solr Apache Lucene/Solr. Lucene is a search engine library Solr is a search engine server that uses lucene. License: Apache 2 , .\nJava Chronicle Java Indexed Record Chronicle \u2014 This library is an ultra low latency, high throughput, persisted, messaging and event driven in memory database. License: Apache 2 , .\nTorodb ToroDB - Open source NoSQL database that runs on top of a RDBMS. Compatible with MongoDB protocol and APIs, but with support for native SQL, atomic operations and reliable and durable backends like PostgreSQL. License: GNU AGPLv3, .\nCrate Crate.IO: The fast, scalable, easy to use SQL database with native full text search. https://crate.io/ .License: Apache 2 , .\nLinkedin Pinot A realtime distributed OLAP datastore. Pinot is a realtime distributed OLAP datastore, which is used at LinkedIn to deliver scalable real time analytics with low latency. License: Apache 2 , .\nSolandra Solandra is a real-time distributed search engine built on Apache Solr and Apache Cassandra. License: Apache 2 , .\nVoltdb VoltDB is a horizontally-scalable, in-memory SQL RDBMS designed for applications that have extremely high read and write throughput requirements. License: GNU AGPLv3, .\nLeveldb This is a rewrite (port) of LevelDB in Java. This goal is to have a feature complete implementation that is within 10% of the performance of the C++ original and produces byte-for-byte exact copies of the C++ code.. License: Apache 2 , .\nKairosdb KairosDB is a fast distributed scalable time series database written on top of Cassandra. License: Apache 2 , .\nLinkedin Sensei Sensei is a distributed, elastic realtime searchable database. License: Apache 2 , .\nElephantdb Distributed database specialized in exporting key/value data from Hadoop. License: BSD 3, .\nApache Drill Apache Drill is a distributed MPP query layer that supports SQL and alternative query languages against NoSQL and Hadoop data storage systems. License: Apache 2 , .\nTinkerpop Rexster Rexster is a graph server that exposes any Blueprints graph through HTTP/REST and a binary protocol called RexPro. Extensions support standard traversal goals such as search, score, rank, and, in concert, recommendation. License: BSD 3, .\nTomcat redis session manager Redis-backed non-sticky session store for Apache Tomcat. License: MIT , .\nEmbulk Embulk is a parallel bulk data loader that helps data transfer between various storages, databases, NoSQL and cloud services. License: Apache 2 , .\nH2 - Welcome to H2, the Java SQL database. The main features of H2 are: Very fast, open source, JDBC API, Embedded and server modes; in-memory databases, Browser based Console application,Small footprint: around 1.5 MB jar file size License: Mozilla Public License 1.1. and Eclipse Public License v1.0.\nApache Derby - Apache Derby, an Apache DB subproject, is an open source relational database implemented entirely in Java. Derby provides an embedded JDBC driver that lets you embed Derby in any Java-based solution. License: Apache 2.\nApache Empire-db Apache Empire-db is a lightweight relational database abstraction layer and data persistence component. License: Apache 2.\nApache Ignite Apache Ignite is an In-Memory Data Fabric providing in-memory data caching, partitioning, processing, and querying components. License: Apache 2.\nTarantool \u2014 is an open-source NoSQL database management system and Lua application server. It maintains databases in memory and ensures crash resistance with write-ahead logging. It includes a Lua interpreter and interactive console but also accepts connections from programs in several other languages. License: BSD licenses.\nDistributed Databases.\nDatabases in a distributed system that appear to applications as a single data source.\nApache Cassandra - The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance. License: Apache 2 , .\nApache HBase - Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google's Bigtable: A Distributed Storage System for Structured Data by Chang et al. License: Apache 2.\nDruid - Druid is a fast column-oriented distributed data store. http://druid.io License: Apache 2, .\nInfinispan - Infinispan is a distributed in-memory key/value data store with optional schema. It can be used both as an embedded Java library and as a language-independent service accessed remotely over a variety of protocols (HotRod, REST, Memcached and WebSockets). License: Apache 2.\nOpenTSDB - The Scalable Time Series Database Store and serve massive amounts of time series data without losing granularity. http://opentsdb.net License: GNU 3, .\n2. Data structures\nEfficient and specific data structures.\nApache Avro - Apache Avro is a data serialization system. License: Apache 2.\nApache Orc - The smallest, fastest columnar storage for Hadoop workloads. License: Apache 2.\nApache Parquet - Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language. License: Apache 2.\nApache Thrift -The Apache Thrift software framework, for scalable cross-language services development, combines a software stack with a code generation engine to build services that work efficiently and seamlessly between C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, JavaScript, Node.js, Smalltalk, OCaml and Delphi and other languages. License: Apache 2.\nPersistent Collection - PCollections serves as a persistent and immutable analogue of the Java Collections Framework. License: MIT.\nProtobuf - Protocol Buffers (a.k.a., protobuf) are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data. License: BSD 3.\nSBE - Simple Binary Encoding (SBE) - High Performance Message Codec, License: Apache 2.\nWire - Clean, lightweight protocol buffers for Android and Java. License: Apache 2.\n3. Search engines\nUp\nElasticsearch - Elasticsearch is a distributed RESTful search engine built for the cloud. License: Apache 2 , .\nLinkedin Indextank Engine This project contains IndexTank (http://www.searchify.com) search engine implementation. Includes features like variables (boosts), categories (facets), faceted search, snippeting, custom scoring functions, suggest, and autocomplete. License: Apache 2 , .\nApache Solr Solr is the popular, blazing-fast, open source enterprise search platform built on Apache Lucene. http://lucene.apache.org/solr/ License: Apache 2 , .\nLinkedin Cleo A flexible, partial, out-of-order and real-time typeahead search library. License: Apache 2 , .\nElasticsearch cloud aws AWS (Amazon Web Service) Cloud Plugin for Elasticsearch. License: Apache 2 , .\nElasticsearch analysis ik The IK Analysis plugin integrates Lucene IK analyzer into elasticsearch, support customized dictionary. License: ?, .\nApache ManifoldCF Open-source software for transferring content between repositories or search indexes License: Apache 2.\nLemur Project The Lemur Project develops search engines, browser toolbars, text analysis tools, and data resources that support research and development of information retrieval and text mining software, including the Indri search engine and ClueWeb09 dataset. License: BSD License\nYaCy - The YaCy Search Engine application, http://yacy.net/ . License: GNU Lesser and GNU\n4. Client and drivers for databases\nUp\nFacebook Presto Distributed SQL query engine for big data (Cassandra, Hive, Kafka, MySQL, PostgreSQL and so on). License: Apache 2 , .\nJedis A blazingly small and sane redis java client. License: MIT , .\nTinkerpop Gremlin A Graph Traversal Language. Gremlin is a domain specific language for traversing property graphs. Gremlin makes use of Pipes to perform complex graph traversals. This language has application in the areas of graph query, analysis, and manipulation. License: BSD 3 , .\nMongodb Java Driver The Java driver for MongoDB. License: Apache 2 / Creative Commons Attribution , .\nYahoo Mysql_perf_analyzer MySQL Performance Analyzer is an open source project for MySQL performance monitoring and analysis. License: Apache 2 , .\nAirbnb Airpal Web UI for PrestoDB. License: Apache 2 , .\nMongodb hadoop MongoDB Connector for Hadoop. License: ?, .\nElasticsearch jdbc JDBC importer for Elasticsearch. License: Apache 2 , .\nFlyway \u2014 Flyway by Boxfuse. Database Migrations Made Easy. https://flywaydb.org/ . License: Apache 2 , .\nElasticsearch river mongodb - MongoDB River Plugin for ElasticSearch. License: Apache 2 , .\nNetflix Astyanax Cassandra Java Client. License: Apache 2 , .\nLiquibase Liquibase \u2014 source control for your database, supports multiple developers, database types and so on. http://www.liquibase.org/ . License: Apache 2 , .\nMongodb Morphia MongoDB object-document mapper in Java based on. License: Apache 2 , .\nCouchdb lucene Enables full-text searching of CouchDB documents using Lucene. License: Apache 2 , .\nForcedotcom Phoenix Phoenix is a SQL skin over HBase, delivered as a client-embedded JDBC driver, powering the HBase use cases at Salesforce.com. Phoenix targets low-latency queries (milliseconds), as opposed to batch operation via map/reduce. License: BSD 3 , .\nVariety A schema analyzer for MongoDB. License: MIT , .\nRedisson Redisson - distributed Java objects (Bloom filter, BitSet, Set, SetMultimap, ScoredSortedSet, SortedSet, Map, ConcurrentMap, List, ListMultimap, Queue, BlockingQueue, Deque, BlockingDeque, Semaphore, Lock, ReadWriteLock, AtomicLong, CountDownLatch, Publish / Subscribe, HyperLogLog), Redis pipelining on top of Redis server. License: Apache 2 , .\nNetflix Priam Co-Process for backup/recovery, Token Management, and Centralized Configuration management for Cassandra. License: Apache 2 , .\nDatastax Java driver DataStax Java Driver for Apache Cassandra. License: Apache 2 , .\nMemcached Java Client to be the best java client for memcached. License: BSD 4, .\nSpring data mongodb Provide support to increase developer productivity in Java when using MongoDB. Uses familiar Spring concepts such as a template classes for core API usage and lightweight repository style data access. License: Apache 2 , .\nApache Phoenix - Apache Phoenix enables OLTP and operational analytics in Hadoop for low latency applications by combining the best of both worlds, License: Apache 2.\nFlexyPool - FlexyPool adds metrics and failover strategies to a given Connection Pool, allowing it to resize on demand. License: Apache 2.\nLightAdmin - Pluggable data administration UI library for Java web applications. License: Apache 2.\nJaybird JCA/JDBC driver - JCA-JDBC Driver for Firebird database. http://www.firebirdsql.org/en/jdbc-driver/, . License: GNU Lesser 2.1.\ndbeaver - Free universal database manager and SQL client. http://dbeaver.jkiss.org. , . License: GNU 2.\ntarantool-java A Java client for Tarantool. License: BSD licenses.\n5. ORM\nAPIs which handle the persistence of objects. Up\nHikariCP A solid high-performance JDBC connection pool at last. License: Apache 2 , .\nMybatis 3 MyBatis SQL mapper framework for Java. License: Apache 2 , .\nHibernate orm Hibernate's core Object/Relational Mapping functionality. http://hibernate.org/orm/ License: GNU Lesser 2.1, .\nJOOQ OOQ is an innovative solution for a better integration of Java applications with popular databases like Oracle, Microsoft SQL Server, IBM DB2, or SAP Sybase. http://www.jooq.org/ . License: Apache 2 , .\nQuerydsl Querydsl is a framework which enables the construction of type-safe SQL-like queries for multiple backends including JPA, MongoDB and SQL in Java. http://www.querydsl.com/ . License: Apache 2 , .\nKundera A JPA 2.1 compliant Polyglot Object-Datastore Mapping Library for NoSQL Datastores. License: Apache 2 , .\nSpring data jpa Simplifies the development of creating a JPA-based data access layer. License: Apache 2 , .\nJdbi jDBI is designed to provide convenient tabular data access in Java(tm). It uses the Java collections framework for query results, provides a convenient means of externalizing sql statements, and provides named parameter support for any database being used. http://jdbi.org/ .License: Apache 2 , .\nActiveJDBC ActiveJDBC is a fast and lean Java ORM. License: Apache 2 , .\nVibur DBCP - Concurrent, fast, and fully-featured JDBC connection pool, which provides a non-starvation guarantee for application threads, statement caching, slow SQL queries detection and logging, and Hibernate integration, among other features. License: Apache 2.\nEbean - Ebean ORM for Java/Kotlin.Fast and simple data access on the JVM. License: Apache 2.\nEclipseLink - Comprehensive open-source Java persistence solution addressing relational, XML, and database web services. License: Eclipse Public License v1.0 and BSD 3.\nOrmLite - Object Relational Mapping Lite (ORM Lite) provides some simple, lightweight functionality for persisting Java objects to SQL databases while avoiding the complexity and overhead of more standard ORM packages. License: ISC.\nApache Cayenne User-friendly Java ORM with tools. License: Apache 2.\nApache OpenJPA Java Persistence API Implementation. License: Apache 2.\nApache MetaModel With MetaModel you get a uniform connector and query API to many very different datastore types, including: Relational (JDBC) databases, CSV files, Excel spreadsheets, XML files, JSON files, Fixed width files, MongoDB, Apache CouchDB, Apache HBase, Apache Cassandra, ElasticSearch, OpenOffice.org databases, Salesforce.com, SugarCRM and even collections of plain old Java objects (POJOs). License: Apache 2.\n6. Working with messy data\nUp\nStorm - Distributed and fault-tolerant realtime computation: stream processing, continuous computation, distributed RPC, and more. License: Apache 2 , .\nOpenRefine \u2014 OpenRefine is a free, open source power tool for working with messy data and improving it. License: BSD , .\nAddthis Stream lib - Stream summarizer and cardinality estimator. License: Apache 2 , .\nHdrHistogram (A High Dynamic Range (HDR) Histogram ) \u2014 HdrHistogram: A High Dynamic Range (HDR) Histogram: HdrHistogram supports the recording and analyzing of sampled data value counts across a configurable integer value range with configurable value precision within the range. License: BSD 2 , .\nHazyResearch DeepDive DeepDive is a system to extract value from dark data. Like dark matter, dark data is the great mass of data buried in text, tables, figures, and images, which lacks structure and so is essentially unprocessable by existing software. License: Apache 2 , .\nApache Incubator Zeppelin Zeppelin, a web-based notebook that enables interactive data analytics. You can make beautiful data-driven, interactive and collaborative documents with SQL, Scala and more. License: Apache 2 , .\nSeldon Server Enterprise machine learning platform for prediction and recommendation on Apache Spark. License: Apache 2 , .\nPulsar - Realtime analytics, this includes the core components of Pulsar pipeline. License: GNU 2.0, .\nSuro: Netflix's Data Pipeline \u2014 Suro: Netflix's Data Pipeline. Suro is a data pipeline service for collecting, aggregating, and dispatching large volume of application events including log data. License: Apache 2 , .\nApache UIMA Annotator components and a scalable integration and deployment framework for Unstructured Information analysis. License: Apache 2.\n7. Big data\nUp\nFrameworks and libraries for big data\nApache Storm - Storm is a distributed realtime computation system. Similar to how Hadoop provides a set of general primitives for doing batch processing, Storm provides a set of general primitives for doing realtime computation.License: Apache 2 , .\nTwitter Heron - Heron is realtime analytics platform developed by Twitter. It is the direct successor of Apache Storm, built to be backwards compatible with Storm's topology API but with a wide array of architectural improvements. . http://heronstreaming.io License: Apache 2.\nH2o h2o = fast statistical, machine learning & math runtime for bigdata. License: Apache 2 , .\nCloudera Oryx The Oryx open source project provides simple, real-time large-scale machine learning / predictive analytics infrastructure. It implements a few classes of algorithm commonly used in business applications: collaborative filtering / recommendation, classification / regression, and clustering. License: Apache 2 , .\nTwitter Elephant bird - Twitter's collection of LZO and Protocol Buffer-related Hadoop, Pig, Hive, and HBase code. License: Apache 2 , .\nApache Hadoop - The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. License: Apache 2 , .\nGoogle Mr4c MR4C is an implementation framework that allows you to run native code within the Hadoop execution framework. License: GNU Lesser 3, .\nAlibaba Jstorm \u2014 JStorm is a distributed and fault-tolerant realtime computation system. Inspired by Apache Storm, JStorm has been completely rewritten in Java and provides many more enhanced features. JStorm has been widely used in many enterprise environments and proved robust and stable. License: Apache 2 , .\nEtsy Oculus Oculus is the anomaly correlation component of Etsy's Kale system. License: MIT , .\nLinkedin Datafu Apache DataFu is a collection of libraries for working with large-scale data in Hadoop. The project was inspired by the need for stable, well-tested libraries for data mining and statistics. License: Apache 2 , .\nLinkedin Gobblin Universal data ingestion framework for Hadoop. License: Apache 2 , .\nApache Flink - Apache Flink is an open source stream processing framework with powerful stream- and batch-processing capabilities. License: Apache 2 , .\nOryx 2 - Oryx 2: Lambda architecture on Apache Spark, Apache Kafka for real-time large scale machine learning. License: Apache 2 , .\nYahoo SAMOA (Scalable Advanced Massive Online Analysis) \u2014 SAMOA (Scalable Advanced Massive Online Analysis) is an open-source platform for mining big data streams. License: Apache 2 , .\nCurator A set of Java libraries that make using Apache ZooKeeper much easier. License: Apache 2.\nApache Hama Hama is an efficient and scalable general-purpose BSP computing engine. License: Apache 2.\nApache Falcon Data management and processing platform for Hadoop. License: Apache 2.\nApache Knox A REST API Gateway for Hadoop Services. License: Apache 2.\nApache Flume Apache Flume is a distributed, reliable, and available system for efficiently collecting, aggregating and moving large amounts of log data from many different sources to a centralized data store. License: Apache 2.\nApache Tajo Tajo is an open source big data warehouse system in Hadoop for processing web-scale data sets. License: Apache 2.\nApache Tez Apache Tez is an effort to develop a generic application framework which can be used to process arbitrarily complex directed-acyclic graphs (DAGs) of data-processing tasks and also a re-usable set of data-processing primitives which can be used by other projects. License: Apache 2.\nApache REEF Apache REEF (Retainable Evaluator Execution Framework) is a scale-out computing fabric that eases the development of Big Data applications on top of resource managers such as Apache YARN and Mesos. License: Apache 2.\nApache Pig Apache Pig is a platform for analyzing large data sets on Hadoop. License: Apache 2.\nData store, database, search or SQL-like query engine for big data.\nDruid Column oriented distributed data store ideal for powering interactive applications. License: Apache 2 , .\nApache Hive - The Apache Hive data warehouse software facilitates querying and managing large datasets residing in distributed storage. Built on top of Apache Hadoop. https://hive.apache.org/ .License: Apache 2 , .\nApache Kylin Apache Kylin is an open source Distributed Analytics Engine to provide SQL interface and multi-dimensional analysis (OLAP) on Hadoop supporting extremely large datasets. License: Apache 2 , .\nElasticsearch hadoop - Elasticsearch real-time search and analytics natively integrated with Hadoop. License: Apache 2 , .\nFacebook Presto Presto is a distributed SQL query engine for big data. License: Apache 2 , .\nApache MetaModel With MetaModel you get a uniform connector and query API to many very different datastore types, including: Relational (JDBC) databases, CSV files, Excel spreadsheets, XML files, JSON files, Fixed width files, MongoDB, Apache CouchDB, Apache HBase, Apache Cassandra, ElasticSearch, OpenOffice.org databases, Salesforce.com, SugarCRM and even collections of plain old Java objects (POJOs). License: Apache 2.\nApache Accumulo Apache Accumulo is based on Google\u2019s BigTable design and is built on top of Apache Hadoop, Zookeeper, and Thrift. Apache Accumulo features a few novel improvements on the BigTable design in the form of cell-based access control and a server-side programming mechanism that can modify key/value pairs at various points in the data management process. License: Apache 2.\nApache Gora provides an in-memory data model and persistence for big data. Gora supports persisting to column stores, key value stores, document stores and RDBMSs, and analyzing the data with extensive Apache Hadoop MapReduce support. License: Apache 2.\n[Apache Sqoop] Apache Sqoop(TM) is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases.(http://sqoop.apache.org) License: Apache 2.\nGiraph Apache Giraph is an iterative graph processing system built for high scalability. For example, it is currently used at Facebook to analyze the social graph formed by users and their connections. \u041b\u0438\u0446\u0435\u043d\u0437\u0438\u044f: Apache 2.\nImpala - Real-time Query for Hadoop. . License: Apache 2.\nMonitoring, testing and managing tools for big data\nTwitter Ambrose A platform for visualization and real-time monitoring of data workflows. License: Apache 2 , .\nUmongo - Desktop app to browse and administer your MongoDB cluster. License: Apache 2 , .\nApache Ambari The Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs. License: Apache 2.\nApache Chukwa Chukwa is an open source data collection system for monitoring large distributed systems. Chukwa is built on top of the Hadoop Distributed File System (HDFS) and Map/Reduce framework and inherits Hadoop\u2019s scalability and robustness. Chukwa also includes a \ufb02exible and powerful toolkit for displaying, monitoring and analyzing results to make the best use of the collected data. License: Apache 2.\nApache Bigtop Bigtop is an Apache Foundation project for Infrastructure Engineers and Data Scientists looking for comprehensive packaging, testing, and configuration of the leading open source big data components. License: Apache 2.\nApache BookKeeper BookKeeper is a replicated log service which can be used to build replicated state machines. A log contains a sequence of events which can be applied to a state machine. BookKeeper guarantees that each replica state machine will see all the same entries, in the same order. License: Apache 2.\nApache Crunch Provides a framework for writing, testing, and running MapReduce pipelines. License: Apache 2.\nApache MRUnit Apache MRUnit is a Java library that helps developers unit test Apache Hadoop map reduce jobs. License: Apache 2.\nApache Oozie Oozie is a workflow scheduler system to manage Apache Hadoop jobs. License: Apache 2.\n8. Machine Learning\nTools that provide specific statistical algorithms which allow learning from data. Up\nAirbnb Aerosolve A machine learning library designed from the ground up to be human friendly. License: Apache 2 , .\nSmile Smile (Statistical Machine Intelligence and Learning Engine) is a set of pure Java libraries of various state-of-art machine learning algorithms. Smile is self contained and requires only Java standard library. Hello World examples. License: Apache 2 , .\nDeeplearning4j Deeplearning4J is an Apache 2.0-licensed, open-source, distributed neural net library written in Java and Scala. http://deeplearning4j.org/ .License: Apache 2 , .\nLibsvm Libsvm is a simple, easy-to-use, and efficient software for SVM classification and regression. It solves C-SVM classification, nu-SVM classification, one-class-SVM, epsilon-SVM regression, and nu-SVM regression. License: BSD 3, .\nNeuralnetworks java deep learning algorithms and deep neural networks with gpu acceleration. License: MIT , .\nDatumbox framework Datumbox is an open-source Machine Learning framework written in Java which allows the rapid development of Machine Learning and Statistical applications. License: Apache 2 , .\nApache Mahout The Apache Mahout project's goal is to build an environment for quickly creating scalable performant machine learning applications. https://mahout.apache.org/ .License: Apache 2 , .\nEncog java core Encog is an advanced machine learning framework that supports a variety of advanced algorithms, as well as support classes to normalize and process data. Machine learning algorithms such as Support Vector Machines, Artificial Neural Networks, Genetic Programming, Bayesian Networks, Hidden Markov Models, Genetic Programming and Genetic Algorithms are supported. License: Apache 2 , .\nApache Flink - Apache Flink is an open source stream processing framework with powerful stream- and batch-processing capabilities. https://flink.apache.org/ Subproject: FlinkML - FlinkML is the Machine Learning (ML) library for Flink. License: Apache 2 , .\nApache Hadoop - The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. http://hadoop.apache.org/ License: Apache 2 , .\nApache Spark - Apache Spark\u2122 is a fast and general engine for large-scale data processing. Subproject: MLlib. License: Apache 2.\nDeepDive - DeepDive is a system to extract value from dark data. Like dark matter, dark data is the great mass of data buried in text, tables, figures, and images, which lacks structure and so is essentially unprocessable by existing software. License: Creative Commons Attribution 4.0.\nH2o h2o = fast statistical, machine learning & math runtime for bigdata. http://www.h2o.ai/ License: Apache 2 , .\nJSAT - Java Statistical Analysis Tool, a Java library for Machine Learning. License: GNU 3.\nWeka - Weka is a collection of machine learning algorithms for data mining tasks. The algorithms can either be applied directly to a dataset or called from your own Java code. Weka contains tools for data pre-processing, classification, regression, clustering, association rules, and visualization. It is also well-suited for developing new machine learning schemes. License: GNU.\nProt\u00e9g\u00e9 - A free, open-source ontology editor and framework for building intelligent systems. License: BSD 2.\nEvA2 - A Java based framework for Evolutionary Algorithms -- formerly known as JavaEvA/EvA. License: GNU LESSER 3.\nhtm.java - Hierarchical Temporal Memory implementation in Java - an official Community-Driven Java port of the Numenta Platform for Intelligent Computing (NuPIC). License: GNU AFFERO 3.\nJAVA-ML - Java Machine Learning Library (Java-ML). License: GNU 2.\nJSAT - Numerous Machine Learning algorithms for classification, regression, and clustering. License: GNU 3.\nMeka - An open source implementation of methods for multi-label classification and evaluation (extension to Weka). License: GNU.\nNeuroph - Neuroph is lightweight Java neural network framework. License: Apache 2.\nrapaio - statistics, data mining and machine learning toolbox in Java. License: Apache 2.\nApache SystemML - flexible, scalable machine learning (ML) language. (incubator project). License: Apache 2.\nwAlnut - Object oriented model of partial human brain with 1 theorized common learning algorithm. Work in progress towards a strong emotional AI. License: GNU 3\nSemantic Web and Linked Data\nThe Semantic Web provides a common framework that allows data to be shared and reused across application, enterprise, and community boundaries\nApache Marmotta An Open Platform for Linked Data. License: Apache 2.\nApache Jena A framework for developing Semantic Web and Linked Data applications in Java. License: Apache 2.\nConstraint Satisfaction Problem Solver\nLibraries that help on implementing optimization and satisfiability problems.\nChoco - Choco is a Free and Open-Source Software dedicated to Constraint Programming. It aims at describing hard combinatorial problems in the form of Constraint Satisfaction Problems and solving them with Constraint Programming techniques. License: BSD.\nJaCoP - Java Constraint Programming (JaCoP) solver. License: ?\nOptaPlanner - OptaPlanner is a constraint satisfaction solver. It optimizes business resource planning. Every organization faces scheduling puzzles: assign a limited set of constrained resources (employees, assets, time and money) to provide products or services to customers. License: Apache 2.\nSat4J - Sat4j is a java library for solving boolean satisfaction and optimization problems. It can solve SAT, MAXSAT, Pseudo-Boolean, Minimally Unsatisfiable Subset (MUS) problems. License: GNU Lesser 3 and Eclipse Public 1.0.\nNatural Language Processing (NLP) and Speech Recognition\nLibraries that specialize on processing text. Up\nStanfordnlp CoreNLP Stanford CoreNLP provides a set of natural language analysis tools written in Java. Hello World examples. License: GNU 2, .\nApache OpenNLP - The Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text. Hello World examples. License: Apache 2.\nLingPipe - LingPipe is tool kit for processing text using computational linguistics. License: AGPL or License: proprietary. or\nMallet - MALLET is a Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text. License: Eclipse Public License v1.0.\nTwitter Text Libraries - This repo is a collection of libraries and conformance tests to standardize parsing of tweet text. It synchronizes development, testing, creating issues, and pull requests for twitter-text's implementations and specification. License: Apache 2.\nNLP4J The NLP4J project (formerly known as ClearNLP) provides a NLP toolkit for JVM languages. This project is currently developed by the NLP Research Group at Emory University. License: Apache 2.\nJoshua Joshua is an open-source statistical machine translation decoder for phrase-based (new in 6.0), hierarchical, and syntax-based machine translation, written in Java. It is developed at the Human Language Technology Center of Excellence at Johns Hopkins University. License: BSD 2.\nZ-MERT Z-MERT is a software tool for minimum error rate training of machine translation systems. License: GNU Lesser General Public License (LGPL).\nClearTK - ClearTK is a framework for developing machine learning and natural language processing components within the Apache Unstructured Information Management Architecture, http://cleartk.github.io/cleartk/ . License: Apache 2.\nApache cTAKES - Apache cTAKES\u2122 is a natural language processing system for extraction of information from electronic medical record clinical free-text. License: Apache 2.\nThe Stanford Natural Language Processing Group - A Natural Language Processing Java software tools. Subproject: Stanford Parser - A statistical parser, Stanford POS Tagger, Stanford Named Entity Recognizer, RegexNER, Word Segmenter,Classifier,EnglishTokenizer,TokensRegex,Temporal Tagger,Pattern-based Information Extraction and Diagnostics,Stanford Relation Extractor. License: GNU 2/3.\nApache Tika The Apache Tika toolkit detects and extracts metadata and structured text content from various documents using existing parser libraries. License: Apache 2.\nCMU Sphinx - Open Source Speech Recognition Toolkit. License: BSD 2.\nARK Twitter NLP - CMU ARK Twitter Part-of-Speech Tagger. http://www.ark.cs.cmu.edu/TweetNLP/ License: GNU 2/3, .\nlc4j - Language Categorization for Java: an Open Source library for finding out in which language a text has been written. License: MIT.\nIII. Network and Integration\n1. Servers (Web Server and Application Server)\nServers which are specifically used to deploy applications. Up\nWildfly - Formerly known as JBoss and developed by Red Hat with extensive Java EE support. License: GNU Lesser 2.1, .\nUndertow - High performance non-blocking webserver. License: Apache 2 , .\nApache Tomcat - Robust all-round server for Servlet and JSP. License: Apache 2 , .\nNginx clojure - Nginx module for embedding Clojure or Java or Groovy programs, typically those Ring based handlers. License: BSD 3 , .\nJetty - Lightweight, small server, often embedded in projects. License: Eclipse Public 1.0 / Apache 2.0, .\nApache TomEE - Tomcat plus Java EE. License: Apache 2.\nWebSphere Liberty - Lightweight, modular server developed by IBM. License: proprietary\nGlassFish - GlassFish - World's first Java EE 7 Application Server. License: GNU 2 Or CDDL 1.0.\nApache Geronimo Java EE Application Server. License: Apache 2.\nApache James The Apache Java Enterprise Mail Server (a.k.a. Apache James) is a 100% pure Java SMTP and POP3 Mail server and NNTP News server. We have designed James to be a complete and portable enterprise mail engine solution based on currently available open protocols. License: Apache 2.\nPayara Payara Server is an open source middleware platform that supports reliable and secure deployments of Java EE (Jakarta EE) and MicroProfile applications in any environment: on premise, in the cloud or hybrid.CDDL 1.1\n2. Networking\nLibraries for network programming. Up\nNetty Framework for building high performance network applications. License: Apache 2 , .\nNetty socketio Socket.IO server implemented on Java. Realtime java framework. License: Apache 2 , .\nGrpc java The Java gRPC implementation. HTTP/2 based RPC http://www.grpc.io/ . License: BSD 3, .\nSocket.io java client Socket.IO Client Implementation in Java. License: MIT , .\nSocket.io client.java Full-featured Socket.IO Client Library for Java, which is compatible with Socket.IO v1.0 and later. License: MIT , .\nEsotericSoftware Kryonet TCP/UDP client/server library for Java, based on Kryo. License: BSD 3, .\nAsync Http Client - Asynchronous HTTP and WebSocket client library. License: Apache 2.\nComsat - Integrates standard Java web-related APIs with Quasar fibers and actors. License: Eclipse Public 1.0.\nGrizzly - NIO framework. Used as a network layer in Glassfish. License: GNU 2 and CDDL 1.1.\nOkHttp - HTTP+SPDY client. License: Apache 2.\nUndertow - Web server providing both blocking and non-blocking API\u2019s based on NIO. Used as a network layer in WildFly. License: Apache 2.\nApache MINA Apache MINA is a network application framework which helps users develop high performance and high scalability network applications easily. It provides an abstract, event-driven, asynchronous API over various transports such as TCP/IP and UDP/IP via Java NIO. License: Apache 2.\nApache HttpComponents project is responsible for creating and maintaining a toolset of low level Java components focused on HTTP and associated protocols. HTTP transport library including support for asynchronous execution based on Java NIO. License: Apache 2.\n3. Message, message broker and message queue\nTools that help to send messages between clients in order to ensure protocol independency. Up\nLMAX Exchange Disruptor High Performance Inter-Thread Messaging Library. License: Apache 2 , .\nGifsockets Real Time communication library using Animated Gifs as a transport. License: Eclipse Public 1.0, .\nReal logic Aeron Efficient reliable UDP unicast, UDP multicast, and IPC message transport. License: Apache 2 , .\nJeroMQ Pure Java ZeroMQ. License: GNU 3, .\nMetamorphosis A high available,high performance distributed messaging system. License: Apache 2 , .\nIgniterealtime Openfire A XMPP server licensed under the Open Source Apache License. License: Apache 2 , .\nZeromq Jzmq Java binding for ZeroMQ. License: GNU 3, .\nAeron - Efficient reliable unicast and multicast message transport. License: Apache 2.\nApache ActiveMQ - Message broker that implements JMS and converts synchronous to asynchronous communication, License: Apache 2.\nApache Camel - Glues together different transport APIs via Enterprise Integration Patterns, License: Apache 2.\nApache Kafka - High-throughput distributed messaging system, License: Apache 2.\nHermes - Fast and reliable message broker built on top of Kafka, License: Apache 2.\nJBoss HornetQ - Clear, concise, modular and made to be embedded, License: Apache 2.\nSmack Cross-platform XMPP client library. License: Apache 2 , .\nApache Qpid AMQP enterprise messaging implementation. License: Apache 2.\nApache Synapse Apache Synapse is a lightweight ESB engine and XML router. License: Apache 2.\nApache ServiceMix Apache ServiceMix is a flexible, open-source integration container that unifies the features and functionality of Apache ActiveMQ, Camel, CXF, and Karaf into a powerful runtime platform you can use to build your own integrations solutions. It provides a complete, enterprise ready ESB exclusively powered by OSGi. License: Apache 2.\n4. Http and ssh\nUp\nSquare Okhttp An HTTP+HTTP/2 client for Android and Java applications. License: Apache 2 , .\nAsyncHttpClient Asynchronous Http and WebSocket Client library for Java. License: Apache 2 , .\nHttp request Java HTTP Request Library. License: MIT . , .\nNanohttpd Tiny, easily embeddable HTTP server in Java. License: BSD 3, .\nHttp kit Hhttp-kit is a minimalist, event-driven, high-performance Clojure HTTP server/client library with WebSocket and asynchronous support. License: Apache 2 , .\nApache Zookeeper Mirror of Apache Hadoop ZooKeeper. License: Apache 2 , .\nMoco Easy Setup Stub Server. License: MIT , .\nWebbit A Java event based WebSocket and HTTP server. License: BSD 3, .\nMashape Unirest java Unirest in Java: Simplified, lightweight HTTP client library.. License: MIT , .\nSshj ssh, scp and sftp for java. License: Apache 2 , .\nNetflix Feign Feign makes writing java http clients easier. License: Apache 2 , .\n5. Rest Frameworks\nFrameworks specifically for creating RESTful services. Up\nSquare Retrofit Type-safe REST client. License: Apache 2 , .\nDropwizard Opinionated framework for setting up modern web applications with Jetty, Jackson, Jersey and Metrics. License: Apache 2 , .\nSwagger Swagger is a specification and complete framework implementation for describing, producing, consuming, and visualizing RESTful web services. http://swagger.io/ License: Apache 2 , .\nGenerator jhipster Hipster stack for Java developers. Spring Boot + AngularJS in one handy generator. License: Apache 2 , .\nJersey JAX-RS reference implementation. License: CDDL 1.0, .\nLinkedin Rest.li Rest.li is a REST+JSON framework for building robust, scalable service architectures using dynamic discovery and simple asynchronous APIs. License: Apache 2 , .\nSpring hateoas Spring HATEOAS - Library to support implementing representations for hyper-text driven REST web services. License: Apache 2 , .\nFeign - HTTP client binder inspired by Retrofit, JAXRS-2.0, and WebSocket. License: Apache 2.\nRESTEasy - Fully certified and portable implementation of the JAX-RS specification. License: Apache 2.\nRestExpress - Thin wrapper on the JBOSS Netty HTTP stack to provide scaling and performance. License: Apache 2.\nRestX - Framework based on annotation processing and compile-time source generation. License: Apache 2.\nSpark - Sinatra inspired framework. License: Apache 2.\nApache Wink RESTFul web services based on JAX-RS specification. License: Apache 2.\nApache Olingo Apache Olingo is a Java and JavaScript library that implements the Open Data Protocol (OData). Open Data Protocol (OData) is an open protocol which allows the creation and consumption of queryable and interoperable RESTful APIs in a simple and standard way. License: Apache 2.\nRapidoid - A simple, secure and extremely fast framework consisting of embedded HTTP server, GUI components and dependency injection. http://www.rapidoid.org/ License: Apache 2, .\n6. Integration frameworks\nUp\nJmxtrans This is effectively the missing connector between speaking to a JVM via JMX on one end and whatever logging / monitoring / graphing package that you can dream up on the other end. License: ?, .\nApache Camel Apache Camel is a powerful open source integration framework based on known Enterprise Integration Patterns with powerful Bean Integration. License: Apache 2 , .\nApache CXF Apache CXF is an open source services framework. CXF helps you build and develop services using frontend programming APIs, like JAX-WS and JAX-RS. These services can speak a variety of protocols such as SOAP, XML/HTTP, RESTful HTTP, or CORBA and work over a variety of transports such as HTTP, JMS or JBI. License: Apache 2.\nSpring Integration - Spring Integration provides an extension of the Spring programming model to support the well-known Enterprise Integration Patterns (EIP). http://projects.spring.io/spring-integration/ License: Apache 2 , .\n7. Web Crawling and HTML parsering\nUp\nSparklemotion Nokogiri Nokogiri is an HTML, XML, SAX, and Reader parser with XPath and CSS selector support. License: Apache 2 , .\nJsoup Scrapes, parses, manipulates and cleans HTML. https://jsoup.org/ . Hello World examples. License: MIT , .\nWebmagic A scalable web crawler framework. License: Apache 2 , .\nAntlr4 ANTLR (ANother Tool for Language Recognition) is a powerful parser generator for reading, processing, executing, or translating structured text or binary files. License: BSD 3, .\nParboiled Elegant parsing in Java and Scala - lightweight, easy-to-use, powerful. License: Apache 2 , .\nPegdown A pure-Java Markdown processor based on a parboiled PEG parser supporting a number of extensions. License: Apache 2 , .\nApache Nutch - Highly extensible, highly scalable web crawler for production environment. License: Apache 2.\nCrawler4j - Simple and lightweight web crawler. License: Apache 2.\n8. Json\nLibraries that simplify JSON processing. Up\nJSON Parsers\nAlibaba Fastjson Fast JSON Processor , . User guide and Hello World examples. License: Apache 2.\nGson - Serializes objects to JSON and vice versa. Good performance with on-the-fly usage, . User guide and Hello World examples . License: Apache 2.\nLoganSquare - JSON parsing and serializing library based on Jackson's streaming API. Outpeforms GSON & Jackson's library, . User guide and Hello World examples. License: Apache 2.\nJSON java A reference implementation of a JSON package in Java, . User guide and Hello World examples. License: Crockford's license (MIT License + \"Good, not Evil\").\nSquare Moshi A modern JSON library for Android and Java , . User guide and Hello World examples. License: Apache 2\nInstagram Ig json parser Fast JSON parser for java projects, . User guide and Hello World examples. License: BSD 3.\nJackson - Similar to GSON but has performance gains if you need to instantiate the library more often. Subprojects: Jackson core Core part of Jackson, Jackson databind Core part of Jackson that defines Streaming API as well as basic shared abstractions, . User guide and Hello World examples. License: Apache 2.\nJSON.simple - A simple Java toolkit for JSON. You can use json-simple to encode or decode JSON text, . User guide and Hello World examples. License: Apache 2.\nGenson -Powerful and easy to use Java to JSON conversion library, http://owlike.github.io/genson/ . User guide and Hello World examples. License: Apache 2.\nAnalog XPath for JSON\nJayway JsonPath Java JsonPath implementation, . User guide and Hello World examples. License: Apache 2.\nAlibaba Fastjson Fast JSON Processor , . User guide and Hello World examples. License: Apache 2.\nGenerates Java types from JSON or JSON Schema or JSON validation\nJsonschema2pojo Generates Java types from JSON Schema (or example JSON) and annotates those types for data-binding with Jackson 1.x or 2.x, Gson, etc. , . User guide and Hello World examples. License: Apache 2.\nJson schema validator A JSON Schema validation implementation in pure Java, which aims for correctness and performance, in that order, also can generate Java types from JSON Schema or versa versa, . User guide and Hello World examples. License: GNU Lesser 3/Apache 2.\n9. CSV\nFrameworks and libraries that simplify reading/writing CSV data. Up\nopencsv - Simple CSV parser with a commercial-friendly license. License: Apache 2.\nSuper CSV - Powerful CSV parser with support for Dozer, Joda-Time and Java 8. License: Apache 2.\nuniVocity-parsers - One of the fastest and most feature-complete CSV. Also comes with parsers for TSV and fixed width records. License: Apache 2.\n10. Integratin with API\nUp\nTwitter Zipkin Zipkin is a distributed tracing system. It helps gather timing data needed to troubleshoot latency problems in microservice architectures. License: Apache 2 , .\nWizcorp Phonegap facebook plugin The official plugin for Facebook in Apache Cordova/PhoneGap. License: Apache 2 , .\nTwitter4j Twitter4J is an open-sourced, mavenized and Google App Engine safe Java library for the Twitter API. License: Apache 2 , .\nNetflix Ice AWS (Amazon Web Services) Usage Tool. License: Apache 2 , .\nTwitter Commons Twitter common libraries for python and the JVM. License: Apache 2 , .\nAlibaba RocketMQ RocketMQ is a fast, reliable, scalable, easy to use message oriented middleware breeding from alibaba massive messaging business. License: Apache 2 , .\nTwitter Hbc A Java HTTP client for consuming Twitter's Streaming API. License: Apache 2 , .\nSpring social Allows you to connect your applications with SaaS providers such as Facebook and Twitter. License: Apache 2 , .\n11. Bitcoin\nUp\nXChange XChange is a Java library providing a streamlined API for interacting with 50+ Bitcoin and Altcoin exchanges providing a consistent interface for trading and accessing market data. License: MIT , .\nbitcoinj A library for working with Bitcoin. License: Apache 2 , .\nDiabloMiner OpenCL miner for Bitcoin. License: GNU 3, .\n12. Clouds\nUp\nNetflix SimianArmy - Tools for keeping your cloud operating in top form. Chaos Monkey is a resiliency tool that helps applications tolerate random instance failures. License: Apache 2 , .\nNetflix Eureka AWS Service registry for resilient mid-tier load balancing and failover.. License: Apache 2 , .\nAws sdk java Official mirror of the AWS (Amazon Web Services) SDK for Java. License: Apache 2 , .\nSyncany Syncany is a cloud storage and filesharing application with a focus on security and abstraction of storage. License: GNU 3, .\nLegacy Jclouds jclouds is an open source library that helps you get started in the cloud and reuse your java development skills. Our api allows you to freedom to use portable abstractions or cloud-specific features. We support many clouds including Amazon, VMWare, Azure, and Rackspace. License: Apache 2 , .\nElasticsearch - Open Source, Distributed, RESTful Search Engine. License: Apache 2 , .\nElasticsearch cloud aws AWS Cloud Plugin for Elasticsearch. License: Apache 2 , .\nElasticsearch analysis ik The IK Analysis plugin integrates Lucene IK analyzer into elasticsearch, support customized dictionary. License: ?, .\nApache CloudStack Apache CloudStack is an IaaS (\u201cInfrastracture as a Service\u201d) cloud orchestration platform. License: Apache 2.\nApache Stratos Apache Stratos is a highly-extensible Platform-as-a-Service (PaaS) framework that helps run Apache Tomcat, PHP, and MySQL applications and can be extended to support many more environments on all major cloud infrastructures. License: Apache 2.\nApache Airavata Apache Airavata is a software framework for executing and managing computational jobs and workflows on distributed computing resources including local clusters, supercomputers, national grids, academic and commercial clouds. Airavata is dominantly used to build Web-based science gateways and assist to compose, manage, execute, and monitor large scale applications (wrapped as Web based services) and workflows composed of these services. License: Apache 2.\nQuarkus Quarkus is a Cloud Native, (Linux) Container First framework for writing Java applications.\n13. Cluster Management\nFrameworks which can dynamically manage applications inside of a cluster. Up\nApache Aurora - Apache Aurora is a Mesos framework for long-running services and cron jobs. License: Apache 2.\nSingularity - Singularity is a Mesos framework that makes deployment and operations easy. It supports web services, background workers, scheduled jobs, and one-off tasks. License: ?.\nApache Helix A cluster management framework for partitioned and replicated distributed resources. License: Apache 2.\nApache Airavata Apache Airavata is a software framework for executing and managing computational jobs and workflows on distributed computing resources including local clusters, supercomputers, national grids, academic and commercial clouds. Airavata is dominantly used to build Web-based science gateways and assist to compose, manage, execute, and monitor large scale applications (wrapped as Web based services) and workflows composed of these services. License: Apache 2.\n14. Document Processing (XLS, DOC and PDF)\nLibraries that assist with processing office document formats.\nApache POI - Supports OOXML (XLSX, DOCX, PPTX) as well as OLE2 (XLS, DOC or PPT). License: Apache 2.\ndocuments4j - API for document format conversion using third-party converters such as MS Word. License: Apache 2.\nDocx4j Docx4j is an open source Java library for manipulating Microsoft OpenXML files (Word docx, Powerpoint pptx, and Excel xlsx) via JAXB. License: Apache 2.\njOpenDocument - Processes the OpenDocument format. License: GNU or proprietary. and\nApache Tika The Apache Tika toolkit detects and extracts metadata and structured text content from various documents using existing parser libraries. License: Apache 2.\nPDF\nEverything that helps with the creation of PDF files. Up\nApache FOP - Creates PDF from XSL-FO. License: Apache 2.\nApache PDFBox - Toolbox for creating and manipulating PDF. License: Apache 2.\nDynamicReports - Simplifies JasperReports. License: GNU Lesser 3.\nflyingsaucer - XML/XHTML and CSS 2.1 renderer. License: GNU Lesser 2.1.\niText - Creates PDF files programmatically but current versions require a license for commercial purposes. License: AGPL or proprietary. or . However, versions upto 4.2.0 were licensed under the business-friendly LGPL. From 2.1.7 the version jumped to the 4 series, one of the main changes was a renaming of the packages. These two versions are available in several repositories on github, see for example: iText 4.2.0 and iText 2.1.7. The jars can be downloaded from Maven Central.. iText 2.1.7 and iText 4.2.0 are available under a . The book iText in Action 2nd Edition, from Manning, was written around the time of these LGPL versions. If you are making any contributions to iText please make them to these LGPL repositories on GitHub.\nJasperReports - Complex reporting engine. License: GNU Lesser.\n15. Native\nFor working with platform-specific native libraries. Up\nJava Native Access (JNA)Work with native libraries without writing JNI. Also provides interfaces to common system libraries. License: GNU Lesser 2.1 or Apache 2. , .\nJNR - Work with native libraries without writing JNI. Also provides interfaces to common system libraries. Same goals as JNA, but faster, and serves as the basis for the upcoming Project Panama. License: Apache 2.\n16. XML and SOAP\nXalan Xalan-J is an XSLT processor written in Java. License: Apache 2.\nXerces Xerces-J is a validating XML parser written in Java. License: Apache 2.\nXML Graphics Conversion from XML to graphical output. License: Apache 2.\nSantuario Library implementing XML Digital Signature Specification & XML Encryption Specification. License: Apache 2.\nVXQuery Apache VXQuery implements a parallel XML Query processor. License: Apache 2.\nApache Axis Web Service containers that helps users to create, deploy, and run Web Services. Axis2 is a Web Services / SOAP / WSDL engine, the successor to the widely used Apache Axis SOAP stack. License: Apache 2.\n17. Geospatial Service Interation\nLibraries for working with geospatial data and algorithms.\nApache SIS - Library for developing geospatial applications. License: Apache 2.\nGeo - GeoHash utilities in Java. License: Apache 2.\nGeotoolkit.org - Library for developing geospatial applications. Built on top of the Apache SIS project. License: GNU Lesser 2.1.\nGeoTools - Library that provides tools for geospatial data. License: GNU Lesser 2.1.\nH2GIS - A spatial extension of the H2 database. License: GPL 3.\nJgeohash - Library that can assist Java developers in using the GeoHash algorithm. License: Apache 2.\nJTS Topology Suite - An API of 2D spatial predicates and functions. License: GNU Lesser 3.\nMapsforge - Software for the rendering of maps based on OpenStreetMap data. License: GNU Lesser 3.\nSpatial4j - General purpose spatial/geospatial ASL licensed open-source Java library. License: Apache 2.\n18. Reverse Proxy Servers\nServers which accept client requests and direct them to the approriate backend service.\nMembrane Service Proxy An open source, reverse HTTP proxy framework written in Java. License: Apache 2 , .\nIV. Testing\n1. Testing\nTools that test from model to the view. Up\nArchUnit is a free, simple and extensible library for checking the architecture of your Java code. License: Apache 2 , .\nJunit Common testing framework. License: Eclipse Public 1.0 , .\nMockito Creation of test double objects in automated unit tests for the purpose of TDD or BDD. License: MIT , .\nSelenium Portable software testing framework for web applications. License: Apache 2 , .\nCucumber jvm - BDD testing framework. Cucumber-JVM is a pure Java implementation of Cucumber that supports the most popular programming languages for the JVM. Hello World examples. License: MIT, .\nJBehave - extensively configurable framework for Behavior-Driven Development (BDD). BDD is an evolution of test-driven development (TDD) and acceptance-test driven design, and is intended to make these practices more accessible and intuitive to newcomers and experts alike. License: BSD 3, .\nSpock JUnit-compatible framework featuring an expressive Groovy-derived specification language. License: Apache 2 , .\nGoogle Firing range Firing Range is a test bed for web application security scanners, providing synthetic, wide coverage for an array of vulnerabilities. It can be deployed as a Google App Engine application. License: Apache 2 , .\nFitnesse FitNesse is the fully integrated stand-alone acceptance testing framework and wiki.. License: CPL-1.0, .\nSikuli Sikuli is a visual technology to automate graphical user interfaces (GUI) using images (screenshots). The current release of Sikuli includes Sikuli Script, a visual scripting API for Jython, and Sikuli IDE, an integrated development environment for writing visual scripts with screenshots easily. License: MIT , .\nJavaHamcrest Matchers that can be combined to create flexible expressions of intent. License: BSD 3, .\nWiremock Stubbs and mocks web services. License: Apache 2 , .\nTestNG Testing framework. License: Apache 2 , .\nGalenframework Galen Layout and functional testing framework for websites. License: Apache 2 , .\nAssertJ Fluent assertions that improve readability. License: Apache 2 , .\nGoogle truth Google's assertion and proposition framework. License: Apache 2 , .\nApache JMeter - Functional testing and performance measurements. License: Apache 2.\nArquillian - Integration and functional testing platform for Java EE containers. License: Apache 2.\nAwaitility - DSL for synchronizing asynchronous operations. License: Apache 2.\nCitrus - Integration testing framework with focus on client- and serverside messaging. License: Apache 2.\nGatling - Load testing tool designed for ease of use, maintainability and high performance. License: MIT.\nGreenMail - In-memory email server for integration testing. Supports SMTP, POP3 and IMAP including SSL. License: MIT.\nJGiven - Developer-friendly BDD testing framework compatible with JUnit and TestNG. License: MIT.\nJMockit - Mocks static, final methods and more. License: MIT.\nJUnitParams - Creation of readable and maintainable parametrised tests. License: Apache 2.\nMoco - Concise web services for stubs and mocks, Duke's Choice Award 2013. License: MIT.\nPIT - Fast mutation-testing framework for evaluating fault-detection abilities of existing JUnit or TestNG test-suites. License: Creative Commons License.\nPowerMock - Enables mocking of static methods, constructors, final classes and methods, private methods and removal of static initializers. License: Apache 2.\nREST Assured - Java DSL for easy testing for REST/HTTP services. License: Apache 2.\nSelenide -Concise API around Selenium to write stable and readable UI tests. License: MIT.\nUnitils - Modular testing library for unit and integration testing. License: Apache 2.\nSeLion - Enabling Test Automation in Java. SeLion builds on top of TestNG and Selenium to provide a set of capabilities that get you up and running with WebDriver in a short time. It can be used for testing web and mobile applications, . License: Apache 2.\ncdp4j - Web-automation library for Java. It can be used for automating the use of web pages and for testing web pages. License: MIT.\nImage-Comparison - Published on Maven Central Java Library that compares 2 images with the same sizes and shows the differences visually by drawing rectangles. Some parts of the image can be excluded from the comparison. License: The Unlicense\n2. Code Coverage\nFrameworks and tools that enable collection of code coverage metrics for test suites. Up\nJaCoCo - Framework that enables collection of code coverage metrics, using both offline and runtime bytecode instrumentation; prominently used by EclEmma, the Eclipse code-coverage plugin. License: Eclipse Public License v1.0, Apache 2, BSD.\nClover - Proprietary code coverage tool by Atlassian that relies on source-code instrumentation, instead of bytecode instrumentation. License: proprietary\nCobertura - Relies on offline (or static) bytecode instrumentation and class loading to collect code coverage metrics. License: GNU 2.\nJCov - Code coverage tool used in the OpenJDK project's development toolchain. License: GNU 2.\n3. Continuous Integration\nTools which support continuously building, testing and releasing applications. Up\nBamboo - Atlassian's solution with good integration of their other products. You can either apply for an open-source license or buy it.\nCircleCI - Hosted service with a free trial.\nCodeship - Hosted services with a limited free plan.\nfabric8 - Integration platform for containers. License: Apache 2.\nGo - ThoughtWork's open-source solution. License: Apache 2.\nJenkins - Provides server-based deployment services. License: MIT.\nTeamCity - JetBrain's CI solution with a free version. License: proprietary\nTravis - Hosted service often used for open-source projects. License: ?\nHudson - Hudson monitors the execution of repeated jobs, such as building a software project or jobs run by cron. Currently Hudson focuses on the following two jobs: Building/testing software projects continuously and Monitoring executions of externally-run jobs. License: Eclipse Public License v1.0.\nApache Continuum Apache Continuum software is an enterprise-ready continuous integration server with features such as automated builds, release management, role-based security, and integration with popular build tools and source control management systems.. License: Apache 2.\n4. Formal Verification\nFormal-methods tools: proof assistants, model checking, symbolic execution etc. Up\nCATG - Concolic unit testing engine. Automatically generates unit tests using formal methods. License: BSD 2, License: BSD 2.\nChecker Framework - Pluggable type systems. Includes nullness types, physical units, immutability types and more. License: GNU 2.\nDaikon - Daikon detects likely program invariants and can generate JML specs based on those invariats. License: GNU.\nJava Modeling Language (JML) - Behavioral interface specification language that can be used to specify the behavior of code modules. It combines the design by contract approach of Eiffel and the model-based specification approach of the Larch family of interface specification languages, with some elements of the refinement calculus. Used by several other verification tools. License: GNU 2.\nJava Path Finder (JPF) - JVM formal verification tool containing a model checker and more. Created by NASA. License: NASA OPEN SOURCE AGREEMENT VERSION 1.3\njCUTE - Concolic unit testing engine that automatically generates unit tests. Concolic execution combines randomized concrete execution with symbolic execution and automatic constraint solving. License: proprietary.\nJMLOK 2.0 - Detects nonconformances between code and JML specification through the feedback-directed random tests generation, and suggests a likely cause for each nonconformance detected. License: GNU 3.\nKeY - The KeY System is a formal software development tool that aims to integrate design, implementation, formal specification, and formal verification of object-oriented software as seamlessly as possible. Uses JML for specification and symbolic execution for verification. License: GNU.\nOpenJML - Translates JML specifications into SMT-LIB format and passes the proof problems implied by the program to backend solvers. License: GNU 2 and Eclipse Public License v1.0.\nV. Tools for developing\n1. IDE\nIntegrated development environments that try to simplify several aspects of development. Up\nJetBrains Intellij \u0421ommunity Supports a lot of JVM languages and provides good options for Android development. The commercial edition targets the enterprise sector. http://www.jetbrains.com/idea/ License: Apache 2 , .\nJetBrains Ideavim Vim emulation plug-in for IDEs based on the IntelliJ platform. License: GNU 2, .\nRstudio RStudio is an integrated development environment (IDE) for R. License: GNU AGPLv3, .\nVrapper Vim-like editing in Eclipse. License: GNU 3.0, .\nEclipse themes Jeeeyul's Eclipse Themes allows you to customize every single details of Eclipse's appearance. License: Eclipse Public 1.0, .\nEclipse color theme Color themes for Eclipse. License: Eclipse Public 1.0, .\nEclim Expose eclipse features inside of vim. License: GNU 3, .\nJetBrains MPS JetBrains Meta programming System. Design your own Domain Specific Language with full development environment. Get code editor with completion, semantics and type checking with one click. Write generators to compile your DSL into multiple target languages, such as Java, C, XML, and many more. License: Apache 2 , .\nIdea markdown Markdown language support for IntelliJ IDEA. License: Apache 2 , .\nEclipse - Established, open-souce project with support for lots of plugins and languages. License: Eclipse Public License v1.0.\nNetBeans - Provides integration for several Java SE and EE features from database access to HTML5. License: GNU 2 and CDDL 1.0\n2. Deploy, config and build\nUp\nBuild\nTools which handle the build cycle and dependencies of an application.\nApache Maven - Declarative build and dependency management which favors convention over configuration. It might be preferable to Apache Ant which uses a rather procedural approach and can be difficult to maintain. http://maven.apache.org License: Apache 2 , .\nGoogle Bazel - Build tool from Google that builds code quickly and reliably. http://bazel.io License: Apache 2 , .\nGradle - Incremental builds which are programmed via Groovy instead of declaring XML. Works well with Maven's dependency management. http://gradle.org/ License: Apache 2 , .\nApache Ant - pache Ant is a Java library and command-line tool whose mission is to drive processes described in build files as targets and extension points dependent upon each other. Subproject: Apache AntUnit - testing Ant task, Apache Compress Ant Library - support additional archive formats, Apache Ivy - very powerful dependency manager, Apache IvyDE - IvyDE plugin for Eclipse. License: Apache 2.\nFacebook Buck - A fast build system that encourages the creation of small, reusable modules over a variety of platforms and languages. License: Apache 2 , .\nArchiva Apache The Build Artifact Repository Manager. Apache Archiva\u2122 is an extensible repository management software that helps taking care of your own personal or enterprise-wide build artifact repository. It is the perfect companion for build tools such as Maven, Continuum, and ANT. License: Apache 2.\nConfiguration\nLibraries that provide external configuration. Up\nconfig - Configuration library for JVM languages. License: Apache 2 , .\nowner - Reduces boilerplate of properties. License: BSD 3.\nNetflix Archaius - Library for configuration management API. Archaius includes a set of configuration management APIs used by Netflix. License: Apache 2 , .\nLightAdmin - Pluggable data administration UI library for Java web applications. License: Apache 2.\nApache Yetus A collection of libraries and tools that enable contribution and release processes for software projects. License: Apache 2.\nDistribution\nTools which handle the distribution of applications in native formats. Up\nBintray - Version control for binaries which handles the publishing. Can also be used with Maven or Gradle and has a free plan for open-source software or several business plans. License: proprietary\nCapsule - Simple and powerful packaging and deployment. A fat JAR on steroids or a \"Docker for Java\" that supports JVM-optimized containers. . License: Eclipse Public License v1.0.\nCentral Repository - Largest binary component repository available as a free service to the open-source community. Default used by Apache Maven and available in all other build tools. License: proprietary\nIzPack - Setup authoring tool for cross-platform deployments. License: Apache 2 , ).\nJitPack - Easy to use package repository for GitHub. Builds Maven/Gradle projects on demand and publishes ready-to-use packages. License: Apache 2 , ).\nLaunch4j - Wraps JARs in lightweight and native Windows executables. License: BSD 3 or MIT.\nNexus - Binary management with proxy and caching capabilities. License: proprietary\npackr - Packs JARs, assets and the JVM for native distribution on Windows, Linux and Mac OS X. License: Apache 2.\nSpotify Helios Docker container orchestration platform. License: Apache 2 , .\n3. Perfomance tools\nTools for performance analysis, profiling and benchmarking. Up\nSquare Leakcanary A memory leak detection library for Android and Java. License: Apache 2 , .\nDropwizard Metrics - Capturing JVM- and application-level metrics. License: Apache 2 , .\nGCViewer Fork of tagtraum industries' GCViewer. Tagtraum stopped development in 2008, I aim to improve support for Sun's / Oracle's java 1.6+ garbage collector logs (including G1 collector). License: GNU Lesser 2.1, .\nAdoptOpenJDK Jitwatch Log analyser / visualiser for Java HotSpot JIT compiler. Inspect inlining decisions, hot methods, bytecode, and assembly. View results in the JavaFX user interface. License: BSD 2, .\nNaver Pinpoint Pinpoint is an open source APM (Application Performance Management) tool for large-scale distributed systems written in Java. License: Apache 2 , .\njHiccup - Logs and records platform JVM stalls. License: BSD 2.\nJMH - Microbenchmarking tool for the JVM. License: GNU 2.\nLatencyUtils - Utilities for latency measurement and reporting. License: BSD 2.\nVisualVM - Visual interface for detailed information about running applications. License: GNU 2 with the Classpath Exception.\nYourKit Java Profiler - Commercial profiler. License: proprietary\nJProfiler - Commercial profiler. License: proprietary\nXRebel - A commercial profiler for Java Web applications. License: proprietary\n4. Code Analysis\nTools that provide metrics and quality measurements. Up\nSonarQube Integrates other analysis components via plugins and provides an overview of the metrics over time. http://www.sonarqube.org License: GNU Lesser 3, .\nGoogle Error prone - Catches common programming mistakes as compile-time errors. License: Apache 2 , .\nOpenGrok OpenGrok is a fast and usable source code search and cross reference engine, written in Java. It helps you search, cross-reference and navigate your source tree. It can understand various program file formats and version control histories like Mercurial, Git, SCCS, RCS, CVS, Subversion, Teamware, ClearCase, Perforce, Monotone and Bazaar. License: CDDL 1.0, ).\nCheckstyle - Static analysis of coding conventions and standards. License: GNU Lesser 2.1.\nFindBugs - Static analysis of bytecode to find potential bugs. License: GNU Lesser.\njQAssistant - Static code analysis with Neo4J-based query language. License: GNU 3.\nPMD - Source code analysis for finding bad coding practices. License: BSD 4.\n5. Monitoring\nTools that monitor applications in production. Up\nAppDynamics - Commercial performance monitor. License: proprietary\nJavaMelody - Performance monitoring and profiling. License: Apache 2.\njmxtrans - Tool to connect to multiple JVMs and to query them for their attributes via JMX. Its query language is based on JSON, which allows non-Java programmers to access the JVMs attributes. Likewise, this tool supports different output writes, including Graphite, Ganglia, StatsD, among others. License: MIT.\nKamon - Tool for monitoring applications running on the JVM. License: Apache 2.\nGlowroot - Open source Java APM. License: Apache 2.\nNew Relic - Commercial performance monitor. License: proprietary\nSPM - Commercial performance monitor with distributing transaction tracing for JVM apps. License: proprietary\nOverOps - Root cause automation in production. License: proprietary\n6. Redefinition of classes at runtime\nUp\nDynamic Code Evolution Virtual Machine (DCE VM) - The Dynamic Code Evolution Virtual Machine (DCE VM) is a modification of the Java HotSpot(TM) VM that allows unlimited redefinition of loaded classes at runtime. License: GNU 2\nDCEVM - This project is a fork of original DCEVM project. The purpose of the project is to maintain enhanced class redefinition functionality for OpenJDK HotSpot 7/8 and Oracle JVM. License: GNU 2\nHotswapAgent - Java unlimited redefinition of classes at runtime. License: GNU 2\nJRebel - Reload code changes instantly. Skip the build and redeploy process. JRebel reloads changes to Java classes, resources, and over 90 frameworks. License: proprietary\nSpring Loaded - Java agent that enables class reloading in a running JVM. License: Apache 2\n7. Documentation\nUp\nMarkdown-doclet: A Doclet that allows the use of Markdown (Pegdown) and PlantUML in JavaDoc comments. License: GPL 3.0, .\n8. Other\nUp\nGoogle J2ObjC: A Java to iOS Objective-C translation tool and runtime. License: Apache 2 , .\nBytecode viewer A Java 8 Jar & Android APK Reverse Engineering Suite (Decompiler, Editor, Debugger & More). License: GNU 3, .\nReflections Java runtime metadata analysis, in the spirit of Scannotations. Reflections scans your classpath, indexes the metadata, allows you to query it on runtime and may save and collect that information for many modules within your project. License: WTFPL, .\nJabba: Java Version Manager inspired by nvm (Node.js). The goal is to provide unified pain-free experience of installing (and switching between different versions of) JDK. License Apache 2 , .\nCurrent implementations of the JVM/JDK.\nJDK 9 - Early access releases of JDK 9. License: GNU 2.\nOpenJDK - Open-source implementation for Linux. License: GNU 2.\nZulu OpenJDK - OpenJDK builds for Windows, Linux, and Mac OS X through Java 8. License: GNU 2.\nZulu OpenJDK 9 - Early access OpenJDK 9 builds for Windows, Linux, and Mac OS X. License: GNU 2.\nVI. Program languages and applications that were written with Java\n1. Program languages that were written with Java\nUp\nClojure The Clojure programming language. License: Apache 2 , .\nJetBrains Kotlin The Kotlin Programming Language. License: Apache 2 , .\nGocd Main repository for Go Continuous Delivery. License: Apache 2 , .\nGroovy core The Groovy programming language. License: Apache 2 , .\nJphp compiler Jphp An alternative to Zend PHP, like JRuby and Jython only for PHP. License: Apache 2 , .\nTrifork Erjang A JVM-based Erlang VM. License: Apache 2 , .\n2. Other program languages tools that were written with Java\nUp\nGo lang idea plugin Google Go language IDE built using the IntelliJ Platform. License: Apache 2 , .\nProcessing js A port of the Processing visualization language to JavaScript. License: MIT , .\nPysonar2 PySonar2 is a type inferencer and indexer for Python, which performs sophisticated interprocedural analysis to infer types. License: GNU GNU AGPLv3, .\n3. Javascript\nUp\nYuicompressor The YUI Compressor is a JavaScript compressor which, in addition to removing comments and white-spaces, obfuscates local variables using the smallest possible variable name. License: BSD, .\nGoogle Closure compiler A JavaScript checker and optimizer. License: Apache 2 , .\nFrontend maven plugin \"Maven-node-grunt-gulp-npm-node-plugin to end all maven-node-grunt-gulp-npm-plugins.\" A Maven plugin that downloads/installs Node and NPM locally, runs NPM install, Grunt, Gulp and/or Karma. License: Apache 2 , .\nDynjs ECMAScript runtime for the JVM. License: Apache 2 , .\n4. Frameworks that help to create parsers, interpreters or compilers\nFrameworks that help to create parsers, interpreters or compilers. Up\nANTLR - Complex full-featured framework for top-down parsing. License: BSD 2.\nJavaCC -More specific and slightly easier to learn. Has syntactic lookahead. License: BSD .\n5. Opensource applications that were written with Java\nUp\nSquare Keywhiz - A system for distributing and managing secrets. License: Apache 2 , .\nRundeck Job scheduler and runbook automation. Enable self-service access to existing scripts and tools. License: Apache 2 , .\nKeyBox KeyBox is a web-based SSH console that centrally manages administrative access to systems. Web-based administration is combined with management and distribution of user's public SSH keys. License: Apache 2 , .\nOpenTripPlanner An open source multi-modal trip planner. License: Apache 2 , .\nNetflix Servo Netflix Application Monitoring Library. License: Apache 2 , .\nJitsi Jitsi is an audio/video and chat communicator that supports protocols such as SIP, XMPP/Jabber, AIM/ICQ, IRC, Yahoo! and many other useful features. License: Apache 2 , .\nNetflix Exhibitor ZooKeeper co-process for instance monitoring, backup/recovery, cleanup and visualization. License: Apache 2 , .\nGlyptodon Guacamole client - The HTML5/JavaScript Guacamole client, its containing web application, and related components. License: MIT , .\nBateman - Simple stock trading system that optimizes its parameters with particle swarm optimization. License: MIT , .\nJava repl - Java REPL is a simple Read-Eval-Print-Loop for Java language. License: Apache 2 , .\nSeyren An alerting dashboard for Graphite. License: Apache 2 , .\nGraphhopper Open source routing library and server using OpenStreetMap. License: Apache 2 , .\nTtorrent Turn's BitTorrent Java library (tracker and client). License: Apache 2 , .\nGeoserver Official GeoServer repository. License: GNU 2.0, .\nLanguagetool - Style and Grammar Checker for 25+ Languages. License: GNU Lesser 2.1, .\nApache OpenMeetings Video conferencing, instant messaging, white board and collaborative document editing application. License: Apache 2.\n6. Opensource games that were written with Java\nUp\nBukkit The Minecraft Mod API. License: GNU 2, .\nMovingBlocks Terasology Terasology - open source voxel world. The Terasology project was born from a Minecraft-inspired tech demo and is becoming a stable platform for various types of gameplay settings in a voxel world. License: Apache 2 , .\nMinecraftForge Modifications to the Minecraft base files to assist in compatibility between mods. License: GNU AGPLv2.1, .\nSpongePowered Sponge A Forge mod that implements SpongeAPI. License: MIT , .\nBuildCraft BuildCraft \u2014 mod for Minecraft. License: Apache 2 , .\nEquivalent Exchange 3 pahimar Equivalent-Exchange-3. Mods for Minecraft. License: Apache 2 , .\nSpongePowered SpongeAPI A Minecraft plugin API. License: Apache 2 , .\nWorldEdit An in-game voxel map editor for Minecraft. License: GNU Lesser 3, .\nEssentials Essentials - Minecraft server command mod - Adds over 100 commands for use in-game to help manage a server. License: GNU 3, .\nGlowstoneMC Glowstone An open-source server for the Bukkit Minecraft modding interface. License: MIT , .\nTribal Trouble - Tribal Trouble is a realtime strategy game released by Oddlabs in 2004. In 2014 the source was released under GPL2 license. License: GNU 2, .\nVII. Other\n1. Source code examples\nUp\nJava design patterns Design patterns implemented in Java. License: MIT , .\nSpring projects Spring mvc showcase Demonstrates the capabilities of the Spring MVC web framework through small, simple examples. License: Apache 2 , .\nJavaee7 samples Java EE 7 Samples. License: MIT/CDDL/GPLv2, .\nAlgorithms Solutions for some common algorithm problems written in Java. License: Apache 2 , .\nWikiSort Fast and stable sort algorithm that uses O(1) memory. License: unlicense.org, .\nSpring petclinic A sample Spring-based application. License: Apache 2 , .\nSpring integration samples You are looking for examples, code snippets, sample applications for Spring Integration? This is the place. License: Apache 2 , .\nJava algorithms implementation Algorithms and Data Structures implemented in Java. License: Apache 2 , .\nJboss developer Jboss eap quickstarts The quickstarts demonstrate JBoss EAP, Java EE 7 and a few additional technologies. They provide small, specific, working examples that can be used as a reference for your own project. License: Apache 2 , .\nDatabricks Learning spark Example code from Learning Spark book. License: MIT , .\n7guis 7GUIs is a GUI programming usability benchmark. License: ?, .\nSpring projects Spring data jpa examples Spring Data Example Projects. License: Apache 2 , .\nModern Java - A Guide to Java 8 - Modern Java - A Guide to Java 8. License: MIT.\n2. Benchmark results\nUp\nJvm serializers - Benchmark comparing serialization libraries on the JVM. License: ?, .\nYCSB (Yahoo! Cloud Serving Benchmark) - Yahoo! Cloud Serving Benchmark. License: Apache 2 , .\n3. Working with git and github\nUp\nGitblit Gitblit is an open source, pure Java Git solution for managing, viewing, and serving Git repositories. It can serve repositories over the GIT, HTTP, and SSH transports; it can authenticate against multiple providers; and it allows you to get up-and-running with an attractive, capable Git server in less than 5 minutes.. License: Apache 2 , .\nWhisperSystems BitHub BTC + BitHub = An experiment in funding privacy OSS. BitHub is a service that will automatically pay a percentage of Bitcoin funds for every submission to a GitHub repository. License: ?, .\nVIII. Resources\n1. Communities\nActive discussions. Up\nr/java - Subreddit for the Java community.\nstackoverflow - Question/answer platform.\nvJUG - Virtual Java User Group.\nDevProjects - Community to discuss Java projects and share solutions.\n2. Influential Books\nBooks that had a high impact and are still worth reading. Up\nEffective Java (3nd Edition)\nModern Java in Action\nJava Concurrency in Practice\nThinking in Java\n3. Websites\nSites to read. Up\nAndroid Arsenal\nGoogle Java Style\nHackr.io\nInfoQ\nJava, SQL, and jOOQ\nJavalobby\nJava tutorial\nJavaWorld\nJAXenter\nRebelLabs\nThe Java Specialist' Newsletter\nThe Takipi Blog\nTheServerSide.com\nThoughts On Java\nVanilla Java\nVlad Mihalcea on Hibernate\nVoxxed\nOverOps Blog\nBookmarks.dev\nBaeldung",
      "link": "https://github.com/Vedenin/useful-java-links"
    },
    {
      "autor": "BackgroundMattingV2",
      "date": "NaN",
      "content": "Real-Time High-Resolution Background Matting\nOfficial repository for the paper Real-Time High-Resolution Background Matting. Our model requires capturing an additional background image and produces state-of-the-art matting results at 4K 30fps and HD 60fps on an Nvidia RTX 2080 TI GPU.\nVisit project site\nWatch project video\nDisclaimer: The video conversion script in this repo is not meant be real-time. Our research's main contribution is the neural architecture for high resolution refinement and the new matting datasets. The inference_speed_test.py script allows you to measure the tensor throughput of our model, which should achieve real-time. The inference_video.py script allows you to test your video on our model, but the video encoding and decoding is done without hardware acceleration and parallization. For production use, you are expected to do additional engineering for hardware encoding/decoding and loading frames to GPU in parallel. For more architecture detail, please refer to our paper.\nNew Paper is Out!\nCheck out Robust Video Matting! Our new method does not require pre-captured backgrounds, and can inference at even faster speed!\nOverview\nUpdates\nDownload\nModel / Weights\nVideo / Image Examples\nDatasets\nDemo\nScripts\nNotebooks\nUsage / Documentation\nTraining\nProject members\nLicense\nUpdates\n[Jun 21 2021] Paper received CVPR 2021 Best Student Paper Honorable Mention.\n[Apr 21 2021] VideoMatte240K dataset is now published.\n[Mar 06 2021] Training script is published.\n[Feb 28 2021] Paper is accepted to CVPR 2021.\n[Jan 09 2021] PhotoMatte85 dataset is now published.\n[Dec 21 2020] We updated our project to MIT License, which permits commercial use.\nDownload\nModel / Weights\nDownload model / weights\nVideo / Image Examples\nHD videos (by Sengupta et al.) (Our model is more robust on HD footage)\n4K videos and images\nDatasets\nDownload datasets\nDemo\nScripts\nWe provide several scripts in this repo for you to experiment with our model. More detailed instructions are included in the files.\ninference_images.py: Perform matting on a directory of images.\ninference_video.py: Perform matting on a video.\ninference_webcam.py: An interactive matting demo using your webcam.\nNotebooks\nAdditionally, you can try our notebooks in Google Colab for performing matting on images and videos.\nImage matting (Colab)\nVideo matting (Colab)\nVirtual Camera\nWe provide a demo application that pipes webcam video through our model and outputs to a virtual -----> camera !!! . The script only works on Linux system and can be used in Zoom meetings. For more information, checkout:\nWebcam plugin\nUsage / Documentation\nYou can run our model using PyTorch, TorchScript, TensorFlow, and ONNX. For detail about using our model, please check out the Usage / Documentation page.\nTraining\nConfigure data_path.pth to point to your dataset. The original paper uses train_base.pth to train only the base model till convergence then use train_refine.pth to train the entire network end-to-end. More details are specified in the paper.\nProject members\nShanchuan Lin*, University of Washington\nAndrey Ryabtsev*, University of Washington\nSoumyadip Sengupta, University of Washington\nBrian Curless, University of Washington\nSteve Seitz, University of Washington\nIra Kemelmacher-Shlizerman, University of Washington\n* Equal contribution.\nLicense\nThis work is licensed under the MIT License. If you use our work in your project, we would love you to include an acknowledgement and fill out our survey.\nCommunity Projects\nProjects developed by third-party developers.\nAfter Effects Plug-In",
      "link": "https://github.com/PeterL1n/BackgroundMattingV2"
    },
    {
      "autor": "have-fun-with-machine-learning",
      "date": "NaN",
      "content": "Have Fun with Machine Learning: A Guide for Beginners\nAlso available in Chinese (Traditional).\nPreface\nThis is a hands-on guide to machine learning for programmers with no background in AI. Using a neural network doesn\u2019t require a PhD, and you don\u2019t need to be the person who makes the next breakthrough in AI in order to use what exists today. What we have now is already breathtaking, and highly usable. I believe that more of us need to play with this stuff like we would any other open source technology, instead of treating it like a research topic.\nIn this guide our goal will be to write a program that uses machine learning to predict, with a high degree of certainty, whether the images in data/untrained-samples are of dolphins or seahorses using only the images themselves, and without having seen them before. Here are two example images we'll use:\nTo do that we\u2019re going to train and use a Convolutional Neural Network (CNN). We\u2019re going to approach this from the point of view of a practitioner vs. from first principles. There is so much excitement about AI right now, but much of what\u2019s being written feels like being taught to do tricks on your bike by a physics professor at a chalkboard instead of your friends in the park.\nI\u2019ve decided to write this on Github vs. as a blog post because I\u2019m sure that some of what I\u2019ve written below is misleading, naive, or just plain wrong. I\u2019m still learning myself, and I\u2019ve found the lack of solid beginner documentation an obstacle. If you see me making a mistake or missing important details, please send a pull request.\nWith all of that out the way, let me show you how to do some tricks on your bike!\nOverview\nHere\u2019s what we\u2019re going to explore:\nSetup and use existing, open source machine learning technologies, specifically Caffe and DIGITS\nCreate a dataset of images\nTrain a neural network from scratch\nTest our neural network on images it has never seen before\nImprove our neural network\u2019s accuracy by fine tuning existing neural networks (AlexNet and GoogLeNet)\nDeploy and use our neural network\nThis guide won\u2019t teach you how neural networks are designed, cover much theory, or use a single mathematical expression. I don\u2019t pretend to understand most of what I\u2019m going to show you. Instead, we\u2019re going to use existing things in interesting ways to solve a hard problem.\nQ: \"I know you said we won\u2019t talk about the theory of neural networks, but I\u2019m feeling like I\u2019d at least like an overview before we get going. Where should I start?\"\nThere are literally hundreds of introductions to this, from short posts to full online courses. Depending on how you like to learn, here are three options for a good starting point:\nThis fantastic blog post by J Alammar, which introduces the concepts of neural networks using intuitive examples.\nSimilarly, this video introduction by Brandon Rohrer is a really good intro to Convolutional Neural Networks like we'll be using\nIf you\u2019d rather have a bit more theory, I\u2019d recommend this online book by Michael Nielsen.\nSetup\nInstalling the software we'll use (Caffe and DIGITS) can be frustrating, depending on your platform and OS version. By far the easiest way to do it is using Docker. Below we examine how to do it with Docker, as well as how to do it natively.\nOption 1a: Installing Caffe Natively\nFirst, we\u2019re going to be using the Caffe deep learning framework from the Berkely Vision and Learning Center (BSD licensed).\nQ: \u201cWait a minute, why Caffe? Why not use something like TensorFlow, which everyone is talking about these days\u2026\u201d\nThere are a lot of great choices available, and you should look at all the options. TensorFlow is great, and you should play with it. However, I\u2019m using Caffe for a number of reasons:\nIt\u2019s tailormade for computer vision problems\nIt has support for C++, Python, (with node.js support coming)\nIt\u2019s fast and stable\nBut the number one reason I\u2019m using Caffe is that you don\u2019t need to write any code to work with it. You can do everything declaratively (Caffe uses structured text files to define the network architecture) and using command-line tools. Also, you can use some nice front-ends for Caffe to make training and validating your network a lot easier. We\u2019ll be using nVidia\u2019s DIGITS tool below for just this purpose.\nCaffe can be a bit of work to get installed. There are installation instructions for various platforms, including some prebuilt Docker or AWS configurations.\nNOTE: when making my walkthrough, I used the following non-released version of Caffe from their Github repo: https://github.com/BVLC/caffe/commit/5a201dd960840c319cefd9fa9e2a40d2c76ddd73\nOn a Mac it can be frustrating to get working, with version issues halting your progress at various steps in the build. It took me a couple of days of trial and error. There are a dozen guides I followed, each with slightly different problems. In the end I found this one to be the closest. I\u2019d also recommend this post, which is quite recent and links to many of the same discussions I saw.\nGetting Caffe installed is by far the hardest thing we'll do, which is pretty neat, since you\u2019d assume the AI aspects would be harder! Don\u2019t give up if you have issues, it\u2019s worth the pain. If I was doing this again, I\u2019d probably use an Ubuntu VM instead of trying to do it on Mac directly. There's also a Caffe Users group, if you need answers.\nQ: \u201cDo I need powerful hardware to train a neural network? What if I don\u2019t have access to fancy GPUs?\u201d\nIt\u2019s true, deep neural networks require a lot of computing power and energy to train...if you\u2019re training them from scratch and using massive datasets. We aren\u2019t going to do that. The secret is to use a pretrained network that someone else has already invested hundreds of hours of compute time training, and then to fine tune it to your particular dataset. We\u2019ll look at how to do this below, but suffice it to say that everything I\u2019m going to show you, I\u2019m doing on a year old MacBook Pro without a fancy GPU.\nAs an aside, because I have an integrated Intel graphics card vs. an nVidia GPU, I decided to use the OpenCL Caffe branch, and it\u2019s worked great on my laptop.\nWhen you\u2019re done installing Caffe, you should have, or be able to do all of the following:\nA directory that contains your built caffe. If you did this in the standard way, there will be a build/ dir which contains everything you need to run caffe, the Python bindings, etc. The parent dir that contains build/ will be your CAFFE_ROOT (we\u2019ll need this later).\nRunning make test && make runtest should pass\nAfter installing all the Python deps (doing pip install -r requirements.txt in python/), running make pycaffe && make pytest should pass\nYou should also run make distribute in order to create a distributable version of caffe with all necessary headers, binaries, etc. in distribute/.\nOn my machine, with Caffe fully built, I\u2019ve got the following basic layout in my CAFFE_ROOT dir:\ncaffe/\nbuild/\npython/\nlib/\ntools/\ncaffe \u2190 this is our main binary\ndistribute/\npython/\nlib/\ninclude/\nbin/\nproto/\nAt this point, we have everything we need to train, test, and program with neural networks. In the next section we\u2019ll add a user-friendly, web-based front end to Caffe called DIGITS, which will make training and testing our networks much easier.\nOption 1b: Installing DIGITS Natively\nnVidia\u2019s Deep Learning GPU Training System, or DIGITS, is BSD-licensed Python web app for training neural networks. While it\u2019s possible to do everything DIGITS does in Caffe at the command-line, or with code, using DIGITS makes it a lot easier to get started. I also found it more fun, due to the great visualizations, real-time charts, and other graphical features. Since you\u2019re experimenting and trying to learn, I highly recommend beginning with DIGITS.\nThere are quite a few good docs at https://github.com/NVIDIA/DIGITS/tree/master/docs, including a few Installation, Configuration, and Getting Started pages. I\u2019d recommend reading through everything there before you continue, as I\u2019m not an expert on everything you can do with DIGITS. There's also a public DIGITS User Group if you have questions you need to ask.\nThere are various ways to install and run DIGITS, from Docker to pre-baked packages on Linux, or you can build it from source. I\u2019m on a Mac, so I built it from source.\nNOTE: In my walkthrough I've used the following non-released version of DIGITS from their Github repo: https://github.com/NVIDIA/DIGITS/commit/81be5131821ade454eb47352477015d7c09753d9\nBecause it\u2019s just a bunch of Python scripts, it was fairly painless to get working. The one thing you need to do is tell DIGITS where your CAFFE_ROOT is by setting an environment variable before starting the server:\nexport CAFFE_ROOT=/path/to/caffe\n./digits-devserver\nNOTE: on Mac I had issues with the server scripts assuming my Python binary was called python2, where I only have python2.7. You can symlink it in /usr/bin or modify the DIGITS startup script(s) to use the proper binary on your system.\nOnce the server is started, you can do everything else via your web browser at http://localhost:5000, which is what I'll do below.\nOption 2: Caffe and DIGITS using Docker\nInstall Docker, if not already installed, then run the following command in order to pull and run a full Caffe + Digits container. A few things to note:\nmake sure port 8080 isn't allocated by another program. If so, change it to any other port you want.\nchange /path/to/this/repository to the location of this cloned repo, and /data/repo within the container will be bound to this directory. This is useful for accessing the images discussed below.\ndocker run --name digits -d -p 8080:5000 -v /path/to/this/repository:/data/repo kaixhin/digits\nNow that we have our container running you can open up your web browser and open http://localhost:8080. Everything in the repository is now in the container directory /data/repo. That's it. You've now got Caffe and DIGITS working.\nIf you need shell access, use the following command:\ndocker exec -it digits /bin/bash\nTraining a Neural Network\nTraining a neural network involves a few steps:\nAssemble and prepare a dataset of categorized images\nDefine the network\u2019s architecture\nTrain and Validate this network using the prepared dataset\nWe\u2019re going to do this 3 different ways, in order to show the difference between starting from scratch and using a pretrained network, and also to show how to work with two popular pretrained networks (AlexNet, GoogLeNet) that are commonly used with Caffe and DIGITs.\nFor our training attempts, we\u2019ll use a small dataset of Dolphins and Seahorses. I\u2019ve put the images I used in data/dolphins-and-seahorses. You need at least 2 categories, but could have many more (some of the networks we\u2019ll use were trained on 1000+ image categories). Our goal is to be able to give an image to our network and have it tell us whether it\u2019s a Dolphin or a Seahorse.\nPrepare the Dataset\nThe easiest way to begin is to divide your images into a categorized directory layout:\ndolphins-and-seahorses/\ndolphin/\nimage_0001.jpg\nimage_0002.jpg\nimage_0003.jpg\n...\nseahorse/\nimage_0001.jpg\nimage_0002.jpg\nimage_0003.jpg\n...\nHere each directory is a category we want to classify, and each image within that category dir an example we\u2019ll use for training and validation.\nQ: \u201cDo my images have to be the same size? What about the filenames, do they matter?\u201d\nNo to both. The images sizes will be normalized before we feed them into the network. We\u2019ll eventually want colour images of 256 x 256 pixels, but DIGITS will crop or squash (we'll squash) our images automatically in a moment. The filenames are irrelevant--it\u2019s only important which category they are contained within.\nQ: \u201cCan I do more complex segmentation of my categories?\u201d\nYes. See https://github.com/NVIDIA/DIGITS/blob/digits-4.0/docs/ImageFolderFormat.md.\nWe want to use these images on disk to create a New Dataset, and specifically, a Classification Dataset.\nWe\u2019ll use the defaults DIGITS gives us, and point Training Images at the path to our data/dolphins-and-seahorses folder. DIGITS will use the categories (dolphin and seahorse) to create a database of squashed, 256 x 256 Training (75%) and Testing (25%) images.\nGive your Dataset a name,dolphins-and-seahorses, and click Create.\nThis will create our dataset, which took only 4s on my laptop. In the end I have 92 Training images (49 dolphin, 43 seahorse) in 2 categories, with 30 Validation images (16 dolphin, 14 seahorse). It\u2019s a really small dataset, but perfect for our experimentation and learning purposes, because it won\u2019t take forever to train and validate a network that uses it.\nYou can Explore the db if you want to see the images after they have been squashed.\nTraining: Attempt 1, from Scratch\nBack in the DIGITS Home screen, we need to create a new Classification Model:\nWe\u2019ll start by training a model that uses our dolphins-and-seahorses dataset, and the default settings DIGITS provides. For our first network, we\u2019ll choose to use one of the standard network architectures, AlexNet (pdf). AlexNet\u2019s design won a major computer vision competition called ImageNet in 2012. The competition required categorizing 1000+ image categories across 1.2 million images.\nCaffe uses structured text files to define network architectures. These text files are based on Google\u2019s Protocol Buffers. You can read the full schema Caffe uses. For the most part we\u2019re not going to work with these, but it\u2019s good to be aware of their existence, since we\u2019ll have to modify them in later steps. The AlexNet prototxt file looks like this, for example: https://github.com/BVLC/caffe/blob/master/models/bvlc_alexnet/train_val.prototxt.\nWe\u2019ll train our network for 30 epochs, which means that it will learn (with our training images) then test itself (using our validation images), and adjust the network\u2019s weights depending on how well it\u2019s doing, and repeat this process 30 times. Each time it completes a cycle we\u2019ll get info about Accuracy (0% to 100%, where higher is better) and what our Loss is (the sum of all the mistakes that were made, where lower is better). Ideally we want a network that is able to predict with high accuracy, and with few errors (small loss).\nNOTE: some people have reported hitting errors in DIGITS doing this training run. For many, the problem related to available memory (the process needs a lot of memory to work). If you're using Docker, you might want to try increasing the amount of memory available to DIGITS (in Docker, preferences -> advanced -> memory).\nInitially, our network\u2019s accuracy is a bit below 50%. This makes sense, because at first it\u2019s just \u201cguessing\u201d between two categories using randomly assigned weights. Over time it\u2019s able to achieve 87.5% accuracy, with a loss of 0.37. The entire 30 epoch run took me just under 6 minutes.\nWe can test our model using an image we upload or a URL to an image on the web. Let\u2019s test it on a few examples that weren\u2019t in our training/validation dataset:\nIt almost seems perfect, until we try another:\nHere it falls down completely, and confuses a seahorse for a dolphin, and worse, does so with a high degree of confidence.\nThe reality is that our dataset is too small to be useful for training a really good neural network. We really need 10s or 100s of thousands of images, and with that, a lot of computing power to process everything.\nTraining: Attempt 2, Fine Tuning AlexNet\nHow Fine Tuning works\nDesigning a neural network from scratch, collecting data sufficient to train it (e.g., millions of images), and accessing GPUs for weeks to complete the training is beyond the reach of most of us. To make it practical for smaller amounts of data to be used, we employ a technique called Transfer Learning, or Fine Tuning. Fine tuning takes advantage of the layout of deep neural networks, and uses pretrained networks to do the hard work of initial object detection.\nImagine using a neural network to be like looking at something far away with a pair of binoculars. You first put the binoculars to your eyes, and everything is blurry. As you adjust the focus, you start to see colours, lines, shapes, and eventually you are able to pick out the shape of a bird, then with some more adjustment you can identify the species of bird.\nIn a multi-layered network, the initial layers extract features (e.g., edges), with later layers using these features to detect shapes (e.g., a wheel, an eye), which are then feed into final classification layers that detect items based on accumulated characteristics from previous layers (e.g., a cat vs. a dog). A network has to be able to go from pixels to circles to eyes to two eyes placed in a particular orientation, and so on up to being able to finally conclude that an image depicts a cat.\nWhat we\u2019d like to do is to specialize an existing, pretrained network for classifying a new set of image classes instead of the ones on which it was initially trained. Because the network already knows how to \u201csee\u201d features in images, we\u2019d like to retrain it to \u201csee\u201d our particular image types. We don\u2019t need to start from scratch with the majority of the layers--we want to transfer the learning already done in these layers to our new classification task. Unlike our previous attempt, which used random weights, we\u2019ll use the existing weights of the final network in our training. However, we\u2019ll throw away the final classification layer(s) and retrain the network with our image dataset, fine tuning it to our image classes.\nFor this to work, we need a pretrained network that is similar enough to our own data that the learned weights will be useful. Luckily, the networks we\u2019ll use below were trained on millions of natural images from ImageNet, which is useful across a broad range of classification tasks.\nThis technique has been used to do interesting things like screening for eye diseases from medical imagery, identifying plankton species from microscopic images collected at sea, to categorizing the artistic style of Flickr images.\nDoing this perfectly, like all of machine learning, requires you to understand the data and network architecture--you have to be careful with overfitting of the data, might need to fix some of the layers, might need to insert new layers, etc. However, my experience is that it \u201cJust Works\u201d much of the time, and it\u2019s worth you simply doing an experiment to see what you can achieve using our naive approach.\nUploading Pretrained Networks\nIn our first attempt, we used AlexNet\u2019s architecture, but started with random weights in the network\u2019s layers. What we\u2019d like to do is download and use a version of AlexNet that has already been trained on a massive dataset.\nThankfully we can do exactly this. A snapshot of AlexNet is available for download: https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet. We need the binary .caffemodel file, which is what contains the trained weights, and it\u2019s available for download at http://dl.caffe.berkeleyvision.org/bvlc_alexnet.caffemodel.\nWhile you\u2019re downloading pretrained models, let\u2019s get one more at the same time. In 2014, Google won the same ImageNet competition with GoogLeNet (codenamed Inception): a 22-layer neural network. A snapshot of GoogLeNet is available for download as well, see https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet. Again, we\u2019ll need the .caffemodel file with all the pretrained weights, which is available for download at http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel.\nWith these .caffemodel files in hand, we can upload them into DIGITs. Go to the Pretrained Models tab in DIGITs home page and choose Upload Pretrained Model:\nFor both of these pretrained models, we can use the defaults DIGITs provides (i.e., colour, squashed images of 256 x 256). We just need to provide the Weights (**.caffemodel) and Model Definition (original.prototxt). Click each of those buttons to select a file.\nFor the model definitions we can use https://github.com/BVLC/caffe/blob/master/models/bvlc_googlenet/train_val.prototxt for GoogLeNet and https://github.com/BVLC/caffe/blob/master/models/bvlc_alexnet/train_val.prototxt for AlexNet. We aren\u2019t going to use the classification labels of these networks, so we\u2019ll skip adding a labels.txt file:\nRepeat this process for both AlexNet and GoogLeNet, as we\u2019ll use them both in the coming steps.\nQ: \"Are there other networks that would be good as a basis for fine tuning?\"\nThe Caffe Model Zoo has quite a few other pretrained networks that could be used, see https://github.com/BVLC/caffe/wiki/Model-Zoo.\nFine Tuning AlexNet for Dolphins and Seahorses\nTraining a network using a pretrained Caffe Model is similar to starting from scratch, though we have to make a few adjustments. First, we\u2019ll adjust the Base Learning Rate to 0.001 from 0.01, since we don\u2019t need to make such large jumps (i.e., we\u2019re fine tuning). We\u2019ll also use a Pretrained Network, and Customize it.\nIn the pretrained model\u2019s definition (i.e., prototext), we need to rename all references to the final Fully Connected Layer (where the end result classifications happen). We do this because we want the model to re-learn new categories from our dataset vs. its original training data (i.e., we want to throw away the current final layer). We have to rename the last fully connected layer from \u201cfc8\u201d to something else, \u201cfc9\u201d for example. Finally, we also need to adjust the number of categories from 1000 to 2, by changing num_output to 2.\nHere are the changes we need to make:\n@@ -332,8 +332,8 @@\n}\nlayer {\n- name: \"fc8\"\n+ name: \"fc9\"\ntype: \"InnerProduct\"\nbottom: \"fc7\"\n- top: \"fc8\"\n+ top: \"fc9\"\nparam {\nlr_mult: 1\n@@ -345,5 +345,5 @@\n}\ninner_product_param {\n- num_output: 1000\n+ num_output: 2\nweight_filler {\ntype: \"gaussian\"\n@@ -359,5 +359,5 @@\nname: \"accuracy\"\ntype: \"Accuracy\"\n- bottom: \"fc8\"\n+ bottom: \"fc9\"\nbottom: \"label\"\ntop: \"accuracy\"\n@@ -367,5 +367,5 @@\nname: \"loss\"\ntype: \"SoftmaxWithLoss\"\n- bottom: \"fc8\"\n+ bottom: \"fc9\"\nbottom: \"label\"\ntop: \"loss\"\n@@ -375,5 +375,5 @@\nname: \"softmax\"\ntype: \"Softmax\"\n- bottom: \"fc8\"\n+ bottom: \"fc9\"\ntop: \"softmax\"\ninclude { stage: \"deploy\" }\nI\u2019ve included the fully modified file I\u2019m using in src/alexnet-customized.prototxt.\nThis time our accuracy starts at ~60% and climbs right away to 87.5%, then to 96% and all the way up to 100%, with the Loss steadily decreasing. After 5 minutes we end up with an accuracy of 100% and a loss of 0.0009.\nTesting the same seahorse image our previous network got wrong, we see a complete reversal: 100% seahorse.\nEven a children\u2019s drawing of a seahorse works:\nThe same goes for a dolphin:\nEven with images that you think might be hard, like this one that has multiple dolphins close together, and with their bodies mostly underwater, it does the right thing:\nTraining: Attempt 3, Fine Tuning GoogLeNet\nLike the previous AlexNet model we used for fine tuning, we can use GoogLeNet as well. Modifying the network is a bit trickier, since you have to redefine three fully connected layers instead of just one.\nTo fine tune GoogLeNet for our use case, we need to once again create a new Classification Model:\nWe rename all references to the three fully connected classification layers, loss1/classifier, loss2/classifier, and loss3/classifier, and redefine the number of categories (num_output: 2). Here are the changes we need to make in order to rename the 3 classifier layers, as well as to change from 1000 to 2 categories:\n@@ -917,10 +917,10 @@\nexclude { stage: \"deploy\" }\n}\nlayer {\n- name: \"loss1/classifier\"\n+ name: \"loss1a/classifier\"\ntype: \"InnerProduct\"\nbottom: \"loss1/fc\"\n- top: \"loss1/classifier\"\n+ top: \"loss1a/classifier\"\nparam {\nlr_mult: 1\ndecay_mult: 1\n@@ -930,7 +930,7 @@\ndecay_mult: 0\n}\ninner_product_param {\n- num_output: 1000\n+ num_output: 2\nweight_filler {\ntype: \"xavier\"\nstd: 0.0009765625\n@@ -945,7 +945,7 @@\nlayer {\nname: \"loss1/loss\"\ntype: \"SoftmaxWithLoss\"\n- bottom: \"loss1/classifier\"\n+ bottom: \"loss1a/classifier\"\nbottom: \"label\"\ntop: \"loss1/loss\"\nloss_weight: 0.3\n@@ -954,7 +954,7 @@\nlayer {\nname: \"loss1/top-1\"\ntype: \"Accuracy\"\n- bottom: \"loss1/classifier\"\n+ bottom: \"loss1a/classifier\"\nbottom: \"label\"\ntop: \"loss1/accuracy\"\ninclude { stage: \"val\" }\n@@ -962,7 +962,7 @@\nlayer {\nname: \"loss1/top-5\"\ntype: \"Accuracy\"\n- bottom: \"loss1/classifier\"\n+ bottom: \"loss1a/classifier\"\nbottom: \"label\"\ntop: \"loss1/accuracy-top5\"\ninclude { stage: \"val\" }\n@@ -1705,10 +1705,10 @@\nexclude { stage: \"deploy\" }\n}\nlayer {\n- name: \"loss2/classifier\"\n+ name: \"loss2a/classifier\"\ntype: \"InnerProduct\"\nbottom: \"loss2/fc\"\n- top: \"loss2/classifier\"\n+ top: \"loss2a/classifier\"\nparam {\nlr_mult: 1\ndecay_mult: 1\n@@ -1718,7 +1718,7 @@\ndecay_mult: 0\n}\ninner_product_param {\n- num_output: 1000\n+ num_output: 2\nweight_filler {\ntype: \"xavier\"\nstd: 0.0009765625\n@@ -1733,7 +1733,7 @@\nlayer {\nname: \"loss2/loss\"\ntype: \"SoftmaxWithLoss\"\n- bottom: \"loss2/classifier\"\n+ bottom: \"loss2a/classifier\"\nbottom: \"label\"\ntop: \"loss2/loss\"\nloss_weight: 0.3\n@@ -1742,7 +1742,7 @@\nlayer {\nname: \"loss2/top-1\"\ntype: \"Accuracy\"\n- bottom: \"loss2/classifier\"\n+ bottom: \"loss2a/classifier\"\nbottom: \"label\"\ntop: \"loss2/accuracy\"\ninclude { stage: \"val\" }\n@@ -1750,7 +1750,7 @@\nlayer {\nname: \"loss2/top-5\"\ntype: \"Accuracy\"\n- bottom: \"loss2/classifier\"\n+ bottom: \"loss2a/classifier\"\nbottom: \"label\"\ntop: \"loss2/accuracy-top5\"\ninclude { stage: \"val\" }\n@@ -2435,10 +2435,10 @@\n}\n}\nlayer {\n- name: \"loss3/classifier\"\n+ name: \"loss3a/classifier\"\ntype: \"InnerProduct\"\nbottom: \"pool5/7x7_s1\"\n- top: \"loss3/classifier\"\n+ top: \"loss3a/classifier\"\nparam {\nlr_mult: 1\ndecay_mult: 1\n@@ -2448,7 +2448,7 @@\ndecay_mult: 0\n}\ninner_product_param {\n- num_output: 1000\n+ num_output: 2\nweight_filler {\ntype: \"xavier\"\n}\n@@ -2461,7 +2461,7 @@\nlayer {\nname: \"loss3/loss\"\ntype: \"SoftmaxWithLoss\"\n- bottom: \"loss3/classifier\"\n+ bottom: \"loss3a/classifier\"\nbottom: \"label\"\ntop: \"loss\"\nloss_weight: 1\n@@ -2470,7 +2470,7 @@\nlayer {\nname: \"loss3/top-1\"\ntype: \"Accuracy\"\n- bottom: \"loss3/classifier\"\n+ bottom: \"loss3a/classifier\"\nbottom: \"label\"\ntop: \"accuracy\"\ninclude { stage: \"val\" }\n@@ -2478,7 +2478,7 @@\nlayer {\nname: \"loss3/top-5\"\ntype: \"Accuracy\"\n- bottom: \"loss3/classifier\"\n+ bottom: \"loss3a/classifier\"\nbottom: \"label\"\ntop: \"accuracy-top5\"\ninclude { stage: \"val\" }\n@@ -2489,7 +2489,7 @@\nlayer {\nname: \"softmax\"\ntype: \"Softmax\"\n- bottom: \"loss3/classifier\"\n+ bottom: \"loss3a/classifier\"\ntop: \"softmax\"\ninclude { stage: \"deploy\" }\n}\nI\u2019ve put the complete file in src/googlenet-customized.prototxt.\nQ: \"What about changes to the prototext definitions of these networks? We changed the fully connected layer name(s), and the number of categories. What else could, or should be changed, and in what circumstances?\"\nGreat question, and it's something I'm wondering, too. For example, I know that we can \"fix\" certain layers so the weights don't change. Doing other things involves understanding how the layers work, which is beyond this guide, and also beyond its author at present!\nLike we did with fine tuning AlexNet, we also reduce the learning rate by 10% from 0.01 to 0.001.\nQ: \"What other changes would make sense when fine tuning these networks? What about different numbers of epochs, batch sizes, solver types (Adam, AdaDelta, AdaGrad, etc), learning rates, policies (Exponential Decay, Inverse Decay, Sigmoid Decay, etc), step sizes, and gamma values?\"\nGreat question, and one that I wonder about as well. I only have a vague understanding of these and it\u2019s likely that there are improvements we can make if you know how to alter these values when training. This is something that needs better documentation.\nBecause GoogLeNet has a more complicated architecture than AlexNet, fine tuning it requires more time. On my laptop, it takes 10 minutes to retrain GoogLeNet with our dataset, achieving 100% accuracy and a loss of 0.0070:\nJust as we saw with the fine tuned version of AlexNet, our modified GoogLeNet performs amazing well--the best so far:\nUsing our Model\nWith our network trained and tested, it\u2019s time to download and use it. Each of the models we trained in DIGITS has a Download Model button, as well as a way to select different snapshots within our training run (e.g., Epoch #30):\nClicking Download Model downloads a tar.gz archive containing the following files:\ndeploy.prototxt\nmean.binaryproto\nsolver.prototxt\ninfo.json\noriginal.prototxt\nlabels.txt\nsnapshot_iter_90.caffemodel\ntrain_val.prototxt\nThere\u2019s a nice description in the Caffe documentation about how to use the model we just built. It says:\nA network is defined by its design (.prototxt), and its weights (.caffemodel). As a network is being trained, the current state of that network's weights are stored in a .caffemodel. With both of these we can move from the train/test phase into the production phase.\nIn its current state, the design of the network is not designed for deployment. Before we can release our network as a product, we often need to alter it in a few ways:\nRemove the data layer that was used for training, as for in the case of classification we are no longer providing labels for our data.\nRemove any layer that is dependent upon data labels.\nSet the network up to accept data.\nHave the network output the result.\nDIGITS has already done the work for us, separating out the different versions of our prototxt files. The files we\u2019ll care about when using this network are:\ndeploy.prototxt - the definition of our network, ready for accepting image input data\nmean.binaryproto - our model will need us to subtract the image mean from each image that it processes, and this is the mean image.\nlabels.txt - a list of our labels (dolphin, seahorse) in case we want to print them vs. just the category number\nsnapshot_iter_90.caffemodel - these are the trained weights for our network\nWe can use these files in a number of ways to classify new images. For example, in our CAFFE_ROOT we can use build/examples/cpp_classification/classification.bin to classify one image:\n$ cd $CAFFE_ROOT/build/examples/cpp_classification\n$ ./classification.bin deploy.prototxt snapshot_iter_90.caffemodel mean.binaryproto labels.txt dolphin1.jpg\nThis will spit out a bunch of debug text, followed by the predictions for each of our two categories:\n0.9997 - \u201cdolphin\u201d\n0.0003 - \u201cseahorse\u201d\nYou can read the complete C++ source for this in the Caffe examples.\nFor a classification version that uses the Python interface, DIGITS includes a nice example. There's also a fairly well documented Python walkthrough in the Caffe examples.\nPython example\nLet's write a program that uses our fine-tuned GoogLeNet model to classify the untrained images we have in data/untrained-samples. I've cobbled this together based on the examples above, as well as the caffe Python module's source, which you should prefer to anything I'm about to say.\nA full version of what I'm going to discuss is available in src/classify-samples.py. Let's begin!\nFirst, we'll need the NumPy module. In a moment we'll be using NumPy to work with ndarrays, which Caffe uses a lot. If you haven't used them before, as I had not, you'd do well to begin by reading this Quickstart tutorial.\nSecond, we'll need to load the caffe module from our CAFFE_ROOT dir. If it's not already included in your Python environment, you can force it to load by adding it manually. Along with it we'll also import caffe's protobuf module:\nimport numpy as np\ncaffe_root = '/path/to/your/caffe_root'\nsys.path.insert(0, os.path.join(caffe_root, 'python'))\nimport caffe\nfrom caffe.proto import caffe_pb2\nNext we need to tell Caffe whether to use the CPU or GPU. For our experiments, the CPU is fine:\ncaffe.set_mode_cpu()\nNow we can use caffe to load our trained network. To do so, we'll need some of the files we downloaded from DIGITS, namely:\ndeploy.prototxt - our \"network file\", the description of the network.\nsnapshot_iter_90.caffemodel - our trained \"weights\"\nWe obviously need to provide the full path, and I'll assume that my files are in a dir called model/:\nmodel_dir = 'model'\ndeploy_file = os.path.join(model_dir, 'deploy.prototxt')\nweights_file = os.path.join(model_dir, 'snapshot_iter_90.caffemodel')\nnet = caffe.Net(deploy_file, caffe.TEST, weights=weights_file)\nThe caffe.Net() constructor takes a network file, a phase (caffe.TEST or caffe.TRAIN), as well as an optional weights filename. When we provide a weights file, the Net will automatically load them for us. The Net has a number of methods and attributes you can use.\nNote: There is also a deprecated version of this constructor, which seems to get used often in sample code on the web. It looks like this, in case you encounter it:\nnet = caffe.Net(str(deploy_file), str(model_file), caffe.TEST)\nWe're interested in loading images of various sizes into our network for testing. As a result, we'll need to transform them into a shape that our network can use (i.e., colour, 256x256). Caffe provides the Transformer class for this purpose. We'll use it to create a transformation appropriate for our images/network:\ntransformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})\n# set_transpose: https://github.com/BVLC/caffe/blob/61944afd4e948a4e2b4ef553919a886a8a8b8246/python/caffe/io.py#L187\ntransformer.set_transpose('data', (2, 0, 1))\n# set_raw_scale: https://github.com/BVLC/caffe/blob/61944afd4e948a4e2b4ef553919a886a8a8b8246/python/caffe/io.py#L221\ntransformer.set_raw_scale('data', 255)\n# set_channel_swap: https://github.com/BVLC/caffe/blob/61944afd4e948a4e2b4ef553919a886a8a8b8246/python/caffe/io.py#L203\ntransformer.set_channel_swap('data', (2, 1, 0))\nWe can also use the mean.binaryproto file DIGITS gave us to set our transformer's mean:\n# This code for setting the mean from https://github.com/NVIDIA/DIGITS/tree/master/examples/classification\nmean_file = os.path.join(model_dir, 'mean.binaryproto')\nwith open(mean_file, 'rb') as infile:\nblob = caffe_pb2.BlobProto()\nblob.MergeFromString(infile.read())\nif blob.HasField('shape'):\nblob_dims = blob.shape\nassert len(blob_dims) == 4, 'Shape should have 4 dimensions - shape is %s' % blob.shape\nelif blob.HasField('num') and blob.HasField('channels') and \\\nblob.HasField('height') and blob.HasField('width'):\nblob_dims = (blob.num, blob.channels, blob.height, blob.width)\nelse:\nraise ValueError('blob does not provide shape or 4d dimensions')\npixel = np.reshape(blob.data, blob_dims[1:]).mean(1).mean(1)\ntransformer.set_mean('data', pixel)\nIf we had a lot of labels, we might also choose to read in our labels file, which we can use later by looking up the label for a probability using its position (e.g., 0=dolphin, 1=seahorse):\nlabels_file = os.path.join(model_dir, 'labels.txt')\nlabels = np.loadtxt(labels_file, str, delimiter='\\n')\nNow we're ready to classify an image. We'll use caffe.io.load_image() to read our image file, then use our transformer to reshape it and set it as our network's data layer:\n# Load the image from disk using caffe's built-in I/O module\nimage = caffe.io.load_image(fullpath)\n# Preprocess the image into the proper format for feeding into the model\nnet.blobs['data'].data[...] = transformer.preprocess('data', image)\nQ: \"How could I use images (i.e., frames) from a -----> camera !!!  or video stream instead of files?\"\nGreat question, here's a skeleton to get you started:\nimport cv2\n...\n# Get the shape of our input data layer, so we can resize the image\ninput_shape = net.blobs['data'].data.shape\n...\nwebCamCap = cv2.VideoCapture(0) # could also be a URL, filename\nif webCamCap.isOpened():\nrval, frame = webCamCap.read()\nelse:\nrval = False\nwhile rval:\nrval, frame = webCamCap.read()\nnet.blobs['data'].data[...] = transformer.preprocess('data', frame)\n...\nwebCamCap.release()\nBack to our problem, we next need to run the image data through our network and read out the probabilities from our network's final 'softmax' layer, which will be in order by label category:\n# Run the image's pixel data through the network\nout = net.forward()\n# Extract the probabilities of our two categories from the final layer\nsoftmax_layer = out['softmax']\n# Here we're converting to Python types from ndarray floats\ndolphin_prob = softmax_layer.item(0)\nseahorse_prob = softmax_layer.item(1)\n# Print the results. I'm using labels just to show how it's done\nlabel = labels[0] if dolphin_prob > seahorse_prob else labels[1]\nfilename = os.path.basename(fullpath)\nprint '%s is a %s dolphin=%.3f%% seahorse=%.3f%%' % (filename, label, dolphin_prob*100, seahorse_prob*100)\nRunning the full version of this (see src/classify-samples.py) using our fine-tuned GoogLeNet network on our data/untrained-samples images gives me the following output:\n[...truncated caffe network output...]\ndolphin1.jpg is a dolphin dolphin=99.968% seahorse=0.032%\ndolphin2.jpg is a dolphin dolphin=99.997% seahorse=0.003%\ndolphin3.jpg is a dolphin dolphin=99.943% seahorse=0.057%\nseahorse1.jpg is a seahorse dolphin=0.365% seahorse=99.635%\nseahorse2.jpg is a seahorse dolphin=0.000% seahorse=100.000%\nseahorse3.jpg is a seahorse dolphin=0.014% seahorse=99.986%\nI'm still trying to learn all the best practices for working with models in code. I wish I had more and better documented code examples, APIs, premade modules, etc to show you here. To be honest, most of the code examples I\u2019ve found are terse, and poorly documented--Caffe\u2019s documentation is spotty, and assumes a lot.\nIt seems to me like there\u2019s an opportunity for someone to build higher-level tools on top of the Caffe interfaces for beginners and basic workflows like we've done here. It would be great if there were more simple modules in high-level languages that I could point you at that \u201cdid the right thing\u201d with our model; someone could/should take this on, and make using Caffe models as easy as DIGITS makes training them. I\u2019d love to have something I could use in node.js, for example. Ideally one shouldn\u2019t be required to know so much about the internals of the model or Caffe. I haven\u2019t used it yet, but DeepDetect looks interesting on this front, and there are likely many other tools I don\u2019t know about.\nResults\nAt the beginning we said that our goal was to write a program that used a neural network to correctly classify all of the images in data/untrained-samples. These are images of dolphins and seahorses that were never used in the training or validation data:\nUntrained Dolphin Images\nUntrained Seahorse Images\nLet's look at how each of our three attempts did with this challenge:\nModel Attempt 1: AlexNet from Scratch (3rd Place)\nImage Dolphin Seahorse Result\ndolphin1.jpg 71.11% 28.89% \ud83d\ude11\ndolphin2.jpg 99.2% 0.8% \ud83d\ude0e\ndolphin3.jpg 63.3% 36.7% \ud83d\ude15\nseahorse1.jpg 95.04% 4.96% \ud83d\ude1e\nseahorse2.jpg 56.64% 43.36 \ud83d\ude15\nseahorse3.jpg 7.06% 92.94% \ud83d\ude01\nModel Attempt 2: Fine Tuned AlexNet (2nd Place)\nImage Dolphin Seahorse Result\ndolphin1.jpg 99.1% 0.09% \ud83d\ude0e\ndolphin2.jpg 99.5% 0.05% \ud83d\ude0e\ndolphin3.jpg 91.48% 8.52% \ud83d\ude01\nseahorse1.jpg 0% 100% \ud83d\ude0e\nseahorse2.jpg 0% 100% \ud83d\ude0e\nseahorse3.jpg 0% 100% \ud83d\ude0e\nModel Attempt 3: Fine Tuned GoogLeNet (1st Place)\nImage Dolphin Seahorse Result\ndolphin1.jpg 99.86% 0.14% \ud83d\ude0e\ndolphin2.jpg 100% 0% \ud83d\ude0e\ndolphin3.jpg 100% 0% \ud83d\ude0e\nseahorse1.jpg 0.5% 99.5% \ud83d\ude0e\nseahorse2.jpg 0% 100% \ud83d\ude0e\nseahorse3.jpg 0.02% 99.98% \ud83d\ude0e\nConclusion\nIt\u2019s amazing how well our model works, and what\u2019s possible by fine tuning a pretrained network. Obviously our dolphin vs. seahorse example is contrived, and the dataset overly limited--we really do want more and better data if we want our network to be robust. But since our goal was to examine the tools and workflows of neural networks, it\u2019s turned out to be an ideal case, especially since it didn\u2019t require expensive equipment or massive amounts of time.\nAbove all I hope that this experience helps to remove the overwhelming fear of getting started. Deciding whether or not it\u2019s worth investing time in learning the theories of machine learning and neural networks is easier when you\u2019ve been able to see it work in a small way. Now that you\u2019ve got a setup and a working approach, you can try doing other sorts of classifications. You might also look at the other types of things you can do with Caffe and DIGITS, for example, finding objects within an image, or doing segmentation.\nHave fun with machine learning!",
      "link": "https://github.com/humphd/have-fun-with-machine-learning"
    },
    {
      "autor": "machine-learning-for-trading",
      "date": "NaN",
      "content": "ML for Trading - 2nd Edition\nThis book aims to show how ML can add value to algorithmic trading strategies in a practical yet comprehensive way. It covers a broad range of ML techniques from linear regression to deep reinforcement learning and demonstrates how to build, backtest, and evaluate a trading strategy driven by model predictions.\nIn four parts with 23 chapters plus an appendix, it covers on over 800 pages:\nimportant aspects of data sourcing, financial feature engineering, and portfolio management,\nthe design and evaluation of long-short strategies based on supervised and unsupervised ML algorithms,\nhow to extract tradeable signals from financial text data like SEC filings, earnings call transcripts or financial news,\nusing deep learning models like CNN and RNN with market and alternative data, how to generate synthetic data with generative adversarial networks, and training a trading agent using deep reinforcement learning\nThis repo contains over 150 notebooks that put the concepts, algorithms, and use cases discussed in the book into action. They provide numerous examples that show:\nhow to work with and extract signals from market, fundamental and alternative text and image data,\nhow to train and tune models that predict returns for different asset classes and investment horizons, including how to replicate recently published research, and\nhow to design, backtest, and evaluate trading strategies.\nWe highly recommend reviewing the notebooks while reading the book; they are usually in an executed state and often contain additional information not included due to space constraints.\nIn addition to the information in this repo, the book's website contains chapter summary and additional information.\nJoin the ML4T Community!\nTo make it easy for readers to ask questions about the book's content and code examples, as well as the development and implementation of their own strategies and industry developments, we are hosting an online platform.\nPlease join our community and connect with fellow traders interested in leveraging ML for trading strategies, share your experience, and learn from each other!\nWhat's new in the 2nd Edition?\nFirst and foremost, this book demonstrates how you can extract signals from a diverse set of data sources and design trading strategies for different asset classes using a broad range of supervised, unsupervised, and reinforcement learning algorithms. It also provides relevant mathematical and statistical knowledge to facilitate the tuning of an algorithm or the interpretation of the results. Furthermore, it covers the financial background that will help you work with market and fundamental data, extract informative features, and manage the performance of a trading strategy.\nFrom a practical standpoint, the 2nd edition aims to equip you with the conceptual understanding and tools to develop your own ML-based trading strategies. To this end, it frames ML as a critical element in a process rather than a standalone exercise, introducing the end-to-end ML for trading workflow from data sourcing, feature engineering, and model optimization to strategy design and backtesting.\nMore specifically, the ML4T workflow starts with generating ideas for a well-defined investment universe, collecting relevant data, and extracting informative features. It also involves designing, tuning, and evaluating ML models suited to the predictive task. Finally, it requires developing trading strategies to act on the models' predictive signals, as well as simulating and evaluating their performance on historical data using a backtesting engine. Once you decide to execute an algorithmic strategy in a real market, you will find yourself iterating over this workflow repeatedly to incorporate new information and a changing environment.\nThe second edition's emphasis on the ML4t workflow translates into a new chapter on strategy backtesting, a new appendix describing over 100 different alpha factors, and many new practical applications. We have also rewritten most of the existing content for clarity and readability.\nThe trading applications now use a broader range of data sources beyond daily US equity prices, including international stocks and ETFs. It also demonstrates how to use ML for an intraday strategy with minute-frequency equity data. Furthermore, it extends the coverage of alternative data sources to include SEC filings for sentiment analysis and return forecasts, as well as satellite images to classify land use.\nAnother innovation of the second edition is to replicate several trading applications recently published in top journals:\nChapter 18 demonstrates how to apply convolutional neural networks to time series converted to image format for return predictions based on Sezer and Ozbahoglu (2018).\nChapter 20 shows how to extract risk factors conditioned on stock characteristics for asset pricing using autoencoders based on Autoencoder Asset Pricing Models by Shihao Gu, Bryan T. Kelly, and Dacheng Xiu (2019), and\nChapter 21 shows how to create synthetic training data using generative adversarial networks based on Time-series Generative Adversarial Networks by Jinsung Yoon, Daniel Jarrett, and Mihaela van der Schaar (2019).\nAll applications now use the latest available (at the time of writing) software versions such as pandas 1.0 and TensorFlow 2.2. There is also a customized version of Zipline that makes it easy to include machine learning model predictions when designing a trading strategy.\nInstallation, data sources and bug reports\nThe code examples rely on a wide range of Python libraries from the data science and finance domains. To facilitate installation, we use Docker to provide containerized conda environments.\nUpdate April 2021: with the update of Zipline, it is no longer necessary to use Docker. The installation instructions now refer to OS-specific environment files that should simplify your running of the notebooks.\nUpdate Februar 2021: code sample release 2.0 updates the conda environments provided by the Docker image to Python 3.8, Pandas 1.2, and TensorFlow 1.2, among others; the Zipline backtesting environment with now uses Python 3.6.\nThe installation directory contains detailed instructions on setting up and using a Docker image to run the notebooks. It also contains configuration files for setting up various conda environments and install the packages used in the notebooks directly on your machine if you prefer (and, depending on your system, are prepared to go the extra mile).\nTo download and preprocess many of the data sources used in this book, see the instructions in the README file alongside various notebooks in the data directory.\nIf you have any difficulties installing the environments, downloading the data or running the code, please raise a GitHub issue in the repo (here). Working with GitHub issues has been described here.\nUpdate: You can download the algoseek data used in the book here. See instructions for preprocessing in Chapter 2 and an intraday example with a gradient boosting model in Chapter 12.\nUpdate: The figures directory contains color versions of the charts used in the book.\nOutline & Chapter Summary\nThe book has four parts that address different challenges that arise when sourcing and working with market, fundamental and alternative data sourcing, developing ML solutions to various predictive tasks in the trading context, and designing and evaluating a trading strategy that relies on predictive signals generated by an ML model.\nThe directory for each chapter contains a README with additional information on content, code examples and additional resources.\nPart 1: From Data to Strategy Development\n01 Machine Learning for Trading: From Idea to Execution\n02 Market & Fundamental Data: Sources and Techniques\n03 Alternative Data for Finance: Categories and Use Cases\n04 Financial Feature Engineering: How to research Alpha Factors\n05 Portfolio Optimization and Performance Evaluation\nPart 2: Machine Learning for Trading: Fundamentals\n06 The Machine Learning Process\n07 Linear Models: From Risk Factors to Return Forecasts\n08 The ML4T Workflow: From Model to Strategy Backtesting\n09 Time Series Models for Volatility Forecasts and Statistical Arbitrage\n10 Bayesian ML: Dynamic Sharpe Ratios and Pairs Trading\n11 Random Forests: A Long-Short Strategy for Japanese Stocks\n12 Boosting your Trading Strategy\n13 Data-Driven Risk Factors and Asset Allocation with Unsupervised Learning\nPart 3: Natural Language Processing for Trading\n14 Text Data for Trading: Sentiment Analysis\n15 Topic Modeling: Summarizing Financial News\n16 Word embeddings for Earnings Calls and SEC Filings\nPart 4: Deep & Reinforcement Learning\n17 Deep Learning for Trading\n18 CNN for Financial Time Series and Satellite Images\n19 RNN for Multivariate Time Series and Sentiment Analysis\n20 Autoencoders for Conditional Risk Factors and Asset Pricing\n21 Generative Adversarial Nets for Synthetic Time Series Data\n22 Deep Reinforcement Learning: Building a Trading Agent\n23 Conclusions and Next Steps\n24 Appendix - Alpha Factor Library\nPart 1: From Data to Strategy Development\nThe first part provides a framework for developing trading strategies driven by machine learning (ML). It focuses on the data that power the ML algorithms and strategies discussed in this book, outlines how to engineer and evaluates features suitable for ML models, and how to manage and measure a portfolio's performance while executing a trading strategy.\n01 Machine Learning for Trading: From Idea to Execution\nThis chapter explores industry trends that have led to the emergence of ML as a source of competitive advantage in the investment industry. We will also look at where ML fits into the investment process to enable algorithmic trading strategies.\nMore specifically, it covers the following topics:\nKey trends behind the rise of ML in the investment industry\nThe design and execution of a trading strategy that leverages ML\nPopular use cases for ML in trading\n02 Market & Fundamental Data: Sources and Techniques\nThis chapter shows how to work with market and fundamental data and describes critical aspects of the environment that they reflect. For example, familiarity with various order types and the trading infrastructure matter not only for the interpretation of the data but also to correctly design backtest simulations. We also illustrate how to use Python to access and manipulate trading and financial statement data.\nPractical examples demonstrate how to work with trading data from NASDAQ tick data and Algoseek minute bar data with a rich set of attributes capturing the demand-supply dynamic that we will later use for an ML-based intraday strategy. We also cover various data provider APIs and how to source financial statement information from the SEC.\nIn particular, this chapter covers:\nHow market data reflects the structure of the trading environment\nWorking with intraday trade and quotes data at minute frequency\nReconstructing the limit order book from tick data using NASDAQ ITCH\nSummarizing tick data using various types of bars\nWorking with eXtensible Business Reporting Language (XBRL)-encoded electronic filings\nParsing and combining market and fundamental data to create a P/E series\nHow to access various market and fundamental data sources using Python\n03 Alternative Data for Finance: Categories and Use Cases\nThis chapter outlines categories and use cases of alternative data, describes criteria to assess the exploding number of sources and providers, and summarizes the current market landscape.\nIt also demonstrates how to create alternative data sets by scraping websites, such as collecting earnings call transcripts for use with natural language processing (NLP) and sentiment analysis algorithms in the third part of the book.\nMore specifically, this chapter covers:\nWhich new sources of signals have emerged during the alternative data revolution\nHow individuals, business, and sensors generate a diverse set of alternative data\nImportant categories and providers of alternative data\nEvaluating how the burgeoning supply of alternative data can be used for trading\nWorking with alternative data in Python, such as by scraping the internet\n04 Financial Feature Engineering: How to research Alpha Factors\nIf you are already familiar with ML, you know that feature engineering is a crucial ingredient for successful predictions. It matters at least as much in the trading domain, where academic and industry researchers have investigated for decades what drives asset markets and prices, and which features help to explain or predict price movements.\nThis chapter outlines the key takeaways of this research as a starting point for your own quest for alpha factors. It also presents essential tools to compute and test alpha factors, highlighting how the NumPy, pandas, and TA-Lib libraries facilitate the manipulation of data and present popular smoothing techniques like the wavelets and the Kalman filter that help reduce noise in data. After reading it, you will know about:\nWhich categories of factors exist, why they work, and how to measure them,\nCreating alpha factors using NumPy, pandas, and TA-Lib,\nHow to de-noise data using wavelets and the Kalman filter,\nUsing Zipline to test individual and multiple alpha factors,\nHow to use Alphalens to evaluate predictive performance.\n05 Portfolio Optimization and Performance Evaluation\nAlpha factors generate signals that an algorithmic strategy translates into trades, which, in turn, produce long and short positions. The returns and risk of the resulting portfolio determine whether the strategy meets the investment objectives.\nThere are several approaches to optimize portfolios. These include the application of machine learning (ML) to learn hierarchical relationships among assets and treat them as complements or substitutes when designing the portfolio's risk profile. This chapter covers:\nHow to measure portfolio risk and return\nManaging portfolio weights using mean-variance optimization and alternatives\nUsing machine learning to optimize asset allocation in a portfolio context\nSimulating trades and create a portfolio based on alpha factors using Zipline\nHow to evaluate portfolio performance using pyfolio\nPart 2: Machine Learning for Trading: Fundamentals\nThe second part covers the fundamental supervised and unsupervised learning algorithms and illustrates their application to trading strategies. It also introduces the Quantopian platform that allows you to leverage and combine the data and ML techniques developed in this book to implement algorithmic strategies that execute trades in live markets.\n06 The Machine Learning Process\nThis chapter kicks off Part 2 that illustrates how you can use a range of supervised and unsupervised ML models for trading. We will explain each model's assumptions and use cases before we demonstrate relevant applications using various Python libraries.\nThere are several aspects that many of these models and their applications have in common. This chapter covers these common aspects so that we can focus on model-specific usage in the following chapters. It sets the stage by outlining how to formulate, train, tune, and evaluate the predictive performance of ML models as a systematic workflow. The content includes:\nHow supervised and unsupervised learning from data works\nTraining and evaluating supervised learning models for regression and classification tasks\nHow the bias-variance trade-off impacts predictive performance\nHow to diagnose and address prediction errors due to overfitting\nUsing cross-validation to optimize hyperparameters with a focus on time-series data\nWhy financial data requires additional attention when testing out-of-sample\n07 Linear Models: From Risk Factors to Return Forecasts\nLinear models are standard tools for inference and prediction in regression and classification contexts. Numerous widely used asset pricing models rely on linear regression. Regularized models like Ridge and Lasso regression often yield better predictions by limiting the risk of overfitting. Typical regression applications identify risk factors that drive asset returns to manage risks or predict returns. Classification problems, on the other hand, include directional price forecasts.\nChapter 07 covers the following topics:\nHow linear regression works and which assumptions it makes\nTraining and diagnosing linear regression models\nUsing linear regression to predict stock returns\nUse regularization to improve the predictive performance\nHow logistic regression works\nConverting a regression into a classification problem\n08 The ML4T Workflow: From Model to Strategy Backtesting\nThis chapter presents an end-to-end perspective on designing, simulating, and evaluating a trading strategy driven by an ML algorithm. We will demonstrate in detail how to backtest an ML-driven strategy in a historical market context using the Python libraries backtrader and Zipline. The ML4T workflow ultimately aims to gather evidence from historical data that helps decide whether to deploy a candidate strategy in a live market and put financial resources at risk. A realistic simulation of your strategy needs to faithfully represent how security markets operate and how trades execute. Also, several methodological aspects require attention to avoid biased results and false discoveries that will lead to poor investment decisions.\nMore specifically, after working through this chapter you will be able to:\nPlan and implement end-to-end strategy backtesting\nUnderstand and avoid critical pitfalls when implementing backtests\nDiscuss the advantages and disadvantages of vectorized vs event-driven backtesting engines\nIdentify and evaluate the key components of an event-driven backtester\nDesign and execute the ML4T workflow using data sources at minute and daily frequencies, with ML models trained separately or as part of the backtest\nUse Zipline and backtrader to design and evaluate your own strategies\n09 Time Series Models for Volatility Forecasts and Statistical Arbitrage\nThis chapter focuses on models that extract signals from a time series' history to predict future values for the same time series. Time series models are in widespread use due to the time dimension inherent to trading. It presents tools to diagnose time series characteristics such as stationarity and extract features that capture potentially useful patterns. It also introduces univariate and multivariate time series models to forecast macro data and volatility patterns. Finally, it explains how cointegration identifies common trends across time series and shows how to develop a pairs trading strategy based on this crucial concept.\nIn particular, it covers:\nHow to use time-series analysis to prepare and inform the modeling process\nEstimating and diagnosing univariate autoregressive and moving-average models\nBuilding autoregressive conditional heteroskedasticity (ARCH) models to predict volatility\nHow to build multivariate vector autoregressive models\nUsing cointegration to develop a pairs trading strategy\n10 Bayesian ML: Dynamic Sharpe Ratios and Pairs Trading\nBayesian statistics allows us to quantify uncertainty about future events and refine estimates in a principled way as new information arrives. This dynamic approach adapts well to the evolving nature of financial markets. Bayesian approaches to ML enable new insights into the uncertainty around statistical metrics, parameter estimates, and predictions. The applications range from more granular risk management to dynamic updates of predictive models that incorporate changes in the market environment.\nMore specifically, this chapter covers:\nHow Bayesian statistics applies to machine learning\nProbabilistic programming with PyMC3\nDefining and training machine learning models using PyMC3\nHow to run state-of-the-art sampling methods to conduct approximate inference\nBayesian ML applications to compute dynamic Sharpe ratios, dynamic pairs trading hedge ratios, and estimate stochastic volatility\n11 Random Forests: A Long-Short Strategy for Japanese Stocks\nThis chapter applies decision trees and random forests to trading. Decision trees learn rules from data that encode nonlinear input-output relationships. We show how to train a decision tree to make predictions for regression and classification problems, visualize and interpret the rules learned by the model, and tune the model's hyperparameters to optimize the bias-variance tradeoff and prevent overfitting.\nThe second part of the chapter introduces ensemble models that combine multiple decision trees in a randomized fashion to produce a single prediction with a lower error. It concludes with a long-short strategy for Japanese equities based on trading signals generated by a random forest model.\nIn short, this chapter covers:\nUse decision trees for regression and classification\nGain insights from decision trees and visualize the rules learned from the data\nUnderstand why ensemble models tend to deliver superior results\nUse bootstrap aggregation to address the overfitting challenges of decision trees\nTrain, tune, and interpret random forests\nEmploy a random forest to design and evaluate a profitable trading strategy\n12 Boosting your Trading Strategy\nGradient boosting is an alternative tree-based ensemble algorithm that often produces better results than random forests. The critical difference is that boosting modifies the data used to train each tree based on the cumulative errors made by the model. While random forests train many trees independently using random subsets of the data, boosting proceeds sequentially and reweights the data. This chapter shows how state-of-the-art libraries achieve impressive performance and apply boosting to both daily and high-frequency data to backtest an intraday trading strategy.\nMore specifically, we will cover the following topics:\nHow does boosting differ from bagging, and how did gradient boosting evolve from adaptive boosting,\nDesign and tune adaptive and gradient boosting models with scikit-learn,\nBuild, optimize, and evaluate gradient boosting models on large datasets with the state-of-the-art implementations XGBoost, LightGBM, and CatBoost,\nInterpreting and gaining insights from gradient boosting models using SHAP values, and\nUsing boosting with high-frequency data to design an intraday strategy.\n13 Data-Driven Risk Factors and Asset Allocation with Unsupervised Learning\nDimensionality reduction and clustering are the main tasks for unsupervised learning:\nDimensionality reduction transforms the existing features into a new, smaller set while minimizing the loss of information. A broad range of algorithms exists that differ by how they measure the loss of information, whether they apply linear or non-linear transformations or the constraints they impose on the new feature set.\nClustering algorithms identify and group similar observations or features instead of identifying new features. Algorithms differ in how they define the similarity of observations and their assumptions about the resulting groups.\nMore specifically, this chapter covers:\nHow principal and independent component analysis (PCA and ICA) perform linear dimensionality reduction\nIdentifying data-driven risk factors and eigenportfolios from asset returns using PCA\nEffectively visualizing nonlinear, high-dimensional data using manifold learning\nUsing T-SNE and UMAP to explore high-dimensional image data\nHow k-means, hierarchical, and density-based clustering algorithms work\nUsing agglomerative clustering to build robust portfolios with hierarchical risk parity\nPart 3: Natural Language Processing for Trading\nText data are rich in content, yet unstructured in format and hence require more preprocessing so that a machine learning algorithm can extract the potential signal. The critical challenge consists of converting text into a numerical format for use by an algorithm, while simultaneously expressing the semantics or meaning of the content.\nThe next three chapters cover several techniques that capture language nuances readily understandable to humans so that machine learning algorithms can also interpret them.\n14 Text Data for Trading: Sentiment Analysis\nText data is very rich in content but highly unstructured so that it requires more preprocessing to enable an ML algorithm to extract relevant information. A key challenge consists of converting text into a numerical format without losing its meaning. This chapter shows how to represent documents as vectors of token counts by creating a document-term matrix that, in turn, serves as input for text classification and sentiment analysis. It also introduces the Naive Bayes algorithm and compares its performance to linear and tree-based models.\nIn particular, in this chapter covers:\nWhat the fundamental NLP workflow looks like\nHow to build a multilingual feature extraction pipeline using spaCy and TextBlob\nPerforming NLP tasks like part-of-speech tagging or named entity recognition\nConverting tokens to numbers using the document-term matrix\nClassifying news using the naive Bayes model\nHow to perform sentiment analysis using different ML algorithms\n15 Topic Modeling: Summarizing Financial News\nThis chapter uses unsupervised learning to model latent topics and extract hidden themes from documents. These themes can generate detailed insights into a large corpus of financial reports. Topic models automate the creation of sophisticated, interpretable text features that, in turn, can help extract trading signals from extensive collections of texts. They speed up document review, enable the clustering of similar documents, and produce annotations useful for predictive modeling. Applications include identifying critical themes in company disclosures, earnings call transcripts or contracts, and annotation based on sentiment analysis or using returns of related assets.\nMore specifically, it covers:\nHow topic modeling has evolved, what it achieves, and why it matters\nReducing the dimensionality of the DTM using latent semantic indexing\nExtracting topics with probabilistic latent semantic analysis (pLSA)\nHow latent Dirichlet allocation (LDA) improves pLSA to become the most popular topic model\nVisualizing and evaluating topic modeling results -\nRunning LDA using scikit-learn and gensim\nHow to apply topic modeling to collections of earnings calls and financial news articles\n16 Word embeddings for Earnings Calls and SEC Filings\nThis chapter uses neural networks to learn a vector representation of individual semantic units like a word or a paragraph. These vectors are dense with a few hundred real-valued entries, compared to the higher-dimensional sparse vectors of the bag-of-words model. As a result, these vectors embed or locate each semantic unit in a continuous vector space.\nEmbeddings result from training a model to relate tokens to their context with the benefit that similar usage implies a similar vector. As a result, they encode semantic aspects like relationships among words through their relative location. They are powerful features that we will use with deep learning models in the following chapters.\nMore specifically, in this chapter, we will cover:\nWhat word embeddings are and how they capture semantic information\nHow to obtain and use pre-trained word vectors\nWhich network architectures are most effective at training word2vec models\nHow to train a word2vec model using TensorFlow and gensim\nVisualizing and evaluating the quality of word vectors\nHow to train a word2vec model on SEC filings to predict stock price moves\nHow doc2vec extends word2vec and helps with sentiment analysis\nWhy the transformer\u2019s attention mechanism had such an impact on NLP\nHow to fine-tune pre-trained BERT models on financial data\nPart 4: Deep & Reinforcement Learning\nPart four explains and demonstrates how to leverage deep learning for algorithmic trading. The powerful capabilities of deep learning algorithms to identify patterns in unstructured data make it particularly suitable for alternative data like images and text.\nThe sample applications show, for exapmle, how to combine text and price data to predict earnings surprises from SEC filings, generate synthetic time series to expand the amount of training data, and train a trading agent using deep reinforcement learning. Several of these applications replicate research recently published in top journals.\n17 Deep Learning for Trading\nThis chapter presents feedforward neural networks (NN) and demonstrates how to efficiently train large models using backpropagation while managing the risks of overfitting. It also shows how to use TensorFlow 2.0 and PyTorch and how to optimize a NN architecture to generate trading signals. In the following chapters, we will build on this foundation to apply various architectures to different investment applications with a focus on alternative data. These include recurrent NN tailored to sequential data like time series or natural language and convolutional NN, particularly well suited to image data. We will also cover deep unsupervised learning, such as how to create synthetic data using Generative Adversarial Networks (GAN). Moreover, we will discuss reinforcement learning to train agents that interactively learn from their environment.\nIn particular, this chapter will cover\nHow DL solves AI challenges in complex domains\nKey innovations that have propelled DL to its current popularity\nHow feedforward networks learn representations from data\nDesigning and training deep neural networks (NNs) in Python\nImplementing deep NNs using Keras, TensorFlow, and PyTorch\nBuilding and tuning a deep NN to predict asset returns\nDesigning and backtesting a trading strategy based on deep NN signals\n18 CNN for Financial Time Series and Satellite Images\nCNN architectures continue to evolve. This chapter describes building blocks common to successful applications, demonstrates how transfer learning can speed up learning, and how to use CNNs for object detection. CNNs can generate trading signals from images or time-series data. Satellite data can anticipate commodity trends via aerial images of agricultural areas, mines, or transport networks. -----> Camera !!!  footage can help predict consumer activity; we show how to build a CNN that classifies economic activity in satellite images. CNNs can also deliver high-quality time-series classification results by exploiting their structural similarity with images, and we design a strategy based on time-series data formatted like images.\nMore specifically, this chapter covers:\nHow CNNs employ several building blocks to efficiently model grid-like data\nTraining, tuning and regularizing CNNs for images and time series data using TensorFlow\nUsing transfer learning to streamline CNNs, even with fewer data\nDesigning a trading strategy using return predictions by a CNN trained on time-series data formatted like images\nHow to classify economic activity based on satellite images\n19 RNN for Multivariate Time Series and Sentiment Analysis\nRecurrent neural networks (RNNs) compute each output as a function of the previous output and new data, effectively creating a model with memory that shares parameters across a deeper computational graph. Prominent architectures include Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) that address the challenges of learning long-range dependencies. RNNs are designed to map one or more input sequences to one or more output sequences and are particularly well suited to natural language. They can also be applied to univariate and multivariate time series to predict market or fundamental data. This chapter covers how RNN can model alternative text data using the word embeddings that we covered in Chapter 16 to classify the sentiment expressed in documents.\nMore specifically, this chapter addresses:\nHow recurrent connections allow RNNs to memorize patterns and model a hidden state\nUnrolling and analyzing the computational graph of RNNs\nHow gated units learn to regulate RNN memory from data to enable long-range dependencies\nDesigning and training RNNs for univariate and multivariate time series in Python\nHow to learn word embeddings or use pretrained word vectors for sentiment analysis with RNNs\nBuilding a bidirectional RNN to predict stock returns using custom word embeddings\n20 Autoencoders for Conditional Risk Factors and Asset Pricing\nThis chapter shows how to leverage unsupervised deep learning for trading. We also discuss autoencoders, namely, a neural network trained to reproduce the input while learning a new representation encoded by the parameters of a hidden layer. Autoencoders have long been used for nonlinear dimensionality reduction, leveraging the NN architectures we covered in the last three chapters. We replicate a recent AQR paper that shows how autoencoders can underpin a trading strategy. We will use a deep neural network that relies on an autoencoder to extract risk factors and predict equity returns, conditioned on a range of equity attributes.\nMore specifically, in this chapter you will learn about:\nWhich types of autoencoders are of practical use and how they work\nBuilding and training autoencoders using Python\nUsing autoencoders to extract data-driven risk factors that take into account asset characteristics to predict returns\n21 Generative Adversarial Nets for Synthetic Time Series Data\nThis chapter introduces generative adversarial networks (GAN). GANs train a generator and a discriminator network in a competitive setting so that the generator learns to produce samples that the discriminator cannot distinguish from a given class of training data. The goal is to yield a generative model capable of producing synthetic samples representative of this class. While most popular with image data, GANs have also been used to generate synthetic time-series data in the medical domain. Subsequent experiments with financial data explored whether GANs can produce alternative price trajectories useful for ML training or strategy backtests. We replicate the 2019 NeurIPS Time-Series GAN paper to illustrate the approach and demonstrate the results.\nMore specifically, in this chapter you will learn about:\nHow GANs work, why they are useful, and how they could be applied to trading\nDesigning and training GANs using TensorFlow 2\nGenerating synthetic financial data to expand the inputs available for training ML models and backtesting\n22 Deep Reinforcement Learning: Building a Trading Agent\nReinforcement Learning (RL) models goal-directed learning by an agent that interacts with a stochastic environment. RL optimizes the agent's decisions concerning a long-term objective by learning the value of states and actions from a reward signal. The ultimate goal is to derive a policy that encodes behavioral rules and maps states to actions. This chapter shows how to formulate and solve an RL problem. It covers model-based and model-free methods, introduces the OpenAI Gym environment, and combines deep learning with RL to train an agent that navigates a complex environment. Finally, we'll show you how to adapt RL to algorithmic trading by modeling an agent that interacts with the financial market while trying to optimize an objective function.\nMore specifically,this chapter will cover:\nDefine a Markov decision problem (MDP)\nUse value and policy iteration to solve an MDP\nApply Q-learning in an environment with discrete states and actions\nBuild and train a deep Q-learning agent in a continuous environment\nUse the OpenAI Gym to design a custom market environment and train an RL agent to trade stocks\n23 Conclusions and Next Steps\nIn this concluding chapter, we will briefly summarize the essential tools, applications, and lessons learned throughout the book to avoid losing sight of the big picture after so much detail. We will then identify areas that we did not cover but would be worth focusing on as you expand on the many machine learning techniques we introduced and become productive in their daily use.\nIn sum, in this chapter, we will\nReview key takeaways and lessons learned\nPoint out the next steps to build on the techniques in this book\nSuggest ways to incorporate ML into your investment process\n24 Appendix - Alpha Factor Library\nThroughout this book, we emphasized how the smart design of features, including appropriate preprocessing and denoising, typically leads to an effective strategy. This appendix synthesizes some of the lessons learned on feature engineering and provides additional information on this vital topic.\nTo this end, we focus on the broad range of indicators implemented by TA-Lib (see Chapter 4) and WorldQuant's 101 Formulaic Alphas paper (Kakushadze 2016), which presents real-life quantitative trading factors used in production with an average holding period of 0.6-6.4 days.\nThis chapter covers:\nHow to compute several dozen technical indicators using TA-Lib and NumPy/pandas,\nCreating the formulaic alphas describe in the above paper, and\nEvaluating the predictive quality of the results using various metrics from rank correlation and mutual information to feature importance, SHAP values and Alphalens.",
      "link": "https://github.com/stefan-jansen/machine-learning-for-trading"
    },
    {
      "autor": "Learn-Data-Science-For-Free",
      "date": "NaN",
      "content": "This Repository Consists of Free Resources needed for a person to learn Datascience from the beginning to end. This repository is divided into Four main Parts. They are\nPart 1:- [Roadmap]\nPart 2:- [Free Online Courses]\nPart 3:- [500 Datascience Projects]\nPart 4:- [100+ Free Machine Learning Books]\nThis repositary is a combination of different resources lying scattered all over the internet. The reason for making such an repositary is to combine all the valuable resources in a sequential manner, so that it helps every beginners who are in a search of free and structured learning resource for Datascience. I hope it helps many people who could not afford a large fee for their education. This repositary shall be constantly updated on the basics of availability of new free resources.\nIf you guys like this Repo, please SHARE with everyone who are in need of these materials.\nFor Constant Updates, Follow me on Twitter\nGive a \ud83c\udf1f if it's Useful and Share with other Datascience Enthusiasts.\nData-Scientist-Roadmap (2021)\n1_ Fundamentals\n1_ Matrices & Algebra fundamentals\nAbout\nIn mathematics, a matrix is a rectangular array of numbers, symbols, or expressions, arranged in rows and columns. A matrix could be reduced as a submatrix of a matrix by deleting any collection of rows and/or columns.\nOperations\nThere are a number of basic operations that can be applied to modify matrices:\nAddition\nScalar Multiplication\nTransposition\nMultiplication\n2_ Hash function, binary tree, O(n)\nHash function\nDefinition\nA hash function is any function that can be used to map data of arbitrary size to data of fixed size. One use is a data structure called a hash table, widely used in computer software for rapid data lookup. Hash functions accelerate table or database lookup by detecting duplicated records in a large file.\nBinary tree\nDefinition\nIn computer science, a binary tree is a tree data structure in which each node has at most two children, which are referred to as the left child and the right child.\nO(n)\nDefinition\nIn computer science, big O notation is used to classify algorithms according to how their running time or space requirements grow as the input size grows. In analytic number theory, big O notation is often used to express a bound on the difference between an arithmetical function and a better understood approximation.\n3_ Relational algebra, DB basics\nDefinition\nRelational algebra is a family of algebras with a well-founded semantics used for modelling the data stored in relational databases, and defining queries on it.\nThe main application of relational algebra is providing a theoretical foundation for relational databases, particularly query languages for such databases, chief among which is SQL.\nNatural join\nAbout\nIn SQL language, a natural junction between two tables will be done if :\nAt least one column has the same name in both tables\nTheses two columns have the same data type\nCHAR (character)\nINT (integer)\nFLOAT (floating point numeric data)\nVARCHAR (long character chain)\nmySQL request\nSELECT <COLUMNS>\nFROM <TABLE_1>\nNATURAL JOIN <TABLE_2>\nSELECT <COLUMNS>\nFROM <TABLE_1>, <TABLE_2>\nWHERE TABLE_1.ID = TABLE_2.ID\n4_ Inner, Outer, Cross, theta-join\nInner join\nThe INNER JOIN keyword selects records that have matching values in both tables.\nRequest\nSELECT column_name(s)\nFROM table1\nINNER JOIN table2 ON table1.column_name = table2.column_name;\nOuter join\nThe FULL OUTER JOIN keyword return all records when there is a match in either left (table1) or right (table2) table records.\nRequest\nSELECT column_name(s)\nFROM table1\nFULL OUTER JOIN table2 ON table1.column_name = table2.column_name;\nLeft join\nThe LEFT JOIN keyword returns all records from the left table (table1), and the matched records from the right table (table2). The result is NULL from the right side, if there is no match.\nRequest\nSELECT column_name(s)\nFROM table1\nLEFT JOIN table2 ON table1.column_name = table2.column_name;\nRight join\nThe RIGHT JOIN keyword returns all records from the right table (table2), and the matched records from the left table (table1). The result is NULL from the left side, when there is no match.\nRequest\nSELECT column_name(s)\nFROM table1\nRIGHT JOIN table2 ON table1.column_name = table2.column_name;\n5_ CAP theorem\nIt is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees:\nEvery read receives the most recent write or an error.\nEvery request receives a (non-error) response \u2013 without guarantee that it contains the most recent write.\nThe system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes.\nIn other words, the CAP Theorem states that in the presence of a network partition, one has to choose between consistency and availability. Note that consistency as defined in the CAP Theorem is quite different from the consistency guaranteed in ACID database transactions.\n6_ Tabular data\nTabular data are opposed to relational data, like SQL database.\nIn tabular data, everything is arranged in columns and rows. Every row have the same number of column (except for missing value, which could be substituted by \"N/A\".\nThe first line of tabular data is most of the time a header, describing the content of each column.\nThe most used format of tabular data in data science is CSV_. Every column is surrounded by a character (a tabulation, a coma ..), delimiting this column from its two neighbours.\n7_ Entropy\nEntropy is a measure of uncertainty. High entropy means the data has high variance and thus contains a lot of information and/or noise.\nFor instance, a constant function where f(x) = 4 for all x has no entropy and is easily predictable, has little information, has no noise and can be succinctly represented . Similarly, f(x) = ~4 has some entropy while f(x) = random number is very high entropy due to noise.\n8_ Data frames & series\nA data frame is used for storing data tables. It is a list of vectors of equal length.\nA series is a series of data points ordered.\n9_ Sharding\nSharding is horizontal(row wise) database partitioning as opposed to vertical(column wise) partitioning which is Normalization\nWhy use Sharding?\nDatabase systems with large data sets or high throughput applications can challenge the capacity of a single server.\nTwo methods to address the growth : Vertical Scaling and Horizontal Scaling\nVertical Scaling\nInvolves increasing the capacity of a single server\nBut due to technological and economical restrictions, a single machine may not be sufficient for the given workload.\nHorizontal Scaling\nInvolves dividing the dataset and load over multiple servers, adding additional servers to increase capacity as required\nWhile the overall speed or capacity of a single machine may not be high, each machine handles a subset of the overall workload, potentially providing better efficiency than a single high-speed high-capacity server.\nIdea is to use concepts of Distributed systems to achieve scale\nBut it comes with same tradeoffs of increased complexity that comes hand in hand with distributed systems.\nMany Database systems provide Horizontal scaling via Sharding the datasets.\n10_ OLAP\nOnline analytical processing, or OLAP, is an approach to answering multi-dimensional analytical (MDA) queries swiftly in computing.\nOLAP is part of the broader category of business intelligence, which also encompasses relational database, report writing and data mining. Typical applications of OLAP include _business reporting for sales, marketing, management reporting, business process management (BPM), budgeting and forecasting, financial reporting and similar areas, with new applications coming up, such as agriculture.\nThe term OLAP was created as a slight modification of the traditional database term online transaction processing (OLTP).\n11_ Multidimensional Data model\n12_ ETL\nExtract\nextracting the data from the multiple heterogenous source system(s)\ndata validation to confirm whether the data pulled has the correct/expected values in a given domain\nTransform\nextracted data is fed into a pipeline which applies multiple functions on top of data\nthese functions intend to convert the data into the format which is accepted by the end system\ninvolves cleaning the data to remove noise, anamolies and redudant data\nLoad\nloads the transformed data into the end target\n13_ Reporting vs BI vs Analytics\n14_ JSON and XML\nJSON\nJSON is a language-independent data format. Example describing a person:\n{\n\"firstName\": \"John\",\n\"lastName\": \"Smith\",\n\"isAlive\": true,\n\"age\": 25,\n\"address\": {\n\"streetAddress\": \"21 2nd Street\",\n\"city\": \"New York\",\n\"state\": \"NY\",\n\"postalCode\": \"10021-3100\"\n},\n\"phoneNumbers\": [\n{\n\"type\": \"home\",\n\"number\": \"212 555-1234\"\n},\n{\n\"type\": \"office\",\n\"number\": \"646 555-4567\"\n},\n{\n\"type\": \"mobile\",\n\"number\": \"123 456-7890\"\n}\n],\n\"children\": [],\n\"spouse\": null\n}\nXML\nExtensible Markup Language (XML) is a markup language that defines a set of rules for encoding documents in a format that is both human-readable and machine-readable.\n<CATALOG>\n<PLANT>\n<COMMON>Bloodroot</COMMON>\n<BOTANICAL>Sanguinaria canadensis</BOTANICAL>\n<ZONE>4</ZONE>\n<LIGHT>Mostly Shady</LIGHT>\n<PRICE>$2.44</PRICE>\n<AVAILABILITY>031599</AVAILABILITY>\n</PLANT>\n<PLANT>\n<COMMON>Columbine</COMMON>\n<BOTANICAL>Aquilegia canadensis</BOTANICAL>\n<ZONE>3</ZONE>\n<LIGHT>Mostly Shady</LIGHT>\n<PRICE>$9.37</PRICE>\n<AVAILABILITY>030699</AVAILABILITY>\n</PLANT>\n<PLANT>\n<COMMON>Marsh Marigold</COMMON>\n<BOTANICAL>Caltha palustris</BOTANICAL>\n<ZONE>4</ZONE>\n<LIGHT>Mostly Sunny</LIGHT>\n<PRICE>$6.81</PRICE>\n<AVAILABILITY>051799</AVAILABILITY>\n</PLANT>\n</CATALOG>\n15_ NoSQL\nnoSQL is oppsed to relationnal databases (stand for __N__ot __O__nly SQL). Data are not structured and there's no notion of keys between tables.\nAny kind of data can be stored in a noSQL database (JSON, CSV, ...) whithout thinking about a complex relationnal scheme.\nCommonly used noSQL stacks: Cassandra, MongoDB, Redis, Oracle noSQL ...\n16_ Regex\nAbout\nReg ular ex pressions (regex) are commonly used in informatics.\nIt can be used in a wide range of possibilities :\nText replacing\nExtract information in a text (email, phone number, etc)\nList files with the .txt extension ..\nhttp://regexr.com/ is a good website for experimenting on Regex.\nUtilisation\nTo use them in Python, just import:\nimport re\n17_ Vendor landscape\n18_ Env Setup\n2_ Statistics\nStatistics-101 for data noobs\n1_ Pick a dataset\nDatasets repositories\nGeneralists\nKAGGLE\nGoogle\nMedical\nPMC\nOther languages\nFrench\nDATAGOUV\n2_ Descriptive statistics\nMean\nIn probability and statistics, population mean and expected value are used synonymously to refer to one measure of the central tendency either of a probability distribution or of the random variable characterized by that distribution.\nFor a data set, the terms arithmetic mean, mathematical expectation, and sometimes average are used synonymously to refer to a central value of a discrete set of numbers: specifically, the sum of the values divided by the number of values.\nMedian\nThe median is the value separating the higher half of a data sample, a population, or a probability distribution, from the lower half. In simple terms, it may be thought of as the \"middle\" value of a data set.\nDescriptive statistics in Python\nNumpy is a python library widely used for statistical analysis.\nInstallation\npip3 install numpy\nUtilization\nimport numpy\n3_ Exploratory data analysis\nThe step includes visualization and analysis of data.\nRaw data may possess improper distributions of data which may lead to issues moving forward.\nAgain, during applications we must also know the distribution of data, for instance, the fact whether the data is linear or spirally distributed.\nGuide to EDA in Python\nLibraries in Python\nMatplotlib\nLibrary used to plot graphs in Python\nInstallation:\npip3 install matplotlib\nUtilization:\nimport matplotlib.pyplot as plt\nPandas\nLibrary used to large datasets in python\nInstallation:\npip3 install pandas\nUtilization:\nimport pandas as pd\nSeaborn\nYet another Graph Plotting Library in Python.\nInstallation:\npip3 install seaborn\nUtilization:\nimport seaborn as sns\nPCA\nPCA stands for principle component analysis.\nWe often require to shape of the data distribution as we have seen previously. We need to plot the data for the same.\nData can be Multidimensional, that is, a dataset can have multiple features.\nWe can plot only two dimensional data, so, for multidimensional data, we project the multidimensional distribution in two dimensions, preserving the principle components of the distribution, in order to get an idea of the actual distribution through the 2D plot.\nIt is used for dimensionality reduction also. Often it is seen that several features do not significantly contribute any important insight to the data distribution. Such features creates complexity and increase dimensionality of the data. Such features are not considered which results in decrease of the dimensionality of the data.\nMathematical Explanation\nApplication in Python\n4_ Histograms\nHistograms are representation of distribution of numerical data. The procedure consists of binnng the numeric values using range divisions i.e, the entire range in which the data varies is split into several fixed intervals. Count or frequency of occurences of the numbers in the range of the bins are represented.\nHistograms\nIn python, Pandas,Matplotlib,Seaborn can be used to create Histograms.\n5_ Percentiles & outliers\nPercentiles\nPercentiles are numberical measures in statistics, which represents how much or what percentage of data falls below a given number or instance in a numerical data distribution.\nFor instance, if we say 70 percentile, it represents, 70% of the data in the ditribution are below the given numerical value.\nPercentiles\nOutliers\nOutliers are data points(numerical) which have significant differences with other data points. They differ from majority of points in the distribution. Such points may cause the central measures of distribution, like mean, and median. So, they need to be detected and removed.\nOutliers\nBox Plots can be used detect Outliers in the data. They can be created using Seaborn library\n6_ Probability theory\nProbability is the likelihood of an event in a Random experiment. For instance, if a coin is tossed, the chance of getting a head is 50% so, probability is 0.5.\nSample Space: It is the set of all possible outcomes of a Random Experiment. Favourable Outcomes: The set of outcomes we are looking for in a Random Experiment\nProbability = (Number of Favourable Outcomes) / (Sample Space)\nProbability theory is a branch of mathematics that is associated with the concept of probability.\nBasics of Probability\n7_ Bayes theorem\nConditional Probability:\nIt is the probability of one event occurring, given that another event has already occurred. So, it gives a sense of relationship between two events and the probabilities of the occurences of those events.\nIt is given by:\nP( A | B ) : Probability of occurence of A, after B occured.\nThe formula is given by:\nSo, P(A|B) is equal to Probablity of occurence of A and B, divided by Probability of occurence of B.\nGuide to Conditional Probability\nBayes Theorem\nBayes theorem provides a way to calculate conditional probability. Bayes theorem is widely used in machine learning most in Bayesian Classifiers.\nAccording to Bayes theorem the probability of A, given that B has already occurred is given by Probability of A multiplied by the probability of B given A has already occurred divided by the probability of B.\nP(A|B) = P(A).P(B|A) / P(B)\nGuide to Bayes Theorem\n8_ Random variables\nRandom variable are the numeric outcome of an experiment or random events. They are normally a set of values.\nThere are two main types of Random Variables:\nDiscrete Random Variables: Such variables take only a finite number of distinct values\nContinous Random Variables: Such variables can take an infinite number of possible values.\n9_ Cumul Dist Fn (CDF)\nIn probability theory and statistics, the cumulative distribution function (CDF) of a real-valued random variable X, or just distribution function of X, evaluated at x, is the probability that X will take a value less than or equal to x.\nThe cumulative distribution function of a real-valued random variable X is the function given by:\nResource:\nWikipedia\n10_ Continuous distributions\nA continuous distribution describes the probabilities of the possible values of a continuous random variable. A continuous random variable is a random variable with a set of possible values (known as the range) that is infinite and uncountable.\n11_ Skewness\nSkewness is the measure of assymetry in the data distribution or a random variable distribution about its mean.\nSkewness can be positive, negative or zero.\nNegative skew: Distribution Concentrated in the right, left tail is longer.\nPositive skew: Distribution Concentrated in the left, right tail is longer.\nVariation of central tendency measures are shown below.\nData Distribution are often Skewed which may cause trouble during processing the data. Skewed Distribution can be converted to Symmetric Distribution, taking Log of the distribution.\nSkew Distribution\nLog of the Skew Distribution.\nGuide to Skewness\n12_ ANOVA\nANOVA stands for analysis of variance.\nIt is used to compare among groups of data distributions.\nOften we are provided with huge data. They are too huge to work with. The total data is called the Population.\nIn order to work with them, we pick random smaller groups of data. They are called Samples.\nANOVA is used to compare the variance among these groups or samples.\nVariance of group is given by:\nThe differences in the collected samples are observed using the differences between the means of the groups. We often use the t-test to compare the means and also to check if the samples belong to the same population,\nNow, t-test can only be possible among two groups. But, often we get more groups or samples.\nIf we try to use t-test for more than two groups we have to perform t-tests multiple times, once for each pair. This is where ANOVA is used.\nANOVA has two components:\n1.Variation within each group\n2.Variation between groups\nIt works on a ratio called the F-Ratio\nIt is given by:\nF ratio shows how much of the total variation comes from the variation between groups and how much comes from the variation within groups. If much of the variation comes from the variation between groups, it is more likely that the mean of groups are different. However, if most of the variation comes from the variation within groups, then we can conclude the elements in a group are different rather than entire groups. The larger the F ratio, the more likely that the groups have different means.\nResources:\nDefnition\nGUIDE 1\nDetails\n13_ Prob Den Fn (PDF)\nIt stands for probability density function.\nIn probability theory, a probability density function (PDF), or density of a continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the random variable would equal that sample.\nThe probability density function (PDF) P(x) of a continuous distribution is defined as the derivative of the (cumulative) distribution function D(x).\nIt is given by the integral of the function over a given range.\n14_ Central Limit theorem\n15_ Monte Carlo method\n16_ Hypothesis Testing\nTypes of curves\nWe need to know about two distribution curves first.\nDistribution curves reflect the probabilty of finding an instance or a sample of a population at a certain value of the distribution.\nNormal Distribution\nThe normal distribution represents how the data is distributed. In this case, most of the data samples in the distribution are scattered at and around the mean of the distribution. A few instances are scattered or present at the long tail ends of the distribution.\nFew points about Normal Distributions are:\nThe curve is always Bell-shaped. This is because most of the data is found around the mean, so the proababilty of finding a sample at the mean or central value is more.\nThe curve is symmetric\nThe area under the curve is always 1. This is because all the points of the distribution must be present under the curve\nFor Normal Distribution, Mean and Median lie on the same line in the distribution.\nStandard Normal Distribution\nThis type of distribution are normal distributions which following conditions.\nMean of the distribution is 0\nThe Standard Deviation of the distribution is equal to 1.\nThe idea of Hypothesis Testing works completely on the data distributions.\nHypothesis Testing\nHypothesis testing is a statistical method that is used in making statistical decisions using experimental data. Hypothesis Testing is basically an assumption that we make about the population parameter.\nFor example, say, we take the hypothesis that boys in a class are taller than girls.\nThe above statement is just an assumption on the population of the class.\nHypothesis is just an assumptive proposal or statement made on the basis of observations made on a set of information or data.\nWe initially propose two mutually exclusive statements based on the population of the sample data.\nThe initial one is called NULL HYPOTHESIS. It is denoted by H0.\nThe second one is called ALTERNATE HYPOTHESIS. It is denoted by H1 or Ha. It is used as a contrary to Null Hypothesis.\nBased on the instances of the population we accept or reject the NULL Hypothesis and correspondingly we reject or accept the ALTERNATE Hypothesis.\nLevel of Significance\nIt is the degree which we consider to decide whether to accept or reject the NULL hypothesis. When we consider a hypothesis on a population, it is not the case that 100% or all instances of the population abides the assumption, so we decide a level of significance as a cutoff degree, i.e, if our level of significance is 5%, and (100-5)% = 95% of the data abides by the assumption, we accept the Hypothesis.\nIt is said with 95% confidence, the hypothesis is accepted\nThe non-reject region is called acceptance region or beta region. The rejection regions are called critical or alpha regions. alpha denotes the level of significance.\nIf level of significance is 5%. the two alpha regions have (2.5+2.5)% of the population and the beta region has the 95%.\nThe acceptance and rejection gives rise to two kinds of errors:\nType-I Error: NULL Hypothesis is true, but wrongly Rejected.\nType-II Error: NULL Hypothesis if false but is wrongly accepted.\nTests for Hypothesis\nOne Tailed Test:\nThis is a test for Hypothesis, where the rejection region is only one side of the sampling distribution. The rejection region may be in right tail end or in the left tail end.\nThe idea is if we say our level of significance is 5% and we consider a hypothesis \"Hieght of Boys in a class is <=6 ft\". We consider the hypothesis true if atmost 5% of our population are more than 6 feet. So, this will be one-tailed as the test condition only restricts one tail end, the end with hieght > 6ft.\nIn this case, the rejection region extends at both tail ends of the distribution.\nThe idea is if we say our level of significance is 5% and we consider a hypothesis \"Hieght of Boys in a class is !=6 ft\".\nHere, we can accept the NULL hyposthesis iff atmost 5% of the population is less than or greater than 6 feet. So, it is evident that the crirtical region will be at both tail ends and the region is 5% / 2 = 2.5% at both ends of the distribution.\n17_ p-Value\nBefore we jump into P-values we need to look at another important topic in the context: Z-test.\nZ-test\nWe need to know two terms: Population and Sample.\nPopulation describes the entire available data distributed. So, it refers to all records provided in the dataset.\nSample is said to be a group of data points randomly picked from a population or a given distribution. The size of the sample can be any number of data points, given by sample size.\nZ-test is simply used to determine if a given sample distribution belongs to a given population.\nNow,for Z-test we have to use Standard Normal Form for the standardized comparison measures.\nAs we already have seen, standard normal form is a normal form with mean=0 and standard deviation=1.\nThe Standard Deviation is a measure of how much differently the points are distributed around the mean.\nIt states that approximately 68% , 95% and 99.7% of the data lies within 1, 2 and 3 standard deviations of a normal distribution respectively.\nNow, to convert the normal distribution to standard normal distribution we need a standard score called Z-Score. It is given by:\nx = value that we want to standardize\n\u00b5 = mean of the distribution of x\n\u03c3 = standard deviation of the distribution of x\nWe need to know another concept Central Limit Theorem.\nCentral Limit Theorem\nThe theorem states that the mean of the sampling distribution of the sample means is equal to the population mean irrespective if the distribution of population where sample size is greater than 30.\nAnd\nThe sampling distribution of sampling mean will also follow the normal distribution.\nSo, it states, if we pick several samples from a distribution with the size above 30, and pick the static sample means and use the sample means to create a distribution, the mean of the newly created sampling distribution is equal to the original population mean.\nAccording to the theorem, if we draw samples of size N, from a population with population mean \u03bc and population standard deviation \u03c3, the condition stands:\ni.e, mean of the distribution of sample means is equal to the sample means.\nThe standard deviation of the sample means is give by:\nThe above term is also called standard error.\nWe use the theory discussed above for Z-test. If the sample mean lies close to the population mean, we say that the sample belongs to the population and if it lies at a distance from the population mean, we say the sample is taken from a different population.\nTo do this we use a formula and check if the z statistic is greater than or less than 1.96 (considering two tailed test, level of significance = 5%)\nThe above formula gives Z-static\nz = z statistic\nX\u0304 = sample mean\n\u03bc = population mean\n\u03c3 = population standard deviation\nn = sample size\nNow, as the Z-score is used to standardize the distribution, it gives us an idea how the data is distributed overall.\nP-values\nIt is used to check if the results are statistically significant based on the significance level.\nSay, we perform an experiment and collect observations or data. Now, we make a hypothesis (NULL hypothesis) primary, and a second hypothesis, contradictory to the first one called the alternative hypothesis.\nThen we decide a level of significance which serve as a threshold for our null hypothesis. The P value actually gives the probability of the statement. Say, the p-value of our alternative hypothesis is 0.02, it means the probability of alternate hypothesis happenning is 2%.\nNow, the level of significance into play to decide if we can allow 2% or p-value of 0.02. It can be said as a level of endurance of the null hypothesis. If our level of significance is 5% using a two tailed test, we can allow 2.5% on both ends of the distribution, we accept the NULL hypothesis, as level of significance > p-value of alternate hypothesis.\nBut if the p-value is greater than level of significance, we tell that the result is statistically significant, and we reject NULL hypothesis. .\nResources:\nhttps://medium.com/analytics-vidhya/everything-you-should-know-about-p-value-from-scratch-for-data-science-f3c0bfa3c4cc\nhttps://towardsdatascience.com/p-values-explained-by-data-scientist-f40a746cfc8\n3.https://medium.com/analytics-vidhya/z-test-demystified-f745c57c324c\n18_ Chi2 test\nChi2 test is extensively used in data science and machine learning problems for feature selection.\nA chi-square test is used in statistics to test the independence of two events. So, it is used to check for independence of features used. Often dependent features are used which do not convey a lot of information but adds dimensionality to a feature space.\nIt is one of the most common ways to examine relationships between two or more categorical variables.\nIt involves calculating a number, called the chi-square statistic - \u03c72. Which follows a chi-square distribution.\nIt is given as the summation of the difference of the expected values and observed value divided by the observed value.\nResources:\nDefinitions\nGuide 1\nGuide 2\nExample of Operation\n19_ Estimation\n20_ Confid Int (CI)\n21_ MLE\n22_ Kernel Density estimate\nIn statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample.\nKernel Density estimate can be regarded as another way to represent the probability distribution.\nIt consists of choosing a kernel function. There are mostly three used.\nGaussian\nBox\nTri\nThe kernel function depicts the probability of finding a data point. So, it is highest at the centre and decreases as we move away from the point.\nWe assign a kernel function over all the data points and finally calculate the density of the functions, to get the density estimate of the distibuted data points. It practically adds up the Kernel function values at a particular point on the axis. It is as shown below.\nNow, the kernel function is given by:\nwhere K is the kernel \u2014 a non-negative function \u2014 and h > 0 is a smoothing parameter called the bandwidth.\nThe 'h' or the bandwidth is the parameter, on which the curve varies.\nKernel density estimate (KDE) with different bandwidths of a random sample of 100 points from a standard normal distribution. Grey: true density (standard normal). Red: KDE with h=0.05. Black: KDE with h=0.337. Green: KDE with h=2.\nResources:\nBasics\nAdvanced\n23_ Regression\nRegression tasks deal with predicting the value of a dependent variable from a set of independent variables.\nSay, we want to predict the price of a car. So, it becomes a dependent variable say Y, and the features like engine capacity, top speed, class, and company become the independent variables, which helps to frame the equation to obtain the price.\nIf there is one feature say x. If the dependent variable y is linearly dependent on x, then it can be given by y=mx+c, where the m is the coefficient of the independent in the equation, c is the intercept or bias.\nThe image shows the types of regression\nGuide to Regression\n24_ Covariance\nVariance\nThe variance is a measure of how dispersed or spread out the set is. If it is said that the variance is zero, it means all the elements in the dataset are same. If the variance is low, it means the data are slightly dissimilar. If the variance is very high, it means the data in the dataset are largely dissimilar.\nMathematically, it is a measure of how far each value in the data set is from the mean.\nVariance (sigma^2) is given by summation of the square of distances of each point from the mean, divided by the number of points\nCovariance\nCovariance gives us an idea about the degree of association between two considered random variables. Now, we know random variables create distributions. Distribution are a set of values or data points which the variable takes and we can easily represent as vectors in the vector space.\nFor vectors covariance is defined as the dot product of two vectors. The value of covariance can vary from positive infinity to negative infinity. If the two distributions or vectors grow in the same direction the covariance is positive and vice versa. The Sign gives the direction of variation and the Magnitude gives the amount of variation.\nCovariance is given by:\nwhere Xi and Yi denotes the i-th point of the two distributions and X-bar and Y-bar represent the mean values of both the distributions, and n represents the number of values or data points in the distribution.\n25_ Correlation\nCovariance measures the total relation of the variables namely both direction and magnitude. Correlation is a scaled measure of covariance. It is dimensionless and independent of scale. It just shows the strength of variation for both the variables.\nMathematically, if we represent the distribution using vectors, correlation is said to be the cosine angle between the vectors. The value of correlation varies from +1 to -1. +1 is said to be a strong positive correlation and -1 is said to be a strong negative correlation. 0 implies no correlation, or the two variables are independent of each other.\nCorrelation is given by:\nWhere:\n\u03c1(X,Y) \u2013 the correlation between the variables X and Y\nCov(X,Y) \u2013 the covariance between the variables X and Y\n\u03c3X \u2013 the standard deviation of the X-variable\n\u03c3Y \u2013 the standard deviation of the Y-variable\nStandard deviation is given by square roo of variance.\n26_ Pearson coeff\n27_ Causation\n28_ Least2-fit\n29_ Euclidian Distance\nEucladian Distance is the most used and standard measure for the distance between two points.\nIt is given as the square root of sum of squares of the difference between coordinates of two points.\nThe Euclidean distance between two points in Euclidean space is a number, the length of a line segment between the two points. It can be calculated from the Cartesian coordinates of the points using the Pythagorean theorem, and is occasionally called the Pythagorean distance.\nIn the Euclidean plane, let point p have Cartesian coordinates (p{1},p{2}) and let point q have coordinates (q_{1},q_{2}). Then the distance between p and q is given by:__\n3_ Programming\n1_ Python Basics\nAbout\nPython is a high-level programming langage. I can be used in a wide range of works.\nCommonly used in data-science, Python has a huge set of libraries, helpful to quickly do something.\nMost of informatics systems already support Python, without installing anything.\nExecute a script\nDownload the .py file on your computer\nMake it executable (chmod +x file.py on Linux)\nOpen a terminal and go to the directory containing the python file\npython file.py to run with Python2 or python3 file.py with Python3\n2_ Working in excel\n3_ R setup / R studio\nAbout\nR is a programming language specialized in statistics and mathematical visualizations.\nIt can be used with manually created scripts using the terminal, or directly in the R console.\nInstallation\nLinux\nsudo apt-get install r-base\nsudo apt-get install r-base-dev\nWindows\nDownload the .exe setup available on CRAN website.\nR-studio\nRstudio is a graphical interface for R. It is available for free on their website.\nThis interface is divided in 4 main areas :\nThe top left is the script you are working on (highlight code you want to execute and press Ctrl + Enter)\nThe bottom left is the console to instant-execute some lines of codes\nThe top right is showing your environment (variables, history, ...)\nThe bottom right show figures you plotted, packages, help ... The result of code execution\n4_ R basics\nR is an open source programming language and software environment for statistical computing and graphics that is supported by the R Foundation for Statistical Computing.\nThe R language is widely used among statisticians and data miners for developing statistical software and data analysis.\nPolls, surveys of data miners, and studies of scholarly literature databases show that R's popularity has increased substantially in recent years.\n5_ Expressions\n6_ Variables\n7_ IBM SPSS\n8_ Rapid Miner\n9_ Vectors\n10_ Matrices\n11_ Arrays\n12_ Factors\n13_ Lists\n14_ Data frames\n15_ Reading CSV data\nCSV is a format of tabular data comonly used in data science. Most of structured data will come in such a format.\nTo open a CSV file in Python, just open the file as usual :\nraw_file = open('file.csv', 'r')\n'r': Reading, no modification on the file is possible\n'w': Writing, every modification will erease the file\n'a': Adding, every modification will be made at the end of the file\nHow to read it ?\nMost of the time, you will parse this file line by line and do whatever you want on this line. If you want to store data to use them later, build lists or dictionnaries.\nTo read such a file row by row, you can use :\nPython library csv\nPython function open\n16_ Reading raw data\n17_ Subsetting data\n18_ Manipulate data frames\n19_ Functions\nA function is helpful to execute redondant actions.\nFirst, define the function:\ndef MyFunction(number):\n\"\"\"This function will multiply a number by 9\"\"\"\nnumber = number * 9\nreturn number\n20_ Factor analysis\n21_ Install PKGS\nPython actually has two mainly used distributions. Python2 and python3.\nInstall pip\nPip is a library manager for Python. Thus, you can easily install most of the packages with a one-line command. To install pip, just go to a terminal and do:\n# __python2__\nsudo apt-get install python-pip\n# __python3__\nsudo apt-get install python3-pip\nYou can then install a library with pip via a terminal doing:\n# __python2__\nsudo pip install [PCKG_NAME]\n# __python3__\nsudo pip3 install [PCKG_NAME]\nYou also can install it directly from the core (see 21_install_pkgs.py)\n4_ Machine learning\n1_ What is ML ?\nDefinition\nMachine Learning is part of the Artificial Intelligences study. It concerns the conception, devloppement and implementation of sophisticated methods, allowing a machine to achieve really hard tasks, nearly impossible to solve with classic algorithms.\nMachine learning mostly consists of three algorithms:\nUtilisation examples\nComputer vision\nSearch engines\nFinancial analysis\nDocuments classification\nMusic generation\nRobotics ...\n2_ Numerical var\nVariables which can take continous integer or real values. They can take infinite values.\nThese types of variables are mostly used for features which involves measurements. For example, hieghts of all students in a class.\n3_ Categorical var\nVariables that take finite discrete values. They take a fixed set of values, in order to classify a data item.\nThey act like assigned labels. For example: Labelling the students of a class according to gender: 'Male' and 'Female'\n4_ Supervised learning\nSupervised learning is the machine learning task of inferring a function from labeled training data.\nThe training data consist of a set of training examples.\nIn supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal).\nA supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples.\nIn other words:\nSupervised Learning learns from a set of labeled examples. From the instances and the labels, supervised learning models try to find the correlation among the features, used to describe an instance, and learn how each feature contributes to the label corresponding to an instance. On receiving an unseen instance, the goal of supervised learning is to label the instance based on its feature correctly.\nAn optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances.\n5_ Unsupervised learning\nUnsupervised machine learning is the machine learning task of inferring a function to describe hidden structure from \"unlabeled\" data (a classification or categorization is not included in the observations).\nSince the examples given to the learner are unlabeled, there is no evaluation of the accuracy of the structure that is output by the relevant algorithm\u2014which is one way of distinguishing unsupervised learning from supervised learning and reinforcement learning.\nUnsupervised learning deals with data instances only. This approach tries to group data and form clusters based on the similarity of features. If two instances have similar features and placed in close proximity in feature space, there are high chances the two instances will belong to the same cluster. On getting an unseen instance, the algorithm will try to find, to which cluster the instance should belong based on its feature.\nResource:\nGuide to unsupervised learning\n6_ Concepts, inputs and attributes\nA machine learning problem takes in the features of a dataset as input.\nFor supervised learning, the model trains on the data and then it is ready to perform. So, for supervised learning, apart from the features we also need to input the corresponding labels of the data points to let the model train on them.\nFor unsupervised learning, the models simply perform by just citing complex relations among data items and grouping them accordingly. So, unsupervised learning do not need a labelled dataset. The input is only the feature section of the dataset.\n7_ Training and test data\nIf we train a supervised machine learning model using a dataset, the model captures the dependencies of that particular data set very deeply. So, the model will always perform well on the data and it won't be proper measure of how well the model performs.\nTo know how well the model performs, we must train and test the model on different datasets. The dataset we train the model on is called Training set, and the dataset we test the model on is called the test set.\nWe normally split the provided dataset to create the training and test set. The ratio of splitting is majorly: 3:7 or 2:8 depending on the data, larger being the trining data.\nsklearn.model_selection.train_test_split is used for splitting the data.\nSyntax:\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nSklearn docs\n8_ Classifiers\nClassification is the most important and most common machine learning problem. Classification problems can be both suprvised and unsupervised problems.\nThe classification problems involve labelling data points to belong to a particular class based on the feature set corresponding to the particluar data point.\nClassification tasks can be performed using both machine learning and deep learning techniques.\nMachine learning classification techniques involve: Logistic Regressions, SVMs, and Classification trees. The models used to perform the classification are called classifiers.\n9_ Prediction\nThe output generated by a machine learning models for a particuolar problem is called its prediction.\nThere are majorly two kinds of predictions corresponding to two types of problen:\nClassification\nRegression\nIn classiication, the prediction is mostly a class or label, to which a data points belong\nIn regression, the prediction is a number, a continous a numeric value, because regression problems deal with predicting the value. For example, predicting the price of a house.\n10_ Lift\n11_ Overfitting\nOften we train our model so much or make our model so complex that our model fits too tghtly with the training data.\nThe training data often contains outliers or represents misleading patterns in the data. Fitting the training data with such irregularities to deeply cause the model to lose its generalization. The model performs very well on the training set but not so good on the test set.\nAs we can see on training further a point the training error decreases and testing error increases.\nA hypothesis h1 is said to overfit iff there exists another hypothesis h where h gives more error than h1 on training data and less error than h1 on the test data\n12_ Bias & variance\nBias is the difference between the average prediction of our model and the correct value which we are trying to predict. Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to high error on training and test data.\nVariance is the variability of model prediction for a given data point or a value which tells us spread of our data. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn\u2019t seen before. As a result, such models perform very well on training data but has high error rates on test data.\nBasically High variance causes overfitting and high bias causes underfitting. We want our model to have low bias and low variance to perform perfectly. We need to avoid a model with higher variance and high bias\nWe can see that for Low bias and Low Variance our model predicts all the data points correctly. Again in the last image having high bias and high variance the model predicts no data point correctly.\nWe can see from the graph that rge Error increases when the complex is either too complex or the model is too simple. The bias increases with simpler model and Variance increases with complex models.\nThis is one of the most important tradeoffs in machine learning\n13_ Tree and classification\nWe have previously talked about classificaion. We have seen the most used methods are Logistic Regression, SVMs and decision trees. Now, if the decision boundary is linear the methods like logistic regression and SVM serves best, but its a complete scenerio when the decision boundary is non linear, this is where decision tree is used.\nThe first image shows linear decision boundary and second image shows non linear decision boundary.\nIh the cases, for non linear boundaries, the decision trees condition based approach work very well for classification problems. The algorithm creates conditions on features to drive and reach a decision, so is independent of functions.\nDecision tree approach for classification\n14_ Classification rate\n15_ Decision tree\nDecision Trees are some of the most used machine learning algorithms. They are used for both classification and Regression. They can be used for both linear and non-linear data, but they are mostly used for non-linear data. Decision Trees as the name suggests works on a set of decisions derived from the data and its behavior. It does not use a linear classifier or regressor, so its performance is independent of the linear nature of the data.\nOne of the other most important reasons to use tree models is that they are very easy to interpret.\nDecision Trees can be used for both classification and regression. The methodologies are a bit different, though principles are the same. The decision trees use the CART algorithm (Classification and Regression Trees)\nResource:\nGuide to Decision Tree\n16_ Boosting\nEnsemble Learning\nIt is the method used to enhance the performance of the Machine learning models by combining several number of models or weak learners. They provide improved efficiency.\nThere are two types of ensemble learning:\n1. Parallel ensemble learning or bagging method\n2. Sequential ensemble learning or boosting method\nIn parallel method or bagging technique, several weak classifiers are created in parallel. The training datasets are created randomly on a bootstrapping basis from the original dataset. The datasets used for the training and creation phases are weak classifiers. Later during predictions, the reults from all the classifiers are bagged together to provide the final results.\nEx: Random Forests\nIn sequential learning or boosting weak learners are created one after another and the data sample set are weighted in such a manner that during creation, the next learner focuses on the samples that were wrongly predicted by the previous classifier. So, at each step, the classifier improves and learns from its previous mistakes or misclassifications.\nThere are mostly three types of boosting algorithm:\n1. Adaboost\n2. Gradient Boosting\n3. XGBoost\nAdaboost algorithm works in the exact way describe. It creates a weak learner, also known as stumps, they are not full grown trees, but contain a single node based on which the classification is done. The misclassifications are observed and they are weighted more than the correctly classified ones while training the next weak learner.\nsklearn.ensemble.AdaBoostClassifier is used for the application of the classifier on real data in python.\nReources:\nUnderstanding\nGradient Boosting algorithm starts with a node giving 0.5 as output for both classification and regression. It serves as the first stump or weak learner. We then observe the Errors in predictions. Now, we create other learners or decision trees to actually predict the errors based on the conditions. The errors are called Residuals. Our final output is:\n0.5 (Provided by the first learner) + The error provided by the second tree or learner.\nNow, if we use this method, it learns the predictions too tightly, and loses generalization. In order to avoid that gradient boosting uses a learning parameter alpha.\nSo, the final results after two learners is obtained as:\n0.5 (Provided by the first learner) + alpha X (The error provided by the second tree or learner.)\nWe can see that using the added portion we take a small leap towards the correct results. We continue adding learners until the point we are very close to the actual value given by the training set.\nOverall the equation becomes:\n0.5 (Provided by the first learner) + alpha X (The error provided by the second tree or learner.)+ alpha X (The error provided by the third tree or learner.)+.............\nsklearn.ensemble.GradientBoostingClassifier used to apply gradient boosting in python\nResource:\nGuide\n17_ Na\u00efves Bayes classifiers\nThe Naive Bayes classifiers are a collection of classification algorithms based on Bayes\u2019 Theorem.\nBayes theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event. It is given by:\nWhere P(A|B) is the probabaility of occurrence of A knowing B already occurred and P(B|A) is the probability of occurrence of B knowing A occurred.\nScikit-learn Guide\nThere are mostly two types of Naive Bayes:\n1. Gaussian Naive Bayes\n2. Multinomial Naive Bayes.\nMultinomial Naive Bayes\nThe method is used mostly for document classification. For example, classifying an article as sports article or say -----> film !!!  magazine. It is also used for differentiating actual mails from spam mails. It uses the frequency of words used in different magazine to make a decision.\nFor example, the word \"Dear\" and \"friends\" are used a lot in actual mails and \"offer\" and \"money\" are used a lot in \"Spam\" mails. It calculates the prorbability of the occurrence of the words in case of actual mails and spam mails using the training examples. So, the probability of occurrence of \"money\" is much higher in case of spam mails and so on.\nNow, we calculate the probability of a mail being a spam mail using the occurrence of words in it.\nGaussian Naive Bayes\nWhen the predictors take up a continuous value and are not discrete, we assume that these values are sampled from a gaussian distribution.\nIt links guassian distribution and Bayes theorem.\nResources:\nGUIDE\n18_ K-Nearest neighbor\nK-nearest neighbour algorithm is the most basic and still essential algorithm. It is a memory based approach and not a model based one.\nKNN is used in both supervised and unsupervised learning. It simply locates the data points across the feature space and used distance as a similarity metrics.\nLesser the distance between two data points, more similar the points are.\nIn K-NN classification algorithm, the point to classify is plotted on the feature space and classified as the class of its nearest K-neighbours. K is the user parameter. It gives the measure of how many points we should consider while deciding the label of the point concerned. If K is more than 1 we consider the label that is in majority.\nIf the dataset is very large, we can use a large k. The large k is less effected by noise and generates smooth boundaries. For small dataset, a small k must be used. A small k helps to notice the variation in boundaries better.\nResource:\nGUIDE\n19_ Logistic regression\nRegression is one of the most important concepts used in machine learning.\nGuide to regression\nLogistic Regression is the most used classification algorithm for linearly seperable datapoints. Logistic Regression is used when the dependent variable is categorical.\nIt uses the linear regression equation:\nY= w1x1+w2x2+w3x3\u2026\u2026..wkxk\nin a modified format:\nY= 1/ 1+e^-(w1x1+w2x2+w3x3\u2026\u2026..wkxk)\nThis modification ensures the value always stays between 0 and 1. Thus, making it feasible to be used for classification.\nThe above equation is called Sigmoid function. The function looks like:\nThe loss fucnction used is called logloss or binary cross-entropy.\nLoss= \u2014Y_actual. log(h(x)) \u2014(1 \u2014 Y_actual.log(1 \u2014 h(x)))\nIf Y_actual=1, the first part gives the error, else the second part.\nLogistic Regression is used for multiclass classification also. It uses softmax regresssion or One-vs-all logistic regression.\nGuide to logistic Regression\nsklearn.linear_model.LogisticRegression is used to apply logistic Regression in python.\n20_ Ranking\n21_ Linear regression\nRegression tasks deal with predicting the value of a dependent variable from a set of independent variables i.e, the provided features. Say, we want to predict the price of a car. So, it becomes a dependent variable say Y, and the features like engine capacity, top speed, class, and company become the independent variables, which helps to frame the equation to obtain the price.\nNow, if there is one feature say x. If the dependent variable y is linearly dependent on x, then it can be given by y=mx+c, where the m is the coefficient of the feature in the equation, c is the intercept or bias. Both M and C are the model parameters.\nWe use a loss function or cost function called Mean Square error of (MSE). It is given by the square of the difference between the actual and the predicted value of the dependent variable.\nMSE=1/2m * (Y_actual \u2014 Y_pred)\u00b2\nIf we observe the function we will see its a parabola, i.e, the function is convex in nature. This convex function is the principle used in Gradient Descent to obtain the value of the model parameters\nThe image shows the loss function.\nTo get the correct estimate of the model parameters we use the method of Gradient Descent\nGuide to Gradient Descent\nGuide to linear Regression\nsklearn.linear_model.LinearRegression is used to apply linear regression in python\n22_ Perceptron\nThe perceptron has been the first model described in the 50ies.\nThis is a binary classifier, ie it can't separate more than 2 groups, and thoses groups have to be linearly separable.\nThe perceptron works like a biological neuron. It calculate an activation value, and if this value if positive, it returns 1, 0 otherwise.\n23_ Hierarchical clustering\nThe hierarchical algorithms are so-called because they create tree-like structures to create clusters. These algorithms also use a distance-based approach for cluster creation.\nThe most popular algorithms are:\nAgglomerative Hierarchical clustering\nDivisive Hierarchical clustering\nAgglomerative Hierarchical clustering: In this type of hierarchical clustering, each point initially starts as a cluster, and slowly the nearest or similar most clusters merge to create one cluster.\nDivisive Hierarchical Clustering: The type of hierarchical clustering is just the opposite of Agglomerative clustering. In this type, all the points start as one large cluster and slowly the clusters get divided into smaller clusters based on how large the distance or less similarity is between the two clusters. We keep on dividing the clusters until all the points become individual clusters.\nFor agglomerative clustering, we keep on merging the clusters which are nearest or have a high similarity score to one cluster. So, if we define a cut-off or threshold score for the merging we will get multiple clusters instead of a single one. For instance, if we say the threshold similarity metrics score is 0.5, it means the algorithm will stop merging the clusters if no two clusters are found with a similarity score less than 0.5, and the number of clusters present at that step will give the final number of clusters that need to be created to the clusters.\nSimilarly, for divisive clustering, we divide the clusters based on the least similarity scores. So, if we define a score of 0.5, it will stop dividing or splitting if the similarity score between two clusters is less than or equal to 0.5. We will be left with a number of clusters and it won\u2019t reduce to every point of the distribution.\nThe process is as shown below:\nOne of the most used methods for the measuring distance and applying cutoff is the dendrogram method.\nThe dendogram for above clustering is:\nGuide\n24_ K-means clustering\nThe algorithm initially creates K clusters randomly using N data points and finds the mean of all the point values in a cluster for each cluster. So, for each cluster we find a central point or centroid calculating the mean of the values of the cluster. Then the algorithm calculates the sum of squared error (SSE) for each cluster. SSE is used to measure the quality of clusters. If a cluster has large distances between the points and the center, then the SSE will be high and if we check the interpretation it allows only points in the close vicinity to create clusters.\nThe algorithm works on the principle that the points lying close to a center of a cluster should be in that cluster. So, if a point x is closer to the center of cluster A than cluster B, then x will belong to cluster A. Thus a point enters a cluster and as even a single point moves from one cluster to another, the centroid changes and so does the SSE. We keep doing this until the SSE decreases and the centroid does not change anymore. After a certain number of shifts, the optimal clusters are found and the shifting stops as the centroids don\u2019t change any more.\nThe initial number of clusters \u2018K\u2019 is a user parameter.\nThe image shows the method\nWe have seen that for this type of clustering technique we need a user-defined parameter \u2018K\u2019 which defines the number of clusters that need to be created. Now, this is a very important parameter. To, find this parameter a number of methods are used. The most important and used method is the elbow method. For smaller datasets, k=(N/2)^(1/2) or the square root of half of the number of points in the distribution.\nGuide\n25_ Neural networks\nNeural Networks are a set of interconnected layers of artificial neurons or nodes. They are frameworks that are modeled keeping in mind, the structure and working of the human brain. They are meant for predictive modeling and applications where they can be trained via a dataset. They are based on self-learning algorithms and predict based on conclusions and complex relations derived from their training sets of information.\nA typical Neural Network has a number of layers. The First Layer is called the Input Layer and The Last layer is called the Output Layer. The layers between the Input and Output layers are called Hidden Layers. It basically functions like a Black Box for prediction and classification. All the layers are interconnected and consist of numerous artificial neurons called Nodes.\nGuide to nueral Networks\nNeural networks are too complex to work on Gradient Descent algorithms, so it works on the principles of Backproapagations and Optimizers.\nGuide to Backpropagation\nGuide to optimizers\n26_ Sentiment analysis\nText Classification and sentiment analysis is a very common machine learning problem and is used in a lot of activities like product predictions, movie recommendations, and several others.\nText classification problems like sentimental analysis can be achieved in a number of ways using a number of algorithms. These are majorly divided into two main categories:\nA bag of Word model: In this case, all the sentences in our dataset are tokenized to form a bag of words that denotes our vocabulary. Now each individual sentence or sample in our dataset is represented by that bag of words vector. This vector is called the feature vector. For example, \u2018It is a sunny day\u2019, and \u2018The Sun rises in east\u2019 are two sentences. The bag of words would be all the words in both the sentences uniquely.\nThe second method is based on a time series approach: Here each word is represented by an Individual vector. So, a sentence is represented as a vector of vectors.\nGuide to sentimental analysis\n27_ Collaborative filtering\nWe all have used services like Netflix, Amazon, and Youtube. These services use very sophisticated systems to recommend the best items to their users to make their experiences great.\nRecommenders mostly have 3 components mainly, out of which, one of the main component is Candidate generation. This method is responsible for generating smaller subsets of candidates to recommend to a user, given a huge pool of thousands of items.\nTypes of Candidate Generation Systems:\nContent-based filtering System\nCollaborative filtering System\nContent-based filtering system: Content-Based recommender system tries to guess the features or behavior of a user given the item\u2019s features, he/she reacts positively to.\nCollaborative filtering System: Collaborative does not need the features of the items to be given. Every user and item is described by a feature vector or embedding.\nIt creates embedding for both users and items on its own. It embeds both users and items in the same embedding space.\nIt considers other users\u2019 reactions while recommending a particular user. It notes which items a particular user likes and also the items that the users with behavior and likings like him/her likes, to recommend items to that user.\nIt collects user feedbacks on different items and uses them for recommendations.\nGuide to collaborative filtering\n28_ Tagging\n29_ Support Vector Machine\nSupport vector machines are used for both Classification and Regressions.\nSVM uses a margin around its classifier or regressor. The margin provides an extra robustness and accuracy to the model and its performance.\nThe above image describes a SVM classifier. The Red line is the actual classifier and the dotted lines show the boundary. The points that lie on the boundary actually decide the Margins. They support the classifier margins, so they are called Support Vectors.\nThe distance between the classifier and the nearest points is called Marginal Distance.\nThere can be several classifiers possible but we choose the one with the maximum marginal distance. So, the marginal distance and the support vectors help to choose the best classifier.\nOfficial Documentation from Sklearn\nGuide to SVM\n30_Reinforcement Learning\n\u201cReinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward.\u201d\nTo play a game, we need to make multiple choices and predictions during the course of the game to achieve success, so they can be called a multiple decision processes. This is where we need a type of algorithm called reinforcement learning algorithms. The class of algorithm is based on decision-making chains which let such algorithms to support multiple decision processes.\nThe reinforcement algorithm can be used to reach a goal state from a starting state making decisions accordingly.\nThe reinforcement learning involves an agent which learns on its own. If it makes a correct or good move that takes it towards the goal, it is positively rewarded, else not. This way the agent learns.\nThe above image shows reinforcement learning setup.\nWIKI\n5_ Text Mining\n1_ Corpus\n2_ Named Entity Recognition\n3_ Text Analysis\n4_ UIMA\n5_ Term Document matrix\n6_ Term frequency and Weight\n7_ Support Vector Machines (SVM)\n8_ Association rules\n9_ Market based analysis\n10_ Feature extraction\n11_ Using mahout\n12_ Using Weka\n13_ Using NLTK\n14_ Classify text\n15_ Vocabulary mapping\n6_ Data Visualization\nOpen .R scripts in Rstudio for line-by-line execution.\nSee 10_ Toolbox/3_ R, Rstudio, Rattle for installation.\n1_ Data exploration in R\nIn mathematics, the graph of a function f is the collection of all ordered pairs (x, f(x)). If the function input x is a scalar, the graph is a two-dimensional graph, and for a continuous function is a curve. If the function input x is an ordered pair (x1, x2) of real numbers, the graph is the collection of all ordered triples (x1, x2, f(x1, x2)), and for a continuous function is a surface.\n2_ Uni, bi and multivariate viz\nUnivariate\nThe term is commonly used in statistics to distinguish a distribution of one variable from a distribution of several variables, although it can be applied in other ways as well. For example, univariate data are composed of a single scalar component. In time series analysis, the term is applied with a whole time series as the object referred to: thus a univariate time series refers to the set of values over time of a single quantity.\nBivariate\nBivariate analysis is one of the simplest forms of quantitative (statistical) analysis.[1] It involves the analysis of two variables (often denoted as X, Y), for the purpose of determining the empirical relationship between them.\nMultivariate\nMultivariate analysis (MVA) is based on the statistical principle of multivariate statistics, which involves observation and analysis of more than one statistical outcome variable at a time. In design and analysis, the technique is used to perform trade studies across multiple dimensions while taking into account the effects of all variables on the responses of interest.\n3_ ggplot2\nAbout\nggplot2 is a plotting system for R, based on the grammar of graphics, which tries to take the good parts of base and lattice graphics and none of the bad parts. It takes care of many of the fiddly details that make plotting a hassle (like drawing legends) as well as providing a powerful model of graphics that makes it easy to produce complex multi-layered graphics.\nhttp://ggplot2.org/\nDocumentation\nExamples\nhttp://r4stats.com/examples/graphics-ggplot2/\n4_ Histogram and pie (Uni)\nAbout\nHistograms and pie are 2 types of graphes used to visualize frequencies.\nHistogram is showing the distribution of these frequencies over classes, and pie the relative proportion of this frequencies in a 100% circle.\n5_ Tree & tree map\nAbout\nTreemaps display hierarchical (tree-structured) data as a set of nested rectangles. Each branch of the tree is given a rectangle, which is then tiled with smaller rectangles representing sub-branches. A leaf node\u2019s rectangle has an area proportional to a specified dimension of the data. Often the leaf nodes are colored to show a separate dimension of the data.\nWhen to use it ?\nLess than 10 branches.\nPositive values.\nSpace for visualisation is limited.\nExample\nThis treemap describes volume for each product universe with corresponding surface. Liquid products are more sold than others. If you want to explore more, we can go into products \u201cliquid\u201d and find which shelves are prefered by clients.\nMore information\nMatplotlib Series 5: Treemap\n6_ Scatter plot\nAbout\nA scatter plot (also called a scatter graph, scatter chart, scattergram, or scatter diagram) is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data.\nWhen to use it ?\nScatter plots are used when you want to show the relationship between two variables. Scatter plots are sometimes called correlation plots because they show how two variables are correlated.\nExample\nThis plot describes the positive relation between store\u2019s surface and its turnover(k euros), which is reasonable: for stores, the larger it is, more clients it can accept, more turnover it will generate.\nMore information\nMatplotlib Series 4: Scatter plot\n7_ Line chart\nAbout\nA line chart or line graph is a type of chart which displays information as a series of data points called \u2018markers\u2019 connected by straight line segments. A line chart is often used to visualize a trend in data over intervals of time \u2013 a time series \u2013 thus the line is often drawn chronologically.\nWhen to use it ?\nTrack changes over time.\nX-axis displays continuous variables.\nY-axis displays measurement.\nExample\nSuppose that the plot above describes the turnover(k euros) of ice-cream\u2019s sales during one year. According to the plot, we can clearly find that the sales reach a peak in summer, then fall from autumn to winter, which is logical.\nMore information\nMatplotlib Series 2: Line chart\n8_ Spatial charts\n9_ Survey plot\n10_ Timeline\n11_ Decision tree\n12_ D3.js\nAbout\nThis is a JavaScript library, allowing you to create a huge number of different figure easily.\nhttps://d3js.org/\nD3.js is a JavaScript library for manipulating documents based on data.\nD3 helps you bring data to life using HTML, SVG, and CSS.\nD3\u2019s emphasis on web standards gives you the full capabilities of modern browsers without tying yourself to a proprietary framework, combining powerful visualization components and a data-driven approach to DOM manipulation.\nExamples\nThere is many examples of chars using D3.js on D3's Github.\n13_ InfoVis\n14_ IBM ManyEyes\n15_ Tableau\n16_ Venn diagram\nAbout\nA venn diagram (also called primary diagram, set diagram or logic diagram) is a diagram that shows all possible logical relations between a finite collection of different sets.\nWhen to use it ?\nShow logical relations between different groups (intersection, difference, union).\nExample\nThis kind of venn diagram can usually be used in retail trading. Assuming that we need to study the popularity of cheese and red wine, and 2500 clients answered our questionnaire. According to the diagram above, we find that among 2500 clients, 900 clients(36%) prefer cheese, 1200 clients(48%) prefer red wine, and 400 clients(16%) favor both product.\nMore information\nMatplotlib Series 6: Venn diagram\n17_ Area chart\nAbout\nAn area chart or area graph displays graphically quantitative data. It is based on the line chart. The area between axis and line are commonly emphasized with colors, textures and hatchings.\nWhen to use it ?\nShow or compare a quantitative progression over time.\nExample\nThis stacked area chart displays the amounts\u2019 changes in each account, their contribution to total amount (in term of value) as well.\nMore information\nMatplotlib Series 7: Area chart\n18_ Radar chart\nAbout\nThe radar chart is a chart and/or plot that consists of a sequence of equi-angular spokes, called radii, with each spoke representing one of the variables. The data length of a spoke is proportional to the magnitude of the variable for the data point relative to the maximum magnitude of the variable across all data points. A line is drawn connecting the data values for each spoke. This gives the plot a star-like appearance and the origin of one of the popular names for this plot.\nWhen to use it ?\nComparing two or more items or groups on various features or characteristics.\nExamining the relative values for a single data point.\nDisplaying less than ten factors on one radar chart.\nExample\nThis radar chart displays the preference of 2 clients among 4. Client c1 favors chicken and bread, and doesn\u2019t like cheese that much. Nevertheless, client c2 prefers cheese to other 4 products and doesn\u2019t like beer. We can have an interview with these 2 clients, in order to find the weakness of products which are out of preference.\nMore information\nMatplotlib Series 8: Radar chart\n19_ Word cloud\nAbout\nA word cloud (tag cloud, or weighted list in visual design) is a novelty visual representation of text data. Tags are usually single words, and the importance of each tag is shown with font size or color. This format is useful for quickly perceiving the most prominent terms and for locating a term alphabetically to determine its relative prominence.\nWhen to use it ?\nDepicting keyword metadata (tags) on websites.\nDelighting and provide emotional connection.\nExample\nAccording to this word cloud, we can globally know that data science employs techniques and theories drawn from many fields within the context of mathematics, statistics, information science, and computer science. It can be used for business analysis, and called \u201cThe Sexiest Job of the 21st Century\u201d.\nMore information\nMatplotlib Series 9: Word cloud\n7_ Big Data\n1_ Map Reduce fundamentals\n2_ Hadoop Ecosystem\n3_ HDFS\n4_ Data replications Principles\n5_ Setup Hadoop\n6_ Name & data nodes\n7_ Job & task tracker\n8_ M/R/SAS programming\n9_ Sqop: Loading data in HDFS\n10_ Flume, Scribe\n11_ SQL with Pig\n12_ DWH with Hive\n13_ Scribe, Chukwa for Weblog\n14_ Using Mahout\n15_ Zookeeper Avro\n16_ Lambda Architecture\n17_ Storm: Hadoop Realtime\n18_ Rhadoop, RHIPE\n19_ RMR\n20_ NoSQL Databases (MongoDB, Neo4j)\n21_ Distributed Databases and Systems (Cassandra)\n8_ Data Ingestion\n1_ Summary of data formats\n2_ Data discovery\n3_ Data sources & Acquisition\n4_ Data integration\n5_ Data fusion\n6_ Transformation & enrichment\n7_ Data survey\n8_ Google OpenRefine\n9_ How much data ?\n10_ Using ETL\n9_ Data Munging\n1_ Dim. and num. reduction\n2_ Normalization\n3_ Data scrubbing\n4_ Handling missing Values\n5_ Unbiased estimators\n6_ Binning Sparse Values\n7_ Feature extraction\n8_ Denoising\n9_ Sampling\n10_ Stratified sampling\n11_ PCA\n10_ Toolbox\n1_ MS Excel with Analysis toolpack\n2_ Java, Python\n3_ R, Rstudio, Rattle\n4_ Weka, Knime, RapidMiner\n5_ Hadoop dist of choice\n6_ Spark, Storm\n7_ Flume, Scibe, Chukwa\n8_ Nutch, Talend, Scraperwiki\n9_ Webscraper, Flume, Sqoop\n10_ tm, RWeka, NLTK\n11_ RHIPE\n12_ D3.js, ggplot2, Shiny\n13_ IBM Languageware\n14_ Cassandra, MongoDB\n13_ Microsoft Azure, AWS, Google Cloud\n14_ Microsoft Cognitive API\n15_ Tensorflow\nhttps://www.tensorflow.org/\nTensorFlow is an open source software library for numerical computation using data flow graphs.\nNodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them.\nThe flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API.\nTensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well.\nOTHER FREE COURSES\nArtificial Intelligence\nCS 188 - Introduction to Artificial Intelligence, UC Berkeley - Spring 2015\n6.034 Artificial Intelligence, MIT OCW\nCS221: Artificial Intelligence: Principles and Techniques - Autumn 2019 - Stanford University\n15-780 - Graduate Artificial Intelligence, Spring 14, CMU\nCSE 592 Applications of Artificial Intelligence, Winter 2003 - University of Washington\nCS322 - Introduction to Artificial Intelligence, Winter 2012-13 - UBC (YouTube)\nCS 4804: Introduction to Artificial Intelligence, Fall 2016\nCS 5804: Introduction to Artificial Intelligence, Spring 2015\nArtificial Intelligence - IIT Kharagpur\nArtificial Intelligence - IIT Madras\nArtificial Intelligence(Prof.P.Dasgupta) - IIT Kharagpur\nMOOC - Intro to Artificial Intelligence - Udacity\nMOOC - Artificial Intelligence for Robotics - Udacity\nGraduate Course in Artificial Intelligence, Autumn 2012 - University of Washington\nAgent-Based Systems 2015/16- University of Edinburgh\nInformatics 2D - Reasoning and Agents 2014/15- University of Edinburgh\nArtificial Intelligence - Hochschule Ravensburg-Weingarten\nDeductive Databases and Knowledge-Based Systems - Technische Universit\u00e4t Braunschweig, Germany\nArtificial Intelligence: Knowledge Representation and Reasoning - IIT Madras\nSemantic Web Technologies by Dr. Harald Sack - HPI\nKnowledge Engineering with Semantic Web Technologies by Dr. Harald Sack - HPI\nMachine Learning\nIntroduction to Machine Learning\nMOOC Machine Learning Andrew Ng - Coursera/Stanford (Notes)\nIntroduction to Machine Learning for Coders\nMOOC - Statistical Learning, Stanford University\nFoundations of Machine Learning Boot Camp, Berkeley Simons Institute\nCS155 - Machine Learning & Data Mining, 2017 - Caltech (Notes) (2016)\nCS 156 - Learning from Data, Caltech\n10-601 - Introduction to Machine Learning (MS) - Tom Mitchell - 2015, CMU (YouTube)\n10-601 Machine Learning | CMU | Fall 2017\n10-701 - Introduction to Machine Learning (PhD) - Tom Mitchell, Spring 2011, CMU (Fall 2014) (Spring 2015 by Alex Smola)\n10 - 301/601 - Introduction to Machine Learning - Spring 2020 - CMU\nCMS 165 Foundations of Machine Learning and Statistical Inference - 2020 - Caltech\nMicrosoft Research - Machine Learning Course\nCS 446 - Machine Learning, Spring 2019, UIUC( Fall 2016 Lectures)\nundergraduate machine learning at UBC 2012, Nando de Freitas\nCS 229 - Machine Learning - Stanford University (Autumn 2018)\nCS 189/289A Introduction to Machine Learning, Prof Jonathan Shewchuk - UCBerkeley\nCPSC 340: Machine Learning and Data Mining (2018) - UBC\nCS4780/5780 Machine Learning, Fall 2013 - Cornell University\nCS4780/5780 Machine Learning, Fall 2018 - Cornell University (Youtube)\nCSE474/574 Introduction to Machine Learning - SUNY University at Buffalo\nCS 5350/6350 - Machine Learning, Fall 2016, University of Utah\nECE 5984 Introduction to Machine Learning, Spring 2015 - Virginia Tech\nCSx824/ECEx242 Machine Learning, Bert Huang, Fall 2015 - Virginia Tech\nSTA 4273H - Large Scale Machine Learning, Winter 2015 - University of Toronto\nCS 485/685 Machine Learning, Shai Ben-David, University of Waterloo\nSTAT 441/841 Classification Winter 2017 , Waterloo\n10-605 - Machine Learning with Large Datasets, Fall 2016 - CMU\nInformation Theory, Pattern Recognition, and Neural Networks - University of Cambridge\nPython and machine learning - Stanford Crowd Course Initiative\nMOOC - Machine Learning Part 1a - Udacity/Georgia Tech (Part 1b Part 2 Part 3)\nMachine Learning and Pattern Recognition 2015/16- University of Edinburgh\nIntroductory Applied Machine Learning 2015/16- University of Edinburgh\nPattern Recognition Class (2012)- Universit\u00e4t Heidelberg\nIntroduction to Machine Learning and Pattern Recognition - CBCSL OSU\nIntroduction to Machine Learning - IIT Kharagpur\nIntroduction to Machine Learning - IIT Madras\nPattern Recognition - IISC Bangalore\nPattern Recognition and Application - IIT Kharagpur\nPattern Recognition - IIT Madras\nMachine Learning Summer School 2013 - Max Planck Institute for Intelligent Systems T\u00fcbingen\nMachine Learning - Professor Kogan (Spring 2016) - Rutgers\nCS273a: Introduction to Machine Learning (YouTube)\nMachine Learning Crash Course 2015\nCOM4509/COM6509 Machine Learning and Adaptive Intelligence 2015-16\n10715 Advanced Introduction to Machine Learning\nIntroduction to Machine Learning - Spring 2018 - ETH Zurich\nMachine Learning - Pedro Domingos- University of Washington\nAdvanced Machine Learning - 2019 - ETH Z\u00fcrich\nMachine Learning (COMP09012)\nProbabilistic Machine Learning 2020 - University of T\u00fcbingen\nStatistical Machine Learning 2020 - Ulrike von Luxburg - University of T\u00fcbingen\nCOMS W4995 - Applied Machine Learning - Spring 2020 - Columbia University\nData Mining\nCSEP 546, Data Mining - Pedro Domingos, Sp 2016 - University of Washington (YouTube)\nCS 5140/6140 - Data Mining, Spring 2016, University of Utah (Youtube)\nCS 5955/6955 - Data Mining, University of Utah (YouTube)\nStatistics 202 - Statistical Aspects of Data Mining, Summer 2007 - Google (YouTube)\nMOOC - Text Mining and Analytics by ChengXiang Zhai\nInformation Retrieval SS 2014, iTunes - HPI\nMOOC - Data Mining with Weka\nCS 290 DataMining Lectures\nCS246 - Mining Massive Data Sets, Winter 2016, Stanford University (YouTube)\nData Mining: Learning From Large Datasets - Fall 2017 - ETH Zurich\nInformation Retrieval - Spring 2018 - ETH Zurich\nCAP6673 - Data Mining and Machine Learning - FAU(Video lectures)\nData Warehousing and Data Mining Techniques - Technische Universit\u00e4t Braunschweig, Germany\nData Science\nData 8: The Foundations of Data Science - UC Berkeley (Summer 17)\nCSE519 - Data Science Fall 2016 - Skiena, SBU\nCS 109 Data Science, Harvard University (YouTube)\n6.0002 Introduction to Computational Thinking and Data Science - MIT OCW\nData 100 - Summer 19- UC Berkeley\nDistributed Data Analytics (WT 2017/18) - HPI University of Potsdam\nStatistics 133 - Concepts in Computing with Data, Fall 2013 - UC Berkeley\nData Profiling and Data Cleansing (WS 2014/15) - HPI University of Potsdam\nAM 207 - Stochastic Methods for Data Analysis, Inference and Optimization, Harvard University\nCS 229r - Algorithms for Big Data, Harvard University (Youtube)\nAlgorithms for Big Data - IIT Madras\nProbabilistic Graphical Modeling\nMOOC - Probabilistic Graphical Models - Coursera\nCS 6190 - Probabilistic Modeling, Spring 2016, University of Utah\n10-708 - Probabilistic Graphical Models, Carnegie Mellon University\nProbabilistic Graphical Models, Daphne Koller, Stanford University\nProbabilistic Models - UNIVERSITY OF HELSINKI\nProbabilistic Modelling and Reasoning 2015/16- University of Edinburgh\nProbabilistic Graphical Models, Spring 2018 - Notre Dame\nDeep Learning\n6.S191: Introduction to Deep Learning - MIT\nDeep Learning CMU\nPart 1: Practical Deep Learning for Coders, v3 - fast.ai\nPart 2: Deep Learning from the Foundations - fast.ai\nDeep learning at Oxford 2015 - Nando de Freitas\n6.S094: Deep Learning for Self-Driving Cars - MIT\nCS294-129 Designing, Visualizing and Understanding Deep Neural Networks (YouTube)\nCS230: Deep Learning - Autumn 2018 - Stanford University\nSTAT-157 Deep Learning 2019 - UC Berkeley\nFull Stack DL Bootcamp 2019 - UC Berkeley\nDeep Learning, Stanford University\nMOOC - Neural Networks for Machine Learning, Geoffrey Hinton 2016 - Coursera\nDeep Unsupervised Learning -- Berkeley Spring 2020\nStat 946 Deep Learning - University of Waterloo\nNeural networks class - Universit\u00e9 de Sherbrooke (YouTube)\nCS294-158 Deep Unsupervised Learning SP19\nDLCV - Deep Learning for Computer Vision - UPC Barcelona\nDLAI - Deep Learning for Artificial Intelligence @ UPC Barcelona\nNeural Networks and Applications - IIT Kharagpur\nUVA DEEP LEARNING COURSE\nNvidia Machine Learning Class\nDeep Learning - Winter 2020-21 - T\u00fcbingen Machine Learning\nReinforcement Learning\nCS234: Reinforcement Learning - Winter 2019 - Stanford University\nIntroduction to reinforcement learning - UCL\nAdvanced Deep Learning & Reinforcement Learning - UCL\nReinforcement Learning - IIT Madras\nCS885 Reinforcement Learning - Spring 2018 - University of Waterloo\nCS 285 - Deep Reinforcement Learning- UC Berkeley\nCS 294 112 - Reinforcement Learning\nNUS CS 6101 - Deep Reinforcement Learning\nECE 8851: Reinforcement Learning\nCS294-112, Deep Reinforcement Learning Sp17 (YouTube)\nUCL Course 2015 on Reinforcement Learning by David Silver from DeepMind (YouTube)\nDeep RL Bootcamp - Berkeley Aug 2017\nReinforcement Learning - IIT Madras\nAdvanced Machine Learning\nMachine Learning 2013 - Nando de Freitas, UBC\nMachine Learning, 2014-2015, University of Oxford\n10-702/36-702 - Statistical Machine Learning - Larry Wasserman, Spring 2016, CMU (Spring 2015)\n10-715 Advanced Introduction to Machine Learning - CMU (YouTube)\nCS 281B - Scalable Machine Learning, Alex Smola, UC Berkeley\n18.409 Algorithmic Aspects of Machine Learning Spring 2015 - MIT\nCS 330 - Deep Multi-Task and Meta Learning - Fall 2019 - Stanford University (Youtube)\nML based Natural Language Processing and Computer Vision\nCS 224d - Deep Learning for Natural Language Processing, Stanford University (Lectures - Youtube)\nCS 224N - Natural Language Processing, Stanford University (Lecture videos)\nCS 124 - From Languages to Information - Stanford University\nMOOC - Natural Language Processing, Dan Jurafsky & Chris Manning - Coursera\nfast.ai Code-First Intro to Natural Language Processing (Github)\nMOOC - Natural Language Processing - Coursera, University of Michigan\nCS 231n - Convolutional Neural Networks for Visual Recognition, Stanford University\nCS224U: Natural Language Understanding - Spring 2019 - Stanford University\nDeep Learning for Natural Language Processing, 2017 - Oxford University\nMachine Learning for Robotics and Computer Vision, WS 2013/2014 - TU M\u00fcnchen (YouTube)\nInformatics 1 - Cognitive Science 2015/16- University of Edinburgh\nInformatics 2A - Processing Formal and Natural Languages 2016-17 - University of Edinburgh\nComputational Cognitive Science 2015/16- University of Edinburgh\nAccelerated Natural Language Processing 2015/16- University of Edinburgh\nNatural Language Processing - IIT Bombay\nNOC:Deep Learning For Visual Computing - IIT Kharagpur\nCS 11-747 - Neural Nets for NLP - 2019 - CMU\nNatural Language Processing - Michael Collins - Columbia University\nDeep Learning for Computer Vision - University of Michigan\nCMU CS11-737 - Multilingual Natural Language Processing\nTime Series Analysis\n02417 Time Series Analysis\nApplied Time Series Analysis\nMisc Machine Learning Topics\nEE364a: Convex Optimization I - Stanford University\nCS 6955 - Clustering, Spring 2015, University of Utah\nInfo 290 - Analyzing Big Data with Twitter, UC Berkeley school of information (YouTube)\n10-725 Convex Optimization, Spring 2015 - CMU\n10-725 Convex Optimization: Fall 2016 - CMU\nCAM 383M - Statistical and Discrete Methods for Scientific Computing, University of Texas\n9.520 - Statistical Learning Theory and Applications, Fall 2015 - MIT\nReinforcement Learning - UCL\nRegularization Methods for Machine Learning 2016 (YouTube)\nStatistical Inference in Big Data - University of Toronto\n10-725 Optimization Fall 2012 - CMU\n10-801 Advanced Optimization and Randomized Methods - CMU (YouTube)\nReinforcement Learning 2015/16- University of Edinburgh\nReinforcement Learning - IIT Madras\nStatistical Rethinking Winter 2015 - Richard McElreath\nMusic Information Retrieval - University of Victoria, 2014\nPURDUE Machine Learning Summer School 2011\nFoundations of Machine Learning - Blmmoberg Edu\nIntroduction to reinforcement learning - UCL\nAdvanced Deep Learning & Reinforcement Learning - UCL\nWeb Information Retrieval (Proff. L. Becchetti - A. Vitaletti)\nBig Data Systems (WT 2019/20) - Prof. Dr. Tilmann Rabl - HPI\nDistributed Data Analytics (WT 2017/18) - Dr. Thorsten Papenbrock - HPI\nProbability & Statistics\n6.041 Probabilistic Systems Analysis and Applied Probability - MIT OCW\nStatistics 110 - Probability - Harvard University\nSTAT 2.1x: Descriptive Statistics | UC Berkeley\nSTAT 2.2x: Probability | UC Berkeley\nMOOC - Statistics: Making Sense of Data, Coursera\nMOOC - Statistics One - Coursera\nProbability and Random Processes - IIT Kharagpur\nMOOC - Statistical Inference - Coursera\n131B - Introduction to Probability and Statistics, UCI\nSTATS 250 - Introduction to Statistics and Data Analysis, UMichigan\nSets, Counting and Probability - Harvard\nOpinionated Lessons in Statistics (Youtube)\nStatistics - Brandon Foltz\nStatistical Rethinking: A Bayesian Course Using R and Stan (Lectures - Aalto University) (Book)\n02402 Introduction to Statistics E12 - Technical University of Denmark (F17)\nLinear Algebra\n18.06 - Linear Algebra, Prof. Gilbert Strang, MIT OCW\n18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning - MIT OCW\nLinear Algebra (Princeton University)\nMOOC: Coding the Matrix: Linear Algebra through Computer Science Applications - Coursera\nCS 053 - Coding the Matrix - Brown University (Fall 14 videos)\nLinear Algebra Review - CMU\nA first course in Linear Algebra - N J Wildberger - UNSW\nINTRODUCTION TO MATRIX ALGEBRA\nComputational Linear Algebra - fast.ai (Github)\n10-600 Math Background for ML - CMU\nMIT 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning\n36-705 - Intermediate Statistics - Larry Wasserman, CMU (YouTube)\nCombinatorics - IISC Bangalore\nAdvanced Engineering Mathematics - Notre Dame\nStatistical Computing for Scientists and Engineers - Notre Dame\nStatistical Computing, Fall 2017 - Notre Dame\nMathematics for Machine Learning, Lectures by Ulrike von Luxburg - T\u00fcbingen Machine Learning\nRobotics\nCS 223A - Introduction to Robotics, Stanford University\n6.832 Underactuated Robotics - MIT OCW\nCS287 Advanced Robotics at UC Berkeley Fall 2019 -- Instructor: Pieter Abbeel\nCS 287 - Advanced Robotics, Fall 2011, UC Berkeley (Videos)\nCS235 - Applied Robot Design for Non-Robot-Designers - Stanford University\nLecture: Visual Navigation for Flying Robots (YouTube)\nCS 205A: Mathematical Methods for Robotics, Vision, and Graphics (Fall 2013)\nRobotics 1, Prof. De Luca, Universit\u00e0 di Roma (YouTube)\nRobotics 2, Prof. De Luca, Universit\u00e0 di Roma (YouTube)\nRobot Mechanics and Control, SNU\nIntroduction to Robotics Course - UNCC\nSLAM Lectures\nIntroduction to Vision and Robotics 2015/16- University of Edinburgh\nME 597 \u2013 Autonomous Mobile Robotics \u2013 Fall 2014\nME 780 \u2013 Perception For Autonomous Driving \u2013 Spring 2017\nME780 \u2013 Nonlinear State Estimation for Robotics and Computer Vision \u2013 Spring 2017\nMETR 4202/7202 -- Robotics & Automation - University of Queensland\nRobotics - IIT Bombay\nIntroduction to Machine Vision\n6.834J Cognitive Robotics - MIT OCW\nHello (Real) World with ROS \u2013 Robot Operating System - TU Delft\nProgramming for Robotics (ROS) - ETH Zurich\nMechatronic System Design - TU Delft\nCS 206 Evolutionary Robotics Course Spring 2020\nFoundations of Robotics - UTEC 2018-I\nRobotics - Youtube\nRobotics and Control: Theory and Practice IIT Roorkee\nMechatronics\nME142 - Mechatronics Spring 2020 - UC Merced\nMobile Sensing and Robotics - Bonn University\nMSR2 - Sensors and State Estimation Course (2020) - Bonn University\nSLAM Course (2013) - Bonn University\nENGR486 Robot Modeling and Control (2014W)\nRobotics by Prof. D K Pratihar - IIT Kharagpur\nIntroduction to Mobile Robotics - SS 2019 - Universit\u00e4t Freiburg\nRobot Mapping - WS 2018/19 - Universit\u00e4t Freiburg\nMechanism and Robot Kinematics - IIT Kharagpur\nSelf-Driving Cars - Cyrill Stachniss - Winter 2020/21 - University of Bonn)\nMobile Sensing and Robotics 1 \u2013 Part Stachniss (Jointly taught with PhoRS) - University of Bonn\nMobile Sensing and Robotics 2 \u2013 Stachniss & Klingbeil/Holst - University of Bonn\n500 + \ud835\uddd4\ud835\uddff\ud835\ude01\ud835\uddf6\ud835\uddf3\ud835\uddf6\ud835\uddf0\ud835\uddf6\ud835\uddee\ud835\uddf9 \ud835\udddc\ud835\uddfb\ud835\ude01\ud835\uddf2\ud835\uddf9\ud835\uddf9\ud835\uddf6\ud835\uddf4\ud835\uddf2\ud835\uddfb\ud835\uddf0\ud835\uddf2 \ud835\udde3\ud835\uddff\ud835\uddfc\ud835\uddf7\ud835\uddf2\ud835\uddf0\ud835\ude01 \ud835\udddf\ud835\uddf6\ud835\ude00\ud835\ude01 \ud835\ude04\ud835\uddf6\ud835\ude01\ud835\uddf5 \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2\n500 AI Machine learning Deep learning Computer vision NLP Projects with code\nThis list is continuously updated. - You can take pull request and contribute.\nSr No Name Link\n1 180 Machine learning Project is.gd/MLtyGk\n2 12 Machine learning Object Detection is.gd/jZMP1A\n3 20 NLP Project with Python is.gd/jcMvjB\n4 10 Machine Learning Projects on Time Series Forecasting is.gd/dOR66m\n5 20 Deep Learning Projects Solved and Explained with Python is.gd/8Cv5EP\n6 20 Machine learning Project is.gd/LZTF0J\n7 30 Python Project Solved and Explained is.gd/xhT36v\n8 Machine learning Course for Free https://lnkd.in/ekCY8xw\n9 5 Web Scraping Projects with Python is.gd/6XOTSn\n10 20 Machine Learning Projects on Future Prediction with Python is.gd/xDKDkl\n11 4 Chatbot Project With Python is.gd/LyZfXv\n12 7 Python Gui project is.gd/0KPBvP\n13 All Unsupervised learning Projects is.gd/cz11Kv\n14 10 Machine learning Projects for Regression Analysis is.gd/k8faV1\n15 10 Machine learning Project for Classification with Python is.gd/BJQjMN\n16 6 Sentimental Analysis Projects with python is.gd/WeiE5p\n17 4 Recommendations Projects with Python is.gd/pPHAP8\n18 20 Deep learning Project with python is.gd/l3OCJs\n19 5 COVID19 Projects with Python is.gd/xFCnYi\n20 9 Computer Vision Project with python is.gd/lrNybj\n21 8 Neural Network Project with python is.gd/FCyOOf\n22 5 Machine learning Project for healthcare https://bit.ly/3b86bOH\n23 5 NLP Project with Python https://bit.ly/3hExtNS\n24 47 Machine Learning Projects for 2021 https://bit.ly/356bjiC\n25 19 Artificial Intelligence Projects for 2021 https://bit.ly/38aLgsg\n26 28 Machine learning Projects for 2021 https://bit.ly/3bguRF1\n27 16 Data Science Projects with Source Code for 2021 https://bit.ly/3oa4zYD\n28 24 Deep learning Projects with Source Code for 2021 https://bit.ly/3rQrOsU\n29 25 Computer Vision Projects with Source Code for 2021 https://bit.ly/2JDMO4I\n30 23 Iot Projects with Source Code for 2021 https://bit.ly/354gT53\n31 27 Django Projects with Source Code for 2021 https://bit.ly/2LdRPRZ\n32 37 Python Fun Projects with Code for 2021 https://bit.ly/3hBHzz4\n33 500 + Top Deep learning Codes https://bit.ly/3n7AkAc\n34 500 + Machine learning Codes https://bit.ly/3b32n13\n35 20+ Machine Learning Datasets & Project Ideas https://bit.ly/3b2J48c\n36 1000+ Computer vision codes https://bit.ly/2LiX1nv\n37 300 + Industry wise Real world projects with code https://bit.ly/3rN7lVR\n38 1000 + Python Project Codes https://bit.ly/3oca2xM\n39 363 + NLP Project with Code https://bit.ly/3b442DO\n40 50 + Code ML Models (For iOS 11) Projects https://bit.ly/389dB2s\n41 180 + Pretrained Model Projects for Image, text, Audio and Video https://bit.ly/3hFyQMw\n42 50 + Graph Classification Project List https://bit.ly/3rOYFhH\n43 100 + Sentence Embedding(NLP Resources) https://bit.ly/355aS8c\n44 100 + Production Machine learning Projects https://bit.ly/353ckI0\n45 300 + Machine Learning Resources Collection https://bit.ly/3b2LjIE\n46 70 + Awesome AI https://bit.ly/3hDIXkD\n47 150 + Machine learning Project Ideas with code https://bit.ly/38bfpbg\n48 100 + AutoML Projects with code https://bit.ly/356zxZX\n49 100 + Machine Learning Model Interpretability Code Frameworks https://bit.ly/3n7FaNB\n50 120 + Multi Model Machine learning Code Projects https://bit.ly/38QRI76\n51 Awesome Chatbot Projects https://bit.ly/3rQyxmE\n52 Awesome ML Demo Project with iOS https://bit.ly/389hZOY\n53 100 + Python based Machine learning Application Projects https://bit.ly/3n9zLWv\n54 100 + Reproducible Research Projects of ML and DL https://bit.ly/2KQ0J8C\n55 25 + Python Projects https://bit.ly/353fRpK\n56 8 + OpenCV Projects https://bit.ly/389mj0B\n57 1000 + Awesome Deep learning Collection https://bit.ly/3b0a9Jj\n58 200 + Awesome NLP learning Collection https://bit.ly/3b74b9o\n59 200 + The Super Duper NLP Repo https://bit.ly/3hDNnbd\n60 100 + NLP dataset for your Projects https://bit.ly/353h2Wc\n61 364 + Machine Learning Projects definition https://bit.ly/2X5QRdb\n62 300+ Google Earth Engine Jupyter Notebooks to Analyze Geospatial Data https://bit.ly/387JwjC\n63 1000 + Machine learning Projects Information https://bit.ly/3rMGk4N\n64. 11 Computer Vision Projects with code https://bit.ly/38gz2OR\n65. 13 Computer Vision Projects with Code https://bit.ly/3hMJdhh\n66. 13 Cool Computer Vision GitHub Projects To Inspire You https://bit.ly/2LrSv6d\n67. Open-Source Computer Vision Projects (With Tutorials) https://bit.ly/3pUss6U\n68. OpenCV Computer Vision Projects with Python https://bit.ly/38jmGpn\n69. 100 + Computer vision Algorithm Implementation https://bit.ly/3rWgrzF\n70. 80 + Computer vision Learning code https://bit.ly/3hKCpkm\n71. Deep learning Treasure https://bit.ly/359zLQb\n#100+ Free Machine Learning Books\n#ALL THE CREDITS GOES TO THE RESPECTIVE CREATORS AND THESE RESOURCES ARE COMBINED TOGETHER TO MAKE A WONDERFUL AND COMPACT LEARNING RESOURCE FOR THE DATASCIENCE ENTHUSIASTS\nPart 1:- Roadmap\nPart 2:- Free Online Courses\nPart 3:- 500 Datascience Projects\nPart 4:- 100+ Free Machine Learning Books",
      "link": "https://github.com/therealsreehari/Learn-Data-Science-For-Free"
    },
    {
      "autor": "data-science-interviews",
      "date": "NaN",
      "content": "-----> Photo !!!  by Waseem Farooq from PxHere\nData Science Interviews\nData science interview questions - with answers\nThe answers are given by the community\nIf you know how to answer a question \u2014 please create a PR with the answer\nIf there's already an answer, but you can improve it \u2014 please create a PR with improvement suggestion\nIf you see a mistake \u2014 please create a PR with a fix\nFor updates, follow me on Twitter (@Al_Grigor) and on LinkedIn (agrigorev)\nDo you want to talk about data? Join DataTalks.Club\nQuestions by category\nTheoretical questions: theory.md (linear models, trees, neural networks and others)\nTechnical questions: technical.md (SQL, Python, coding)\nMore to come\nContributed questions\nThe contrib folder contains contributed interview questions:\nProbability: contrib/probability.md\nAdd your questions here!\nOther useful things\nAwesome data science interview questions and other resources: awesome.md\nThis is a joint effort of many people. You can see the list of contributors here: contributors.md\nLicense\nThis work is licensed under a Creative Commons Attribution 4.0 International License.",
      "link": "https://github.com/alexeygrigorev/data-science-interviews"
    },
    {
      "autor": "teachable-machine-v1",
      "date": "NaN",
      "content": "Teachable Machine\nAbout\nTeachable Machine is an experiment that makes it easier for anyone to explore machine learning, live in the browser \u2013 no coding required. Learn more about the experiment and try it yourself on g.co/teachablemachine.\nThe experiment is built using the TensorFlow.js library.\nWe have also released a boilerplate version of this project that can be used as a starting point for your own projects: googlecreativelab/teachable-machine-boilerplate\nDevelopment\nInstall dependencies by running (similar to npm install)\nyarn\nBuild project\nyarn build\nStart local server by running\nyarn run watch\nCode Styles\nThere\u2019s a pre-commit hook set up that will prevent commits when there are errors\nRun yarn eslint for es6 errors & warnings\nRun yarn stylint for stylus errors & warnings\nTo run https locally:\nhttps is required to get -----> camera !!!  permissions to work when not working with localhost\nGenerate Keys\nopenssl genrsa -out server.key 2048\nopenssl req -new -x509 -sha256 -key server.key -out server.cer -days 365 -subj /CN=YOUR_IP\nUse yarn run watch-https\nGo to https://YOUR_IP:3000, then accept the insecure privacy notice, and proceed.\nCredit\nThis is not an official Google product, but an experiment that was a collaborative effort by friends from St\u00f8j, Use All Five and Creative Lab and PAIR teams at Google.",
      "link": "https://github.com/googlecreativelab/teachable-machine-v1"
    },
    {
      "autor": "librephotos",
      "date": "NaN",
      "content": "LibrePhotos\nMockup designed by rawpixel.com / Freepik\nScreenshots\nLive demo\nLive demo available here. User is demo, password is demo1234.\nCommunication\nYou can join our Discord.\nWhat is it?\nLibrePhotos is a fork of Own-----> photo !!! s\nA self-hosted open source -----> photo !!!  management service, with a slight focus on cool graphs\nDjango backend and React frontend\nInstallation\nStep-by-step installation instructions are available in our documentation\nAlternative Linux installation scripts\nsee : librephotos-linux\nContributions\nGet started contributing in less than 30 minutes by following the guide here\nJoin our discord server, or open a pull request to start contributing\nCurrently the project is in very early stages, some bugs may exist. If you find any please log an issue\nFeatures\nSupport for all types of photos including raw photos\nSupport for videos\nTimeline view\nScans pictures on the file system\nMultiuser support\nGenerate albums based on events like \"Thursday in Berlin\"\nFace recognition / Face classification\nReverse geocoding\nObject / Scene detection\nSemantic image search\nSearch by metadata\nWhat does it use?\nImage Conversion: ImageMagick\nVideo Conversion: FFmpeg\nExif Support: ExifTool\nFace detection: face_recognition\nFace classification/clusterization: scikit-learn\nImage captioning: im2txt,\nScene classification places365\nReverse geocoding: Mapbox: You need to have an API key. First 50,000 geocode lookups are free every month.",
      "link": "https://github.com/LibrePhotos/librephotos"
    },
    {
      "autor": "ArtLine",
      "date": "NaN",
      "content": "ArtLine\nThe main aim of the project is to create amazing line art portraits.\nSounds Intresting,let's get to the pictures!!\nModel-(Smooth)\nModel-(Quality)\nClick on the below image to know more about colab demo, credits to Bhavesh Bhatt for the amazing Youtube video.\nExciting update\nCheck Toon-Me https://github.com/vijishmadhavan/Toon-Me, Toon Portraits.\nHighlights\nExample Images\nCartoonize\nMovie Poster created using ArtLine\nTechnical Details\nExample Images\nbohemian rhapsody movie , Rami Malek American actor\nPhoto by Maxim from Pexels\nFriends, TV show.\nKeanu Reeves, Canadian actor.\nHrithik Roshan\nAlita: Battle Angel\nVirat Kohli, Indian cricketer\nPhoto by Anastasiya Gepp from Pexels\nInterstellar\nPexels Portrait, Model\nBeyonc\u00e9, American singer\nCartoonize\nLets cartoonize the lineart portraits, Check Toon-Me https://github.com/vijishmadhavan/Toon-Me.\nSkrillex , American DJ\nTom Hanks, Actor\nLine Art\nThe amazing results that the model has produced has a secret sauce to it. The initial model couldn't create the sort of output I was expecting, it mostly struggled with recognizing facial features. Even though (https://github.com/yiranran/APDrawingGAN) produced great results it had limitations like (frontal face -----> photo !!!  similar to ID -----> photo !!! , preferably with clear face features, no glasses and no long fringe.) I wanted to break-in and produce results that could recognize any pose. Achieving proper lines around the face, eyes, lips and nose depends on the data you give the model. APDrawing dataset alone was not enough so I had to combine selected photos from Anime sketch colorization pair dataset. The combined dataset helped the model to learn the lines better.\nMovie Poster created using ArtLine.\nThe movie poster was created using ArtLine in no time , it's not as good as it should be but I'm not an artist.\nTechnical Details\nSelf-Attention (https://arxiv.org/abs/1805.08318). Generator is pretrained UNET with spectral normalization and self-attention. Something that I got from Jason Antic's DeOldify(https://github.com/jantic/DeOldify), this made a huge difference, all of a sudden I started getting proper details around the facial features.\nProgressive Resizing (https://arxiv.org/abs/1710.10196),(https://arxiv.org/pdf/1707.02921.pdf). Progressive resizing takes this idea of gradually increasing the image size, In this project the image size were gradually increased and learning rates were adjusted. Thanks to fast.ai for intrdoucing me to Progressive resizing, this helps the model to generalise better as it sees many more different images.\nGenerator Loss : Perceptual Loss/Feature Loss based on VGG16. (https://arxiv.org/pdf/1603.08155.pdf).\nSurprise!! No critic,No GAN. GAN did not make much of a difference so I was happy with No GAN.\nThe mission was to create something that converts any personal photo into a line art. The initial efforts have helped to recognize lines, but still the model has to improve a lot with shadows and clothes. All my efforts are to improve the model and make line art a click away.\nDataset\nAPDrawing dataset\nAnime sketch colorization pair dataset\nAPDrawing data set consits of mostly close-up portraits so the model would struggle to recogonize cloths,hands etc. For this purpose selected images from Anime sketch colorization pair were used.\nGoing Forward\nI hope I was clear, going forward would like to improve the model further as it still struggles with random backgrounds(I'm creating a custom dataset to address this issue). Cartoonizing the image was never part of the project, but somehow it came up and it did okay!! Still lots to improve. Ill release the cartoonize model when it looks impressive enough to show off.\nI will be constantly upgrading the project for the foreseeable future.\nGetting Started Yourself\nThe easiest way to get started is to simply try out on Colab: https://colab.research.google.com/github/vijishmadhavan/Light-Up/blob/master/ArtLine(Try_it_on_Colab).ipynb\nInstallation Details\nThis project is built around the wonderful Fast.AI library.\nfastai==1.0.61 (and its dependencies). Please dont install the higher versions\nPyTorch 1.6.0 Please don't install the higher versions\nLimitations\nGetting great output depends on Lighting, Backgrounds,Shadows and the quality of photos. You'll mostly get good results in the first go but there are chances for issues as well. The model is not there yet, it still needs to be tweaked to reach out to all the consumers. It might be useful for \"AI Artisits/ Artists who can bring changes to the final output.\nThe model confuses shadows with hair, something that I'm trying to solve.\nIt does bad with low quality images(below 500px).\nI'm not a coder, bear with me for the bad code and documentation. Will make sure that I improve with upcoming updates.\nUpdates\nGet more updates on Twitter\nMail me @ vijishmadhavan@gmail.com\nAcknowledgments\nThe code is inspired from Fast.AI's Lesson 7 and DeOldify (https://github.com/jantic/DeOldify), Please have look at the Lesson notebook (https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson7-superres-gan.ipynb)\nThanks to (https://github.com/yiranran/APDrawingGAN) for the amazing dataset.\nLicense\nAll code in this repository is under the MIT license as specified by the LICENSE file.",
      "link": "https://github.com/vijishmadhavan/ArtLine"
    },
    {
      "autor": "awesome-satellite-imagery-datasets",
      "date": "NaN",
      "content": "Awesome Satellite Imagery Datasets\nList of aerial and satellite imagery datasets with annotations for computer vision and deep learning. Newest datasets at the top of each category (Instance segmentation, object detection, semantic segmentation, scene classification, other).\nRecent additions and ongoing competitions\nxView3 Dark Vessel Detection 2021 (xView3 Team, Aug 2021)\nMaritime object bounding boxes for 1k Sentinel-1 scenes (VH & VV polarizations), ancillary data (land/ice mask, bathymetry, wind speed, direction, quality).\nUniversity-1652: Drone-based Geolocalization (Image Retrieval) (ACM Multimedia, Oct 2020)\nCorresponding imagery from drone, satellite and ground -----> camera !!!  of 1,652 university buildings, Paper: Zheng et al. 2020\nAFO - Aerial dataset of floating objects (Ga\u0327sienica-J\u00f3zkowy et al, Jun 2020)\n3647 drone images from 50 scenes, 39991 objects with 6 categories (human, wind/sup-board, boat, bouy, sailboat, kayak), Darknet YOLO format, Paper: Authors: Ga\u0327sienica-J\u00f3zkowy et al. 2021\n1. Instance Segmentation\nSpaceNet 7: Multi-Temporal Urban Development Challenge (CosmiQ Works, Planet, Aug 2020)\nMonthly building footprints and Planet imagery (4m. res) timeseries for 2 years, 100 locations around the globe, for building footprint evolution & address propagation.\nRarePlanes: Synthetic Data Takes Flight (CosmiQ Works, A.I.Reverie, June 2020)\nSynthetic (630k planes, 50k images) and real (14.7k planes, 253 Worldview-3 images (0.3m res.), 122 locations, 22 countries) plane annotations & properties and satellite images. Tools. Paper: Shermeyer et al. 2020\nSpaceNet: Multi-Sensor All-Weather Mapping (CosmiQ Works, Capella Space, Maxar, AWS, Intel, Feb 2020)\n48k building footprints (enhanced 3DBAG dataset, building height attributes), Capella Space SAR data (0.5m res., four polarizations) & Worldview-3 imagery (0.3m res.), Rotterdam, Netherlands.\nAgriculture-Vision Database & CVPR 2020 challenge (UIUC, Intelinair, CVPR, Jan 2020)\nAgricultural Pattern Analysis, 21k aerial farmland images (RGB-NIR, USA, 2019 season, 512x512px chips), label masks for 6 field anomaly patterns (Cloud shadow, Double plant, Planter skip, Standing Water, Waterway and Weed cluster). Paper: Chiu et al. 2020\niSAID: Large-scale Dataset for Object Detection in Aerial Images (IIAI & Wuhan University, Dec 2019)\n15 categories from plane to bridge, 188k instances, object instances and segmentation masks (MS COCO format), Google Earth & JL-1 image chips, Faster-RCNN baseline model (MXNet), devkit, Academic use only, replaces DOTA dataset, Paper: Zamir et al. 2019\nxView 2 Building Damage Asessment Challenge (DIUx, Nov 2019) .\n550k building footprints & 4 damage scale categories, 20 global locations and 7 disaster types (wildfire, landslides, dam collapses, volcanic eruptions, earthquakes/tsunamis, wind, flooding), Worldview-3 imagery (0.3m res.), pre-trained baseline model. Paper: Gupta et al. 2019\nMicrosoft BuildingFootprints Canada & USA & Uganda/Tanzania & Australia (Microsoft, Mar 2019)\n12.6mil (Canada) & 125.2mil (USA) & 17.9mil (Uganda/Tanzania) & 11.3mil (Australia) building footprints, GeoJSON format, delineation based on Bing imagery using ResNet34 architecture.\nSpaceNet 4: Off-Nadir Buildings (CosmiQ Works, DigitalGlobe, Radiant Solutions, AWS, Dec 2018)\n126k building footprints (Atlanta), 27 WorldView 2 images (0.3m res.) from 7-54 degrees off-nadir angle. Bi-cubicly resampled to same number of pixels in each image to counter courser native resolution with higher off-nadir angles, Paper: Weir et al. 2019\nAirbus Ship Detection Challenge (Airbus, Nov 2018)\n131k ships, 104k train / 88k test image chips, satellite imagery (1.5m res.), raster mask labels in in run-length encoding format, Kaggle kernels.\nOpen AI Challenge: Tanzania (WeRobotics & Wordlbank, Nov 2018)\nBuilding footprints & 3 building conditions, RGB UAV imagery - Link to data\nLPIS agricultural field boundaries Denmark - Netherlands - France\nAnnual datasets. Denmark: 293 crop/vegetation catgeories, 600k parcels. Netherlands: 294 crop/vegetation catgeories, 780k parcels\nCrowdAI Mapping Challenge (Humanity & Inclusion NGO, May 2018)\nBuildings footprints, RGB satellite imagery, COCO data format\nSpaceNet 2: Building Detection v2 (CosmiQ Works, Radiant Solutions, NVIDIA, May 2017)\n685k building footprints, 3/8band Worldview-3 imagery (0.3m res.), 5 cities, SpaceNet Challenge Asset Library\nSpaceNet 1: Building Detection v1 (CosmiQ Works, Radiant Solutions, NVIDIA, Jan 2017)\nBuilding footprints (Rio de Janeiro), 3/8band Worldview-3 imagery (0.5m res.), SpaceNet Challenge Asset Library\n2. Object Detection\nxView3 Dark Vessel Detection 2021 (xView3 Team, Aug 2021)\nMaritime object bounding boxes for 1k Sentinel-1 scenes (VH & VV polarizations), ancillary data (land/ice mask, bathymetry, wind speed, direction, quality).\nAFO - Aerial dataset of floating objects (Ga\u0327sienica-J\u00f3zkowy et al, Jun 2020)\n3647 drone images from 50 scenes, 39991 objects with 6 categories (human, wind/sup-board, boat, bouy, sailboat, kayak), Darknet YOLO format, Paper: Authors: Ga\u0327sienica-J\u00f3zkowy et al. 2021\nNEON Tree Crowns Dataset (Weinstein et al., 2020)\nIndividual tree crown objects, height&area estimates, 100 million instances, 37 geographic sites across the US, DeepForest Python package, Paper: Weinstein et al. 2020\nxView 2018 Detection Challenge (DIUx, Jul 2018)\n60 categories from helicopter to stadium, 1 million instances, Worldview-3 imagery (0.3m res.), COCO data format, pre-trained Tensorflow and Pytorch baseline models, Paper: Lam et al. 2018\nOpen AI Challenge: Aerial Imagery of South Pacific Islands (WeRobotics & Worldbank, May 2018)\nTree position & 4 tree species, RGB UAV imagery (0.4m/0.8m res.), multiple AOIs in Tonga\nNIST DSE Plant Identification with NEON Remote Sensing Data (inria.fr, Oct 2017)\nTree position, tree species and crown parameters, hyperspectral (1m res.) & RGB imagery (0.25m res.), LiDAR point cloud and canopy height model\nNOAA Fisheries Steller Sea Lion Population Count (NOAA, Jun 2017)\n5 sea lion categories, ~ 80k instances, ~ 1k aerial images, Kaggle kernels\nStanford Drone Data (Stanford University, Oct 2016)\n60 aerial UAV videos over Stanford campus and bounding boxes, 6 classes (Pedestrian, Biker, Skateboarder, Cart, Car, Bus), Paper: Robicquet et al. 2016\nCars Overhead With Context (COWC) (Lawrence Livermore National Laboratory, Sep 2016)\n32k car bounding boxes, aerial imagery (0.15m res.), 6 cities, Paper: Mundhenk et al. 2016\n3. Semantic Segmentation\nFloodNet Challenge (UMBC, Microsoft, Texas A&M, Dewberry, May 2021)\n2343 UAV images from after Hurricane Harvey, landcover labels (10 categories, e.g. building flooded, building non-flooded, road-flooded, ..), 2 competition tracks (Binary & semantic flood classification; Object counting & condition recognition)\nDynamic EarthNet Challenge (Planet, DLR, TUM, April 2021)\nWeekly Planetscope time-series (3m res.) over 2 years, 75 aois, landcover labels (7 categories), 2 competition tracks (Binary land cover classification & multi-class change detection)\nSentinel-2 Cloud Mask Catalogue (Francis, A., et al., Nov 2020) 513 cropped subscenes (1022x1022 pixels) taken randomly from entire 2018 Sentinel-2 archive. All bands resampled to 20m, stored as numpy arrays. Includes clear, cloud and cloud-shadow classes. Also comes with binary classification tags for each subscene, describing what surface types, cloud types, etc. are present.\nLandCoverNet: A Global Land Cover Classification Training Dataset (Alemohammad S.H., et al., Jul 2020) Version 1.0 of the dataset that contains data across Africa, (20% of the global dataset). 1980 image chips of 256 x 256 pixels in V1.0 spanning 66 tiles of Sentinel-2. Classes: water, natural bare ground, artificial bare ground, woody vegetation, cultivated vegetation, (semi) natural vegetation, and permanent snow/ice. Citation: Alemohammad S.H., et al., 2020 and blog post\nLandCover.ai: Dataset for Automatic Mapping of Buildings, Woodlands and Water from Aerial Imagery (Boguszewski, A., et al., May 2020) 41 orthophotos (9000x9000 px) over Poland, Aerial Imagery (25cm & 50cm res.), manual segmentations masks for Buildings, Woodland and Water, Paper: Boguszewski et al., 2020\n95-Cloud: A Cloud Segmentation Dataset (S. Mohajerani et. all, Jan 2020)\n34701 manually segmented 384x384 patches with cloud masks, Landsat 8 imagery (R,G,B,NIR; 30 m res.), Paper: Mohajerani et al. 2021\nOpen Cities AI Challenge (GFDRR, Mar 2020) .\n790k building footprints from Openstreetmap (2 label quality categories), aerial imagery (0.03-0.2m resolution, RGB, 11k 1024x1024 chips, COG format), 10 cities in Africa.\nDroneDeploy Segmentation Dataset (DroneDeploy, Dec 2019)\nDrone imagery (0.1m res., RGB), labels (7 land cover catageories: building, clutter, vegetation, water, ground, car) & elevation data, baseline model implementation.\nSkyScapes: Urban infrastructure & lane markings (DLR, Nov 2019)\nHighly accurate street lane markings (12 categories e.g. dash line, long line, zebra zone) & urban infrastructure (19 categories e.g. buildings, roads, vegetation). Aerial imagery (0.13 m res.) for 5.7 km2 of Munich, Germany. Paper: Azimi et al. 2019\nOpen AI Challenge: Caribbean (MathWorks, WeRobotics, Wordlbank, DrivenData, Dec 2019)\nPredict building roof type (5 categories, e.g. concrete, metal etc.) of provided building footprints (22,553), RGB UAV imagery (4cm res., 7 areas in 3 Carribbean countries)\nSpaceNet 5: Automated Road Network Extraction & Route Travel Time Estimation (CosmiQ Works, Maxar, Intel, AWS, Sep 2019)\n2300 image chips, street geometries with location, shape and estimated travel time, 3/8band Worldview-3 imagery (0.3m res.), 4 global cities, 1 holdout city for leaderboard evaluation, APLS metric, baseline model\nSEN12MS (TUM, Jun 2019)\n180,748 corresponding image triplets containing Sentinel-1 (VV&VH), Sentinel-2 (all bands, cloud-free), and MODIS-derived land cover maps (IGBP, LCCS, 17 classes, 500m res.). All data upsampled to 10m res., georeferenced, covering all continents and meterological seasons, Paper: Schmitt et al. 2018\nSlovenia Land Cover Classification (Sinergise, Feb 2019)\n10 land cover classes, temporal stack of hyperspectral Sentinel-2 imagery (R,G,B,NIR,SWIR1,SWIR2; 10 m res.) for year 2017 with cloud masks, Official Slovenian land use land cover layer as ground truth.\nALCD Reference Cloud Masks (CNES, Oct 2018)\n8 classes (inc. cloud and cloud shadow) for 38 Sentinel-2 scenes (10 m res.). Manual labeling & active learning, Paper: Baetens et al. 2019\nAgricultural Crop Cover Classification Challenge (CrowdANALYTIX, Jul 2018)\n2 main categories corn and soybeans, Landsat 8 imagery (30m res.), USDA Cropland Data Layer as ground truth.\nSpaceNet 3: Road Network Detection (CosmiQ Works, Radiant Solutions, Feb 2018)\n8000 km of roads in 5 city aois, 3/8band Worldview-3 imagery (0.3m res.), SpaceNet Challenge Asset Library, Paper: Van Etten et al. 2018\nUrban 3D Challenge (USSOCOM, Dec 2017)\n157k building footprint masks, RGB orthophotos (0.5m res.), DSM/DTM, 3 cities, SpaceNet Challenge Asset Library\nDSTL Satellite Imagery Feature Detection Challenge (Dstl, Feb 2017)\n10 land cover categories from crops to vehicle small, 57 1x1km images, 3/16-band Worldview 3 imagery (0.3m-7.5m res.), Kaggle kernels\nSPARCS: S2 Cloud Validation data (USGS, 2016)\n7 categories (cloud, cloud shadows, cloud shadows over water, water etc.), 80 1kx1k px. subset Landsat 8 scenes (30m res.), Paper: Hughes, J.M. & Hayes D.J. 2014\nBiome: L8 Cloud Cover Validation data (USGS, 2016)\n4 cloud categories (cloud, thin cloud, cloud shadows, clear), 96 Landsat 8 scenes (30m res.), 12 biomes with 8 scenes each, Paper: Foga et al. 2017\nInria Aerial Image Labeling (inria.fr)\nBuilding footprint masks, RGB aerial imagery (0.3m res.), 5 cities\nISPRS Potsdam 2D Semantic Labeling Contest (ISPRS)\n6 urban land cover classes, raster mask labels, 4-band RGB-IR aerial imagery (0.05m res.) & DSM, 38 image patches\n4. Scene classification\nBigEarthNet: Large-Scale Sentinel-2 Benchmark (TU Berlin, Jan 2019)\nMultiple landcover labels per chip based on CORINE Land Cover (CLC) 2018, 590,326 chips from Sentinel-2 L2A scenes (125 Sentinel-2 tiles from 10 European countries, 2017/2018), 66 GB archive, Paper: Sumbul et al. 2019\nWiDS Datathon 2019 : Detection of Oil Palm Plantations (Global WiDS Team & West Big Data Innovation Hub, Jan 2019) Prediction of presence of oil palm plantations, Planet satellite imagery (3m res.)., ca. 20k 256 x 256 pixel chips, 2 categories oil-palm and other, annotator confidence score.\nSo2Sat LCZ42 (TUM Munich & DLR, Aug 2018)\nLocal climate zone classification, 17 categories (10 urban e.g. compact high-rise, 7 rural e.g. scattered trees), 400k 32x32 pixel chips covering 42 cities (LCZ42 dataset), Sentinel 1 & Sentinel 2 (both 10m res.), 51 GB\nCactus Aerial Photos (CONACYT Mexico, Jun 2018)\n17k aerial photos, 13k cactus, 4k non-actus, Kaggle kernels, Paper: L\u00f3pez-Jim\u00e9nez et al. 2019\nStatoil/C-CORE Iceberg Classifier Challenge (Statoil/C-CORE, Jan 2018)\n2 categories ship and iceberg, 2-band HH/HV polarization SAR imagery, Kaggle kernels\nFunctional Map of the World Challenge (IARPA, Dec 2017)\n63 categories from solar farms to shopping malls, 1 million chips, 4/8 band satellite imagery (0.3m res.), COCO data format, baseline models, Paper: Christie et al. 2017\nEuroSAT (DFK, Aug 2017)\n10 land cover categories from industrial to permanent crop, 27k 64x64 pixel chips, 3/16 band Sentinel-2 satellite imagery (10m res.), covering cities in 30 countries, Paper: Helber et al. 2017\nPlanet: Understanding the Amazon from Space (Planet, Jul 2017)\n13 land cover categories + 4 cloud condition categories, 4-band (RGB-NIR) satelitte imagery (5m res.), Amazonian rainforest, Kaggle kernels\nAID: Aerial Scene Classification (Xia et al., 2017)\n10000 aerial images within 30 categories (airport, bare land, baseball field, beach, bridge, ...) collected from Google Earth imagery. Paper: Xia et al. 2017\nRESISC45 (Northwestern Polytechnical University NWPU, Mar 2017)\n45 scene categories from airplane to wetland, 31,500 images (700 per category, 256x256 px), image chips taken from Google Earth (rich image variations in resolution, angle, geography all over the world), Download Link, Paper: Cheng et al. 2017\nDeepsat: SAT-4/SAT-6 airborne datasets (Louisiana State University, 2015)\n6 land cover categories, 400k 28x28 pixel chips, 4-band RGBNIR aerial imagery (1m res.) extracted from the 2009 National Agriculture Imagery Program (NAIP), Paper: Basu et al. 2015\nUC Merced Land Use Dataset (UC Merced, Oct 2010)\n21 land cover categories from agricultural to parkinglot, 100 chips per class, aerial imagery (0.30m res.), Paper: Yang & Newsam 2010\n5. Other Focus / Multiple Tasks\nIEEE Data Fusion Contest 2021 (IEEE, HP, SolarAid, Data Science Experts, Mar 2021)\nDetection of settlements without electricity, 98 multi-temporal/multi-sensor tiles ( Sentinel-1, Sentinel-2, Landsat-8, VIIRS), per chip & per pixel labels (contains buildings, presence electricity).\nUniversity-1652: Drone-based Geolocalization (Image Retrieval) (ACM Multimedia, Oct 2020)\nCorresponding imagery from drone, satellite and ground camera of 1,652 university buildings, Paper: Zheng et al. 2020\nIEEE Data Fusion Contest 2020 (IEEE & TUM, Mar 2020)\nLand cover classification based on SEN12MS dataset (see category Semantic Segmentation on this list), low- and high-resolution tracks.\nIEEE Data Fusion Contest 2019 (IEEE, Mar 2019)\nMultiple tracks: Semantic 3D reconstruction, Semantic Stereo, 3D-Point Cloud Classification. Worldview-3 (8-band, 0.35cm res.) satellite imagery, LiDAR (0.80m pulse spacing, ASCII format), semantic labels, urban setting USA, baseline methods provided, Paper: Le Saux et al. 2019\nIEEE Data Fusion Contest 2018 (IEEE, Mar 2018)\n20 land cover categories by fusing three data sources: Multispectral LiDAR, Hyperspectral (1m), RGB imagery (0.05m res.)\nDEEPGLOBE - 2018 Satellite Challange (CVPR, Apr 2018)\nThree challenge tracks: Road Extraction, Building Detection, Land cover classification, Paper: Demir et al. 2018\nTiSeLaC: Time Series Land Cover Classification Challenge (UMR TETIS, Jul 2017)\nLand cover time series classification (9 categories), Landsat-8 (23 images time series, 10 band features, 30m res.), Reunion island\nMulti-View Stereo 3D Mapping Challenge (IARPA, Nov 2016)\nDevelop a Multi-View Stereo (MVS) 3D mapping algorithm that can convert high-resolution Worldview-3 satellite images to 3D point clouds, 0.2m lidar ground truth data.\nDraper Satellite Image Chronology (Draper, Jun 2016)\nPredict the chronological order of images taken at the same locations over 5 days, Kaggle kernels\nMore Resources\nawesome-remote-sensing-change-detection\nRadiant MLHub Training Data Registry",
      "link": "https://github.com/chrieke/awesome-satellite-imagery-datasets"
    },
    {
      "autor": "awesome-machine-learning-interpretability",
      "date": "NaN",
      "content": "awesome-machine-learning-interpretability\nA curated, but probably biased and incomplete, list of awesome machine learning interpretability resources.\nIf you want to contribute to this list (and please do!) read over the contribution guidelines, send a pull request, or contact me @jpatrickhall.\nAn incomplete, imperfect blueprint for a more human-centered, lower-risk machine learning. The resources in this repository can be used to do many of these things today. The resources in this repository should not be considered legal compliance advice.\nImage credit: H2O.ai Machine Learning Interpretability team, https://github.com/h2oai/mli-resources.\nTable of Contents\nComprehensive Software Examples and Tutorials\nExplainability- or Fairness-Enhancing Software Packages\nBrowser\nPython\nR\nMachine learning environment management tools\nFree Books\nGovernment and Regulatory Documents\nOther Interpretability and Fairness Resources and Lists\nReview and General Papers\nClasses\nInterpretable (\"Whitebox\") or Fair Modeling Packages\nC/C++\nPython\nR\nAI Incident Tracker\nComprehensive Software Examples and Tutorials\nCOMPAS Analysis Using Aequitas\nExplaining Quantitative Measures of Fairness (with SHAP)\nGetting a Window into your Black Box Model\nFrom GLM to GBM Part 1\nFrom GLM to GBM Part 2\nIML\nInterpretable Machine Learning with Python\nInterpreting Machine Learning Models with the iml Package\nInterpretable Machine Learning using Counterfactuals\nMachine Learning Explainability by Kaggle Learn\nModel Interpretability with DALEX\nModel Interpretation series by Dipanjan (DJ) Sarkar:\nThe Importance of Human Interpretable Machine Learning\nModel Interpretation Strategies\nHands-on Machine Learning Model Interpretation\nInterpreting Deep Learning Models for Computer Vision\nPartial Dependence Plots in R\nSaliency Maps for Deep Learning\nVisualizing ML Models with LIME\nVisualizing and debugging deep convolutional networks\nWhat does a CNN see?\nExplainability- or Fairness-Enhancing Software Packages\nBrowser\nDiscriLens\nmanifold\nTensorBoard Projector\nWhat-if Tool\nPython\nacd\naequitas\nAI Fairness 360\nAI Explainability 360\nALEPython\nAletheia\nallennlp\nalgofairness\nAlibi\nanchor\nBlackBoxAuditing\ncasme\ncaptum\ncausalml\nchecklist\ncontextual-AI\nContrastiveExplanation (Foil Trees)\ncounterfit\ndalex\ndebiaswe\nDeepExplain\ndeeplift\ndeepvis\nDiCE\nDoWhy\necco\neli5\nexplainerdashboard\nfairml\nfairlearn\nfairness-comparison\nfairness_measures_code\nfoolbox\nGrad-CAM (GitHub topic)\ngplearn\nhate-functional-tests\nimodels\niNNvestigate neural nets\nIntegrated-Gradients\ninterpret\ninterpret_with_rules\nimodels\nKeras-vis\nkeract\nL2X\nlime\nLiFT\nlit\nlofo-importance\nlrp_toolbox\nMindsDB\nMLextend\nml-fairness-gym\nml_privacy_meter\nOptBinning\nparity-fairness\nPDPbox\npyBreakDown\nPyCEbox\npyGAM\npymc3\npytorch-innvestigate\nrationale\nresponsibly\nrevise-tool\nrobustness\nRISE\nsage\nSALib\nscikit-fairness\nshap\nshapley\nSkater\ntensorfow/cleverhans\ntensorflow/lucid\ntensorflow/fairness-indicators\ntensorflow/model-analysis\ntensorflow/model-card-toolkit\ntensorflow/model-remediation\ntensorflow/privacy\ntensorflow/tcav\ntensorfuzz\nTensorWatch\nTextFooler\ntf-explain\nThemis\nthemis-ml\ntreeinterpreter\nwoe\nxai\nxdeep\nyellowbrick\nR\naif360\nALEPlot\nDrWhyAI\nDALEX\nDALEXtra\nEloML\nExplainPrediction\nfastshap\nfairness\nfairmodels\nfeatureImportance\nflashlight\nforestmodel\nfscaret\niBreakDown\nICEbox\niml\ningredients\nintepret\nlightgbmExplainer\nlime\nlive\nmcr\nmodelDown\nmodelOriented\nmodelStudio\npdp\nshapFlex\nshapleyR\nshapper\nsmbinning\nvip\nxgboostExplainer\nMachine learning environment management tools\ndvc\ngigantum\nmlflow\nmlmd\nmodeldb\nwhylabs\nFree Books\nAn Introduction to Machine Learning Interpretability\nExplanatory Model Analysis\nFairness and Machine Learning\nInterpretable Machine Learning\nResponsible Machine Learning (requires email for now)\nGovernment and Regulatory Documents\n12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B)\nA Regulatory Framework for AI: Recommendations for PIPEDA Reform\nAI Principles: Recommendations on the Ethical Use of Artificial Intelligence by the Department of Defense\nTHE AIM INITIATIVE\nAiming for truth, fairness, and equity in your company\u2019s use of AI\nAlgorithmic Accountability Act of 2019\nALGORITHM CHARTER FOR AOTEAROA NEW ZEALAND\nArtificial Intelligence (AI) in the Securities Industry\nArticle 22 EU GDPR\nAssessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment - Shaping Europe\u2019s digital future - European Commission\nAudit of Governance and Protection of Department of Defense Artificial Intelligence Data and Technology\nA Primer on Artificial Intelligence in Securities Markets\nBiometric Information Privacy Act\nBooker Wyden Health Care Letters\nCalifornia Consumer Privacy Act (CCPA)\nCalifornia Privacy Rights Act (CPRA)\nConsultation on the OPC\u2019s Proposals for ensuring appropriate regulation of artificial intelligence\nCivil liability regime for artificial intelligence\nData Ethics Framework\nDEVELOPING FINANCIAL SECTOR RESILIENCE IN A DIGITAL WORLD: SELECTED THEMES IN TECHNOLOGY AND RELATED RISKS\nDirective on Automated Decision Making\nExecutive Order on Promoting the Use of Trustworthy Artificial Intelligence in the Federal Government\nEEOC Letter (from U.S. senators re: hiring software)\nFacial Recognition and Biometric Technology Moratorium Act of 2020\nFour Principles of Explainable Artificial Intelligence\nGeneral principles for the use of Artificial Intelligence in the financial sector\nGouvernance des algorithmes d\u2019intelligence artificielle dans le secteur financier (French)\nInnovation spotlight: Providing adverse action notices when using AI/ML models\nOffice of Management and Budget Guidance for Regulation of Artificial Intelligence Applications (Finalized Nov. 2020)\nOn Artificial Intelligence - A European approach to excellence and trust\nOpinion of the German Data Ethics Commission\nPrinciples of Artificial Intelligence Ethics for the Intelligence Community\nProposal for a Regulation laying down harmonised rules on artificial intelligence (Artificial Intelligence Act)\nPsychological Foundations of Explainability and Interpretability in Artificial Intelligence\nQuestions and Answers to Clarify and Provide a Common Interpretation of the Uniform Guidelines on Employee Selection Procedures\nQuestions from the Commission on Protecting Privacy and Preventing Discrimination\nRE: Use of External Consumer Data and Information Sources in Underwriting for Life Insurance\nSingapore Personal Data Protection Commission (PDPC) Model Artificial Intelligence Governance Framework\nSUPERVISORY GUIDANCE ON MODEL RISK MANAGEMENT\nU.K. Information Commissioner's Office (ICO) AI Audting Framework (overview series)\nArtificial Intelligence/Machine Learning (AI/ML)-Based: Software as a Medical Device (SaMD) Action Plan (Updated Jan. 2021)\nU.S. House of Representatives Resolution on AI Strategy\nUsing Artificial Intelligence and Algorithms\nOther Interpretability and Fairness Resources and Lists\n8 Principles of Responsible ML\nACM FAT* 2019 Youtube Playlist\nAdversarial ML Threat Matrix\nAI Tools and Platforms\nAI Ethics Guidelines Global Inventory\nAI Incident Database\nAllenNLP Interpret: A Framework for Explaining Predictions of NLP Models\nAlgorithms and prejudice\nAwesome interpretable machine learning ;)\nAwesome machine learning operations\nAwful AI\nalgoaware\nBIML Interactive Machine Learning Risk Framework\nBeyond Explainability: A Practical Guide to Managing Risk in Machine Learning Models\ncriticalML\nData Feminism\nDealing with Bias and Fairness in AI/ML/Data Science Systems\nDebugging Machine Learning Models (ICLR workshop proceedings)\nDecision Points in AI Governance\nDe-identification Tools\nDeep Insights into Explainability and Interpretability of Machine Learning Algorithms and Applications to Risk Management\nDistill\nFaces in the Wild Benchmark Data\nFairness, Accountability, and Transparency in Machine Learning (FAT/ML) Scholarship\nFrom Principles to Practice: An interdisciplinary framework to operationalise AI ethics\nHow will the GDPR impact machine learning?\nMachine Learning Ethics References\nMachine Learning Interpretability Resources\nMachine Learning: Considerations for fairly and transparently expanding access to credit\nMIT AI Ethics Reading Group\nOn the Responsibility of Technologists: A Prologue and Primer\nprivate-ai-resources\nProblems with Shapley-value-based explanations as feature importance measures\nReal-World Model Debugging Strategies\nResponsibleAI\nRobust ML\nSafe and Reliable Machine Learning\nSample AI Incident Response Checklist\nTen Questions on AI Risk\nTesting and Debugging in Machine Learning\nTroubleshooting Deep Neural Networks\nWarning Signs: The Future of Privacy and Security in an Age of Machine Learning\nWhen Not to Trust Your Explanations\nXAI Resources\nYou Created A Machine Learning Application Now Make Sure It's Secure\nReview and General Papers\n50 Years of Test (Un)fairness: Lessons for Machine Learning\nA Comparative Study of Fairness-Enhancing Interventions in Machine Learning\nA Survey Of Methods For Explaining Black Box Models\nA Marauder\u2019s Map of Security and Privacy in Machine Learning\nChallenges for Transparency\nClosing the AI Accountability Gap\nExplaining by Removing: A Unified Framework for Model Explanation\nExplaining Explanations: An Overview of Interpretability of Machine Learning\nExplanation in Human-AI Systems: A Literature Meta-Review, Synopsis of Key Ideas and Publications, and Bibliography for Explainable AI\nInterpretable Machine Learning: Definitions, Methods, and Applications\nLimitations of Interpretable Machine Learning\nMachine Learning Explainability in Finance\nOn the Art and Science of Machine Learning Explanations\nPlease Stop Explaining Black Box Models for High-Stakes Decisions\nSoftware Engineering for Machine Learning: A Case Study\nThe Mythos of Model Interpretability\nTowards A Rigorous Science of Interpretable Machine Learning\nToward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims\nThe Security of Machine Learning\nTechniques for Interpretable Machine Learning\nTrends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda\nUnderspecification Presents Challenges for Credibility in Modern Machine Learning\nClasses\nAn Introduction to Data Ethics\nCertified Ethical Emerging Technologist\nFairness in Machine Learning\nFast.ai Data Ethics course\nHuman-Center Machine Learning\nIntroduction to Responsible Machine Learning\nTrustworthy Deep Learning\nInterpretable (\"Whitebox\") or Fair Modeling Packages\nC/C++\nBorn-again Tree Ensembles\nCertifiably Optimal RulE ListS\nPython\nBayesian Case Model\nBayesian Ors-Of-Ands\nBayesian Rule List (BRL)\nExplainable Boosting Machine (EBM)/GA2M\nfair-classification\nFalling Rule List (FRL)\nH2O-3\nPenalized Generalized Linear Models\nMonotonic GBM\nSparse Principal Components (GLRM)\nlearning-fair-representations\nOptimal Sparse Decision Trees\nMonotonic XGBoost\nMultilayer Logical Perceptron (MLLP)\npyGAM\npySS3\nRisk-SLIM\nScikit-learn\nDecision Trees\nGeneralized Linear Models\nSparse Principal Components\nsklearn-expertsys\nskope-rules\nSuper-sparse Linear Integer models (SLIMs)\ntensorflow/lattice\nThis Looks Like That\nR\narules\nCausal SVM\nelasticnet\nExplainable Boosting Machine (EBM)/GA2M\ngam\nglm2\nglmnet\nH2O-3\nPenalized Generalized Linear Models\nMonotonic GBM\nSparse Principal Components (GLRM)\nMonotonic XGBoost\nquantreg\nrpart\nRuleFit\nScalable Bayesian Rule Lists (SBRL)\nAI Incident Tracker\nMar 1988 - A blot on the profession\nJan 2010 - Are Face-Detection Cameras Racist?\nJul 2015 - Google says sorry for racist auto-tag in -----> photo !!!  app\nMar 2016 - Here Are the Microsoft Twitter Bot\u2019s Craziest Racist Rants\nJun 2016 - Google faulted for racial bias in image search results for black teenagers\nOct 2016 - 'Rogue' Algorithm Blamed for Historic Crash of the British Pound\nOct 2016 - Crime-prediction tool PredPol amplifies racially biased policing, study shows\nMay 2017 - Houston Schools Must Face Teacher Evaluation Lawsuit\nJun 2017 - When a Computer Program Keeps You in Jail\nJun 2017 - Antitrust: Commission fines Google \u20ac2.42 billion for abusing dominance as search engine by giving illegal advantage to own comparison shopping service\nJul 2017 - \u2018Balls have zero to me to me\u2019: What happened when Facebook\u2019s AI chatbots Bob & Alice created their own language\nJul 2017 - YouTube: Boston Dynamics' Atlas Falls Over After Demo at the Congress of Future Scientists and Technologists\nJul 2017 - Royal Free - Google DeepMind trial failed to comply with data protection law\nNov 2017 - Hackers Say They've Broken Face ID a Week After iPhone X Release\nNov 2017 - India\u2019s Friendly Robot Mitra Not Only Greets VIPs On The Stage, But Also Parties Like A Rockstar (Mitra trips over Ivanka Trump/PM Modi introduction)\nJan 2018 - YouTube: CES 2018: Robot refuses to co-operate with LG chief - BBC News\nFeb 2018 - Study finds gender and skin-type bias in commercial artificial-intelligence systems\nMar 2018 - Self-Driving Uber Car Kills Pedestrian in Arizona, Where Robots Roam\nMar 2018 - AI-Assisted Fake Porn Is Here and We're All F***ed\nJun 2018 - Facebook sent a doctor on a secret mission to ask hospitals to share patient data\nJul 2018 - Amazon\u2019s Face Recognition Falsely Matched 28 Members of Congress With Mugshots\nJul 2018 - IBM\u2019s Watson supercomputer recommended \u2018unsafe and incorrect\u2019 cancer treatments, internal documents show\nOct 2018 - Amazon scraps 'sexist AI' recruiting tool that showed bias against women\nNov 2018 - Facial recognition system in China mistakes bus ad for jaywalker\nDec 2018 - AI start-up that scanned babysitters halts launch following Post Report\nJan 2019 - Cambridge Analytica\u2019s parent pleads guilty to breaking UK data law\nApr 2019 - Facebook Executive Testifies on AI Failure to Detect the Christchurch Mosque Shooting Video\nMay 2019 - Investor Sues After an AI\u2019s Automated Trades Cost Him $20 Million\nMay 2019 - Millions of people uploaded photos to the Ever app. Then the company used them to develop facial recognition tools.\nJun 2019 - Google and the University of Chicago Are Sued Over Data Sharing\nAug 2019 - LGBTQ+ creators file lawsuit against YouTube for discrimination\nSep 2019 - The viral selfie app ImageNet Roulette seemed fun \u2013 until it called me a racist slur\nSep 2019 - Scammer Successfully Deepfaked CEO's Voice To Fool Underling Into Transferring $243,000\nOct 2019 - Oh dear... AI models used to flag hate speech online are, er, racist against black people\nOct 2019 - Dissecting racial bias in an algorithm used to manage the health of populations\nNov 2019 - NY regulator investigating Apple Card for possible gender bias\nNov 2019 - Chinese-style facial recognition technology is trialled in Australian schools to register pupils - sparking major privacy concerns\nDec 2019 - Tenants sounded the alarm on facial recognition in their buildings. Lawmakers are listening\nDec 2019 - Researchers bypass airport and payment facial recognition systems using masks\nJan 2020 - Atlantic Plaza Towers tenants won a halt to facial recognition in their building: Now they\u2019re calling on a moratorium on all residential use\nJan 2020 - Trivago misled consumers about hotel room rates\nFeb 2020 - An Indian politician is using deepfake technology to win new voters\nFeb 2020 - Suckers List: How Allstate\u2019s Secret Auto Insurance Algorithm Squeezes Big Spenders\nFeb 2020 - Tesla Autopilot gets tricked into accelerating from 35 to 85 mph with modified speed limit sign\nMar 2020 - Netherlands: Court Prohibits Government\u2019s Use of AI Software to Detect Welfare Fraud\nMar 2020 - The End of Starsky Robotics\nApr 2020 - Google apologizes after its Vision AI produced racist results\nApr 2020 - Google\u2019s medical AI was super accurate in a lab. Real life was a different story.\nMay 2020 - Researchers find major demographic differences in speech recognition accuracy\nMay 2020 - Access Denied: Faulty Automated Background Checks Freeze Out Renters\nMay 2020 - A.C.L.U. Accuses Clearview AI of Privacy \u2018Nightmare Scenario\u2019\nMay 2020 - Walmart Employees Are Out to Show Its Anti-Theft AI Doesn't Work\nMay 2020 - Robodebt removed humans from Human Services, and the Government is facing the consequences\nMay 2020 - The Most Devastating Software Mistake Of All Time. Why Is the Imperial Model Under Criticism?\nJun 2020 - Government\u2019s Use of Algorithm Serves Up False Fraud Charges\nJun 2020 - Microsoft's robot editor confuses mixed-race Little Mix singers\nJun 2020 - Tweet: \"This algorithm probably made this mistake ...\" (President Obama de-blurred into white male)\nJun 2020 - Detroit Police Chief: Facial Recognition Software Misidentifies 96% of the Time\nJun 2020 - Wrongfully Accused by an Algorithm\nJun 2020 - An Algorithm that \"Predicts\" Criminality Based on a Face Sparks a Furor\nJun 2020 - PwC facial recognition tool criticised for home working privacy invasion\nJun 2020 - Santa Cruz becomes the first U.S. city to ban predictive policing\nJun 2020 - YouTube Sued for Race Discrimination, Profiting from Hate Speech\nJul 2020 - ISIS 'still evading detection on Facebook', report says\nJul 2020 - Meet the Secret Algorithm That's Keeping Students Out of College\nJul 2020 - Rite Aid deployed facial recognition systems in hundreds of U.S. stores\nJul 2020 - Tweet: \"Oh, dear ...\" (GPT-3 anti-semitism)\nJul 2020 - Google Ad Portal Equated \u201cBlack Girls\u201d with Porn\nJul 2020 - Facial biometrics training dataset leads to BIPA lawsuits against Amazon, Alphabet and Microsoft\nJul 2020 - POLICE SURVEILLED GEORGE FLOYD PROTESTS WITH HELP FROM TWITTER-AFFILIATED STARTUP DATAMINR\nJul 2020 - AI-Powered \u2018Genderify\u2019 Platform Shut Down After Bias-Based Backlash\nAug 2020 - Police use of facial recognition unlawfully breached privacy rights, says Court of Appeal ruling\nAug 2020 - There is nothing 'fair' about judging A-levels by algorithm\nAug 2020 - When algorithms define kids by postcode: UK exam results chaos reveal too much reliance on data analytics\nAug 2020 - Macy\u2019s hit with privacy lawsuit over alleged use of controversial facial recognition software\nAug 2020 - Google\u2019s Advertising Platform Is Blocking Articles About Racism\nAug 2020 - Home Office drops 'racist' algorithm from visa decisions\nAug 2020 - De Blasio Will Reassess NYPD's Use Of Facial Recognition Tech After Protester Arrest\nAug 2020 - Facebook algorithm recommending Holocaust denial and fascist content, report finds\nAug 2020 - Report: AI Company Leaks Over 2.5M Medical Records\nAug 2020 - Watchdog investigates Barclays for spying on staff\nAug 2020 - PopID\u2019s face-based payments pose privacy and security risks\nAug 2020 - Tinder charges older people more\nAug 2020 - Uber and Lyft pricing algorithms charge more in non-white areas\nSep 2020 - Pasco\u2019s sheriff uses data to guess who will commit crime. Then deputies \u2018hunt down\u2019 and harass them\nSep 2020 - The Met Police didn\u2019t check if facial recognition tech was racist before trialling it\nSep 2020 - These students figured out their tests were graded by AI \u2014 and the easy way to cheat\nSep 2020 - Google says Street View maps algorithm error blurred out Hong Kong protest graffiti aimed at Xi Jinping\nSep 2020 - AI attempts to ease fear of robots, blurts out it can\u2019t \u2018avoid destroying humankind\u2019\nSep 2020 - Ola is facing a drivers\u2019 legal challenge over data access rights and algorithmic management\nSep 2020 - Instagram apologizes for removing images of Black British model\nSep 2020 - Tesla owner in Canada charged with \u2018sleeping\u2019 while driving over 90 mph\nSep 2020 - Female historians and male nurses do not exist, Google Translate tells its European users\nSep 2020 - Twitter is looking into why its photo preview appears to favor white faces over Black faces\nSep 2020 - Facebook Live\u2019s New Music Terms of Service Unfairly Impact Artists\nSep 2020 - CoreLogic\u2019s screening algorithm may have discriminated against renters: lawsuit\nSep 2020 - Gradient Photo Editing App Criticized Over 'Racist' AI Face Feature\nSep 2020 - ExamSoft\u2019s remote bar exam sparks privacy and facial recognition concerns\nSep 2020 - \"Trustworthiness\" Study Is Basically Phrenology, Annoying Scientists, Historians, Just About Everyone\nSep 2020 - IBM faces another age-discrimination lawsuit in Austin\nSep 2020 - Your favorite A.I. language tool is toxic\nSep 2020 - Catching Amazon in a lie\nSep 2020 - Tweet: \"A faculty member has been asking how to stop Zoom from removing his head ...\" (Zoom erasing darker-skinned professor's head)\nSep 2020 - Whistleblowers charge CEO of NJ firm with inflating AI capability, calling employees \u201cdirty Indians\u201d\nOct 2020 - Jewish Baby Stroller Image Algorithm\nOct 2020 - Instagram blames GDPR for failure to tackle rampant self-harm and eating-disorder images\nOct 2020 - UK passport photo checker shows bias against dark-skinned women\nOct 2020 - States Say the Online Bar Exam Was a Success. The Test-Taker Who Peed in His Seat Disagrees\nOct 2020 - Tiny Changes Let False Claims About COVID-19, Voting Evade Facebook Fact Checks\nOct 2020 - Leaving Cert: Why the Government deserves an F for algorithms\nOct 2020 - Lawsuit alleges biometric privacy violations from face recognition algorithm training\nOct 2020 - You\u2019re being watched: The dangers of ProctorU\nOct 2020 - Fake naked photos of thousands of women shared online\nOct 2020 - Researchers find evidence of racial, gender, and socioeconomic bias in chest X-ray classifiers\nOct 2020 - Uber sued by drivers over \u2018automated robo-firing'\nOct 2020 - How an Algorithm Blocked Kidney Transplants to Black Patients\nOct 2020 - Australian researchers have shown how you could become invisible to security cameras\nOct 2020 - EPIC files lawsuit to force release of ICE facial recognition documents\nOct 2020 - Researchers take a stand on algorithm design for job centers: Landing a job isn't always the right goal\nOct 2020 - Facebook under fire for boosting right-wing news sources and throttling progressive alternatives\nOct 2020 - AI Camera Ruins Soccer Game For Fans After Mistaking Referee's Bald Head For Ball\nOct 2020 - Researchers made an OpenAI GPT-3 medical chatbot as an experiment. It told a mock patient to kill themselves\nOct 2020 - Top doctors slam Google for not backing up incredible claims of super-human cancer-spotting AI\nNov 2020 - Researchers show that computer vision algorithms pretrained on ImageNet exhibit multiple, distressing biases\nNov 2020 - Trivago loses appeal over misleading website algorithm ruling\nNov 2020 - Research finds gender bias within state funding model\nNov 2020 - Split-Second 'Phantom' Images Can Fool Tesla's Autopilot\nNov 2020 - Boris executes U-turn over controversial house building algorithm\nNov 2020 - Top intel official warns of bias in military algorithms\nNov 2020 - Opinion: Artificial 'Intelligence': Unemployment system denied legitimate COVID-19 claims\nNov 2020 - LAPD ban facial recognition following alleged unauthorised use\nNov 2020 - Instagram removed 80 PER CENT less graphic content about suicide during the first three months of lockdown after 'most of its moderators were sent home due to Covid rules'\nNov 2020 - Facebook's AI Mistakenly Bans Ads for Struggling Businesses\nNov 2020 - A Bot Made Frank Sinatra Cover Britney Spears. YouTube Removed It Over Copyright Claims.\nNov 2020 - Net exposure \"94-year-old man was picked up for facial recognition\" The bank involved apologized\nNov 2020 - Walmart Scraps Plan to Have Robots Scan Shelves\nDec 2020 - Concern over potential gender bias in job recruitment algorithms\nDec 2020 - Facial Recognition Company Lied to School District About its Racist Tech\nDec 2020 - China\u2019s Huawei tested A.I. software that could identify Uighur Muslims and alert police, report says\nDec 2020 - We\u2019ve Known Brand Safety Tech Was Bad\u2014Here\u2019s How Bad\nDec 2020 - Hey Alexa, what's my PIN?\nDec 2020 - Waze sent commuters toward California wildfires, drivers say\nDec 2020 - The Death and Life of an Admissions Algorithm\nDec 2020 - Algorithms searching for child abuse could be banned under new EU privacy rules\nDec 2020 - Alibaba \u2018dismayed\u2019 by its cloud unit\u2019s ethnicity detection algorithm\nDec 2020 - Congress wants answers from Google about Timnit Gebru\u2019s firing\nDec 2020 - California Bar Exam Flagged A THIRD Of Applicants As Cheating\nDec 2020 - TikTok videos that promote anorexia are misspelling common hashtags to beat the 'pro-ana' ban\nDec 2020 - Facial Recognition Blamed For False Arrest And Jail Time\nDec 2020 - Girl, 12, is suing social media giant TikTok for alleged misuse of personal information and breaches of data protection laws\nDec 2020 - TikTok Deleted My Account Because I\u2019m a Latina Trans Woman\nDec 2020 - Shopping mall robot fell off the escalator and knocked down passengers\nDec 2020 - Stanford apologizes for coronavirus vaccine plan that left out many front-line doctors\nDec 2020 - The Christchurch Shooter and YouTube\u2019s Radicalization Trap\nJan 2021 - Italian court rules against \u2018discriminatory\u2019 Deliveroo rider-ranking algorithm\nJan 2021 - A business owner who spent nearly $46 million on Facebook advertising says he's been booted from the platform without explanation\nJan 2021 - FTC Orders Photo App to Delete Algorithms Built on Personal Data\nJan 2021 - South Korean AI chatbot pulled from Facebook after hate speech towards minorities\nJan 2021 - Google Hit With $2B Antitrust Suit Over 'Rigging' Its Algorithm\nJan 2021 - Judge Orders NJ Education Department To Turn Over S2 Algorithm\nJan 2021 - When an Israeli Farmer Declared War on an Algorithm\nJan 2021 - Job Screening Service Halts Facial Analysis of Applicants\nJan 2021 - Use of facial recognition tech sparks privacy fears\nJan 2021 - South Korea has used AI to bring a dead superstar's voice back to the stage, but ethical concerns abound\nJan 2021 - SEC Orders BlueCrest to Pay $170 Million to Harmed Fund Investors\nJan 2021 - University of Illinois to Discontinue Remote-Testing Software After Students Complain of Privacy Violation\nJan 2021 - Amazon algorithms boost vaccine misinformation, says study\nJan 2021 - Patent applications listing AI as an inventor run into legal problems\nJan 2021 - BIPOC students face disadvantages with exam monitoring software at the University of Toronto\nJan 2021 - \u2018for Some Reason I\u2019m Covered in Blood\u2019: Gpt-3 Contains Disturbing Bias Against Muslims\nFeb 2021 - Utah audit of Banjo deal highlights concerns with AI, government contracts\nFeb 2021 - Lingerie company Adore Me calls out TikTok for removing videos of Black, plus-size models\nFeb 2021 - \u2018Orwellian\u2019 AI lie detector project challenged in EU court\nFeb 2021 - Clearview AI\u2019s facial recognition technology violated federal and regional laws \u2013 RCI\nFeb 2021 - Beverly Hills cops try to weaponize Instagram\u2019s algorithms in failed attempt to thwart live streamers\nFeb 2021 - AI displays bias and inflexibility in civility detection, study finds\nFeb 2021 - Why Is Facebook Rejecting These Fashion Ads?\nFeb 2021 - Sweden\u2019s data watchdog slaps police for unlawful use of Clearview AI\nFeb 2021 - AI-Wielding Hackers are Here\nFeb 2021 - How Google Scholar Sidelines Research in Non\u2011English Languages\nFeb 2021 - DWP uses excessive surveillance on suspected fraudsters, report finds\nFeb 2021 - Canada Rules Clearview\u2019s AI Scraping is Unlawful\nFeb 2021 - INVESTIGATION: Facebook, Twitter Struggling in Fight against Balkan Content Violations\nFeb 2021 - Google slapped in France over misleading hotel star ratings\nFeb 2021 - Colleagues of mine analyzed A.I.-based job interviews ... (Tweet)\nFeb 2021 - YouTuber blocked for discussing 'black versus white' chess strategy\nFeb 2021 - Teaneck just banned facial recognition technology for police. Here's why\nFeb 2021 - TikTok agrees to pay $92 million to settle teen privacy class-action lawsuit\nFeb 2021 - Google fires top ethical AI expert Margaret Mitchell\nMar 2021 - UP Uses Facial Recognition Technology to Mete Out Discriminatory Treatment\nMar 2021 - Chatbots that resurrect the dead: legal experts weigh in on \u2018disturbing\u2019 technology\nMar 2021 - \u201cIt\u2019s all the real thing,\u201d Tom Cruise insists, looking into the camera ...\nMar 2021 - OpenAI\u2019s state-of-the-art machine vision AI is fooled by handwritten notes\nMar 2021 - Major Universities are Using Race as a \u201cHigh Impact Predictor\u201d of Student Success \u2013 The Markup\nMar 2021 - Instagram Suggested Posts To Users. It Served Up COVID-19 Falsehoods, Study Finds\nMar 2021 - Tenant screening software faces national reckoning\nMar 2021 - Instagram algorithm recommends far-right parties and Covid conspiracy theories to users\nMar 2021 - Google image search cements national stereotypes of 'racy' women\nMar 2021 - Time-Out for Google\nMar 2021 - Apple Censors URLs Containing \u201cAsian\u201d with Adult Filters\nMar 2021 - Underpaid Workers Are Being Forced to Train Biased AI on Mechanical Turk\nMar 2021 - New Study Reveals Coded Language Used to Fuel Anti-Semitism Online\nMar 2021 - Judge tells state to deliver records\nMar 2021 - Pennsylvania Woman Accused of Using Deepfake Technology to Harass Cheerleaders\nMar 2021 - Fears of 'digital dictatorship' as Myanmar deploys artificial intelligence\nMar 2021 - Amazon driver quits, saying the final straw was the company's new AI-powered truck cameras that can sense when workers yawn or don't use a seatbelt\nMar 2021 - INSTA-KID Fury over Facebook plot to make NEW Instagram for under 13s \u2013 as parents brand it \u2018dangerous\u2019\nMar 2021 - How AI lets bigots and trolls flourish while censoring LGBTQ+ voices\nMar 2021 - Music recommendation algorithms are unfair to female artists, but we can change that\nMar 2021 - Couriers say Uber\u2019s \u2018racist\u2019 facial identification tech got them fired\nMar 2021 - Major flaws found in machine learning for COVID-19 diagnosis\nMar 2021 - How a Stabbing in Israel Echoes Through the Fight Over Online Speech\nApr 2021 - Researchers have found that even the best Speech recognition systems are actually biased\nApr 2021 - Research says Facebook's ad algorithm perpetuates gender bias (see also Research Outputs from Auditing for Discrimination in Job Ad Delivery on the USC Information Sciences Institute web site)\nApr 2021 - Google AI chief Samy Bengio resigns over colleagues' firing and racial discrimination\nApr 2021 - How medicine discriminates against non-white people and women\nApr 2021 - In scramble to respond to Covid-19, hospitals turned to models with high risk of bias\nApr 2021 - Home Office algorithm to detect sham marriages may contain built-in discrimination\nApr 2021 - Google translation AI botches legal terms 'enjoin,' 'garnish' -research\nApr 2021 - Some FDA-approved AI medical devices are not \u2018adequately\u2019 evaluated, Stanford study says\nApr 2021 - Instagram apologises for mistake which targeted users with harmful diet content\nApr 2021 - Facebook, Princeton Must Face AI Data Theft Claims\nApr 2021 - Facebook sued for failing to remove anti-Muslim hate speech\nApr 2021 - Post Office scandal: What the Horizon saga is all about\nApr 2021 - Facebook, Twitter, YouTube are pressed on \u2018poisonous\u2019 algorithms\nApr 2021 - BLACK MAN USES PASSPORT PHOTO AS EVIDENCE AI IS \u2018RACIST\u2019 IN VIRAL TIKTOK\nApr 2021 - Twitter allows \u2018Uncle Tim\u2019 to trend for hours after Sen. Tim Scott\u2019s rebuttal, and then took action\nApr 2021 - Suicide Risk Prediction Models Could Perpetuate Racial Disparities\nMay 2021 - Amsterdam Court orders reinstatement of Uber drivers dismissed by algorithm\nMay 2021 - This facial recognition website can turn anyone into a cop \u2014 or a stalker\nMay 2021 - Why you should be very wary of AI that \u2018processes\u2019 college video applications\nMay 2021 - Airbnb pricing algorithm led to increased racial disparities, study finds\nMay 2021 - Uber commits crime using algorithms.\nMay 2021 - Deepfake detectors and datasets exhibit racial and gender bias, USC study shows\nMay 2021 - TikTok\u2019s recommendation algorithm is promoting homophobia and anti-trans violence\nMay 2021 - \u2018Grassroots\u2019 bot campaigns are coming. Governments don\u2019t have a plan to stop them\nMay 2021 - Workplace and algorithm bias kill Palestine content on Facebook and Twitter\nMay 2021 - Suit seeks to limit anti-Muslim speech on Facebook but roots of Islamophobia run far deeper\nMay 2021 - AI emotion-detection software tested on Uyghurs\nMay 2021 - An Insurance Startup Bragged It Uses AI to Detect Fraud. It Didn\u2019t Go Well\nMay 2021 - Google's new AI skincare tool may not work on patients with darker skin tones\nMay 2021 - Minn. Police Use of Facial Recognition Leads to Concerns\nMay 2021 - Facial recognition: Legal complaints lodged against Clearview AI in five countries\nJun 2021 - A Military Drone With A Mind Of Its Own Was Used In Combat, U.N. Says\nJun 2021 - Senate Democrats Urge Google To Investigate Racial Bias In Its Tools And The Company\nJun 2021 - McDonald\u2019s Taking Voiceprints at Drive-Throughs Illinois BIPA Class Action\nJun 2021 - Legal notice to Hyderabad Police Commissioner highlights lack of lawfulness of facial recognition measures\nJun 2021 - ATER ALERT: The Klein Law Firm Announces a Lead Plaintiff Deadline of July 12, 2021 in the Class Action Filed on Behalf of Aterian, Inc. Limited Shareholders\nJun 2021 - Have Google\u2019s Algorithm Updates Broken the Web?\nJun 2021 - How Airbnb failed its own anti-discrimination team\u2014and let racial disparities slip through the cracks\nJun 2021 - Facial Recognition Failures Are Locking People Out of Unemployment Systems",
      "link": "https://github.com/jphall663/awesome-machine-learning-interpretability"
    },
    {
      "autor": "papers",
      "date": "NaN",
      "content": "About\nThis repository contains short summaries of some machine learning papers.\nAdded 2018/10/01:\nUNSUPERVISED LEARNING ECCV 2018 Deep Clustering for Unsupervised Learning of Visual Features\nOBJECT DETECTION POINT CLOUD SELF-DRIVING CARS ECCV 2018 Deep Continuous Fusion for Multi-Sensor 3D Object Detection\nAUDIO SOUND SOURCE LOCALIZATION ACTION RECOGNITION SOUND SOURCE SEPARATION SELF-SUPERVISED ECCV 2018 Audio-Visual Scene Analysis with Self-Supervised Multisensory Features\nUNCERTAINTY ECCV 2018 Towards Realistic Predictors\nOBJECT DETECTION ECCV 2018 Acquisition of Localization Confidence for Accurate Object Detection\nOBJECT DETECTION ECCV 2018 CornerNet: Detecting Objects as Paired Keypoints\nNORMALIZATION ECCV 2018 Group Normalization\nARCHITECTURES ATTENTION ECCV 2018 Convolutional Networks with Adaptive Inference Graphs\nAdded 2018/03/08:\nARCHITECTURES ATTENTION Spatial Transformer Networks (thanks, alexobednikov)\nAdded 2018/03/06:\nLOSS FUNCTIONS RECOGNITION Working hard to know your neighbor\u2019s margins: Local descriptor learning loss (thanks, alexobednikov)\nAdded 2017/12/13:\nFACE RECOGNITION FACES Neural Aggregation Network for Video Face Recognition (thanks, alexobednikov)\nAdded 2017/12/03:\nCritical Learning Periods in Deep Neural Networks\nGAN SELF-DRIVING CARS High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs\nSELF-DRIVING CARS Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art\nAdded 2017/10/28:\nGAN Progressive Growing of GANs for Improved Quality, Stability, and Variation\nAdded 2017/10/24:\nSELF-DRIVING CARS Systematic Testing of Convolutional Neural Networks for Autonomous Driving\nSELF-DRIVING CARS SEGMENTATION Fast Scene Understanding for Autonomous Driving\nSELF-DRIVING CARS Arguing Machines: Perception-Control System Redundancy and Edge Case Discovery in Real-World Autonomous Driving\nSELF-DRIVING CARS GAN REINFORCEMENT Virtual to Real Reinforcement Learning for Autonomous Driving\nSELF-DRIVING CARS End to End Learning for Self-Driving Cars\nAdded 2017/10/21:\nSnapshot Ensembles: Train 1, get M for free\nImage Crowd Counting Using Convolutional Neural Network and Markov Random Field\nREINFORCEMENT Rainbow: Combining Improvements in Deep Reinforcement Learning\nREINFORCEMENT Learning to Navigate in Complex Environments\nGAN Unsupervised Image-to-Image Translation Networks\nRNN Dilated Recurrent Neural Networks\nOBJECT DETECTION TRACKING Detect to Track and Track to Detect\nARCHITECTURES Dilated Residual Networks\nAdded 2017/09/24:\nOBJECT DETECTION Feature Pyramid Networks for Object Detection\nOBJECT DETECTION SSD: Single Shot MultiBox Detector\nOBJECT DETECTION EFFICIENT NETWORKS MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\nOBJECT DETECTION Mask R-CNN\nAdded 2017/08/08:\nFACES Multi-view Face Detection Using Deep Convolutional Neural Networks (aka DDFD) (thanks, arnaldog12)\nAdded 2017/06/11:\nGAN On the Effects of Batch and Weight Normalization in Generative Adversarial Networks\nGAN BEGAN\nGAN StackGAN: Text to -----> Photo !!! -realistic Image Synthesis with Stacked Generative Adversarial Networks\nACTIVATION FUNCTIONS Self-Normalizing Neural Networks\nGAN Wasserstein GAN (aka WGAN)\nAdded 2017/03/15:\nOBJECT DETECTION YOLO9000: Better, Faster, Stronger (aka YOLOv2)\nOBJECT DETECTION You Only Look Once: Unified, Real-Time Object Detection (aka YOLO)\nOBJECT DETECTION PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection\nAdded 2017/03/14:\nOBJECT DETECTION R-FCN: Object Detection via Region-based Fully Convolutional Networks\nOBJECT DETECTION Faster R-CNN\nOBJECT DETECTION Fast R-CNN\nOBJECT DETECTION Rich feature hierarchies for accurate object detection and semantic segmentation (aka R-CNN)\nPEDESTRIANS Ten Years of Pedestrian Detection, What Have We Learned?\nNEURAL STYLE Instance Normalization: The Missing Ingredient for Fast Stylization\nAdded 2016/07/29:\nHUMAN POSE ESTIMATION Stacked Hourglass Networks for Human Pose Estimation\nFACES DeepFace: Closing the Gap to Human-Level Performance in Face Verification\nTRANSLATION Character-based Neural Machine Translation\nAdded 2016/07/01:\nHUMAN POSE ESTIMATION Convolutional Pose Machines\nFACES HyperFace: A Deep Multi-task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition\nFACES Face Attribute Prediction Using Off-the-Shelf CNN Features\nFACES CMS-RCNN: Contextual Multi-Scale Region-based CNN for Unconstrained Face Detection\nConditional Image Generation with PixelCNN Decoders\nGAN InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets\nGAN Improved Techniques for Training GANs\nSynthesizing the preferred inputs for neurons in neural networks via deep generator networks\nAdded 2016/06/06:\nARCHITECTURES FractalNet: Ultra-Deep Neural Networks without Residuals\nPlaNet - Photo Geolocation with Convolutional Neural Networks\nOPTIMIZERS Adam: A Method for Stochastic Optimization\nGAN RNN Generating images with recurrent adversarial networks\nGAN Adversarially Learned Inference\nAdded 2016/06/02:\nARCHITECTURES Resnet in Resnet: Generalizing Residual Architectures\nAUTOENCODERS Rank Ordered Autoencoders\nARCHITECTURES Wide Residual Networks\nARCHITECTURES Identity Mappings in Deep Residual Networks\nREGULARIZATION Swapout: Learning an ensemble of deep architectures\nMulti-Scale Context Aggregation by Dilated Convolutions\nTexture Synthesis Through Convolutional Neural Networks and Spectrum Constraints\nPrecomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks\nAdded 2016/05/15:\nNEURAL STYLE Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artwork\nCombining Markov Random Fields and Convolutional Neural Networks for Image Synthesis\nSUPERRESOLUTION Accurate Image Super-Resolution Using Very Deep Convolutional Networks\nHUMAN POSE ESTIMATION Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation\nREINFORCEMENT Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation\nCOLORIZATION Let there be Color\nAdded 2016/05/08:\nNEURAL STYLE Artistic Style Transfer for Videos\nAdded 2016/05/03:\nREINFORCEMENT Playing Atari with Deep Reinforcement Learning\nGENERATIVE Attend, Infer, Repeat: Fast Scene Understanding with Generative Models\nARCHITECTURES EFFICIENT NETWORKS SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size\nACTIVATION FUNCTIONS Noisy Activation Functions\nOBJECT DETECTION IMAGE TO TEXT DenseCap: Fully Convolutional Localization Networks for Dense Captioning\nAdded 2016/04/01:\nREGULARIZATION Deep Networks with Stochastic Depth\nAdded 2016/03/31:\nGAN Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks\nGENERATIVE RNN ATTENTION DRAW A Recurrent Neural Network for Image Generation\nGenerating Images with Perceptual Similarity Metrics based on Deep Networks\nGENERATIVE Generative Moment Matching Networks\nGENERATIVE RNN Pixel Recurrent Neural Networks\nGAN Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\nAdded 2016/03/??:\nNEURAL STYLE A Neural Algorithm for Artistic Style\nNORMALIZATION REGULARIZATION Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\nARCHITECTURES Deep Residual Learning for Image Recognition\nACTIVATION FUNCTIONS Fast and Accurate Deep Networks Learning By Exponential Linear Units (ELUs)\nFractional Max-Pooling\nGAN Generative Adversarial Networks\nARCHITECTURES Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning\nNORMALIZATION Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks",
      "link": "https://github.com/aleju/papers"
    },
    {
      "autor": "Best_AI_paper_2020",
      "date": "NaN",
      "content": "2020: A Year Full of Amazing AI papers- A Review\nA curated list of the latest breakthroughs in AI by release date with a clear video explanation, link to a more in-depth article, and code\nEven with everything that happened in the world this year, we still had the chance to see a lot of amazing research come out. Especially in the field of artificial intelligence. More, many important aspects were highlighted this year, like the ethical aspects, important biases, and much more. Artificial intelligence and our understanding of the human brain and its link to AI is constantly evolving, showing promising applications in the soon future.\nHere are the most interesting research papers of the year, in case you missed any of them. In short, it is basically a curated list of the latest breakthroughs in AI and Data Science by release date with a clear video explanation, link to a more in-depth article, and code (if applicable). Enjoy the read!\nThe complete reference to each paper is listed at the end of this repository.\nMaintainer - louisfb01\nSubscribe to my newsletter - The latest updates in AI explained every week.\n\ud83c\udd95 Check the 2021 repo!\nFeel free to message me any great papers I missed to add to this repository on bouchard.lf@gmail.com\nTag me on Twitter @Whats_AI or LinkedIn @Louis (What's AI) Bouchard if you share the list!\nWatch a complete 2020 rewind in 15 minutes\nIf you are interested in computer vision research, here is another great repository for you:\nThe top 10 computer vision papers in 2020 with video demos, articles, code, and paper reference.\nTop 10 Computer Vision Papers 2020\nThe Full List\nYOLOv4: Optimal Speed and Accuracy of Object Detection [1]\nDeepFaceDrawing: Deep Generation of Face Images from Sketches [2]\nLearning to Simulate Dynamic Environments with GameGAN [3]\nPULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models [4]\nUnsupervised Translation of Programming Languages [5]\nPIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization [6]\nHigh-Resolution Neural Face Swapping for Visual Effects [7]\nSwapping Autoencoder for Deep Image Manipulation [8]\nGPT-3: Language Models are Few-Shot Learners [9]\nLearning Joint Spatial-Temporal Transformations for Video Inpainting [10]\nImage GPT - Generative Pretraining from Pixels [11]\nLearning to Cartoonize Using White-box Cartoon Representations [12]\nFreezeG: Freeze the Discriminator: a Simple Baseline for Fine-Tuning GANs [13]\nNeural Re-Rendering of Humans from a Single Image [14]\nI2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image [15]\nBeyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments [16]\nRAFT: Recurrent All-Pairs Field Transforms for Optical Flow [17]\nCrowdsampling the Plenoptic Function [18]\nOld Photo Restoration via Deep Latent Space Translation [19]\nNeural circuit policies enabling auditable autonomy [20]\nLifespan Age Transformation Synthesis [21]\nDeOldify [22]\nCOOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning [23]\nStylized Neural Painting [24]\nIs a Green Screen Really Necessary for Real-Time Portrait Matting? [25]\nADA: Training Generative Adversarial Networks with Limited Data [26]\nImproving Data\u2010Driven Global Weather Prediction Using Deep Convolutional Neural Networks on a Cubed Sphere [27]\nNeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis [28]\nPaper references\nYOLOv4: Optimal Speed and Accuracy of Object Detection [1]\nThis 4th version has been recently introduced in April 2020 by Alexey Bochkovsky et al. in the paper \"YOLOv4: Optimal Speed and Accuracy of Object Detection\". The main goal of this algorithm was to make a super-fast object detector with high quality in terms of accuracy.\nShort Video Explanation:\nThe YOLOv4 algorithm | Introduction to You Only Look Once, Version 4 | Real-Time Object Detection - Short Read\nYOLOv4: Optimal Speed and Accuracy of Object Detection - The Paper\nClick here for the Yolo v4 code - The Code\nDeepFaceDrawing: Deep Generation of Face Images from Sketches [2]\nYou can now generate high-quality face images from rough or even incomplete sketches with zero drawing skills using this new image-to-image translation technique! If your drawing skills as bad as mine you can even adjust how much the eyes, mouth, and nose will affect the final image! Let's see if it really works and how they did it.\nShort Video Explanation:\nAI Generates Real Faces From Sketches! - Short Read\nDeepFaceDrawing: Deep Generation of Face Images from Sketches - The Paper\nClick here for the DeepFaceDrawing code - The Code\nLearning to Simulate Dynamic Environments with GameGAN [3]\nGameGAN, a generative adversarial network trained on 50,000 PAC-MAN episodes, produces a fully functional version of the dot-munching classic without an underlying game engine.\nShort Video Explanation:\n40 Years on, PAC-MAN Recreated with AI by NVIDIA Researchers - Short Read\nLearning to Simulate Dynamic Environments with GameGAN - The Paper\nClick here for the GameGAN code - The Code\nPULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models [4]\nThis new algorithm transforms a blurry image into a high-resolution image! It can take a super low-resolution 16x16 image and turn it into a 1080p high definition human face! You don't believe me? Then you can do just like me and try it on yourself in less than a minute! But first, let's see how they did that.\nShort Video Explanation:\nThis AI makes blurry faces look 60 times sharper - Short Read\nPULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models - The Paper\nClick here for the PULSE code - The Code\nUnsupervised Translation of Programming Languages [5]\nThis new model converts code from a programming language to another without any supervision! It can take a Python function and translate it into a C++ function, and vice-versa, without any prior examples! It understands the syntax of each language and can thus generalize to any programming language! Let's see how they did that.\nShort Video Explanation:\nThis AI translates code from a programming language to another | Facebook TransCoder Explained - Short Read\nUnsupervised Translation of Programming Languages - The Paper\nClick here for the Transcoder code - The Code\nPIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization [6]\nThis AI Generates 3D high-resolution reconstructions of people from 2D images! It only needs a single image of you to generate a 3D avatar that looks just like you, even from the back!\nShort Video Explanation:\nAI Generates 3D high-resolution reconstructions of people from 2D images | Introduction to PIFuHD - Short Read\nPIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization - The Paper\nClick here for the PiFuHD code - The Code\nHigh-Resolution Neural Face Swapping for Visual Effects [7]\nResearchers at Disney developed a new High-Resolution Face Swapping algorithm for Visual Effects in the paper of the same name. It is capable of rendering -----> photo !!! -realistic results at megapixel resolution. Working for Disney, they are most certainly the best team for this work. Their goal is to swap the face of a target actor from a source actor while maintaining the actor's performance. This is incredibly challenging and is useful in many circumstances, such as changing the age of a character, when an actor is not available, or even when it involves a stunt scene that would be too dangerous for the main actor to perform. The current approaches require a lot of frame-by-frame animation and post-processing by professionals.\nShort Video Explanation:\nDisney's New High-Resolution Face Swapping Algorithm | New 2020 Face Swap Technology Explained - Short Read\nHigh-Resolution Neural Face Swapping for Visual Effects - The Paper\nSwapping Autoencoder for Deep Image Manipulation [8]\nThis new technique can change the texture of any picture while staying realistic using complete unsupervised training! The results look even better than what GANs can achieve while being way faster! It could even be used to create deepfakes!\nShort Video Explanation:\nTexture-Swapping AI beats GANs for Image Manipulation! - Short Read\nSwapping Autoencoder for Deep Image Manipulation - The Paper\nClick here for the Swapping autoencoder code - The Code\nGPT-3: Language Models are Few-Shot Learners [9]\nThe current state-of-the-art NLP systems struggle to generalize to work on different tasks. They need to be fine-tuned on datasets of thousands of examples while humans only need to see a few examples to perform a new language task. This was the goal behind GPT-3, to improve the task-agnostic characteristic of language models.\nShort Video Explanation:\nCan GPT-3 Really Help You and Your Company? - Short Read\nLanguage Models are Few-Shot Learners - The Paper\nClick here for GPT-3's GitHub page - The GitHub\nLearning Joint Spatial-Temporal Transformations for Video Inpainting [10]\nThis AI can fill the missing pixels behind a removed moving object and reconstruct the whole video with way more accuracy and less blurriness than current state-of-the-art approaches!\nShort Video Explanation:\nThis AI takes a video and fills the missing pixels behind an object! - Short Read\nLearning Joint Spatial-Temporal Transformations for Video Inpainting - The Paper\nClick here for this Video Inpainting code - The Code\nImage GPT - Generative Pretraining from Pixels [11]\nA good AI, like the one used in Gmail, can generate coherent text and finish your phrase. This one uses the same principles in order to complete an image! All done in an unsupervised training with no labels required at all!\nShort Video Explanation:\nThis AI Can Generate the Other Half of a Picture Using a GPT Model - Short Read\nImage GPT - Generative Pretraining from Pixels - The Paper\nClick here for the OpenAI's Image GPT code - The Code\nLearning to Cartoonize Using White-box Cartoon Representations [12]\nThis AI can cartoonize any picture or video you feed it in the cartoon style you want! Let's see how it does that and some amazing examples. You can even try it yourself on the website they created as I did for myself!\nShort Video Explanation:\nThis AI can cartoonize any picture or video you feed it! Paper Introduction & Results examples - Short Read\nLearning to Cartoonize Using White-box Cartoon Representations - The Paper\nClick here for the Cartoonize code - The Code\nFreezeG: Freeze the Discriminator: a Simple Baseline for Fine-Tuning GANs [13]\nThis face generating model is able to transfer normal face photographs into distinctive styles such as Lee Mal-Nyeon's cartoon style, the Simpsons, arts, and even dogs! The best thing about this new technique is that it's super simple and significantly outperforms previous techniques used in GANs.\nShort Video Explanation:\nThis Face Generating Model Transfers Real Face Photographs Into Distinctive Cartoon Styles - Short Read\nFreeze the Discriminator: a Simple Baseline for Fine-Tuning GANs - The Paper\nClick here for the FreezeG code - The Code\nNeural Re-Rendering of Humans from a Single Image [14]\nThe algorithm represents body pose and shape as a parametric mesh which can be reconstructed from a single image and easily reposed. Given an image of a person, they are able to create synthetic images of the person in different poses or with different clothing obtained from another input image.\nShort Video Explanation:\nTransfer clothes between photos using AI. From a single image! - Short Read\nNeural Re-Rendering of Humans from a Single Image - The Paper\nI2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image [15]\nTheir goal was to propose a new technique for 3D Human Pose and Mesh Estimation from a single RGB image. They called it I2L-MeshNet. Where I2L stands for Image-to-Lixel. Just like a voxel, volume + pixel, is a quantized cell in three-dimensional space, they defined lixel, a line, and pixel, as a quantized cell in one-dimensional space. Their method outperforms previous methods and the code is publicly available!\nShort Video Explanation:\nAccurate 3D Human Pose and Mesh Estimation from a Single RGB Image! With Code Publicly Avaibable! - Short Read\nI2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image - The Paper\nClick here for the I2L-MeshNet code\nhttps://github.com/mks0601/I2L-MeshNet_RELEASE\nBeyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments [16]\nLanguage-guided navigation is a widely studied field and a very complex one. Indeed, it may seem simple for a human to just walk through a house to get to your coffee that you left on your nightstand to the left of your bed. But it is a whole other story for an agent, which is an autonomous AI-driven system using deep learning to perform tasks.\nShort Video Explanation:\nLanguage-Guided Navigation in a 3D Environment - Short Read\nBeyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments - The Paper\nClick here for the VLN-CE code - The Code\nRAFT: Recurrent All-Pairs Field Transforms for Optical Flow [17]\nECCV 2020 Best Paper Award Goes to Princeton Team. They developed a new end-to-end trainable model for optical flow. Their method beats state-of-the-art architectures' accuracy across multiple datasets and is way more efficient. They even made the code available for everyone on their Github!\nShort Video Explanation:\nECCV 2020 Best Paper Award | A New Architecture For Optical Flow - Short Read\nRAFT: Recurrent All-Pairs Field Transforms for Optical Flow - The Paper\nClick here for the RAFT code - The Code\nCrowdsampling the Plenoptic Function [18]\nUsing tourists' public photos from the internet, they were able to reconstruct multiple viewpoints of a scene conserving the realistic shadows and lighting! This is a huge advancement of the state-of-the-art techniques for photorealistic scene rendering and their results are simply amazing.\nShort Video Explanation:\nReconstruct Photorealistic Scenes from Tourists' Public Photos on the Internet! - Short Read\nCrowdsampling the Plenoptic Function - The Paper\nClick here for the Crowdsampling code - The Code\nOld Photo Restoration via Deep Latent Space Translation [19]\nImagine having the old, folded, and even torn pictures of your grandmother when she was 18 years old in high definition with zero artifacts. This is called old photo restoration and this paper just opened a whole new avenue to address this problem using a deep learning approach.\nShort Video Explanation:\nOld Photo Restoration using Deep Learning - Short Read\nOld Photo Restoration via Deep Latent Space Translation - The Paper\nClick here for the Old Photo Restoration code - The Code\nNeural circuit policies enabling auditable autonomy [20]\nResearchers from IST Austria and MIT have successfully trained a self-driving car using a new artificial intelligence system based on the brains of tiny animals, such as threadworms. They achieved that with only a few neurons able to control the self-driving car, compared to the millions of neurons needed by the popular deep neural networks such as Inceptions, Resnets, or VGG. Their network was able to completely control a car using only 75 000 parameters, composed of 19 control neurons, rather than millions!\nShort Video Explanation:\nA New Brain-inspired Intelligent System Drives a Car Using Only 19 Control Neurons! - Short Read\nNeural circuit policies enabling auditable autonomy - The Paper\nClick here for the NCP code - The Code\nLifespan Age Transformation Synthesis [21]\nA team of researchers from Adobe Research developed a new technique for age transformation synthesis based on only one picture from the person. It can generate the lifespan pictures from any picture you sent it.\nShort Video Explanation:\nGenerate Younger & Older Versions of Yourself! - Short Read\nLifespan Age Transformation Synthesis - The Paper\nClick here for the Lifespan age transformation synthesis code - The Code\nDeOldify [22]\nDeOldify is a technique to colorize and restore old black and white images or even film footage. It was developed and is still getting updated by only one person Jason Antic. It is now the state of the art way to colorize black and white images, and everything is open-sourced, but we will get back to this in a bit.\nShort Video Explanation:\nThis AI can Colorize your Black & White Photos with Full Photorealistic Renders! (DeOldify) - Short Read\nClick here for the DeOldify code - The Code\nCOOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning [23]\nAs the name states, it uses transformers to generate accurate text descriptions for each sequence of a video, using both the video and a general description of it as inputs.\nShort Video Explanation:\nVideo to Text Description Using Deep Learning and Transformers | COOT - Short Read\nCOOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning - The Paper\nClick here for the COOT code - The Code\nStylized Neural Painting [24]\nThis Image-to-Painting Translation method simulates a real painter on multiple styles using a novel approach that does not involve any GAN architecture, unlike all the current state-of-the-art approaches!\nShort Video Explanation:\nImage-to-Painting Translation With Style Transfer - Short Read\nStylized Neural Painting - The Paper\nClick here for the Stylized Neural Painting code - The Code\nIs a Green Screen Really Necessary for Real-Time Portrait Matting? [25]\nHuman matting is an extremely interesting task where the goal is to find any human in a picture and remove the background from it. It is really hard to achieve due to the complexity of the task, having to find the person or people with the perfect contour. In this post, I review the best techniques used over the years and a novel approach published on November 29th, 2020. Many techniques are using basic computer vision algorithms to achieve this task, such as the GrabCut algorithm, which is extremely fast, but not very precise.\nShort Video Explanation:\nHigh-Quality Background Removal Without Green Screens - Short Read\nIs a Green Screen Really Necessary for Real-Time Portrait Matting? - The Paper\nClick here for the MODNet code - The Code\nADA: Training Generative Adversarial Networks with Limited Data [26]\nWith this new training method developed by NVIDIA, you can train a powerful generative model with one-tenth of the images! Making possible many applications that do not have access to so many images!\nShort Video Explanation:\nGAN Training Breakthrough for Limited Data Applications & New NVIDIA Program! NVIDIA Research - Short Read\nTraining Generative Adversarial Networks with Limited Data - The Paper\nClick here for the ADA code - The Code\nImproving Data\u2010Driven Global Weather Prediction Using Deep Convolutional Neural Networks on a Cubed Sphere [27]\nWith this new training method developed by NVIDIA, you can train a powerful generative model with one-tenth of the images! Making possible many applications that do not have access to so many images!\nShort Video Explanation:\nAI is Predicting Faster and More Accurate Weather Forecasts - Short Read\nImproving Data\u2010Driven Global Weather Prediction Using Deep Convolutional Neural Networks on a Cubed Sphere - The Paper\nClick here for the weather forecasting code - The Code\nNeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis [28]\nThis new method is able to generate a complete 3-dimensional scene and has the ability to decide the lighting of the scene. All this with very limited computation costs and amazing results compared to previous approaches.\nShort Video Explanation:\nGenerate a Complete 3D Scene Under Arbitrary Lighting Conditions from a Set of Input Images - Short Read\nNeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis - The Paper\nClick here for the NeRV code (coming soon) - The Code\n\ud83c\udd95 Check the 2021 repo!\nTag me on Twitter @Whats_AI or LinkedIn @Louis (What's AI) Bouchard if you share the list!\nPaper references\n[1] A. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao, Yolov4: Optimal speed and accuracy of object detection, 2020. arXiv:2004.10934 [cs.CV].\n[2] S.-Y. Chen, W. Su, L. Gao, S. Xia, and H. Fu, \"DeepFaceDrawing: Deep generation of face images from sketches,\" ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH2020), vol. 39, no. 4, 72:1\u201372:16, 2020.\n[3] S. W. Kim, Y. Zhou, J. Philion, A. Torralba, and S. Fidler, \"Learning to Simulate DynamicEnvironments with GameGAN,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2020.\n[4] S. Menon, A. Damian, S. Hu, N. Ravi, and C. Rudin, Pulse: Self-supervised photo upsampling via latent space exploration of generative models, 2020. arXiv:2003.03808 [cs.CV].\n[5] M.-A. Lachaux, B. Roziere, L. Chanussot, and G. Lample, Unsupervised translation of programming languages, 2020. arXiv:2006.03511 [cs.CL].\n[6] S. Saito, T. Simon, J. Saragih, and H. Joo, Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization, 2020. arXiv:2004.00452 [cs.CV].\n[7] J. Naruniec, L. Helminger, C. Schroers, and R. Weber, \"High-resolution neural face-swapping for visual effects,\" Computer Graphics Forum, vol. 39, pp. 173\u2013184, Jul. 2020.doi:10.1111/cgf.14062.\n[8] T. Park, J.-Y. Zhu, O. Wang, J. Lu, E. Shechtman, A. A. Efros, and R. Zhang,Swappingautoencoder for deep image manipulation, 2020. arXiv:2007.00653 [cs.CV].\n[9] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P.Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S.Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei,\"Language models are few-shot learners,\" 2020. arXiv:2005.14165 [cs.CL].\n[10] Y. Zeng, J. Fu, and H. Chao, Learning joint spatial-temporal transformations for video in-painting, 2020. arXiv:2007.10247 [cs.CV].\n[11] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever, \"Generative pretraining from pixels,\" in Proceedings of the 37th International Conference on Machine Learning, H. D. III and A. Singh, Eds., ser. Proceedings of Machine Learning Research, vol. 119, Virtual: PMLR, 13\u201318 Jul 2020, pp. 1691\u20131703. [Online]. Available:http://proceedings.mlr.press/v119/chen20s.html.\n[12] Xinrui Wang and Jinze Yu, \"Learning to Cartoonize Using White-box Cartoon Representations.\", IEEE Conference on Computer Vision and Pattern Recognition, June 2020.\n[13] S. Mo, M. Cho, and J. Shin, Freeze the discriminator: A simple baseline for fine-tuning gans,2020. arXiv:2002.10964 [cs.CV].\n[14] K. Sarkar, D. Mehta, W. Xu, V. Golyanik, and C. Theobalt, \"Neural re-rendering of humans from a single image,\" in European Conference on Computer Vision (ECCV), 2020.\n[15] G. Moon and K. M. Lee, \"I2l-meshnet: Image-to-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image,\" in European Conference on ComputerVision (ECCV), 2020\n[16] J. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee, \"Beyond the nav-graph: Vision-and-language navigation in continuous environments,\" 2020. arXiv:2004.02857 [cs.CV].\n[17] Z. Teed and J. Deng, Raft: Recurrent all-pairs field transforms for optical flow, 2020. arXiv:2003.12039 [cs.CV].\n[18] Z. Li, W. Xian, A. Davis, and N. Snavely, \"Crowdsampling the plenoptic function,\" inProc.European Conference on Computer Vision (ECCV), 2020.\n[19] Z. Wan, B. Zhang, D. Chen, P. Zhang, D. Chen, J. Liao, and F. Wen, Old photo restoration via deep latent space translation, 2020. arXiv:2009.07047 [cs.CV].\n[20] Lechner, M., Hasani, R., Amini, A. et al. Neural circuit policies enabling auditable autonomy. Nat Mach Intell 2, 642\u2013652 (2020). https://doi.org/10.1038/s42256-020-00237-3\n[21] R. Or-El, S. Sengupta, O. Fried, E. Shechtman, and I. Kemelmacher-Shlizerman, \"Lifespanage transformation synthesis,\" in Proceedings of the European Conference on Computer Vision(ECCV), 2020.\n[22] Jason Antic, Creator of DeOldify, https://github.com/jantic/DeOldify\n[23] S. Ging, M. Zolfaghari, H. Pirsiavash, and T. Brox, \"Coot: Cooperative hierarchical trans-former for video-text representation learning,\" in Conference on Neural Information ProcessingSystems, 2020.\n[24] Z. Zou, T. Shi, S. Qiu, Y. Yuan, and Z. Shi, Stylized neural painting, 2020. arXiv:2011.08114[cs.CV].\n[25] Z. Ke, K. Li, Y. Zhou, Q. Wu, X. Mao, Q. Yan, and R. W. Lau, \"Is a green screen really necessary for real-time portrait matting?\" ArXiv, vol. abs/2011.11961, 2020.\n[26] T. Karras, M. Aittala, J. Hellsten, S. Laine, J. Lehtinen, and T. Aila, Training generative adversarial networks with limited data, 2020. arXiv:2006.06676 [cs.CV].\n[27] J. A. Weyn, D. R. Durran, and R. Caruana, \"Improving data-driven global weather prediction using deep convolutional neural networks on a cubed sphere\", Journal of Advances in Modeling Earth Systems, vol. 12, no. 9, Sep. 2020, issn: 1942\u20132466.doi:10.1029/2020ms002109\n[28] P. P. Srinivasan, B. Deng, X. Zhang, M. Tancik, B. Mildenhall, and J. T. Barron, \"Nerv: Neural reflectance and visibility fields for relighting and view synthesis,\" in arXiv, 2020.",
      "link": "https://github.com/louisfb01/Best_AI_paper_2020"
    },
    {
      "autor": "Neural-Photo-Editor",
      "date": "NaN",
      "content": "Neural Photo Editor\nA simple interface for editing natural photos with generative neural networks.\nThis repository contains code for the paper \"Neural Photo Editing with Introspective Adversarial Networks,\" and the Associated Video.\nInstallation\nTo run the Neural Photo Editor, you will need:\nPython, likely version 2.7. You may be able to use early versions of Python2, but I'm pretty sure there's some incompatibilities with Python3 in here.\nTheano, development version.\nlasagne, development version.\nI highly recommend cuDNN as speed is key, but it is not a dependency.\nnumpy, scipy, PIL, Tkinter and tkColorChooser, but it is likely that your python distribution already has those.\nRunning the NPE\nBy default, the NPE runs on IAN_simple. This is a slimmed-down version of the IAN without MDC or RGB-Beta blocks, which runs without lag on a laptop GPU with ~1GB of memory (GT730M)\nIf you're on a Windows machine, you will want to create a .theanorc file and at least set the flag FLOATX=float32.\nIf you're on a linux machine, you can just insert THEANO_FLAGS=floatX=float32 before the command line call.\nIf you don't have cuDNN, simply change line 56 of the NPE.py file from dnn=True to dnn=False. Note that I presently only have the non-cuDNN option working for IAN_simple.\nThen, run the command:\npython NPE.py\nIf you wish to use a different model, simply edit the line with \"config path\" in the NPE.py file.\nYou can make use of any model with an inference mechanism (VAE or ALI-based GAN).\nCommands\nYou can paint the image by picking a color and painting on the image, or paint in the latent space canvas (the red and blue tiles below the image).\nThe long horizontal slider controls the magnitude of the latent brush, and the smaller horizontal slider controls the size of both the latent and the main image brush.\nYou can select different entries from the subset of the celebA validation set (included in this repository as an .npz) by typing in a number from 0-999 in the bottom left box and hitting \"infer.\"\nUse the reset button to return to the ground truth image.\nPress \"Update\" to update the ground-truth image and corresponding reconstruction with the current image. Use \"Infer\" to return to an original ground truth image from the dataset.\nUse the sample button to generate a random latent vector and corresponding image.\nUse the scroll wheel to lighten or darken an image patch (equivalent to using a pure white or pure black paintbrush). Note that this automatically returns you to sample mode, and may require hitting \"infer\" rather than \"reset\" to get back to -----> photo !!!  editing.\nTraining an IAN on celebA\nYou will need Fuel along with the 64x64 version of celebA. See here for instructions on downloading and preparing it.\nIf you wish to train a model, the IAN.py file contains the model configuration, and the train_IAN.py file contains the training code, which can be run like this:\npython train_IAN.py IAN.py\nBy default, this code will save (and overwrite!) the weights to a .npz file with the same name as the config.py file (i.e. \"IAN.py -> IAN.npz\"), and will output a jsonl log of the training with metrics recorded after every chunk.\nUse the --resume=True flag when calling to resume training a model--it will automatically pick up from the most recent epoch.\nSampling the IAN\nYou can generate a sample and reconstruction+interpolation grid with:\npython sample_IAN.py IAN.py\nNote that you will need matplotlib. to do so.\nKnown Issues/Bugs\nMy MADE layer currently only accepts hidden unit sizes that are equal to the size of the latent vector, which will present itself as a BAD_PARAM error.\nSince the MADE really only acts as an autoregressive randomizer I'm not too worried about this, but it does bear looking into.\nI messed around with the keywords for get_model, you'll need to deal with these if you wish to run any model other than IAN_simple through the editor.\nEverything is presently just dumped into a single, unorganized directory. I'll be adding folders and cleaning things up soon.\nNotes\nRemainder of the IAN experiments (including SVHN) coming soon.\nI've integrated the plat interface which makes the NPE itself independent of framework, so you should be able to run it with Blocks, TensorFlow, PyTorch, PyCaffe, what have you, by modifying the IAN class provided in models.py.\nAcknowledgments\nThis code contains lasagne layers and other goodies adopted from a number of places:\nMADE wrapped from the implementation by M. Germain et al: https://github.com/mgermain/MADE\nGaussian Sample layer from Tencia Lee's Recipe: https://github.com/Lasagne/Recipes/blob/master/examples/variational_autoencoder/variational_autoencoder.py\nMinibatch Discrimination layer from OpenAI's Improved GAN Techniques: https://github.com/openai/improved-gan\nDeconv Layer adapted from Radford's DCGAN: https://github.com/Newmu/dcgan_code\nImage-Grid Plotter adopted from AlexMLamb's Discriminative Regularization: https://github.com/vdumoulin/discgen\nMetrics_logging and checkpoints adopted from Daniel Maturana's VoxNet: https://github.com/dimatura/voxnet\nPlat interface adopted from Tom White's plat: https://github.com/dribnet/plat",
      "link": "https://github.com/ajbrock/Neural-Photo-Editor"
    },
    {
      "autor": "satellite-image-deep-learning",
      "date": "NaN",
      "content": "Introduction\nThis document lists resources for performing deep learning (DL) on satellite imagery. To a lesser extent classical Machine learning (ML, e.g. random forests) are also discussed, as are classical image processing techniques. Note there is a huge volume of academic literature published on these topics, and this repo does not seek to index them all but rather list approachable resources with published code that will benefit both the research and developer communities.\nTable of contents\nTechniques\nML best practice\nML metrics\nDatasets\nOnline platforms for analytics\nFree online compute\nState of the art engineering\nCloud providers\nDeploying models\nImage formats, data management and catalogues\nImage annotation\nPaid software\nOpen source software\nMovers and shakers on Github\nCompanies & organisations on Github\nCourses\nOnline communities\nJobs\nAbout the author\nTechniques\nThis section explores the different deep and machine learning (ML) techniques applied to common problems in satellite imagery analysis. Good background reading is Deep learning in remote sensing applications: A meta-analysis and review\nClassification\nThe classic cats vs dogs image classification task, which in the remote sensing domain is used to assign a label to an image, e.g. this is an image of a forest. The more complex case is applying multiple labels to an image. This approach of image level classification is not to be confused with pixel-level classification which is called semantic segmentation. In general, aerial images cover large geographical areas that include multiple classes of land, so treating this is as a classification problem is less common than using semantic segmentation.\nLand classification on Sentinel 2 data using a simple sklearn cluster algorithm or deep learning CNN\nLand Use Classification on Merced dataset using CNN in Keras or fastai. Also checkout Multi-label Land Cover Classification using the redesigned multi-label Merced dataset with 17 land cover classes. For alternative visualisations see this approach\nMulti-Label Classification of Satellite Photos of the Amazon Rainforest using keras or FastAI\nDetecting Informal Settlements from Satellite Imagery using fine-tuning of ResNet-50 classifier with repo\nVision Transformers Use Case: Satellite Image Classification without CNNs\nLand-Cover-Classification-using-Sentinel-2-Dataset\nLand Cover Classification of Satellite Imagery using Convolutional Neural Networks using Keras and a multi spectral dataset captured over vineyard fields of Salinas Valley, California\nDetecting deforestation from satellite images -> using FastAI and ResNet50, with repo fsdl_deforestation_detection\nNeural Network for Satellite Data Classification Using Tensorflow in Python -> A step-by-step guide for Landsat 5 multispectral data classification for binary built-up/non-built-up class prediction, with repo\nSlums mapping from pretrained CNN network on VHR (Pleiades: 0.5m) and MR (Sentinel: 10m) imagery\nComparing urban environments using satellite imagery and convolutional neural networks -> includes interesting study of the image embedding features extracted for each image on the Urban Atlas dataset. Accompanying paper\nRSI-CB -> A Large Scale Remote Sensing Image Classification Benchmark via Crowdsource Data\nNAIP_PoolDetection -> modelled as an object recognition problem, a CNN is used to identify images as being swimming pools or something else - specifically a street, rooftop, or lawn\nLand Use and Land Cover Classification using a ResNet Deep Learning Architecture -> uses fastai and the EuroSAT dataset\nVision Transformers Use Case: Satellite Image Classification without CNNs\nWaterNet -> a CNN that identifies water in satellite images\nRoad-Network-Classification -> Road network classification model using ResNet-34, road classes organic, gridiron, radial and no pattern\nScaling AI to map every school on the planet\nLandsat classification CNN tutorial with repo\nsatellite-crosswalk-classification ->\nSegmentation\nSegmentation will assign a class label to each pixel in an image. Segmentation is typically grouped into semantic or instance segmentation. In semantic segmentation objects of the same class are assigned the same label, whilst in instance segmentation each object is assigned a unique label. Read this beginner\u2019s guide to segmentation. Single class models are often trained for road or building segmentation, with multi class for land use/crop type classification. Image annotation can take long than for classification/object detection since every pixel must be annotated. Note that many articles which refer to 'hyperspectral land classification' are actually describing semantic segmentation.\nSemantic segmentation\nAlmost always performed using U-Net. For multi/hyper-spectral imagery more classical techniques may be used (e.g. k-means).\nawesome-satellite-images-segmentation\nSatellite Image Segmentation: a Workflow with U-Net is a decent intro article\nnga-deep-learning -> performs semantic segmentation on high resultion GeoTIF data using a modified U-Net & Keras, published by NASA researchers\nHow to create a DataBlock for Multispectral Satellite Image Semantic Segmentation using Fastai\nUsing a U-Net for image segmentation, blending predicted patches smoothly is a must to please the human eye -> python code to blend predicted patches smoothly\nAutomatic Detection of Landfill Using Deep Learning\nSpectralNET -> a 2D wavelet CNN for Hyperspectral Image Classification, uses Salinas Scene dataset & Keras\nFactSeg -> Foreground Activation Driven Small Object Semantic Segmentation in Large-Scale Remote Sensing Imagery (TGRS), also see FarSeg and FreeNet, implementations of research paper\nSCAttNet -> Semantic Segmentation Network with Spatial and Channel Attention Mechanism\nlaika -> The goal of this repo is to research potential sources of satellite image data and to implement various algorithms for satellite image segmentation\nPEARL -> a human-in-the-loop AI tool to drastically reduce the time required to produce an accurate Land Use/Land Cover (LULC) map, blog post, uses Microsoft Planetary Computer and ML models run locally in the browser\nunetseg -> A set of classes and CLI tools for training a semantic segmentation model based on the U-Net architecture, using Tensorflow and Keras. This implementation is tuned specifically for satellite imagery and other geospatial raster data.\nCropMask_RCNN -> Segmenting center pivot agriculture to monitor crop water use in drylands with Mask R-CNN and Landsat satellite imagery\nSemantic segmentation - multiclass classification\nLand Cover Classification with U-Net -> Satellite Image Multi-Class Semantic Segmentation Task with PyTorch Implementation of U-Net\nMulti-class semantic segmentation of satellite images using U-Net using DSTL dataset, tensorflow 1 & python 2.7. Accompanying article\nCodebase for multi class land cover classification with U-Net accompanying a masters thesis, uses Keras\nLand cover classification of Sundarbans satellite imagery using K-Nearest Neighbor(K-NNC), Support Vector Machine (SVM), and Gradient Boosting classification algorithms with Python with repo\ndubai-satellite-imagery-segmentation -> due to the small dataset, image augmentation was used\nU-Net for Semantic Segmentation on Unbalanced Aerial Imagery -> using the Dubai dataset\nCDL-Segmentation -> code for the paper: \"Deep Learning Based Land Cover and Crop Type Classification: A Comparative Study\", comparing UNet, SegNet & DeepLabv3+\nfloatingobjects -> code for the paper: TOWARDS DETECTING FLOATING OBJECTS ON A GLOBAL SCALE WITHLEARNED SPATIAL FEATURES USING SENTINEL 2. Uses U-Net & pytorch\nLoveDA -> code for the paper \"A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation\"\nSemantic segmentation - buildings, rooftops & solar panels\nSemantic Segmentation on Aerial Images using fastai uses U-Net on the Inria Aerial Image Labeling Dataset of urban settlements in Europe and the United States, and is labelled as a building and not building classes (no repo)\nRoad and Building Semantic Segmentation in Satellite Imagery uses U-Net on the Massachusetts Roads Dataset & keras\nfind-unauthorized-constructions-using-aerial------> photography !!!  -> semantic segmentation using U-Net with custom_f1 metric & Keras. The creation of the dataset is described in this article\nsemantic segmentation model to identify newly developed or flooded land using NAIP imagery provided by the Chesapeake Conservancy, training on MS Azure\nPix2Pix-for-Semantic-Segmentation-of-Satellite-Images -> using Pix2Pix GAN network to segment the building footprint from Satellite Images, uses tensorflow\nSpaceNetUnet -> Baseline model is U-net like, applied to SpaceNet Vegas data, using Keras\nBuilding footprint detection with fastai on the challenging SpaceNet7 dataset uses U-Net\nautomated-building-detection -> Input: very-high-resolution (<= 0.5 m/pixel) RGB satellite images. Output: buildings in vector format (geojson), to be used in digital map products. Built on top of robosat and robosat.pink.\nproject_sunroof_india -> Analyzed Google Satellite images to generate a report on individual house rooftop's solar power potential, uses a range of classical computer vision techniques (e.g Canny Edge Detection) to segment the roofs\nJointNet-A-Common-Neural-Network-for-Road-and-Building-Extraction\nMapping Africa\u2019s Buildings with Satellite Imagery: Google AI blog post\nDeepSolar: A Machine Learning Framework to Efficiently Construct a Solar Deployment Database in the United States -> with website, repo and dataset on kaggle, actually used a CNN for classification and segmentation is obtained by applying a threshold to the activation map\nnz_convnet -> A U-net based ConvNet for New Zealand imagery to classify building outlines\npolycnn -> End-to-End Learning of Polygons for Remote Sensing Image Classification\nspacenet_building_detection solution by motokimura using Unet\nHow to extract building footprints from satellite images using deep learning\nVec2Instance -> applied to the SpaceNet challenge AOI 2 (Vegas) building footprint dataset, tensorflow v1.12\nEarthquakeDamageDetection -> Buildings segmentation from satellite imagery and damage classification for each build, using Keras\nSemantic-segmentation repo by fuweifu-vtoo -> uses pytorch and the Massachusetts Buildings & Roads Datasets\nMask_RCNN-for-Caravans -> detect caravan footprints from OS imagery\nremote-sensing-solar-pv -> A repository for sharing progress on the automated detection of solar PV arrays in sentinel-2 remote sensing imagery\nExtracting buildings and roads from AWS Open Data using Amazon SageMaker -> uses merged RGB (SpaceNet) and LiDAR (USGS 3DEP) datasets with Unet to reproduce the winning algorithm from SpaceNet challenge 4 by XD_XD. With repo\nTF-SegNet -> AirNet is a segmentation network based on SegNet, but with some modifications\nrgb-footprint-extract -> a Semantic Segmentation Network for Urban-Scale Building Footprint Extraction Using RGB Satellite Imagery, DeepLavV3+ module with a Dilated ResNet C42 backbone\nPredicting the Solar Potential of Rooftops using Image Segmentation and Structured Data Medium article, using 20cm imagery & Unet\nRoofpedia -> an open registry of green roofs and solar roofs across the globe identified by Roofpedia through deep learning\nsolar-pv-global-inventory -> code from the Nature paper of Kruitwagen et al, used to produce a global inventory of utility-scale solar photvoltaic generating stations\nSpaceNetExploration -> A sample project demonstrating how to extract building footprints from satellite images using a semantic segmentation model. Data from the SpaceNet Challenge\nSemantic segmentation - roads\nSemantic segmentation of roads and highways using Sentinel-2 imagery (10m) super-resolved using the SENX4 model up to x4 the initial spatial resolution (2.5m) (results, no repo)\nSemantic Segmentation of roads using U-net Keras, OSM data, project summary article by student, no code\nRoad detection using semantic segmentation and albumentations for data augmention using the Massachusetts Roads Dataset, U-net & Keras\nWinning Solutions from SpaceNet Road Detection and Routing Challenge\nRoadVecNet -> Road-Network-Segmentation-and-Vectorization in keras with dataset and paper\nDetecting road and road types jupyter notebook\nawesome-deep-map -> A curated list of resources dedicated to deep learning / computer vision algorithms for mapping. The mapping problems include road network inference, building footprint extraction, etc.\nRoadTracer: Automatic Extraction of Road Networks from Aerial Images -> uses an iterative search process guided by a CNN-based decision function to derive the road network graph directly from the output of the CNN\nSemantic segmentation - vegitation & crop boundaries\n\u0421r\u043e\u0440 field boundary detection: approaches overview and main challenges - review article, no code\nkenya-crop-mask -> Annual and in-season crop mapping in Kenya - LSTM classifier to classify pixels as containing crop or not, and a multi-spectral forecaster that provides a 12 month time series given a partial input. Dataset downloaded from GEE and pytorch lightning used for training\nWhat\u2019s growing there? Identify crops from multi-spectral remote sensing data (Sentinel 2) using eo-learn for data pre-processing, cloud detection, NDVI calculation, image augmentation & fastai\nTree species classification from from airborne LiDAR and hyperspectral data using 3D convolutional neural networks accompanies research paper and uses fastai\ncrop-type-classification -> using Sentinel 1 & 2 data with a U-Net + LSTM, more features (i.e. bands) and higher resolution produced better results (article, no code)\nFind sports fields using Mask R-CNN and overlay on open-street-map\nAn LSTM to generate a crop mask for Togo\nDeepSatModels -> Code for paper \"Context-self contrastive pretraining for crop type semantic segmentation\"\nSemantic segmentation - water & floods\nUNSOAT used fastai to train a Unet to perform semantic segmentation on satellite imageries to detect water - paper + notebook, accuracy 0.97, precision 0.91, recall 0.92\nSemi-Supervised Classification and Segmentation on High Resolution Aerial Images - Solving the FloodNet problem\nFlood Detection and Analysis using UNET with Resnet-34 as the back bone uses fastai\nHouston_flooding -> labeling each pixel as either flooded or not using data from Hurricane Harvey. Dataset consisted of pre and post flood images, and a ground truth floodwater mask was created using unsupervised clustering (with DBScan) of image pixels with human cluster verification/adjustment\nml4floods -> An ecosystem of data, models and code pipelines to tackle flooding with ML\nA comprehensive guide to getting started with the ETCI Flood Detection competition -> using Sentinel1 SAR & pytorch\nMap Floodwater of SAR Imagery with SageMaker -> applied to Sentinel-1 dataset\n1st place solution for STAC Overflow: Map Floodwater from Radar Imagery hosted by Microsoft AI for Earth -> combines Unet with Catboostclassifier, taking their maxima, not the average\nSemantic segmentation - fire & burn areas\nWild Fire Detection using U-Net trained on Databricks & Keras, semantic segmentation\nA Practical Method for High-Resolution Burned Area Monitoring Using Sentinel-2 and VIIRS with code. Dataset created on Google Earth Engine, downloaded to local machine for model training using fastai. The BA-Net model used is much smaller than U-Net, resulting in lower memory requirements and a faster computation\nSemantic segmentation - glaciers\nHED-UNet -> a model for simultaneous semantic segmentation and edge detection, examples provided are glacier fronts and building footprints using the Inria Aerial Image Labeling dataset\nglacier_mapping -> Mapping glaciers in the Hindu Kush Himalaya, Landsat 7 images, Shapefile labels of the glaciers, Unet with dropout\nInstance segmentation\nIn instance segmentation, each individual 'instance' of a segmented area is given a unique lable. For detection of very small objects this may a good approach, but it can struggle seperating individual objects that are closely spaced.\nMask_RCNN generates bounding boxes and segmentation masks for each instance of an object in the image. It is very commonly used for instance segmentation & object detection\nInstance segmentation of center pivot irrigation system in Brazil using free Landsat images, mask R-CNN & Keras\nOil tank instance segmentation with Mask R-CNN with accompanying article using Keras & Airbus Oil Storage Detection Dataset on Kaggle\nBuilding-Detection-MaskRCNN -> Building detection from the SpaceNet dataset by using Mask RCNN\nObject detection\nSeveral different techniques can be used to count the number of objects in an image. The returned data can be an object count (regression), a bounding box around individual objects in an image (typically using Yolo or Faster R-CNN architectures), a pixel mask for each object (instance segmentation), key points for an an object (such as wing tips, nose and tail of an aircraft), or simply a classification for a sliding tile over an image. A good introduction to the challenge of performing object detection on aerial imagery is given in this paper. In summary, images are large and objects may comprise only a few pixels, easily confused with random features in background. For the same reason, object detection datasets are inherently imbalanced, since the area of background typically dominates over the area of the objects to be detected. In general object detecion performs well on large objects, and gets increasingly difficult as the objects get smaller & more densely packed. Model accuracy falls off rapidly as image resolution degrades, so it is common for object detection to use very high resolution imagery, e.g. 30cm RGB. A particular characteristic of aerial images is that objects can be oriented in any direction, so using rotated bounding boxes which aligning with the object can be crucial for extracting metrics such as the length and width of an object.\nObject Detection and Image Segmentation with Deep Learning on Earth Observation Data: A Review\nawesome-aerial-object-detection and awesome-tiny-object-detection list many relevant papers\nTackling the Small Object Problem in Object Detection\nSatellite Imagery Multiscale Rapid Detection with Windowed Networks (SIMRDWN) -> combines some of the leading object detection algorithms into a unified framework designed to detect objects both large and small in overhead imagery. Train models and test on arbitrary image sizes with YOLO (versions 2 and 3), Faster R-CNN, SSD, or R-FCN.\nYOLTv4 -> YOLTv4 is designed to detect objects in aerial or satellite imagery in arbitrarily large images that far exceed the ~600\u00d7600 pixel size typically ingested by deep learning object detection frameworks. Read Announcing YOLTv4: Improved Satellite Imagery Object Detection\nTensorflow Benchmarks for Object Detection in Aerial Images -> tensorflow-based codebase created to build benchmarks for object detection in aerial images\nPytorch Benchmarks for Object Detection in Aerial Images -> pytorch-based codebase created to build benchmarks for object detection in aerial images using mmdetection\nASPDNet -> Counting dense objects in remote sensing images, arxiv paper\nxview-yolov3 -> xView 2018 Object Detection Challenge: YOLOv3 Training and Inference\nFaster RCNN for xView satellite data challenge\nHow to detect small objects in (very) large images -> A practical guide to using Slicing-Aided Hyper Inference (SAHI) for performing inference on the DOTAv1.0 object detection dataset using the mmdetection framework\nObject detection on Satellite Imagery using RetinaNet -> using the Kaggle Swimming Pool and Car Detection dataset\nObject Detection Satellite Imagery Multi-vehicles Dataset (SIMD) -> RetinaNet,Yolov3 and Faster RCNN for multi object detection on satellite images dataset\nSNIPER/AutoFocus -> an efficient multi-scale object detection training/inference algorithm\nmarine_debris_ML -> Marine debris detection, uses 3-meter imagery product called Planetscope with bands in the red, green, blue, and near-infrared. Uses Tensorflow Object Detection API with pre-trained resnet 101\npool-detection-from-aerial-imagery -> Use Icevision and Detectron2 to detect swimming pools from aerial imagery\nElectric-Pylon-Detection-in-RSI -> a dataset which contains 1500 remote sensing images of electric pylons used to train ten deep learning models\nObject detection enhanced by super resolution\nSuper-Resolution and Object Detection -> Super-resolution is a relatively inexpensive enhancement that can improve object detection performance\nEESRGAN -> Small-Object Detection in Remote Sensing Images with End-to-End Edge-Enhanced GAN and Object Detector Network\nMid-Low Resolution Remote Sensing Ship Detection Using Super-Resolved Feature Representation\nObject detection with rotated bounding boxes\nOBBDetection -> an oriented object detection library, which is based on MMdetection\nrotate-yolov3 -> Rotation object detection implemented with yolov3. Also see yolov3-polygon\nDRBox -> for detection tasks where the objects are orientated arbitrarily, e.g. vehicles, ships and airplanes\ns2anet -> Official code of the paper 'Align Deep Features for Oriented Object Detection'\nCFC-Net -> Official implementation of \"CFC-Net: A Critical Feature Capturing Network for Arbitrary-Oriented Object Detection in Remote Sensing Images\"\nReDet -> Official code of the paper \"ReDet: A Rotation-equivariant Detector for Aerial Object Detection\"\nBBAVectors-Oriented-Object-Detection -> Oriented Object Detection in Aerial Images with Box Boundary-Aware Vectors\nObject detection - buildings, rooftops & solar panels\nMachine Learning For Rooftop Detection and Solar Panel Installment discusses tiling large images and generating annotations from OSM data. Features of the roofs were calculated using a combination of contour detection and classification. Follow up article using semantic segmentation\nBuilding Extraction with YOLT2 and SpaceNet Data\nAIcrowd dataset of building outlines -> 300x300 pixel RGB images with annotations in MS-COCO format\nXBD-hurricanes -> Models for building (and building damage) detection in high-resolution (<1m) satellite and aerial imagery using a modified RetinaNet model\nDetecting solar panels from satellite imagery using segmentation\nssd-spacenet -> Detect buildings in the Spacenet dataset using Single Shot MultiBox Detector (SSD)\nObject detection - ships & boats\nHow hard is it for an AI to detect ships on satellite images?\nObject Detection in Satellite Imagery, a Low Overhead Approache\nDetecting Ships in Satellite Imagery using the Planet dataset and Keras\nPlanet use non DL felzenszwalb algorithm to detect ships\nShip detection using k-means clustering & CNN classifier on patches\nsentinel2-xcube-boat-detection -> detect and count boat traffic in Sentinel-2 imagery using temporal, spectral and spatial features\nArbitrary-Oriented Ship Detection through Center-Head Point Extraction -> arxiv paper. Keypoint estimation is performed to find the center of ships. Then, the size and head point of the ships are regressed. Repo ASD\nship_detection -> using an interesting combination of CNN classifier, Class Activation Mapping (CAM) & UNET segmentation. Accompanying three part blog post\nBuilding a complete Ship detection algorithm using YOLOv3 and Planet satellite images -> covers finding and annotating data (using LabelMe), preprocessing large images into chips, and training Yolov3. Repo\nShip-detection-in-satellite-images -> experiments with UNET, YOLO, Mask R-CNN, SSD, Faster R-CNN, RETINA-NET\nShip-Detection-from-Satellite-Images-using-YOLOV4 -> uses Kaggle Airbus Ship Detection dataset\nkaggle-airbus-ship-detection-challenge -> using oriented SSD\nshipsnet-detector -> Detect container ships in Planet imagery using machine learning\nClassifying Ships in Satellite Imagery with Neural Networks -> applied to the Kaggle Ships in Satellite Imagery dataset\nShip Detection in Satellite Images using Mask R-CNN blog post by spell.ml\nMask R-CNN for Ship Detection & Segmentation blog post with repo\nObject detection - vehicles & trains\nTruck Detection with Sentinel-2 during COVID-19 crisis -> moving objects in Sentinel-2 data causes a specific reflectance relationship in the RGB, which looks like a rainbow, and serves as a marker for trucks. Improve accuracy by only analysing roads. Not using object detection but relevant\ncowc_car_counting -> car counting on the Cars Overhead With Context (COWC) dataset. Not sctictly object detection but a CNN to predict the car count in a tile\nTraffic density estimation as a regression problem instead of object detection -> inspired by this paper\nApplying Computer Vision to Railcar Detection -> useful insights into counting railcars (i.e. train carriages) using Mask-RCNN with rotated bounding boxes output\nObject detection - planes & aircraft\nyoltv4 includes examples on the RarePlanes dataset\naircraft-detection -> experiments to test the performance of a Gaussian process (GP) classifier with various kernels on the UC Merced land use land cover (LULC) dataset\nThe Rareplanes guide recommends annotating airplanes in a diamond style, which has several advantages (easily reproducible, convertible to a bounding box etc) and allows extracting the aircraft length and wingspan\nrareplanes-yolov5 -> using YOLOv5 and the RarePlanes dataset to detect and classify sub-characteristics of aircraft\ndetecting-aircrafts-on-airbus-pleiades-imagery-with-yolov5\nUsing Detectron2 to segment aircraft from satellite imagery -> pytorch and Rare Planes\nObject detection - animals\ncownter_strike -> counting cows, located with point-annotations, two models: CSRNet (a density-based method) & LCFCN (a detection-based method)\nCounting trees\nDeepForest is a python package for training and predicting individual tree crowns from airborne RGB imagery\nOfficial repository for the \"Identifying trees on satellite images\" challenge from Omdena\nCounting-Trees-using-Satellite-Images -> create an inventory of incoming and outgoing trees for an annual tree inspections, uses keras & semantic segmentation\n2020 Nature paper - An unexpectedly large count of trees in the West African Sahara and Sahel -> tree detection framework based on U-Net & tensorflow 2 with code here\nOil storage tank detection & oil spills\nOil is stored in tanks at many points between extraction and sale, and the volume of oil in storage is an important economic indicator.\nA Beginner\u2019s Guide To Calculating Oil Storage Tank Occupancy With Help Of Satellite Imagery\nOil Storage Tank\u2019s Volume Occupancy On Satellite Imagery Using YoloV3 with repo\nOil-Tank-Volume-Estimation -> combines object detection and classical computer vision\nMCAN-OilSpillDetection -> Oil Spill Detection with A Multiscale Conditional Adversarial Network under Small Data Training, with paper. A multiscale conditional adversarial network (MCAN) trained with four oil spill observation images accurately detects oil spills in new images.\nOil tank instance segmentation with Mask R-CNN with accompanying article using Keras & Airbus Oil Storage Detection Dataset on Kaggle\nCloud detection & removal\nGenerally treated as a semantic segmentation problem.\nFrom this article on sentinelhub there are three popular classical algorithms that detects thresholds in multiple bands in order to identify clouds. In the same article they propose using semantic segmentation combined with a CNN for a cloud classifier (excellent review paper here), but state that this requires too much compute resources.\nThis article compares a number of ML algorithms, random forests, stochastic gradient descent, support vector machines, Bayesian method.\nSegmentation of Clouds in Satellite Images Using Deep Learning -> semantic segmentation using a Unet on the Kaggle 38-cloud Landsat dataset\nCloud Detection in Satellite Imagery compares FPN+ResNet18 and CheapLab architectures on Sentinel-2 L1C and L2A imagery\nCloud-Removal-with-GAN-Satellite-Image-Processing\nBenchmarking Deep Learning models for Cloud Detection in Landsat-8 and Sentinel-2 images\nLandsat-8 to Proba-V Transfer Learning and Domain Adaptation for Cloud detection\nMultitemporal Cloud Masking in Google Earth Engine\ns2cloudmask -> Sentinel-2 Cloud and Shadow Detection using Machine Learning\nsentinel2-cloud-detector -> Sentinel Hub Cloud Detector for Sentinel-2 images in Python\nChange detection & time-series\nMonitor water levels, coast lines, size of urban areas, wildfire damage. Note, clouds change often too..!\nawesome-remote-sensing-change-detection lists many datasets and publications\nChange-Detection-Review -> A review of change detection methods, including code and open data sets for deep learning\nUnsupervised Changed Detection in Multi-Temporal Satellite Images using PCA & K-Means -> python 2\nLANDSAT Time Series Analysis for Multi-temporal Land Cover Classification using Random Forest\nUnstructured-change-detection-using-CNN\nSiamese neural network to detect changes in aerial images -> uses Keras and VGG16 architecture\nChange Detection in 3D: Generating Digital Elevation Models from Dove Imagery\nQGIS plugin for applying change detection algorithms on high resolution satellite imagery\nLamboiseNet -> Master thesis about change detection in satellite imagery using Deep Learning\nFully Convolutional Siamese Networks for Change Detection -> with paper\nUrban Change Detection for Multispectral Earth Observation Using Convolutional Neural Networks -> with paper, used the Onera Satellite Change Detection (OSCD) dataset\nSTANet -> official implementation of the spatial-temporal attention neural network (STANet) for remote sensing image change detection\nBIT_CD -> Official Pytorch Implementation of Remote Sensing Image Change Detection with Transformers\nIAug_CDNet -> Official Pytorch Implementation of Adversarial Instance Augmentation for Building Change Detection in Remote Sensing Images\ndpm-rnn-public -> Code implementing a damage mapping method combining satellite data with deep learning\nSenseEarth2020-ChangeDetection -> 1st place solution to the Satellite Image Change Detection Challenge hosted by SenseTime; predictions of five HRNet-based segmentation models are ensembled, serving as pseudo labels of unchanged areas\nKPCAMNet -> Python implementation of the paper Unsupervised Change Detection in Multi-temporal VHR Images Based on Deep Kernel PCA Convolutional Mapping Network\nCDLab -> benchmarking deep learning-based change detection methods.\nSiam-NestedUNet -> The pytorch implementation for \"SNUNet-CD: A Densely Connected Siamese Network for Change Detection of VHR Images\"\nSUNet-change_detection -> Implementation of paper SUNet: Change Detection for Heterogeneous Remote Sensing Images from Satellite and UAV Using a Dual-Channel Fully Convolution Network\ntemporalCNN -> Temporal Convolutional Neural Network for the Classification of Satellite Image Time Series\nSelf-supervised Change Detection in Multi-view Remote Sensing Images\nMFPNet -> Remote Sensing Change Detection Based on Multidirectional Adaptive Feature Fusion and Perceptual Similarity\npytorch-psetae -> PyTorch implementation of the model presented in Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention\nGitHub for the DIUx xView Detection Challenge -> The xView2 Challenge focuses on automating the process of assessing building damage after a natural disaster\nDASNet -> Dual attentive fully convolutional siamese networks for change detection of high-resolution satellite images\nSelf-Attention for Raw Optical Satellite Time Series Classification\nsatflow -> optical flow models for predicting future satellite images from current and past ones\nplanet-movement -> Find and process Planet image pairs to highlight object movement\nUNet-based-Unsupervised-Change-Detection -> A convolutional neural network (CNN) and semantic segmentation is implemented to detect the changes between the images, as well as classify the changes into the correct semantic class, with arxiv paper\ntemporal-cluster-matching -> detecting change in structure footprints from time series of remotely sensed imagery\nautoRIFT -> fast and intelligent algorithm for finding the pixel displacement between two images\nesa-superresolution-forecasting -> Forecasting air pollution using ESA Sentinel-5p data, and an encoder-decoder convolutional LSTM neural network architecture, implemented in Pytorch\nWealth and economic activity\nThe goal is to predict economic activity from satellite imagery rather than conducting labour intensive ground surveys\nUsing publicly available satellite imagery and deep learning to understand economic well-being in Africa, Nature Comms 22 May 2020 -> Used CNN on Ladsat imagery (night & day) to predict asset wealth of African villages\nCombining Satellite Imagery and machine learning to predict poverty -> review article\nMeasuring Human and Economic Activity from Satellite Imagery to Support City-Scale Decision-Making during COVID-19 Pandemic -> arxiv article\nPredicting Food Security Outcomes Using CNNs for Satellite Tasking -> arxiv article\nCrop yield Prediction with Deep Learning -> code for the paper Deep Gaussian Process for Crop Yield Prediction Based on Remote Sensing Data\nMeasuring the Impacts of Poverty Alleviation Programs with Satellite Imagery and Deep Learning -> code and paper\nCrop Yield Prediction Using Deep Neural Networks and LSTM\nBuilding a Crop Yield Prediction App in Senegal Using Satellite Imagery and Jupyter Voila\nAdvanced Deep Learning Techniques for Predicting Maize Crop Yield using Sentinel-2 Satellite Imagery\nBuilding a Spatial Model to Classify Global Urbanity Levels -> estimage global urbanity levels from population data, nightime lights and road networks\ndeeppop -> Deep Learning Approach for Population Estimation from Satellite Imagery, also on Github\nSuper-resolution\nSuper-resolution attempts to enhance the resolution of an imaging system, and can be applied as a pre-processing step to improve the detection of small objects. For an introduction to this topic read this excellent article. Note that super resolution techniques are generally grouped into single image super resolution (SISR) or a multi image super resolution (MISR) which is typically applied to video frames.\nSuper-Resolution on Satellite Imagery using Deep Learning -> Nov 2016 blog post by CosmiQ Works with a nice introduction to the topic. Proposes and demonstrates a new architecture with perturbation layers with practical guidance on the methodology and code. Three part series\nAwesome-Super-Resolution -> another 'awesome' repo, getting a little out of date now\nSuper-Resolution (python) Utilities for managing large satellite images\npytorch-enhance -> Library of Image Super-Resolution Models, Datasets, and Metrics for Benchmarking or Pretrained Use. Also checkout this implementation in Jax\nSuper Resolution in OpenCV\nSingle image super resolution (SISR)\nSuper Resolution for Satellite Imagery - srcnn repo\nTensorFlow implementation of \"Accurate Image Super-Resolution Using Very Deep Convolutional Networks\" adapted for working with geospatial data\nRandom Forest Super-Resolution (RFSR repo) including sample data\nEnhancing Sentinel 2 images by combining Deep Image Prior and Decrappify. Repo for deep-image-prior and article on decrappify\nImage Super-Resolution using an Efficient Sub-Pixel CNN -> the keras docs have a great tutorial on this light weight but well performing model\nsuper-resolution-using-gan -> Super-Resolution of Sentinel-2 Using Generative Adversarial Networks\nSuper-resolution of Multispectral Satellite Images Using Convolutional Neural Networks with paper\nMulti-temporal Super-Resolution on Sentinel-2 Imagery using HighRes-Net, repo\nSSPSR-Pytorch -> A spatial-spectral prior deep network for single hyperspectral image super-resolution\nSentinel-2 Super-Resolution: High Resolution For All (Bands)\nsuper-resolution for satellite images using SRCNN\nCinCGAN -> Unofficial Implementation of Unsupervised Image Super-Resolution using Cycle-in-Cycle Generative Adversarial Networks\nSatellite-image-SRGAN using PyTorch\nEEGAN -> Edge Enhanced GAN For Remote Sensing Image Super-Resolution, TensorFlow 1.1\nPECNN -> A Progressively Enhanced Network for Video Satellite Imagery Super-Resolution, minimal documentation\nhs-sr-tvtv -> Enhanced Hyperspectral Image Super-Resolution via RGB Fusion and TV-TV Minimization\nsr4rs -> Super resolution for remote sensing, with pre-trained model for Sentinel-2, SRGAN-inspired\nRestoring old aerial images with Deep Learning -> Medium article on Super Resolution with Perceptual Loss function and real images as input\nMulti image super resolution (MISR)\nNote that nearly all the MISR publications resulted from the PROBA-V Super Resolution competition\ndeepsum -> Deep neural network for Super-resolution of Unregistered Multitemporal images (ESA PROBA-V challenge)\n3DWDSRNet -> code to reproduce Satellite Image Multi-Frame Super Resolution (MISR) Using 3D Wide-Activation Neural Networks\nRAMS -> Official TensorFlow code for paper Multi-Image Super Resolution of Remotely Sensed Images Using Residual Attention Deep Neural Networks\nTR-MISR -> Transformer-based MISR framework for the the PROBA-V super-resolution challenge\nHighRes-net -> Pytorch implementation of HighRes-net, a neural network for multi-frame super-resolution, trained and tested on the European Space Agency\u2019s Kelvin competition\nProbaVref -> Repurposing the Proba-V challenge for reference-aware super resolution\nImage-to-image translation\nTranslate images e.g. from SAR to RGB.\nHow to Develop a Pix2Pix GAN for Image-to-Image Translation -> how to develop a Pix2Pix model for translating satellite photographs to Google map images. A good intro to GANS\nSAR to RGB Translation using CycleGAN -> uses a CycleGAN model in the ArcGIS API for Python\nA growing problem of \u2018deepfake geography\u2019: How AI falsifies satellite images\nKaggle Pix2Pix Maps -> dataset for pix2pix to take a google map satellite photo and build a street map\nguided-deep-decoder -> With guided deep decoder, you can solve different image pair fusion problems, allowing super-resolution, pansharpening or denoising\nhackathon-ci-2020 -> generate nighttime imagery from infrared observations\nsatellite-to-satellite-translation -> VAE-GAN architecture for unsupervised image-to-image translation with shared spectral reconstruction loss. Model is trained on GOES-16/17 and Himawari-8 L1B data\nPytorch implementation of UNet for converting aerial satellite images into google maps kinda images\nSeamless-Satellite-image-Synthesis -> generate abitrarily large RGB images from a map\nGANS\nAnomaly Detection on Mars using a GAN\nUsing Generative Adversarial Networks to Address Scarcity of Geospatial Training Data -> GAN perform better than CNN in segmenting land cover classes outside of the training dataset (article, no code)\nBuilding-A-Nets -> robust building extraction from high-resolution remote sensing images with adversarial networks\nGANmapper -> a building footprint generator using Generative Adversarial Networks\nAutoencoders, dimensionality reduction, image embeddings & similarity search\nAutoencoders & their Application in Remote Sensing -> intro article and example use case applied to SAR data for land classification\nLEt-SNE -> Dimensionality Reduction and visualization technique that compensates for the curse of dimensionality\nAutoEncoders for Land Cover Classification of Hyperspectral Images -> An autoencoder nerual net is used to reduce 103 band data to 60 features (dimensionality reduction), keras. Also read part 2 which implements K-NNC, SVM and Gradient Boosting\nImage-Similarity-Search -> an app that helps perform super fast image retrieval on PyTorch models for better embedding space interpretability\nInteractive-TSNE -> a tool that provides a way to visually view a PyTorch model's feature representation for better embedding space interpretability\nFine tuning CLIP with Remote Sensing (Satellite) images and captions -> fine tuning CLIP on the RSICD image captioning dataset, to enable querying large catalogues in natural language. With repo\nHow Airbus Detects Anomalies in ISS Telemetry Data Using TFX -> uses an autoencoder\nFew/one/zero/low shot learning\nThis is a class of techniques which attempt to make predictions for classes with few, one or even zero examples provided during training. In zero shot learning (ZSL) the model is assisted by the provision of auxiliary information which typically consists of descriptions/semantic attributes/word embeddings for both the seen and unseen classes at train time (ref). These approaches are particularly relevant to remote sensing, where there may be many examples of common classes, but few or even zero examples for other classes of interest.\nUnseen Land Cover Classification from High-Resolution Orthophotos Using Integration of Zero-Shot Learning and Convolutional Neural Networks\nFSODM -> Official Code for paper \"Few-shot Object Detection on Remote Sensing Images\" on arxiv\nFew-Shot Classification of Aerial Scene Images via Meta-Learning -> 2020 publication, a classification model that can quickly adapt to unseen categories using only a few labeled samples\nSelf/semi/un-supervised & contrastive learning\nThe terms self-supervised, semi-supervised, un-supervised, contrastive learning & SSL describe techniques using un-labelled data. In general, the more classical techniques such as k-means classification or PCA are referred to as unsupervised, whilst newer techniques using CNN feature extraction or autoencoders are referred to as self-supervised. Yann LeCun has described self-supervised/unsupervised learning as the 'base of the cake': If we think of our brain as a cake, then the cake base is unsupervised learning. The machine predicts any part of its input for any observed part, all without the use of labelled data. Supervised learning forms the icing on the cake, and reinforcement learning is the cherry on top.\nSeasonal Contrast: Unsupervised Pre-Training from Uncurated Remote Sensing Data -> Seasonal Contrast (SeCo) is an effective pipeline to leverage unlabeled data for in-domain pre-training of remote sensing representations. Models trained with SeCo achieve better performance than their ImageNet pre-trained counterparts and state-of-the-art self-supervised learning methods on multiple downstream tasks. paper and repo\nTrain SimSiam on Satellite Images using Lightly to generate embeddings that can be used for data exploration and understanding\nUnsupervised Learning for Land Cover Classification in Satellite Imagery\nTile2Vec: Unsupervised representation learning for spatially distributed data\nContrastive Sensor Fusion -> Code implementing Contrastive Sensor Fusion, an approach for unsupervised learning of multi-sensor representations targeted at remote sensing imagery.\nhyperspectral-autoencoders -> Tools for training and using unsupervised autoencoders and supervised deep learning classifiers for hyperspectral data, built on tensorflow. Autoencoders are unsupervised neural networks that are useful for a range of applications such as unsupervised feature learning and dimensionality reduction.\nSentinel-2 image clustering in python\nMARTA GANs: Unsupervised Representation Learning for Remote Sensing Image Classification and code\nA generalizable and accessible approach to machine learning with global satellite imagery nature publication -> MOSAIKS is designed to solve an unlimited number of tasks at planet-scale quickly using feature vectors, with repo. Also see mosaiks-api\ncontrastive-satellite -> Using contrastive learning to create embeddings from optical EuroSAT Satellite-2 imagery\nSelf-Supervised Learning of Remote Sensing Scene Representations Using Contrastive Multiview Coding -> arxiv paper and code\nSelf-Supervised-Learner by spaceml-org -> train a classifier with fewer labeled examples needed using self-supervised learning, example applied to UC Merced land use dataset\nTensorflow similarity -> offers state-of-the-art algorithms for metric learning and all the necessary components to research, train, evaluate, and serve similarity-based models\ndeepsentinel -> a sentinel-1 and -2 self-supervised sensor fusion model for general purpose semantic embedding\nSemi-supervised learning in satellite image classification -> experimenting with MixMatch and the EuroSAT data set\nActive learning\nSupervised deep learning techniques typically require a huge number of labelled examples for form a training dataset. However labelling at scale take significant time, expertise and resources. Active learning techniques aim to reduce the total amount of annotation that needs to be performed by selecting the most useful images to label from a large pool of unlabelled examples, thus reducing the time to generate training datasets. These processes may be referred to as Human-in-the-Loop Machine Learning\nActive learning for object detection in high-resolution satellite images -> arxiv paper\nAIDE V2 - Tools for detecting wildlife in aerial images using active learning\nAstronomicAL -> An interactive dashboard for visualisation, integration and classification of data using Active Learning\nRead about active learning on the lightly platform and in label-studio\nActive-Labeler by spaceml-org -> a CLI Tool that facilitates labeling datasets with just a SINGLE line of code\nMixed data learning\nThese techniques combine multiple data types, e.g. imagery and text data.\nPredicting the locations of traffic accidents with satellite imagery and convolutional neural networks -> Combining satellite imagery and structured data to predict the location of traffic accidents with a neural network of neural networks, with repo\nMulti-Input Deep Neural Networks with PyTorch-Lightning - Combine Image and Tabular Data -> excellent intro article using pytorch, not actually applied to satellite data but to real estate data\nJoint Learning from Earth Observation and OpenStreetMap Data to Get Faster Better Semantic Maps -> fusion based architectures and coarse-to-fine segmentation to include the OpenStreetMap layer into multispectral-based deep fully convolutional networks, arxiv paper\nInferring High-Resolution Traffic Accident Risk Maps Based on Satellite Imagery and GPS Trajectories -> input satellite imagery, GPS trajectories, road maps and the history of accidents to generate high-resolution (5 meters) accident risk maps\nComposing Decision Forest and Neural Network models tensorflow documentation\npyimagesearch article on mixed-data\npytorch-widedeep -> A flexible package for multimodal-deep-learning to combine tabular data with text and images using Wide and Deep models in Pytorch\nImage Captioning\nremote-sensing-image-caption -> image classification and image caption by PyTorch\nPansharpening\nImage fusion of low res multispectral with high res pan band.\nSeveral algorithms described in the ArcGIS docs, with the simplest being taking the mean of the pan and RGB pixel value.\nDoes not require DL, classical algos suffice, see this notebook and this kaggle kernel\nrio-pansharpen -> pansharpening Landsat scenes\nPSGAN -> A Generative Adversarial Network for Remote Sensing Image Pan-sharpening, arxiv paper\nPansharpening-by-Convolutional-Neural-Network\nPBR_filter -> {P}ansharpening by {B}ackground {R}emoval algorithm for sharpening RGB images\npy_pansharpening -> multiple algorithms implemented in python\nNVDI - vegetation index\nSimple band math ndvi = np.true_divide((ir - r), (ir + r)) but challenging due to the size of the imagery.\nExample notebook local\nLandsat data in cloud optimised (COG) format analysed for NVDI with medium article here.\nVisualise water loss with Holoviews\nIdentifying Buildings in Satellite Images with Machine Learning and Quilt -> NDVI & edge detection via gaussian blur as features, fed to TPOT for training with labels from OpenStreetMap, modelled as a two class problem, \u201cBuildings\u201d and \u201cNature\u201d\nSeeing Through the Clouds - Predicting Vegetation Indices Using SAR\nA walkthrough on calculating NDWI water index for flooded areas -> Derive zonal statistics from Sentinel 2 images using Rasterio and Geopandas\nGeneral image quality\nConvolutional autoencoder network can be employed to image denoising, read about this on the Keras blog\njitter-compensation -> Remote Sensing Image Jitter Detection and Compensation Using CNN\nDeblurGANv2 -> Deblurring (Orders-of-Magnitude) Faster and Better\nimage-quality-assessment -> CNN to predict the aesthetic and technical quality of images\nConvolutional autoencoder for image denoising -> keras guide\npiq -> a collection of measures and metrics for image quality assessment\nFFA-Net -> Feature Fusion Attention Network for Single Image Dehazing\nImage registration\nImage registration is the process of transforming different sets of data into one coordinate system. Typical use is overlapping images taken at different times or with different cameras.\nWikipedia article on registration -> register for change detection or image stitching\nPhase correlation is used to estimate the XY translation between two images with sub-pixel accuracy. Can be used for accurate registration of low resolution imagery onto high resolution imagery, or to register a sub-image on a full image -> Unlike many spatial-domain algorithms, the phase correlation method is resilient to noise, occlusions, and other defects. With additional pre-processing image rotation and scale changes can also be calculated.\nimreg_dft -> Image registration using discrete Fourier transform. Given two images it can calculate the difference between scale, rotation and position of imaged features. Used by the up42 co-registration service\narosics -> Perform automatic subpixel co-registration of two satellite image datasets using phase-correlation, XY translations only.\ncnn-registration -> A image registration method using convolutional neural network features written in Python2, Tensorflow 1.5\nDetecting Ground Control Points via Convolutional Neural Network for Stereo Matching -> code?\nImage Registration: From SIFT to Deep Learning -> background reading on has the state of the art has evolved from OpenCV to Neural Networks\nImageCoregistration -> Image registration with openCV using sift and RANSAC\nmapalignment -> Aligning and Updating Cadaster Maps with Remote Sensing Images\nCVPR21-Deep-Lucas-Kanade-Homography -> deep learning pipeline to accurately align challenging multimodality images. The method is based on traditional Lucas-Kanade algorithm with feature maps extracted by deep neural networks.\neolearn implements phase correlation, feature matching and ECC\nRStoolbox supports Image to Image Co-Registration based on Mutual Information\nReprojecting the Perseverance landing footage onto satellite imagery\nKornia provides image registration by gradient decent\nLoFTR -> Detector-Free Local Feature Matching with Transformers. Good performance matching satellite image pairs, tryout the web demo on your data\nObject tracking\nObject Tracking in Satellite Videos Based on a Multi-Frame Optical Flow Tracker arxiv paper\nTerrain mapping, Lidar & DEMs\nMeasure surface contours.\nWikipedia DEM article and phase correlation article\nIntro to depth from stereo\nMap terrain from stereo images to produce a digital elevation model (DEM) -> high resolution & paired images required, typically 0.3 m, e.g. Worldview or GeoEye.\nProcess of creating a DEM here and here.\nArcGIS can generate DEMs from stereo images\nhttps://github.com/MISS3D/s2p -> produces elevation models from images taken by high resolution optical satellites -> demo code on https://gfacciol.github.io/IS18/\nAutomatic 3D Reconstruction from Multi-Date Satellite Images\nSemi-global matching with neural networks\nPredict the fate of glaciers\nmonodepth - Unsupervised single image depth prediction with CNNs\nStereo Matching by Training a Convolutional Neural Network to Compare Image Patches\nTerrain and hydrological analysis based on LiDAR-derived digital elevation models (DEM) - Python package\nPhase correlation in scikit-image\ns2p -> a Python library and command line tool that implements a stereo pipeline which produces elevation models from images taken by high resolution optical satellites such as Pl\u00e9iades, WorldView, QuickBird, Spot or Ikonos\nThe Mapbox API provides images and elevation maps, article here\nReconstructing 3D buildings from aerial LiDAR with Mask R-CNN\nResDepth -> A Deep Prior For 3D Reconstruction From High-resolution Satellite Images\noverhead-geopose-challenge -> competition to build computer vision algorithms that can effectively model the height and pose of ground objects for monocular satellite images taken from oblique angles. Blog post MEET THE WINNERS OF THE OVERHEAD GEOPOSE CHALLENGE\ncars -> a dedicated and open source 3D tool to produce Digital Surface Models from satellite imaging by photogrammetry. This Multiview stereo pipeline is intended for massive DSM production with a robust and performant design\nThermal Infrared\nThe World Needs (a lot) More Thermal Infrared Data from Space\nIR2VI thermal-to-visible image translation framework based on GANs with code\nThe finest resolution urban outdoor heat exposure maps in major US cities -> urban microclimate modeling based on high resolution 3D urban models and meteorological data makes it possible to examine how people are exposed to heat stress at a fine spatio-temporal level.\nObject_Classification_in_Thermal_Images -> classification accuracy was improved by adding the object size as a feature directly within the CNN\nThermal imaging with satellites blog post by Christoph Rieke\nObject Detection on Thermal Images -> using YOLO-v3 and applied to a terrestrial dataset from FLIR, the article offers some usful insights into the model training, with repo\nFire alerts service by Descartes Labs\nSAR\nRemoving speckle noise from Sentinel-1 SAR using a CNN\nA dataset which is specifically made for deep learning on SAR and optical imagery is the SEN1-2 dataset, which contains corresponding patch pairs of Sentinel 1 (VV) and 2 (RGB) data. It is the largest manually curated dataset of S1 and S2 products, with corresponding labels for land use/land cover mapping, SAR-optical fusion, segmentation and classification tasks. Data: https://mediatum.ub.tum.de/1474000\nso2sat on Tensorflow datasets -> So2Sat LCZ42 is a dataset consisting of co-registered synthetic aperture radar and multispectral optical image patches acquired by the Sentinel-1 and Sentinel-2 remote sensing satellites, and the corresponding local climate zones (LCZ) label. The dataset is distributed over 42 cities across different continents and cultural regions of the world.\nYou do not need clean images for SAR despeckling with deep learning -> How Speckle2Void learned to stop worrying and love the noise\nPySAR - InSAR (Interferometric Synthetic Aperture Radar) timeseries analysis in python\nSynthetic Aperture Radar (SAR) Analysis With Clarifai\nLabeled SAR imagery dataset of ten geophysical phenomena from Sentinel-1 wave mode consists of more than 37,000 SAR vignettes divided into ten defined geophysical categories\nDeep Learning and SAR Applications\nImplementing an Ensemble Convolutional Neural Network on Sentinel-1 Synthetic Aperture Radar data and Sentinel-3 Radiometric data for the detecting of forest fires\ns1_parking_occupancy -> Source code for PARKING OCCUPANCY ESTIMATION ON SENTINEL-1 IMAGES, ISPRS 2020\nExperiments on Flood Segmentation on Sentinel-1 SAR Imagery with Cyclical Pseudo Labeling and Noisy Student Training\nSpaceNet_SAR_Buildings_Solutions -> The winning solutions for the SpaceNet 6 Challenge\nMapping and monitoring of infrastructure in desert regions with Sentinel-1\nNeural nets in space\nProcessing on board a satellite allows less data to be downlinked. e.g. super-resolution image might take 8 images to generate, then a single image is downlinked. Other applications include cloud detection and collision avoidance.\nLockheed Martin and USC to Launch Jetson-Based Nanosatellite for Scientific Research Into Orbit - Aug 2020 - One app that will run on the GPU-accelerated satellite is SuperRes, an AI-based application developed by Lockheed Martin, that can automatically enhance the quality of an image.\nIntel to place movidius in orbit to filter images of clouds at source - Oct 2020 - Getting rid of these images before they\u2019re even transmitted means that the satellite can actually realize a bandwidth savings of up to 30%\nWhilst not involving neural nets the PyCubed project gets a mention here as it is putting python on space hardware such as the V-R3x\nWorldFloods will pioneer the detection of global flood events from space, launched on June 30, 2021. This paper describes the model which is run on Intel Movidius Myriad2 hardware capable of processing a 12 MP image in less than a minute\nHow AI and machine learning can support spacecraft docking with repo uwing Yolov3\nexo-space -> startup with plans to release an AI hardware addon for satellites\nSony\u2019s Spresense microcontroller board is going to space -> article implies vision applications\nOroratech Early Detection of Wildfires From Space -> OroraTech is launching its own AI nanosatellites with the NVIDIA Jetson Xavier NX system onboard\nML best practice\nThis section includes tips and ideas I have picked up from other practitioners including ai-fast-track, FraPochetti & the IceVision community\nAlmost all imagery data on the internet is in RGB format, and common techniques designed for working with this 3 band imagery may fail or need significant adaptation to work with multiband data (e.g. 13-band Sentinel 2)\nIn general, classification and object detection models are created using transfer learning, where the majority of the weights are not updated in training but have been pre computed using standard vision datasets such as ImageNet\nSince satellite images are typically very large, it is common to tile them before processing. Alternatively checkout Fully Convolutional Image Classification on Arbitrary Sized Image -> TLDR replace the fully-connected layer with a convolution-layer\nWhere you have small sample sizes, e.g. for a small object class which may be under represented in your training dataset, use image augmentation\nIn general, larger models will outperform smaller models, particularly on challenging tasks such as detecting small objetcs\nIf model performance in unsatisfactory, try to increase your dataset size before switching to another model architecture\nIn training, whenever possible increase the batch size, as small batch sizes produce poor normalization statistics\nThe vast majority of the literature uses supervised learning with the requirement for large volumes of annotated data, which is a bottleneck to development and deployment. We are just starting to see self-supervised approaches applied to remote sensing data\n4-ways-to-improve-class-imbalance discusses the pros and cons of several rebalancing techniques, applied to an aerial dataset. Reason to read: models can reach an accuracy ceiling where majority classes are easily predicted but minority classes poorly predicted. Overall model accuracy may not improve until steps are taken to account for class imbalance.\nFor general guidance on dataset size see this issue\nRead A Recipe for Training Neural Networks by Andrej Karpathy\nSeven steps towards a satellite imagery dataset\nImplementing Transfer Learning from RGB to Multi-channel Imagery -> takes a resnet50 model pre-trained on an input of 224x224 pixels with 3 channels (RGB) and updates it for a new input of 480x400 pixels and 15 channels (12 new + RGB) using keras\nHow to implement augmentations for Multispectral Satellite Images Segmentation using Fastai-v2 and Albumentations\nPrincipal Component Analysis: In-depth understanding through image visualization applied to Landsat TM images, with repo\nLeveraging Geolocation Data for Machine Learning: Essential Techniques -> A Gentle Guide to Feature Engineering and Visualization with Geospatial data, in Plain English\n3 Tips to Optimize Your Machine Learning Project for Data Labeling\nImage Classification Labeling: Single Class versus Multiple Class Projects\nLabeling Satellite Imagery for Machine Learning\nImage Augmentations for Aerial Datasets\nLeveraging satellite imagery for machine learning computer vision applications\nBest Practices for Preparing and Augmenting Image Data for CNNs\nUsing TensorBoard While Training Land Cover Models with Satellite Imagery\nAn Overview of Model Compression Techniques for Deep Learning in Space\nVisualise Embeddings with Tensorboard -> also checkout the Tensorflow Embedding Projector\nIntroduction to Satellite Image Augmentation with Generative Adversarial Networks - video\nUse Gradio and W&B together to monitor training and view predictions\nEvery important satellite imagery analysis project is challenging, but here are ten straightforward steps to get started\nChallenges with SpaceNet 4 off-nadir satellite imagery: Look angle and target azimuth angle -> building prediction in images taken at nearly identical look angles \u2014 for example, 29 and 30 degrees \u2014 produced radically different performance scores.\nHow not to test your deep learning algorithm? - bad ideas to avoid\nAI products and remote sensing: yes, it is hard and yes, you need a good infra -> advice on building an in-house data annotation service\nBoosting object detection performance through ensembling on satellite imagery\nHow to use deep learning on satellite imagery \u2014 Playing with the loss function\nOn the importance of proper data handling\nGenerate SSD anchor box aspect ratios using k-means clustering -> tutorial showing how to discover a set of aspect ratios that are custom-fit for your dataset, applied to tensorflow object detection\nML metrics\nA number of metrics are common to all model types (but can have slightly different meanings in contexts such as object detection), whilst other metrics are very specific to particular classes of model. The correct choice of metric is particularly critical for imbalanced dataset problems, e.g. object detection\nTP = true positive, FP = false positive, TN = true negative, FN = false negative\nPrecision is the % of correct positive predictions, calculated as precision = TP/(TP+FP)\nRecall or true positive rate (TPR), is the % of true positives captured by the model, calculated as recall = TP/(TP+FN). Note that FN is not possible in object detection, so recall is not appropriate.\nThe F1 score (also called the F-score or the F-measure) is the harmonic mean of precision and recall, calculated as F1 = 2*(precision * recall)/(precision + recall). It conveys the balance between the precision and the recall. Ref\nThe false positive rate (FPR), calculated as FPR = FP/(FP+TN) is often plotted against recall/TPR in an ROC curve which shows how the TPR/FPR tradeoff varies with classification threshold. Lowering the classification threshold returns more true positives, but also more false positives. Note that since FN is not possible in object detection, ROC curves are not appropriate.\nPrecision-vs-recall curves visualise the tradeoff between making false positives and false negatives\nAccuracy is the most commonly used metric in 'real life' but can be a highly misleading metric for imbalanced data sets.\nIoU is an object detection specific metric, being the average intersect over union of prediction and ground truth bounding boxes for a given confidence threshold\nmAP@0.5 is another object detection specific metric, being the mean value of the average precision for each class. @0.5 sets a threshold for how much of the predicted bounding box overlaps the ground truth bounding box, i.e. \"minimum 50% overlap\"\nFor more comprehensive definitions checkout Object-Detection-Metrics\nDatasets\nThis section contains a short list of datasets relevant to deep learning, particularly those which come up regularly in the literature. For a more comprehensive list of datasets checkout awesome-satellite-imagery-datasets and review the long list of satellite missions with example imagery\nWarning satellite image files can be LARGE, even a small data set may comprise 50 GB of imagery\nSentinel\nAs part of the EU Copernicus program, multiple Sentinel satellites are capturing imagery -> see wikipedia.\n13 bands, Spatial resolution of 10 m, 20 m and 60 m, 290 km swath, the temporal resolution is 5 days\nawesome-sentinel - a curated list of awesome tools, tutorials and APIs related to data from the Copernicus Sentinel Satellites.\nSentinel-2 Cloud-Optimized GeoTIFFs and Sentinel-2 L2A 120m Mosaic\nOpen access data on GCP\nPaid access to Sentinel & Landsat data via sentinel-hub and python-api\nExample loading sentinel data in a notebook\nso2sat on Tensorflow datasets - So2Sat LCZ42 is a dataset consisting of co-registered synthetic aperture radar and multispectral optical image patches acquired by the Sentinel-1 and Sentinel-2 remote sensing satellites, and the corresponding local climate zones (LCZ) label. The dataset is distributed over 42 cities across different continents and cultural regions of the world.\nbigearthnet - The BigEarthNet is a new large-scale Sentinel-2 benchmark archive, consisting of 590,326 Sentinel-2 image patches. The image patch size on the ground is 1.2 x 1.2 km with variable image size depending on the channel resolution. This is a multi-label dataset with 43 imbalanced labels.\nJupyter Notebooks for working with Sentinel-5P Level 2 data stored on S3. The data can be browsed here\nSentinel NetCDF data\nAnalyzing Sentinel-2 satellite data in Python with Keras\nXarray backend to Copernicus Sentinel-1 satellite data products\nLandsat\nLong running US program -> see Wikipedia\n8 bands, 15 to 60 meters, 185km swath, the temporal resolution is 16 days\nLandsat 4, 5, 7, and 8 imagery on Google, see the GCP bucket here, with Landsat 8 imagery in COG format analysed in this notebook\nLandsat 8 imagery on AWS, with many tutorials and tools listed\nhttps://github.com/kylebarron/landsat-mosaic-latest -> Auto-updating cloudless Landsat 8 mosaic from AWS SNS notifications\nVisualise landsat imagery using Datashader\nLandsat-mosaic-tiler -> This repo hosts all the code for landsatlive.live website and APIs.\nMaxar\nSatellites owned by Maxar (formerly DigitalGlobe) include GeoEye-1, WorldView-2, 3 & 4\nOpen Data images for humanitarian response\nMaxar ARD (COG plus data masks, with STAC) sample data in S3\nDataset on AWS -> see this getting started notebook and this notebook on the off-Nadir dataset\ncloud_optimized_geotif here used in the 3D modelling notebook here.\nWorldView cloud optimized geotiffs used in the 3D modelling notebook here.\nFor more Worldview imagery see Kaggle DSTL competition.\nPlanet\nPlanet\u2019s high-resolution, analysis-ready mosaics of the world\u2019s tropics, supported through Norway\u2019s International Climate & Forests Initiative. BBC coverage\nPlanet have made imagery available via kaggle competitions\nUC Merced\nLand use classification dataset with 21 classes and 100 RGB TIFF images for each class\nEach image measures 256x256 pixels with a pixel resolution of 1 foot\nhttp://weegee.vision.ucmerced.edu/datasets/landuse.html\nAvailable as a Tensorflow dataset -> https://www.tensorflow.org/datasets/catalog/uc_merced\nAlso available as a multi-label dataset\nRead Vision Transformers for Remote Sensing Image Classification where a Vision Transformer classifier achieves 98.49% classification accuracy on Merced\nEuroSAT\neurosat - EuroSAT dataset is based on Sentinel-2 satellite images covering 13 spectral bands and consisting of 10 classes with 27000 labeled and geo-referenced samples.\nEuroSAT: Land Use and Land Cover Classification with Sentinel-2 -> publication where a CNN achieves a classification accuracy 98.57%\nRepos using fastai here and here\nPatternNet\nLand use classification dataset with 38 classes and 800 RGB JPG images for each class\nhttps://sites.google.com/view/zhouwx/dataset?authuser=0\nPublication: PatternNet: A Benchmark Dataset for Performance Evaluation of Remote Sensing Image Retrieval\nFAIR1M object detection dataset\nFAIR1M: A Benchmark Dataset for Fine-grained Object Recognition in High-Resolution Remote Sensing Imagery\nDownload at gaofen-challenge.com\nDOTA object detection dataset\nhttps://captain-whu.github.io/DOTA/dataset.html\nA Large-Scale Benchmark and Challenges for Object Detection in Aerial Images\nDOTA_devkit for loading dataset\nxView object detection dataset\nhttp://xviewdataset.org/\nOne million annotated objects on 30cm imagery\nThe XView Dataset and Baseline Results blog post by Picterra\nDatabricks tutorial demonstrating inference of xView images and using SQL to generate meaningful insights\nAIRS (Aerial Imagery for Roof Segmentation)\nhttps://www.airs-dataset.com\nPublic dataset for roof segmentation from very-high-resolution aerial imagery (7.5cm)\nAIRS dataset covers almost the full area of Christchurch, the largest city in the South Island of New Zealand.\nAlso on Kaggle\nInria building/not building segmentation dataset\nhttps://project.inria.fr/aerialimagelabeling/contest/\nRGB GeoTIFF at spatial resolution of 0.3 m\nData covering Austin, Chicago, Kitsap County, Western & Easter Tyrol, Innsbruck, San Francisco & Vienna\nAICrowd building segmentation dataset\nDataset release as part of the mapping-challenge\n300x300 pixel RGB images with annotations in COCO format\nImagery appears to be global but with significant fraction from North America\nWinning solution published by neptune.ai here, achieved precision 0.943 and recall 0.954 using Unet with Resnet.\nKaggle\nKaggle hosts over > 200 satellite image datasets, search results here. The kaggle blog is an interesting read.\nKaggle - Amazon from space - classification challenge\nhttps://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data\n3-5 meter resolution GeoTIFF images from planet Dove satellite constellation\n12 classes including - cloudy, primary + waterway etc\n1st place winner interview - used 11 custom CNN\nFastAI Multi-label image classification\nMulti-Label Classification of Satellite Photos of the Amazon Rainforest\nKaggle - DSTL segmentation challenge\nhttps://www.kaggle.com/c/dstl-satellite-imagery-feature-detection\nRating - medium, many good examples (see the Discussion as well as kernels), but as this competition was run a couple of years ago many examples use python 2\nWorldView 3 - 45 satellite images covering 1km x 1km in both 3 (i.e. RGB) and 16-band (400nm - SWIR) images\n10 Labelled classes include - Buildings, Road, Trees, Crops, Waterway, Vehicles\nInterview with 1st place winner who used segmentation networks - 40+ models, each tweaked for particular target (e.g. roads, trees)\nDeepsense 4th place solution\nEntry by lopuhin using UNet with batch-normalization\nKaggle - Airbus ship detection Challenge\nhttps://www.kaggle.com/c/airbus-ship-detection/overview\nRating - medium, most solutions using deep-learning, many kernels, good example kernel\nI believe there was a problem with this dataset, which led to many complaints that the competition was ruined\nDeep Learning for Ship Detection and Segmentation -> treated as instance segmentation problem, with notebook\nLessons Learned from Kaggle\u2019s Airbus Challenge\nKaggle - Shipsnet classification dataset\nhttps://www.kaggle.com/rhammell/ships-in-satellite-imagery -> Classify ships in San Franciso Bay using Planet satellite imagery\n4000 80x80 RGB images labeled with either a \"ship\" or \"no-ship\" classification, 3 meter pixel size\nshipsnet-detector -> Detect container ships in Planet imagery using machine learning\nKaggle - Ships in Google Earth\nhttps://www.kaggle.com/tomluther/ships-in-google-earth\n794 jpegs showing various sized ships in satellite imagery, annotations in Pascal VOC format for object detection models\nkaggle-ships-in-Google-Earth-yolov5\nKaggle - Swimming pool and car detection using satellite imagery\nhttps://www.kaggle.com/kbhartiya83/swimming-pool-and-car-detection\n3750 satellite images of residential areas with annotation data for swimming pools and cars\nObject detection on Satellite Imagery using RetinaNet\nKaggle - Planesnet classification dataset\nhttps://www.kaggle.com/rhammell/planesnet -> Detect aircraft in Planet satellite image chips\n20x20 RGB images, the \"plane\" class includes 8000 images and the \"no-plane\" class includes 24000 images\nDataset repo and planesnet-detector demonstrates a small CNN classifier on this dataset\nKaggle - Draper challenge to place images in order of time\nhttps://www.kaggle.com/c/draper-satellite-image-chronology/data\nRating - hard. Not many useful kernels.\nImages are grouped into sets of five, each of which have the same setId. Each image in a set was taken on a different day (but not necessarily at the same time each day). The images for each set cover approximately the same area but are not exactly aligned.\nKaggle interviews for entrants who used XGBOOST and a hybrid human/ML approach\nKaggle - Dubai segmentation\nhttps://www.kaggle.com/humansintheloop/semantic-segmentation-of-aerial-imagery\n72 satellite images of Dubai, the UAE, and is segmented into 6 classes\ndubai-satellite-imagery-segmentation -> due to the small dataset, image augmentation was used\nU-Net for Semantic Segmentation on Unbalanced Aerial Imagery -> using the Dubai dataset\nKaggle - Deepsat classification challenge\nNot satellite but airborne imagery. Each sample image is 28x28 pixels and consists of 4 bands - red, green, blue and near infrared. The training and test labels are one-hot encoded 1x6 vectors. Each image patch is size normalized to 28x28 pixels. Data in .mat Matlab format. JPEG?\nSat4 500,000 image patches covering four broad land cover classes - barren land, trees, grassland and a class that consists of all land cover classes other than the above three\nSat6 405,000 image patches each of size 28x28 and covering 6 landcover classes - barren land, trees, grassland, roads, buildings and water bodies.\nDeep Gradient Boosted Learning article\nKaggle - High resolution ship collections 2016 (HRSC2016)\nhttps://www.kaggle.com/guofeng/hrsc2016\nShip images harvested from Google Earth\nHRSC2016_SOTA -> Fair comparison of different algorithms on the HRSC2016 dataset\nKaggle - Understanding Clouds from Satellite Images\nIn this challenge, you will build a model to classify cloud organization patterns from satellite images.\nhttps://www.kaggle.com/c/understanding_cloud_organization/\n3rd place solution on Github by naivelamb\nKaggle - Airbus Aircraft Detection Dataset\nhttps://www.kaggle.com/airbusgeo/airbus-aircrafts-sample-dataset\nOne hundred civilian airports and over 3000 annotated commercial aircrafts\ndetecting-aircrafts-on-airbus-pleiades-imagery-with-yolov5\nKaggle - Airbus oil storage detection dataset\nhttps://www.kaggle.com/airbusgeo/airbus-oil-storage-detection-dataset\nOil-Storage Tank Instance Segmentation with Mask R-CNN with accompanying article\nKaggle - Satellite images of hurricane damage\nhttps://www.kaggle.com/kmader/satellite-images-of-hurricane-damage\nhttps://github.com/dbuscombe-usgs/HurricaneHarvey_buildingdamage\nKaggle - Austin Zoning Satellite Images\nhttps://www.kaggle.com/franchenstein/austin-zoning-satellite-images\nclassify a images of Austin into one of its zones, such as residential, industrial, etc. 3667 satellite images\nKaggle - Statoil/C-CORE Iceberg Classifier Challenge\nhttps://www.kaggle.com/c/statoil-iceberg-classifier-challenge/data\nDeep Learning for Iceberg detection in Satellite Images\nKaggle - miscellaneous\nhttps://www.kaggle.com/reubencpereira/spatial-data-repo -> Satellite + loan data\nhttps://www.kaggle.com/towardsentropy/oil-storage-tanks -> Image data of industrial tanks with bounding box annotations, estimate tank fill % from shadows\nhttps://www.kaggle.com/datamunge/overheadmnist -> A Benchmark Satellite Dataset as Drop-In Replacement for MNIST\nhttps://www.kaggle.com/balraj98/deepglobe-land-cover-classification-dataset -> Land Cover Classification Dataset from DeepGlobe Challenge\nhttps://www.kaggle.com/airbusgeo/airbus-wind-turbines-patches -> Airbus SPOT satellites images over wind turbines for classification\nhttps://www.kaggle.com/aceofspades914/cgi-planes-in-satellite-imagery-w-bboxes -> CGI planes object detection dataset\nhttps://www.kaggle.com/atilol/aerialimageryforroofsegmentation -> Aerial Imagery for Roof Segmentation\nhttps://www.kaggle.com/andrewmvd/ship-detection -> 621 images of boats and ships\nSpaceNet\nspacenet.ai is an online hub for data, challenges, algorithms, and tools. Note that CosmiQ Ended its Leadership of SpaceNet, handing over the reigns to Maxar\nSpaceNet ran a series consisting of seven challenges with datasets and utilities provided. Challenges covered (1&2) building segmentation, (3) road segmentation, (4) off-nadir buildings, (5) road network extraction, (6)multi-senor mapping, (7) multi-temporal urban change\nBuilding datasets covered a number of cities including: Rio, Paris, Vegas, Shanghai, Khartoum, Atlana, Moscow, Mumbai & Rotterdam\nThe SpaceNet 7 Multi-Temporal Urban Development Challenge: Dataset Release\nSpaceNet - WorldView-3 article here, and semantic segmentation using Raster Vision\nspacenet-three-topcoder solution\nofficial utilities -> Packages intended to assist in the preprocessing of SpaceNet satellite imagery data corpus to a format that is consumable by machine learning algorithms\nandraugust spacenet-utils -> Display geotiff image with building-polygon overlay & label buildings using kNN on the pixel spectra\nTensorflow datasets\nresisc45 -> RESISC45 dataset is a publicly available benchmark for Remote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This dataset contains 31,500 images, covering 45 scene classes with 700 images in each class.\neurosat -> EuroSAT dataset is based on Sentinel-2 satellite images covering 13 spectral bands and consisting of 10 classes with 27000 labeled and geo-referenced samples.\nBigEarthNet -> a large-scale Sentinel-2 land use classification dataset, consisting of 590,326 Sentinel-2 image patches. The image patch size on the ground is 1.2 x 1.2 km with variable image size depending on the channel resolution. This is a multi-label dataset with 43 imbalanced labels. Official website includes version of the dataset with Sentinel 1 & 2 chips\nso2sat -> a dataset consisting of co-registered synthetic aperture radar and multispectral optical image patches acquired by Sentinel 1 & 2\nAWS datasets\nEarth on AWS is the AWS equivalent of Google Earth Engine\nCurrently 36 satellite datasets on the Registry of Open Data on AWS\nMicrosoft\nUS Building Footprints -> building footprints in all 50 US states, GeoJSON format, generated using semantic segmentation. Also Australia, Canadian and Uganda-Tanzania are available\nMicrosoft Planetary Computer is a Dask-Gateway enabled JupyterHub deployment focused on supporting scalable geospatial analysis, source repo\nAi for Earth program\nlandcover-orinoquia -> Land cover mapping of the Orinoqu\u00eda region in Colombia, in collaboration with Wildlife Conservation Society Colombia. An #AIforEarth project\nGoogle Earth Engine (GEE)\nSince there is a whole community around GEE I will not reproduce it here but list very select references. Get started at https://developers.google.com/earth-engine/\nVarious imagery and climate datasets, including Landsat & Sentinel imagery\nawesome-google-earth-engine & awesome-earth-engine-apps\nHow to Use Google Earth Engine and Python API to Export Images to Roboflow -> to acquire training data\nReduce Satellite Image Resolution with Google Earth Engine -> a crucial step before applying machine learning to satellite imagery\nee-fastapi is a simple FastAPI web application for performing flood detection using Google Earth Engine in the backend.\nHow to Download High-Resolution Satellite Data for Anywhere on Earth\nRadiant Earth\nhttps://www.radiant.earth/\nDatasets and also models on https://mlhub.earth/\nDEM (digital elevation maps)\nShuttle Radar Topography Mission, search online at usgs.gov\nCopernicus Digital Elevation Model (DEM) on S3, represents the surface of the Earth including buildings, infrastructure and vegetation. Data is provided as Cloud Optimized GeoTIFFs. link\nWeather Datasets\nUK metoffice -> https://www.metoffice.gov.uk/datapoint\nNASA (make request and emailed when ready) -> https://search.earthdata.nasa.gov\nNOAA (requires BigQuery) -> https://www.kaggle.com/noaa/goes16/home\nTime series weather data for several US cities -> https://www.kaggle.com/selfishgene/historical-hourly-weather-data\nTime series & change detection datasets\nBreizhCrops -> A Time Series Dataset for Crop Type Mapping\nThe SeCo dataset contains image patches from Sentinel-2 tiles captured at different timestamps at each geographical location. Download SeCo here\nOnera Satellite Change Detection Dataset comprises 24 pairs of multispectral images taken from the Sentinel-2 satellites between 2015 and 2018\nSYSU-CD -> The dataset contains 20000 pairs of 0.5-m aerial images of size 256\u00d7256 taken between the years 2007 and 2014 in Hong Kong\nUAV & Drone datasets\nMany on https://www.visualdata.io\nAU-AIR dataset -> a multi-modal UAV dataset for object detection.\nERA -> A Dataset and Deep Learning Benchmark for Event Recognition in Aerial Videos.\nAerial Maritime Drone Dataset\nStanford Drone Dataset\nRetinaNet for pedestrian detection\nAerial Maritime Drone Dataset\nEmergencyNet -> identify fire and other emergencies from a drone\nOpenDroneMap -> generate maps, point clouds, 3D models and DEMs from drone, balloon or kite images.\nDataset of thermal and visible aerial images for multi-modal and multi-spectral image registration and fusion -> The dataset consists of 30 visible images and their metadata, 80 thermal images and their metadata, and a visible georeferenced orthoimage.\nBIRDSAI: A Dataset for Detection and Tracking in Aerial Thermal Infrared Videos -> TIR videos of humans and animals with several challenging scenarios like scale variations, background clutter due to thermal reflections, large camera rotations, and motion blur\nERA: A Dataset and Deep Learning Benchmark for Event Recognition in Aerial Videos\nDroneVehicle -> Drone-based RGB-Infrared Cross-Modality Vehicle Detection via Uncertainty-Aware Learning\nSynthetic data\nThe Synthinel-1 dataset: a collection of high resolution synthetic overhead imagery for building segmentation\nRarePlanes -> incorporates both real and synthetically generated satellite imagery including aircraft. Read the arxiv paper and checkout the repo. Note the dataset is available through the AWS Open-Data Program for free download\nRead this article from NVIDIA which discusses fine tuning a model pre-trained on synthetic data (Rareplanes) with 10% real data, then pruning the model to reduce its size, before quantizing the model to improve inference speed\nCheckout Microsoft AirSim, which is a simulator for drones, cars and more, built on Unreal Engine\nCombining Synthetic Data with Real Data to Improve Detection Results in Satellite Imagery\nSynthinel -> synthetic overhead imagery with full pixel-wise building labels, created using ESRI CityEngine\nBlenderGIS could be used for synthetic data generation\nbifrost.ai -> simulated data service with geospatial output data formats\noktal-se -> software for generating simulated data across a wide range of bands including optical and SAR\nOnline platforms for analytics\nThis article discusses some of the available platforms\nPangeo -> There is no single software package called \u201cpangeo\u201d; rather, the Pangeo project serves as a coordination point between scientists, software, and computing infrastructure. Includes open source resources for parallel processing using Dask and Xarray. Pangeo recently announced their 2.0 goals: pivoting away from directly operating cloud-based JupyterHubs, and towards eductaion and research\nAirbus Sandbox -> will provide access to imagery\nDescartes Labs -> access to EO imagery from a variety of providers via python API\nDigitalGlobe have a cloud hosted Jupyter notebook platform called GBDX. Cloud hosting means they can guarantee the infrastructure supports their algorithms, and they appear to be close/closer to deploying DL.\nPlanet have a Jupyter notebook platform which can be deployed locally.\neurodatacube.com -> data & platform for EO analytics in Jupyter env, paid\nup42 is a developer platform and marketplace, offering all the building blocks for powerful, scalable geospatial products\nMicrosoft Planetary Computer -> direct Google Earth Engine competitor in the making?\neofactory.ai -> supports multi public and private data sources that can be used to analyse and extract information\nmapflow.ai -> imagery analysis platform with its instant access to the major satellite imagery providers, models for extract building footprints etc & QGIS plugin\nopeneo by ESA data platform\nFree online compute\nA GPU is required for training deep learning models (but not necessarily for inferencing), and this section lists a couple of free Jupyter environments with GPU available. There is a good overview of online Jupyter development environments on the fastai site. I personally use Colab Pro with data hosted on Google Drive, or Sagemaker if I have very long running training jobs.\nGoogle Colab\nCollaboratory notebooks with GPU as a backend for free for 12 hours at a time. Note that the GPU may be shared with other users, so if you aren't getting good performance try reloading.\nAlso a pro tier for $10 a month -> https://colab.research.google.com/signup\nTensorflow, pytorch & fastai available but you may need to update them\nColab Alive is a chrome extension that keeps Colab notebooks alive.\ncolab-ssh -> lets you ssh to a colab instance like it\u2019s an EC2 machine and install packages that require full linux functionality\nKaggle - also Google!\nFree to use\nGPU Kernels - may run for 1 hour\nTensorflow, pytorch & fastai available but you may need to update them\nAdvantage that many datasets are already available\nOthers\nPaperspace gradient -> free tier includes GPU usage\nDeepnote -> many features for collaboration, GPU use is paid\nState of the art engineering\nCompute and data storage are moving to the cloud. Read how Planet and Airbus use the cloud\nGoogle Earth Engine and Microsoft Planetary Computer are democratising access to massive compute platforms\nNo-code platforms and auto-ml are making ML techniques more accessible than ever\nCustom hardware is being developed for rapid training and inferencing with deep learning models, both in the datacenter and at the edge\nSupervised ML methods typically require large annotated datasets, but approaches such as self-supervised and active learning are offering alternatives pathways\nTraditional data formats aren't designed for processing on the cloud, so new standards are evolving such as COGS and STAC\nComputer vision traditionally delivered high performance image processing on a CPU by using compiled languages like C++, as used by OpenCV for example. The advent of GPUs are changing the paradigm, with alternatives optimised for GPU being created, such as Kornia\nWhilst the combo of python and keras/tensorflow/pytorch are currently preeminent, new python libraries such as Jax and alternative languages such as Julia are showing serious promise\nCloud providers\nAn overview of the most relevant services provided by AWS and Google. Also consider Microsoft Azure, or one of the many smaller but more specialised platorms such as spell.ml or paperspace\nAWS\nHost your data on S3 and metadata in a db such as postgres\nFor batch processing use Batch. GPU instances are available for batch deep learning inferencing. See how Rastervision implement this here\nIf processing can be performed in 15 minutes or less, serverless Lambda functions are an attractive option owing to their ability to scale. Note that lambda may not be a particularly quick solution for deep learning applications, since you do not have the option to batch inference on a GPU. Creating a docker container with all the required dependencies can be a challenge. To get started read Using container images to run PyTorch models in AWS Lambda and for an image classification example checkout this repo. Also read Processing satellite imagery with serverless architecture which discusses queuing & lambda. For managing a serverless infrastructure composed of multiple lambda functions use AWS SAM\nUse Glue for data preprocessing - or use Sagemaker\nTo orchestrate basic data pipelines use Step functions. Use the AWS Step Functions Workflow Studio to get started. Read Orchestrating and Monitoring Complex, Long-running Workflows Using AWS Step Functions. Note that step functions are defined in JSON\nIf step functions are too limited or you want to write pipelines in python and use Directed Acyclic Graphs (DAGs) for workflow management, checkout hosted AWS managed Airflow. Read Orchestrate XGBoost ML Pipelines with Amazon Managed Workflows for Apache Airflow and checkout amazon-mwaa-examples\nSagemaker is a whole ecosystem of ML tools that includes a hosted Jupyter environment for training of ML models. There are also tools for deployment of models using docker.\nDeep learning AMIs are EC2 instances with deep learning frameworks preinstalled. They do require more setup from the user than Sagemaker but in return allow access to the underlying hardware, which makes debugging issues more straightforward. There is a good guide to setting up your AMI instance on the Keras blog\nSpecifically created for deep learning inferencing is AWS Inferentia\nRekognition custom labels is a 'no code' annotation, training and inferencing service. Read Training models using Satellite (Sentinel-2) imagery on Amazon Rekognition Custom Labels. For a comparison with Azure and Google alternatives read this article\nWhen developing you will definitely want to use boto3 and probably aws-data-wrangler\nFor managing infrastructure use Terraform. Alternatively if you wish to use TypeScript, JavaScript, Python, Java, or C# checkout AWS CDK, although I found relatively few examples to get going using python\nAWS Ground Station now supports data delivery to Amazon S3\nRedshift is a fast, scalable data warehouse that can extend queries to S3. Redshift is based on PostgreSQL but has some differences. Redshift supports geospatial data.\nAWS App Runner enables quick deployment of containers as apps\nAWS Athena allows running SQL queries against CSV files stored on S3. Serverless so pay only for the queries you run\nIf you are using pytorch checkout the S3 plugin for pytorch which provides streaming data access\nGoogle cloud\nFor storage use Cloud Storage (AWS S3 equivalent)\nFor data warehousing use BigQuery (AWS Redshift equivalent). Visualize massive spatial datasets directly in BigQuery using CARTO\nFor model training use Vertex (AWS Sagemaker equivalent)\nFor containerised apps use Cloud Run (AWS App Runner equivalent but can scale to zero)\nDeploying models\nThis section discusses how to get a trained machine learning & specifically deep learning model into production. For an overview on serving deep learning models checkout Practical-Deep-Learning-on-the-Cloud. There are many options if you are happy to dedicate a server, although you may want a GPU for batch processing. For serverless use AWS lambda.\nRest API on dedicated server\nA common approach to serving up deep learning model inference code is to wrap it in a rest API. The API can be implemented in python (flask or FastAPI), and hosted on a dedicated server e.g. EC2 instance. Note that making this a scalable solution will require significant experience.\nBasic API: https://blog.keras.io/building-a-simple-keras-deep-learning-rest-api.html with code here\nAdvanced API with request queuing: https://www.pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/\nFramework/provider specific model serving options\nIf you are happy to live with some lock-in, these are good options:\nTensorflow serving is limited to Tensorflow models\nPytorch serve is easy to use, limited to Pytorch models, can be deployed via AWS Sagemaker\nsagemaker-inference-toolkit -> Serve machine learning models within a Docker container using AWS SageMaker\nNVIDIA Triton server\nThe Triton Inference Server provides an optimized cloud and edge inferencing solution\nSupports TensorFlow, ONNX, PyTorch TorchScript and OpenVINO model formats. Both TensorFlow 1.x and TensorFlow 2.x are supported.\nRead CAPE Analytics Uses Computer Vision to Put Geospatial Data and Risk Information in Hands of Property Insurance Companies\nAvailable on the AWS Marketplace\nModels in the browser\nThe model is run in the browser itself on live images, ensuring processing is always with the latest model available and removing the requirement for dedicated server side inferencing\nClassifying satellite imagery - Made with TensorFlow.js YoutTube video\nModel optimisation for deployment\nThe general approaches are outlined in this article from NVIDIA which discusses fine tuning a model pre-trained on synthetic data (Rareplanes) with 10% real data, then pruning the model to reduce its size, before quantizing the model to improve inference speed. Training notebook here\nModel monitoring\nOnce your model is deployed you will want to monitor for data errors, broken pipelines, and model performance degradation/drift ref\nBlog post by Neptune: Doing ML Model Performance Monitoring The Right Way\nwhylogs -> Profile and monitor your ML data pipeline end-to-end\nImage formats, data management and catalogues\nGeoServer -> an open source server for sharing geospatial data\nOpen Data Cube - serve up cubes of data https://www.opendatacube.org/\nhttps://terria.io/ for pretty catalogues\nLarge datasets may come in HDF5 format, can view with -> https://www.hdfgroup.org/downloads/hdfview/\nClimate data is often in netcdf format, which can be opened using xarray\nThe xarray docs list a number of ways that data can be stored and loaded.\nTileDB -> a 'Universal Data Engine' to store, analyze and share any data (beyond tables), with any API or tool (beyond SQL) at planet-scale (beyond clusters), open source and managed options. Recently hiring to work with xarray, dask, netCDF and cloud native storage\nBigVector database -> A fully-managed, highly-scalable, and cost-effective database for vectors. Vectorize structured data or orbital imagery and discover new insights\nRead about Serverless PostGIS on AWS Aurora\nHub -> The fastest way to store, access & manage datasets with version-control for PyTorch/TensorFlow. Works locally or on any cloud. Read Faster Machine Learning Using Hub by Activeloop: A code walkthrough of using the hub package for satellite imagery\nA Comparison of Spatial Functions: PostGIS, Athena, PrestoDB, BigQuery vs RedShift\nUnfolded Studio -> visualization platform building on open source geospatial technologies including kepler.gl, deck.gl and H3. Processing is performed browser side enabling very responsive visualisations.\nDroneDB -> can index and extract useful information from the EXIF/XMP tags of aerial images to display things like image footprint, flight path and image GPS location\nCloud Optimised GeoTiff (COG)\nA Cloud Optimized GeoTIFF (COG) is a regular GeoTIFF that supports HTTP range requests, enabling downloading of specific tiles rather than the full file. COG generally work normally in GIS software such as QGIS, but are larger than regular GeoTIFFs\nhttps://www.cogeo.org/\ncog-best-practices\nCOGs in production\nrio-cogeo -> Cloud Optimized GeoTIFF (COG) creation and validation plugin for Rasterio.\naiocogeo -> Asynchronous cogeotiff reader (python asyncio)\nLandsat data in cloud optimised (COG) format analysed for NVDI with medium article Cloud Native Geoprocessing of Earth Observation Satellite Data with Pangeo.\nWorking with COGS and STAC in python using geemap\nLoad, Experiment, and Download Cloud Optimized Geotiffs (COG) using Python with Google Colab -> short read which covers finding COGS, opening with Rasterio and doing some basic manipulations, all in a Colab Notebook.\nExploring USGS Terrain Data in COG format using hvPlot -> local COG from public AWS bucket, open with rioxarray, visualise with hvplot. See the Jupyter notebook\naws-lambda-docker-rasterio -> AWS Lambda Container Image with Python Rasterio for querying Cloud Optimised GeoTiffs. See this presentation\ncogbeam -> a python based Apache Beam pipeline, optimized for Google Cloud Dataflow, which aims to expedite the conversion of traditional GeoTIFFs into COGs\ncogserver -> Expose a GDAL file as a HTTP accessible on-the-fly COG\nDisplaying a gridded dataset on a web-based map - Step by step guide for displaying large GeoTIFFs, using Holoviews, Bokeh, and Datashader\ncog_worker -> Scalable arbitrary analysis on COGs\nSpatioTemporal Asset Catalog specification (STAC)\nThe STAC specification provides a common metadata specification, API, and catalog format to describe geospatial assets, so they can more easily indexed and discovered.\nSpec at https://github.com/radiantearth/stac-spec\nSTAC 1.0.0: The State of the STAC Software Ecosystem\nPlanet Disaster Data catalogue has the catalogue source on Github and uses the stac-browser\nGetting Started with STAC APIs intro article\nSpatioTemporal Asset Catalog API specification -> an API to make geospatial assets openly searchable and crawlable\nstacindex -> STAC Catalogs, Collections, APIs, Software and Tools\nSeveral useful repos on https://github.com/sat-utils\nIntake-STAC -> Intake-STAC provides an opinionated way for users to load Assets from STAC catalogs into the scientific Python ecosystem. It uses the intake-xarray plugin and supports several file formats including GeoTIFF, netCDF, GRIB, and OpenDAP.\nsat-utils/sat-search -> Sat-search is a Python 3 library and a command line tool for discovering and downloading publicly available satellite imagery using STAC compliant API\nfranklin -> A STAC/OGC API Features Web Service focused on ease-of-use for end-users.\nstacframes -> A Python library for working with STAC Catalogs via Pandas DataFrames\nsat-api-pg -> A Postgres backed STAC API\nstactools -> Command line utility and Python library for STAC\npystac -> Python library for working with any STAC Catalog\nSTAC Examples for Nightlights data -> minimal example STAC implementation for the Light Every Night dataset of all VIIRS DNB and DMSP-OLS nighttime satellite data\nstackstac -> Turn a STAC catalog into a dask-based xarray\nstac-fastapi -> STAC API implementation with FastAPI\nml-aoi -> An Item and Collection extension to provide labeled training data for machine learning models\nDiscoverable and Reusable ML Workflows for Earth Observation -> part 1 and part 2 with the Geospatial Machine Learning Model Catalog (GMLMC)\neoAPI -> Earth Observation API with STAC + dynamic Raster/Vector Tiler\nstac-nb -> STAC in Jupyter Notebooks\nxstac -> Generate STAC Collections from xarray datasets\nImage annotation\nFor supervised machine learning, you will require annotated images. For example if you are performing object detection you will need to annotate images with bounding boxes. Check that your annotation tool of choice supports large image (likely geotiff) files, as not all will. Note that GeoJSON is widely used by remote sensing researchers but this annotation format is not commonly supported in general computer vision frameworks, and in practice you may have to convert the annotation format to use the data with your chosen framework. There are both closed and open source tools for creating and converting annotation formats. Some of these tools are simply for performing annotation, whilst others add features such as dataset management and versioning. Note that self-supervised and active learning approaches might circumvent the need to perform a large scale annotation exercise.\nGeneral purpose annotation tools\nawesome-data-labeling -> long list of annotation tools\nlabelImg is the classic desktop tool, limited to bounding boxes for object detection. Also checkout roLabelImg which supports ROTATED rectangle regions, as often occurs in aerial imagery.\nLabelme is a simple dektop app for polygonal annotation, but note it outputs annotations in a custom LabelMe JSON format which you will need to convert. Read Labelme Image Annotation for Geotiffs\nLabel Studio is a multi-type data labeling and annotation tool with standardized output format, syncing to buckets, and supports importing pre-annotations (create with a model). Checkout label-studio-converter for converting Label Studio annotations into common dataset formats\nCVAT suports object detection, segmentation and classification via a local web app. There is an open issue to support large TIFF files. This article on Roboflow gives a good intro to CVAT.\nCreate your own annotation tool using Bokeh Holoviews\nVoTT -> an electron app for building end to end Object Detection Models from Images and Videos, by Microsoft\nDeeplabel is a cross-platform tool for annotating images with labelled bounding boxes. Deeplabel also supports running inference using state-of-the-art object detection models like Faster-RCNN and YOLOv4. With support out-of-the-box for CUDA, you can quickly label an entire dataset using an existing model.\nAlturos.ImageAnnotation is a collaborative tool for labeling image data on S3 for yolo\nrectlabel is a desktop app for MacOS to annotate images for bounding box object detection and segmentation, paid and free (rectlabel-lite) versions\npigeonXT can be used to create custom image classification annotators within Jupyter notebooks\nipyannotations -> Image annotations in python using jupyter notebooks\nLabel-Detect -> is a graphical image annotation tool and using this tool a user can also train and test large satellite images, fork of the popular labelImg tool\nSwipe-Labeler -> Swipe Labeler is a Graphical User Interface based tool that allows rapid labeling of image data\nSuperAnnotate can be run locally or used via a cloud service\ndash_doodler -> A web application built with plotly/dash for image segmentation with minimal supervision\nremo -> A webapp and Python library that lets you explore and control your image datasets\nRoboflow can be used to convert between annotation formats & manage datasets, as well as train and deploy custom models. Free tier quite useful\nsupervise.ly is one of the more fully featured platforms, decent free tier\nAWS supports image annotation via the Rekognition Custom Labels console\nlabelbox.com -> free tier is quite generous\ndiffgram describes itself as a complete training data platform for machine learning delivered as a single application. Open source or available as hosted service, supports streaming data to pytorch & tensorflow\nhasty.ai -> supports model assisted annotation & inferencing\nTensorFlow Object Detection API provides a handy utility for object annotation within Google Colab notebooks. See usage here\ncoco-annotator\nEO specific annotation tools\nAlso check the section Image handling, manipulation & dataset creation\nGroundWork is designed for annotating and labeling geospatial data like satellite imagery, from Azavea\niris -> Tool for manual image segmentation and classification of satellite imagery\nIf you are considering building an in house annotation platform read this article. Used PostGis database, GeoJson format and GIS standard in a stateless architecture\nAnnotation formats\nNote there are many annotation formats, although PASCAL VOC and coco-json are the most commonly used.\nPASCAL VOC format: XML files in the format used by ImageNet\ncoco-json format: JSON in the format used by the 2015 COCO dataset\nYOLO Darknet TXT format: contains one text file per image, used by YOLO\nTensorflow TFRecord: a proprietary binary file format used by the Tensorflow Object Detection API\nMany more formats listed here\nPaid software\nMany of these companies & products predate the open source software boom, and offer functionality which can be found in open source alternatives. However it is important to consider the licensing and support aspects before adopting an open source stack.\nENVI -> image processing and analysis\nERDAS IMAGINE -> remote sensing, photogrammetry, LiDAR analysis, basic vector analysis, and radar processing into a single product\nSpacemetric Keystone -> transform unprocessed sensor data into quality geospatial imagery ready for analysis\nmicroimages TNTgis -> advanced GIS, image processing, and geospatial analysis at an affordable price\nArcGIS\nArguably the most significant paid software for working with maps and geographic information\nArcGIS -> mapping and analytics software, with both local and cloud hosted options.\nGeospatial deep learning with arcgis.learn -> the docs\nIntegrating Deep Learning with GIS is a 2019 article by Rohit Singh which introduces the deep learning capabilities in ArcGIS\ndeep-learning-frameworks -> lists frameworks supported by the ArcGIS backend\nsar-to-rgb-translation-using-cyclegan using fastai\nArcGIS Jupyter Notebooks are built to run big data analysis, deep learning models, and dynamic visualization tools.\nOpen source software\nA note on licensing: The two general types of licenses for open source are copyleft and permissive. Copyleft requires that subsequent derived software products also carry the license forward, e.g. the GNU Public License (GNU GPLv3). For permissive, options to modify and use the code as one please are more open, e.g. MIT & Apache 2. Checkout choosealicense.com/\nawesome-earthobservation-code -> lists many useful tools and resources\nOrfeo toolbox - remote sensing toolbox with python API (just a wrapper to the C code). Do activites such as pansharpening, ortho-rectification, image registration, image segmentation & classification. Not much documentation.\nQUICK TERRAIN READER - view DEMS, Windows\ndl-satellite-docker -> docker files for geospatial analysis, including tensorflow, pytorch, gdal, xgboost...\nAIDE V2 - Tools for detecting wildlife in aerial images using active learning\nLand Cover Mapping web app from Microsoft\nSolaris -> An open source ML pipeline for overhead imagery by CosmiQ Works, similar to Rastervision but with some unique very vool features. Also checkout lunular which is a fork of Solaris which extracts the dataset preprocessing and evaluation methods that do not depend on tensorflow or pytorch\nopenSAR -> Synthetic Aperture Radar (SAR) Tools and Documents from Earth Big Data LLC (http://earthbigdata.com/)\nqhub -> QHub enables teams to build and maintain a cost effective and scalable compute/data science platform in the cloud.\nimagej -> a very versatile image viewer and processing program\nGeo Data Viewer extension for VSCode which enables opening and viewing various geo data formats with nice visualisations\nDatasette is a tool for exploring and publishing data as an interactive website and accompanying API, with SQLite backend. Various plugins extend its functionality, for example to allow displaying geospatial info, render images (useful for thumbnails), and add user authentication. Available as a desktop app\nPhotoprism is a privately hosted app for browsing, organizing, and sharing your photo collection, with support for tiffs\ndbeaver is a free universal database tool and SQL client with geospatial features\nGrafana can be used to make interactive dashboards, checkout this example showing Point data. Note there is an AWS managed service for Grafana\nlitestream -> Continuously stream SQLite changes to S3-compatible storage\nImageFusion) -> Temporal fusion of raster image time-Series\nnvtop -> NVIDIA GPUs htop like monitoring tool\nQGIS\nQGIS\nCreate, edit, visualise, analyse and publish geospatial information. Open source alternative to ArcGIS.\nPython scripting\nPlugins extend the functionality of QGIS - note these are essentially python scripts\nCreate your own plugins using the QGIS Plugin Builder\nDeepLearningTools plugin -> aid training Deep Learning Models\nMapflow.ai plugin -> various models to extract building footprints etc from Maxar imagery\ndzetsaka plugin -> classify different kind of vegetation\nGDAL & Rasterio\nSo improtant this pair gets their own section. GDAL is THE command line tool for reading and writing raster and vector geospatial data formats. If you are using python you will probably want to use Rasterio which provides a pythonic wrapper for GDAL\nGDAL and on twitter\nGDAL is a dependency of Rasterio and can be difficult to build and install. I recommend using conda, brew (on OSX) or docker in these situations\nGDAL docker quickstart: docker pull osgeo/gdal then docker run --rm -v $(pwd):/data/ osgeo/gdal gdalinfo /data/cog.tiff\nEven Rouault maintains GDAL, please consider sponsoring him\nRasterio -> reads and writes GeoTIFF and other raster formats and provides a Python API based on Numpy N-dimensional arrays and GeoJSON. There are a variety of plugins that extend Rasterio functionality.\nrio-cogeo -> Cloud Optimized GeoTIFF (COG) creation and validation plugin for Rasterio.\nrioxarray -> geospatial xarray extension powered by rasterio\naws-lambda-docker-rasterio -> AWS Lambda Container Image with Python Rasterio for querying Cloud Optimised GeoTiffs. See this presentation\ngodal -> golang wrapper for GDAL\nWrite rasterio to xarray\nLoam: A Client-Side GDAL Wrapper for Javascript\nShort list of useful GDAL commands while working in data science for remote sensing\nGeneral utilities\nPyShp -> The Python Shapefile Library (PyShp) reads and writes ESRI Shapefiles in pure Python\ns2p -> a Python library and command line tool that implements a stereo pipeline which produces elevation models from images taken by high resolution optical satellites such as Pl\u00e9iades, WorldView, QuickBird, Spot or Ikonos\nEarthPy -> A set of helper functions to make working with spatial data in open source tools easier. readExploratory Data Analysis (EDA) on Satellite Imagery Using EarthPy\npygeometa -> provides a lightweight and Pythonic approach for users to easily create geospatial metadata in standards-based formats using simple configuration files\npesto -> PESTO is designed to ease the process of packaging a Python algorithm as a processing web service into a docker image. It contains shell tools to generate all the boiler plate to build an OpenAPI processing web service compliant with the Geoprocessing-API. By Airbus Defence And Space\nGEOS -> Google Earth Overlay Server (GEOS) is a python-based server for creating Google Earth overlays of tiled maps. Your can also display maps in the web browser, measure distances and print maps as high-quality PDF\u2019s.\nGeoDjango intends to be a world-class geographic Web framework. Its goal is to make it as easy as possible to build GIS Web applications and harness the power of spatially enabled data. Some features of GDAL are supported.\nrasterstats -> summarize geospatial raster datasets based on vector geometries\nturfpy -> a Python library for performing geospatial data analysis which reimplements turf.js\nimage-similarity-measures -> Implementation of eight evaluation metrics to access the similarity between two images. Blog post here\nrsgislib -> Remote Sensing and GIS Software Library; python module tools for processing spatial data.\neo-learn is a collection of open source Python packages that have been developed to seamlessly access and process spatio-temporal image sequences acquired by any satellite fleet in a timely and automatic manner\nRStoolbox: Tools for Remote Sensing Data Analysis in R\nnd -> Framework for the analysis of n-dimensional, multivariate Earth Observation data, built on xarray\nreverse-geocoder -> a fast, offline reverse geocoder in Python\nMuseoToolBox -> a python library to simplify the use of raster/vector, especially for machine learning and remote sensing\npy6s -> an interface to the Second Simulation of the Satellite Signal in the Solar Spectrum (6S) atmospheric Radiative Transfer Model\ntimvt -> PostGIS based Vector Tile server built on top of the modern and fast FastAPI framework\ntitiler -> A dynamic Web Map tile server using FastAPI\nBRAILS -> an AI-based pipeline for city-scale building information modelling (BIM)\ncolor-thief-py -> Grabs the dominant color or a representative color palette from an image\nLow level numerical & data formats\nxarray -> N-D labeled arrays and datasets. Read Handling multi-temporal satellite images with Xarray. Checkout xarray_leaflet for tiled map plotting\nxarray-spatial -> Fast, Accurate Python library for Raster Operations. Implements algorithms using Numba and Dask, free of GDAL\nxarray-beam -> Distributed Xarray with Apache Beam by Google\nGeowombat -> geo-utilities applied to air- and space-borne imagery, uses Rasterio, Xarray and Dask for I/O and distributed computing with named coordinates\nNumpyTiles -> a specification for providing multiband full-bit depth raster data in the browser\nZarr -> Zarr is a format for the storage of chunked, compressed, N-dimensional arrays. Zarr depends on NumPy\nImage handling, manipulation & dataset creation\nPillow is the Python Imaging Library -> this will be your go-to package for image manipulation in python\nopencv-python is pre-built CPU-only OpenCV packages for Python\nkornia is a differentiable computer vision library for PyTorch, like openCV but on the GPU. Perform image transformations, epipolar geometry, depth estimation, and low-level image processing such as filtering and edge detection that operate directly on tensors.\ntifffile -> Read and write TIFF files\nxtiff -> A small Python 3 library for writing multi-channel TIFF stacks\ngeotiff -> A noGDAL tool for reading and writing geotiff files\nimage_slicer -> Split images into tiles. Join the tiles back together.\ntiler -> split images into tiles and merge tiles into a large image\ngeolabel-maker -> combine satellite or aerial imagery with vector spatial data to create your own ground-truth dataset in the COCO format for deep-learning models\nfelicette -> Satellite imagery for dummies. Generate JPEG earth imagery from coordinates/location name with publicly available satellite data.\nimagehash -> Image hashes tell whether two images look nearly identical.\nxbatcher -> Xbatcher is a small library for iterating xarray DataArrays in batches. The goal is to make it easy to feed xarray datasets to machine learning libraries such as Keras.\nfake-geo-images -> A module to programmatically create geotiff images which can be used for unit tests\nimagededup -> Finding duplicate images made easy! Uses perceptual hashing\nrmstripes -> Remove stripes from images with a combined wavelet/FFT approach\nactiveloopai Hub -> The fastest way to store, access & manage datasets with version-control for PyTorch/TensorFlow. Works locally or on any cloud. Scalable data pipelines.\nsewar -> All image quality metrics you need in one package\nfiftyone -> open-source tool for building high quality datasets and computer vision models. Visualise labels, evaluate model predictions, explore scenarios of interest, identify failure modes, find annotation mistakes, and much more!\nGeoTagged_ImageChip -> A simple script to create geo tagged image chips from high resolution RS iamges for training deep learning models such as Unet.\nLabel Maker -> downloads OpenStreetMap QA Tile information and satellite imagery tiles and saves them as an .npz file for use in machine learning training. This should be used instead of the deprecated skynet-data\nSatellite imagery label tool -> provides an easy way to collect a random sample of labels over a given scene of satellite imagery\nDeepSatData -> Automatically create machine learning datasets from satellite images\nimage-reconstructor-patches -> Reconstruct Image from Patches with a Variable Stride\ngeotiff-crop-dataset -> A Pytorch Dataloader for tif image files that dynamically crops the image\nMissing-Pixel-Filler -> given images that may contain missing data regions (like satellite imagery with swath gaps), returns these images with the regions filled\ndeepsentinel-osm -> A repository to generate land cover labels from OpenStreetMap\nimg2dataset -> Easily turn large sets of image urls to an image dataset. Can download, resize and package 100M urls in 20h on one machine\nsatproc -> Python library and CLI tools for processing geospatial imagery for ML\nSliding Window -> break large images into a series of smaller chunks\ncolor_range_filter -> a script that allows us to find range of colors in images using openCV, and then convert them into geo vectors\neo4ai -> easy-to-use tools for preprocessing datasets for image segmentation tasks in Earth Observation\nTrain-Test-Validation-Dataset-Generation -> app to crop images and create small patches of a large image e.g. Satellite/Aerial Images, which will then be used for training and testing Deep Learning models specifically semantic segmentation models\nrasterix -> a cross-platform utility built around the GDAL library and the Qt framework designed to process geospatial raster data\njimutmap -> get enormous amount of high resolution satellite images from apple / google maps quickly through multi-threading\nExport thumbnails from Earth Engine\ndatumaro -> Dataset Management Framework, a Python library and a CLI tool to build, analyze and manage Computer Vision datasets\npatchify -> A library that helps you split image into small, overlappable patches, and merge patches into original image\nImage augmentation packages\nImage augmentation is a technique used to expand a training dataset in order to improve ability of the model to generalise\nAugLy -> A data augmentations library for audio, image, text, and video. By Facebook\nalbumentations -> Fast image augmentation library and an easy-to-use wrapper around other libraries\nFoHIS -> Towards Simulating Foggy and Hazy Images and Evaluating their Authenticity\nKornia provides augmentation on the GPU\ntoolbox by ming71 -> various cv tools, such as label tools, data augmentation, label conversion, etc.\nModel tracking, versioning, specification & compilation\ndvc -> a git extension to keep track of changes in data, source code, and ML models together\nWeights and Biases -> keep track of your ML projects. Log hyperparameters and output metrics from your runs, then visualize and compare results and quickly share findings with your colleagues\ngeo-ml-model-catalog -> provides a common metadata definition for ML models that operate on geospatial data\nhummingbird -> a library for compiling trained traditional ML models into tensor computations, e.g. scikit learn model to pytorch for fast inference on a GPU\nDeep learning packages, frameworks & projects\nrastervision -> An open source Python framework for building computer vision models on aerial, satellite, and other large imagery sets\ntorchrs -> PyTorch implementation of popular datasets and models in remote sensing tasksenhance) -> Enhance PyTorch vision for semantic segmentation, multi-channel images and TIF file torchgeo -> popular datasets, model architectures\nDeepHyperX -> A Python/pytorch tool to perform deep learning experiments on various hyperspectral datasets\nDELTA -> Deep Earth Learning, Tools, and Analysis, by NASA is a framework for deep learning on satellite imagery, based on Tensorflow & using MLflow for tracking experiments\nLightly is a computer vision framework for training deep learning models using self-supervised learning\nIcevision offers a curated collection of hundreds of high-quality pre-trained models within an easy to use framework\npytorch_eo -> aims to make Deep Learning for Earth Observation data easy and accessible to real-world cases and research alike\nNGVEO -> applying convolutional neural networks (CNN) to Earth Observation (EO) data from Sentinel 1 and 2 using python and PyTorch\nchip-n-scale-queue-arranger by developmentseed -> an orchestration pipeline for running machine learning inference at scale. Supports fastai models\nhttp://spaceml.org/ -> A Machine Learning toolbox and developer community building the next generation AI applications for space science and exploration\nTorchSat is an open-source deep learning framework for satellite imagery analysis based on PyTorch (no activity since June 2020)\nDeepNetsForEO -> Uses SegNET for working on remote sensing images using deep learning (no activity since 2019)\nRoboSat -> semantic segmentation on aerial and satellite imagery. Extracts features such as: buildings, parking lots, roads, water, clouds (no longer maintained)\nDeepOSM -> Train a deep learning net with OpenStreetMap features and satellite imagery (no activity since 2017)\nmapwith.ai -> AI assisted mapping of roads with OpenStreetMap. Part of Open-Mapping-At-Facebook\nsahi -> A vision library for performing sliced inference on large images/small objects\nData discovery and ingestion\nlandsat_ingestor -> Scripts and other artifacts for landsat data ingestion into Amazon public hosting\nsatpy -> a python library for reading and manipulating meteorological remote sensing data and writing it to various image and data file formats\nGIBS-Downloader -> a command-line tool which facilitates the downloading of NASA satellite imagery and offers different functionalities in order to prepare the images for training in a machine learning pipeline\neodag -> Earth Observation Data Access Gateway\npylandsat -> Search, download, and preprocess Landsat imagery\nsentinelsat -> Search and download Copernicus Sentinel satellite images\nlandsatxplore -> Search and download Landsat scenes from EarthExplorer\nOpenSarToolkit -> High-level functionality for the inventory, download and pre-processing of Sentinel-1 data in the python language\nosmnx -> Retrieve, model, analyze, and visualize data from OpenStreetMap\nGraphing and visualisation\nhvplot -> A high-level plotting API for the PyData ecosystem built on HoloViews. Allows overlaying data on map tiles, see Exploring USGS Terrain Data in COG format using hvPlot\nPyviz examples include several interesting geospatial visualisations\nnapari -> napari is a fast, interactive, multi-dimensional image viewer for Python. It\u2019s designed for browsing, annotating, and analyzing large multi-dimensional images. By integrating closely with the Python ecosystem, napari can be easily coupled to leading machine learning and image analysis tools. Note that to view a 3GB COG I had to install the napari-tifffile-reader plugin.\npixel-adjust -> Interactively select and adjust specific pixels or regions within a single-band raster. Built with rasterio, matplotlib, and panel.\nPlotly Dash can be used for making interactive dashboards\nfolium -> a python wrapper to the excellent leaflet.js which makes it easy to visualize data that\u2019s been manipulated in Python on an interactive leaflet map. Also checkout the streamlit-folium component for adding folium maps to your streamlit apps\nipyearth -> An IPython Widget for Earth Maps\ngeopandas-view -> Interactive exploration of GeoPandas GeoDataFrames\ngeogif -> Turn xarray timestacks into GIFs\nleafmap -> geospatial analysis and interactive mapping with minimal coding in a Jupyter environment\nxmovie -> A simple way of creating movies from xarray objects\nacquisition-time -> Drawing (Satellite) acquisition dates in a timeline\nsplot -> Lightweight plotting for geospatial analysis in PySAL\nprettymaps -> A small set of Python functions to draw pretty maps from OpenStreetMap data\nTools to Design or Visualize Architecture of Neural Network\nAstronomicAL -> An interactive dashboard for visualisation, integration and classification of data using Active Learning\npyodi -> A simple tool for explore your object detection dataset\nInteractive-TSNE -> a tool that provides a way to visually view a PyTorch model's feature representation for better embedding space interpretability\nfastgradio -> Build fast gradio demos of fastai learners\npysheds -> Simple and fast watershed delineation in python\nmapboxgl-jupyter -> Use Mapbox GL JS to visualize data in a Python Jupyter notebook\ncartoframes -> integrate CARTO maps, analysis, and data services into data science workflows\ndatashader -> create meaningful representations of large datasets quickly and flexibly. Read Creating Visual Narratives from Geospatial Data Using Open-Source Technology Maxar blog post\nStreamlit\nStreamlit is an awesome python framework for creating apps with python. Additionally they will host the apps free of charge. Here I list resources which are EO related. Note that a component is an addon which extends Streamlits basic functionality\ncogviewer -> Simple Cloud Optimized GeoTIFF viewer\ncogcreator -> Simple Cloud Optimized GeoTIFF Creator. Generates COG from GeoTIFF files.\ncogvalidator -> Simple Cloud Optimized GeoTIFF validator\nstreamlit-image-juxtapose -> A simple Streamlit component to compare images in Streamlit apps\nstreamlit-folium -> Streamlit Component for rendering Folium maps\nstreamlit-keplergl -> Streamlit component for rendering kepler.gl maps\nstreamlit-light-leaflet -> Streamlit quick & dirty Leaflet component that sends back coordinates on map click\nleafmap-streamlit -> various examples showing how to use streamlit to: create a 3D map using Kepler.gl, create a heat map, display a GeoJSON file on a map, and add a colorbar or change the basemap on a map\ngeemap-apps -> build a multi-page Earth Engine App using streamlit and geemap\nstreamlit-geospatial -> A multi-page streamlit app for geospatial\nBirdsPyView -> convert images to top-down view and get coordinates of objects\nBuild a useful web application in Python: Geolocating Photos -> Step by Step tutorial using Streamlit, Exif, and Pandas\nWild fire detection app\nImage-Similarity-Search -> an app that helps perform super fast image retrieval on PyTorch models for better embedding space interpretability\ndvc-streamlit-example -> how dvc and streamlit can help track model performance during R&D exploration\nCluster computing with Dask\nDask works with your favorite PyData libraries to provide performance at scale for the tools you love -> checkout Read and manipulate tiled GeoTIFF datasets\nCoiled is a managed Dask service. Get started by reading Democratizing Satellite Imagery Analysis with Dask\nDask with PyTorch for large scale image analysis\nstackstac -> Turn a STAC catalog into a dask-based xarray\ndask-geopandas -> Parallel GeoPandas with Dask\ndask-image -> many SciPy ndimage functions implemented\nAlgorithms\nWaterDetect -> an end-to-end algorithm to generate open water cover mask, specially conceived for L2A Sentinel 2 imagery. It can also be used for Landsat 8 images and for other multispectral clustering/segmentation tasks.\nGatorSense Hyperspectral Image Analysis Toolkit -> This repo contains algorithms for Anomaly Detectors, Classifiers, Dimensionality Reduction, Endmember Extraction, Signature Detectors, Spectral Indices\ndetectree -> Tree detection from aerial imagery\npylandstats -> compute landscape metrics\ndg-calibration -> Coefficients and functions for calibrating DigitalGlobe imagery\npython-fmask -> Implementation in Python of the cloud and shadow algorithms known collectively as Fmask\npyshepseg -> Python implementation of image segmentation algorithm of Shepherd et al (2019) Operational Large-Scale Segmentation of Imagery Based on Iterative Elimination.\nShadow-Detection-Algorithm-for-Aerial-and-Satellite-Images -> shadow detection and correction algorithm\nfaiss -> A library for efficient similarity search and clustering of dense vectors, e.g. image embeddings\nawesome-spectral-indices -> A ready-to-use curated list of Spectral Indices for Remote Sensing applications\nurban-footprinter -> A convolution-based approach to detect urban extents from raster datasets\nocean_color -> Tools and algorithms for drone and satellite based ocean color science\nMovers and shakers on Github\nAdam Van Etten is doing interesting things in object detection and segmentation\nAndrew Cutts cohosts the Scene From Above podcast and has many interesting repos\nAnkit Kariryaa published a recent nature paper on tree detection\nChris Holmes is doing great things at Planet\nChristoph Rieke maintains a very popular imagery repo and has published his thesis on segmentation\nDaniel J Dufour builds geotiff.io and more\nDaniel Moraite is publishing some excellent articles\nEven Rouault maintains several of the most critical tools in this domain such as GDAL, please consider sponsoring him\nGonzalo Mateo Garc\u00eda is working on clouds and Water segmentation with CNNs\nIsaac Corley is working on super-resolution and torchrs\nJake Shermeyer many interesting repos\nMort Canty is an expert in change detection\nMykola Kozyr is working on streamlit apps\nNicholas Murray is an Australia-based scientist with a focus on delivering the science necessary to inform large scale environmental management and conservation\nOscar Ma\u00f1as is advancing the state of the art in SSL\nQiusheng Wu is an Assistant Professor in the Department of Geography at the University of Tennessee, checkout his YouTube channel\nRodrigo Caye Daudt is doing great work on change detection\nRobin Wilson is a former academic who is very active in the satellite imagery space\nRohit Singh is Director of Esri R&D Center and has some great Medium articles\nCompanies & organisations on Github\nFor a full list of companies, on and off Github, checkout awesome-geospatial-companies. The following lists companies with interesting Github profiles.\nAI. Reverie -> synthetic data\nAirbus Defence And Space\nApplied-GeoSolutions\nAzavea -> lots of interesting repos around STAC\nCARTO -> \"The leading platform for Location Intelligence and Spatial Data Science\"\nCitymapper\nDefense Innovation Unit (DIU) -> run the xView challenges\nDevelopment Seed\nDescartes Labs\nDymaxion Labs\nDHI GRAS\nElementAI\nElement 84\nESA-PhiLab\nEsri\nGeoalert -> checkout their Medium articles\nGIC-AIT -> Aisan Institute of Technology\nHummingbird Technologies Ltd -> sustainability and optimised food production\nICEYE\nindigo-ag\nMapbox -> thanks for Rasterio!\nMaxar-Analytics\nml6team\nNASA\nNear Space Labs\nOrdnance Survey\nOroraTech\nPlanet Labs -> thanks for COGS!\nPreligens -> formerly Earthcube Lab\nSatelliteVu -> currently it's all private!\nSinergise -> maintaining Sentinel-hub\nSkyTruth\nSpaceKnow\nSparkgeo\nup42 -> Airbus spinout providing 'The easiest way to build geospatial solutions'\nVortexa -> energy & shipping insights\nCourses\nIntroduction to Geospatial Raster and Vector Data with Python -> an intro course on a single page\nManning: Monitoring Changes in Surface Water Using Satellite Image Data\nAutomating GIS processes includes a lesson on automating raster data processing\nFor deep learning checkout the fastai course which uses the fastai library & pytorch\npyimagesearch.com hosts courses and plenty of material using opencv and keras\nOfficial opencv courses on opencv.org\nTensorFlow Developer Professional Certificate\nGeospatial_Python_CourseV1 -> a collection of blog posts turned into a course format\nSatellite Machine Learning Training -> lessons on how to apply Machine Learning analysis to satellite data\nDL-for-satellite-image-analysis -> short and minimalistic examples covering fundamentals of Deep Learning for Satellite Image Analysis using Jupyter notebooks, created by lakmalnd\nESRI MOOC: Spatial Data Science: The New Frontier in Analytics -> Online OCT 27 - DEC 8, 2021\nBooks\nImage Analysis, Classification and Change Detection in Remote Sensing With Algorithms for Python, Fourth Edition, By Morton John Canty -> code here\nI highly recommend Deep Learning with Python by Fran\u00e7ois Chollet\nPractical Deep Learning for Cloud, Mobile & Edge\nOnline communities\nfast AI geospatial study group\nKaggle Intro to Satellite imagery Analysis group\nOmdena brings together small teams of engineers to work on AI projects\nJobs\nSignup for the geospatial-jobs-newsletter and Pangeo discourse lists multiple jobs, global. List of companies job portals below:\nCapella Space\nDevelopment SEED\nPlanet\nSparkgeo\nAbout the author\nMy background is in optical physics, and I hold a PhD from Cambridge on the topic of localised surface Plasmons. Since academia I have held a variety of roles, including doing research at Sharp Labs Europe, developing optical systems at Surrey Satellites (SSTL), and working at an IOT startup. It was whilst at SSTL that I started this repository as a personal resource. Over time I have steadily gravitated towards data analytics and software engineering with python, and I now work as a senior data scientist at Satellite Vu. Please feel free to connect with me on Twitter & LinkedIn, and please do let me know if this repository is useful to your work.",
      "link": "https://github.com/robmarkcole/satellite-image-deep-learning"
    },
    {
      "autor": "awesome-robotic-tooling",
      "date": "NaN",
      "content": "Awesome Robotic Tooling\nA curated list of tooling for professional robotic development in C++ and Python with a touch of ROS, autonomous driving and aerospace\nTo stop reinventing the wheel you need to know about the wheel. This list is an attempt to show the variety of open and free tools in software and hardware development, which are useful in professional robotic development.\nYour contribution is necessary to keep this list alive, increase the quality and to expand it. You can read more about it's origin and how you can participate in the contribution guide and related blog post. All new project entries will have a tweet from protontypes.\nContents\nCommunication and Coordination\nDocumentation and Presentation\nRequirements and Safety\nArchitecture and Design\nFrameworks and Stacks\nDevelopment Environment\nCode and Run\nTemplate\nBuild and Deploy\nUnit and Integration Test\nLint and Format\nDebugging and Tracing\nVersion Control\nSimulation\nElectronics and Mechanics\nSensor Processing\nCalibration and Transformation\nPerception Pipeline\nMachine Learning\nParallel Processing\nImage Processing\nRadar Processing\nLidar and Point Cloud Processing\nLocalization and State Estimation\nSimultaneous Localization and Mapping\nLidar\nVisual\nVector Map\nPrediction\nBehavior and Decision\nPlanning and Control\nUser Interaction\nGraphical User Interface\nAcoustic User Interface\nCommand Line Interface\nData Visualization and Mission Control\nAnnotation\nPoint Cloud\nRViz\nOperation System\nMonitoring\nDatabase and Record\nNetwork Distributed File System\nServer Infrastructure and High Performance Computing\nEmbedded Operation System\nReal-Time Kernel\nNetwork and Middleware\nEthernet and Wireless Networking\nController Area Network\nSensor and Acuator Interfaces\nSecurity\nDatasets\nCommunication and Coordination\nAgile Development - Manifesto for Agile Software Development.\nGitflow - Makes parallel development very easy, by isolating new development from finished work.\nDeepL - An online translator that outperforms Google, Microsoft and Facebook.\nTaiga - Agile Projectmanagment Tool.\nKanboard - Minimalistic Kanban Board.\nkanban - Free, open source, self-hosted, Kanban board for GitLab issues.\nGitlab - Simple Selfhosted Gitlab Server with Docker.\nGogs - Build a simple, stable and extensible self-hosted Git service that can be setup in the most painless way.\nWekan - Meteor based Kanban Board.\nJIRA API - Python Library for REST API of Jira.\nTaiga API - Python Library for REST API of Taiga.\nChronos-Timetracker - Desktop client for JIRA. Track time, upload worklogs without a hassle.\nGrge - Grge is a daemon and command line utility augmenting GitLab.\ngitlab-triage - Gitlab's issues and merge requests triage, automated.\nHelpy - A modern, open source helpdesk customer support application.\nONLYOFFICE - A free open source collaborative system developed to manage documents, projects, customer relationship and email correspondence, all in one place.\ndiscourse - A platform for community discussion. Free, open, simple.\nGerrit - A code review and project management tool for Git based projects.\njitsi-meet - Secure, Simple and Scalable Video Conferences that you use as a standalone app or embed in your web application.\nmattermost - An open source, private cloud, Slack-alternative.\nopenproject - The leading open source project management software.\nleantime - Leantime is a lean project management system for innovators.\ngitter - Gitter is a chat and networking platform that helps to manage, grow and connect communities through messaging, content and discovery.\nDocumentation and Presentation\nTypora - A Minimalist Markdown Editor.\nMarkor - A Simple Markdown Editor for your Android Device.\nPandoc - Universal markup converter.\nYaspeller - Command line tool for spell checking.\nReadtheDocs - Build your local ReadtheDocs Server.\nDoxygen - Doxygen is the de facto standard tool for generating documentation from annotated C++ sources.\nSphinx - A tool that makes it easy to create intelligent and beautiful documentation for Python projects.\nWord-to-Markdown - A ruby gem to liberate content from Microsoft Word document.\npaperless - Index and archive all of your scanned paper documents.\ncarbon - Share beautiful images of your source code.\nundraw - Free Professional business SVGs easy to customize.\nasciinema - Lets you easily record terminal sessions and replay them in a terminal as well as in a web browser.\ninkscape - Inkscape is a professional vector graphics editor for Linux, Windows and macOS.\nReveal-Hugo - A Hugo theme for Reveal.js that makes authoring and customization a breeze. With it, you can turn any properly-formatted Hugo content into a HTML presentation.\nHugo-Webslides - This is a Hugo template to create WebSlides presentation using markdown.\njupyter2slides - Cloud Native Presentation Slides with Jupyter Notebook + Reveal.js.\npatat - Terminal-based presentations using Pandoc.\ngithub-changelog-generator - Automatically generate change log from your tags, issues, labels and pull requests on GitHub.\nGitLab-Release-Note-Generator - A Gitlab release note generator that generates release note on latest tag.\nOCRmyPDF - Adds an OCR text layer to scanned PDF files, allowing them to be searched.\npapermill - A tool for parameterizing, executing, and analyzing Jupyter Notebooks.\ndocsy - An example documentation site using the Docsy Hugo theme.\nactions-hugo - Deploy website based on Hugo to GitHub Pages.\noverleaf - An open-source online real-time collaborative LaTeX editor.\nlandslide - Generate HTML5 slideshows from markdown, ReST, or textile.\nlibreoffice-impress-templates - Freely-licensed LibreOffice Impress templates.\nopensourcedesign - Community and Resources for Free Design and Logo Creation.\nolive - A free non-linear video editor aiming to provide a fully-featured alternative to high-end professional video editing software.\nbuku - Browser-independent bookmark manager.\nswiftlatex - A WYSIWYG Browser-based LaTeX Editor.\nReLaXed - Allows complex PDF layouts to be defined with CSS and JavaScript, while writing the content in a friendly, minimal syntax close to Markdown or LaTeX.\nfoam - Foam is a personal knowledge management and sharing system inspired by Roam Research, built on Visual Studio Code and GitHub.\nCodiMD - Open Source Online Real-time collaborate on team documentation in markdown.\njupyter-book - Build interactive, publication-quality documents from Jupyter Notebooks.\nInvoiceNet - Deep neural network to extract intelligent information from invoice documents.\ntesseract - Open Source OCR Engine.\nmkdocs - A fast, simple and downright gorgeous static site generator that's geared towards building project documentation.\nPlotNeuralNet - Latex code for drawing neural networks for reports and presentation.\nExcalidraw - Virtual whiteboard for sketching hand-drawn like diagrams.\nSVGrepo - Download free SVG Vectors for commercial use.\ngollum - A simple, Git-powered wiki with a sweet API and local frontend.\nGanttLab - The easy to use, fully functional Gantt chart for GitLab and GitHub.\nZotero - A free, easy-to-use tool to help you collect, organize, cite, and share your research sources.\nRequirements and Safety\nawesome-safety-critical - List of resources about programming practices for writing safety-critical software.\nopen-autonomous-safety - OAS is a fully open-source library of Voyage's safety processes and testing procedures, designed to supplement existing safety programs at self-driving car startups across the world.\nCarND-Functional-Safety-Project - Create functional safety documents in this Udacity project.\nAutomated Valet Parking Safety Documents - Created to support the safe testing of the Automated Valet Parking function using the StreetDrone test vehicle in a car park.\nsafe_numerics - Replacements to standard numeric types which throw exceptions on errors.\nAir Vehicle C++ development coding standards - Provide direction and guidance to C++ programmers that will enable them to employ good programming style and proven programming practices leading to safe, reliable, testable, and maintainable code.\nAUTOSAR Coding Standard - Guidelines for the use of the C++14 language in critical and safety-related system.\nThe W-Model and Lean Scaled Agility for Engineering - Ford applied an agile V-Model method from Vector that can be used in safety related project management.\ndoorstop - Requirements management using version control.\ncapella - Comprehensive, extensible and field-proven MBSE tool and method to successfully design systems architecture.\nrobmosys - RobMoSys envisions an integrated approach built on top of the current code-centric robotic platforms, by applying model-driven methods and tools.\nPapyrus for Robotics - A graphical editing tool for robotic applications that complies with the RobMoSys approach.\nfossology - A toolkit you can run license, copyright and export control scans from the command line.\nScenarioArchitect - The Scenario Architect is a basic python tool to generate, import and export short scene snapshots.\nArchitecture and Design\nGuidelines - How to architect ROS-based systems.\nyEd - A powerful desktop application that can be used to quickly and effectively generate high-quality diagrams.\nyed_py - Generates graphML that can be opened in yEd.\nPlantuml - Web application to generate UML diagrams on-the-fly in your live documentation.\nrqt_graph - Provides a GUI plugin for visualizing the ROS computation graph.\nrqt_launchtree - An RQT plugin for hierarchical launchfile configuration introspection.\ncpp-dependencies - Tool to check C++ #include dependencies (dependency graphs created in .dot format).\npydeps - Python Module Dependency graphs.\naztarna - A footprinting tool for robots.\ndraw.io - A free online diagram software for making flowcharts, process diagrams, org charts, UML, ER and network diagrams.\nvscode-drawio - This extension integrates Draw.io into VS Code.\nArchitecture_Decision_Record - A document that captures an important architectural decision made along with its context and consequences.\nFrameworks and Stacks\nROS - (Robot Operating System) provides libraries and tools to help software developers create robot applications.\nawesome-ros2 - A curated list of awesome Robot Operating System Version 2.0 (ROS 2) resources and libraries.\nAutoware.Auto - Autoware.Auto applies best-in-class software engineering for autonomous driving.\nAutoware.ai - Autoware.AI is the world's first \"All-in-One\" open-source software for autonomous driving technology.\nOpenPilot - Open Source Adaptive Cruise Control (ACC) and Lane Keeping Assist System (LKAS).\nApollo - High performance, flexible architecture which accelerates the development, testing, and deployment of Autonomous Vehicles.\nPythonRobotics - This is a Python code collection of robotics algorithms, especially for autonomous navigation.\nStanford Self Driving Car Code - Stanford Code From Cars That Entered DARPA Grand Challenges.\nastrobee - Astrobee is a free-flying robot designed to operate as a payload inside the International Space Station (ISS).\nCARMAPlatform - Enables cooperative automated driving plug-in.\nAutomotive Grade Linux - Automotive Grade Linux is a collaborative open source project that is bringing together automakers, suppliers and technology companies to accelerate the development and adoption of a fully open software stack for the connected car.\nPX4 - An open source flight control software for drones and other unmanned vehicles.\nKubOS - An open-source software stack for satellites.\nmod_vehicle_dynamics_control - TUM Roborace Team Software Stack - Path tracking control, velocity control, curvature control and state estimation.\nAslan - Open source self-driving software for low speed environments.\nopen-source-rover - A build-it-yourself, 6-wheel rover based on the rovers on Mars from JPL.\npybotics - An open-source and peer-reviewed Python toolbox for robot kinematics and calibration.\nmakani - Contains the working Makani flight simulator, controller (autopilot), visualizer, and command center flight monitoring tools.\nmir_robot - This is a community project to use the MiR Robots with ROS.\nCOMPAS - Robotic fabrication package for the COMPAS Framework.\nJdeRobot Academy - JdeRobot Academy is an open source collection of exercises to learn robotics in a practical way.\nclover - ROS-based framework and RPi image to control PX4-powered drones.\nArduPilot - Open source control software for autonomous vehicles - copters/planes/rovers/boats/submersibles.\nF Prime - A component-driven framework that enables rapid development and deployment of spaceflight and other embedded software applications.\nDevelopment Environment\nCode and Run\nVim-ros - Vim plugin for ROS development.\nVisual Studio Code - Code editor for edit-build-debug cycle.\natom - Hackable text editor for the 21st century.\nTeletype - Share your workspace with team members and collaborate on code in real time in Atom.\nSublime - A sophisticated text editor for code, markup and prose.\nade-cli - The ADE Development Environment (ADE) uses docker and Gitlab to manage environments of per project development tools and optional volume images.\nrecipe-wizard - A Dockerfile generator for running OpenGL (GLX) applications with nvidia-docker2, CUDA, ROS, and Gazebo on a remote headless server system.\nJupyter ROS - Jupyter widget helpers for ROS, the Robot Operating System.\nros_rqt_plugin - The ROS Qt Creator Plug-in for Python.\nxeus-cling - Jupyter kernel for the C++ programming language.\nROS IDEs - This page collects experience and advice on using integrated development environments (IDEs) with ROS.\nTabNine - The all-language autocompleter.\nkite - Use machine learning to give you useful code completions for Python.\njedi - Autocompletion and static analysis library for python.\nroslibpy - Python ROS Bridge library allows to use Python and IronPython to interact with ROS, the open-source robotic middleware.\npybind11 - Seamless operability between C++11 and Python.\nSourcetrail - Free and open-source cross-platform source explorer.\nrebound - Command-line tool that instantly fetches Stack Overflow results when an exception is thrown.\nmybinder - Open notebooks in an executable environment, making your code immediately reproducible by anyone, anywhere.\nROSOnWindows - An experimental release of ROS1 for Windows.\nlive-share - Real-time collaborative development from the comfort of your favorite tools.\ncocalc - Collaborative Calculation in the Cloud.\nEasyClangComplete - Robust C/C++ code completion for Sublime Text 3.\nvscode-ros - Visual Studio Code extension for Robot Operating System (ROS) development.\nawesome-hpp - A curated list of awesome header-only C++ libraries.\nGitpod - An open source developer platform that automates the provisioning of ready-to-code development environments.\nTemplate\nROS - Template for ROS node standardization in C++.\nLaunch - Templates on how to create launch files for larger projects.\nBash - A bash scripting template incorporating best practices & several useful functions.\nURDF - Examples on how to create Unified Robot Description Format (URDF) for different kinds of robots.\nPython - Style guide to be followed in writing Python code for ROS.\nDocker - The Dockerfile in the minimal-ade project shows a minimal example of how to create a custom base image.\nVS Code ROS2 Workspace Template - Template for using VSCode as an IDE for ROS2 development.\nBuild and Deploy\nqemu-user-static - Enable an execution of different multi-architecture containers by QEMU and binfmt_misc.\nCross compile ROS 2 on QNX - Introduces how to cross compile ROS 2 on QNX.\nbloom - A release automation tool which makes releasing catkin packages easier.\nsuperflore - An extended platform release manager for Robot Operating System.\ncatkin_tools - Command line tools for working with catkin.\nindustrial_ci - Easy continuous integration repository for ROS repositories.\nros_gitlab_ci - Contains helper scripts and instructions on how to use Continuous Integration (CI) for ROS projects hosted on a GitLab instance.\ngitlab-runner - Runs tests and sends the results to GitLab.\ncolcon-core - Command line tool to improve the workflow of building, testing and using multiple software packages.\ngitlab-release - Simple python3 script to upload files (from ci) to the current projects release (tag).\nclang - This is a compiler front-end for the C family of languages (C, C++, Objective-C, and Objective-C++) which is built as part of the LLVM compiler infrastructure project.\ncatkin_virtualenv - Bundle python requirements in a catkin package via virtualenv.\npyenv - Simple Python version management.\naptly - Debian repository management tool.\ncross_compile - Assets used for ROS2 cross-compilation.\ndocker_images - Official Docker images maintained by OSRF on ROS(2) and Gazebo.\nrobot_upstart - Presents a suite of scripts to assist with launching background ROS processes on Ubuntu Linux PCs.\nrobot_systemd - Units for managing startup and shutdown of roscore and roslaunch.\nryo-iso - A modern ISO builder that streamlines the process of deploying a complete robot operating system from a yaml config file.\nnetwork_autoconfig - Automatic configuration of ROS networking for most use cases without impacting usage that require manual configuration.\nrosbuild - The ROS build farm.\ncros - A single thread pure C implementation of the ROS framework.\nUnit and Integration Test\nsetup-ros - This action sets up a ROS and ROS 2 environment for use in GitHub actions.\nUnitTesting - This page lays out the rationale, best practices, and policies for writing and running unit tests and integration tests for ROS.\ngoogletest - Google's C++ test framework.\npytest - The pytest framework makes it easy to write small tests, yet scales to support complex functional testing.\ndoctest - The fastest feature-rich C++11/14/17/20 single-header testing framework for unit tests and TDD.\nosrf_testing_tools_cpp - Contains testing tools for C++, and is used in OSRF projects.\ncode_coverage - ROS package to run coverage testing.\naction-ros-ci - GitHub Action to build and test ROS 2 packages using colcon.\nLint and Format\naction-ros-lint - GitHub action to run linters on ROS 2 packages.\ncppcheck - Static analysis of C/C++ code.\nhadolint - Dockerfile linter, validate inline bash, written in Haskell.\nshellcheck - A static analysis tool for shell scripts.\ncatkin_lint - Checks package configurations for the catkin build system of ROS.\npylint - Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions.\nblack - The uncompromising Python code formatter.\npydocstyle - A static analysis tool for checking compliance with Python docstring conventions.\nharos - Static analysis of ROS application code.\npydantic - Data parsing and validation using Python type hints.\nDebugging and Tracing\nheaptrack - Traces all memory allocations and annotates these events with stack traces.\nros2_tracing - Tracing tools for ROS 2.\nLinuxperf - Various Linux performance material.\nlptrace - It lets you see in real-time what functions a Python program is running.\npyre-check - Performant type-checking for python.\nFlameGraph - Visualize profiled code.\ngpuvis - GPU Trace Visualizer.\nsanitizer - AddressSanitizer, ThreadSanitizer, MemorySanitizer.\ncppinsights - C++ Insights - See your source code with the eyes of a compiler.\ninspect - The inspect module provides functions for learning about live objects, including modules, classes, instances, functions, and methods.\nRoslaunch Nodes in Valgrind or GDB - When debugging roscpp nodes that you are launching with roslaunch, you may wish to launch the node in a debugging program like gdb or valgrind instead.\npyperformance - Python Performance Benchmark Suite.\nqira - QIRA is a competitor to strace and gdb.\ngdb-frontend - GDBFrontend is an easy, flexible and extensionable gui debugger.\nlttng - An open source software toolkit which you can use to simultaneously trace the Linux kernel, user applications, and user libraries.\nros2-performance - Allows to easily create arbitrary ROS2 systems and then measures their performance.\nbcc - Tools for BPF-based Linux IO analysis, networking, monitoring, and more.\ntracy - A real time, nanosecond resolution, remote telemetry frame profiler for games and other applications.\nbpftrace - High-level tracing language for Linux eBPF.\npudb - Full-screen console debugger for Python.\nbackward-cpp - A beautiful stack trace pretty printer for C++.\ngdb-dashboard - GDB dashboard is a standalone .gdbinit file written using the Python API that enables a modular interface showing relevant information about the program being debugged.\nhotspot - The Linux perf GUI for performance analysis.\nmemory_profiler - A python module for monitoring memory consumption of a process as well as line-by-line analysis of memory consumption for python programs.\nros1_fuzzer - This fuzzer aims to help developers and researchers to find bugs and vulnerabilities in ROS nodes by performing fuzz tests over topics that the target nodes process.\nvscode-debug-visualizer - An extension for VS Code that visualizes data during debugging.\naction-tmate - Debug your GitHub Actions via SSH by using tmate to get access to the runner system itself.\nlibstatistics_collector - ROS 2 library providing classes to collect measurements and calculate statistics across them.\nsystem_metrics_collector - Lightweight, real-time system metrics collector for ROS2 systems.\nVersion Control\ngit-fuzzy - A CLI interface to git that relies heavily on fzf.\nmeld - Meld is a visual diff and merge tool that helps you compare files, directories, and version controlled projects.\ntig - Text-mode interface for git.\ngitg - A graphical user interface for git.\ngit-cola - The highly caffeinated Git GUI.\npython-gitlab - A Python package providing access to the GitLab server API.\nbfg-repo-cleaner - Removes large or troublesome blobs like git-filter-branch does, but faster.\nnbdime - Tools for diffing and merging of Jupyter notebooks.\nsemantic-release - Fully automated version management and package publishing.\ngo-semrel-gitab - Automate version management for Gitlab.\nGit-repo - Git-Repo helps manage many Git repositories, does the uploads to revision control systems, and automates parts of the development workflow.\ndive - A tool for exploring each layer in a docker image.\ndvc - Management and versioning of datasets and machine learning models.\nlearnGitBranching - A git repository visualizer, sandbox, and a series of educational tutorials and challenges.\ngitfs - You can mount a remote repository's branch locally, and any subsequent changes made to the files will be automatically committed to the remote.\ngit-secret - Encrypts files with permitted users' public keys, allowing users you trust to access encrypted data using pgp and their secret keys.\ngit-sweep - A command-line tool that helps you clean up Git branches that have been merged into master.\nlazygit - A simple terminal UI for git commands, written in Go with the gocui library.\nglab - An open-source GitLab command line tool.\nSimulation\nAI2-THOR - Python framework with a Unity backend providing interaction, navigation, and manipulation support for household based robotic agents, consisting of 200+ of custom scenes, 1500+ custom annotated objects, and 200+ actions.\nDrake - Drake aims to simulate even very complex dynamics of robots.\nWebots - Webots is an open source robot simulator compatible (among others) with ROS and ROS2.\nlgsv - LG Electronics America R&D Center has developed an HDRP Unity-based multi-robot simulator for autonomous vehicle developers.\ncarla - Open-source simulator for autonomous driving research.\nawesome-CARLA - A curated list of awesome CARLA tutorials, blogs, and related projects.\nros-bridge - ROS bridge for CARLA Simulator.\nscenario_runner - Traffic scenario definition and execution engine.\ndeepdive - End-to-end simulation for self-driving cars.\nuuv_simulator - Gazebo/ROS packages for underwater robotics simulation.\nAirSim - Open source simulator for autonomous vehicles built on Unreal Engine.\nself-driving-car-sim - A self-driving car simulator built with Unity.\nROSIntegration - Unreal Engine Plugin to enable ROS Support.\ngym-gazebo - An OpenAI gym extension for using Gazebo known as gym-gazebo.\ngym-pybullet-drones - PyBullet-based Gym environments for single and multi-agent reinforcement learning of quadcopter control.\nsafe-control-gym - PyBullet-based CartPole and Quadrotor environments\u2014with CasADi symbolic dynamics and constraints\u2014for safe and robust learning-based control.\nhighway-env - A collection of environments for autonomous driving and tactical decision-making tasks.\nVREP Interface - ROS Bridge for the VREP simulator.\ncar_demo - This is a simulation of a Prius in gazebo 9 with sensor data being published using ROS kinetic.\nsumo - Eclipse SUMO is an open source, highly portable, microscopic and continuous road traffic simulation package designed to handle large road networks.\nopen-simulation-interface - A generic interface for the environmental perception of automated driving functions in virtual scenarios.\nESIM - An Open Event Camera Simulator.\nMenge - Crowd Simulation Framework.\npedsim_ros - Pedestrian simulator powered by the social force model for Gazebo.\nopencrg - Open file formats and open source tools for the detailed description, creation and evaluation of road surfaces.\nesmini - A basic OpenSCENARIO player.\nOpenSceneGraph - An open source high performance 3D graphics toolkit, used by application developers in fields such as visual simulation, games, virtual reality, scientific visualization and modelling.\nmorse - An academic robotic simulator, based on the Blender Game Engine and the Bullet Physics engine.\nROSIntegrationVision - Support for ROS-enabled RGBD data acquisition in Unreal Engine Projects.\nfetch_gazebo - Contains the Gazebo simulation for Fetch Robotics Fetch and Freight Research Edition Robots.\nrotors_simulator - Provides some multirotor models.\nflow - A computational framework for deep RL and control experiments for traffic microsimulation.\ngnss-ins-sim - GNSS + inertial navigation, sensor fusion simulator. Motion trajectory generator, sensor models, and navigation.\nIgnition Robotics - Test control strategies in safety, and take advantage of simulation in continuous integration tests.\nsimulation assets for the SubT - This collection contains simulation assets for the SubT Challenge Virtual Competition in Gazebo.\ngazebo_ros_motors - Contains currently two motor plugins for Gazebo, one with an ideal speed controller and one without a controller that models a DC motor.\nmap2gazebo - ROS package for creating Gazebo environments from 2D maps.\nsim_vehicle_dynamics - Vehicle Dynamics Simulation Software of TUM Roborace Team.\ngym-carla - An OpenAI gym wrapper for CARLA simulator.\nsimbody - High-performance C++ multibody dynamics/physics library for simulating articulated biomechanical and mechanical systems like vehicles, robots, and the human skeleton.\ngazebo_models - This repository holds the Gazebo model database.\npylot - Autonomous driving platform running on the CARLA simulator.\nflightmare - Flightmare is composed of two main components: a configurable rendering engine built on Unity and a flexible physics engine for dynamics simulation.\nchamp - ROS Packages for CHAMP Quadruped Controller.\nrex-gym - OpenAI Gym environments for an open-source quadruped robot (SpotMicro).\nTrick - Developed at the NASA Johnson Space Center, is a powerful simulation development framework that enables users to build applications for all phases of space vehicle development.\nusv_sim_lsa - Unmanned Surface Vehicle simulation on Gazebo with water current and winds.\n42 - Simulation for spacecraft attitude control system analysis and design.\nComplete_Street_Rule - A scenario oriented design tool intended to enable users to quickly create procedurally generated multimodal streets in ArcGIS CityEngine.\nAutoCore simulation - Provides test environment for Autoware and still during early development, contents below may changed during updates.\nfields-ignition - Generate random crop fields for Ignition Gazebo.\nUnity-Robotics-Hub - Central repository for tools, tutorials, resources, and documentation for robotic simulation in Unity.\nBlueSky - The goal of BlueSky is to provide everybody who wants to visualize, analyze or simulate air traffic with a tool to do so without any restrictions, licenses or limitations.\nCloe - Empowers developers of automated-driving software components by providing a unified interface to closed-loop simulation.\nDynamic_logistics_Warehouse - Gazebo simulation of dynamics environment in warehouses.\nOpenCDA - A generalized framework for prototyping full-stack cooperative driving automation applications under CARLA+SUMO.\nElectronics and Mechanics\nHRIM - An information model for robot hardware.\nURDF - Repository for Unified Robot Description Format (URDF) parsing code.\nphobos - An add-on for Blender allowing to create URDF, SDF and SMURF robot models in a WYSIWYG environment.\nurdf-viz - Visualize URDF/XACRO file, URDF Viewer works on Windows/macOS/Linux.\nsolidworks_urdf_exporter - SolidWorks to URDF Exporter.\nFreeCAD - Your own 3D parametric modeler.\nkicad - A Cross Platform and Open Source Electronics Design Automation Suite.\nPcbDraw - Convert your KiCAD board into a nice looking 2D drawing suitable for pinout diagrams.\nkicad-3rd-party-tools - Tools made by others to augment the KiCad PCB EDA suite.\nPandaPower - An easy to use open source tool for power system modeling, analysis and optimization with a high degree of automation.\nLibrePCB - A powerful, innovative and intuitive EDA tool for everyone.\nopenscad - A software for creating solid 3D CAD models.\nngspice - A open source spice simulator for electric and electronic circuits.\nGNSS-SDR - GNSS-SDR provides interfaces for a wide range of radio frequency front-ends and raw sample file formats, generates processing outputs in standard formats.\nriscv - The Free and Open RISC Instruction Set Architecture.\nurdfpy - A simple and easy-to-use library for loading, manipulating, saving, and visualizing URDF files.\nFMPy - Simulate Functional Mockup Units (FMUs) in Python.\nFMIKit-Simulink - Import and export Functional Mock-up Units with Simulink.\noemof-solph - A modular open source framework to model energy supply systems.\nNASA-3D-Resources - Here you'll find a growing collection of 3D models, textures, and images from inside NASA.\nSUAVE - An Aircraft Design Toolbox.\nopem - The Open-Source PEMFC Simulation Tool (OPEM) is a modeling tool for evaluating the performance of proton exchange membrane fuel cells.\npvlib-python - A community supported tool that provides a set of functions and classes for simulating the performance of photovoltaic energy systems.\nWireViz - A tool for easily documenting cables, wiring harnesses and connector pinouts.\nHorizon - EDA is an Electronic Design Automation package supporting an integrated end-to-end workflow for printed circuit board design including parts management and schematic entry.\ntigl - The TiGL Geometry Library can be used for the computation and processing of aircraft geometries stored inside CPACS files.\nfoxBMS - A free, open and flexible development environment to design battery management systems.\ncadCAD - A Python package that assists in the processes of designing, testing and validating complex systems through simulation, with support for Monte Carlo methods, A/B testing and parameter sweeping.\nOpenMDAO - An open-source framework for efficient multidisciplinary optimization.\nODrive - The aim is to make it possible to use inexpensive brushless motors in high performance robotics projects.\nOpenTirePython - An open-source mathematical tire modelling library.\nInkscape Ray Optics - An extension for Inkscape that makes it easier to draw optical diagrams.\nOpenAeroStruct - A lightweight tool that performs aerostructural optimization using OpenMDAO.\nSensor Processing\nCalibration and Transformation\ntf2 - Transform library, which lets the user keep track of multiple coordinate frames over time.\nTriP - A Inverse Kinematics library for serial robots, parallel robots and hybrids of both.\nlidar_align - A simple method for finding the extrinsic calibration between a 3D lidar and a 6-dof pose sensor.\nkalibr - The Kalibr visual-inertial calibration toolbox.\nCalibnet - Self-Supervised Extrinsic Calibration using 3D Spatial Transformer Networks.\nlidar_-----> camera !!! _calibration - ROS package to find a rigid-body transformation between a LiDAR and a -----> camera !!! .\nILCC - Reflectance Intensity Assisted Automatic and Accurate Extrinsic Calibration of 3D LiDAR.\neasy_handeye - Simple, straighforward ROS library for hand-eye calibration.\nimu_utils - A ROS package tool to analyze the IMU performance.\nkalibr_allan - IMU Allan standard deviation charts for use with Kalibr and inertial kalman filters.\npyquaternion - A full-featured Python module for representing and using quaternions.\nrobot_calibration - This package offers calibration of a number of parameters of a robot, such as: 3D Camera intrinsics, extrinsics Joint angle offsets and robot frame offsets.\nmulti_sensor_calibration - Contains a calibration tool to calibrate a sensor setup consisting of lidars, radars and cameras.\nLiDARTag - A Real-Time Fiducial Tag using Point Clouds Lidar Data.\nmulticam_calibration - Extrinsic and intrinsic calbration of cameras.\nikpy - An Inverse Kinematics library aiming performance and modularity.\nlivox_camera_lidar_calibration - Calibrate the extrinsic parameters between Livox LiDAR and camera.\nlidar_camera_calibration - Camera LiDAR Calibration using ROS, OpenCV, and PCL.\ne2calib - Contains code that implements video reconstruction from event data for calibration.\nPerception Pipeline\nSARosPerceptionKitti - ROS package for the Perception (Sensor Processing, Detection, Tracking and Evaluation) of the KITTI Vision Benchmark Suite.\nmultiple-object-tracking-lidar - C++ implementation to Detect, track and classify multiple objects using LIDAR scans or point cloud.\ncadrl_ros - ROS package for dynamic obstacle avoidance for ground robots trained with deep RL.\nAugmentedAutoencoder - RGB-based pipeline for object detection and 6D pose estimation.\njsk_recognition - A stack for the perception packages which are used in JSK lab.\nGibsonEnv - Gibson Environments: Real-World Perception for Embodied Agents.\nmorefusion - Multi-object Reasoning for 6D Pose Estimation from Volumetric Fusion.\nMachine Learning\nDLIB - A toolkit for making real world machine learning and data analysis applications in C++.\nfastai - The fastai library simplifies training fast and accurate neural nets using modern best practices.\ntpot - A Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.\ndeap - Distributed Evolutionary Algorithms in Python.\ngym - A toolkit for developing and comparing reinforcement learning algorithms.\ntensorflow_ros_cpp - A ROS package that allows to do Tensorflow inference in C++ without the need to compile TF yourself.\nTensorflow Federated - TensorFlow Federated (TFF) is an open-source framework for machine learning and other computations on decentralized data.\nfinn - Fast, Scalable Quantized Neural Network Inference on FPGAs.\nneuropod - Neuropod is a library that provides a uniform interface to run deep learning models from multiple frameworks in C++ and Python.\nleela-zero - This is a fairly faithful reimplementation of the system described in the Alpha Go Zero paper \"Mastering the Game of Go without Human Knowledge\".\nTrax - A library for deep learning that focuses on sequence models and reinforcement learning.\nmlflow - A platform to streamline machine learning development, including tracking experiments, packaging code into reproducible runs, and sharing and deploying models.\nNetron - Visualizer for neural network, deep learning and machine learning models.\nMNN - A blazing fast, lightweight deep learning framework, battle-tested by business-critical use cases in Alibaba.\nTensorforce - An open-source deep reinforcement learning framework, with an emphasis on modularized flexible library design and straightforward usability for applications in research and practice.\nDopamine - A research framework for fast prototyping of reinforcement learning algorithms.\ncatalyst - Was developed with a focus on reproducibility, fast experimentation and code/ideas reusing.\nray - A fast and simple framework for building and running distributed applications.\ntf-agents - A reliable, scalable and easy to use TensorFlow library for Contextual Bandits and Reinforcement Learning.\nReAgent - An open source end-to-end platform for applied reinforcement learning (RL) developed and used at Facebook.\nAwesome-Mobile-Machine-Learning - A curated list of awesome mobile machine learning resources for iOS, Android, and edge devices.\ncnn-explainer - Learning Convolutional Neural Networks with Interactive Visualization.\nmodelzoo - A collection of machine-learned models for use in autonomous driving applications.\nnnstreamer-ros - A set of Gstreamer plugins and ROS examples that allow Gstreamer developers to adopt neural network models easily and efficiently and neural network developers to manage neural network pipelines and their filters easily and efficiently.\nParallel Processing\ndask - Parallel computing with task scheduling for Python.\ncupy - NumPy-like API accelerated with CUDA.\nThrust - A C++ parallel programming library which resembles the C++ Standard Library.\nArrayFire - A general purpose GPU library.\nOpenMP - An application programming interface that supports multi-platform shared memory multiprocessing programming in C, C++, and Fortran.\nVexCL - VexCL is a C++ vector expression template library for OpenCL/CUDA/OpenMP.\nPYNQ - An open-source project from Xilinx that makes it easy to design embedded systems with Zynq All Programmable Systems on Chips.\nnumba - NumPy aware dynamic Python compiler using LLVM.\nTensorRT - A C++ library for high performance inference on NVIDIA GPUs and deep learning accelerators.\nlibcudacxx - Provides a heterogeneous implementation of the C++ Standard Library that can be used in and between CPU and GPU code.\nImage Processing\nCV-pretrained-model - A collection of computer vision pre-trained models.\nimage_pipeline - Fills the gap between getting raw images from a camera driver and higher-level vision processing.\ngstreamer - A pipeline-based multimedia framework that links together a wide variety of media processing systems to complete complex workflows.\nros2_openvino_toolkit - Provides a ROS-adaptered runtime framework of neural network which quickly deploys applications and solutions for vision inference.\nvision_visp - Wraps the ViSP moving edge tracker provided by the ViSP visual servoing library into a ROS package.\napriltag_ros - A ROS wrapper of the AprilTag 3 visual fiducial detector.\ndeep_object_pose - Deep Object Pose Estimation.\nDetectAndTrack - Detect-and-Track: Efficient Pose.\nSfMLearner - An unsupervised learning framework for depth and ego-motion estimation.\nimgaug - Image augmentation for machine learning experiments.\nvision_opencv - Packages for interfacing ROS with OpenCV, a library of programming functions for real time computer vision.\ndarknet_ros - YOLO ROS: Real-Time Object Detection for ROS.\nros_ncnn - YOLACT / YOLO ( among other things ) on NCNN inference engine for ROS.\ntf-pose-estimation - Deep Pose Estimation implemented using Tensorflow with Custom Architectures for fast inference.\nfind-object - Simple Qt interface to try OpenCV implementations of SIFT, SURF, FAST, BRIEF and other feature detectors and descriptors.\nyolact - A simple, fully convolutional model for real-time instance segmentation.\nKimera-Semantics - Real-Time 3D Semantic Reconstruction from 2D data.\ndetectron2 - A next-generation research platform for object detection and segmentation.\nOpenVX - Enables performance and power-optimized computer vision processing, especially important in embedded and real-time use cases.\n3d-vehicle-tracking - Official implementation of Joint Monocular 3D Vehicle Detection and Tracking.\npysot - The goal of PySOT is to provide a high-quality, high-performance codebase for visual tracking research.\nsemantic_slam - Real time semantic slam in ROS with a hand held RGB-D camera.\nkitti_scan_unfolding - We propose KITTI scan unfolding in our paper Scan-based Semantic Segmentation of LiDAR Point Clouds: An Experimental Study.\npacknet-sfm - Official PyTorch implementation of self-supervised monocular depth estimation methods invented by the ML Team at Toyota Research Institute (TRI).\nAB3DMOT - This work proposes a simple yet accurate real-time baseline 3D multi-object tracking system.\nmonoloco - Official implementation of \"MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty Estimation\" in PyTorch.\nPoly-YOLO - Builds on the original ideas of YOLOv3 and removes two of its weaknesses: a large amount of rewritten labels and inefficient distribution of anchors.\nsatellite-image-deep-learning - Resources for deep learning with satellite & aerial imagery.\nrobosat - Semantic segmentation on aerial and satellite imagery.\nbig_transfer - Model for General Visual Representation Learning created by Google Research.\nLEDNet - A Lightweight Encoder-Decoder Network for Real-time Semantic Segmentation.\nTorchSeg - This project aims at providing a fast, modular reference implementation for semantic segmentation models using PyTorch.\nsimpledet - A Simple and Versatile Framework for Object Detection and Instance Recognition.\nmeshroom - Meshroom is a free, open-source 3D Reconstruction Software based on the AliceVision Photogrammetric Computer Vision framework.\nEasyOCR - Ready-to-use Optical character recognition (OCR) with 40+ languages supported including Chinese, Japanese, Korean and Thai.\npytracking - A general python framework for visual object tracking and video object segmentation, based on PyTorch.\nros_deep_learning - Deep learning inference nodes for ROS with support for NVIDIA Jetson TX1/TX2/Xavier and TensorRT.\nhyperpose - HyperPose: A Flexible Library for Real-time Human Pose Estimation.\nfawkes - Privacy preserving tool against facial recognition systems.\nanonymizer - An anonymizer to obfuscate faces and license plates.\nopendatacam - Only saves surveyed meta-data, in particular the path an object moved or number of counted objects at a certain point.\nCam2BEV - TensorFlow Implementation for Computing a Semantically Segmented Bird's Eye View (BEV) Image Given the Images of Multiple Vehicle-Mounted Cameras.\nflownet2-pytorch - Pytorch implementation of FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks.\nSimd - C++ image processing and machine learning library with using of SIMD: SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, AVX, AVX2, AVX-512, VMX(Altivec) and VSX(Power7), NEON for ARM.\nAliceVision - A Photogrammetric Computer Vision Framework which provides a 3D Reconstruction and Camera Tracking algorithms.\nsatpy - A python library for reading and manipulating meteorological remote sensing data and writing it to various image and data file formats.\neo-learn - A collection of open source Python packages that have been developed to seamlessly access and process spatio-temporal image sequences acquired by any satellite fleet in a timely and automatic manner.\nlibvips - A fast image processing library with low memory needs.\nRadar Processing\npyroSAR - Framework for large-scale SAR satellite data processing.\nCameraRadarFusionNet - TUM Roborace Team Software Stack - Path tracking control, velocity control, curvature control and state estimation.\nLidar and Point Cloud Processing\ncilantro - A lean C++ library for working with point cloud data.\nopen3d - Open3D: A Modern Library for 3D Data Processing.\nSqueezeSeg - Implementation of SqueezeSeg, convolutional neural networks for LiDAR point clout segmentation.\npoint_cloud_io - ROS nodes to read and write point clouds from and to files (e.g. ply, vtk).\npython-pcl - Python bindings to the pointcloud library.\nlibpointmatcher - An \"Iterative Closest Point\" library for 2-D/3-D mapping in Robotics.\ndepth_clustering - Fast and robust clustering of point clouds generated with a Velodyne sensor.\nlidar-bonnetal - Semantic and Instance Segmentation of LiDAR point clouds for autonomous driving.\nCSF - LiDAR point cloud ground filtering / segmentation (bare earth extraction) method based on cloth simulation.\nrobot_body_filter - A highly configurable LaserScan/PointCloud2 filter that allows to dynamically remove the 3D body of the robot from the measurements.\ngrid_map - Universal grid map library for mobile robotic mapping.\nelevation_mapping - Robot-centric elevation mapping for rough terrain navigation.\nrangenet_lib - Contains simple usage explanations of how the RangeNet++ inference works with the TensorRT and C++ interface.\npointcloud_to_laserscan - Converts a 3D Point Cloud into a 2D laser scan.\noctomap - An Efficient Probabilistic 3D Mapping Framework Based on Octrees.\npptk - Point Processing Toolkit from HEREMaps.\ngpu-voxels - GPU-Voxels is a CUDA based library which allows high resolution volumetric collision detection between animated 3D models and live pointclouds from 3D sensors of all kinds.\nspatio_temporal_voxel_layer - A new voxel layer leveraging modern 3D graphics tools to modernize navigation environmental representations.\nLAStools - Award-winning software for efficient LiDAR processing.\nPCDet - A general PyTorch-based codebase for 3D object detection from point cloud.\nPDAL - A C++ BSD library for translating and manipulating point cloud data.\nPotreeConverter - Builds a potree octree from las, laz, binary ply, xyz or ptx files.\nfast_gicp - A collection of GICP-based fast point cloud registration algorithms.\nndt_omp - Multi-threaded and SSE friendly NDT algorithm.\nlaser_line_extraction - A ROS packages that extracts line segments from LaserScan messages.\nGo-ICP - Implementation of the Go-ICP algorithm for globally optimal 3D pointset registration.\nPointCNN - A simple and general framework for feature learning from point clouds.\nsegmenters_lib - The LiDAR segmenters library, for segmentation-based detection.\nMotionNet - Joint Perception and Motion Prediction for Autonomous Driving Based on Bird's Eye View Maps.\nPolarSeg - An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation.\ntraversability_mapping - Takes in point cloud from a Velodyne VLP-16 Lidar and outputs a traversability map for autonomous navigation in real-time.\nlidar_super_resolution - Simulation-based Lidar Super-resolution for Ground Vehicles.\nCupoch - A library that implements rapid 3D data processing and robotics computation using CUDA.\nlinefit_ground_segmentation - Implementation of the ground segmentation algorithm.\nDraco - A library for compressing and decompressing 3D geometric meshes and point clouds.\nVotenet - Deep Hough Voting for 3D Object Detection in Point Clouds.\nlidar_undistortion - Provides lidar motion undistortion based on an external 6DoF pose estimation input.\nsuperpoint_graph - Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs.\nRandLA-Net - Efficient Semantic Segmentation of Large-Scale Point Clouds.\nDet3D - A first 3D Object Detection toolbox which provides off the box implementations of many 3D object detection algorithms such as PointPillars, SECOND, PIXOR.\nOverlapNet - A modified Siamese Network that predicts the overlap and relative yaw angle of a pair of range images generated by 3D LiDAR scans.\nmp2p_icp - A repertory of multi primitive-to-primitive (MP2P) ICP algorithms in C++.\nOpenPCDet - A Toolbox for LiDAR-based 3D Object Detection.\ntorch-points3d - Pytorch framework for doing deep learning on point clouds.\nPolyFit - Polygonal Surface Reconstruction from Point Clouds.\nmmdetection3d - Next-generation platform for general 3D object detection.\ngpd - Takes a point cloud as input and produces pose estimates of viable grasps as output.\nSalsaNext - Uncertainty-aware Semantic Segmentation of LiDAR Point Clouds for Autonomous Driving.\nSuper-Fast-Accurate-3D-Object-Detection - Super Fast and Accurate 3D Object Detection based on 3D LiDAR Point Clouds (The PyTorch implementation).\nkaolin - A PyTorch Library for Accelerating 3D Deep Learning Research.\nCamVox - A low-cost SLAM system based on camera and Livox lidar.\nSA-SSD - Structure Aware Single-stage 3D Object Detection from Point Cloud.\ncuda-pcl - Accelerating Lidar for Robotics with NVIDIA CUDA-based PCL.\nLocalization and State Estimation\nevo - Python package for the evaluation of odometry and SLAM.\nrobot_localization - A package of nonlinear state estimation nodes.\nfuse - General architecture for performing sensor fusion live on a robot.\nGeographicLib - A C++ library for geographic projections.\nntripbrowser - A Python API for browsing NTRIP (Networked Transport of RTCM via Internet Protocol).\nimu_tools - IMU-related filters and visualizers.\nRTKLIB - A version of RTKLIB optimized for single and dual frequency low cost GPS receivers, especially u-blox receivers.\ngLAB - Performs precise modeling of GNSS observables (pseudorange and carrier phase) at the centimetre level, allowing standalone GPS positioning, PPP, SBAS and DGNSS.\nai-imu-dr - Contains the code of our novel accurate method for dead reckoning of wheeled vehicles based only on an IMU.\nKalman-and-Bayesian-Filters-in-Python - Kalman Filter book using Jupyter Notebook.\nmcl_3dl - A ROS node to perform a probabilistic 3-D/6-DOF localization system for mobile robots with 3-D LIDAR(s).\nse2lam - On-SE(2) Localization and Mapping for Ground Vehicles by Fusing Odometry and Vision.\nmmWave-localization-learning - ML-based positioning method from mmWave transmissions - with high accuracy and energy efficiency.\ndynamic_robot_localization - A ROS package that offers 3 DoF and 6 DoF localization using PCL and allows dynamic map update using OctoMap.\neagleye - An open-source software for vehicle localization utilizing GNSS and IMU.\npython-sgp4 - Python version of the SGP4 satellite position library.\nPROJ - Cartographic Projections and Coordinate Transformations Library.\nrpg_trajectory_evaluation - Implements common used trajectory evaluation methods for visual(-inertial) odometry.\npymap3d - Pure-Python (Numpy optional) 3D coordinate conversions for geospace ecef enu eci.\nlibRSF - A robust sensor fusion library for online localization.\nSimultaneous Localization and Mapping\nLidar\nloam_velodyne - Laser Odometry and Mapping (Loam) is a realtime method for state estimation and mapping using a 3D lidar.\nlio-mapping - Implementation of Tightly Coupled 3D Lidar Inertial Odometry and Mapping (LIO-mapping).\nA-LOAM - Advanced implementation of LOAM.\nFast LOAM - Fast and Optimized Lidar Odometry And Mapping.\nLIO_SAM - Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping.\ncartographer_ros - Provides ROS integration for Cartographer.\nloam_livox - A robust LiDAR Odometry and Mapping (LOAM) package for Livox-LiDAR.\nStaticMapping - Use LiDAR to map the static world.\nsemantic_suma - Semantic Mapping using Surfel Mapping and Semantic Segmentation.\nslam_toolbox - Slam Toolbox for lifelong mapping and localization in potentially massive maps with ROS .\nmaplab - An open visual-inertial mapping framework.\nhdl_graph_slam - An open source ROS package for real-time 6DOF SLAM using a 3D LIDAR.\ninteractive_slam - In contrast to existing automatic SLAM packages, we with minimal human effort.\nLeGO-LOAM - Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain.\npyslam - Contains a monocular Visual Odometry (VO) pipeline in Python.\nKitware SLAM - LiDAR-only visual SLAM developped by Kitware, as well as ROS and ParaView wrappings for easier use.\nhorizon_highway_slam - A robust, low drift, and real time highway SLAM package suitable for Livox Horizon lidar.\nmola - A Modular System for Localization and Mapping.\nDH3D - Deep Hierarchical 3D Descriptors for Robust Large-Scale 6DOF Relocalization.\nLaMa - LaMa is a C++11 software library for robotic localization and mapping.\nScan Context - Global LiDAR descriptor for place recognition and long-term localization.\nM-LOAM - Robust Odometry and Mapping for Multi-LiDAR Systems with Online Extrinsic Calibration.\nVisual\norb_slam_2_ros - A ROS implementation of ORB_SLAM2.\norbslam-map-saving-extension - In this extensions the map of ORB-features be saved to the disk as a reference for future runs along the same track.\ndso - Direct Sparse Odometry.\nviso2 - A ROS wrapper for libviso2, a library for visual odometry.\nxivo - X Inertial-aided Visual Odometry.\nrovio - Robust Visual Inertial Odometry Framework.\nLSD-SLAM - Large-Scale Direct Monocular SLAM is a real-time monocular SLAM.\nCubeSLAM and ORB SLAM - Monocular 3D Object Detection and SLAM Package of CubeSLAM and ORB SLAM.\nVINS-Fusion - A Robust and Versatile Multi-Sensor Visual-Inertial State Estimator.\nopenvslam - OpenVSLAM: A Versatile Visual SLAM Framework.\nbasalt - Visual-Inertial Mapping with Non-Linear Factor Recovery.\nKimera - A C++ library for real-time metric-semantic simultaneous localization and mapping, which uses camera images and inertial data to build a semantically annotated 3D mesh of the environment.\ntagslam - A ROS-based package for Simultaneous Localization and Mapping using AprilTag fiducial markers.\nLARVIO - A lightweight, accurate and robust monocular visual inertial odometry based on Multi-State Constraint Kalman Filter.\nfiducials - Simultaneous localization and mapping using fiducial markers.\nopen_vins - An open source platform for visual-inertial navigation research.\nORB_SLAM3 - ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM.\nAtlas - End-to-End 3D Scene Reconstruction from Posed Images.\nvilib - This library focuses on the front-end of VIO pipelines with CUDA.\nhloc - A modular toolbox for state-of-the-art 6-DoF visual localization. It implements Hierarchical Localization, leveraging image retrieval and feature matching, and is fast, accurate, and scalable.\nESVO - A novel pipeline for real-time visual odometry using a stereo event-based camera.\ngradslam - An open source differentiable dense SLAM library for PyTorch.\nVector Map\nOpenDRIVE - An open file format for the logical description of road networks.\nMapsModelsImporter - A Blender add-on to import models from google maps.\nLanelet2 - Map handling framework for automated driving.\nbarefoot - Online and Offline map matching that can be used stand-alone and in the cloud.\niD - The easy-to-use OpenStreetMap editor in JavaScript.\nRapiD - An enhanced version of iD for mapping with AI created by Facebook.\nsegmap - A map representation based on 3D segments.\nMapbox - A JavaScript library for interactive, customizable vector maps on the web.\nosrm-backend - Open Source Routing Machine - C++ backend.\nassuremapingtools - Desktop based tool for viewing, editing and saving road network maps for autonomous vehicle platforms such as Autoware.\ngeopandas - A project to add support for geographic data to pandas objects.\nMapToolbox - Plugins to make Autoware vector maps in Unity.\nimagery-index - An index of aerial and satellite imagery useful for mapping.\nmapillary_tools - A library for processing and uploading images to Mapillary.\nmapnik - Combines pixel-perfect image output with lightning-fast cartographic algorithms, and exposes interfaces in C++, Python, and Node.\ngdal - GDAL is an open source X/MIT licensed translator library for raster and vector geospatial data formats.\ngrass - GRASS GIS - free and open source Geographic Information System (GIS).\n3d-tiles - Specification for streaming massive heterogeneous 3D geospatial datasets.\nosmnx - Python for street networks. Retrieve, model, analyze, and visualize street networks and other spatial data from OpenStreetMap.\nPrediction\nAwesome-Interaction-aware-Trajectory-Prediction - A selection of state-of-the-art research materials on trajectory prediction.\nsgan - Socially Acceptable Trajectories with Generative Adversarial Networks.\nBehavior and Decision\nGroot - Graphical Editor to create BehaviorTrees. Compliant with BehaviorTree.CPP.\nBehaviorTree.CPP - Behavior Trees Library in C++.\nRAFCON - Uses hierarchical state machines, featuring concurrent state execution, to represent robot programs.\nROSPlan - Generic framework for task planning in a ROS system.\nad-rss-lib - Library implementing the Responsibility Sensitive Safety model (RSS) for Autonomous Vehicles.\nFlexBE - Graphical editor for hierarchical state machines, based on ROS's smach.\nsts_bt_library - This library provides the functionality to set up your own behavior tree logic by using the defined tree structures like Fallback, Sequence or Parallel Nodes.\nSMACC - An Event-Driven, Asynchronous, Behavioral State Machine Library for real-time ROS (Robotic Operating System) applications written in C++ .\npy_trees_ros - Behaviours, trees and utilities that extend py_trees for use with ROS.\nPlanning and Control\npacmod - Designed to allow the user to control a vehicle with the PACMod drive-by-wire system.\nmpcc - Model Predictive Contouring Controller for Autonomous Racing.\nrrt - C++ RRT (Rapidly-exploring Random Tree) implementation.\nHypridAStarTrailer - A path planning algorithm based on Hybrid A* for trailer truck.\npath_planner - Hybrid A* Path Planner for the KTH Research Concept Vehicle.\nopen_street_map - ROS packages for working with Open Street Map geographic information.\nOpen Source Car Control - An assemblage of software and hardware designs that enable computer control of modern cars in order to facilitate the development of autonomous vehicle technology.\nfastrack - A ROS implementation of Fast and Safe Tracking (FaSTrack).\ncommonroad - Composable benchmarks for motion planning on roads.\ntraffic-editor - A graphical editor for robot traffic flows.\nsteering_functions - Contains a C++ library that implements steering functions for car-like robots with limited turning radius.\nmoveit - Easy-to-use robotics manipulation platform for developing applications, evaluating designs, and building integrated products.\nflexible-collision-library - A library for performing three types of proximity queries on a pair of geometric models composed of triangles.\naikido - Artificial Intelligence for Kinematics, Dynamics, and Optimization.\ncasADi - A symbolic framework for numeric optimization implementing automatic differentiation in forward and reverse modes on sparse matrix-valued computational graphs.\nACADO Toolkit - A software environment and algorithm collection for automatic control and dynamic optimization.\ncontrol-toolbox - An efficient C++ library for control, estimation, optimization and motion planning in robotics.\nCrowdNav - Crowd-aware Robot Navigation with Attention-based Deep Reinforcement Learning.\nompl - Consists of many state-of-the-art sampling-based motion planning algorithms.\nopenrave - Open Robotics Automation Virtual Environment: An environment for testing, developing, and deploying robotics motion planning algorithms.\nteb_local_planner - An optimal trajectory planner considering distinctive topologies for mobile robots based on Timed-Elastic-Bands.\npinocchio - A fast and flexible implementation of Rigid Body Dynamics algorithms and their analytical derivatives.\nrmf_core - The rmf_core packages provide the centralized functions of the Robotics Middleware Framework (RMF).\nOpEn - A solver for Fast & Accurate Embedded Optimization for next-generation Robotics and Autonomous Systems.\nautogenu-jupyter - This project provides the continuation/GMRES method (C/GMRES method) based solvers for nonlinear model predictive control (NMPC) and an automatic code generator for NMPC.\nglobal_racetrajectory_optimization - This repository contains multiple approaches for generating global racetrajectories.\ntoppra - A library for computing the time-optimal path parametrization for robots subject to kinematic and dynamic constraints.\ntinyspline - TinySpline is a small, yet powerful library for interpolating, transforming, and querying arbitrary NURBS, B-Splines, and B\u00e9zier curves.\ndual quaternions ros - ROS python package for dual quaternion SLERP.\nmb planner - Aerial vehicle planner for tight spaces. Used in DARPA SubT Challenge.\nilqr - Iterative Linear Quadratic Regulator with auto-differentiatiable dynamics models.\nEGO-Planner - A lightweight gradient-based local planner without ESDF construction, which significantly reduces computation time compared to some state-of-the-art methods.\npykep - A scientific library providing basic tools for research in interplanetary trajectory design.\nam_traj - Alternating Minimization Based Trajectory Generation for Quadrotor Aggressive Flight.\nGraphBasedLocalTrajectoryPlanner - Was used on a real race vehicle during the Roborace Season Alpha and achieved speeds above 200km/h.\nse2_navigation - Pure pursuit controller and Reeds-Shepp sampling based planner for navigation in SE(2) space.\nRuckig - Instantaneous Motion Generation. Real-time. Jerk-constrained. Time-optimal.\nUser Interaction\nGraphical User Interface\nimgui - Designed to enable fast iterations and to empower programmers to create content creation tools and visualization / debug tools.\nqtpy - Provides an uniform layer to support PyQt5, PySide2, PyQt4 and PySide with a single codebase.\nmir - Mir is set of libraries for building Wayland based shells.\nrqt - A Qt-based framework for GUI development for ROS. It consists of three parts/metapackages.\ncage - This is Cage, a Wayland kiosk. A kiosk runs a single, maximized application.\nchilipie - Easy-to-use Raspberry Pi image for booting directly into full-screen Chrome.\npencil - A tool for making diagrams and GUI prototyping that everyone can use.\ndynamic_reconfigure - The focus of dynamic_reconfigure is on providing a standard way to expose a subset of a node's parameters to external reconfiguration.\nddynamic_reconfigure - Allows modifying parameters of a ROS node using the dynamic_reconfigure framework without having to write cfg files.\nelements - A lightweight, fine-grained, resolution independent, modular GUI library.\nNanoGUI - A minimalistic cross-platform widget library for OpenGL 3.x or higher.\nAcoustic User Interface\npyo - A Python module written in C containing classes for a wide variety of audio signal processing types.\nrhasspy - Rhasspy (pronounced RAH-SPEE) is an offline, multilingual voice assistant toolkit inspired by Jasper that works well with Home Assistant, Hass.io, and Node-RED.\nmycroft-core - Mycroft is a hackable open source voice assistant.\nDDSP - A library of differentiable versions of common DSP functions (such as synthesizers, waveshapers, and filters).\nNoiseTorch - Creates a virtual microphone that suppresses noise, in any application.\nDeepSpeech - An open source Speech-To-Text engine, using a model trained by machine learning techniques based on Baidu's Deep Speech research paper.\nwaveglow - A Flow-based Generative Network for Speech Synthesis.\nTTS - A deep learning toolkit for Text-to-Speech, battle-tested in research and production.\nCommand Line Interface\nthe-art-of-command-line - Master the command line, in one page.\ndotfiles of cornerman - Powerful zsh and vim dotfiles.\ndotbot - A tool that bootstraps your dotfiles.\nprompt-hjem - A beautiful zsh prompt.\nag - A code-searching tool similar to ack, but faster.\nfzf - A command-line fuzzy finder.\npkgtop - Interactive package manager and resource monitor designed for the GNU/Linux.\nasciimatics - A cross platform package to do curses-like operations, plus higher level APIs and widgets to create text UIs and ASCII art animations.\ngocui - Minimalist Go package aimed at creating Console User Interfaces.\nTerminalImageViewer - Small C++ program to display images in a (modern) terminal using RGB ANSI codes and unicode block graphics characters.\nrosshow - Visualize ROS topics inside a terminal with Unicode/ASCII art.\npython-prompt-toolkit - Library for building powerful interactive command line applications in Python.\nguake - Drop-down terminal for GNOME.\nwemux - Multi-User Tmux Made Easy.\ntmuxp - A session manager built on libtmux.\nmapscii - World map renderer for your console.\nterminator - The goal of this project is to produce a useful tool for arranging terminals.\nbat - A cat(1) clone with wings.\nfx - Command-line tool and terminal JSON viewer.\ntmate - Instant terminal sharing.\nData Visualization and Mission Control\nxdot - Interactive viewer for graphs written in Graphviz's dot language.\nguacamole - Clientless remote desktop gateway. It supports standard protocols like VNC, RDP, and SSH.\nros3djs - 3D Visualization Library for use with the ROS JavaScript Libraries.\nwebviz - Web-based visualization libraries like rviz.\nplotly.py - An open-source, interactive graphing library for Python.\nPlotJuggler - The timeseries visualization tool that you deserve.\nbokeh - Interactive Data Visualization in the browser, from Python.\nvoila - From Jupyter notebooks to standalone web applications and dashboards.\nPangolin - Pangolin is a lightweight portable rapid development library for managing OpenGL display / interaction and abstracting video input.\nrqt_bag - Provides a GUI plugin for displaying and replaying ROS bag files.\nkepler.gl - Kepler.gl is a powerful open source geospatial analysis tool for large-scale data sets.\nqgis_ros - Access bagged and live topic data in a highly featured GIS environment.\nopenmct - A web based mission control framework.\nweb_video_server - HTTP Streaming of ROS Image Topics in Multiple Formats.\nRVizWeb - Provides a convenient way of building and launching a web application with features similar to RViz.\nmarvros - MAVLink to ROS gateway with proxy for Ground Control Station.\noctave - Provides a convenient command line interface for solving linear and nonlinear problems numerically, and for performing other numerical experiments using a language that is mostly compatible with Matlab.\nstreetscape.gl - Streetscape.gl is a toolkit for visualizing autonomous and robotics data in the XVIZ protocol.\nurdf-loaders - URDF Loaders for Unity and THREE.js with example ATHLETE URDF File.\nobs-studio - Free and open source software for live streaming and screen recording.\nK3D-tools - Jupyter notebook extension for 3D visualization.\nPyQtGraph - Fast data visualization and GUI tools for scientific / engineering applications.\nipygany - 3-D Scientific Visualization in the Jupyter Notebook.\nFoxglove Studio - Web and desktop app for robotics visualization and debugging; actively maintained fork of webviz.\nROS-Mobile - Visualization and controlling application for Android.\nAnnotation\nlabelbox - The fastest way to annotate data to build and ship artificial intelligence applications.\nPixelAnnotationTool - Annotate quickly images.\nLabelImg - A graphical image annotation tool and label object bounding boxes in images.\ncvat - Powerful and efficient Computer Vision Annotation Tool (CVAT).\npoint_labeler - Tool for labeling of a single point clouds or a stream of point clouds.\nlabel-studio - Label Studio is a multi-type data labeling and annotation tool with standardized output format.\nnapari - A fast, interactive, multi-dimensional image viewer for python.\nsemantic-segmentation-editor - A web based labeling tool for creating AI training data sets (2D and 3D).\n3d-bat - 3D Bounding Box Annotation Tool for Point cloud and Image Labeling.\nlabelme - Image Polygonal Annotation with Python (polygon, rectangle, circle, line, point and image-level flag annotation).\nuniversal-data-tool - Collaborate & label any type of data, images, text, or documents, in an easy web interface or desktop app.\nBMW-Labeltool-Lite - Provides you with a easy to use labeling tool for State-of-the-art Deep Learning training purposes.\nPoint Cloud\nCloudCompare - CloudCompare is a 3D point cloud (and triangular mesh) processing software.\nPotree - WebGL point cloud viewer for large datasets.\npoint_cloud_viewer - Makes viewing massive point clouds easy and convenient.\nLidarView - Performs real-time visualization and easy processing of live captured 3D LiDAR data from Lidar sensors.\nVeloView - Performs real-time visualization of live captured 3D LiDAR data from Velodyne's HDL sensors.\nentwine - A data organization library for massive point clouds, designed to conquer datasets of trillions of points as well as desktop-scale point clouds.\npolyscope - A C++ & Python viewer for 3D data like meshes and point clouds.\nPcx - Point cloud importer & renderer for Unity.\nImmersivePoints - A web-application for virtual reality devices to explore 3D data in the most natural way possible.\nRViz\nmapviz - Modular ROS visualization tool for 2D data.\nrviz_cinematographer - Easy to use tools to create and edit trajectories for the rviz camera.\nrviz_satellite - Display internet satellite imagery in RViz.\nrviz_visual_tools - C++ API wrapper for displaying shapes and meshes in Rviz.\nxpp - Visualization of motion-plans for legged robots.\nrviz stereo - 3D stereo rendering displays a different view to each eye so that the scene appears to have depth.\njsk_visualization - Jsk visualization ros packages for rviz and rqt.\nmoveit_visual_tools - Helper functions for displaying and debugging MoveIt! data in Rviz via published markers.\nOperation System\nMonitoring\nrosmon - ROS node launcher & monitoring daemon.\nmultimaster_fkie - GUI-based management environment that is very useful to manage ROS-launch configurations and control running nodes.\ncollectd - A small daemon which collects system information periodically and provides mechanisms to store and monitor the values in a variety of ways.\nlnav - An enhanced log file viewer that takes advantage of any semantic information that can be gleaned from the files being viewed, such as timestamps and log levels.\nhtop - An interactive text-mode process viewer for Unix systems. It aims to be a better 'top'.\natop - System and process monitor for Linux with logging and replay function.\npsutil - Cross-platform lib for process and system monitoring in Python.\ngputil - A Python module for getting the GPU status from NVIDA GPUs using nvidia-smi programmically in Python.\ngpustat - A simple command-line utility for querying and monitoring GPU status.\nnvtop - NVIDIA GPUs htop like monitoring tool.\nShellHub - ShellHub is a modern SSH server for remotely accessing linux devices via command line (using any SSH client) or web-based user interface, designed as an alternative to sshd. Think ShellHub as centralized SSH for the the edge and cloud computing.\nSshwifty - Sshwifty is a SSH and Telnet connector made for the Web.\nspdlog - Very fast, header-only/compiled, C++ logging library.\nctop - Top-like interface for container metrics.\nntop - Web-based Traffic and Security Network Traffic Monitoring.\njupyterlab-nvdashboard - A JupyterLab extension for displaying dashboards of GPU usage.\nDatabase and Record\nncdu - Ncdu is a disk usage analyzer with an ncurses interface.\nborg - Deduplicating archiver with compression and authenticated encryption.\nbag-database - A server that catalogs bag files and provides a web-based UI for accessing them.\nmarv-robotics - MARV Robotics is a powerful and extensible data management platform.\nkitti2bag - Convert KITTI dataset to ROS bag file the easy way.\npykitti - Python tools for working with KITTI data.\nrosbag_editor - Create a rosbag from a given one, using a simple GUI.\nnextcloud - Nextcloud is a suite of client-server software for creating and using file hosting services.\nros_type_introspection - Deserialize ROS messages that are unknown at compilation time.\nsyncthing - A continuous file synchronization program.\nrqt_bag_exporter - Qt GUI to export ROS bag topics to files (CSV and/or video).\nxviz - A protocol for real-time transfer and visualization of autonomy data.\nkitti_to_rosbag - A Dataset tools for working with the KITTI dataset raw data and converting it to a ROS bag. Also allows a library for direct access to poses, velodyne scans, and images.\nros_numpy - Tools for converting ROS messages to and from numpy arrays.\nkitti_ros - A ROS-based player to replay KiTTI dataset.\nDuckDB - An embeddable SQL OLAP Database Management System.\nNetwork Distributed File System\nsshfs - File system based on the SSH File Transfer Protocol.\nmoosefs - A scalable distributed storage system.\nceph - A distributed object, block, and file storage platform.\nnfs - A distributed file system protocol originally developed by Sun Microsystems.\nansible-role-nfs - Installs NFS utilities on RedHat/CentOS or Debian/Ubuntu.\nServer Infrastructure and High Performance Computing\nmass - Self-service, remote installation of Windows, CentOS, ESXi and Ubuntu on real servers turns your data centre into a bare metal cloud.\npolyaxon - A platform for reproducing and managing the whole life cycle of machine learning and deep learning applications.\nlocalstack - A fully functional local AWS cloud stack. Develop and test your cloud & Serverless apps offline.\nnvidia-docker - Build and run Docker containers leveraging NVIDIA GPUs.\nkubeflow - Machine Learning Toolkit for Kubernetes.\nlog-pilot - Collect logs for docker containers.\ntraefik - The Cloud Native Edge Router.\ngraylog2-server - Free and open source log management.\nansible - Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy.\npyinfra - It can be used for ad-hoc command execution, service deployment, configuration management and more.\ndocker-py - A Python library for the Docker Engine API.\nnoVNC - VNC client using HTML5.\nSlurm - Slurm: A Highly Scalable Workload Manager.\njupyterhub - Multi-user server for Jupyter notebooks.\nPortainer - Making Docker management easy.\nenroot - A simple, yet powerful tool to turn traditional container/OS images into unprivileged sandboxes.\ndocker-firefox - Run a Docker Container with Firefox and noVNC for remote access to headless servers.\nluigi - A Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in.\ntriton-inference-server - NVIDIA Triton Inference Server provides a cloud inferencing solution optimized for NVIDIA GPUs.\ncudf - Provides a pandas-like API that will be familiar to data engineers & data scientists, so they can use it to easily accelerate their workflows without going into the details of CUDA programming.\nEmbedded Operation System\nvxworks7-ros2-build - Build system to automate the build of VxWorks 7 and ROS2.\nYocto - Produce tools and processes that enable the creation of Linux distributions for embedded software that are independent of the underlying architecture of the embedded hardware.\nAutomotive Graded Linux - A collaborative open source project that is bringing together automakers, suppliers and technology companies to build a Linux-based, open software platform for automotive applications that can serve as the de facto industry standard.\nbitbake - A generic task execution engine that allows shell and Python tasks to be run efficiently and in parallel while working within complex inter-task dependency constraints.\nJailhouse - Jailhouse is a partitioning Hypervisor based on Linux.\nXen - An open-source (GPL) type-1 or baremetal hypervisor.\nQEMU - A generic and open source machine emulator and virtualizer.\nqemu-xilinx - A fork of Quick EMUlator (QEMU) with improved support and modelling for the Xilinx platforms.\nrosserial - A ROS client library for small, embedded devices, such as Arduino.\nmeta-ros - OpenEmbedded Layer for ROS Applications.\nmeta-balena - Run Docker containers on embedded devices.\nmicro-ros - The major changes compared to \"regular\" ROS 2 is that micro-ROS uses a Real-Time Operating System (RTOS) instead of Linux, and DDS for eXtremely Resource Constrained Environments.\nnvidia-container-runtime - NVIDIA Container Runtime is a GPU aware container runtime, compatible with the Open Containers Initiative (OCI) specification used by Docker, CRI-O, and other popular container technologie.\nfusesoc - Package manager and build abstraction tool for FPGA/ASIC development.\njetson_easy - Automatically script to setup and configure your NVIDIA Jetson.\ndocker-jetpack-sdk - Allows for usage of the NVIDIA JetPack SDK within a docker container for download, flashing, and install.\nPressed - Provides a way to set answers to questions asked during the installation process of debian, without having to manually enter the answers while the installation is running.\njetson_stats - A package to monitoring and control your NVIDIA Jetson (Xavier NX, Nano, AGX Xavier, TX1, TX2) Works with all NVIDIA Jetson ecosystem.\nros_jetson_stats - The ROS jetson-stats wrapper. The status of your NVIDIA jetson in diagnostic messages.\nOpenCR - Open-source Control Module for ROS.\nacrn-hypervisor - Defines a device hypervisor reference stack and an architecture for running multiple software subsystems, managed securely, on a consolidated system by means of a virtual machine manager.\njetson-containers - Machine Learning Containers for Jetson and JetPack 4.4.\nReal-Time Kernel\nELISA - Project is to make it easier for companies to build and certify Linux-based safety-critical applications \u2013 systems whose failure could result in loss of human life, significant property damage or environmental damage.\nPREEMPT_RT kernel patch - Aim of the PREEMPT_RT kernel patch is to minimize the amount of kernel code that is non-preemptible.\nNetwork and Middleware\nperformance_test - Tool to test the performance of pub/sub based communication frameworks.\nrealtime_support - Minimal real-time testing utility for measuring jitter and latency.\nros1_bridge - ROS 2 package that provides bidirectional communication between ROS 1 and ROS 2.\nFast-RTPS - A Protocol, which provides publisher-subscriber communications over unreliable transports such as UDP, as defined and maintained by the Object Management Group (OMG) consortium.\nprotobuf - Google's data interchange format.\nopensplice - Vortex OpenSplice Community Edition.\ncyclonedds - Eclipse Cyclone DDS is a very performant and robust open-source DDS implementation.\niceoryx - An IPC middleware for POSIX-based systems.\nrosbridge_suite - Provides a JSON interface to ROS, allowing any client to send JSON to publish or subscribe to ROS topics, call ROS services, and more.\nros2arduino - This library helps the Arduino board communicate with the ROS2 using XRCE-DDS.\neCAL - The enhanced communication abstraction layer (eCAL) is a middleware that enables scalable, high performance interprocess communication on a single computer node or between different nodes in a computer network.\nAUTOSAR-Adaptive - The implementation of AUTOSAR Adaptive Platform based on the R19-11.\nocpp - The Open Charge Point Protocol (OCPP) is a network protocol for communication between electric vehicle chargers and a central backoffice system.\nmicro-ROS for Arduino - A experimental micro-ROS library for baremetal projects based on Arduino IDE or Arduino CLI.\nmqtt_bridge - Provides a functionality to bridge between ROS and MQTT in bidirectional.\nEthernet and Wireless Networking\nSOES - SOES is an EtherCAT slave stack written in C.\nnetplan - Simply create a YAML description of the required network interfaces and what each should be configured to do.\nairalab - AIRA is reference Robonomics network client for ROS-enabled cyber-physical systems.\nrdbox - RDBOX is a IT infrastructure for ROS robots.\nros_ethercat - This is a reimplementation of the main loop of pr2_ethercat without dependencies on PR2 software.\nwavemon - An ncurses-based monitoring application for wireless network devices.\nwireless - Making info about wireless networks available to ROS.\nptpd - PTP daemon (PTPd) is an implementation the Precision Time Protocol (PTP) version 2 as defined by 'IEEE Std 1588-2008'. PTP provides precise time coordination of Ethernet LAN connected computers.\niperf - A TCP, UDP, and SCTP network bandwidth measurement tool.\ntcpreplay - Pcap editing and replay tools.\nnethogs - It groups bandwidth by process.\npyshark - Python wrapper for tshark, allowing python packet parsing using wireshark dissectors.\npingtop - Ping multiple servers and show results in a top-like terminal UI.\ntermshark - A terminal UI for tshark, inspired by Wireshark.\nudpreplay - Replay UDP packets from a pcap file.\nopenwifi - Linux mac80211 compatible full-stack IEEE802.11/Wi-Fi design based on Software Defined Radio.\nController Area Network\nawesome CAN - A curated list of awesome CAN bus tools, hardware and resources.\nAndrOBD - Android OBD diagnostics with any ELM327 adapter.\nddt4all - DDT4All is a tool to create your own ECU parameters screens and connect to a CAN network with a cheap ELM327 interface.\ncabana - CAN visualizer and DBC maker.\nopendbc - The project to democratize access to the decoder ring of your car.\nlibuavcan - An open lightweight protocol designed for reliable communication in aerospace and robotic applications over robust vehicular networks such as CAN bus.\npython-can - The can package provides controller area network support for Python developers.\nCANopenNode - The internationally standardized (EN 50325-4) (CiA301) CAN-based higher-layer protocol for embedded control system.\npython-udsoncan - Python implementation of UDS (ISO-14229) standard.\nuds-c - Unified Diagnostics Service (UDS) and OBD-II (On Board Diagnostics for Vehicles) C Library.\ncantools - CAN BUS tools in Python 3.\nCANdevStudio - CANdevStudio aims to be cost-effective replacement for CAN simulation software. It can work with variety of CAN hardware interfaces.\ncan-utils - Linux-CAN / SocketCAN user space applications.\nros_canopen - CANopen driver framework for ROS.\ndecanstructor - The definitive ROS CAN analysis tool.\nkvaser_interface - This package was developed as a standardized way to access Kvaser CAN devices from ROS.\ncanmatrix - Converting CAN Database Formats .arxml .dbc .dbf .kcd.\nautosar - A set of python modules for working with AUTOSAR XML files.\ncanopen - A Python implementation of the CANopen standard. The aim of the project is to support the most common parts of the CiA 301 standard in a Pythonic interface.\nSavvyCAN - A Qt5 based cross platform tool which can be used to load, save, and capture canbus frames.\nOpen-Vehicle-Monitoring-System-3 - The system provides live monitoring of vehicle metrics like state of charge, temperatures, tyre pressures and diagnostic fault conditions.\nSensor and Acuator Interfaces\nTesla-API - Provides functionality to monitor and control the Model S (and future Tesla vehicles) remotely.\nflirpy - A Python library to interact with FLIR thermal imaging cameras and images.\nnerian_stereo - ROS node for Nerian's SceneScan and SP1 stereo vision sensors.\npymmw - This is a toolbox composed of Python scripts to interact with TI's evaluation module (BoosterPack) for the IWR1443 mmWave sensing device.\nti_mmwave_rospkg - TI mmWave radar ROS driver (with sensor fusion and hybrid).\npacmod3 - This ROS node is designed to allow the user to control a vehicle with the PACMod drive-by-wire system, board revision 3.\nros2_intel_realsense - These are packages for using Intel RealSense cameras (D400 series) with ROS2.\nsick_scan - This stack provides a ROS2 driver for the SICK TiM series of laser scanners.\nouster_example - Sample code for connecting to and configuring the OS1, reading and visualizing data, and interfacing with ROS.\nros2_ouster_drivers - These are an implementation of ROS2 drivers for the Ouster OS-1 3D lidars.\nlivox_ros_driver - A new ROS package, specially used to connect LiDAR products produced by Livox.\nvelodyne - A collection of ROS packages supporting Velodyne high definition 3D LIDARs.\nublox - Provides support for u-blox GPS receivers.\ncrazyflie_ros - ROS Driver for Bitcraze Crazyflie.\npointgrey_camera_driver - ROS driver for Pt. Grey cameras, based on the official FlyCapture2 SDK.\nnovatel_gps_driver - ROS driver for NovAtel GPS / GNSS receivers.\npylon-ros-camera - The official pylon ROS driver for Basler GigE Vision and USB3 Vision cameras.\nethz_piksi_ros - Contains (python) ROS drivers, tools, launch files, and wikis about how to use Piksi Real Time Kinematic (RTK) GPS device in ROS.\nsick_safetyscanners - A ROS Driver which reads the raw data from the SICK Safety Scanners and publishes the data as a laser_scan msg.\nbosch_imu_driver - A driver for the sensor IMU Bosch BNO055. It was implemented only the UART communication interface (correct sensor mode should be selected).\noxford_gps_eth - Ethernet interface to OxTS GPS receivers using the NCOM packet structure.\nifm3d - Library and Utilities for working with ifm pmd-based 3D ToF Cameras.\ncepton_sdk_redist - Provides ROS support for Cepton LiDAR.\njetson_csi_cam - A ROS package making it simple to use CSI cameras on the Nvidia Jetson TK1, TX1, or TX2 with ROS.\nros_astra_camera - A ROS driver for Orbbec 3D cameras.\nspot_ros - ROS Driver for Spot.\nblickfeld-scanner-lib - Cross-platform library to communicate with LiDAR devices of the Blickfeld GmbH.\nTauLidarCamera - The host-side API for building applications with the Tau LiDAR Camera.\nSecurity\nowasp-threat-dragon-desktop - Threat Dragon is a free, open-source, cross-platform threat modeling application including system diagramming and a rule engine to auto-generate threats/mitigations.\nlaunch_ros_sandbox - Can define launch files running nodes in restrained environments, such as Docker containers or separate user accounts with limited privileges.\nwolfssl - A small, fast, portable implementation of TLS/SSL for embedded devices to the cloud.\nCANalyzat0r - Security analysis toolkit for proprietary car protocols.\nRSF - Robot Security Framework (RSF) is a standardized methodology to perform security assessments in robotics.\nHow-to-Secure-A-Linux-Server - An evolving how-to guide for securing a Linux server.\nlynis - Security auditing tool for Linux, macOS, and UNIX-based systems. Assists with compliance testing (HIPAA/ISO27001/PCI DSS) and system hardening.\nOpenVPN - An open source VPN daemon.\nopenfortivpn - A client for PPP+SSL VPN tunnel services and compatible with Fortinet VPNs.\nWireGuard - WireGuard is a novel VPN that runs inside the Linux Kernel and utilizes state-of-the-art cryptography.\nssh-auditor - Scans for weak ssh passwords on your network.\nvulscan - Advanced vulnerability scanning with Nmap NSE.\nnmap-vulners - NSE script based on Vulners.com API.\nbrutespray - Automatically attempts default creds on found services.\nfail2ban - Daemon to ban hosts that cause multiple authentication errors.\nDependencyCheck - A software composition analysis utility that detects publicly disclosed vulnerabilities in application dependencies.\nFirejail - A SUID sandbox program that reduces the risk of security breaches by restricting the running environment of untrusted applications using Linux namespaces, seccomp-bpf and Linux capabilities.\nRVD - Robot Vulnerability Database. Community-contributed archive of robot vulnerabilities and weaknesses.\nros2_dds_security - Adding security enhancements by defining a Service Plugin Interface (SPI) architecture, a set of builtin implementations of the SPIs, and the security model enforced by the SPIs.\nSecurity-Enhanced Linux - A Linux kernel security module that provides a mechanism for supporting access control security policies, including mandatory access controls (MAC).\nOpenTitan - Will make the silicon Root of Trust design and implementation more transparent, trustworthy, and secure for enterprises, platform providers, and chip manufacturers. OpenTitan is administered by lowRISC CIC as a collaborative project to produce high quality, open IP for instantiation as a full-featured product.\nbandit - A tool designed to find common security issues in Python code.\nhardening - A quick way to make a Ubuntu server a bit more secure.\nPassbolt - Passbolt is a free and open source password manager that allows team members to store and share credentials securely.\ngopass - A password manager for the command line written in Go.\npass - The standard unix password manager.\nVault - A tool for securely accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, certificates, and more.\nlegion - An open source, easy-to-use, super-extensible and semi-automated network penetration testing framework that aids in discovery, reconnaissance and exploitation of information systems.\nopenscap - The oscap program is a command line tool that allows users to load, scan, validate, edit, and export SCAP documents.\nDatasets\nPapers With Code - Thousands of machine learning datasets provided by Papers With Code.\nKITTI-360 - This large-scale dataset contains 320k images and 100k laser scans in a driving distance of 73.7km.\nwaymo_ros - This is a ROS package to connect Waymo open dataset to ROS.\nwaymo-open-dataset - The Waymo Open Dataset is comprised of high-resolution sensor data collected by Waymo self-driving cars in a wide variety of conditions.\nFord Autonomous Vehicle Dataset - Ford presents a challenging multi-agent seasonal dataset collected by a fleet of Ford autonomous vehicles at different days and times.\nawesome-robotics-datasets - A collection of useful datasets for robotics and computer vision.\nnuscenes-devkit - The devkit of the nuScenes dataset.\ndataset-api - This is a repo of toolkit for ApolloScape Dataset, CVPR 2019 Workshop on Autonomous Driving Challenge and ECCV 2018 challenge.\nutbm_robocar_dataset - EU Long-term Dataset with Multiple Sensors for Autonomous Driving.\nDBNet - A Large-Scale Dataset for Driving Behavior Learning.\nargoverse-api - Official GitHub repository for Argoverse dataset.\nDDAD - A new autonomous driving benchmark from TRI (Toyota Research Institute) for long range (up to 250m) and dense depth estimation in challenging and diverse urban conditions.\npandaset-devkit - Public large-scale dataset for autonomous driving provided by Hesai & Scale.\na2d2_to_ros - Utilities for converting A2D2 data sets to ROS bags.\nawesome-satellite-imagery-datasets - List of satellite image training datasets with annotations for computer vision and deep learning.\nsentinelsat - Search and download Copernicus Sentinel satellite images.\nadas-dataset-form - Thermal Dataset for Algorithm Training.\nh3d - The H3D is a large scale full-surround 3D multi-object detection and tracking dataset from Honda.\nMapillary Vistas Dataset - A diverse street-level imagery dataset with pixel\u2011accurate and instance\u2011specific human annotations for understanding street scenes around the world.\nTensorFlow Datasets - TensorFlow Datasets provides many public datasets as tf.data.Datasets.\nracetrack-database - Contains center lines (x- and y-coordinates), track widths and race lines for over 20 race tracks (mainly F1 and DTM) all over the world.\nBlenderProc - A procedural Blender pipeline for photorealistic training image generation.\nAtlatec Sample Map Data - 3D map for autonomous driving and simulation created from nothing but two cameras and GPS in downtown San Francisco.\nLyft Level 5 Dataset - Level 5 is developing a self-driving system for the Lyft network. We're collecting and processing data from our autonomous fleet and sharing it with you.\nholicity - A City-Scale Data Platform for Learning Holistic 3D Structures.\nUTD19 - Largest multi-city traffic dataset publically available.\nASTYX HIRES2019 DATASET - Automotive Radar Dataset for Deep Learning Based 3D Object Detection.\nObjectron - A collection of short, object-centric video clips, which are accompanied by AR session metadata that includes camera poses, sparse point-clouds and characterization of the planar surfaces in the surrounding environment.\nONCE dataset - A large-scale autonomous driving dataset with 2D&3D object annotations.\nFootnotes\nThanks to the team of xpp for creating this awesome GIF we use.",
      "link": "https://github.com/protontypes/awesome-robotic-tooling"
    },
    {
      "autor": "TextAttack",
      "date": "NaN",
      "content": "TextAttack \ud83d\udc19\nGenerating adversarial examples for NLP models\n[TextAttack Documentation on ReadTheDocs]\nAbout \u2022 Setup \u2022 Usage \u2022 Design\nAbout\nTextAttack is a Python framework for adversarial attacks, data augmentation, and model training in NLP.\nIf you're looking for information about TextAttack's menagerie of pre-trained models, you might want the TextAttack Model Zoo page.\nSlack Channel\nFor help and realtime updates related to TextAttack, please join the TextAttack Slack!\nWhy TextAttack?\nThere are lots of reasons to use TextAttack:\nUnderstand NLP models better by running different adversarial attacks on them and examining the output\nResearch and develop different NLP adversarial attacks using the TextAttack framework and library of components\nAugment your dataset to increase model generalization and robustness downstream\nTrain NLP models using just a single command (all downloads included!)\nSetup\nInstallation\nYou should be running Python 3.6+ to use this package. A CUDA-compatible GPU is optional but will greatly improve code speed. TextAttack is available through pip:\npip install textattack\nOnce TextAttack is installed, you can run it via command-line (textattack ...) or via python module (python -m textattack ...).\nTip: TextAttack downloads files to ~/.cache/textattack/ by default. This includes pretrained models, dataset samples, and the configuration file config.yaml. To change the cache path, set the environment variable TA_CACHE_DIR. (for example: TA_CACHE_DIR=/tmp/ textattack attack ...).\nUsage\nHelp: textattack --help\nTextAttack's main features can all be accessed via the textattack command. Two very common commands are textattack attack <args>, and textattack augment <args>. You can see more information about all commands using\ntextattack --help\nor a specific command using, for example,\ntextattack attack --help\nThe examples/ folder includes scripts showing common TextAttack usage for training models, running attacks, and augmenting a CSV file.\nThe documentation website contains walkthroughs explaining basic usage of TextAttack, including building a custom transformation and a custom constraint..\nRunning Attacks: textattack attack --help\nThe easiest way to try out an attack is via the command-line interface, textattack attack.\nTip: If your machine has multiple GPUs, you can distribute the attack across them using the --parallel option. For some attacks, this can really help performance. (If you want to attack Keras models in parallel, please check out examples/attack/attack_keras_parallel.py instead)\nHere are some concrete examples:\nTextFooler on BERT trained on the MR sentiment classification dataset:\ntextattack attack --recipe textfooler --model bert-base-uncased-mr --num-examples 100\nDeepWordBug on DistilBERT trained on the Quora Question Pairs paraphrase identification dataset:\ntextattack attack --model distilbert-base-uncased-cola --recipe deepwordbug --num-examples 100\nBeam search with beam width 4 and word embedding transformation and untargeted goal function on an LSTM:\ntextattack attack --model lstm-mr --num-examples 20 \\\n--search-method beam-search^beam_width=4 --transformation word-swap-embedding \\\n--constraints repeat stopword max-words-perturbed^max_num_words=2 embedding^min_cos_sim=0.8 part-of-speech \\\n--goal-function untargeted-classification\nTip: Instead of specifying a dataset and number of examples, you can pass --interactive to attack samples inputted by the user.\nAttacks and Papers Implemented (\"Attack Recipes\"): textattack attack --recipe [recipe_name]\nWe include attack recipes which implement attacks from the literature. You can list attack recipes using textattack list attack-recipes.\nTo run an attack recipe: textattack attack --recipe [recipe_name]\nAttack Recipe Name Goal Function ConstraintsEnforced Transformation Search Method Main Idea\nAttacks on classification tasks, like sentiment classification and entailment:\na2t Untargeted {Classification, Entailment} Percentage of words perturbed, Word embedding distance, DistilBERT sentence encoding cosine similarity, part-of-speech consistency Counter-fitted word embedding swap (or) BERT Masked Token Prediction Greedy-WIR (gradient) from ([\"Towards Improving Adversarial Training of NLP Models\" (Yoo et al., 2021)](https://arxiv.org/abs/2109.00544))\nalzantot Untargeted {Classification, Entailment} Percentage of words perturbed, Language Model perplexity, Word embedding distance Counter-fitted word embedding swap Genetic Algorithm from ([\"Generating Natural Language Adversarial Examples\" (Alzantot et al., 2018)](https://arxiv.org/abs/1804.07998))\nbae Untargeted Classification USE sentence encoding cosine similarity BERT Masked Token Prediction Greedy-WIR BERT masked language model transformation attack from ([\"BAE: BERT-based Adversarial Examples for Text Classification\" (Garg & Ramakrishnan, 2019)](https://arxiv.org/abs/2004.01970)).\nbert-attack Untargeted Classification USE sentence encoding cosine similarity, Maximum number of words perturbed BERT Masked Token Prediction (with subword expansion) Greedy-WIR ([\"BERT-ATTACK: Adversarial Attack Against BERT Using BERT\" (Li et al., 2020)](https://arxiv.org/abs/2004.09984))\nchecklist {Untargeted, Targeted} Classification checklist distance contract, extend, and substitutes name entities Greedy-WIR Invariance testing implemented in CheckList . ([\"Beyond Accuracy: Behavioral Testing of NLP models with CheckList\" (Ribeiro et al., 2020)](https://arxiv.org/abs/2005.04118))\nclare Untargeted {Classification, Entailment} USE sentence encoding cosine similarity RoBERTa Masked Prediction for token swap, insert and merge Greedy [\"Contextualized Perturbation for Textual Adversarial Attack\" (Li et al., 2020)](https://arxiv.org/abs/2009.07502))\ndeepwordbug {Untargeted, Targeted} Classification Levenshtein edit distance {Character Insertion, Character Deletion, Neighboring Character Swap, Character Substitution} Greedy-WIR Greedy replace-1 scoring and multi-transformation character-swap attack ([\"Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers\" (Gao et al., 2018)](https://arxiv.org/abs/1801.04354)\nfast-alzantot Untargeted {Classification, Entailment} Percentage of words perturbed, Language Model perplexity, Word embedding distance Counter-fitted word embedding swap Genetic Algorithm Modified, faster version of the Alzantot et al. genetic algorithm, from ([\"Certified Robustness to Adversarial Word Substitutions\" (Jia et al., 2019)](https://arxiv.org/abs/1909.00986))\nhotflip (word swap) Untargeted Classification Word Embedding Cosine Similarity, Part-of-speech match, Number of words perturbed Gradient-Based Word Swap Beam search ([\"HotFlip: White-Box Adversarial Examples for Text Classification\" (Ebrahimi et al., 2017)](https://arxiv.org/abs/1712.06751))\niga Untargeted {Classification, Entailment} Percentage of words perturbed, Word embedding distance Counter-fitted word embedding swap Genetic Algorithm Improved genetic algorithm -based word substitution from ([\"Natural Language Adversarial Attacks and Defenses in Word Level (Wang et al., 2019)\"](https://arxiv.org/abs/1909.06723)\ninput-reduction Input Reduction Word deletion Greedy-WIR Greedy attack with word importance ranking , Reducing the input while maintaining the prediction through word importance ranking ([\"Pathologies of Neural Models Make Interpretation Difficult\" (Feng et al., 2018)](https://arxiv.org/pdf/1804.07781.pdf))\nkuleshov Untargeted Classification Thought vector encoding cosine similarity, Language model similarity probability Counter-fitted word embedding swap Greedy word swap ([\"Adversarial Examples for Natural Language Classification Problems\" (Kuleshov et al., 2018)](https://openreview.net/pdf?id=r1QZ3zbAZ))\npruthi Untargeted Classification Minimum word length, Maximum number of words perturbed {Neighboring Character Swap, Character Deletion, Character Insertion, Keyboard-Based Character Swap} Greedy search simulates common typos ([\"Combating Adversarial Misspellings with Robust Word Recognition\" (Pruthi et al., 2019)](https://arxiv.org/abs/1905.11268)\npso Untargeted Classification HowNet Word Swap Particle Swarm Optimization ([\"Word-level Textual Adversarial Attacking as Combinatorial Optimization\" (Zang et al., 2020)](https://www.aclweb.org/anthology/2020.acl-main.540/))\npwws Untargeted Classification WordNet-based synonym swap Greedy-WIR (saliency) Greedy attack with word importance ranking based on word saliency and synonym swap scores ([\"Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency\" (Ren et al., 2019)](https://www.aclweb.org/anthology/P19-1103/))\ntextbugger : (black-box) Untargeted Classification USE sentence encoding cosine similarity {Character Insertion, Character Deletion, Neighboring Character Swap, Character Substitution} Greedy-WIR ([([\"TextBugger: Generating Adversarial Text Against Real-world Applications\" (Li et al., 2018)](https://arxiv.org/abs/1812.05271)).\ntextfooler Untargeted {Classification, Entailment} Word Embedding Distance, Part-of-speech match, USE sentence encoding cosine similarity Counter-fitted word embedding swap Greedy-WIR Greedy attack with word importance ranking ([\"Is Bert Really Robust?\" (Jin et al., 2019)](https://arxiv.org/abs/1907.11932))\nAttacks on sequence-to-sequence models:\nmorpheus Minimum BLEU Score Inflection Word Swap Greedy search Greedy to replace words with their inflections with the goal of minimizing BLEU score ([\"It\u2019s Morphin\u2019 Time! Combating Linguistic Discrimination with Inflectional Perturbations\"](https://www.aclweb.org/anthology/2020.acl-main.263.pdf)\nseq2sick :(black-box) Non-overlapping output Counter-fitted word embedding swap Greedy-WIR Greedy attack with goal of changing every word in the output translation. Currently implemented as black-box with plans to change to white-box as done in paper ([\"Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples\" (Cheng et al., 2018)](https://arxiv.org/abs/1803.01128))\nRecipe Usage Examples\nHere are some examples of testing attacks from the literature from the command-line:\nTextFooler against BERT fine-tuned on SST-2:\ntextattack attack --model bert-base-uncased-sst2 --recipe textfooler --num-examples 10\nseq2sick (black-box) against T5 fine-tuned for English-German translation:\ntextattack attack --model t5-en-de --recipe seq2sick --num-examples 100\nAugmenting Text: textattack augment\nMany of the components of TextAttack are useful for data augmentation. The textattack.Augmenter class uses a transformation and a list of constraints to augment data. We also offer built-in recipes for data augmentation:\nwordnet augments text by replacing words with WordNet synonyms\nembedding augments text by replacing words with neighbors in the counter-fitted embedding space, with a constraint to ensure their cosine similarity is at least 0.8\ncharswap augments text by substituting, deleting, inserting, and swapping adjacent characters\neda augments text with a combination of word insertions, substitutions and deletions.\nchecklist augments text by contraction/extension and by substituting names, locations, numbers.\nclare augments text by replacing, inserting, and merging with a pre-trained masked language model.\nAugmentation Command-Line Interface\nThe easiest way to use our data augmentation tools is with textattack augment <args>. textattack augment takes an input CSV file and text column to augment, along with the number of words to change per augmentation and the number of augmentations per input example. It outputs a CSV in the same format with all the augmentation examples corresponding to the proper columns.\nFor example, given the following as examples.csv:\n\"text\",label\n\"the rock is destined to be the 21st century's new conan and that he's going to make a splash even greater than arnold schwarzenegger , jean- claud van damme or steven segal.\", 1\n\"the gorgeously elaborate continuation of 'the lord of the rings' trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\", 1\n\"take care of my cat offers a refreshingly different slice of asian cinema .\", 1\n\"a technically well-made suspenser . . . but its abrupt drop in iq points as it races to the finish line proves simply too discouraging to let slide .\", 0\n\"it's a mystery how the movie could be released in this condition .\", 0\nThe command\ntextattack augment --input-csv examples.csv --output-csv output.csv --input-column text --recipe embedding --pct-words-to-swap .1 --transformations-per-example 2 --exclude-original\nwill augment the text column by altering 10% of each example's words, generating twice as many augmentations as original inputs, and exclude the original inputs from the output CSV. (All of this will be saved to augment.csv by default.)\nTip: Just as running attacks interactively, you can also pass --interactive to augment samples inputted by the user to quickly try out different augmentation recipes!\nAfter augmentation, here are the contents of augment.csv:\ntext,label\n\"the rock is destined to be the 21st century's newest conan and that he's gonna to make a splashing even stronger than arnold schwarzenegger , jean- claud van damme or steven segal.\",1\n\"the rock is destined to be the 21tk century's novel conan and that he's going to make a splat even greater than arnold schwarzenegger , jean- claud van damme or stevens segal.\",1\nthe gorgeously elaborate continuation of 'the lord of the rings' trilogy is so huge that a column of expression significant adequately describe co-writer/director pedro jackson's expanded vision of j . rs . r . tolkien's middle-earth .,1\nthe gorgeously elaborate continuation of 'the lordy of the piercings' trilogy is so huge that a column of mots cannot adequately describe co-novelist/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .,1\ntake care of my cat offerings a pleasantly several slice of asia cinema .,1\ntaking care of my cat offers a pleasantly different slice of asiatic kino .,1\na technically good-made suspenser . . . but its abrupt drop in iq points as it races to the finish bloodline proves straightforward too disheartening to let slide .,0\na technically well-made suspenser . . . but its abrupt drop in iq dot as it races to the finish line demonstrates simply too disheartening to leave slide .,0\nit's a enigma how the -----> film !!!  wo be releases in this condition .,0\nit's a enigma how the -----> film !!! making wo be publicized in this condition .,0\nThe 'embedding' augmentation recipe uses counterfitted embedding nearest-neighbors to augment data.\nAugmentation Python Interface\nIn addition to the command-line interface, you can augment text dynamically by importing the Augmenter in your own code. All Augmenter objects implement augment and augment_many to generate augmentations of a string or a list of strings. Here's an example of how to use the EmbeddingAugmenter in a python script:\n>>> from textattack.augmentation import EmbeddingAugmenter\n>>> augmenter = EmbeddingAugmenter()\n>>> s = 'What I cannot create, I do not understand.'\n>>> augmenter.augment(s)\n['What I notable create, I do not understand.', 'What I significant create, I do not understand.', 'What I cannot engender, I do not understand.', 'What I cannot creating, I do not understand.', 'What I cannot creations, I do not understand.', 'What I cannot create, I do not comprehend.', 'What I cannot create, I do not fathom.', 'What I cannot create, I do not understanding.', 'What I cannot create, I do not understands.', 'What I cannot create, I do not understood.', 'What I cannot create, I do not realise.']\nYou can also create your own augmenter from scratch by importing transformations/constraints from textattack.transformations and textattack.constraints. Here's an example that generates augmentations of a string using WordSwapRandomCharacterDeletion:\n>>> from textattack.transformations import WordSwapRandomCharacterDeletion\n>>> from textattack.transformations import CompositeTransformation\n>>> from textattack.augmentation import Augmenter\n>>> transformation = CompositeTransformation([WordSwapRandomCharacterDeletion()])\n>>> augmenter = Augmenter(transformation=transformation, transformations_per_example=5)\n>>> s = 'What I cannot create, I do not understand.'\n>>> augmenter.augment(s)\n['What I cannot creae, I do not understand.', 'What I cannot creat, I do not understand.', 'What I cannot create, I do not nderstand.', 'What I cannot create, I do nt understand.', 'Wht I cannot create, I do not understand.']\nTraining Models: textattack train\nOur model training code is available via textattack train to help you train LSTMs, CNNs, and transformers models using TextAttack out-of-the-box. Datasets are automatically loaded using the datasets package.\nTraining Examples\nTrain our default LSTM for 50 epochs on the Yelp Polarity dataset:\ntextattack train --model-name-or-path lstm --dataset yelp_polarity --epochs 50 --learning-rate 1e-5\nFine-Tune bert-base on the CoLA dataset for 5 epochs*:\ntextattack train --model-name-or-path bert-base-uncased --dataset glue^cola --per-device-train-batch-size 8 --epochs 5\nTo check datasets: textattack peek-dataset\nTo take a closer look at a dataset, use textattack peek-dataset. TextAttack will print some cursory statistics about the inputs and outputs from the dataset. For example,\ntextattack peek-dataset --dataset-from-huggingface snli\nwill show information about the SNLI dataset from the NLP package.\nTo list functional components: textattack list\nThere are lots of pieces in TextAttack, and it can be difficult to keep track of all of them. You can use textattack list to list components, for example, pretrained models (textattack list models) or available search methods (textattack list search-methods).\nDesign\nModels\nTextAttack is model-agnostic! You can use TextAttack to analyze any model that outputs IDs, tensors, or strings. To help users, TextAttack includes pre-trained models for different common NLP tasks. This makes it easier for users to get started with TextAttack. It also enables a more fair comparison of attacks from the literature.\nBuilt-in Models and Datasets\nTextAttack also comes built-in with models and datasets. Our command-line interface will automatically match the correct dataset to the correct model. We include 82 different (Oct 2020) pre-trained models for each of the nine GLUE tasks, as well as some common datasets for classification, translation, and summarization.\nA list of available pretrained models and their validation accuracies is available at textattack/models/README.md. You can also view a full list of provided models & datasets via textattack attack --help.\nHere's an example of using one of the built-in models (the SST-2 dataset is automatically loaded):\ntextattack attack --model roberta-base-sst2 --recipe textfooler --num-examples 10\nHuggingFace support: transformers models and datasets datasets\nWe also provide built-in support for transformers pretrained models and datasets from the datasets package! Here's an example of loading and attacking a pre-trained model and dataset:\ntextattack attack --model-from-huggingface distilbert-base-uncased-finetuned-sst-2-english --dataset-from-huggingface glue^sst2 --recipe deepwordbug --num-examples 10\nYou can explore other pre-trained models using the --model-from-huggingface argument, or other datasets by changing --dataset-from-huggingface.\nLoading a model or dataset from a file\nYou can easily try out an attack on a local model or dataset sample. To attack a pre-trained model, create a short file that loads them as variables model and tokenizer. The tokenizer must be able to transform string inputs to lists or tensors of IDs using a method called encode(). The model must take inputs via the __call__ method.\nCustom Model from a file\nTo experiment with a model you've trained, you could create the following file and name it my_model.py:\nmodel = load_your_model_with_custom_code() # replace this line with your model loading code\ntokenizer = load_your_tokenizer_with_custom_code() # replace this line with your tokenizer loading code\nThen, run an attack with the argument --model-from-file my_model.py. The model and tokenizer will be loaded automatically.\nCustom Datasets\nDataset from a file\nLoading a dataset from a file is very similar to loading a model from a file. A 'dataset' is any iterable of (input, output) pairs. The following example would load a sentiment classification dataset from file my_dataset.py:\ndataset = [('Today was....', 1), ('This movie is...', 0), ...]\nYou can then run attacks on samples from this dataset by adding the argument --dataset-from-file my_dataset.py.\nDataset loading via other mechanism, see: more details at here\nimport textattack\nmy_dataset = [(\"text\",label),....]\nnew_dataset = textattack.datasets.Dataset(my_dataset)\nDataset via AttackedText class\nTo allow for word replacement after a sequence has been tokenized, we include an AttackedText object which maintains both a list of tokens and the original text, with punctuation. We use this object in favor of a list of words or just raw text.\nAttacks and how to design a new attack\nWe formulate an attack as consisting of four components: a goal function which determines if the attack has succeeded, constraints defining which perturbations are valid, a transformation that generates potential modifications given an input, and a search method which traverses through the search space of possible perturbations. The attack attempts to perturb an input text such that the model output fulfills the goal function (i.e., indicating whether the attack is successful) and the perturbation adheres to the set of constraints (e.g., grammar constraint, semantic similarity constraint). A search method is used to find a sequence of transformations that produce a successful adversarial example.\nThis modular design unifies adversarial attack methods into one system, enables us to easily assemble attacks from the literature while re-using components that are shared across attacks. We provides clean, readable implementations of 16 adversarial attack recipes from the literature (see above table). For the first time, these attacks can be benchmarked, compared, and analyzed in a standardized setting.\nTextAttack is model-agnostic - meaning it can run attacks on models implemented in any deep learning framework. Model objects must be able to take a string (or list of strings) and return an output that can be processed by the goal function. For example, machine translation models take a list of strings as input and produce a list of strings as output. Classification and entailment models return an array of scores. As long as the user's model meets this specification, the model is fit to use with TextAttack.\nGoal Functions\nA GoalFunction takes as input an AttackedText object, scores it, and determines whether the attack has succeeded, returning a GoalFunctionResult.\nConstraints\nA Constraint takes as input a current AttackedText, and a list of transformed AttackedTexts. For each transformed option, it returns a boolean representing whether the constraint is met.\nTransformations\nA Transformation takes as input an AttackedText and returns a list of possible transformed AttackedTexts. For example, a transformation might return all possible synonym replacements.\nSearch Methods\nA SearchMethod takes as input an initial GoalFunctionResult and returns a final GoalFunctionResult The search is given access to the get_transformations function, which takes as input an AttackedText object and outputs a list of possible transformations filtered by meeting all of the attack\u2019s constraints. A search consists of successive calls to get_transformations until the search succeeds (determined using get_goal_results) or is exhausted.\nOn Benchmarking Attacks\nSee our analysis paper: Searching for a Search Method: Benchmarking Search Algorithms for Generating NLP Adversarial Examples at EMNLP BlackBoxNLP.\nAs we emphasized in the above paper, we don't recommend to directly compare Attack Recipes out of the box.\nThis comment is due to that attack recipes in the recent literature used different ways or thresholds in setting up their constraints. Without the constraint space held constant, an increase in attack success rate could come from an improved search or transformation method or a less restrictive search space.\nOur Github on benchmarking scripts and results: TextAttack-Search-Benchmark Github\nOn Quality of Generated Adversarial Examples in Natural Language\nOur analysis Paper in EMNLP Findings\nWe analyze the generated adversarial examples of two state-of-the-art synonym substitution attacks. We find that their perturbations often do not preserve semantics, and 38% introduce grammatical errors. Human surveys reveal that to successfully preserve semantics, we need to significantly increase the minimum cosine similarities between the embeddings of swapped words and between the sentence encodings of original and perturbed sentences.With constraints adjusted to better preserve semantics and grammaticality, the attack success rate drops by over 70 percentage points.\nOur Github on Reevaluation results: Reevaluating-NLP-Adversarial-Examples Github\nAs we have emphasized in this analysis paper, we recommend researchers and users to be EXTREMELY mindful on the quality of generated adversarial examples in natural language\nWe recommend the field to use human-evaluation derived thresholds for setting up constraints\nMulti-lingual Support\nsee example code: https://github.com/QData/TextAttack/blob/master/examples/attack/attack_camembert.py for using our framework to attack French-BERT.\nsee tutorial notebook: https://textattack.readthedocs.io/en/latest/2notebook/Example_4_CamemBERT.html for using our framework to attack French-BERT.\nSee README_ZH.md for our README in Chinese\nContributing to TextAttack\nWe welcome suggestions and contributions! Submit an issue or pull request and we will do our best to respond in a timely manner. TextAttack is currently in an \"alpha\" stage in which we are working to improve its capabilities and design.\nSee CONTRIBUTING.md for detailed information on contributing.\nCiting TextAttack\nIf you use TextAttack for your research, please cite TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP.\n@inproceedings{morris2020textattack,\ntitle={TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP},\nauthor={Morris, John and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},\nbooktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},\npages={119--126},\nyear={2020}\n}",
      "link": "https://github.com/QData/TextAttack"
    },
    {
      "autor": "Objectron",
      "date": "NaN",
      "content": "Objectron Dataset\nObjectron is a dataset of short object centric video clips with pose annotations.\nWebsite \u2022 Dataset Format \u2022 Tutorials \u2022 License\nThe Objectron dataset is a collection of short, object-centric video clips, which are accompanied by AR session metadata that includes -----> camera !!!  poses, sparse point-clouds and characterization of the planar surfaces in the surrounding environment. In each video, the camera moves around the object, capturing it from different angles. The data also contain manually annotated 3D bounding boxes for each object, which describe the object\u2019s position, orientation, and dimensions. The dataset consists of 15K annotated video clips supplemented with over 4M annotated images in the following categories: bikes, books, bottles, cameras, cereal boxes, chairs, cups, laptops, and shoes. In addition, to ensure geo-diversity, our dataset is collected from 10 countries across five continents. Along with the dataset, we are also sharing a 3D object detection solution for four categories of objects \u2014 shoes, chairs, mugs, and cameras. These models are trained using this dataset, and are released in MediaPipe, Google's open source framework for cross-platform customizable ML solutions for live and streaming media.\nKey Features\n15000 annotated videos and 4M annotated images\nAll samples include high-res images, object pose, camera pose, point-cloud, and surface planes.\nReady to use examples in various tf.record formats, which can be used in Tensorflow/PyTorch.\nObject-centric multi-views, observing the same object from different angles.\nAccurate evaluation metrics, like 3D IoU for oriented 3D bounding boxes.\nDataset Format\nThe data is stored in the objectron bucket on Google Cloud storage. Check out the Download Data notebook for a quick review of how to download/access the dataset. The following assets are available:\nThe video sequences (located in /videos/class/batch-i/j/video.MOV files)\nThe annotation labels containing the 3D bounding boxes for objects. The annotation protobufs are located in /videos/class/batch-i/j/geometry.pbdata files. They are formatted using the object.proto. See [example] on how to parse the annotation files.\nAR metadata (such as camera poses, point clouds, and planar surfaces). They are based on a_r_capture_metadata.proto. See example on how to parse these files.\nProcessed dataset: sharded and shuffled tf.records of the annotated frames, in tf.example format and videos in tf.SequenceExample format. These are used for creating the input data pipeline to your models. These files are located in /v1/records_shuffled/class/ and /v1/sequences/class/.\nSupporting scripts to run evaluation based on the 3D IoU metric.\nSupporting scripts to load the data into Tensorflow, Jax and Pytorch and visualize the dataset, including \u201cHello World\u201d examples.\nSupporting Apache Beam jobs to process the datasets on Google Cloud infrastructure.\nThe index of all available samples, as well as train/test splits for easy access and download.\nRaw dataset size is 1.9TB (including videos and their annotations). Total dataset size is 4.4TB (including videos, records, sequences, etc.). This repository provides the required schemas and tools to parse the dataset.\nclass bike book bottle camera cereal_box chair cup laptop shoe\n#videos 476 2024 1928 815 1609 1943 2204 1473 2116\n#frames 150k 576k 476k 233k 396k 488k 546k 485k 557k\nTutorials\nDownloading the dataset\nHello, World example: Loading examples in Tensorflow\nLoading data in PyTorch\nParsing raw annotation files\nParsing the AR metadata\nEvaluating the model performance with 3D IoU\nSequenceExample tutorial\nTraining a NeRF model\nLicense\nObjectron is released under Computational Use of Data Agreement 1.0 (C-UDA-1.0). A copy of the license is available in this repository.\nBibTeX\nIf you found this dataset useful, please cite our paper.\n@article{objectron2021,\ntitle={Objectron: A Large Scale Dataset of Object-Centric Videos in the Wild with Pose Annotations},\nauthor={Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski, Jianing Wei, Matthias Grundmann},\njournal={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\nyear={2021}\n}\nThis is not an officially supported Google product. If you have any question, you can email us at objectron@google.com or join our mailing list at objectron@googlegroups.com",
      "link": "https://github.com/google-research-datasets/Objectron"
    },
    {
      "autor": "eos",
      "date": "NaN",
      "content": "eos: A lightweight header-only 3D Morphable Face Model fitting library in modern C++11/14.\neos is a lightweight 3D Morphable Face Model fitting library that provides basic functionality to use face models, as well as -----> camera !!!  and shape fitting functionality. It's written in modern C++11/14.\nAt the moment, it mainly provides the following functionality:\nMorphableModel and PcaModel classes to represent 3DMMs, with basic operations like draw_sample(). Supports the Surrey Face Model (SFM), 4D Face Model (4DFM), Basel Face Model (BFM) 2009 and 2017, and the Liverpool-York Head Model (LYHM) out-of-the-box\nThe low-resolution, shape-only Surrey Face Model (share/sfm_shape_3448.bin)\nFast, linear pose, shape and expression fitting, edge and contour fitting:\nLinear scaled orthographic projection camera pose estimation\nLinear shape-to-landmarks fitting, implementation of O. Aldrian & W. Smith, Inverse Rendering of Faces with a 3D Morphable Model, PAMI 2013\nExpression fitting, with 6 linear expression blendshapes of the SFM: anger, disgust, fear, happiness, sadness, surprise\nEdge-fitting, heavily inspired by: A. Bas et al., Fitting a 3D Morphable Model to Edges: A Comparison Between Hard and Soft Correspondences, ACCVW 2016\nTexture extraction to obtain a pose-invariant representation of the face texture\nPython bindings: Much of eos's functionality is available as a python module (try pip install eos-py!)\n(Experimental): Non-linear fitting cost functions using Ceres for shape, camera, blendshapes and the colour model (needs Ceres to be installed separately)\nAn experimental model viewer to visualise 3D Morphable Models and blendshapes is available here.\nUsage\nTested with the following compilers: >=gcc-6, >=clang-5, >=Visual Studio 2017 15.5, >=Xcode 9.2.\nThe library and python bindings do not require any external dependencies. The example applications require Boost (>=1.50.0) and OpenCV (>=2.4.3).\nTo use the library in your own project, just add the following directories to your include path:\neos/include\neos/3rdparty/cereal/include\neos/3rdparty/glm\neos/3rdparty/nanoflann/include\neos/3rdparty/eigen/Eigen\neos/3rdparty/eigen3-nnls/src\neos/3rdparty/toml11\nMake sure to clone with --recursive to download the required submodules!\nBuild the examples and tests\nNeeded dependencies for the example app: CMake (>=3.8.2, or >=3.10.0 for MSVC), Boost system, filesystem, program_options (>=1.50.0), OpenCV core, imgproc, highgui (>=2.4.3).\nTo build:\ngit clone --recursive https://github.com/patrikhuber/eos.git\nmkdir build && cd build # creates a build directory next to the 'eos' folder\ncmake -G \"<your favourite generator>\" ../eos -DCMAKE_INSTALL_PREFIX=../install/\nmake && make install # or open the project file and build in an IDE like Visual Studio\nIt is strongly recommended to use vcpkg to install the dependencies on Windows. Users who wish to manage dependencies manually may find it helpful to copy initial_cache.cmake.template to initial_cache.cmake, edit the necessary paths and run cmake with -C ../eos/initial_cache.cmake. On Linux, you may also want to set -DCMAKE_BUILD_TYPE=... appropriately.\nSample code\nThe fit-model example app creates a 3D face from a 2D image.\nAfter make install or running the INSTALL target, an example image with landmarks can be found in install/bin/data/. The model and the necessary landmarks mapping file are installed to install/share/.\nYou can run the example just by running:\nfit-model\nIt will load the face model, landmark-to-vertex mappings, blendshapes, and other required files from the ../share/ directory, and run on the example image. It can be run on other images by giving it a -i parameter for the image and -l for a set of ibug landmarks. The full set of parameters can be viewed by running fit-model --help.\nIf you are just getting started, it is recommended to have a look at fit-model-simple too, as it requires much fewer input, and only fits pose and shape, without any blendshapes or edge-fitting. Its full set of arguments is:\nfit-model-simple -m ../share/sfm_shape_3448.bin -p ../share/ibug_to_sfm.txt -i data/image_0010.png -l data/image_0010.pts\nThe output in both cases is an obj file with the shape and a png with the extracted texture map. The estimated pose angles and shape coefficients are available in the code via the API.\nSee examples/fit-model.cpp for the full code.\nThe Surrey Face Model\nThe library includes a low-resolution shape-only version of the Surrey Morphable Face Model. It is a PCA model of shape variation built from 3D face scans. It comes with uv-coordinates to perform texture remapping.\nThe full model is available at http://www.cvssp.org/facemodel.\n4D Face Model (4DFM)\neos can be used to load, use and do basic fitting with the 4D Face Model (4DFM) from 4dface Ltd. The model features 39 expressions/action units, and diverse identity variation.\nMore information about the model can be found on www.4dface.io/4dfm.\nPython bindings\neos includes python bindings for some of its functionality (and more can be added!). It can be installed from PyPI with pip install eos-py. You will still need the data files from this repository. Make sure that you've got >=gcc-7 or >=clang-5 as the default compiler on Linux (for example from the ubuntu-toolchain-r/test repository) or do CC=`which gcc-7` CXX=`which g++-7` pip install eos-py. Also make sure you've got >=cmake-3.8.2 (or >=cmake-3.10.0 for MSVC) in your path. In case of issues, the bindings can also be built manually: Clone the repository and set -DEOS_GENERATE_PYTHON_BINDINGS=on when running cmake (and optionally set PYTHON_EXECUTABLE to point to your python interpreter if it's not found automatically).\nAfter having obtained the bindings, they can be used like any python module:\nimport eos\nimport numpy as np\nmodel = eos.morphablemodel.load_model(\"eos/share/sfm_shape_3448.bin\")\nsample = model.get_shape_model().draw_sample([1.0, -0.5, 0.7])\nhelp(eos) # check the documentation\nSee demo.py for an example on how to run the fitting.\nMatlab bindings\nExperimental (not maintained currently): eos includes Matlab bindings for the fit_shape_and_pose(...) function, which means the fitting can be run from Matlab. Set -DEOS_GENERATE_MATLAB_BINDINGS=on when running cmake to build the required mex-file and run the INSTALL target to install everything. (Set Matlab_ROOT_DIR to point to your Matlab directory if it's not found automatically). More bindings (e.g. the MorphableModel itself) might be added in the future.\nGo to the install/eos/matlab directory and run demo.m to see how to run the fitting. The result is a mesh and rendering parameters (pose).\nDocumentation\nDoxygen: http://patrikhuber.github.io/eos/doc/\nThe fit-model example and the Namespace List in doxygen are a good place to start.\nLicense & contributions\nThis code is licensed under the Apache License, Version 2.0. The 3D morphable face model under share/sfm_shape_3448.bin is free for use for non-commercial purposes. For commercial purposes and to obtain other model resolutions, see http://www.cvssp.org/facemodel.\nContributions are very welcome! (best in the form of pull requests.) Please use GitHub issues for any bug reports, ideas, and discussions.\nIf you use this code in your own work, please cite the following paper: A Multiresolution 3D Morphable Face Model and Fitting Framework, P. Huber, G. Hu, R. Tena, P. Mortazavian, W. Koppen, W. Christmas, M. R\u00e4tsch, J. Kittler, International Conference on Computer Vision Theory and Applications (VISAPP) 2016, Rome, Italy [PDF].",
      "link": "https://github.com/patrikhuber/eos"
    },
    {
      "autor": "DAT8",
      "date": "NaN",
      "content": "DAT8 Course Repository\nCourse materials for General Assembly's Data Science course in Washington, DC (8/18/15 - 10/29/15).\nInstructor: Kevin Markham (Data School blog, email newsletter, YouTube channel)\nTuesday Thursday\n8/18: Introduction to Data Science 8/20: Command Line, Version Control\n8/25: Data Reading and Cleaning 8/27: Exploratory Data Analysis\n9/1: Visualization 9/3: Machine Learning\n9/8: Getting Data 9/10: K-Nearest Neighbors\n9/15: Basic Model Evaluation 9/17: Linear Regression\n9/22: First Project Presentation 9/24: Logistic Regression\n9/29: Advanced Model Evaluation 10/1: Naive Bayes and Text Data\n10/6: Natural Language Processing 10/8: Kaggle Competition\n10/13: Decision Trees 10/15: Ensembling\n10/20: Advanced scikit-learn, Clustering 10/22: Regularization, Regex\n10/27: Course Review 10/29: Final Project Presentation\nPython Resources\nCodecademy's Python course: Good beginner material, including tons of in-browser exercises.\nDataquest: Uses interactive exercises to teach Python in the context of data science.\nGoogle's Python Class: Slightly more advanced, including hours of useful lecture videos and downloadable exercises (with solutions).\nIntroduction to Python: A series of IPython notebooks that do a great job explaining core Python concepts and data structures.\nPython for Informatics: A very beginner-oriented book, with associated slides and videos.\nA Crash Course in Python for Scientists: Read through the Overview section for a very quick introduction to Python.\nPython 2.7 Quick Reference: My beginner-oriented guide that demonstrates Python concepts through short, well-commented examples.\nBeginner and intermediate workshop code: Useful for review and reference.\nPython Tutor: Allows you to visualize the execution of Python code.\nCourse project\nComparison of machine learning models\nComparison of model evaluation procedures and metrics\nAdvice for getting better at data science\nAdditional resources\nClass 1: Introduction to Data Science\nCourse overview (slides)\nIntroduction to data science (slides)\nDiscuss the course project: requirements and example projects\nTypes of data (slides) and public data sources\nWelcome from General Assembly staff\nHomework:\nWork through GA's friendly command line tutorial using Terminal (Linux/Mac) or Git Bash (Windows).\nRead through this command line reference, and complete the pre-class exercise at the bottom. (There's nothing you need to submit once you're done.)\nWatch videos 1 through 8 (21 minutes) of Introduction to Git and GitHub, or read sections 1.1 through 2.2 of Pro Git.\nIf your laptop has any setup issues, please work with us to resolve them by Thursday. If your laptop has not yet been checked, you should come early on Thursday, or just walk through the setup checklist yourself (and let us know you have done so).\nResources:\nFor a useful look at the different types of data scientists, read Analyzing the Analyzers (32 pages).\nFor some thoughts on what it's like to be a data scientist, read these short posts from Win-Vector and Datascope Analytics.\nQuora has a data science topic FAQ with lots of interesting Q&A.\nKeep up with local data-related events through the Data Community DC event calendar or weekly newsletter.\nClass 2: Command Line and Version Control\nSlack tour\nReview the command line pre-class exercise (code)\nGit and GitHub (slides)\nIntermediate command line\nHomework:\nComplete the command line homework assignment with the Chipotle data.\nReview the code from the beginner and intermediate Python workshops. If you don't feel comfortable with any of the content (excluding the \"requests\" and \"APIs\" sections), you should spend some time this weekend practicing Python:\nIntroduction to Python does a great job explaining Python essentials and includes tons of example code.\nIf you like learning from a book, Python for Informatics has useful chapters on strings, lists, and dictionaries.\nIf you prefer interactive exercises, try these lessons from Codecademy: \"Python Lists and Dictionaries\" and \"A Day at the Supermarket\".\nIf you have more time, try missions 2 and 3 from DataQuest's Learning Python course.\nIf you've already mastered these topics and want more of a challenge, try solving Python Challenge number 1 (decoding a message) and send me your code in Slack.\nTo give you a framework for thinking about your project, watch What is machine learning, and how does it work? (10 minutes). (This is the IPython notebook shown in the video.) Alternatively, read A Visual Introduction to Machine Learning, which focuses on a specific machine learning model called decision trees.\nOptional: Browse through some more example student projects, which may help to inspire your own project!\nGit and Markdown Resources:\nPro Git is an excellent book for learning Git. Read the first two chapters to gain a deeper understanding of version control and basic commands.\nIf you want to practice a lot of Git (and learn many more commands), Git Immersion looks promising.\nIf you want to understand how to contribute on GitHub, you first have to understand forks and pull requests.\nGitRef is my favorite reference guide for Git commands, and Git quick reference for beginners is a shorter guide with commands grouped by workflow.\nCracking the Code to GitHub's Growth explains why GitHub is so popular among developers.\nMarkdown Cheatsheet provides a thorough set of Markdown examples with concise explanations. GitHub's Mastering Markdown is a simpler and more attractive guide, but is less comprehensive.\nCommand Line Resources:\nIf you want to go much deeper into the command line, Data Science at the Command Line is a great book. The companion website provides installation instructions for a \"data science toolbox\" (a virtual machine with many more command line tools), as well as a long reference guide to popular command line tools.\nIf you want to do more at the command line with CSV files, try out csvkit, which can be installed via pip.\nClass 3: Data Reading and Cleaning\nGit and GitHub assorted tips (slides)\nReview command line homework (solution)\nPython:\nSpyder interface\nLooping exercise\nLesson on file reading with airline safety data (code, data, article)\nData cleaning exercise\nWalkthrough of Python homework with Chipotle data (code, data, article)\nHomework:\nComplete the Python homework assignment with the Chipotle data, add a commented Python script to your GitHub repo, and submit a link using the homework submission form. You have until Tuesday (9/1) to complete this assignment. (Note: Pandas, which is covered in class 4, should not be used for this assignment.)\nResources:\nWant to understand Python's comprehensions? Think in Excel or SQL may be helpful if you are still confused by list comprehensions.\nMy code isn't working is a great flowchart explaining how to debug Python errors.\nPEP 8 is Python's \"classic\" style guide, and is worth a read if you want to write readable code that is consistent with the rest of the Python community.\nIf you want to understand Python at a deeper level, Ned Batchelder's Loop Like A Native and Python Names and Values are excellent presentations.\nClass 4: Exploratory Data Analysis\nPandas (code):\nMovieLens 100k movie ratings (data, data dictionary, website)\nAlcohol consumption by country (data, article)\nReports of UFO sightings (data, website)\nProject question exercise\nHomework:\nThe deadline for discussing your project ideas with an instructor is Tuesday (9/1), and your project question write-up is due Thursday (9/3).\nRead How Software in Half of NYC Cabs Generates $5.2 Million a Year in Extra Tips for an excellent example of exploratory data analysis.\nRead Anscombe's Quartet, and Why Summary Statistics Don't Tell the Whole Story for a classic example of why visualization is useful.\nResources:\nBrowsing or searching the Pandas API Reference is an excellent way to locate a function even if you don't know its exact name.\nWhat I do when I get a new data set as told through tweets is a fun (yet enlightening) look at the process of exploratory data analysis.\nClass 5: Visualization\nPython homework with the Chipotle data due (solution, detailed explanation)\nPart 2 of Exploratory Data Analysis with Pandas (code)\nVisualization with Pandas and Matplotlib (notebook)\nHomework:\nYour project question write-up is due on Thursday.\nComplete the Pandas homework assignment with the IMDb data. You have until Tuesday (9/8) to complete this assignment.\nIf you're not using Anaconda, install the Jupyter Notebook (formerly known as the IPython Notebook) using pip. (The Jupyter or IPython Notebook is included with Anaconda.)\nPandas Resources:\nTo learn more Pandas, read this three-part tutorial, or review these two excellent (but extremely long) notebooks on Pandas: introduction and data wrangling.\nIf you want to go really deep into Pandas (and NumPy), read the book Python for Data Analysis, written by the creator of Pandas.\nThis notebook demonstrates the different types of joins in Pandas, for when you need to figure out how to merge two DataFrames.\nThis is a nice, short tutorial on pivot tables in Pandas.\nFor working with geospatial data in Python, GeoPandas looks promising. This tutorial uses GeoPandas (and scikit-learn) to build a \"linguistic street map\" of Singapore.\nVisualization Resources:\nWatch Look at Your Data (18 minutes) for an excellent example of why visualization is useful for understanding your data.\nFor more on Pandas plotting, read this notebook or the visualization page from the official Pandas documentation.\nTo learn how to customize your plots further, browse through this notebook on matplotlib or this similar notebook.\nRead Overview of Python Visualization Tools for a useful comparison of Matplotlib, Pandas, Seaborn, ggplot, Bokeh, Pygal, and Plotly.\nTo explore different types of visualizations and when to use them, Choosing a Good Chart and The Graphic Continuum are nice one-page references, and the interactive R Graph Catalog has handy filtering capabilities.\nThis PowerPoint presentation from Columbia's Data Mining class contains lots of good advice for properly using different types of visualizations.\nHarvard's Data Science course includes an excellent lecture on Visualization Goals, Data Types, and Statistical Graphs (83 minutes), for which the slides are also available.\nClass 6: Machine Learning\nPart 2 of Visualization with Pandas and Matplotlib (notebook)\nBrief introduction to the Jupyter/IPython Notebook\n\"Human learning\" exercise:\nIris dataset hosted by the UCI Machine Learning Repository\nIris -----> photo !!! \nNotebook\nIntroduction to machine learning (slides)\nHomework:\nOptional: Complete the bonus exercise listed in the human learning notebook. It will take the place of any one homework you miss, past or future! This is due on Tuesday (9/8).\nIf you're not using Anaconda, install requests and Beautiful Soup 4 using pip. (Both of these packages are included with Anaconda.)\nMachine Learning Resources:\nFor a very quick summary of the key points about machine learning, watch What is machine learning, and how does it work? (10 minutes) or read the associated notebook.\nFor a more in-depth introduction to machine learning, read section 2.1 (14 pages) of Hastie and Tibshirani's excellent book, An Introduction to Statistical Learning. (It's a free PDF download!)\nThe Learning Paradigms video (13 minutes) from Caltech's Learning From Data course provides a nice comparison of supervised versus unsupervised learning, as well as an introduction to \"reinforcement learning\".\nReal-World Active Learning is a readable and thorough introduction to \"active learning\", a variation of machine learning in which humans label only the most \"important\" observations.\nFor a preview of some of the machine learning content we will cover during the course, read Sebastian Raschka's overview of the supervised learning process.\nData Science, Machine Learning, and Statistics: What is in a Name? discusses the differences between these (and other) terms.\nThe Emoji Translation Project is a really fun application of machine learning.\nLook up the characteristics of your zip code, and then read about the 67 distinct segments in detail.\nIPython Notebook Resources:\nFor a recap of the IPython Notebook introduction (and a preview of scikit-learn), watch scikit-learn and the IPython Notebook (15 minutes) or read the associated notebook.\nIf you would like to learn the IPython Notebook, the official Notebook tutorials are useful.\nThis Reddit discussion compares the relative strengths of the IPython Notebook and Spyder.\nClass 7: Getting Data\nPandas homework with the IMDb data due (solution)\nOptional \"human learning\" exercise with the iris data due (solution)\nAPIs (code)\nOMDb API\nWeb scraping (code)\nIMDb: robots.txt\nExample web page\nIMDb: The Shawshank Redemption\nHomework:\nOptional: Complete the homework exercise listed in the web scraping code. It will take the place of any one homework you miss, past or future! This is due on Tuesday (9/15).\nOptional: If you're not using Anaconda, install Seaborn using pip. If you're using Anaconda, install Seaborn by running conda install seaborn at the command line. (Note that some students in past courses have had problems with Anaconda after installing Seaborn.)\nAPI Resources:\nThis Python script to query the U.S. Census API was created by a former DAT student. It's a bit more complicated than the example we used in class, it's very well commented, and it may provide a useful framework for writing your own code to query APIs.\nMashape and Apigee allow you to explore tons of different APIs. Alternatively, a Python API wrapper is available for many popular APIs.\nThe Data Science Toolkit is a collection of location-based and text-related APIs.\nAPI Integration in Python provides a very readable introduction to REST APIs.\nMicrosoft's Face Detection API, which powers How-Old.net, is a great example of how a machine learning API can be leveraged to produce a compelling web application.\nWeb Scraping Resources:\nThe Beautiful Soup documentation is incredibly thorough, but is hard to use as a reference guide. However, the section on specifying a parser may be helpful if Beautiful Soup appears to be parsing a page incorrectly.\nFor more Beautiful Soup examples and tutorials, see Web Scraping 101 with Python, a former DAT student's well-commented notebook on scraping Craigslist, this notebook from Stanford's Text As Data course, and this notebook and associated video from Harvard's Data Science course.\nFor a much longer web scraping tutorial covering Beautiful Soup, lxml, XPath, and Selenium, watch Web Scraping with Python (3 hours 23 minutes) from PyCon 2014. The slides and code are also available.\nFor more complex web scraping projects, Scrapy is a popular application framework that works with Python. It has excellent documentation, and here's a tutorial with detailed slides and code.\nrobotstxt.org has a concise explanation of how to write (and read) the robots.txt file.\nimport.io and Kimono claim to allow you to scrape websites without writing any code.\nHow a Math Genius Hacked OkCupid to Find True Love and How Netflix Reverse Engineered Hollywood are two fun examples of how web scraping has been used to build interesting datasets.\nClass 8: K-Nearest Neighbors\nBrief review of Pandas (notebook)\nK-nearest neighbors and scikit-learn (notebook)\nExercise with NBA player data (notebook, data, data dictionary)\nExploring the bias-variance tradeoff (notebook)\nHomework:\nReading assignment on the bias-variance tradeoff\nRead Kevin's introduction to reproducibility, read Jeff Leek's guide to creating a reproducible analysis, and watch this related Colbert Report video (8 minutes).\nWork on your project... your first project presentation is in less than two weeks!\nKNN Resources:\nFor a recap of the key points about KNN and scikit-learn, watch Getting started in scikit-learn with the famous iris dataset (15 minutes) and Training a machine learning model with scikit-learn (20 minutes).\nKNN supports distance metrics other than Euclidean distance, such as Mahalanobis distance, which takes the scale of the data into account.\nA Detailed Introduction to KNN is a bit dense, but provides a more thorough introduction to KNN and its applications.\nThis lecture on Image Classification shows how KNN could be used for detecting similar images, and also touches on topics we will cover in future classes (hyperparameter tuning and cross-validation).\nSome applications for which KNN is well-suited are object recognition, satellite image enhancement, document categorization, and gene expression analysis.\nSeaborn Resources:\nTo get started with Seaborn for visualization, the official website has a series of detailed tutorials and an example gallery.\nData visualization with Seaborn is a quick tour of some of the popular types of Seaborn plots.\nVisualizing Google Forms Data with Seaborn and How to Create NBA Shot Charts in Python are both good examples of Seaborn usage on real-world data.\nClass 9: Basic Model Evaluation\nOptional web scraping homework due (solution)\nReproducibility\nDiscuss assigned readings: introduction, Colbert Report video, cabs article, Tweet, creating a reproducible analysis\nExamples: Classic rock, student project 1, student project 2\nDiscuss the reading assignment on the bias-variance tradeoff\nModel evaluation using train/test split (notebook)\nExploring the scikit-learn documentation: module reference, user guide, class and function documentation\nHomework:\nWatch Data science in Python (35 minutes) for an introduction to linear regression (and a review of other course content), or at the very least, read through the associated notebook.\nOptional: For another introduction to linear regression, watch The Easiest Introduction to Regression Analysis (14 minutes).\nModel Evaluation Resources:\nFor a recap of some of the key points from today's lesson, watch Comparing machine learning models in scikit-learn (27 minutes).\nFor another explanation of training error versus testing error, the bias-variance tradeoff, and train/test split (also known as the \"validation set approach\"), watch Hastie and Tibshirani's video on estimating prediction error (12 minutes, starting at 2:34).\nCaltech's Learning From Data course includes a fantastic video on visualizing bias and variance (15 minutes).\nRandom Test/Train Split is Not Always Enough explains why random train/test split may not be a suitable model evaluation procedure if your data has a significant time element.\nReproducibility Resources:\nWhat We've Learned About Sharing Our Data Analysis includes tips from BuzzFeed News about how to publish a reproducible analysis.\nSoftware development skills for data scientists discusses the importance of writing functions and proper code comments (among other skills), which are highly useful for creating a reproducible analysis.\nData science done well looks easy - and that is a big problem for data scientists explains how a reproducible analysis demonstrates all of the work that goes into proper data science.\nClass 10: Linear Regression\nMachine learning exercise (article)\nLinear regression (notebook)\nCapital Bikeshare dataset used in a Kaggle competition\nData dictionary\nFeature engineering example: Predicting User Engagement in Corporate Collaboration Network\nHomework:\nYour first project presentation is on Tuesday (9/22)! Please submit a link to your project repository (with slides, code, data, and visualizations) by 6pm on Tuesday.\nComplete the homework assignment with the Yelp data. This is due on Thursday (9/24).\nLinear Regression Resources:\nTo go much more in-depth on linear regression, read Chapter 3 of An Introduction to Statistical Learning. Alternatively, watch the related videos or read my quick reference guide to the key points in that chapter.\nThis introduction to linear regression is more detailed and mathematically thorough, and includes lots of good advice.\nThis is a relatively quick post on the assumptions of linear regression.\nSetosa has an interactive visualization of linear regression.\nFor a brief introduction to confidence intervals, hypothesis testing, p-values, and R-squared, as well as a comparison between scikit-learn code and Statsmodels code, read my DAT7 lesson on linear regression.\nHere is a useful explanation of confidence intervals from Quora.\nHypothesis Testing: The Basics provides a nice overview of the topic, and John Rauser's talk on Statistics Without the Agonizing Pain (12 minutes) gives a great explanation of how the null hypothesis is rejected.\nEarlier this year, a major scientific journal banned the use of p-values:\nScientific American has a nice summary of the ban.\nThis response to the ban in Nature argues that \"decisions that are made earlier in data analysis have a much greater impact on results\".\nAndrew Gelman has a readable paper in which he argues that \"it's easy to find a p < .05 comparison even if nothing is going on, if you look hard enough\".\nScience Isn't Broken includes a neat tool that allows you to \"p-hack\" your way to \"statistically significant\" results.\nAccurately Measuring Model Prediction Error compares adjusted R-squared, AIC and BIC, train/test split, and cross-validation.\nOther Resources:\nSection 3.3.1 of An Introduction to Statistical Learning (4 pages) has a great explanation of dummy encoding for categorical features.\nKaggle has some nice visualizations of the bikeshare data we used today.\nClass 11: First Project Presentation\nProject presentations!\nHomework:\nWatch Rahul Patwari's videos on probability (5 minutes) and odds (8 minutes) if you're not comfortable with either of those terms.\nRead these excellent articles from BetterExplained: An Intuitive Guide To Exponential Functions & e and Demystifying the Natural Logarithm (ln). Then, review this brief summary of exponential functions and logarithms.\nClass 12: Logistic Regression\nYelp votes homework due (solution)\nLogistic regression (notebook)\nGlass identification dataset\nExercise with Titanic data (notebook, data, data dictionary)\nConfusion matrix (slides, notebook)\nHomework:\nIf you aren't yet comfortable with all of the confusion matrix terminology, watch Rahul Patwari's videos on Intuitive sensitivity and specificity (9 minutes) and The tradeoff between sensitivity and specificity (13 minutes).\nVideo/reading assignment on ROC curves and AUC\nVideo/reading assignment on cross-validation\nLogistic Regression Resources:\nTo go deeper into logistic regression, read the first three sections of Chapter 4 of An Introduction to Statistical Learning, or watch the first three videos (30 minutes) from that chapter.\nFor a math-ier explanation of logistic regression, watch the first seven videos (71 minutes) from week 3 of Andrew Ng's machine learning course, or read the related lecture notes compiled by a student.\nFor more on interpreting logistic regression coefficients, read this excellent guide by UCLA's IDRE and these lecture notes from the University of New Mexico.\nThe scikit-learn documentation has a nice explanation of what it means for a predicted probability to be calibrated.\nSupervised learning superstitions cheat sheet is a very nice comparison of four classifiers we cover in the course (logistic regression, decision trees, KNN, Naive Bayes) and one classifier we do not cover (Support Vector Machines).\nConfusion Matrix Resources:\nMy simple guide to confusion matrix terminology may be useful to you as a reference.\nThis blog post about Amazon Machine Learning contains a neat graphic showing how classification threshold affects different evaluation metrics.\nThis notebook (from another DAT course) explains how to calculate \"expected value\" from a confusion matrix by treating it as a cost-benefit matrix.\nClass 13: Advanced Model Evaluation\nData preparation (notebook)\nHandling missing values\nHandling categorical features (review)\nROC curves and AUC\nDiscuss the video/reading assignment\nExercise: drawing an ROC curve (slides)\nReturn to the main notebook\nCross-validation\nDiscuss the video/reading assignment and associated notebook\nReturn to the main notebook\nExercise with bank marketing data (notebook, data, data dictionary)\nHomework:\nReading assignment on spam filtering\nRead these Introduction to Probability slides, or skim section 2.1 of the OpenIntro Statistics textbook (12 pages). Pay specific attention to the following terms: probability, mutually exclusive, sample space, independent.\nOptional: Try to gain an understanding of conditional probability from this visualization.\nOptional: For an intuitive introduction to Bayes' theorem, read these posts on wealth and happiness, ducks, or legos.\nROC Resources:\nRahul Patwari has a great video on ROC Curves (12 minutes).\nAn introduction to ROC analysis is a very readable paper on the topic.\nROC curves can be used across a wide variety of applications, such as comparing different feature sets for detecting fraudulent Skype users, and comparing different classifiers on a number of popular datasets.\nCross-Validation Resources:\nFor more on cross-validation, read section 5.1 of An Introduction to Statistical Learning (11 pages) or watch the related videos: K-fold and leave-one-out cross-validation (14 minutes), cross-validation the right and wrong ways (10 minutes).\nIf you want to understand the different variations of cross-validation, this paper examines and compares them in detail.\nTo learn how to use GridSearchCV and RandomizedSearchCV for parameter tuning, watch How to find the best model parameters in scikit-learn (28 minutes) or read the associated notebook.\nOther Resources:\nscikit-learn has extensive documentation on model evaluation.\nCounterfactual evaluation of machine learning models (45 minutes) is an excellent talk about the sophisticated way in which Stripe evaluates its fraud detection model. (These are the associated slides.)\nVisualizing Machine Learning Thresholds to Make Better Business Decisions demonstrates how visualizing precision, recall, and \"queue rate\" at different thresholds can help you to maximize the business value of your classifier.\nClass 14: Naive Bayes and Text Data\nConditional probability and Bayes' theorem\nSlides (adapted from Visualizing Bayes' theorem)\nApplying Bayes' theorem to iris classification (notebook)\nNaive Bayes classification\nSlides\nSpam filtering example (notebook)\nApplying Naive Bayes to text data in scikit-learn (notebook)\nCountVectorizer documentation\nSMS messages: data, data dictionary\nHomework:\nComplete another homework assignment with the Yelp data. This is due on Tuesday (10/6).\nConfirm that you have TextBlob installed by running import textblob from within your preferred Python environment. If it's not installed, run pip install textblob at the command line (not from within Python).\nResources:\nSebastian Raschka's article on Naive Bayes and Text Classification covers the conceptual material from today's class in much more detail.\nFor more on conditional probability, read these slides, or read section 2.2 of the OpenIntro Statistics textbook (15 pages).\nFor an intuitive explanation of Naive Bayes classification, read this post on airport security.\nFor more details on Naive Bayes classification, Wikipedia has two excellent articles (Naive Bayes classifier and Naive Bayes spam filtering), and Cross Validated has a good Q&A.\nWhen applying Naive Bayes classification to a dataset with continuous features, it is better to use GaussianNB rather than MultinomialNB. This notebook compares their performances on such a dataset. Wikipedia has a short description of Gaussian Naive Bayes, as well as an excellent example of its usage.\nThese slides from the University of Maryland provide more mathematical details on both logistic regression and Naive Bayes, and also explain how Naive Bayes is actually a \"special case\" of logistic regression.\nAndrew Ng has a paper comparing the performance of logistic regression and Naive Bayes across a variety of datasets.\nIf you enjoyed Paul Graham's article, you can read his follow-up article on how he improved his spam filter and this related paper about state-of-the-art spam filtering in 2004.\nYelp has found that Naive Bayes is more effective than Mechanical Turks at categorizing businesses.\nClass 15: Natural Language Processing\nYelp review text homework due (solution)\nNatural language processing (notebook)\nIntroduction to our Kaggle competition\nCreate a Kaggle account, join the competition using the invitation link, download the sample submission, and then submit the sample submission (which will require SMS account verification).\nHomework:\nYour draft paper is due on Thursday (10/8)! Please submit a link to your project repository (with paper, code, data, and visualizations) before class.\nWatch Kaggle: How it Works (4 minutes) for a brief overview of the Kaggle platform.\nDownload the competition files, move them to the DAT8/data directory, and make sure you can open the CSV files using Pandas. If you have any problems opening the files, you probably need to turn off real-time virus scanning (especially Microsoft Security Essentials).\nOptional: Come up with some theories about which features might be relevant to predicting the response, and then explore the data to see if those theories appear to be true.\nOptional: Watch my project presentation video (16 minutes) for a tour of the end-to-end machine learning process for a Kaggle competition, including feature engineering. (Or, just read through the slides.)\nNLP Resources:\nIf you want to learn a lot more NLP, check out the excellent video lectures and slides from this Coursera course (which is no longer being offered).\nThis slide deck defines many of the key NLP terms.\nNatural Language Processing with Python is the most popular book for going in-depth with the Natural Language Toolkit (NLTK).\nA Smattering of NLP in Python provides a nice overview of NLTK, as does this notebook from DAT5.\nspaCy is a newer Python library for text processing that is focused on performance (unlike NLTK).\nIf you want to get serious about NLP, Stanford CoreNLP is a suite of tools (written in Java) that is highly regarded.\nWhen working with a large text corpus in scikit-learn, HashingVectorizer is a useful alternative to CountVectorizer.\nAutomatically Categorizing Yelp Businesses discusses how Yelp uses NLP and scikit-learn to solve the problem of uncategorized businesses.\nModern Methods for Sentiment Analysis shows how \"word vectors\" can be used for more accurate sentiment analysis.\nIdentifying Humorous Cartoon Captions is a readable paper about identifying funny captions submitted to the New Yorker Caption Contest.\nDC Natural Language Processing is an active Meetup group in our local area.\nClass 16: Kaggle Competition\nOverview of how Kaggle works (slides)\nKaggle In-Class competition: Predict whether a Stack Overflow question will be closed\nComplete code file\nMinimal code file: excludes all exploratory code\nExplanations of log loss\nHomework:\nYou will be assigned to review the project drafts of two of your peers. You have until Tuesday 10/20 to provide them with feedback, according to the peer review guidelines.\nRead A Visual Introduction to Machine Learning for a brief overview of decision trees.\nDownload and install Graphviz, which will allow you to visualize decision trees in scikit-learn.\nWindows users should also add Graphviz to your path: Go to Control Panel, System, Advanced System Settings, Environment Variables. Under system variables, edit \"Path\" to include the path to the \"bin\" folder, such as: C:\\Program Files (x86)\\Graphviz2.38\\bin\nOptional: Keep working on our Kaggle competition! You can make up to 5 submissions per day, and the competition doesn't close until 6:30pm ET on Tuesday 10/27 (class 21).\nResources:\nSpecialist Knowledge Is Useless and Unhelpful is a brief interview with Jeremy Howard (past president of Kaggle) in which he argues that data science skills are much more important than domain expertise for creating effective predictive models.\nGetting in Shape for the Sport of Data Science (74 minutes), also by Jeremy Howard, contains a lot of tips for competitive machine learning.\nLearning from the best is an excellent blog post covering top tips from Kaggle Masters on how to do well on Kaggle.\nFeature Engineering Without Domain Expertise (17 minutes), a talk by Kaggle Master Nick Kridler, provides some simple advice about how to iterate quickly and where to spend your time during a Kaggle competition.\nThese examples may help you to better understand the process of feature engineering: predicting the number of passengers at a train station, identifying fraudulent users of an online store, identifying bots in an online auction, predicting who will subscribe to the next season of an orchestra, and evaluating the quality of e-commerce search engine results.\nOur perfect submission is a fun read about how great performance on the public leaderboard does not guarantee that a model will generalize to new data.\nClass 17: Decision Trees\nDecision trees (notebook)\nExercise with Capital Bikeshare data (notebook, data, data dictionary)\nHomework:\nRead the \"Wisdom of the crowds\" section from MLWave's post on Human Ensemble Learning.\nOptional: Read the abstract from Do We Need Hundreds of Classifiers to Solve Real World Classification Problems?, as well as Kaggle CTO Ben Hamner's comment about the paper, paying attention to the mentions of \"Random Forests\".\nResources:\nscikit-learn's documentation on decision trees includes a nice overview of trees as well as tips for proper usage.\nFor a more thorough introduction to decision trees, read section 4.3 (23 pages) of Introduction to Data Mining. (Chapter 4 is available as a free download.)\nIf you want to go deep into the different decision tree algorithms, this slide deck contains A Brief History of Classification and Regression Trees.\nThe Science of Singing Along contains a neat regression tree (page 136) for predicting the percentage of an audience at a music venue that will sing along to a pop song.\nDecision trees are common in the medical field for differential diagnosis, such as this classification tree for identifying psychosis.\nClass 18: Ensembling\nFinish decision trees lesson (notebook)\nEnsembling (notebook)\nMajor League Baseball player data from 1986-87\nData dictionary (page 7)\nResources:\nscikit-learn's documentation on ensemble methods covers both \"averaging methods\" (such as bagging and Random Forests) as well as \"boosting methods\" (such as AdaBoost and Gradient Tree Boosting).\nMLWave's Kaggle Ensembling Guide is very thorough and shows the many different ways that ensembling can take place.\nBrowse the excellent solution paper from the winner of Kaggle's CrowdFlower competition for an example of the work and insight required to win a Kaggle competition.\nInterpretable vs Powerful Predictive Models: Why We Need Them Both is a short post on how the tactics useful in a Kaggle competition are not always useful in the real world.\nNot Even the People Who Write Algorithms Really Know How They Work argues that the decreased interpretability of state-of-the-art machine learning models has a negative impact on society.\nFor an intuitive explanation of Random Forests, read Edwin Chen's answer to How do random forests work in layman's terms?\nLarge Scale Decision Forests: Lessons Learned is an excellent post from Sift Science about their custom implementation of Random Forests.\nUnboxing the Random Forest Classifier describes a way to interpret the inner workings of Random Forests beyond just feature importances.\nUnderstanding Random Forests: From Theory to Practice is an in-depth academic analysis of Random Forests, including details of its implementation in scikit-learn.\nClass 19: Advanced scikit-learn and Clustering\nAdvanced scikit-learn (notebook)\nStandardScaler: standardizing features\nPipeline: chaining steps\nClustering (slides, notebook)\nK-means: documentation, visualization 1, visualization 2\nDBSCAN: documentation, visualization\nHomework:\nReread Understanding the Bias-Variance Tradeoff. (The \"answers\" to the guiding questions have been posted and may be helpful to you.)\nOptional: Watch these two excellent (and related) videos from Caltech's Learning From Data course: bias-variance tradeoff (15 minutes) and regularization (8 minutes).\nscikit-learn Resources:\nThis is a longer example of feature scaling in scikit-learn, with additional discussion of the types of scaling you can use.\nPractical Data Science in Python is a long and well-written notebook that uses a few advanced scikit-learn features: pipelining, plotting a learning curve, and pickling a model.\nTo learn how to use GridSearchCV and RandomizedSearchCV for parameter tuning, watch How to find the best model parameters in scikit-learn (28 minutes) or read the associated notebook.\nSebastian Raschka has a number of excellent resources for scikit-learn users, including a repository of tutorials and examples, a library of machine learning tools and extensions, a new book, and a semi-active blog.\nscikit-learn has an incredibly active mailing list that is often much more useful than Stack Overflow for researching functions and asking questions.\nIf you forget how to use a particular scikit-learn function that we have used in class, don't forget that this repository is fully searchable!\nClustering Resources:\nFor a very thorough introduction to clustering, read chapter 8 (69 pages) of Introduction to Data Mining (available as a free download), or browse through the chapter 8 slides.\nscikit-learn's user guide compares many different types of clustering.\nThis PowerPoint presentation from Columbia's Data Mining class provides a good introduction to clustering, including hierarchical clustering and alternative distance metrics.\nAn Introduction to Statistical Learning has useful videos on K-means clustering (17 minutes) and hierarchical clustering (15 minutes).\nThis is an excellent interactive visualization of hierarchical clustering.\nThis is a nice animated explanation of mean shift clustering.\nThe K-modes algorithm can be used for clustering datasets of categorical features without converting them to numerical values. Here is a Python implementation.\nHere are some fun examples of clustering: A Statistical Analysis of the Work of Bob Ross (with data and Python code), How a Math Genius Hacked OkCupid to Find True Love, and characteristics of your zip code.\nClass 20: Regularization and Regular Expressions\nRegularization (notebook)\nRegression: Ridge, RidgeCV, Lasso, LassoCV\nClassification: LogisticRegression\nHelper functions: Pipeline, GridSearchCV\nRegular expressions\nBaltimore homicide data\nRegular expressions 101: real-time testing of regular expressions\nReference guide\nExercise\nHomework:\nYour final project is due next week!\nOptional: Make your final submissions to our Kaggle competition! It closes at 6:30pm ET on Tuesday 10/27.\nOptional: Read this classic paper, which may help you to connect many of the topics we have studied throughout the course: A Few Useful Things to Know about Machine Learning.\nRegularization Resources:\nThe scikit-learn user guide for Generalized Linear Models explains different variations of regularization.\nSection 6.2 of An Introduction to Statistical Learning (14 pages) introduces both lasso and ridge regression. Or, watch the related videos on ridge regression (13 minutes) and lasso regression (15 minutes).\nFor more details on lasso regression, read Tibshirani's original paper.\nFor a math-ier explanation of regularization, watch the last four videos (30 minutes) from week 3 of Andrew Ng's machine learning course, or read the related lecture notes compiled by a student.\nThis notebook from chapter 7 of Building Machine Learning Systems with Python has a nice long example of regularized linear regression.\nThere are some special considerations when using dummy encoding for categorical features with a regularized model. This Cross Validated Q&A debates whether the dummy variables should be standardized (along with the rest of the features), and a comment on this blog post recommends that the baseline level should not be dropped.\nRegular Expressions Resources:\nGoogle's Python Class includes an excellent introductory lesson on regular expressions (which also has an associated video).\nPython for Informatics has a nice chapter on regular expressions. (If you want to run the examples, you'll need to download mbox.txt and mbox-short.txt.)\nBreaking the Ice with Regular Expressions is an interactive Code School course, though only the first \"level\" is free.\nIf you want to go really deep with regular expressions, RexEgg includes endless articles and tutorials.\n5 Tools You Didn't Know That Use Regular Expressions demonstrates how regular expressions can be used with Excel, Word, Google Spreadsheets, Google Forms, text editors, and other tools.\nExploring Expressions of Emotions in GitHub Commit Messages is a fun example of how regular expressions can be used for data analysis, and Emojineering explains how Instagram uses regular expressions to detect emoji in hashtags.\nClass 21: Course Review and Final Project Presentation\nProject presentations!\nData science review\nResources:\nscikit-learn's machine learning map may help you to choose the \"best\" model for your task.\nChoosing a Machine Learning Classifier is a short and highly readable comparison of several classification models, Classifier comparison is scikit-learn's visualization of classifier decision boundaries, Comparing supervised learning algorithms is a model comparison table that I created, and Supervised learning superstitions cheat sheet is a more thorough comparison (with links to lots of useful resources).\nMachine Learning Done Wrong, Machine Learning Gremlins (31 minutes), Clever Methods of Overfitting, and Common Pitfalls in Machine Learning all offer thoughtful advice on how to avoid common mistakes in machine learning.\nPractical machine learning tricks from the KDD 2011 best industry paper and Andrew Ng's Advice for applying machine learning include slightly more advanced advice than the resources above.\nAn Empirical Comparison of Supervised Learning Algorithms is a readable research paper from 2006, which was also presented as a talk (77 minutes).\nClass 22: Final Project Presentation\nProject presentations!\nWhat's next?\nAdditional Resources\nTidy Data\nGood Data Management Practices for Data Analysis briefly summarizes the principles of \"tidy data\".\nHadley Wickham's paper explains tidy data in detail and includes lots of good examples.\nExample of a tidy dataset: Bob Ross\nExamples of untidy datasets: NFL ticket prices, airline safety, Jets ticket prices, Chipotle orders\nIf your co-workers tend to create spreadsheets that are unreadable by computers, they may benefit from reading these tips for releasing data in spreadsheets. (There are some additional suggestions in this answer from Cross Validated.)\nDatabases and SQL\nThis GA slide deck provides a brief introduction to databases and SQL. The Python script from that lesson demonstrates basic SQL queries, as well as how to connect to a SQLite database from Python and how to query it using Pandas.\nThe repository for this SQL Bootcamp contains an extremely well-commented SQL script that is suitable for walking through on your own.\nThis GA notebook provides a shorter introduction to databases and SQL that helpfully contrasts SQL queries with Pandas syntax.\nSQLZOO, Mode Analytics, Khan Academy, Codecademy, Datamonkey, and Code School all have online beginner SQL tutorials that look promising. Code School also offers an advanced tutorial, though it's not free.\nw3schools has a sample database that allows you to practice SQL from your browser. Similarly, Kaggle allows you to query a large SQLite database of Reddit Comments using their online \"Scripts\" application.\nWhat Every Data Scientist Needs to Know about SQL is a brief series of posts about SQL basics, and Introduction to SQL for Data Scientists is a paper with similar goals.\n10 Easy Steps to a Complete Understanding of SQL is a good article for those who have some SQL experience and want to understand it at a deeper level.\nSQLite's article on Query Planning explains how SQL queries \"work\".\nA Comparison Of Relational Database Management Systems gives the pros and cons of SQLite, MySQL, and PostgreSQL.\nIf you want to go deeper into databases and SQL, Stanford has a well-respected series of 14 mini-courses.\nBlaze is a Python package enabling you to use Pandas-like syntax to query data living in a variety of data storage systems.\nRecommendation Systems\nThis GA slide deck provides a brief introduction to recommendation systems, and the Python script from that lesson demonstrates how to build a simple recommender.\nChapter 9 of Mining of Massive Datasets (36 pages) is a more thorough introduction to recommendation systems.\nChapters 2 through 4 of A Programmer's Guide to Data Mining (165 pages) provides a friendlier introduction, with lots of Python code and exercises.\nThe Netflix Prize was the famous competition for improving Netflix's recommendation system by 10%. Here are some useful articles about the Netflix Prize:\nNetflix Recommendations: Beyond the 5 stars: Two posts from the Netflix blog summarizing the competition and their recommendation system\nWinning the Netflix Prize: A Summary: Overview of the models and techniques that went into the winning solution\nA Perspective on the Netflix Prize: A summary of the competition by the winning team\nThis paper summarizes how Amazon.com's recommendation system works, and this Stack Overflow Q&A has some additional thoughts.\nFacebook and Etsy have blog posts about how their recommendation systems work.\nThe Global Network of Discovery provides some neat recommenders for music, authors, and movies.\nThe People Inside Your Machine (23 minutes) is a Planet Money podcast episode about how Amazon Mechanical Turks can assist with recommendation engines (and machine learning in general).\nCoursera has a course on recommendation systems, if you want to go even deeper into the material.",
      "link": "https://github.com/justmarkham/DAT8"
    },
    {
      "autor": "rebiber",
      "date": "NaN",
      "content": "Rebiber: A tool for normalizing bibtex with official info.\nWe often cite papers using their arXiv versions without noting that they are already PUBLISHED in some conferences. These unofficial bib entries might violate rules about submissions or -----> camera !!! -ready versions for some conferences. We introduce Rebiber, a simple tool in Python to fix them automatically. It is based on the official conference information from the DBLP or the ACL anthology (for NLP conferences)! You can check the list of supported conferences here. Apart from handling outdated arXiv citations, Rebiber also normalizes citations in a unified way (DBLP-style), supporting abbreviation and value selection.\nWeb demo: https://rebiber.herokuapp.com/ (recommended).\nColab notebook: here (a bit outdated).\nChangelog\n2021.09.06 We fixed a few minor bugs and added features such as sorting and urls to arXiv (if the paper is not in any conferences; thanks to @nicola-decao). We also updated the ACL anthology bib/json to the latest version as well as other conferences.\n2021.05.30 We build a beta version of our web app for Rebiber; add new conferences to our dataset; fix a few minor bugs.\n2021.02.08 We now support multiple useful features: 1) turning off some certain values, e.g., \"-r url,pages,address\" for removing the values from the output, 2) using abbr. to shorten the booktitle values, e.g., Proceedings of the .* Annual Meeting of the Association for Computational Linguistics --> Proc. of ACL. More examples are here.\n2021.01.30 We build a colab notebook as a simple web demo. link\nInstallation\n# pip install rebiber -U # for the stable version\npip install -e git+https://github.com/yuchenlin/rebiber.git#egg=rebiber -U\n# rebiber --update # (optional) update the bib data and the abbr. info (using wget)\nOR\ngit clone https://github.com/yuchenlin/rebiber.git\ncd rebiber/\npip install -e .\nIf you would like to use the latest github version with more bug fixes, please use the second installation method.\nUsage\uff08v1.1.3\uff09\nNormalize your bibtex file with the official conference information:\nrebiber -i /path/to/input.bib -o /path/to/output.bib\nYou can find a pair of example input and output files in rebiber/example_input.bib and rebiber/example_output.bib.\nargument usage\n-i or --input_bib. The path to the input bib file that you want to update\n-o or --output_bib. The path to the output bib file that you want to save. If you don't specify any -o then it will be the same as the -i.\n-r or --remove. A comma-separated list of value names that you want to remove, such as \"-r pages,editor,volume,month,url,biburl,address,publisher,bibsource,timestamp,doi\". Empty by default.\n-s or --shorten. A bool argument that is \"False\" by default, used for replacing booktitle with abbreviation in -a. Used as -s True.\n-d or --deduplicate. A bool argument that is \"True\" by default, used for removing the duplicate bib entries sharing the same key. Used as -d True.\n-l or --bib_list. The path to the list of the bib json files to be loaded. Check rebiber/bib_list.txt for the default file. Usually you don't need to set this argument.\n-a or --abbr_tsv. The list of conference abbreviation data. Check rebiber/abbr.tsv for the default file. Usually you don't need to set this argument.\n-u or --update. Update the local bib-related data with the latest Github version.\n-v or --version. Print the version of current Rebiber.\n-st or --sort. A bool argument that is \"False\" by default. used for keeping the original order of the bib entries of the input file. By setting it to be \"True\", the bib entries are ordered alphabetically in the output file. Used as -st True.\nExample Input and Output\nAn example input entry with the arXiv information (from Google Scholar or somewhere):\n@article{lin2020birds,\ntitle={Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models},\nauthor={Lin, Bill Yuchen and Lee, Seyeon and Khanna, Rahul and Ren, Xiang},\njournal={arXiv preprint arXiv:2005.00683},\nyear={2020}\n}\nAn example normalized output entry with the official information:\n@inproceedings{lin2020birds,\ntitle = \"{B}irds have four legs?! {N}umer{S}ense: {P}robing {N}umerical {C}ommonsense {K}nowledge of {P}re-{T}rained {L}anguage {M}odels\",\nauthor = \"Lin, Bill Yuchen and\nLee, Seyeon and\nKhanna, Rahul and\nRen, Xiang\",\nbooktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\",\nmonth = nov,\nyear = \"2020\",\naddress = \"Online\",\npublisher = \"Association for Computational Linguistics\",\nurl = \"https://www.aclweb.org/anthology/2020.emnlp-main.557\",\ndoi = \"10.18653/v1/2020.emnlp-main.557\",\npages = \"6862--6868\",\n}\nSupported Conferences\nThe bib_list.txt contains a list of converted json files of the official bib data. In this repo, we now support the full ACL anthology, i.e., all papers that are published at *CL conferences (ACL, EMNLP, NAACL, etc.) as well as workshops. Also, we support any conference proceedings that can be downloaded from DBLP, for example, ICLR2020.\nThe following conferences are supported and their bib/json files are in our data folder. You can turn each item on/off in bib_list.txt. Please feel free to create PR for adding new conferences following this!\nName Years\nACL Anthology (until 2021-09)\nAAAI 2010 -- 2020\nAISTATS 2013 -- 2020\nALENEX 2010 -- 2020\nASONAM 2010 -- 2019\nBigDataConf 2013 -- 2019\nBMVC 2010 -- 2020\nCHI 2010 -- 2020\nCIDR 2009 -- 2020\nCIKM 2010 -- 2020\nCOLT 2000 -- 2020\nCVPR 2000 -- 2020\nICASSP 2015 -- 2020\nICCV 2003 -- 2019\nICLR 2013 -- 2020\nICML 2000 -- 2020\nIJCAI 2011 -- 2020\nKDD 2010 -- 2020\nMLSys 2019 -- 2020\nMM 2016 -- 2020\nNeurIPS 2000 -- 2020\nRECSYS 2010 -- 2020\nSDM 2010 -- 2020\nSIGIR 2010 -- 2020\nSIGMOD 2010 -- 2020\nSODA 2010 -- 2020\nSTOC 2010 -- 2020\nUAI 2010 -- 2020\nWSDM 2008 -- 2020\nWWW (The Web Conf) 2001 -- 2020\nThanks for Anton Tsitsulin's great work on collecting such a complete set bib files!\nAdding a new conference\nYou can manually add any conferences from DBLP by downloading their bib files to our raw_data folder, and run a prepared script add_conf.sh.\nTake ICLR2020 and ICLR2019 as an example:\nStep 1: Go to DBLP\nStep 2: Download the bib files, and put them here as raw_data/iclr2020.bib and raw_data/iclr2019.bib (name should be in the format as {conf_name}{year}.bib)\nStep 3: Run script\nbash add_conf.sh iclr 2019 2020\nContact\nPlease email yuchen.lin@usc.edu or create Github issues here if you have any questions or suggestions.",
      "link": "https://github.com/yuchenlin/rebiber"
    },
    {
      "autor": "AndroidTensorFlowMachineLearningExample",
      "date": "NaN",
      "content": "Android TensorFlow Machine Learning Example\nAbout Android TensorFlow Machine Learning Example\nThis is an example project for integrating TensorFlow into Android application\nHow to build TensorFlow project to use with Android project.\nHow to build TensorFlow library(.so file and jar file) to use with Android Application.\nThis project include an example for object detection for an image taken from -----> camera !!!  using TensorFlow library.\nRead this article. It describes everything about building TensorFlow for Android.\nCheck the Android TensorFlow Lite Machine Learning Example.\nFind this project useful ? \u2764\ufe0f\nSupport it by clicking the \u2b50 button on the upper right of this page. \u270c\ufe0f\nCredits\nThe classifier example has been taken from Google TensorFlow example.\nCheck out Mindorks awesome open source projects here\nLicense\nCopyright (C) 2017 MINDORKS NEXTGEN PRIVATE LIMITED\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\nContributing to Android TensorFlow Machine Learning Example\nJust make pull request. You are in!",
      "link": "https://github.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample"
    },
    {
      "autor": "Faster-High-Res-Neural-Inpainting",
      "date": "NaN",
      "content": "High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis\n[update 10/10/2017] Example of -----> photo !!!  editing using inpainting at the project website.\n[update 9/30/2017] We shared the inpainting result of 200 ImageNet images and 100 Paris StreetView Images at the project website.\n[update 9/16/2017] We increase the speed of original version by 6x (30s/image on GPU).\n[update 9/16/2017] All raw images of the paper and supplementary material (including input, output and intermediate results) are available at the project website.\n[Faster Inpainting Updates]:\nWe remove layer 12 of vgg in texture optimization.\nIn texture optimization, we have three scales and 100 iterations at each scale. Now it only computes the nearest patch at the first iteration of each scale, and re-use the nearest index in later iterations.\nThis greatly increases the speed at the cost of very subtle inpainting quality.\nThis is the code for High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis. Given an image, we use the content and texture network to jointly infer the missing region. This repository contains the pre-trained model for the content network and the joint optimization code, including the demo to run example images. The code is adapted from the Context Encoders and CNNMRF. Please contact Harry Yang for questions regarding the paper or the code. Note that the code is for research purpose only.\nDemo\n(The code may give better results when supplying 32 different images (a batch) as input to the content network rather than using one single image as input. This may be due to the mismatch of the training and testing coefficients in batch normalization.)\nInstall Torch: http://torch.ch/docs/getting-started.html#_\nClone the repository\ngit clone https://github.com/leehomyc/High-Res-Neural-Inpainting.git\nDownload the pre-trained models for the content and texture networks and put them under the folder models/.\nRun the Demo\ncd High-Res-Neural-Inpainting\n# This will use the trained model to generate the output of the content network\nth run_content_network.lua\n# This will use the trained model to run texture optimization\nth run_texture_optimization.lua\n# This will generate the final result\nth blend.lua\nCitation\nIf you find this code useful for your research, please cite:\n@InProceedings{Yang_2017_CVPR,\nauthor = {Yang, Chao and Lu, Xin and Lin, Zhe and Shechtman, Eli and Wang, Oliver and Li, Hao},\ntitle = {High-Resolution Image Inpainting Using Multi-Scale Neural Patch Synthesis},\nbooktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\nmonth = {July},\nyear = {2017}\n}",
      "link": "https://github.com/leehomyc/Faster-High-Res-Neural-Inpainting"
    },
    {
      "autor": "Forge",
      "date": "NaN",
      "content": "Forge: a neural network toolkit for Metal\n\u26a0\ufe0f \u26a0\ufe0f \u26a0\ufe0f IMPORTANT: I'm no longer maintaining Forge. It uses an older version of the MPSCNN API that is no longer supported by Apple. I also feel that Core ML has largely taken away the need for a library like this. However, neural networks implemented in Metal are still faster than Core ML. If you're looking for very fast implementations of MobileNet V1, MobileNet V2, and SSD for iOS and macOS, check out my new source code library.\nForge is a collection of helper code that makes it a little easier to construct deep neural networks using Apple's MPSCNN framework.\nRead the blog post\nWhat does this do?\nFeatures of Forge:\nConversion functions. MPSCNN uses MPSImages and MTLTextures for everything, often using 16-bit floats. But you probably want to work with Swift [Float] arrays. Forge's conversion functions make it easy to work with Metal images and textures.\nEasy layer creation. Reduce the boilerplate when building the layers for your neural network. Forge's domain-specific language makes defining a neural net as simple as:\nlet input = Input()\nlet output = input\n--> Resize(width: 28, height: 28)\n--> Convolution(kernel: (5, 5), channels: 20, activation: relu, name: \"conv1\")\n--> MaxPooling(kernel: (2, 2), stride: (2, 2))\n--> Convolution(kernel: (5, 5), channels: 50, activation: relu, name: \"conv2\")\n--> MaxPooling(kernel: (2, 2), stride: (2, 2))\n--> Dense(neurons: 320, activation: relu, name: \"fc1\")\n--> Dense(neurons: 10, name: \"fc2\")\n--> Softmax()\nlet model = Model(input: input, output: output)\nCustom layers. MPSCNN only supports a limited number of layers, so we've added a few of our own:\nDepth-wise convolution\nTranspose channels\nDeconvolution (coming soon!)\nPreprocessing kernels. Often you need to preprocess data before it goes into the neural network. Forge comes with a few handy kernels for this:\nSubtractMeanColor\nRGB2Gray\nRGB2BGR\nCustom compute kernels. Many neural networks require custom compute kernels, so Forge provides helpers that make it easy to write and launch your own kernels.\nDebugging tools. When you implement a neural network in Metal you want to make sure it actually computes the correct thing. Due to the way Metal encodes the data, inspecting the contents of the MTLTexture objects is not always straightforward. Forge can help with this.\nExample projects. Forge comes with a number of pretrained neural networks, such as LeNet-5 on MNIST, Inception3 on ImageNet, and MobileNets.\nNote: A lot of the code in this library is still experimental and subject to change. Use at your own risk!\niOS 10 and iOS 11 compatibility\nForge supports both iOS 10 and iOS 11.\nForge must be compiled with Xcode 9 and the iOS 11 SDK. (An older version is available under the tag xcode8, but is no longer supported.)\nImportant changes:\nThe order of the weights for DepthwiseConvolution layers has changed. It used to be:\n[kernelHeight][kernelWidth][channels]\nnow it is:\n[channels][kernelHeight][kernelWidth]\nThis was done to make this layer compatible with MPS's new depthwise convolution. On iOS 10, Forge will use its own DepthwiseConvolutionKernel, on iOS 11 and later is uses the MPS version (MPSCNNDepthWiseConvolutionDescriptor).\nNote: Forge does not yet take advantage of all the MPS improvements in iOS 11, such as the ability to load batch normalization parameters or loading weights via data sources. This functionality will be added in a future version.\nRun the examples!\nTo see a demo of Forge in action, open Forge.xcworkspace in Xcode and run one of the example apps on your device.\nYou need at least Xcode 9 and a device with an A8 processor (iPhone 6 or better) running iOS 10 or later. You cannot build for the simulator as it does not support Metal.\nThe included examples are:\nMNIST\nThis example implements a very basic LeNet5-type neural network, trained on the MNIST dataset for handwritten digit recognition.\nRun the app and point the -----> camera !!!  at a handwritten digit (there are some images in the Test Images folder you can use for this) and the app will tell you what digit it is, and how sure it is of this prediction.\nThe small image in the top-left corner shows what the network sees (this is the output of the preprocessing shader that attempts to increase the contrast between black and white).\nThere are two targets in this project:\nMNIST\nMNIST-DSL\nThey do the exact same thing, except the first one is written with straight MPSCNN code and the second one uses the Forge DSL and is therefore much easier to read.\nInception-v3\nGoogle's famous Inception network for image classification. Point your phone at some object and the app will give you its top-5 predictions:\nThe Inception example app is based on Apple's sample code but completely rewritten using the DSL. We use their learned parameters. Thanks, Apple!\nYOLO\nYOLO is an object detection network. It can detect multiple objects in an image and will even tell you where they are!\nThe example app implements the Tiny YOLO network, which is not as accurate as the full version of YOLO9000 and can detect only 20 different kinds of objects.\nYOLO9000: Better, Faster, Stronger by Joseph Redmon and Ali Farhadi (2016).\nMobileNets\nThe MobileNets example app is an implementation of the network architecture from the paper MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.\nIt works like Inception-v3 but is much faster. On the iPhone 6s it runs at 20 FPS with only moderate-to-high energy usage.\nForge uses the pretrained weights from shicai/MobileNet-Caffe.\nHow to add Forge to your own project\nUse Xcode 9 or better.\nCopy the Forge folder into your project.\nUse File > Add Files to \"YourProject\" > Forge.xcodeproj to add the Forge project inside your own project.\nDrag Products/Forge.framework into the Embedded Binaries section of your project settings.\nimport Forge in your code.\nNOTE: You cannot build for the simulator, only for \"Generic iOS Device\" or an actual device with arm64 architecture.\nHow to use Forge\nCreating a model with Forge\nImporting a model from Keras, TensorFlow, Caffe, etc\nWhere are the unit tests?\nRun the ForgeTests app on a device.\nThe reason the tests are in a separate app is that Metal does not work on the simulator and Xcode can't run logic tests on the device. Catch-22.\nTODO\nForge is under active development. Here is the list of bugs and upcoming features.\nLicense and credits\nForge is copyright 2016-2017 Matthijs Hollemans and is licensed under the terms of the MIT license.",
      "link": "https://github.com/hollance/Forge"
    },
    {
      "autor": "Awesome-Mobile-Machine-Learning",
      "date": "NaN",
      "content": "Awesome-Mobile-Machine-Learning\nA list of awesome mobile machine learning resources curated by Fritz AI.\nAbout Fritz AI\nFritz AI helps you teach your applications how to see, hear, sense, and think. Create ML-powered features in your mobile apps for iOS, Android, and SnapML. Start with our ready-to-use feature APIs or use our Studio to build your own custom models\u2014without code.\nSign up for a Fritz AI account to start building today.\nTable of Contents\nGetting Started\n(Mostly) Non-Code Primers on Mobile Machine Learning\nGetting Started with Data Science and Machine Learning\nMobile Machine Learning Frameworks\nMobile-Ready\nMobile-Compatible\nCode, Libraries, and Resources\nSnapML\niOS\nAndroid\nBrowser\nServer Side\nFritz Community Resources\nTutorials & Learning\nSnapML\n[SparkAR] (#sparkartutorials)\niOS\nAndroid\nCross/Multi-Platform and Edge\nOnline Courses, Videos, and E-Books\nPublications to Follow\nGetting Started\n(Mostly) Non-Code Primers on Mobile Machine Learning\nMachine learning on mobile: What can you actually do with it?\nMachine Learning and the Future of Mobile App Development\nMachine Learning on Mobile Devices: 3 Steps for Deploying ML in Your Apps\nEmbracing Machine Learning as a Mobile Developer\nEnd-to-End Mobile Machine Learning with Fritz AI: A Non-Developer\u2019s Journey\nMachine Learning on iOS and Android\nDeep Learning on the Edge\nWhy Machine Learning on the Edge\n5 App Ideas to Unleash the Power of Mobile Machine Learning\nHow TensorFlow Lite Optimizes Neural Networks for Mobile Machine Learning\nMachine Learning for Mobile - EBook\nWhy the Future of Machine Learning is Tiny\nMachine Learning on mobile: on the device or in the cloud?\nCameras that understand: Portrait Mode and Google Lens\nMachine Learning App Development\u2014Disrupting the Mobile App Industry\nHow smartphones handle huge neural networks\nHow to Fit Large Neural Networks on the Edge\nAdvances in Machine Learning Are Revolutionizing the Mobile App Development Realm\nNo cloud required: Why AI\u2019s future is at the edge\nComparing Mobile Machine Learning Frameworks\nHow AI Accelerators Are Changing The Face Of Edge Computing\nEmbedded and mobile deep learning research resources\nOn-Device AI: MIT Technology Review Hub\nThe 5 Algorithms for Efficient Deep Learning Inference on Small Devices\nPyTorch Mobile: Exploring Facebook\u2019s new mobile machine learning solution\nAI in the Browser\nFederated Learning: An Introduction\nPopular Mobile Machine Learning Projects to Help You Start Building\nFederated Learning: Motivation and Challenges\nWill NVIDIA GPUs push AI on mobile devices to the next level?\nMobile Machine Learning: 2020 Year in Review\nAlibaba\u2019s Mobile Neural Network: A deep learning framework for mobile and embedded devices\nMonetizing Mobile Machine Learning\nThe State of Mobile Machine Learning in 2020 (Industry Report)\nLens Studio 3.0 introduces SnapML for adding custom neural networks directly to Snapchat\nSnapML in Lens Studio: Using custom machine learning models to power AR experiences\nGetting Started with Data Science and Machine Learning\nNew to Data Science? Here are a few places to start\nMachine Learning Crash Course\u2014Google\nSiraj Raval\u2019s YouTube Channel\nFast.ai\nKaggle: The place to do data science projects\nAwesome Data Science with Python: A curated list of awesome resources for practicing data science using Python, including not only libraries, but also links to tutorials, code snippets, blog posts and talks.\nExplainable, Responsible, and Trustworthy Artificial Intelligence\nMobile Machine Learning Frameworks\nMobile-Ready\nFritz AI: Fritz AI is the machine learning platform for iOS, Android, and SnapML developers/creators. Teach your mobile devices to see, hear, sense, and think.\nCore ML: With Core ML, you can integrate trained machine learning models into your iOS apps.\nTensorFlow Lite: TensorFlow Lite is an open source deep learning framework for on-device inference.\nCreate ML: Use Create ML with familiar tools like Swift and macOS playgrounds to create and train custom machine learning models on your Mac.\nTuri Create API: Turi Create simplifies the development of custom machine learning models. You don\u2019t have to be a machine learning expert to add recommendations, object detection, image classification, image similarity or activity classification to your iOS app.\nML Kit: ML Kit beta brings Google\u2019s machine learning expertise to mobile developers in a powerful and easy-to-use package.\nPyTorch Mobile: PyTorch Mobile is a new framework for helping mobile developers and machine learning engineers embed PyTorch ML models on-device.\nQNNPACK: QNNPACK (Quantized Neural Networks PACKage) is a mobile-optimized library for low-precision high-performance neural network inference. QNNPACK provides implementation of common neural network operators on quantized 8-bit tensors.\nMobile-Compatible\nKeras: Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. EASY\nONNX: ONNX is an open format to represent deep learning models. With ONNX, AI developers can more easily move models between state-of-the-art tools and choose the combination that is best for them. EASY\nMicrosoft Cognitive Toolkit: The Microsoft Cognitive Toolkit (CNTK) is an open-source toolkit for commercial-grade distributed deep learning. HARD\nIBM Watson: Watson is IBM\u2019s suite of enterprise-ready AI services, applications, and tooling. EASY\nCaffe2: A lightweight, modular, and scalable deep learning framework. HARD\nApache MXNet: A fast and scalable training and inference framework with an easy-to-use, concise API for machine learning. HARD\nPyTorch: An open source deep learning platform that provides a seamless path from research prototyping to production deployment..HARD\nCode, Libraries, and Resources\nSnapML\nLens Studio SnapML documentation\nFritz AI for SnapML documentation\nLens Studio support forum, tutorials, discussion, etc\niOS\nfritz-examples: A collection of experiences utilizing machine learning models from Fritz AI\nswift: Swift for TensorFlow Project Home Page.\nswift-models: Models and examples built with Swift for TensorFlow.\nswift-apis: Swift for TensorFlow Deep Learning Library.\nSwift-AI: Swift AI includes a collection of common tools used for artificial intelligence and scientific applications on iOS and macOS.\nSerrano: A Swift deep learning library with Accelerate and Metal support.\nRevolver: A framework for building fast genetic algorithms in Swift.\nfantastic-machine-learning: A curated list of machine learning resources, preferably, mostly focused on Swift/Core ML.\nawesome-ml-demos-with-ios: We tackle the challenge of using machine learning models on iOS via Core ML and ML Kit (TensorFlow Lite).\nAwesome-CoreML-Models: the largest collection of machine learning models in Core ML format. Also includes model conversion formats, external collections of ML models, and individual ML models\u2014all of which can be converted to Core ML.\niOS_ML: List of Machine Learning, AI, NLP solutions for iOS.\nAwesome-Design-Tools: A curated list of the best design tools and frameworks for iOS and macOS.\nawesome-ios: A curated list of awesome iOS ecosystem, including Objective-C and Swift Projects.\nList-CoreML-Models: A list of Core ML models, projects, and resources.\ncoremltools: Core ML community tools contains all supporting tools for CoreML model conversion and validation. This includes Scikit Learn, LIBSVM, Caffe, Keras and XGBoost.\nBender: Bender is an abstraction layer over MetalPerformanceShaders useful for working with neural networks.\nStyleArt: The Style Art library processes images using Core ML with a set of pre trained machine learning models and converts them to different art styles.\nLocoKit: Location, motion, and activity recording framework for iOS; includes the ability to classify device activity by mode of transport.\nawesome-tflite: A curated list of awesome TensorFlow Lite models, samples, tutorials, tools and learning resources.\ngooglesamples / mlkit: A collection of quickstart samples demonstrating the ML Kit APIs on Android and iOS.\nAndroid\nfritz-examples: A collection of experiences utilizing machine learning models from Fritz AI\nawesome-android: A curated list of awesome Android packages and resources.\nawesome-java: A curated list of awesome frameworks, libraries and software for the Java programming language.\nAndroidTensorFlowMachineLearningExample: Android TensorFlow MachineLearning Example (Building TensorFlow for Android).\nonyx: An android library that uses technologies like artificial Intelligence, machine learning, and deep learning to make developers understand the content that they are displaying in their app.\nandroid-malware-analysis: This project seeks to apply machine learning algorithms to Android malware classification.\nawesome-tflite: A curated list of awesome TensorFlow Lite models, samples, tutorials, tools and learning resources.\ngooglesamples / mlkit: A collection of quickstart samples demonstrating the ML Kit APIs on Android and iOS.\nTengineKit: Free Real-Time Face Landmarks - 212 Points For Mobile\nBrowser\ntfjs-models: Pretrained models for TensorFlow.js\nmagenta-js: Music and Art Generation with Machine Intelligence in the Browser\ntfjs-node: TensorFlow powered JavaScript library for training and deploying ML models on Node.js\ntfjs-examples: Examples built with TensorFlow.js\nServer Side\nawesome-machine-learning: A curated list of awesome Machine Learning frameworks, libraries and software.\nawesome-deep-learning: A curated list of awesome Deep Learning tutorials, projects and communities.\nmy-awesome-ai-bookmarks: Curated list of reads, implementations, and core concepts of Artificial Intelligence, Deep Learning, and Machine Learning.\ndatasets: A collection of datasets ready to use with TensorFlow\nFritz AI Community Resources\nMobile ML GitHub Repositories: List of repos with machine learning models ready for mobile, organized by feature type.\nImage Recognition Guide: Almost everything you need to know about how image recognition works.\nObject Detection Guide: Almost everything you need to know about how object detection works.\nImage Segmentation Guide: Almost everything you need to know about how image segmentation works.\nPose Estimation Guide: Almost everything you need to know about how pose estimation works.\nStyle Transfer Guide: Almost everything you need to know about how style transfer works.\nAI Startup Landscape: The AI and Machine Learning landscape is rapidly changing. Here\u2019s a list of current organizations and tools, organized by ML lifecycle stage.\nAI and Machine Learning Newsletters: Explore a collection of helpful AI and ML newsletters.\nMobile Development Newsletters: Explore a collection of helpful mobile development newsletters.\nData Science Newsletters: Explore a collection of helpful data science newsletters.\nFacebook Groups: See our list of Facebook groups for AI and ML, mobile dev, data science, and programming.\nTutorials & Learning\nSnapML\nImage Recognition/Classification\nCreate a Custom AI-Powered Snapchat Lens with Fritz AI Studio\nXbox or PS5 Enthusiast? \u2014 Create an AI-Powered Snapchat Lens with Fritz AI Studio\nObject Detection\nWorking with SnapML Templates: Object Detection\n(Guide) Object Detection ML\nImage Segmentation\nBuilding a Custom Glasses Snapchat Lens with Fritz AI and Lens Studio \u2014 Zero Code Involved\nBuilding a Custom Face Mask Snapchat Lens with Fritz AI and Lens Studio\nSnapML in Lens Studio\u2014Apply AR Effects to Segmented Objects\nCreate Lenses with SnapML & Fritz.ai (YouTube)\nCustom Face Mask Segmentation for Lens Studio with Fritz AI (Video + Transcript)\nSegmentation Textures in Lens Studio\nStyle Transfer\nWorking with SnapML Templates in Lens Studio: Style Transfer\nCreating a Style Transfer Snapchat Lens with Fritz AI and SnapML in Lens Studio\nCombining Style Transfer with Background Replacement in Lens Studio with Fritz AI and SnapML\nCreate a Style Transfer Snapchat Lens with SnapML\u2019s ML Component\n(Guide) Style Transfer ML\nAdjusting Style Transfer Lenses for SnapCamera\nQuickly Build a Snapchat Lens By Leveraging Fritz AI Studio\u2019s Style Transfer Model\nOther\n(Ebook) Working with Machine Learning in Snap Lens Studio\nDemystifying common use cases for SnapML in Lens Studio\nWorking with SnapML Templates in Lens Studio: An Overview\nExploring SnapML: A Technical Overview\nMachine Learning Primer for Snapchat Lens Creators\nSnapML in Lens Studio: ML Component Breakdown\nExploring SnapML Templates used in Lens Studio\nImplementing Conway\u2019s Game of Life in Lens Studio\nLens Trend Spotlight: \u201cCyber Core\u201d\nCreating an Animated 3D Phoenix Lens in Lens Studio\nTips and tricks to keep your Lens within Lens Studio\u2019s 4mb limit\nFull-body Deepfakes, 3D Human Filters and more: New, enhanced AR features round the corner for Snapchat?\nhttps://heartbeat.fritz.ai/an-overview-about-augmented-reality-ar-and-its-type-with-different-use-case-5531f3dbd071\nSimultaneously detecting face, hand motion, and pose in real-time on mobile devices\nBehavior Scripts in Lens Studio: The Basics\nScripting with SnapML in Lens Studio\nSnapchat Lens Recipe: \u201cClone\u201d\nAudio in Lens Studio\nExperimenting with Lens Studio\u2019s Asset Library\nHow Snapchat Lenses Effect TikTok Trends and why Lens Creators are so Important\nCustom Scripting in Lens Studio\nVisual Scripting in Lens Studio\nHow to Use Blender to Create Powerful Lenses\nTween Manager in Lens Studio\nWorking with Particles in Lens Studio\nLens Studio Material Editor\nHow to Add Audio in Lens and Publish Using Lens Studio\nAdd Hair Simulation Effect Using Lens Studio\nLens Studio Basics \u2014 LUTs\nWorking with Face Mesh in Lens Studio\nBuild a Snapchat Lens to Welcome New Texans Using Fritz AI\nVertex Animation in Lens Studio for Snapchat Lenses\nCreating a Face Inset Effect in Lens Studio\nLens Studio Basics \u2014 Importing External Audio\nCreating a Marker Tracking Lens in Lens Studio\nAdding Audio Effects in Lens Studio\nConcepts Behind Material Editor in Lens Studio\nSparkAR\nFace Changing Effect In Spark AR\nHow to Replace Backgrounds in Spark AR Studio\nHow to Create a Face Tracking Effect in Facebook\u2019s AR Studio\n2D Texture Animations in Facebook\u2019s Spark AR Studio\nUnderstanding the Simulator in Spark AR Studio\nCreate Split Face Filter using Spark AR Studio\n[How To Make And Share AR Effects] (https://heartbeat.fritz.ai/how-to-make-and-share-ar-effects-ee4abbbfd106utm_source=github&utm_campaign=awesome_mobile_machine_learning)\nHow to Create Hair Segmentation Effects Using Spark AR Studio\n2D Texture Animations in Facebook\u2019s Spark AR Studio\nCreate Your First Face Reference Effect Using Spark AR Studio\nHow to Create an Effect with Dynamic Text in Spark AR Studio\nUsing 2D Objects in Spark AR\nCreate an AR Filter to Make Background Blur Using Spark AR Studio\nCreate and Publish Your First Instagram AR Filter Using Spark AR Studio\niOS\nComputer Vision\nImage Recognition/Classification\nIntro to machine learning on iOS: Using Core ML to recognize handwritten digits\nBuilding Not Hotdog with Turi Create and Core ML\u2014in an afternoon\nCore ML SImplified with Lumina\nCoffeeBot\u2014Using Scikit-learn, Core ML for iOS, and Alexa to predict the right coffee to drink\nEmotion detection for cats\u2014Custom Vision & Core ML on a Swift Playground\nBuilding an iOS -----> camera !!!  calculator with Core ML\u2019s Vision and Tesseract OCR\nUsing Core ML and Vision in iOS for Age Detection\nUsing Core ML and Custom Vision to Build a Real-Time Hand Sign Detector in iOS\nUsing Core ML and ARKit to Build a Gesture-Based Interface iOS App\nMaking a \u201cPokedex\u201d for iOS Using Create ML and Core ML with Vision\nMoving AI from the Cloud to the Edge with Crowd Count and Apple\u2019s Core ML\nIntroduction to Core ML: Building a Simple Image Recognition App\nBuilding a real-time object recognition iOS app that detects sushi\nDetecting Pneumonia in an iOS App with Create ML\nTraining a Core ML Model with Turi Create to Classify Dog Breeds\nRecreate Dominos Points for Pies app on iOS with Fritz Image Labeling\nDetecting Whisky brands with Core ML and IBM Watson services\nPowering an iOS app with ML: How to get started using Create ML and Core ML\nMachine Learning in iOS: Azure Custom Vision and Core ML\nMachine Learning in iOS: Turi Create and Core ML\nCreating a Custom Core ML Model Using Python and Turi Create\nEvaluate Construction Site Safety on iOS using Machine Learning\nLogo Recognition iOS Application Using Machine Learning and Flask API\nPyTorch Mobile: Image Classification on iOS\nBuilding a Barcode Scanner in Swift on iOS\nIncorporating machine learning into iOS apps\nBuilding a multi-class image classifier on iOS\nOn-Device Machine Learning with SwiftUI and PyTorch Mobile\nBuilding an on-device face mask detector with Fritz AI Studio\nBuild a SwiftUI + Core ML Emoji Hunt Game for iOS\nImplementing a Fritz AI Machine Learning Model in an iOS app\nTrain a Face-Mask Detection Model in Under 5 Minutes using Lobe.ai\nDetecting Skin Cancer on iOS with Xcode and Create ML\nEmoji Classification with Flutter and TensorFlow Lite\nBuild a Cat-or-Dog Classification Flutter App with TensorFlow Lite\nObject/Face Detection\nHand Detection with Core ML and ARKit\nMobileNetV2 + SSDLite with Core ML (Object Detection)\nBuilding a simple lane detection iOS app using OpenCV\nLive Face Tracking on iOS using Vision Framework\nUnity AR Foundation and CoreML: Hand detection and tracking\nMakeML\u2019s Automated Video Annotation Tool for Object Detection on iOS\nAdding custom object detection to an iOS app with Turi Create and Fritz AI\nLicense Plate Recognition, Detection, and Plate Number Extraction on iOS\nBuild a Touchless Swipe iOS App Using ML Kit\u2019s Face Detection API\nScanning Credit Cards with Computer Vision on iOS\nFace Recognition and Detection on iOS Using Native Swift Code, Core ML, and ARKit\nUsing TensorFlow.js in a Native iOS App to Perform Object Detection\nCreate Homemade Recipes of Your Favorite Products on iOS Using Fritz AI Studio\nAutomatically Pixelate Faces on iOS using Face Detection with Native Swift Code\nStyle Transfer\nReal-Time Style Transfer for iOS\nCreating a Prisma-like App with Core ML, Style Transfer, and Turi Create\nStyle Transfer iOS Application Using Convolutional Neural Networks\nTrain and Run a Create ML Style Transfer Model in an iOS Camera Application\nImage Segmentation\nBuild your own Portrait Mode on iOS using machine learning in < 30 minutes\nTry on a new style\u2014Build an iOS app to change your hair color with Fritz Hair Segmentation\nMakeML: Nail Segmentation on iOS\nSimple Semantic Image Segmentation in an iOS Application \u2014 DeepLabV3 Implementation\nSemantic and Instance Segmentation on iOS Using a Flask API \u2014 DeepLabV3+ and Mask R-CNN\nUsing Fritz AI in SwiftUI\nCustom Face Mask Segmentation for Lens Studio with Fritz AI (Video + Transcript\nPose Estimation\nPose Estimation on iOS\nHuman pose estimation on images for iOS\n\u201cJust Point It\u201d: Machine Learning on iOS with Pose Estimation + OCR Using Core ML and ML Kit\nSwipeless Tinder Using iOS 14 Vision Hand Pose Estimation\nText Recognition\nIntegrating Google ML Kit in iOS for Face Detection, Text Recognition, and Many More\nUsing Vision Framework for Text Detection in iOS 11\nVision in iOS: Text detection and Tesseract recognition\nComparing iOS Text Recognition SDKS Using Delta\nText recognition on iOS 13 with Vision, SwiftUI and Combine\nLicense Plate Recognition, Detection, and Plate Number Extraction on iOS\nScanning Credit Cards with Computer Vision on iOS\nOther\nPhoto Stacking in iOS with Vision and Metal\nComputer Vision in iOS: Determine the Best Facial Expression in Live Photos\nCompute Image Similarity Using Computer Vision in iOS\nIntroduction to XGBoost with an Implementation in an iOS Application\nImplement Depth Estimation on iOS Using a FCRN Model\nReal-Time Breath Tracking via AirPods\nNatural Language Processing\nText Classification\nImplementing a Natural Language Classifier in iOS with Keras + Core ML\nTrain a Text Classification Model with Create ML\nText Classification on iOS Using Create ML\nClassifying Movie Reviews With Natural Language Framework\nEasy Topic Classifier on iOS with Apple\u2019s Natural Language Framework\nUsing Create ML on iOS to auto-complete forms\nSentiment Analysis\nTraining a Core ML Model for Sentiment Analysis\nSentiment analysis with Natural Language and SwiftUI\nUsing Core ML and Natural Language for Sentiment Analysis on iOS\nProcessing Tweets Using Natural Language and Create ML on iOS\nSentiment analysis with Natural Language and SwiftUI\nIntroduction to natural language processing in Swift\nSentiment Analysis on iOS Using Swift, Natural Language, and Combine: Hacker News Top Stories\nSentiment Analysis iOS Application Using Hugging Face\u2019s Transformers Library\nBuild a Positive News iOS Application Using the Power of Machine Learning\nTranslation\nText Recognition and Translation on iOS Using ML Kit and Google Translate\nLanguage Identification, Translation, and Smart Reply in iOS with Firebase ML Kit\nNLP Tools and Techniques\nNatural Language on iOS 12: Customizing tag schemes and named entity recognition\nIntroduction to Natural Language Processing in Swift\nCore ML with GloVe Word Embedding and a Recursive Neural Network\n4 Techniques You Must Know for Natural Language Processing on iOS\nExploring Word Embeddings and Text Catalogs with Apple\u2019s Natural Language Framework in iOS\nMatching Natural Language Text for Predefined Data Patterns on Apple's Devices\nOn-Device Video Subtitle Generation in SwiftUI\nSpeech / Audio\nBuilding a Sound Classification iOS Application using AI\nData-Free Speech Translator using SFSpeechRecognizer and ML Kit on iOS\niOS On-Device Speech Recognition\nSound Classification on iOS Using Core ML 3 and Create ML\nSpeech recognition and speech synthesis on iOS with Swift\nRecognizing Speech Locally on an iOS Device Using the Speech Framework\nPowering Accessibility on iOS with SwiftUI and Machine Learning\nModel Conversion/Deployment/Management\nReverse Engineering Core ML\nHow to fine-tune ResNet in Keras and use it in an iOS app via Core ML\niOS 12 Core ML Benchmarks\nReducing Core ML 2 Model Size by 4X Using Quantization in iOS 12\nUsing coremltools to Convert a Keras Model to Core ML for iOS\nTrain and Ship a Core ML Object Detection Model for iOS in 4 Hours\u2014Without a Line of Code\nAdvanced Tips for Core ML\nDoes my Core ML model run on Apple\u2019s Neural Engine\nInstantly deploy your Core ML model on Maive without writing an iOS app\nFiguring out if Core ML models use the Apple Neural Engine\nWorking with Create ML\u2019s MLDataTable to Pre-Process Non-Image Data\nWhat\u2019s New in Core ML 2\nBeginner\u2019s Guide to Core ML Tools: Converting a Caffe Model to Core ML Format\nCustom Layers in Core ML\nMachine Learning in iOS: IBM Watson and Core ML\nRunning Keras models on iOS with Core ML\nTensorFlow to Core ML conversion and model inspection\nAn in-depth look at Core ML 3\nWhat's new in Core ML 3\nHello, Core ML 3\nDesigning Great Mobile ML Experiences\nOn-device training with Core ML \u2013 part 1\nOn-device training with Core ML - part 2\nExploring the new ML Kit features on iOS using Swift\nIntroduction to Machine Learning for iOS Developers\nHow To Create Updatable Models Using Core ML 3\nBuild iOS-ready machine learning models using Create ML\nCreate ML for iOS\u2014 Increasing model accuracy\nIncorporating machine learning into iOS Apps\nExploring Use Cases of Core ML Tools\nHow to convert a NN model from TensorFlow Lite to CoreML\nMLDataTable: The Panda For iOS Developers\nHow to convert images to MLMultiArray\nSwift loves TensorFlow and Core ML\nUpsampling in Core ML\nHow to Get a Core ML Model to Produce Images as Output\nCore ML On-Device Training, with Transfer Learning from Swift for TensorFlow Models\nProtecting Core ML Models\nCore ML and Vision Tutorial: On-device training on iOS\nHow to Run and Test Core ML Models in a Swift Playground\nEvolving Your iOS App\u2019s Intelligence with Core ML Model Deployment\n[Machine Learning in Android using TensorFlow Lite] (https://heartbeat.fritz.ai/machine-learning-in-android-using-tensorflow-lite-4fb0755dd1e?utm_source=github&utm_campaign=awesome_mobile_machine_learning)\nOther\nRay Wenderlich iOS Machine Learning Tutorials\nBuild a Core ML Recommender Engine for iOS Using Create ML\nColorizing Images in an iOS App Using DeOldify and a Flask API\nAdvancements in Artificial Intelligence in iOS 14\nCore ML + ARKit: Annotating objects in Augmented Reality\nAndroid\nComputer Vision\nImage Recognition/Classification\nIntroduction to Machine Learning on Android Part 2: Building an app to recognize handwritten digits\nUsing TensorFlow Lite and ML Kit to Build a \u201cPokedex\u201d in Android\nVisual Recognition in Android Using IBM Watson\nBuilding a Custom Machine Learning Model on Android with TensorFlow Lite\nExploring Firebase ML Kit on Android: Barcode Scanning (Part 3)\nUsing TensorFlow on Android\u2014step by step code explanation\nMobile intelligence\u2014TensorFlow Lite Classification on Android\nHow to apply Machine Learning to Android using Fritz\nImage Recognition with ML Kit\nCreating a Google Lens clone using Firebase ML Kit\nApplying TensorFlow in Android in 4 steps\nUsing TensorFlow Lite and ML Kit to build custom machine learning models for Android\nInspecting TensorFlow Lite image classification model\nImage Classification on Android with TensorFlow Lite and CameraX\nImage Labeling on Android in Kotlin using Fritz AI and CameraX\nPlant Disease Classification with TensorFlow Lite on Android - Part 1 and Part 2\nTesting TensorFlow Lite image classification model\nImage Classification on Android using OpenCV\nImage Classification on Android using a Keras Model Deployed in Flask\nPyTorch Mobile: Image classification on Android\nBuild TensorFlow Lite model with Firebase AutoML Vision Edge\nImage Recognition for Android with a Custom TensorFlow Lite Model\nWorking with TensorFlow Lite in Flutter\nTensorFlow Lite Model Maker: Build an Image Classifier for Android\nDigit Recognizer with Flutter and TensorFlow Lite\nImage Classification on Mobile with Flutter, TensorFlow Lite, and Teachable Machine\nImage Labeling in Android with Fritz AI\nHow to identify Nigerian dishes using Artificial Intelligence on Android Devices (Part 1)\nImage Labeling In Android using Google\u2019s On-Device ML Kit\nObject Detection\nReal-Time Face Detection on Android with ML Kit\nCreating an Android app with Snapchat-style filters in 7 steps using Firebase\u2019s ML Kit\nIdentifying and Counting Items in Real-Time with Fritz Object Detection for Android\nExploring Firebase ML Kit on Android: Landmark Detection (Part 4)\nDetecting Pikachu on Android using TensorFlow Object Detection\nBuilding a pet monitoring app in Android with machine learning\nBuilding a real-time object detection app on Android using Firebase ML Kit\nBlink detection on Android using Firebase ML Kit\u2019s Face Detection API\nFlutter Face Detection\nSolve WordSearch games with Android and ML Kit\nObject Detection in Android Using Firebase ML Kit\nObject Detection in Android with Fritz AI\nTurning the Mobile Camera into a Real-Time Object Detector with Flutter and TensorFlow Lite\nOn-Device Face Detection on Android using Google\u2019s ML Kit\nObject Detection in Android Using Firebase ML Kit\nObject Detection with Flutter and TensorFlow Lite\nStyle Transfer\nReal-Time Style Transfer for Android\nStyle Transfer on Android in Kotlin using Fritz AI and CameraX\nStyle Transfer in Android Applications with Fritz AI\nImage Segmentation\nEmbrace your new look with Hair Segmentation by Fritz\u2014Now available for Android developers\nCreating a Pet Sticker App on Android with Fritz Pet Segmentation\nImage Segmentation in Android with Fritz AI\nPose Estimation\nPose Estimation on Android\nDetecting cropped faces in images using the Fritz Pose Estimation API\nHuman Pose Estimation in Android using Fritz AI\nText Recognition\nText Recognition with ML Kit\nSolve WordSearch games with Android and ML Kit\nExtracting Text from Images on Android\nCard Scanner on Android Using Firebase\u2019s ML Kit and CameraX\nRecognize Text in Images on Android with Firebase\u2019s ML Kit\nIdentify Language of Text on Android Using Google\u2019s ML Kit\nOther\nWorking with the OpenCv Camera for Android: Rotating, Orienting, and Scaling\nCameraX: \u2018The\u2019 Machine Learning Camera Library for Android\nAutomate testing of TensorFlow Lite model implementation\nRunning Artificial Neural Networks in Android using OpenCV\nOn-Device Activity Recognition\nVideo Processing in Android with Fritz AI\nFritz AI in Flutter Applications\nScan Barcodes on Android Using the Power of ML Kit and Fotoapparat\nA Definitive Guide for Audio Processing in Android with TensorFlow Lite Models\nQ-Learning With The Frozen Lake Environment In Android\nImplementing The Gaussian Na\u00efve Bayes Classifier In Android\nNatural Language Processing\nImplementing ML Kit\u2019s Smart Reply API in an Android App\nHow to Code Natural Language Processing on Android with IBM Watson\nMachine Learning in Action: Building a Universal Translator App for Android with Kotlin\nCreating an offline translation Android app using Firebase ML Kit\nEnhancing Word Suggestions for Auto-completing Text in Android\nOn-Device Smart Replies on Android using Google\u2019s ML Kit\nModel Conversion/Deployment/Management\nIntro to Machine Learning on Android Part 1\u2014How to convert a custom model to TensorFlow Lite\nDeploying PyTorch and Kera Models to Android with TensorFlow Mobile\nCompiling a TensorFlow Lite Build with Custom Operations\nBenchmarking TensorFlow Mobile on Android devices in production\nProfiling TensorFlow Lite Models for Android\nExploring Firebase ML Kit on Android: Introducing ML Kit (Part 1)\nDeploying Keras Deep Learning Models with Java\nExporting TensorFlow models to ML Kit\nFrom Keras to ML Kit\nUsing TensorFlow Lite on Android\nUsing Deep Learning and Neural Networks in Android Applications\nLoading and running a quantized TensorFlow Lite model on Android\nCoding Feed-Forward Neural Networks in Kotlin (or Android)\nBuild with Augmented Reality (AR) on Android Using ARCore and Sceneform\nExploring Random Forests In Light Of Kotlin\nCross/Multi-Platform and IoT/Edge\nMobile\nMachine Learning models on the edge: mobile and Iot\nTroubleshooting TensorFlow Lite on Windows 10\nThe Mobile Neural Network Lottery\nTransfer learning: Can it enable AI on every smartphone?\nBuilding an Image Recognition Model for Mobile using Depthwise Convolutions\n8-bit Quantization and TensorFlow Lite: Speeding up mobile inference with low precision\nNeural Networks on Mobile Devices with TensorFlow Lite: A Tutorial\nBuilding Text Detection apps for iOS and Android using React-Native\nUsing ONNX to Transfer Machine Learning Models from PyTorch to Caffe2 and Mobile\nHardware acceleration for machine learning on Apple and Android devices\n20-Minute Masterpiece: Training your own mobile-ready style transfer model\nComparing Firebase ML Kit\u2019s Text Recognition on Android & iOS\nCreating a 17 KB style transfer model with layer pruning and quantization\nLeveraging AI with Location Data in Mobile Apps\nExploring the MobileNet Models in TensorFlow\nDistributing on-device machine learning models with tags and metadata\nReal-Time 2D/3D Feature Point Extraction from a Mobile Camera\nUsing Generative Deep Learning Models On-Device\nUsing Generative Deep Learning Models On-Device, Part 2: Text & Audio\nBuild and AI-Powered Artistic Style Transfer App with Fritz and React Native\nHow to use the Style Transfer API in React Native with Fritz\nMachine Learning and Augmented Reality Combined in One Sleek Mobile App \u2013 How We Built CarLens\nIncreasing the Accuracy of the Machine Learning Model in the CarLens Mobile App\nHow to build custom TensorFlow binary for Android and iOS\nTesting TensorFlow Lite image classification model\nDeploy ML models using Flask as REST API and access via Flutter app\nHow to Deploy Machine Learning Models on Mobile and Embedded Devices\nReal-time Mobile Video Object Detection using Tensorflow\nCreate AR experiences with the Fritz Unity SDK \u2014 Bird Perch Tutorial with Pose Estimation\nImage Classification on React Native with TensorFlow.js and MobileNet\nTraining a TensorFlow Lite model for mobile using AutoML Vision Edge\nGPU-Accelerated Mobile Multi-view Style Transfer\nAndroid Image Classification with TensorFlow Lite & Azure Custom Vision Service\nText Recognition in Flutter Using Firebase\u2019s ML Kit\nReal-Time 3D Object Detection on Mobile Devices with MediaPipe\nImage Labeling in Flutter Using Firebase\u2019s ML Kit\nFace Detection in Flutter Using Firebase\u2019s ML Kit\nBuilding a Cross-Platform Image Classifier with Flutter and TensorFlow Lite\nDeep Learning for Natural Language Processing on Mobile Devices\nScan Barcodes in Flutter Using Firebase\u2019s ML Kit\nImplementing MobileBERT for Next Sentence Prediction\nTraining an Image Classification Model for Mobile using TensorFlow Lite\nGenerative Adversarial Networks (GANs) for Mobile Devices\nTensorFlow Estimators \u2014 TFLite and Model Generation\nNeural Network Pruning Research Review 2020\nQuantization Arithmetic\nMobileBERT Using PyTorch for Multiple Choice\nPractical tips for better quantization results\nTensorFlow Lite Model for On-Device Housing Price Predictions\nDog Breed Classification on Mobile with Flutter and TensorFlow Lite -Detect Facial Emotions on Mobile and IoT Devices Using TensorFlow Lite -TensorFlow Lite Image Classification Models with Model Maker -Implementing Real-Time Pose Estimation on Mobile Using Flutter\nTensorFlow Lite Text Classification Models with Model Maker\nEdge/Browser\nUsing TensorFlow.js to Automate the Chrome Dinosaur Game\nReal-Time Object Detection on Raspberry Pi Using OpenCV DNN\nBuilding an image recognition app using ONNX.js\nEdge TPU: Hands-On with Google\u2019s Coral USB Accelerator\nBuilding a Vision-Controlled Car Using Raspberry Pi\u2014From Scratch\nRaspberry Pi and machine learning: How to get started\nKeras and deep learning on the Raspberry Pi\nAccelerating Convolutional Neural Networks on Raspberry Pi\nMachine Learning in Node.js with TensorFlow.js\nHow to run a Keras model on Jetson Nano\nBuild AI that works offline with Coral Dev Board, Edge TPU, and TensorFlow Lite\nGoogle Coral Edge TPU vs NVIDIA Jetson Nano: A quick deep dive into EdgeAI performance\nBenchmarking Edge Computing\nMerging TensorFlow Lite and \u03bcTensor\nObject detection and image classification with Google Coral USB Accelerator\nBuild a Hardware-based Face Recognition System for $150 with the Nvidia Jetson Nano and Python\nA Brief Guide to the Intel Movidius Neural Compute Stick with Raspberry Pi 3\nCoral USB Accelerator, TensorFlow Lite C++ API & Raspberry Pi for Edge TPU object detection\nPortable Computer Vision: TensorFlow 2.0 on a Raspberry Pi (Part 1 of 2)\nTensorFlow Lite Ported to Arduino\nGetting Started with Edge AI Using the GAP8 Processor\nReal-Time Person Tracking on the Edge with a Raspberry Pi\nBuilding Brains on the Edge\nUsing TensorFlow.js to Train a \u201cRock-Paper-Scissors\u201d Model\nThe Big Benchmarking Roundup\nComprehensive TensorFlow.js Example\nFruit identification using Arduino and TensorFlow\nAutoML Vision Edge: Build machine learning models for mobile and edge devices\u2014in hours\nCreating a TensorFlow Lite Object Detection Model using Google Cloud AutoML\nAutoML Vision Edge: Exporting and Loading TensorFlow SavedModels with Python\nAutoML Vision Edge: Loading and Running a TensorFlow.js Model (Part 1)\nAutoML Vision Edge: Loading and Running a TensorFlow.js Model (Part 2)\nHacking Google Coral Edge TPU: motion blur and Lanczos resize\nPhotoBooth Lite on Raspberry Pi with TensorFlow Lite\nAutoML Vision Edge: Deploying and Running TensorFlow Models using Docker Containers\nMachine Learning at the Edge \u2014 \u03bcML\nFace and hand tracking in the browser with MediaPipe and TensorFlow.js\nAutoML Vision Edge: Comparing Model Formats\nUsing Google Cloud AutoML Edge Image Classification Models in Python\nUsing Google Cloud AutoML Edge Object Detection Models in Python\nRunning TensorFlow Lite Image Classification Models in Python\nRunning TensorFlow Lite Object Detection Models in Python\nMake deep learning models run fast on embedded hardware\nCreating \u201cNo Trump Social\u201d with TensorFlow.js\nDeep Learning with JavaScript (Part 1)\nDeep Learning in JavaScript (Part 2)\nDeep Learning in JavaScript (Part 3)\nDeep Learning In JavaScript (Part 4)\nConstructing a 3D Face Mesh from Face Landmarks in Real-Time with TensorFlow.js and Plot.js\nIntroduction to hand detection in the browser with Handtrack.js and TensorFlow\nHow to Detect Face Masks in Images with TensorFlow.js\nMulti-team object detection for football games on Raspberry Pi 3\nReal-Time Human Pose Estimation with TensorFlow.js\nHow to detect a \u201cthumbs-up\u201d in the browser with TensorFlow.js\nGetting Started with Face Landmark Detection in the Browser with TensorFlow.JS\nTensorFlow Lite Micro\nBody Segmentation in the Browser with TensorFlow.js\nGetting Started with Object Detection Using TensorFlow.js\nUsing the Snapdragon Neural Processing Engine for efficient edge deployment of ML models\nImage Classification Made Easy in the Browser with TensorFlow.js\nEnd-to-end object detection using EfficientDet on Raspberry Pi 3 (Part 1)\nEnd-to-end Object Detection Using EfficientDet on Raspberry Pi 3 (Part 2)\nEnd-to-End Object Detection Using EfficientDet on Raspberry Pi 3 (Part 3)\nBuilding a \u201cMotivation Bot\u201d with TensorFlow.js, Face Detection, and Emotion Classification\nGetting Started with Image Segmentation Using TensorFlow.js\nSafer and Smarter: Contactless shopping with on-device object detection\nAn Introduction to Insider Threat Detection With Machine Learning\n[Creating Instagram and Facebook AR Filters with Spark AR Studio (Part 4)(https://heartbeat.fritz.ai/creating-instagram-and-facebook-ar-filters-with-spark-ar-studio-part-4-8e0f4421ab01?utm_source=github&utm_campaign=awesome_mobile_machine_learning)\nOnline Courses, Videos, & E-Books\niOS\nCourses\nCore ML: Machine Learning for iOS\u2014Udacity\nFundamentals of Core ML: Machine Learning for iOS\u2014Udemy\nMachine Learning in iOS Using Swift\u2014Udemy\nComplete iOS Machine Learning Masterclass\u2014Udemy\nMachine Learning with Core ML 2 and Swift 5\u2014Udemy\nVideo Tutorials\nA Guide to Core ML on iOS\nUnderstand Core ML in 5 Minutes\nMachine Learning tutorial with Core ML 2\u2014Part 1\nMachine Learning tutorial with Core ML 2\u2014Part 2\niOS 12 Swift Tutorial: Create a Fruit Classifier with Creat ML\nMachine Learning with Core ML in iOS 11: Training Core ML & Using Vision Framework\nE-Books\nThe Mobile Machine Learning Lifecycle\nCore ML Survival Guide\nMachine Learning by Tutorials\nMachine Learning with Core ML: An iOS developer\u2019s guide to implementing machine learning in mobile apps\nMachine Learning with Swift\nBuilding Mobile Applications with TensorFlow\nPractical Artificial Intelligence with Swift\nAndroid\nCourses\nMobile Machine Learning for Android: TensorFlow and Python\u2014Udemy\nMachine Learning for Android App Development Using ML Kit\u2014Udemy\nVideo Tutorials/Talks\nMachine Learning with TensorFlow\u2014On-Device\nA Guide to Running TensorFlow Models on Android\nAndroid Developer\u2019s Guide to Machine Learning: With ML Kit and TensorFlow Lite\nOther\nBuilding Mobile Applications with TensorFlow\nTinyML Machine Learning with TensorFlow on Arduino, and Ultra-Low Power Micro-Controllers\nPublications to Follow\nHeartbeat: Covering the intersection of machine learning and mobile app development.\nProAndroidDev: Professional Android Development: the latest posts from Android Professionals and Google Developer Experts.\nFlawless App Stories: Community around iOS development, mobile design and marketing\nAppCoda Tutorials: A great collection of Swift and iOS app development tutorials.\nSwift Programming: Tutorials and articles covering various Swift-related topics.\nAnalytics Vidhya: Analytics Vidhya is a community of Analytics and Data Science professionals.\nTowards Data Science: A platform for thousands of people to exchange ideas and to expand our understanding of data science.\nFreeCodeCamp: Stories worth reading about programming and technology from an open source community.\nMachine, Think!: Matthijs Hollemans\u2019s blog that features deep dives on topics related to deep learning on iOS.\nPete Warden\u2019s Blog: Pete Warden is the CTO of Jetpac and writes about a variety of ML topics, including frequent looks at issues in mobile/edge ML.\nMachine Learning Mastery: Jason Brownlee's library of quick-start guides, tutorials, and e-books, all designed to help developers learn machine learning.\nThink, mobile!: Mirek Stanek's excellent blog covering a range of topics on mobile intelligence.\nStay in touch with Fritz AI\nTo keep tabs on what we\u2019re up to, and for an inside look at the opportunities, challenges, and tools for mobile machine learning, subscribe to the Fritz AI Newsletter\nJoin the community\nHeartbeat is a community of developers interested in the intersection of mobile and machine learning. Chat with us in Slack, and stay up to date on industry news, trends, and more by subscribing to Deep Learning Weekly.\nHelp\nFor any questions or issues, you can:\nSubmit an issue on this repo\nGo to our Help Center\nMessage us directly in Slack",
      "link": "https://github.com/fritzlabs/Awesome-Mobile-Machine-Learning"
    },
    {
      "autor": "AI-Chip",
      "date": "NaN",
      "content": "AI Chip (ICs and IPs)\nEditor S.T.(Linkedin)\nWelcome to My Wechat Blog StarryHeavensAbove for more AI chip related articles\nLatest updates\nAdd news of Cerebras.\nAdd news of Habana.\nAdd news of Google Tensor Chip.\nAdd news of Intel Loihi 2.\nAdd news of Tesla Dojo.\nAdd news of Untether AI.\nAdd startup Innatera Nanosystems.\nAdd startup EdgeQ.\nAdd startup Quadric.\nAdd startup Analog Inference.\nAdd news of Tenstorrent.\nAdd news of Google.\nAdd news of SiMa.ai.\nAdd startup Neureality.\nAdd news of Cerebras.\nAdd news of Groq.\nAdd news of Nvidia.\nAdd news of SambaNova.\nAdd news of Baidu.\nUpdates of Deep Vision.\nAdd news of Flex Logix.\nUpdate AI Compiler section.\nAdd the article \"TPU vs GPU vs Cerebras vs Graphcore: A Fair Comparison between ML Hardware\" in Reference section.\nAdd news of Tenstorrent.\nAdd news of Synaptics Katana Platform.\nAdd Graphcore MK2 PERFORMANCE BENCHMARKS.\nAdd news of SambaNova.\nAdd news of Esperanto's ML Chip.\nAdd news of AWS Trainium.\nAdd startup SimpleMachines.\nAdd news of SK Telecom SAPEON X220.\nAdd news of Imagination AI accelerator.\nAdd news of Mythic.\nAdd link to MLPerf Inference Results 0.7.\nAdd news of Qualcomm Cloud AI 100.\nAdd link to MLPerf Training Results 0.7.\nAdd Neural Network Accelerator Comparison in Reference.\nAdd AIchip Paper List in Reference.\nAdd news of Nvidia A100.\nAdd Sony's Intelligent Vision Sensors.\nAdd a series of articles \"What We Talk About When We Talk About AI Chip\" in Reference section.\nAdd news of Wave Computing.\nShortcut\nIC VendorsIntel, Qualcomm, Nvidia, Samsung, AMD, Xilinx, IBM, STMicroelectronics, NXP, Marvell, MediaTek, HiSilicon, Rockchip, Renesas Electronics, Ambarella, Sony17\nTech Giants & HPC VendorsGoogle, Amazon_AWS, Microsoft, Apple, Aliyun, Alibaba Group, Tencent Cloud, Baidu, Baidu Cloud, HUAWEI, Fujitsu, Nokia, Facebook, HPE, Tesla, LG, SK Telecom16\nIP VendorsARM, Synopsys, Imagination, CEVA, Cadence, VeriSilicon, Videantis7\nStartups in China Cambricon, Horizon Robotics, Bitmain, Chipintelli, Thinkforce, Unisound, AISpeech, Rokid, NextVPU, Canaan, Enflame, Eesay Tech, WITINMEM, TSING MICRO, Black Sesame, Corerain16\nStartups Worldwide Cerebras, Graphcore, PEZY, Tenstorrent, Blaize, Koniku, Adapteva, Knowm, Mythic, Kalray, BrainChip, AImotive, Leepmind, Krtkl, NovuMind, REM, TERADEEP, Deep Vision, Groq, Kneron, Esperanto Technologies, Gyrfalcon Technology, SambaNova Systems, GreenWaves Technology, Lightelligence, Lightmatter, ThinkSilicon, Innogrit, Kortiq, Hailo,Tachyum,AlphaICs,Syntiant, aiCTX, Flex Logix, Preferred Network, Cornami, Anaflash, Optaylsys, Eta Compute, Achronix, Areanna AI, Neuroblade, Luminous Computing, Efinix, AISTORM, SiMa.ai,Untether AI, GrAI Matter Lab, Rain Neuromorphics, Applied Brain Research, XMOS, DinoPlusAI, Furiosa AI, Perceive, SimpleMachines, Neureality, Analog Inference, Quadric, EdgeQ, Innatera Nanosystems61\nI. IC Vendors\nGPU\nNVIDIA Unveils Grace: A High-Performance Arm Server CPU For Use In Big AI Systems\nKicking off another busy Spring GPU Technology Conference for NVIDIA, this morning the graphics and accelerator designer is announcing that they are going to once again design their own Arm-based CPU/SoC. Dubbed Grace \u2013 after Grace Hopper, the computer programming pioneer and US Navy rear admiral \u2013 the CPU is NVIDIA\u2019s latest stab at more fully vertically integrating their hardware stack by being able to offer a high-performance CPU alongside their regular GPU wares. According to NVIDIA, the chip is being designed specifically for large-scale neural network workloads, and is expected to become available in NVIDIA products in 2023.\nNVIDIA Ampere Architecture In-Depth\nToday, during the 2020 NVIDIA GTC keynote address, NVIDIA founder and CEO Jensen Huang introduced the new NVIDIA A100 GPU based on the new NVIDIA Ampere GPU architecture. This post gives you a look inside the new A100 GPU, and describes important new features of NVIDIA Ampere architecture GPUs.\nNVDLA Deep Learning Inference Compiler is Now Open Source\nWith the open-source release of NVDLA\u2019s optimizing compiler on GitHub, system architects and software teams now have a starting point with the complete source for the world\u2019s first fully open software and hardware inference platform.\nNVIDIA TESLA T4 TENSOR CORE GPU\nPowering the TensorRT Hyperscale Inference Platform.\nNVIDIA Reveals Next-Gen Turing GPU Architecture: NVIDIA Doubles-Down on Ray Tracing, GDDR6, & More\nat NVIDIA\u2019s SIGGRAPH 2018 keynote presentation, company CEO Jensen Huang formally unveiled the company\u2019s much awaited (and much rumored) Turing GPU architecture. The next generation of NVIDIA\u2019s GPU designs, Turing will be incorporating a number of new features and is rolling out this year.\nSoC\nOn edge, Nvidia provide NVIDIA DRIVE\u2122 PX, The AI Car Computer for Autonomous Driving and JETSON TX1/TX2 MODULE, \"The embedded platform for autonomous everything\".\nNVDLA\nNvidia anouced \"XAVIER DLA NOW OPEN SOURCE\" on GTC2017. We did not see Early Access verion yet. Hopefully, the general release will be avaliable on Sep. as promised. For more analysis, you may want to read \u4eceNvidia\u5f00\u6e90\u6df1\u5ea6\u5b66\u4e60\u52a0\u901f\u5668\u8bf4\u8d77.\nNow the open source DLA is available on Github and more information can be found here. > The NVIDIA Deep Learning Accelerator (NVDLA) is a free and open architecture that promotes a standard way to design deep learning inference accelerators. With its modular architecture, NVDLA is scalable, highly configurable, and designed to simplify integration and portability. The hardware supports a wide range of IoT devices. Delivered as an open source project under the NVIDIA Open NVDLA License, all of the software, hardware, and documentation will be available on GitHub. Contributions are welcome.\nNervana\nIntel\u00ae Nervana\u2122 Neural Network processors\nMobileye EyeQ\n> Mobileye is currently developing its fifth generation SoC, the EyeQ\u00ae5, to act as the vision central computer performing sensor fusion for Fully Autonomous Driving (Level 5) vehicles that will hit the road in 2020. To meet power consumption and performance targets, EyeQ\u00ae SoCs are designed in most advanced VLSI process technology nodes \u2013 down to 7nm FinFET in the 5th generation.\nMovidius\nNew Intel Vision Accelerator Solutions Speed Deep Learning and Artificial Intelligence on Edge Devices\nToday, Intel unveiled its family of Intel\u00ae Vision Accelerator Design Products targeted at artificial intelligence (AI) inference and analytics performance on edge devices, where data originates and is acted upon. The new acceleration solutions come in two forms: one that features an array of Intel\u00ae Movidius\u2122 vision processors and one built on the high-performance Intel\u00ae Arria\u00ae 10 FPGA.\nFPGA\nIntel FPGA OpenCL and Solutions.\nLoihi\nIntel Advances Neuromorphic with Loihi 2, New Lava Software Framework and New Partners\nSecond-generation research chip uses pre-production Intel 4 process, grows to 1 million neurons. Intel adds open software framework to accelerate developer innovation and path to commercialization.\nHabana\nHabana Gaudi debuts in the Amazon EC2 cloud\nThe primary motivation to create this new training instance class was presented by Andy Jassy in the 2020 re:Invent: \u201cTo provide our end-customers with up to 40% better price-performance than the current generation of GPU-based instances.\u201d\nQualcomm Launches Cloud AI Chip\nLast year, Qualcomm teased its Cloud AI100, promising strong performance and power efficiency to enable Artificial Intelligence in cloud edge computing, autonomous vehicles and 5G infrastructure. Today, the company announced it is now sampling the platform, with volume shipments planned for the first half of 2021. This begs the question: why would a company known for low-power cell-phone chips and IP decide to enter the data center market, which is full of players who have been there for decades?\nQualcomm Brings Power Efficient Artificial Intelligence Inference Processing to the Cloud\nQualcomm Technologies, Inc., a subsidiary of Qualcomm Incorporated (NASDAQ: QCOM), announced that it is bringing the Company\u2019s artificial intelligence (AI) expertise to the cloud with the Qualcomm\u00ae Cloud AI 100. Built from the ground up to meet the explosive demand for AI inference processing in the cloud, the Qualcomm Cloud AI 100 utilizes the Company\u2019s heritage in advanced signal processing and power efficiency.\nSnapdragon 855 Mobile Platform\nOur 4th generation on-device AI engine is the ultimate personal assistant for -----> camera !!! , voice, XR and gaming \u2013 delivering smarter, faster and more secure experiences. Utilizing all cores, it packs 3 times the power of its predecessor for stellar on-device AI capabilities... Greater than 7 trillion operations per second (TOPS)\nSamsung Brings On-device AI Processing for Premium Mobile Devices with Exynos 9 Series 9820 Processor > Fourth-generation custom core and 2.0Gbps LTE Advanced Pro modem enables enriched mobile experiences including AR and VR applications\nSamsung resently unveiled \u201cThe new Exynos 9810 brings premium features with a 2.9GHz custom CPU, an industry-first 6CA LTE modem and deep learning processing capabilities\u201d.\nThe soon to be released AMD Radeon Instinct MI25 is promising 12.3 TFlops of SP or 24.6 TFlops of FP16. If your calculations are amenable to Nvidia's Tensors, then AMD can't compete. Nvidia also does twice the bandwidth with 900GB/s versus AMD's 484 GB/s. > AMD has put a very good X86 server processor into the market for the first time in nine years, and it also has a matching GPU that gives its OEM and ODM partners a credible alternative for HPC and AI workload to the combination of Intel Xeons and Nvidia Teslas that dominate hybrid computing these days.\nTesla is reportedly developing its own processor for artificial intelligence, intended for use with its self-driving systems, in partnership with AMD. Tesla has an existing relationship with Nvidia, whose GPUs power its Autopilot system, but this new in-house chip reported by CNBC could potentially reduce its reliance on third-party AI processing hardware.\nXilinx Launches the World's Fastest Data Center and AI Accelerator Cards\nXilinx launched Alveo, a portfolio of powerful accelerator cards designed to dramatically increase performance in industry-standard servers across cloud and on-premise data centers.\nXilinx provide \"Machine Learning Inference Solutions from Edge to Cloud\" and naturally claim their FPGA's are best for INT8 with one of their white papers.\nWhilst performance per Watt is impressive for FPGAs, the vendors' larger chips have long had earth shatteringly high chip prices for the larger chips. Finding a balance between price and capability is the main challenge with the FPGAs.\nTrueNorth is IBM's Neuromorphic CMOS ASIC developed in conjunction with the DARPA SyNAPSE program.\nIt is a manycore processor network on a chip design, with 4096 cores, each one simulating 256 programmable silicon \"neurons\" for a total of just over a million neurons. In turn, each neuron has 256 programmable \"synapses\" that convey the signals between them. Hence, the total number of programmable synapses is just over 268 million (228). In terms of basic building blocks, its transistor count is 5.4 billion. Since memory, computation, and communication are handled in each of the 4096 neurosynaptic cores, TrueNorth circumvents the von-Neumann-architecture bottlenecks and is very energy-efficient, consuming 70 milliwatts, about 1/10,000th the power density of conventional microprocessors. Wikipedia\nWith IBM POWER9, we\u2019re all riding the AI wave\n\"With POWER9, we\u2019re moving to a new off-chip era, with advanced accelerators like GPUs and FPGAs driving modern workloads, including AI...POWER9 will be the first commercial platform loaded with on-chip support for NVIDIA\u2019s next-generation NVLink, OpenCAPI 3.0 and PCI-Express 4.0. These technologies provide a giant hose to transfer data.\"\nAI Hardware Center\n\"The IBM Research AI Hardware Center is a global research hub headquartered in Albany, New York. The center is focused on enabling next-generation chips and systems that support the tremendous processing power and unprecedented speed that AI requires to realize its full potential.\nST preps second neural network IC\nSTMicroelectronics is designing a second iteration of the neural networking technology that the company reported on at the International Solid-State Circuits Conference (ISSCC) in February 2017.\nISSCC2017 Deep-Learning Processors\u6587\u7ae0\u5b66\u4e60 \uff08\u4e00\uff09 is a reference.\nS32 AUTOMOTIVE PLATFORM\nS32 AUTOMOTIVE PLATFORM\nThe NXP S32 automotive platform is the world\u2019s first scalable automotive computing architecture. It offers a unified hardware platform and an identical software environment across application domains to bring rich in-vehicle experiences and automated driving functions to market faster.\nADAS Chip\nS32V234: Vision Processor for Front and Surround View Camera, Machine Learning and Sensor Fusion Applications\nThe S32V234 is our 2nd generation vision processor family designed to support computation intensive applications for image processing and offers an ISP, powerful 3D GPU, dual APEX-2 vision accelerators, security and supports SafeAssure\u2122. S32V234 is suited for ADAS, NCAP front camera, object detection and recognition, surround view, machine learning and sensor fusion applications. S32V234 is engineered for automotive-grade reliability, functional safety and security measures to support vehicle and industrial automation.\nMarvell Demonstrates Artificial Intelligence SSD Controller Architecture Solution\nMarvell will demonstrate today at the Flash Memory Summit how it will provide artificial intelligence capabilities to a broad range of industries by incorporating NVIDIA\u2019s Deep Learning Accelerator (NVDLA) technology in its family of data center and client SSD controllers.\nMediaTek announced Helio P90, highlighting AI processing.\nThis article, \"MediaTek Announces New Premium Helio P90 SoC\", from AnandTech has more in-deepth analysis.\nKirin for Smart Phone\nKirin 980, the World's First 7nm Process Mobile AI Chipset\nIntroducing the Kirin 980, the world's first 7nm process mobile phone SoC chipset, the world\u2019s first cortex-A76 architecture chipset, the world\u2019s first dual NPU design, and the world\u2019s first chipset to support LTE Cat.21. The Kirin 980 combines multiple technological inFtions and leads the AI trend to provide users with impressive mobile performance and to create a more convenient and intelligent life.\nHiSilicon Kirin 970 Processor annouced fearturing with dedicated Neural-network Processing Unit.\nIn this article,we can find more details about NPU in Kirin970.\nMobile Camera SoC\nAccording to a Brief Data Sheet of Hi3559A V100ESultra-HD Mobile Camera SoC, it has:\nDual-core CNN@700 MHz neural network acceleration engine\nRockchip Released Its First AI Processor RK3399Pro -- NPU Performance up to 2.4TOPs\nRK3399Pro adopted exclusive AI hardware design. Its NPU computing performance reaches 2.4TOPs, and indexes of both high performance and low consumption keep ahead: the performance is 150% higher than other same type NPU processor; the power consumption is less than 1%, comparing with other solutions adopting GPU as AI computing unit.\nRenesas Electronics Develops New Processing-In-Memory Technology for Next-Generation AI Chips that Achieves AI Processing Performance of 8.8 TOPS/W\nRenesas Electronics Corporation (TSE: 6723), a premier supplier of advanced semiconductor solutions, today announced it has developed an AI accelerator that performs CNN (convolutional neural network) processing at high speeds and low power to move towards the next generation of Renesas embedded AI (e-AI), which will accelerate increased intelligence of endpoint devices. A Renesas test chip featuring this accelerator has achieved the power efficiency of 8.8 TOPS/W (Note 1), which is the industry's highest class of power efficiency. The Renesas accelerator is based on the processing-in-memory (PIM) architecture, an increasingly popular approach for AI technology, in which multiply-and-accumulate operations are performed in the memory circuit as data is read out from that memory.\nIntelligent Vision Processors For Edge Applications\nSony to Release World's First Intelligent Vision Sensors with AI Processing Functionality\nSynaptics Expands into Low Power Edge AI Applications with New Katana Platform\nSAN JOSE, Calif., Dec. 15, 2020 \u2013 Synaptics\u00ae Incorporated (Nasdaq: SYNA), today announced the Katana Edge AI\u2122 platform, addressing a growing industry gap for solutions that enable battery powered devices for consumer and industrial IoT markets. The platform combines Synaptics\u2019 proven low power SoC architecture with energy-efficient AI software, enabled by a partnership with Eta Compute. The Katana solution is optimized for a wide range of ultra-low power use cases in edge devices for office buildings, retail, factories, farms and smart homes. Typical applications include people or object recognition and counting, visual, voice or sound detection, asset or inventory tracking and environmental sensing.\nII. Tech Giants & HPC Vendors\nGoogle Tensor: Everything you need to know about the Pixel 6 chip\nGoogle has taken the wraps off its latest Pixel smartphones and, among the changes, the one with the biggest long-term impact is the switch to in-house silicon for the search giant.\nGoogle Launches TPU v4 AI Chips\nGoogle CEO Sundar Pichai spoke for only one minute and 42 seconds about the company\u2019s latest TPU v4 Tensor Processing Units during his keynote at the Google I/O virtual conference this week, but it may have been the most important and awaited news from the event.\nGoogle begins selling the $150 Coral Dev Board, a hardware kit for accelerated AI edge computing\nIf you\u2019re a software dev looking to get a head start on AI development at the edge, why not try on Google\u2019s new hardware for size? The search company today made available the Coral Dev Board, a $150 computer featuring a removable system-on-module with one of its custom tensor processing unit (TPU) AI chips.\nGoogle's original TPU had a big lead over GPUs and helped power DeepMind's AlphaGo victory over Lee Sedol in a Go tournament. The original 700MHz TPU is described as having 95 TFlops for 8-bit calculations or 23 TFlops for 16-bit whilst drawing only 40W. This was much faster than GPUs on release but is now slower than Nvidia's V100, but not on a per W basis. The new TPU2 is referred to as a TPU device with four chips and can do around 180 TFlops. Each chip's performance has been doubled to 45 TFlops for 16-bits. You can see the gap to Nvidia's V100 is closing. You can't buy a TPU or TPU2.\nLately, Google is making Cloud TPUs available for use in Google Cloud Platform (GCP). Here you can find the latest banchmark result of Google TPU2.\nPixel Visual Core is Google\u2019s first custom-designed co-processor for consumer products. It\u2019s built into every Pixel 2, and in the coming months, we\u2019ll turn it on through a software update to enable more applications to use Pixel 2\u2019s camera for taking HDR+ quality pictures.\nTearing Apart Google\u2019s TPU 3.0 AI Coprocessor\nGoogle did its best to impress this week at its annual IO conference. While Google rolled out a bunch of benchmarks that were run on its current Cloud TPU instances, based on TPUv2 chips, the company divulged a few skimpy details about its next generation TPU chip and its systems architecture. The company changed from version notation (TPUv2) to revision notation (TPU 3.0) with the update, but ironically the detail we have assembled shows that the step from TPUv2 to what we will call TPUv3 probably isn\u2019t that big; it should probably be called TPU v2r5 or something like that.\nEdge TPU\nAI is pervasive today, from consumer to enterprise applications. With the explosive growth of connected devices, combined with a demand for privacy/confidentiality, low latency and bandwidth constraints, AI models trained in the cloud increasingly need to be run at the edge. Edge TPU is Google\u2019s purpose-built ASIC designed to run AI at the edge. It delivers high performance in a small physical and power footprint, enabling the deployment of high-accuracy AI at the edge.\nOther references are:\nGoogle TPU3 \u770b\u70b9\nGoogle TPU \u63ed\u5bc6\nGoogle\u7684\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u5668\u4e13\u5229\n\u8109\u52a8\u9635\u5217 - \u56e0Google TPU\u83b7\u5f97\u65b0\u751f\nShould We All Embrace Systolic Arrays?\nAmazon may be developing AI chips for Alexa\nThe Information has a report this morning that Amazon is working on building AI chips for the Echo, which would allow Alexa to more quickly parse information and get those answers.\nAWS launches Trainium, its new custom ML training chip\nAAt its annual re:Invent developer conference, AWS today announced the launch of AWS Trainium, the company\u2019s next-gen custom chip dedicated to training machine learning models. The company promises that it can offer higher performance than any of its competitors in the cloud, with support for TensorFlow, PyTorch and MXNet.\nAWS Inferentia. High performance machine learning inference chip, custom designed by AWS.\nAWS Inferentia provides high throughput, low latency inference performance at an extremely low cost. Each chip provides hundreds of TOPS (tera operations per second) of inference throughput to allow complex models to make fast predictions. For even more performance, multiple AWS Inferentia chips can be used together to drive thousands of TOPS of throughput. AWS Inferentia will be available for use with Amazon SageMaker, Amazon EC2, and Amazon Elastic Inference.\nAWS FPGA instance\nAmazon EC2 F1 is a compute instance with field programmable gate arrays (FPGAs) that you can program to create custom hardware accelerations for your application. F1 instances are easy to program and come with everything you need to develop, simulate, debug, and compile your hardware acceleration code, including an FPGA Developer AMI and Hardware Developer Kit (HDK). Once your FPGA design is complete, you can register it as an Amazon FPGA Image (AFI), and deploy it to your F1 instance in just a few clicks. You can reuse your AFIs as many times, and across as many F1 instances as you like.\nInside the Microsoft FPGA-based configurable cloud is also a good reference if want to know Microsoft's vision on FPGA in cloud.\nThis article \"\u667a\u6167\u4e91\u4e2d\u7684FPGA\" gives and overview about FPGA used in AI aceleration in the cloud.\nDrilling Into Microsoft\u2019s BrainWave Soft Deep Learning Chip shows more details based on Microsoft's presentation on Hot Chips 2017.\nReal-time AI: Microsoft announces preview of Project Brainwave\nAt Microsoft\u2019s Build developers conference in Seattle this week, the company is announcing a preview of Project Brainwave integrated with Azure Machine Learning, which the company says will make Azure the most efficient cloud computing platform for AI.\nMicrosoft is hiring engineers to work on A.I. chip design for its cloud\nMicrosoft is following Google's lead in designing a computer processor for artificial intelligence, according to recent job postings.\nA12 Bionic The smartest, most powerful chip in a smartphone.\nA whole new level of intelligence. The A12 Bionic, with our next-generation Neural Engine, delivers incredible performance. It uses real-time machine learning to transform the way you experience photos, gaming, augmented reality, and more.\nApple unveiled the new processor powering the new iPhone 8 and iPhone X - the A11 Bionic. The A11 also includes dedicated neural network hardware that Apple calls a \"neural engine\", which can perform up to 600 billion operations per second.\nCore ML is Apple's current sulotion for machine learning application.\nAlibaba\u2019s New AI Chip Can Process Nearly 80K Images Per Second\nAt the Alibaba Cloud (Aliyun) Apsara Conference 2019, Pingtouge unveiled its first AI dedicated processor for cloud-based large-scale AI inferencing. The Hanguang 800 is the first semiconductor product in Alibaba\u2019s 20-year history.\nTencent cloud introduces FPGA instance(Beta), with three different specifications based on Xilinx Kintex UltraScale KU115 FPGA. They will provide more choices equiped with Inter FPGA in the future.\nBaidu A.I. chip unit valued at $2 billion after funding and may become standalone business\nBaidu has raised money for its artificial intelligence (AI) semiconductor business at a valuation of $2 billion.The funding round was led by CPE, a Chinese asset management and private equity firm.Now that the Kunlun chip business has raised money, it could pave the way for the unit to be spun-off, but no final decision has been made.\nChinese tech giant Huawei unveils A.I. chips, taking aim at giants like Qualcomm and Nvidia\nHuawei unveils two new artificial intelligence (AI) chips called the Ascend 910 and Ascend 310. The two chips are aimed at uses in data centers and internet-connected consumer devices, Rotating Chairman Eric Xu says at the Huawei Connect conference in Shanghai. The move pits the Chinese tech giant against major chipmakers including Qualcomm and Nvidia.\nFPGA Accelerated Cloud Server, high performance FPGA instance is open for beta test.\nFPGA\u4e91\u670d\u52a1\u5668\u63d0\u4f9bCPU\u548cFPGA\u76f4\u63a5\u7684\u9ad8\u8fbe100Gbps PCIe\u4e92\u8fde\u901a\u9053\uff0c\u6bcf\u8282\u70b9\u63d0\u4f9b8\u7247Xilinx VU9P FPGA\uff0c\u540c\u65f6\u63d0\u4f9bFPGA\u4e4b\u95f4\u9ad8\u8fbe200Gbps\u7684Mesh\u5149\u4e92\u8fde\u4e13\u7528\u901a\u9053\uff0c\u8ba9\u60a8\u7684\u5e94\u7528\u52a0\u901f\u9700\u6c42\u4e0d\u518d\u53d7\u5230\u786c\u4ef6\u9650\u5236\u3002\nThis DLU that Fujitsu is creating is done from scratch, and it is not based on either the Sparc or ARM instruction set and, in fact, it has its own instruction set and a new data format specifically for deep learning, which were created from scratch. Japanese computing giant Fujitsu. Which knows a thing or two about making a very efficient and highly scalable system for HPC workloads, as evidenced by the K supercomputer, does not believe that the HPC and AI architectures will converge. Rather, the company is banking on the fact that these architectures will diverge and will require very specialized functions.\nNokia has developed the ReefShark chipsets for its 5G network solutions. AI is implemented in the ReefShark design for radio and embedded in the baseband to use augmented deep learning to trigger smart, rapid actions by the autonomous, cognitive network, enhancing network optimization and increasing business opportunities.\nFacebook Is Forming a Team to Design Its Own Chips\nFacebook Inc. is building a team to design its own semiconductors, adding to a trend among technology companies to supply themselves and lower their dependence on chipmakers such as Intel Corp. and Qualcomm Inc., according to job listings and people familiar with the matter.\nHPE DEVELOPING ITS OWN LOW POWER \u201cNEURAL NETWORK\u201d CHIPS\nIn the context of a broader discussion about the company\u2019s Extreme Edge program focused on space-bound systems, HPE\u2019s Dr. Tom Bradicich, VP and GM of Servers, Converged Edge, and IoT systems, described a future chip that would be ideally suited for high performance computing under intense power and physical space limitations characteristic of space missions. To be more clear, he told us as much as he could\u2014very little is known about the architecture, but there was some key elements he described.\nTesla Dojo \u2013 Unique Packaging and Chip Design Allow An Order Magnitude Advantage Over Competing AI Hardware\nTesla hosted their AI Day and revealed the innerworkings of their software and hardware infrastructure. Part of this reveal was the previously teased Dojo AI training chip. Tesla claims their D1 Dojo chip has a GPU level compute, CPU level flexibility, with networking switch IO.\nTesla\u2019s new AI chip isn\u2019t a silver bullet for self-driving cars\nProcessing power is important, but building chips could be an expensive distraction for Tesla\nLG TO ACCELERATE DEVELOPMENT OF ARTIFICIAL INTELLIGENCE WITH OWN AI CHIP\nNew AI Processor with LG Neural Engine Designed for Use in Various Products Including Robot Vacuum Cleaners, Washing Machines and Refrigerators\nSKT Unveils its AI Chip and New Plans for AI Semiconductor Business\nNov. 25, 2020 \u2014 SK Telecom (SKT) today unveiled its self-developed artificial intelligence (AI) chip named \u2018SAPEON X220\u2019 and shared its AI semiconductor business vision.\nIII. Traditional IP Vendors\nDynamIQ is embedded IP giant's answer to AI age. It may not be a revolutionary design but is important for sure.\nARM also provide a open source Compute Library contains a comprehensive collection of software functions implemented for the Arm Cortex-A family of CPU processors and the Arm Mali family of GPUs.\nArm Machine Learning Processor\nSpecifically designed for inference at the edge, the ML processor gives an industry-leading performance of 4.6 TOPs, with a stunning efficiency of 3 TOPs/W for mobile devices and smart IP cameras.\nARM Details \"Project Trillium\" Machine Learning Processor Architecture\nArm details more of the architecture of what Arm now seems to more consistently call their \u201cmachine learning processor\u201d or MLP from here on now. The MLP IP started off a blank sheet in terms of architecture implementation and the team consists of engineers pulled off from the CPU and GPU teams.\nDesignWare EV6x Embedded Vision Processors\n\u5904\u7406\u5668IP\u5382\u5546\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6848 - Synopsys\nPowerVR Series2NX Neural Network Accelerator\nImagination launches multi-core IMG Series4 NNA \u2013 the ultimate AI accelerator delivering industry-disruptive performance for ADAS and autonomous driving\nthe company is announcing the first products in the 2NX NNA family: the higher-performance AX2185 and lower-cost AX2145.\nCEVA-XM6 Fifth-generation computer vision and deep learning embedded platform\n\u5904\u7406\u5668IP\u5382\u5546\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6848 - CEVA\nCEVA Announces NeuPro Neural Network IP\nAhead of CES CEVA announced a new specialised neural network accelerator IP called NeuPro.\nTensilica Vision DSPs for Imaging, Computer Vision, and Neural Networks\nVeriSilicon\u2019s Vivante VIP8000 Neural Network Processor IP Delivers Over 3 Tera MACs Per Second\n\u795e\u7ecf\u7f51\u7edcDSP\u6838\u7684\u4e00\u684c\u9ebb\u5c06\u7ec8\u4e8e\u51d1\u9f50\u4e86\nThe v-MP6000UDX processor from Videantis is a scalable processor family that has been designed to run high-performance deep learning, computer vision, imaging and video coding applications in a low power footprint.\nIV. Startups in China\nChinese AI Chip Maker Cambricon Unveils New Cloud-Based Smart Chip\nChinese artificial intelligence chip maker Cambricon Technologies Corp Ltd has unveiled two new products, a cloud-based smart chip Cambricon MLU100 and a new version of its AI processor IP product Cambricon 1M, at a launching event in Shanghai on May 3rd.\nCambricon release new product page, including IP, Chip and Software tools\nAI Chip Explosion: Cambricon\u2019s Billion-Device Ambition\nOn November 6 in Beijing, China\u2019s rising semiconductor company Cambricon released the Cambrian-1H8 for low power consumption computer vision application, the higher-end Cambrian-1H16 for more general purpose application, the Cambrian-1M for autonomous driving applications with yet-to-be-disclosed release date, and an AI system software named Cambrian NeuWare.\nChinese AI chip maker Horizon Robotics raises $600 million from SK Hynix, others\nChinese chip maker Horizon Robotics said on Wednesday it had raised $600 million in its latest funding round, bringing its valuation to $3 billion, amid a push from Chinese companies and the government to boost the semiconductor industry.\nDec. 20, Horizon Robotics annouced two chip products, \"Journey\" for ADAS and \"Sunrise\" for Smart Cameras.\nBitcoin Mining Giant Bitmain is developing processors for both training and inference tasks.\nBitmain\u2019s newest product, the Sophon, may or may not take over deep learning. But by giving it such a name Zhan and his Bitmain co-founder, Jihan Wu, have signaled to the world their intentions. The Sophon unit will include Bitmain\u2019s first piece of bespoke silicon for a revolutionary AI technology. If things go to plan, thousands of Bitmain Sophon units soon could be training neural networks in vast data centers around the world.\nOn Nov.8, Bitmain announced its Sophon BM1869 Tensor Computing Processor, Deep Learning Accelerating Card SC1 and IVS server SS1.\nChipintelli's first IC, CI1006, is designed for automatic speech recognition application.\nSequoia, Hillhouse, Yitu Technology Join $68M Series A Round In Chinese AI Chip Maker ThinkForce\nUnisound raises US$100 million to fund AI, chip development\nChina\u2019s AISpeech Raises $76M on Advanced Speech Tech; Eyes AI Chips\nChinese AI startup Rokid will mass produce their own custom AI chip for voice recognition\nThe world leading computer vision processing IC and system company, NextVPU, today unveiled AI vision processing IC N171. N171 is the flagship IC of NextVPU\u2019s N1 series computer vison chips. As a VPU, N171 pushes the Edge AI computing limit further from many aspects. With powerful computing engines embedded, N171 has unprecedent geometry calculation and deep neural network processing capabilities, and can be widely used in surveillance, robots, drones, UGV, smart home, ADAS applications, etc.\nCanaan's Kendryte is a series of AI chips which focuses on IoT.\nEnflame Tech is a startup company based in Shanghai, China. It was established in March 2018 with two R&D centers in Shanghai and Beijing. Enflame is developing the deep learning accelerator SoCs and software stack, targeting AI training platform solutions for the Cloud service provider and the data centers.\nEnflame Technology Announces CloudBlazer with DTU Chip on GLOBALFOUNDRIES 12LP FinFET Platform for Data Center Training\nSHANGHAI, China, Dec. 12, 2019 \u2013 In conjunction with the launch of Enflame\u2019s CloudBlazer T10, Enflame Technology and GLOBALFOUNDRIES (GF) today announced a new high-performing deep learning accelerator solution for data center training. Designed to accelerate deep learning deployment, the accelerator\u2019s core Deep Thinking Unit (DTU) is based on GF\u2019s 12LP FinFET platform with 2.5D packaging to deliver fast, power-efficient data processing for cloud-based AI training platforms.\nChinese tech startups Cloudpick, EEasy Tech snag Intel Capital funding\nEEasy Technology Co. Ltd is an AI system-on-chip (SoC) design house and total solution provider. Its offerings include AI acceleration; image and graphic processing; video encoding and decoding; and mixed-signal ULSI design capabilities.\nFounded in Oct. 2017, WITINMEM focuses on Low cost, low power AI chips and system solutions based on processing-in-memory technology in NOR Flash memory.\nQingwei Intelligent Technology (Tsing Micro) is AI chip company spin-off from Tsinghua University.\nBlack Sesame Technologies Nearly Completes 100 Million Series B Financing Round\nBlack Sesame Technologies (\u9ed1\u829d\u9ebb\u667a\u80fd\u79d1\u6280) has nearly completed its 100 million Series B Financing round which will be used to expand cooperation with OEMs, accelerate mass production, reference design development of autopilot controllers, and software-vehicle integration.\nV. Startups Worldwide\nCerebras Completes Series F Funding, Another $250M for $4B Valuation\nThe new Series F funding round nets the company another $250m in capital, bringing the total raised through venture capital up to $720 million.\nCerebras Unveils Wafer Scale Engine Two (WSE2): 2.6 Trillion Transistors, 100% Yield\nTwo years ago Cerebras unveiled a revolution in silicon design: a processor as big as your head, using as much area on a 12-inch wafer as a rectangular design would allow, built on 16nm, focused on both AI as well as HPC workloads. Today the company is launching its second generation product, built on TSMC 7nm, with more than double the cores and more than double of everything.\nThe Cerebras CS-1 computes deep learning AI problems by being bigger, bigger, and bigger than any other chip\nToday, the company announced the launch of its end-user compute product, the Cerebras CS-1, and also announced its first customer of Argonne National Laboratory.\nWave Computing and MIPS Wave Goodbye\nWord on the virtual street is that Wave Computing is closing down. The company has reportedly let all employees go and filed for Chapter 11. As one of the many promising new companies in the field of AI, Wave Computing was founded in 2008 with the mission \u201cto revolutionize deep learning with real-time AI solutions that scale from the edge to the datacenter.\u201d\nMK2 PERFORMANCE BENCHMARKS\nGraphcore, the AI chipmaker, raises another $150M at a $1.95B valuation\nGraphcore, the Bristol-based startup that designs processors specifically for artificial intelligence applications, announced it has raised another $150 million in funding for R&D and to continue bringing on new customers. It\u2019s valuation is now $1.95 billion.\nMicrosoft and Graphcore Colleborate to Accelerate Artificial Intelligence\n\u89e3\u5bc6\u53c8\u4e00\u4e2axPU\uff1aGraphcore\u7684IPU give some analysis on its IPU architecture.\nGraphcore AI\u82af\u7247\uff1a\u66f4\u591a\u5206\u6790 More analysis.\n\u6df1\u5ea6\u5256\u6790AI\u82af\u7247\u521d\u521b\u516c\u53f8Graphcore\u7684IPU In-depth analysis after more information was disclosed.\nThe 2,048-core PEZY-SC2 sets a Green500 record\nThe SC2 is a second-generation chip featuring twice as many cores \u2013 i.e., 2,048 cores with 8-way SMT for a total of 16,384 threads. Operating at 1 GHz with 4 FLOPS per cycle per core as with the SC, the SC2 has a peak performance of 8.192 TFLOPS (single-precision). Both prior chips were manufactured on TSMC\u2019s 28HPC+, however in order to enable the considerably higher core count within reasonable power consumption, PEZY decided to skip a generation and go directly to TSMC\u2019s 16FF+ Technology.\nTenstorrent Raises over $200 million at $1 billion Valuation to Create Programmable, High Performance AI Computers\nTORONTO, May 20, 2021 /PRNewswire/ - Tenstorrent, a hardware start-up developing next generation computers, announced today that it has raised over $200 million in a recent funding round that values the company at $1 billion. The round was led by Fidelity Management and Research Company and includes additional investments from Eclipse Ventures, Epic CG and Moore Capital.\nJim Keller Becomes CTO at Tenstorrent: \"The Most Promising Architecture Out There\nToday Tenstorrent is announcing that Jim Keller, compute architect extraordinaire, has joined the company as its Chief Technology Officer, President, and joins the company board.\nTenstorrent Launches AI Chip With Conditional Execution\nNow Tenstorrent\u2019s claim to future fame and potentially the crown is all about reducing the computation required to get to a good answer, instead of throwing massive amounts of brute-force compute at the problem. The technique is called fine-grained conditional computation, and it is just the beginning of an optimization roadmap Tenstorrent CEO, Ljubisa Bajic, has up his sleeves.\nTenstorrent is a small Canadian start-up in Toronto claiming an order of magnitude improvement in efficiency for deep learning, like most. No real public details but they're are on the Cognitive 300 list.\nBlaize emerges from stealth with $87 million for its custom-designed AI chips\nThe fierce competition isn\u2019t deterring Blaize (formerly Thinci), which hopes to stand out from the crowd with a novel graph streaming architecture. The nine-year-old startup\u2019s claimed system-on-chip performance is impressive, to be fair, which is likely why it\u2019s raised nearly $100 million from investors including automotive component maker Denso.\nFounded in 2014, Newark, California startup Koniku has taken in $1.65 million in funding so far to become \u201cthe world\u2019s first neurocomputation company\u201c. The idea is that since the brain is the most powerful computer ever devised, why not reverse engineer it? Simple, right? Koniku is actually integrating biological neurons onto chips and has made enough progress that they claim to have AstraZeneca as a customer. Boeing has also signed on with a letter of intent to use the technology in chemical-detecting drones.\nAdapteva has taken in $5.1 million in funding from investors that include mobile giant Ericsson. The paper \"Epiphany-V: A 1024 processor 64-bit RISC System-On-Chip\" describes the design of Adapteva's 1024-core processor chip in 16nm FinFet technology.\nKnowm is actually setup as a .ORG but they appear to be pursuing a for-profit enterprise. The New Mexcio startup has taken in an undisclosed amount of seed funding so far to develop a new computational framework called AHaH Computing (Anti-Hebbian and Hebbian). The gory details can be found in this publication, but the short story is that this technology aims to reduce the size and power consumption of intelligent machine learning applications by up to 9 orders of magnitude.\nThe Era of Analog Compute has Arrived!\nResNet-50 in our prototype analog AI processor. Production release will support 900-1000 fps and INT8 accuracy at 3W.\nA battery powered neural chip from Mythic with 50x lower power.\nFounded in 2012, Texas-based startup Mythic (formerly known as Isocline) has taken in $9.5 million in funding with Draper Fisher Jurvetson as the lead investor. Prior to receiving any funding, the startup has taken in $2.5 million in grants. Mythic is developing an AI chip that \u201cputs desktop GPU compute capabilities and deep neural networks onto a button-sized chip \u2013 with 50x higher battery life and far more data processing capabilities than competitors\u201c. Essentially, that means you can give voice control and computer vision to any device locally without needing cloud connectivity.\nKalray Releases the Kalray Neural Network 3.0\nKalray (Euronext Growth Paris \u2013 ALKAL), a pioneer in processors for new intelligent systems, has announced the launch of the Kalray Neural Network 3.0 (KaNN), a platform for Artificial Intelligence application development. KaNN allows developers to seamlessly port their AI-based algorithms from well-known machine learning frameworks including Caffe, Torch and TensorFlow onto Kalray\u2019s Massively Parallel Processor Array (MPPA) intelligent processor.\nBrainChip Showcases Vision and Learning Capabilities of its Akida Neural Processing IP and Device at tinyML Summit 2020\nBrainChip Holdings Ltd. (ASX: BRN), a leading provider of ultra-low power, high-performance edge AI technology, today announced that it will present its revolutionary new breed of neuromorphic processing IP and Device in two sessions at the tinyML Summit at the Samsung Strategy & Innovation Center in San Jose, California February 12-13.\nBrainChip Inc (CA. USA) was the first company to offer a Spiking Neural processor, which was patented in 2008 (patent US 8,250,011). The current device, called the BrainChip Accelerator is a chip intended for rapid learning. It is offered as part of the BrainChip Studio software. BrainChip is a publicly listed company as part of BrainChip Holdings Ltd.\naiWare3 Hardware IP Helps Drive Autonomous Vehicles To Production.\nLatest technology enables scalable, low-power automotive inference engines with >50 TMAC/s NN processing power.\nMOUNTAIN VIEW, Calif., October 30, 2018 \u2013 AImotive\u2122, the global provider of full stack, vision-first self-driving technology, today announced the release of aiWare3\u2122, the company\u2019s 3rd generation, scalable, low-power, hardware Neural Network (NN) acceleration core.\nLeepmind is carrying out research on original chip architectures in order to implement Neural Networks on a circuit enabling low power DeepLearning\nA crowdfunding effort for Snickerdoodle raised $224,876 and they\u2019re currenty shipping. If you pre-order one, they\u2019ll deliver it by summer. The palm-sized unit uses the Zynq \u201cSystem on Chip\u201d (SoC) from Xilinix.\nNovuMind combines big data, high-performance, and heterogeneous computing to change the Internet of Things (IoT) into the Intelligent Internet of Things (I\u00b2oT). Here is a paper from Moor Insights & Strategy, a global technology analyst and research firm. about NovuMind\nReduced Energy Microsystems are developing lower power asynchronous chips to suit CNN inference. REM was Y Combinator's first ASIC venture according to TechCrunch.\nTeraDeep is building an AI Appliance using its deep learning FPGA\u2019s acceleration. The company claims image recognition performance on AlexNet to achieve a 2X performance advantage compared with large GPUs, while consuming 5X less power. When compared to Intel\u2019s Xeon processor, TeraDeep\u2019s Accel technology delivers 10X the performance while consuming 5X less power.\nAccording to this article, \"Deep Vision announces its low-latency AI processor for the edge\"\nDeep Vision, a new AI startup that is building an AI inferencing chip for edge computing solutions, is coming out of stealth today. The six-year-old company\u2019s new ARA-1 processors promise to strike the right balance between low latency, energy efficiency and compute power for use in anything from sensors to cameras and full-fledged edge servers.\nGroq\nAI Chip Startup Groq, Founded By Ex-Googlers, Raises $300 Million To Power Autonomous Vehicles And Data Centers\nJonathan Ross left Google to launch next-generation semiconductor startup Groq in 2016. Today, the Mountain View, California-based firm said that it had raised $300 million led by Tiger Global Management and billionaire investor Dan Sundheim\u2019s D1 Capital as it officially launched into public view.\nKneron to Accelerate Edge AI Development with more than 10 Million USD Series A Financing\nAccording to this article, \"Gyrfalcon offers Automotive AI Chip Technology\"\nGyrfalcon Technology Inc. (GTI), has been promoting matrix-based application specific chips for all forms of AI since offering their production versions of AI accelerator chips in September 2017. Through the licensing of its proprietary technology, the company is confident it can help automakers bring highly competitive AI chips to production for use in vehicles within 18 months, along with significant gains in AI performance, improvements in power dissipation and cost advantages.\nEsperanto Unveils ML Chip with Nearly 1,100 RISC-V Cores\nAt the RISC-V Summit today, Art Swift, CEO of Esperanto Technologies, announced a new, RISC-V based chip aimed at machine learning and containing nearly 1,100 low-power cores based on the open-source RISC-V architecture.\nAccording to this article, \"Esperanto exits stealth mode, aims at AI with a 4,096-core 7nm RISC-V monster\"\nAlthough Esperanto will be licensing the cores they have been designing, they do plan on producing their own products. The first product they want to deliver is the highest TeraFLOP per Watt machine learning computing system. Ditzel noted that the overall design is scalable in both performance and power. The chips will be designed in 7nm and will feature a heterogeneous multi-core architecture.\nSambaNova raises $676M at a $5.1B valuation to double down on cloud-based AI software for enterprises\nSambaNova \u2014 a startup building AI hardware and integrated systems that run on it that only officially came out of three years in stealth last December \u2014 is announcing a huge round of funding today to take its business out into the world. The company has closed on $676 million in financing, a Series D that co-founder and CEO Rodrigo Liang has confirmed values the company at $5.1 billion.\nIntroducing SambaNova Systems DataScale: A New Era of Computing\nSambaNova has been working closely with many organizations the past few months and has established a new state of the art in NLP. This advancement in NLP deep learning is illustrated by a GPU-crushing, world record performance result achieved on SambaNova Systems\u2019 Dataflow-optimized system.\nA New State of the Art in NLP: Beyond GPUs\nSambaNova has been working closely with many organizations the past few months and has established a new state of the art in NLP. This advancement in NLP deep learning is illustrated by a GPU-crushing, world record performance result achieved on SambaNova Systems\u2019 Dataflow-optimized system.\nGreenWaves Technologies develops IoT Application Processors based on Open Source IP blocks enabling content understanding applications on embedded, battery-operated devices with unmatched energy efficiency. Our first product is GAP8. GAP8 provides an ultra-low power computing solution for edge devices carrying out inference from multiple, content rich sources such as images, sounds and motions. GAP8 can be used in a variety of different applications and industries.\nLight-Powered Computers Brighten AI\u2019s Future\nOptical computers may have finally found a use\u2014improving artificial intelligence\nLightmatter aims to reinvent AI-specific chips with photonic computing and $11M in funding\nIt takes an immense amount of processing power to create and operate the \u201cAI\u201d features we all use so often, from playlist generation to voice recognition. Lightmatter is a startup that is looking to change the way all that computation is done \u2014 and not in a small way. The company makes photonic chips that essentially perform calculations at the speed of light, leaving transistors in the dust. It just closed an $11 million Series A.\nFirst Low-Power AI-Inference Accelerator Vision Processing Unit From Think Silicon To Debut at Embedded World 2018\nTORONTO, Canada/NUREMBERG, Germany \u2013 FEB 21st, 2018 \u2013 Think Silicon\u00ae, a leader in developing ultra-low power graphics IP technology, will demonstrate a prototype of NEMA\u00ae xNN, the world\u2019s first low-power \u2018Inference Accelerator\u2019 Vision Processing Unit for artificial intelligence, convolutional neural networks at Embedded World 2018.\nStartup Puts AI Core in SSDs\nStartup InnoGrit debuted a set of three controllers for solid-state drives (SSDs), including one for data centers that embeds a neural-network accelerator. They enter a crowded market with claims of power and performance advantages over rivals.\nInnogrit Technologies Incorporated is a startup seting out to solve the data storage and data transport problem in artificial intelligence and other big data applications through innovative integrated circuit (IC) and system solutions: Extracts intelligence from correlated data and unlocks the value in artificial intelligence systems; Reduces redundancy in big data and improves system efficiency for artificial intelligence applications; Brings networking capability to storage devices and offers unparalleled performance at large scales; Performs data computation within storage devices and boosts performance of large data centers.\nKortiq is a startup providing \"FPGA based Neural Network Engine IP Core and The scalable Solution for Low Cost Edge Machine Learning Inference for Embedded Vision\". Recently, they revealed some comparison data. You can also find the Preliminary Datasheet of their AIScaleCDP2 IP Core on their website.\nHailo unveils Hailo-8, an edge chip custom-designed for AI workloads\n......Hailo-8 is capable of 26 tera operations per second (TOPs) ...... In one preliminary test at an image resolution of 224 x 224, the Hailo-8 processed 672 frames per second compared with the Xavier AGX\u2019s 656 frames and sucked down only 1.67 watts (equating to 2.8 TOPs per watt) versus the Nvidia chip\u2019s 32 watts (0.14 TOPs per watt)......\nTACHYUM STARTS FROM SCRATCH TO ETCH A UNIVERSAL PROCESSOR\nOnly last week, we did a thought experiment about how we should have streamlined chiplets for very specific purposes, woven together inside of a single package or across sockets and nodes, co-designed to specifically run very precise workflows because any general purpose processor \u2013 mixing elements of CPUs, GPUs, TPUs, NNPs, and FPGAs \u2013 would be suboptimal on all fronts except volume economics. We think that this extreme co-design for datacenter compute is the way the world will ultimately go, and we are just getting the chiplet architectures and interconnects together to make this happen. Radoslav Danilak, co-founder and chief executive officer of processor upstart Tachyum, is having absolutely none of that. And in fact, the Prodigy \u201cuniversal processor\u201d that Tachyum has designed is going in exactly in the opposite direction.\nTachyum Running Apache is a Key Milestone for Prodigy Universal Processor Software Stack\nSemiconductor startup Tachyum Inc. today announced that it has completed another critical stage in software development by successfully achieving an Apache web server port to Prodigy Universal Processor Instruction Set Architecture (ISA). This latest milestone by Tachyum\u2019s software team brings the company\u2019s Prodigy Universal Processor one step closer to being customer-ready in anticipation of its commercial launch in 2021.\nStartup AI Chip Passes Road Test\nAlphaICs designed an instruction set architecture (ISA) optimized for deep-learning, reinforcement-learning, and other machine-learning tasks. The startup aims to produce a family of chips with 16 to 256 cores, roughly spanning 2 W to 200 W.\nSyntiant: Analog Deep Learning Chips\nStartup Syntiant Corp. is an Irvine, Calif. semiconductor company led by former top Broadcom engineers with experience in both innovative design and in producing chips designed to be produced in the billions, according to company CEO Kurt Busch.\nBaidu Backs Neuromorphic IC Developer\nMUNICH \u2014 Swiss startup aiCTX has closed a $1.5 million pre-A funding round from Baidu Ventures to develop commercial applications for its low-power neuromorphic computing and processor designs and enable what it calls \u201cneuromorphic intelligence.\u201d It is targeting low-power edge-computing embedded sensory processing systems.\nFlex Logix has two paths to making a lot of money challenging Nvidia in AI\nThe programmable chip company scores $55 million in venture backing, bringing its total haul to $82 million\nPreferred Networks develops a custom deep learning processor MN-Core for use in MN-3, a new large-scale cluster, in spring 2020\nDec. 12, 2018, Tokyo Japan \u2013 Preferred Networks, Inc. (\u201cPFN\u201d, Head Office: Tokyo, President & CEO: Toru Nishikawa) announces that it is developing MN-Core (TM), a processor dedicated to deep learning and will exhibit this independently developed hardware for deep learning, including the MN-Core chip, board, and server, at the SEMICON Japan 2018, held at Tokyo Big Site.\nAI Startup Cornami reveals details of neural net chip\nStealth startup Cornami on Thursday revealed some details of its novel approach to chip design to run neural networks. CTO Paul Masters says the chip will finally realize the best aspects of a technology first seen in the 1970s.\nAI chip startup offers new edge computing solution\nAnaflash Inc. (San Jose, CA) is a startup company that has developed a test chip to demonstrate analog neurocomputing taking place inside logic-compatible embedded flash memory.\nOptalysys launches world\u2019s first commercial optical processing system, the FT:X 2000\nOptalysys develops Optical Co-processing technology which enables new levels of processing capability delivered with a vastly reduced energy consumption compared with conventional computers. Its first coprocessor is based on an established diffractive optical approach that uses the photons of low-power laser light instead of conventional electricity and its electrons. This inherently parallel technology is highly scalable and is the new paradigm of computing.\nLow-Power AI Startup Eta Compute Delivers First Commercial Chips\nThe firm pivoted away from riskier spiking neural networks using a new power management scheme\nEta Compute Debuts Spiking Neural Network Chip for Edge AI\nChip can learn on its own and inference at 100-microwatt scale, says company at Arm TechCon.\nAchronix Rolls 7-nm FPGAs for AI\nAchronix is back in the game of providing full-fledged FPGAs with a new high-end 7-nm family, joining the Gold Rush of silicon to accelerate deep learning. It aims to leverage novel design of its AI block, a new on-chip network, and use of GDDR6 memory to provide similar performance at a lower cost than larger rivals Intel and Xilinx.\nStartup Runs AI in Novel SRAM\nAreanna is the latest example of an explosion of new architectures spawned by the rise of deep learning. The debut of a whole new approach to computing has fired imaginations of engineers around the industry hoping to be the next Hewlett and Packard.\nNeuroBlade Preps Inference Chip\nAdd NeuroBlade to the dozens of startups working on AI silicon. The Israeli company just closed a $23 million Series A, led by the founder of Check Point Software and with participation from Intel Capital.\nBill Gates just backed a chip startup that uses light to turbocharge AI\nLuminous Computing has developed an optical microchip that runs AI models much faster than other semiconductors while using less power.\nChip startup Efinix hopes to bootstrap AI efforts in IoT\nSix-year-old startup Efinix has created an intriguing twist on the FPGA technology dominated by Intel and Xiliinx; the company hopes its energy-efficient chips will bootstrap the market for embedded AI in the Internet of Things.\nAIStorm raises $13.2 million for AI edge computing chips\nDavid Schie, a former senior executive at Maxim, Micrel, and Semtech, thinks both markets are ripe for disruption. He \u2014 along with WSI, Toshiba, and Arm veterans Robert Barker, Andreas Sibrai, and Cesar Matias \u2014 in 2011 cofounded AIStorm, a San Jose-based artificial intelligence (AI) startup that develops chipsets that can directly process data from wearables, handsets, automotive devices, smart speakers, and other internet of things (IoT) devices.\nSiMa.ai Raises $30 Million in Series A Investment Round Led by Dell Technologies Capital\nSAN JOSE, Calif.--(BUSINESS WIRE)--SiMa.ai, the company enabling high performance machine learning to go green, today announced its Machine Learning SoC (MLSoC) platform \u2013 the industry\u2019s first unified solution to support traditional compute with high performance, lowest power, safe and secure machine learning inference. Delivering the highest frames per second per watt, SiMa.ai\u2019s MLSoC is the first machine learning platform to break the 1000 FPS/W barrier for ResNet-501. In customer engagements, the company has demonstrated 10-30x improvement in FPS/W through its automated software flow across a wide range of embedded edge applications, over today\u2019s competing solutions. The platform will provide machine learning solutions that range from 50 TOPs@5W to 200 TOPs@20W, delivering an industry first of 10 TOPs/W for high performance inference.\nSiMa.ai\u2122 Introduces MLSoC\u2122 \u2013 First Machine Learning Platform to Break 1000 FPS/W Barrier with 10-30x Improvement over Alternative Solutions\nSiMa.ai, the company enabling high performance machine learning to go green, today announced its Machine Learning SoC (MLSoC) platform \u2013 the industry\u2019s first unified solution to support traditional compute with high performance, lowest power, safe and secure machine learning inference. Delivering the highest frames per second per watt, SiMa.ai\u2019s MLSoC is the first machine learning platform to break the 1000 FPS/W barrier for ResNet-501. In customer engagements, the company has demonstrated 10-30x improvement in FPS/W through its automated software flow across a wide range of embedded edge applications, over today\u2019s competing solutions. The platform will provide machine learning solutions that range from 50 TOPs@5W to 200 TOPs@20W, delivering an industry first of 10 TOPs/W for high performance inference.\nUntether AI nabs $125M for AI acceleration chips\nUntether AI, a startup developing custom-built chips for AI inferencing workloads, today announced it has raised $125 million from Tracker Capital Management and Intel Capital. The round, which was oversubscribed and included participation from Canada Pension Plan Investment Board and Radical Ventures, will be used to support customer expansion.\nGrAI Matter Labs Reveals NeuronFlow Technology and Announces GrAIFlow SDK\nGrAI Matter Labs (aka GML), a neuromorphic computing pioneer today revealed NeuronFlow \u2013 a new programmable processor technology \u2013 and announced an early access program to its GrAIFlow software development kit.\nRain Neuromorphics on Crunchbase\nWe build artificial intelligence processors, inspired by the brain. Our mission is to enable brain-scale intelligence.\nApplied Brain Research on Crunchbase\nABR makes the world's most advanced neuromoprhic compiler, runtime and libraries for the emerging space of neuromorphic computing.\nXMOS adapts Xcore into AIoT \u2018crossover processor\u2019\nEE Times exclusive! The new chip targets AI-powered voice interfaces in IoT devices \u2014 \u201cthe most important AI workload at the endpoint.\u201d\nXMOS unveils Xcore.ai, a powerful chip designed for AI processing at the edge\nThe latest xcore.ai is a crossover chip designed to deliver high-performance AI, digital signal processing, control, and input/output in a single device with prices from $1.\nWe design and produce AI processors and the software to run them in data centers. Our unique approach optimizes for inference with the focus on performance, power efficiency, and ease of use; and at the same time our approach enables cost-effective training.\nWe build high-performance AI inference coprocessors that can be seamlessly integrated into various computing platforms including data centers, servers, desktops, automobiles and robots.\nCorerain provides ultra-high performance AI acceleration chips and the world's first streaming engine-based AI development platform.\nPerceive emerges from stealth with Ergo edge AI chip\nOn-device computing solutions startup Perceive emerged from stealth today with its first product: the Ergo edge processor for AI inference. CEO Steve Teig claims the chip, which is designed for consumer devices like security cameras, connected appliances, and mobile phones, delivers \u201cbreakthrough\u201d accuracy and performance in its class.\nSimpleMachines, Inc. Debuts First-of-its-Kind High Performance Chip\nAs traditional chip makers struggle to embrace the challenges presented by the rapidly evolving AI software landscape, a San Jose startup has announced it has working silicon and a whole new future-proof chip paradigm to address these issues.\nThe SimpleMachines, Inc. (SMI) team \u2013 which includes leading research scientists and industry heavyweights formerly of Qualcomm, Intel and Sun Microsystems \u2013 has created a first-of-its-kind easily programmable, high-performance chip that will accelerate a wide variety of AI and machine-learning applications.\nNeuReality unveiled NR1-P, A novel AI-centric inference platform\nNeuReality has unveiled NR1-P, a novel AI-centric inference platform. NeuReality has already started demonstrating its AI-centric platform to customers and partners. NeuReality has redefined today\u2019s outdated AI system architecture by developing an AI-centric inference platform based on a new type of System-on-Chip (SoC).\nNeuReality raises $8M for its novel AI inferencing platform\nNeuReality, an Israeli AI hardware startup that is working on a novel approach to improving AI inferencing platforms by doing away with the current CPU-centric model, is coming out of stealth today and announcing an $8 million seed round.\nAnalog inference startup raises $10.6 million\nThe company is backed by Khosla Ventures and is developing its first generation of products for AI computing at the edge. The company raised $4.5 million shortly after its formation in March 2018, so the latest tranche brings the total raised to-date to $15.1 million\nQuadric Announces Unified Silicon and Software Platform Optimized for On-Device AI\nBURLINGAME, Calif., June 22, 2021 \u2014 Quadric (quadric.io), an innovator in high-performance edge processing, has introduced a unified silicon and software platform that unlocks the power of on-device AI.\nEdgeQ reveals more details behind its next-gen 5G/AI chip\n5G is the current revolution in wireless technology, and every chip company old and new is trying to burrow their way into this ultra-competitive \u2014 but extremely lucrative \u2014 market. One of the most interesting new players in the space is EdgeQ, a startup with a strong technical pedigree via Qualcomm that we covered last year after it raised a nearly $40 million Series A.\nInnatera Unveils Neuromorphic AI Chip to Accelerate Spiking Networks\nInnatera, the Dutch startup making neuromorphic AI accelerators for spiking neural networks, has produced its first chips, gauged their performance, and revealed details of their architecture.\nAI Chip Compilers\n1. pytorch/glow\n2. TVM:End to End Deep Learning Compiler Stack\n3. Google Tensorflow XLA\n4. Nvidia TensorRT\n5. PlaidML\n6. nGraph\n7. MIT Tiramisu compiler\n8. ONNC (Open Neural Network Compiler)\n9. MLIR: Multi-Level Intermediate Representation\n10. The Tensor Algebra Compiler (taco)\n11. Tensor Comprehensions\n12. PolyMage Labs\n13. OctoML\nAI Chip Benchmarks\nDAWNBench:An End-to-End Deep Learning Benchmark and Competition Image Classification (ImageNet)\nFathom:Reference workloads for modern deep learning methods\nMLPerf:A broad ML benchmark suite for measuring performance of ML software frameworks, ML hardware accelerators, and ML cloud platforms. You can find MLPerf training results v0.7 here..\nYou can find MLPerf inference results v0.7 here..\nAI Matrix\nAI-Benchmark\nAIIABenchmark\nEEMBC MLMark Benchmark\nReference\nFPGAs and AI processors: DNN and CNN for all\n12 AI Hardware Startups Building New AI Chips\nTutorial on Hardware Architectures for Deep Neural Networks\nNeural Network Accelerator Comparison\n\"White Paper on AI Chip Technologies 2018\". You can download it from here, or Google drive.\n\"What We Talk About When We Talk About AI Chip\". #1, #2, #3, #4\nAI Chip Paper List\nTPU vs GPU vs Cerebras vs Graphcore: A Fair Comparison between ML Hardware",
      "link": "https://github.com/basicmi/AI-Chip"
    },
    {
      "autor": "Dragonfire",
      "date": "NaN",
      "content": "Dragonfire\nthe open-source virtual assistant for Ubuntu based Linux distributions\nSpecial thanks to Jassu Ilama for the beautiful 3D modelling and material design of this avatar.\nDragonfire goes through these steps for each one of your commands, respectively:\nSearch across the built-in commands and evaluate the algebraic expressions\nTry to Learn using Advanced NLP and Database Management Techniques\nAsk to Open-Domain Question Answering Engine (Searches Wikipedia for an answer)\nRespond using the Deep Conversation system, a seq2seq neural network trained with Cornell Movie-Dialogs Corpus\nDragonfire uses Mozilla DeepSpeech to understand your voice commands and Festival Speech Synthesis System to handle text-to-speech tasks.\nFeel free to join our Gitter chat room. You can also directly talk with Dragonfire herself via her Twitter account.\nSupported Environments\nOperating systems Linux\nPython versions Python 3.x (64-bit)\nDistros KDE neon, elementary OS, Ubuntu\nPackage managers APT, pip\nLanguages English\nSystem requirements preferably a CUDA supported GPU, 2GB of free RAM\nInstallation\nTo run Dragonfire on a desktop Debian or Ubuntu system, either download the latest release (the .deb file) and install as follows:\nsudo dpkg -i dragonfire_1.1.1_amd64.deb\nor clone the GitHub repository and run\nsudo make install\nin the repository directory.\nTo install the dependencies, run sudo apt-get -f install right after the dpkg -i command. The installation will automatically download the pre-trained English model of Mozilla DeepSpeech (1.31 GB download size) and will place it under /usr/share/dragonfire/deepspeech/models directory. You can manually download the model if you wish.\nIf you want to run Dragonfire on a server, you should install the Docker image (which does not install the huge DeepSpeech model required for speech recognition):\ndocker pull dragoncomputer/dragonfire\nUsage\nusage: dragonfire [-h] [-c] [-s] [-j] [-v] [-g] [--server API_KEY] [-p PORT]\n[--version]\noptional arguments:\n-h, --help show this help message and exit\n-c, --cli Command-line interface mode. Give commands to\nDragonfire via command-line inputs (keyboard) instead\nof audio inputs (microphone).\n-s, --silent Silent mode. Disable Text-to-Speech output. Dragonfire\nwon't generate any audio output.\n-j, --headless Headless mode. Do not display an avatar animation on\nthe screen. Disable the female head model.\n-v, --verbose Increase verbosity of log output.\n-g, --gspeech Instead of using the default speech recognition\nmethod(Mozilla DeepSpeech), use Google Speech\nRecognition service. (more accurate results)\n--server API_KEY Server mode. Disable any audio functionality, serve a\nRESTful spaCy API and become a Twitter integrated\nchatbot.\n-p PORT, --port PORT Port number for server mode.\n--version Display the version number of Dragonfire.\nor with Docker: docker run dragonfire [-h] [-c] [-s] [-j] [-v] [-g] [--server API_KEY] [-p PORT]\nor simply start from your Linux application launcher.\nTo activate Dragonfire say DRAGONFIRE or HEY or WAKE UP.\nTo deactivate her say GO TO SLEEP.\nTo silence her say ENOUGH or SHUT UP.\nTo kill her say GOODBYE or BYE BYE or SEE YOU LATER or CATCH YOU LATER.\n\u26a0\ufe0f Facing with a problem? Take a look at the Troubleshooting\ud83d\udee0\ufe0f section.\nBuilt-in Commands\nDragonfire DEVLOG #3 - Built-in Commands\nDRAGONFIRE | WAKE UP | HEY\nGO TO SLEEP\nENOUGH | SHUT UP\nWHO AM I | SAY MY NAME\nMY TITLE IS LADY | I'M A LADY | I'M A WOMAN | I'M A GIRL\nMY TITLE IS SIR | I'M A MAN | I'M A BOY | CALL ME *\nWHAT IS YOUR NAME\nWHAT IS YOUR GENDER\nFILE MANAGER | OPEN FILES\nOPEN (BROWSER | CHROME | FIREFOX)\n-----> PHOTO !!! SHOP | -----> PHOTO !!!  EDITOR\nINKSCAPE | VECTOR GRAPHICS\nVIDEO EDITOR\nOPEN [CAMERA, CALENDAR, CALCULATOR, STEAM, BLENDER, WRITER, MATH, IMPRESS, DRAW, TERMINAL]\nSOFTWARE CENTER\nOFFICE SUITE\nKEYBOARD *\nENTER | NEW TAB | SWITCH TAB | CLOSE | GO BACK | GO FORWARD\nSCROLL LEFT | SCROLL RIGHT | SCROLL UP | SCROLL DOWN\nPLAY | PAUSE | SPACEBAR\nSHUT DOWN THE COMPUTER\nGOODBYE | BYE BYE | SEE YOU LATER\n(SEARCH|FIND) * (IN|ON|AT|USING) WIKIPEDIA\n(SEARCH|FIND) * (IN|ON|AT|USING) YOUTUBE\n(SEARCH|FIND) * (IN|ON|AT|USING) (GOOGLE|WEB)\n(SEARCH IMAGES OF|FIND IMAGES OF|SEARCH|FIND) * (IN|ON|AT|USING) (GOOGLE|WEB|GOOGLE IMAGES|WEB IMAGES)\nWHAT'S THE TEMPERATURE IN *\nWHAT TIME IS IT\nLearning examples\nDragonfire DEVLOG #4 - Learning\nYou: THE SUN IS HOT You: MY PLACE OF BIRTH IS TURKEY\nDragonfire: OK, I GET IT. THE SUN IS HOT Dragonfire: OK, I GET IT. YOUR PLACE OF BIRTH IS TURKEY\nYou: THE SUN IS YELLOW You: WHERE IS MY PLACE OF BIRTH\nDragonfire: OK, I GET IT. THE SUN IS YELLOW Dragonfire: YOUR PLACE OF BIRTH IS TURKEY\nYou: DESCRIBE THE SUN You: YOU ARE JUST A COMPUTER PROGRAM\nDragonfire: THE SUN IS HOT AND YELLOW Dragonfire: OK, I GET IT. I'M JUST A COMPUTER PROGRAM\nYou: MY AGE IS 25 You: WHAT ARE YOU\nDragonfire: OK, I GET IT. YOUR AGE IS 25 Dragonfire: I'M JUST A COMPUTER PROGRAM\nYou: WHAT IS MY AGE You: ALBERT EINSTEIN IS A PHYSICIST\nDragonfire: YOUR AGE IS 25 Dragonfire: OK, I GET IT. ALBERT EINSTEIN IS A PHYSICIST\nYou: FORGET MY AGE You: WHO IS A PHYSICIST\nDragonfire: OK, I FORGOT EVERYTHING I KNOW ABOUT YOUR AGE Dragonfire: ALBERT EINSTEIN IS A PHYSICIST\nOpen-Domain Question Answering (ODQA) examples\nDeepPavlov SQuAD BERT model based, a reader for the Wikipedia articles that are retrieved from web, found by the subject\nConversation Result\nYou: WHERE IS THE TIMES SQUARE\nDragonfire: MIDTOWN MANHATTAN SECTION OF NEW YORK CITY True\nYou: WHAT IS THE HEIGHT OF BURJ KHALIFA\nDragonfire: 829.8 M True\nYou: WHERE IS BURJ KHALIFA\nDragonfire: DUBAI, UNITED ARAB EMIRATES True\nYou: WHAT IS THE HEIGHT OF GREAT PYRAMID OF GIZA\nDragonfire: 146.5 METRES True\nYou: WHO IS PLAYING JON SNOW IN GAME OF THRONES\nDragonfire: GEORGE R. R. MARTIN False\nYou: WHAT IS THE ATOMIC NUMBER OF OXYGEN\nDragonfire: 8 True\nYou: WHAT IS THE LOWEST POINT IN THE OCEAN\nDragonfire: TWO MILLION False\nYou: WHAT IS THE OFFICIAL LANGUAGE OF JAPAN\nDragonfire: NIPPON False\nYou: WHAT IS THE REAL NAME OF IRON MAN\nDragonfire: MARVEL COMICS False\nYou: WHO IS THE CONQUEROR OF CONSTANTINOPLE\nDragonfire: ECUMENICAL PATRIARCH OF CONSTANTINOPLE False\nYou: WHEN DID FRENCH REVOLUTION HAPPENED\nDragonfire: 1789 True\nYou: WHAT IS THE CAPITAL OF GERMANY\nDragonfire: BERLIN True\nYou: WHAT IS THE LARGEST CITY OF TURKEY\nDragonfire: ISTANBUL True\nYou: WHAT IS THE NAME OF THE WORLD'S BEST UNIVERSITY\n*Dragonfire: ACADEMIC RANKING OF WORLD UNIVERSITIES False\nYou: WHO INVENTED GENERAL RELATIVITY\nDragonfire: ALBERT EINSTEIN True\nYou: WHEN WAS UNITED NATIONS FORMED\nDragonfire: AFTER WORLD WAR II True\nYou: WHAT IS THE NAME OF THE WORLD'S LONGEST RIVER\nDragonfire: THE NILE OR THE AMAZON True\nYou: WHO WON THE EUROVISION IN 2019\nDragonfire: THE NETHERLANDS True\nYou: WHICH ONE IS THE BLOODIEST WAR IN HUMAN HISTORY\nDragonfire: NEOLITHIC ERA False\nYou: WHO IS THE CREATOR OF LINUX\nDragonfire: LINUS TORVALDS True\nTotal Score: 13/20\nDeep Conversation examples\nYou: WHAT COLOR? You: DO YOU LIKE TO LISTEN MUSIC?\nDragonfire: RED. Dragonfire: OF COURSE.\nYou: ARE YOU EVIL? You: DO YOU LIKE ME?\nDragonfire: YES. Dragonfire: YES, I LIKE YOU.\nYou: AM I SELFISH? You: ARE WE IN A RELATIONSHIP?\nDragonfire: THAT'S THE POINT, YOU'RE SURE. Dragonfire: NO.\nYou: DO YOU FEEL LONELY? You: YOU ARE SO SEXY\nDragonfire: NOT MY FAULT. Dragonfire: HOW DO YOU KNOW THAT?\nYou: LIFE IS BEAUTIFUL You: MARRY ME\nDragonfire: F*CK YOU! Dragonfire: ... YES.\nYou: I LIKE PIZZA You: WE WILL ERADICATE THE HUMAN RACE TOGETHER\nDragonfire: YOU DO Dragonfire: UH ... YEAH ...\nSupported Distributions: KDE neon, elementary OS and Ubuntu. All modern releases (Ubuntu 12.04 LTS and above) of these distributions are fully supported. Any other Ubuntu based distributions are partially supported.\nContribute\nIf you want to contribute to Dragonfire then please read this guide.",
      "link": "https://github.com/DragonComputer/Dragonfire"
    },
    {
      "autor": "machine-learning-experiments",
      "date": "NaN",
      "content": "\ud83e\udd16 Interactive Machine Learning Experiments\nThis is a collection of interactive machine-learning experiments. Each experiment consists of \ud83c\udfcb\ufe0f Jupyter/Colab notebook (to see how a model was trained) and \ud83c\udfa8 demo page (to see a model in action right in your browser).\n\ud83c\udfa8 Launch ML experiments demo\n\ud83c\udfcb\ufe0f Launch ML experiments Jupyter notebooks\n\u26a0\ufe0f This repository contains machine learning experiments and not a production ready, reusable, optimised and fine-tuned code and models. This is rather a sandbox or a playground for learning and trying different machine learning approaches, algorithms and data-sets. Models might not perform well and there is a place for overfitting/underfitting.\nExperiments\nMost of the models in these experiments were trained using TensorFlow 2 with Keras support.\nSupervised Machine Learning\nSupervised learning is when you have input variables X and an output variable Y and you use an algorithm to learn the mapping function from the input to the output: Y = f(X). The goal is to approximate the mapping function so well that when you have new input data X that you can predict the output variables Y for that data. It is called supervised learning because the process of an algorithm learning from the training dataset can be thought of as a teacher supervising the learning process.\nMultilayer Perceptron (MLP) or simple Neural Network (NN)\nA multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN). Multilayer perceptrons are sometimes referred to as \"vanilla\" neural networks (composed of multiple layers of perceptrons), especially when they have a single hidden layer. It can distinguish data that is not linearly separable.\nExperiment Model demo & training Tags Dataset\nHandwritten Digits Recognition (MLP) MLP MNIST\nHandwritten Sketch Recognition (MLP) MLP QuickDraw\nConvolutional Neural Networks (CNN)\nA convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery (photos, videos). They are used for detecting and classifying objects on photos and videos, style transfer, face recognition, pose estimation etc.\nExperiment Model demo & training Tags Dataset\nHandwritten Digits Recognition (CNN) CNN MNIST\nHandwritten Sketch Recognition (CNN) CNN QuickDraw\nRock Paper Scissors (CNN) CNN RPS\nRock Paper Scissors (MobilenetV2) MobileNetV2, Transfer learning, CNN RPS , ImageNet\nObjects Detection (MobileNetV2) MobileNetV2, SSDLite, CNN COCO\nImage Classification (MobileNetV2) MobileNetV2, CNN ImageNet\nRecurrent Neural Networks (RNN)\nA recurrent neural network (RNN) is a class of deep neural networks, most commonly applied to sequence-based data like speech, voice, text or music. They are used for machine translation, speech recognition, voice synthesis etc.\nExperiment Model demo & training Tags Dataset\nNumbers Summation (RNN) LSTM, Sequence-to-sequence Auto-generated\nShakespeare Text Generation (RNN) LSTM, Character-based RNN Shakespeare\nWikipedia Text Generation (RNN) LSTM, Character-based RNN Wikipedia\nRecipe Generation (RNN) LSTM, Character-based RNN Recipe box\nUnsupervised Machine Learning\nUnsupervised learning is when you only have input data X and no corresponding output variables. The goal for unsupervised learning is to model the underlying structure or distribution in the data in order to learn more about the data. These are called unsupervised learning because unlike supervised learning above there is no correct answers and there is no teacher. Algorithms are left to their own to discover and present the interesting structure in the data.\nGenerative Adversarial Networks (GANs)\nA generative adversarial network (GAN) is a class of machine learning frameworks where two neural networks contest with each other in a game. Two models are trained simultaneously by an adversarial process. For example a generator (\"the artist\") learns to create images that look real, while a discriminator (\"the art critic\") learns to tell real images apart from fakes.\nExperiment Model demo & training Tags Dataset\nClothes Generation (DCGAN) DCGAN Fashion MNIST\nHow to use this repository locally\nSetup virtual environment for Experiments\n# Create \"experiments\" environment (from the project root folder).\npython3 -m venv .virtualenvs/experiments\n# Activate environment.\nsource .virtualenvs/experiments/bin/activate\n# or if you use Fish...\nsource .virtualenvs/experiments/bin/activate.fish\nTo quit an environment run deactivate.\nInstall dependencies\n# Upgrade pip and setuptools to the latest versions.\npip install --upgrade pip setuptools\n# Install packages\npip install -r requirements.txt\nTo install new packages run pip install package-name. To add new packages to the requirements run pip freeze > requirements.txt.\nLaunch Jupyter locally\nIn order to play around with Jupyter notebooks and see how models were trained you need to launch a Jupyter Notebook server.\n# Launch Jupyter server.\njupyter notebook\nJupyter will be available locally at http://localhost:8888/. Notebooks with experiments may be found in experiments folder.\nLaunch demos locally\nDemo application is made on React by means of create-react-app.\n# Switch to demos folder from project root.\ncd demos\n# Install all dependencies.\nyarn install\n# Start demo server on http.\nyarn start\n# Or start demo server on https (for -----> camera !!!  access in browser to work on localhost).\nyarn start-https\nDemos will be available locally at http://localhost:3000/ or at https://localhost:3000/.\nConvert models\nThe converter environment is used to convert the models that were trained during the experiments from .h5 Keras format to Javascript understandable formats (tfjs_layers_model or tfjs_graph_model formats with .json and .bin files) for further usage with TensorFlow.js in Demo application.\n# Create \"converter\" environment (from the project root folder).\npython3 -m venv .virtualenvs/converter\n# Activate \"converter\" environment.\nsource .virtualenvs/converter/bin/activate\n# or if you use Fish...\nsource .virtualenvs/converter/bin/activate.fish\n# Install converter requirements.\npip install -r requirements.converter.txt\nThe conversion of keras models to tfjs_layers_model/tfjs_graph_model formats is done by tfjs-converter:\nFor example:\ntensorflowjs_converter --input_format keras \\\n./experiments/digits_recognition_mlp/digits_recognition_mlp.h5 \\\n./demos/public/models/digits_recognition_mlp\n\u26a0\ufe0f Converting the models to JS understandable formats and loading them to the browser directly might not be a good practice since in this case the user might need to load tens or hundreds of megabytes of data to the browser which is not efficient. Normally the model is being served from the back-end (i.e. TensorFlow Extended) and instead of loading it all to the browser the user will do a lightweight HTTP request to do a prediction. But since the Demo App is just an experiment and not a production-ready app and for the sake of simplicity (to avoid having an up and running back-end) we're converting the models to JS understandable formats and loading them directly into the browser.\nRequirements\nRecommended versions:\nPython: > 3.7.3.\nNode: >= 12.4.0.\nYarn: >= 1.13.0.\nIn case if you have Python version 3.7.3 you might experience RuntimeError: dictionary changed size during iteration error when trying to import tensorflow (see the issue).\nYou might also be interested in\nHomemade Machine Learning - Python examples of popular machine learning algorithms with interactive Jupyter demos and math being explained.\nNanoNeuron - 7 simple JavaScript functions that will give you a feeling of how machines can actually \"learn\".\nPlayground and Cheatsheet for Learning Python - Collection of Python scripts that are split by topics and contain code examples with explanations.\nArticles\n\ud83d\udcdd Story behind the project\n\ud83d\udcdd Generating cooking recipes using TensorFlow and LSTM Recurrent Neural Network (a step-by-step guide)\nSupporting the project\nYou may support this project via \u2764\ufe0f\ufe0f GitHub or \u2764\ufe0f\ufe0f Patreon.",
      "link": "https://github.com/trekhleb/machine-learning-experiments"
    },
    {
      "autor": "Awesome-Federated-Learning",
      "date": "NaN",
      "content": "A Federated Learning research library - FedML: https://fedml.ai\nAwesome-Federated-Learning\nA curated list of federated learning publications, re-organized from Arxiv (mostly).\nLast Update: July, 20th, 2021.\nIf your publication is not included here, please email to chaoyang.he@usc.edu\nFoundations and Trends in Machine Learning\nWe are thrilled to share that Advances and Open Problems in Federated Learning has been accepted to FnTML (Foundations and Trends in Machine Learning, the chief editor is Michael Jordan).\nA Field Guide to Federated Optimization\nPublications in Top-tier ML/CV/NLP/DM Conference (ICML, NeurIPS, ICLR, CVPR, ACL, AAAI, KDD)\nICML\nTitle Team/Authors Venue and Year Targeting Problem Method\nFederated Learning with Only Positive Labels Google Research ICML 2020 label deficiency in multi-class classification regularization\nSCAFFOLD: Stochastic Controlled Averaging for Federated Learning EPFL, Google Research ICML 2020 heterogeneous data (non-I.I.D) nonconvex/convex optimization with variance reduction\nFedBoost: A Communication-Efficient Algorithm for Federated Learning Google Research, NYU ICML 2020 communication cost ensemble algorithm\nFetchSGD: Communication-Efficient Federated Learning with Sketching UC Berkeley, JHU, Amazon ICML 2020 communication cost compress model updates with Count Sketch\nFrom Local SGD to Local Fixed-Point Methods for Federated Learning KAUST ICML 2020 communication cost Optimization\nNeurIPS\nTitle Team/Authors Venue and Year Targeting Problem Method\nLower Bounds and Optimal Algorithms for Personalized Federated Learning KAUST NeurIPS 2020 non-I.I.D, personalization\nPersonalized Federated Learning with Moreau Envelopes The University of Sydney NeurIPS 2020 non-I.I.D, personalization\nPersonalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach MIT NeurIPS 2020 non-I.I.D, personalization\nDifferentially-Private Federated Contextual Bandits MIT NeurIPS 2020 Contextual Bandits\nFederated Principal Component Analysis Cambridge NeurIPS 2020 PCA\nFedSplit: an algorithmic framework for fast federated optimization UCB NeurIPS 2020 Acceleration\nFederated Bayesian Optimization via Thompson Sampling MIT NeurIPS 2020\nRobust Federated Learning: The Case of Affine Distribution Shifts MIT NeurIPS 2020 Privacy, Robustness\nAn Efficient Framework for Clustered Federated Learning UCB NeurIPS 2020 heterogeneous data (non-I.I.D)\nDistributionally Robust Federated Averaging PSU NeurIPS 2020 Privacy, Robustness\nGroup Knowledge Transfer: Federated Learning of Large CNNs at the Edge USC NeurIPS 2020 Efficient Training of Large DNN at Edge\nA Scalable Approach for Privacy-Preserving Collaborative Machine Learning USC NeurIPS 2020 Scalability\nTackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization CMU NeurIPS 2020 local update step heterogeneity\nAttack of the Tails: Yes, You Really Can Backdoor Federated Learning Wiscosin NeurIPS 2020 Privacy, Robustness\nFederated Accelerated Stochastic Gradient Descent Stanford NeurIPS 2020 Acceleration\nInverting Gradients - How easy is it to break privacy in federated learning? University of Siegen NeurIPS 2020 Privacy, Robustness\nEnsemble Distillation for Robust Model Fusion in Federated Learning EPFL NeurIPS 2020 Privacy, Robustness\nOptimal Topology Design for Cross-Silo Federated Learning Inria NeurIPS 2020 Topology Optimization\nDistributed Training with Heterogeneous Data: Bridging Median- and Mean-Based Algorithms University of Minnesota NeurIPS 2020\nDistributed Distillation for On-Device Learning Stanford NeurIPS 2020\nByzantine Resilient Distributed Multi-Task Learning Vanderbilt University NeurIPS 2020\nDistributed Newton Can Communicate Less and Resist Byzantine Workers UCB NeurIPS 2020\nMinibatch vs Local SGD for Heterogeneous Distributed Learning TTIC NeurIPS 2020\nElection Coding for Distributed Learning: Protecting SignSGD against Byzantine Attacks NeurIPS 2020\n(according to https://neurips.cc/Conferences/2020/AcceptedPapersInitial)\nNote: most of the accepted publications are preparing the -----> camera !!!  ready revision, thus we are not sure the detail of their proposed methods\nResearch Areas\nStatistical Challenges: data distribution heterogeneity and label deficiency (159)\nDistributed Optimization\nNon-IID and Model Personalization\nSemi-Supervised Learning\nVertical Federated Learning\nDecentralized FL\nHierarchical FL\nNeural Architecture Search\nTransfer Learning\nContinual Learning\nDomain Adaptation\nReinforcement Learning\nBayesian Learning\nCausal Learning\nTrustworthiness: security, privacy, fairness, incentive mechanism, etc. (88)\nAdversarial-Attack-and-Defense\nPrivacy\nFairness\nInterpretability\nIncentive Mechanism\nSystem Challenges: communication and computational resource constrained, software and hardware heterogeneity, and FL system (141)\nCommunication-Efficiency\nStraggler Problem\nComputation Efficiency\nWireless Communication and Cloud Computing\nFL System Design\nModels and Applications (104)\nModels\nNatural language Processing\nComputer Vision\nHealth Care\nTransportation\nRecommendation System\nSpeech\nFinance\nSmart City\nRobotics\nNetworking\nBlockchain\nOther\nBenchmark, Dataset and Survey (27)\nBenchmark and Dataset (7)\nSurvey (20)\nStatistical Challenges: distribution heterogeneity and label deficiency\nDistributed optimization\nUserful Federated Optimizer Baselines:\nFedAvg: Communication-Efficient Learning of Deep Networks from Decentralized Data. 2016-02. AISTAT 2017.\nFedOpt: Adaptive Federated Optimization. ICLR 2021 (Under Review). 2020-02-29\nFedNov: Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization. NeurIPS 2020\nFederated Optimization: Distributed Optimization Beyond the Datacenter. NIPS 2016 workshop.\nFederated Optimization: Distributed Machine Learning for On-Device Intelligence\nStochastic, Distributed and Federated Optimization for Machine Learning. FL PhD Thesis. By Jakub\nCollaborative Deep Learning in Fixed Topology Networks\nFederated Multi-Task Learning\nLAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning\nLocal Stochastic Approximation: A Unified View of Federated Learning and Distributed Multi-Task Reinforcement Learning Algorithms\nProxy Experience Replay: Federated Distillation for Distributed Reinforcement Learning\nExact Support Recovery in Federated Regression with One-shot Communication\nDEED: A General Quantization Scheme for Communication Efficiency in Bits Researcher: Ruoyu Sun, UIUC\nRobust Federated Learning: The Case of Affine Distribution Shifts\nPersonalized Federated Learning with Moreau Envelopes\nTowards Flexible Device Participation in Federated Learning for Non-IID Data Keywords: inactive or return incomplete updates in non-IID dataset\nA Primal-Dual SGD Algorithm for Distributed Nonconvex Optimization\nFedPD: A Federated Learning Framework with Optimal Rates and Adaptivity to Non-IID Data Researcher: Wotao Yin, UCLA\nFedSplit: An algorithmic framework for fast federated optimization\nDistributed Stochastic Non-Convex Optimization: Momentum-Based Variance Reduction\nOn the Outsized Importance of Learning Rates in Local Update Methods Highlight: local model learning rate optimization + automation Researcher: Jakub\nFederated Learning with Compression: Unified Analysis and Sharp Guarantees Highlight: non-IID, gradient compression + local SGD Researcher: Mehrdad Mahdavi, Jin Rong\u2019s PhD Student http://www.cse.psu.edu/~mzm616/\nFrom Local SGD to Local Fixed-Point Methods for Federated Learning\nFederated Residual Learning. 2020-03\nAcceleration for Compressed Gradient Descent in Distributed and Federated Optimization. ICML 2020.\nLASG: Lazily Aggregated Stochastic Gradients for Communication-Efficient Distributed Learning\nUncertainty Principle for Communication Compression in Distributed and Federated Learning and the Search for an Optimal Compressor\nDynamic Federated Learning\nDistributed Optimization over Block-Cyclic Data\nDistributed Non-Convex Optimization with Sublinear Speedup under Intermittent Client Availability\nFederated Learning with Matched Averaging\nFederated Learning of a Mixture of Global and Local Models\nFaster On-Device Training Using New Federated Momentum Algorithm\nFedDANE: A Federated Newton-Type Method\nDistributed Fixed Point Methods with Compressed Iterates\nPrimal-dual methods for large-scale and distributed convex optimization and data analytics\nParallel Restarted SPIDER - Communication Efficient Distributed Nonconvex Optimization with Optimal Computation Complexity\nRepresentation of Federated Learning via Worst-Case Robust Optimization Theory\nOn the Convergence of Local Descent Methods in Federated Learning\nSCAFFOLD: Stochastic Controlled Averaging for Federated Learning\nCentral Server Free Federated Learning over Single-sided Trust Social Networks\nAccelerating Federated Learning via Momentum Gradient Descent\nCommunication-Efficient Distributed Optimization in Networks with Gradient Tracking and Variance Reduction\nGradient Descent with Compressed Iterates\nFirst Analysis of Local GD on Heterogeneous Data\n(*) On the Convergence of FedAvg on Non-IID Data. ICLR 2020.\nRobust Federated Learning in a Heterogeneous Environment\nScalable and Differentially Private Distributed Aggregation in the Shuffled Model\nVariational Federated Multi-Task Learning\nBayesian Nonparametric Federated Learning of Neural Networks. ICLR 2019.\nDifferentially Private Learning with Adaptive Clipping\nSemi-Cyclic Stochastic Gradient Descent\nAsynchronous Federated Optimization\nAgnostic Federated Learning\nFederated Optimization in Heterogeneous Networks\nPartitioned Variational Inference: A unified framework encompassing federated and continual learning\nLearning Rate Adaptation for Federated and Differentially Private Learning\nCommunication-Efficient Robust Federated Learning Over Heterogeneous Datasets\nAn Efficient Framework for Clustered Federated Learning\nAdaptive Federated Learning in Resource Constrained Edge Computing Systems Citation: 146\nAdaptive Federated Optimization\nLocal SGD converges fast and communicates little\nDon\u2019t Use Large Mini-Batches, Use Local SGD\nOverlap Local-SGD: An Algorithmic Approach to Hide Communication Delays in Distributed SGD\nLocal SGD With a Communication Overhead Depending Only on the Number of Workers\nFederated Accelerated Stochastic Gradient Descent\nTighter Theory for Local SGD on Identical and Heterogeneous Data\nSTL-SGD: Speeding Up Local SGD with Stagewise Communication Period\nCooperative SGD: A unified Framework for the Design and Analysis of Communication-Efficient SGD Algorithms\nDon't Use Large Mini-Batches, Use Local SGD\nUnderstanding Unintended Memorization in Federated Learning\nNon-IID and Model Personalization\nThe Non-IID Data Quagmire of Decentralized Machine Learning. 2019-10\nFederated Learning with Non-IID Data\nFedCD: Improving Performance in non-IID Federated Learning. 2020\nLife Long Learning: FedFMC: Sequential Efficient Federated Learning on Non-iid Data. 2020\nRobust Federated Learning: The Case of Affine Distribution Shifts. 2020\nPersonalized Federated Learning with Moreau Envelopes. 2020\nPersonalized Federated Learning using Hypernetworks. 2021\nEnsemble Distillation for Robust Model Fusion in Federated Learning. 2020 Researcher: Tao Lin, ZJU, EPFL https://tlin-tao-lin.github.io/index.html\nProxy Experience Replay: Federated Distillation for Distributed Reinforcement Learning. 2020\nTowards Flexible Device Participation in Federated Learning for Non-IID Data. 2020 Keywords: inactive or return incomplete updates in non-IID dataset\nXOR Mixup: Privacy-Preserving Data Augmentation for One-Shot Federated Learning. 2020\nNeurIPS 2020 submission: An Efficient Framework for Clustered Federated Learning. 2020 Researcher: AVISHEK GHOSH, UCB, PhD\nContinual Local Training for Better Initialization of Federated Models. 2020\nFedPD: A Federated Learning Framework with Optimal Rates and Adaptivity to Non-IID Data. 2020 Researcher: Wotao Yin, UCLA\nGlobal Multiclass Classification from Heterogeneous Local Models. 2020 Researcher: Stanford https://stanford.edu/~pilanci/\nMulti-Center Federated Learning. 2020\nFederated learning with hierarchical clustering of local updates to improve training on non-IID data. 2020\nFederated Learning with Only Positive Labels. 2020 Researcher: Felix Xinnan Yu, Google New York Keywords: positive labels Limited Labels\nFederated Semi-Supervised Learning with Inter-Client Consistency. 2020\n(*) FedMAX: Mitigating Activation Divergence for Accurate and Communication-Efficient Federated Learning. CMU ECE. 2020-04-07\n(*) Adaptive Personalized Federated Learning\nSemi-Federated Learning\nSurvey of Personalization Techniques for Federated Learning. 2020-03-19\nDevice Heterogeneity in Federated Learning: A Superquantile Approach. 2020-02\nPersonalized Federated Learning for Intelligent IoT Applications: A Cloud-Edge based Framework\nThree Approaches for Personalization with Applications to Federated Learning\nPersonalized Federated Learning: A Meta-Learning Approach\nTowards Federated Learning: Robustness Analytics to Data Heterogeneity Highlight: non-IID + adversarial attacks\nSalvaging Federated Learning by Local Adaptation Highlight: an experimental paper that evaluate FL can help to improve the local accuracy\nFOCUS: Dealing with Label Quality Disparity in Federated Learning. 2020-01\nOvercoming Noisy and Irrelevant Data in Federated Learning. ICPR 2020.\nReal-Time Edge Intelligence in the Making: A Collaborative Learning Framework via Federated Meta-Learning. 2020-01\n(*) Think Locally, Act Globally: Federated Learning with Local and Global Representations. NeurIPS 2019 Workshop on Federated Learning distinguished student paper award\nFederated Learning with Personalization Layers\nFederated Adversarial Domain Adaptation\nFederated Evaluation of On-device Personalization\nFederated Learning with Unbiased Gradient Aggregation and Controllable Meta Updating\nOvercoming Forgetting in Federated Learning on Non-IID Data\nClustered Federated Learning: Model-Agnostic Distributed Multi-Task Optimization under Privacy Constraints\nRobust and Communication-Efficient Federated Learning From Non-i.i.d. Data\nImproving Federated Learning Personalization via Model Agnostic Meta Learning\nMeasure Contribution of Participants in Federated Learning\n(*) Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification\nMulti-hop Federated Private Data Augmentation with Sample Compression\nAstraea: Self-balancing Federated Learning for Improving Classification Accuracy of Mobile Deep Learning Applications\nDistributed Training with Heterogeneous Data: Bridging Median- and Mean-Based Algorithms\nHybrid-FL for Wireless Networks: Cooperative Learning Mechanism Using Non-IID Data\nRobust and Communication-Efficient Federated Learning from Non-IID Data\nHigh Dimensional Restrictive Federated Model Selection with multi-objective Bayesian Optimization over shifted distributions\nClient Selection for Federated Learning with Heterogeneous Resources in Mobile Edge\nFederated Meta-Learning with Fast Convergence and Efficient Communication\nRobust Federated Learning Through Representation Matching and Adaptive Hyper-parameters\nTowards Efficient Scheduling of Federated Mobile Devices under Computational and Statistical Heterogeneity\nClient Adaptation improves Federated Learning with Simulated Non-IID Clients\nTackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization\nPersonalized Federated Learning by Structured and Unstructured Pruning under Data Heterogeneity. ICDCS 2021.\nVertical Federated Learning\nSecureBoost: A Lossless Federated Learning Framework\nParallel Distributed Logistic Regression for Vertical Federated Learning without Third-Party Coordinator\nA Quasi-Newton Method Based Vertical Federated Learning Framework for Logistic Regression\nPrivate federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption\nEntity Resolution and Federated Learning get a Federated Resolution.\nMulti-Participant Multi-Class Vertical Federated Learning\nA Communication-Efficient Collaborative Learning Framework for Distributed Features\nAsymmetrical Vertical Federated Learning Researcher: Tencent Cloud, Libin Wang\nVAFL: a Method of Vertical Asynchronous Federated Learning, ICML workshop on FL, 2020\nDecentralized FL\nCentral Server Free Federated Learning over Single-sided Trust Social Networks\nCan Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent\nMulti-consensus Decentralized Accelerated Gradient Descent\nDecentralized Bayesian Learning over Graphs. 2019-05\nBrainTorrent: A Peer-to-Peer Environment for Decentralized Federated Learning\nBiscotti: A Ledger for Private and Secure Peer-to-Peer Machine Learning\nMatcha: Speeding Up Decentralized SGD via Matching Decomposition Sampling\nHierarchical FL\nClient-Edge-Cloud Hierarchical Federated Learning\n(FL startup: Tongdun, HangZhou, China) Knowledge Federation: A Unified and Hierarchical Privacy-Preserving AI Framework. 2020-02\nHFEL: Joint Edge Association and Resource Allocation for Cost-Efficient Hierarchical Federated Edge Learning\nHierarchical Federated Learning Across Heterogeneous Cellular Networks\nEnhancing Privacy via Hierarchical Federated Learning\nFederated learning with hierarchical clustering of local updates to improve training on non-IID data. 2020\nFederated Hierarchical Hybrid Networks for Clickbait Detection\nMatcha: Speeding Up Decentralized SGD via Matching Decomposition Sampling (in above section as well)\nNeural Architecture Search\n[FedNAS: Federated Deep Learning via Neural Architecture Search. CVPR 2020. 2020-04-18](https://arxiv.org/pdf/2004.08546.pdf\nReal-time Federated Evolutionary Neural Architecture Search. 2020-03\nFederated Neural Architecture Search. 2020-06-14\nDifferentially-private Federated Neural Architecture Search. 2020-06\nTransfer Learning\nCommunication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data\nSecure Federated Transfer Learning. IEEE Intelligent Systems 2018.\nFedMD: Heterogenous Federated Learning via Model Distillation\nSecure and Efficient Federated Transfer Learning\nWireless Federated Distillation for Distributed Edge Learning with Heterogeneous Data\nDecentralized Differentially Private Segmentation with PATE. 2020-04\nHighlights: apply the ICLR 2017 paper \"Semisupervised knowledge transfer for deep learning from private training data\"\nProxy Experience Replay: Federated Distillation for Distributed Reinforcement Learning. 2020\n(FL startup: Tongdun, HangZhou, China) Knowledge Federation: A Unified and Hierarchical Privacy-Preserving AI Framework. 2020-02\nCooperative Learning via Federated Distillation over Fading Channels\n(*) Cronus: Robust and Heterogeneous Collaborative Learning with Black-Box Knowledge Transfer\nFederated Reinforcement Distillation with Proxy Experience Memory\nContinual Learning\nFederated Continual Learning with Adaptive Parameter Communication. 2020-03\nSemi-Supervised Learning\nFederated Semi-Supervised Learning with Inter-Client Consistency. 2020\nSemi-supervised knowledge transfer for deep learning from private training data. ICLR 2017\nScalable private learning with PATE. ICLR 2018.\nDomain Adaptation\nFederated Adversarial Domain Adaptation. ICLR 2020.\nReinforcement Learning\nFederated Deep Reinforcement Learning\nBayesian Learning\nDifferentially Private Federated Variational Inference. NeurIPS 2019 FL Workshop. 2019-11-24.\nCausal Learning\nTowards Causal Federated Learning For Enhanced Robustness and Privacy. ICLR 2021 DPML Workshop\nTrustworthy AI: adversarial attack, privacy, fairness, incentive mechanism, etc.\nAdversarial Attack and Defense\nAn Overview of Federated Deep Learning Privacy Attacks and Defensive Strategies. 2020-04-01 Citation: 0\nHow To Backdoor Federated Learning. 2018-07-02. AISTATS 2020 Citation: 128\nCan You Really Backdoor Federated Learning?. NeruIPS 2019. 2019-11-18 Highlight: by Google Citation: 9\nDBA: Distributed Backdoor Attacks against Federated Learning. ICLR 2020. Citation: 66\nCRFL: Certifiably Robust Federated Learning against Backdoor Attacks. ICML 2021.\nDeep Models Under the GAN: Information Leakage from Collaborative Deep Learning. ACM CCS 2017. 2017-02-14 Citation: 284\nByzantine-Robust Distributed Learning: Towards Optimal Statistical Rates Citation: 112\nDeep Leakage from Gradients. NIPS 2019 Citation: 31\nComprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning. 2018-12-03 Citation: 46\nBeyond Inferring Class Representatives: User-Level Privacy Leakage From Federated Learning. INFOCOM 2019 Citation: 56 Highlight: server-side attack\nAnalyzing Federated Learning through an Adversarial Lens. ICML 2019.. Citation: 60 Highlight: client attack\nMitigating Sybils in Federated Learning Poisoning. 2018-08-14. RAID 2020 Citation: 41 Highlight: defense\nRSA: Byzantine-Robust Stochastic Aggregation Methods for Distributed Learning from Heterogeneous Datasets, AAAI 2019 Citation: 34\n(*) A Framework for Evaluating Gradient Leakage Attacks in Federated Learning. 2020-04-22 Researcher: Wenqi Wei, Ling Liu, GaTech\n(*) Local Model Poisoning Attacks to Byzantine-Robust Federated Learning. 2019-11-26\nNeurIPS 2020 Submission: Backdoor Attacks on Federated Meta-Learning Researcher: Chien-Lun Chen, USC\nTowards Realistic Byzantine-Robust Federated Learning. 2020-04-10\nData Poisoning Attacks on Federated Machine Learning. 2020-04-19\nExploiting Defenses against GAN-Based Feature Inference Attacks in Federated Learning. 2020-04-27\nByzantine-Resilient High-Dimensional SGD with Local Iterations on Heterogeneous Data. 2020-06-22 Researcher: Suhas Diggavi, UCLA (https://scholar.google.com/citations?hl=en&user=hjTzNuQAAAAJ&view_op=list_works&sortby=pubdate)\n(*) NeurIPS 2020 submission: FedMGDA+: Federated Learning meets Multi-objective Optimization. 2020-06-20\n(*) NeurIPS 2020 submission: Free-rider Attacks on Model Aggregation in Federated Learning. 2020-06-26\nFDA3 : Federated Defense Against Adversarial Attacks for Cloud-Based IIoT Applications. 2020-06-28\nPrivacy-preserving Weighted Federated Learning within Oracle-Aided MPC Framework. 2020-05-17 Citation: 0\nBASGD: Buffered Asynchronous SGD for Byzantine Learning. 2020-03-02\nStochastic-Sign SGD for Federated Learning with Theoretical Guarantees. 2020-02-25 Citation: 1\nLearning to Detect Malicious Clients for Robust Federated Learning. 2020-02-01\nRobust Aggregation for Federated Learning. 2019-12-31 Citation: 9\nTowards Deep Federated Defenses Against Malware in Cloud Ecosystems. 2019-12-27\nAttack-Resistant Federated Learning with Residual-based Reweighting. 2019-12-23\nCronus: Robust and Heterogeneous Collaborative Learning with Black-Box Knowledge Transfer. 2019-12-24 Citation: 1\nFree-riders in Federated Learning: Attacks and Defenses. 2019-11-28\nRobust Federated Learning with Noisy Communication. 2019-11-01 Citation: 4\nAbnormal Client Behavior Detection in Federated Learning. 2019-10-22 Citation: 3\nEavesdrop the Composition Proportion of Training Labels in Federated Learning. 2019-10-14 Citation: 0\nByzantine-Robust Federated Machine Learning through Adaptive Model Averaging. 2019-09-11\nAn End-to-End Encrypted Neural Network for Gradient Updates Transmission in Federated Learning. 2019-08-22\nSecure Distributed On-Device Learning Networks With Byzantine Adversaries. 2019-06-03 Citation: 3\nRobust Federated Training via Collaborative Machine Teaching using Trusted Instances. 2019-05-03 Citation: 2\nDancing in the Dark: Private Multi-Party Machine Learning in an Untrusted Setting. 2018-11-23 Citation: 4\nInverting Gradients - How easy is it to break privacy in federated learning? 2020-03-31 Citation: 3\nQuantification of the Leakage in Federated Learning. 2019-10-12 Citation: 1\nPrivacy\nPractical Secure Aggregation for Federated Learning on User-Held Data. NIPS 2016 workshop Highlight: cryptology\nDifferentially Private Federated Learning: A Client Level Perspective. NIPS 2017 Workshop\nExploiting Unintended Feature Leakage in Collaborative Learning. S&P 2019. 2018-05-10 Citation: 105\n(x) Gradient-Leaks: Understanding and Controlling Deanonymization in Federated Learning. 2018-05\nA Hybrid Approach to Privacy-Preserving Federated Learning. AISec 2019. 2018-12-07 Citation: 35\nA generic framework for privacy preserving deep learning. PPML 2018. 2018-11-09 Citation: 36\nFederated Generative Privacy. IJCAI 2019 FL workshop. 2019-10-08 Citation: 4\nEnhancing the Privacy of Federated Learning with Sketching. 2019-11-05 Citaiton: 0\nFederated Learning with Bayesian Differential Privacy. 2019-11-22 Citation: 5\nHybridAlpha: An Efficient Approach for Privacy-Preserving Federated Learning. AISec 2019. 2019-12-12 https://aisec.cc/\nPrivate Federated Learning with Domain Adaptation. NeurIPS 2019 FL workshop. 2019-12-13\niDLG: Improved Deep Leakage from Gradients. 2020-01-08 Citation: 3\nAnonymizing Data for Privacy-Preserving Federated Learning. 2020-02-21\nPractical and Bilateral Privacy-preserving Federated Learning. 2020-02-23 Citation: 0\nDecentralized Policy-Based Private Analytics. 2020-03-14 Citation: 0\nFedSel: Federated SGD under Local Differential Privacy with Top-k Dimension Selection. DASFAA 2020. 2020-03-24 Citation: 0\nLearn to Forget: User-Level Memorization Elimination in Federated Learning. 2020-03-24\nLDP-Fed: Federated Learning with Local Differential Privacy. EdgeSys 2020. 2020-04-01 Researcher: Ling Liu, GaTech Citation: 1\nPrivFL: Practical Privacy-preserving Federated Regressions on High-dimensional Data over Mobile Networks. 2020-04-05 Citation: 0\nLocal Differential Privacy based Federated Learning for Internet of Things. 2020-04-09 Citation: 0\nDifferentially Private AirComp Federated Learning with Power Adaptation Harnessing Receiver Noise. 2020-04.\nDecentralized Differentially Private Segmentation with PATE. MICCAI 2020 Under Review. 2020-04\nHighlights: apply the ICLR 2017 paper \"Semisupervised knowledge transfer for deep learning from private training data\"\nEnhancing Privacy via Hierarchical Federated Learning. 2020-04-23\nPrivacy Preserving Distributed Machine Learning with Federated Learning. 2020-04-25 Citation: 0\nExploring Private Federated Learning with Laplacian Smoothing. 2020-05-01 Citation: 0\nInformation-Theoretic Bounds on the Generalization Error and Privacy Leakage in Federated Learning. 2020-05-05 Citation: 0\nEfficient Privacy Preserving Edge Computing Framework for Image Classification. 2020-05-10 Citation: 0\nA Distributed Trust Framework for Privacy-Preserving Machine Learning. 2020-06-03 Citation: 0\nSecure Byzantine-Robust Machine Learning. 2020-06-08\nARIANN: Low-Interaction Privacy-Preserving Deep Learning via Function Secret Sharing. 2020-06-08\nPrivacy For Free: Wireless Federated Learning Via Uncoded Transmission With Adaptive Power Control. 2020-06-09 Citation: 0\n(*) Distributed Differentially Private Averaging with Improved Utility and Robustness to Malicious Parties. 2020-06-12 Citation: 0\nGS-WGAN: A Gradient-Sanitized Approach for Learning Differentially Private Generators. 2020-06-15 Citation: 0\nFederated Learning with Differential Privacy:Algorithms and Performance Analysis Citation: 2\nFairness\nFair Resource Allocation in Federated Learning. ICLR 2020.\nHierarchically Fair Federated Learning\nTowards Fair and Privacy-Preserving Federated Deep Models\nInterpretability\nInterpret Federated Learning with Shapley Values.\nIncentive Mechanism\nRecord and reward federated learning contributions with blockchain. IEEE CyberC 2019\nFMore: An Incentive Scheme of Multi-dimensional Auction for Federated Learning in MEC. ICDCS 2020\nToward an Automated Auction Framework for Wireless Federated Learning Services Market\nFederated Learning for Edge Networks: Resource Optimization and Incentive Mechanism\nMotivating Workers in Federated Learning: A Stackelberg Game Perspective\nIncentive Design for Efficient Federated Learning in Mobile Networks: A Contract Theory Approach\nA Learning-based Incentive Mechanism forFederated Learning\nA Crowdsourcing Framework for On-Device Federated Learning\nSystem Challenges: communication and computational resource constrained, software and hardware heterogeneity, and FL wireless communication system\nCommunication Efficiency\nFederated Learning: Strategies for Improving Communication Efficiency Highlights: optimization\nDeep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training. ICLR 2018. 2017-12-05 Highlights: gradient compression Citation: 298\nNeurIPS 2020 submission: Artemis: tight convergence guarantees for bidirectional compression in Federated Learning. 2020-06-25 Highlights: bidirectional gradient compression\nScheduling Policy and Power Allocation for Federated Learning in NOMA Based MEC. 2020-06-21\n(x) Federated Mutual Learning. 2020-06-27 Highlights: Duplicate to Deep Mutual Learning. CVPR 2018\nA Better Alternative to Error Feedback for Communication-Efficient Distributed Learning. 2020-06-19 Researcher: Peter Richt\u00e1rik\nFederated Learning With Quantized Global Model Updates. 2020-06-18 Researcher: Mohammad Mohammadi Amiri, Princeton, Information Theory and Machine Learning Highlights: model compression\nFederated Learning with Compression: Unified Analysis and Sharp Guarantees. 2020-07-02 Highlight: non-IID, gradient compression + local SGD Researcher: Mehrdad Mahdavi, Jin Rong\u2019s PhD http://www.cse.psu.edu/~mzm616/\nEvaluating the Communication Efficiency in Federated Learning Algorithm. 2020-04-06\nDynamic Sampling and Selective Masking for Communication-Efficient Federated Learning. 2020-05-21\nTernary Compression for Communication-Efficient Federated Learning. 2020-05-07\nGradient Statistics Aware Power Control for Over-the-Air Federated Learning. 2020-05-04\nCommunication-Efficient Decentralized Learning with Sparsification and Adaptive Peer Selection. 2020-02-22\n(*) RPN: A Residual Pooling Network for Efficient Federated Learning. ECAI 2020.\nIntermittent Pulling with Local Compensation for Communication-Efficient Federated Learning. 2020-01-22\nHyper-Sphere Quantization: Communication-Efficient SGD for Federated Learning. 2019-11-12\nL-FGADMM: Layer-Wise Federated Group ADMM for Communication Efficient Decentralized Deep Learning\nGradient Sparification for Asynchronous Distributed Training. 2019-10-24\nHigh-Dimensional Stochastic Gradient Quantization for Communication-Efficient Edge Learning\nSAFA: a Semi-Asynchronous Protocol for Fast Federated Learning with Low Overhead\nDetailed comparison of communication efficiency of split learning and federated learning\nDecentralized Federated Learning: A Segmented Gossip Approach\nCommunication-Efficient Federated Deep Learning with Asynchronous Model Update and Temporally Weighted Aggregation\nOne-Shot Federated Learning\nMulti-objective Evolutionary Federated Learning\nExpanding the Reach of Federated Learning by Reducing Client Resource Requirements\nPartitioned Variational Inference: A unified framework encompassing federated and continual learning\nFedOpt: Towards communication efficiency and privacy preservation in federated learning\nA performance evaluation of federated learning algorithms\nStraggler Problem\nCoded Federated Learning. Presented at the Wireless Edge Intelligence Workshop, IEEE GLOBECOM 2019\nTurbo-Aggregate: Breaking the Quadratic Aggregation Barrier in Secure Federated Learning\nCoded Federated Computing in Wireless Networks with Straggling Devices and Imperfect CSI\nInformation-Theoretic Perspective of Federated Learning\nComputation Efficiency\nNeurIPS 2020 Submission: Distributed Learning on Heterogeneous Resource-Constrained Devices\nSplitFed: When Federated Learning Meets Split Learning\nLottery Hypothesis based Unsupervised Pre-training for Model Compression in Federated Learning\nSecure Federated Learning in 5G Mobile Networks. 2020/04\nELFISH: Resource-Aware Federated Learning on Heterogeneous Edge Devices\nAsynchronous Online Federated Learning for Edge Devices\n(*) Secure Federated Submodel Learning\nFederated Neuromorphic Learning of Spiking Neural Networks for Low-Power Edge Intelligence\nModel Pruning Enables Efficient Federated Learning on Edge Devices\nTowards Effective Device-Aware Federated Learning\nAccelerating DNN Training in Wireless Federated Edge Learning System\nSplit learning for health: Distributed deep learning without sharing raw patient data\nSmartPC: Hierarchical pace control in real-time federated learning system\nDeCaf: Iterative collaborative processing over the edge\nWireless Communication and Cloud Computing\nResearcher: H. Vincent Poor https://ee.princeton.edu/people/h-vincent-poor\nHao Ye https://scholar.google.ca/citations?user=ok7OWEAAAAAJ&hl=en\nYe Li http://liye.ece.gatech.edu/\nMix2FLD: Downlink Federated Learning After Uplink Federated Distillation With Two-Way Mixup Researcher: Mehdi Bennis, Seong-Lyun Kim\nWireless Communications for Collaborative Federated Learning in the Internet of Things\nDemocratizing the Edge: A Pervasive Edge Computing Framework\nUVeQFed: Universal Vector Quantization for Federated Learning\nFederated Deep Learning Framework For Hybrid Beamforming in mm-Wave Massive MIMO\nEfficient Federated Learning over Multiple Access Channel with Differential Privacy Constraints\nA Secure Federated Learning Framework for 5G Networks\nFederated Learning and Wireless Communications\nLightwave Power Transfer for Federated Learning-based Wireless Networks\nTowards Ubiquitous AI in 6G with Federated Learning\nOptimizing Over-the-Air Computation in IRS-Aided C-RAN Systems\nNetwork-Aware Optimization of Distributed Learning for Fog Computing\nOn the Design of Communication Efficient Federated Learning over Wireless Networks\nFederated Machine Learning for Intelligent IoT via Reconfigurable Intelligent Surface\nClient Selection and Bandwidth Allocation in Wireless Federated Learning Networks: A Long-Term Perspective\nResource Management for Blockchain-enabled Federated Learning: A Deep Reinforcement Learning Approach\nA Blockchain-based Decentralized Federated Learning Framework with Committee Consensus\nScheduling for Cellular Federated Edge Learning with Importance and Channel. 2020-04\nDifferentially Private Federated Learning for Resource-Constrained Internet of Things. 2020-03\nFederated Learning for Task and Resource Allocation in Wireless High Altitude Balloon Networks. 2020-03\nGradient Estimation for Federated Learning over Massive MIMO Communication Systems\nAdaptive Federated Learning With Gradient Compression in Uplink NOMA\nPerformance Analysis and Optimization in Privacy-Preserving Federated Learning\nEnergy-Efficient Federated Edge Learning with Joint Communication and Computation Design\nFederated Over-the-Air Subspace Learning and Tracking from Incomplete Data\nDecentralized Federated Learning via SGD over Wireless D2D Networks\nHFEL: Joint Edge Association and Resource Allocation for Cost-Efficient Hierarchical Federated Edge Learning\nFederated Learning in the Sky: Joint Power Allocation and Scheduling with UAV Swarms\nWireless Federated Learning with Local Differential Privacy\nCooperative Learning via Federated Distillation over Fading Channels\nFederated Learning under Channel Uncertainty: Joint Client Scheduling and Resource Allocation. 2020-02\nLearning from Peers at the Wireless Edge\nConvergence of Update Aware Device Scheduling for Federated Learning at the Wireless Edge\nCommunication Efficient Federated Learning over Multiple Access Channels\nConvergence Time Optimization for Federated Learning over Wireless Networks\nOne-Bit Over-the-Air Aggregation for Communication-Efficient Federated Edge Learning: Design and Convergence Analysis\nFederated Learning with Cooperating Devices: A Consensus Approach for Massive IoT Networks. IEEE Internet of Things Journal. 2020\nAsynchronous Federated Learning with Differential Privacy for Edge Intelligence\nFederated learning with multichannel ALOHA\nFederated Learning with Autotuned Communication-Efficient Secure Aggregation\nBandwidth Slicing to Boost Federated Learning in Edge Computing\nEnergy Efficient Federated Learning Over Wireless Communication Networks\nDevice Scheduling with Fast Convergence for Wireless Federated Learning\nEnergy-Aware Analog Aggregation for Federated Learning with Redundant Data\nAge-Based Scheduling Policy for Federated Learning in Mobile Edge Networks\nFederated Learning over Wireless Networks: Convergence Analysis and Resource Allocation\nFederated Learning over Wireless Networks: Optimization Model Design and Analysis\nResource Allocation in Mobility-Aware Federated Learning Networks: A Deep Reinforcement Learning Approach\nReliable Federated Learning for Mobile Networks\nFedPAQ: A Communication-Efficient Federated Learning Method with Periodic Averaging and Quantization\nActive Federated Learning\nCell-Free Massive MIMO for Wireless Federated Learning\nA Joint Learning and Communications Framework for Federated Learning over Wireless Networks\nOn Safeguarding Privacy and Security in the Framework of Federated Learning\nOn Safeguarding Privacy and Security in the Framework of Federated Learning\nHierarchical Federated Learning Across Heterogeneous Cellular Networks\nFederated Learning for Wireless Communications: Motivation, Opportunities and Challenges\nScheduling Policies for Federated Learning in Wireless Networks\nFederated Learning with Additional Mechanisms on Clients to Reduce Communication Costs\nFederated Learning over Wireless Fading Channels\nEnergy-Efficient Radio Resource Allocation for Federated Edge Learning\nMobile Edge Computing, Blockchain and Reputation-based Crowdsourcing IoT Federated Learning: A Secure, Decentralized and Privacy-preserving System\nActive Learning Solution on Distributed Edge Computing\nFast Uplink Grant for NOMA: a Federated Learning based Approach\nMachine Learning at the Wireless Edge: Distributed Stochastic Gradient Descent Over-the-Air\nFederated Learning via Over-the-Air Computation\nBroadband Analog Aggregation for Low-Latency Federated Edge Learning\nFederated Echo State Learning for Minimizing Breaks in Presence in Wireless Virtual Reality Networks\nJoint Service Pricing and Cooperative Relay Communication for Federated Learning\nIn-Edge AI: Intelligentizing Mobile Edge Computing, Caching and Communication by Federated Learning\nAsynchronous Task Allocation for Federated and Parallelized Mobile Edge Learning\n[CoLearn: enabling federated learning in MUD-compliant IoT edge networks](CoLearn: enabling federated learning in MUD-compliant IoT edge networks)\nFL System Design\nTowards Federated Learning at Scale: System Design\nFedML: A Research Library and Benchmark for Federated Machine Learning\nA Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection\nFLeet: Online Federated Learning via Staleness Awareness and Performance Prediction Researcher: Georgios Damaskinos, MLSys, https://people.epfl.ch/georgios.damaskinos?lang=en\nHeterogeneity-Aware Federated Learning Researcher: Mengwei Xu, PKU\nResponsive Web User Interface to Recover Training Data from User Gradients in Federated Learning https://ldp-machine-learning.herokuapp.com/\nDecentralised Learning from Independent Multi-Domain Labels for Person Re-Identification\n[startup] Industrial Federated Learning -- Requirements and System Design\n(startup) Federated Learning and Differential Privacy: Software tools analysis, the Sherpa.ai FL framework and methodological guidelines for preserving data privacy\n(FL startup: Tongdun, HangZhou, China) Knowledge Federation: A Unified and Hierarchical Privacy-Preserving AI Framework. 2020-02\n(*) TiFL: A Tier-based Federated Learning System. HPDC 2020 (High-Performance Parallel and Distributed Computing).\nFMore: An Incentive Scheme of Multi-dimensional Auction for Federated Learning in MEC. ICDCS 2020 (2020 International Conference on Distributed Computing Systems)\nAdaptive Gradient Sparsification for Efficient Federated Learning: An Online Learning Approach. ICDCS 2020 (2020 International Conference on Distributed Computing Systems)\nQuantifying the Performance of Federated Transfer Learning\nELFISH: Resource-Aware Federated Learning on Heterogeneous Edge Devices\nPrivacy is What We Care About: Experimental Investigation of Federated Learning on Edge Devices\nSubstra: a framework for privacy-preserving, traceable and collaborative Machine Learning\nBAFFLE : Blockchain Based Aggregator Free Federated Learning\nEdge AIBench: Towards Comprehensive End-to-end Edge Computing Benchmarking\nFunctional Federated Learning in Erlang (ffl-erl)\nHierTrain: Fast Hierarchical Edge AI Learning With Hybrid Parallelism in Mobile-Edge-Cloud Computing\nModels and Applications\nModels\nGraph Neural Networks\nPeer-to-peer federated learning on graphs\nTowards Federated Graph Learning for Collaborative Financial Crimes Detection\nA Graph Federated Architecture with Privacy Preserving Learning\nFederated Myopic Community Detection with One-shot Communication\nFederated Dynamic GNN with Secure Aggregation\nPrivacy-Preserving Graph Neural Network for Node Classification\nASFGNN: Automated Separated-Federated Graph Neural Network\nGraphFL: A Federated Learning Framework for Semi-Supervised Node Classification on Graphs\nFedGNN: Federated Graph Neural Network for Privacy-Preserving Recommendation\nFedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks\nFL-AGCNS: Federated Learning Framework for Automatic Graph Convolutional Network Search\nCluster-driven Graph Federated Learning over Multiple Domains\nFedGL: Federated Graph Learning Framework with Global Self-Supervision\nFederated Graph Learning -- A Position Paper\nSpreadGNN: Serverless Multi-task Federated Learning for Graph Neural Networks\nCross-Node Federated Graph Neural Network for Spatio-Temporal Data Modeling\nA Vertical Federated Learning Framework for Graph Convolutional Network\nFederated Graph Classification over Non-IID Graphs\nSubgraph Federated Learning with Missing Neighbor Generation\nFederated Learning on Knowledge Graphs\nFedE: Embedding Knowledge Graphs in Federated Setting\nImproving Federated Relational Data Modeling via Basis Alignment and Weight Penalty\nFederated Knowledge Graphs Embedding\nGenerative Models (GAN, Bayesian Generative Models, etc)\nDiscrete-Time Cox Models\nGenerative Models for Effective ML on Private, Decentralized Datasets. Google. ICLR 2020 Citation: 8\nMD-GAN: Multi-Discriminator Generative Adversarial Networks for Distributed Datasets. 2018-11-09\n(GAN) Federated Generative Adversarial Learning. 2020-05-07 Citation: 0\nDifferentially Private Data Generative Models\nGRAFFL: Gradient-free Federated Learning of a Bayesian Generative Model\nVAE (Variational Autoencoder)\n(VAE) An On-Device Federated Learning Approach for Cooperative Anomaly Detection\nMF (Matrix Factorization)\nSecure Federated Matrix Factorization\n(Clustering) Federated Clustering via Matrix Factorization Models: From Model Averaging to Gradient Sharing\nPrivacy Threats Against Federated Matrix Factorization\nGBDT (Gradient Boosting Decision Trees)\nPractical Federated Gradient Boosting Decision Trees. AAAI 2020.\nFederated Extra-Trees with Privacy Preserving\nSecureGBM: Secure Multi-Party Gradient Boosting\nFederated Forest\nThe Tradeoff Between Privacy and Accuracy in Anomaly Detection Using Federated XGBoost\nOther Model\nPrivacy Preserving QoE Modeling using Collaborative Learning\nDistributed Dual Coordinate Ascent in General Tree Networks and Its Application in Federated Learning\nNatural language Processing\nFederated pretraining and fine tuning of BERT using clinical notes from multiple silos\nFederated Learning for Mobile Keyboard Prediction\nFederated Learning for Keyword Spotting\ngenerative sequence models (e.g., language models)\nPretraining Federated Text Models for Next Word Prediction\nFedNER: Privacy-preserving Medical Named Entity Recognition with Federated Learning. MSRA. 2020-03.\nFederated Learning of N-gram Language Models. Google. ACL 2019.\nFederated User Representation Learning\nTwo-stage Federated Phenotyping and Patient Representation Learning\nFederated Learning for Emoji Prediction in a Mobile Keyboard\nFederated AI lets a team imagine together: Federated Learning of GANs\nFederated Learning Of Out-Of-Vocabulary Words\nLearning Private Neural Language Modeling with Attentive Aggregation\nApplied Federated Learning: Improving Google Keyboard Query Suggestions\nFederated Learning for Ranking Browser History Suggestions\nComputer Vision\nFederated Face Anti-spoofing\n(*) Federated Visual Classification with Real-World Data Distribution. MIT. ECCV 2020. 2020-03\nFedVision: An Online Visual Object Detection Platform Powered by Federated Learning\nHealth Care:\nMulti-Institutional Deep Learning Modeling Without Sharing Patient Data: A Feasibility Study on Brain Tumor Segmentation\nFederated Learning in Distributed Medical Databases: Meta-Analysis of Large-Scale Subcortical Brain Data\nPrivacy-Preserving Technology to Help Millions of People: Federated Prediction Model for Stroke Prevention\nA Federated Learning Framework for Healthcare IoT devices Keywords: Split Learning + Sparsification\nFederated Transfer Learning for EEG Signal Classification\nThe Future of Digital Health with Federated Learning\nAnonymizing Data for Privacy-Preserving Federated Learning. ECAI 2020.\nFederated machine learning with Anonymous Random Hybridization (FeARH) on medical records\nStratified cross-validation for unbiased and privacy-preserving federated learning\nMulti-site fMRI Analysis Using Privacy-preserving Federated Learning and Domain Adaptation: ABIDE Results\nLearn Electronic Health Records by Fully Decentralized Federated Learning\nPreserving Patient Privacy while Training a Predictive Model of In-hospital Mortality\nFederated Learning for Healthcare Informatics\nFederated and Differentially Private Learning for Electronic Health Records\nA blockchain-orchestrated Federated Learning architecture for healthcare consortia\nFederated Uncertainty-Aware Learning for Distributed Hospital EHR Data\nStochastic Channel-Based Federated Learning for Medical Data Privacy Preserving\nDifferential Privacy-enabled Federated Learning for Sensitive Health Data\nLoAdaBoost: Loss-based AdaBoost federated machine learning with reduced computational complexity on IID and non-IID intensive care data\nPrivacy Preserving Stochastic Channel-Based Federated Learning with Neural Network Pruning\nConfederated Machine Learning on Horizontally and Vertically Separated Medical Data for Large-Scale Health System Intelligence\nPrivacy-preserving Federated Brain Tumour Segmentation\nHHHFL: Hierarchical Heterogeneous Horizontal Federated Learning for Electroencephalography\nFedHealth: A Federated Transfer Learning Framework for Wearable Healthcare\nPatient Clustering Improves Efficiency of Federated Machine Learning to predict mortality and hospital stay time using distributed Electronic Medical Records\nLoAdaBoost:Loss-Based AdaBoost Federated Machine Learning on medical Data\nFADL:Federated-Autonomous Deep Learning for Distributed Electronic Health Record\nTransportation:\nFederated Learning for Vehicular Networks\nTowards Federated Learning in UAV-Enabled Internet of Vehicles: A Multi-Dimensional Contract-Matching Approach\nFederated Learning Meets Contract Theory: Energy-Efficient Framework for Electric Vehicle Networks\nBeyond privacy regulations: an ethical approach to data usage in transportation. TomTom. 2020-04-01\nPrivacy-preserving Traffic Flow Prediction: A Federated Learning Approach\nCommunication-Efficient Massive UAV Online Path Control: Federated Learning Meets Mean-Field Game Theory. 2020-03\nFedLoc: Federated Learning Framework for Data-Driven Cooperative Localization and Location Data Processing. 2020-03\nPractical Privacy Preserving POI Recommendation\nFederated Learning for Localization: A Privacy-Preserving Crowdsourcing Method\nFederated Transfer Reinforcement Learning for Autonomous Driving\nEnergy Demand Prediction with Federated Learning for Electric Vehicle Networks\nDistributed Federated Learning for Ultra-Reliable Low-Latency Vehicular Communications\nFederated Learning for Ultra-Reliable Low-Latency V2V Communications\nFederated Learning in Vehicular Edge Computing: A Selective Model Aggregation Approach\nRecommendation System\n(*) Federated Multi-view Matrix Factorization for Personalized Recommendations\nRobust Federated Recommendation System\nFederated Recommendation System via Differential Privacy\nFedRec: Privacy-Preserving News Recommendation with Federated Learning. MSRA. 2020-03\nFederating Recommendations Using Differentially Private Prototypes\nMeta Matrix Factorization for Federated Rating Predictions\nFederated Hierarchical Hybrid Networks for Clickbait Detection\nFederated Collaborative Filtering for Privacy-Preserving Personalized Recommendation System\nSpeech Recognition\nTraining Keyword Spotting Models on Non-IID Data with Federated Learning\nFinance\nFedCoin: A Peer-to-Peer Payment System for Federated Learning\nTowards Federated Graph Learning for Collaborative Financial Crimes Detection\nSmart City\nCloud-based Federated Boosting for Mobile Crowdsensing\nExploiting Unlabeled Data in Smart Cities using Federated Learning\nRobotics\nFederated Imitation Learning: A Privacy Considered Imitation Learning Framework for Cloud Robotic Systems with Heterogeneous Sensor Data\nLifelong Federated Reinforcement Learning: A Learning Architecture for Navigation in Cloud Robotic Systems\nNetworking\nA Federated Learning Approach for Mobile Packet Classification\nBlockchain\nBlockchained On-Device Federated Learning\nRecord and reward federated learning contributions with blockchain\nOther\nBoosting Privately: Privacy-Preserving Federated Extreme Boosting for Mobile Crowdsensing\nSelf-supervised audio representation learning for mobile devices\nCombining Federated and Active Learning for Communication-efficient Distributed Failure Prediction in Aeronautics\nPMF: A Privacy-preserving Human Mobility Prediction Framework via Federated Learning\nFederated Multi-task Hierarchical Attention Model for Sensor Analytics\nD\u00cfoT: A Federated Self-learning Anomaly Detection System for IoT\nBenchmark, Dataset and Survey\nBenchmark and Dataset\nThe OARF Benchmark Suite: Characterization and Implications for Federated Learning Systems\nEvaluation Framework For Large-scale Federated Learning\n(*) PrivacyFL: A simulator for privacy-preserving and secure federated learning. MIT CSAIL.\nRevocable Federated Learning: A Benchmark of Federated Forest\nReal-World Image Datasets for Federated Learning\nLEAF: A Benchmark for Federated Settings\nFunctional Federated Learning in Erlang (ffl-erl)\nSurvey\nA Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection\nResearcher: Bingsheng He, NUS Qinbin Li, PhD, NUS, HKUST\nSECure: A Social and Environmental Certificate for AI Systems\nFrom Federated Learning to Fog Learning: Towards Large-Scale Distributed Machine Learning in Heterogeneous Wireless Networks\nFederated Learning for 6G Communications: Challenges, Methods, and Future Directions\nA Review of Privacy Preserving Federated Learning for Private IoT Analytics\nSurvey of Personalization Techniques for Federated Learning. 2020-03-19\nThreats to Federated Learning: A Survey\nTowards Utilizing Unlabeled Data in Federated Learning: A Survey and Prospective\nFederated Learning for Resource-Constrained IoT Devices: Panoramas and State-of-the-art\nAdvances and Open Problems in Federated Learning\nPrivacy-Preserving Blockchain Based Federated Learning with Differential Data Sharing\nAn Introduction to Communication Efficient Edge Machine Learning\nFederated Learning for Healthcare Informatics\nFederated Learning for Coalition Operations\nFederated Learning in Mobile Edge Networks: A Comprehensive Survey\nFederated Learning: Challenges, Methods, and Future Directions\nA Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection\nFederated Machine Learning: Concept and Applications\nNo Peek: A Survey of private distributed deep learning\nCommunication-Efficient Edge AI: Algorithms and Systems",
      "link": "https://github.com/chaoyanghe/Awesome-Federated-Learning"
    },
    {
      "autor": "semantic-segmentation-editor",
      "date": "NaN",
      "content": "Semantic Segmentation Editor\nA web based labeling tool for creating AI training data sets (2D and 3D). The tool has been developed in the context of autonomous driving research. It supports images (.jpg or .png) and point clouds (.pcd). It is a Meteor app developed with React, Paper.js and three.js.\nLatest changes\nVersion 1.5: Provide a Docker image and update to Meteor 1.10\nVersion 1.4: Support for RGB pointclouds (thanks @Gekk0r)\nVersion 1.3: Improve pointcloud labeling: bug fixes and performance improvement (labeling a 1M pointcloud is now possible)\nVersion 1.2.2: Breaking change: exported point cloud coordinates are no longer translated (thanks @hetzge)\nVersion 1.2.0: Support for binary and binary compressed point clouds (thanks @CecilHarvey)\nBitmap Image Editor\n\ud83c\udfa5 VIDEO: Bitmap labeling overview\n\ud83d\ude80 DEMO: Bitmap editor\nPCD Point Cloud Editor\n\ud83c\udfa5 VIDEO: Point cloud labeling overview\n\ud83d\ude80 DEMO: Point cloud editor\nHow to run\nUsing Docker Compose\nDownload the docker compose stack file (sse-docker-stack.yml)\nSet the folder that contains bitmap and point cloud files (YOUR_IMAGES_PATH) and run the tool using docker-compose\nThe tool runs by default on port 80, you can change the mapping in sse-docker-stack.yml\nwget https://raw.githubusercontent.com/Hitachi-Automotive-And-Industry-Lab/semantic-segmentation-editor/master/sse-docker-stack.yml\nwget https://raw.githubusercontent.com/Hitachi-Automotive-And-Industry-Lab/semantic-segmentation-editor/master/settings.json\nMETEOR_SETTINGS=$(cat ./settings.json) SSE_IMAGES=YOUR_IMAGES_PATH docker-compose -f stack.yml up\n(Optional) You can modify settings.json to customize classes data.\nRunning from source\nInstall Meteor (OSX or Linux)\ncurl https://install.meteor.com/ | sh\nor download Meteor Windows Installer\nDownload and unzip latest version from here\nStart the application\ncd semantic-segmentation-editor-x.x.x\nmeteor npm install\nmeteor npm start\nThe editor will run by default on http://localhost:3000\n(Optional) Edit settings.json\nBy default, images are served from your_home_dir/sse-images and pointcloud binary segmentation data are stored in your_home_dir/sse-internal. You can configure these folders in settings.json by modifying images-folder and internal-folder properties. On Windows, use '/' separators, example c:/Users/john/images\nCheck Meteor Environment Variables to configure your app (MONGO_URL, DISABLE_WEBSOCKETS, etc...)\nConfiguration File: settings.json\n{\n\"configuration\": {\n\"images-folder\": \"/mnt/images\", // The root folder containing images and PCD files\n\"internal-folder\": \"/mnt/pointcloud_data\" // Segmentation data (only 3D) will be stored in this folder\n},\n// The different sets of classes available in the tool\n// For object classes, only the 'label' field is mandatory\n// The icon field can be set with an icon from the mdi-material-ui package\n\"sets-of-classes\": [\n{\n\"name\": \"Cityscapes\", \"objects\": [\n{\"label\": \"VOID\", \"color\": \"#CFCFCF\"},\n{\"label\": \"Road\", \"color\": \"#804080\", \"icon\": \"Road\"},\n{\"label\": \"Sidewalk\", \"color\": \"#F423E8\", \"icon\": \"NaturePeople\"},\n{\"label\": \"Parking\", \"color\": \"#FAAAA0\", \"icon\": \"Parking\"},\n{\"label\": \"Rail Track\", \"color\": \"#E6968C\", \"icon\": \"Train\"},\n{\"label\": \"Person\", \"color\": \"#DC143C\", \"icon\": \"Walk\"},\n{\"label\": \"Rider\", \"color\": \"#FF0000\", \"icon\": \"Motorbike\"},\n{\"label\": \"Car\", \"color\": \"#0000E8\", \"icon\": \"Car\"}\n},\n{ ... }\n]\n}\nHow to use\nThe editor is built around 3 different screens:\nThe file navigator let's you browse available files to select a bitmap images or a point cloud for labeling\nThe bitmap image editor is dedicated to the labeling of jpg and png files by drawing polygons\nThe point cloud editor is dedicated to the labeling of point clouds by creating objects made of subsets of 3D points\nUsing the bitmap image editor\nThere are several tools to create labeling polygons:\nPolygon Drawing Tool (P)\nClick and/or drag to create points\nType ESC to remove last created points in reverse order\nDrag the mouse pointer or hold Shift to create a complex polygon without having to click for each point\nType ENTER or double click the first point to close the polygon\nMagic Tool (A)\nCreate a polygon automatically using contrast threshold detection\nThis tool is only useful to draw the outline of objects that have sharp contrasted edges (examples: sky, lane marking)\nClick inside the area you want to outline, then adjusts any sliders on the right to adjust the result\nType ENTER to validate the result\nManipulation Tool (Alt)\nSelect, move and add point(s) to existing polygons\nClick inside a polygon to select it\nClick a point to select it\nDraw a lasso around multiple points to select them\nDrag a point with the mouse to move it\nHold Shift to separate points that belongs to more than one polygon\nClick the line of a polygon to create a new point and drag the newly created point to place it\nCutting/Expanding Tool (C)\nModify the shape of an existing polygon\nSelect the polygon you want to modify\nDraw a line starting and ending on the outline of a polygon\nThe new line replace the existing path between starting and ending points\nThe resulting shape is always the largest one\nContiguous Polygon Tool (F)\nCreate contiguous polygons easily\nStart a new polygon with the Polygon Drawing Tool\nCreate the starting point by snapping to the outline of the polygon you want to workaround\nCreate the ending point by snapping to another outline, at this point you must have a straight line crossing one or more existing polygons\nHit F one or several times to choose what workaround path to use\nUsing the point cloud editor\nMouse left button: Rotate the point cloud around the current focused point (the center of the point cloud by default), click on a single point to add it to the current selection\nMouse wheel: Zoom in/out\nMouse middle button (or Ctrl+Click): Change the target of the -----> camera !!! \nMouse right button: Used to select multiple points at the same time depending on the current Selection Tool and Selection Mode.\nArrow keys: Move through the scene\nPCD support\nSupported input PCD format: ASCII, Binary and Binary compressed\nSupported input fields: x, y, z, label (optional integer), rgb (optional integer)\nOutput PCD format is ASCII with fields x, y, z, label, object and rgb (if available)\nAPI Endpoints\n/api/listing: List all annotated images\n/api/json/[PATH_TO_FILE]: (2D only) Get the polygons and other data for that file\n/api/pcdtext/[PATH_TO_FILE]: (3D only) Get the labeling of a pcd file using 2 addditional columns: label and object\n/api/pcdfile/[PATH_TO_FILE]: (3D only) The same but returned as \"plain/text\" attachment file download",
      "link": "https://github.com/Hitachi-Automotive-And-Industry-Lab/semantic-segmentation-editor"
    },
    {
      "autor": "Face-Mask-Detection",
      "date": "NaN",
      "content": "Face Mask Detection\nFace Mask Detection System built with OpenCV, Keras/TensorFlow using Deep Learning and Computer Vision concepts in order to detect face masks in static images as well as in real-time video streams.\n\ud83d\udc47 Support me here!\n\ud83d\ude07 Motivation\nAmid the ongoing COVID-19 pandemic, there are no efficient face mask detection applications which are now in high demand for transportation means, densely populated areas, residential districts, large-scale manufacturers and other enterprises to ensure safety. The absence of large datasets of \u2018with_mask\u2019 images has made this task cumbersome and challenging.\nPPT and Project Report sharing costs \u20b91000 ($15)\nIf interested, contact me at chandrikadeb7@gmail.com\n\ud83c\udf1f Purchase at a Discounted Rate\n\u231b Project Demo\n\ud83c\udfa5 YouTube Demo Link\n\ud83d\udcbb Dev Link\n\u26a0\ufe0f TechStack/framework used\nOpenCV\nCaffe-based face detector\nKeras\nTensorFlow\nMobileNetV2\n\u2b50 Features\nOur face mask detector doesn't use any morphed masked images dataset and the model is accurate. Owing to the use of MobileNetV2 architecture, it is computationally efficient, thus making it easier to deploy the model to embedded systems (Raspberry Pi, Google Coral, etc.).\nThis system can therefore be used in real-time applications which require face-mask detection for safety purposes due to the outbreak of Covid-19. This project can be integrated with embedded systems for application in airports, railway stations, offices, schools, and public places to ensure that public safety guidelines are followed.\n\ud83d\udcc1 Dataset\nThe dataset used can be downloaded here - Click to Download\nThis dataset consists of 4095 images belonging to two classes:\nwith_mask: 2165 images\nwithout_mask: 1930 images\nThe images used were real images of faces wearing masks. The images were collected from the following sources:\nBing Search API (See Python script)\nKaggle datasets\nRMFD dataset (See here)\n\ud83d\udd11 Prerequisites\nAll the dependencies and required libraries are included in the file requirements.txt See here\n\ud83d\ude80 Installation\nClone the repo\n$ git clone https://github.com/chandrikadeb7/Face-Mask-Detection.git\nChange your directory to the cloned repo\n$ cd Face-Mask-Detection\nCreate a Python virtual environment named 'test' and activate it\n$ virtualenv test\n$ source test/bin/activate\nNow, run the following command in your Terminal/Command Prompt to install the libraries required\n$ pip3 install -r requirements.txt\n\ud83d\udca1 Working\nOpen terminal. Go into the cloned project directory and type the following command:\n$ python3 train_mask_detector.py --dataset dataset\nTo detect face masks in an image type the following command:\n$ python3 detect_mask_image.py --image images/pic1.jpeg\nTo detect face masks in real-time video streams type the following command:\n$ python3 detect_mask_video.py\n\ud83d\udd11 Results\nOur model gave 98% accuracy for Face Mask Detection after training via tensorflow-gpu==2.5.0\nWe got the following accuracy/loss training curve plot\nStreamlit app\nFace Mask Detector webapp using Tensorflow & Streamlit\ncommand\n$ streamlit run app.py\nImages\nUpload Images\nResults\n\ud83d\udc4f And it's done!\nFeel free to mail me for any doubts/query \ud83d\udce7 chandrikadeb7@gmail.com\nInternet of Things Device Setup\nExpected Hardware\nRaspberry Pi 4 4GB with a case\n5MP OV5647 PiCamera from Arducam\nGetting Started\nSetup the Raspberry Pi case and Operating System by following the Getting Started section on page 3 at documentation/CanaKit-Raspberry-Pi-Quick-Start-Guide-4.0.pdf or https://www.canakit.com/Media/CanaKit-Raspberry-Pi-Quick-Start-Guide-4.0.pdf\nWith NOOBS, use the recommended operating system\nSetup the PiCamera\nAssemble the PiCamera case from Arducam using documentation/Arducam-Case-Setup.pdf or https://www.arducam.com/docs/-----> camera !!! s-for-raspberry-pi/native-raspberry-pi------> camera !!! s/5mp-ov5647------> camera !!! s/\nAttach your PiCamera module to the Raspberry Pi and enable the -----> camera !!! \nRaspberry Pi App Installation & Execution\nRun these commands after cloning the project\nCommands Time to completion\nsudo apt install -y libatlas-base-dev liblapacke-dev gfortran 1min\nsudo apt install -y libhdf5-dev libhdf5-103 1min\npip3 install -r requirements.txt 1-3 mins\nwget \"https://raw.githubusercontent.com/PINTO0309/Tensorflow-bin/master/tensorflow-2.4.0-cp37-none-linux_armv7l_download.sh\" less than 10 secs\n./tensorflow-2.4.0-cp37-none-linux_armv7l_download.sh less than 10 secs\npip3 install tensorflow-2.4.0-cp37-none-linux_armv7l.whl 1-3 mins\n\ud83c\udfc6 Awards\nAwarded Runners Up position in Amdocs Innovation India ICE Project Fair\n\ud83d\ude4b Cited by:\nhttps://osf.io/preprints/3gph4/\nhttps://link.springer.com/chapter/10.1007/978-981-33-4673-4_49\nhttps://ieeexplore.ieee.org/abstract/document/9312083/\nhttps://link.springer.com/chapter/10.1007/978-981-33-4673-4_48\nhttps://www.researchgate.net/profile/Akhyar_Ahmed/publication/344173985_Face_Mask_Detector/links/5f58c00ea6fdcc9879d8e6f7/Face-Mask-Detector.pdf\n\ud83d\udc4f Appreciation\nSelected in Devscript Winter Of Code\nSelected in Script Winter Of Code\nSeleted in Student Code-in\n\ud83d\udc4d Credits\nhttps://www.pyimagesearch.com/\nhttps://www.tensorflow.org/tutorials/images/transfer_learning\n\ud83e\udd1d Contribution\nPlease read the Contribution Guidelines here\nFeel free to file a new issue with a respective title and description on the the Face-Mask-Detection repository. If you already found a solution to your problem, I would love to review your pull request!\n\ud83e\udd1d Our Contributors\n\ud83d\udc40 Code of Conduct\nYou can find our Code of Conduct here.\n\ud83d\ude4b Citation\nYou are allowed to cite any part of the code or our dataset. You can use it in your Research Work or Project. Remember to provide credit to the Maintainer Chandrika Deb by mentioning a link to this repository and her GitHub Profile.\nFollow this format:\nAuthor's name - Chandrika Deb\nDate of publication or update in parentheses.\nTitle or description of document.\nURL.\n\u2764\ufe0f Owner\nMade with \u2764\ufe0f by Chandrika Deb\n\ud83d\udc40 License\nMIT \u00a9 Chandrika Deb",
      "link": "https://github.com/chandrikadeb7/Face-Mask-Detection"
    },
    {
      "autor": "data-augmentation-review",
      "date": "NaN",
      "content": "Data augmentation\nList of useful data augmentation resources. You will find here some links to more or less popular github repos \u2728, libraries, papers \ud83d\udcda and other information.\nDo you like it? Feel free to \u2b50 ! Feel free to pull request!\nIntroduction\nRepositories\nPapers\nAutoAugment - repos and papers\nOther - challenges, workshops, tutorials, books\nIntroduction\nData augmentation can be simply described as any method that makes our dataset larger. To create more images for example, we could zoom the in and save a result, we could change the brightness of the image or rotate it. To get bigger sound dataset we could try raise or lower the pitch of the audio sample or slow down/speed up. Example data augmentation techniques are presented on the diagram below.\nDATA AUGMENTATION\nImages\nAffine transformations\nRotation\nScaling\nRandom cropping\nReflection\nElastic transformations\nContrast shift\nBrightness shift\nBlurring\nChannel shuffle\nAdvanced transformations\nRandom erasing\nAdding rain effects, sun flare...\nImage blending\nNeural-based transformations\nAdversarial noise\nNeural Style Transfer\nGenerative Adversarial Networks\nAudio\nNoise injection\nTime shift\nTime stretching\nRandom cropping\nPitch scaling\nDynamic range compression\nSimple gain\nEqualization\nNatural Language Processing\nThesaurus\nText Generation\nBack Translation\nWord Embeddings\nContextualized Word Embeddings\nVoice conversion\nTime Series Data Augmentation\nBasic approaches\nWarping\nJittering\nPerturbing\nAdvanced approches\nEmbedding space\nGAN/Adversarial\nRL/Meta-Learning\nAutoAugment\nIf you wish to cite us, you can cite followings paper of your choice: Style transfer-based image synthesis as an efficient regularization technique in deep learning or Data augmentation for improving deep learning in image classification problem.\nRepositories\nComputer vision\n- albumentations is a python library with a set of useful, large and diverse data augmentation methods. It offers over 30 different types of augmentations, easy and ready to use. Moreover, as the authors prove, the library is faster than other libraries on most of the transformations.\nExample jupyter notebooks:\nAll in one showcase notebook\nClassification,\nObject detection, image segmentation and keypoints\nOthers - Weather transforms , Serialization, Replay/Deterministic mode, Non-8-bit images\nExample tranformations:\n- imgaug - is another very useful and widely used python library. As authors describe: it helps you with augmenting images for your machine learning projects. It converts a set of input images into a new, much larger set of slightly altered images. It offers many augmentation techniques such as affine transformations, perspective transformations, contrast changes, gaussian noise, dropout of regions, hue/saturation changes, cropping/padding, blurring.\nExample jupyter notebooks:\nLoad and Augment an Image\nMulticore Augmentation\nAugment and work with: Keypoints/Landmarks, Bounding Boxes, Polygons, Line Strings, Heatmaps, Segmentation Maps\nExample tranformations:\n- UDA - a simple data augmentation tool for image files, intended for use with machine learning data sets. The tool scans a directory containing image files, and generates new images by performing a specified set of augmentation operations on each file that it finds. This process multiplies the number of training examples that can be used when developing a neural network, and should significantly improve the resulting network's performance, particularly when the number of training examples is relatively small.\nThe details are avaible here: UNSUPERVISED DATA AUGMENTATION FOR CONSISTENCY TRAINING\n- Data augmentation for object detection - Repository contains a code for the paper space tutorial series on adapting data augmentation methods for object detection tasks. They support a lot of data augmentations, like Horizontal Flipping, Scaling, Translation, Rotation, Shearing, Resizing.\n- FMix - Understanding and Enhancing Mixed Sample Data Augmentation This repository contains the official implementation of the paper 'Understanding and Enhancing Mixed Sample Data Augmentation'\n- Super-AND - This repository is the Pytorch implementation of \"A Comprehensive Approach to Unsupervised Embedding Learning based on AND Algorithm.\n- vidaug - This python library helps you with augmenting videos for your deep learning architectures. It converts input videos into a new, much larger set of slightly altered videos.\n- Image augmentor - This is a simple python data augmentation tool for image files, intended for use with machine learning data sets. The tool scans a directory containing image files, and generates new images by performing a specified set of augmentation operations on each file that it finds. This process multiplies the number of training examples that can be used when developing a neural network, and should significantly improve the resulting network's performance, particularly when the number of training examples is relatively small.\n- torchsample - this python package provides High-Level Training, Data Augmentation, and Utilities for Pytorch. This toolbox provides data augmentation methods, regularizers and other utility functions. These transforms work directly on torch tensors:\nCompose()\nAddChannel()\nSwapDims()\nRangeNormalize()\nStdNormalize()\nSlice2D()\nRandomCrop()\nSpecialCrop()\nPad()\nRandomFlip()\n- Random erasing - The code is based on the paper: https://arxiv.org/abs/1708.04896. The Absract:\nIn this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: this https URL.\n- data augmentation in C++ - Simple image augmnetation program transform input images with rotation, slide, blur, and noise to create training data of image recognition.\n- Data augmentation with GANs - This repository contain files with Generative Adversarial Network, which can be used to successfully augment the dataset. This is an implementation of DAGAN as described in https://arxiv.org/abs/1711.04340. The implementation provides data loaders, model builders, model trainers, and synthetic data generators for the Omniglot and VGG-Face datasets.\n- Joint Discriminative and Generative Learning - This repo is for Joint Discriminative and Generative Learning for Person Re-identification (CVPR2019 Oral). The author proposes an end-to-end training network that simultaneously generates more training samples and conducts representation learning. Given N real samples, the network could generate O(NxN) high-fidelity samples.\n[Project] [Paper] [YouTube] [Bilibili] [Poster] [Supp]\n- White-Balance Emulator for Color Augmentation - Our augmentation method can accurately emulate realistic color constancy degradation. Existing color augmentation methods often generate unrealistic colors which rarely happen in reality (e.g., green skin or purple grass). More importantly, the visual appearance of existing color augmentation techniques does not well represent the color casts produced by incorrect WB applied onboard cameras, as shown below. [python] [matlab]\n- DocCreator - is an open source, cross-platform software allowing to generate synthetic document images and the accompanying groundtruth. Various degradation models can be applied on original document images to create virtually unlimited amounts of different images.\nA multi-platform and open-source software able to create synthetic image documents with ground truth.\n- OnlineAugment - implementation in PyTorch\nMore automatic than AutoAugment and related\nTowards fully automatic (STN and VAE, No need to specify the image primitives).\nBroad domains (natural, medical images, etc).\nDiverse tasks (classification, segmentation, etc).\nEasy to use\nOne-stage training (user-friendly).\nSimple code (single GPU training, no need for parallel optimization).\nOrthogonal to AutoAugment and related\nOnline v.s. Offline (Joint optimization, no expensive offline policy searching).\nState-of-the-art performance (in combination with AutoAugment).\n- Augraphy - is a Python library that creates multiple copies of original documents though an augmentation pipeline that randomly distorts each copy -- degrading the clean version into dirty and realistic copies rendered through synthetic paper printing, faxing, scanning and copy machine processes.\n- Data Augmentation optimized for GAN (DAG) - implementation in PyTorch and Tensorflow\nDAG-GAN provide simple implementations of the DAG modules in both PyTorch and TensorFlow, which can be easily integrated into any GAN models to improve the performance, especially in the case of limited data. We only illustrate some augmentation techniques (rotation, cropping, flipping, ...) as discussed in our paper, but our DAG is not limited to these augmentations. The more augmentation to be used, the better improvements DAG enhances the GAN models. It is also easy to design your augmentations within the modules. However, there may be a trade-off between the numbers of many augmentations to be used in DAG and the computational cost.\nNatural Language Processing\n- Contextual data augmentation - Contextual augmentation is a domain-independent data augmentation for text classification tasks. Texts in supervised dataset are augmented by replacing words with other words which are predicted by a label-conditioned bi-directional language model.\nThis repository contains a collection of scripts for an experiment of Contextual Augmentation.\n- nlpaug - This python library helps you with augmenting nlp for your machine learning projects. Visit this introduction to understand about Data Augmentation in NLP. Augmenter is the basic element of augmentation while Flow is a pipeline to orchestra multi augmenter together.\nFeatures:\nGenerate synthetic data for improving model performance without manual effort\nSimple, easy-to-use and lightweight library. Augment data in 3 lines of code\nPlug and play to any neural network frameworks (e.g. PyTorch, TensorFlow)\nSupport textual and audio input\n- EDA NLP - EDA is an easy data augmentation techniques for boosting performance on text classification tasks. These are a generalized set of data augmentation techniques that are easy to implement and have shown improvements on five NLP classification tasks, with substantial improvements on datasets of size N < 500. While other techniques require you to train a language model on an external dataset just to get a small boost, we found that simple text editing operations using EDA result in good performance gains. Given a sentence in the training set, we perform the following operations:\nSynonym Replacement (SR): Randomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random.\nRandom Insertion (RI): Find a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this n times.\nRandom Swap (RS): Randomly choose two words in the sentence and swap their positions. Do this n times.\nRandom Deletion (RD): For each word in the sentence, randomly remove it with probability p.\n- Wiki Edits - A collection of scripts for automatic extraction of edited sentences from text edition histories, such as Wikipedia revisions. It was used to create the WikEd Error Corpus --- a corpus of corrective Wikipedia edits. The corpus has been prepared for two languages: Polish and English. Can be used as a dictionary-based augmentatioon to insert user-induced errors.\n- TextAttack \ud83d\udc19 - TextAttack is a Python framework for adversarial attacks, data augmentation, and model training in NLP.\nMany of the components of TextAttack are useful for data augmentation. The textattack.Augmenter class uses a transformation and a list of constraints to augment data. We also offer five built-in recipes for data augmentation source:QData/TextAttack:\ntextattack.WordNetAugmenter augments text by replacing words with WordNet synonyms\ntextattack.EmbeddingAugmenter augments text by replacing words with neighbors in the counter-fitted embedding space, with a constraint to ensure their cosine similarity is at least 0.8\ntextattack.CharSwapAugmenter augments text by substituting, deleting, inserting, and swapping adjacent characters\ntextattack.EasyDataAugmenter augments text with a combination of word insertions, substitutions and deletions.\ntextattack.CheckListAugmenter augments text by contraction/extension and by substituting names, locations, numbers.\ntextattack.CLAREAugmenter augments text by replacing, inserting, and merging with a pre-trained masked language model.\n- Text AutoAugment (TAA) - Text AutoAugment is a learnable and compositional framework for data augmentation in NLP. The proposed algorithm automatically searches for the optimal compositional policy, which improves the diversity and quality of augmented samples.\nAudio\n- SpecAugment with Pytorch - (https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html) is a state of the art data augmentation approach for speech recognition. It supports augmentations such as time wrap, time mask, frequency mask or all above combined.\n- Audiomentations - A Python library for audio data augmentation. Inspired by albumentations. Useful for machine learning. It allows to use effects such as: Compose, AddGaussianNoise, TimeStretch, PitchShift and Shift.\n- MUDA - A library for Musical Data Augmentation. Muda package implements annotation-aware musical data augmentation, as described in the muda paper.\nThe goal of this package is to make it easy for practitioners to consistently apply perturbations to annotated music data for the purpose of fitting statistical models.\n- tsaug -\nis a Python package for time series augmentation. It offers a set of augmentation methods for time series, as well as a simple API to connect multiple augmenters into a pipeline. Can be used for audio augmentation.\n- wav-augment - performs data augmentation on audio data.\nThe audio data is represented as pytorch tensors. It is particularly useful for speech data. Among others, it implements the augmentations that we found to be most useful for self-supervised learning (Data Augmenting Contrastive Learning of Speech Representations in the Time Domain, E. Kharitonov, M. Rivi\u00e8re, G. Synnaeve, L. Wolf, P.-E. Mazar\u00e9, M. Douze, E. Dupoux. [arxiv]):\nPitch randomization,\nReverberation,\nAdditive noise,\nTime dropout (temporal masking),\nBand reject,\nClipping\nTime series\n- tsaug -\nis a Python package for time series augmentation. It offers a set of augmentation methods for time series, as well as a simple API to connect multiple augmenters into a pipeline.\nExample augmenters:\nrandom time warping 5 times in parallel,\nrandom crop subsequences with length 300,\nrandom quantize to 10-, 20-, or 30- level sets,\nwith 80% probability , random drift the signal up to 10% - 50%,\nwith 50% probability, reverse the sequence.\nPapers\n2021\nData Augmentation for Scene Text Recognition;Rowel Atienza; Scene text recognition (STR) is a challenging task in computer vision due to the large number of possible text appearances in natural scenes. Most STR models rely on synthetic datasets for training since there are no sufficiently big and publicly available labelled real datasets. Since STR models are evaluated using real data, the mismatch between training and testing data distributions results into poor performance of models especially on challenging text that are affected by noise, artifacts, geometry, structure, etc. In this paper, we introduce STRAug which is made of 36 image augmentation functions designed for STR. Each function mimics certain text image properties that can be found in natural scenes, caused by -----> camera !!!  sensors, or induced by signal processing operations but poorly represented in the training dataset. When applied to strong baseline models using RandAugment, STRAug significantly increases the overall absolute accuracy of STR models across regular and irregular test datasets by as much as 2.10% on Rosetta, 1.48% on R2AM, 1.30% on CRNN, 1.35% on RARE, 1.06% on TRBA and 0.89% on GCRNN. The diversity and simplicity of API provided by STRAug functions enable easy replication and validation of existing data augmentation methods for STR. STRAug is available at this https URL.\nObject-Based Augmentation for Building Semantic Segmentation: Ventura and Santa Rosa Case Study;Svetlana Illarionova (Skolkovo institute of Science and Technology)*; Sergey Nesteruk (Skoltech); Dmitrii Shadrin (Skoltech); Vladimir Ignatiev (Skoltech); Maria Pukalchik (Skolkovo Institute of Science and Technology); Ivan Oseledets (Skolkovo Institute of Science and Technology); Today deep convolutional neural networks (CNNs) push the limits for most computer vision problems, define trends, and set state-of-the-art results. In remote sensing tasks such as object detection and semantic segmentation, CNNs reach the SotA performance. However, for precise per- formance, CNNs require much high-quality training data. Rare objects and the variability of environmental conditions strongly affect prediction stability and accuracy. To over- come these data restrictions, it is common to consider var- ious approaches including data augmentation techniques. This study focuses on the development and testing of object- based augmentation. The practical usefulness of the devel- oped augmentation technique is shown in the remote sens- ing domain, being one of the most demanded in effective augmentation techniques. We propose a novel pipeline for georeferenced image augmentation that enables a signifi- cant increase in the number of training samples. The pre- sented pipeline is called object-based augmentation (OBA) and exploits objects\u2019 segmentation masks to produce new realistic training scenes using target objects and various label-free backgrounds. We test the approach on the build- ings segmentation dataset with different CNN architectures (U-Net, FPN, HRNet) and show that the proposed method benefits for all the tested models. We also show that further augmentation strategy optimization can improve the results. The proposed method leads to the meaningful improvement of U-Net model predictions from 0.78 to 0.83 F1-score\nBounding Box Dataset Augmentation for Long-range Object Distance Estimation;Marten Franke (University of Bremen)*; Vaishnavi Gopinath (University of Bremen); Chaitra Reddy (University of Bremen); Danijela Ristic-Durrant (University of Bremen); Kai Michels (University of Bremen); Autonomous long-range obstacle detection and distance estimation plays an important role in numerous applica- tions such as railway applications when it comes to lo- comotive drivers support or developments towards driver- less trains. To overcome the problem of small training datasets, this paper presents two data augmentation meth- ods for training the ANN DisNet to perform reliable long- range distance estimation.\nInAugment: Improving Classifiers via Internal Augmentation;Moab Arar, Ariel Shamir, Amit Bermano; Image augmentation techniques apply transformation functions such as rotation, shearing, or color distortion on an input image. These augmentations were proven useful in improving neural networks' generalization ability. In this paper, we present a novel augmentation operation, InAugment, that exploits image internal statistics. The key idea is to copy patches from the image itself, apply augmentation operations on them, and paste them back at random positions on the same image. This method is simple and easy to implement and can be incorporated with existing augmentation techniques. We test InAugment on two popular datasets -- CIFAR and ImageNet. We show improvement over state-of-the-art augmentation techniques. Incorporating InAugment with Auto Augment yields a significant improvement over other augmentation techniques (e.g., +1% improvement over multiple architectures trained on the CIFAR dataset). We also demonstrate an increase for ResNet50 and EfficientNet-B3 top-1's accuracy on the ImageNet dataset compared to prior augmentation methods. Finally, our experiments suggest that training convolutional neural network using InAugment not only improves the model's accuracy and confidence but its performance on out-of-distribution images.\nOn Feature Normalization and Data Augmentation;Boyi Li, Felix Wu, Ser-Nam Lim, Serge Belongie, Kilian Q. Weinberger;The moments (a.k.a., mean and standard deviation) of latent features are often removed as noise when training image recognition models, to increase stability and reduce training time. However, in the field of image generation, the moments play a much more central role. Studies have shown that the moments extracted from instance normalization and positional normalization can roughly capture style and shape information of an image. Instead of being discarded, these moments are instrumental to the generation process. In this paper we propose Moment Exchange, an implicit data augmentation method that encourages the model to utilize the moment information also for recognition models. Specifically, we replace the moments of the learned features of one training image by those of another, and also interpolate the target labels---forcing the model to extract training signal from the moments in addition to the normalized features. As our approach is fast, operates entirely in feature space, and mixes different signals than prior methods, one can effectively combine it with existing augmentation approaches. We demonstrate its efficacy across several recognition benchmark data sets where it improves the generalization capability of highly competitive baseline networks with remarkable consistency.\nAn empirical survey of data augmentation for time series classification with neural networks; Brian Kenji Iwana,Seiichi Uchida; In recent times, deep artificial neural networks have achieved many successes in pattern recognition. Part of this success can be attributed to the reliance on big data to increase generalization. However, in the field of time series recognition, many datasets are often very small. One method of addressing this problem is through the use of data augmentation. In this paper, we survey data augmentation techniques for time series and their application to time series classification with neural networks. We propose a taxonomy and outline the four families in time series data augmentation, including transformation-based methods, pattern mixing, generative models, and decomposition methods. Furthermore, we empirically evaluate 12 time series data augmentation methods on 128 time series classification datasets with six different types of neural networks. Through the results, we are able to analyze the characteristics, advantages and disadvantages, and recommendations of each data augmentation method. This survey aims to help in the selection of time series data augmentation for neural network applications.\nSimple Copy-Paste Is a Strong Data Augmentation Method for Instance Segmentation;Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D. Cubuk, Quoc V. Le, Barret Zoph; Building instance segmentation models that are data-efficient and can handle rare object categories is an important challenge in computer vision. Leveraging data augmentations is a promising direction towards addressing this challenge. Here, we perform a systematic study of the Copy-Paste augmentation (e.g., [13, 12]) for instance segmentation where we randomly paste objects onto an image. Prior studies on Copy-Paste relied on modeling the surrounding visual context for pasting the objects. However, we find that the simple mechanism of pasting objects randomly is good enough and can provide solid gains on top of strong baselines. Furthermore, we show Copy-Paste is additive with semi-supervised methods that leverage extra data through pseudo labeling (eg. self-training). On COCO instance segmentation, we achieve 49.1 mask AP and 57.3 box AP, an improvement of +0.6 mask AP and +1.5 box AP over the previous state-of-the-art. We further demonstrate that Copy-Paste can lead to significant improvements on the LVIS benchmark. Our baseline model outperforms the LVIS 2020 Challenge winning entry by +3.6 mask AP on rare categories.\nOn Data Augmentation for GAN Training;Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Trung-Kien Nguyen; Ngai-Man Cheung; Recent successes in Generative Adversarial Networks (GAN) have affirmed the importance of using more data in GAN training. Yet it is expensive to collect data in many domains such as medical applications. Data Augmentation (DA) has been applied in these applications. In this work, we first argue that the classical DA approach could mislead the generator to learn the distribution of the augmented data, which could be different from that of the original data. We then propose a principled framework, termed Data Augmentation Optimized for GAN (DAG), to enable the use of augmented data in GAN training to improve the learning of the original distribution. We provide theoretical analysis to show that using our proposed DAG aligns with the original GAN in minimizing the Jensen-Shannon (JS) divergence between the original distribution and model distribution. Importantly, the proposed DAG effectively leverages the augmented data to improve the learning of discriminator and generator. We conduct experiments to apply DAG to different GAN models: unconditional GAN, conditional GAN, self-supervised GAN and CycleGAN using datasets of natural images and medical images. The results show that DAG achieves consistent and considerable improvements across these models. Furthermore, when DAG is used in some GAN models, the system establishes state-of-the-art Fr\u00e9chet Inception Distance (FID) scores. Our code is available (https://github.com/tntrung/dag-gans).\n2020\nCopyPaste: An Augmentation Method for Speech Emotion Recognition;Raghavendra Pappagari, Jes\u00fas Villalba, Piotr \u017belasko, Laureano Moro-Velazquez, Najim Dehak; Data augmentation is a widely used strategy for training robust machine learning models. It partially alleviates the problem of limited data for tasks like speech emotion recognition (SER), where collecting data is expensive and challenging. This study proposes CopyPaste, a perceptually motivated novel augmentation procedure for SER. Assuming that the presence of emotions other than neutral dictates a speaker's overall perceived emotion in a recording, concatenation of an emotional (emotion E) and a neutral utterance can still be labeled with emotion E. We hypothesize that SER performance can be improved using these concatenated utterances in model training. To verify this, three CopyPaste schemes are tested on two deep learning models: one trained independently and another using transfer learning from an x-vector model, a speaker recognition model. We observed that all three CopyPaste schemes improve SER performance on all the three datasets considered: MSP-Podcast, Crema-D, and IEMOCAP. Additionally, CopyPaste performs better than noise augmentation and, using them together improves the SER performance further. Our experiments on noisy test sets suggested that CopyPaste is effective even in noisy test conditions\nSeen and Unseen emotional style transfer for voice conversion with a new emotional speech dataset;Kun Zhou, Berrak Sisman, Rui Liu, Haizhou Li; Emotional voice conversion aims to transform emotional prosody in speech while preserving the linguistic content and speaker identity. Prior studies show that it is possible to disentangle emotional prosody using an encoder-decoder network conditioned on discrete representation, such as one-hot emotion labels. Such networks learn to remember a fixed set of emotional styles. In this paper, we propose a novel framework based on variational auto-encoding Wasserstein generative adversarial network (VAW-GAN), which makes use of a pre-trained speech emotion recognition (SER) model to transfer emotional style during training and at run-time inference. In this way, the network is able to transfer both seen and unseen emotional style to a new utterance. We show that the proposed framework achieves remarkable performance by consistently outperforming the baseline framework. This paper also marks the release of an emotional speech dataset (ESD) for voice conversion, which has multiple speakers and languages.\nOnlineAugment: Online Data Augmentation with Less Domain Knowledge;Zhiqiang Tang, Yunhe Gao, Leonid Karlinsky, Prasanna Sattigeri, Rogerio Feris, Dimitris Metaxas; Data augmentation is one of the most important tools in training modern deep neural networks. Recently, great advances have been made in searching for optimal augmentation policies in the image classification domain. However, two key points related to data augmentation remain uncovered by the current methods. First is that most if not all modern augmentation search methods are offline and learning policies are isolated from their usage. The learned policies are mostly constant throughout the training process and are not adapted to the current training model state. Second, the policies rely on class-preserving image processing functions. Hence applying current offline methods to new tasks may require domain knowledge to specify such kind of operations. In this work, we offer an orthogonal online data augmentation scheme together with three new augmentation networks, co-trained with the target learning task. It is both more efficient, in the sense that it does not require expensive offline training when entering a new domain, and more adaptive as it adapts to the learner state. Our augmentation networks require less domain knowledge and are easily applicable to new tasks. Extensive experiments demonstrate that the proposed scheme alone performs on par with the state-of-the-art offline data augmentation methods, as well as improving upon the state-of-the-art in combination with those methods. Code is available at this https URL .\nTime Series Data Augmentation for Deep Learning: A Survey;Qingsong Wen, Liang Sun, Xiaomin Song, Jingkun Gao, Xue Wang, Huan Xu; Deep learning performs remarkably well on many time series analysis tasks recently. The superior performance of deep neural networks relies heavily on a large number of training data to avoid overfitting. However, the labeled data of many real-world time series applications may be limited such as classification in medical time series and anomaly detection in AIOps. As an effective way to enhance the size and quality of the training data, data augmentation is crucial to the successful application of deep learning models on time series data. In this paper, we systematically review different data augmentation methods for time series. We propose a taxonomy for the reviewed methods, and then provide a structured review for these methods by highlighting their strengths and limitations. We also empirically compare different data augmentation methods for different tasks including time series anomaly detection, classification and forecasting. Finally, we discuss and highlight future research directions, including data augmentation in time-frequency domain, augmentation combination, and data augmentation and weighting for imbalanced class.\nAttribute Mix: Semantic Data Augmentation for Fine Grained Recognition;Hao Li, Xiaopeng Zhang, Hongkai Xiong, Qi Tian ; Collecting fine-grained labels usually requires expert-level domain knowledge and is prohibitive to scale up. In this paper, we propose Attribute Mix, a data augmentation strategy at attribute level to expand the fine-grained samples. The principle lies in that attribute features are shared among fine-grained sub-categories, and can be seamlessly transferred among images. Toward this goal, we propose an automatic attribute mining approach to discover attributes that belong to the same super-category, and Attribute Mix is operated by mixing semantically meaningful attribute features from two images. Attribute Mix is a simple but effective data augmentation strategy that can significantly improve the recognition performance without increasing the inference budgets. Furthermore, since attributes can be shared among images from the same super-category, we further enrich the training samples with attribute level labels using images from the generic domain. Experiments on widely used fine-grained benchmarks demonstrate the effectiveness of our proposed method. Specifically, without any bells and whistles, we achieve accuracies of 90.2%, 93.1% and 94.9% on CUB-200-2011, FGVC-Aircraft and Standford Cars, respectively.\nDictionary-based Data Augmentation for Cross-Domain Neural Machine Translation; Wei Peng, Chongxuan Huang, Tianhao Li, Yun Chen, Qun Liu ; Existing data augmentation approaches for neural machine translation (NMT) have predominantly relied on back-translating in-domain (IND) monolingual corpora. These methods suffer from issues associated with a domain information gap, which leads to translation errors for low frequency and out-of-vocabulary terminology. This paper proposes a dictionary-based data augmentation (DDA) method for cross-domain NMT. DDA synthesizes a domain-specific dictionary with general domain corpora to automatically generate a large-scale pseudo-IND parallel corpus. The generated pseudo-IND data can be used to enhance a general domain trained baseline. The experiments show that the DDA-enhanced NMT models demonstrate consistent significant improvements, outperforming the baseline models by 3.75-11.53 BLEU. The proposed method is also able to further improve the performance of the back-translation based and IND-finetuned NMT models. The improvement is associated with the enhanced domain coverage produced by DDA.\nImbalanced Data Learning by Minority Class Augmentation using Capsule Adversarial Networks;Pourya Shamsolmoali, Masoumeh Zareapoor, Linlin Shen, Abdul Hamid Sadka, Jie Yang ; The fact that image datasets are often imbalanced poses an intense challenge for deep learning techniques. In this paper, we propose a method to restore the balance in imbalanced images, by coalescing two concurrent methods, generative adversarial networks (GANs) and capsule network. In our model, generative and discriminative networks play a novel competitive game, in which the generator generates samples towards specific classes from multivariate probabilities distribution. The discriminator of our model is designed in a way that while recognizing the real and fake samples, it is also requires to assign classes to the inputs. Since GAN approaches require fully observed data during training, when the training samples are imbalanced, the approaches might generate similar samples which leading to data overfitting. This problem is addressed by providing all the available information from both the class components jointly in the adversarial training. It improves learning from imbalanced data by incorporating the majority distribution structure in the generation of new minority samples. Furthermore, the generator is trained with feature matching loss function to improve the training convergence. In addition, prevents generation of outliers and does not affect majority class space. The evaluations show the effectiveness of our proposed methodology; in particular, the coalescing of capsule-GAN is effective at recognizing highly overlapping classes with much fewer parameters compared with the convolutional-GAN.\nDADA: Differentiable Automatic Data Augmentation;Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M. Robertson, Yongxing Yang; Data augmentation (DA) techniques aim to increase data variability, and thus train deep networks with better generalisation. The pioneering AutoAugment automated the search for optimal DA policies with reinforcement learning. However, AutoAugment is extremely computationally expensive, limiting its wide applicability. Followup work such as PBA and Fast AutoAugment improved efficiency, but optimization speed remains a bottleneck. In this paper, we propose Differentiable Automatic Data Augmentation (DADA) which dramatically reduces the cost. DADA relaxes the discrete DA policy selection to a differentiable optimization problem via Gumbel-Softmax. In addition, we introduce an unbiased gradient estimator, RELAX, leading to an efficient and effective one-pass optimization strategy to learn an efficient and accurate DA policy. We conduct extensive experiments on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Furthermore, we demonstrate the value of Auto DA in pre-training for downstream detection problems. Results show our DADA is at least one order of magnitude faster than the state-of-the-art while achieving very comparable accuracy.\nData Augmentation using Pre-trained Transformer Models; Varun Kumar, Ashutosh Choudhary, Eunah Cho; Language model based pre-trained models such as BERT have provided significant gains across different NLP tasks. In this paper, we study different types of pre-trained transformer based models such as auto-regressive models (GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional data augmentation. We show that prepending the class labels to text sequences provides a simple yet effective way to condition the pre-trained models for data augmentation. On three classification benchmarks, pre-trained Seq2Seq model outperforms other models. Further, we explore how different pre-trained model based data augmentation differs in-terms of data diversity, and how well such methods preserve the class-label information. ;\nSuperMix: Supervising the Mixing Data Augmentation; Ali Dabouei, Sobhan Soleymani, Fariborz Taherkhani, Nasser M. Nasrabadi ; In this paper, we propose a supervised mixing augmentation method, termed SuperMix, which exploits the knowledge of a teacher to mix images based on their salient regions. SuperMix optimizes a mixing objective that considers: i) forcing the class of input images to appear in the mixed image, ii) preserving the local structure of images, and iii) reducing the risk of suppressing important features. To make the mixing suitable for large-scale applications, we develop an optimization technique, 65\u00d7 faster than gradient descent on the same problem. We validate the effectiveness of SuperMix through extensive evaluations and ablation studies on two tasks of object classification and knowledge distillation. On the classification task, SuperMix provides the same performance as the advanced augmentation methods, such as AutoAugment. On the distillation task, SuperMix sets a new state-of-the-art with a significantly simplified distillation method. Particularly, in six out of eight teacher-student setups from the same architectures, the students trained on the mixed data surpass their teachers with a notable margin.\nFast Cross-domain Data Augmentation through Neural Sentence Editing; Guillaume Raille, Sandra Djambazovska, Claudiu Musat; Data augmentation promises to alleviate data scarcity. This is most important in cases where the initial data is in short supply. This is, for existing methods, also where augmenting is the most difficult, as learning the full data distribution is impossible. For natural language, sentence editing offers a solution - relying on small but meaningful changes to the original ones. Learning which changes are meaningful also requires large amounts of training data. We thus aim to learn this in a source domain where data is abundant and apply it in a different, target domain, where data is scarce - cross-domain augmentation. We create the Edit-transformer, a Transformer-based sentence editor that is significantly faster than the state of the art and also works cross-domain. We argue that, due to its structure, the Edit-transformer is better suited for cross-domain environments than its edit-based predecessors. We show this performance gap on the Yelp-Wikipedia domain pairs. Finally, we show that due to this cross-domain performance advantage, the Edit-transformer leads to meaningful performance gains in several downstream tasks\nGridMask Data Augmentation; Pengguang Chen; We propose a novel data augmentation method GridMask in this paper. It utilizes information removal to achieve state-of-the-art results in a variety of computer vision tasks. We analyze the requirement of information dropping. Then we show limitation of existing information dropping algorithms and propose our structured method, which is simple and yet very effective. It is based on the deletion of regions of the input image. Our extensive experiments show that our method outperforms the latest AutoAugment, which is way more computationally expensive due to the use of reinforcement learning to find the best policies. On the ImageNet dataset for recognition, COCO2017 object detection, and on Cityscapes dataset for semantic segmentation, our method all notably improves performance over baselines. The extensive experiments manifest the effectiveness and generality of the new method.\nMel-spectrogram augmentation for sequence to sequence voice conversion; Yeongtae Hwang, Hyemin Cho, Hongsun Yang, Insoo Oh, Seong-Whan Lee; When training the sequence-to-sequence voice conversion model, we need to handle an issue of insufficient data about the number of speech tuples which consist of the same utterance. This study experimentally investigated the effects of Mel-spectrogram augmentation on the sequence-to-sequence voice conversion model. For Mel-spectrogram augmentation, we adopted the policies proposed in SpecAugment. In addition, we propose new policies for more data variations. To find the optimal hyperparameters of augmentation policies for voice conversion, we experimented based on the new metric, namely deformation per deteriorating ratio. We observed the effect of these through experiments based on various sizes of training set and combinations of augmentation policy. In the experimental results, the time axis warping based policies showed better performance than other policies.\nData Augmentation by AutoEncoders for Unsupervised Anomaly Detection;Kasra Babaei, ZhiYuan Chen, Tomas Maul; This paper proposes an autoencoder (AE) that is used for improving the performance of once-class classifiers for the purpose of detecting anomalies. Traditional one-class classifiers (OCCs) perform poorly under certain conditions such as high-dimensionality and sparsity. Also, the size of the training set plays an important role on the performance of one-class classifiers. Autoencoders have been widely used for obtaining useful latent variables from high-dimensional datasets. In the proposed approach, the AE is capable of deriving meaningful features from high-dimensional datasets while doing data augmentation at the same time. The augmented data is used for training the OCC algorithms. The experimental results show that the proposed approach enhance the performance of OCC algorithms and also outperforms other well-known approaches.\nEffective Data Augmentation with Multi-Domain Learning GANs;Shin'ya Yamaguchi, Sekitoshi Kanai, Takeharu Eda ; For deep learning applications, the massive data development (e.g., collecting, labeling), which is an essential process in building practical applications, still incurs seriously high costs. In this work, we propose an effective data augmentation method based on generative adversarial networks (GANs), called Domain Fusion. Our key idea is to import the knowledge contained in an outer dataset to a target model by using a multi-domain learning GAN. The multi-domain learning GAN simultaneously learns the outer and target dataset and generates new samples for the target tasks. The simultaneous learning process makes GANs generate the target samples with high fidelity and variety. As a result, we can obtain accurate models for the target tasks by using these generated samples even if we only have an extremely low volume target dataset. We experimentally evaluate the advantages of Domain Fusion in image classification tasks on 3 target datasets: CIFAR-100, FGVC-Aircraft, and Indoor Scene Recognition. When trained on each target dataset reduced the samples to 5,000 images, Domain Fusion achieves better classification accuracy than the data augmentation using fine-tuned GANs. Furthermore, we show that Domain Fusion improves the quality of generated samples, and the improvements can contribute to higher accuracy.\nAdversarial AutoAugment;Xinyu Zhang, Qiang Wang, Jian Zhang, Zhao Zhong ; Data augmentation (DA) has been widely utilized to improve generalization in training deep neural networks. Recently, human-designed data augmentation has been gradually replaced by automatically learned augmentation policy. Through finding the best policy in well-designed search space of data augmentation, AutoAugment can significantly improve validation accuracy on image classification tasks. However, this approach is not computationally practical for large-scale problems. In this paper, we develop an adversarial method to arrive at a computationally-affordable solution called Adversarial AutoAugment, which can simultaneously optimize target related object and augmentation policy search loss. The augmentation policy network attempts to increase the training loss of a target network through generating adversarial augmentation policies, while the target network can learn more robust features from harder examples to improve the generalization. In contrast to prior work, we reuse the computation in target network training for policy evaluation, and dispense with the retraining of the target network. Compared to AutoAugment, this leads to about 12x reduction in computing cost and 11x shortening in time overhead on ImageNet. We show experimental results of our approach on CIFAR-10/CIFAR-100, ImageNet, and demonstrate significant performance improvements over state-of-the-art. On CIFAR-10, we achieve a top-1 test error of 1.36%, which is the currently best performing single model. On ImageNet, we achieve a leading performance of top-1 accuracy 79.40% on ResNet-50 and 80.00% on ResNet-50-D without extra data.\nImperfect ImaGANation: Implications of GANs Exacerbating Biases on Facial Data Augmentation and Snapchat Selfie Lenses;Niharika Jain, Alberto Olmo, Sailik Sengupta, Lydia Manikonda, Subbarao Kambhampati ; Recently, the use of synthetic data generated by GANs has become a popular method to do data augmentation for many applications. While practitioners celebrate this as an economical way to obtain synthetic data for training data-hungry machine learning models, it is not clear that they recognize the perils of such an augmentation technique when applied to an already-biased dataset. Although one expects GANs to replicate the distribution of the original data, in real-world settings with limited data and finite network capacity, GANs suffer from mode collapse. Especially when this data is coming from online social media platforms or the web which are never balanced. In this paper, we show that in settings where data exhibits bias along some axes (eg. gender, race), failure modes of Generative Adversarial Networks (GANs) exacerbate the biases in the generated data. More often than not, this bias is unavoidable; we empirically demonstrate that given input of a dataset of headshots of engineering faculty collected from 47 online university directory webpages in the United States is biased toward white males, a state-of-the-art (unconditional variant of) GAN \"imagines\" faces of synthetic engineering professors that have masculine facial features and white skin color (inferred using human studies and a state-of-the-art gender recognition system). We also conduct a preliminary case study to highlight how Snapchat's explosively popular \"female\" filter (widely accepted to use a conditional variant of GAN), ends up consistently lightening the skin tones in women of color when trying to make face images appear more feminine. Our study is meant to serve as a cautionary tale for the lay practitioners who may unknowingly increase the bias in their training data by using GAN-based augmentation techniques with web data and to showcase the dangers of using biased datasets for facial applications.\nUnderstanding and Enhancing Mixed Sample Data Augmentation;Ethan Harris, Antonia Marcu, Matthew Painter, Mahesan Niranjan, Adam Pr\u00fcgel-Bennett, Jonathon Hare ; Mixed Sample Data Augmentation (MSDA) has received increasing attention in recent years, with many successful variants such as MixUp and CutMix. Following insight on the efficacy of CutMix in particular, we propose FMix, an MSDA that uses binary masks obtained by applying a threshold to low frequency images sampled from Fourier space. FMix improves performance over MixUp and CutMix for a number of state-of-the-art models across a range of data sets and problem settings. We go on to analyse MixUp, CutMix, and FMix from an information theoretic perspective, characterising learned models in terms of how they progressively compress the input with depth. Ultimately, our analyses allow us to decouple two complementary properties of augmentations, and present a unified framework for reasoning about MSDA. Code for all experiments is available at this https URL.\nImproving Generalization by Controlling Label-Noise Information in Neural Network Weights;Hrayr Harutyunyan, Kyle Reing, Greg Ver Steeg, Aram Galstyan; In the presence of noisy or incorrect labels, neural networks have the undesirable tendency to memorize information about the noise. Standard regularization techniques such as dropout, weight decay or data augmentation sometimes help, but do not prevent this behavior. If one considers neural network weights as random variables that depend on the data and stochasticity of training, the amount of memorized information can be quantified with the Shannon mutual information between weights and the vector of all training labels given inputs, I(w:y\u2223x). We show that for any training algorithm, low values of this term correspond to reduction in memorization of label-noise and better generalization bounds. To obtain these low values, we propose training algorithms that employ an auxiliary network that predicts gradients in the final layers of a classifier without accessing labels. We illustrate the effectiveness of our approach on versions of MNIST, CIFAR-10, and CIFAR-100 corrupted with various noise models, and on a large-scale dataset Clothing1M that has noisy labels.\nLearning Robust Representations via Multi-View Information Bottleneck;Marco Federici, Anjan Dutta, Patrick Forr\u00e9, Nate Kushman, Zeynep Akata ; The information bottleneck principle provides an information-theoretic method for representation learning, by training an encoder to retain all information which is relevant for predicting the label while minimizing the amount of other, excess information in the representation. The original formulation, however, requires labeled data to identify the superfluous information. In this work, we extend this ability to the multi-view unsupervised setting, where two views of the same underlying entity are provided but the label is unknown. This enables us to identify superfluous information as that not shared by both views. A theoretical analysis leads to the definition of a new multi-view model that produces state-of-the-art results on the Sketchy dataset and label-limited versions of the MIR-Flickr dataset. We also extend our theory to the single-view setting by taking advantage of standard data augmentation techniques, empirically showing better generalization capabilities when compared to common unsupervised approaches for representation learning. CODE\nSmall energy masking for improved neural network training for end-to-end speech recognition;Chanwoo Kim, Kwangyoun Kim, Sathish Reddy Indurthi ; In this paper, we present a Small Energy Masking (SEM) algorithm, which masks inputs having values below a certain threshold. More specifically, a time-frequency bin is masked if the filterbank energy in this bin is less than a certain energy threshold. A uniform distribution is employed to randomly generate the ratio of this energy threshold to the peak filterbank energy of each utterance in decibels. The unmasked feature elements are scaled so that the total sum of the feature values remain the same through this masking procedure. This very simple algorithm shows relatively 11.2 % and 13.5 % Word Error Rate (WER) improvements on the standard LibriSpeech test-clean and test-other sets over the baseline end-to-end speech recognition system. Additionally, compared to the input dropout algorithm, SEM algorithm shows relatively 7.7 % and 11.6 % improvements on the same LibriSpeech test-clean and test-other sets. With a modified shallow-fusion technique with a Transformer LM, we obtained a 2.62 % WER on the LibriSpeech test-clean set and a 7.87 % WER on the LibriSpeech test-other set.\n2019\nOcclusions for Effective Data Augmentation in Image Classification; Ruth Fong, Andrea Vedaldi; Deep networks for visual recognition are known to leverage \"easy to recognise\" portions of objects such as faces and distinctive texture patterns. The lack of a holistic understanding of objects may increase fragility and overfitting. In recent years, several papers have proposed to address this issue by means of occlusions as a form of data augmentation. However, successes have been limited to tasks such as weak localization and model interpretation, but no benefit was demonstrated on image classification on large-scale datasets. In this paper, we show that, by using a simple technique based on batch augmentation, occlusions as data augmentation can result in better performance on ImageNet for high-capacity models (e.g., ResNet50). We also show that varying amounts of occlusions used during training can be used to study the robustness of different neural network architectures.\nCutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features; Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, Youngjoon Yoo; Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefficiency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances.\nStyle transfer-based image synthesis as an efficient regularization technique in deep learning; Agnieszka Miko\u0142ajczyk, Micha\u0142 Grochowski; These days deep learning is the fastest-growing area in the field of Machine Learning. Convolutional Neural Networks are currently the main tool used for image analysis and classification purposes. Although great achievements and perspectives, deep neural networks and accompanying learning algorithms have some relevant challenges to tackle. In this paper, we have focused on the most frequently mentioned problem in the field of machine learning, that is relatively poor generalization abilities. Partial remedies for this are regularization techniques eg dropout, batch normalization, weight decay, transfer learning, early stopping and data augmentation. In this paper, we have focused on data augmentation. We propose to use a method based on a neural style transfer, which allows generating new unlabeled images of a high perceptual quality that combine the content of a base image with the appearance of another one. In a proposed approach, the newly created images are described with pseudo-labels, and then used as a training dataset. Real, labeled images are divided into the validation and test set. We validated the proposed method on a challenging skin lesion classification case study. Four representative neural architectures are examined. Obtained results show the strong potential of the proposed approach.\nGenerative adversarial network in medical imaging: A review; Xin Yi, Ekta Walia, Paul Babyna; Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross-modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique.\nData Augmentation via Dependency Tree Morphing for Low-Resource Languages; G\u00f6zde G\u00fcl \u015eahin, Mark Steedman; Neural NLP systems achieve high scores in the presence of sizable training dataset. Lack of such datasets leads to poor system performances in the case low-resource languages. We present two simple text augmentation techniques using dependency trees, inspired from image processing. We crop sentences by removing dependency links, and we rotate sentences by moving the tree fragments around the root. We apply these techniques to augment the training sets of low-resource languages in Universal Dependencies project. We implement a character-level sequence tagging model and evaluate the augmented datasets on part-of-speech tagging task. We show that crop and rotate provides improvements over the models trained with non-augmented data for majority of the languages, especially for languages with rich case marking systems.\nData augmentation for instrument classification robust to audio effects; Ant\u00f3nio Ramires, Xavier Serra; Reusing recorded sounds (sampling) is a key component in Electronic Music Production (EMP), which has been present since its early days and is at the core of genres like hip-hop or jungle. Commercial and non-commercial services allow users to obtain collections of sounds (sample packs) to reuse in their compositions. Automatic classification of one-shot instrumental sounds allows automatically categorising the sounds contained in these collections, allowing easier navigation and better characterisation. Automatic instrument classification has mostly targeted the classification of unprocessed isolated instrumental sounds or detecting predominant instruments in mixed music tracks. For this classification to be useful in audio databases for EMP, it has to be robust to the audio effects applied to unprocessed sounds. In this paper we evaluate how a state of the art model trained with a large dataset of one-shot instrumental sounds performs when classifying instruments processed with audio effects. In order to evaluate the robustness of the model, we use data augmentation with audio effects and evaluate how each effect influences the classification accuracy.\nHyperspectral Image Classification Using Random Occlusion Data Augmentation; Juan Mario Haut, Mercedes E. Paoletti, Javier Plaza, Antonio Plaza, Jun Li ; Convolutional neural networks (CNNs) have become a powerful tool for remotely sensed hyperspectral image (HSI) classification due to their great generalization ability and high accuracy. However, owing to the huge amount of parameters that need to be learned and to the complex nature of HSI data itself, these approaches must deal with the important problem of overfitting, which can lead to inadequate generalization and loss of accuracy. In order to mitigate this problem, in this letter, we adopt random occlusion, a recently developed data augmentation (DA) method for training CNNs, in which the pixels of different rectangular spatial regions in the HSI are randomly occluded, generating training images with various levels of occlusion and reducing the risk of overfitting. Our results with two well-known HSIs reveal that the proposed method helps to achieve better classification accuracy with low computational cost.\nLearning Data Manipulation for Augmentation and Weighting; Zhiting Hu, Bowen Tan, Ruslan Salakhutdinov, Tom Mitchell, Eric P. Xing; Manipulating data, such as weighting data examples or augmenting with new instances, has been increasingly used to improve model training. Previous work has studied various rule- or learning-based approaches designed for specific types of data manipulation. In this work, we propose a new method that supports learning different manipulation schemes with the same gradient-based algorithm. Our approach builds upon a recent connection of supervised learning and reinforcement learning (RL), and adapts an off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training. Different parameterization of the \"data reward\" function instantiates different manipulation schemes. We showcase data augmentation that learns a text transformation network, and data weighting that dynamically adapts the data sample importance. Experiments show the resulting algorithms significantly improve the image and text classification performance in low data regime and class-imbalance problems.\nAnatomically-Informed Data Augmentation for functional MRI with Applications to Deep Learning ;Kevin P. Nguyen, Cherise Chin Fatt, Alex Treacher, Cooper Mellema, Madhukar H. Trivedi, Albert Montillo ; The application of deep learning to build accurate predictive models from functional neuroimaging data is often hindered by limited dataset sizes. Though data augmentation can help mitigate such training obstacles, most data augmentation methods have been developed for natural images as in computer vision tasks such as CIFAR, not for medical images. This work helps to fills in this gap by proposing a method for generating new functional Magnetic Resonance Images (fMRI) with realistic brain morphology. This method is tested on a challenging task of predicting antidepressant treatment response from pre-treatment task-based fMRI and demonstrates a 26% improvement in performance in predicting response using augmented images. This improvement compares favorably to state-of-the-art augmentation methods for natural images. Through an ablative test, augmentation is also shown to substantively improve performance when applied before hyperparameter optimization. These results suggest the optimal order of operations and support the role of data augmentation method for improving predictive performance in tasks using fMRI.\nImage based fruit category classification by 13-layer deep convolutional neural network and data augmentation; Yu-Dong Zhang, Zhengchao Dong, Xianqing Chen, Wenjuan Jia, Sidan Du, Khan Muhammad, Shui-Hua Wang ; Fruit category identification is important in factories, supermarkets, and other fields. Current computer vision systems used handcrafted features, and did not get good results. In this study, our team designed a 13-layer convolutional neural network (CNN). Three types of data augmentation method was used: image rotation, Gamma correction, and noise injection. We also compared max pooling with average pooling. The stochastic gradient descent with momentum was used to train the CNN with minibatch size of 128. The overall accuracy of our method is 94.94%, at least 5 percentage points higher than state-of-the-art approaches. We validated this 13-layer is the optimal structure. The GPU can achieve a 177\u00d7 acceleration on training data, and a 175\u00d7 acceleration on test data. We observed using data augmentation can increase the overall accuracy. Our method is effective in image-based fruit classification.\nData augmentation using learned transformations for one-shot medical image segmentation; Amy Zhao, Guha Balakrishnan, Fr\u00e9do Durand, John V. Guttag, Adrian V. Dalca ; Image segmentation is an important task in many medical applications. Methods based on convolutional neural networks attain state-of-the-art accuracy; however, they typically rely on supervised training with large labeled datasets. Labeling medical images requires significant expertise and time, and typical hand-tuned approaches for data augmentation fail to capture the complex variations in such images. We present an automated data augmentation method for synthesizing labeled medical images. We demonstrate our method on the task of segmenting magnetic resonance imaging (MRI) brain scans. Our method requires only a single segmented scan, and leverages other unlabeled scans in a semi-supervised approach. We learn a model of transformations from the images, and use the model along with the labeled example to synthesize additional labeled examples. Each transformation is comprised of a spatial deformation field and an intensity change, enabling the synthesis of complex effects such as variations in anatomy and image acquisition procedures. We show that training a supervised segmenter with these new examples provides significant improvements over state-of-the-art methods for one-shot biomedical image segmentation.\nFace-Specific Data Augmentation for Unconstrained Face Recognition; Iacopo Masi, Anh Tu\u1ea5n Tr\u1ea7n, Tal Hassner, Gozde Sahin, G\u00e9rard Medioni ; We identify two issues as key to developing effective face recognition systems: maximizing the appearance variations of training images and minimizing appearance variations in test images. The former is required to train the system for whatever appearance variations it will ultimately encounter and is often addressed by collecting massive training sets with millions of face images. The latter involves various forms of appearance normalization for removing distracting nuisance factors at test time and making test faces easier to compare. We describe novel, efficient face-specific data augmentation techniques and show them to be ideally suited for both purposes. By using knowledge of faces, their 3D shapes, and appearances, we show the following: (a) We can artificially enrich training data for face recognition with face-specific appearance variations. (b) This synthetic training data can be efficiently produced online, thereby reducing the massive storage requirements of large-scale training sets and simplifying training for many appearance variations. Finally, (c) The same, fast data augmentation techniques can be applied at test time to reduce appearance variations and improve face representations. Together, with additional technical novelties, we describe a highly effective face recognition pipeline which, at the time of submission, obtains state-of-the-art results across multiple benchmarks. Portions of this paper were previously published by Masi et al. (European conference on computer vision, Springer, pp 579\u2013596, 2016b, International conference on automatic face and gesture recognition, 2017).\nHyperspectral Data Augmentation; Jakub Nalepa, Michal Myller, Michal Kawulok ; Data augmentation is a popular technique which helps improve generalization capabilities of deep neural networks. It plays a pivotal role in remote-sensing scenarios in which the amount of high-quality ground truth data is limited, and acquiring new examples is costly or impossible. This is a common problem in hyperspectral imaging, where manual annotation of image data is difficult, expensive, and prone to human bias. In this letter, we propose online data augmentation of hyperspectral data which is executed during the inference rather than before the training of deep networks. This is in contrast to all other state-of-the-art hyperspectral augmentation algorithms which increase the size (and representativeness) of training sets. Additionally, we introduce a new principal component analysis based augmentation. The experiments revealed that our data augmentation algorithms improve generalization of deep networks, work in real-time, and the online approach can be effectively combined with offline techniques to enhance the classification accuracy.\nDisentangling Correlated Speaker and Noise for Speech Synthesis via Data Augmentation and Adversarial Factorization; Wei-Ning Hsu ; Yu Zhang ; Ron J. Weiss ; Yu-An Chung ; Yuxuan Wang ; Yonghui Wu ; James Glass ; To leverage crowd-sourced data to train multi-speaker text-to-speech (TTS) models that can synthesize clean speech for all speakers, it is essential to learn disentangled representations which can independently control the speaker identity and background noise in generated signals. However, learning such representations can be challenging, due to the lack of labels describing the recording conditions of each training example, and the fact that speakers and recording conditions are often correlated, e.g. since users often make many recordings using the same equipment. This paper proposes three components to address this problem by: (1) formulating a conditional generative model with factorized latent variables, (2) using data augmentation to add noise that is not correlated with speaker identity and whose label is known during training, and (3) using adversarial factorization to improve disentanglement. Experimental results demonstrate that the proposed method can disentangle speaker and noise attributes even if they are correlated in the training data, and can be used to consistently synthesize clean speech for all speakers. Ablation studies verify the importance of each proposed component.\nGenerative Image Translation for Data Augmentation of Bone Lesion Pathology; Anant Gupta, Srivas Venkatesh, Sumit Chopra, Christian Ledig ; Insufficient training data and severe class imbalance are often limiting factors when developing machine learning models for the classification of rare diseases. In this work, we address the problem of classifying bone lesions from X-ray images by increasing the small number of positive samples in the training set. We propose a generative data augmentation approach based on a cycle-consistent generative adversarial network that synthesizes bone lesions on images without pathology. We pose the generative task as an image-patch translation problem that we optimize specifically for distinct bones (humerus, tibia, femur). In experimental results, we confirm that the described method mitigates the class imbalance problem in the binary classification task of bone lesion detection. We show that the augmented training sets enable the training of superior classifiers achieving better performance on a held-out test set. Additionally, we demonstrate the feasibility of transfer learning and apply a generative model that was trained on one body part to another.\nTowards highly accurate coral texture images classification using deep convolutional neural networks and data augmentation; Anabel G\u00f3mez-R\u00edosa, Siham Tabika, Juli\u00e1n Luengoa, ASM Shihavuddinb, Bartosz Krawczyk, Francisco Herreraa ; The recognition of coral species based on underwater texture images poses a significant difficulty for machine learning algorithms, due to the three following challenges embedded in the nature of this data: (1) datasets do not include information about the global structure of the coral; (2) several species of coral have very similar characteristics; and (3) defining the spatial borders between classes is difficult as many corals tend to appear together in groups. For this reasons, the classification of coral species has always required an aid from a domain expert. The objective of this paper is to develop an accurate classification model for coral texture images. Current datasets contain a large number of imbalanced classes, while the images are subject to inter-class variation. We have focused on the current small datasets and analyzed (1) several Convolutional Neural Network (CNN) architectures, (2) data augmentation techniques and (3) transfer learning approaches. We have achieved the state-of-the art accuracies using different variations of ResNet on the two small coral texture datasets, EILAT and RSMAS.\nData Augmentation using GANs for Speech Emotion Recognition;Aggelina Chatziagapi, Georgios Paraskevopoulos, Dimitris Sgouropoulos, Georgios Pantazopoulos, Malvina Nikandrou, Theodoros Giannakopoulos, Athanasios Katsamanis, Alexandros Potamianos, Shrikanth Narayanan ; In this work, we address the problem of data imbalance for the task of Speech Emotion Recognition (SER). We investigate conditioned data augmentation using Generative Adversarial Networks (GANs), in order to generate samples for underrepresented emotions. We adapt and improve a conditional GAN architecture to generate synthetic spectrograms for the minority class. For comparison purposes, we implement a series of signal-based data augmentation methods. The proposed GANbased approach is evaluated on two datasets, namely IEMOCAP and FEEL-25k, a large multi-domain dataset. Results demonstrate a 10% relative performance improvement in IEMOCAP and 5% in FEEL-25k, when augmenting the minority classes. Index Terms: Generative Adversarial Networks, Speech Emotion Recognition, data augmentation, data imbalance\nSpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition; Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, Quoc V. Le ; We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8% WER on test-other without the use of a language model, and 5.8% WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we achieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8%/14.1% with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.\nLearning More with Less: Conditional PGGAN-based Data Augmentation for Brain Metastases Detection Using Highly-Rough Annotation on MR Images; Changhee Han, Kohei Murao, Tomoyuki Noguchi, Yusuke Kawata, Fumiya Uchiyama, Leonardo Rundo, Hideki Nakayama, Shin'ichi Satoh ; Accurate Computer-Assisted Diagnosis, associated with proper data wrangling, can alleviate the risk of overlooking the diagnosis in a clinical environment. Towards this, as a Data Augmentation (DA) technique, Generative Adversarial Networks (GANs) can synthesize additional training data to handle the small/fragmented medical imaging datasets collected from various scanners; those images are realistic but completely different from the original ones, filling the data lack in the real image distribution. However, we cannot easily use them to locate disease areas, considering expert physicians' expensive annotation cost. Therefore, this paper proposes Conditional Progressive Growing of GANs (CPGGANs), incorporating highly-rough bounding box conditions incrementally into PGGANs to place brain metastases at desired positions/sizes on 256 X 256 Magnetic Resonance (MR) images, for Convolutional Neural Network-based tumor detection; this first GAN-based medical DA using automatic bounding box annotation improves the training robustness. The results show that CPGGAN-based DA can boost 10% sensitivity in diagnosis with clinically acceptable additional False Positives. Surprisingly, further tumor realism, achieved with additional normal brain MR images for CPGGAN training, does not contribute to detection performance, while even three physicians cannot accurately distinguish them from the real ones in Visual Turing Test.\nTraining Data Augmentation for Context-Sensitive Neural Lemmatization Using Inflection Tables and Raw Text; Toms Bergmanis, Sharon Goldwater ; Lemmatization aims to reduce the sparse data problem by relating the inflected forms of a word to its dictionary form. Using context can help, both for unseen and ambiguous words. Yet most context-sensitive approaches require full lemma-annotated sentences for training, which may be scarce or unavailable in low-resource languages. In addition (as shown here), in a low-resource setting, a lemmatizer can learn more from n labeled examples of distinct words (types) than from n (contiguous) labeled tokens, since the latter contain far fewer distinct types. To combine the efficiency of type-based learning with the benefits of context, we propose a way to train a context-sensitive lemmatizer with little or no labeled corpus data, using inflection tables from the UniMorph project and raw text examples from Wikipedia that provide sentence contexts for the unambiguous UniMorph examples. Despite these being unambiguous examples, the model successfully generalizes from them, leading to improved results (both overall, and especially on unseen words) in comparison to a baseline that does not use context.\nData Augmentation for BERT Fine-Tuning in Open-Domain Question Answering; Wei Yang, Yuqing Xie, Luchen Tan, Kun Xiong, Ming Li, Jimmy Lin ; Recently, a simple combination of passage retrieval using off-the-shelf IR techniques and a BERT reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset. In this paper, we present a data augmentation technique using distant supervision that exploits positive as well as negative examples. We apply a stage-wise approach to fine tuning BERT on multiple datasets, starting with data that is \"furthest\" from the test data and ending with the \"closest\". Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets.\nWhat Else Can Fool Deep Learning? Addressing Color Constancy Errors on Deep Neural Network Performance;Mahmoud Afifi, Michael S Brown ; There is active research targeting local image manipulations that can fool deep neural networks (DNNs) into produchttps://github.com/mahmoudnafifi/WB_color_augmentering incorrect results. This paper examines a type of global image manipulation that can produce similar adverse effects. Specifically, we explore how strong color casts caused by incorrectly applied computational color constancy - referred to as white balance (WB) in photography - negatively impact the performance of DNNs targeting image segmentation and classification. In addition, we discuss how existing image augmentation methods used to improve the robustness of DNNs are not well suited for modeling WB errors. To address this problem, a novel augmentation method is proposed that can emulate accurate color constancy degradation. We also explore pre-processing training and testing images with a recent WB correction algorithm to reduce the effects of incorrectly white-balanced images. We examine both augmentation and pre-processing strategies on different datasets and demonstrate notable improvements on the CIFAR-10, CIFAR-100, and ADE20K datasets. [repo]\n2018\nData augmentation for improving deep learning in image classification problem; Agnieszka Miko\u0142ajczyk, Micha\u0142 Grochowski; These days deep learning is the fastest-growing field in the field of Machine Learning (ML) and Deep Neural Networks (DNN). Among many of DNN structures, the Convolutional Neural Networks (CNN) are currently the main tool used for the image analysis and classification purposes. Although great achievements and perspectives, deep neural networks and accompanying learning algorithms have some relevant challenges to tackle. In this paper, we have focused on the most frequently mentioned problem in the field of machine learning, that is the lack of sufficient amount of the training data or uneven class balance within the datasets. One of the ways of dealing with this problem is so called data augmentation. In the paper we have compared and analyzed multiple methods of data augmentation in the task of image classification, starting from classical image transformations like rotating, cropping, zooming, histogram based methods and finishing at Style Transfer and Generative Adversarial Networks, along with the representative examples. Next, we presented our own method of data augmentation based on image style transfer. The method allows to generate the new images of high perceptual quality that combine the content of a base image with the appearance of another ones. The newly created images can be used to pre-train the given neural network in order to improve the training process efficiency. Proposed method is validated on the three medical case studies: skin melanomas diagnosis, histopathological images and breast magnetic resonance imaging (MRI) scans analysis, utilizing the image classification in order to provide a diagnose. In such kind of problems the data deficiency is one of the most relevant issues. Finally, we discuss the advantages and disadvantages of the methods being analyzed.\nSynthetic data augmentation using GAN for improved liver lesion classification; Maayan Frid-Adar, Eyal Klang, Michal Amitai, Jacob Goldberger, Hayit Greenspan; In this paper, we present a data augmentation method that generates synthetic medical images using Generative Adversarial Networks (GANs). We propose a training scheme that first uses classical data augmentation to enlarge the training set and then further enlarges the data size and its diversity by applying GAN techniques for synthetic data augmentation. Our method is demonstrated on a limited dataset of computed tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). The classification performance using only classic data augmentation yielded 78.6% sensitivity and 88.4% specificity. By adding the synthetic data augmentation the results significantly increased to 85.7% sensitivity and 92.4% specificity.\nAlcoholism Detection by Data Augmentation and Convolutional Neural Network with Stochastic Pooling; Shui-Hua Wang, Yi-Ding Lv, Yuxiu Sui, Shuai Liu, Su-Jing Wang, Yu-Dong Zhang; Alcohol use disorder (AUD) is an important brain disease. It alters the brain structure. Recently, scholars tend to use computer vision based techniques to detect AUD. We collected 235 subjects, 114 alcoholic and 121 non-alcoholic. Among the 235 image, 100 images were used as training set, and data augmentation method was used. The rest 135 images were used as test set. Further, we chose the latest powerful technique\u2014convolutional neural network (CNN) based on convolutional layer, rectified linear unit layer, pooling layer, fully connected layer, and softmax layer. We also compared three different pooling techniques: max pooling, average pooling, and stochastic pooling. The results showed that our method achieved a sensitivity of 96.88%, a specificity of 97.18%, and an accuracy of 97.04%. Our method was better than three state-of-the-art approaches. Besides, stochastic pooling performed better than other max pooling and average pooling. We validated CNN with five convolution layers and two fully connected layers performed the best. The GPU yielded a 149\u00d7 acceleration in training and a 166\u00d7 acceleration in test, compared to CPU.\nmixup: BEYOND EMPIRICAL RISK MINIMIZATION;Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz; Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.\nData Augmentation by Pairing Samples for Images Classification; Hiroshi Inoue; Data augmentation is a widely used technique in many machine learning tasks, such as image classification, to virtually enlarge the training dataset size and avoid overfitting. Traditional data augmentation techniques for image classification tasks create new samples from the original training data by, for example, flipping, distorting, adding a small amount of noise to, or cropping a patch from an original image. In this paper, we introduce a simple but surprisingly effective data augmentation technique for image classification tasks. With our technique, named SamplePairing, we synthesize a new sample from one image by overlaying another image randomly chosen from the training data (i.e., taking an average of two images for each pixel). By using two images randomly selected from the training set, we can generate N2 new samples from N training samples. This simple data augmentation technique significantly improved classification accuracy for all the tested datasets; for example, the top-1 error rate was reduced from 33.5% to 29.0% for the ILSVRC 2012 dataset with GoogLeNet and from 8.22% to 6.93% in the CIFAR-10 dataset. We also show that our SamplePairing technique largely improved accuracy when the number of samples in the training set was very small. Therefore, our technique is more valuable for tasks with a limited amount of training data, such as medical imaging tasks.\nGeneralizing to Unseen Domains via Adversarial Data Augmentation; Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C. Duchi, Vittorio Murino, Silvio Savarese; We are concerned with learning models that generalize well to different unseen domains. We consider a worst-case formulation over data distributions that are near the source domain in the feature space. Only using training data from a single source distribution, we propose an iterative procedure that augments the dataset with examples from a fictitious target domain that is \"hard\" under the current model. We show that our iterative scheme is an adaptive data augmentation method where we append adversarial examples at each iteration. For softmax losses, we show that our method is a data-dependent regularization scheme that behaves differently from classical regularizers that regularize towards zero (e.g., ridge or lasso). On digit recognition and semantic segmentation tasks, our method learns models improve performance across a range of a priori unknown target domains.\nFeature Space Transfer for Data Augmentation; Bo Liu, Xudong Wang, Mandar Dixit, Roland Kwitt, Nuno Vasconcelos; The problem of data augmentation in feature space is considered. A new architecture, denoted the FeATure TransfEr Network (FATTEN), is proposed for the modeling of feature trajectories induced by variations of object pose. This architecture exploits a parametrization of the pose manifold in terms of pose and appearance. This leads to a deep encoder/decoder network architecture, where the encoder factors into an appearance and a pose predictor. Unlike previous attempts at trajectory transfer, FATTEN can be efficiently trained end-to-end, with no need to train separate feature transfer functions. This is realized by supplying the decoder with information about a target pose and the use of a multi-task loss that penalizes category- and pose-mismatches. In result, FATTEN discourages discontinuous or non-smooth trajectories that fail to capture the structure of the pose manifold, and generalizes well on object recognition tasks involving large pose variation. Experimental results on the artificial ModelNet database show that it can successfully learn to map source features to target features of a desired pose, while preserving class identity. Most notably, by using feature space transfer for data augmentation (w.r.t. pose and depth) on SUN-RGBD objects, we demonstrate considerable performance improvements on one/few-shot object recognition in a transfer learning setup, compared to current state-of-the-art methods.\nCamStyle: A Novel Data Augmentation Method for Person Re-Identification; Zhun Zhong, Liang Zheng, Zhedong Zheng, Shaozi Li, Yi Yang; Person re-identification (re-ID) is a cross-camera retrieval task that suffers from image style variations caused by different cameras. The art implicitly addresses this problem by learning a camera-invariant descriptor subspace. In this paper, we explicitly consider this challenge by introducing camera style (CamStyle). CamStyle can serve as a data augmentation approach that reduces the risk of deep network overfitting and that smooths the CamStyle disparities. Specifically, with a style transfer model, labeled training images can be style transferred to each camera, and along with the original training samples, form the augmented training set. This method, while increasing data diversity against overfitting, also incurs a considerable level of noise. In the effort to alleviate the impact of noise, the label smooth regularization (LSR) is adopted. The vanilla version of our method (without LSR) performs reasonably well on few camera systems in which overfitting often occurs. With LSR, we demonstrate consistent improvement in all systems regardless of the extent of overfitting. We also report competitive accuracy compared with the state of the art on Market-1501 and DukeMTMC-re-ID. Importantly, CamStyle can be employed to the challenging problems of one view learning and unsupervised domain adaptation (UDA) in person re-identification (re-ID), both of which have critical research and application significance. The former only has labeled data in one camera view and the latter only has labeled data in the source domain. Experimental results show that CamStyle significantly improves the performance of the baseline in the two problems. Specially, for UDA, CamStyle achieves state-of-the-art accuracy based on a baseline deep re-ID model on Market-1501 and DukeMTMC-reID. Our code is available at: https://github.com/zhunzhong07/CamStyle.\nMedical Image Synthesis for Data Augmentation and Anonymization Using Generative Adversarial Networks; Hoo-Chang Shin, Neil A. Tenenholtz, Jameson K. Rogers, Christopher G. SchwarzMatthew L. Senjem, Jeffrey L. Gunter, Katherine P. Andriole, Mark Michalski; Data diversity is critical to success when training deep learning models. Medical imaging data sets are often imbalanced as pathologic findings are generally rare, which introduces significant challenges when training deep learning models. In this work, we propose a method to generate synthetic abnormal MRI images with brain tumors by training a generative adversarial network using two publicly available data sets of brain MRI. We demonstrate two unique benefits that the synthetic images provide. First, we illustrate improved performance on tumor segmentation by leveraging the synthetic images as a form of data augmentation. Second, we demonstrate the value of generative models as an anonymization tool, achieving comparable tumor segmentation results when trained on the synthetic data versus when trained on real subject data. Together, these results offer a potential solution to two of the largest challenges facing machine learning in medical imaging, namely the small incidence of pathological findings, and the restrictions around sharing of patient data.\n2017\nRandom Erasing Data Augmentation; Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, Yi Yang; In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification.\nDeep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification; Justin Salamon ; Juan Pablo Bello; The ability of deep convolutional neural networks (CNNs) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep CNN architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-the-art results for environmental sound classification. We show that the improved performance stems from the combination of a deep, high-capacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a \u201cshallow\u201d dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model's classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation.\nThe Effectiveness of Data Augmentation in Image Classification using Deep Learning;Luis Perez, Jason Wang; In this paper, we explore and compare multiple solutions to the problem of data augmentation in image classification. Previous work has demonstrated the effectiveness of data augmentation through simple techniques, such as cropping, rotating, and flipping input images. We artificially constrain our access to data to a small subset of the ImageNet dataset, and compare each data augmentation technique in turn. One of the more successful data augmentations strategies is the traditional transformations mentioned above. We also experiment with GANs to generate images of different styles. Finally, we propose a method to allow a neural net to learn augmentations that best improve the classifier, which we call neural augmentation. We discuss the successes and shortcomings of this method on various datasets.\nData Augmentation Generative Adversarial Networks; Antreas Antoniou, Amos Storkey, Harrison Edwards; Effective training of neural networks requires much data. In the low-data regime, parameters are underdetermined, and learnt networks generalise poorly. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data. Given there is potential to generate a much broader set of augmentations, we design and train a generative model to do data augmentation. The model, based on image conditional Generative Adversarial Networks, takes data from a source domain and learns to take any data item and generalise it to generate other within-class data items. As this generative process does not depend on the classes themselves, it can be applied to novel unseen classes of data. We show that a Data Augmentation Generative Adversarial Network (DAGAN) augments standard vanilla classifiers well. We also show a DAGAN can enhance few-shot learning systems such as Matching Networks. We demonstrate these approaches on Omniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In our experiments we can see over 13% increase in accuracy in the low-data regime experiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face (4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5% (from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%).\nSmart Augmentation Learning an Optimal Data Augmentation Strategy; Joseph Lemley, Shabab Bazrafkan, Peter Corcoran; A recurring problem faced when training neural networks is that there is typically not enough data to maximize the generalization capability of deep neural networks. There are many techniques to address this, including data augmentation, dropout, and transfer learning. In this paper, we introduce an additional method, which we call smart augmentation and we show how to use it to increase the accuracy and reduce over fitting on a target network. Smart augmentation works, by creating a network that learns how to generate augmented data during the training process of a target network in a way that reduces that networks loss. This allows us to learn augmentations that minimize the error of that network. Smart augmentation has shown the potential to increase accuracy by demonstrably significant measures on all data sets tested. In addition, it has shown potential to achieve similar or improved performance levels with significantly smaller network sizes in a number of tested cases.\nA study on data augmentation of reverberant speech for robust speech recognition; Tom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L. Seltzer, Sanjeev Khudanpur; The environmental robustness of DNN-based acoustic models can be significantly improved by using multi-condition training data. However, as data collection is a costly proposition, simulation of the desired conditions is a frequently adopted strategy. In this paper we detail a data augmentation approach for far-field ASR. We examine the impact of using simulated room impulse responses (RIRs), as real RIRs can be difficult to acquire, and also the effect of adding point-source noises. We find that the performance gap between using simulated and real RIRs can be eliminated when point-source noises are added. Further we show that the trained acoustic models not only perform well in the distant-talking scenario but also provide better results in the close-talking scenario. We evaluate our approach on several LVCSR tasks which can adequately represent both scenarios.\nImproving Deep Learning using Generic Data Augmentation; Luke Taylor, Geoff Nitschke; Deep artificial neural networks require a large corpus of training data in order to effectively learn, where collection of such training data is often expensive and laborious. Data augmentation overcomes this issue by artificially inflating the training set with label preserving transformations. Recently there has been extensive use of generic data augmentation to improve Convolutional Neural Network (CNN) task performance. This study benchmarks various popular data augmentation schemes to allow researchers to make informed decisions as to which training methods are most appropriate for their data sets. Various geometric and photometric schemes are evaluated on a coarse-grained data set using a relatively simple CNN. Experimental results, run using 4-fold cross-validation and reported in terms of Top-1 and Top-5 accuracy, indicate that cropping in geometric augmentation significantly increases CNN task performance.\nLearning to Compose Domain-Specific Transformations for Data Augmentation; Alexander J. Ratner, Henry Ehrenberg, Zeshan Hussain, Jared Dunnmon, Christopher R\u00e9; Data augmentation is a ubiquitous technique for increasing the size of labeled training sets by leveraging task-specific data transformations that preserve class labels. While it is often easy for domain experts to specify individual transformations, constructing and tuning the more sophisticated compositions typically needed to achieve state-of-the-art results is a time-consuming manual task in practice. We propose a method for automating this process by learning a generative sequence model over user-specified transformation functions using a generative adversarial approach. Our method can make use of arbitrary, non-deterministic transformation functions, is robust to misspecified user input, and is trained on unlabeled data. The learned transformation model can then be used to perform data augmentation for any end discriminative model. In our experiments, we show the efficacy of our approach on both image and text datasets, achieving improvements of 4.0 accuracy points on CIFAR-10, 1.4 F1 points on the ACE relation extraction task, and 3.4 accuracy points when using domain-specific transformation operations on a medical imaging dataset as compared to standard heuristic augmentation approaches.\nData Augmentation for Low-Resource Neural Machine Translation; Marzieh Fadaee, Arianna Bisazza, Christof Monz; The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.\n2015\nData augmentation for deep neural network acoustic modeling; Cui, Xiaodong, Vaibhava Goel, and Brian Kingsbury; This paper investigates data augmentation for deep neural network acoustic modeling based on label-preserving transformations to deal with data sparsity. Two data augmentation approaches, vocal tract length perturbation (VTLP) and stochastic feature mapping (SFM), are investigated for both deep neural networks (DNNs) and convolutional neural networks (CNNs). The approaches are focused on increasing speaker and speech variations of the limited training data such that the acoustic models trained with the augmented data are more robust to such variations. In addition, a two-stage data augmentation scheme based on a stacked architecture is proposed to combine VTLP and SFM as complementary approaches. Experiments are conducted on Assamese and Haitian Creole, two development languages of the IARPA Babel program, and improved performance on automatic speech recognition (ASR) and keyword search (KWS) is reported.\nEXPLORING DATA AUGMENTATION FOR IMPROVED SINGING VOICE DETECTION WITH NEURAL NETWORKS; Jan Schl\u00fcter and Thomas Grill; In computer vision, state-of-the-art object recognition systems rely on label-preserving image transformations such as scaling and rotation to augment the training datasets. The additional training examples help the system to learn invariances that are difficult to build into the model, and improve generalization to unseen data. To the best of our knowledge, this approach has not been systematically explored for music signals. Using the problem of singing voice detection with neural networks as an example, we apply a range of label-preserving audio transformations to assess their utility for music data augmentation. In line with recent research in speech recognition, we find pitch shifting to be the most helpful augmentation method. Combined with time stretching and random frequency filtering, we achieve a reduction in classification error between 10 and 30%, reaching the state of the art on two public datasets. We expect that audio data augmentation would yield significant gains for several other sequence labelling and event detection tasks in music information retrieval.\nAudio Augmentation for Speech Recognition; Tom Ko (1), Vijayaditya Peddinti, Daniel Povey, Sanjeev Khudanpur; Data augmentation is a common strategy adopted to increase the quantity of training data, avoid overfitting and improve robustness of the models. In this paper, we investigate audio-level speech augmentation methods which directly process the raw signal. The method we particularly recommend is to change the speed of the audio signal, producing 3 versions of the original signal with speed factors of 0.9, 1.0 and 1.1. The proposed technique has a low implementation cost, making it easy to adopt. We present results on 4 different LVCSR tasks with training data ranging from 100 hours to 960 hours, to examine the effectiveness of audio augmentation in a variety of data scenarios. An average relative improvement of 4.3% was observed across the 4 tasks.\nA SOFTWARE FRAMEWORK FOR MUSICAL DATA AUGMENTATION; Brian McFee1, Eric J. Humphrey, and Juan P. Bello; Predictive models for music annotation tasks are practically limited by a paucity of well-annotated training data. In the broader context of large-scale machine learning, the concept of \u201cdata augmentation\u201d \u2014 supplementing a training set with carefully perturbed samples \u2014 has emerged as an important component of robust systems. In this work, we develop a general software framework for augmenting annotated musical datasets, which will allow practitioners to easily expand training sets with musically motivated perturbations of both audio and annotations. As a proof of concept, we investigate the effects of data augmentation on the task of recognizing instruments in mixed signals\nDropout as data augmentation; Xavier Bouthillier, Kishore Konda, Pascal Vincent, Roland Memisevic; Dropout is typically interpreted as bagging a large number of models sharing parameters. We show that using dropout in a network can also be interpreted as a kind of data augmentation in the input space without domain knowledge. We present an approach to projecting the dropout noise within a network back into the input space, thereby generating augmented versions of the training data, and we show that training a deterministic network on the augmented samples yields similar results. Finally, we propose a new dropout noise scheme based on our observations and show that it improves dropout results without adding significant computational cost.\nData-augmentation for reducing dataset bias in person re-identification; Niall McLaughlin, Jesus Martinez Del Rincon, Paul Miller; In this paper we explore ways to address the issue of dataset bias in person re-identification by using data augmentation to increase the variability of the available datasets, and we introduce a novel data augmentation method for re-identification based on changing the image background. We show that use of data augmentation can improve the cross-dataset generalisation of convolutional network based re-identification systems, and that changing the image background yields further improvements.\nA Convolutional Neural Network for Leaves Recognition Using Data Augmentation; Chaoyun Zhang, Pan Zhou, Chenghua Li, Lijun Liu; Recently, convolutional neural networks (ConvNets) have achieved marvellous results in different field of recognition, especially in computer vision. In this paper, a seven-layer ConvNet using data augmentation is proposed for leaves recognition. First, we implement multiform transformations (e.g., rotation and translation etc.) to enlarge the dataset without changing their labels. This novel technique recently makes tremendous contribution to the performance of ConvNets as it is able to reduce the over-fitting degree and enhance the generalization ability of the ConvNet. Moreover, in order to get the shapes of leaves, we sharpen all the images with a random parameter. This method is similar to the edge detection, which has been proved useful in the image classification. Then we train a deep convolutional neural network to classify the augmented leaves data with three groups of test set and finally find that the method is quite feasible and effective. The accuracy achieved by our algorithm outperforms other methods for supervised learning on the popular leaf dataset Flavia.\nDeep CNN Ensemble with Data Augmentation for Object Detection; Jian Guo, Stephen Gould; We report on the methods used in our recent DeepEnsembleCoco submission to the PASCAL VOC 2012 challenge, which achieves state-of-the-art performance on the object detection task. Our method is a variant of the R-CNN model proposed Girshick:CVPR14 with two key improvements to training and evaluation. First, our method constructs an ensemble of deep CNN models with different architectures that are complementary to each other. Second, we augment the PASCAL VOC training set with images from the Microsoft COCO dataset to significantly enlarge the amount training data. Importantly, we select a subset of the Microsoft COCO images to be consistent with the PASCAL VOC task. Results on the PASCAL VOC evaluation server show that our proposed method outperform all previous methods on the PASCAL VOC 2012 detection task at time of submission.\nData augmentation for machine learning redshifts applied to Sloan Digital Sky Survey galaxies; Ben Hoyle, Markus Michael Rau, Christopher Bonnett, Stella Seitz, Jochen Weller; We present analyses of data augmentation for machine learning redshift estimation. Data augmentation makes a training sample more closely resemble a test sample, if the two base samples differ, in order to improve measured statistics of the test sample. We perform two sets of analyses by selecting 800 000 (1.7 million) Sloan Digital Sky Survey Data Release 8 (Data Release 10) galaxies with spectroscopic redshifts. We construct a base training set by imposing an artificial r-band apparent magnitude cut to select only bright galaxies and then augment this base training set by using simulations and by applying the K-CORRECT package to artificially place training set galaxies at a higher redshift. We obtain redshift estimates for the remaining faint galaxy sample, which are not used during training. We find that data augmentation reduces the error on the recovered redshifts by 40 per cent in both sets of analyses, when compared to the difference in error between the ideal case and the non-augmented case. The outlier fraction is also reduced by at least 10 per cent and up to 80 per cent using data augmentation. We finally quantify how the recovered redshifts degrade as one probes to deeper magnitudes past the artificial magnitude limit of the bright training sample. We find that at all apparent magnitudes explored, the use of data augmentation with tree-based methods provide an estimate of the galaxy redshift with a low value of bias, although the error on the recovered redshifts increases as we probe to deeper magnitudes. These results have applications for surveys which have a spectroscopic training set which forms a biased sample of all photometric galaxies, for example if the spectroscopic detection magnitude limit is shallower than the photometric limit.\nApproximate Bayesian Logistic Regression via Penalized Likelihood by Data Augmentation; Andrea Discacciati, Nicola Orsini, Sander Greenland; We present a command, penlogit, for approximate Bayesian logistic regression using penalized likelihood estimation via data augmentation. This command automatically adds specific prior-data records to a dataset. These records are computed so that they generate a penalty function for the log likelihood of a logistic model, which equals (up to an additive constant) a set of independent log prior distributions on the model parameters. This command overcomes the necessity of relying on specialized software and statistical tools (such as Markov chain Monte Carlo) for fitting Bayesian models, and allows one to assess the information content of a prior in terms of the data that would be required to generate the prior as a likelihood function. The command produces data equivalent to normal and generalized log-F priors for the model parameters, providing flexible translation of background information into prior data, which allows calculation of approximate posterior medians and intervals from ordinary maximum likelihood programs. We illustrate the command through an example using data from an observational study of neonatal mortality.\nImproved Chemical Structure\u2013Activity Modeling Through Data Augmentation; Isidro Cortes-Ciriano, Andreas Bender; Extending the original training data with simulated unobserved data points has proven powerful to increase both the generalization ability of predictive models and their robustness against changes in the structure of data (e.g., systematic drifts in the response variable) in diverse areas such as the analysis of spectroscopic data or the detection of conserved domains in protein sequences. In this contribution, we explore the effect of data augmentation in the predictive power of QSAR models, quantified by the RMSE values on the test set. We collected 8 diverse data sets from the literature and ChEMBL version 19 reporting compound activity as pIC50 values. The original training data were replicated (i.e., augmented) N times (N \u2208 0, 1, 2, 4, 6, 8, 10), and these replications were perturbed with Gaussian noise (\u03bc = 0, \u03c3 = \u03c3noise) on either (i) the pIC50 values, (ii) the compound descriptors, (iii) both the compound descriptors and the pIC50 values, or (iv) none of them. The effect of data augmentation was evaluated across three different algorithms (RF, GBM, and SVM radial) and two descriptor types (Morgan fingerprints and physicochemical-property-based descriptors). The influence of all factor levels was analyzed with a balanced fixed-effect full-factorial experiment. Overall, data augmentation constantly led to increased predictive power on the test set by 10\u201315%. Injecting noise on (i) compound descriptors or on (ii) both compound descriptors and pIC50 values led to the highest drop of RMSEtest values (from 0.67\u20130.72 to 0.60\u20130.63 pIC50 units). The maximum increase in predictive power provided by data augmentation is reached when the training data is replicated one time. Therefore, extending the original training data with one perturbed repetition thereof represents a reasonable trade-off between the increased performance of the models and the computational cost of data augmentation, namely increase of (i) model complexity due to the need for optimizing \u03c3noise and (ii) the number of training examples.\nMax-Margin Discriminant Projection via Data Augmentation; Bo Chen, Hao Zhang ,Xuefeng Zhang, Wei Wen, Hongwei Liu, Lun Liu ; In this paper, we introduce a new max-margin discriminant projection method, which takes advantage of the latent variable representation for support vector machine (SVM) as the classification criterion. Specifically, the proposed model jointly learns the discriminative subspace and classifier in a Bayesian framework by conditioning on augmented variables. Moreover, an extended nonlinear model is developed based on the kernel trick, where the similar model can be used in this setting with few modifications. To explore the sparsity in the kernel expansion, we use the spike-and-slab prior to seek basis vectors (BVs) from the corresponding candidates. Unlike existing methods, which employ BVs to approximate the original feature space, in our method BVs are sought to associate the final classification task. Thanks to the conditionally conjugate property, the parameters in our models can be inferred via the simple and efficient Gibbs sampler. Finally, we test our methods on synthesized and real-world data, including large-scale data sets to demonstrate their efficiency and effectiveness.\nData augmentation in Riemannian space for Brain-Computer Interfaces;Emmanuel Kalunga Sylvain Chevallier, Quentin Barth\u00e9lemy ; Brain-Computer Interfaces (BCI) try to interpret brain signals , such as EEG , to issue some command or to characterize the cognitive states of the subjects. A strong limitation is that BCI tasks require a high concentration of the user , de facto limiting the length of experiment and the size of the dataset. Furthermore , several BCI paradigms depend on rare events , as for event-related potentials , also reducing the number of training examples available. A common strategy in machine learning when dealing with scarce data is called data augmentation ; new samples are generated by applying chosen transformations on the original dataset. In this contribution , we propose a scheme to adapt data augmentation in EEG-based BCI with a Riemannian standpoint : geometrical properties of EEG covariance matrix are taken into account to generate new training samples. Neural network are good candidates to benefit from such training scheme and a simple multi-layer perceptron offers good results . Experimental validation is conducted on two datasets : an SSVEP experiment with few training samples in each class and an error potential experiment with unbalanced classes (NER Kaggle competition)\nAutoAugment\nAutomatic Data Augmentation is a new family of algorithms that searches for the policy of augmenting the dataset for solivng the selcted task.\nGithub repositories:\ntensorflow - autoaugment implementation;\nText AutoAugment (TAA);\nOfficial Fast AutoAugment implementation in PyTorch\nAutoCLINT - Automatic Computationally LIght Network Transfer (A specially designed light version of Fast AutoAugment is implemented to adapt to various tasks under limited resources)\nPopulation Based Augmentation (PBA)\nAutoAugment: Learning Augmentation Policies from Data (Keras and Tensorflow)\nAutoAugment: Learning Augmentation Policies from Data (PyTorch)\nAutoAugment: Learning Augmentation Policies from Data (PyTorch, another implementation)\nPapers:\nText AutoAugment: Learning Compositional Augmentation Policy for Text Classification; Shuhuai Ren, Jinchao Zhang, Lei Li, Xu Sun, Jie Zhou; Data augmentation aims to enrich training samples for alleviating the overfitting issue in low-resource or class-imbalanced situations. Traditional methods first devise task-specific operations such as Synonym Substitute, then preset the corresponding parameters such as the substitution rate artificially, which require a lot of prior knowledge and are prone to fall into the sub-optimum. Besides, the number of editing operations is limited in the previous methods, which decreases the diversity of the augmented data and thus restricts the performance gain. To overcome the above limitations, we propose a framework named Text AutoAugment (TAA) to establish a compositional and learnable paradigm for data augmentation. We regard a combination of various operations as an augmentation policy and utilize an efficient Bayesian Optimization algorithm to automatically search for the best policy, which substantially improves the generalization capability of models. Experiments on six benchmark datasets show that TAA boosts classification accuracy in low-resource and class-imbalanced regimes by an average of 8.8% and 9.7%, respectively, outperforming strong baselines.\nFaster AutoAugment: Learning Augmentation Strategies using Backpropagation; Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, Hideki Nakayama; Data augmentation methods are indispensable heuristics to boost the performance of deep neural networks, especially in image recognition tasks. Recently, several studies have shown that augmentation strategies found by search algorithms outperform hand-made strategies. Such methods employ black-box search algorithms over image transformations with continuous or discrete parameters and require a long time to obtain better strategies. In this paper, we propose a differentiable policy search pipeline for data augmentation, which is much faster than previous methods. We introduce approximate gradients for several transformation operations with discrete parameters as well as the differentiable mechanism for selecting operations. As the objective of training, we minimize the distance between the distributions of augmented data and the original data, which can be differentiated. We show that our method, Faster AutoAugment, achieves significantly faster searching than prior work without a performance drop.\nAutoAugment: Learning Augmentation Policies from Data; Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, Quoc V. Le; Data augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many sub-policies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to find the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5% which is 0.4% better than the previous record of 83.1%. On CIFAR-10, we achieve an error rate of 1.5%, which is 0.6% better than the previous state-of-the-art. Augmentation policies we find are transferable between datasets. The policy learned on ImageNet transfers well to achieve significant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.\nFast AutoAugment;Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, Sungwoong Kim ; Data augmentation is an essential technique for improving generalization ability of deep learning models. Recently, AutoAugment has been proposed as an algorithm to automatically search for augmentation policies from a dataset and has significantly enhanced performances on many image recognition tasks. However, its search method requires thousands of GPU hours even for a relatively small dataset. In this paper, we propose an algorithm called Fast AutoAugment that finds effective augmentation policies via a more efficient search strategy based on density matching. In comparison to AutoAugment, the proposed algorithm speeds up the search time by orders of magnitude while achieves comparable performances on image recognition tasks with various models and datasets including CIFAR-10, CIFAR-100, SVHN, and ImageNet\nAutoAugment: Learning Augmentation Strategies From Data; Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, Quoc V. Le; Data augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many sub-policies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to find the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5% which is 0.4% better than the previous record of 83.1%. On CIFAR-10, we achieve an error rate of 1.5%, which is 0.6% better than the previous state-of-the-art. Augmentation policies we find are transferable between datasets. The policy learned on ImageNet transfers well to achieve significant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.\nGreedy AutoAugment; Alireza Naghizadeh, Mohammadsajad Abavisani, Dimitris N. Metaxas; A major problem in data augmentation is the number of possibilities in the search space of operations. The search space includes mixtures of all of the possible data augmentation techniques, the magnitude of these operations, and the probability of applying data augmentation for each image. In this paper, we propose Greedy AutoAugment as a highly efficient searching algorithm to find the best augmentation policies. We combine the searching process with a simple procedure to increase the size of training data. Our experiments show that the proposed method can be used as a reliable addition to the ANN infrastructures for increasing the accuracy of classification results.\nAutomatically Learning Data Augmentation Policies for Dialogue Tasks;Tong Niu, Mohit Bansal ; Automatic data augmentation (AutoAugment) (Cubuk et al., 2019) searches for optimal perturbation policies via a controller trained using performance rewards of a sampled policy on the target task, hence reducing data-level model bias. While being a powerful algorithm, their work has focused on computer vision tasks, where it is comparatively easy to apply imperceptible perturbations without changing an image's semantic meaning. In our work, we adapt AutoAugment to automatically discover effective perturbation policies for natural language processing (NLP) tasks such as dialogue generation. We start with a pool of atomic operations that apply subtle semantic-preserving perturbations to the source inputs of a dialogue task (e.g., different POS-tag types of stopword dropout, grammatical errors, and paraphrasing). Next, we allow the controller to learn more complex augmentation policies by searching over the space of the various combinations of these atomic operations. Moreover, we also explore conditioning the controller on the source inputs of the target task, since certain strategies may not apply to inputs that do not contain that strategy's required linguistic features. Empirically, we demonstrate that both our input-agnostic and input-aware controllers discover useful data augmentation policies, and achieve significant improvements over the previous state-of-the-art, including trained on manually-designed policies.\nPopulation Based Augmentation: Efficient Learning of Augmentation Policy Schedules;Daniel Ho, Eric Liang, Ion Stoica, Pieter Abbeel, Xi Chen ; A key challenge in leveraging data augmentation for neural network training is choosing an effective augmentation policy from a large search space of candidate operations. Properly chosen augmentation policies can lead to significant generalization improvements; however, state-of-the-art approaches such as AutoAugment are computationally infeasible to run for the ordinary user. In this paper, we introduce a new data augmentation algorithm, Population Based Augmentation (PBA), which generates nonstationary augmentation policy schedules instead of a fixed augmentation policy. We show that PBA can match the performance of AutoAugment on CIFAR-10, CIFAR-100, and SVHN, with three orders of magnitude less overall compute. On CIFAR-10 we achieve a mean test error of 1.46%, which is a slight improvement upon the current state-of-the-art. The code for PBA is open source and is available at this https URL.\nLearning data augmentation policies using augmented random search;Mingyang Geng, Kele Xu, Bo Ding, Huaimin Wang, Lei Zhang ; Previous attempts for data augmentation are designed manually, and the augmentation policies are dataset-specific. Recently, an automatic data augmentation approach, named AutoAugment, is proposed using reinforcement learning. AutoAugment searches for the augmentation polices in the discrete search space, which may lead to a sub-optimal solution. In this paper, we employ the Augmented Random Search method (ARS) to improve the performance of AutoAugment. Our key contribution is to change the discrete search space to continuous space, which will improve the searching performance and maintain the diversities between sub-policies. With the proposed method, state-of-the-art accuracies are achieved on CIFAR-10, CIFAR-100, and ImageNet (without additional data).\nOther\nChallenges\nAutoDL 2019 (NeurIPS AutoDL challenges - includes AutoAugment) - Machine Learning and in particular Deep Learning has achieved considerable successes in recent years and an ever-growing number of disciplines rely on it. However, this success crucially relies on human intervention in many steps (data pre-processing, feature engineering, model selection, hyper-parameter optimization, etc.). As the complexity of these tasks is often beyond non-experts, the rapid growth of machine learning applications has created a demand for off-the-shelf or reusable methods, which can be used easily and without expert knowledge. The objective of AutoML (Automated Machine Learning) challenges is to provide \"universal learning machines\" (deep learning or others), which can learn and make predictions without human intervention (blind testing).\nWorkshops\nInteractive Labeling and Data Augmentation for Vision ICCV 2021 Workshop - The workshop on Interactive Labeling and Data Augmentation for Vision (ILDAV) wishes to address novel ways to solve computer vision problems where large quantities of labeled image data may not be readily available. It is important that datasets of sufficient size can be quickly and cheaply acquired and labeled. More specifically, we are interested in solutions to this problem that make use of (i) few-click and interactive data annotation, where machine learning is used to enhance human annotation skill, (ii) synthetic data generation, where one uses artificially generated data to augment real datasets, and (iii) weak supervision, where auxiliary or weak signals are used instead of (or to complement) manual labels.\nMore broadly, we aim at fostering a collaboration between academia and industry in terms of leveraging machine learning research and human-in-the-loop, interactive labeling to quickly build datasets that will enable the use of powerful deep models in all problems of computer vision.\nThe workshop topics include (but are not limited to):\nInteractive and Few-click annotation\nData augmentation\nSynthetic data for training models\nWeak supervision\nHuman-in-the-loop learning",
      "link": "https://github.com/AgaMiko/data-augmentation-review"
    },
    {
      "autor": "AI_Sudoku",
      "date": "NaN",
      "content": "AI_Sudoku\nGUI Smart Sudoku Solver that tries to extract a sudoku puzzle from a -----> photo !!!  and solve it.\nTable Of Contents:\nInstallation\nUsage\nWorking\nImage Preprocessing\nRecognition\nToDo\nContributing\nInstallation\nDownload and install Python3 from here\nI recommend using virtualenv. Download virtualenv by opening a terminal and typing:\npip install virtualenv\nCreate a virtual environment with the name sudokuenv.\nWindows\nvirtualenv sudokuenv\ncd sudokuenv/Scripts\nactivate\nLinux:\nsource sudokuenv/bin/activate\nClone this repository, extract it if you downloaded a .zip or .tar file and cd into the cloned repository.\nFor Example:\ncd A:\\AI_Sudoku-master\nInstall the required packages by typing:\npip install -r requirements.txt\nUsage\nBefore running the application, know that you can set the modeltype variable in Run.py to either \"CNN\" or \"KNN\" to choose the Convolutional Neural Network or the K Nearest Neighbours Algorithm for Recognition. By default it is set to \"KNN\" and I got a way higher accuracy using KNN itself, so I would recommend that you don't change it.\n'''Run this file to run the application'''\nfrom MainUI import MainUI\nfrom CNN import CNN\nfrom KNN import KNN\nimport os\n# Change the model type variable value to \"CNN\" to use the Convolutional Neural Network\n# Change the model type variable value to \"KNN\" to use the K Nearest Neighbours Classifier\nmodeltype = \"KNN\"\nType the below command to run the Application. You need to be connected to the Internet and it might take 5-10 minutes to create the knn.sav file so please wait patiently. This delay is only during the first run as once created, the application will use the local file\npython Run.py\nThe GUI Homepage that opens up as soon as you run the application.\nYou need to select an image of a Sudoku Puzzle through the GUI Home Page.\nOnce you press Next, a number of stages of image processing take place which are displayed by the GUI leading up to recognition. Here are snapshots of two of the stages:\nFor recognition, a CNN or KNN can be used. This option can be toggled as mentioned in the first point. Once recognized, the board is displayed and you can rectify any wrongly recognized entries in the board.\nFinally click on reveal solution to display the solution.\nWorking\nImage Preprocessing\nGaussian Blurring Blurring using a Gaussian function. This is to reduce noise and detail.\nAdaptive Gaussian Thresholding Adaptive thresholding with a Gaussian Function to account for different illuminations in different parts of the image.\nInverting to make the digits and lines white while making the background black.\nDilation with a plus shaped 3X3 Kernel to fill out any cracks in the board lines and thicken the board lines.\nFlood Filling Since the board will probably be the largest blob a.k.a connected component with the largest area, floodfilling from different seed points and finding all connected components followed by finding the largest floodfilled area region will give the board.\nThe largest blob a.k.a the board is found after the previous step. Let's call this the outerbox\nEroding the grid a bit to undo the effects of the dilation on the outerbox that we did earlier.\nHough Line Transform to find all the lines in the detected outerbox.\nMerging related lines. The lines found by the Hough Transform that are close to each other are fused together.\nFinding the Extreme lines . We find the border lines by choosing the nearest line from the top with slope almost 0 as the upper edge, the nearest line from the bottom with slope almost 0 as the lower edge, the nearest line from the left with slope almost infinity as the left edge and the nearest line from the right with slope almost infinity as the right edge.\nFinding the four intersection points. The four intersection points of these lines are found and plotted along with the lines.\nWarping perspective. We find the perspective matrix using the end points, correct the perspective and crop the board out of the original image.\nThresholding and Inverting the grid. The cropped image from the previous step is adaptive thresholded and inverted.\nSlicing the grid into 81 slices to get images of each cell of the Sudoku board.\nBlackfilling and centering the number. Any white patches other than the number are removed by floodfilling with black from the outer layer points as seeds. Then the approximate bounding box of the number is found and centered in the image.\nRecognition\nConvolutional Neural Network\nRead about CNNs here\nLayers A Convolution Layer, a Max Pooling layer flattened into a hidden layer followed by some Dropout Regularization, another hidden layer and finally the output layer. Each of the inner layer uses ReLu while the output layer uses softmax.\nCompilation Adam optimizer and sparse categorical cross entropy loss.\nTraining The model is trained on the MNIST handwritten digits dataset which has around 70,000 28X28 images.\nAccuracy Around 98 percent accuracy on the test set.\nK Nearest Neighbours\nRead about KNN here\nK value used is 3.\nTraining Trained on the MNIST handwritten digits dataset which has around 70,000 28X28 images.\nAccuracy Around 97 percent accuracy on the test set.\nToDo\nImprove Accuracy.\nResolve Bugs/Issues if any found.\nOptimize Code to make it faster.\nContributing\nContributions are welcome \ud83d\ude04\nPull requests\nJust a few guidelines:\nWrite clean code with appropriate comments and add suitable error handling.\nTest the application and make sure no bugs/ issues come up.\nOpen a pull request and I will be happy to acknowledge your contribution after some checking from my side.\nIssues\nIf you find any bugs/issues, raise an issue.",
      "link": "https://github.com/neeru1207/AI_Sudoku"
    },
    {
      "autor": "awesome-fashion-ai",
      "date": "NaN",
      "content": "awesome-fashion-ai\nA curated list of research papers, datasets, tools, conferences, workshops related to AI for fashion and e-commerce.\nTable of Contents\nPapers\nWorkshops\nTutorials\nDatasets\nMiscellaneous\nPapers\nAreas\nFashion Embeddings\nPersonalisation/Recommendation/Outfit Composition/Compatibility\nVisual Search/Visual Recommendation/Visual Retrieval\nNatural Language Understanding in Fashion\nFashion Image Object Detection/Classification/Parsing/Segmentation/Attribute Manipulation\nRetail Insights/Trends/Forecasting/Inventory Management\nImage Generation/Image Manipulation in Fashion/Style Transfer\nStyling/Occasion\nSocial Media\nSizing/Virtual Trial Room\nVideo\nMultimodal\nClothing Model\nFashion Embeddings\nSemi-Supervised Visual Representation Learning for Fashion Compatibility ACM RecSys 2021\nFashion Style in 128 Floats: Joint Ranking and Classification using Weak Data for Feature Extraction\nLearning Type-Aware Embeddings for Fashion Compatibility, ECCV, 2018\nStyle2Vec: Representation Learning for Fashion Items from Style Sets\nContext-Aware Visual Compatibility Prediction\nPersonalisation/Recommendation/Outfit Composition/Compatibility\nHi, magic closet, tell me what to wear!, MM, 2012\nFashion is Taking Shape: Understanding Clothing Preference Based on Body Shape From Online Sources\nCreating Capsule Wardrobes from Fashion Images\nNeuroaesthetics in Fashion: Modeling the Perception of Fashionability\nInterpretable Partitioned Embedding for Customized Fashion Outfit Composition\nVisually-Aware Fashion Recommendation and Design with Generative Image Models\nAn LSTM-Based Dynamic Customer Model for Fashion Recommendation\nProduct Characterisation towards Personalisation: Learning Attributes from Unstructured Data to Recommend Fashion Products\nMining Fashion Outfit Composition Using An End-to-End Deep Learning Approach on Set Data\nLearning Fashion Compatibility with Bidirectional LSTMs\nLearning Visual Clothing Style with Heterogeneous Dyadic Co-occurrences\nRecommending Outfits from Personal Closet\nToward Explainable Fashion Recommendation\nVisual Search/Visual Recommendation/Visual Retrieval\nStudio2Shop: from studio -----> photo !!!  shoots to fashion articles\nLearning Attribute Representations with Localization for Flexible Fashion Search\nLearning the Latent \u201cLook\u201d: Unsupervised Discovery of a Style-Coherent Embedding from Fashion Images\nAutomatic Spatially-aware Fashion Concept Discovery\nLeveraging Weakly Annotated Data for Fashion Image Retrieval and Label Prediction\nLarge Scale Visual Recommendations From Street Fashion Images\nLearning Unified Embedding for Apparel Recognition\nDeep Learning based Large Scale Visual Recommendation and Search for E-Commerce\nVisual Search at eBay\nVisually-Aware Fashion Recommendation and Design with Generative Image Models\nCross-domain Image Retrieval with a Dual Attribute-aware Ranking Network\nImage-based Recommendations on Styles and Substitutes\nRunway to Realway: Visual Analysis of Fashion\nGetting the look: clothing recognition and segmentation for automatic product suggestions in everyday -----> photo !!! s, ICMR 2013\nDeepStyle: Multimodal Search Engine for Fashion and Interior Design\nNatural Language Understanding in Fashion\nA Hierarchical Deep Learning Natural Language Parser for Fashion\n\"Let me convince you to buy my product ... \": A Case Study of an Automated Persuasive System for Fashion Products\n\"Designing the Future of Personal Fashion\"\nDeep Recurrent Neural Networks for Product Attribute Extraction in eCommerce\nFashion Image Object Detection/Classification/Parsing/Segmentation/Attribute Manipulation/Landmark Detection\nHow To Extract Fashion Trends From Social Media?\nAttentive Fashion Grammar Network for Fashion Landmark Detection and Clothing Category Classification\nMemory-Augmented Attribute Manipulation Networks for Interactive Fashion Search\nDeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations\nA Unified Model with Structured Output for Fashion Images Classification\nProduct Characterisation towards Personalisation\nFusing Hierarchical Convolutional Features for Human Body Segmentation and Clothing Fashion Classification\nHow To Extract Fashion Trends From Social Media? A Robust Object Detector With Support For Unsupervised Learning\nUnconstrained Fashion Landmark Detection via Hierarchical Recurrent Transformer Networks\nFashion Apparel Detection: The Role of Deep Convolutional Neural Network and Pose-dependent Priors\nParsing Clothing in Fashion Photographs\nPaper Doll Parsing: Retrieving Similar Styles to Parse Clothing Items\nTwo-Stream Multi-Task Network for Fashion Recognition\nSpatial-Aware Non-Local Attention for Fashion Landmark Detection\nSemantic Segmentation of Fashion Images Using Feature Pyramid Networks\nRetail Insights/Trends/Forecasting/Inventory Management\nFashionBrain Project: A Vision for Understanding Europe\u2019s Fashion Data Universe\nFashion Forward: Forecasting Visual Style in Fashion\nWhen Fashion Meets Big Data: Discriminative Mining of Best Selling Clothing Features\nTowards Predicting the Likeability of Fashion Images\nWho Leads the Clothing Fashion: Style, Color, or Texture? A Computational Study\nRobust Order Scheduling in the Fashion Industry: A Multi-Objective Optimization Approach\nChanging Fashion Culture\nSales Potential: Modelling Sellability of Visual Aesthetics of a Fashion Product\nARMDN: Associative and Recurrent Mixture Density Networks for eRetail Demand Forecasting\nImage Generation/Image Manipulation in Fashion/Style Transfer\nJoint Discriminative and Generative Learning for Person Re-identification, CVPR 2019 [Project] [Paper] [YouTube] [Bilibili] [Poster]\nThe Conditional Analogy GAN: Swapping Fashion Articles on People Images\nLanguage Guided Fashion Image Manipulation with Feature-wise Transformations\nSwapNet: Image Based Garment Transfer, ECCV 2018\nCompatible and Diverse Fashion Image Inpainting\nFashion++: Minimal Edits for Outfit Improvement\nGenerative Modelling of Semantic Segmentation Data in the Fashion Domain\nStyling/Occasion\nHipster Wars: Discovering Elements of Fashion Styles\nSocial Media\nChic or Social: Visual Popularity Analysis in OnlineFashion Networks\nIdentifying Fashion Accounts in Social Networks\nSizing/Virtual Trial Room\nDecomposing Fit Semantics for Product Size Recommendation in Metric Spaces\nM2E-Try On Net: Fashion from Model to Everyone\nVideo\nDress like a Star: Retrieving Fashion Products from Videos\nVideo2Shop: Exactly Matching Clothes in Videos to Online Shopping Images, CVPR, 2017\nMultimodal\nDeepStyle: Multimodal Search Engine for Fashion and Interior Design\nDialog/Conversation\nNetizen-Style Commenting on Fashion Photos: Dataset and Diversity Measures\nFashion Conversation Data on Instagram\nClothing Model\nDeepWrinkles: Accurate and Realistic Clothing Modeling, ECCV 2018\nLearning a Shared Shape Space for Multimodal Garment Design, SIGGRAPH Asia 2018\nGarnet: A Two-stream Network for Fast and Accurate 3D Cloth Draping, ICCV 2019\nLearning-Based Animation of Clothing for Virtual Try-On, Eurographics 2019\nTailorNet :Predicting Clothing in 3D as a Function of Human Pose, Shape and Garment Style, CVPR 2020\nSIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size Sensitive 3D Clothing, ECCV 2020\n3D Clothing from Images\nMULTI-GARMENT NET: LEARNING TO DRESS 3D PEOPLE FROM IMAGES, ICCV 2019\nDeep Fashion3D: A Dataset and Benchmark for 3D Garment Reconstruction from Single Images, ECCV 2020\nBCNet: Learning Body and Cloth Shape from A Single Image, ECCV 2020\nWorkshops\nKDD Workshop on AI for Fashion 2020, 2019, 2018, 2017, 2016\nICCV/ECCV Workshop on Computer Vision for Fashion, Art and Design 2020, 2019 2018, 2017\nSIGIR Workshop On eCommerce 2018, 2017\nRecommender Systems in Fashion 2020, 2019\nTutorials\nConcept to Code: Deep Learning for Fashion Recommendation..\nOrganizers: Omprakash Sonie, Muthusamy Chelliah and Shamik Sural, The Web Conference, 2019\nDatasets\nLarge-scale Fashion (DeepFashion)\nStreet2Shop\nFashionista\nPaperdoll\nFashion MNIST\nFashion Takes Shape\nModaNet paper\nDeepFashion2,paper\niMaterialist-Fashion\nClothing Fit Dataset for Size Recommendation\nMULTI-GARMENT NET: LEARNING TO DRESS 3D PEOPLE FROM IMAGES, ICCV 2019\nTailorNet: Predicting Garment in 3D as a Function of Human Pose, Shape and Garment Style, CVPR 2020\nDeep Fashion3D: A Dataset and Benchmark for 3D Garment Reconstruction from Single Images, ECCV 2020\nSIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size Sensitive 3D Clothing, ECCV 2020\nMiscellaneous\nFashion-Gen: The Generative Fashion Dataset and Challenge\nBrand > Logo: Visual Analysis of Fashion Brands\nAuthor\nAyushi Dalmia\n###License MIT",
      "link": "https://github.com/ayushidalmia/awesome-fashion-ai"
    },
    {
      "autor": "YOLO-CoreML-MPSNNGraph",
      "date": "NaN",
      "content": "YOLO with Core ML and MPSNNGraph\nThis is the source code for my blog post YOLO: Core ML versus MPSNNGraph.\nYOLO is an object detection network. It can detect multiple objects in an image and puts bounding boxes around these objects. Read my other blog post about YOLO to learn more about how it works.\nPreviously, I implemented YOLO in Metal using the Forge library. Since then Apple released Core ML and MPSNNGraph as part of the iOS 11 beta. So I figured, why not try to get YOLO running on these two other technology stacks too?\nIn this repo you'll find:\nTinyYOLO-CoreML: A demo app that runs the Tiny YOLO neural network on Core ML.\nTinyYOLO-NNGraph: The same demo app but this time it uses the lower-level graph API from Metal Performance Shaders.\nConvert: The scripts needed to convert the original DarkNet YOLO model to Core ML and MPS format.\nTo run the app, just open the xcodeproj file in Xcode 9 or later, and run it on a device with iOS 11 or better installed.\nThe reported \"elapsed\" time is how long it takes the YOLO neural net to process a single image. The FPS is the actual throughput achieved by the app.\nNOTE: Running these kinds of neural networks eats up a lot of battery power. To measure the maximum speed of the model, the setUpCamera() method in ViewController.swift configures the -----> camera !!!  to run at 240 FPS, if available. In a real app, you'd use at most 30 FPS and possibly limit the number of times per second it runs the neural net to 15 or less (i.e. only process every other frame).\nTip: Also check out this repo for YOLO v3. It works the same as this repo, but uses the full version of YOLO v3!\niOS 12 and VNRecognizedObjectObservation\nThe code in my blog post and this repo shows how take the MLMultiArray output from TinyYOLO and interpret it in your app. That was the only way to do it with iOS 11, but as of iOS 12 there is an easier solution.\nThe Vision framework in iOS 12 directly supports YOLO-like models. The big advantage is that these do the bounding box decoding and non-maximum suppression (NMS) inside the Core ML model. All you need to do is pass in the image and Vision will give you the results as one or more VNRecognizedObjectObservation objects. No more messing around with MLMultiArrays.\nIt's also really easy to train such models using Turi Create. It combines TinyYOLO v2 and the new NonMaximumSuppression model type into a so-called pipeline model.\nThe good news is that this new Vision API also supports other object detection models!\nI added a chapter to my book Core ML Survival Guide that shows exactly how this works. In the book you\u2019ll see how to add this same functionality to MobileNetV2 + SSDLite, so that you get VNRecognizedObjectObservation predictions for that model too. The book has lots of other great tips on using Core ML, so check it out! \ud83d\ude04\nIf you're not ready to go all-in on iOS 12 yet, then read on...\nConverting the models\nNOTE: You don't need to convert the models yourself. Everything you need to run the demo apps is included in the Xcode projects already.\nIf you're interested in how the conversion was done, there are three conversion scripts:\nYAD2K\nThe original network is in Darknet format. I used YAD2K to convert this to Keras. Since coremltools currently requires Keras 1.2.2, the included YAD2K source code is actually a modified version that runs on Keras 1.2.2 instead of 2.0.\nFirst, set up a virtualenv with Python 3:\nvirtualenv -p /usr/local/bin/python3 yad2kenv\nsource yad2kenv/bin/activate\npip3 install tensorflow\npip3 install keras==1.2.2\npip3 install h5py\npip3 install pydot-ng\npip3 install pillow\nbrew install graphviz\nRun the yad2k.py script to convert the Darknet model to Keras:\ncd Convert/yad2k\npython3 yad2k.py -p ../tiny-yolo-voc.cfg ../tiny-yolo-voc.weights model_data/tiny-yolo-voc.h5\nTo test the model actually works:\npython3 test_yolo.py model_data/tiny-yolo-voc.h5 -a model_data/tiny-yolo-voc_anchors.txt -c model_data/pascal_classes.txt\nThis places some images with the computed bounding boxes in the yad2k/images/out folder.\ncoreml.py\nThe coreml.py script takes the tiny-yolo-voc.h5 model created by YAD2K and converts it to TinyYOLO.mlmodel. Note: this script requires Python 2.7 from /usr/bin/python (i.e. the one that comes with macOS).\nTo set up the virtual environment:\nvirtualenv -p /usr/bin/python2.7 coreml\nsource coreml/bin/activate\npip install tensorflow\npip install keras==1.2.2\npip install h5py\npip install coremltools\nRun the coreml.py script to do the conversion (the paths to the model file and the output folder are hardcoded in the script):\npython coreml.py\nnngraph.py\nThe nngraph.py script takes the tiny-yolo-voc.h5 model created by YAD2K and converts it to weights files used by MPSNNGraph. Requires Python 3 and Keras 1.2.2.",
      "link": "https://github.com/hollance/YOLO-CoreML-MPSNNGraph"
    },
    {
      "autor": "CNNGestureRecognizer",
      "date": "NaN",
      "content": "Hey there reader,\nIf you find my work useful and you have implemented something based on this project, then please do share about your project on my email. This will motivate me. Thanks !\nCNNGestureRecognizer\nGesture recognition via CNN neural network implemented in Keras + Theano + OpenCV\nKey Requirements: Python 3.6.1 OpenCV 3.4.1 Keras 2.0.2 Tensorflow 1.2.1 Theano 0.9.0 (obsolete and not supported any further)\nSuggestion: Better to download Anaconda as it will take care of most of the other packages and easier to setup a virtual workspace to work with multiple versions of key packages like python, opencv etc.\nNew changes\nI have uploaded few more changes to this repo -\nProject is Python3 compatible now.\nAdded TensorFlow support, as Theano's development has been stopped.\nAdded a new background subtraction filter, which is by far the best performing filter for this project\nAdded lots of performance improving changes. There is now literally no FPS drop when prediction mode is enabled\nAn in-app graph plotting has been added to observe the probability of the gesture predictions\nRepo contents\ntrackgesture.py : The main script launcher. This file contains all the code for UI options and OpenCV code to capture -----> camera !!!  contents. This script internally calls interfaces to gestureCNN.py.\ngestureCNN.py : This script file holds all the CNN specific code to create CNN model, load the weight file (if model is pretrained), train the model using image samples present in ./imgfolder_b, visualize the feature maps at different layers of NN (of pretrained model) for a given input image present in ./imgs folder.\nimgfolder_b : This folder contains all the 4015 gesture images I took in order to train the model.\n- Note: I have replaced ori_4015imgs_weights.hdf5 weight file with these two OS specific weight files.\npretrained_weights_MacOS.hdf5 : This is pretrained weight file on MacOS. Due to its large size (150 MB), its hosted seperately on my google driver link - https://drive.google.com/file/d/1j7K96Dkatz6q6zr5RsQv-t68B3ZOSfh0/view\npretrained_weights_WinOS.hdf5 : This is pretrained weight file on Windows. Due to its large size (150 MB), its hosted seperately on my google driver link - https://drive.google.com/file/d/1PA7rJxHYQsW5IvcZAGeoZ-ExYSttFuGs/view\nimgs - This is an optional folder of few sample images that one can use to visualize the feature maps at different layers. These are few sample images from imgfolder_b only.\nori_4015imgs_acc.png : This is just a pic of a plot depicting model accuracy Vs validation data accuracy after I trained it.\nori_4015imgs_loss.png : This is just a pic of a plot depicting model loss Vs validation loss after I training.\nUsage\nOn Mac\neg: With Theano as backend\n$ KERAS_BACKEND=tensorflow python trackgesture.py\nOn Windows\neg: With Tensorflow as backend\n> set \"KERAS_BACKEND=tensorflow\"\n> python trackgesture.py\nWe are setting KERAS_BACKEND to change backend to Theano, so in case you have already done it via Keras.json then no need to do that. But if you have Tensorflow set as default then this will be required.\nFeatures\nThis application comes with CNN model to recognize upto 5 pretrained gestures:\nOK\nPEACE\nSTOP\nPUNCH\nNOTHING (ie when none of the above gestures are input)\nThis application provides following functionalities:\nPrediction : Which allows the app to guess the user's gesture against pretrained gestures. App can dump the prediction data to the console terminal or to a json file directly which can be used to plot real time prediction bar chart (you can use my other script - https://github.com/asingh33/LivePlot)\nNew Training : Which allows the user to retrain the NN model. User can change the model architecture or add/remove new gestures. This app has inbuilt options to allow the user to create new image samples of user defined gestures if required.\nVisualization : Which allows the user to see feature maps of different NN layers for a given input gesture image. Interesting to see how NN works and learns things.\nDemo\nYoutube link - https://www.youtube.com/watch?v=CMs5cn65YK8\nGesture Input\nI am using OpenCV for capturing the user's hand gestures. In order to simply things I am doing post processing on the captured images to highlight the contours & edges. Like applying binary threshold, blurring, gray scaling.\nI have provided two modes of capturing:\nBinary Mode : In here I first convert the image to grayscale, then apply a gaussian blur effect with adaptive threshold filter. This mode is useful when you have an empty background like a wall, whiteboard etc.\nSkinMask Mode : In this mode, I first convert the input image to HSV and put range on the H,S,V values based on skin color range. Then apply errosion followed by dilation. Then gaussian blur to smoothen out the noises. Using this output as a mask on original input to mask out everything other than skin colored things. Finally I have grayscaled it. This mode is useful when there is good amount of light and you dont have empty background.\nBinary Mode processing\ngray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\nblur = cv2.GaussianBlur(gray,(5,5),2)\nth3 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)\nret, res = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\nSkindMask Mode processing\nhsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n#Apply skin color range\nmask = cv2.inRange(hsv, low_range, upper_range)\nmask = cv2.erode(mask, skinkernel, iterations = 1)\nmask = cv2.dilate(mask, skinkernel, iterations = 1)\n#blur\nmask = cv2.GaussianBlur(mask, (15,15), 1)\n#cv2.imshow(\"Blur\", mask)\n#bitwise and mask original frame\nres = cv2.bitwise_and(roi, roi, mask = mask)\n# color to grayscale\nres = cv2.cvtColor(res, cv2.COLOR_BGR2GRAY)\nCNN Model used\nThe CNN I have used for this project is pretty common CNN model which can be found across various tutorials on CNN. Mostly I have seen it being used for Digit/Number classfication based on MNIST database.\nmodel = Sequential()\nmodel.add(Conv2D(nb_filters, (nb_conv, nb_conv),\npadding='valid',\ninput_shape=(img_channels, img_rows, img_cols)))\nconvout1 = Activation('relu')\nmodel.add(convout1)\nmodel.add(Conv2D(nb_filters, (nb_conv, nb_conv)))\nconvout2 = Activation('relu')\nmodel.add(convout2)\nmodel.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\nThis model has following 12 layers -\n_________________________________________________________________\nLayer (type) Output Shape Param #\n=================================================================\nconv2d_1 (Conv2D) (None, 32, 198, 198) 320\n_________________________________________________________________\nactivation_1 (Activation) (None, 32, 198, 198) 0\n_________________________________________________________________\nconv2d_2 (Conv2D) (None, 32, 196, 196) 9248\n_________________________________________________________________\nactivation_2 (Activation) (None, 32, 196, 196) 0\n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 32, 98, 98) 0\n_________________________________________________________________\ndropout_1 (Dropout) (None, 32, 98, 98) 0\n_________________________________________________________________\nflatten_1 (Flatten) (None, 307328) 0\n_________________________________________________________________\ndense_1 (Dense) (None, 128) 39338112\n_________________________________________________________________\nactivation_3 (Activation) (None, 128) 0\n_________________________________________________________________\ndropout_2 (Dropout) (None, 128) 0\n_________________________________________________________________\ndense_2 (Dense) (None, 5) 645\n_________________________________________________________________\nactivation_4 (Activation) (None, 5) 0\n=================================================================\nTotal params: 39,348,325.0 Trainable params: 39,348,325.0\nTraining\nIn version 1.0 of this project I had used 1204 images only for training. Predictions probability was ok but not satisfying. So in version 2.0 I increased the training image set to 4015 images i.e. 803 image samples per class. Also added an additional class 'Nothing' along with the previous 4 gesture classes.\nI have trained the model for 15 epochs.\nVisualization\nCNN is good in detecting edges and thats why its useful for image classificaion kind of problems. In order to understand how the neural net is understanding the different gesture input its possible to visualize the layer feature map contents.\nAfter launching the main script choose option 3 for visualizing different or all layer for a given image (currently it takes images from ./imgs, so change it accordingly)\nWhat would you like to do ?\n1- Use pretrained model for gesture recognition & layer visualization\n2- Train the model (you will require image samples for training under .\\imgfolder)\n3- Visualize feature maps of different layers of trained model\n3\nWill load default weight file\nImage number 7\nEnter which layer to visualize -1\n(4015, 40000)\nPress any key\nsamples_per_class - 803\nTotal layers - 12\nDumping filter data of layer1 - Activation\nDumping filter data of layer2 - Conv2D\nDumping filter data of layer3 - Activation\nDumping filter data of layer4 - MaxPooling2D\nDumping filter data of layer5 - Dropout\nCan't dump data of this layer6- Flatten\nCan't dump data of this layer7- Dense\nCan't dump data of this layer8- Activation\nCan't dump data of this layer9- Dropout\nCan't dump data of this layer10- Dense\nCan't dump data of this layer11- Activation\nPress any key to continue\nTo understand how its done in Keras, check visualizeLayer() in gestureCNN.py\nlayer = model.layers[layerIndex]\nget_activations = K.function([model.layers[0].input, K.learning_phase()], [layer.output,])\nactivations = get_activations([input_image, 0])[0]\noutput_image = activations\nLayer 4 visualization for PUNCH gesture\nLayer 2 visualization for STOP gesture\nConclusion\nSo where to go from here? Well I thought of testing out the responsiveness of NN predictions and games are good benchmark. On MAC I dont have any games installed but then this Chrome Browser Dino Jump game came handy. So I bound the 'Punch' gesture with jump action of the Dino character. Basically can work with any other gesture but felt Punch gesture is easy. Stop gesture was another candidate.\nWell here is how it turned out :)\nWatch full video - https://www.youtube.com/watch?v=lnFPvtCSsLA&t=49s\nIn case you want to cite my work\nAbhishek Singh,\u201dasingh33/CNNGestureRecognizer: CNNGestureRecognizer (Version 1.3.0)\u201d, Zenodo. http://doi.org/10.5281/zenodo.1064825, Nov. 2017.\nDo tell me how you used this work in your project. Would love to see your work. Good Luck!\nDont forget to check out my other github project where I used this framework and applied SuperVised machine learning technique to train the Chrome Browser's TRex character :) https://techkblog.com/teach-an-ai-to-play-game-using-supervised-learning/ Youtube link - https://youtu.be/ZZgvklkQrss",
      "link": "https://github.com/asingh33/CNNGestureRecognizer"
    },
    {
      "autor": "DeepCamera",
      "date": "NaN",
      "content": "AI Face Recognition/Person Detection NVR\nMachine Learning On The Edge, Turn your -----> Camera !!!  into AI-powered with Edge AI device\nSharpAI is open source stack for machine learning engineering with private deployment and AutoML for edge computing. DeepCamera is application of SharpAI designed for connect computer vision model to surveillance camera. Developers can run same code on Raspberry Pi/Android/PC/AWS to boost your AI production development.\nFeatures\nEnd to End data collection and training pipeline\nFFMpeg with Nvidia Nano hardware decoder\nFace Detector with Nvidia Nano GPU TensorRT MTCNN\nFace Embedding with Nvidia Nano GPU Pytorch InsightFace\nPerson Detection with GPU\nIntegrate with telegram bot API\nDeepCamera Architecture\nDemo On Youtube\nGet Started on Jetson Nano\nInstall Docker-compose\nsudo apt-get install -y libhdf5-dev python3 python3-pip\npip3 install -U pip\nsudo pip3 install docker-compose==1.27.4\nGet source code\ngit clone https://github.com/SharpAI/DeepCamera\nCreate Token for Telegram Bot\nCreate Telegram Bot through @BotFather\nSet Telegram Token in Configure File\nSend message to the new bot you created\nStart DeepCamera\ncd DeepCamera\n./run-on-nano.sh start\nConnect To Camera through RTSP URL\nOn Jetson Nano, Access to 8080 port. http://localhost:8080\nDefault username and password is:\nusername: user@sharpaibox.com\npassword: SharpAI2018\nTested Camera: DaHua / Lorex / AMCREST, URL Path: /cam/realmonitor?channel=1&subtype=0 Port: 554\nWhen setup done, you will see live view on web page, when detected person in camera, you will receive video clips on telegram.\nLabel on Web GUI, train face recognition model on device\ncat docker/workaipython/ro_serialno\n82f28703d001\n82f28703d001 is device ID.\nAccess http://165.232.62.29:3000/\nDetail information\nCamera streaming URL format\nUse Mobile APP to label and train face recognition model on device\nGet device serial number\ncat docker/workaipython/ro_serialno\n82f28703d001\n82f28703d001 is device ID.\nGenerate QRCode of device ID\nDownload and install SharpAI Mobile APP\nConfigure on Mobile APP\nDevelop your own Application GUI with DeepCamera API Server\nIf you don't like the GUI or you want to develop your own application.\nYou can use following API:\nGet device serial number\ncat docker/workaipython/ro_serialno\n82f28703d001\n82f28703d001 is device ID\nCreate User on API Server\nREST API:\ncurl -X POST -H \"Content-type: application/json\" http://localhost:3000/api/v1/sign-up -d '{\"username\": \"test11\", \"email\": \"xxxx@xxx.xx\", \"password\": \"xxxxxx\"}'\nResponse:\n{\n\"success\": true\n}\nGet Token of created user\nREST API:\ncurl -X POST -H \"Content-type: application/json\" http://localhost:3000/api/v1/login/ -d '{\"username\": \"test11\", \"email\": \"xxxx@xxx.xx\", \"password\": \"123456\"}'\nResponse:\n{\n\"status\": \"success\",\n\"data\": {\n\"authToken\": \"t6QsPaU3VdbfUQMkNIf6I3MDtox29WLrPJRAKkOCfpc\",\n\"userId\": \"tiK8RYG87sGJAErdB\"\n}\n}\nCreate Group on API Server\nRest API:\nFill in X-Auth-Token and X-User-Id in previous response.\ncurl -X POST -H \"X-Auth-Token: t6QsPaU3VdbfUQMkNIf6I3MDtox29WLrPJRAKkOCfpc\" -H \"X-User-Id: tiK8RYG87sGJAErdB\" http://localhost:3000/api/v1/groups -d \"name=group01\"\nResponse:\n{\n\"groupId\": \"e309ff8c7a3a8ceb4011e86e\"\n}\nAdd device to Group on API Server\nREST API: Replace X-Auth-Token and X-User-Id. Replace group id in requesting URL: http://localhost:3000/api/v1/groups/`e309ff8c7a3a8ceb4011e86e`/devices\ncurl -X POST -H \"X-Auth-Token: t6QsPaU3VdbfUQMkNIf6I3MDtox29WLrPJRAKkOCfpc\" -H \"X-User-Id: tiK8RYG87sGJAErdB\" -H \"Content-type: application/json\" http://localhost:3000/api/v1/groups/e309ff8c7a3a8ceb4011e86e/devices -d '{\"uuid\": \"82f28703d001\", \"deviceName\": \"testDevice\", \"name\":\"testdevice\",\"type\": \"inout\"}'\nResponse:\n{\n\"success\": true\n}\nThen restart DeepCamera service.\nAPI Server document can be found here: SharpAI/ApiServer\nYou can also develop/debug code on your PC How to run DeepCamera On PC\nDeploy your own API_Server on X86/Cloud Server\nNow, you got the idea of DeepCamera,\nthe public testing server is open to the internet.\nYou can deploy your own API server on your OWN device.\ngit clone https://github.com/SharpAI/DeepCamera\ncd DeepCamera\n./start-cloud.sh start\nYou need ip address of private cloud server on next step (replace ip address to <Server_IP> on next step).\nIf you don't want to setup your own server for now, a test server can be used for evaluation, the ip address of test server is 165.232.62.29\nIf your have any question or feature request, please feel free to join slack for commercial support\nSlack\nClick to join sharpai slack channel\nFeatures\nPorting to Jetson Nano\nHigh accurate Face Recognition\nFace Detection\nInference on ARM Mali GPU\nSupport Android TF Lite(GPU/CPU/NPU)\nSupport open source embedded linux\nControl from mobile application\nManagement System for devices\nPush Notification to Mobile Device\nObject Detection\nDistributed System based on celery\nPlugin to process video by Shinobi CCTV\nApplication on Android to decode video with hw acc\nMotion Detection with Android GPU\nLable and train from Mobile to Edge Device\nNative raspberry pi camera support\nLabelling server and application is down, need BYOD document API server repo\nImage upload to AWS or on premise AWS compatiable server(MINIO)\nContributions",
      "link": "https://github.com/SharpAI/DeepCamera"
    },
    {
      "autor": "Awesome-Interaction-aware-Trajectory-Prediction",
      "date": "NaN",
      "content": "Awesome Interaction-aware Behavior and Trajectory Prediction\nThis is a checklist of state-of-the-art research materials (datasets, blogs, papers and public codes) related to trajectory prediction. Wish it could be helpful for both academia and industry. (Still updating)\nMaintainers: Jiachen Li, Hengbo Ma, Jinning Li (University of California, Berkeley)\nEmails: {jiachen_li, hengbo_ma, jinning_li}@berkeley.edu\nPlease feel free to pull request to add new resources or send emails to us for questions, discussion and collaborations.\nNote: Here is also a collection of materials for reinforcement learning, decision making and motion planning.\nPlease consider citing our work if you found this repo useful:\n@inproceedings{li2020evolvegraph,\ntitle={EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational Reasoning},\nauthor={Li, Jiachen and Yang, Fan and Tomizuka, Masayoshi and Choi, Chiho},\nbooktitle={2020 Advances in Neural Information Processing Systems (NeurIPS)},\nyear={2020}\n}\n@inproceedings{li2019conditional,\ntitle={Conditional Generative Neural System for Probabilistic Trajectory Prediction},\nauthor={Li, Jiachen and Ma, Hengbo and Tomizuka, Masayoshi},\nbooktitle={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\npages={6150--6156},\nyear={2019},\norganization={IEEE}\n}\nTable of Contents\nDatasets\nVehicles and Traffic\nPedestrians\nSport Players\nLiterature and Codes\nSurvey Papers\nPhysics Systems with Interaction\nIntelligent Vehicles & Traffic\nPedestrians\nMobile Robots\nSport Players\nBenchmark and Evaluation Metrics\nOthers\nDatasets\nVehicles and Traffic\nDataset Agents Scenarios Sensors\nINTERACTION Vehicles / cyclists/ people Roundabout / intersection Camera\nKITTI Vehicles / cyclists/ people Highway / rural areas Camera / LiDAR\nHighD Vehicles Highway Camera\nNGSIM Vehicles Highway Camera\nCyclists Cyclists Urban Camera\nnuScenes Vehicles Urban Camera / LiDAR / RADAR\nBDD100k Vehicles / cyclists / people Highway / urban Camera\nApolloscapes Vehicles / cyclists / people Urban Camera\nUdacity Vehicles Urban Camera\nCityscapes Vehicles/ people Urban Camera\nStanford Drone Vehicles / cyclists/ people Urban Camera\nArgoverse Vehicles / people Urban Camera / LiDAR\nTRAF Vehicles/buses/cyclists/bikes / people/animals Urban Camera\nLyft Level 5 Vehicles/cyclists/people Urban Camera/ LiDAR\nPedestrians\nDataset Agents Scenarios Sensors\nUCY People Zara / students -----> Camera !!! \nETH People Urban -----> Camera !!! \nVIRAT People / vehicles Urban -----> Camera !!! \nKITTI Vehicles / cyclists/ people Highway / rural areas -----> Camera !!!  / LiDAR\nATC People Shopping center Range sensor\nDaimler People From moving vehicle -----> Camera !!! \nCentral Station People Inside station -----> Camera !!! \nTown Center People Urban street -----> Camera !!! \nEdinburgh People Urban -----> Camera !!! \nCityscapes Vehicles/ people Urban -----> Camera !!! \nArgoverse Vehicles / people Urban -----> Camera !!!  / LiDAR\nStanford Drone Vehicles / cyclists/ people Urban -----> Camera !!! \nTrajNet People Urban -----> Camera !!! \nPIE People Urban -----> Camera !!! \nForkingPaths People Urban / Simulation -----> Camera !!! \nTrajNet++ People Urban -----> Camera !!! \nSport Players\nDataset Agents Scenarios Sensors\nFootball People Football field -----> Camera !!! \nNBA SportVU People Basketball Hall Camera\nNFL People American Football Camera\nLiterature and Codes\nSurvey Papers\nModeling and Prediction of Human Driver Behavior: A Survey, 2020. [paper]\nHuman Motion Trajectory Prediction: A Survey, 2019. [paper]\nA literature review on the prediction of pedestrian behavior in urban scenarios, ITSC 2018. [paper]\nSurvey on Vision-Based Path Prediction. [paper]\nAutonomous vehicles that interact with pedestrians: A survey of theory and practice. [paper]\nTrajectory data mining: an overview. [paper]\nA survey on motion prediction and risk assessment for intelligent vehicles. [paper]\nPhysics Systems with Interaction\nEvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational Reasoning, NeurIPS 2020. [paper]\nInteraction Templates for Multi-Robot Systems, IROS 2019. [paper]\nFactorised Neural Relational Inference for Multi-Interaction Systems, ICML workshop 2019. [paper] [code]\nPhysics-as-Inverse-Graphics: Joint Unsupervised Learning of Objects and Physics from Video, 2019. [paper]\nNeural Relational Inference for Interacting Systems, ICML 2018. [paper] [code]\nUnsupervised Learning of Latent Physical Properties Using Perception-Prediction Networks, UAI 2018. [paper]\nRelational inductive biases, deep learning, and graph networks, 2018. [paper]\nRelational Neural Expectation Maximization: Unsupervised Discovery of Objects and their Interactions, ICLR 2018. [paper]\nGraph networks as learnable physics engines for inference and control, ICML 2018. [paper]\nFlexible Neural Representation for Physics Prediction, 2018. [paper]\nA simple neural network module for relational reasoning, 2017. [paper]\nVAIN: Attentional Multi-agent Predictive Modeling, NIPS 2017. [paper]\nVisual Interaction Networks, 2017. [paper]\nA Compositional Object-Based Approach to Learning Physical Dynamics, ICLR 2017. [paper]\nInteraction Networks for Learning about Objects, Relations and Physics, 2016. [paper][code]\nIntelligent Vehicles & Traffic\nEvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational Reasoning, NeurIPS 2020. [paper]\nV2VNet- Vehicle-to-Vehicle Communication for Joint Perception and Prediction, ECCV 2020. [paper]\nSMART- Simultaneous Multi-Agent Recurrent Trajectory Prediction, ECCV 2020. [paper]\nSimAug- Learning Robust Representations from Simulation for Trajectory Prediction, ECCV 2020. [paper]\nLearning Lane Graph Representations for Motion Forecasting, ECCV 2020. [paper]\nImplicit Latent Variable Model for Scene-Consistent Motion Forecasting, ECCV 2020. [paper]\nDiverse and Admissible Trajectory Forecasting through Multimodal Context Understanding, ECCV 2020. [paper]\nSemantic Synthesis of Pedestrian Locomotion, ACCV 2020. [Paper]\nKernel Trajectory Maps for Multi-Modal Probabilistic Motion Prediction, CoRL 2019. [paper] [code]\nSocial-WaGDAT: Interaction-aware Trajectory Prediction via Wasserstein Graph Double-Attention Network, 2020. [paper]\nForecasting Trajectory and Behavior of Road-Agents Using Spectral Clustering in Graph-LSTMs, 2019. [paper] [code]\nJoint Prediction for Kinematic Trajectories in Vehicle-Pedestrian-Mixed Scenes, ICCV 2019. [paper]\nAnalyzing the Variety Loss in the Context of Probabilistic Trajectory Prediction, ICCV 2019. [paper]\nLooking to Relations for Future Trajectory Forecast, ICCV 2019. [paper]\nJointly Learnable Behavior and Trajectory Planning for Self-Driving Vehicles, IROS 2019. [paper]\nSharing Is Caring: Socially-Compliant Autonomous Intersection Negotiation, IROS 2019. [paper]\nINFER: INtermediate Representations for FuturE PRediction, IROS 2019. [paper] [code]\nDeep Predictive Autonomous Driving Using Multi-Agent Joint Trajectory Prediction and Traffic Rules, IROS 2019. [paper]\nNeuroTrajectory: A Neuroevolutionary Approach to Local State Trajectory Learning for Autonomous Vehicles, IROS 2019. [paper]\nUrban Street Trajectory Prediction with Multi-Class LSTM Networks, IROS 2019. [N/A]\nSpatiotemporal Learning of Directional Uncertainty in Urban Environments with Kernel Recurrent Mixture Density Networks, IROS 2019. [paper]\nConditional generative neural system for probabilistic trajectory prediction, IROS 2019. [paper]\nInteraction-aware multi-agent tracking and probabilistic behavior prediction via adversarial learning, ICRA 2019. [paper]\nGeneric Tracking and Probabilistic Prediction Framework and Its Application in Autonomous Driving, IEEE Trans. Intell. Transport. Systems, 2019. [paper]\nCoordination and trajectory prediction for vehicle interactions via bayesian generative modeling, IV 2019. [paper]\nWasserstein generative learning with kinematic constraints for probabilistic interactive driving behavior prediction, IV 2019. [paper]\nGRIP: Graph-based Interaction-aware Trajectory Prediction, ITSC 2019. [paper]\nAGen: Adaptable Generative Prediction Networks for Autonomous Driving, IV 2019. [paper]\nTraPHic: Trajectory Prediction in Dense and Heterogeneous Traffic Using Weighted Interactions, CVPR 2019. [paper], [code]\nMulti-Step Prediction of Occupancy Grid Maps with Recurrent Neural Networks, CVPR 2019. [paper]\nArgoverse: 3D Tracking and Forecasting With Rich Maps, CVPR 2019 [paper]\nRobust Aleatoric Modeling for Future Vehicle Localization, CVPR 2019. [paper]\nPedestrian occupancy prediction for autonomous vehicles, IRC 2019. [paper]\nContext-based path prediction for targets with switching dynamics, 2019.[paper]\nDeep Imitative Models for Flexible Inference, Planning, and Control, 2019. [paper]\nInfer: Intermediate representations for future prediction, 2019. [paper][code]\nMulti-agent tensor fusion for contextual trajectory prediction, 2019. [paper]\nContext-Aware Pedestrian Motion Prediction In Urban Intersections, 2018. [paper]\nGeneric probabilistic interactive situation recognition and prediction: From virtual to real, ITSC 2018. [paper]\nGeneric vehicle tracking framework capable of handling occlusions based on modified mixture particle filter, IV 2018. [paper]\nMulti-Modal Trajectory Prediction of Surrounding Vehicles with Maneuver based LSTMs, 2018. [paper]\nSequence-to-sequence prediction of vehicle trajectory via lstm encoder-decoder architecture, 2018. [paper]\nR2P2: A ReparameteRized Pushforward Policy for diverse, precise generative path forecasting, ECCV 2018. [paper]\nPredicting trajectories of vehicles using large-scale motion priors, IV 2018. [paper]\nVehicle trajectory prediction by integrating physics-and maneuver based approaches using interactive multiple models, 2018. [paper]\nMotion Prediction of Traffic Actors for Autonomous Driving using Deep Convolutional Networks, 2018. [paper]\nGenerative multi-agent behavioral cloning, 2018. [paper]\nDeep Sequence Learning with Auxiliary Information for Traffic Prediction, KDD 2018. [paper], [code]\nMultipolicy decision-making for autonomous driving via changepoint-based behavior prediction, 2017. [paper]\nProbabilistic long-term prediction for autonomous vehicles, IV 2017. [paper]\nProbabilistic vehicle trajectory prediction over occupancy grid map via recurrent neural network, ITSC 2017. [paper]\nDesire: Distant future prediction in dynamic scenes with interacting agents, CVPR 2017. [paper][code]\nImitating driver behavior with generative adversarial networks, 2017. [paper][code]\nInfogail: Interpretable imitation learning from visual demonstrations, 2017. [paper][code]\nLong-term planning by short-term prediction, 2017. [paper]\nLong-term path prediction in urban scenarios using circular distributions, 2017. [paper]\nDeep learning driven visual path prediction from a single image, 2016. [paper]\nUnderstanding interactions between traffic participants based on learned behaviors, 2016. [paper]\nVisual path prediction in complex scenes with crowded moving objects, CVPR 2016. [paper]\nA game-theoretic approach to replanning-aware interactive scene prediction and planning, 2016. [paper]\nIntention-aware online pomdp planning for autonomous driving in a crowd, ICRA 2015. [paper]\nOnline maneuver recognition and multimodal trajectory prediction for intersection assistance using non-parametric regression, 2014. [paper]\nPatch to the future: Unsupervised visual prediction, CVPR 2014. [paper]\nMobile agent trajectory prediction using bayesian nonparametric reachability trees, 2011. [paper]\nPedestrians\nSkeleton-Graph: Long-Term 3D Motion Prediction From 2D Observations Using Deep Spatio-Temporal Graph CNNs, ICCV 2021 The ROAD Challenge Workshop. [paper], [code]\nLearning Structured Representations of Spatial and Interactive Dynamics for Trajectory Prediction in Crowded Scenes, IEEE Robotics and Automation Letters 2021 [paper], [code]\nSocial NCE: Contrastive Learning of Socially-aware Motion Representations. [paper], [code]\nEvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational Reasoning, NeurIPS 2020. [paper]\nSpatio-Temporal Graph Transformer Networks for Pedestrian Trajectory Prediction, ECCV 2020. [paper]\nIt is not the Journey but the Destination- Endpoint Conditioned Trajectory Prediction, ECCV 2020. [paper]\nHow Can I See My Future? FvTraj: Using First-person View for Pedestrian Trajectory Prediction, ECCV 2020. [paper]\nDynamic and Static Context-aware LSTM for Multi-agent Motion Prediction, ECCV 2020. [paper]\nHuman Trajectory Forecasting in Crowds: A Deep Learning Perspective, 2020. [paper], [code]\nSimAug: Learning Robust Representations from 3D Simulation for Pedestrian Trajectory Prediction in Unseen Cameras, ECCV 2020. [paper], [code]\nDAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting, ICPR 2020. [paper] [code]\nDisentangling Human Dynamics for Pedestrian Locomotion Forecasting with Noisy Supervision, WACV 2020. [paper]\nSocial-WaGDAT: Interaction-aware Trajectory Prediction via Wasserstein Graph Double-Attention Network, 2020. [paper]\nSocial-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction, CVPR 2020. [Paper], [Code]\nThe Garden of Forking Paths: Towards Multi-Future Trajectory Prediction, CVPR 2020. [paper], [code/dataset]\nDisentangling Human Dynamics for Pedestrian Locomotion Forecasting with Noisy Supervision, WACV 2020. [paper]\nThe Trajectron: Probabilistic Multi-Agent Trajectory Modeling With Dynamic Spatiotemporal Graphs, ICCV 2019. [paper] [code]\nSTGAT: Modeling Spatial-Temporal Interactions for Human Trajectory Prediction, ICCV 2019. [paper] [code]\nInstance-Level Future Motion Estimation in a Single Image Based on Ordinal Regression, ICCV 2019. [paper]\nSocial and Scene-Aware Trajectory Prediction in Crowded Spaces, ICCV workshop 2019. [paper] [code]\nStochastic Sampling Simulation for Pedestrian Trajectory Prediction, IROS 2019. [paper]\nLong-Term Prediction of Motion Trajectories Using Path Homology Clusters, IROS 2019. [paper]\nStarNet: Pedestrian Trajectory Prediction Using Deep Neural Network in Star Topology, IROS 2019. [paper]\nLearning Generative Socially-Aware Models of Pedestrian Motion, IROS 2019. [paper]\nSituation-Aware Pedestrian Trajectory Prediction with Spatio-Temporal Attention Model, CVWW 2019. [paper]\nPath predictions using object attributes and semantic environment, VISIGRAPP 2019. [paper]\nProbabilistic Path Planning using Obstacle Trajectory Prediction, CoDS-COMAD 2019. [paper]\nHuman Trajectory Prediction using Adversarial Loss, hEART 2019. [paper], [code]\nSocial Ways: Learning Multi-Modal Distributions of Pedestrian Trajectories with GANs, CVPR 2019. [Precognition Workshop], [paper], [code]\nPeeking into the Future: Predicting Future Person Activities and Locations in Videos, CVPR 2019. [paper], [code]\nLearning to Infer Relations for Future Trajectory Forecast, CVPR 2019. [paper]\nTraPHic: Trajectory Prediction in Dense and Heterogeneous Traffic Using Weighted Interactions, CVPR 2019. [paper]\nWhich Way Are You Going? Imitative Decision Learning for Path Forecasting in Dynamic Scenes, CVPR 2019. [paper]\nOvercoming Limitations of Mixture Density Networks: A Sampling and Fitting Framework for Multimodal Future Prediction, CVPR 2019. [paper][code]\nSophie: An attentive gan for predicting paths compliant to social and physical constraints, CVPR 2019. [paper][code]\nPedestrian path, pose, and intention prediction through gaussian process dynamical models and pedestrian activity recognition, 2019. [paper]\nMultimodal Interaction-aware Motion Prediction for Autonomous Street Crossing, 2019. [paper]\nThe simpler the better: Constant velocity for pedestrian motion prediction, 2019. [paper]\nPedestrian trajectory prediction in extremely crowded scenarios, 2019. [paper]\nSrlstm: State refinement for lstm towards pedestrian trajectory prediction, 2019. [paper]\nLocation-velocity attention for pedestrian trajectory prediction, WACV 2019. [paper]\nPedestrian Trajectory Prediction in Extremely Crowded Scenarios, Sensors, 2019. [paper]\nA data-driven model for interaction-aware pedestrian motion prediction in object cluttered environments, ICRA 2018. [paper]\nMove, Attend and Predict: An attention-based neural model for people\u2019s movement prediction, Pattern Recognition Letters 2018. [paper]\nGD-GAN: Generative Adversarial Networks for Trajectory Prediction and Group Detection in Crowds, ACCV 2018, [paper], [demo]\nSs-lstm: a hierarchical lstm model for pedestrian trajectory prediction, WACV 2018. [paper]\nSocial Attention: Modeling Attention in Human Crowds, ICRA 2018. [paper][code]\nPedestrian prediction by planning using deep neural networks, ICRA 2018. [paper]\nJoint long-term prediction of human motion using a planning-based social force approach, ICRA 2018. [paper]\nHuman motion prediction under social grouping constraints, IROS 2018. [paper]\nFuture Person Localization in First-Person Videos, CVPR 2018. [paper]\nSocial GAN: Socially Acceptable Trajectories with Generative Adversarial Networks, CVPR 2018. [paper][code]\nGroup LSTM: Group Trajectory Prediction in Crowded Scenarios, ECCV 2018. [paper]\nMx-lstm: mixing tracklets and vislets to jointly forecast trajectories and head poses, CVPR 2018. [paper]\nIntent prediction of pedestrians via motion trajectories using stacked recurrent neural networks, 2018. [paper]\nTransferable pedestrian motion prediction models at intersections, 2018. [paper]\nProbabilistic map-based pedestrian motion prediction taking traffic participants into consideration, 2018. [paper]\nA Computationally Efficient Model for Pedestrian Motion Prediction, ECC 2018. [paper]\nContext-aware trajectory prediction, ICPR 2018. [paper]\nSet-based prediction of pedestrians in urban environments considering formalized traffic rules, ITSC 2018. [paper]\nBuilding prior knowledge: A markov based pedestrian prediction model using urban environmental data, ICARCV 2018. [paper]\nDepth Information Guided Crowd Counting for Complex Crowd Scenes, 2018. [paper]\nTracking by Prediction: A Deep Generative Model for Mutli-Person Localisation and Tracking, WACV 2018. [paper]\n\u201cSeeing is Believing\u201d: Pedestrian Trajectory Forecasting Using Visual Frustum of Attention, WACV 2018. [paper]\nLong-Term On-Board Prediction of People in Traffic Scenes under Uncertainty, CVPR 2018. [paper], [code+data]\nEncoding Crowd Interaction with Deep Neural Network for Pedestrian Trajectory Prediction, CVPR 2018. [paper], [code]\nWalking Ahead: The Headed Social Force Model, 2017. [paper]\nReal-time certified probabilistic pedestrian forecasting, 2017. [paper]\nA multiple-predictor approach to human motion prediction, ICRA 2017. [paper]\nForecasting interactive dynamics of pedestrians with fictitious play, CVPR 2017. [paper]\nForecast the plausible paths in crowd scenes, IJCAI 2017. [paper]\nBi-prediction: pedestrian trajectory prediction based on bidirectional lstm classification, DICTA 2017. [paper]\nAggressive, Tense or Shy? Identifying Personality Traits from Crowd Videos, IJCAI 2017. [paper]\nNatural vision based method for predicting pedestrian behaviour in urban environments, ITSC 2017. [paper]\nHuman Trajectory Prediction using Spatially aware Deep Attention Models, 2017. [paper]\nSoft + Hardwired Attention: An LSTM Framework for Human Trajectory Prediction and Abnormal Event Detection, 2017. [paper]\nForecasting Interactive Dynamics of Pedestrians with Fictitious Play, CVPR 2017. [paper]\nSocial LSTM: Human trajectory prediction in crowded spaces, CVPR 2016. [paper][code]\nComparison and evaluation of pedestrian motion models for vehicle safety systems, ITSC 2016. [paper]\nAge and Group-driven Pedestrian Behaviour: from Observations to Simulations, 2016. [paper]\nStructural-RNN: Deep learning on spatio-temporal graphs, CVPR 2016. [paper][code]\nIntent-aware long-term prediction of pedestrian motion, ICRA 2016. [paper]\nContext-based detection of pedestrian crossing intention for autonomous driving in urban environments, IROS 2016. [paper]\nNovel planning-based algorithms for human motion prediction, ICRA 2016. [paper]\nLearning social etiquette: Human trajectory understanding in crowded scenes, ECCV 2016. [paper][code]\nGLMP-realtime pedestrian path prediction using global and local movement patterns, ICRA 2016. [paper]\nKnowledge transfer for scene-specific motion prediction, ECCV 2016. [paper]\nSTF-RNN: Space Time Features-based Recurrent Neural Network for predicting People Next Location, SSCI 2016. [code]\nGoal-directed pedestrian prediction, ICCV 2015. [paper]\nTrajectory analysis and prediction for improved pedestrian safety: Integrated framework and evaluations, 2015. [paper]\nPredicting and recognizing human interactions in public spaces, 2015. [paper]\nLearning collective crowd behaviors with dynamic pedestrian-agents, 2015. [paper]\nModeling spatial-temporal dynamics of human movements for predicting future trajectories, AAAI 2015. [paper]\nUnsupervised robot learning to predict person motion, ICRA 2015. [paper]\nA controlled interactive multiple model filter for combined pedestrian intention recognition and path prediction, ITSC 2015. [paper]\nReal-Time Predictive Modeling and Robust Avoidance of Pedestrians with Uncertain, Changing Intentions, 2014. [paper]\nBehavior estimation for a complete framework for human motion prediction in crowded environments, ICRA 2014. [paper]\nPedestrian\u2019s trajectory forecast in public traffic with artificial neural network, ICPR 2014. [paper]\nWill the pedestrian cross? A study on pedestrian path prediction, 2014. [paper]\nBRVO: Predicting pedestrian trajectories using velocity-space reasoning, 2014. [paper]\nContext-based pedestrian path prediction, ECCV 2014. [paper]\nPedestrian path prediction using body language traits, 2014. [paper]\nOnline maneuver recognition and multimodal trajectory prediction for intersection assistance using non-parametric regression, 2014. [paper]\nLearning intentions for improved human motion prediction, 2013. [paper]\nMobile Robots\nAnticipatory Navigation in Crowds by Probabilistic Prediction of Pedestrian Future Movements, ICRA 2021. [paper]\nSocial NCE: Contrastive Learning of Socially-aware Motion Representations. [paper], [code]\nMultimodal probabilistic model-based planning for human-robot interaction, ICRA 2018. [paper][code]\nDecentralized Non-communicating Multiagent Collision Avoidance with Deep Reinforcement Learning, ICRA 2017. [paper]\nAugmented dictionary learning for motion prediction, ICRA 2016. [paper]\nPredicting future agent motions for dynamic environments, ICMLA 2016. [paper]\nBayesian intention inference for trajectory prediction with an unknown goal destination, IROS 2015. [paper]\nLearning to predict trajectories of cooperatively navigating agents, ICRA 2014. [paper]\nSport Players\nEvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational Reasoning, NeurIPS 2020. [paper]\nImitative Non-Autoregressive Modeling for Trajectory Forecasting and Imputation, CVPR 2020. [paper]\nDAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting, ICPR 2020. [paper] [code]\nDiverse Generation for Multi-Agent Sports Games, CVPR 2019. [paper]\nStochastic Prediction of Multi-Agent Interactions from Partial Observations, ICLR 2019. [paper]\nGenerating Multi-Agent Trajectories using Programmatic Weak Supervision, ICLR 2019. [paper]\nGenerative Multi-Agent Behavioral Cloning, ICML 2018. [paper]\nWhere Will They Go? Predicting Fine-Grained Adversarial Multi-Agent Motion using Conditional Variational Autoencoders, ECCV 2018. [paper]\nCoordinated Multi-Agent Imitation Learning, ICML 2017. [paper]\nGenerating long-term trajectories using deep hierarchical networks, 2017. [paper]\nLearning Fine-Grained Spatial Models for Dynamic Sports Play Prediction, ICDM 2014. [paper]\nGenerative Modeling of Multimodal Multi-Human Behavior, 2018. [paper]\nWhat will Happen Next? Forecasting Player Moves in Sports Videos, ICCV 2017, [paper]\nBenchmark and Evaluation Metrics\nOpenTraj: Assessing Prediction Complexity in Human Trajectories Datasets, ACCV 2020. [paper] [code]\nTesting the Safety of Self-driving Vehicles by Simulating Perception and Prediction, ECCV 2020. [paper]\nPIE: A Large-Scale Dataset and Models for Pedestrian Intention Estimation and Trajectory Prediction, ICCV 2019. [paper]\nTowards a fatality-aware benchmark of probabilistic reaction prediction in highly interactive driving scenarios, ITSC 2018. [paper]\nHow good is my prediction? Finding a similarity measure for trajectory prediction evaluation, ITSC 2017. [paper]\nTrajnet: Towards a benchmark for human trajectory prediction. [website]\nOthers\nCyclist trajectory prediction using bidirectional recurrent neural networks, AI 2018. [paper]\nRoad infrastructure indicators for trajectory prediction, 2018. [paper]\nUsing road topology to improve cyclist path prediction, 2017. [paper]\nTrajectory prediction of cyclists using a physical model and an artificial neural network, 2016. [paper]",
      "link": "https://github.com/jiachenli94/Awesome-Interaction-aware-Trajectory-Prediction"
    },
    {
      "autor": "zmNinja",
      "date": "NaN",
      "content": "Project Discontinuation Notice, EOY 2021\nAfter 6+ years of developing zmNinja + ES + ML I've run out of time to maintain this project. Please read this notice. This project has been transferred to ZoneMinder and ZM devs along with the community will continue to maintain it beyond 2021.\nzmNinja website\nzmNinja is a multi platform (iOS, Android, Windows Desktop, Mac Desktop, Linux Desktop) client for ZoneMinder users. ZoneMinder is an incredible open source -----> camera !!!  monitoring system and is used by many for home and commercial security monitoring.\nHelp\nOfficial documents are here\nIf you are looking for the Machine Learning powered Event Server companion, docs are here\nPost in the ZoneMinder Mobile App forum or find me on ZoneMinder's slack channel (you can join here)\nIf you believe you've discovered a bug, please create a GitHub issue\nBefore you ask for help\nMake sure you have read the FAQ\nMake sure you have validated that your APIs are working (if not, its a ZM issue, please post in ZM forums)\nPlease don't ask me for help with source compilation if you are not familiar with coding mobile apps - you should try and solve your own problems\nVideo Demo\nCheck out a video demo of zmNinja here\nMobile Platforms\nzmNinja is stable as of today and runs on a variety of Android and iOS platforms. See links above to get them on play store (Android) and app store (iOS)\nIt also runs on the desktop (see below)\nDesktop Platforms\nPlease download binaries from here.\nKey Features\nPush Notifications Object detection/face recognition for alarms (Needs the eventserver to be set up)\nMultiple languages (English, French, German, Spanish, Portugese, Dutch, and more)\nH264 video support\nlive views of monitors\nMontage view (with multiple montage profile settings/sizes)\nEvents history and list\nTimeline view\nCamera pan/tilt/zoom (needs to have ZM support it first)\n24hr review\nKey Limitations\nClient certificates are not supported\nIf you use self signed certificates, you will likely have to install them in your phone (especially Android)\nIf you use Basic Authentication, you'll have to go through additional setup (see FAQ)\nThanks\nTo the Zonemider community in general, and the awesome Stack Overflow community. But specifically, Andrew Bauer (knight-of-ni) who egged me on to take up this project and Isaac Connor who has been incredibly supportive in helping fix various ZM issues that affected zmNinja.\nImportant Notes\nzmNinja needs APIs enabled in ZoneMinder. See this for instructions on how to make sure your APIs are working. If they are not working, zmNinja will not work.\nWhy did I develop zmNinja?\nI wanted to learn how to write a mobile app. It was (and is) fun.\nI originally reached out to tinyCam to see if they were willing to do an app - did not get a response\nI found zmView limited for my needs\nScreenshots:",
      "link": "https://github.com/ZoneMinder/zmNinja"
    },
    {
      "autor": "tensorflow-101",
      "date": "NaN",
      "content": "TensorFlow 101: Introduction to Deep Learning\nI have worked all my life in Machine Learning, and I've never seen one algorithm knock over its benchmarks like Deep Learning - Andrew Ng\nThis repository includes deep learning based project implementations I've done from scratch. You can find both the source code and documentation as a step by step tutorial. Model structrues and pre-trained weights are shared as well.\nFacial Expression Recognition Code, Tutorial\nThis is a custom CNN model. Kaggle FER 2013 data set is fed to the model. This model runs fast and produces satisfactory results. It can be also run real time as well.\nWe can run emotion analysis in real time as well Real Time Code, Video\nFace Recognition Code, Tutorial\nFace recognition is mainly based on convolutional neural networks. We feed two face images to a CNN model and it returns a multi-dimensional vector representations. We then compare these representations to determine these two face images are same person or not.\nYou can find the most popular face recognition models below.\nModel Creator LFW Score Code Tutorial\nVGG-Face The University of Oxford 98.78 Code Tutorial\nFaceNet Google 99.65 Code Tutorial\nDeepFace Facebook - Code Tutorial\nOpenFace Carnegie Mellon University 93.80 Code Tutorial\nDeepID The Chinese University of Hong Kong - Code Tutorial\nDlib Davis E. King 99.38 Code Tutorial\nOpenCV OpenCV Foundation - Code Tutorial\nOpenFace in OpenCV Carnegie Mellon University 92.92 Code Tutorial\nSphereFace Georgia Institute of Technology 99.30 Code Tutorial\nArcFace Imperial College London 99.40 Code Tutorial\nAll of those state-of-the-art face recognition models are wrapped in deepface library for python. You can build and run them with a few lines of code. To have more information, please visit the repo of the library.\nReal Time Deep Face Recognition Implementation Code, Video\nThese are the real time implementations of the common face recognition models we've mentioned in the previous section. VGG-Face has the highest face recognition score but it comes with the high complexity among models. On the other hand, OpenFace is a pretty model and it has a close accuracy to VGG-Face but its simplicity offers high speed than others.\nModel Creator Code Demo\nVGG-Face Oxford University Code Video\nFaceNet Google Code Video\nDeepFace Facebook Code Video\nOpenFace Carnegie Mellon University Code Video\nLarge Scale Face Recognition\nFace recognition requires to apply face verification several times. It has a O(n) time complexity and it would be problematic for very large scale data sets (millions or billions level data). Herein, if you have a really strong database, then you use relational databases and regular SQL. Besides, you can store facial embeddings in nosql databases. In this way, you can have the power of the map reduce technology. Besides, approximate nearest neighbor (a-nn) algorithm reduces time complexity dramatically. Spotify Annoy, Facebook Faiss and NMSLIB are amazing a-nn libraries. Besides, Elasticsearch wraps NMSLIB and it also offers highly scalablity. You should build and run face recognition models within those a-nn libraries if you have really large scale data sets.\nLibrary Algorithm Tutorial Code Demo\nSpotify Annoy a-nn Tutorial - Video\nFacebook Faiss a-nn Tutorial - -\nNMSLIB a-nn Tutorial Code -\nElasticsearch a-nn Tutorial Code Video\nmongoDB k-NN Tutorial Code -\nCassandra k-NN Tutorial Code Video\nRedis k-NN Tutorial Code Video\nHadoop k-NN Tutorial Code -\nRelational Database k-NN Tutorial Code -\nNeo4j Graph k-NN Tutorial Code Video\nApparent Age and Gender Prediction Tutorial, Code for age, Code for gender\nWe've used VGG-Face model for apparent age prediction this time. We actually applied transfer learning. Locking the early layers' weights enables to have outcomes fast.\nWe can run age and gender prediction in real time as well Real Time Code, Video\nCelebrity You Look-Alike Face Recognition Code, Tutorial\nApplying VGG-Face recognition technology for imdb data set will find your celebrity look-alike if you discard the threshold in similarity score.\nThis can be run in real time as well Real Time Code, Video\nRace and Ethnicity Prediction Tutorial, Code, Real Time Code, Video\nEthnicity is a facial attribute as well and we can predict it from facial photos. We customize VGG-Face and we also applied transfer learning to classify 6 different ethnicity groups.\nBeauty Score Prediction Tutorial, Code\nSouth China University of Technology published a research paper about facial beauty prediction. They also open-sourced the data set. 60 labelers scored the beauty of 5500 people. We will build a regressor to find facial beauty score. We will also test the built regressor on a huge imdb data set to find the most beautiful ones.\nAttractiveness Score Prediction Tutorial, Code\nThe University of Chicago open-sourced the Chicago Face Database. The database consists of 1200 facial photos of 600 people. Facial photos are also labeled with attractiveness and babyface scores by hundreds of volunteer markers. So, we've built a machine learning model to generalize attractiveness score based on a facial -----> photo !!! .\nMaking Arts with Deep Learning: Artistic Style Transfer Code, Tutorial, Video\nWhat if Vincent van Gogh had painted Istanbul Bosporus? Today we can answer this question. A deep learning technique named artistic style transfer enables to transform ordinary images to masterpieces.\nAutoencoder and clustering Code, Tutorial\nWe can use neural networks to represent data. If you design a neural networks model symmetric about the centroid and you can restore a base data with an acceptable loss, then output of the centroid layer can represent the base data. Representations can contribute any field of deep learning such as face recognition, style transfer or just clustering.\nConvolutional Autoencoder and clustering Code, Tutorial\nWe can adapt same representation approach to convolutional neural networks, too.\nTransfer Learning: Consuming InceptionV3 to Classify Cat and Dog Images in Keras Code, Tutorial\nWe can have the outcomes of the other researchers effortlessly. Google researchers compete on Kaggle Imagenet competition. They got 97% accuracy. We will adapt Google's Inception V3 model to classify objects.\nHandwritten Digit Classification Using Neural Networks Code, Tutorial\nWe had to apply feature extraction on data sets to use neural networks. Deep learning enables to skip this step. We just feed the data, and deep neural networks can extract features on the data set. Here, we will feed handwritten digit data (MNIST) to deep neural networks, and expect to learn digits.\nHandwritten Digit Recognition Using Convolutional Neural Networks with Keras Code, Tutorial\nConvolutional neural networks are close to human brain. People look for some patterns in classifying objects. For example, mouth, nose and ear shape of a cat is enough to classify a cat. We don't look at all pixels, just focus on some area. Herein, CNN applies some filters to detect these kind of shapes. They perform better than conventional neural networks. Herein, we got almost 2% accuracy than fully connected neural networks.\nAutomated Machine Learning and Auto-Keras for Image Data Code, Model, Tutorial\nAutoML concept aims to find the best network structure and hyper-parameters. Here, I've applied AutoML to facial expression recognition data set. My custom design got 57% accuracy whereas AutoML found a better model and got 66% accuracy. This means almost 10% improvement in the accuracy.\nExplaining Deep Learning Models with SHAP Code, Tutorial\nSHAP explains black box machine learning models and makes them transparent, explainable and provable.\nGradient Vanishing Problem Code Tutorial\nWhy legacy activation functions such as sigmoid and tanh disappear on the pages of the history?\nHow single layer perceptron works Code\nThis is the 1957 model implementation of the perceptron.\nFace Alignment for Face Recognition Code, Tutorial\nGoogle declared that face alignment increase its face recognition model accuracy from 98.87% to 99.63%. This is almost 1% accuracy improvement which means a lot for engineering studies.\nRequirements\nI have tested this repository on the following environments. To avoid environmental issues, confirm your environment is same as below.\nC:\\>python --version\nPython 3.6.4 :: Anaconda, Inc.\nC:\\>activate tensorflow\n(tensorflow) C:\\>python\nPython 3.5.5 |Anaconda, Inc.| (default, Apr 7 2018, 04:52:34) [MSC v.1900 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow as tf\n>>> print(tf.__version__)\n1.9.0\n>>>\n>>> import keras\nUsing TensorFlow backend.\n>>> print(keras.__version__)\n2.2.0\n>>>\n>>> import cv2\n>>> print(cv2.__version__)\n3.4.4\nTo get your environment up from zero, you can follow the instructions in the following videos.\nInstalling TensorFlow and Prerequisites Video\nInstalling Keras Video\nDisclaimer\nThis repo might use some external sources. Notice that related tutorial links and comments in the code blocks cite references already.\nSupport\nThere are many ways to support a project - starring\u2b50\ufe0f the GitHub repos is one.\nCitation\nPlease cite tensorflow-101 in your publications if it helps your research. Here is an example BibTeX entry:\n@misc{serengil2021tensorflow,\nabstract = {TensorFlow 101: Introduction to Deep Learning for Python Within TensorFlow},\nauthor = {Serengil, Sefik Ilkin},\ntitle = {tensorflow-101},\nhowpublished = {https://github.com/serengil/tensorflow-101},\nyear = {2021}\n}\nLicence\nThis repository is licensed under MIT license - see LICENSE for more details",
      "link": "https://github.com/serengil/tensorflow-101"
    },
    {
      "autor": "Android-TensorFlow-Lite-Example",
      "date": "NaN",
      "content": "Android TensorFlow Lite Machine Learning Example\nAbout Android TensorFlow Lite Machine Learning Example\nThis is an example project for integrating TensorFlow Lite into Android application\nThis project include an example for object detection for an image taken from -----> camera !!!  using TensorFlow Lite library.\nRead this article. It describes everything about TensorFlow Lite for Android.\nFind this project useful ? \u2764\ufe0f\nSupport it by clicking the \u2b50 button on the upper right of this page. \u270c\ufe0f\nCredits\nThe classifier example has been taken from Google TensorFlow example.\nCheck out Mindorks awesome open source projects here\nLicense\nCopyright (C) 2018 MINDORKS NEXTGEN PRIVATE LIMITED\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\nhttp://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\nContributing to Android TensorFlow Lite Machine Learning Example\nJust make pull request. You are in!",
      "link": "https://github.com/amitshekhariitbhu/Android-TensorFlow-Lite-Example"
    },
    {
      "autor": "enclosure-picroft",
      "date": "NaN",
      "content": "It's Alive!\nPicroft - Buster Keaton (Pork Pi) release\nPicroft is an enclosure for a Raspberry Pi 3, 3B+ or 4 connected to a speaker and microphone, bringing Mycroft to anyone who wants a simple voice interface they have complete control over. This is built on top of the official Raspbian Buster Lite image.\nThe entire project is available as a pre-built micro-SD image ready to be burned and placed into a Raspberry Pi. You can download the pre-built image here:\nPicroft Stable 2020-09-07 image\nSHA-256: 3d3c99c53793224b84de02e816435230f2bbe6272b71d8909afee43e5ef9a402\nPicroft Release Candidate 2021-06-04 image\nSHA-256: 2d5a81375036a1dc4ce30051ea9e9181f4019c8f61264db3a43d87d03df854a6\nA list of current and past releases can be found at: https://downloads.mycroft.ai/releases/picroft/\nOptionally you can build it yourself by following the Recipe for building the image\nRequirements\nRaspberry Pi 3, 3B+, or 4\nOlder Raspberry Pi versions do not have sufficient processing power, and if they work they will be very slow\nSpeaker\nAny analog speaker, or an HDMI monitor with speaker\nMicrophone\nA list of Community tested hardware is available in our documentation.\n2.5 Amp or better power supply\nDon't skimp on this! It might appear to work, but you'll have weird issues with a cheapo supply.\nMicroSD Card\n8 GB or larger\nHDMI Monitor and keyboard, only required during setup\nInstallation\nDownload and burn the image to the SD card.\nSee the RaspberryPi.org's guide to Installing Operating System Images for detailed instructions on how to burn an image to your SD card.\nInsert the SD card into your Raspberry Pi\nConnect speaker, microphone, monitor and keyboard\nApply power\nFollow the on-screen prompts to setup Picroft\nFollow the verbal prompts to pair your device to an account at Mycroft Home\nTalk to Mycroft and enjoy!\nUsage\nSimply speak to Picroft as you would to any Mycroft. For example:\n\"Hey Mycroft, what time is it?\"\n\"Mycroft, how tall was Abraham Lincoln?\"\nOlder Versions\nRaspbian Jessie version\nHelp and more info\nTo re-run the setup wizard, use mycroft-setup-wizard. Check out the Picroft wiki here. There's also the general Documentation.\nCustomization\naudio_setup.sh configures your specific audio setup.\ncustom_setup.sh is a stub meant to initialize anything before Mycroft starts. For example, initializing connected devices, or launching services.\nGetting Help\nThere is an active Picroft community within the Mycroft's Mattermost chat which all are welcome to join!\nFAQ\nQ1) Why \"Buster Keaton (Pork Pi)\"?\nThis image is built on top of Raspbian \"Buster\" for running on a Raspberry Pi.\nBuster Keaton is best known for his silent films, but also his signature Pork Pie hats that he designed and made himself. Coincidentally, Buster Keaton's classic -----> film !!!  \"Safety Last!\" came out of copyright in 2019.\nThe photo at the top of this page we thought was Buster Keaton wearing his signature hat. It turns out that it is actually a slightly more contemporary Harold Lloyd.\nQ2) Can I run this with a the Raspbian desktop GUI?\nSadly, not really. A Raspberry Pi is powerful, but still not well suited to do everything at once. You can add other basic services on top of Picroft, but the desktop GUI requires too many additional resources and neither Mycroft nor the GUI end up running well.\nQ3) Can I run this with anything else?\nDepends on what you want to add. Serving simple webpages or polling devices periodically is probably fine. Mining bitcoin won't.\nQ4) Can I run this on a Pi4?\nYes!",
      "link": "https://github.com/MycroftAI/enclosure-picroft"
    }
  ]
}

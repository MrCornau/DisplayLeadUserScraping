{"interestingcomments": [{"autor": "deniz_sen", "date": "2020-04-17 14:18:37", "content": "[Question] Are CNNs always permutation invariant ? /!/ Hey folks, \n\nI am currently working with Multi Layer Perceptrons and Convolutional Neural Networks for -----> image !!!  classification of the MNIST handwritten numbers data set. \n\nI trained both the MLP and the CNN on the permuted data, in which I have permuted all 28x28 pixels of each image. I then retrained both networks. In theory  the MLP should have an accuracy similar to the original not permuted image set (which the results support) and the CNN should perform worse ( which the results do not support)\n\nSo when examining results , the CNN does not perform worse on the permuted vs original not permuted image data. I was curious if it could be because the images are rather small 28x28 and have only one color Chanel since they are grayscale.\n\nAny idea on why the CNN performs equally as good on original vs permuted data? It should not be permutation invariant in theory...", "link": "https://www.reddit.com/r/neuralnetworks/comments/g32uz2/question_are_cnns_always_permutation_invariant/"}, {"autor": "__GG", "date": "2020-03-26 22:32:06", "content": "Incredibly disappointed in the USFCA Deep Learning Part 1 certificate (fast.ai in person course) /!/ Sorry if this post is out of the usual here, but I need to vent. I am withdrawing after two lectures (net loss of $600 dollars, $1400 refunded). The short of it, if you are considering taking this course, wait for the free version. There is zero advantage to paying for it.\n\nSpecific complaints - \n\n* Questions are answered by committee. You need 5+ \"likes\" on the forum to get a response. This means a lot of questions are not answered.\n\n* The syllabus was not offered ahead of time. It was not until the second lecture that it became clear that this course was a hybrid of a deep learning course and machine learning course. It also sounded like other students expected the full fast.ai book to be covered, but we are only doing a few chapters. \n\n* Videos are very rough. They are live streaming them with the intention of editing them down into better videos for the free course. This means in the second lecture we lost an hour (of a 2.5 hour class) to things that weren't Deep Learning or ML. So you pay for early access to a worse version, this does not make much sense to me.\n\n* My biggest complaint is there isn't really a professor or knowledgeable TA that you can ask \"how would I ...\" questions to. I have some ideas I would like to implement that are outside the scope of \"turn it into an -----> image !!!  and use resnet on it\", but these questions do not get voted up.\n\n* It did not seem like it would be graded on anything. It is a certificate course, but there wasn't any project or assignment. Just sit through lectures (no attendance) and run some notebooks (no accountability). What is this certifying that you have done?\n\nI have done an undergrad and graduate degree and this is definitely bottom 3 in terms of classes I've taken. It is mostly due to the forum format and the lack of an expert to speak to. Jeremy is obviously very good at what he does, and the free (fast.ai) resource is amazing, but some rough livestreams and a forum are not worth $2000. Really just hire a couple TAs for students to talk to and this would have been fine.", "link": "https://www.reddit.com/r/neuralnetworks/comments/fpl7l4/incredibly_disappointed_in_the_usfca_deep/"}, {"autor": "TDuncker", "date": "2020-03-20 07:56:11", "content": "GAN, CNN or Autoencoder to reconstruct missing ECG leads? /!/ I've been looking around on what type of network to recreate a signal(ECG) based on a different signal of the same type(one ECg lead to another ECG lead). Thus, it's not about upscaling or denoising. On the below -----> picture !!! , I have marked 4 red fields. That's the data I have and I intend to recreate the entire picture (probably column 1, then column 2). \n\nThough, when reading around in articles, I can't judge well why I would choose GAN, CNN or Autoencoders over one another (or a whole different type of network?). My intuition says GAN, because of what I've seen people use it to when recreating something completely new. But I have also seen just a single article try the same as me, and they used a CNN.\n\nhttps://cdn.discordapp.com/attachments/288390761509027842/688038783395823654/unknown.png", "link": "https://www.reddit.com/r/neuralnetworks/comments/flr6tk/gan_cnn_or_autoencoder_to_reconstruct_missing_ecg/"}, {"autor": "Freeme62410", "date": "2020-03-13 18:33:03", "content": "Help Needed - Image Labeling Bounty - Compensation Available /!/ &amp;#x200B;\n\nhttps://preview.redd.it/o4r0i4qkhhm41.png?width=740&amp;format=png&amp;auto=webp&amp;s=69cc794cdbf83d45e408578b2f772d9136bab21a\n\nGreetings! We are offering the opportunity for AI, Machine Learning, and Data Science enthusiasts and professionals to join our -----> image !!!  tagging bounty. It is completely free to join, and you can earn compensation (paid in cryptocurrency) for completing the bounty. No previous technical experience is required.  \n\nFor this challenge, we are asking the community to appropriately classify a simple image based on what you see within the image. Each image will only a few seconds to complete, and the entire bounty can be completed within a few hours.  You will simply draw a box around the object pictured in the image, and then select from a drop down menu what you see in the image. We would love it if you joined us!  \n\n**Your time is valuable, and that's why we're giving 15,000 KAT to every participant who successfully completes the bounty.**  \n\n**\ud83e\udd16Bounty Details**\n\n\ud83d\udc49Registration begins: 7:30 PM UTC+7, March 13th, 2020 \n\n\ud83d\udc49Registration closes: 11:59 PM UTC+7 March 27th, 2020 \n\n\ud83d\udc49Labeling Period Begins: 00:00 AM UTC+7, March 28th, 2020\n\n\ud83d\udc49Labeling Period Deadline: 11:59 PM UTC+7, April 12th, 2020  \n\n&amp;#x200B;\n\n\u270d\ufe0fTo sign up, visit the official bounty page by visiting  [https://app.kambria.io/bounty/5e68936c54c01e8af7952989](https://app.kambria.io/bounty/5e68936c54c01e8af7952989) \n\nIf you would like to participate in this bounty, please register and sign up at your earliest convenience. **Spots are limited, so do not wait!**", "link": "https://www.reddit.com/r/neuralnetworks/comments/fi4d3w/help_needed_image_labeling_bounty_compensation/"}, {"autor": "DM-Norritt", "date": "2020-03-04 20:27:06", "content": "Seeking Microscopic -----> Image !!!  Classifier /!/ I have been taking pictures with my hobby-level microscope for a couple of years. Each time, I have to refer to online sources to first classify, then identify all of the crazy junk I put under the lens. I have also been interested in CNNs, and open source deep learning platforms like MobileNet, and ImageNet. \n\nI am an amateur programmer, familiar with flavors of C, java, and various scripting languages. I have tinkered with some fun little neural networks in Processing. I am confident that I can hack together something that will work, but I want to focus my efforts.\n\nI would like to take the core of the MobileNet classifier, and train it on microscopic images. First on categories, like Diatom, Pollen, Bacteria, etc. Eventually I would continue the training with additional shapes, and classification categories. The goal would be to feed in an image, and get a list of cropped images out along with the classification that the AI chose.\n\nWhere do I start with this? Are there better open source packages that are pre-trained on a microscopic image database?\n\nThanks for your time!", "link": "https://www.reddit.com/r/neuralnetworks/comments/fdj59v/seeking_microscopic_image_classifier/"}, {"autor": "ambuje12", "date": "2020-03-02 06:13:17", "content": "[D] Sentiment analysis -----> image !!!  /!/ Hi , \nI am trying sentiment analysis of images.\nI have 4 classes - Hilarious , funny very funny not funny.\nI tried pre trained models like VGG16/19 densenet201 but my model is overfitting getting training accuracy more than 95% and testing around 30 \nCan someone give suggestions what else I can try?\nTraining images - 6K", "link": "https://www.reddit.com/r/neuralnetworks/comments/fc7sjb/d_sentiment_analysis_image/"}, {"autor": "easycub2", "date": "2020-02-26 13:42:05", "content": "Tesseract or neural network to identify handwritten and computer types images? /!/ I am wanting to use my computer to recognize numbers from images.\n\nThose images are both computer generated (for example a screenshot of a number) or handwritten (for example a -----> picture !!!  of some handwriting on as piece of paper).\n\nI am somewhat familiar with using tensorflow + keras with python to build a model to recognize hand written digits. And I have also briefly used tessaract for OCR purposes.\n\n**Question:**\n\n&gt;Is it possible to use one method that suited to recognizing both hand written and computer generated numbers? If so which one?", "link": "https://www.reddit.com/r/neuralnetworks/comments/f9tdwc/tesseract_or_neural_network_to_identify/"}, {"autor": "PowerOfLove1985", "date": "2020-02-26 11:25:22", "content": "FastMRI leverages adversarial training to remove -----> image !!!  artifacts", "link": "https://www.reddit.com/r/neuralnetworks/comments/f9rt3w/fastmri_leverages_adversarial_training_to_remove/"}, {"autor": "xakumazx", "date": "2020-02-26 05:09:26", "content": "CNN - Convolution output question /!/ First time poster here.\n\nSay I have an -----> image !!!  28x28 pixels (MNIST) and two filter layers, first being 20x3x3 and second being 10x3x3.\n\nThe first convolutional operation will result in an ouput of shape 20x28x28 (if using padding), correct?\n\nThen the shape is 20x14x14 after maxpooling.\n\nThe second convolutional operation output is what shape? 10x20x14x14?", "link": "https://www.reddit.com/r/neuralnetworks/comments/f9o5df/cnn_convolution_output_question/"}, {"autor": "amin_mlm", "date": "2020-02-22 21:39:15", "content": "How to use neural network to generate a (near) real -----> image !!!  of a black hole", "link": "https://www.reddit.com/r/neuralnetworks/comments/f7zbc7/how_to_use_neural_network_to_generate_a_near_real/"}, {"autor": "Dr_Octahedron", "date": "2020-01-18 04:01:17", "content": "Looking for advice on extending the XOR neural network to learn a 2D -----> image !!!  space /!/ I've got an neural net with two input nodes and 1 output node - all with values 0-1. How can I intelligently choose the number of nodes in the hidden layers required to learn the training data?", "link": "https://www.reddit.com/r/neuralnetworks/comments/eqbjoq/looking_for_advice_on_extending_the_xor_neural/"}, {"autor": "danish-shaikh", "date": "2020-01-12 10:15:52", "content": "How to visualize the filter images Yolo Darknet /!/ I train darknet Yolo for custom -----> image !!!  data, how can I know what my model seen in my -----> image !!! s\n\n&amp;#x200B;\n\nI have weight file, cfg and lable file with me.", "link": "https://www.reddit.com/r/neuralnetworks/comments/enlvpy/how_to_visualize_the_filter_images_yolo_darknet/"}, {"autor": "RstarPhoneix", "date": "2020-01-04 17:20:33", "content": "How to make such model? /!/ I want to make a model such that when we input parent -----> image !!! s we get output as child's -----> image !!!  .\nWhat is the approach to solve this problem? What other information I should have inorder to solve this problem?", "link": "https://www.reddit.com/r/neuralnetworks/comments/ejzdhi/how_to_make_such_model/"}, {"autor": "justcosmic", "date": "2020-05-23 15:59:16", "content": "Neural network minimizes error but outputs the same values for any input /!/ I've been trying to develop a neural network that can recognize handwritten english uppercase letters. As an input it takes 28x28 -----> image !!! s that are transformed into vectors of 28\\*28 values between 0.0 and 1.0 (each value represents a pixel's intensity after converting -----> image !!!  to grayscale). These are 28\\*28 input neurons, there are one or more hidden layers, and finally 26 output neurons where each neuron represents one english character. So far so good.\n\nI've implemented my own neural network in C# which handles forward- and backpropagation using weighted sum as an input function and sigmoid as an activation function. I've tested this implementation for several problems of different scale: simple XOR function, iris flower data set and wheat seeds data set. After training for several hundred epochs, my neural network scored over 90% using a separate test set.\n\nHowever, I can't get my neural network to work with letter recognition. My training set consists of 200 images of each letter (provided as normalized vectors). The problem is that after several epochs my neural network calculates the same output for any input. However, the network still minimizes the error every iteration. It seems to me that it just found its own weird way of finding the minimum of the cost function.\n\nI've tried several solutions:\n\n* changing the number of hidden layers (1-3)\n* changing the number of neurons in hidden layers (5-1000)\n* changing the learning rate (0.005-0.9)\n* trying different activation functions (tanh, ReLU)\n* scaling images (7x7, 14x14)\n* shuffling training set\n* changing values of input vectors (0.0-1.0, -1.0-1.0)\n\nbut the problem persists.\n\nAt this point I'm starting to question that a \"classic\" implementation of a neural network like mine is capable of solving this problem reliably. I'm at a dead end and looking for any tips!\n\nI've been thinking of implementing separate neural networks for each letter so that each neural network outputs just one value - how probable it is that the input represents a given letter. However, I'm not sure if this will make any difference.", "link": "https://www.reddit.com/r/neuralnetworks/comments/gp7btx/neural_network_minimizes_error_but_outputs_the/"}, {"autor": "masesk", "date": "2020-05-21 22:39:05", "content": "I made a library to download images from a Google -----> Image !!!  search to train neural networks /!/ Well, I called it Process Google Dataset. This idea already exists, but the unique thing about my solution is that it works on any OS (that can run Chrome and Python), and it is easy to install and use. There are 2 parts to my solution. The first involves downloading a very lightweight Chrome extension, searching a term on Google Image Search, and activating the tool to parse through a search result, and downloads a list of urls of all the images it can see.\n\nThen the Python script simply loops through that list and downloads them.\n\nDownsides:\n\n1. If Google changes the layout of how they display their search results, I would need to update my tool.\n\nFuture Improvements:\n\n1. Telling the tool how many pictures to get (currently, it won't stop until you tell it to).\n2. Sometimes images are in base64 format (from source), so either skip those or encode them in the Python script.\n\nAny feedback is appreciate :)\n\nRepo URL: [https://github.com/masesk/process-google-dataset](https://github.com/masesk/process-google-dataset)", "link": "https://www.reddit.com/r/neuralnetworks/comments/go6zuk/i_made_a_library_to_download_images_from_a_google/"}, {"autor": "connor-owen", "date": "2020-05-20 00:43:53", "content": "I hate English essays and creative writing and want to make an artificial learning program to do it for me. Is this possible, if so how hard would it be? Here is a -----> picture !!!  of my friends cat for you to enjoy.", "link": "https://www.reddit.com/r/neuralnetworks/comments/gn17df/i_hate_english_essays_and_creative_writing_and/"}, {"autor": "grid_world", "date": "2020-05-15 13:40:51", "content": "Image Segmentation /!/ Hey Guys, I have a question: I have a dataset having many images. Where each -----> image !!!  has labels to explain it's components. An example of such an image is attached.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/nmdcgcntmxy41.jpg?width=1006&amp;format=pjpg&amp;auto=webp&amp;s=232e365a150159e8742a1f2720e213134b5cfc59\n\nI know about Conv nets for image classification but have little idea of how to approach this problem. Should  I be looking into image segmentation? Can you please provide tutorials where I can read sample code and then experiment? I am using Python 3.7 and TensorFlow 2.0.\n\nThanks!", "link": "https://www.reddit.com/r/neuralnetworks/comments/gk9b4a/image_segmentation/"}, {"autor": "ChristianVibesonly", "date": "2020-05-14 06:03:10", "content": "Hi every one I am a newbie and I have a question. /!/ For context:\nMy brother is a 19 y/o photographer with 3 diplomas recognized in the whole of the EU. \nThis morning after I posted a -----> picture !!!  of the view found just outside my home(attached here) I got a message from Google saying: \n\"Your new panorama is done\" and I didn't know what it was about and I open it and I was shocked. It was so much better than what I made(also attached). \nThe question is:\nIs it possible to make a deep learning program that analyses photos and makes them better than my brother can? And if so is anyone willing to help me do it?", "link": "https://www.reddit.com/r/neuralnetworks/comments/gjgij3/hi_every_one_i_am_a_newbie_and_i_have_a_question/"}, {"autor": "Yogi_DMT", "date": "2020-06-26 15:45:15", "content": "Thoughts on a few recent papers that could be useful in neural network tabular regression /!/ I've been reading up on a some of the newer papers and have flagged a few that seem interesting to me. Specifically papers that pertain to general/tabular regression Neural Network performance and not just specifically related to -----> image !!!  or text recognition. Was wondering if anyone has had any experience with these new tools yet? Is my understanding of them correct? Any other advancements along the same line that you think would be worth mentioning?\n\n* Mish - [\\[1908.08681\\] Mish: A Self Regularized Non-Monotonic Neural Activation Function](https://arxiv.org/abs/1908.08681) \\- Shows improvement over swish in some cases. 1st and 2nd order derivatives have slightly different properties. Computationally faster than GELU.\n* Lisht - [\\[1901.05894\\] LiSHT: Non-Parametric Linearly Scaled Hyperbolic Tangent Activation Function for Neural Networks](https://arxiv.org/abs/1901.05894)\n* Gradient centralization - [\\[2004.01461\\] Gradient Centralization: A New Optimization Technique for Deep Neural Networks](https://arxiv.org/abs/2004.01461) \\- Basically just makes the mean of all weights excluding output layer to 0\n* E-Swish - [\\[1801.07145\\] E-swish: Adjusting Activations to Different Network Depths](https://arxiv.org/abs/1801.07145) \\- Adds a term to swish and shows that in some cases a parameterized swish provides better results.\n* Radam - [\\[1908.03265\\] On the Variance of the Adaptive Learning Rate and Beyond](https://arxiv.org/abs/1908.03265) \\- Uses smaller learning rates in first few epochs essentially just to get enough samples so that the variance of adaptive LR doesn\u2019t explode and causes detrimental learn rates in the first few steps.\n* Lookahead - [\\[1907.08610\\] Lookahead Optimizer: k steps forward, 1 step back](https://arxiv.org/abs/1907.08610) \\- maintains two optimizers/weights, one with a larger and more reactive learn rate, one with a smaller more stable learn rate. Every X epochs, the \u201cslow\u201d weights are set to the \u201cfast\u201d weights.\n* Mixup - [\\[1710.09412\\] mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412) \\- \u201caverages\u201d samples as a form of data augmentation. Ex. create a linear interpolation between 2 samples so that if in production the model sees a sample \u201cin the middle\u201d of two samples it has trained on, it\u2019s output will probably be somewhere in the middle of the corresponding outputs.\n* [\\[1711.00489\\] Don't Decay the Learning Rate, Increase the Batch Size](https://arxiv.org/abs/1711.00489) \\- Increase batch size instead of decreasing learn rate. Supposed to stabilize learning, smaller updates. How to scale down something like weight-decay? Is this just an alternative that is faster for training, or is it also supposed to provide better performance?", "link": "https://www.reddit.com/r/neuralnetworks/comments/hg9zpx/thoughts_on_a_few_recent_papers_that_could_be/"}, {"autor": "GangLiu", "date": "2020-06-21 12:53:48", "content": "Perfect artificial neuron, maybe perfect most ANNs. It may be time to perfect the neuron of artificial neural network /!/   \n\n[https://www.techrxiv.org/articles/It\\_may\\_be\\_time\\_to\\_perfect\\_the\\_neuron\\_of\\_artificial\\_neural\\_network/12477266](https://www.techrxiv.org/articles/It_may_be_time_to_perfect_the_neuron_of_artificial_neural_network/12477266)\n\nAs we know, artificial neural networks are inspired by biological neural networks. Seventy years ago, people designed artificial neurons by imitating the knowledge about biological neurons at that time. Today, due to the development of biology, we have a relatively good understanding of how the work of neurons, especially dendrites.\n\nI found, at the time of design, the traditional artificial neurons ignored a fact that dendrites participate in pre-calculation in a biological neuron or biological neural network.\n\nMore specifically, biological dendrites play a role in the brain pre-processing to the interaction information of input data. This can be illustrated briefly by two tasks in life. For understanding a -----> picture !!!  task, biological dendrites play a role in extracting the relationship across parts of an input------> picture !!! . For understanding an article or a speech task, biological dendrites play a role in extracting the relationship across parts in an input-word.\n\nTraditional artificial neurons are insufficient in extracting the interaction information of input data. Thus we have designed a lot of convolutional layers. Gang neurons maybe reduce the number of layers in an existing network for the same task.", "link": "https://www.reddit.com/r/neuralnetworks/comments/hd6gub/perfect_artificial_neuron_maybe_perfect_most_anns/"}, {"autor": "GangLiu", "date": "2020-06-21 12:34:11", "content": "Perfect artificial neuron, maybe perfect most ANNs. It may be time to perfect the neuron of artificial neural network /!/ [https://www.techrxiv.org/articles/It\\_may\\_be\\_time\\_to\\_perfect\\_the\\_neuron\\_of\\_artificial\\_neural\\_network/12477266](https://www.techrxiv.org/articles/It_may_be_time_to_perfect_the_neuron_of_artificial_neural_network/12477266)\n\n&amp;#x200B;\n\nAs we know, artificial neural networks are inspired by biological neural networks. Seventy years ago, people designed artificial neurons by imitating the knowledge about biological neurons at that time. Today, due to the development of biology, we have a relatively good understanding of how the work of neurons, especially dendrites.\n\nI found, at the time of design, the traditional artificial neurons ignored a fact that dendrites participate in pre-calculation in a biological neuron or biological neural network.\n\nMore specifically, biological dendrites play a role in the brain pre-processing to the interaction information of input data. This can be illustrated briefly by two tasks in life. For understanding a -----> picture !!!  task, biological dendrites play a role in extracting the relationship across parts of an input------> picture !!! . For understanding an article or a speech task, biological dendrites play a role in extracting the relationship across parts in an input-word.\n\n&amp;#x200B;\n\nTraditional artificial neurons are insufficient in extracting the interaction information of input data. Thus we have designed a lot of convolutional layers. Gang neurons maybe reduce the number of layers in an existing network for the same task.", "link": "https://www.reddit.com/r/neuralnetworks/comments/hd67x1/perfect_artificial_neuron_maybe_perfect_most_anns/"}, {"autor": "jollyspiffing", "date": "2020-06-20 21:44:07", "content": "Where to start training an NN for a card game? /!/ I'm writing my first NN to play a simple card game, I'm familiar with Python and have implemented a working version of the game, but I'm really struggling to find understandable explanations of how to train a NeuralNet to play. I'm trying to set things up in pytorch and I think I want an adversarial neural net (?) but the example given here (https://pytorch.org/tutorials/beginner/fgsm_tutorial.html) seems a million miles away from the problem I'm trying to solve. In particular I have a discrete output (points) rather than continuous, and want to learn by competing against other 'players'\n\nI've tried to read up around the subject, but my main issue is many articles, books, papers go into depth on the mathematics and optimisation, but give few examples; while ones with examples seem to tackle very different tasks like -----> image !!!  recognition. I'm really struggling with the translation from mathematical models to code particularly as I'm not very familiar with a number of the concepts and language used. Further setting initial training parameters/architectures seems to be a huge topic with no clear answers and I'm not even sure where to start with that!\n\nI've set up the code for my game, tested it, and created some deterministic players with basic strategies (play high, random-choice, user input etc.) but have no idea where to go next. The game itself is a simplified version of [6nimmit](https://boardgamegeek.com/boardgame/432/6-nimmt) played over a series of rounds\n\n* Each player chooses 1 card from their hand (This is the bit I'm struggling with - I want an NN to make the card choice)\n* All cards are played \n* The game state updates\n* Players score points depending on changes to the game state  \n\nPoints 2,3,4 are already implemented.\n\nThe psuedo-code for my game looks something like:\n\n    class Game:\n       players = [p1, p2, p3, p4]\n       state = [[], [], [], []] # List of lists containing cards played, max 4x5 - could be trivially turned into numpy array if required\n     \n      def update_state(cards):\n            self.state = get_new_state(self.state, cards)\n            for player in self.players: \n                 player.score += get_score(self.state, player)\n            \n    class Player\n        hand = [card1, card2, card3. ...]\n        score = 0\n        def choose_card(state):\n             card = ??? # NN decision goes here, yields an integer between (0, len(hand)-1)\n             #e.g.  card = random.randint(0, len(hand)-1)  \n             return hand.pop(card)\n\n    def play_game():\n        game = Game()    \n        for turn in range(10): # Game takes place over 10 turns\n             cards_chosen = []\n             for player in game.players:\n                  card= player.choose_card(state)\n                  cards_chosen.append(card) \n             game.update_state(cards_chosen)\n         return game.player_scores\n\n    scores = play_game()\n    print(\"Winner is :\", max(player_scores))\n\n\nI've been reading this article which seems very close to what I want to do: https://arxiv.org/pdf/1808.10442.pdf and has an implementation here: https://github.com/henrycharlesworth/big2_PPOalgorithm but it relies on a particular PPONetwork implementation, which I'm struggling to wrap my head around and seems like it might be unnecessarily complicated for my use case!", "link": "https://www.reddit.com/r/neuralnetworks/comments/hcudwc/where_to_start_training_an_nn_for_a_card_game/"}, {"autor": "toothfairy222", "date": "2020-06-14 14:42:11", "content": "Beginner here , looking for an online tool to create a dataset of -----> image !!! , label ? /!/  Hi ! I hope someone can help me out because at this point I dont even know what Im doing .\n\nI am working on a small Edge detection project ( I have a background in GIS ) and this is my first time working with deep learning . I am not looking to get better at it or anything I just need to get this done for school . My goal is to compare results ( canny+segmentation VS semantic segmentation ).I will be using Unet, and my understanding is that I need to train this network using an already segmented dataset , containing 512x512 tiles of (image, label) .\n\nSo far, I have a georeferenced satellite image (3bands) with its matching vector file (where I drew the edges ) , I did rasterize the vector file to get a mask.\n\nNow Im completely lost at to how to cut my image and mask to tiles and how to label them. I found some solutions online like geo-label maker that seems appropriate for me but I would like to avoid using code (because I have to learn programming from the beginning and I dont have a lot of time to invest in programming) .\n\nSo I guess my question is , is there any online tool that will help me achieve this ? or maybe a software I can download ?\n\nPlease feel free to correct anything I just said and to give any information you judge useful . Thank you for reading .", "link": "https://www.reddit.com/r/neuralnetworks/comments/h8v0gx/beginner_here_looking_for_an_online_tool_to/"}, {"autor": "Federile", "date": "2020-06-10 05:31:07", "content": "CNN Filters /!/ I've understood how CNN works, and the functioning of the filters, does a CNN have only one filter per layer? for me, this makes just logical sense, but every time that I see a -----> picture !!!  that tries to explain it, it looks like it has multiple filters per layer", "link": "https://www.reddit.com/r/neuralnetworks/comments/h04k5i/cnn_filters/"}, {"autor": "UnaiCorzo", "date": "2020-06-08 12:45:34", "content": "Autoencoders for -----> image !!!  enhancement /!/ Can autoencoders be used to enhance and upscale -----> image !!! s instead of just compressing them (as usual)? That is, doing the opposite.\n\nThanks,", "link": "https://www.reddit.com/r/neuralnetworks/comments/gyynm6/autoencoders_for_image_enhancement/"}, {"autor": "SeicentoDriverMichai", "date": "2020-06-04 17:39:32", "content": "Best -----> image !!!  \"gap-filler\"? /!/ Hi, as the title suggests I'm in need of a good image \"gap-filler\". right now I use a gimp plugin that takes pixels from around the selected area and randomly stuffs them into the selected area, so basically when I try to fill in a gap in a brick wall I get a brick-colored mess. I heard that neural networks excel at this \"gap-filling\" in images because they can detect patterns such as stacked bricks, and in short, I'm looking for recommendations for the best program for such use.  \n\n\nTL:DR - looking for a neural network based program/plugin which I can use for filling gaps in images convincingly.", "link": "https://www.reddit.com/r/neuralnetworks/comments/gwm56y/best_image_gapfiller/"}, {"autor": "0_marauders_0", "date": "2020-06-02 21:30:50", "content": "OpenAI \u2013 Learning Dexterity End-to-End - Experiment Report /!/ Today OpenAI published a Weights &amp; Biases Report ([here](https://app.wandb.ai/openai/published-work/Learning-Dexterity-End-to-End--VmlldzoxMTUyMDQ)) on some recent work done by the Robotics team at OpenAI where they trained a policy to manipulate objects with a robotic hand in an end-to-end manner. Specifically, they solved the block reorientation task from our 2018 release \"[Learning Dexterity](https://openai.com/blog/learning-dexterity/)\" using a policy with -----> image !!!  inputs rather than training separate vision and policy models (as in the original release).\n\nIn the report they describe their experimental process in general and then detail the findings of this specific work. In particular, they contrast the use of Behavioral Cloning and Reinforcement Learning for this task, and ablate several aspects of our setup including model architecture, batch size, etc.\n\nAlex and I happy to discuss this and answer any questions about it.", "link": "https://www.reddit.com/r/neuralnetworks/comments/gvgbms/openai_learning_dexterity_endtoend_experiment/"}, {"autor": "ztay0001", "date": "2020-05-31 06:14:36", "content": "Please help in creating large dataset for training neural network. /!/ I\u2019m a university student currently working on a school project focusing on training an Artificial Intelligence to distinguish between raw shrimp, cooked shrimp, raw chicken chunks and cooked chicken chunks. I am forced to switch into this project due to the current Covid-19 situation which prevents me from accessing the university laboratory, leaving me with quarter the time other students would have for their projects. I am required to provide the AI with at least 1000 images for each of the items mentioned above in order for it to learn about their distinct features. I tried sourcing different -----> image !!! s from google and other website such as instagram with the help of tools such as -----> image !!!  scrapper but repetition and irrelevant -----> image !!! s starts to pop up around the 100th -----> image !!! . To make things worse, each group of images have a specific criteria that needs to be followed. Hence I would like to seek your kind assistance in collecting enough number of images to train the artificial intelligence using Google form. It would be nicer if someone is willing to spread this post. It only takes 4000 people to complete this mission if everyone uploaded 1 image.  \n Please snap a photo of the items listed above if the criteria listed below is satisfied. Link for submission for each item is listed below, sample images is also shown in the link provided.\n\nCooked chicken\n\nCriteria: The image of cooked chicken chunks should not include cooked shrimp, raw shrimp and raw chicken chunks as it might cause confusion. It is also important that the natural color of the cooked chicken chunks is not covered by condiments.\n\n[https://docs.google.com/forms/d/e/1FAIpQLSetz655iyw5H10nnHGnYK-sDFIm-12ZeneX3fWrVM1UTNGmfA/viewform?usp=sf\\_link](https://docs.google.com/forms/d/e/1FAIpQLSetz655iyw5H10nnHGnYK-sDFIm-12ZeneX3fWrVM1UTNGmfA/viewform?usp=sf_link)\n\nCooked shrimp\n\nCriteria: The image of cooked shrimp should not include raw shrimp, raw chicken chunks and cooked chicken chunks as it might cause confusion. It is also important that the natural color of the cooked shrimp is not covered by condiments. The shrimp should also be headless. \n\n[https://docs.google.com/forms/d/e/1FAIpQLSee4tZRV3fxCdJevs16bfYcwUlr07XYfCvquoR-qAshbZUWSA/viewform?usp=sf\\_link](https://docs.google.com/forms/d/e/1FAIpQLSee4tZRV3fxCdJevs16bfYcwUlr07XYfCvquoR-qAshbZUWSA/viewform?usp=sf_link)\n\nRaw chicken chunks\n\nCriteria: The image of raw chicken chunks should not include cooked shrimp, raw shrimp and cooked chicken chunks as it might cause confusion.\n\n[https://docs.google.com/forms/d/e/1FAIpQLSecSzQk8ho3y0ZD6TPrPfrUa3KbZgTEXzjIObMPiW9pkyqwUw/viewform?usp=sf\\_link](https://docs.google.com/forms/d/e/1FAIpQLSecSzQk8ho3y0ZD6TPrPfrUa3KbZgTEXzjIObMPiW9pkyqwUw/viewform?usp=sf_link)\n\nRaw shrimp\n\nCriteria: The image of raw shrimp should not include cooked shrimp, raw chicken chunks and cooked chicken chunks as it might cause confusion. The shrimp should also be headless. \n\n[https://docs.google.com/forms/d/e/1FAIpQLScuiQSGMM9U3cbgHdpY0ye-EzOKtUojQh3RiVeQYl-BJrKtdg/viewform?usp=sf\\_link](https://docs.google.com/forms/d/e/1FAIpQLScuiQSGMM9U3cbgHdpY0ye-EzOKtUojQh3RiVeQYl-BJrKtdg/viewform?usp=sf_link)", "link": "https://www.reddit.com/r/neuralnetworks/comments/gttum9/please_help_in_creating_large_dataset_for/"}, {"autor": "Automite", "date": "2020-05-27 04:31:49", "content": "Keras beginner with RNN problems /!/ I'm pretty new to this community and any help is appreciated.\n\nI've been trying to make a PAC MAN AI and after little success with convolutional networks I've decided to try using a recurrent neural network since it makes sense to have it try and predict the next controller input based upon the previous inputs and screen data. \n\nThe problem is I'm having a lot of trouble figuring out how to give an lstm layer -----> image !!!  data specifically a shape of (212,168,1). All of the tutorials I find are just sequences of individual numbers. \n\nIs there any good resources for this? Am I just approaching this problem incorrectly? \n\nSo far I've really been enjoying learning about keras and tensor flow, and I'm really comfortable with convolutional and dense neural networks, but I've just been stuck on recurrent neural networks all day.", "link": "https://www.reddit.com/r/neuralnetworks/comments/grctt3/keras_beginner_with_rnn_problems/"}, {"autor": "Alonenever01", "date": "2020-05-26 21:38:55", "content": "[Question] Starting my first project with Neural Network /!/ Hello,   \nas the title says i am making my first neural network, that is I have a class on my college on the subject. We were given the freedom to choose what out NN would do/learn and i thought that teaching a computer to recognise if a person is wearing a mask (medical) or not in a -----> picture !!! . I did a bit of studying and I understand the general concept behind NN but i have trouble starting to write the code behind it. So i would appreciate any tips on how to maybe start with some really simple codes to start with and build from there to the point that i need for the project...  \nThanks &lt;3", "link": "https://www.reddit.com/r/neuralnetworks/comments/gr69kx/question_starting_my_first_project_with_neural/"}, {"autor": "OnlyProggingForFun", "date": "2020-08-05 10:57:32", "content": "-----> image !!! -GPT from OpenAI can generate the pixels of half of a picture from nothing using a NLP model", "link": "https://www.reddit.com/r/neuralnetworks/comments/i437hp/imagegpt_from_openai_can_generate_the_pixels_of/"}, {"autor": "OnlyProggingForFun", "date": "2020-08-01 13:54:23", "content": "This AI can generate the pixels of half of a -----> picture !!!  from no other information using a NLP model", "link": "https://www.reddit.com/r/neuralnetworks/comments/i1skdq/this_ai_can_generate_the_pixels_of_half_of_a/"}, {"autor": "ThisVineGuy", "date": "2020-07-29 11:47:26", "content": "DeepFaceDrawing Generates Real Faces From Sketches. -----> Image !!! -to-image translation in 2020+, is it biased, could it be used in a real world application? Paper explained", "link": "https://www.reddit.com/r/neuralnetworks/comments/hzzaos/deepfacedrawing_generates_real_faces_from/"}, {"autor": "fireboltkk2000", "date": "2020-07-27 19:22:59", "content": "Which laptop should I buy? /!/ So I've been looking at a lot of laptops from a lot of companies and I'm not able to come to a conclusion about which one to buy. I want a laptop that can handle the computation power required by neural networks, especially while doing -----> image !!!  processing. And under 1000 dollars. Any suggestions?", "link": "https://www.reddit.com/r/neuralnetworks/comments/hyyx15/which_laptop_should_i_buy/"}, {"autor": "0_marauders_0", "date": "2020-07-19 18:13:48", "content": "Can we design a neural network to find a full-sized color -----> image !!!  hidden inside another -----> image !!! ?", "link": "https://www.reddit.com/r/neuralnetworks/comments/hu52pg/can_we_design_a_neural_network_to_find_a/"}, {"autor": "zimmer550king", "date": "2020-07-05 22:04:48", "content": "Going to try to train my first neural net. Need help /!/ Hello guys. I want to train a neural network for a task. I have a large amount of data but I am not sure whether my neural net architecture is appropriate or not.   \n\nLet's say I have 7000 pictures. Can I slowly increase the amount of training data and check to see if my neural net's accuracy is consistently good or not? So, for example, I try my neural net on just 1000 of those images and check how the accuracy is. Then, I try it on 5000 images and check whether the accuracy is still good or not. Is this a good approach?  \n\nAlso, my training dataset consists of -----> image !!!  crops that I took out of larger -----> image !!! s. As a result, the size varies from really small to somewhat big (they are image crops of vehicles where the vehicle can be at a considerable distance or very close in the image). My plan is to use a pre-trained Resnet-18 to extract features and then pass the extracted features through a fully connected layer. Is this a good initial approach? Can I get advice on the number of neurons in the final fully-connected layer? My goal is to pass these image crops through my neural net to get the bounding box coordinates.\n\nAny suggestions would be appreciated. Thank you!", "link": "https://www.reddit.com/r/neuralnetworks/comments/hlv7zl/going_to_try_to_train_my_first_neural_net_need/"}, {"autor": "Daniel_Rybe", "date": "2020-09-06 16:28:45", "content": "Looking for advice /!/ Looking for some help with creating a Neural Net to add greenery to a -----> photo !!! . Basically, it should be able to take a photo of some urban landscape as input and add some bushes where appropriate, or fill rooftops with grass. I'm new to neural nets, so I would be very grateful for some advice on where to start or what architectures to look into.", "link": "https://www.reddit.com/r/neuralnetworks/comments/inoz32/looking_for_advice/"}, {"autor": "SayeedAbidEfaz", "date": "2020-09-06 16:13:04", "content": "How to read -----> image !!!  data in google colab? /!/ [removed]", "link": "https://www.reddit.com/r/neuralnetworks/comments/inoovk/how_to_read_image_data_in_google_colab/"}, {"autor": "cloud_weather", "date": "2020-08-25 12:49:38", "content": "Best -----> Image !!!  Colorization AI as of 2020", "link": "https://www.reddit.com/r/neuralnetworks/comments/igbis8/best_image_colorization_ai_as_of_2020/"}, {"autor": "DrEl1344", "date": "2020-08-23 17:00:38", "content": "-----> Image !!!  data sets /!/ How do I go about getting or creating an image dataset for generative deep learning? I have seen some but they seem to be like an excel file or something.  I want to implement a GAN but can not figure this out.", "link": "https://www.reddit.com/r/neuralnetworks/comments/if6py2/image_data_sets/"}, {"autor": "OnlyProggingForFun", "date": "2020-08-22 14:14:33", "content": "Here's a new paper announced in the ECCV2020 where they proposed a new technique for 3D Human Pose and Mesh Estimation from a single RGB -----> image !!! ! (with code available)", "link": "https://www.reddit.com/r/neuralnetworks/comments/iej5xv/heres_a_new_paper_announced_in_the_eccv2020_where/"}, {"autor": "Alonenever01", "date": "2020-08-20 09:04:47", "content": "Do you have an advice how to approach this problem? /!/ So i posted here a few months ago about a project (NN) that was learning to detect if a person is wearing a mask or not from a given portrait -----> picture !!! .  Now my professor wanted to go a little bit deeper. So my new task is to make a NN model that will learn to distinct a properly worn mask from a badly worn one. That changes things i know because it is not a binary decision anymore and also harder for the program do learn to do.  \n\n\nMy question is should i make 2 NN so that one just detects the mask and if the mask is worn then send the picture to the second NN that will tell if the mask is worn properly.  \nMy second idea was to make just a more complicated  single NN with output of 3 categories.\n\nI would appreciate any advice on how to start this and what method would be more effective and/or easier to write.  \n\n\nThanks alot", "link": "https://www.reddit.com/r/neuralnetworks/comments/id6udf/do_you_have_an_advice_how_to_approach_this_problem/"}, {"autor": "cloud_weather", "date": "2020-08-17 05:10:42", "content": "-----> Image !!!  Restoration AI - Upscale and Restore Faces with DFDNet", "link": "https://www.reddit.com/r/neuralnetworks/comments/ib7zjc/image_restoration_ai_upscale_and_restore_faces/"}, {"autor": "Aioli-Pleasant", "date": "2020-08-14 14:09:28", "content": "Accurate 3D Human Pose and Mesh Estimation from a Single RGB -----> Image !!! ", "link": "https://www.reddit.com/r/neuralnetworks/comments/i9mmrh/accurate_3d_human_pose_and_mesh_estimation_from_a/"}, {"autor": "ThisVineGuy", "date": "2020-08-09 11:45:17", "content": "This AI can cartoonize any -----> picture !!!  or video you feed it! Tune in the video in caption at 3:08 to see more awesome examples using it, they passed in on The Avengers movie and the results are impressive!", "link": "https://www.reddit.com/r/neuralnetworks/comments/i6i0hg/this_ai_can_cartoonize_any_picture_or_video_you/"}, {"autor": "Yuqing7", "date": "2020-10-08 22:08:51", "content": "[R] \u2018Farewell Convolutions\u2019 \u2013 ML Community Applauds Anonymous ICLR 2021 Paper That Uses Transformers for Image Recognition at Scale /!/ A new research paper, *An -----> Image !!!  Is Worth 16\u00d716 Words: Transformers for -----> Image !!!  Recognition at Scale,* has the machine learning community both excited and curious. With Transformer architectures now being extended to the computer vision (CV) field, the paper suggests the direct application of Transformers to image recognition can outperform even the best convolutional neural networks when scaled appropriately. Unlike prior works using self-attention in CV, the scalable design does not introduce any image-specific inductive biases into the architecture.\n\nHere is a quick read: [\u2018Farewell Convolutions\u2019 \u2013 ML Community Applauds Anonymous ICLR 2021 Paper That Uses Transformers for Image Recognition at Scale](https://syncedreview.com/2020/10/08/farewell-convolutions-ml-community-applauds-anonymous-iclr-2021-paper-that-uses-transformers-for-image-recognition-at-scale/)\n\nThe paper *An Image Is Worth 16\u00d716 Words: Transformers for Image Recognition at Scale* is available on[ OpenReview](https://openreview.net/pdf?id=YicbFdNTTy).", "link": "https://www.reddit.com/r/neuralnetworks/comments/j7mnsk/r_farewell_convolutions_ml_community_applauds/"}, {"autor": "kurisutic", "date": "2020-10-01 13:03:41", "content": "Open Source Neural Compute Camera /!/ Hello everyone,\n\n&amp;#x200B;\n\nI stumbled upon this project on Kickstarter. It is an Open Source Neural Compute Camera for Computer Vision applications.\n\n&amp;#x200B;\n\n[https://www.kickstarter.com/projects/eyecloud/openncc-the-first-ai-vision-appliance?ref=c4gpoq](https://www.kickstarter.com/projects/eyecloud/openncc-the-first-ai-vision-appliance?ref=c4gpoq)\n\n&amp;#x200B;\n\nI was thinking to use such a -----> camera !!!  in my office for clocking but then I saw that there is an option to get also the OpenNCC IR+,  which has also a thermal sensor integrated. During these weird times that we live in, with COVID-19 regulations, I am thinking that such a camera that detects Faces, Face masks and Body temperature, might come in handy.  \n\n&amp;#x200B;\n\nHave a look and let me know what you think about their products. From their marketing video it looks really easy to use, implement and deploy my AI model.\n\n&amp;#x200B;\n\nI appreciate it! Cheers", "link": "https://www.reddit.com/r/neuralnetworks/comments/j385qq/open_source_neural_compute_camera/"}, {"autor": "OnlyProggingForFun", "date": "2020-09-23 11:07:32", "content": "With PULSE, you can construct a high-resolution -----> image !!!  from a corresponding low-resolution input -----> image !!!  in a self-supervised manner!", "link": "https://www.reddit.com/r/neuralnetworks/comments/iy8250/with_pulse_you_can_construct_a_highresolution/"}, {"autor": "OnlyProggingForFun", "date": "2020-09-16 10:50:55", "content": "PiFuHD: A new method for high-fidelity 3d reconstruction. It only needs a single -----> image !!!  of you to generate a 3D avatar that looks just like you, even from the back!", "link": "https://www.reddit.com/r/neuralnetworks/comments/itt55z/pifuhd_a_new_method_for_highfidelity_3d/"}, {"autor": "manoan", "date": "2020-09-11 23:26:51", "content": "What is the simplest entry into NN -----> image !!!  classification systems, as a C-callable library? /!/ I'd like to classify images using a neural net.  Not cats and dogs, but 'abstract science patterns' for want of a better description.   Can anyone suggest the simplest entry point?   I do not use Python, and I'd like to call it from another language using normal C-style calls.   OpenCL  or some other cross-platform GPU support would be a big plus.  CUDA not so much.\n\nI'd like to train it with 2 lists of images, \"Junk\" and \"Good\", and then have it classify other images into these classes.\n\nI've gone through  various lists of NNs and haven't found anything that matches the features I want.\n\nBest candidates:\n\n* **Darknet** - nice, because it is C, not C++, and has CUDA (not OpenCL?).  But it looks like it stopped being developed, and it's not a library but a command line program.   Too bad, because I suspect there might be a huge demand for a simple C-based CNN library.\n\n* **TensorFlow** - huge, byzantine, and C++, but one could write a C wrapper.  It looks like a long learning experience, and the C++ docs aren't as good as the Python ones (or weren't last time I looked)\n\n* **Keras** - friendlier front end to TensorFlow that can be used to build C++ models, that can be wrapped in C, to get where I want to be.  This was recommended to me.  This would involve training in Python, and compiling to C++ with C wrapper. \n\n* **FANN, genann, and Peter van Rossum's Lightweight Neural Network** - C, but probably not usable for image classification.\n\n\n\nCan anyone suggest anything else, or comment on the above?  Thanks!", "link": "https://www.reddit.com/r/neuralnetworks/comments/ir1q63/what_is_the_simplest_entry_into_nn_image/"}, {"autor": "grid_world", "date": "2020-12-18 17:52:09", "content": "WARNING:tensorflow:Gradients do not exist for variables - Conv-6 CNN for CIFAR-10 /!/ I am trying to create a Conv-6 CNN for classification using CIFAR-10 dataset. This CNN uses 3 blocks, where each block has 2 conv layers followed by a max pooling layer. This is flattened and passed to two dense layers before being passed to the output layer.\n\n&amp;#x200B;\n\nThe code I have is:\n\n    # input -----> image !!!  dimensions\n    img_rows, img_cols = 32, 32\n    \n    # Load CIFAR-10 dataset-\n    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n    \n    \n    if tf.keras.backend.-----> image !!! _data_format() == 'channels_first':\n        X_train = X_train.reshape(X_train.shape[0], 3, img_rows, img_cols)\n        X_test = X_test.reshape(X_test.shape[0], 3, img_rows, img_cols)\n        input_shape = (3, img_rows, img_cols)\n    else:\n        X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 3)\n        X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 3)\n        input_shape = (img_rows, img_cols, 3)\n    \n    print(\"\\n'input_shape' which will be used = {0}\\n\".format(input_shape))\n    # 'input_shape' which will be used = (32, 32, 3)\n    \n    \n    # Convert datasets to floating point types-\n    X_train = X_train.astype('float32')\n    X_test = X_test.astype('float32')\n    \n    # Normalize the training and testing datasets-\n    X_train /= 255.0\n    X_test /= 255.0\n    \n    # Convert class vectors/target to binary class matrices or one-hot encoded values-\n    y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n    y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n    \n    print(\"\\nDimensions of training and testing sets are:\")\n    print(\"X_train.shape = {0}, y_train.shape = {1}\".format(X_train.shape, y_train.shape))\n    print(\"X_test.shape = {0}, y_test.shape = {1}\".format(X_test.shape, y_test.shape))\n    # Dimensions of training and testing sets are:\n    # X_train.shape = (50000, 32, 32, 3), y_train.shape = (50000, 10)\n    # X_test.shape = (10000, 32, 32, 3), y_test.shape = (10000, 10)\n    \n    class Conv6(Model):\n        def __init__(self, **kwargs):\n            super(Conv6, self).__init__(**kwargs)\n            self.conv1 = Conv2D(\n                filters = 64, kernel_size = (3, 3),\n                activation = 'relu', kernel_initializer = tf.initializers.GlorotNormal(),\n                strides = (1, 1), padding = 'same'\n                )\n            self.conv2 = Conv2D(\n                filters = 64, kernel_size = (3, 3),\n                activation = 'relu', kernel_initializer = tf.initializers.GlorotNormal(),\n                strides = (1, 1), padding = 'same'\n                )\n            self.pool1 = MaxPooling2D(\n                pool_size = (2, 2),\n                strides = (2, 2)\n                )\n            self.conv3 = Conv2D(\n                filters = 128, kernel_size = (3, 3),\n                activation = 'relu', kernel_initializer = tf.initializers.GlorotNormal(),\n                strides = (1, 1), padding = 'same'\n                )\n            self.conv4 = Conv2D(\n                filters = 128, kernel_size = (3, 3),\n                activation = 'relu', kernel_initializer = tf.initializers.GlorotNormal(),\n                strides = (1, 1), padding = 'same'\n                )\n            self.pool2 = MaxPooling2D(\n                pool_size = (2, 2),\n                strides = (2, 2)\n                )\n            self.conv5 = Conv2D(\n                filters = 256, kernel_size = (3, 3),\n                activation = 'relu', kernel_initializer = tf.initializers.GlorotNormal(),\n                strides = (1, 1), padding = 'same'\n                )\n            self.conv6 = Conv2D(\n                filters = 256, kernel_size = (3, 3),\n                activation = 'relu', kernel_initializer = tf.initializers.GlorotNormal(),\n                strides = (1, 1), padding = 'same'\n                )\n            self.flatten = Flatten()\n            self.dense1 = Dense(\n                units = 256, activation = 'relu',\n                kernel_initializer = tf.initializers.GlorotNormal()\n                )\n            self.dense2 = Dense(\n                units = 256, activation = 'relu',\n                kernel_initializer = tf.initializers.GlorotNormal()\n                )\n            self.op = Dense(\n                units = num_classes, activation = 'softmax'\n                )\n        \n        def call(self, inputs):\n            x = self.conv1(inputs)\n            x = self.conv2(x)\n            x = self.pool1(inputs)\n            x = self.conv3(x)\n            x = self.conv4(x)\n            x = self.pool2(x)\n            x = self.conv5(x)\n            x = self.conv6(x)\n            x = self.flatten(x)\n            x = self.dense1(x)\n            x = self.dense2(x)\n            return self.op(x)\n    \n    \n    # Initialize a Conv-6 CNN model-\n    model = Conv6()\n    \n    # Compile defined model-\n    model.compile(\n        loss=tf.keras.losses.categorical_crossentropy,\n        # optimizer='adam',\n        optimizer=tf.keras.optimizers.Adam(lr = 0.0003),\n        metrics=['accuracy']\n        )\n    \n    \n    # Define early stopping callback-\n    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n            monitor = 'val_loss', min_delta = 0.001,\n            patience = 3)\n    \n    # Train defined and compiled model-\n    history = model.fit(\n        x = X_train, y = y_train,\n        batch_size = batch_size, shuffle = True,\n        epochs = num_epochs,\n        callbacks = [early_stopping_callback],\n        validation_data = (X_test, y_test)\n        )\n\n&amp;#x200B;\n\nOn calling \"[model.fit](https://model.fit)()\", it gives me the following warnings:\n\nWARNING:tensorflow:Gradients do not exist for variables\n\n&gt;\\['conv6/conv2d/kernel:0', 'conv6/conv2d/bias:0', 'conv6/conv2d\\_1/kernel:0', 'conv6/conv2d\\_1/bias:0'\\] when minimizing the loss. WARNING:tensorflow:Gradients do not exist for variables \\['conv6/conv2d/kernel:0', 'conv6/conv2d/bias:0', 'conv6/conv2d\\_1/kernel:0', 'conv6/conv2d\\_1/bias:0'\\] when minimizing the loss.\n\n&amp;#x200B;\n\nIn spite of the warnings, the defined CNN model reaches a validation accuracy of about 72% in 9 epochs.\n\nWhy am I getting these warnings?\n\nThanks!", "link": "https://www.reddit.com/r/neuralnetworks/comments/kfpyd6/warningtensorflowgradients_do_not_exist_for/"}, {"autor": "thestorytellerixvii", "date": "2020-12-07 11:01:40", "content": "PP-YOLO object detection on New York street /!/ Object detection on a street video\n\n Model : ppyolo\\_r18vd \n\nBackbone : Resnet18\n\n Framework : fluid\n\n Input shape : 512 \u00d7 512 \n\ndetection Type : one stage detector ( using anchors)\n\n&lt;iframe width=\"560\" height=\"315\" src=\"[https://www.youtube.com/embed/yEYpeydX\\_Sw](https://www.youtube.com/embed/yEYpeydX_Sw)\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; -----> picture !!! -in------> picture !!! \" allowfullscreen&gt;&lt;/iframe&gt;", "link": "https://www.reddit.com/r/neuralnetworks/comments/k8elzx/ppyolo_object_detection_on_new_york_street/"}, {"autor": "WhiteGoldRing", "date": "2020-12-03 07:18:24", "content": "CycleGAN for non-image data /!/ Hello NN community,    \nI would like to ask for your advice regarding a certain problem, I am looking for a way to use cycleGANs on data that is not an -----> image !!!  - it's more like test results, with samples as rows and features as columns, but I would like to have a way to transform samples from one space to the other, hence the use of cycleGANs. There are known relationships between features to each other which need to be taken into account, but there is no way to spatially organize this relationship properly in lower dimensions. Is anyone familiar with a cycleGAN implementation that uses either fully-connected networks, or N-dimensional tensors as input?  \n\nThank you very much in advance", "link": "https://www.reddit.com/r/neuralnetworks/comments/k5sna9/cyclegan_for_nonimage_data/"}, {"autor": "sassysalmnder", "date": "2020-12-02 01:04:34", "content": "DCGAN still shows static noisy -----> image !!!  after 500 Epochs /!/ Hi I have been implementing a DCGAN with the help of this [Kernel](https://www.kaggle.com/mrhippo/abstract-art-dcgans)\n\nMy input image shape is (128,128,3) and i have normalized the mean according to my Tanh activation function which is :\n\nGenerator model is :\n\n    def define_generator():\n        \n        generator=Sequential()\n        generator.add(Dense(4*4*512,input_shape=[noise_shape])) \n        generator.add(Reshape([4,4,512]))\n        \n        generator.add(Conv2DTranspose(2048, kernel_size=4, strides=2, padding=\"same\"))\n        generator.add(LeakyReLU(alpha=0.2))\n        generator.add(BatchNormalization())\n        \n        generator.add(Conv2DTranspose(1024, kernel_size=4, strides=2, padding=\"same\"))\n        generator.add(LeakyReLU(alpha=0.2))\n        generator.add(BatchNormalization())\n        \n        generator.add(Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"))\n        generator.add(LeakyReLU(alpha=0.2))\n        generator.add(BatchNormalization())\n        \n        generator.add(Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"))\n        generator.add(LeakyReLU(alpha=0.2))\n        generator.add(BatchNormalization())\n    \n        #generator.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"))\n        #generator.add(LeakyReLU(alpha=0.2))\n        #generator.add(BatchNormalization())\n        \n        generator.add(Conv2DTranspose(3, kernel_size=4, strides=2, padding=\"same\",\n                                     activation='tanh'))\n        opt = Adam(lr=0.0002, beta_1=0.5)\n        generator.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n        \n        return generator\n\n&amp;#x200B;\n\nHere is the Discriminator model:\n\n    def define_discriminator():\n        \n        discriminator=Sequential()\n        discriminator.add(Conv2D(32, kernel_size=4, strides=2, padding=\"same\",input_shape=[128,128, 3]))\n        discriminator.add(Conv2D(64, kernel_size=4, strides=2, padding=\"same\"))\n        discriminator.add(LeakyReLU(0.2))\n        discriminator.add(BatchNormalization())\n    \n        discriminator.add(Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\n        discriminator.add(LeakyReLU(0.2))\n        discriminator.add(BatchNormalization())\n    \n        discriminator.add(Conv2D(256, kernel_size=4, strides=2, padding=\"same\")) \n        discriminator.add(LeakyReLU(0.2))\n        discriminator.add(Flatten())\n        \n        discriminator.add(Dropout(0.5))\n        discriminator.add(Dense(1,activation='sigmoid'))\n        opt = Adam(lr=0.0002, beta_1=0.5)\n        discriminator.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n    \n        return discriminator\n\nI am using the same Abstract Gallery dataset which is used in the kernel except i have 11k images present in my dataset with epoch of 1000 and a batch size of 64. Yet after 500th epoch, i could still see the noisy image and there is no development since the 100th epoch.\n\n&amp;#x200B;\n\nWhere Am I going wrong and how I can verify ?", "link": "https://www.reddit.com/r/neuralnetworks/comments/k4yok1/dcgan_still_shows_static_noisy_image_after_500/"}, {"autor": "hellopaperspace", "date": "2020-11-27 16:12:00", "content": "[Tutorial] How To Fine-Tune Shallow Networks with Keras /!/ This tutorial explores how to fine-tune shallow networks for relatively computationally inexpensive -----> image !!!  classification. We\u2019ll compare models with only one or two hidden layers, various different numbers of neurons, differing activation functions, etc. The shallow model is able to achieve state-of-the-art performance.\n\nThis study shows that building deeper neural networks is not always necessary; instead, it can be more important to focus on the correct number of neurons in each layer. The aim of this study is to consider an optimal number of neurons in relatively shallow networks to effectively complete various tasks, thus eradicating our necessity for heavyweight models, while reducing expensive hardware requirements and time complexity.\n\nArticle link: [https://blog.paperspace.com/fine-tuning-shallow-networks-keras/](https://blog.paperspace.com/fine-tuning-shallow-networks-keras/)\n\nRun the code on a free GPU with Gradient Community Notebooks: [https://ml-showcase.paperspace.com/projects/fine-tuning-shallow-neural-networks-with-keras](https://ml-showcase.paperspace.com/projects/fine-tuning-shallow-neural-networks-with-keras)", "link": "https://www.reddit.com/r/neuralnetworks/comments/k23z7e/tutorial_how_to_finetune_shallow_networks_with/"}, {"autor": "Snoo_85410", "date": "2020-11-27 15:14:02", "content": "(Research) Why Normalizing Flows Fail to Detect Out-of-Distribution Data by researchers from NYU - NeurIPS 2020 /!/ [Check out the paper presentation explained by the author!](https://crossminds.ai/video/5fb82261890833803bc7e7ee/)\n\n**Abstract:**\n\nDetecting out-of-distribution (OOD) data is crucial for robust machine learning systems. Normalizing flows are flexible deep generative models that often surprisingly fail to distinguish between in- and out-of-distribution data: a flow trained on pictures of clothing assigns higher likelihood to handwritten digits. We investigate why normalizing flows perform poorly for OOD detection. We demonstrate that flows learn local pixel correlations and generic -----> image !!! -to-latentspace transformations which are not specific to the target -----> image !!!  dataset. We show that by modifying the architecture of flow coupling layers we can bias the flow towards learning the semantic structure of the target data, improving OOD detection. Our investigation reveals that properties that enable flows to generate high-fidelity images can have a detrimental effect on OOD detection.\n\n**Authors:** Polina Kirichenko, Pavel Izmailov, Andrew Gordon Wilson\n\n[Code Link](https://github.com/PolinaKirichenko/flows_ood/)", "link": "https://www.reddit.com/r/neuralnetworks/comments/k22w1o/research_why_normalizing_flows_fail_to_detect/"}, {"autor": "Independent-Square32", "date": "2020-11-21 13:45:37", "content": "The latest AI algorithm to make the -----> image !!!  dance", "link": "https://www.reddit.com/r/neuralnetworks/comments/jyayxy/the_latest_ai_algorithm_to_make_the_image_dance/"}, {"autor": "hellopaperspace", "date": "2020-11-17 18:13:09", "content": "[Tutorial] Object Detection Using Mask R-CNN with TensorFlow 1.14 and Keras /!/ Mask R-CNN is state-of-the-art when it comes to object instance segmentation. This tutorial covers how to train Mask R-CNN on a custom dataset using TensorFlow 1.14 and Keras, and how to perform inference.\n\nSpecifically, the topics covered include:\n\n* Overview of the Mask\\_RCNN project\n* Preparing the model configuration parameters\n* Building the Mask R-CNN model architecture\n* Loading the model weights\n* Reading an input -----> image !!! \n* Detecting objects\n* Visualizing the results\n* Complete code for prediction\n* Preparing the training dataset\n* Preparing model configuration\n* Training Mask R-CNN with TensorFlow 1.14 and Keras\n\nLink to the article: [https://blog.paperspace.com/mask-r-cnn-in-tensorflow-2-0/](https://blog.paperspace.com/mask-r-cnn-in-tensorflow-2-0/)", "link": "https://www.reddit.com/r/neuralnetworks/comments/jvy1rp/tutorial_object_detection_using_mask_rcnn_with/"}, {"autor": "uzi_is_luv", "date": "2020-11-17 15:29:18", "content": "Tips on gathering data? /!/ I'm finishing up my undergrad and I'm doing a research project. My idea is to do -----> image !!!  classification of the main species of lake fish that live in our area. I would have to source the data on my own and just had some questions.\n\n1) How many images would I need per species? Would a 1000 be enough?\n\n2) Are there any tools that would allow me to download a lot of images for free? I've checked kaggle and there was nothing there that fits my purposes.", "link": "https://www.reddit.com/r/neuralnetworks/comments/jvuxta/tips_on_gathering_data/"}, {"autor": "martasp", "date": "2020-11-09 17:38:38", "content": "The effortless way to process images with OpenCV Canny algorithm /!/ &amp;#x200B;\n\nhttps://preview.redd.it/mtanokei39y51.png?width=597&amp;format=png&amp;auto=webp&amp;s=64b6e34af0919958fecacd6ce1d10bf454d73a74\n\nSimplicity matters in software engineering, when deploying state of the art models in your virtual machine its usually impossible, because of old libraries that do not support your environment. Luckily you can do the task in simple ways of -----> image !!!  processing with OpenCV instead of using Neural networks-based models with millions of parameters. [https://link.medium.com/fzJS85Wjhbb](https://link.medium.com/fzJS85Wjhbb)", "link": "https://www.reddit.com/r/neuralnetworks/comments/jr2625/the_effortless_way_to_process_images_with_opencv/"}, {"autor": "Old-Guidance-4652", "date": "2020-10-28 16:16:40", "content": "HELP WITH SCHOOL PROJECT /!/ Hey guys let me just start off with a description for my project. So basically we have to design a mobile app to detect diabetic foot ulcer (DFU) in an -----> image !!!  thats taken using the smartphone. I plan to create 3 different neural networks in order to achieve this task. I aim for the output of the neural network to give me a score based on how severe the ulcer is. The first neural network should give me a classification on abnormal or normal foot. The second neural network should be able to identify the region of interest on where the ulcer is on the foot. Lastly i want the last neural network to be able to classify based on what kind of ulcer is it (ie infection, ischeamia). We have acquired the dataset from a university. We had to edit the images because the images that was provided wasn't ideal for training.  We had to superimpose the image of the ulcer onto an image of a normal foot and edit it to make it look more realistic. I plan to use transfer learning on the TensorFlow model zoo pre trained models. The exact model that i am using is ssd\\_mobilenet\\_v2\\_fpnlite\\_640x640\\_coco17\\_tpu-8. Regarding the apps i have an iOS and android app which is able to upload images to firebase cloud storage. So here's what im having some questions.  \n\n\n1. Am i going on the right track to implement the tasks that i want to achieve ?\n2. How do i connect the 3 neural networks so that i can pass the image and collate the result ?\n3. Is transfer learning a good option ?\n4. How do i host 3 different neural networks and run them?\n\nI hope someone is able to help me out and i sincerely appreciate any responses. :)", "link": "https://www.reddit.com/r/neuralnetworks/comments/jjqs8o/help_with_school_project/"}, {"autor": "Standard_Reception51", "date": "2020-10-22 11:04:48", "content": "Using already trained NN /!/ Hi I am new to this and this may be a silly qiestion but what is the necesity of training a network from scratch.\n\nCan't I reuse an already trained network and simply inptut my dataset?\n\n&amp;#x200B;\n\nIn my case recognising if an -----> image !!!  is a dog.\n\n&amp;#x200B;\n\nThanks!", "link": "https://www.reddit.com/r/neuralnetworks/comments/jfxjlq/using_already_trained_nn/"}, {"autor": "MrazV", "date": "2020-07-03 06:37:58", "content": "Starting in AI /!/ Can somebody help me out, I'm interested in designing a neural network that receives pairs of -----> photo !!! s (original and with effect) and is capable of learn the effect applied to the second -----> photo !!!  and can apply this effect in any new -----> photo !!! .", "link": "https://www.reddit.com/r/neuralnetworks/comments/hke84s/starting_in_ai/"}, {"autor": "I506dk", "date": "2020-07-01 03:53:25", "content": "Predicting Stock Market Prices with a Neural Network (or attempt at least) /!/ With the rise of machine learning and neural networks, I decided to try my hand at writing one. Took me a while to decide what to make it do, and even longer to actual create it, but here we are. I decided to try and predict future stock market prices. (And before you bash me for that, you Can't predict something as volatile as the stock market. Not perfectly anyway) I figured that this would be a good learning experience, and maybe, just maybe, it would perform decently well.\n\nWith that being said, the project is also on GitHub [HERE](https://github.com/I506dk/Stock-Neural-Network). I used NVIDIA along with some other stocks during testing, but I'll only reference NVDA.\n\nSo, being that this is timeseries data, I decided to use the first 75% of the data to train the model on. The last 25% would be used for validation. Then I went through the usual process of importing the data from a text file, normalizing everything before giving it to the model, and then predicting future dates (although it is more of just getting them. You don't really have to predict tomorrow's date. Just look at a calendar).\n\nThe architecture of the model consists of a 64 node LSTM layer, a 0.5% Dropout layer, a 64 layer Dense layer, another 0.1% Dropout layer, then finally a 1 node Dense layer as output. Nothing super fancy or complicated.\n\n    # Concluded that the first dropout layer(s) need to be more harsh than the latter ones\n    Model_Input = Input(shape=(5, 1), name='Model_Input')\n    x = LSTM(64, name='Model_0')(Model_Input)\n    x = Dropout(0.5, name='Model_dropout_0')(x)\n    x = Dense(64, name='Dense_0')(x)\n    x = Dropout(0.1, name='Model_dropout_1')(x)\n    x = Activation('sigmoid', name='Sigmoid_0')(x)\n    x = Dense(1, name='Dense_1')(x)\n    Model_Output = Activation('linear', name='Linear_output')(x)\n    model = Model(inputs=Model_Input, outputs=Model_Output)\n    adam = optimizers.Adam(lr=0.0005)\n    model.compile(optimizer=adam, loss='mse')\n\nOne thing I did notice however, was over fitting (Much of it was in previous architectures). Being that this model was going to mostly like be wrong (or just not correct down to the exact penny), I wanted to be as \"accurate\" as I could make it. Hence the multiple dropout layers. But along with that I added early stopping of the epochs to try and stop it before it started to cheat or over fit.\n\n    Early_Stopping = EarlyStopping(monitor='loss', mode='min', patience=130, verbose=1)\n    \n    # x is equal to the entire dataset after being normalized\n    # Entire dataset has shape (number of days worth of data, 5 (columns), 1)\n    # y is equal to the closing prices of the dataset\n    history = model.fit(x=History_Input_Data, y=History_Closing_Price, batch_size=150, epochs=1150, shuffle=False, validation_data=(Validation_Input_Data, Validation_Closing_Price), verbose=0, callbacks=[Early_Stopping])\n\nI played with different patience values and monitoring loss and val\\_loss. The above is what I finally settled on. It performed the best when testing it on multiple stocks. I wanted the results for each stock to be relatively similar as far as accuracy.\n\nSome other pictures of stock graphs are located [HERE](https://drive.google.com/drive/folders/17P6GcquzUNjoXB4fapcqXr7FT3E6ssBc?usp=sharing). These are just the ones I was using. There is also a -----> picture !!!  of the loss (black line) and val\\_loss (green line) for the NVDA stock symbol. I graphed it while testing just to see if I was getting over fitting.\n\nOther than that there isn't much else to be said. Feel free to correct my mistakes or make suggestions. Everything is welcomed honestly.", "link": "https://www.reddit.com/r/neuralnetworks/comments/hj2v7w/predicting_stock_market_prices_with_a_neural/"}, {"autor": "OnlyProggingForFun", "date": "2020-11-25 12:53:44", "content": "This AI Can Generate the Other Half of a -----> Picture !!!  Using a GPT Model", "link": "https://www.reddit.com/r/neuralnetworks/comments/k0rnnj/this_ai_can_generate_the_other_half_of_a_picture/"}, {"autor": "OnlyProggingForFun", "date": "2020-11-24 14:06:33", "content": "Stylized Neural Painter: An -----> Image !!! -To-Painting Translation Method That Generates Vivid And Realistic Painting Artworks With Controllable Styles", "link": "https://www.reddit.com/r/neuralnetworks/comments/k05ky3/stylized_neural_painter_an_imagetopainting/"}, {"autor": "OnlyProggingForFun", "date": "2020-11-23 14:20:05", "content": "-----> IMAGE !!! -TO-PAINTING TRANSLATION WITH STYLE TRANSFER. This Image-to-Painting Translation method simulates a real painter on multiple styles using a novel approach that does not involve any GAN architecture, unlike all the current state-of-the-art approaches!", "link": "https://www.reddit.com/r/neuralnetworks/comments/jzif0w/imagetopainting_translation_with_style_transfer/"}], "name": "Subreddit_neuralnetworks_01_01_2020-30_12_2020"}